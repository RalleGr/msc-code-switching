<doc id="575" url="https://es.wikipedia.org/wiki?curid=575" title="Célula fotoeléctrica">
Célula fotoeléctrica

Una célula fotoeléctrica, también llamada celda solar, célula solar, fotocélula o célula fotovoltaica, es un dispositivo electrónico que permite transformar la energía lumínica (fotones) en energía eléctrica (flujo de electrones libres) mediante el efecto fotoeléctrico, generando energía solar fotovoltaica. Compuesto de un material que presenta efecto fotoeléctrico: absorben fotones de luz y emiten electrones. Cuando estos electrones libres son capturados, el resultado es una corriente eléctrica que puede ser utilizada como electricidad.

La eficiencia de conversión media obtenida por las células disponibles comercialmente (producidas a partir de silicio monocristalino) está alrededor del 16 %, pero según la tecnología utilizada varía desde el 6% de las células de silicio amorfo hasta el 22 % de las células de silicio monocristalino. También existen las células multicapa, normalmente de arseniuro de galio, que alcanzan eficiencias del 30 %. En laboratorio se ha superado el 46 % con células experimentales.

La vida útil media a máximo rendimiento se sitúa en torno a los 25 años, período a partir del cual la potencia entregada disminuye por debajo de un valor considerable.

Al grupo de células fotoeléctricas para energía solar se le conoce como panel fotovoltaico.
Los paneles fotovoltaicos consisten en una red de células solares conectadas como circuito en serie para aumentar la tensión de salida hasta el valor deseado (usualmente se utilizan 12 V o 24 V) a la vez que se conectan varias redes como circuito paralelo para aumentar la corriente eléctrica que es capaz de proporcionar el dispositivo.

El tipo de corriente eléctrica que proporcionan es corriente continua, por lo que si necesitamos corriente alterna o aumentar su tensión, tendremos que añadir un inversor y/o un convertidor de potencia

El efecto fotovoltaico fue experimentalmente demostrado por primera vez por el físico francés Edmond Becquerel. En 1849, a los 19 años, construyó la primera célula fotovoltaica del mundo en el laboratorio de su padre. Willoughby Smith describió por primera vez el «Effect of Light on Selenium during the passage of an Electric Current» [Efecto de la luz sobre el selenio durante el paso de una corriente eléctrica] el 20 de febrero de 1873 en la revista Nature. En 1883 Charles Fritts construyó la primera célula fotovoltaica de estado sólido mediante el recubrimiento del selenio semiconductor con una capa delgada de oro para formar las uniones; el dispositivo tuvo solo alrededor del 1% de eficiencia.

En 1888 el físico ruso Aleksandr Stoletov construyó la primera célula basada en el efecto fotoeléctrico externo descubierto por Heinrich Hertz en 1887.

En 1905 Albert Einstein propuso una nueva teoría cuántica de la luz y explicó el efecto fotoeléctrico en un artículo de referencia, por el que recibió el Premio Nobel de Física en 1921.

Vadim Lashkaryov descubrió uniones "p"-"n" en Cuformula_1O y protocélulas de sulfuro de plata en 1941.

Russell Ohl patentó la moderna célula solar en unión semiconductora en 1946 mientras trabajaba en la serie de avances que conducirían al transistor.

La primera célula fotovoltaica práctica se mostró públicamente el 25 de abril de 1954 en los Laboratorios Bell. Los inventores fueron Daryl Chapin, Calvin Souther Fuller y Gerald Pearson.

Las células solares adquirieron notoriedad con su incorporación en el satélite artificial Vanguard I en 1958, y su subsiguiente utilización en satélites más avanzados, durante la década de 1960.

Las mejoras fueron graduales durante las siguientes dos décadas. Sin embargo, este éxito fue también la razón de que los costos se mantuvieran altos, porque los usuarios de aplicaciones espaciales estaban dispuestos a pagar las mejores células posibles, sin tener ninguna razón para invertir en las de menor costo, en soluciones menos eficientes. El precio estaba determinado en gran parte por la industria de los semiconductores; su traslado a los circuitos integrados en la década de 1960 llevó a la disponibilidad de lingotes más grandes a precios relativamente más bajos. Al caer su precio, el precio de las células resultantes también lo hizo. Estos efectos bajaron los costos en 1971 a unos $ 100 por vatio.

En un semiconductor expuesto a la luz, un fotón de energía arranca un electrón, creando a la vez un «hueco» en el átomo excitado. Normalmente, el electrón encuentra rápidamente otro hueco para volver a llenarlo, y la energía proporcionada por el fotón, por tanto, se disipa en forma de calor. El principio de una célula fotovoltaica es obligar a los electrones y a los «huecos» a avanzar hacia el lado opuesto del material en lugar de simplemente recombinarse en él: así, se producirá una diferencia de potencial y por lo tanto tensión entre las dos partes del material, como ocurre en una pila.

Para ello, se crea un campo eléctrico permanente, a través de una unión pn, entre dos capas dopadas respectivamente, p y n. En las células de silicio, que son mayoritariamente utilizadas, se encuentran por tanto:



En el momento de la creación de la unión pn, los electrones libres de la capa n entran instantáneamente en la capa p y se recombinan con los huecos en la región p. Existirá así durante toda la vida de la unión, una carga "positiva" en la región n a lo largo de la unión (porque faltan electrones) y una carga "negativa" en la región en p a lo largo de la unión (porque los "huecos" han desaparecido); el conjunto forma la «Zona de Carga de Espacio» (ZCE) o "zona de barrera" y existe un campo eléctrico entre las dos, de n hacia p. Este campo eléctrico hace de la ZCE un diodo, que solo permite el flujo de portadores en una dirección: en ausencia de una fuente de corriente exterior y bajo la sola influencia del campo generado en la ZCE los electrones solo pueden moverse de la región p a la n, pero no en la dirección opuesta y por el contrario los "huecos" no pasan más que de n hacia p.

En funcionamiento, cuando un fotón arranca un electrón a la matriz, creando un electrón libre y un "hueco", bajo el efecto de este campo eléctrico cada uno va en dirección opuesta: los electrones se acumulan en la región n (para convertirse en polo negativo), mientras que los "huecos" se acumulan en la región dopada p (que se convierte en el polo positivo). Este fenómeno es más eficaz en la ZCE, donde casi no hay portadores de carga (electrones o "huecos"), ya que son anulados, o en la cercanía inmediata a la ZCE: cuando un fotón crea un par electrón-hueco, se separaron y es improbable que encuentren a su opuesto, pero si la creación tiene lugar en un sitio más alejado de la unión, el electrón (convertido en "hueco") mantiene una gran oportunidad para recombinarse antes de llegar a la zona n. Pero la ZCE es necesariamente muy delgada, así que no es útil dar un gran espesor a la célula. Efectivamente, el grosor de la capa n es muy pequeño, ya que esta capa solo se necesita básicamente para crear la ZCE que hace funcionar la célula. En cambio, el grosor de la capa p es mayor: depende de un compromiso entre la necesidad de minimizar las recombinaciones "electrón-hueco", y por el contrario permitir la captación del mayor número de fotones posible, para lo que se requiere cierto mínimo espesor.

En resumen, una célula fotovoltaica es el equivalente de un generador de energía a la que se ha añadido un diodo. Para lograr una célula solar práctica, además es preciso añadir contactos eléctricos (que permitan extraer la energía generada), una capa que proteja la célula pero deje pasar la luz, una capa antireflectante para garantizar la correcta absorción de los fotones, y otros elementos que aumenten la eficiencia del misma.

El silicio es actualmente el material más comúnmente usado para la fabricación de células fotovoltaicas. Se obtiene por reducción de la sílice, compuesto más abundante en la corteza de la Tierra, en particular en la arena o el cuarzo.

El primer paso es la producción de silicio metalúrgico, puro al 98%, obtenido de piedras de cuarzo provenientes de un filón mineral (la técnica de producción industrial no parte de la arena). El silicio se purifica mediante procedimientos químicos (Lavado + Decapado) empleando con frecuencia destilaciones de compuestos clorados de silicio, hasta que la concentración de impurezas es inferior al 0.2 partes por millón. Así se obtiene el silicio semiconductor con un grado de pureza superior al requerido para la generación de energía solar fotovoltaica. Este ha constituido la base del abastecimiento de materia prima para aplicaciones solares hasta la fecha, representando en la actualidad casi las tres cuartas partes del aprovisionamiento de las industrias.

Sin embargo, para usos específicamente solares, son suficientes (dependiendo del tipo de impureza y de la técnica de cristalización), concentraciones de impurezas del orden de una parte por millón. Al material de esta concentración se le suele denominar "silicio de grado solar".

Con el silicio fundido, se realiza un proceso de crecimiento cristalino que consiste en formar capas monomoleculares alrededor de un germen de cristalización o de un cristalito inicial. Nuevas moléculas se adhieren preferentemente en la cara donde su adhesión libera más energía. Las diferencias energéticas suelen ser pequeñas y pueden ser modificadas por la presencia de dichas impurezas o cambiando las condiciones de cristalización. La semilla o germen de cristalización que provoca este fenómeno es extraída del silicio fundido, que va solidificando de forma cristalina, resultando, si el tiempo es suficiente, un monocristal y si es menor, un policristal. La temperatura a la que se realiza este proceso es superior a los 1500 °C.

El procedimiento más empleado en la actualidad es el Proceso Czochralski, pudiéndose emplear también técnicas de colado. El silicio cristalino así obtenido tiene forma de lingotes.

Estos lingotes son luego cortados en láminas delgadas cuadradas (si es necesario) de 200 micrómetros de espesor, que se llaman «obleas». Después del tratamiento para la inyección del enriquecido con dopante (P, As, Sb o B) y obtener así los semiconductores de silicio tipo P o N.

Después del corte de las obleas, las mismas presentan irregularidades superficiales y defectos de corte, además de la posibilidad de que estén sucias de polvo o virutas del proceso de fabricación. Esta situación puede disminuir considerablemente el rendimiento del panel fotovoltaico así que se realizan un conjunto de procesos para mejorar las condiciones superficiales de las obleas tales como un lavado preliminar, la eliminación de defectos por ultrasonidos, el decapado, el pulido o la limpieza con productos químicos. Para las celdas con más calidad (monocristal) se realiza un tratado de texturizado para hacer que la oblea absorba con más eficiencia la radiación solar incidente.

Posteriormente, las obleas son «metalizadas», un proceso que consiste en la colocación de unas cintas de metal incrustadas en la superficie conectadas a contactos eléctricos que son las que absorben la energía eléctrica que generan las uniones P/N a causa de la irradiación solar y la transmiten.

La producción de células fotovoltaicas requiere energía, y se estima que un módulo fotovoltaico debe trabajar alrededor de 2 a 3 años según su tecnología para producir la energía que fue necesaria para su producción (módulo de retorno de energía).

Las técnicas de fabricación y características de los principales tipos de células se describen en los siguientes 3 párrafos. Existen otros tipos de células que están en estudio, pero su uso es casi insignificante.

Los materiales y procesos de fabricación son objeto de programas de investigación ambiciosos para reducir el costo y el reciclado de las células fotovoltaicas. Las tecnologías de película delgada sobre sustratos sin marcar recibió la aceptación de la industria más moderna. En 2006 y 2007, el crecimiento de la producción mundial de paneles solares se ha visto obstaculizado por la falta de células de silicio y los precios no han caído tanto como se esperaba. La industria busca reducir la cantidad de silicio utilizado. Las células monocristalinas han pasado de 300 micras de espesor a 200 y se piensa que llegarán rápidamente a las 180 y 150 micras, reduciendo la cantidad de silicio y la energía requerida, así como también el precio.

El silicio durante su transformación, produce un gas que se proyecta sobre una lámina de vidrio. La celda es gris muy oscuro. Es la célula de las calculadoras y relojes llamados «solares».

Estás células fueron las primeras en ser manufacturadas, ya que se podían emplear los mismos métodos de fabricación de diodos.

Al enfriarse, el silicio fundido se solidifica formando un único cristal de grandes dimensiones. Luego se corta el cristal en delgadas capas que dan lugar a las células. Estas células generalmente son de un azul uniforme.

Durante el enfriamiento del silicio en un molde, se forman varios cristales. La fotocélula es de aspecto azulado, pero no es uniforme, se distinguen diferentes colores creados por los diferentes cristales.

¿Policristalino o multicristalino? Hablamos aquí de silicio multicristalino (ref. IEC TS 61836, vocabulario fotovoltaico internacional ). El término policristalino se utiliza para las capas depositadas sobre un sustrato (granos pequeños).

Apilamiento monolítico de dos células individuales. Mediante la combinación de dos células (capa delgada de silicio amorfo sobre silicio cristalino, por ejemplo) que absorben en el espectro al mismo tiempo se solapan, mejorando el rendimiento en comparación con las células individuales separadas, sean amorfas, cristalinas o microcristalinas.

Estas células tienen una alta eficiencia y han sido desarrolladas para aplicaciones espaciales. Las células multiunión están compuestas de varias capas delgadas usando la epitaxia por haz molecular.

Un células de triple unión, por ejemplo, se compone de semiconductores "GaAs, Ge y GaInP2". Cada tipo de semiconductores se caracteriza por un máximo de longitud de onda más allá del cual no es capaz de convertir los fotones en energía eléctrica (ver banda prohibida). Por otro lado, por debajo de esta longitud de onda, el exceso de energía transportada por el fotón se pierde. De ahí el valor de la selección de materiales con longitudes de onda tan cerca el uno al otro como sea posible, de forma que absorban la mayoría del espectro solar, generando un máximo de electricidad a partir del flujo solar. El uso de materiales compuestos de cajas cuánticas permitirá llegar al 65 % en el futuro (con un máximo teórico de 87 %). Los dispositivos de células de uniones múltiples GaAs son más eficaces. Spectrolab ha logrado el 40,7 % de eficiencia (diciembre de 2006) y un consorcio (liderado por investigadores de la Universidad de Delaware) ha obtenido un rendimiento de 42,8 %(septiembre de 2007). El coste de estas células es de aproximadamente 40 $/cm².

La técnica consiste en depositar un material semiconductor que contiene cobre, galio, indio y selenio sobre un soporte.

Una preocupación, sin embargo: los recursos de materias primas. Estas nuevas técnicas utilizan metales raros, como indio, cuya producción mundial es de 25 toneladas por año y el precio a fecha de abril de 2007 es de 1000 dólares por kg; el teluro, cuya producción mundial es de 250 toneladas al año; el galio con una producción de 55 toneladas al año y el germanio con una producción de 90 toneladas al año. Aunque las cantidades de estas materias primas necesarias para la fabricación de células solares son infinitesimales, un desarrollo masivo de paneles fotovoltaicos solares debería tener en cuenta esta disponibilidad limitada.

Las células fotovoltaicas se utilizan a veces solas (iluminación de jardín, calculadoras, etc.) o agrupadas en paneles solares fotovoltaicos.

Se utilizan para reemplazar a las baterías (cuya energía es por mucho la más cara para el usuario), las células han invadido las calculadoras, relojes, aparatos, etc.

Es posible aumentar su rango de utilización almacenándola mediante un condensador o pilas. Cuando se utiliza con un dispositivo para almacenar energía, es necesario colocar un diodo en serie para evitar la descarga del sistema durante la noche.

Se utilizan para producir electricidad para muchas aplicaciones (satélites, parquímetros, etc.) y para la alimentación de los hogares o en una red pública en el caso de una central solar fotovoltaica.

Las células fotoeléctricas se clasifican en tres generaciones que indican el orden de importancia y relevancia que han tenido históricamente. En el presente hay investigación en las tres generaciones mientras que las tecnologías de la primera generación son las que más están representadas en la producción comercial con el 89.6 % de producción en 2007.

Las células de la primera generación tienen gran superficie, alta calidad y se pueden unir fácilmente. Las tecnologías de la primera generación no permiten ya avances significativos en la reducción de los costes de producción. Los dispositivos formados por la unión de células de silicio se están acercando al límite de eficacia teórica que es del 31 % y tienen un periodo de amortización de 5-7 años.

Los materiales de la segunda generación han sido desarrollados para satisfacer las necesidades de suministro de energía y el mantenimiento de los costes de producción de las células solares. Las técnicas de fabricación alternativas, como la deposición química de vapor, y la galvanoplastia tiene más ventajas, ya que reducen la temperatura del proceso de forma significativa.

Uno de los materiales con más éxito en la segunda generación han sido las películas finas de teluro de cadmio (CdTe), CIGS, de silicio amorfo y de silicio microamorfo (estos últimos consistentes en una capa de silicio amorfo y microcristalino). Estos materiales se aplican en una película fina en un sustrato de apoyo tal como el vidrio o la cerámica, la reducción de material y por lo tanto de los costos es significativa. Estas tecnologías prometen hacer mayores las eficiencias de conversión, en particular, el CIGS-CIS, el DSC y el CdTe que son los que ofrecen los costes de producción significativamente más baratos. Estas tecnologías pueden tener eficiencias de conversión más altas combinadas con costos de producción mucho más baratos.

Entre los fabricantes, existe una tendencia hacia las tecnologías de la segunda generación, pero la comercialización de estas tecnologías ha sido difícil. En 2007, First Solar produjo 200 MW de células fotoeléctricas de CdTe, el quinto fabricante más grande de células en 2007. Wurth Solar comercializó su tecnología de CIGS en 2007 produciendo 15 MW. Nanosolar comercializó su tecnología de CIGS en 2007 y con una capacidad de producción de 430 MW para 2008 en los EE. UU. y Alemania. Honda Soltec también comenzó a comercializar su base de paneles solares CIGS en 2008.

En 2007, la producción de CdTe representó 4.7 % del mercado, el silicio de película fina el 5.2 %, y el CIGS 0.5 %.

Se denominan células solares de tercera generación a aquellas que permiten eficiencias de conversión eléctrica teóricas mucho mayores que las actuales y a un precio de producción mucho menor. La investigación actual se dirige a la eficiencia de conversión del 30-60 %, manteniendo los materiales y técnicas de fabricación a un bajo costo. Se puede sobrepasar el límite teórico de eficiencia de conversión de energía solar para un solo material, que fue calculado en 1961 por Shockley y Queisser en el 31 %. Existen diversos métodos para lograr esta alta eficiencia incluido el uso de células fotovoltaicas con multiunión, la concentración del espectro incidente, el uso de la generación térmica por luz ultravioleta para aumentar la tensión, o el uso del espectro infrarrojo para la actividad nocturna.

El récord de eficiencia, sin concentración solar, está actualmente establecido en el 45 %. En concentrada, el Instituto de Tecnología de Massachusetts están probando células solares que pueden superar la eficiencia del 80 % y que están compuestas de una capa de nanotubos de carbono con cristales fotónicos, para crear un “absorbedor-emisor”.




</doc>
<doc id="576" url="https://es.wikipedia.org/wiki?curid=576" title="Célula madre">
Célula madre

Las células madre son células que se encuentran en todos los organismos pluricelulares y que tienen la capacidad de dividirse (a través de la mitosis) y diferenciarse en diversos tipos de células especializadas, además de autorrenovarse para producir más células madre. En los mamíferos, existen diversos tipos de células madre que se pueden clasificar teniendo en cuenta su potencia celular, es decir, el número de diferentes tipos celulares en los que puede diferenciarse. En los organismos adultos, las células madre y las células progenitoras actúan en la regeneración o reparación de los tejidos del organismo.

Las células madre —en inglés "stem cells" (donde "stem" significa tronco, traduciéndose a menudo como «células troncales»)— tienen la capacidad de dividirse asimétricamente dando lugar a dos células hijas, una de las cuales tiene las mismas propiedades que la célula madre original (autorrenovación) y la otra adquiere la capacidad de poder diferenciarse si las condiciones ambientales son adecuadas.

La mayoría de los tejidos de un organismo adulto, poseen una población residente de células madre adultas que permiten su renovación periódica o su regeneración cuando se produce algún daño tisular. Algunas células madre adultas son capaces de diferenciarse en más de un tipo celular como las células madre mesenquimales y las células madre hematopoyéticas, mientras que otras son precursoras directas de las células del tejido en el que se encuentran, como por ejemplo las células madre de la piel, músculo intestino o las células madre gonadales (células madre germinales).

Las células madre embrionarias son aquellas que forman parte de la masa celular interna de un embrión de 4-5 días de edad. Estas son pluripotentes lo cual significa que pueden dar origen a las tres capas germinales: ectodermo, mesodermo y endodermo. Una característica fundamental de las células madre embrionarias es que pueden mantenerse (en el embrión o en determinadas condiciones de cultivo) de forma indefinida, formando al dividirse una célula idéntica a ellas mismas, y manteniendo una población estable de células madre. Existen técnicas experimentales donde se pueden obtener células madre embrionarias sin que esto implique la destrucción del embrión.

Teniendo en cuenta su potencia, las células madre pueden clasificarse en seis tipos:

Pueden crecer y formar un organismo completo, tanto los componentes embrionarios (como por ejemplo, las tres capas embrionarias, el linaje germinal y los tejidos que darán lugar al saco vitelino), como los extraembrionarios (como la placenta). Es decir, pueden formar todos los tipos celulares. La célula madre totipotente por excelencia es el cigoto, formado cuando un óvulo es fecundado por un espermatozoide.

No pueden formar un organismo completo, pero sí cualquier otro tipo de célula correspondiente a los tres linajes embrionarios (endodermo, ectodermo y mesodermo). Pueden, por lo tanto, formar linajes celulares. Se encuentran en distintas etapas del desarrollo embrionario. Las células madre pluripotentes más estudiadas son las células madre embrionarias (en inglés "embryonic stem cells" o "ES cells") que se pueden aislar de la masa celular interna del blastocisto. El blastocisto está formado por una capa externa denominada trofoblasto, formada por unas 70 células, y una masa celular interna constituida por unas 30 células que son las células madre embrionarias que tienen la capacidad de diferenciarse en todos los tipos celulares que aparecen en el organismo adulto, dando lugar a los tejidos y órganos. En la actualidad se utilizan como modelo para estudiar el desarrollo embrionario y para entender cuáles son los mecanismos y las señales que permiten a una célula pluripotente llegar a formar cualquier célula plenamente diferenciada del organismo.

Son células madre embrionarias pluripotentes que se derivan de los esbozos gonadales del embrión. Estos esbozos gonadales se encuentran en una zona específica del embrión denominada cresta gonadal, que dará lugar a las gónadas, ovario o testículo, y a los óvulos y espermatozoides respectivamente. Tienen una capacidad de diferenciación similar a las de las células madre embrionarias, pero su aislamiento resulta más difícil.

Hoy se pueden manipular células humanas de adulto y generar células con pluripotencialidad inducida (iPS), que se ha visto poseen el mismo potencial de crecimiento y diferenciación de las células madre embrionarias, e irán sustituyendo o ampliando con creces las posibilidades biotecnológicas soñadas para las embrionarias. El compromiso de Shinya Yamanaka, diseñador de esta tecnología y ganador del premio nobel por su descubrimiento, en relación con su uso hacia otros fines, es un ejemplo de la ética y la responsabilidad del investigador y supone asumir que la ciencia triunfa al servicio del hombre. Las ventajas técnicas de las iPS son muchas, las más importantes son: no inducen rechazo inmunológico lo que abre la posibilidad de crear fármacos específicos para un paciente determinado; no requiere la utilización de óvulos humanos, la técnica es muy fácil de realizarse y su costo es reducido.

Son aquellas que solo pueden generar células de su misma capa o linaje de origen embrionario (por ejemplo: una célula madre mesenquimal de médula ósea, al tener naturaleza mesodérmica, dará origen a células de esa capa como miocitos, adipocitos u osteocitos, entre otras). Otro ejemplo son las células madre hematopoyéticas —células madre de la sangre que puede diferenciarse en los múltiples tipos celulares de la sangre—.

También llamadas células progenitoras son células madre que tiene la capacidad de diferenciarse en solo un tipo de células. Por ejemplo las células madre musculares, también denominadas células satélite solo pueden diferenciarse en células musculares.

Además de por el criterio de potencia, las células madre también pueden clasificarse en cuanto a si se encuentran en el embrión o en tejidos adultos.

Las células madre adultas se encuentran en tejidos y órganos adultos y poseen la capacidad de diferenciarse para dar lugar a células adultas del tejido en el que se encuentran. En humanos, se conocen hasta ahora alrededor de 20 tipos distintos de células madre adultas, que son las encargadas de regenerar los tejidos en continuo desgaste como la piel, la sangre, el intestino, el miocardio o bien tejidos que han sufrido un daño (como por ejemplo el hígado).

En esta clasificación se incluyen células madre unipotentes, como las células madre hematopoyéticas de la médula ósea (encargadas de la formación de la sangre). En la misma médula ósea, aunque también en sangre del cordón umbilical, en sangre periférica y en la grasa corporal se ha encontrado otro tipo de células madre adultas, denominadas mesenquimales que puede diferenciarse en numerosos tipos de células de los tres derivados embrionarios (musculares, vasculares, nerviosas, hematopoyéticas, óseas, etc.).

Estas solo pueden diferenciarse en pocos tipos de células, como las células madre linfoides o mieloides.

Existen diferentes técnicas para la obtención de células madre. Las células madre embrionarias y algunas células madre adultas pueden aislarse desde su localización original en embriones o tejidos y mantenerse en condiciones especiales de cultivo de manera más o menos indefinida. Las fuentes que se utilizan de manera rutinaria o que han empezado a postularse son:


Además de la expansión de células madre obtenidas del organismo, se han desarrollado técnicas para reprogramar células somáticas y convertirlas en células madre pluripotentes.


Del cordón umbilical se puede aislar una población de células madre multipotentes que poseen características embrionarias (expresan los factores de transcripción OCT-4 y Nanog) y hematopoyéticas (expresan el marcador de leucocitos CD45). Estas células madre adultas pueden diferenciarse en células de la sangre y del sistema inmunológico.

Las células madre del cordón umbilical son relativamente fáciles de obtener y presentan una baja inmunogenicidad, debido a la baja expresión del complejo mayor de histocompatibilidad (MHC), por lo que se han comenzado a utilizar en terapias para curar diversas enfermedades:

Además, tienen numerosas ventajas: se pueden almacenar durante 15 años aproximadamente, pueden convertirse en cualquier tipo de célula, tienen un mayor grado de aceptación en familiares que las células de la médula, no tienen virus, se obtienen de manera sencilla sin provocar dilemas éticos y el número de células obtenidas es mayor que el de las extraídas de la médula.

Gracias a los últimos avances científicos se demostró que el líquido amniótico contiene células de tejidos embrionarios y extraembrionarios diferenciadas y no diferenciadas derivadas del ectodermo, del mesodermo y del endodermo. La tipología y las características de las células del líquido amniótico varían según el momento de la gestación y en función de la existencia de posibles patologías fetales. Recientemente, se ha tenido constancia de experimentos que demuestran la presencia de células madre fetales mesenquimales con potencial diferenciador hacia elementos celulares derivados de tres hojas embrionarias, por ejemplo.

Las células madre de líquido amniótico se expanden fácilmente en cultivo, mantienen la estabilidad genética y se pueden inducir a la diferenciación (estudios de Paolo De Coppi, Antony Atala, Giuseppe Simoni, etc.) también en células hematopoyéticas. Por eso representan una nueva fuente de células que podría tener múltiples aplicaciones en ingeniería de los tejidos y en la terapia celular, sobre todo para el tratamiento de anomalías congénitas en el periodo perinatal.

Las células madre de líquido amniótico no presentan controversia ética y pueden conservarse para uso propio.

Descritas por Shi en 2003, las células madre dentales son de origen mesenquimatoso y se encuentran en la pulpa dental de los dientes primarios o permanentes lo cual las hace una fuente de fácil acceso . Además a diferencia de las células troncales de origen hematopoyético, las mesenquimatosas poseen una gran plasticidad para convertirse en células nerviosas o cardíacas lo cual ha atraído a una gran cantidad de investigadores para establecer una posible terapia genética. Diversos bancos de células madre de origen dental públicos y privados se han creado en diversos países con la finalidad de criopreservar estas células.  
En mayo de 2004, abrió en Inglaterra el primer banco de células madre del mundo. Le siguieron muchos países y antes de que terminara el año 2005, había en el mundo más de 100 bancos.

Existen diversos tipos de células madre, según el momento de vida de las células. Son dos las opciones de preservación que es necesario considerar. Estas alternativas darán a tus hijos y a ti acceso a las diversas terapias que están disponibles hoy en día y a las que lo estarán en el futuro.

El científico japonés Shinya Yamanaka, galardonado con el Premio Nobel de Medicina de 2012, advirtió en declaraciones a los periodistas de los "enormes" riesgos de ciertas "terapias con células madre" que no han sido ensayadas y que están siendo ofrecidas en las clínicas y hospitales de un número creciente de países.

Las células madre podrían tener multitud de usos clínicos y podrían ser empleadas en medicina regenerativa, inmunoterapia y terapia génica. De hecho en animales se han obtenido grandes éxitos con el empleo de células madre para tratar enfermedades hematológicas, diabetes de tipo 1, párkinson, destrucción neuronal e infartos. Pero aún en el 2012 no existían estudios concluyentes en humanos y la Agencia Española del Medicamento, dependiente del Ministerio de Sanidad, advirtió en octubre de 2012 sobre el riesgo de su uso indiscriminado.

Algunos descubrimientos médicos permiten creer que los tratamientos con células madre pueden curar enfermedades y aliviar el dolor. Existen algunos tratamientos con células madre, pero la mayoría todavía se encuentran en una etapa experimental. Investigaciones médicas anticipan que un día con el uso de la tecnología, derivada de investigaciones para las células madre adultas y embrionarias, se podrá tratar el cáncer, diabetes, lesiones de la espina dorsal y daños en los músculos, entre otras enfermedades. Muchos tratamientos prometedores para enfermedades graves han sido aplicados usando células madre adultas. La ventaja de las células madre adultas sobre las embrionarias es que no hay problema en que sean rechazadas, porque normalmente las células madre son extraídas del paciente. Todavía existe un gran problema tanto científico como ético sobre esto.

En los últimos años se está investigando en la proliferación in vitro de las células madre de cordón umbilical para aumentar el número de células madre y cubrir la necesidad para un trasplante. Estos estudios son muy prometedores y pueden permitir en un futuro utilizar células madre de cordón umbilical en terapia génica: podemos así tratar enfermedades causadas por la deficiencia o defecto de un determinado gen. Introduciendo un determinado gen en la proliferación de las células madre in vitro y trasplantar tales células en el paciente receptor. El uso de otros tipos de células como portadores de genes buenos en pacientes con enfermedades causadas por deficiencias o déficits genéticos, se está experimentando clínicamente.

Recientemente han sido utilizadas las células madre encontradas en la sangre del cordón umbilical para tratar pacientes con cáncer. Durante la quimioterapia, la mayoría de las células en crecimiento mueren por los agentes cito tóxicos. El efecto secundario de la quimioterapia es lo que los trasplantes de células madre tratan de revertir; la sustancia que se encuentra sana dentro del hueso del paciente, el tuétano, es remplazada por aquellas perdidas en el tratamiento. En la mayoría de los tratamientos actuales que usan células madre, es preferible obtenerlas de un donante con el mismo tipo de sangre a usar las del paciente mismo. Solo si es necesario usar las propias células madre (siempre como último recurso y si no se encontró un donante con el mismo tipo de sangre) y si el paciente no tiene guardada su propia colección de células madre (sangre del cordón umbilical), entonces la sustancia contenedora en los huesos será removida antes de la quimioterapia, y reinyectada después.

El trasplante de células madre hematopoyéticas se ha usado desde hace 50 años con éxito para tratar múltiples enfermedades: talasemias, anemia de células falciformes, anemia de Fanconi, errores congénitos del metabolismo, anemia aplásica grave, inmunodeficiencias combinadas graves (SCID)... También han sido empleadas para el tratamiento de tumores: leucemias agudas mieloides y linfoides, leucemias crónicas mieloides, mielodisplasias, linfomas, mielomas, tumores sólidos de riños, mama, ovario y neuroblastoma, etc. La investigación sobre las células madre surgió de los hallazgos de Ernest A. McCulloch y de James E. Till en la Universidad de Toronto en los 1960s.

Esto se consigue mediante el trasplante de médula ósea. La médula ósea contiene las células madre precursoras de las células sanguíneas y linfáticas. Se solía sacar del hueso de la cadera, pero actualmente se está sacando de la sangre periférica tras tratamiento con factores estimulantes del crecimiento. El éxito del trasplante de médula, al igual que en cualquier otro trasplante, depende de la compatibilidad HLA. Pero además de poder producirse rechazo del individuo al tejido trasplantado, el trasplante de médula ósea presenta la particularidad de que también puede darse en sentido inverso, rechazo del tejido trasplantado al individuo (GVHD: "graft versus host disease").

Sin embargo el rechazo GVHD puede presentar una ventaja y ser de interés como inmunoterapia, ya que puede reconocer a las células malignas con las que compite como extrañas y permitir una remisión más rápida de la leucemia.

Tras destruir la médula por radiación o quimioterapia se realiza el trasplante. A las dos semanas aparecen nuevas células sanguíneas y tras varios meses (autólogos) o más de un año (alotrasplantes) se restituye la función inmune.

También es posible el empleo de células madre de cordón con la misma finalidad.


El infarto agudo de miocardio pertenece a los síndromes coronarios agudos, estos se caracterizan por presenta un cuadro clínico compuesto por una afección isquémica(falta de irrigación) a alguna zona del miocardio por lo que conlleva a que se de una necrosis del mismo, esto vienen dada por una  obstrucción inicial , que puede ser aguda y total de alguna de las arterias coronarias que lo irrigan. El infarto agudo del miocardio es considerado como causa principal en muerte de hombres y mujeres a nivel mundial. Muchos de los factores coadyuvantes vienen dados por la mala alimentación y por llevar una vida muy sedentaria, se afirma que muchos problemas de alimentación se pueden evitar llevando una dieta saludable y ejercicio físico constante.


La clonación es el hecho de transferir el núcleo de una célula somática de un paciente a la célula sin núcleo de una donadora de óvulos. Esta transferencia actuará como un óvulo fecundado y comenzará con el proceso de división de la célula. Esto obviamente traerá problemas en la sociedad puesto que muchos ciudadanos piensan que no se debería "jugar a ser Dios" y crear un individuo exactamente igual a otro. Esto a su vez trae consigo diversos problemas genéticos a causa de hechos como, por ejemplo, que en las mitocondrias se encuentra el ADN de otro individuo. Se han hecho muchas investigaciones con la clonación. Sin embargo existen discrepancias en cuanto a ética y moral entre investigadores. La doctora Hwang fue una de ellas, ella donó sus óvulos para su investigación además de pedir a sus compañeras dentro de la investigación que también donaran. Esto trajo un problema ético, puesto que los investigadores no pueden recibir remuneración monetaria como ella así lo hizo, además un investigador no debe tener ningún éxito personal sino para toda la comunidad.

El hecho de que estas células actualmente implican el uso de embriones humanos y de tejido cadavérico fetal conlleva un cuidadoso examen de las cuestiones éticas relacionados con el progreso de la investigación biomédica. Contrariamente, las investigaciones médicas opinan que es necesario proceder con las investigaciones de las células madre embrionarias porque las tecnologías resultantes podrían tener un gran potencial médico, y que el exceso embrionario creado por la fertilización in vitro puede ser donado para las investigaciones. Esto en cambio, produjo conflictos con el movimiento Pro-Life (Pro-Vida), quienes adjudican la protección de embriones humanos. El constante debate ha hecho que autoridades de todo el mundo busquen regularidad en los trabajos y marquen el hecho de que las investigaciones de las células madre embrionarias representan un desafío ético y social.

De acuerdo con muchas religiones y sistemas éticos, la vida humana comienza en la fecundación. Según sus argumentos, cualquier medida intencional para detener el desarrollo después de la concepción se considera como la destrucción de una vida humana. Otros críticos no tienen un problema moral con la investigación con células madre humanas, pero tienen miedo de un precedente para la experimentación humana. Algunos críticos apoyan la idea de la investigación, pero quieren que se impongan estrictas normas legales que impidan la experimentación genética con humanos, como la clonación y que garanticen que los embriones humanos solo se obtengan a través de fuentes apropiadas. Prevenir que la investigación con células madre humanas se convierta en una pendiente resbaladiza hacia experimentos genéticos humanos es considerado por la mayoría de la sociedad un punto importante en la controversia de las células madre humanas.

Dentro de la comunidad médica, existen diferentes posturas, entre ellas que «los blastocitos o embriones son organismos vivos que dentro de 9 meses serán seres humanos con derechos, por esto, no es ético el destruir el blastocito o embrión para obtener las células madre», mientras que otros consideran que en la edad temprana de un embrión lo que se tiene es un brote de células con su masa interna.

Además de los problemas éticos que conlleva la destrucción del blastocito, también se encuentra anti-ético el hecho de que se necesiten una cantidad alta de óvulos para la creación de embriones, que serán destruidos luego, y cómo se obtienen esos óvulos. La donante de óvulos es tratada primero con algunas drogas y hormonas para que esta cree muchos óvulos que serán donados. Estas drogas pueden traer problemas de salud lo cual es anti-ético hacer daño a un paciente con conocimiento.

La finalidad natural, primaria y principal de la medicina y del progreso técnico-científico es la defensa y la protección de la vida humana. La ciencia tiene sentido en la medida que se ajusta a la ética natural salvaguardando la vida. Una ciencia sin la guía de los criterios éticos acaba revertiéndose en contra del ser humano, para cuyo servicio nació.

Los debates han motivado al movimiento Pro-Life, el cual se preocupa por los derechos y el estado de un embrión como un humano de temprana edad. Este movimiento cree que las investigaciones relacionadas con las células madre, instrumentaliza y viola lo que llaman la santidad de la vida y deberían ser consideradas como un asesinato. Las ideas fundamentales de aquellos que se oponen a estas investigaciones son la defensa de lo que llaman inviolabilidad de la vida humana y que la vida humana empezaría cuando un espermatozoide fertiliza un óvulo para formar una sola célula.

Una parte de las investigaciones usa embriones que fueron creados pero no usados en la fertilización in vitro para derivar una nueva línea de células madre. La mayoría de estos embriones tiende a ser destruida, o guardada por largos períodos, pasando su tiempo de vida. Solamente en Estados Unidos, se han estimado alrededor de 400 000 embriones en este estado.

Las investigaciones médicas señalan que las células madre tienen el potencial para alterar dramáticamente el acercamiento a la comprensión y tratamiento de enfermedades, y para aliviar sufrimiento. En el futuro, la mayoría de las investigaciones médicas anticipan el uso de tecnologías derivadas de las investigaciones de células madre para tratar varias. Heridas en la espina dorsal y el párkinson son dos ejemplos que han sido reconocidos por personas famosas (por ahora, Christopher Reeve y Michael J. Fox).

En agosto de 2000, el Instituto Nacional de Salud de Estados Unidos dijo:
[...] Investigaciones sobre células madre pluripotentes [...] prometen nuevos tratamientos y posibles curas para muchas enfermedades y lesiones, como párkinson, diabetes, problemas del corazón, esclerosis múltiple, quemaduras y lesiones de la espina dorsal. La NIH cree que el potencial médico de las células madre pluripotentes beneficiarán las tecnologías médicas y serán compatibles con la ética.

Recientemente, investigaciones de Advanced Cell Technology (Tecnología Celular Avanzada) en Woecester lograron obtener células madre de un ratón sin matar a los embriones. Si esta técnica se mejora será posible eliminar algunos de los problemas éticos relacionados con las investigaciones embrionarias de células madre.

En 2007, se descubrió otra técnica gracias a los equipos de investigaciones de Estados Unidos y Japón. Se reprogramaron las células de la piel humana para funcionar más como células embrionarias cuando se les introduce un virus. Extraer y clonar células madre es caro y complejo, pero el nuevo método de reprogramación es mucho más barato. Sin embargo, la técnica puede alterar el ADN de las nuevas células madre, causando cáncer de piel.

En 2007 se empezó a trabajar con células madre pluripotentes inducidas ("CPMI") mediante la manipulación de solo cuatro genes; más tarde, se ha conseguido reducir el número a solo dos de esos cuatro genes; e incluso, con solo introducir en la célula las cuatro proteínas codificadas por los cuatro genes. El proceso consiste en extraer una célula del paciente a tratar, manipular dichos 4 o 2 genes o introducirle las cuatro proteínas codificadas por esos cuatro genes, cultivarlas e introducirlas en el paciente o provocar su diferenciación hacia el tipo celular que se necesite (uno o varios, ya que las células madre así creadas se comportan como células embrionarias). Aún no hay experiencia en seres humanos y está por resolver el pequeño pero cierto riesgo de tumores.

La clonación terapéutica/embrionaria va muy de la mano con este tema. Sin embargo existen países que se oponen a ambas clonaciones o solo una o ninguna. También se oponen a la experimentación con células madre. Por ejemplo:






</doc>
<doc id="580" url="https://es.wikipedia.org/wiki?curid=580" title="Canica">
Canica

Una canica es una pequeña esfera de vidrio, alabastro, cerámica, arcilla, metal, cristal, acero, piedra, mármol, madera o porcelana que se utiliza en diversos juegos infantiles. También se denomina así a algunos juegos en los que se utilizan las canicas. Estos juegos son prácticamente universales, y aunque existen muchas variantes, la esencia es casi siempre la misma: lanzar una o varias canicas para intentar aproximarse a otras o a agujeros objetivo. Cuando se gana una mano se suelen tomar las canicas del otro jugador o de los jugadores contrarios.

Además de como juego, las canicas son muy utilizadas para uso industrial, siendo principalmente utilizadas en el interior de los envases de aerosoles y en rodamientos.

Las canicas tienen una gran variedad de nombres alternativos. También reciben las denominaciones de "tolonchas", "bellugas", "boliches", "bolichas", "bolitas", "caicos", "boles y caniques", "cayucos", "balitas", "bochas", "bolindres", "pingos", "pelotitas", "polcas", "bolas", "piquis", "polquitas", "caniques", "chivas", "cincos", "chibolas", "bolillas", "mosaicos", "metras", "balas", "garbinches", "bolondronas", "corote", "salva", "bolinchas", "tiros", "cachinas", "maras", "mables"', "meblís", "mollejones", "pepitas", "metras", "cristalas", "polcas", entre otros, según la zona y el país.

Si bien no se conoce el verdadero origen de las canicas y son juegos tradicionales, su origen aparentemente se remonta hasta el Antiguo Egipto y la Roma Precristiana. En efecto, se han encontrado canicas presentes en la tumba de un niño egipcio de alrededor del año 3000 a. C. En Creta, por su parte, los niños jugaban con canicas construidas a partir de materiales preciosos. En la Antigua Roma era un juego infantil cuya popularidad se extendió hasta la Edad Media.

Otros materiales utilizados en la Antigüedad son los huesos de aceitunas, avellanas o castañas. Hasta a principios del siglo XX, algunas todavía eran hechas de piedra.

Además de como elemento lúdico, las canicas se han utilizado también en artesanía, como objetos de decoración.

En América, se cree que la costumbre de las canicas se extendió desde Europa. No obstante, también existen indicios que indican que podrían haberse utilizado en la América precolombina. En Chile las canicas se introdujeron en el siglo XIX.

En 1953, Víctor Hugo «Tito» Chiarlo, antiguo trabajador de una cristalería argentina de San Jorge, Provincia de Santa Fe, fundó junto a Domingo Vrech la fábrica de canicas Tinka, motivado por una máquina que llegó a la cristalería donde trabajaba, importada desde Italia. La empresa es la primera y única del rubro en Argentina, si bien desde la década de 1990 también existen canicas importadas desde China y México. Desde una producción inicial de 12 mil canicas diarias, al año 2013 la fábrica Tinka alcanzaba las 400 mil unidades por día.

A mediados del siglo XX, en el Valle del Cauca, Colombia, se jugaba con bolas de corozo grande o chiquito, y semillas chascaraíces, que más tarde fueron reemplazadas por canicas de cristal.

Existen muchos tipos de canicas, y reciben diversos nombres de acuerdo a la zona geográfica en que se encuentren, así como dependiendo de sus tamaños, diseños y colores. Algunos de estos tipos son los siguientes:


Adicionalmente están las vergel, las ónix y los piojines.








En Argentina:

En Chile:
en Chile, aparte de juegos masificados en otros países como el de "la cuarta" o "la troya", existen algunos otros:

En Colombia:

En México:
los juegos más populares son la "choya", el "cocol" o "rombo" y el "círculo".


Adicionalmente están la "macateta" (Ecuador), la "matacocha" (Guatemala), "la hueca" y "el pepe" (Venezuela), entre otros.

En Venezuela:
las canicas se conocen como "metras". El juego tradicional consiste en hacer un hoyo en la tierra (con una chapa) y a cierta distancia una raya; un jugador trata de posicionar su metra en el hoyo, si lo logra pasa el otro jugador y este tendrá que intentar pegarle a la metra del jugador contrario, además el jugador que perdió la metra será eliminado; y así sucesivamente hasta que solo quede un jugador.

La cultura de las canicas ha desarrollado un amplio léxico coloquial, que varía dependiendo del país. Solo algunos términos empleados son los siguientes:


</doc>
<doc id="582" url="https://es.wikipedia.org/wiki?curid=582" title="Circunferencia">
Circunferencia

La circunferencia es una curva plana y cerrada tal que todos sus puntos están a igual distancia del centro.
Distíngase de círculo, cuyo lugar geométrico queda determinado por una circunferencia y la región del plano que encierra esta.

El interés por conocer la longitud de una circunferencia surge en Babilonia. Cuando usaban los carros con ruedas, era primordial relacionar el diámetro o radio con la circunferencia.

Elementos relevantes de la circunferencia, heredados por el círculo:


La longitud de una circunferencia en función del radio formula_2 o del diámetro formula_6 es:

donde formula_9 es la constante pi.

El área del círculo o de la región del plano delimitada por una circunferencia:

Posiciones de los puntos respecto de la circunferencia:

Posiciones de las rectas respecto de la circunferencia:

Se llama punto de tangencia cada uno de los puntos que comparte la circunferencia con los diferentes elementos tangentes, es decir, el punto donde se produce la tangencia. En todo punto de la circunferencia se pueden hacer tangencias.

Posiciones entre circunferencias:

Posición de los ángulos respecto de una circunferencia, puede ser:


En el ángulo central su amplitud formula_13 y el radio formula_2 de la circunferencia, determina la longitud del arco formula_15 resaltado en la figura en azul. Si el ángulo está en grados:

\cdot\pi\cdot r</math>|1=El ángulo central indica qué fracción de circunferencia que tiene el arco, así, si formula_7 entonces:

Si el ángulo está en radianes:

El arco capaz relaciona el ángulo central, inscrito, semi-inscrito y ex-inscrito siempre que las intersecciones de los lados mantengan la misma distancia.

Si el ángulo inscrito, semi-inscrito y ex-inscrito tienen la misma amplitud formula_13, entonces, determinan la misma longitud de arco, de color azul en la imagen, sobre una misma circunferencia de radio formula_2. Si el ángulo está en grados:

\cdot\pi\cdot r</math>|1=Como el ángulo central mide el doble que el ángulo inscrito, semi-inscrito y ex-inscrito, este hecho se sustituye en la fórmula usada en el ángulo central quedando:

Si el ángulo está en radianes:

Diversos tipos de ángulos aparecen en el análisis de la potencia de un punto respecto de una circunferencia.

Diremos que una circunferencia está circunscrita a un polígono cuando todos los vértices de dicho polígono están sobre esta, se dice que este polígono está inscrito.

Diremos que una circunferencia está inscrita a un polígono cuando sea tangente a todos los lados de dicho polígono, se dice que este polígono está circunscrito.

La circunferencia se puede representar mediante ecuaciones o funciones que determinan la posición de cada uno de sus puntos. Para ello solo hace falta garantizar que la distancia de cada punto formula_25 de la circunferencia a su centro formula_1 sea constante para cada una de las ecuaciones y funciones que se tenga.

Una circunferencia queda determinada por un centro formula_27 y un radio formula_2, por tanto, su ecuación queda determinada al imponer que la distancia de sus puntos, formula_29, al centro sea constante, es decir, formula_30 dando la siguiente ecuación:

Su representación en un sistema de coordenadas viene dada por cada punto de la forma formula_32 que satisfacen la ecuación.

La ecuación anterior es más sencilla si está centrada en el origen de coordenadas formula_33

La circunferencia de centro en el origen de coordenadas y radio uno se denomina circunferencia unidad o circunferencia goniométrica y su ecuación es:

Su función implícita es formula_36 y para representar la circunferencia se buscan los puntos del plano que cumplen la ecuación formula_37



La circunferencia con centro en formula_27 y radio formula_2 se puede parametrizar usando funciones trigonométricas de un solo parámetro formula_42 para obtener una función paramétrica formula_43

También se puede parametrizar con funciones funciones racionales como

</math>

Si se sustituye sobre la circunferencia unidad formula_51 nos dará la intersección de la proyección sobre esta circunferencia y por tanto los puntos de esta paramétricamente:

finalmente sustituyendo sobre el haz y arreglando las fracciones queda formula_55

donde formula_56 incluye el punto en el infinito.

En el plano complejo, una circunferencia con centro formula_57 y radio formula_2 a partir de la ecuación de la circunferencia formula_59 se obtiene la forma paramétrica:

donde formula_62

Como en la función paramétrica, la circunferencia puede representarse en cualquier subespacio de dimensión dos de un espacio vectorial usando dos vectores ortonormales formula_63 y formula_64, y por tanto generadores de dicho subespacio, permitiendo construir la circunferencia en cualquier plano oblicuo con centro formula_65 y radio formula_2 que viene dada o descrita por la función vectorial:

Toda curva plana dada en coordenadas polares es de la forma formula_70 formula_71 donde formula_72 es la distancia al centro o polo formula_73 y formula_42 el ángulo respecto el eje OX, por tanto la expresión de una circunferencia con centro en el polo y radio formula_2 es:

Cuando el centro está en el punto formula_76 con radio formula_2 la circunferencia es:


Según el área que se trabaje, hay formas de identificar y usar una circunferencia implícitamente, además de sus funciones y ecuaciones.

En topología, se denomina circunferencia a cualquier curva cerrada simple que sea homeomorfa a la circunferencia usual de la geometría (es decir, la esfera 1–dimensional). Se la puede definir como el espacio cociente determinado al identificar como uno los dos extremos de un intervalo cerrado. Sin embargo, los geómetras llaman 2-esfera a la circunferencia, mientras que los topólogos se refieren a ella como 1-esfera y la indican como formula_81, dando lugar a posibles confusiones.

La dimensión de la circunferencia es 1. De igual modo, la dimensión de una recta no acotada, o de un arco — esto es de un conjunto homeomorfo con un intervalo cerrado — y de una curva cerrada simple, i.e. un conjunto homeomorfo con una circunferencia, es igual a 1. También el caso de una poligonal cerrada.

En el tema de ecuaciones diferenciales, una circunferencia puede determinarse mediante una curva integral de una ecuación diferencial como:

En teoría local de la curva, se considera como circunferencia una curva de "curvatura" constante sin "torsión".

Un par de circunferencias que se desplazan, tangencial e interiormente, una sobre la otra guardando una razón entre sus radios de 1:2. Investigadas, originalmente, por el matemático italiano, Girolamo de Cardano

Usada en una alternativa definitoria de la elipse y de la hipérbola. Siendo estas el lugar de los centros de las circunferencias tangentes a la llamada "circunferencia directriz".

Al tratar de la curvatura de una curva o de una superficie, en el punto de contacto, además de la tangente se toma en cuenta la circunferencia de la curvatura, llamada " circunferencia osculatriz"




</doc>
<doc id="584" url="https://es.wikipedia.org/wiki?curid=584" title="Crenarchaeota">
Crenarchaeota

Las crenarqueotas (Crenarchaeota), también llamadas eocitos, sulfobacterias o crenotas son un filo de arqueas. Inicialmente se pensaba que incluía solo organismos hipertermófilos, frecuentemente quimiosintetizadores dependientes del azufre. Sin embargo, estudios recientes los han identificado como las arqueas más abundantes en el ecosistema marino. Crenarchaeota es uno de los dos grupos principales de arqueas y originalmente fue separado del otro grupo (Euryarchaeota) basándose en las secuencias del ARNr. Esta división se ha visto apoyada por algunas características fisiológicas, como la carencia de histonas. (Sin embargo, se ha encontrado alguna especie de Crenarchaeota que posee histonas).

Se ha encontrado que a diferencia de otros grupos de organismos, Crenarchaeota tiene una única maquinaria de división celular.

Podemos distinguir dos grupos de crenarqueotas:

Este grupo (órdenes Thermoproteales, Sulfolobales, Desulfurococcales y Caldisphaerales) incluye las especies con las temperaturas de crecimiento más altas de cualquier organismo conocido. El crecimiento óptimo se realiza entre 75 y 105 °C, mientras que la temperatura máxima de crecimiento para crecer
"Pyrolobus" es tan alta como 113 °C. La mayoría de estas especies no pueden crecer por debajo de 70 °C, aunque pueden sobrevivir por períodos largos a bajas temperaturas. Algunas especies son acidófilas con un pH óptimo entre 1,5 y 4 y mueren en un pH 7, mientras que otras son neutrófilas o ligeramente acidófilas, creciendo óptimamente a un pH de 5,5–7,5. Se encuentran en hábitats volcánicos tales como manantiales calientes continentales y en fuentes hidrotermales del fondo oceánico, a poca o mucha profundidad.

Los modos metabólicos son diversos, comprendiendo desde quimioorganotrofos a quimiolitotrofos. Los quimiolitotrofos aerobios obtienen energía por la oxidación de varios compuestos sulfúricos, hidrógeno o hierro ferroso, mientras que los quimiolitotrofos anaerobios reducen azufre, tiosulfato o producen nitratos, sulfuro de hidrógeno o amoníaco. Los quimioorganotrofos crecen sobre sustratos orgánicos complejos, azúcares, aminoácidos o polímeros. Varias especies son productores primarios usando el dióxido de carbono como fuente única de carbono y obteniendo energía por la oxidación de sustancias inorgánicas tales como azufre o hidrógeno, o por la reducción de azufre o nitrato.

Una de las especies más conocidas de Crenarchaeota es el "Sulfolobus solfataricus". Este organismo fue aislado originalmente a partir de muestras tomadas de manantiales geotermales sulfúricos en Italia y crece a 80 °C y con un pH de 2-4. Desde entonces se han encontrado especies del mismo género en todo el mundo. A diferencia de la gran mayoría de termófilos cultivados, esta especie puede crecer aeróbicamente y utilizando fuentes de energía orgánicas tales como el azúcar. Estos factores permiten que su cultivo sea mucho más fácil que el de los organismos anaerobios y han llevado a que "Sulfolobus" se convierta en un organismo modelo para el estudio de los hipertermófilos y de un grupo extenso de virus que se desarrollan dentro de ellos.

Análisis ambientales recientes basados en secuencias ARNr indican que Crenarchaeota también se distribuye extensamente en ambientes a baja temperatura tales como suelos, sedimentos, agua dulce y océanos. Aunque ninguno ha podido ser cultivado, el ambiente de obtención (junto con los datos genómicos) hace suponer que son organismos mesófilos o psicrófilos. Este extenso grupo de archaea parece derivar de antepasados termofílicos que invadieron diversos hábitats de baja temperatura.

Quizás lo más asombroso sea su alta abundancia relativa en las aguas superficiales invernales en la Antártida (-1,8 °C), en donde llegan a abarcar el 20% del ARNr microbiano total. Exámenes similares en aguas templadas de la costa de California demuestran que estos organismos tienden a ser los más abundantes en profundidades inferiores a 100 m. De acuerdo con estas medidas, parece que estos organismos son muy abundantes en el océano y serían uno de los contribuyentes principales a la fijación del carbono. Esto, junto al hecho de que se han encontrado secuencias de ARNr de Crenarchaeota en cada hábitat de baja temperatura en la cual fueron buscadas, sugiere que pueden estar distribuidos globalmente y jugar un papel importante en la biosfera.

La filogenia del ARNr 16S da el siguiente resultado (el grupo entre comillas es parafilético):




</doc>
<doc id="586" url="https://es.wikipedia.org/wiki?curid=586" title="Casuarinaceae">
Casuarinaceae

Casuarinaceae es una familia del orden Fagales que comprende 4 géneros y unas 90 especies aceptadas de árboles y arbustos tropicales y subtropicales de morfología muy singular dentro de las Angiospermas.

Son plantas actinoricicas que fijan el nitrógeno por simbiosis con bacterias filamentosas ("Frankia").

Son plantas leñosas, arbóreas o arbustivas, siempreverdes -refiriéndose a los artículos clorofílicos de las ramillas, no a las hojas-, monoicas o dioicas (incluso un mismo género puede tener especies monoicas y otras dioicas), con ramillas jóvenes delgadas estriadas, equisetiformes, articuladas, clorofílicas, y hojas inconspicuas, marcescentes o no, escuamiformes, en verticilos que delimitan cada artículo y que, a primera vista y para los neofitas -y menos neofitas-, se asemejan curiosamente a las Gimnospermas. Las flores son muy simplificadas, pero su desarrollo es muy complejo; están dispuestas en inflorescencias espiciformes las masculinas, o capituliformes las femeninas, en verticilos alternando con filas de brácteas escuamiformes; las masculinas con 4 bractéolas –las internas interpretadas a veces como sépalos–, y un solo estambre de antera basifija bilocular y las femeninas sin perianto, con un pistilo bífido rojizo y un gineceo bicarpelar con solo uno de los carpelos -que están fusionados- fertíl. Las infrutescencias son coniformes, compuestas por numerosos frutos individuales samaroides rodeados por las 2 bractéolas escamosas lignificadas, tuberculadas o no, muy acrescentes y que se abren ampliamente cuando maduran para liberar dichos frutos que contienen una única semilla cada uno.

La familia es nativa del Viejo Mundo, los trópicos indo-malasios, Australia y las hasta Colombo y Madagascar. Una cuantas de sus especies son ampliamente naturalizadas y cultivadas como ornamental en todo el mundo tropical, subtropical y templado.

El nombre común más usado para las especies de Casuarinaceae es roble hembra (sheoak or she-oak). Otros nombres incluyen palo hierro (ironwood), roble toro (bull-oak or buloke), y palo res (beefwood).

La familia fue descrita por Robert Brown y publicado en "A Voyage to Terra Australis", vol. 2, p. 571, 1814. El género tipo es: "Casuarina".



</doc>
<doc id="587" url="https://es.wikipedia.org/wiki?curid=587" title="Casuarina">
Casuarina

Casuarina (nombre común casuarinas) es un género de arbustos y árboles perennes compuesto por una quincena de especies -monoicas o dioicas- aceptadas, de unas 35 descritas.

Sus esbeltas y delicadas ramas con verticilos de inconspicuas hojas escuamoides y sus infrutescencias estrobiloides les dan apariencia de pinos. Sin embargo, a pesar de lo que pueda parecer, no son Gimnospermas sino Angiospermas con caracteres morfológicos muy particulares y que tienen una familia propia (Casuarinaceae).

Son originarios de Australia y las islas del Pacífico, pero son muy comunes en las regiones tropicales y subtropicales.

Comúnmente conocidas como roble hembra (she-oak o sheoak), palo hierro (ironwood), o palo res (beefwood), las casuarinas son comúnmente cultivadas en áreas tropicales, subtropicales y templadas en todo el mundo. 




</doc>
<doc id="588" url="https://es.wikipedia.org/wiki?curid=588" title="Ceuthostoma">
Ceuthostoma

Ceuthostoma es un género arbóreo con 2 especies en la familia "Casuarinaceae".




</doc>
<doc id="589" url="https://es.wikipedia.org/wiki?curid=589" title="Carya">
Carya

Carya, caria o pacana es un género de plantas arbóreas, más raramente arbustivas, de la familia Juglandaceae. Incluye una treintena de taxones aceptados —entre específicos, infraespecíficos e híbridos— de los casi 150 descritos. 

carya: derivado del griego "κάρυον", "nuez".

Árboles, más raramente arbustos, de 3 hasta 50 metros de altura, monoicos como todos los representantes de la familia, con corteza de color gris o parduzco, lisa con fisuras en individuos jóvenes, tornándose arrugada y ocasionalmente profundamente surcada o exfoliada en escamas laminares o bien en cintas largas o anchas. Las hojas son imparipinnadas, con peciolo de glabro a pubescente o escamoso y 3-21 folíolos peciolulados de margen asserado —los distales más grande— que miden de 2-26 por 1-14 cm. Los amentos masculinos, colgantes, son de 3 fascículos de flores con 3-15 estambres pubesentes o no. Las flores femeninas se organizan en espigas terminales. Los frutos son drupáceos ("drupa involucrada" o "trima") de involucro dehiscentes en 4 valvas de suturas lisas o aladas, con nuez comprimida o no, angulosa o lisa, suave o verrugosa, de cáscara fina o gruesa y de color pardo, ocasionalmente moteado de negro o marrón. La semilla puede ser dulce o amarga. 

Una docena de especies son nativas de Norteamérica (11 en EE. UU., 1 en México), y unas 5–6 de China e Indochina. Un cierto número de especies han sido introducidas en diversas regiones del mundo con fines ornamentales, alimenticios o industriales.



Por ejemplo:

El género tiene cierta importancia económica: por ejemplo, "Carya illinoinensis", el pacano y su fruto la pacana, es el más importante productor nativo de «nueces» de Norteamérica, y la madera de los verdaderos «nogales americanos» es sin igual para la fabricación de mangos de herramientas y palos de golf por su solidez y resistencia a los golpes. También las pacanas son una buena fuente alimenticia para la fauna salvaje y están muy apreciadas en la gastronomía humana. Además, la madera del pacano es bastante popular en los asados en Estados Unidos y otros países, siendo utilizada frecuentemente en los ahumados de carnes, conocido por dar un característico toque de aroma y sabor al quemarse mientras se cocina la carne.

Por otra parte, "Carya cordiformis", "C. glabra" y "C. ovata" están cultivados extensivamente en la Europa central como madera de construcción.


</doc>
<doc id="590" url="https://es.wikipedia.org/wiki?curid=590" title="Cyclocarya paliurus">
Cyclocarya paliurus

La Cyclocarya es un género monotípico de la familia Juglandaceae. Su única especie: Cyclocarya paliurus estaba anteriormente incluida en el género "Pterocarya" como "Pterocarya paliurus". Es nativa del este y centro de China.

Se trata de un árbol caduco que alcanza hasta los 30 m de altura. El follaje es similar al de "Pterocarya", con las hojas pinnadas de 20-25 cm de largo con cinco a once foliolos, los folíolos son de 5-14 cm de largo y 2-6 cm de ancho. Las flores son amentos, las de sexo masculino (polen) se producen en racimos (no por separado como en "Pterocarya"), los amentos femeninos de 25-30 cm de largo en su madurez, tienen varias pequeñas nueces con alas de 2.5-6 cm de diámetro.

Se encuentra en los bosques húmedos de las montañas, a una altura de 400-2500 metros, en Anhui, Fujian, Cantón, Guangxi, Guizhou, Hainan, Hubei, Hunan, Jiangsu, Jiangxi, Sichuan, Taiwán, Yunnan y Zhejiang.

"Cyclocarya paliurus" fue descrita por (Batalin) Iljinsk. y publicado en "Trudy Bot. Inst. Akad. Nauk S.S.S.R., Ser. 1, Fl. Sist. Vyssh. Rast." 10: 115 1953.



</doc>
<doc id="592" url="https://es.wikipedia.org/wiki?curid=592" title="Catabrosa">
Catabrosa

Catabrosa es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de las regiones templadas del Hemisferio Norte. Comprende 62 especies descritas y de estas, solo 2 aceptadas. 
Las plantas son perennes con láminas foliares planas. Inflorescencia en forma de una panícula laxa. Espiguillas generalmente 2 florecidas, a veces floreados-3-1 o, subteretes; Glumas desiguales, más cortas que la espiguilla, delgadas, sin nervios la inferior, la superior prominente (1 -) 3 nervada, glabra; lemas oblonga cuando aplanada, redondeada en la parte de atrás, membranosa con un ápice escarioso, prominentemente 3-nervada; pálea 2-quilla, similar en textura a la lemma; estambres 3.
El género fue descrito por Ambroise Marie François Joseph Palisot de Beauvois y publicado en "Essai d'une Nouvelle Agrostographie" 97. 1812. La especie tipo es: "Catabrosa aquatica" (L.) P.Beauv.


El número cromosómico básico del género es x = 5, con números cromosómicos somáticos de 2n = 10 y 20, ya que hay especies diploides y una serie poliploide. Cromosomas relativamente «grandes».
Catabrosa: nombre genérico que deriva del griego "katabrosis" = (una comida o devorar), en alusión a las glumas dentadas.



</doc>
<doc id="593" url="https://es.wikipedia.org/wiki?curid=593" title="Chaetopogon fasciculatus">
Chaetopogon fasciculatus

Chaetopogon, es un género monotípico de plantas herbáceas perteneciente a la familia de las poáceas. Su única especie: Chaetopogon fasciculatus, es originaria de la región del Mediterráneo.
Tiene tallos de 3-40 cm de altura, erectos, geniculado-ascendentes o decumbentes, glabros. Hojas glabras, con lígula de 2-4 mm, y limbo de hasta 10 x 1,5 cm, plano o convoluto en la desecación. Panícula de 1-10 cm, subcilíndrica o elipsoidea, con ramas escábridas y pedúnculos de c. 0,2 mm. Espiguillas de 2,5-4 mm. Glumas de 2,5-4 mm, agudas, aculeado-escábridas; la inferior terminada en arista de 3-10 (-15) mm, recta. Lema de 2,5-3 mm, obtuso-dentada, glabra. Anteras de 1,4-2,2 mm. Cariopsis de 2 x 0,3 mm, linear. Tiene un número de cromosomas de 2n = 14. Florece de abril a junio.
"Chaetopogon fasciculatus" fue descrita por (Link) Hayek y publicado en "Repertorium Specierum Novarum Regni Vegetabilis, Beihefte" 30(3): 335. 1932. 

El número cromosómico básico del género es x = 7, con números cromosómicos somáticos de 2n = 14.


</doc>
<doc id="594" url="https://es.wikipedia.org/wiki?curid=594" title="Coix">
Coix

Coix es un género de plantas herbáceas de la familia de las poáceas. Es originario de Asia tropical. 
El nombre del género deriva del griego "koix", una especie de palma. 

El número cromosómico básico del género es x = 5, con números cromosómicos somáticos de 2n = 10, 20 y 40, ya que hay especies diploides y una serie poliploide. Cromosomas relativamente "pequeños".



</doc>
<doc id="595" url="https://es.wikipedia.org/wiki?curid=595" title="Cortaderia">
Cortaderia

Cortaderia es un género con 25 a 35 especies de plantas herbáceas y perennes pertenecientes a la familia de las poáceas. Se distribuyen por Sudamérica (15-20 especies), Nueva Zelanda (4 especies) y Nueva Guinea (una especie).


Las especies de "Cortaderia" pueden medir de 1,5-3,5 m de altura, con inflorescencias blancas y violáceas, semejantes a un plumero. Se utilizan frecuentemente como planta ornamental. 

El nombre común cortaderia, estrictamente referido a "C. selloana", se aplica con frecuencia a todas las especies del género (y a veces, incorrectamente, para "Erianthus" y "Saccharum ravennae").

Algunas especies, notablemente "C. selloana", "C. jubata" y "C. rudiuscula", se han convertido en especies invasoras en áreas de Nueva Zelanda, costas de California, Hawái. En esas áreas, y en cualquier lugar con clima mediterráneo, debería evitarse la siembra de especies de "Cortaderia".

El nombre del género "Cortaderia" proviene del castellano "para cortar", debido a las hojas con bordes filosos aserrados.

El número cromosómico básico del género es x = 9, con números cromosómicos somáticos de 2n = 36, 72, 90 y 108, ya que hay especies diploides y una serie poliploide. Cromosomas relativamente "pequeños".

Debido a su potencial colonizador y constituir una amenaza grave para las especies autóctonas, los hábitats o los ecosistemas, todas las especies y subespecies del género "Cortaderia" han sido incluidas en el , regulado por el Real Decreto 630/2013, de 2 de agosto, estando prohibida en España su introducción en el medio natural, posesión, transporte, tráfico y comercio.



</doc>
<doc id="596" url="https://es.wikipedia.org/wiki?curid=596" title="Corynephorus">
Corynephorus

Corynephorus, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de Europa de la región del Mediterráneo.
Son plantas anuales o perennes. Hojas con vainas de márgenes libres; lígula lanceolada, subaguda, membranosa; limbo cetáceo, generalmente convoluto. Inflorescencia en panícula laxa, con ramas desnudas en la base. Espiguillas comprimidas lateralmente, con 2 flores hermafroditas y articuladas con la raquilla. Glumas más largas que las flores, ligeramente desiguales, carenadas; la inferior uninervada; la superior trinervada. Raquilla prolongada por encima de las flores, hirsuta. Lema aguda, con 5 nervios, membranosa, glabra; arista subbasal, articulada, con columna retorcida parda con 1 corona terminal de aguijones antrorsos y parte apical clavada hialina. Callo hirsuto. Pálea ligeramente más corta que la lema, bidentada, con 2 quillas. Lodículas bilobadas. Ovario glabro. Cariopsis elipsoidea, surcada, adherente a la pálea y lema. Hilo puntiforme.
El género fue descrito por Ambroise Marie François Joseph Palisot de Beauvois y publicado en "Essai d'une Nouvelle Agrostographie" 90, 159. 1812. La especie tipo es: "Corynephorus canescens" (L.) P. Beauv. 
Corynephorus: nombre genérico derivado del griego "korynephorus" (procrear), refiriéndose a la arista lema.

El número cromosómico básico del género es x = 7, con números cromosómicos somáticos de 2n = 14, diploides. Cromosomas relativamente "grandes".



</doc>
<doc id="597" url="https://es.wikipedia.org/wiki?curid=597" title="Crypsis">
Crypsis

Crypsis, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de la región del Mediterráneo hasta el norte de China.
Son plantas anuales. Tiene la inflorescencia en panícula densa, capituliforme, o espiciforme, frecuentemente cubierta en la base por la vaina de 1 o más hojas superiores. Espiguillas muy comprimidas lateralmente, con 1 sola flor hermafrodita, articulada por encima o por debajo de las glumas. Glumas subiguales, más cortas que la flor, uninervada. Lema membranosa, uninervada, mútica. Pálea con 1-2 nervios poco visibles, redondeada o con 1 quilla. Lodículas generalmente ausentes. Androceo con 2 o 3 estambres. Cariopsis obovoideo u oblongoideo, con embrión casi de su misma longitud; pericarpo no soldado a la semilla. Hilo redondeado.
El género fue descrito por William Aiton y publicado en "Hortus Kewensis"; or, a catalogue. . . 1: 48. 1789. La especie tipo es: "Crypsis aculeata"
El nombre del género deriva del griego "kryptos" = (oculto, encubierto), refiriéndose a la parte de inflorescencia oculta. 

El número cromosómico básico del género es x = 8 y 9, con números cromosómicos somáticos de 2n = 16, 18, 32, 36 y 54, ya que hay especies diploides y una serie poliploide. Cromosomas relativamente "pequeños".




</doc>
<doc id="598" url="https://es.wikipedia.org/wiki?curid=598" title="Ctenopsis garcini">
Ctenopsis garcini

Ctenopsis garcini es la única especie del género monotípico de plantas herbáceas perteneciente a la familia de las poáceas. Es originaria del Mediterráneo y Asia occidental.

Es una planta anual cespitosa con tallos de 10-40 cm de alto; herbácea. Culmo con nodos glabros. Entrenudos huecos. Las hojas basales no agregadas; no auriculadas. Láminas de las hojas lineales; estrechas; setaceas; (enrolladas); sin venación. Lígula persistente como una membrana de 0,7-1 mm de largo. Plantas bisexual, con espiguillas bisexuales; con flores hermafroditas. Inflorescencia en un solo racimo, o paniculada (a veces escasamente ramificada). Espiguilla fértil con ejes persistentes. Espiguillas solitarias; pediceladas. Espiguillas femeninas fértiles de 3,5-12 mm de largo, comprimidos lateralmente, desarticulándose por encima de las glumas.

"Ctenopsis garcini" fue descrita por (L.) Naudin y publicado en "Annales des Sciences Naturelles; Botanique, sér. 5", 6: 13. 1866.
El nombre del género deriva del griego "ktenos" (peine) y "opsis" (aspecto), aludiendo a una inflorescencia forma de peine. 
Número de la base del cromosoma, x = 7. 2n = 14.



</doc>
<doc id="599" url="https://es.wikipedia.org/wiki?curid=599" title="Cutandia">
Cutandia

Cutandia, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario del Mediterráneo y Asia occidental.
Son plantas anuales. Hojas con vainas de márgenes libres; lígula membranosa, lacerada; limbo plano o convoluto. Inflorescencia en panícula laxa, ramificada, a menudo con ramas divaricadas en el extremo. Espiguillas comprimidas lateralmente, con 5-12 flores hermafroditas; raquilla glabra, desarticulándose en la madurez. Glumas 2, desiguales, subcoriáceas, más cortas que las flores; la inferior con 3 nervios; la superior membranosa, con 2 quillas escábridas. Lodículas ovado-acuminadas, a menudo más o menos bilobadas. Androceo con 3 estambres. Ovario glabro. Cariopsis linear-oblonga, glabra.
El género fue descrito por Heinrich Moritz Willkomm y publicado en "Botanische Zeitung (Berlin)" 18: 130. 1860. La especie tipo es: "Cutandia scleropoides" Willk. 
El nombre del género fue otorgado en honor del botánico español Vicente Cutanda. 
Número de la base del cromosoma, x = 7. 2n = 14. Cromosomas "grandes".




</doc>
<doc id="600" url="https://es.wikipedia.org/wiki?curid=600" title="Cynodon">
Cynodon

Cynodon es un género de planta herbácea perenne de la familia de las gramíneas (Poaceae).Tiene una decena de especies aceptadas de casi un centenar descritas.

Son plantas perennes, rizomatosas o estoloníferas, formando eventualmente césped. Los tallos, hojosos, son robustos o delgados y con internudos cortos. Las hojas son lineares a filiformes, llanas, con lígula membranosa o ciliada. La inflorescencia es digitada, a veces con más de un verticilo de espigas con las espiguillas sentadas unilateralmente, imbricadas y lateralmente comprimidas. Dichas espiguillas tienen 1, raramente 2, flores con las glumas subiguales, estrechas, herbáceas, de ápice acuminado y con un solo nervio o la superior con 3; son persistentes las 2 o solo la inferior. La lema es navicular y con la quilla generalmente pubescente, con 3 venas y sin arista, mientras la pálea es biaquillada. Su fruto (Cariopsis) es de forma elíptica y comprimido lateralmente, con el embrión alcanzando la mitad de su longitud.

Casi todas las especies son nativas de las zonas tropicales del Viejo Mundo, especialmente de África, mientras una de ellas es pantropical y extendida en todas las regiones cálidas y templada del mundo.
Algunas especies, más comúnmente "Cynodon dactylon", crece como césped en prados, en regiones templadas, donde son valoradas por su tolerancia a las sequías, comparadas con los demás céspedes para prado. En muchos casos se la considera una maleza difícil de erradicar con herbicidas o mediante control mecánico sin dañar el resto de la vegetación debido a los rizomas y estolones que, si bien se rompen, dejan estructuras vegetales subterráneas y luego se repropaga.

El género fue descrito por Louis Claude Marie Richard y publicado en Christiaan Hendrik Persoon "Synopsis Plantarum", vol. 1, nº 159, p. 85, 1805. La especie tipo es: "Cynodon dactylon" (L.) Pers. 
Cynodon: nombre genérico que deriva del griego "χυνο", perro y "όδού", diente, tal vez en alusión a las duras yemas basales cónicas y afiladas en los rizomas. 
El número cromosómico básico del género es x = 9 y 10, con números cromosómicos somáticos de 2n = 16, 18, 27, 36, 40 y 54, ya que hay especies diploides y una serie poliploide. Cromosomas relativamente "pequeños". Nucléolos persistentes.

Comprende las siguientes especies:



</doc>
<doc id="601" url="https://es.wikipedia.org/wiki?curid=601" title="Cynosurus">
Cynosurus

Cynosurus es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es el único género de la subtribu Cynosurinae. Es originario de Europa, Asia occidental, el norte y el sur de África.
Son plantas anuales o perennes. Lígula una membrana; láminas lineares, aplanadas. Inflorescencia una panícula terminal condensada de espiguillas dimorfas pareadas, 1 de cada par estéril y 1 bisexual. Espiguillas estériles conspicuas, flabeladas, con 2 glumas y varias lemas estériles, persistentes, la raquilla no desarticulándose. Espiguillas bisexuales en su mayor parte ocultas por las estériles, comprimidas lateralmente, con (1-)2-5 flósculos; desarticulación arriba de las glumas y entre los flósculos; glumas 1-nervias; lemas cortamente aristadas, con una costilla media visible y 4 nervaduras laterales inconspicuas; páleas casi tan largas como las lemas, 2-carinadas; estambres 3; estilos 2. Fruto una cariopsis, encerrada entre la lema y la pálea; hilo linear. 
El género fue descrito por Carlos Linneo y publicado en "Species Plantarum" 1: 72–73. 1753. La especie tipo es: "Cynosurus cristatus" L. 
"Cynosurus": nombre genérico que deriva de las palabras griegas κυνός "kynos", "perro" y de ουρά "oura", "cola", refiriéndose a la forma de la panícula. 

El número cromosómico básico del género es x = 7, con números cromosómicos somáticos de 2n = 14, diploide Cromosomas relativamente "pequeños".




</doc>
<doc id="603" url="https://es.wikipedia.org/wiki?curid=603" title="CODASYL">
CODASYL

CODASYL (también escrito Codasyl) es el acrónimo para "Conference on Data Systems Languages", un consorcio de industrias informáticas formado en 1959 con el objeto de regular el desarrollo de un lenguaje de programación estándar que pudiera ser utilizado en multitud de ordenadores. De todos estos esfuerzos resultó el lenguaje COBOL.

Los miembros de CODASYL pertenecían a industrias e instituciones gubernamentales relacionadas con el proceso de datos. Su principal meta era promover un análisis, diseño e implementación de los sistemas de datos más efectivos. La organización trabajó en varios lenguajes a lo largo del tiempo pero nunca llegaron a establecer estándar alguno, proceso que dejaron en manos de ANSI.

En 1965 CODASYL formó la "List Processing Task Force" (en español, "Grupo de Trabajo para el Procesado de Listas"). Este grupo se dedicó a desarrollar extensiones del lenguaje COBOL para el procesamiento de colecciones de registros; el nombre surgió a causa del sistema IDS (Integrated Data System) desarrollado por Charles Bachman (sistema que supuso el mayor aporte técnico al proyecto), y que manejaba las distintas relaciones mediante cadenas de punteros. En 1967 el grupo fue renombrado como "Grupo de Trabajo sobre Bases de Datos", y su primer informe fechado en enero de 1968 se tituló "COBOL extensions to handle data bases" (en español, "Extensiones COBOL para el manejo de bases de datos"). En octubre de 1969 el DBTG publicó las primeras especificaciones para el modelo de base de datos en red, el cual acabó por ser conocido como Modelo Codasyl. Propiamente estas especificaciones definían varios lenguajes por separado: un lenguaje de descripción de datos (DDL, siglas en inglés) para definir el esquema de la base de datos, otro DDL para crear uno o más subesquemas para definir vistas de la base de datos en aplicaciones; y un lenguaje de manipulación de datos (DML) que definía palabras clave para incluir en el código COBOL las llamadas y actualizaciones de la base de datos. Aunque los trabajos siempre se centraron en COBOL, la idea de un lenguaje independiente comenzó a emerger, impulsada por las pretensiones de IBM de utilizar el PL/I como reemplazo de COBOL.

En 1971, en gran parte como respuesta a la necesidad de la independencia del nuevo lenguaje de programación, el trabajo fue reorganizado: el desarrollo del DDL fue continuado por el "Data Description Language Committee", mientras que el desarrollo del COBOL DML fue asumido por el "COBOL Language Committee". En retrospectiva, esta división tuvo desafortunadas consecuencias. Los dos grupos nunca fueron capaces de sincronizar sus especificaciones, obligando a los distribuidores a subsanar los problemas generados por las diferencias entre ellas. Finalmente se hizo inevitable la aparición de una falta de interoperabilidad entre implementaciones.

Algunas empresas implementaron productos de bases de datos rudamente conformes a las especificaciones del DBTG, siendo de todas ellas las más conocidas: Honeywell Integrated Data Store (IDS/2), Cullinet Integrated Database Management System (IDMS), Univac DMS-1100 o Digital Equipment Corporation DBMS32.

El modelo Codasyl definió una serie de elementos básicos que definían su estructura de
datos. Son los siguientes:

- Elemento de datos.- Unidad de datos más pequeña que se puede referenciar. Puede ser de distintos tipos, y puede definirse como dependiente de valores de otros elementos (datos derivados).

- Agregado de datos.- Se asemeja a los campos de un fichero o a los atributos de otros modelos.

- Registro.- Colección nominada de elementos de datos. Unidad básica de acceso y manipulación. Se asemeja a los registros en ficheros y a las entidades en el modelo E/R.

- Conjunto (SET).- Colección nominada de dos o más tipos de registros que establece una vinculación entre ellos. Origen de muchas restricciones. Las interrelaciones 1:N se representan aquí mediante SET.

- Área.- Subdivisión nominada del espacio direccionable de la base de datos que contiene ocurrencias de registros.

- Clave de base de datos 􀃆 identificador interno único para cada ocurrencia de registro.

Proporciona su dirección en la base de datos. Es un obstáculo para conseguir la independencia lógica / física. Suponía problemas el reutilizar una clave cuando se reorganizaba la base de datos.

CODASYL: CONJUNTOS (SET)

El conjunto es uno de los más importantes elementos del modelo Codasyl, pues constituye el elemento básico para la representación de interrelaciones. Mediante SET se establecen relaciones jerárquicas (1:N) a dos niveles. El nodo raíz es el propietario y los nodos descendientes (pueden ser de varios tipos) son los miembros.

CARACTERÍSTICAS BÁSICAS DEL MODELO CODASYL

Se pueden resumir las características básicas del modelo en :

- Un SET es una colección nominada de dos o más tipos de registros que representan un tipo de interrelación 1:N (en consecuencia también 1:1).

- Cada SET tendrá un tipo de registro propietario y uno o más tipos de registros miembro.

- El número de SET que se pueden declarar en el sistema es ilimitado.

- Cualquier registro puede ser propietario de uno o varios SET.

- Cualquier registro puede ser miembro de uno o varios SET.

- Podrán existir SET singulares en los que el propietario es el sistema (una entidad se interrelaciona consigo mismo).

- A pesar de que una entidad sea miembro de un SET, existe la posibilidad de que ciertas ocurrencias de esa entidad no estén ligadas al SET, con lo que no tendrían propietario y quedarían no ligadas respecto de ese SET.

RESTRICCIONES INHERENTES DEL MODELO CODASYL.

Cuando hablábamos del modelo en red general, decíamos que era un modelo muy flexible a coste de no tener restricciones inherentes. Esta ausencia de restricciones hace que sea muy difícil de implementar, y a la larga suele reportar escaso rendimiento, por lo que como también decíamos no pasa de ser un modelo teórico.

El modelo Codasyl está basado en el modelo en red general, pero a diferencia de este, es un modelo utilizado. Esto es debido a que Codasyl ha incluido restricciones inherentes que hacen que sea posible su implementación y que se obtenga un alto rendimiento del sistema. 

Las restricciones son las siguientes:

- Solo se admiten tipos de interrelaciones jerárquicas de dos niveles (propietario y miembro). Si se admite la combinación de varios SET para generar jerarquías multinivel.

- En el nivel propietario solo se permite un tipo de registro.

- En el mismo SET no se permite que a un registro ser a la vez propietario y miembro, no está admitida la reflexividad. Aunque esta restricción se eliminó con el tiempo, los productos basados en Codasyl la siguen utilizando.

- Una misma ocurrencia de miembro no puede pertenecer en un mismo tipo de SET a más de un propietario. Esto hace que se simplifique la implementación física de los SET, ya que sus ocurrencias se pueden organizar como una cadena.




</doc>
<doc id="606" url="https://es.wikipedia.org/wiki?curid=606" title="Commodore 64">
Commodore 64

Commodore 64 (C64, CBM 64/CBM64, C=64,C-64, VIC-64) es una computadora doméstica de 8 bits desarrollada por Commodore International en agosto de 1982 a un precio inicial de 595 dólares. Sucede a la Commodore VIC-20 y a la Commodore MAX Machine, presentando 64 kilobytes (65.536 bytes) de RAM, con gráficos y sonido muy por encima de otros equipos contemporáneos.

Utilizaba una unidad de casete además de una disquetera de tipo 5 1/4 pulgadas. Disponía de un teclado profesional muy robusto, distintas tomas de conexión y poseía infinidad de videojuegos, aplicaciones, gráficos y multimedia. Contaba con una paleta de 16 colores y un intérprete BASIC. Aceptaba la conexión directa de periféricos sin necesidad de una interfaz de conexión, (como alguno de sus más directos competidores) incorporando dos puertos de conexión de mandos de juego (joysticks), puertos serie IEC, RS232 y C2N, salida a televisión, salidas de vídeo compuesto y audio mediante conector DIN de alta fidelidad y un puerto de expansión para cartuchos. Algunos cartuchos incorporaban lenguajes de programación como COBOL, o un BASIC más avanzado, o expansión de RAM, más algunas utilidades para congelar los juegos y poder copiarlos. Su reloj funcionaba a menos de 1 Megahercio, pero sus excelentes capacidades gráficas y sonoras, hicieron de ella la computadora personal favorita de millones de usuarios caseros. Hoy en día existen programas que emulan su funcionamiento al completo, para GNU/Linux, Windows y otros sistemas operativos.

El Commodore 64 sigue siendo el modelo de computadora doméstica más vendida en el mundo. Se estiman sus ventas entre 12,5 y 17 millones de unidades

En enero de 1981, MOS Technology, Inc., diseñadora de circuitos integrados subsidiaria de Commodore, inició un proyecto para diseñar los circuitos gráfico y de audio para la nueva generación de videoconsolas. El diseño de los circuitos, llamados VIC-II (gráficos) y SID (audio) fue completado en noviembre de 1981.

Entonces se comenzó un proyecto para una videoconsola de Commodore que usara ambos circuitos, llamada "Ultimax" o "Commodore MAX Machine", creada por Yashi Terakura de "Commodore Japón". Este proyecto fue cancelado después de fabricarse algunas unidades para el mercado japonés.

Al mismo tiempo, Robert "Bob" Russell, programador de sistemas y arquitecto del VIC-20, y Robert "Bob" Yannes, ingeniero del SID, eran críticos para la línea de productos de Commodore, la cual era una continuación de la línea PET, dirigida a usuarios de negocios. Con el apoyo de Al Charpentier, ingeniero del VIC-II, y de Charles Winterble, gestor de MOS Technology, propusieron a Jack Tramiel, oficial ejecutivo en jefe de Commodore, una verdadera secuela de bajo coste del VIC-20. Tramiel dictaminó que la máquina debía tener 64 KB de RAM. Aunque 64 KB de DRAM costaban más de 100 dólares en ese momento, Tramiel sabía que los precios caían y que muy pronto tendrían un precio aceptable antes de iniciarse la producción. En noviembre, Tramiel fijó como fecha de entrega el primer fin de semana de enero, para coincidir con el Consumer Electronics Show de 1982.

El producto tenía el nombre en clave de VIC-40, al ser el sucesor del popular VIC-20. El equipo que lo construyó fue formado por Robert Russell, Robert Yannes y David A. Ziembicki. El diseño, los prototipos y algún software de prueba fue acabado a tiempo para el show, después de haber estado el equipo trabajando sin descanso incluso en los fines de semana del día de Acción de Gracias y Navidad.

Cuando el producto iba a ser presentado, el VIC-40 fue renombrado a C64 para ajustarse a las líneas actuales de negocios de Commodore, las cuales incluían el "P128" y el "B256", ambos nombrados con una letra y su respectivo tamaño de memoria.

El C64 tuvo una presentación impresionante, tal y como recuerda el ingeniero de producción David A. Ziembicki: "Todo lo que veíamos en nuestro pabellón era gente de Atari con la boca abierta, diciendo '¿Cómo pueden hacer esto por 595$?'". La respuesta era, según se vio, la integración vertical: gracias a ser Commodore la dueña de las plantas de fabricación de semiconductores de MOS Technology, cada C64 tenía un costo estimado de producción de solo 135 dólares.

Además del C64 con su diseño característico heredado del VIC-20, Commodore y otros fabricantes pusieron a la venta unos modelos derivados del C64 de base.

En 1982, Commodore desarrolló el Commodore MAX Machine en Japón, llamado Ultimax en los EE.UU., y VC-10 en Alemania. El MAX fue pensado para ser una consola de juegos y se basó en una versión muy reducida del hardware del C64. El MAX se suspendió meses después de su introducción, a causa de las malas ventas en Japón.

En 1983, para competir con Apple II en el sector de la educación en Estados Unidos, Commodore ponía en el mercado el Educator 64 que consistía esencialmente en un C64 y un monitor monocromático verde, dentro de una caja de un Commodore PET serie 4000. Las escuelas prefirieron las versiones todo-en-uno en metal de este modelo al estándar C64 con sus componentes separados, ya que podían ser fácilmente dañados, ser sometidos a actos de vandalismo y/o robo.

En 1984, Commodore proponía el primer ordenador portable con pantalla color, el Commodore SX-64, una versión portátil del C64. Tenía una pantalla CRT color de 5" (127 mm) e incluía sólo una unidad de disquete 1541, no incluía la unidad de casete (Datasette).

En 1985 aparece el Commodore 128 (y más tarde el 128D) que es retro-compatible con el C64. Es una versión mejorada del C64 con el doble de memoria (RAM), con posibilidad de mostrar en pantalla 80 columnas en modo texto, además de un diseño nuevo de la caja y del teclado. También incluya un procesador Zilog Z80 para usarlo en modo CP/M en alternancia con el Commodore BASIC 2.0

En 1986, Commodore lanzó el Commodore 64C (C64C) que era funcionalmente idéntico al original, pero con un diseño más parecido al Commodore 128. Las modificaciones fueron más allá del simple cambio de look, ya que el C64C incorporaba nuevas versiones del chip de sonido SID, del chip vídeo Vic y del chip de entrada/salida (I/O), adaptados a un nuevo voltaje en el núcleo (Vcore) de 9V (en vez de 12V). A menudo, el C64C fue vendido con el sistema operativo GEOS. En paralelo, la nueva unidad de disco Commodore 1541C cambio su look para adaptarse al nuevo C64C, además de ser más silenciosa y más fiable.

En 1990, el C64 fue re-editado en forma de una consola de juegos, llamado Commodore 64 Games System (C64GS). Se hizo una modificación simple de la placa base del C64C para orientar el conector del cartucho en posición vertical. Solo tenía las conexiones mínimas para conectar el audio, el vídeo, los mandos de juegos y el cartucho. Su ROM fue modificada para simplificar el arranque del sistema con una pantalla de inicio al estilo "Inserte el cartucho de juego". Fue diseñado para competir con la Nintendo Entertainment System y el Sega Master System, pero sus malas ventas comparadas con sus rivales fue un fracaso comercial para Commodore y nunca fue lanzado fuera de Europa.

En 2000, el fabricante Tulip Computers propone un producto basado en el C64, el C64 Direct-to-TV. Este peculiar modelo del C64 está contenido dentro de un mando de juego, de tipo Joystick. A la base, es un sistema cerrado ya que solo propone 30 juegos clásicos del C64, 2 conectores RCA (audio/vídeo) y en su interior, un ASIC funcionando a 32MHz emulando al procesador 6510, al VIC-II, al SID, al CIA y al PLA. Existen modificaciones para extender sus capacidades y mejorar su conectividad con lectores externos por ejemplo.

Otros modelos del C64, muy específico al mercado alemán, aparecieron durante esos años, como el Commodore 64 "Aldi", el Commodore 64 Golden Edition y el Commodore 64G

La Commodore 64 fue la computadora que inspiró a muchos músicos y programadores y es posiblemente el ordenador de 8 bits de culto más importante, junto con el Spectrum. A día de hoy existe una comunidad de usuarios muy activa que siguen programando para el C64, haciendo auténticas filigranas. También existe una subcultura musical dentro del Commodore 64.

También inspiró a otros empresarios como por ejemplo, la tecla Commodore, esta era una tecla especial que daba muchas funciones al presionarla combinada con otras teclas, al igual que los teclados compatibles con Windows tienen el logotipo del sistema operativo de Microsoft, y los teclados para Apple Macintosh, el logotipo de Apple Inc..

La interfaz gráfica GEOS se podía hacer arrancar desde una memoria EPROM en los cartuchos de expansión, con lo cual al encender la Commodore 64, esta iniciaba la interfaz gráfica vía hardware dándole una velocidad insuperable contra el dátasete y la disquetera. Con esto quedaba una computadora óptima para trabajo directo, incluso se logró mantener un reloj en la interfaz GEOS con lo cual mejoraba mucho sus capacidades.

Vale la pena hacer notar que la Commodore 64 tenía una excelente arquitectura de hardware dada su capacidad de expansión mediante cartuchos y una gran versatilidad.

También tenía periféricos como mouse, impresora, disquetera, discos duros, módem telefónico, joysticks, lápiz óptico, teclado de música, monitores, casetes, etc.

En el año 2005 la compañía Creative Micro Designs lanzó al mercado la CMD SuperCPU, que se conectaba en el puerto de expansión proporcionado una CPU de 16 bits WDC 65C816 a 20 MHz, que además soporta expansión de 16 MB de memoria RAM y la conexión de un disco duro.

Actualmente se pueden descargar juegos de la "Commodore 64" en la Consola Virtual de la Wii





Muchos países fabricaron clones de este modelo no siempre bajo licencia Commodore.

En Argentina la firma Drean, un fabricante de electrodomésticos argentino, compró en 1982 la licencia para fabricar localmente la C64, comenzando la producción en su planta en la provincia de San Luis en 1983. Luego, ante al éxito de ventas, fabricaría los modelos 16 en 1985 (con poco éxito de ventas), 64C a fines de 1986 (con la mayoría de sus partes fabricadas localmente) y 128 a inicios de 1986 (con unos pocos cientos de unidades producidas).

El modelo 64 se asemejaba a su par estadounidense, diferenciándose exteriormente en las siglas de encendido y en el anexo de la palabra "Drean" anteponiéndose a Commodore 64 en la carcasa de la máquina. Las mismas diferían de color en sus años de fabricación, algunas respetaban a las de Estados Unidos y otras no, ya que los plásticos se fabricaban localmente. Internamente la versión Argentina tenía salida de video PAL-N en lugar de NTSC; además, por este motivo, tenía una frecuencia de reloj mayor, lo que se podía notar en algunos juegos ya que estos fueron diseñados en su mayoría para la versión estadounidense. Otra diferencia destacable era la fuente de alimentación adaptada para 220V en lugar de los 110V originales. Estos clones legítimos eran de excelente calidad, aunque algunos usuarios se quejaban de la dureza del teclado, y 8 de cada 10 "Commodores" vendidas llegaron a ser de Drean. La Drean Commodore 64C era internamente igual al modelo 64. La diferencia radicaba en el formato de la carcasa, que era más ergonómica, similar al teclado del modelo 128. A diferencia de los otros modelos, incluía el disquete de GEOS. Drean también importó software, que era distribuido en forma de cartuchos, casetes y disquetes, con su marca o bajo la marca "Peek".

La producción finalizó a mediados de 1988, cuando Commodore International le retiró la licencia a Drean, luego de haber producido unas 300.000 unidades.





</doc>
<doc id="607" url="https://es.wikipedia.org/wiki?curid=607" title="Commodore 128">
Commodore 128

El ordenador doméstico/personal "Commodore 128" ("'"C128, CBM 128, C=128) fue la última máquina de 8 bits comercializada por Commodore Business Machines (CBM). Presentada en enero de 1985 en el CES de Las Vegas, apareció tres años después de su predecesor, el exitoso Commodore 64. El diseñador principal del hardware de la C128 fue Bil Herd.

El C128 es un sucesor significativamente expandido del C64 y, a diferencia del anterior Plus/4, casi completamente compatible con el C64. Incluye 128 KB de RAM, en dos bancos de 64 KB y una salida de vídeo RGBI de 80 columnas (gestionada por el chip 8563 VDC con 16 KB de RAM de vídeo dedicado), al igual que una caja y un teclado sustancialmente rediseñados, el cual incluía cuatro teclas de cursores (los Commodore anteriores tenían dos, requiriendo pulsar mayúsculas para mover el cursor hacia arriba o a la izquierda), además de las teclas Alt, ayuda, escape, tabulador (no presente en los modelos anteriores) y un teclado numérico. La falta de teclado numérico, tecla Alt y tecla escape en el C64 era un problema con algunas programas de productividad en CP/M cuando se usaban con el cartucho Z80 del C64. Algunas de las teclas añadidas equivalen a las presentes en el teclado de la IBM PC. Mientras que el modo de 40 columnas del C128 duplica el del C64, 1 KB extra de RAM de color está disponible para los programadores, ya que está multiplexada en la dirección de memoria 1. La fuente de alimentación representa una gran mejora con respecto al diseño poco fiable de la fuente del C64, siendo más grande y equipada con un ventilador de refrigeración y un fusible reemplazable. En lugar de un solo microprocesador 6510 como en el C64, el C128 incorpora un diseño de doble CPU. La primaria, una 8502, es una versión ligeramente mejorada de la 6510, capaz de subir el reloj hasta 2 MHz. La segunda CPU es un Zilog Z80, que se usa para hacer funcionar software CP/M, al igual que para iniciar el modo de selección de sistema operativo durante el arranque. Los dos procesadores no pueden trabajar de manera simultánea, por lo que el C128 no es un sistema multiproceso. El C128 tiene tres modos operativos: modo C128 (modo nativo), en el que funciona a 1 o 2 MHz con la CPU 8502 y dispone de los modos de texto de 40 y 80 columnas; modo CP/M, que usa el Z80 en modo texto tanto de 40 como de 80 columnas y el modo C64, que es prácticamente compatible al 100% con el anterior ordenador. La selección de estos modos está implementada a través del chip Z80, el cual controla el bus en el arranque inicial y comprueba si está presente algún cartucho C64/C128 y si la tecla Commodore (selector de modo C64) está activa durante el arranque. Basado en lo que encuentra, cambiará al modo de operación apropiado.

El C128D es considerado único debido a que su placa madre soporta cuatro tipos diferentes de RAM:

Además de 3 procesadores (8502 como principal, Z80 para CP/M y un 6502 para la disquetera) y dos chips de vídeo diferentes (VIC-IIe y VDC).

Mientras las capacidades gráficas y de sonido del C64 eran consideradas generalmente excelentes, los operativos de marketing en la dirección de Commodore estaban preocupados por sostener (si no prolongar) el atractivo de los ordenadores caseros entre los no-programadores. Estos operativos sintieron que el la pantalla de 40 columnas del VIC-II, aunque excelente para juegos, podía ser inadecuada para aplicaciones de productividad como tratamiento de textos. Además, sentían que las nuevas capacidades de sonido y gráficos tendrían un manejo pesado si se controlaban exclusivamente a través de las instrucciones estándar POKE y PEEK, así que se embarcaron en reescribir el BASIC 2.0 para que tuviera instrucciones que reflejaran con más precisión sus capacidades. Además, el aumento de la capacidad de las unidades de disco en los mercados competidores (como las dedicadas a los compatibles PC y los Apple) llevó a los oficiales de alto rango de Commodore a preocuparse por el siguiente paso. Los diseñadores de la C128 tuvieron éxito en resolver la mayoría de esas preocupaciones. Un nuevo chip, el VDC, provee al C128 con una pantalla de 80 columnas compatible con la norma CGA (también llamada RGBI por" Red-Green-Blue-Intensity", rojo-verde-azul-intensidad). El nuevo microprocesador 8502 es completamente retrocompatible con el 6510 del C64, pero puede funcionar al doble de velocidad si se desea. Aun así, el chip VIC-II que controla la pantalla de 40 columnas no puede operar a la mayor velocidad de reloj, así que la pantalla de 40 columnas aparece distorsionada en modo codice_1. En el modo 80 columnas, el editor toma ventaja de las características del VDC para permitir texto parpadeante y subrayado, activado mediante códigos de escape. Los modos de 40 y 80 columnas son independientes y pueden estar activos a la vez. Un programador con una pantalla de vídeo compuesto y otra RGB podría usar una de las pantallas como "bloc de notas". La pantalla activa puede ser intercambiada con ESC-X. Se añadió un botón de reset al sistema. El BASIC 2.0 del C64 fue reemplazado por el BASIC 7.0, que incluye instrucciones diseñadas específicamente para usar las ventajas de la máquina. Un editor de sprites y un monitor de código máquina fueron añadidos posteriormente para beneficio de los programadores . La parte del editor de pantalla del kernel fue mejorada aún más para soportar un sistema de ventanas rudimentario y fue relocalizada en una ROM separada. En modo 80 columnas el editor hace uso de las mejoras del VDC para permitir el parpadeo y subrayado de texto, activado mediante códigos de escape. Un botón de reinicio fue añadido al sistema. Dos nuevas unidades de disco fueron presentadas junto con la C128: la efímera 1570 y la 1571. Más adelante, se presentó la unidad 1581 de 3'5 pulgadas. Todas estas unidades son más fiables que la 1541 y prometían unas mejores prestaciones mediante un "modo ráfaga". La unidad 1581 también tiene más RAM en placa que sus predecesores, haciendo posible abrir un mayor número de ficheros simultáneamente. El C128 también tiene el doble de RAM que el C64 y una mayor proporción está disponible para la programación en BASIC, debido al chip MMU con cambio de bancos. Esto permite que el código de los programas BASIC sean almacenado por separado de las variables, mejorando mucho la habilidad de la máquina para manejar programas complejos, acelerando la recolección de basura y facilitando el despulgamiento para los programadores. Un programa en ejecución puede recibir un codice_2, inspeccionar la variables o alterarlas en modo directo y la ejecución se continua usando la instrucción BASIC codice_3. La ROM del C128 contiene un huevo de pascua: Introduciendo el comando "SYS 32800,123,45,6" en modo nativo, revela una pantalla de 40 columnas con un listado y un mensaje de los desarrolladores principales de la máquina. También escribiendo las instrucciones QUIT u OFF, produce un "?UNIMPLEMENTED COMMAND ERROR". Estas instrucciones existen en previsión de un ordenador portátil con pantalla LCD nunca producido y están pensados para salir del intérprete BASIC e ignorar el teclado durante la ejecución de programas sensibles, respectivamente. Las mayores capacidades del hardware del C128, especialmente el incremento de la RAM, la resolución de pantalla y la velocidad del bus serial, la convirtieron en la plataforma preferida para usar el sistema operativo gráfico GEOS.

El segundo de los dos procesadores es el Zilog Z80, el cual permite al C128 funcionar en CP/M. El C128 fue vendido con CP/M 3.0 (también conocido como CP/M Plus, retrocompatible con CP/M 2.2) y el emulador del terminal ADM31/3A. Un cartucho con el CP/M ya estaba disponible para el C64, pero era caro y está limitado a los programas en discos con formato Commodore. Para tener disponible una gran biblioteca de aplicaciones instantáneamente en su lanzamiento, el CP/M del C128 y su unidad de disquetes 1571 fueron diseñadas para leer casi todos los programas en CP/M específicos de Kaypro sin modificaciones. Desafortunadamente, el C128 era notablemente más lento trabajando en CP/M que la mayoría de sistemas CP/M dedicados, ya que el procesador Z80 funcionaba a una velocidad efectiva de solo 2 MHz (en lugar de los más comunes 4 MHz) y a su uso de CP/M 3.0, cuya complejidad lo hace inherentemente más lento que el anterior y más extendido sistema CP/M 2.2. A partir del código fuente de la implementación de CP/M para el C128, queda claro que los ingenieros planearon originalmente que fuera posible hacer funcionar a CP/M también en el modo "rápido", con la salida de 40 columnas desconectada y el Z80 funcionando a una velocidad efectiva de 4 MHz. A pesar de ello, esta característica no funcionó correctamente en la primera generación del C128. Una característica inusual del C128 entre los sistemas CP/M es que algunos de los servicios de bajo nivel de la BIOS son ejecutados por el 8502 en lugar de por el Z80. Este le transfiere el control al 8502 después de situar los parámetros pertinentes en las posiciones de memoria designadas. El Z80 se apaga entonces, siendo despertado por el 8502 al completar la rutina de la BIOS, con el/los valor/es de estado disponibles en la RAM para su inspección. El CP/M fue posiblemente el menos usado de los tres posibles modos de operación. Pensado para darle al nuevo ordenador una gran librería de programas de grado profesional, los cuales no tenía Commodore, el CP/M ya había pasado hacía tiempo su mejor momento cuando se presentó el C128. Además, CP/M es muy diferente del Commodore DOS incluido en la ROM de las unidades de disco.

Incorporando completamente el BASIC original del C64 y el kernel, el C128 consigue prácticamente el 100% de compatibilidad con el Commodore 64. El modo 64 puede ser accedido de tres manera:


Poniendo a tierra las líneas /EXROM y/o /GAME del puerto del cartucho causa que el ordenador automáticamente inicie en el modo C64. Esta característica duplica fielmente el comportamiento del C64 cuando un cartucho (como el BASIC de Simons) es enchufado en el puerto y activa esas dos líneas; pero, a diferencia del C64, donde el cambio del mapeado de memoria de estas líneas se realiza directamente en hardware, el firmware de arranque del Z80 del C128 comprueba las líneas al arrancar y entonces cambia de modo según sea necesario. Los cartuchos de modo nativo C128 son reconocidos e iniciados por la comprobación del kernel en las posiciones definidas en el mapa de memoria. El modo C64 casi duplica exactamente las características del hardware del C64. Muchas de las características adicionales del C128 están desactivadas o no disponibles en este modo. La pantalla de 80 columnas, el modo rápido, la MMU y el BASIC 7.0 no están disponible en este modo. Las 4 teclas de cursor en la parte superior del teclado no se reconocen, forzando al usuario a usar la incómoda disposición mediante mayúsculas del C64, que están incluidas en la parte inferior del teclado. También se ignoran el teclado numérico y la fila superior de teclas, excluyendo las teclas F1 a F8. Algunas de estas características pueden ser reactivadas mediante programas, pero los programas comerciales las ignorarían. Algunos de los pocos programas del C64 que fallan en un C128 funcionan correctamente cuando la tecla codice_5 es pulsada (o la tecla ASCII/Nacional en los modelos internacionales del C128). Esto tiene que ver con el mayor puerto E/S de la CPU del C128. Donde la tecla codice_6 presente en ambas máquinas es simplemente una parte mecánica que bloquea la tecla codice_7 izquierda, la tecla codice_8 en el C128 puede ser leída mediante el puerto E/S incluido en el 8502. Algunos programas del C64 se confunden con este bit de E/S extra, manteniendo la tecla codice_8 pulsada se fuerza la línea de E/S, igualando la configuración del C64 y resolviendo el problema. Un puñado de programas del C64 que escriben en $D030 (53296), a menudo como parte del bucle de inicio de los registros del chip VIC-II. Este registro de mapeo de memoria, no usado en el C64, determinado por la velocidad del reloj del sistema. Debido a que este registro ya es completamente funcional en el modo C64, una escritura inadvertida podría contaminar la pantalla de 40 columnas al cambiar la CPU a 2 MHz, en la cual la velocidad del reloj del procesador de vídeo VIC-II no puede producir una pantalla coherente. Afortunadamente, muchos programas sufren este fallo. En julio de 1986, "COMPUTE!'s Gazette" publicó un programa escrito que explotaba esta diferencia usando una matriz de interrupciones para permitir el modo rápido cuando se alcanza la parte inferior de la pantalla visible y entonces desactivarlo cuando el retrazo de la pantalla vuelve a empezar en la parte superior. Usando el reloj más alto durante el periodo de retorno vertical, la pantalla de vídeo estándar se mantiene mientras se aumenta la velocidad de ejecución en un 20%. Un modo de diferenciar entre un C64 y un C128 operando en modo C64, típicamente usado desde dentro de un programa en funcionamiento, es escribir un valor diferente a codice_10 en la dirección codice_11, un registro que es usado para decodificar las teclas extra del C128 (el teclado numérico y algunas otras teclas). En el C64 esta localización de memoria siempre contiene el valor $FF, sin importar lo que se escriba en él, pero en un C128 en modo C64, el valor de la localización (un registro de mapeo de memoria) puede ser cambiado. Así, comprobando el valor de la posición después de escribir en el revelará la verdadera plataforma de hardware.

Para manejar la relativamente grande cantidad de ROM y RAM, diez veces el espacio de direcciones de 64 KB del 8502, el C128 usa la MMU 8722 para crear diferentes mapas de memoria, en los que diferentes combinaciones de RAM y ROM son mostradas dependiendo del patrón de bits escrito en el registro de configuración de la MMU en la dirección de memoria $FF00. Las unidades de expansión de RAM de Commodore utilizan un controlador DMA externo para escribir y leer uno o más bytes (hasta rangos completos de bytes) entre la memoria RAM del C128 y la RAM de la unidad de expansión. Otra característica de la MMU es que permite relocalizar la página cero y la pila, gracias al registro de Página Directa.

La compleja arquitectura del C128 incluye cuatro tipos de acceso diferente a la RAM (128 kB de RAM principal, 64 kB VDC de Video RAM, 2 kNibbles de la RAM del VIC-II Color, 2 kB de RAM de la disquetera, 128 o 512 kB de RAM de la REU), dos CPUs (la 8502 principal, la Z80 para el CP/M; el 128D también incorpora un 6502 en la unidad de disco) y dos chips de vídeo diferentes (VIC-IIe y VDC) para sus variados modos de operación.

A finales de 1985 Commodore lanzó al mercado europeo una nueva versión del C128 con un chasis rediseñado. Llamado Commodore 128D, este nuevo modelo europeo presenta un chasis de plástico con un asa de trasporte en un costado, incorporando una unidad de disco 1571 en el chasis principal, reemplaza el teclado integrado por uno separable y añade un ventilador de refrigeración. El teclado incluye dos patas plegables para cambiar el ángulo de escritura. En la última parte de 1986, Commodore lanzó una versión del C128D en Estados Unidos y Europa llamada C128DCR ("cost reduced", coste reducido). El modelo DCR incluye un chasis de acero estampado en lugar de la versión de plástico del C128D (sin asa de trasporte), una fuente de alimentación modular similar a la incluida con el C128D, al igual que un teclado separado y una disquetera 1571. En la placa madre, Commodore consolidó algunos de los componentes para ahorrar costes de producción y reemplazó el controlador de vídeo 8563 con el técnicamente más avanzado MOS Technology 8568 (también incluido en los algunos modelos posteriores de C128D). Como medida de ahorro, el ventilador de refrigeración que integraba el modelo D fue eliminado, aunque los enganches en la fuente de alimentación se mantuvieron. Internamente, la ROM del C128DCR, llamada "ROM 1986" por la fecha de copyright mostrada en la pantalla de inicio, contiene muchas correcciones de errores, incluyendo el famoso donde el carácter 'Q' se mantiene como minúscula cuando CAPS LOCK está activo y el 8568 VDC está equipado con 64KB de RAM de vídeo, el máximo direccionable, igual a cuatro veces lo que el C128 original. El incremento en RAM de vídeo hizo posible, entre otras cosas, generar gráficos de alta resolución con una paleta de color más flexible, aunque poco software comercial saco ventaja de esta capacidad. A pesar de las mejoras en las capacidades de vídeo RGB, Commodore no actualizó el BASIC 7.0 con la posibilidad de manipular gráficos RGB. Manejar el VDC en modo gráfico continuó requiriendo el uso de llamadas a primitivas del editor de pantalla en ROM (o sus equivalentes en ensamblador), o usando extensiones de terceros del BASIC. La extensión más popular fue el "BASIC 8" de Free Spirit Software, que añadío comandos para gráficos VDC de alta resolución al BASIC 7.0. BASIC 8 estaba disponible en dos discos (disco de editor y disco de runtime) y con un chip ROM para instalación en el zócalo de ROM de funciones interno del C128.

Debido a que el C128 puede hacer funcionar virtualmente todo el software del C64 y con la siguiente generación, los ordenadores caseros de 16 y 32 bits, principalmente el Commodore Amiga y el Atari ST, ganando terreno, se creó relativamente poco software para el modo nativo del C128 (probablemente del orden de 100 a 200 títulos comerciales, más los programas en dominio público y escritos en las revistas) Mientras el C128 vendió 4 millones de unidades entre 1985 y 1989, su popularidad palidece en comparación con la de su predecesor. De esto se culpó a la falta de software nativo y al marketing menos agresivo de Commodore, enfocado mayormente en el Amiga en ese tiempo. Una explicación adicional puede encontrarse en el hecho de que el C64 vendió muchas unidades a la gente interesada principalmente en videojuegos, a los que la cara C128 no les daba demasiado valor como para actualizar. Unas pocas aventuras de Infocom tomaron ventaja de la pantalla de 80 columnas e incrementaron la capacidad de memoria y unos cuantos juegos del C64 fueron portados de manera nativa, como Kickstart 2 y The Last V8 de Mastertonic y Ultima V de Origin Systems, pero la mayoría de juegos funcionaban en modo 64. El C128 fue ciertamente una mejor máquina de negocios que el C64, pero no mucho mejor como máquina de juegos y la gente que quería una máquina para negocios compraba los compatibles IBM PC casi en exclusiva durante el tiempo en que el C128 fue vendido. Con su lenguaje de programación BASIC avanzado, su compatibilidad con CP/M y sus paquetes de software nativo "amistosos con el usuario" como Jane, Commodore intentó crear un mercado para negocios pequeños para el C128, incluso marcando "Personal Computer" en la caja, pero esta estrategia no tuvo éxito frente a sus contemporáneas compatibles IBM PC de bajo coste, como la Leading Edge Model D y la Tandy 1000 que, en algunos casos, se vendían por menos que un sistema C128 completo. Hubo un programa CAD profesional, "Home Designer" de BRiWALL, pero de nuevo, la mayor parte de ese trabajo se hacía en PC durante la vida comercial del C128. La principal razón por la que se mantuvieron las ventas del C128 bastante bien fue probablemente porque era una máquina mejor para los aficionados a la programación que el C64. También, cuando el C128(D/DCR) paró su producción en 1989, se informó que los costes de manufacturarlo igualaban los del Amiga 500, aunque el C128D tenía que venderse por cientos de dólares menos para dejar la imagen del Amiga como tope de gama intacta. Bil Herd ha indicado que los objetivos de diseño del 128 no incluían inicialmente la compatibilidad al 100% con el C64. Alguna forma de compatibilidad se intentó después de que Herd fuera contactado durante la presentación del Plus/4 por una mujer decepcionada porque los paquetes de software educativo que había escrito para el C64 no fueran a funcionar en el nuevo ordenador de Commodore. Más tarde, el departamento de marketing de Commodore demandó una compatibilidad total. Herd dio como razón para incluir un procesador Z80 en el C128 para asegurar la compatibilidad al 100%, debido a que el soporte para el cartucho del Z80 del C64 hubiera significado que el C128 tendría que haber suministrado energía adicional al puerto de cartuchos. También indicó que el chip de vídeo VDC y el Z80 eran fuentes de problemas durante el diseño de la máquina. Herd añadió que "sólo esperaba que el C128 se vendiera durante un año, nos imaginamos que un par de millones estarían bien y, por supuesto, no minaría al Amiga o incluso al C64".

Las primeras versiones del C128 ocasionalmente experimentaban problemas de fiabilidad causados por la temperatura, debido al uso de una protección electromagnética sobre la placa principal. La protección está equipada con apoyos que tocan la parte superior de los chips principales, causando que la protección actúe como un gran disipador térmico. Una combinación de falta de contacto entre la protección y los chips, la limitada conductividad térmica inherente a los empaquetados plásticos de los chips, al igual que la relativamente pobre conductividad térmica de la protección misma (que está hecha de mu-metal), daba como resultado un sobrecalentamiento y en algunos casos fallos. El chip de sonido SID es particularmente vulnerable a este problema. El remedio más común es quitar la protección, la cual Commodore solo añadió para cumplir con las regulaciones de radiofrecuencia de la FCC. El BASIC 7.0 del Commodore 128, el lenguaje de programación que viene incluido en el ordenador, puede colgarse o provocar un reinicio ejecutando codice_12. Este bug está presente en todas las máquinas Commodore de 8 bits.









</doc>
<doc id="615" url="https://es.wikipedia.org/wiki?curid=615" title="Compilador">
Compilador

En informática, un compilador es un tipo de traductor que transforma un programa entero de un lenguaje de programación (llamado código fuente) a otro. Usualmente el lenguaje objetivo es código máquina, aunque también puede ser traducido a un código intermedio ("bytecode") o a texto. A diferencia de los intérpretes, los compiladores reúnen diversos elementos o fragmentos en una misma unidad (un programa ejecutable o una librería), que puede ser almacenada y reutilizada. Este proceso de traducción se conoce como "compilación".

La construcción de un compilador involucra la división del proceso en una serie de fases que variará con su complejidad. Generalmente estas fases se agrupan en dos tareas: el análisis del programa fuente y la síntesis del programa objeto.


Alternativamente, las fases descritas para las tareas de análisis y síntesis se pueden agrupar en:


Esta división permite que el mismo generador se utilice para crear el código máquina de varios lenguajes de programación distintos y que el mismo analizador que sirve para examinar el código fuente de un lenguaje de programación concreto sirva para producir código máquina en varias plataformas. Suele incluir la generación y optimización del código dependiente de la máquina.

En 1946 se desarrolló la primera computadora digital. En un principio, estas máquinas ejecutaban instrucciones consistentes en códigos numéricos que señalaban a los circuitos de la máquina los estados correspondientes a cada operación, lo que se denominó lenguaje máquina.

Pronto los primeros usuarios de estos ordenadores descubrieron la ventaja de escribir sus programas mediante claves más fáciles de recordar que esos códigos; al final, todas esas claves juntas se traducían manualmente a lenguaje máquina. Estas claves constituyen los llamados lenguajes ensambladores.

Pese a todo, el lenguaje ensamblador seguía siendo el de una máquina, pero más fácil de manejar. Los trabajos de investigación se orientaron hacia la creación de un lenguaje que expresara las distintas acciones a realizar de una manera lo más sencilla posible para una persona. El primer compilador fue escrito por Grace Hopper, en 1952 para el lenguaje de programación A-0. En 1950 John Backus dirigió una investigación en IBM sobre un lenguaje algebraico. En 1954 se empezó a desarrollar un lenguaje que permitía escribir fórmulas matemáticas de manera traducible por un ordenador; le llamaron FORTRAN (FORmulae TRANslator). Fue el primer lenguaje de alto nivel y se introdujo en 1957 para el uso de la computadora IBM modelo 704.

Surgió así por primera vez el concepto de un traductor como un programa que traducía un lenguaje a otro lenguaje. En el caso particular de que el lenguaje a traducir es un lenguaje de alto nivel y el lenguaje traducido de bajo nivel, se emplea el término compilador.

El trabajo de realizar un compilador fue complicado de realizar. El primer compilador de FORTRAN tardó 18 años-persona en realizarse y era muy sencillo. Este desarrollo de FORTRAN estaba muy influenciado por la máquina objeto en la que iba a ser implementado. Como un ejemplo de ello tenemos el hecho de que los espacios en blanco fuesen ignorados, debido a que el periférico que se utilizaba como entrada de programas (una lectora de tarjetas perforadas) no contaba correctamente los espacios en blanco.

El primer compilador autocontenido, es decir, capaz de compilar su propio código fuente fue el creado para Lisp por Hart y Levin en el MIT en 1962. Desde 1970 se ha convertido en una práctica común escribir el compilador en el mismo lenguaje que este compila, aunque PASCAL y C han sido alternativas muy usadas.

Crear un compilador autocontenido genera un problema llamado bootstrapping, es decir el primer compilador creado para un lenguaje tiene que o bien ser compilado por un compilador escrito en otro lenguaje o bien compilado al ejecutar el compilador en un intérprete.

Esta taxonomía de los tipos de compiladores no es excluyente, por lo que puede haber compiladores que se adscriban a varias categorías:


En las primeras épocas de la informática, los compiladores eran considerados un "software" de los más complejos existentes.

Los primeros compiladores se realizaron programándolos directamente en lenguaje máquina o en ensamblador. Una vez que se dispone de un compilador, se pueden escribir nuevas versiones del compilador (u otros compiladores distintos) en el lenguaje que compila ese compilador.

Existen herramientas que facilitan la tarea de escribir compiladores o intérpretes informáticos. Estas herramientas permiten generar el esqueleto del analizador sintáctico a partir de una definición formal del "lenguaje de partida", especificada normalmente mediante una gramática formal y barata, dejando únicamente al programador del compilador la tarea de programar las acciones semánticas asociadas.

Es el proceso por el cual se traducen las instrucciones escritas en un determinado lenguaje de programación a lenguaje máquina. Además de un traductor, se pueden necesitar otros programas para crear un programa objeto ejecutable. Un programa fuente se puede dividir en módulos almacenados en archivos distintos. La tarea de reunir el programa fuente a menudo se confía a un programa distinto, llamado preprocesador. El preprocesador también puede expandir abreviaturas, llamadas a macros, a proposiciones del lenguaje fuente.

Normalmente la creación de un programa ejecutable (un típico archivo ".exe" para Windows o DOS) conlleva dos pasos. El primer paso se llama "compilación" (propiamente dicho) y traduce el código fuente escrito en un lenguaje de programación almacenado en un archivo a código en bajo nivel (normalmente en código objeto, no directamente a lenguaje máquina). El segundo paso se llama "enlazado" en el cual se enlaza el código de bajo nivel generado de todos los ficheros y subprogramas que se han mandado a compilar y se añade el código de las funciones que hay en las bibliotecas del compilador para que el ejecutable pueda comunicarse directamente con el sistema operativo, traduciendo así finalmente el código objeto a código máquina, y generando un módulo ejecutable.

Estos dos pasos se pueden hacer por separado, almacenando el resultado de la fase de compilación en archivos objetos (un típico.obj para Microsoft Windows, DOS o para Unix); para enlazarlos en fases posteriores, o crear directamente el ejecutable; con lo que la fase de compilación se almacena solo temporalmente. Un programa podría tener partes escritas en varios lenguajes (por ejemplo C, C++ y Asm), que se podrían compilar de forma independiente y luego enlazar juntas para formar un único módulo ejecutable.

El proceso de traducción se compone internamente de varias etapas o fases, que realizan distintas operaciones lógicas. Es útil pensar en estas fases como en piezas separadas dentro del traductor, y pueden en realidad escribirse como operaciones codificadas separadamente aunque en la práctica a menudo se integren juntas.

El análisis léxico constituye la primera fase, aquí se lee el programa fuente de izquierda a derecha y se agrupa en componentes léxicos (tókenes), que son secuencias de caracteres que tienen un significado. Además, todos los espacios en blanco, líneas en blanco, comentarios y demás información innecesaria se elimina del programa fuente. También se comprueba que los símbolos del lenguaje (palabras clave, operadores, etc.) se han escrito correctamente.

Como la tarea que realiza el analizador léxico es un caso especial de coincidencia de patrones, se necesitan los métodos de especificación y reconocimiento de patrones, se usan principalmente los autómatas finitos que acepten expresiones regulares. Sin embargo, un analizador léxico también es la parte del traductor que maneja la entrada del código fuente, y puesto que esta entrada a menudo involucra un importante gasto de tiempo, el analizador léxico debe funcionar de manera tan eficiente como sea posible.

En esta fase los caracteres o componentes léxicos se agrupan jerárquicamente en frases gramaticales que el compilador utiliza para sintetizar la salida. Se comprueba si lo obtenido de la fase anterior es sintácticamente correcto (obedece a la gramática del lenguaje). Por lo general, las frases gramaticales del programa fuente se representan mediante un árbol de análisis sintáctico.

La estructura jerárquica de un programa normalmente se expresa utilizando reglas recursivas. Por ejemplo, se pueden dar las siguientes reglas como parte de la definición de expresiones:


Las reglas 1 y 2 son reglas básicas (no recursivas), en tanto que la regla 3 define expresiones en función de operadores aplicados a otras expresiones.

La división entre análisis léxico y análisis sintáctico es algo arbitraria. Un factor para determinar la división es si una construcción del lenguaje fuente es inherentemente recursiva o no. Las construcciones léxicas no requieren recursión, mientras que las construcciones sintácticas suelen requerirla. No se requiere recursión para reconocer los identificadores, que suelen ser cadenas de letras y dígitos que comienzan con una letra. Normalmente, se reconocen los identificadores por el simple examen del flujo de entrada, esperando hasta encontrar un carácter que no sea ni letra ni dígito, y agrupando después todas las letras y dígitos encontrados hasta ese punto en un componente léxico llamado identificador. Por otra parte, esta clase de análisis no es suficientemente poderoso para analizar expresiones o proposiciones. Por ejemplo, no podemos emparejar de manera apropiada los paréntesis de las expresiones, o las palabras "begin" y "end" en proposiciones sin imponer alguna clase de estructura jerárquica o de anidamiento a la entrada.

La fase de análisis semántico revisa el programa fuente para tratar de encontrar errores semánticos y reúne la información sobre los tipos para la fase posterior de generación de código. En ella se utiliza la estructura jerárquica determinada por la fase de análisis sintáctico para identificar los operadores y operandos de expresiones y proposiciones.

Un componente importante del análisis semántico es la verificación de tipos. Aquí, el compilador verifica si cada operador tiene operandos permitidos por la especificación del lenguaje fuente. Por ejemplo, las definiciones de muchos lenguajes de programación requieren que el compilador indique un error cada vez que se use un número real como índice de una matriz. Sin embargo, la especificación del lenguaje puede imponer restricciones a los operandos, por ejemplo, cuando un operador aritmético binario se aplica a un número entero y a un número real. Revisa que los arreglos tengan definido el tamaño correcto.

Consiste en generar el código objeto equivalente al programa fuente. Solo se genera código objeto cuando el programa fuente está libre de errores de análisis, lo cual no quiere decir que el programa se ejecute correctamente, ya que un programa puede tener errores de concepto o expresiones mal calculadas. Por lo general el código objeto es código de máquina relocalizable o código ensamblador. Las posiciones de memoria se seleccionan para cada una de las variables usadas por el programa. Después, cada una de las instrucciones intermedias se traduce a una secuencia de instrucciones de máquina que ejecuta la misma tarea. Un aspecto decisivo es la asignación de variables a registros.

Después de los análisis sintáctico y semántico, algunos compiladores generan una representación intermedia explícita del programa fuente. Se puede considerar esta representación intermedia como un programa para una máquina abstracta. Esta representación intermedia debe tener dos propiedades importantes; debe ser fácil de producir y fácil de traducir al programa objeto.

La representación intermedia puede tener diversas formas. Existe una forma intermedia llamada «código de tres direcciones» que es como el lenguaje ensamblador de una máquina en la que cada posición de memoria puede actuar como un registro. El código de tres direcciones consiste en una secuencia de instrucciones, cada una de las cuales tiene como máximo tres operandos. Esta representación intermedia tiene varias propiedades:

La fase de optimización de código consiste en mejorar el código intermedio, de modo que resulte un código máquina más rápido de ejecutar. Esta fase de la etapa de síntesis es posible sobre todo si el traductor es un compilador (difícilmente un intérprete puede optimizar el código objeto). Hay mucha variación en la cantidad de optimización de código que ejecutan los distintos compiladores. En los que hacen mucha optimización, llamados «compiladores optimizadores», una parte significativa del tiempo del compilador se ocupa en esta fase. Sin embargo, hay optimizaciones sencillas que mejoran sensiblemente el tiempo de ejecución del programa objeto sin retardar demasiado la compilación.

La interacción entre los algoritmos utilizados por las fases del compilador y las estructuras de datos que soportan estas fases es, naturalmente, muy fuerte. El escritor del compilador se esfuerza por implementar estos algoritmos de una manera tan eficaz como sea posible, sin aumentar demasiado la complejidad. De manera ideal, un compilador debería poder compilar un programa en un tiempo proporcional al tamaño del mismo.

Cuando un analizador léxico reúne los caracteres en un token, generalmente representa el token de manera simbólica, es decir, como un valor de un tipo de datos enumerado que representa el conjunto de tokens del lenguaje fuente. En ocasiones también es necesario mantener la cadena de caracteres misma u otra información derivada de ella, tal como el nombre asociado con un token identificador o el valor de un token de número.

En la mayoría de los lenguajes el analizador léxico solo necesita generar un token a la vez. En este caso se puede utilizar una variable global simple para mantener la información del token. En otros casos (cuyo ejemplo más notable es FORTRAN), puede ser necesario un arreglo (o vector) de tókenes.

Si el analizador sintáctico genera un árbol sintáctico, por lo regular se construye como una estructura estándar basada en un puntero que se asigna de manera dinámica a medida que se efectúa el análisis sintáctico. El árbol entero puede entonces conservarse como una variable simple que apunta al nodo raíz. Cada nodo en la estructura es un registro cuyos campos representan la información recolectada tanto por el analizador sintáctico como, posteriormente, por el analizador semántico. Por ejemplo, el tipo de datos de una expresión puede conservarse como un campo en el nodo del árbol sintáctico para la expresión.

En ocasiones, para ahorrar espacio, estos campos se asignan de manera dinámica, o se almacenan en otras estructuras de datos, tales como la tabla de símbolos, que permiten una asignación y desasignación selectivas. En realidad, cada nodo del árbol sintáctico por sí mismo puede requerir de atributos diferentes para ser almacenado, de acuerdo con la clase de estructura del lenguaje que represente. En este caso, cada nodo en el árbol sintáctico puede estar representado por un registro variable, con cada clase de nodo conteniendo solamente la información necesaria para ese caso.

Esta estructura de datos mantiene la información asociada con los identificadores: funciones, variables, constantes y tipos de datos. La tabla de símbolos interactúa con casi todas las fases del compilador: el analizador léxico, el analizador sintáctico o el analizador semántico pueden introducir identificadores dentro de la tabla; el analizador semántico agregará tipos de datos y otra información; y las fases de optimización y generación de código utilizarán la información proporcionada por la tabla de símbolos para efectuar selecciones apropiadas de código objeto.

Puesto que la tabla de símbolos tendrá solicitudes de acceso con tanta frecuencia, las operaciones de inserción, eliminación y acceso necesitan ser eficientes, preferiblemente operaciones de tiempo constante. Una estructura de datos estándar para este propósito es la tabla de dispersión o de cálculo de dirección, aunque también se pueden utilizar diversas estructuras de árbol. En ocasiones se utilizan varias tablas y se mantienen en una lista o pila.

La búsqueda y la inserción rápida son esenciales también para la tabla de literales, la cual almacena constantes y cadenas utilizadas en el programa. Sin embargo, una tabla de literales necesita impedir las eliminaciones porque sus datos se aplican globalmente al programa y una constante o cadena aparecerá solo una vez en esta tabla. La tabla de literales es importante en la reducción del tamaño de un programa en la memoria al permitir la reutilización de constantes y cadenas. También es necesaria para que el generador de código construya direcciones simbólicas para las literales y para introducir definiciones de datos en el archivo de código objeto.

De acuerdo con la clase de código intermedio (por ejemplo, código de tres direcciones o código P) y de las clases de optimizaciones realizadas, este código puede conservarse como un arreglo de cadenas de texto, un archivo de texto temporal o bien una lista de estructuras ligadas. En los compiladores que realizan optimizaciones complejas debe ponerse particular atención a la selección de representaciones que permitan una fácil reorganización.

Después de los análisis sintáctico y semántico, algunos compiladores generan una representación intermedia explícita del programa fuente. Se puede considerar esta representación intermedia como un programa para una máquina abstracta. Esta representación intermedia debe tener dos propiedades importantes; debe ser fácil de producir y fácil de traducir al programa objeto.

La representación intermedia puede tener diversas formas. Existe una forma intermedia llamada «código de tres direcciones», que es como el lenguaje ensamblador para una máquina en la que cada posición de memoria puede actuar como un registro. El código de tres direcciones consiste en una secuencia de instrucciones, cada una de las cuales tiene como máximo tres operandos. El programa fuente de (1) puede aparecer en código de tres direcciones como

Esta representación intermedia tiene varias propiedades. Primera, cada instrucción de tres direcciones tiene a lo sumo un operador, además de la asignación. Por tanto, cuando se generan esas instrucciones el compilador tiene que decidir el orden en que deben efectuarse, las operaciones; la multiplicación precede a la adición al programa fuente de.
Segunda, el compilador debe generar un nombre temporal para guardar los valores calculados por cada instrucción. Tercera, algunas instrucciones de «tres direcciones» tienen menos de tres operadores, por ejemplo la primera y la última instrucciones de asignación.


La fase de optimización de código trata de mejorar el código intermedio de modo que resulte un código de máquina más rápido de ejecutar. Algunas optimizaciones son triviales. Por ejemplo, un algoritmo natural genera el código intermedio (2) utilizando una instrucción para cada operador de la representación del árbol después del análisis semántico, aunque hay una forma mejor de realizar los mismos cálculos usando las dos instrucciones

Este sencillo algoritmo no tiene nada de malo, puesto que el problema se puede solucionar en la fase de optimización de código. Esto es, el compilador puede deducir que la conversión de 60 de entero a real se puede hacer de una vez por todas en el momento de la compilación, de modo que la operación "entreal( )" se puede eliminar. Además, temp3 se usa solo una vez, para transmitir su valor a id1. Entonces resulta seguro sustituir a id1 por temp3, a partir de lo cual la última proposición de (2) no se necesita y se obtiene el código de (3).

Hay muchas variaciones en la cantidad de optimización de código que ejecutan los distintos compiladores. En lo que hacen mucha optimización llamados «compiladores optimizadores», una parte significativa del tiempo del compilador se ocupa en esta fase. Sin embargo, hay optimizaciones sencillas que mejoran sensiblemente el tiempo de ejecución del programa objeto sin retardar demasiado la compilación.

Al principio las computadoras no tenían la suficiente memoria para guardar un programa completo durante la compilación. Este problema se resolvió mediante el uso de archivos temporales para mantener los productos de los pasos intermedios durante la traducción o bien al compilar «al vuelo», es decir, manteniendo solo la información suficiente de las partes anteriores del programa fuente que permita proceder a la traducción.

Las limitaciones de memoria son ahora un problema mucho menor, y es posible requerir que una unidad de compilación entera se mantenga en memoria, en especial si se dispone de la compilación por separado en el lenguaje. Con todo, los compiladores ocasionalmente encuentran útil generar archivos intermedios durante alguna de las etapas del procesamiento. Algo típico de estos es la necesidad de direcciones de "corrección hacia atrás" durante la generación de código.





</doc>
<doc id="617" url="https://es.wikipedia.org/wiki?curid=617" title="Cláusula de Horn">
Cláusula de Horn

En lógica proposicional, una fórmula lógica es una cláusula de Horn si es una cláusula (disyunción de literales) con, como máximo, un literal positivo. Se llaman así por el lógico Alfred Horn, el primero en señalar la importancia de estas cláusulas en 1951.

Esto es un ejemplo de una cláusula de Horn:

formula_1

Una fórmula como esta también puede reescribirse de forma equivalente como una implicación:

formula_2

Una cláusula de Horn con exactamente un literal positivo es una cláusula "definite"; en álgebra universal las cláusulas "definites" resultan (aparecen) como cuasi-identidades. Una cláusula de Horn sin ningún literal positivo es a veces llamada cláusula objetivo (goal) o consulta (query), especialmente en programación lógica.

Una fórmula de Horn es una cadena textual (string) de cuantificadores existenciales o universales seguidos por una conjunción de cláusulas de Horn.

La sintaxis de una cláusula de Horn en PROLOG tiene el siguiente aspecto:
que podría leerse así: "A es hija de B si A es mujer y B es padre de A".

En términos lógicos representa la siguiente implicación:

formula_3

Por definición de implicación se obtiene la siguiente cláusula de Horn:

formula_4

Obsérvese que, en PROLOG, el símbolo ":-" separa la conclusión de las condiciones. En PROLOG, las variables se escriben comenzando por una letra mayúscula.
Todas las condiciones deben cumplirse simultáneamente para que la conclusión sea válida; por tanto, la coma (en algunas versiones de PROLOG se sustituye la coma por el símbolo &) que separa las distintas condiciones es equivalente a la conjunción copulativa.

En cambio la disyunción normalmente no se representa mediante símbolos especiales (aunque puede hacerse con el símbolo ";"), sino añadiendo reglas nuevas al programa. En este caso:
que podrían leerse así: "A es hija de B si A es mujer y B es padre de A o A es hija de B si A es mujer y B es madre de A".




</doc>
<doc id="618" url="https://es.wikipedia.org/wiki?curid=618" title="C++">
C++

C++ es un lenguaje de programación diseñado en 1979 por Bjarne Stroustrup. La intención de su creación fue extender al lenguaje de programación C mecanismos que permiten la manipulación de objetos. En ese sentido, desde el punto de vista de los lenguajes orientados a objetos, C++ es un lenguaje híbrido.

Posteriormente se añadieron facilidades de programación genérica, que se sumaron a los paradigmas de programación estructurada y programación orientada a objetos. Por esto se suele decir que el C++ es un lenguaje de programación multiparadigma.

Actualmente existe un estándar, denominado ISO C++, al que se han adherido la mayoría de los fabricantes de compiladores más modernos. Existen también algunos intérpretes, tales como ROOT.

El nombre "C++" fue propuesto por Rick Mascitti en el año 1983, cuando el lenguaje fue utilizado por primera vez fuera de un laboratorio científico. Antes se había usado el nombre "C con clases". En C++, la expresión "C++" significa "incremento de C" y se refiere a que C++ es una extensión de C.


A continuación se cita un programa de ejemplo Hola mundo escrito en C++:
/* Esta cabecera permite usar los objetos que encapsulan los descriptores stdout
y stdin: cout(«) y cin(»)*/

using namespace std;

int main()

Al usar la directiva codice_1 se le dice al compilador que busque e interprete todos los elementos definidos en el archivo que acompaña la directiva (en este caso, codice_2). Para evitar sobrescribir los elementos ya definidos al ponerles igual nombre, se crearon los espacios de nombres o codice_3 del singular en inglés. En este caso hay un espacio de nombres llamado codice_4, que es donde se incluyen las definiciones de todas las funciones y clases que conforman la biblioteca estándar de C++. Al incluir la sentencia codice_5 le estamos diciendo al compilador que usaremos el espacio de nombres codice_4 por lo que no tendremos que incluirlo cuando usemos elementos de este espacio de nombres, como pueden ser los objetos codice_7 y codice_8, que representan el flujo de salida estándar (típicamente la pantalla o una ventana de texto) y el flujo de entrada estándar (típicamente el teclado).

La definición de funciones es igual que en C, salvo por la característica de que si codice_9 no va a recoger argumentos, no tenemos por qué ponérselos, a diferencia de C, donde había que ponerlos explícitamente, aunque no se fueran a usar. Queda solo comentar que el símbolo codice_10 se conoce como operador de inserción, y a "grosso modo" está enviando a codice_7 lo que queremos mostrar por pantalla para que lo pinte, en este caso la cadena codice_12. El mismo operador codice_10 se puede usar varias veces en la misma sentencia, de forma que gracias a esta característica podremos concatenar el objeto codice_14 al final, cuyo resultado será imprimir un retorno de línea.

C++ tiene los siguientes tipos fundamentales:

El modificador codice_26 se puede aplicar a enteros para obtener números sin signo (por omisión los enteros contienen signo), con lo que se consigue un rango mayor de números naturales.

Según la máquina y el compilador que se utilice los tipos primitivos pueden ocupar un determinado tamaño en memoria. La siguiente lista ilustra el número de bits que ocupan los distintos tipos primitivos en la arquitectura x86.

Otras arquitecturas pueden requerir distintos tamaños de tipos de datos primitivos. C++ no dice nada acerca de cuál es el número de bits en un byte, ni del tamaño de estos tipos; más bien, ofrece solamente las siguientes "garantías de tipos":


Para la versión del estándar que se publicó en 1998, se decidió añadir el tipo de dato codice_16, que permite el uso de caracteres UNICODE, a diferencia del tradicional codice_15, que contempla simplemente al código de caracteres ASCII extendido. A su vez, se ha definido para la mayoría de las funciones y clases, tanto de C como de C++, una versión para trabajar con codice_16, donde usualmente se prefija el carácter "w" al nombre de la función (en ocasiones el carácter es un infijo). Por ejemplo:


Cabe resaltar que en C se define codice_16 como:
typedef unsigned short wchar_t;
Mientras que en C++ es en sí mismo un tipo de dato.

La palabra reservada codice_25 define en C++ el concepto de no existencia o no atribución de un tipo en una variable o declaración. Es decir, una función declarada como codice_25 no devolverá ningún valor. Esta palabra reservada también puede usarse para indicar que una función no recibe parámetros, como en la siguiente declaración:
int funcion (void);

Aunque la tendencia actual es la de no colocar la palabra "void".

Además se utiliza para determinar que una función no retorna un valor, como en:

void funcion (int parametro);
Cabe destacar que codice_25 no es un tipo. Una función como la declarada anteriormente no puede retornar un valor por medio de return: la palabra clave va sola. No es posible una declaración del tipo:

void t; //Está mal
En este sentido, codice_25 se comporta de forma ligeramente diferente a como lo hace en C, especialmente en cuanto a su significado en declaraciones y prototipos de funciones.

Sin embargo, la forma especial void * indica que el tipo de datos es un puntero. Por ejemplo:
void *memoria;
Indica que memoria es un puntero a "alguna parte", donde se guarda información de "algún tipo". El programador es responsable de definir estos "algún", eliminando toda ambigüedad. Una ventaja de la declaración ""void *"" es que puede representar a la vez varios tipos de datos, dependiendo de la operación de "cast" escogida. La memoria que hemos apuntado en alguna parte, en el ejemplo anterior, bien podría almacenar un entero, un flotante, una cadena de texto o un programa, o combinaciones de estos. Es responsabilidad del programador recordar qué tipo de datos hay y garantizar el acceso adecuado.

Además de los valores que pueden tomar los tipos anteriormente mencionados, existe un valor llamado NULL, sea el caso numérico para los enteros, carácter para el tipo char, cadena de texto para el tipo string, etc. El valor NULL, expresa, por lo regular, la representación de una Macro, asignada al valor "0".

Tenemos entonces que:
void* puntero = NULL;
int entero = NULL;
bool boleana = NULL;
char caracter = NULL;
El valor de las variables anteriores nos daría 0. A diferencia de la variable "caracter", que nos daría el equivalente a NULL, '\0', para caracteres.

Todo programa en C++ debe tener la función principal codice_50 (a no ser que se especifique en tiempo de compilación otro punto de entrada, que en realidad es la función que tiene el codice_50)

int main()

La función principal del código fuente main debe tener uno de los siguientes prototipos:
int main()
int main(int argc, char** argv)

Aunque no es estándar algunas implementaciones permiten
int main(int argc, char** argv, char** env)

La primera es la forma por omisión de un programa que no recibe parámetros ni argumentos. La segunda forma tiene dos parámetros: "argc", un número que describe el número de argumentos del programa (incluyendo el nombre del programa mismo), y "argv", un puntero a un array de punteros, de "argc" elementos, donde el elemento argv[i] representa el "i"-ésimo argumento entregado al programa. En el tercer caso se añade la posibilidad de poder acceder a las variables de entorno de ejecución de la misma forma que se accede a los argumentos del programa, pero reflejados sobre la variable "env".

El tipo de retorno de main es un valor entero int. Al finalizar la función codice_9, debe incluirse el valor de retorno (por ejemplo, return 0;, aunque el estándar prevé solamente dos posibles valores de retorno: EXIT_SUCCESS y EXIT_FAILURE, definidas en el archivo cstdlib), o salir por medio de la función exit. Alternativamente puede dejarse en blanco, en cuyo caso el compilador es responsable de agregar la salida adecuada.

Los objetos en C++ son abstraídos mediante una clase. Según el paradigma de la programación orientada a objetos un objeto consta de:


Un ejemplo de clase que podemos tomar es la clase perro. Cada perro comparte unas características (atributos). Su número de patas, el color de su pelaje o su tamaño son algunos de sus atributos. Las funciones que lo hagan ladrar, cambiar su comportamiento... esas son las funciones de la clase.

Este es otro ejemplo de una clase:
class Punto
//por omisión, los miembros son 'private' para que solo se puedan modificar desde la propia clase.
private:
protected:
public:

Son unos métodos especiales que se ejecutan automáticamente al crear un objeto de la clase. En su declaración no se especifica el tipo de dato que devuelven, y poseen el mismo nombre que la clase a la que pertenecen.
Al igual que otros métodos, puede haber varios constructores sobrecargados, aunque no pueden existir constructores virtuales.

Como característica especial a la hora de implementar un constructor, justo después de la declaración de los parámetros, se encuentra lo que se llama "lista de inicializadores". Su objetivo es llamar a los constructores de los atributos que conforman el objeto a construir.

Cabe destacar que no es necesario declarar un constructor al igual que un destructor, pues el compilador lo puede hacer, aunque no es la mejor forma de programar.

Tomando el ejemplo de la Clase Punto, si deseamos que cada vez que se cree un objeto de esta
clase las coordenadas del punto sean igual a cero podemos agregar un constructor como se muestra
a continuación:
class Punto

};

// Main para demostrar el funcionamiento de la clase


using namespace std;

int main () {

Si compilamos y ejecutamos el anterior programa, obtenemos una salida que debe ser similar a la siguiente:

Coordenada X: 0
Coordenada Y: 0

Existen varios tipos de constructores en C++:


Constructores + Memoria heap
Un objeto creado de la forma que se vio hasta ahora, es un objeto que vive dentro del scope(las llaves { }) en el que fue creado. Para que un objeto pueda seguir viviendo cuando se saque del scope en el que se creó, se lo debe crear en memoria heap. Para esto, se utiliza el operador new, el cual asigna memoria para almacenar al objeto creado, y además llama a su constructor(por lo que se le pueden enviar parámetros). El operador new se utiliza de la siguiente manera:
int main() {

Además, con el operador new[] se pueden crear arrays (colecciones o listas ordenadas) de tamaño dinámico:
Punto *asignar(int cuantos) {

Los destructores son funciones miembro especiales llamadas automáticamente en la ejecución del programa, y por tanto no tienen por qué ser llamadas explícitamente por el programador. Sus principales cometidos son:

Los destructores son invocados automáticamente al alcanzar el flujo del programa el fin del ámbito en el que está declarado el objeto. El único caso en el que se debe invocar explícitamente al destructor de un objeto, es cuando este fue creado mediante el operador new, es decir, que este vive en memoria heap, y no en la pila de ejecución del programa. La invocación del destructor de un objeto que vive en heap se realiza a través del operador delete o delete[] para arrays. Ejemplo:
int main() {

Si no se utilizara el operador delete y delete[] en ese caso, la memoria ocupada por unEntero y arrayDeEnteros respectivamente, quedaría ocupada sin sentido. Cuando una porción de memoria queda ocupada por una variable que ya no se utiliza, y no hay forma de acceder a ella, se denomina un 'memory leak'. En aplicaciones grandes, si ocurren muchos memory leaks, el programa puede terminar ocupando bastante más memoria RAM de la que debería, lo que no es para nada conveniente. Es por esto, que el manejo de memoria heap debe usarse conscientemente.

Existen dos tipos de destructores pueden ser públicos o privados, según si se declaran:

El uso de destructores es clave en el concepto de Adquirir Recursos es Inicializar.

Función miembro es aquella que está declarada en ámbito de clase. Son similares a las funciones habituales, con la salvedad de que el compilador realizara el proceso de Decoración de nombre ("Name Mangling" en inglés): Cambiará el nombre de la función añadiendo un identificador de la clase en la que está declarada, pudiendo incluir caracteres especiales o identificadores numéricos. Este proceso es invisible al programador. Además, las funciones miembro reciben implícitamente un parámetro adicional: El puntero this, que referencia al objeto que ejecuta la función.

Las funciones miembro se invocan accediendo primero al objeto al cual refieren, con la sintaxis: myobject.mymemberfunction(), esto es un claro ejemplo de una función miembro.

Caso especial es el de las funciones miembro estáticas. A pesar de que son declaradas dentro de la clase, con el uso de la palabra clave static no recibirán el puntero this. Gracias a esto no es necesario crear ninguna instancia de la clase para llamar a esta función, sin embargo, solo se podrá acceder a los miembros estáticos de la clase dado que estos no están asociados al objeto sino al tipo. La sintaxis para llamar a esta función estática es mytype::mystaticmember().

Las plantillas son el mecanismo de C++ para implantar el paradigma de la programación genérica. Permiten que una clase o función trabaje con tipos de datos abstractos, especificándose más adelante cuales son los que se quieren usar. Por ejemplo, es posible construir un vector genérico que pueda contener cualquier tipo de estructura de datos. De esta forma se pueden declarar objetos de la clase de este vector que contengan enteros, flotantes, polígonos, figuras, fichas de personal, etc.

La declaración de una plantilla se realiza anteponiendo la declaración template <typename A...> a la declaración de la estructura (clase, estructura o función) deseado.

Por ejemplo:
template <typename T>
T max(const T &x, const T &y) {

La función max() es un ejemplo de programación genérica, y dados dos parámetros de un tipo T (que puede ser int, long, float, double, etc.) devolverá el mayor de ellos (usando el operador >). Al ejecutar la función con parámetros de un cierto tipo, el compilador intentará "calzar" la plantilla a ese tipo de datos, o bien generará un mensaje de error si fracasa en ese proceso.

El siguiente ejemplo:
template <typename A> int myfunction(A a);
crea una plantilla bajo la cual pueden ser definidas en el código de cabecera cualesquiera funciones especializadas para un tipo de datos como int myfunction(int), int myfunction(std::string), int myfunction(bool), etcétera:
int myfunction (int a) {

int myfunction (std::string a) {

int myfunction (bool a) {

Cada una de estas funciones tiene su propia definición (cuerpo). Cada cuerpo diferente, no equivalente ("no convertible") corresponde a una especialización. Si una de estas funciones no fuera definida, el compilador tratará de aplicar las conversiones de tipos de datos que le fuesen permitidas para "calzar" una de las plantillas, o generará un mensaje de error si fracasa en ese proceso.

Todas las definiciones habilitadas de una plantilla deben estar disponibles al momento de la compilación, por lo cual no es posible actualmente "compilar" una plantilla como archivo de objeto, sino simplemente compilar especializaciones de la plantilla. Por lo tanto, las plantillas se distribuyen junto con el código fuente de la aplicación. En otras palabras, no es posible compilar la plantilla std::vector< > a código objeto, pero sí es posible, por ejemplo, compilar un tipo de datos std::vector<std::string>.

En C++ es posible definir clases abstractas. Una clase abstracta, o clase base abstracta (ABC), es una que está diseñada solo como clase "padre" de las cuales se deben derivar clases hijas. Una clase abstracta se usa para representar aquellas entidades o métodos que después se implementarán en las clases derivadas, pero la clase abstracta en sí no contiene ninguna implementación -- solamente representa los métodos que se deben implementar. Por ello, no es posible instanciar una clase abstracta, pero sí una clase concreta que implemente los métodos definidos en ella.

Las clases abstractas son útiles para definir interfaces, es decir, un conjunto de métodos que definen el comportamiento de un módulo determinado. Estas definiciones pueden utilizarse sin tener en cuenta la implementación que se hará de ellos.

En C++ los métodos de las clases abstractas se definen como funciones virtuales puras.
class Abstracta

class ConcretaA : public Abstracta
};

class ConcretaB : public Abstracta
};

En el ejemplo, la clase ConcretaA es una implementación de la clase Abstracta, y la clase ConcretaB es otra implementación.
Debe notarse que el = 0 es la notación que emplea C++ para definir funciones virtuales puras.

Una adición a las características de C son los espacios de nombre ("namespace" en inglés), los cuales pueden describirse como áreas virtuales bajo las cuales ciertos nombres de variable o tipos tienen validez. Esto permite evitar las ocurrencias de conflictos entre nombres de funciones, variables o clases.

El ejemplo más conocido en C++ es el espacio de nombres std::, el cual almacena todas las definiciones nuevas en C++ que difieren de C (algunas estructuras y funciones), así como las funcionalidades propias de C++ ("streams") y los componentes de la biblioteca STL.

Por ejemplo:
// Las funciones en esta cabecera existen dentro del espacio de nombres std::

namespace mi_paquete{

int main()

Como puede verse, las invocaciones directas a "mi_valor" darán acceso solamente a la variable descrita localmente; para acceder a la variable del espacio de nombres "mi_paquete" es necesario acceder específicamente el espacio de nombres. Un atajo recomendado para programas sencillos es la directiva using namespace, que permite acceder a los nombres de variables del paquete deseado en forma directa, siempre y cuando no se produzca alguna ambigüedad o conflicto de nombres.

Existen varios tipos de herencia entre clases en el lenguaje de programación C++. Estos son:

La herencia en C++ es un mecanismo de abstracción creado para poder facilitar y mejorar el diseño de las clases de un programa. Con ella se pueden crear nuevas clases a partir de clases ya hechas, siempre y cuando tengan un tipo de relación especial.

En la herencia, las clases derivadas "heredan" los datos y las funciones miembro de las clases base, pudiendo las clases derivadas redefinir estos comportamientos (polimorfismo) y añadir comportamientos nuevos propios de las clases derivadas.
Para no romper el principio de encapsulamiento (ocultar datos cuyo conocimiento no es necesario para el uso de las clases), se proporciona un nuevo modo de visibilidad de los datos/funciones: "protected". Cualquier cosa que tenga visibilidad protected se comportará como pública en la clase Base y en las que componen la jerarquía de herencia, y como privada en las clases que NO sean de la jerarquía de la herencia.

Antes de utilizar la herencia, nos tenemos que hacer una pregunta, y si tiene sentido, podemos intentar usar esta jerarquía: Si la frase <claseB> ES-UN <claseA> tiene sentido, entonces estamos ante un posible caso de herencia donde clase A será la clase base y clase B la derivada.

Ejemplo: clases Barco, Acorazado, Carguero, etc. Un Acorazado ES-UN Barco, un Carguero ES-UN Barco, un Trasatlántico ES-UN Barco, etc.

En este ejemplo tendríamos las cosas generales de un Barco (en C++)

class Barco {

y ahora las características de las clases derivadas, podrían (a la vez que heredan las de barco) añadir cosas propias del subtipo de barco que vamos a crear, por ejemplo:
class Carguero: public Barco { // Esta es la manera de especificar que hereda de Barco

class Acorazado: public Barco {

Por último, hay que mencionar que existen 3 clases de herencia que se diferencian en el modo de manejar la visibilidad de los componentes de la clase resultante:


La herencia múltiple es el mecanismo que permite al programador hacer clases derivadas a partir, no de una sola clase base, sino de varias. Para entender esto mejor, pongamos un ejemplo:
Cuando ves a quien te atiende en una tienda, como persona que es, podrás suponer que puede hablar, comer, andar, pero, por otro lado, como empleado que es, también podrás suponer que tiene un jefe, que puede cobrarte dinero por la compra, que puede devolverte el cambio, etc. Si esto lo trasladamos a la programación sería herencia múltiple (clase empleado_tienda):
class Persona {
};

class Empleado {
};

class EmpleadoTienda: public Persona, Empleado {
};
Por tanto, es posible utilizar más de una clase para que otra herede sus características.

La sobrecarga de operadores es una forma de hacer polimorfismo. Es posible definir el comportamiento de un operador del lenguaje para que trabaje con tipos de datos definidos por el usuario. No todos los operadores de C++ son factibles de sobrecargar, y, entre aquellos que pueden ser sobrecargados, se deben cumplir condiciones especiales. En particular, los operadores sizeof y :: no son sobrecargables.

No es posible en C++ crear un operador nuevo.

Los comportamientos de los operadores sobrecargados se implementan de la misma manera que una función, salvo que esta tendrá un nombre especial: codice_53

Los siguientes operadores pueden ser sobrecargados:


Dado que estos operadores son definidos para un tipo de datos definido por el usuario, este es libre de asignarles cualquiera semántica que desee. Sin embargo, se considera de primera importancia que las semánticas sean tan parecidas al comportamiento natural de los operadores como para que el uso de los operadores sobrecargados sea intuitivo. Por ejemplo, el uso del operador unario - debiera cambiar el "signo" de un "valor".

Los operadores sobrecargados no dejan de ser funciones, por lo que pueden devolver un valor, si este valor es del tipo de datos con el que trabaja el operador, permite el encadenamiento de sentencias. Por ejemplo, si tenemos 3 variables A, B y C de un tipo T y sobrecargamos el operador = para que trabaje con el tipo de datos T, hay dos opciones: si el operador no devuelve nada una sentencia como "A=B=C;" (sin las comillas) daría error, pero si se devuelve un tipo de datos T al implementar el operador, permitiría concatenar cuantos elementos se quisieran, permitiendo algo como "A=B=C=D=...;"

Los lenguajes de programación suelen tener una serie de bibliotecas de funciones integradas para la manipulación de datos a nivel más básico. En C++, además de poder usar las bibliotecas de C, se puede usar la nativa STL (Standard Template Library), propia del lenguaje. Proporciona una serie plantillas (templates) que permiten efectuar operaciones sobre el almacenado de datos, procesado de entrada/salida.

Las clases basic_ostream y basic_stream, y los objetos cout y cin, proporcionan la entrada y salida estándar de datos (teclado/pantalla). También está disponible cerr, similar a cout, usado para la salida estándar de errores.
Estas clases tienen sobrecargados los operadores « y », respectivamente, con el objeto de ser útiles en la inserción/extracción de datos a dichos flujos. Son operadores inteligentes, ya que son capaces de adaptarse al tipo de datos que reciben, aunque tendremos que definir el comportamiento de dicha entrada/salida para clases/tipos de datos definidos por el usuario. Por ejemplo:
ostream& operator«(ostream& fs, const Punto& punto)

De esta forma, para mostrar un punto, solo habría que realizar la siguiente expresión:
Punto p(4,5);
cout « "Las coordenadas son: " « p « endl;

Es posible formatear la entrada/salida, indicando el número de dígitos decimales a mostrar, si los textos se pasarán a minúsculas o mayúsculas, si los números recibidos están en formato octal o hexadecimal, etc.

Tipo de flujo para el manejo de ficheros. La definición previa de "ostreams/istreams" es aplicable a este apartado.
Existen tres clases (ficheros de lectura, de escritura o de lectura/escritura): ifstream,ofstream y fstream.

Como abrir un fichero:
(nombre_variable_fichero).open("nombre_fichero.dat/txt", ios::in); para abrirlo en modo lectura.
(nombrevariablefichero).open("nombre_fichero.dat/txt", ios::out); para abrirlo en modo escritura.

Ejemplo:
f.open("datos.txt", ios::in);

Como cerrar el fichero:
nombre_variable_fichero.close();

Ejemplo:
f.close();

Leer un fichero:

Escribir un fichero:

Pueden abrirse pasando al constructor los parámetros relativos a la ubicación del fichero y el modo de apertura:

Se destacan dos clases, ostringstream e istringstream. Todo lo anteriormente dicho es aplicable a estas clases.
Tratan a una cadena como si de un flujo de datos se tratase. ostringstream permite elaborar una cadena de texto insertando datos cual flujo, e istringstream puede extraer la información contenida en una cadena (pasada como parámetro en su constructor) con el operador codice_54.
Ejemplos:
ostringstream s;
s « nombre « "," « edad « "," « estatura « "," « punto(5,6) « endl;
cout « s.str();
istringstream s(cadena);
s » nombre » edad » estatura » p;

Son clases plantillas especiales utilizadas para almacenar tipos de datos genéricos, sean cuales sean. Todos los contenedores son homogéneos, es decir, una vez que se declaran para contener un tipo de dato determinado, en ese contenedor, solo se podrán meter elementos de ese tipo.
Según la naturaleza del almacenado, disponemos de varios tipos:
Para añadir elementos al final del vector, se utiliza el método push_back(const T&). Por otro lado, para eliminar un elemento del final del vector, se debe usar el método pop_back().

using namespace std;

int main() {

Además de los métodos push_back(const T&) y pop_back(), se agregan los métodos push_front(const T&) y pop_front(), que realizan lo mismo que los ya explicados, pero en el comienzo de la cola.

using namespace std;

int main() {



using namespace std;

int main() {


Pueden considerarse como una generalización de la clase de "puntero". Un iterador es un tipo de dato que permite el recorrido y la búsqueda de elementos en los contenedores.
Como las estructuras de datos (contenedores) son clases genéricas, y los operadores (algoritmos) que deben operar sobre ellas son también genéricos (funciones genéricas), Stepanov y sus colaboradores tuvieron que desarrollar el concepto de iterador como elemento o nexo de conexión entre ambos. El nuevo concepto resulta ser una especie de punteros que señalan a los diversos miembros del contenedor (punteros genéricos que como tales no existen en el lenguaje).

Combinando la utilización de templates y un estilo específico para denotar tipos y variables, la STL ofrece una serie de funciones que representan operaciones comunes, y cuyo objetivo es "parametrizar" las operaciones en que estas funciones se ven involucradas de modo que su lectura, comprensión y mantenimiento, sean más fáciles de realizar.

Un ejemplo es la función copy, la cual simplemente copia variables desde un lugar a otro. Más estrictamente, copia los contenidos cuyas ubicaciones están delimitadas por dos iteradores, al espacio indicado por un tercer iterador. La sintaxis es: De este modo, todos los datos que están entre inicio_origen y fin_origen, excluyendo el dato ubicado en este último, son copiados a un lugar descrito o apuntado por inicio_destino.

Un algoritmo muy importante que viene implementado en la biblioteca STL, es el sort. El algoritmo sort, ordena cualquier tipo de contenedor, siempre y cuando se le pasen como argumentos, desde donde y hasta donde se quiere ordenarlo.

int main() {

Entre las funciones más conocidas están swap (variable1, variable2), que simplemente intercambia los valores de variable1 y variable2; max (variable1, variable2) y su símil min (variable1, variable2), que retornan el máximo o mínimo entre dos valores; find (inicio, fin, valor) que busca valor en el espacio de variables entre inicio y fin; etcétera.

Los algoritmos son muy variados, algunos incluso tienen versiones específicas para operar con ciertos iteradores o contenedores, y proveen un nivel de abstracción extra que permite obtener un código más "limpio", que "describe" lo que se está haciendo, en vez de hacerlo paso a paso explícitamente.

El 12 de agosto de 2011, Herb Sutter, presidente del comité de estándares de C++, informó la aprobación unánime del nuevo estándar. La publicación del mismo se realizó en algún momento del 2011.

Entre las características del nuevo estándar se pueden destacar:


Además se ha actualizado la biblioteca estándar del lenguaje.

En 2011 C++11 inauguró una nueva era en la historia de C++, iniciando un ciclo trienal de lanzamiento de nuevas versiones. A C++11 le siguió C++14 y luego C++17, que es la versión actual en 2019; C++20 se encuentra próximo a estandarizarse, y ya se está trabajando en la versión C++23. Los compiladores intentan adelantarse incorporando de manera experimental algunas novedades antes de los lanzamientos oficiales. Pero cada nueva versión de C++ incluye tal cantidad de agregados que los compiladores más adelantados no suelen terminar de incorporarlos hasta dos o tres años después del lanzamiento de esa versión.

En C++, cualquier tipo de datos que sea "declarado completo" ("fully qualified", en inglés) se convierte en un tipo de datos único. Las condiciones para que un tipo de datos T sea "declarado completo" son "a grandes rasgos" las siguientes:


En general, esto significa que cualquier tipo de datos definido haciendo uso de las cabeceras completas, es un tipo de datos completo.

En particular, y, a diferencia de lo que ocurría en C, los tipos definidos por medio de struct o enum son tipos completos. Como tales, ahora son sujetos a sobrecarga, conversiones implícitas, etcétera.

Los tipos enumerados, entonces, ya no son simplemente alias para tipos enteros, sino que son tipos de datos únicos en C++. El tipo de datos bool, igualmente, pasa a ser un tipo de datos único, mientras que en C funcionaba en algunos casos como un alias para alguna clase de dato de tipo entero.

Uno de los compiladores libres de C++ es el de GNU, el compilador G++ (parte del proyecto GCC, que engloba varios compiladores para distintos lenguajes). Otros compiladores comunes son Intel C++ Compiler, el compilador de Xcode, el compilador de Borland C++, el compilador de CodeWarrior C++, el compilador g++ de Cygwin, el compilador g++ de MinGW, el compilador de Visual C++, Carbide.c++, entre otros.






A pesar de su adopción generalizada, muchos programadores han criticado el lenguaje C ++, incluyendo Linus Torvalds, Richard Stallman, y Ken Thompson. Los problemas incluyen una falta de reflexión o recolector de basura, tiempos de compilación lentos, perceived feature creep, y mensajes de error detallados, particularmente de la metaprogramación de plantilla.

Para evitar los problemas que existen en C ++, y para aumentar la productividad, algunas personas sugieren lenguajes alternativos más recientes que C ++, como D, Go, Rust y Vala.





</doc>
<doc id="620" url="https://es.wikipedia.org/wiki?curid=620" title="Código alfanumérico">
Código alfanumérico

Con un código de un bit podemos representar 2=2 combinaciones. Para representar los diez dígitos (0-9) y las 26 letras minúsculas necesitamos como mínimo 6 bits (2=32, 2=64 combinaciones). Si además se quieren representar las letras mayúsculas y otros símbolos de utilidad necesitaremos un mayor número de bits. En general con el término de carácter o código alfanumérico se incluyen: 


La información que la computadora debe procesar está formada por letras, números y símbolos especiales. 

Los ordenadores trabajan con voltajes fijos que se representan con los números 0 y 1 que forman la base de un sistema binario. Con la presencia o ausencia de tensión eléctrica no solo se pueden representar números sino los estados de una variable lógica Con un conjunto de variables lógicas se puede definir cualquier sistema lógico.



</doc>
<doc id="621" url="https://es.wikipedia.org/wiki?curid=621" title="Código de caracteres de 8 bits">
Código de caracteres de 8 bits

Con un código de caracteres de 8 bits (1 byte) se pueden representar hasta 2 = 256 caracteres diferentes. 

Existe un código, que desde que fue definido en 1963, ha sido adoptado como el estándar para la transmisión de datos. Este código denominado ASCII ("American Standard Code for Information Interchange") permite representar hasta 128 caracteres diferentes, para ello necesita 7 bits (2 = 128 permutaciones). Normalmente el código ASCII se extiende a 8 bits (1 byte) añadiendo un bit de control, llamado bit de paridad.

A pesar de que el código ASCII es el más ampliamente difundido, algunas empresas de sistemas informáticos crearon sus propios códigos alfanuméricos. Tal es el caso de IBM, en cuyos equipos suele utilizarse el código alfanumérico denominado EBCDIC. 

El conjunto de caracteres alfabéticos representables en ASCII no engloba los caracteres acentuados, ni otros diacríticos. Así, surgieron otros códigos de 8 bits compatibles con ASCII especialmente creados para representar otras lenguas además del inglés. El ISO 8859-1, por ejemplo, posee caracteres apropiados para el español, portugués, francés y otras lenguas latinas.



</doc>
<doc id="622" url="https://es.wikipedia.org/wiki?curid=622" title="Ciencia">
Ciencia

La ciencia (del latín "scientĭa", ‘conocimiento’) es un <section begin=definición/>sistema ordenado de conocimientos estructurados que estudia, investiga e interpreta los fenómenos naturales, sociales y artificiales.<section end=definición/> El conocimiento científico se obtiene mediante observación y experimentación en ámbitos específicos. Dicho conocimiento es organizado y clasificado sobre la base de principios explicativos, ya sean de forma teórica o práctica. A partir de estos se generan preguntas y razonamientos, se formulan hipótesis, se deducen principios y leyes científicas, y se construyen modelos científicos, teorías científicas y sistemas de conocimientos por medio de un método científico.

La ciencia considera y tiene como fundamento la observación experimental. Este tipo de observación se organiza por medio de métodos, modelos y teorías con el fin de generar nuevo conocimiento. Para ello se establecen previamente unos criterios de verdad y un método de investigación. La aplicación de esos métodos y conocimientos conduce a la generación de nuevos conocimientos en forma de predicciones concretas, cuantitativas y comprobables referidas a observaciones pasadas, presentes y futuras. Con frecuencia esas predicciones se pueden formular mediante razonamientos y estructurar como reglas o leyes generales, que dan cuenta del comportamiento de un sistema y predicen cómo actuará dicho sistema en determinadas circunstancias.

Desde la revolución científica, el conocimiento científico ha aumentado tanto que los científicos se han vuelto especialistas y sus publicaciones se han vuelto muy difíciles de leer para los no especialistas. Esto ha dado lugar a diversos esfuerzos de divulgación científica, tanto para acercar la ciencia al gran público, como para facilitar la compresión y colaboración entre científicos de distintos campos.





</doc>
<doc id="625" url="https://es.wikipedia.org/wiki?curid=625" title="Command &amp; Conquer">
Command &amp; Conquer

Command & Conquer es una franquicia de videojuegos de estrategia en tiempo real, originalmente creada por Westwood Studios y posteriormente adquirida por Electronic Arts. "Command & Conquer" es considerada un hito y una figura icónica en la industria de los videojuegos de PC, y su comunidad de seguidores es sumamente activa en cooperación con los actuales poseedores de la licencia.

En su inicio, la historia de Command & Conquer estaba diseñada para ser una continuidad histórica alternativa a nuestra propia realidad. Sin embargo, con la aparición de nuevas entregas, la historia terminó ramificándose en tres variantes. La primera de ellas sigue la historia original de las Guerras Tiberianas; la segunda, Red Alert, trata sobre la guerra de los aliados contra los soviéticos; y la tercera, denominada Generals, muestra un futuro mundial no muy ajeno a la actual realidad política global. Originalmente Red Alert había sido diseñada como una precuela de Tiberian Dawn, pero con la aparición de posteriores juegos quedó oficialmente desvinculada y muy probablemente continúe de esta forma debido a la perdida del público tras abandonar la idea original y dedicarla a la propaganda militar con la migración del género e introduciendo bandos "árabes y chinos" y alejándose de la historia "tiberiana" tanto como de su público objetivo. Motivo por el que se han censurado muchos juegos "incitar a supuestos conflictos entre naciones" y por lo que parece que el poco aclamado generals 2 no ha visto la luz. En el caso de las Guerras Tiberianas, como en las de Red Alert, este artículo sigue las líneas argumentales utilizadas por Westwood, aunque en estos juegos generalmente hay dos líneas argumentales posibles.

El guion original de Command & Conquer, cuya primera entrega ahora es conocida como Tiberian Dawn ("Amanecer Tiberiano"), basa su argumento en la aparición del Tiberio, una sustancia de origen extraterrestre que en 1995 se infiltra en el ecosistema planetario mutándolo mediante la extracción de los recursos del mismo y su concentración en cristales relativamente fáciles de recolectar. La humanidad descubre que la sustancia traída por el meteorito es una fuente importante de materia prima, perfecta como fuente de recursos industriales y energéticos (como la madera lo fue en su tiempo).

Pronto, sin embargo, se descubre que el Tiberio es altamente dañino para todas las formas de vida basadas en el carbono, humanos incluidos. Este factor genera una creciente inestabilidad mundial, particularmente en países de climas cálidos, donde el Tiberio se expande a mayor velocidad (probablemente debido a que la mutación molecular se ve favorecida por una mayor actividad térmica).

En Tiberian Dawn, un asteroide de un material inicialmente desconocido impacta el planeta en diversas áreas, cubriendo en mayor o menor medida al 75% de la superficie. Inicialmente y después de que la situación política se tranquiliza, una poderosa organización paramilitar seudorreligiosa de oscuros orígenes conocida como la Hermandad de Nod se ve interesada en la explotación y desarrollo de tecnologías basadas en el Tiberio, cuyas ganancias económicas obtenidas de estas acciones le permiten subvencionar sus planes de dominación mundial. Ante esta situación, la Organización de las Naciones Unidas autoriza la creación de una fuerza militar denominada GDI ("Global Defense Initiative" o Iniciativa Global de Defensa) para hacer frente a la amenaza de Nod. Pronto el mundo se ve envuelto en una guerra abierta entre ambas facciones. En esta guerra, los ejércitos de la GDI hacen frente a Nod logrando considerables victorias en el campo de batalla. Sin embargo, una campaña mediática por parte de Nod hace difícil mantener la GDI como institución, desacreditándola públicamente y forzándola a actuar en la clandestinidad. Tras un fallido ataque contra EE.UU. por parte de Nod y la recuperación de su estatus legal, la GDI localiza a Kane, líder de la Hermandad de Nod, y su centro de mando en el Templo Base de Sarajevo. La GDI realiza un asalto acorazado conjuntamente con un ataque de bombardeo mediante el Cañón Orbital de Iones en un supremo esfuerzo por poner fin a la Primera Guerra Tiberiana.

La Segunda Guerra Tiberiana, realizada con el título de Tiberian Sun (el cual se traduce literalmente como Sol Tiberiano, aunque podría también entenderse como Mediodía Tiberiano) transcurre 20 años después de los eventos de Tiberian Dawn. Tiberian Sun muestra una tierra devastada por el Tiberio, en donde el 10% de la superficie planetaria tiene niveles de contaminación tiberiana que dañan los ecosistemas y las poblaciones humanas. Mientras, grandes migraciones de diversas áreas del planeta tratan de dirigirse a las regiones polares donde el Tiberio prácticamente no existe.

En ese contexto, los gobiernos nacionales han desaparecido y la GDI se ha convertido en la heredera política y territorial de las naciones más potentes de la ONU. La Hermandad de Nod, que sobrevivió a la muerte de su líder, ocupa diversos territorios sumamente ricos en Tiberio y está dividida en diversas células autosostenidas que pronto se unifican bajo la acción del general Anton Slavik y dirigidas por el supuestamente muerto Kane, el cual emprende una campaña mundial con el fin último de acelerar la infestación tiberiana y forzar a la humanidad a una mutación masiva que los adapte a este ambiente (aunque los eventos en Tiberium Wars podrían indicar que en realidad sus fines eran otros).

Con posterioridad a Tiberian Sun ocurren los eventos Firestorm. En esta ocasión la GDI y la Hermandad de Nod deben hacer frente a la amenaza de CABAL, un superordenador originalmente perteneciente a Nod que intenta reducir a la humanidad a ejércitos de cyborgs bajo su mando. La guerra contra CABAL fuerza a la GDI y a la Nod a aliarse, al final, su poderío bélico y tecnología militar combinada logran derrotar a CABAL, aunque se insinúa en el video final de la Nod que CABAL habría logrado escapar a la aniquilación e incluso habría actuado bajo las órdenes de Kane.

El primer Shooter de la saga narra las aventuras del comando Nick "Havoc" Parker de la GDI en su lucha contra las fuerzas especiales de la Hermandad de Nod, los Mano Negra, durante la campaña de Renegade (se creó únicamente una para la GDI). Havoc se enfrenta a los guerreros más poderosos de los Mano Negra a la con la ayuda de ex miembros de un equipo elite que trabajaba para GDI, así como la misma GDI para detener los planes de Nod para crear ejércitos de soldados mutantes cibernéticamente mejorados mediante el proyecto "ReGenesis". En este episodio se demuestra que Kane (el líder de la hermandad) sigue vivo antes de que este sea derrotado en Sarajevo.

La Tercera Guerra Tiberiana, situada en el 2047, nos muestra a la GDI asentada en las "zonas azules", territorios con poca o nula infestación tiberiana, donde una próspera población vive en modernas y futuristas ecociudades. Este territorio abarca el 20 por ciento de la Tierra y son considerados los últimos refugios de la civilización humana (al observar las áreas azules en el vídeo de presentación inicial de la GDI, se pueden observar entre otras zonas, la Costa Este de Estados Unidos, Tierra del Fuego, Sídney, Japón, Groenlandia e Islandia).

La Hermandad de Nod, por su parte, se ha convertido en un seudoestado mundial que abarca las "zonas amarillas" (50 por ciento del planeta), territorios donde la guerra, la desaparición de gobiernos nacionales, la infestación del Tiberio y los desórdenes ecológicos y climáticos han producido la ruina y el empobrecimiento de la población de estos sectores (se mencionan y observan las zonas de la Costa Este de Brasil, el Desierto del Amazonas, los Páramos de Carolina, el Centro de Australia y Europa del Este); pronto la Hermandad de Nod se vuelven los únicos mantenedores de orden, seguridad y estabilidad en las zonas amarillas. Este hecho ha conseguido que la mayoría de la humanidad (80 por ciento de la población mundial), situada precisamente en estas zonas, se vuelvan partidarios de la Hermandad de Nod frente a los "opresores" de la GDI. Según la información ofrecida por el vídeo inicial de la GDI, Nod habría estado en un estado de letargo y secretismo, produciendo el cierre de bases de la GDI en varias zonas amarillas.

El 30 por ciento restante del planeta abarca lo que se conoce como "zonas rojas", territorios situados en la línea ecuatorial (se destacan, la Costa Mediterránea, Centroamérica, Costa Oeste de Estados Unidos, el Páramo del Amazonas y Centro de África) donde la infestación tiberiana es tan grande que hace la vida humana imposible. El clima en esas regiones es virtualmente apocalíptico, con huracanes categoría 8, tornados de hasta 1700 km/h, lluvias radiactivas. También en esas zonas, el tiberio ha comenzado a infestar los océanos, formando arrecifes de varios kilómetros cuadrados.

La aparición de una tercera facción de origen alienígena en medio del conflicto iniciado por Nod, llamados Scrin, los cuales fueron los que enviaron a la Tierra el tiberium, con el plan de convertir a la tierra en una zona de recolección del mismo, anteriormente se habló de esta facción en entregas anteriores, hace que Tiberium Wars, que abarca la Tercera Guerra Tiberiana, sea la primera entrada de la línea del tiberio en disponer de tres bandos completamente distintos.

Para comprender la historia necesitarás jugar con las tres facciones ya que cada una te contará todos los hechos como van sucediendo parte por parte en cada una de esta facciones. También se tiene que tener en claro que cada facción tiene una superioridad en un tipo de unidad.

Los eventos relatados en "La ira de Kane" muestran todo lo ocurrido desde el fin de la Segunda Guerra del Tiberio, en el preciso momento en que CABAL fue destruido y la Hermandad de Nod fue desintegrada, hasta 10 años después del final de la tercera guerra del tiberio.

Cuenta tanto con unidades como edificios nuevos, que en ocasiones son de tecnología anterior a los mostrados en Tiberium Wars, debido a la anterioridad en la línea del tiempo. Además, se complementa con un total de 6 subfacciones nuevas, 2 por cada facción base.

Como curiosidad, en la subfacción de la GDI "Garras de Acero" encontramos unidades originales del Tiberian Sun tales como el Titán o el Wolverine.

Cuenta la desaparición del Tácito y como el NOD lo recupera y cómo resucitan a nuevos cyborgs más experimentados llamados: los Iluminados y los despiertos.

En el juego, cada una de las facciones tiene su propio y particular estilo de juego:

Sus mayores armas: El MARV o el Mamoth (tanque y andador).

Mejores armas: Avatar, bombarderos invisibles y su gran variedad de unidades (como el
tanque invisible o el cañón de rayos).

Mejores Armas: Unidades espaciales.

Esta línea se basa en el argumento de que Albert Einstein viaja mediante la Cronosfera (Una máquina para viajar tanto en el tiempo como en el espacio) de 1946 a 1924, una vez en esta época elimina a Adolf Hitler de la historia para evitar la Segunda Guerra Mundial, sin embargo, al no existir Hitler ni la Alemania Nazi los soviéticos bajo el mando de Joseph Stalin no encuentran razón para llegar a una cooperación con los aliados occidentales e inician una campaña de conquista euroasiática, este conflicto sería conocido como la Gran Segunda Guerra Mundial.

Originalmente este juego era un protosecuela de Tiberian Dawn, incluso hace su aparición Kane como asesor de Stalin. La historia comienza con el inicio de la invasión de Europa oriental por parte de la Unión Soviética, los aliados inicialmente se ven derrotados por un atacante superior en número y en poder blindado, debido a esto deben hacer uso de la astucia mediante ataques quirúrgicos para poder ralentizar el ataque de los soviéticos, con esto logran ganar algo de tiempo, sin embargo pronto casi toda Europa Occidental es ocupada por los rusos, en esta situación se revela que para forzar la derrota de los aliados Stalin ordena un ataque nuclear contra las más grandes ciudades de los aliados, incluyendo a Londres y París, en una carrera contra el tiempo un grupo de comandos dirigidos por la agente Tanya logran infiltrarse en la central de guía de armas nucleares de Leningrado (actualmente San Petersburgo) y neutralizan el ataque anulando las cargas explosivas de los misiles. El ataque de la Unión Soviética y la bravura demostrada por los europeos al detener lo que claramente se muestra como una amenaza mundial incitan a Estados Unidos, India y Japón a unirse a la lucha contra los comunistas, finalmente, las fuerza unidas de estas naciones en combinación con la tecnología de la cronosfera logran derrotar a los soviéticos, ocupar su territorio y eliminar a Stalin.

"Red Alert" tuvo dos expansiones, "Counterstrike" y "The Aftermath", esta última agrega nuevas unidades a ambos bandos, tales como el cronotanque aliado y el soldado de shock soviético, ambas expansiones agregan nuevas campañas para ambos bandos.

"Red Alert 2" reafirma la independencia de esta línea argumental con respecto a la de las guerras tiberianas y nos sitúa en la Tercera Guerra Mundial, la cual estalla con la invasión a Estados Unidos por parte de una Unión Soviética rearmada y más poderosa que nunca, Bajo el mando del premier Romanov, su mano derecha Yuri (Experimento con habilidades psíquicas y control mental) y el general Vladimir, los aliados deben hacer frente a los soviéticos, los cuales logran capturar gran parte de las Américas, sin embargo, y pese al poder bélico de los soviéticos los aliados al mando del presidente Michael Dugan y el general Carville y equipados con la tecnología de guerra futurista ideada por el profesor Einstein (Incluyendo La Máquina de Tormenta, La Cronoesfera, El Satélite Espía, El Generador Gap y La Torre de Prisma) logran expulsar a los soviéticos del continente a la par que detienen su avance en Europa. Determinados a obtener la victoria de una vez por todas los aliados teletransportan un potente ejército a Moscú y tras una violenta lucha capturan al premier Romanov, líder de los soviéticos, forzando a estos a rendirse en la versión de la victoria aliada. En cambio, en la versión de la victoria soviética, Yuri se vuelve contra Vladimir controlando la mente de Romanov para así tener el dominio mundial, después de capturar la mente del General Vladimir, Romanov sale del trance hipnótico e informa a su ejército de la traición de Yuri ordenando destruirlo y después terminan conquistando el territorio Americano con la destrucción de la última cronosfera, pero la tecnología Yuri aparentemente destruida, no lo fue.

La expansión de "Red Alert 2", en esta entrega aparece como personaje central Yuri, el cual traiciona a los soviéticos y revela un ejército psíquico y una red de dispositivos de control mental alrededor del planeta listos para ser usados, la historia comienza poco tiempo después del fin de los eventos de Red Alert 2, los líderes aliados son llamados a una reunión de emergencia, Yuri se infiltra en la transmisión y les revela su plan de usar los Dominadores Psíquicos para controlar mentalmente a toda la población del planeta. Después de un intento de persuadir a Yuri de que desista de sus planes el presidente Dugan ordena un ataque aéreo al Dominador Psíquico, el ataque falla pero uno de los aviones se estrella contra la fuente de poder del dispositivo, desactivándolo, los aliados se aprovechan de este fallo para activar una máquina de viaje en el tiempo con la cual van al pasado e inician una campaña planetaria contra Yuri, debido al peligro en el que este se ha convertido, los soviéticos deciden realizar un cese al fuego con los aliados y se unen para derrotar a Yuri, dicha batalla llega a tener como campo de batalla incluso la luna (en el caso de la campaña de los Soviéticos), al final los Aliados terminan acorralando a Yuri en la Antártida, capturándolo y, posteriormente, encerrándolo en el Aislador Psíquico del Profesor Einstein. En cambio, en la versión de la victoria Soviética, la Fortaleza de Yuri es destruida, pero este a la vez trata de retroceder en el tiempo para utilizar de nuevo los Dominadores Psíquicos; lo que el no sabe es que si no utilizaba la energía de reserva, esto podría hacer que volviera hacia otra época muy antigua y, así, Yuri llega a la época de la prehistoria, en donde es eliminado por un dinosaurio. El resultado de este viaje en el tiempo permite evitar que el general Carville sea eliminado y al final de la historia aparece en la línea temporal de Red Alert explicándole al presidente Dugan de esta continuidad los eventos que modificaron la historia (en el caso de la victoria Aliada). En la versión soviética, las tropas comandadas por Romanov logran invadir Estados Unidos y triunfar sobre los aliados.

La tercera entrega de la serie, salió a la venta el 28 y 31 de octubre de 2008 y cuenta con tres facciones: los Aliados, los Soviéticos y el Imperio del Sol Naciente. A pesar de que tiene muchas características de los juegos anteriores (secuencias cinemáticas, agilidad del juego) introduce una mayor presencia de las unidades marítimas, así como una campaña cooperativa. El juego ocurre en un universo paralelo en el cual la Segunda Guerra Mundial nunca ocurrió y la Unión Soviética se elevó en cambio como una amenaza en los años 50. Las primeras guerras mundiales, son entre los soviéticos y los aliados, pero ahora hay una nueva y tercera facción conocida como el Imperio del Sol naciente, que es, nada más ni nada menos, que el Japón Imperial, que quiere dominar el mundo mediante un poderoso ejército que se basa en la superioridad tecnológica.

El juego es una continuación del "Command and Conquer Red Alert 2". En esta sección los soviéticos, al estar ya derrotados, usan su propia máquina del tiempo y matan a Albert Einstein cambiando la línea de tiempo del juego, dándoles un gran poder a los soviéticos y una retirada de los aliados de Europa de este. Pero este cambio también hizo a ambos mandos otro enemigo en común, el imperio del sol naciente, que empezó atacando a los soviéticos y posteriormente los aliados.

Aunque originalmente las facciones de "Red Alert" eran solo dos, con la llegada de Yuri's Revenge vimos la introducción de una tercera facción, cada uno de estos ejércitos muestran características diametralmente opuestas a los otros dos:





Considerado un hilo argumentativo totalmente ajeno al resto del universo de Command & Conquer, Generals nos brinda un juego ambientado en un futuro próximo donde los Estados Unidos y la República Popular China, superpotencias mundiales, luchan contra el terrorismo global del GLA (Global Liberation Army), una organización subversiva internacional de corte árabe.
Este juego es el único que podría ser considerado como situado en nuestra propia línea temporal, Generals trae como novedad el uso de la barra de mando tipo Starcraft como sustituto de la tradicional barra lateral de control de los anteriores Command & Conquer, además, usa el sistema de construcción semejante a Age of Empires en vez del clásico MCV (Mobile Construction Vehicle o Vehículo de Construcción Móvil traducido).
Estas y otras características tales como la ausencia de vídeos de live-action han hecho de que algunos jugadores no consideren a Generals un Command & Conquer, sin embargo esto no le quita a Generals el ser uno de los juegos más populares de la saga, un factor que lo hace muy interesante son las habilidades de general, las cuales se obtienen a medida que el jugador gana puntos de experiencia que permiten activarlas.

En un futuro no muy lejano el mundo vive el azote de la liberación mundial dirigido por el GLA, una organización de origen árabe empeñada en liberar los pueblos tercermundistas de la soberanía occidental, ante este levantamiento las superpotencias de EE.UU. y China inician operaciones militares que desembocan en un conflicto armado a nivel global y en que cada bando desplegara letales dispositivos y tácticas radicalmente diferenciadas.
La historia comienza con un ataque por parte del EE.UU en plena plaza de Tiannanmen, haciendo semejante ataque aparecer como si fuera realizado por parte del GLA, lo cual conduce a una escalada militar China, pronto el ejército chino empuja a los GLA hacia sus bases centrales en medio oriente, dejando a la organización en estado crítico, pero pronto una eficaz campaña de saqueo y operaciones furtivas dirigidas por el GLA contra China le permiten recuperar el terreno perdido y finalmente, tras robar agentes bioquímicos protegidos por los estadounidenses, lanzan un ataque con armas de destrucción masiva a los ejércitos de EE.UU. Esta acción fuerza a EE.UU. a incrementar la injusta guerra contra el GLA y tratar de detener a toda costa su campaña de liberación global.

Zero Hour continúa con la línea histórica de "Generals", brindando la posibilidad de acceder a ejércitos especializados dirigidos por generales veteranos en determinadas tácticas y estrategias.
La campaña de Zero Hour narra como es que el ejército de EE.UU. prosigue la lucha contra las fuerzas del GLA, las cuales se han reagrupado alrededor de un General especializado en armas bioquímicas conocido como el doctor Thrax, este prosigue la campaña de liberación del GLA contra los ejércitos estadounidenses, y consigue derrotar a los estadounidenses en Arabia, sin embargo EE.UU logra una vez más reagruparse y mediante una atrevida campaña de ataques quirúrgicos fuerza a los chinos a retirarse de Europa, dejando este continente en manos del caos, sin embargo los GLA hacen su aparición logrando tras encarnizadas batallas la liberación de Europa y la formación de una alianza militar entre esta y los pueblos tercermundistas.

"Command & Conquer: Generals" dispone de tres facciones, posteriormente en "Zero Hour" estas se ven incrementadas con ejércitos especializados dirigidos por sus respectivos generales:





"Command & Conquer: Renegade" difiere del resto en que pasó a ser un videojuego del género disparos en primera persona, donde tomábamos el protagonismo a través del capitán Nick "Havoc" Parker.

"Red Alert", "Red Alert 2" y "Generals" modificaron el guion acercándolo más a nuestra historia de igual forma integrando muchos de nuestros avances tecnológicos.

El 31 de agosto de 2007, "Command & Conquer Gold", incluido en la recopilación "Command & Conquer: The First Decade", es liberado para su descarga freeware con motivo del 12º aniversario de la aparición de la saga.

El 31 de agosto de 2008, "", es liberado para su descarga de forma gratuita con motivo del 13º aniversario de la aparición de la saga.


En mayo de 2020 EA anunció, en el contexto del lanzamiento de Remastered Collection de C&C, que va a liberar bajo licencia GPL el código fuente de dos librerías DLL de Tiberian Dawn y Red Alert para facilitar el desarrollo de mods por parte de la comunidad.



</doc>
<doc id="627" url="https://es.wikipedia.org/wiki?curid=627" title="Compresión de datos">
Compresión de datos

En ciencias de la computación, la compresión de datos es la reducción del volumen de datos tratables para representar una determinada información empleando una menor cantidad de espacio. Al acto de compresión de datos se denomina «compresión», y al contrario «descompresión».

El espacio que ocupa una información codificada (datos, señal digital, etc.) sin compresión es el producto entre la frecuencia de muestreo y la resolución. Por tanto, cuantos más bits se empleen mayor será el tamaño del archivo. No obstante, la resolución viene impuesta por el sistema digital con que se trabaja y no se puede alterar el número de bits a voluntad; por ello, se utiliza la compresión, para transmitir la misma cantidad de información que ocuparía una gran resolución en un número inferior de bits.

La compresión es un caso particular de la codificación, cuya característica principal es que el código resultante tiene menor tamaño que el original.

La compresión de datos se basa fundamentalmente en buscar repeticiones en series de datos para después almacenar solo el dato junto al número de veces que se repite. Así, por ejemplo, si en un fichero aparece una secuencia como "AAAAAA", ocupando 6 bytes se podría almacenar simplemente "6A" que ocupa solo 2 bytes, en algoritmo RLE.

En realidad, el proceso es mucho más complejo, ya que raramente se consigue encontrar patrones de repetición tan exactos (salvo en algunas imágenes). Se utilizan algoritmos de compresión:


A la hora de hablar de compresión hay que tener presentes dos conceptos:


La información que transmiten los datos puede ser de tres tipos:


Teniendo en cuenta estos tres tipos de información, se establecen tres tipologías de compresión de la información:


El objetivo de la compresión es siempre reducir el tamaño de la información, intentando que esta reducción de tamaño no afecte al contenido. No obstante, la reducción de datos puede afectar o no a la calidad de la información:




</doc>
<doc id="632" url="https://es.wikipedia.org/wiki?curid=632" title="Cadmio">
Cadmio

El cadmio es un elemento químico de número atómico 48 situado en el grupo 12 de la tabla periódica de los elementos. Su símbolo es Cd. Es un metal pesado, blando, blanco azulado, relativamente poco abundante. Es uno de los metales más tóxicos. Normalmente se encuentra en menas de zinc y se emplea especialmente en pilas.

El cadmio (en latín, "cadmia", y en griego "kadmeia", que significa "calamina", el nombre que recibía antiguamente el carbonato de zinc) fue descubierto en Alemania en 1817 por Friedrich Strohmeyer. Lo encontró como impureza de algunas muestras de carbonato de zinc. Strohmeyer observó que esas muestras, en particular, cambiaban de color al calentarlas, lo cual no le ocurría al carbonato de zinc puro. Strohmeyer fue lo suficientemente persistente para continuar la observación consiguiendo aislar el elemento mediante el tueste y posterior reducción del sulfuro.

El cadmio es un metal pesado, de color blanco azulado, el cual podemos encontrar en toda la corteza terrestre.

Su estado de oxidación más común es el +2. Puede presentar el estado de oxidación +1, pero es muy inestable.

No se encuentra en la naturaleza en estado puro sino que por afinidad química está asociado con metales como el zinc, el plomo y el cobre.

Asociado a la contaminación ambiental e industrial, es uno de los mayores tóxicos , ya que reúne las cuatro características básicas más peligrosas de un tóxico:


Las concentraciones de cadmio en los diferentes compartimentos de la naturaleza son las siguientes:


Podemos hallar cadmio en la atmósfera, el agua y el suelo. De forma natural grandes cantidades de cadmio son liberadas al ambiente, aproximadamente 25.000 toneladas al año, de las cuales gran parte son vertidas a los ríos procedente de la descomposición de las rocas, mientras que una pequeña parte es liberada a la atmósfera a través de los incendios forestales, actividad volcánica, quema de combustibles fósiles y residuos urbanos e industriales. 

En este apartado detallaremos como llega el cadmio a los diferentes ecosistemas:

Actualmente se relaciona la contaminación de este metal con la industria del zinc y del plomo. También se producen emisiones de cadmio, aunque en menor grado, en la combustión de basuras, combustión de carbón, producción de cementos y en la industria del acero

Los daños a nivel global del cadmio son poco importantes, en este metal se ha observado que la relación de contaminación es de regional a local.

La concentración de cadmio, procedente de las fuentes citadas previamente, es elevada alrededor de las minas, zonas industriales y áreas urbanas, disminuyendo a medida que nos alejamos de estas hacia las zonas rurales.

El cadmio que llega al agua principalmente es de vertidos urbanos e industriales. Esta contaminación depende de la cercanía de las zonas acuáticas a las zonas urbanas. Por otro lado parte del cadmio atmosférico es depositado en la superficie acuática y figura como el 23 % del cadmio contaminante, siendo la vía principal de entrada en agua.

La mayor parte del cadmio vertido por el ser humano va a depositarse en el suelo. Al igual que ocurre en el agua la vía principal de deposición es la vía atmosférica, seguida de los vertidos humanos y el uso de fertilizantes.

La concentración de cadmio en el suelo sigue aumentando con el tiempo, debido al incremento de los índices de emisión de origen humano, creyéndose que esta concentración se doblará cada 50 – 80 años.

El cadmio produce efectos tóxicos en los organismos vivos, aun en concentraciones muy pequeñas.

Es el conjunto de fenómenos que experimenta el cadmio desde su entrada en el organismo hasta su eliminación. Consta de los siguientes procesos:

El cadmio se puede absorber por tres vías diferentes; oral, respiratoria o dérmica. Sin embargo, la exposición dérmica es relativamente insignificante, es la absorción tras la vía inhalatoria y oral las de mayor interés.

La alimentación es una de las fuentes importantes de entrada de cadmio en el organismo. Una dieta deficiente en hierro según los resultados experimentales de "Flanagan et al." (1978) puede acelerar su velocidad de absorción, así como la falta de otras sustancias como es el calcio o la proteína. Podemos concluir que la fisiología del individuo (edad, reservas de hierro, calcio, zinc, embarazos…) determina la absorción tras la exposición oral.

Se ha observado mediante estudios experimentales que la concentración de cadmio está entre el 2-3% en una persona sana y asciende a un 6-8% en personas con deficiencia de hierro.

La inhalación representa una importante vía de entrada de este metal, y la absorción de este metal es mayor si el organismo presenta una deficiencia de hierro calcio y zinc.

El cadmio y sus sales presentan baja volatilidad y existen en el aire como materia finamente particulada. Cuando son inhaladas, una parte de estas partículas se deposita en el tracto respiratorio y los pulmones, mientras que el resto son exhaladas. El tamaño de partícula determina la absorción pulmones; las partículas mayores de 10 µm de diámetro se eliminan fácilmente mientras que las de 0,1 µm penetran en los alvéolos absorbiéndose y transportándose por el organismo. En el caso del tabaco las partículas son de pequeño tamaño teniendo como consecuencia una deposición mayor a nivel alveolar.

Una vez es absorbido por los pulmones o por el tracto intestinal el cadmio es transportado por la sangre a otras partes del cuerpo, concentrándose principalmente en el hígado y el riñón. La acumulación de Cd en riñón e hígado depende de la intensidad, del tiempo de exposición y del estado óptimo de la función de excreción renal

La metalotioneína (MT) es el medio de transporte del cadmio en el plasma sanguíneo. Estas Son un grupo de proteínas que unen metales, ricas en residuos de cisteína, cuya síntesis ocurre principalmente en hígado y riñón. El complejo Cadmio - metalotioneína se considera menos tóxico que el Cadmio sin enlazar, por tanto esta proteína tiene efecto detoxificante. La liberación de este complejo a la sangre es lenta, lo que provoca una acumulación del complejo Cd-MT en el hígado. A continuación el cadmio unido a MT se transporta por la sangre hasta el riñón, donde se filtra a través del glomérulo y se reabsorbe en el túbulo proximal, se produce una catálisis del complejo cadmio-MT en los lisosomas, liberándose iones de cadmio que inducen nuevamente la síntesis de MT en la célula renal. Por este motivo el cadmio tiene una media de vida biológica bastante larga.

Del metabolismo hay poco que decir ya que el cadmio no sufre ninguna conversión metabólica directa tales como oxidación, reducción o alquilación.

Metabólicamente es importante la unión del cadmio a la metalotioneína ya que deja inerte su toxicidad.

El cadmio se excreta de manera escasa y muy lenta, esto explica la larga vida biológica de este elemento.

Tras la absorción la principal vía de eliminación de cadmio es a través de la orina, por lo que se considera que el cadmio urinario refleja la carga corporal de cadmio.

El cadmio es uno de los metales tóxicos emitidos al medio ambiente que más tiende a acumularse en los alimentos. La principal fuente de contaminación de cadmio en el ser humano es la ingesta de vegetales contaminados con este metal (Norvell et al. 2000).

Una característica considerable del cadmio es su fácil traspaso del suelo a los vegetales, es uno de los metales que mejor se absorben por las plantas, siendo mayormente cereales como el trigo, arroz y, en menor medida el maíz. La retención del cadmio en la superficie vegetal depende de factores como: el tamaño de la partícula, factores climáticos, velocidad de deposición y características de las hojas. El pH es considerado uno de los factores que más repercusión tienen en la relación cadmio – vegetal, una disminución del pH del suelo facilita el traspaso del metal a la planta.

El fenómeno de la lluvia ácida es de gran importancia en áreas industrializadas, ya que esta disminuye el pH del suelo, y como consecuencia hace aumentar la absorción por parte de las plantas, es decir, la acumulación.

En relación a la contaminación por agua, los que tienen mayor incidencia tienen son los crustáceos, peces y los moluscos bivalvos (Storelli 2009; Ololade y col. 2011).

El agua apta para beber no puede superar valores del orden de 2 µg/L, esto quiere decir que no es una vía considerable de exposición.

La tecnología alimentaria también juega un papel importante, ya que los productos pueden ser contaminados en el tratamiento de los alimentos y en la manipulación de estos, sobre todo en el caso de los embalajes.

Los efectos de la toxicidad por Cd dependen del tipo de exposición, ya sea a través de la inhalación de aire contaminado (particularmente cerca de fundidoras, incineradoras o procedente del humo del cigarro) , consumo de alimentos y aguas contaminadas.
En fumadores, se ha encontrado que la concentración de Cd en la sangre es de 1-4 µg/l, un valor de cuatro a cinco a veces más alto que en los no fumadores.



</doc>
<doc id="633" url="https://es.wikipedia.org/wiki?curid=633" title="Carbono">
Carbono

El carbono (del latín, "carbo", 'carbón') es un elemento químico con símbolo C, número atómico 6 y masa atómica 12,01. Es un no metal y tetravalente, disponiendo de cuatro electrones para formar enlaces químicos covalentes. Tres isótopos del carbono se producen de forma natural, los estables C y C y el isótopo radiactivo C, que decae con una vida media de unos 5730 años. El carbono es uno de los pocos elementos conocidos desde la antigüedad, y es el pilar básico de la química orgánica. Está presente en la Tierra en estado de cuerpo simple (carbón y diamantes), de compuestos inorgánicos ( y CaCO) y de compuestos orgánicos (biomasa, petróleo y gas natural). También se han sintetizado muchas nuevas estructuras basadas en el carbono: carbón activado, negro de humo, fibras, nanotubos, fullerenos y grafeno.

El carbono es el 15.º elemento más abundante en la corteza terrestre, y el cuarto elemento más abundante en el universo en masa después del hidrógeno, el helio y el oxígeno. La abundancia del carbono, su diversidad única de compuestos orgánicos y su inusual capacidad para formar polímeros a las temperaturas comúnmente encontradas en la Tierra, permite que este elemento sirva como componente común de toda la vida conocida. Es el segundo elemento más abundante en el cuerpo humano en masa (aproximadamente el 18,5%) después del oxígeno.

Los átomos de carbono pueden unirse de diferentes maneras, denominadas alótropos del carbono, reflejo de las condiciones de formación. Los más conocidos que ocurren naturalmente son el grafito, el diamante y el carbono amorfo. Las propiedades físicas del carbono varían ampliamente con la forma alotrópica. Por ejemplo, el grafito es opaco y negro, mientras que el diamante es altamente transparente. El grafito es lo suficientemente blando como para formar una raya en el papel (de ahí su nombre, del verbo griego "γράφειν" que significa 'escribir'), mientras que el diamante es el material natural más duro conocido. El grafito es un buen conductor eléctrico mientras que el diamante tiene una baja conductividad eléctrica. En condiciones normales, el diamante, los nanotubos de carbono y el grafeno tienen las conductividades térmicas más altas de todos los materiales conocidos. Todos los alótropos del carbono son sólidos en condiciones normales, siendo el grafito la forma termodinámicamente estable. Son químicamente resistentes y requieren altas temperaturas para reaccionar incluso con oxígeno.

El estado de oxidación más común del carbono en los compuestos inorgánicos es +4, mientras que +2 se encuentra en el monóxido de carbono y en complejos carbonilos de metales de transición. Las mayores fuentes de carbono inorgánico son las calizas, dolomitas y dióxido de carbono, pero cantidades significativas se producen en depósitos orgánicos de carbón, turba, petróleo y clatratos de metano. El carbono forma un gran número de compuestos, más que cualquier otro elemento, con casi diez millones de compuestos descritos hasta la fecha (con 500.000 compuestos nuevos por año), siendo sin embargo ese número solo una fracción del número de compuestos teóricamente posibles bajo condiciones estándar. Por esta razón, a menudo el carbono se ha descrito como el «rey de los elementos».

La combustión del carbono en todas sus formas ha sido la base del desarrollo tecnológico desde tiempos prehistóricos. Los materiales basados en el carbono tienen aplicaciones en numerosas áreas de vanguardia tecnológica: materiales compuestos, baterías de iones de litio, descontaminación del aire y del agua, electrodos para hornos de arco, en la síntesis de aluminio, etc.

El carbono es un elemento notable por varias razones. Sus formas alotrópicas incluyen, una de las sustancias más blandas (el grafito) y una de las más duras (el diamante) y, desde el punto de vista económico, es de los materiales más baratos (carbón) y uno de los más caros (diamante). Más aún, presenta una gran afinidad para enlazarse químicamente con otros átomos pequeños, incluyendo otros átomos de carbono con los que puede formar largas cadenas, y su pequeño radio atómico le permite formar enlaces múltiples. Así, con el oxígeno forma el dióxido de carbono, vital para el crecimiento de las plantas (ver ciclo del carbono); con el hidrógeno forma numerosos compuestos denominados genéricamente hidrocarburos, esenciales para la industria y el transporte en la forma de combustibles fósiles; y combinado con oxígeno e hidrógeno forma gran variedad de compuestos como, por ejemplo, los ácidos grasos, esenciales para la vida, y los ésteres que dan sabor a las frutas; además es vector, a través del ciclo carbono-nitrógeno, de parte de la energía producida por el Sol.

Se conocen cinco formas alotrópicas del carbono, además del amorfo: grafito, diamante, fullereno, grafeno y carbino.

Una de las formas en las cuales se encuentra el carbono es el grafito, caracterizado por tener sus átomos "en los vértices de hexágonos que tapizan un plano", es de color negro, opaco y blando, y es el material del cual está hecha la parte interior de los lápices de madera. El grafito tiene exactamente los mismos átomos del diamante, pero por estar dispuestos en diferente forma tienen distintas propiedades físicas y químicas. Los diamantes naturales se forman en lugares donde el carbono ha sido sometido a grandes presiones y altas temperaturas. Su estructura es tetraédrica, que da como resultado una red tridimensional y a diferencia del grafito tiene un grado de dureza alto: 10 Mohs. Los diamantes se pueden crear artificialmente, sometiendo el grafito a temperaturas y presiones muy altas. El precio del grafito es menor al de los diamantes naturales, pero si se han elaborado adecuadamente tienen la misma dureza, color y transparencia.

La forma amorfa es esencialmente grafito, pero no llega a adoptar una estructura cristalina macroscópica. Esta es la forma presente en la mayoría de los carbones y en el hollín.

A presión normal, el carbono adopta la forma del grafito, en la que cada átomo está unido a otros tres en un plano compuesto de celdas hexagonales; este estado se puede describir como tres electrones de valencia en orbitales híbridos planos sp² y el cuarto en el orbital "p".

Las dos formas de grafito conocidas alfa (hexagonal) y beta (romboédrica) tienen propiedades físicas idénticas. Los grafitos naturales contienen más del 30 % de la forma beta, mientras que el grafito sintético contiene únicamente la forma alfa. La forma alfa puede transformarse en beta mediante procedimientos mecánicos, y esta recristalizar en forma alfa al calentarse por encima de 1000 °C.

Debido a la deslocalización de los electrones del orbital pi, el grafito es conductor de la electricidad, propiedad que permite su uso en procesos de electroerosión. El material es blando y las diferentes capas, a menudo separadas por átomos intercalados, se encuentran unidas por enlaces de Van de Waals, siendo relativamente fácil que unas deslicen respecto de otras, lo que le da utilidad como lubricante.

A muy altas presiones, el carbono adopta la forma del diamante, en el cual cada átomo está unido a otros cuatro átomos de carbono, encontrándose los 4 electrones en orbitales sp³, como en los hidrocarburos. El diamante presenta la misma estructura cúbica que el silicio y el germanio y, gracias a la resistencia del enlace químico carbono-carbono, es, junto con el nitruro de boro, la sustancia más dura conocida. La transición a grafito a temperatura ambiente es tan lenta que es indetectable. Bajo ciertas condiciones, el carbono cristaliza como lonsdaleíta, una forma similar al diamante pero hexagonal.

El orbital híbrido sp que forma enlaces covalentes solo es de interés en química, manifestándose en algunos compuestos, como por ejemplo el acetileno.

Los fullerenos fueron descubiertos hace 15 años tienen una estructura similar al grafito, pero el empaquetamiento hexagonal se combina con pentágonos (y en ciertos casos, heptágonos), lo que curva los planos y permite la aparición de estructuras de forma esférica, elipsoidal o cilíndrica. El constituido por 60 átomos de carbono, que presenta una estructura tridimensional y geometría similar a un balón de fútbol, es especialmente estable. Los fullerenos en general, y los derivados del C en particular, son objeto de intensa investigación en química desde su descubrimiento a mediados de los 1980.

A esta familia pertenecen también los nanotubos de carbono, que pueden describirse como capas de grafito enrolladas en forma cilíndrica y rematadas en sus extremos por hemiesferas (fulerenos), y que constituyen uno de los primeros productos industriales de la nanotecnología.

El principal uso industrial del carbono es como un componente de hidrocarburos, especialmente los combustibles fósiles (petróleo y gas natural). Del primero se obtienen, por destilación en las refinerías, gasolinas, queroseno y aceites, siendo además la materia prima empleada en la obtención de plásticos. El segundo se está imponiendo como fuente de energía por su combustión más limpia. Otros usos son:

El carbón (del latín "carbo -ōnis", "carbón") fue descubierto en la prehistoria y ya era conocido en la antigüedad, en la que se manufacturaba mediante la combustión incompleta de materiales orgánicos. Los últimos alótropos conocidos, los fullerenos (C), fueron descubiertos como subproducto en experimentos realizados con gases moleculares en la década de los 80. Se asemejan a un balón de fútbol, por lo que coloquialmente se les llama futbolenos.

Newton, en 1704, intuyó que el diamante podía ser combustible, pero no se consiguió quemar un diamante hasta 1772 en que Lavoisier demostró que en la reacción de combustión se producía CO.

Tennant demostró que el diamante era carbono puro en 1797. El isótopo más común del carbono es el C; en 1961 este isótopo se eligió para reemplazar al isótopo oxígeno-16 como base de los pesos atómicos, y se le asignó un peso atómico de 12.

Los primeros compuestos de carbono se identificaron en la materia viva a principios del siglo XIX, y por ello el estudio de los compuestos de carbono se llamó química orgánica.

El carbono no se creó durante el "Big Bang" porque hubiera necesitado la triple colisión de partículas alfa (núcleos atómicos de helio) y el Universo se expandió y enfrió demasiado rápido para que la probabilidad de que ello aconteciera fuera significativa. Donde sí ocurre este proceso es en el interior de las estrellas (en la fase RH (Rama horizontal)) donde este elemento es abundante, encontrándose además en otros cuerpos celestes como los cometas y en las atmósferas de los planetas. Algunos meteoritos contienen diamantes microscópicos que se formaron cuando el Sistema Solar era aún un disco protoplanetario.

En combinaciones con otros elementos, el carbono se encuentra en la atmósfera terrestre y disuelto en el agua, y acompañado de menores cantidades de calcio, magnesio y hierro forma enormes masas rocosas (caliza, dolomita, mármol, etc).

El grafito se encuentra en grandes cantidades en Rusia, Estados Unidos, México, Groenlandia y la India.

Los diamantes naturales se encuentran asociados a rocas volcánicas (kimberlita y lamproita). Los mayores depósitos de diamantes se encuentran en el África (Sudáfrica, Namibia, Botsuana, República del Congo y Sierra Leona). Existen además depósitos importantes en Canadá, Rusia, Brasil y Australia.

El más importante óxido de carbono es el dióxido de carbono (CO), un componente minoritario de la atmósfera terrestre (del orden del 0,04 % en peso) producido y usado por los seres vivos (ver ciclo del carbono). En el agua forma trazas de ácido carbónico (HCO) —las burbujas de muchos refrescos— pero, al igual que otros compuestos similares, es inestable, aunque a través de él pueden producirse iones carbonato estables por resonancia. Algunos minerales importantes, como la calcita, son carbonatos.

Los otros óxidos son el monóxido de carbono (CO) y el más raro subóxido de carbono (CO). El monóxido se forma durante la combustión incompleta de materias orgánicas y es incoloro e inodoro. Dado que la molécula de CO contiene un enlace triple, es muy polar, por lo que manifiesta una acusada tendencia a unirse a la hemoglobina, formando un nuevo compuesto muy peligroso denominado Carboxihemoglobina, impidiéndoselo al oxígeno, por lo que se dice que es un asfixiante de sustitución. El ion cianuro (CN), tiene una estructura similar y se comporta como los iones haluro.

Con metales, el carbono forma tanto carburos como acetiluros, ambos muy ácidos. A pesar de tener una electronegatividad alta, el carbono puede formar carburos covalentes como es el caso de carburo de silicio (SiC) cuyas propiedades se asemejan a las del diamante.
En 1961 la IUPAC adoptó el isótopo C como la base para la masa atómica de los elementos químicos.

El carbono-14 es un radioisótopo con un periodo de semidesintegración de 5730 años que se emplea de forma extensiva en la datación de especímenes orgánicos.

Los isótopos naturales y estables del carbono son el C (98,89 %) y el C (1,11 %). Las proporciones de estos isótopos en un ser vivo se expresan en variación (±‰) respecto de la referencia VPDB ("Vienna Pee Dee Belemnite", fósiles cretácicos de belemnites, en Carolina del Sur). El δC-13 del CO de la atmósfera terrestre es −7‰. El carbono fijado por fotosíntesis en los tejidos de las plantas es significativamente más pobre en C que el CO de la atmósfera.

La mayoría de las plantas presentan valores de δC-13 entre −24 y −34‰. Otras plantas acuáticas, de desierto, de marismas saladas y hierbas tropicales, presentan valores de δC-13 entre −6 y −19‰ debido a diferencias en la reacción de fotosíntesis. Un tercer grupo intermedio constituido por las algas y líquenes presentan valores entre −12 y −23‰. El estudio comparativo de los valores de δC-13 en plantas y organismos puede proporcionar información valiosa relativa a la cadena alimenticia de los seres vivos.

Los compuestos de carbono tienen un amplio rango de toxicidad. El monóxido de carbono, presente en los gases de escape de los motores de combustión y el cianuro (CN) son extremadamente muy tóxicos para los mamíferos, entre ellos las personas. Los gases orgánicos eteno, etino y metano son explosivos e inflamables en presencia de aire. Por el contrario, muchos otros compuestos no son tóxicos sino esenciales para la vida.
El carbono puro tiene una toxicidad extremadamente baja para los humanos y puede ser manejado e incluso ingerido en forma segura en la forma de grafito o carboncillo. Es resistente a la disolución y ataque químico, incluso en los contenidos acidificados del tracto digestivo. Esto resulta en que una vez que entra a los tejidos corporales lo más probable es que permanezcan allí en forma indefinida. El negro de carbón fue probablemente el primer pigmento en ser usado para hacer tatuajes y se encontró que Ötzi el hombre del hielo tenía tatuajes hechos con carbón que sobrevivieron durante su vida y 5200 años después de su muerte. Sin embargo, la inhalación en grandes cantidades del polvo de carbón u hollín (negro de carbón) puede ser peligroso, al irritar los tejidos del pulmón y causar una enfermedad conocida como neumoconiosis de los mineros del carbón. De forma similar el polvo de diamante usado como un abrasivo puede ser dañino si se ingiere o inhala. También las micropartículas de carbón producidas por los gases de escape de los motores diésel se pueden acumular en los pulmones al ser inhaladas. En estos ejemplos, los efectos dañinos pueden resultar de la contaminación de las partículas de carbón con elementos químicos orgánicos o de metales pesados más que del carbón en sí mismo.

Generalmente el carbono tiene baja toxicidad para casi toda la vida en la Tierra, sin embargo, para algunas criaturas es tóxico - por ejemplo, las nanopartículas de carbón son toxinas mortales para la "Drosophila".

También el carbono se puede quemar vigorosa y brillantemente en la presencia de aire a alta temperatura, como en el caso del Incendio de Windscale, el que fue causado por la repentina liberación de energía Wigner acumulada en el núcleo de grafito. Grandes acumulaciones de carbón, que han permanecido inertes por centenares de millones de años en la ausencia de oxígeno, pueden incendiarse espontáneamente cuando son expuestas al aire, como por ejemplo en los desechos de las minas de carbón.

Entre la gran variedad de compuestos de carbono se pueden incluir venenos letales tales como la tetradotoxina, la ricina lectina obtenida de las semillas de ricino ("Ricinus communis"), el cianuro (CN) y el envenenamiento por monóxido de carbono.




</doc>
<doc id="634" url="https://es.wikipedia.org/wiki?curid=634" title="Carl Sagan">
Carl Sagan

Carl Edward Sagan (Nueva York, 9 de noviembre de 1934-Seattle, 20 de diciembre de 1996) fue un astrónomo, astrofísico, cosmólogo, astrobiólogo, escritor y divulgador científico estadounidense. Inicialmente fue profesor asociado de la Universidad de Harvard y posteriormente profesor principal de la Universidad de Cornell. En esta última, fue el primer científico en ocupar la Cátedra David Duncan de Astronomía y Ciencias del Espacio, creada en 1976, y además director del Laboratorio de Estudios Planetarios.

Fue un defensor del pensamiento escéptico científico y del método científico, pionero de la exobiología, promotor de la búsqueda de inteligencia extraterrestre a través del proyecto SETI. Impulsó el envío de mensajes a bordo de sondas espaciales, destinados a informar a posibles civilizaciones extraterrestres acerca de la cultura humana. Mediante sus observaciones de la atmósfera de Venus, fue de los primeros científicos en estudiar el efecto invernadero a escala planetaria.

Carl Sagan ganó gran popularidad gracias a la galardonada serie documental de TV "", producida en 1980, de la que fue narrador y coautor. También publicó numerosos artículos científicos, y fue autor, coautor o editor de más de una veintena de libros de divulgación científica, siendo los más populares sus libros "Cosmos", publicados como complemento de la serie, y "Contacto", en el que se basa la película homónima de 1997. En 1978 ganó el Premio Pulitzer de Literatura General de No Ficción por su libro "Los dragones del Edén".

A lo largo de su vida, Sagan recibió numerosos premios y condecoraciones por su labor como comunicador de la ciencia y la cultura. Hoy es considerado uno de los divulgadores de la ciencia más carismáticos e influyentes, gracias a su capacidad de transmitir las ideas científicas y los aspectos culturales al público no especializado con sencillez no exenta de rigor.

Nació en Brooklyn, Nueva York, en una familia de judíos ucranianos. Su padre, Sam Sagan, era un obrero de la industria textil nacido en Kamianets-Podilsky, Ucrania, y su madre, Rachel Molly Gruber, era ama de casa. Carl recibió su nombre en honor de la madre biológica de Rachel, Chaiya Clara, en palabras de Sagan "la madre que ella nunca conoció". Tenía una hermana llamada Carol.

La familia vivía en un modesto apartamento cerca del océano Atlántico, en Bensonhurst, un barrio de Brooklyn. Según Sagan, eran judíos reformistas, el más liberal de los tres principales grupos judíos. Tanto Carl como su hermana coinciden en que su padre no era especialmente religioso, pero que su madre indudablemente creía en Dios, y participaba activamente en el templo...; y solo servía carne kosher. Durante el auge de la Gran Depresión, su padre tuvo que aceptar un empleo como acomodador de cine.

Según el biógrafo Keay Davidson, la guerra interior de Sagan era resultado de la estrecha relación que mantenía con sus padres, quienes eran opuestos en muchos sentidos. Sagan atribuía sus posteriores impulsos analíticos a su madre, una mujer que conoció la pobreza extrema siendo niña, y que había crecido casi sin hogar en la ciudad de Nueva York, durante la I Guerra Mundial y la década de 1920. Tenía las ambiciones propias de una mujer joven, pero bloqueadas por las restricciones sociales, por su pobreza, por ser mujer y esposa, y por ser de etnia judía. Davidson señala que ella, por tanto, adoraba a su hijo; él haría realidad sus sueños no cumplidos.

Sin embargo, su capacidad para sorprenderse venía de su padre, que era un tranquilo y bondadoso fugitivo del Zar. En su tiempo libre, regalaba manzanas a los pobres o ayudaba a suavizar las tensiones entre patronos y obreros en la tumultuosa industria textil de Nueva York. Aunque intimidado por la brillantez de Carl, por sus infantiles parloteos sobre estrellas y dinosaurios, se tomó con calma la curiosidad de su hijo, como parte de su educación. Años más tarde, como escritor y científico, Carl recurriría a sus recuerdos de la infancia para ilustrar ideas científicas, como hizo en su libro "El mundo y sus demonios". Sagan describe así la influencia de sus padres en su pensamiento posterior:

Sagan recordaba que vivió una de sus mejores experiencias cuando, con cuatro o cinco años de edad, sus padres lo llevaron a la Exposición Universal de Nueva York de 1939, lo cual fue un punto de inflexión en su vida. Tiempo después recordaba el mapa móvil de la "América del Mañana":

En otras exhibiciones, recordaba cómo una lámpara que iluminaba una célula fotoeléctrica creaba un sonido crujiente, y cómo el sonido de un diapasón se convertía en una onda en un osciloscopio. También fue testigo de la tecnología del futuro que reemplazaría a la radio: la televisión. Sagan escribió:

También pudo ver uno de los eventos más publicitados de la Exposición: el entierro de una cápsula del tiempo en Flushing Meadows, que contenía recuerdos de la década de 1930 para ser recuperados por las generaciones venideras de un futuro milenio. "La cápsula del tiempo emocionó a Carl", escribe Davidson. De adulto, Sagan y sus colegas crearon cápsulas del tiempo similares, pero para enviarlas a la galaxia: la placa de la Pioneer y el disco de oro de las Voyager fueron producto de los recuerdos de Sagan sobre la Exposición Universal.

Durante la II Guerra Mundial, la familia de Sagan estuvo preocupada por el destino de sus parientes europeos. Sagan, sin embargo y por lo general, no fue consciente de los detalles sobre el curso de la guerra. Escribió: "Cierto es que tuvimos parientes que quedaron atrapados en el Holocausto. Hitler no era un sujeto popular en nuestra casa... Pero, por otro lado, yo estuve bastante aislado de los horrores de la guerra". Su hermana, Carol, dijo que su madre por encima de todo quería proteger a Carl... Ella lo estaba pasando extraordinariamente mal con la II Guerra Mundial y el Holocausto. En su libro "El mundo y sus demonios" (1996), Sagan incluye sus recuerdos sobre aquel período conflictivo, cuando su familia se enfrentó a la realidad de la guerra en Europa, pero trató de evitar que esta socavara su espíritu optimista.

Poco después de ingresar en la escuela elemental, Sagan comenzó a expresar una fuerte curiosidad por la naturaleza. Sagan recordaba sus primeras visitas en solitario a la biblioteca pública, a la edad de cinco años, cuando su madre le regaló un carné de lector. Quería saber qué eran las estrellas, ya que ninguno de sus amigos ni sus padres sabían darle una respuesta clara:

Por la época en que tenía seis o siete años, Sagan y un amigo fueron al Museo Americano de Historia Natural de la ciudad de Nueva York. Allí estuvieron en el Planetario Hayden y pasearon por las exhibiciones de objetos espaciales del museo, como los meteoritos, y las muestras de dinosaurios y animales en entornos naturales. Sagan escribió sobre esas visitas:

Los padres de Sagan ayudaron a alimentar el creciente interés de este por la ciencia comprándole juegos de química y materiales de lectura. Su interés por el espacio era, sin embargo, su principal foco, especialmente después de leer las historias de ciencia-ficción de escritores como Edgar Rice Burroughs, quienes estimulaban su imaginación acerca de cómo sería la vida en otros planetas, como Marte. Según el biógrafo Ray Spangenburg, estos primeros años en los que Sagan trataba de comprender los misterios de los planetas, se convirtieron en una fuerza motora en su vida, una chispa continua para su intelecto, y una búsqueda que jamás sería olvidada.

Carl Sagan se graduó en la Rahway High School de Rahway, Nueva Jersey, en 1951. Se matriculó en la Universidad de Chicago, donde participó en la Ryerson Astronomical Society. En esta universidad se graduó 1954 en Artes con honores especiales y generales, en 1955 se graduó en Ciencias y en 1956 obtuvo un máster en Física, para luego doctorarse en Astronomía y Astrofísica en 1960. Durante su etapa de pregrado, Sagan trabajó en el laboratorio del genetista Hermann Joseph Muller. De 1960 a 1962, Sagan disfrutó de una Beca Miller para la Universidad de California, Berkeley. De 1962 a 1968, trabajó en el Smithsonian Astrophysical Observatory en Cambridge, Massachusetts.

Sagan impartió clases e investigó en la Universidad de Harvard hasta 1968, año en que se incorporó a la Universidad Cornell en Ithaca, Nueva York, donde impartió un curso de pensamiento crítico hasta su muerte en 1996. En 1971, fue nombrado profesor titular y director del Laboratorio de Estudios Planetarios. De 1972 a 1981, Sagan fue Director Asociado del Centro de Radiofísica e Investigación Espacial de Cornell. Desde 1976 hasta su muerte, fue el primer titular de la Cátedra David Duncan de Astronomía y Ciencias del Espacio. En Londres, impartió la edición de 1977 de las Royal Institution Christmas Lectures.

Sagan estuvo vinculado al programa espacial estadounidense desde los inicios de este. Desde la década de 1950, trabajó como asesor de la NASA, donde uno de sus cometidos fue dar las instrucciones del Programa Apolo a los astronautas participantes antes de partir hacia la Luna. Sagan participó en muchas de las misiones que enviaron naves espaciales robóticas a explorar el sistema solar, preparando experimentos para varias expediciones. Concibió la idea de añadir un mensaje universal y perdurable a las naves destinadas a abandonar el sistema solar que pudiese ser potencialmente comprensible por cualquier inteligencia extraterrestre que lo encontrase. Sagan preparó el primer mensaje físico enviado al espacio exterior: una placa anodizada, unida a la sonda espacial Pioneer 10, lanzada en 1972. La Pioneer 11, que llevaba otra copia de la placa, fue lanzada al año siguiente. Sagan continuó refinando sus diseños; el mensaje más elaborado que ayudó a desarrollar y preparar fue el Disco de Oro de las Voyager, que fue enviado con las sondas espaciales Voyager en 1977. Sagan se opuso frecuentemente a la decisión de financiar el transbordador espacial y la estación espacial a expensas de futuras misiones robóticas.

De 1968 a 1979, Sagan fue editor de la "Revista Icarus", publicación para profesionales sobre investigación planetaria. Fue cofundador de la "Sociedad Planetaria", el mayor grupo del mundo dedicado a la investigación espacial, con más de cien mil miembros en más de 149 países, y fue miembro del Consejo de Administración del Instituto SETI. Sagan ejerció también de Presidente de la División de Ciencia Planetaria (DPS) de la Sociedad Astronómica Americana, de Presidente de la Sección de Planetología de la American Geophysical Union y de Presidente de la Sección de Astronomía de la Asociación Estadounidense para el Avance de la Ciencia.

Las contribuciones de Sagan fueron vitales para el descubrimiento de las altas temperaturas superficiales del planeta Venus. A comienzos de la década de 1960 nadie sabía a ciencia cierta cuáles eran las condiciones básicas de la superficie de dicho planeta, y Sagan enumeró las posibilidades en un informe que posteriormente fue divulgado en un libro de Time-Life titulado "Planetas". En su opinión, Venus era un planeta seco y muy caliente, oponiéndose al paraíso templado que otros imaginaban. Había investigado las emisiones de radio procedentes de Venus y llegado a la conclusión de que la temperatura superficial de este debía de ser de unos 380 °C. Como científico visitante del Laboratorio de Propulsión a Chorro de la NASA, participó en las primeras misiones del Programa Mariner a Venus, trabajando en el diseño y gestión del proyecto. En 1962, la sonda Mariner 2 confirmó sus conclusiones sobre las condiciones superficiales del planeta.

Sagan fue de los primeros en plantear la hipótesis de que una de las lunas de Saturno, Titán, podría albergar océanos de compuestos líquidos en su superficie, y que una de las lunas de Júpiter, Europa, podría tener océanos de agua subterráneos. Esto haría que Europa fuese potencialmente habitable por formas de vida. El océano subterráneo de agua de Europa fue posteriormente confirmado de forma indirecta por la sonda espacial Galileo. El misterio de la bruma rojiza de Titán también fue resuelto con la ayuda de Sagan, debiéndose a moléculas orgánicas complejas en constante lluvia sobre la superficie de la luna saturniana.

Sagan también contribuyó a mejorar la comprensión de las atmósferas de Venus y Júpiter y de los cambios estacionales de Marte. Determinó que la atmósfera de Venus es extremadamente caliente y densa, con presiones aumentando gradualmente hasta la superficie planetaria. También percibió el calentamiento global como un peligro creciente de origen humano, y comparó su progreso en la Tierra con la evolución natural de Venus: camino a convertirse en un planeta caliente y no apto para la vida como consecuencia de un efecto invernadero fuera de control. También estudió las variaciones de color de la superficie de Marte y concluyó que no se trataba de cambios estacionales o vegetales, como muchos creían, sino de desplazamientos del polvo superficial causados por tormentas de viento.

Sin embargo, Sagan es más conocido por sus investigaciones sobre la posibilidad de la vida extraterrestre, incluyendo la demostración experimental de la producción de aminoácidos mediante radiación y a partir de reacciones químicas básicas. Él y su colega de Cornell, Edwin Ernest Salpeter, especularon sobre la posibilidad de la existencia de vida en las nubes de Júpiter, dada la composición de la densa atmósfera del planeta, rica en moléculas orgánicas.

Sagan creía que la ecuación de Drake, a falta de estimaciones más razonables, sugiere la formación de un gran número de civilizaciones extraterrestres, pero la falta de evidencia de la existencia de las mismas, resaltada por la paradoja de Fermi, indicaría la tendencia de las civilizaciones tecnológicas hacia la autodestrucción. Esto dio pie a su interés en identificar y dar a conocer las diversas maneras en que la humanidad podría destruirse a sí misma, con la esperanza de poder evitar dicha catástrofe y, finalmente, posibilitar que los seres humanos se conviertan en una especie capaz de viajar por el espacio. La profunda preocupación de Sagan acerca de una potencial destrucción de la civilización humana en un holocausto nuclear quedó plasmada en una memorable secuencia en el último episodio de la serie "Cosmos", titulado "¿Quién habla en nombre de la Tierra?". Sagan acababa de dimitir de su puesto en el Consejo Científico Asesor de las Fuerzas Aéreas estadounidenses y de rechazar voluntariamente su autorización de acceso a asuntos ultra secretos en protesta por la Guerra de Vietnam. Tras su matrimonio con la escritora y activista, Ann Druyan, en junio de 1981, Sagan incrementó su actividad política, concretamente en su oposición a la carrera armamentística nuclear, durante la presidencia de Ronald Reagan.

En el clímax de la Guerra Fría, Sagan dedicó parte de sus esfuerzos a concienciar a la opinión pública sobre los efectos de una guerra nuclear cuando un modelo matemático del clima sugirió que un intercambio nuclear de proporciones suficientes podría desestabilizar el delicado equilibrio de la vida en la Tierra. Fue uno de los cinco autores (el autor "S") del informe TTAPS, como fue conocido dicho artículo de investigación. Finalmente, fue coautor del artículo científico que planteaba la hipótesis de un invierno nuclear global tras una guerra nuclear. En su libro "El mundo y sus demonios", Carl Sagan relató su participación en los debates políticos sobre el invierno nuclear. También fue coautor del libro "A Path Where No Man Thought: Nuclear Winter and the End of the Arms Race" ("Un camino que ningún humano pensó: el invierno nuclear y el fin de la carrera armamentista"), un análisis exhaustivo del fenómeno del invierno nuclear.

En marzo de 1983, Reagan dio a conocer la llamada Iniciativa de Defensa Estratégica, un proyecto en el que se invirtieron miles de millones de dólares para desarrollar un completo sistema de defensa contra ataques con misiles nucleares, que fue popularmente conocido como "Programa Guerra de las Galaxias". Sagan se opuso al proyecto, argumentando que era técnicamente imposible desarrollar un sistema semejante con el nivel de perfección requerido, y que sería mucho más caro elaborarlo que para un enemigo el eludirlo mediante señuelos u otros medios, y que su construcción desestabilizaría seriamente la balanza nuclear entre los Estados Unidos y la Unión Soviética, tornando imposible cualquier progreso hacia el desarme nuclear.

Cuando el líder soviético Mijaíl Gorbachov declaró una moratoria unilateral sobre las pruebas de armamento nuclear, que comenzaría el 6 de agosto de 1985, en el 40 aniversario de los bombardeos atómicos sobre Hiroshima y Nagasaki, el gobierno de Reagan desestimó la dramática iniciativa tachándola de propaganda, y rechazó seguir el ejemplo soviético. En respuesta, activistas anti-nucleares y pacifistas estadounidenses llevaron a cabo una serie de protestas en el emplazamiento de pruebas de Nevada, que se iniciarían el domingo de Pascua de 1986 y continuarían hasta 1987. Cientos de personas fueron arrestadas, incluyendo a Sagan, quien fue detenido en dos ocasiones al tratar de saltar un cordón de seguridad.

Sagan defendió la búsqueda de vida extraterrestre, instando a la comunidad científica a utilizar radiotelescopios para buscar señales procedentes de formas de vida extraterrestres potencialmente inteligentes. Sagan fue tan persuasivo que, en 1982, logró publicar en la revista "Science" una petición de defensa del Proyecto SETI firmada por 70 científicos entre los que se encontraban siete ganadores del Premio Nobel, lo que supuso un enorme espaldarazo a la respetabilidad de un campo tan controvertido. Sagan también ayudó al Dr. Frank Drake para preparar el mensaje de Arecibo, una emisión de radio dirigida al espacio desde el radiotelescopio de Arecibo el 16 de noviembre de 1974, destinada a informar sobre la existencia de la Tierra a posibles seres extraterrestres.

Sagan fue consumidor y defensor del uso de la marihuana. Bajo el pseudónimo "Mr. X", aportó un ensayo sobre el cannabis fumado al libro de 1971, "Marihuana Reconsidered". El ensayo explicaba que el uso de la marihuana había ayudado a inspirar parte de los trabajos de Sagan y a mejorar sus experiencias sensoriales e intelectuales. Tras la muerte de Sagan, su amigo Lester Grinspoon desveló esta información al biógrafo Keay Davidson. La publicación de la biografía "Carl Sagan: Una vida", en 1999, atrajo la atención de los medios hacia este aspecto de la vida de Sagan. Poco después de su muerte, su viuda, Ann Druyan, aceptó formar parte de la junta asesora de la NORML, una fundación dedicada a la reforma de la legislación sobre el cannabis.

Sagan contrajo matrimonio tres veces: en 1957, con la bióloga Lynn Margulis, madre del escritor Dorion Sagan y del programador y empresario informático Jeremy Sagan; en 1968, con la artista y guionista Linda Salzman, madre del escritor y guionista Nick Sagan; y en 1981, con la escritora y activista Ann Druyan, madre de la productora, guionista y directora Sasha Sagan y de Sam Sagan; unión que duraría hasta la muerte del científico en 1996.

El escritor Isaac Asimov describió a Sagan como una de las dos únicas personas que había conocido cuyo intelecto superaba al suyo, siendo la otra el informático y experto en inteligencia artificial, Marvin Minsky.

Sagan escribía a menudo sobre la religión y sobre la relación entre esta y la ciencia, expresando su escepticismo sobre la convencional conceptualización de Dios como ser sapiente:

En otra descripción de su punto de vista sobre Dios, Sagan afirma rotundamente:

En el libro "El mundo y sus demonios" (1995), Sagan ejemplifica la falacia del argumento especial con ejemplos exclusivamente religiosos:

En 1996, en respuesta a una pregunta acerca de sus creencias religiosas, Sagan contestó: "Soy agnóstico." El punto de vista de Sagan sobre la religión ha sido interpretado como una forma de panteísmo comparable a la creencia de Einstein en el Dios de Spinoza. Sagan sostenía que la idea de un creador del universo era difícil de probar o refutar, y que el único descubrimiento científico que podría desafiarla sería el de un universo infinitamente viejo. Según su última esposa, Ann Druyan, Sagan no era creyente:

En 2006, Ann Druyan editó las Conferencias Gifford sobre Teología Natural, impartidas por Sagan en Glasgow, en el año 1985, incluyéndolas en un libro llamado "La diversidad de la ciencia: una visión personal de la búsqueda de Dios", en el que el astrónomo expone su punto de vista sobre la divinidad en el mundo natural.
Sagan también está considerado como librepensador y escéptico; una de sus frases más famosas, de la serie "Cosmos", es: "Afirmaciones extraordinarias requieren evidencias extraordinarias". Dicha frase está basada en otra casi idéntica de su colega fundador del Comité para la Investigación Escéptica, Marcello Truzzi: "Una afirmación extraordinaria requiere una prueba extraordinaria". Esta idea tuvo su origen en Pierre-Simon Laplace (1749–1827), matemático y astrónomo francés, quien dijo que "el peso de la evidencia de una afirmación extraordinaria debe ser proporcional a su rareza".

A lo largo de su vida, los libros de Sagan fueron desarrollados sobre su visión del mundo, naturalista y escéptica. En "El mundo y sus demonios", Sagan presentó herramientas para probar argumentos y detectar falacias y fraudes, abogando esencialmente por el uso extensivo del pensamiento crítico y del método científico. La recopilación "Miles de millones", publicada en 1997 tras la muerte de Sagan, contiene ensayos, como su visión sobre el aborto, y el relato de su viuda, Ann Druyan, sobre su muerte como escéptico, agnóstico y librepensador.

Sagan advirtió contra la tendencia humana hacia el antropocentrismo. Fue asesor de los Alumnos de Cornell por el Trato Ético hacia los Animales. Hacia el final del capítulo "Blues para un planeta rojo", del libro "Cosmos", Sagan escribió: «Si hay vida en Marte creo que no deberíamos hacer nada con el planeta. Marte pertenecería entonces a los marcianos, aunque los marcianos fuesen solo microbios».

Sagan mostró interés en los informes sobre el fenómeno ovni al menos desde el 3 de agosto de 1952, cuando escribió una carta al Secretario de Estado estadounidense Dean Acheson preguntándole cómo responderían los EE. UU. si los platillos volantes resultaran ser de origen extraterrestre. Posteriormente, en 1964, mantuvo varias conversaciones sobre el asunto con Jacques Vallée. A pesar de su escepticismo acerca de la obtención de cualquier respuesta extraordinaria a la cuestión ovni, Sagan creía que los científicos debían estudiar el fenómeno, aunque solo fuese por el gran interés que el asunto despertaba en el público.

Stuart Appelle comenta que Sagan «escribió frecuentemente sobre lo que él percibía como falacias lógicas y empíricas acerca de los ovnis y las experiencias de abducción. Sagan rechazaba la explicación extraterrestre del fenómeno pero tenía la sensación de que examinar los informes ovni tendría beneficios empíricos y pedagógicos, y que el asunto sería, por tanto, una materia de estudio legítima».

En 1966, Sagan fue miembro del Comité Ad Hoc para la Revisión del Proyecto Libro Azul, promovido por la Fuerza Aérea de los EE. UU. para investigar el fenómeno ovni. El comité concluyó que el Libro Azul dejaba qué desear como estudio científico, y recomendó la realización de un proyecto de corte universitario para someter el fenómeno a un escrutinio más científico. El resultado fue la formación del Comité Condon (1966-1968), liderado por el físico Edward Condon, y que, en su informe final, dictaminó formalmente que los ovnis, con independencia de su origen y significado, no se comportaban de manera consistente para representar una amenaza a la seguridad nacional.

Ron Westrum escribe: «El punto culminante del tratamiento que Sagan dio a la cuestión ovni fue el simposio de la AAAS de 1969. Los participantes expusieron un amplio abanico de opiniones formadas en el tema, incluyendo no solo a partidarios como James McDonald y J. Allen Hynek sino también a escépticos como los astrónomos William Hartmann y Donald Menzel. La lista de ponentes estaba equilibrada, y es mérito de Sagan el que dicho evento tuviera lugar a pesar de la presión ejercida por Edward Condon». Junto al físico Thornton Page, Sagan editó las conferencias y debates presentados en el simposio; estos se publicaron en 1972 bajo el título "UFOs: A Scientific Debate". En algunos de los numerosos libros de Sagan se examina la cuestión ovni (al igual que en uno de los episodios de "Cosmos") y se afirma la existencia de un trasfondo religioso del fenómeno.

En 1980, Sagan volvió a revelar su punto de vista sobre los viajes interestelares en la serie "Cosmos". En una de sus últimas obras escritas, Sagan expuso que la probabilidad de que naves espaciales extraterrestres visitasen la Tierra era muy pequeña. Sin embargo, Sagan creía que era plausible que la preocupación causada por la Guerra Fría contribuyese a que los gobiernos desorientasen a los ciudadanos acerca de los ovnis, y que «algunos de los análisis e informes sobre ovnis, y quizá archivos voluminosos, hayan sido declarados inaccesibles al público que paga los impuestos... Es hora de que esos archivos sean desclasificados y puestos a disposición de todos». También previno acerca de sacar conclusiones sobre datos eliminados sobre los ovnis e insistió en que no existían claras evidencias de que posibles alienígenas hubieran visitado la Tierra ni en el pasado ni en el presente.

En sus últimos años, Sagan abogó por la creación de una búsqueda organizada de objetos cercanos a la Tierra (NEO, por sus siglas en inglés) que pudieran impactar contra esta. Muchos expertos, entre otras soluciones, sugirieron la creación de grandes bombas nucleares, para poder alterar la órbita de un NEO susceptible a impactar contra la Tierra. Para Sagan, esto vendría a presentar un "dilema de la desviación": al existir la capacidad de alejar un asteroide de la Tierra, también existe la capacidad de desviar un objeto no amenazante hacia esta, creando así una auténtica arma de destrucción masiva.

Debido a los incendios petroleros de Kuwait iniciados en enero de 1991, Sagan y sus colegas de "TTAPS" advirtieron que si el incendio se mantenía por varios meses, una cantidad suficiente de humo procedente de estos "podría alcanzar una altura tal que llegase a desmantelar la actividad agrícola en el sur de Asia". Estas afirmaciones fueron el tema de un debate televisado entre Carl Sagan y el físico Frederick Singer para el programa televisivo Nightline, en el cual Sagan afirmó que los efectos del humo serían similares a los de un invierno nuclear.

Los incendios continuaron por varios meses antes de poder ser sofocados y no causaron ningún enfriamiento de talla continental. Sagan posteriormente reconoció, en "El mundo y sus demonios", que dicha predicción no resultó ser correcta: estaba "oscuro como boca de lobo a mediodía y las temperaturas cayeron entre 4 y 6 °C en el Golfo Pérsico, pero no fue mucho el humo que alcanzó altitudes estratosféricas y Asia se salvó." En 2007, un estudio aplicó modelos computacionales modernos a los incendios petroleros de Kuwait, encontrando que las columnas individuales de humo no son capaces de elevarse hasta la estratosfera, pero que el humo procedente de fuegos que abarquen una gran superficie, como algunos incendios forestales o los incendios de ciudades enteras producto de un ataque nuclear, sí que elevarían cantidades significativas de humo a niveles estratosféricos.

Sagan ejerció brevemente como asesor en la película "", dirigida por Stanley Kubrick. Propuso que la película sugiriese, sin mostrarlo, la existencia de una superinteligencia extraterrestre.

En 1994, los ingenieros de Apple Computer bautizaron al Power Macintosh 7100 con el nombre en clave "Carl Sagan" con la esperanza de que Apple ganara "miles de millones" con las ventas del mismo. El nombre solo fue utilizado internamente, pero a Sagan le preocupaba que se convirtiera en un medio de promoción del producto y envió a Apple una carta de desistimiento. Apple aceptó, pero los ingenieros respondieron cambiando el nombre en clave interno a "BHA" (siglas de "Butt-Head Astronomer" - "Astrónomo Cabezota"). Entonces Sagan, denunció a Apple por difamación ante el tribunal federal. El tribunal aceptó la petición de Apple de desestimar la acusación de Sagan y opinó, en "obiter dictum", que un lector situado en el contexto comprendería que Apple estaba "tratando claramente de responder de forma humorística y satírica", y que "se fuerza la razón al concluir que el acusado trataba de criticar la reputación o competencia del demandante como astrónomo. No se ataca en serio la pericia de un científico al usar la expresión indeterminada "cabezota"." Sagan, entonces, denunció el uso inicial de su nombre por alusiones, pero volvió a perder y Sagan apeló la resolución. En noviembre de 1995, se llegó a un acuerdo extrajudicial, y la oficina de patentes y marcas de Apple emitió un comunicado conciliatorio: "Apple siempre ha sentido un gran respeto hacia el Dr. Sagan. Nunca fue intención de Apple el causarle al Dr. Sagan o a su familia ninguna vergüenza o preocupación."

Dos años después de diagnosticársele una mielodisplasia, y después de someterse a tres trasplantes de médula ósea procedente de su hermana, el Dr. Carl Sagan falleció de neumonía a los 62 años de edad en el Centro de Investigación del Cáncer Fred Hutchinson de Seattle, Washington, el 20 de diciembre de 1996. Fue enterrado en el Cementerio Lakeview, Ithaca, Nueva York.

Carl Sagan ha recibido diversos premios, condecoraciones y honores entre los que destacan:


La película "Contacto", de 1997, basada en la novela homónima de Sagan y acabada tras la muerte de este, finaliza con la dedicatoria "Para Carl".

También en 1997 se inauguró en Ithaca, Nueva York, el Sagan Planet Walk, una recreación del sistema solar, con una extensión de 1,2 km, desde el centro de la zona peatonal (llamada "The Commons") hasta el Sciencenter, un museo de la ciencia participativo, del que Sagan fue miembro fundador de la junta de asesores.

El lugar de aterrizaje de la nave no tripulada Mars Pathfinder fue rebautizado como Carl Sagan Memorial Station el 5 de julio de 1997. Además, el asteroide 2709 Sagan lleva dicho nombre en honor al científico.

Nick Sagan, hijo de Carl, es autor de varios episodios de la franquicia "Star Trek". El episodio de la serie "" titulado Terra Prime, muestra una breve imagen de los restos del robot explorador Sojourner, que formó parte de la misión "Mars Pathfinder", situados junto a un monumento conmemorativo en la Carl Sagan Memorial Station, sobre la superficie marciana. El monumento muestra una frase de Sagan: "Sea cual sea la razón por la que esteis en Marte, estoy encantado de que esteis aquí, y yo desearía estar con vosotros." Steve Squyres, alumno de Sagan, dirigió el equipo que depositó con éxito el Spirit Rover y el Opportunity Rover sobre Marte en 2004.

El 9 de noviembre de 2001, en el 67º aniversario del nacimiento de Sagan, el Ames Research Center de la NASA dedicó al científico el emplazamiento del Centro Carl Sagan para el Estudio de la Vida en el Cosmos. El responsable de la NASA, Daniel Goldin dijo: "Carl fue un visionario increíble, y ahora su legado podrá ser preservado y ampliado por un laboratorio de investigación y formación del siglo XXI dedicado a mejorar nuestra comprensión de la vida en el universo y a enarbolar la causa de la exploración espacial por siempre jamás." Ann Druyan estuvo en la apertura de puertas del Centro, el 22 de octubre de 2006.

Para conmemorar el décimo aniversario de la muerte de Sagan, David Morrison, uno de sus antiguos alumnos, recordó "las inmensas contribuciones de Sagan a la investigación planetaria, a la comprensión pública de la ciencia, y al movimiento escéptico" en la revista "Skeptical Inquirer".

Existen, al menos, tres premios que llevan el nombre de Sagan en honor a este:


En 2006, la Medalla Carl Sagan le fue concedida al astrobiólogo y escritor David Grinspoon, hijo de Lester Grinspoon, amigo de Sagan.

El 20 de diciembre de 2006, el décimo aniversario de la muerte de Sagan, el bloguero Joel Schlosberg organizó un "blogatón" para conmemorar el evento. La idea fue apoyada por Nick Sagan, y contó con la participación de muchos miembros de la comunidad bloguera.

En agosto de 2007, el Grupo de Investigaciones Independientes (IIG) otorgó a Sagan, a título póstumo, un premio a toda su carrera científica, honor también concedido a Harry Houdini y a James Randi.

En 2009, la compañía discográfica Third Man Records, organizó un proyecto de música electrónica denominado Symphony of Science a cargo del músico John Boswell, compuesto a partir de fragmentos sonoros y vídeos remezclados de varias obras de divulgación científica, incluida la serie "Cosmos". Los vídeos resultantes almacenados en YouTube han recibido más de veinte millones de visionados. Gracias a las tareas de remezcla, se ha conseguido que Sagan "cante" en el tema "A Glorious Dawn" y "colabore" en otros.

Desde 2009, por inicitiava del Center for Inquiry, varias organizaciones en pro del humanismo secular y la investigación científica promueven la celebración del Día de Carl Sagan el 9 de noviembre de cada año.

El cortometraje de ciencia ficción sueco 2014 Wanderers utiliza fragmentos de la narración de Sagan de su libro "Pale Blue Dot", reproducidos sobre imágenes creadas digitalmente de la posible expansión futura de la humanidad en el espacio exterior.

En febrero de 2015, la banda de música sinfónica con sede en Finlandia Nightwish lanzó la canción "Sagan" como una canción extra sin álbum para su single "Élan". La canción, escrita por el compositor/tecladista de la banda Tuomas Holopainen, es un homenaje a la vida y obra del difunto Carl Sagan.

En agosto de 2015, se anunció que Warner Bros estaba planeando una película biográfica de la vida de Sagan.

A partir de su aparición en "Cosmos" y de sus frecuentes apariciones en el programa "The Tonight Show Starring Johnny Carson", se le acuñó a Sagan la muletilla "miles de millones y miles de millones" —en inglés estadounidense, "billions and billions"—. Sagan afirmaba que él nunca utilizó esa frase en la serie. Lo más parecido que llegó a expresar está en el libro "Cosmos", donde habla de "miles "y" miles de millones":
Sin embargo, su frecuente uso de la palabra "billions", enfatizando la pronunciación de la "b" (de forma intencionada para no recurrir a alternativas más farragosas, como decir "billions with a "b"", para que el espectador distinguiese claramente dicha palabra de "millions" —millones—), le convirtieron en el blanco favorito de humoristas como Johnny Carson, Gary Kroeger, Mike Myers, Bronson Pinchot, Penn Jillette, Harry Shearer, y otros. Frank Zappa satirizó la expresión en su canción "Be In My Video", junto con el término "luz atómica" ("atomic light"). Sagan se tomó todo esto de buen humor hasta tal punto que su último libro se tituló "Miles de millones", iniciándolo con un análisis burlesco de la famosa expresión, señalando que el propio Carson era un aficionado a la astronomía y que sus números a menudo incluían elementos de ciencia real.

Sus habituales descripciones de enormes cantidades a escala cósmica inculcaron en la percepción popular la maravilla de la inmensidad del espacio y el tiempo, como por ejemplo, su frase "El número total de estrellas en el Universo es mayor que el de todos los granos de arena de todas las playas del planeta Tierra." Como homenaje humorístico, se ha definido un "sagan" como una unidad de medida equivalente, al menos, a cuatro mil millones, puesto que el número más pequeño que puede ser descrito como "miles de millones y miles de millones" es dos mil millones más dos mil millones.

Sagan fue conocido por su labor como divulgador de la ciencia, por sus esfuerzos para incrementar la comprensión científica del público en general y por su posición en favor del escepticismo científico y contra las pseudociencias. Escribió libros de divulgación científica que reflejan y desarrollan algunos de los temas tratados en "Cosmos", entre los que destacan "Los dragones del Edén: Especulaciones sobre la evolución de la inteligencia humana" (1977), que ganó un Premio Pulitzer y se convirtió en el libro de ciencia en inglés más vendido de todos los tiempos; y "El cerebro de Broca: Reflexiones sobre el romance de la ciencia".

También escribió, en 1985, la exitosa novela de ciencia-ficción "Contacto", basada en un proyecto de guion que ideó con su esposa en 1979, pero no viviría para ver la adaptación cinematográfica del mismo, estrenada en 1997. Luego de "Cosmos", escribió un libro llamado "Un punto azul pálido: Una visión del futuro humano en el espacio", que fue seleccionado como libro destacado de 1995 por "The New York Times". En enero de ese año, Sagan apareció en el programa de Charlie Rose, en el PBS. También escribió una introducción al exitoso libro de Stephen Hawking, "Breve historia del tiempo", en su primera edición en lengua inglesa (1988). Dicha introducción fue sustituida en posteriores ediciones debido a que Sagan era el propietario de los derechos de copia.

Los años corresponden a las fechas de primera publicación en lengua inglesa. El ISBN puede no estar relacionado con el año.


En 1980, Sagan fue presentador, coautor y coproductor, junto a su esposa Ann Druyan y Steven Soter, de la popular serie de televisión de trece capítulos, "", producida por el PBS, y que seguía el formato de la también serie "El ascenso del hombre", presentada por Jacob Bronowski. Esta abarcó un amplio espectro de materias científica que incluían el origen de la vida y la evolución del Universo y de la cultura de la especie humana, planteada esta como medio de autoconocimiento del primero. Es su obra de divulgación más popular e influyente, y la que le hizo mundialmente famoso.

La serie ganó un Premio Emmy y un Premio Peabody. Ha sido emitida en más de 60 países y vista por más de 600 millones de personas, convirtiéndose en el programa del PBS más visto de la historia. Además, la revista "Time" publicó un artículo de portada sobre Sagan poco después del estreno, refiriéndose a él como "el creador, autor principal, narrador y presentador de la nueva serie de la televisión pública Cosmos."

El éxito y fama cosechados por Sagan, debidos a su dedicación a la divulgación, le causaron problemas profesionales y que algunos colegas le ridiculizasen. En la década de 1990 se difundió entre el mundo académico la idea de que se dedicaba más a la divulgación que a investigar, y perdió así la oportunidad de ingresar como numerario en la Universidad de Harvard y en la Academia Nacional de Ciencias de Estados Unidos. Sin embargo la producción científica de Sagan había mantenido los mismos niveles. Este tipo de situaciones, que son relativamente comunes entre científicos que se dedican además a la divulgación de la ciencia y se exponen a la opinión pública, se conocen como «efecto Sagan» a raíz del caso del astrofísico.






</doc>
<doc id="636" url="https://es.wikipedia.org/wiki?curid=636" title="Decimal codificado en binario">
Decimal codificado en binario

En sistemas de computación, Binary-Coded Decimal (BCD) o Decimal codificado en binario es un estándar para representar números decimales en el sistema binario, en donde cada dígito decimal es codificado con una secuencia de 4 bits. Con esta codificación especial de los dígitos decimales en el sistema binario, se pueden realizar operaciones aritméticas como suma, resta, multiplicación y división.

Cada dígito decimal tiene una representación binaria codificada con 4 bits:

Los números decimales, se codifican en BCD con los bits que representan sus dígitos. Por ejemplo, la codificación en BCD del número decimal 59237 es:

La representación anterior (en BCD) es diferente de la representación del mismo número decimal en binario puro:

En BCD cada cifra que representa un dígito decimal (0, 1...8 y 9) se representa con su equivalente binario en cuatro bits (nibble o cuarteto) (esto es así porque es el número de bits necesario para representar el nueve, el número más alto que se puede representar en BCD). En la siguiente tabla se muestran los códigos BCD más empleados:

Como se observa, con el BCD solo se utilizan 10 de las 16 posibles combinaciones que se pueden formar con números de 4 bits, por lo que el sistema pierde capacidad de representación, aunque se facilita la compresión de los números. Esto es porque el BCD solo se usa para representar cifras, no números en su totalidad. Esto quiere decir que para números de más de una cifra hacen falta dos números BCD.

Desde que los sistemas informáticos empezaron a almacenar los datos en conjuntos de ocho bits (octeto), hay dos maneras comunes de almacenar los datos BCD:

De este modo, el número 127 sería representado como (11110001, 11110010, 11110111) en el EBCDIC o (00010010, 01111100) en el BCD empaquetado.

El BCD sigue siendo ampliamente utilizado para almacenar datos, en aritmética binaria o en electrónica. Los números se pueden mostrar fácilmente en visualizadores de siete segmentos enviando cada cuarteto BCD a un visualizador. La BIOS de un ordenador personal almacena generalmente la fecha y la hora en formato BCD; probablemente por razones históricas se evitó la necesidad de su conversión en ASCII.

La ventaja del código BCD frente a la representación binaria clásica es que no hay límite para el tamaño de un número. Los números que se representan en formato binario están generalmente limitados por el número mayor que se pueda representar con 8, 16, 32 o 64 bits. Por el contrario, utilizando BCD, añadir un nuevo dígito solo implica añadir una nueva secuencia de 4 bits.

La conversión de números decimales a exceso 3 (XS3) se realiza de la siguiente forma:

Ejemplo: "Transformar el decimal 67 a XS3"

Tomamos cada dígito y le sumamos 3:

6+3=9

7+3=10

Ahora cada cantidad es transformada a binario:

9=1001

10= 1010

Por lo que el resultado de la conversión a XS3 será el número 10011010

El BCD es muy común en sistemas electrónicos donde se debe mostrar un valor numérico, especialmente en los sistemas digitales no programados (sin microprocesador o microcontrolador).

Utilizando el código BCD, se simplifica la manipulación de los datos numéricos que deben ser mostrados por ejemplo en un visualizador de siete segmentos. Esto lleva a su vez una simplificación en el diseño físico del circuito ("hardware"). Si la cantidad numérica fuera almacenada y manipulada en binario natural, el circuito sería mucho más complejo que si se utiliza el BCD.
Hay un programa que se llama b1411 que sirve para dividir al sistema binario en dos combinaciones.
Una por ejemplo es la de sistemas digitales.

IBM utilizó los términos decimal codificado en binario y BCD, para el código binario de seis bits con el que se podían representar números, letras mayúsculas, y caracteres especiales. Una variante del BCD fue utilizada en la mayoría de las primeras computadoras de IBM, incluyendo IBM1620 e IBM 1400. Con la introducción des System/360, el BCD fue substituido por el EBCDIC, de ocho bits.

Las posiciones de los bits, en el BCD de seis bits, generalmente fueron etiquetadas como "B, A, 8, 4, 2" y "1". Para codificar los dígitos numéricos, "A" y "B" eran cero. La letra A fue codificada como "(B, A, 1)", etcétera.

En 1972, el Tribunal Supremo de Estados Unidos anuló la decisión de una instancia más baja de la corte que había permitido una patente para convertir números codificados BCD a binario en una computadora (véase Gottschalk v Benson en inglés). Este fue uno de los primeros casos importantes en la determinación de la patentabilidad del software y de los algoritmos.




</doc>
<doc id="637" url="https://es.wikipedia.org/wiki?curid=637" title="Coma fija">
Coma fija

La representación de coma fija es una forma de notación científica que consiste en destinar una cantidad fija de dígitos para la parte entera y otra para la parte fraccionaria. La cantidad de dígitos destinados a la parte fraccionaria indica en definitiva la posición de la coma dentro del número. Esta posición, que es siempre fija, la podemos indicar con un factor de escala implícito que ubica la coma en el lugar requerido. Es decir, podemos representar un número fraccionario como un número entero multiplicado por un factor de escala. En general, el factor de escala puede ser arbitrario e indica cuál es la longitud del intervalo que separa dos representaciones consecutivas,
por ejemplo: dn-1dn-2…d0,d-1d-2…d-m=dn+m-1…d0 · b-m. 

Esta longitud es siempre fija para cualquier par de representaciones consecutivas en todo el rango de representación. En este sistema, el programador debe modificar el factor de escala cuando alguna operación produce un resultado fuera del rango de representación.

Por ejemplo: si la suma de dos números en punto fijo produce acarreo, se debe modificar el factor de escala si no se quiere perder significación en el resultado. Esto implica modificar el factor de escala de todos los números en punto fijo que utiliza el programa con la consecuente pérdida de precisión. Estos sistemas de representación ofrecen un rango y una precisión limitados.

Este sistema presenta cierta dificultad al operar con sumas y restas. El computador debe analizar el signo de los operadores para decidir la operación que tiene que hacer. Así, si la operación es una suma, pero uno de los operadores es negativo, se ha de cambiar por una resta. Por el contrario, las operaciones de multiplicar y dividir se tratan sin dificultad, operándose, por un lado, con las magnitudes, y por otro, con los signos.

Con este sistema se representan los enteros desde el formula_1 hasta el formula_2, siendo "n" el número de bits. Por tanto el rango de representación es formula_3 y la resolución es de “1″. El “cero” presenta las dos representaciones 000…00 y 100…00, lo que a veces genera dificultades. 

Por ejemplo, el valor de los siguientes números, todos ellos representados con 8 bits: 

De los 8 bits hemos fijado y reservado 5 para la parte entera y 3 para la fraccionaria. En los anteriores ejemplos la coma está fija y sirve para separar la parte entera de la parte fraccionaria. 

Al usar la notación en coma fija, queda muy limitado el número de cantidades a representar y todas ellas deben tener la misma resolución. En el caso anterior no podremos representar números enteros mayores o iguales que 32 (2) ni números más pequeños que 0,125 (2). Debido a este problema, su uso se vio reducido con la aparición de la representación en coma flotante.




</doc>
<doc id="638" url="https://es.wikipedia.org/wiki?curid=638" title="Coma flotante">
Coma flotante

La representación de punto flotante (en inglés "floating point") es una forma de notación científica usada en los computadores con la cual se pueden representar números reales extremadamente grandes y pequeños de una manera muy eficiente y compacta, y con la que se pueden realizar operaciones aritméticas. El estándar actual para la representación en coma flotante es el IEEE 754.

Como la representación en coma flotante es casi idéntica a la notación científica tradicional, con algunos añadidos y algunas diferencias, primero se describirá la notación científica para entender cómo funciona, y luego se describirá la representación de coma flotante y las diferencias. 

La notación científica se usa para representar números reales. Siendo r el número real a representar, la representación en notación científica está compuesta de tres partes:

formula_1


Un signo en el coeficiente indica si el número real es positivo o negativo.

El coeficiente tiene una cantidad determinada de dígitos significativos, los cuales indican la precisión del número representado, cuantos más dígitos tenga el coeficiente, más precisa es la representación. Por ejemplo, π lo podemos representar en notación científica, con 3 cifras significativas, 3,14 x 10, o con 12 cifras significativas, 3,14159265359 x 10, teniendo en la segunda representación mucha más precisión que la primera.

El coeficiente es multiplicado por la base elevada a un exponente entero. En nuestro sistema decimal la base es 10. Al multiplicar el coeficiente por la base elevada a una potencia entera, lo que estamos haciendo es desplazando la coma del coeficiente tantas posiciones (tantos dígitos) como indique el exponente. La coma se desplaza hacia la derecha si el exponente es positivo, o hacia la izquierda si es negativo.

Ejemplo de cómo cambia un número al variar el exponente de la base:


Un ejemplo de número en notación científica es el siguiente:

El coeficiente es -1,23456789, tiene 9 dígitos significativos, y está multiplicado por la base diez elevada a la 3. El signo del coeficiente indica si el número real representado por la notación científica es positivo o negativo.

El valor de la potencia nos indica cuántas posiciones (cuántos dígitos) debe ser desplazada la coma del coeficiente para obtener el número real final. El signo de la potencia nos indica si ese desplazamiento de la coma debe hacerse hacia la derecha o hacia la izquierda. Una potencia positiva indica que el desplazamiento de la coma es hacia la derecha, mientras que un signo negativo indica que el desplazamiento debe ser hacia la izquierda. Si el exponente es cero, la coma no se desplaza ninguna posición. La razón de la denominación de "coma flotante", es porque la coma se desplaza o "flota" tantos dígitos como indica el exponente de la base, al cambiar el exponente, la coma "flota" a otra posición.

En el número representado en la notación científica anterior, -1,23456789 x 10, el exponente es 3 positivo, lo que indica que la coma del coeficiente -1,23456789 debe ser desplazada 3 posiciones hacia la derecha, dando como resultado el número real equivalente:

Abajo, una tabla con ejemplos de números reales de tres dígitos significativos y su representación en notación científica:

Como puede verse en la tabla, la representación en notación científica de los números reales es mucho más compacta cuando los números son muy grandes en magnitud, o cuando son de magnitud muy pequeña (cercanos a cero), por eso es muy usada en ciencia, donde hay que lidiar con cifras enormes como la masa del Sol, 1,98892 × 10 kg, o muy pequeñas como la carga del electrón, -1,602176487 × 10 culombios, y también por eso se usa, en forma de coma flotante, para la representación de números reales en el computador.

Para la entrada y el despliegue de números en notación científica, los computadores y las calculadoras pueden representarlos de diferentes maneras. Por ejemplo, dependiendo del sistema, la velocidad de la luz, 2,99792458 x 10, puede representarse como sigue:

Un valor real se puede extender con una cantidad arbitraria de dígitos. La coma flotante permite representar solo una cantidad limitada de dígitos de un número real, solo se trabajará con los dígitos más significativos, (los de mayor peso) del número real, de tal manera que un número real generalmente no se podrá representar con total precisión sino como una aproximación que dependerá de la cantidad de dígitos significativos que tenga la representación en coma flotante con que se está trabajando. La limitación se halla cuando existen dígitos de peso menor al de los dígitos de la parte significativa. En dicho caso estos suelen ser redondeados, y si son muy pequeños son truncados. Sin embargo, y según el uso, la relevancia de esos datos puede ser despreciable, razón por la cual el método es interesante pese a ser una potencial fuente de error.

En la representación binaria de coma flotante, el bit de mayor peso define el valor del signo, 0 para positivo, 1 para negativo. Le siguen una serie de bits que definen el exponente. El resto de bits son la parte significativa.

Debido a que la parte significativa está generalmente normalizada, en estos casos, el bit más significativo de la parte significativa siempre es 1, así que no se representa cuando se almacena sino que es asumido implícitamente. Para poder realizar los cálculos ese bit implícito se hace explícito antes de operar con el número en coma flotante. Hay otros casos donde el bit más significativo no es un 1, como con la representación del número cero, o cuando el número es muy pequeño en magnitud y rebasa la capacidad del exponente, en cuyo caso los dígitos significativos se representan de una manera denormalizada para así no perder la precisión de un solo golpe sino progresivamente. En estos casos, el bit más significativo es cero y el número va perdiendo precisión poco a poco (mientras que al realizar cálculos este se haga más pequeño en magnitud) hasta que al final se convierte en cero.

Emplearemos varios ejemplos para describir la notación de coma flotante. Abajo tenemos 3 números en una representación de coma flotante de 16 bits. El bit de la izquierda es el signo, luego hay 6 bits para el exponente, seguidos de 9 bits para la parte significativa:

formula_2

El signo es expresado por el bit de la izquierda, con 0 indicando que el número es positivo y 1 indicando que el número es negativo. En los ejemplos de arriba, el primer número es negativo y los dos últimos son positivos.

El exponente indica cuánto se debe desplazar hacia la derecha o hacia la izquierda la coma binaria de la parte significativa. En este caso, el exponente ocupa 6 bits capaces de representar 64 valores diferentes, es decir, es un exponente binario (de base 2) que va desde -31 a +32, representando potencias de 2 entre 2 y 2, indicando que la coma binaria se puede desplazar hasta 31 dígitos binarios hacia la izquierda (un número muy cercano a cero), y hasta 32 dígitos binarios hacia la derecha (un número muy grande).

Pero el exponente no se almacena como un número binario con signo (desde -31 hasta +32) sino como un entero positivo equivalente que va entre 0 y 63. Para ello, al exponente se le debe sumar un desplazamiento (bias), que en este caso de exponente de 6 bits (64 valores), es 31 (31 es la mitad de los 64 valores que se pueden representar, menos 1), y al final, el rango del exponente de -31 a +32 queda representado internamente como un número entre 0 y 63, donde los números entre 31 y 63 representan los exponentes entre 0 y 32, y los números entre 0 y 30 representan los exponentes entre -31 y -1 respectivamente:

La parte significativa, en este caso, está formada por 10 dígitos binarios significativos, de los cuales tenemos 9 dígitos explícitos más 1 implícito que no se almacena.

Esta parte significativa generalmente está normalizada y tendrá siempre un 1 como el bit más significativo. Debido a que, salvo ciertas excepciones, el bit más significativo del significante siempre es 1, para ahorrar espacio y para aumentar la precisión en un bit, este bit no se almacena, y por ello se denomina bit oculto o implícito, sin embargo, antes de realizar los cálculos este bit implícito debe convertirse en un bit explícito.

La notación genérica para la coma flotante descrita arriba, representa respectivamente los siguientes números reales (expresados en binario). El color rojo indica el bit más significativo, que cuando se almacena es implícito (ver arriba la parte significativa en la representación de coma flotante), pero cuando se hacen los cálculos, o cuando se muestra la información se vuelve explícito:

Para un tamaño determinado de bytes, la notación en coma flotante puede ser más lenta de procesar y es menos precisa que la notación en coma fija, ya que además de almacenar el número (parte significativa), también debe almacenarse el exponente, pero permite un mayor rango en los números que se pueden representar.

Debido a que las operaciones aritméticas que se realizan con números en coma flotante son complejas de realizar, muchos sistemas destinan un procesador especial para la realización específica de este tipo de operaciones, denominado unidad de coma flotante o tienen incorporados componentes especializados. En los casos donde no exista esta facilidad, o que el hardware de coma flotante no pueda realizar determinadas operaciones, se utilizan bibliotecas de software para realizar los cálculos.
Formatos binarios de los números en coma flotante del estándar IEEE 754 (2008).




</doc>
<doc id="639" url="https://es.wikipedia.org/wiki?curid=639" title="Código de corrección de errores">
Código de corrección de errores

Código de corrección de errores, código autocorrector o código de autochequeo ("Error-Correcting Code", ECC) sirve para garantizar la integridad de los datos.

El hecho de añadir un único bit de paridad no siempre resulta suficiente para datos que se mueven constantemente de un lado para otro, especialmente en el caso de transmisión de datos a largas distancias, donde las señales transmitidas están expuestas a interferencias eléctricas (por ejemplo: en una red de computadoras, donde los datos viajan de una computadora a otra que puede estar, incluso, en otro país).

Este problema ha conducido al desarrollo de códigos que detectan más de un error e incluso corrigen los errores que encuentran.

Algunas de estas técnicas han sido desarrolladas por el ingeniero estadounidense Richard W. Hamming, y se conocen como Códigos de Hamming.



</doc>
<doc id="640" url="https://es.wikipedia.org/wiki?curid=640" title="Códigos de bloque">
Códigos de bloque

Los códigos de bloque son técnicas utilizadas para transformar un conjunto de datos binarios "N" en otro un poco más largo "K" donde se agregan unos bits de más para dar redundancia al código saliente K, donde (K>N). El número de dígitos de comprobación o redundancia será M=K-N; donde M son la cantidad dígitos adicionados.

El principio que se utiliza en los códigos de bloque consiste en estructurar los datos en bloques de longitud fija y añadir a cada bloque un cierto número de bits llamados bits de redundancia. 

Solo ciertas combinaciones de bits son aceptables y forman una colección de palabras de código válidas. 

Cuando los datos se transmiten al receptor, hay dos posibilidades: 



</doc>
<doc id="643" url="https://es.wikipedia.org/wiki?curid=643" title="Civilization (serie)">
Civilization (serie)

Civilization es una serie de videojuegos de estrategia por turnos iniciada por Sid Meier cuando lanzó el videojuego "Civilization" en 1991 y que ha sido continuada hasta hoy con varias secuelas.

El primer "Civilization" no fue publicado en España pero "Civilization II" y "Civilization III" sí, en 1996 y 2002 respectivamente. Mientras que las dos primeras versiones fueron publicadas por Microprose, la desaparición de esa empresa hizo que la francesa Infogrames (posteriormente rebautizada con el histórico nombre de Atari) adquiriera los derechos de ambos juegos y de la propia saga; en colaboración con Firaxis Games, la empresa del creador del juego Sid Meier, publicaron Civilization III y IV.

Tras la desaparición de Microprose, los derechos de los tres juegos de la saga han estado en posesión de Atari (antes Infogrames) hasta noviembre de 2004, fecha en la que fueron vendidos por 15,5 millones de euros a la empresa Take Two Interactive, que distribuirá los productos de la franquicia a través de su filial 2K Games, para paliar la mala situación económica de Atari. Firaxis y 2K Games publicaron Civilization IV en octubre de 2005.

Civilization ha creado el esquema que han seguido la mayoría de los juegos de estrategia basados en turnos; además, se han creado juegos a su sombra, como "Freeciv" o los juegos de Activision "" y "". También Firaxis creó otro juego de similar mecánica pero de ambientación de ciencia-ficción, Sid Meier's Alpha Centauri, publicado en 1999 y su secuela "". En estos juegos se añadió la posibilidad de diseñar las unidades y una configuración detallada de la forma de gobierno (llamada «Ingeniería social»). Estas innovaciones se perdieron en Civilization III. 

"Civilization" fue desarrollado originalmente para el sistema operativo DOS para PC. Tuvo que soportar numerosas revisiones para varias plataformas (incluyendo Microsoft Windows, Macintosh, Atari ST, GNU/Linux, AmigaOS y Super Nintendo) N-Gage. La última versión (Civilization Revolution) se encuentra para Nintendo DS, Wii, PlayStation 3 y xbox 360.

"Civilization" fue el primer videojuego de la serie, publicado en 1991. Es un juego de un solo jugador, con versión adaptada al modo multijugador. El jugador toma el rol del regente de una civilización empezando con una simple unidad-poblador (a veces con dos) y trata de construir un imperio compitiendo contra otras civilizaciones. El objetivo del juego es dirigir esta civilización desde su inicio hasta llegar al espacio o conquistar todo el planeta (que puede ser la Tierra o un planeta creado al azar). Considerado por muchos como el mejor juego de estrategia para ordenador de la historia, cuenta con una gran comunidad de aficionados.

Este juego llegaría de la mano de Sid Meier a la consola PlayStation y a las PC con una idea nueva entre todos los juegos de estrategia: empezar una civilización desde una tribu nómada y llevarla a lo más alto posible en tecnología, poder y dinero. Un juego bastante largo donde se debe hacer investigación científica, defensa de ciudades, planificación de ataques hasta a seis civilizaciones más, treguas, tratados de paz y alianzas. Está diseñado para ser ejecutado en ventana de Windows con una nueva interfaz y gráficos en perspectiva isométrica que mejoraban a los de visión aérea de la versión anterior.

En esta edición se creó el concepto de «cultura» y «zona de influencia», con este último como resultado del crecimiento del primero. Los puntos de cultura se ganan por medio de la construcción, ya sea de "estructuras que generan cultura" en las ciudades como templos, catedrales, coliseos, bibliotecas, etc. o por medio de maravillas del mundo que traen cultura en grandes niveles. La cultura se gana por turnos y cuando se llega a ciertos niveles, se amplía el rango de cobertura de la zona de influencia de la ciudad en cuestión. La «zona de influencia» o «territorio» se refiere a la zona en la cual se ejerce soberanía por parte de una nación (más específicamente, se refiere a la suma de las zonas de influencia de las ciudades). Como normalmente las ciudades tienden a estar agrupadas (al menos al principio del juego), se forma un territorio el cual se considera nacional. Este concepto introdujo numerosos cambios relacionados en el sistema de juego. También se creó el concepto de «recursos», bienes presentes en el territorio del país cuyo consumo otorgará la posibilidad de crear mejores unidades y estructuras, o ayudará a hacer feliz a la población.

"Play the World" incluye la opción de juego en modo multijugador con la característica principal de que es posible jugarlo en tiempo real y no basado en turnos como las anteriores versiones. Además de nuevas tropas, e imágenes incluye dos nuevos modos de juego: regicidio y eliminación. En regicidio existe una nueva tropa, el rey o la reina y el objetivo del juego es destruir al rey enemigo.

"Conquests" incluye más mapas, diferentes tipos de gobierno (fascismo, imperialismo, consejo tribal, feudalismo), nuevos especialistas en la ciudad (unidades de policía e ingenieros civiles), más recursos, más maravillas, más civilizaciones, nuevas características del mapa (como volcanes) y algunas nuevas habilidades para las tropas como lo son el esclavismo y los ataques silenciosos.

Desarrollado por Firaxis y publicado por 2K Games el 25 de octubre de 2005, a agosto del 2006. Contiene todo el bagaje de experiencias de las versiones previas de "Civilization" así como nuevas mejoras que incorporan más responsabilidades al juego. Desde cierto punto de vista, cada vez es más difícil el juego pues cada vez hay más elementos a tener en cuenta. En "Civilization IV" aparece el concepto de religión. Ahora, en el árbol de tecnologías se ha incorporado el descubrimiento de siete religiones, las cuales podrán ser acogidas por la población de las ciudades para maximizar el nacionalismo y/o para controlar a la población. La ciudad sagrada tiene un gran ventaja: Puede ver todas las ciudades con templos de esa religión y recibe oro de esos templos, aunque sean de otra civilización. También aparece el concepto de grandes personajes para reemplazar el concepto del «Gran líder». Es posible conseguir grandes personajes como «gran mercader» o «gran artista», que podrán hacer avanzar la investigación científica de la civilización, o incluso introducirla en una edad de oro. 

Como se ha mencionado antes, existe una expansión llamada "", la cual introduce algunos elementos nuevos como el vasallaje entre civilizaciones, así como ciertas mejoras en el sistema de juego. Por otro lado, la expansión incluye la actualización de la versión del juego original, donde se solventan problemas de incompatibilidad que tenía el juego con tarjetas gráficas de la marca ATI, problema que incluso indujo a introducir una guía explicativa con un procedimiento alternativo para la solución de este problema en la página web oficial del juego.

"Civilization IV: Beyond the Sword" es una expansión oficial de "Civilization IV" lanzada durante la primavera de 2007. Esta expansión introdujo el concepto de espionaje para esta entrega, además de nuevos edificios, unidades y maravillas.

"Civilization IV: Colonization" es la segunda entrega de la saga Colonization, en el cual el objetivo es conquistar un continente inexplorado, poblado por indígenas. También recrea la economía de la época, como el comercio con Europa y los nativos. Utiliza el motor gráfico de "Civilization IV" y fue lanzado durante 2008.

En primavera de 2008 se lanzó una nueva versión para consolas de esta saga llamada "Civilization Revolution". El juego toma muchas características de Civilization IV, como el detalle de que una unidad de batalla esté compuesta a efectos visuales de un pelotón de hombres, además de esos típicos gráficos al estilo «caricatura». Posee características únicas, como por ejemplo la ausencia de trabajadores o que las rutas se compran y no se construyen.

En febrero de 2010 se anuncia el desarrollo de Civilization V con su salida anunciada para el 24 de septiembre para los mercados fuera de Estados Unidos. Como primera novedad por las imágenes mostradas se puede deducir que el tablero de juego pasará a utilizar casillas hexagonales. En esta edición se mejoran en gran medida los combates militares, agregando mayor realismo en cada batalla. Ahora las unidades de asedio y las de largo alcance son muy útiles, permitiéndonos debilitar a unidades y ciudades antes de ingresar en el combate cuerpo a cuerpo.

El 24 de octubre de 2014 sale a la venta Civilization: Beyond Earth. Como sucesor espiritual de Alpha Centauri, comparte parte del equipo de desarrollo de dicho juego, así como muchos conceptos introducidos en el título de 1999. Su puesta en escena es única en toda la saga Civilization, ya que está ambientado en el futuro, con la humanidad viajando a través del espacio y fundando colonias en planetas extra-terrestres. Beyond Earth es una entrega única en la saga. En contraste con el resto, que tienen lugar en diferentes periodos históricos de la historia humana, esta entrega está situada en un futuro distante. Utiliza el mismo motor que Civilization V (lo que significa que no hay unidades amontonadas, con la excepción de civiles y militares, unidades "orbitales" y hexágonos).

Es la sexta entrega principal de la serie. Desarrollado por Firaxis Games. Un título publicado por 2K Games y distribuido por Take-Two Interactive. El juego fue lanzado el 21 de octubre de 2016 para Windows, para OS X y Linux.

"Civilization" es considerado como un juego con un elevadísimo componente educativo. Se trata de un juego de estrategia y conquista basado en la evolución de la humanidad desde sus orígenes, desde el nacimiento de las primeras ciudades hasta la era espacial. Es un juego perfecto para interesar a los jóvenes por la historia del mundo, la organización, gestión, problemáticas sociales y económicas (polución, guerras, hambruna, catástrofes, etc.), diplomacia y el aspecto militar, ofreciendo la posibilidad de experimentar esa evolución bajo el propio mandato y dirección del jugador.

El aspecto educativo estuvo presente desde el principio en el juego. Los programadores añadieron una sección al juego llamada "Civilopedia", esto es, la enciclopedia de Civilization, en forma de ayuda del juego. El jugador podía consultarla cuando quisiera, por ejemplo para leer una introducción histórica acerca de cada unidad militar o avance científico. El concepto de "Civilopedia" tuvo tal éxito que se incorporó a varios de los juegos que Microprose publicó con posterioridad: por ejemplo, la «Colonizopedia» de Sid Meier's Colonization o la «UFOpaedia» de la saga X-COM.




</doc>
<doc id="646" url="https://es.wikipedia.org/wiki?curid=646" title="Ciencia ficción">
Ciencia ficción

Ciencia ficción es la denominación de uno de los géneros derivados de la literatura de ficción, junto con la literatura fantástica y la narrativa de terror. Algunos autores estiman que el término es una mala traducción del inglés "science fiction" y que la correcta es "ficción científica". Nacida como género en la década de 1920 (aunque hay obras reconocibles muy anteriores) y exportada posteriormente a otros medios, como el cinematográfico, historietístico y televisivo, gozó de un gran auge en la segunda mitad del debido al interés popular acerca del futuro que despertó el espectacular avance tanto científico como tecnológico alcanzado durante todos estos años.

Es un género especulativo que relata acontecimientos posibles desarrollados en un marco imaginario, cuya verosimilitud se fundamenta narrativamente en los campos de las ciencias físicas, naturales y sociales. La acción puede girar en torno a un abanico grande de posibilidades (viajes interestelares, conquista del espacio, consecuencias de una hecatombe terrestre o cósmica, evolución humana a causa de mutaciones, evolución de los robots, realidad virtual, existencia de civilizaciones alienígenas, etc.). Esta acción puede tener lugar en un tiempo pasado, presente o futuro, o, incluso, en tiempos alternativos ajenos a la realidad conocida, y tener por escenario espacios físicos (reales o imaginarios, terrestres o extraterrestres) o el espacio interno de la mente. Los personajes son igualmente diversos: a partir del patrón natural humano, recorre y explota modelos antropomórficos hasta desembocar en la creación de entidades artificiales de forma humana (robot, androide, cíborg) o en criaturas no antropomórficas.

Entre los estudiosos del género no se ha podido llegar a un consenso amplio sobre una definición formal, siendo este un tema de gran controversia. En general se considera ciencia ficción a los cuentos o historias que versan sobre el impacto que producen los avances científicos, tecnológicos, sociales o culturales, presentes o futuros, sobre la sociedad o los individuos. 

Su nombre deriva de una traducción demasiado literal del término en inglés, ya que la traducción apropiada siguiendo las reglas del castellano sería «ficción de/sobre la ciencia» (dos sustantivos, como el nombre original en inglés), y algunos lo llevan a traducir «ficción científica» (sustantivo más adjetivo) pero esto sería en inglés «scientific fiction». Si bien muchos expertos opinan que debería utilizarse este último, "ficción científica", el término ya está arraigado a la cultura popular.

El término original en inglés se escribe con un guion de unión cuando ocupa la función de un adjetivo o de un complemento. Por ejemplo: "science-fiction novel" («novela de ciencia ficción»). Para tales casos, en inglés, puede usarse si se lo desea la abreviatura «sci-fi». Este uso anglosajón del guion ha dado lugar a nuevos malentendidos lingüísticos pues el guion en español aglutina sustantivos donde el segundo modifica al primero, es decir, al contrario que en inglés. Por tanto, el uso «ciencia-ficción» en castellano no solo es una falta de ortografía sino que se distancia aún más del significado original en inglés. En español la regla ortográfica del término «ciencia ficción», escrito correctamente siempre sin guion, no es otra que la de la adjetivación del segundo substantivo, como en los términos «hombre lobo» u «hombre rana», escritos siempre sin guion. En castellano también se utilizan las iniciales «CF» para referirse al género.

El término «ciencia ficción» fue acuñado en 1926 por Hugo Gernsback cuando lo incorporó a la portada de una de las revistas de narrativa especulativa más conocidas de los años 1920 en Estados Unidos: "Amazing Stories". El uso más temprano del mismo parece datar de 1851 y es atribuido a William Wilson, pero se trata de un uso aislado y el término no se generalizó con su acepción actual, hasta que Gernsback lo utilizó de forma consistente (después de hacer un intento previo con el término «scientifiction» que no llegó a cuajar).

Es muy posible que hoy se usara la palabra «cientificción», pero Gernsback se vio obligado a vender su primera publicación, que tenía ese nombre. Sin darse cuenta, había vendido los derechos sobre el término y muy a pesar suyo se vio obligado a dejar de usarlo y utilizar en su lugar el término «ciencia ficción». 

De modo, que hasta el año 1926 la ciencia ficción no existía como tal. Hasta esa fecha las narraciones que hoy día no dudamos en calificar de ciencia ficción recibían diversos nombres, tales como «viajes fantásticos», «relatos de mundos perdidos», «utopías», o «novelas científicas». 

El canadiense John Clute denomina a esta época anterior a la eclosión del género "proto ciencia ficción". 

A pesar de la existencia de una protociencia ficción francesa que consta de "Le voyageur philosophe dans un pays inconnu aux habitants de la Terre" (1761) de Daniel Jost de Villeneuve y "El año 2440" (1771) del prerromántico francés Louis-Sébastien Mercier, e incluso de una española (integrada por el "Viaje estático al mundo planetario", 1780, de Lorenzo Hervás y Panduro y el "Viaje de un filósofo a Selenópolis" (1804) de Antonio Marqués y Espejo, para muchos (para los anglosajones, sobre todo) la primera obra de ciencia ficción con contenidos similares a los del género, tal y como hoy se entiende, se remonta a 1818, año en que se publica "Frankenstein o El moderno Prometeo" de Mary Shelley. Aunque algunos ven elementos de ciencia ficción en leyendas y mitos muchos siglos antes. En la mitología griega, se cuenta que Dédalo, el padre de Ícaro y constructor del Laberinto de Creta, construyó estatuas de madera que eran capaces de moverse solas (¿una primitiva referencia a los modernos robots?). Y en el folclore judío también está presente el mito del Golem. Incluso el viaje a la Luna fue objeto de iniciativas literarias antes de 1818. Luciano de Samosata, , en una novela corta, "Historia Verdadera", relata un viaje a la Luna en un barco arrastrado por una providencial tromba de agua. Sin embargo, las más conocidas primerizas historias de viajes a la Luna son la de Cyrano de Bergerac, en el , y la del Barón de Münchhausen, . Sin embargo, Carl Sagan e Isaac Asimov coinciden en que "Somnium" (1634) de Johannes Kepler es el primer relato de ciencia ficción como tal. "Somnium" describe a un aventurero que viaja a la Luna y muestra la preocupación de Kepler por el tema de cómo se verían los movimientos de la Tierra desde la Luna.

Habrá algunos que cuestionen la calificación de estas obras como ciencia ficción (ni siquiera como proto ciencia ficción). El propio John Clute excluye la obra de Bergerac frente a otros que consideran que "Otros Mundos" es auténtica ciencia ficción, ya que a pesar de estar escrito en tono de comedia recurre a los términos científicos de la época. En cualquier caso, cualquiera de estos clásicos cuentos heredan gran parte del espíritu del racionalismo cartesiano del que sentó las bases de la ciencia moderna. 

Resulta difícil establecer límites. Clute, en su enciclopedia ilustrada, pone en duda la existencia del género antes de finales del pero cita como precursor a Tomás Moro; que en su más famosa obra, "Utopía" (1516), describe en forma de narración una sociedad perfecta que reside felizmente en la isla Utopía.

Sin embargo, como se comenta más arriba, casi todos los expertos reconocen que la obra que supuso un antes y un después en la concepción de la literatura de ficción científica fue la obra de Shelley.

Los primeros años tras la aparición de "Frankenstein" dieron pocos frutos. Se puede destacar quizá otra de las obras de Shelley como "El último hombre".

En la década de 1830, el estadounidense Edgar Allan Poe anticipó igualmente la narrativa de ciencia ficción (o ficción científica) en relatos como "La incomparable aventura de un tal Hans Pfaal", "El poder de las palabras", "Revelación mesmérica", "La verdad sobre el caso del señor Valdemar", "Un descenso al Maelström", "Von Kempelen y su descubrimiento", etc. Dichos relatos reúnen algunos de los elementos primitivos de la ciencia ficción, como el mesmerismo y los viajes en globo —muy en boga en aquella época— y la especulación cosmológica, también presente en su visionario ensayo "Eureka", en el cual parecen describirse los agujeros negros y algo parecido al Big Crunch ("op. cit". p. 11).

Posteriormente, en la década de 1850 aparece el que probablemente pasa por ser uno de los autores más prolíficos del en el campo de las aventuras de corte científico: Julio Verne, quien en 1863 publicó su primera obra con contenido de ficción científica: "Cinco semanas en globo". La aparición de esta obra supone un hito. A partir de su publicación, este género empieza a transformar sus intenciones. La ciencia subyacente pasa de ser un motivo de inquietud o de preocupación por lo desconocido, a ser un soporte de historias de aventuras y descubrimientos.

La rama europea de la ciencia ficción comenzó propiamente a finales del con las novelas científicas de Julio Verne (1828-1905), cuya ciencia se centraba más bien en invenciones, así como con las novelas de crítica social con orientación científica de H. G. Wells (1866-1946). Sin embargo, aunque Wells suele ser reconocido como el gran iniciador del género, Roger Luckhurst demuestra que solo fue el más influyente de una corriente que comenzó pocos años antes.

Wells y Verne rivalizaron en la primitiva ciencia ficción. Los relatos y novelas cortas con temas fantásticos aparecieron en las publicaciones periódicas en los últimos años del , y muchos de ellos emplearon ideas científicas como una excusa para lanzarse a la imaginación. Aunque es más conocido por otros trabajos, Sir Arthur Conan Doyle también escribió ciencia ficción. El único libro en el que Charles Dickens se aventura en el territorio de la especulación científica y los extraños misterios de la naturaleza (en contraposición a los claramente sobrenaturales fantasmas de Navidad) fue en su novela "Bleak House" (1852) en la que uno de sus personajes muere por «combustión humana espontánea». Dickens investigó casos registrados de tal efecto antes de escribir sobre el tema para ser capaz de contestar a los escépticos que se escandalizaran con su novela.

El siguiente gran escritor británico de ciencia ficción tras H. G. Wells fue John Wyndham (1903-1969). A este autor le gustaba referirse a la ciencia ficción con el nombre de «fantasía lógica». Antes de la Segunda Guerra Mundial Wyndham escribió exclusivamente para las revistas "pulp", pero tras la contienda se hizo famoso entre el público en general, más allá de la estrecha audiencia de los fanes de la ciencia ficción. La fama le vino de la mano de sus novelas "El día de los trífidos" (1951), "El kraken acecha" (1953), "Las crisálidas" (1955) y "Los cuclillos de Midwich" (1957).

Fuera del ámbito anglosajón hay que destacar la figura de Karel Čapek, introductor del término robot en su obra teatral "R.U.R." y creador del clásico de la ciencia ficción "La guerra de las salamandras" en 1937.

Mucho tiempo antes de que la novela de H.G. Wells, La máquina del tiempo, viera la luz, el escritor Enrique Gaspar ya había publicado una novela sobre viajes temporales. De su imaginación nació "El Anacronópete", que podría ser considerada como la primera novela en la que una máquina del tiempo aparece como elemento central. Editada en Barcelona a principios de 1887. No obstante, a finales del y principios del , numerosos escritores de prestigio escriben relatos, novelas y obras de teatro de ciencia ficción, como por ejemplo Miguel de Unamuno, Azorín, Vicente Blasco Ibáñez, Agustín de Foxá, Ramiro de Maeztu o Jardiel Poncela. Muchos de estos relatos fueron publicados en una antología por Santiáñez-Tió e incluso se van editando poco a poco textos inéditos o de difícil acceso.

En los Estados Unidos de América el género puede remontarnos a Mark Twain y su novela "Un yanqui en la corte del rey Arturo", una novela que exploraba términos científicos aunque fueran enmarcados en una ficción caballeresca. Mediante el recurso a la «transmigración del alma» y la «transposición de épocas y cuerpos» el yankee de Twain es transportado hacia atrás en el tiempo y arrastra consigo todo el conocimiento de la tecnología del . Los resultados son catastróficos, ya que la caballeresca aristocracia del rey Arturo se ve pervertida por el notable poder de destrucción que ofrecen máquinas como las ametralladoras, los explosivos y el alambre de espino. Escrita en 1889, "Un yankee" parece predecir sucesos que tendrían lugar 25 años después en 1914, cuando las viejas ideas caballerescas europeas en lo tocante al arte de la guerra acabarían hechas pedazos por las armas y las tácticas de la Primera Guerra Mundial.

Otro autor que escribió algunas historias de este tipo es Jack London. El autor de las novelas de aventuras en el salvaje Yukon, Alaska, y el Klondike, también escribió historias sobre extraterrestres ("The Red One"), sobre el futuro ("El talón de hierro") o sobre los conflictos del futuro ("La invasión sin precedentes"). También escribió una historia sobre la invisibilidad y otra sobre un arma de energía para la que no existía defensa alguna. Estas historias impactaron en el público estadounidense y comenzaron a perfilar algunos de los temas clásicos de la ciencia ficción.

Pero el autor estadounidense que mejor simboliza el nacimiento en Estados Unidos de la ciencia ficción como género de masas es Edgar Rice Burroughs quien, poco antes de la Primera Guerra Mundial, publicó "Bajo las lunas de Marte" (1912) en varios números de una revista especializada en aventuras. Burroughs siguió publicando en este medio durante el resto de su vida, tanto fantasía científica como historias de otros géneros (misterio, horror, fantasía y, cómo no, su personaje más conocido: Tarzán); pero, las historias de John Carter (ciclo de Marte) y Carson Napier (ciclo de Venus), aparecidas en aquellas páginas, hoy día se consideran joyas de la ciencia ficción más temprana.

No obstante, el desarrollo de la ciencia ficción estadounidense como género literario específico hay que retrasarlo hasta 1926, año en el que Hugo Gernsback funda "Amazing Stories", creándose la primera revista dedicada exclusivamente a las historias de ciencia ficción. Por otra parte, dado que como es bien conocido, fue él quien eligió el término "scientifiction" para describir a este género incipiente, el nombre de Gernsback y el vocablo al que dio origen han quedado unidos para la posteridad. Las historias que se publicaban en esta y otras exitosas revistas "pulp" ("Weird Tales", "Black Mask"...), no gozaban del aval de la crítica seria, que en su mayoría las consideraban sensacionalismo literario, sin embargo fue en estas revistas, que mezclaban a partes iguales la fantasía científica con el terror, donde empezaron a brillar algunos de los grandes nombres del género, como Howard Phillips Lovecraft, Fritz Leiber, Robert Bloch, Robert E. Howard, etc. Todo ello atrajo a muchos lectores a las historias de especulación científica propiamente dicha.

Con el surgir en 1938 del editor John W. Campbell y su actividad en la revista "Astounding Science Fiction" (fundada en 1930) y con la consagración de los nuevos maestros del género: Isaac Asimov, Arthur C. Clarke y Robert A. Heinlein, la ciencia ficción empezó a ganar estatus como género literario, especialmente con este último, que fue el primer autor que consiguió que se editaran historias del género en publicaciones más generales, y fue también el que le dio mayor madurez al género e influyó poderosamente en su desarrollo posterior. 

Las incursiones en el género de autores que no se dedicaban exclusivamente a la ciencia ficción también generaron un mayor respeto hacia el mismo; caben destacar Karel Čapek, Aldous Huxley, C. S. Lewis y en castellano Adolfo Bioy Casares y Jorge Luis Borges. 

Después de la Segunda Guerra Mundial se produce una transición del género. Es la época en la que los cuentos empiezan a ser desplazados por las novelas y los argumentos ganan en complejidad. Las revistas mostraban llamativas portadas con monstruos de ojos de mosca y mujeres medio desnudas, dando una imagen atrayente para lo que era su público principal: los adolescentes.
Se fundan nuevas revistas: hasta 15 nuevas publicaciones en un solo año; y alguna incluso atraviesa el océano Atlántico como la francesa "Galaxie" (prima hermana de la estadounidense "Galaxy" que empieza a publicarse el año 1950), pero ahora el género empieza a salir del terreno exclusivo del "pulp".

Posiblemente, el que puede tal vez considerarse como primer título notable de la posguerra no fue escrito por un autor habitualmente catalogado como escritor de ciencia ficción y, de hecho, el libro ni siquiera fue catalogado como tal por su editor; pero sin duda lo es, y le dio a su autor fama mundial; nos referimos a "1984" (1948) de George Orwell. Pero la mejor tarjeta de visita del período de los años 1950 es su interminable lista de escritores que han sido la columna vertebral del género hasta casi finales de siglo: Robert A. Heinlein, Isaac Asimov, Clifford D. Simak, Arthur C. Clarke, Poul Anderson, Philip K. Dick, Ray Bradbury, Frank Herbert, Stanislav Lem y muchos otros.

En cuanto a los títulos, de esta época son libros que hoy son considerados clásicos: "Crónicas marcianas" o "Fahrenheit 451" de Ray Bradbury, "Mercaderes del espacio" de Frederik Pohl y Cyril M. Kornbluth, "Más que humano" de Theodore Sturgeon; sin olvidar "El fin de la eternidad" de Isaac Asimov, y "Lotería solar" o "El hombre en el castillo" de Philip K. Dick. Algunas de ellas serían adaptadas al cine o la televisión; "La naranja mecánica" de Anthony Burgess es un buen ejemplo de ello. También es en esta época cuando empiezan a otorgarse los premios Hugo, cuya primera edición fue en 1953.

En realidad, pese a que desde el punto de vista académico se ha venido en calificar como «edad de oro» a la etapa comprendida entre los años 1938 y 1950, para muchos, esta época debería extenderse unos quince años.

Otra novela importante de este período es "Dune" (1965) de Frank Herbert.

Los años transcurridos entre 1965 y 1972 son el período de mayor experimentación literaria de la historia del género. En Reino Unido, se puede asociar con la llegada de Michael Moorcock a la dirección de la revista "New Worlds". Moorcock, entonces un joven de 24 años, dio espacio a las nuevas técnicas ejemplificadas en la literatura de William Burroughs y J.G. Ballard. Los temas empezaron a distanciarse de los tan manidos robots e imperios galácticos de las edades de oro y plata de la ciencia ficción, centrándose en temas hasta entonces inexplorados: la consciencia, los mundos interiores, relativización de los valores morales, etcétera.

En Estados Unidos, los ecos de los cambios experimentados en el panorama británico tuvieron su reflejo. Autores como Samuel Ray Delany, Judith Merril, Fritz Leiber, Roger Zelazny, Philip K. Dick, Ursula K. LeGuin, Philip José Farmer y Robert Silverberg, representan la esencia de las nuevas vías de este género literario.

En la década de 1980 las computadoras cada vez más ubicuas y la aparición de las primeras redes informáticas globales dispararon la imaginación de jóvenes autores, convencidos de que tales prodigios producirían profundas transformaciones en la sociedad. Este germen cristalizó principalmente a través del llamado movimiento ciberpunk, un término que aglutinaba una visión pesimista y desencantada de un futuro dominado por la tecnología y el capitalismo salvaje con un ideario «punk» rebelde y subversivo, frecuentemente anarquista. Una nueva generación de escritores surgió bajo esta etiqueta, encabezados por William Gibson, Bruce Sterling y Neal Stephenson.

A principios de la década de 1990 ocurrió un cambio significativo en la literatura de ciencia ficción. Autores antes plenamente ciberpunk o que nunca habían pertenecido a esa corriente, comenzaron a rechazar explícitamente los clichés de dicho género, y de paso, a considerar a la tecnología con una visión más positiva. Es notorio que esto ocurría casi al mismo tiempo que se daba la acelerada introducción de las computadoras e Internet en la vida cotidiana. Conforme los autores empezaron realmente a usar las computadoras y la red global, sus opiniones y obras empezaron a cambiar y a rechazar la rebeldía y exaltación de la marginalidad del ciberpunk.

En las novelas postciberpunk, es mucho más frecuente que los protagonistas sean integrantes respetables de sus comunidades: científicos, militares, policías e incluso políticos. Aun en el caso de personajes más marginales, su interés suele residir en mantener o mejorar el statu quo, no en destruirlo, tal y como era lo típico en el ciberpunk; y cuando no lo hacen, suelen ser los antagonistas.

La primera novela etiquetada como postciberpunk es "Snow Crash" (1992) de Neal Stephenson. Además de Stephenson, han sido etiquetados como postciberpunk autores tan dispares como Nancy Kress, Greg Egan, Tad Williams, Charles Stross o Richard Morgan.

En épocas recientes, a la ciencia ficción se le han agregado varios subgéneros cuyos nombres usan también el postfijo «punk». Esto por analogía con el «ciberpunk», que es ciencia ficción centrada en la cibernética. Estos subgéneros responden en ocasiones a impulsos estilísticos de los autores, o a la demanda de los lectores y espectadores, pidiendo más obras con el mismo estilo de ciertas obras originales. Entre estos subgéneros están:

La ciencia ficción está ineludiblemente ligada a las revistas. La propia expresión "ciencia ficción" apareció en una de ellas. Probablemente, la primera revista periódica con algunos cuentos de este género (todavía sin nombre oficial) se podría considerar "The Argosy" 1896. No obstante, "The Argosy" no era una revista exclusivamente dedicada a las historias fantásticas con contenido científico. Otra revista temprana fue "All Story", que comenzó a publicarse en 1911; en ella aparecieron la mayoría de los cuentos de Edgar R. Burroughs de fantasía científica.

Sin embargo, las dos revistas precursoras más famosas no llegarían hasta la década de 1920; en 1923 empezó a publicarse "Weird Tales" (cuya versión española se llamó "Narraciones Terroríficas"), y 1926, año en el que Hugo Gernsback acuñó el término con el que definitivamente se conocería el género para la otra de las dos «precursoras oficiales»: "Amazing Stories". "Amazing" fue la primera de todas ellas en dedicarse de forma exclusiva a la ficción de corte científico y tuvo una larga trayectoria. Sus primeras historias eran principalmente reimpresiones de obras de Poe, Wells y Verne; pero también se publicaron relatos inéditos de gente como Burroughs y Merrit. "Amazing" se puede considerar como la revista más influyente durante muchos años y un punto de referencia durante todo el curso de su existencia. En 1980, tras su última etapa bajo la edición de Kim Mohan, la revista dejó de publicarse y, aunque varios editores han intentado resucitarla desde entonces, actualmente se puede considerar fuera de circulación.

En 1930 surgió otra de las revistas clásicas que todos los historiadores incluyen en su relación de publicaciones de la «edad de oro», "Astounding Stories", la que más tarde sería reeditada por John W. Campbell como "Astounding Science Fiction" (1938) y que finalmente derivaría en la actual "Analog Science Fiction and Fact" (1960) y en la que escribieron los grandes escritores del género de aquellos días, entre los que se incluyen a Isaac Asimov, Robert A. Heinlein y Poul Anderson. "Astounding/Analog" (también conocida por sus siglas "ASF") es considerada una revista de corte más «cientificista» que otras, siendo una de las publicaciones esenciales desde sus inicios hasta el presente. En 1971, tras la muerte de Campbell, "Analog" pasó a ser editada por Ben Bova, también conocido por ser el valedor de Orson Scott Card y aquel que lo lanzó a la fama. Desde 1978 la edita Stanley Schmidt.

En 1949 empezó a publicarse otra revista que tiene en su haber la mayor serie de colaboraciones (en este caso ensayos científicos) de Isaac Asimov, un total de 399 colaboraciones mensuales a lo largo de 33 años. Se trata de "The Magazine of Fantasy & Science Fiction". Esta revista fue primeramente editada por Antony Boucher, y su editor actual, Gordon van Gelder, mantiene una revista de gran calidad literaria. En sus páginas se han publicado clásicos como "Flores para Algernon" de Daniel Keyes.

Otra de las revistas que no podíamos dejar de mencionar es "Galaxy" (1950). Inicialmente editada por Horace Leonard Gold tiene en su haber las mejores críticas literarias gracias a la aceptación del público de un género que empezaba a consagrarse fuera de los círculos del "pulp". Con ver la lista de autores que publicaron en su primer número podemos hacernos una idea de su calidad y empuje: Clifford D. Simak, Theodore Sturgeon, Fritz Leiber o Isaac Asimov. Esta revista llegó a publicarse en Europa (en Francia y Alemania), tuvo cierto éxito durante casi treinta años hasta que en 1980 dejó de publicarse. A principio de los años 1990 el hijo de su fundador retomó la publicación de "Galaxy", pero finalmente la empresa terminó de forma infructuosa en 1995.

El género está en alza. Todos los años aparecen nuevas revistas. Algunas intentan aprovechar el tirón publicitario de un nombre conocido para entrar en un mercado muy competitivo. Es, por ejemplo, el caso de "Asimov's Science Fiction" que empezó a publicarse en 1977 bajo la dirección del propio Isaac Asimov y con George H. Scithers como editor. Este hecho, no obstante, no tiene porqué restar calidad a estas empresas y, por ejemplo, las historias publicadas en "Asimov's" han sido galardonadas con frecuencia con premios Hugo y Nébula.

También en español, llegaron a publicarse algunas revistas clásicas, como la anteriormente mencionada "Narraciones". Aunque también hubo iniciativas puramente autóctonas. De ellas, la más conocida comenzó su vida en 1968. Se trata de "Nueva Dimensión" ("ND"), fundada por Domingo Santos, y estuvo en circulación hasta 1983, habiendo obtenido durante esos años varios premios internacionales. Otra revista, esta mucho más moderna, con cierto renombre es "Gigamesh", que empezó a publicarse en 1991; no obstante, nunca ha llegado a tener la repercusión literaria de "ND". Tras varias publicaciones sin periodicidad alguna ha dejado también de publicarse. También la revista "Galaxia", que bajo la dirección de León Arsenal, obtuvo en 2003 el premio a la mejor publicación de literatura fantástica, concedido por la Sociedad Europea de Ciencia ficción. Como vemos, muchas revistas han sufrido una trayectoria muy irregular, con sucesivas resucitaciones y desapariciones, hecho que ha impedido que lleguen a ser conocidas de forma extensa. Volvió a aparecer durante un tiempo una de estas últimas: "Asimov Ciencia Ficción" (versión española de su homónima estadounidense), pero cerró definitivamente al cabo de unos pocos años. Ninguna de las importaciones de la célebre revista estadounidense en España ha tenido éxito. 

Ya en los últimos años, ha aparecido el magazín en línea "Scifiworld Magazine" que dedicado principalmente al género fantástico en el medio audiovisual informa cada mes de las novedades del género junto a interesantes artículos de diversa índole. A partir de julio de 2006, la revista pasa a formar parte de la cadena de televisión Sci Fi y pasa a llamarse "scifi.es". Otras publicaciones digitales han sido Revista Exégesis, surgida en el 2009 y especializada en el cómic de ciencia ficción; y en el género de cuento figuran Axxón (una de las más antiguas revista digitales de ciencia ficción, originada en 1989); "Alfa Eridiani", "Cosmocápsula", entre otras.

También en 2006, la A.C. Xatafi comienza la publicación digital de la revista "Hélice: reflexiones críticas sobre ficción especulativa". Desde entonces ha mantenido una regularidad notable con un considerable éxito. Esta revista ha sido la primera en plantear una dignificación del género en España mediante estudios y críticas de mayor nivel, entre la difusión y el academicismo, con una consideración profesional de la figura del crítico. Ganó el premio Ignotus de la AEFCFT a la mejor revista publicada en 2007.

En 2008 también la A.C. Xatafi publicó el primer número digital de "Artifex", revista de cuentos de género que recoge el relevo de la edición en papel. Su precursora, tras pasar por varios formatos, se ha mantenido durante años como el referente para la publicación de relatos de ciencia ficción en España.

2008 es también el año en el que aparece la versión impresa de "Scifiworld Magazine", independizados ya del canal SyFy, y que hasta el momento ha sobrepasado los 40 números convirtiéndose en la revista más longeva en España dedicada a la ciencia ficción, la fantasía y el terror en la cultura y el entretenimiento.

El género de la ciencia ficción ha estado presente en el cine bien mediante la adaptación de cuentos y novelas, bien mediante la producción de películas con guiones especialmente creados para la pantalla. El cine de ciencia ficción se ha utilizado en ocasiones para la crítica de aspectos políticos o sociales y para la exploración de cuestiones filosóficas como la propia definición del ser humano.

El género ha existido desde los comienzos del cine mudo, cuando el "Viaje a la Luna" (1902) de Georges Méliès asombró a su audiencia con sus efectos fotográficos. Desde la década de 1930 hasta la de 1950, el género consistía principalmente en películas de serie B de bajo presupuesto. Tras el hito de Stanley Kubrick de "" de 1968, el cine de ciencia ficción fue tomado más en serio. A finales de la década de 1970, películas de presupuesto alto con efectos especiales se convirtieron en populares entre la audiencia. Películas como "Star Wars" o "Close Encounters of the Third Kind" allanaron el camino de éxitos de ventas en las siguientes décadas como "" (1979), "E.T., el extraterrestre" (1982), "Blade Runner" (1982) , "Hombres de negro" (1997) y "El quinto elemento" (1997).

La ciencia ficción apareció primeramente en televisión durante la época de oro de la ciencia ficción, primero en Gran Bretaña y después en los Estados Unidos. Los efectos especiales y otras técnicas de producción permiten que los creadores presenten una imagen viviente de un mundo imaginario que no se limita a la realidad; esto hace de la televisión un medio excelente para la ciencia ficción, que a su vez contribuye a su popularidad de esta forma. 

Debido a su modo de presentación visual, la televisión emplea mucha menos exposición que los libros para explicar los apuntalamientos de la puesta de ficción. Como resultado, la definición y los límites del género son observados de una forma menos estricta que en los medios impresos. Como el costo de crear un programa de televisión es relativamente alto en comparación con el costo de escribir e imprimir libros, los programas de televisión están obligados a atraer a una audiencia mucho mayor que la ficción impresa. Algunos escritores y lectores creen que un efecto de mínimo común denominador le resta calidad de la ciencia ficción en televisión, en relación con los libros. 

Al debilitarse los límites del género, los guionistas y espectadores deben utilizar estándares más inclusivos que los autores y lectores, de tal modo que en muchos contextos se considera que la categoría de ciencia ficción en televisión incluye a todos los géneros especulativos, entre ellos el de fantasía y el de terror. En Reino Unido, a este grupo se le llama «telefantasía». 

Los ejemplos más famosos y duraderos sobre trabajos en este campo son Doctor Who, Star Trek, Galáctica y Stargate, aunque muchas otras series han atraído audiencias grandes y pequeñas durante décadas.

La historieta o cómic de ciencia ficción constituye uno de los géneros más importantes en los que puede dividirse la producción historietística. Los años 1970 y 1980 fue el momento de mayor auge de la ciencia ficción en este medio, que popularizó el género entre millones de lectores. Las historietas ofrecieron las escenas más acertadas de la navegación interestelar, de los alunizajes, de las bombas atómicas o de las sociedades hiperindustrializadas.

La ciencia ficción también está presente en numerosos y .

Los dos premios más importantes del género son los premios Hugo y los premios Nébula.

Los premios Hugo, llamados así en memoria del pionero de la ciencia ficción Hugo Gernsback, son concedidos en diversas categorías por la Sociedad mundial de ciencia ficción (WSFS) durante la celebración anual de la "Worldcon". Durante la misma se entrega además el premio John W. Campbell al mejor autor novel del año.

Los premios Nébula son concedidos anualmente también en varias categorías por la Asociación de escritores de ciencia ficción y fantasía de Estados Unidos (SFWA). Esta asociación además concede los cotizados premios Gran Maestro a los más importantes escritores del género por la labor de toda una vida.

Algunos otros premios también tienen nombres de otros insignes autores y editores del ramo: John W. Campbell Memorial (no confundir con el del mismo nombre al mejor autor novel) y los premios Clarke, Sturgeon y Philip K. Dick.

También las publicaciones especializadas otorgan algunos premios de relevancia como es el caso de la revista estadounidense "Locus Magazine", que anualmente otorga los premios Locus.

En Europa, la Sociedad Europea de Ciencia Ficción (ESFS) se creó en 1972 y reúne a diversos profesionales del sector. Inicialmente programaba una convención bianual que a partir de 1982 se convirtió en anual, durante la cual se otorgan los premios europeos de ciencia ficción en los que se nomina al mejor: autor, traductor, promotor, publicación periódica, editorial, artista y revista.

En España, existen dos grandes premios. Los premios Ignotus, otorgados por la AEFCFT, que son votados por los socios y por los asistentes a la convención nacional anual Hispacón. Serían los equivalentes españoles a los Hugo. Han sido otorgados desde 1991 y cuentan con varias categorías. Por otra parte, el Premio Xatafi-Cyberdark es otorgado por la A.C. Xatafi y por la librería virtual Cyberdark. Los premiados son elegidos por un jurado rotativo compuesto por varios críticos de toda España que a lo largo de un año discuten en lista privada sobre todo lo publicado el año anterior. Se concede desde 2006 e incluye las categorías de Mejor libro español, Mejor libro extranjero, Mejor cuento español, Mejor cuento extranjero y Mejor iniciativa editorial en España. Desde 2012, la revista Scifiworld concede también un premio en el que incluye obras literarias y audiovisuales.

Otros países también tienen sus premios nacionales: el premio Seiun en Japón, los BSFA británicos, los Ditmar australianos, etcétera.

En Estados Unidos, cuna del género, se otorgan los Saturno por la Academia de cine de ciencia ficción, fantasía y horror, siendo, probablemente, los premios más importantes del género.

En Europa los premios están más relacionados con festivales concretos, en los que se exhiben diferentes películas. El Festival de Cine de Sitges junto con el Festival Internacional de Cine Fantástico de Bruselas son las dos citas europeas más importantes del género.

En Latinoamérica existen pocos festivales especializados, uno de ellos es el Buenos Aires Rojo Sangre.

Esta clasificación dicotómica, literalmente "dura" y "blanda", se refiere a dos tendencias opuestas a la hora de elaborar los planteamientos científicos sobre los que se basa la obra.

En el caso de la ciencia ficción "hard" los elementos científicos y técnicos están tratados con el máximo rigor, incluso cuando estos entran dentro de la pura especulación, y la narración se subordina a este rigor. La película de ciencia ficción "hard" por excelencia es "". Gran parte de la ciencia ficción soviética se inscribe en esta línea.

Respecto a la ciencia ficción "soft" escribe:

Obviamente la distinción entre ambas vertientes es difusa y podemos encontrarnos obras que comparten ambos enfoques. Pero, por lo general, los autores de ciencia ficción se pueden englobar en una categoría u otra.


En la ciencia ficción se tratan una gran cantidad de temas. Algunos de ellos son:

De igual manera que la ciencia ficción ha tomado muchos de sus argumentos y elementos de ambientación de conceptos o creaciones de la ciencia, esta ha tomado en ocasiones elementos de la literatura de ciencia ficción para convertirlos en conceptos reales o hipótesis de trabajo de cara al futuro científico o tecnológico.

Los casos más conocidos de esta transferencia son los del término robot empleado por primera vez por el escritor checo Karel Čapek -el cual deriva de la palabra «"robota"», que en su idioma significa «trabajo duro y pesado»; dado que se entendía por estos como máquinas específicas para realizar estas funciones- en su obra "R.U.R. (Robots Universales de Rossum)", el término derivado robótica, creado en las novelas de robots de Isaac Asimov, el ascensor espacial, imaginado por Arthur C. Clarke y Charles Sheffield de manera independiente, o el concepto de órbita geoestacionaria, desarrollado por Herman Potočnik y posteriormente por Arthur C. Clarke. Es por ello que también se conoce como órbita de Clarke.

Otros conceptos han sido profusamente desarrollados por la ciencia ficción incluso antes de ser tenidos en cuenta por la ciencia. Por ejemplo, Julio Verne en "De la Tierra a la Luna" (1865) describió cómo tres hombres son lanzados desde Florida hacia la Luna. De ese mismo punto partieron los astronautas del Apolo 11 cien años después. En "The world set free" ("El mundo liberado", 1914), H.G. Wells predijo la energía nuclear y la utilización de la bomba atómica en una futura guerra con Alemania. Y en la novela " Ralph 124C 41+" (1911), Hugo Gernsback describió detalladamente el radar antes de haber sido inventado. La ciencia ficción también ha especulado sobre la antimateria, los agujeros de gusano o la nanotecnología antes que la propia ciencia. 

Algunos conceptos han tenido una notable influencia, a pesar de no ser en la actualidad más que meras invenciones de la imaginación. Por ejemplo, la psicohistoria de Asimov ha influido levemente en la forma de ver la sociología desde un punto de vista matemático.

Finalmente, y de modo sorprendente, algunas invenciones de la ciencia ficción han inspirado alguna de las líneas de investigación actual, como la comunicación instantánea (ansible, taquiones).

Dentro de la terminología de la ciencia ficción, existen palabras que resultan comunes para los lectores asiduos del género pero no para los nuevos lectores.

Sin embargo, no están creadas como una forma de lenguaje identificativo, sino que la mayor parte de las veces son ideas y conceptos interesantes que se han convertido en dominio público, dentro del género e incluso fuera de él, en el mundo de la ciencia.

Estos términos son muy usados dentro de los relatos y novelas de ciencia ficción. Como ejemplo tenemos el hiperespacio, que es una especie de «espacio alternativo» por el que se puede viajar de un punto a otro; las sociedades o mentes colmena, que son sistemas con inteligencia compuestos por la mente de muchos seres y no solo de uno, etcétera.





Recursos y glosarios


</doc>
<doc id="647" url="https://es.wikipedia.org/wiki?curid=647" title="Verificación de redundancia cíclica">
Verificación de redundancia cíclica

La verificación por redundancia cíclica (CRC) es un código de detección de errores usado frecuentemente en redes digitales y en dispositivos de almacenamiento para detectar cambios accidentales en los datos. Los bloques de datos ingresados en estos sistemas contiene un "valor de verificación adjunto", basado en el residuo de una división de polinomios; el cálculo es repetido, y la acción de corrección puede tomarse en contra de los datos presuntamente corruptos en caso de que el valor de verificación no concuerde. Este código es un tipo de función que recibe un flujo de datos de cualquier longitud como entrada y devuelve un valor de longitud fija como salida. El término suele ser usado para designar tanto a la función como a su resultado. Pueden ser usadas como suma de verificación para detectar la alteración de datos durante su transmisión o almacenamiento. Las CRC son populares porque su implementación en "hardware" binario es simple, son fáciles de analizar matemáticamente y son particularmente efectivas para errores ocasionados por ruido en los canales de transmisión. La CRC fue inventada y propuesta por W. Wesley Peterson en un artículo publicado en 1961.

El CRC es un código de detección de error cuyo cálculo es una larga división de computación en el que se descarta el cociente y el resto se convierte en el resultado, con la importante diferencia de que la aritmética que usamos conforma que el cálculo utilizado es el arrastre de un campo, en este caso los bits. El tamaño del resto es siempre menor que la longitud del divisor, que, por lo tanto, determina el tamaño del resultado. La definición de un CRC especifica el divisor que se utilizará, entre otras cosas. Aunque un CRC se puede construir utilizando cualquier tipo de regla finita, todos los CRC de uso común emplean una base finita binaria, esta base consta de dos elementos, generalmente el 0 y 1. El resto de este artículo se centrará en este tipo de composición, es decir el ámbito binario y los principios generales de los CRC.

Es útil para detección de errores, pero, en condiciones de seguridad, no podemos confiar en que el CRC puede verificar plenamente que los datos son los correctos en caso de que se hayan producido cambios deliberados y no aleatorios.

A menudo se piensa que si, cuando llega un mensaje, este y su CRC coinciden, quiere decir que el mensaje no ha podido ser alterado durante su transmisión, aunque se haya transmitido por un canal abierto.

Esta suposición es falsa porque CRC es un mal método de cifrado de datos. De hecho, el CRC no se trata realmente de un método de cifrado, lo que realmente hace es utilizarse para el control de integridad de datos, pero en algunos casos se supone que se utilizarán para el cifrado.

Cuando un CRC se calcula, el mensaje se conserva (no cifrado) y la constante de tamaño CRC se sitúa hacia el final (es decir, el mensaje puede ser tan fácil como leer antes de la posición que ocupa el CRC).

Además, la longitud del CRC es por lo general mucho más pequeña que la longitud del mensaje, es imposible para una relación de 1:1 entre la CRC y el mensaje.

Así, numerosos códigos producirán el mismo CRC.

Por supuesto, estos códigos están diseñados para ser lo suficientemente diferentes como para variar (y por lo general solo en uno o dos bits). Pequeños cambios en la palabra clave producirían una gran diferencia entre un CRC y otro; por ese motivo es posible detectar el error.

Si la manipulación del mensaje (cambios de los bits) es deliberada, entonces se tomara una nueva clave, produciendo un falso CRC el cual puede ser calculado para el nuevo mensaje y sustituir el CRC real en el final del paquete y esta modificación no podrá ser detectada.

La CRC sirve para verificar la integridad, pero no para saber si el mensaje es correcto.

Por el contrario, un medio eficaz para proteger a los mensajes contra la manipulación intencional es el uso de un código de autenticación de mensajes como .

La mecánica de la informática con su lenguaje binario produce unas CRC simples.
Los bits representados de entrada son alineados en una fila, y el ("n" + 1) representa el patrón de bits del divisor CRC (llamado polinomio) se coloca debajo de la parte izquierda del final de la fila. Aquí está la primera de ellas para el cálculo de 3 bits de CRC:
Si la entrada que está por encima del extremo izquierdo del divisor es 0, no se hace nada y se pasa el divisor a la derecha de uno en uno.
Si la entrada que está por encima de la izquierda del divisor es 1, el divisor es Or exclusiva en la entrada (en otras palabras, por encima de la entrada de cada bit el primer bit conmuta con el divisor).
El divisor es entonces desplazado hacia la derecha, y el proceso se repite hasta que el divisor llega a la derecha, en la parte final de la fila de entrada. Aquí está el último cálculo:
Desde la izquierda se divide por cero todos los bits de entrada, cuando este proceso termina el único bits en la fila de entrada que puede ser distinto de cero es n bits más a la derecha, en la parte final de la fila.
Estos "n" bits son el resto de la división, y será también el valor de la función CRC (es el CRC escogido a menos que la especificación de algún proceso posterior lo cambie).

Este apartado se refiere al análisis matemático de este proceso de división, como pone de manifiesto la manera de elegir un divisor que garantiza la detección de errores buenas propiedades.
En este análisis, los dígitos de las cadenas de bits son considerados como los coeficientes de un polinomio en algunos variables x/coeficientes, que son elementos del campo finito binario en lugar de los números decimales.
Este «polinomio» forma unas cadenas de bits que pueden observarse como elementos de un anillo.
Un anillo es, hablando en términos generales, un conjunto de elementos. Es decir, como los números que pueden ser operados por una operación semejante pero no idéntica a la de la suma y además alguna otra operación semejante a la multiplicación. Estas operaciones poseen muchas de las propiedades de la aritmética: conmutativa, asociativa, y distributiva.

El CRC se utiliza como una detección de errores de código, el cual tiene una serie de aplicaciones usadas cuando se implementa mediante normas, convirtiéndolo así en un sistema práctico.

Estas son algunas de las aplicaciones:








</doc>
<doc id="650" url="https://es.wikipedia.org/wiki?curid=650" title="Cliente">
Cliente

El término clientes de contabilidad puede referirse:


También puede referirse:


</doc>
<doc id="651" url="https://es.wikipedia.org/wiki?curid=651" title="Cometa">
Cometa

Los cometas son los cuerpos celestes constituidos por hielo, polvo y rocas que orbitan alrededor del Sol siguiendo diferentes trayectorias elípticas, parabólicas o hiperbólicas. Los cometas, junto con los asteroides, planetas y satélites, forman parte del sistema solar. La mayoría de estos cuerpos celestes describen órbitas elípticas de gran excentricidad, lo que produce su acercamiento al Sol con un período considerable. A diferencia de los asteroides, los cometas son cuerpos sólidos compuestos de materiales que se subliman en las cercanías del Sol. A gran distancia (a partir de 5-10UA) desarrollan una atmósfera que envuelve al núcleo, llamada "coma" o cabellera, que está formada por gas y polvo. A medida que el cometa se acerca al Sol, el viento solar azota la coma y se genera la "cola" característica, la cual está formada por polvo y el gas de la coma ionizado.

Fue después del invento del telescopio cuando los astrónomos comenzaron a estudiar a los cometas con más detalle, advirtiendo entonces que la mayoría tienen apariciones periódicas. Edmund Halley fue el primero en darse cuenta de ello y pronosticó en 1705 la aparición del cometa Halley en 1758, para el cual calculó que tenía un periodo de 76 años, aunque murió antes de comprobar su predicción. Debido a su pequeño tamaño y órbita muy alargada, solo es posible ver los cometas cuando están cerca del Sol y por un corto periodo de tiempo.

Los cometas son generalmente descubiertos de manera visual o usando telescopios de campo ancho u otros medios de magnificación espacial óptica, tales como los binoculares. Sin embargo, aún sin acceso a un equipo óptico, es posible descubrir un cometa rasante solar en línea si se dispone de una computadora y conexión a Internet. En los años recientes, el Observatorio Rasante Virtual de David (David J. Evans) (DVSO) ha permitido a muchos astrónomos aficionados de todo el mundo descubrir nuevos cometas en línea (frecuentemente en tiempo real) usando las últimas imágenes del Telescopio Espacial SOHO. Un caso reciente (28 de noviembre de 2013) de un cometa rasante del Sol que resultó volatilizado al aproximarse al Sol ha sido ISON que procedía probablemente de la nube de Oort. Las órbitas periódicas tienen forma de elipses muy excéntricas.

La palabra proviene del vocablo latín "comēta" y este a su vez del vocablo griego "κομήτης", cuyo significado es «cabellera».</ref> Sin embargo, algunos cometas pueden tener un mayor contenido de polvo, lo que hace que se llamen "bolas de tierra heladas" .

Los cometas provienen principalmente de dos lugares, la nube de Oort, situada entre 50000 y del Sol, y el cinturón de Kuiper, localizado más allá de la órbita de Neptuno.

Se cree que los cometas de largo periodo tienen su origen en la nube de Oort, que lleva el nombre del astrónomo Jan Hendrik Oort. Esta nube consiste de restos de la condensación de la nébula solar. Esto significa que muchos de los cometas que se acercan al Sol siguen órbitas elípticas tan alargadas que solo regresan al cabo de miles de años. Cuando alguna estrella pasa muy cerca del sistema solar, las órbitas de los cometas de la nube de Oort se ven perturbadas: algunos salen despedidos fuera del sistema solar, pero otros acortan sus órbitas. Para explicar el origen de los cometas de corto periodo, como el Halley, Gerard Kuiper propuso la existencia de un cinturón de cometas situados más allá de Neptuno, el cinturón de Kuiper.

Las órbitas de los cometas están cambiando constantemente: sus orígenes están en el sistema solar exterior y tienen la propensión a ser altamente afectados (o perturbados) por acercamientos relativos a los planetas mayores. Algunos son movidos a órbitas muy cercanas al Sol y se destruyen cuando se aproximan, mientras que otros son enviados fuera del sistema solar para siempre.

Si su órbita es elíptica y de período largo o muy largo, provienen de la hipotética nube de Oort, pero si su órbita es de período corto o medio-corto, provienen del cinturón de Edgeworth-Kuiper, a pesar de que hay excepciones como la del Halley, con un período de 76 años (corto), que proviene de la nube de Oort.

Conforme los cometas van sublimando, acercándose al Sol y cumpliendo órbitas, van sublimando su material, y perdiéndolo por consecuencia, disminuyendo de magnitud. Tras un cierto número de órbitas, el cometa se habrá "apagado", y cuando se acaben los últimos materiales volátiles, se convertirá en un asteroide normal y corriente, ya que no podrá volver a recuperar masa. Ejemplos de cometas sin materiales volátiles son: 7968-Elst-Pizarro y 3553-Don Quixote.

Los cometas llegan a tener diámetros de algunas decenas de kilómetros y están compuestos de agua, hielo seco, amoníaco, metano, hierro, magnesio, sodio y silicatos. Debido a las bajas temperaturas de los lugares donde se hallan, estas sustancias se encuentran congeladas. Algunas investigaciones apuntan a que los materiales que componen los cometas son materia orgánica y resultan determinantes para la vida, lo que daría lugar a que en la temprana formación de los planetas impactaran contra la tierra y dieran origen a los seres vivos.

Cuando se descubre un cometa se lo ve aparecer como un punto luminoso, con un movimiento perceptible sobre el fondo de estrellas llamadas fijas. Lo primero que se ve es el núcleo o coma; luego, cuando el astro se acerca más al Sol, comienza a desarrollar lo que conocemos como la cola del cometa, que le confiere un aspecto fantástico.

Al acercarse al Sol, el núcleo se calienta y el hielo sublima, pasando directamente al estado gaseoso. Los gases del cometa se proyectan hacia atrás, lo que motiva la formación de la cola apuntando en dirección opuesta al Sol y extendiéndose millones de kilómetros.

Los cometas presentan diferentes tipos de colas. Las más comunes son la de polvo y la de gas. La cola de gas se dirige siempre en el sentido perfectamente contrario al de la luz del Sol, mientras que la cola de polvo retiene parte de la inercia orbital, alineándose entre la cola principal y la trayectoria del cometa. El choque de los fotones que recibe el cometa como una lluvia, aparte de calor, aportan luz, que es visible al ejercer el cometa de pantalla, reflejando así cada partícula de polvo la luz solar. En el cometa Hale-Bopp se descubrió un tercer tipo de cola compuesta por iones de sodio.
Las colas de los cometas llegan a extenderse de forma considerable, alcanzando millones de kilómetros. En el caso del cometa 1P/Halley, en su aparición de 1910, la cola llegó a medir cerca de 30 millones de kilómetros, un quinto de la distancia de la Tierra al Sol. Cada vez que un cometa pasa cerca del Sol se desgasta, debido a que el material que va perdiendo nunca es repuesto. Se espera que, en promedio, un cometa pase unas dos mil veces cerca del Sol antes de sublimarse completamente. A lo largo de la trayectoria de un cometa, este va dejando grandes cantidades de pequeños fragmentos de material; cuando casi todo el hielo volátil ha sido expulsado y ya no le queda suficiente para tener coma, se dice que es un cometa extinto.

Cuando la Tierra atraviesa la órbita de un cometa, estos fragmentos penetran en la atmósfera en forma de estrellas fugaces o también llamadas lluvia de meteoros. En mayo y octubre se pueden observar las lluvias de meteoros producidas por el material del cometa Halley: las Eta Acuáridas y las Oriónidas.

Los astrónomos sugieren que los cometas retienen, en forma de hielo y polvo, la composición de la nebulosa primitiva con que se formó el Sistema Solar y de la cual se condensaron luego los planetas y sus lunas. Por esta razón el estudio de los cometas puede dar indicios de las características de aquella nube primordial.

Hasta el siglo XVI, periodo en que Tycho Brahe realizó estudios que revelaron que los cometas debían provenir de fuera de la atmósfera terrestre, no se estableció definidamente si eran fenómenos atmosféricos u objetos interplanetarios. Luego, en el siglo XVII, Edmund Halley utilizó la teoría de la gravitación, desarrollada por Isaac Newton, para intentar calcular el número de órbitas de los cometas, descubriendo que uno de ellos volvía a las cercanías del Sol cada 76 o 77 años aproximadamente. Este cometa fue denominado cometa Halley y de fuentes antiguas se sabe que ha sido observado por humanos desde el año 

El segundo cometa al que se le descubrió una órbita periódica fue el cometa Encke, en 1821. Como el cometa de Halley, tuvo el nombre de su calculador, el matemático y físico alemán Johann Encke, que descubrió que era un cometa periódico. El cometa de Encke tiene el más corto periodo de un cometa, solamente 3,3 años, y en consecuencia tiene el mayor número de apariciones registradas. Fue también el primer cometa cuya órbita era influida por fuerzas que no eran del tipo gravitacional. Ahora es un cometa muy tenue para ser observado a simple vista, aunque pudo haber sido un cometa brillante hace algunos miles de años, antes que su superficie de hielo fuera evaporada. Sin embargo, no se sabe si ha sido observado antes de 1786, pero análisis mejorados de su órbita temprana sugieren que se corresponde con observaciones mencionadas en fuentes antiguas.

La composición de los cometas no fue probada hasta el periodo de la era espacial. A principios del siglo XIX, un matemático alemán, Friedrich Bessel originó la teoría de que había objetos sólidos en estado de vaporación: del estudio de su brillo, Bessel expuso que los movimientos no-gravitacionales del cometa Encke fueron causados por fuerzas de chorro creadas como material evaporado de la superficie del objeto. Esta idea fue olvidada por más de cien años, y luego, independientemente, Fred Lawrence Whipple propuso la misma idea en 1950. Para Whipple un cometa es un núcleo rocoso mezclado con hielo y gases, es decir, utilizando su terminología, "una bola de nieve sucia". El modelo propuesto por ambos pronto comenzó a ser aceptado por la comunidad científica y fue confirmado cuando una flota de vehículos espaciales voló a través de la nube luminosa de partículas que rodeaban el núcleo congelado del cometa Halley en 1986 para fotografiar el núcleo y se observaron los chorros de material que se evaporaba. Luego, la sonda Deep Space 1 voló cerca del cometa Borrelly el 21 de septiembre de 2001, confirmando que las características del Halley son también comunes a otros cometas.

Históricamente los cometas se han clasificado en función de su periodo orbital, de periodo corto o de periodo largo en función de si su periodo es menor o mayor de 200 años respectivamente. La razón de este criterio es que antiguamente cuando la capacidad de cálculo era más limitada, ese era el periodo orbital máximo para el que era relativamente fiable averiguar si ya había sido avistado con anterioridad o no y, por tanto, establecer su periodicidad.

Son los que su periodo orbital es menor de 200 años. Adicionalmente los cometas de periodo corto se agrupan en dos categorías:


Se ha propuesto utilizar el parámetro de Tisserand respecto Júpiter, (T), que en el fondo es una medida de la influencia gravitatoria de dicho planeta sobre otro cuerpo, para clasificar los cometas ya que permanece prácticamente invariante durante la evolución dinámica de los mismos e indicaría asimismo una historia evolutiva diferente para cada grupo de cuerpos definido de este modo. Además se ha demostrado que al menos el 10% de los cometas de la Familia de Júpiter evolucionarán a periodos de entre 20 y 200 años dentro de los próximos 400 años, es decir, pasarán de estar clasificados como de la Familia de Júpiter a la de Tipo Halley. En cambio el 92% de los cometas de periodo corto se mantienen dentro de su clasificación cuando se les clasifica por el parámetro de Tisserand: 


Con esta clasificación hay tres cometas, 126P/IRAS, P96/Machholz y 8P/Tuttle que pasarían de ser considerados de la Familia de Júpiter a ser de Tipo Halley a pesar de sus reducidos periodos orbitales.

En 1994 la IAU estableció una serie de criterios para la designación de los cometas para ser empleados a partir de 1995.

A partir de ese momento, los cometas se designan provisionalmente por su año de descubrimiento seguido de una codificación alfanumérica en la que el primer elemento es una letra mayúscula que designa la quincena del mes en que fue descubierto, desde A-primera quincena del mes de enero hasta Y-segunda quincena del mes de diciembre, y un número que indica el orden en que fue descubierto dentro de esa quincena.

La clasificación del cometa se indica mediante prefijos:

Una vez establecida la periodicidad de un cometa, se le asigna un número de orden secuencial que precede a la clasificación. El Minor Planet Center es el encargado del mantenimiento de la lista de cometas peródicos numerados.

Ya desde finales del siglo XIX se tiene la costumbre de denominar a los cometas con los nombres de sus descubridores. Tal costumbre se mantiene y la IUA únicamente ha redactado reglas más claras a la hora de dar el crédito de los descubrimientos, dado también que han proliferado proyectos de búsqueda sistemática de objetos que han dado lugar a un gran número de descubrimientos. Los nombres de los descubridores se añaden a la designación sistemática entre paréntesis y en un número máximo de dos, excepcionalmente tres. Generalmente, solo en el caso de que los cometas estén numerados, se utiliza únicamente la denominación en lugar de la designación sistemática puesto que en ese caso ya no puede haber ambigüedades, aunque no obstante se recomienda el uso de esa designación. Así por ejemplo, el cometa Halley puede designarse como 1P/ 1682 U1 (Halley) o 1P/Halley o incluso puede abreviarse como 1P.

Algunos de los cometas más famosos:

Los cometas han llamado la atención de los hombres de todas las civilizaciones. Generalmente eran considerados un mal augurio. Se ha relacionado la súbita aparición de cometas con hechos históricos, como batallas, nacimientos (véase Jesucristo) o muertes. Estas creencias perduran hasta nuestros días, aunque tienen mucho menos predicamento que en la antigüedad.

En la antigüedad, su aparición venía acompañada de malos presagios. Los astrólogos le atribuían el augurio de muerte inminente de algún rey o emperador. Pero lo cierto es que, si bien este tipo de creencias ha sido superado por la mayoría de las personas, existe todavía el temor de un posible impacto sobre la superficie de la Tierra de efectos apocalípticos.




</doc>
<doc id="655" url="https://es.wikipedia.org/wiki?curid=655" title="Código unívocamente descodificable">
Código unívocamente descodificable

Un código unívocamente descodificable es un tipo de código no-singular si cualquier secuencia finita de signos del alfabeto usado por el código es la imagen de, a lo sumo, un 
mensaje, es decir, la función de codificación "E" es una función inyectiva.
Código cuya extensión es no-singular. Sea A un alfabeto fuente y B un alfabeto código. Se llama función codificadora a cualquier función. f: A+ -> B+. El código correspondiente es Unívocamente Decodificable (UD) si f es inyectiva. Hace parte del área de la matemática discreta y los algoritmos computacionales.

Una forma de calcular la mejor longitud media es mediante la 
Inecuación de Kraft. La idea básica es asignar longitudes mayores a 
las palabras con menor probabilidad.

Para aclarar todo esto, debemos ir por pasos:



</doc>
<doc id="657" url="https://es.wikipedia.org/wiki?curid=657" title="Circuito de conmutación">
Circuito de conmutación

En electricidad y electrónica, las leyes del álgebra de Boole y de la lógica binaria, pueden estudiarse mediante circuitos de conmutación. Un circuito de conmutación estará compuesto por una serie de contactos que representarán las variables lógicas de entrada y una o varias cargas que representarán las variables lógicas o funciones de salida.
Los contactos pueden ser normalmente abiertos (NA) o normalmente cerrados (NC). Los primeros permanecerán abiertos mientras no se actúe sobre ellos (por ejemplo al pulsar sobre interruptor, saturar un transistor, etc.). Los contactos NC funcionarán justamente al contrario. Esto significa que si se actúa sobre un contacto NA se cerrará y si se hace sobre uno NC se abrirá.

Los circuitos de conmutación se basan en interruptores que permiten o no la circulación de una corriente eléctrica, estos interruptores pueden ser manuales si se actúan directamente, como un interruptor de la luz, por ejemplo; eléctricos: relés o contactores, si su actuación es electro-mecánica, o electrónicos, transistores o puertas lógicas, si se basan en la tecnología electrónica.

Por sencillez, representaremos un interruptor o conmutador por sus contactos eléctricos, si un interruptor conecta dos puntos a y b, diremos que está abierto si no permite la circulación eléctrica entre esos dos puntos: a y b. Diremos que está cerrado si permite la circulación eléctrica entre esos dos puntos.

Un interruptor diremos que esta normalmente abierto (NA) si cuando no se actúa sobre él está abierto, a la posición normal también se le denomina posición de reposo, que el interruptor tendrá normalmente por la actuación de un muelle o resorte que lo lleva a esa posición.

Cuando se actúa sobre un interruptor normalmente abierto (NA), el interruptor se cierra, permitiendo la circulación eléctrica a su través.

Venciendo la fuerza ejercida por el muelle o resorte, y dando lugar al contacto eléctrico entre sus terminales.

En la figura se representa un pulsador normalmente abierto, en reposo en la parte superior, con el muelle en reposo y sus contactos separados, en la parte inferior se ve ese mismo pulsador actuado, con el muelle comprimido y sus terminales eléctricos en contacto, permitiendo la circulación eléctrica entre los puntos a y b.

Si entre dos puntos a y b, colocamos un interruptor normalmente cerrado (NC), que cuando no se actúa sobre él está cerrado, en este caso, la relajación del muelle o resorte da lugar a poner en contacto los terminales eléctricos del interruptor, permitiendo la circulación eléctrica a su través, el interruptor está cerrado. Si actuamos sobre él venciendo la acción del muelle, separando los contactos, el interruptor se abre, no permitiendo la circulación eléctrica. En estos interruptores el resultado es el contrario de la acción, si actuamos sobre el interruptor el interruptor se abre, cortando el paso de la corriente eléctrica, si no actuamos sobre el, se cierra permitiendo la circulación eléctrica.

Como se ha visto, los interruptores pueden ser actuados manualmente, o mecánicamente mediante fines de carrera, presostatos u otros elementos que partiendo de una acción exterior den lugar a una conexión o desconexión eléctrica.

Pero un circuito puede actuar sobre otro circuito, mediante relés o contactores, de modo que podemos disponer de un circuito de conmutación, cuyo resultado es la actuación sobre otro circuito, en estos casos la presencia o no de una corriente eléctrica da lugar a la modificación del estado de un interruptor, que pasara de su posición de reposo a la de actuado.

En la figura podemos ver, una serie de interruptores de este tipo. La actuación sobre ellos se hace mediante un solenoide, que genera un campo magnético y que desplaza el núcleo ferromagnético de la armadura, venciendo al muelle, y cambiando los contactos eléctricos. Cuando la corriente eléctrica no actúa, el muelle eleva al interruptor a la posición de reposo.

En un circuito de conmutación se realiza un análisis de la lógica del circuito, haciendo abstracción de los detalles de funcionamiento de los mecanismos que intervienen, así como del dimensionado de los aparatos y resto del circuito para las intensidades de corriente y diferencia de potencial con los que trabaja, prestando atención prioritaria a la lógica de la conmutación, por ello no son necesarios, algunos de los detalles eléctricos, propios de los circuitos eléctricos, y si es necesario determinar un convenio de representación de los circuitos que impida errores en su interpretación, teniendo en cuenta lo siguiente:

Se deberán de tener en cuenta los siguientes convenios (ver Figura 1): 

Un interruptor múltiple, es el que con solo un mando mueve varios contactos simultáneamente, este tipo de interruptor, no tan sencillo, se emplea para conmutar varios circuitos al mismo tiempo, electivamente separados.

Este tipo de interruptor puede tener contactos directos e inversos, en la figura los dos primeros son directos y el tercero inverso, que a su vez pueden ser de distinta sección, según la intensidad de corriente que circule por cada uno de ellos.

Un relé o Contactor, es un interruptor automático controlado eléctricamente, de este modo una señal eléctrica da lugar a nuevos contactos que, a su vez, alimentan o dejan de alimentar otros circuitos.

En la figura, se puede ver la representación esquemática de un relé. Los contactos se representan en reposo, en la posición que tendrían cuando la bobina no está alimentada; cuando recibe tensión, la armadura se desplaza, cambiando la posición de los contactos.

De este modo la Figura 3 representa la función lógica Y (AND), esto es, L=a·b· ... ·n. De acuerdo con la tabla de verdad de dicha función, El circuito está cerrado solo si cada uno de los interruptores que intervienen está cerrados.
Del mismo modo la Figura 4 representa la función lógica O (OR), esto es, L= a+b+ ... +n; y de acuerdo con su tabla de verdad, el circuito está cerrado si al menos uno de los interruptores está cerrado.
El conmutador está formado por un interruptor directo y otro inverso, ver Figura 5, que actúan conjuntamente, de modo que con una sola actuación se aísla un circuito y se conecta otro, conmutando los dos circuitos.

En la figura puede verse que la conexión de la izquierda está conectada con la inferior de la derecha cuando a no está actuado.

Si a esta actuada la salida es por la conexión superior de la derecha.

Dos conmutadores conectados según la Figura 6, da como resultado un circuito, que está abierto o cerrado alternativamente, con tan solo modificar uno de los dos conmutadores, si los dos están en la misma posición el circuito está conectado, si se modifica uno cualesquiera de los dos, se desconecta, que volverá a conectarse al actuar sobre uno de ellos, sin importar cual. este circuito se utiliza comúnmente para el encendido de luces en escaleras o la operación desde dos puntos distintos. también es llamada three way o tres vías.

Un interruptor de cruce permuta las dos líneas de entrada (a, b) con las dos de salida (c, d), en las figuras 7 y 8, se pueden ver dos esquemas equivalentes de este tipo de interruptor.

En una posición se conecta a con c y b con d y en la otra se permutan conectándose a con d y c con b.

En estas dos figuras se puede apreciar perfectamente, que distintas distribuciones de los aparatos y distintos cableados pueden dar lugar a los mismos resultados.

La construcción de un Oscilador, con medios exclusivamente electromecánicos, se hace sencillamente, conectando la bobina de un relé a uno de sus contactos normalmente conectados (NC), cuando el relé se excita, el contacto (NC) se desconecta, desconectando la bobina, que da lugar a que el contacto (NC) entre en contacto de nuevo.

Este es el mecanismo en el que se basa el timbre eléctrico clásico.

En teoria de autómatas un sistema combinacional es un sistema lógico basado en el álgebra de Boole, tanto en tecnología electrónica como electromecánica. Podemos ver una implementación en conmutadores, de sistemas combinacionales, ordenando por el número de variables que intervienen.

Partiendo de un número n de variables, cada una de las cuales puede tomar el valor verdadero: 1, o falso: 0, por combinatoria, podemos saber que el número total de combinaciones: C, que se pueden presentar es:

el número de combinaciones que se pueden dar con n variable, cada una de las cuales puede tomar uno entre dos valores lógicos es de dos elevado a n, esto es, el número de combinaciones: C, tiene crecimiento exponencial respecto al número de variable n:

Si consideramos que un "sistema combinacional" de n variables binarias, puede presentar un resultado verdadero: 1, o falso: 0, para cada una de las posibles combinaciones de entrada tenemos que se pueden construir un número de funciones: F con n variables de entrada, donde:

Que da como resultado la siguiente tabla:

Para componer una tabla de verdad, pondremos las n variables en una línea horizontal, debajo de estas variables desarrollamos las distintas combinaciones que se pueden formar con 1 y 0, dando lugar a las distintas C, número de combinaciones. Normalmente solo se representa la función para la que se confecciona la tabla de verdad, y en todo caso funciones parciales que ayuden en su cálculo, en la figura, se pueden ver todas las funciones posibles F, que pueden darse para el número de variables dado.

Así podemos ver que para dos variables binarias: a y b, n= 2 , que pueden tomar los valores 1 y 0, se pueden desarrollar cuatro combinaciones: C= 4, con estos valores se pueden definir dieciséis resultados distintos, F= 16, cada una de las cuales seria una función de dos variables binarias. Para otro número de variables se obtendrán los resultados correspondientes, dado el crecimiento exponencial de F, cuando n toma valores mayores de cuatro o cinco, la representación en un cuadro resulta compleja, y si se quiere representar las combinaciones posibles F, resulta ya complejo para n= 3.

Esta función se denomina tautología y presenta el resultado verdadero (1) en todos los casos.

Esta función se representa:

Estas funciones son equivalentes:

Esta función se denomina contradicción y presenta el resultado falso(0) en todos los casos.

Esta función se representa:

Estas funciones son equivalentes:

Esta función se denomina tautología y presenta el resultado verdadero (1) en todos los casos.

Esta función se representa:

Estas funciones son equivalentes:

Esta función se denomina: afirmación lógica, asigna a la función el mismo valor lógico que tiene la variable.

Esta función se representa:

Esta función se denomina: negación lógica, asigna a la función el valor lógico opuesto al que tiene la variable.

Esta función se representa:

Esta función se denomina contradicción y presenta el resultado falso(0) en todos los casos.

Esta función se representa:

Estas funciones son equivalentes:

Esta función se una tautología y presenta el resultado verdadero(1) en todos los casos.

Esta función se representa:

Estas funciones son equivalentes:

Esta función se denomina: Disyunción lógica, y presenta el resultado falso(0) solo cuando las dos variablea: a, b; son falsos(0).

Esta función se representa:

Esta función se representa:

Esta función se representa:

Estas funciones son equivalentes:

Esta función se representa:

Esta función se representa:

Estas funciones son equivalentes:

Esta función se representa:

Esta función se representa:

Esta función se representa:

Esta función se representa:

Esta función se representa:

Estas funciones son equivalentes:

Esta función se representa:

Esta función se representa:

Estas funciones son equivalentes:

Esta función se representa:

Esta función se representa:

Esta función se representa:

Estas funciones son equivalentes:




</doc>
<doc id="660" url="https://es.wikipedia.org/wiki?curid=660" title="Claude Elwood Shannon">
Claude Elwood Shannon

Claude Elwood Shannon (30 de abril de 1916 - 24 de febrero de 2001) fue un matemático, ingeniero eléctrico y criptógrafo estadounidense recordado como «el padre de la teoría de la información».

Shannon es reconocido por haber fundado el campo de la teoría de la información con la publicación "Una teoría matemática de la comunicación", que supuso un hito en 1948. Es quizás igualmente conocido por haber sentado las bases de la teoría del diseño de circuitos digitales en 1937, con apenas 21 años de edad. Mientras realizaba su maestría en el Massachusetts Institute of Technology (MIT), demostró en su tesis que las aplicaciones electrónicas de álgebra booleana podrían construir cualquier relación lógico-numérica. Shannon contribuyó asimismo al campo del criptoanálisis para la defensa de Estados Unidos durante la Segunda Guerra Mundial, con trabajos sobre el descifrado de códigos y la seguridad en las telecomunicaciones.

Los primeros años de su vida los pasó en Gaylord, donde se graduó de la secundaria en 1932. Desde joven, Shannon mostró una clara inclinación hacia las cosas mecánicas. Resaltaba respecto a sus compañeros en las asignaturas de ciencias. Su héroe de la niñez era Edison, con quien luego descubrió que tenía un parentesco y a cuyas investigaciones se aproximó bastante.

En 1932 ingresó en la Universidad de Míchigan, donde su hermana Catherine se doctoró como matemática. En 1936 obtuvo los títulos de ingeniero electricista y matemático. Su interés por la matemática y la ingeniería continuó durante toda su vida.

En 1936 aceptó el puesto de asistente de investigación en el departamento de ingeniería eléctrica en el Instituto Tecnológico de Massachusetts (MIT). Su situación le permitió continuar estudiando mientras trabajaba por horas para el departamento, donde trabajó en el computador analógico más avanzado de esa era, el analizador diferencial de Vannevar Bush.

En ese momento surgió su interés hacia los circuitos de relés complejos. Intentando simplificar centralitas telefónicas de relés, se dio cuenta de que estos podían usarse para hacer cálculos. Sumado esto a su gusto por la lógica y el álgebra booleana, pudo desarrollar esta idea durante el verano de 1937, que pasó en los laboratorios Bell en la ciudad de Nueva York.

En su tesis doctoral en el MIT demostró cómo el álgebra booleana se podía utilizar en el análisis y la síntesis de la conmutación y de los circuitos digitales. La tesis despertó un interés considerable cuando apareció en 1938 en las publicaciones especializadas. En 1940 le fue concedido el Premio Aldred Noble para ingenieros estadounidenses por parte de la Sociedad Estadounidense de Ingenieros Civiles de los Estados Unidos, otorgado cada año a una persona de no más de treinta años. Un cuarto de siglo más tarde, Herman Goldstine, en su libro "Las computadoras desde Pascal hasta Von Neumann", citó su tesis como una de las más importantes de la historia que ayudó a cambiar el diseño de circuitos digitales.

Durante el verano de 1938 realizó trabajos de investigación en el MIT y le fue concedida la beca Bolles cuando trabajaba como ayudante de enseñanza mientras realizaba un doctorado en matemática. En 1940 estudió un máster en ingeniería eléctrica y se doctoró en filosofía de la matemática.

Shannon pasó quince años en los laboratorios Bell, una asociación muy fructífera con muchos matemáticos y científicos de primera línea como Harry Nyquist, Walter Houser Brattain, John Bardeen y William Bradford Shockley, inventores del transistor; George Stibitz, quien construyó computadoras basadas en relés; Warren Weaver, quien escribió una extensa y aclaradora introducción a su obra "Una teoría matemática de la comunicación" y muchos otros más.

Durante este período Shannon trabajó en muchas áreas, y lo más notable fue todo lo referente a la teoría de la información, que se publicó en 1948 con el nombre de "Una teoría matemática de la comunicación". En este trabajo se demostró que todas las fuentes de información (telégrafo eléctrico, teléfono, radio, la gente que habla, las cámaras de televisión, etcétera) pueden medirse, y que los canales de comunicación tienen una unidad de medida similar, determinando la velocidad máxima de transferencia o capacidad de canal. Demostró también que la información se puede transmitir sobre un canal si y solamente si la magnitud de la fuente no excede la capacidad de transmisión del canal que la conduce, y sentó las bases para la corrección de errores, supresión de ruidos y redundancia.

En el área de las computadoras y de la inteligencia artificial, publicó en 1949 un trabajo que describía la programación de una computadora para jugar al ajedrez, convirtiéndose en la base de posteriores desarrollos.

En el campo de la biblioteconomía y la documentación, el desarrollo booleano revolucionó las búsquedas en catálogos de bibliotecas o en bases de datos de centros de documentación.

A lo largo de su vida recibió numerosas condecoraciones y reconocimientos de universidades e instituciones de todo el mundo.

Ante la pregunta de un periodista de si las máquinas podían pensar, replicó: «¡Naturalmente! ¡Usted y yo somos máquinas y vaya si pensamos!».




</doc>
<doc id="663" url="https://es.wikipedia.org/wiki?curid=663" title="Cibercafé">
Cibercafé

Un cibercafé (de "ciber-" y "café"), ciber café, café internet o solo ciber, es un local público donde se ofrece a los clientes acceso a internet y, aunque no en todos, también servicios de bar, restaurante o cafetería. Para ello, el local dispone de computadoras y usualmente cobra una tarifa fija por un período determinado para el uso de dichos equipos, incluido el acceso a Internet y a diversos programas, tales como procesadores de texto, programas de edición gráfica, copia de CD o DVD, etc. También, hay algunos cibercafés que no necesariamente venden alguno de esos productos de cafetería. Los cibercafés han contribuido de forma considerable a la masificación de Internet, especialmente en comunidades de bajo poder adquisitivo. Fueron muy populares a fines de los años noventa y a lo largo de la década de los 2000.

Generalmente cuando se ofrece sólo acceso a Internet a través de equipos informáticos a tal efecto se les conoce como centro de navegación o sala de navegación. Informalmente también se los conoce como "locutorio" (en Argentina), "telecentro" (en República Dominicana), "centro de internet " (en Colombia, pronunciado "/síber/") "cyber" (en Venezuela, pronunciado "/sáiber/") y Cyber (en Ecuador).

Los primeros cibercafés fueron abiertos en Londres (Inglaterra) en 1994. El primer café fue el Café Cyberia, que abrió sus puertas en septiembre de 1994. Su fundadora, Eva Pascoe, dice que la idea se le vino a la cabeza a principios de los años 1990, cuando pasaba mucho tiempo lejos de su familia trabajando en su tesis doctoral. En esos tiempos, era de las pocas personas que tenía acceso a una cuenta de correo electrónico, servicio puramente académico por aquellos días; pero al no tener nadie más en su familia una dirección de correo electrónico, debía gastar cantidades considerables de dinero en cuentas telefónicas.

Un día, sentada en un café cerca de su colegio, pensó que podría ser divertido poder ir a ese establecimiento con su ordenador portátil y enviar correos mientras se tomaba un descanso en su rutina habitual. Echó un vistazo alrededor y pudo reconocer algunos amigos de los que sabía que tenían conexión a Internet desde sus casas. Después, hablando con ellos, pensaron en cómo sería tener conexión permanente a Internet desde un café y pagar una pequeña tarifa para poder intercambiar mensajes con sus amigos y familiares, enviar correo y tener mensajería instantánea. Tres meses después, en septiembre, abrieron el primer café Internet en Londres. Desde ese momento hasta hoy, los cibercafés se han multiplicado por todo el mundo.

Ya entrada la primera década del siglo XXI, gracias a la masificación de Internet en muchos países de la que fueron partícipes, los cibercafés se convirtieron en negocios muy populares. Sin embargo, la sobreoferta, la competencia desleal y la caída de los precios han hecho que el sector haya sufrido una importante crisis, razón por la cual muchos negocios se han visto abocados al cierre.

Otra circunstancia que ha puesto en la mira de las autoridades a los cafés Internet es el aumento de delitos informáticos realizados desde estos sitios de acceso público, donde se dificulta el control del usuario. Además, con el acceso libre a páginas pornográficas sin tener filtros adecuados para menores de edad, se ha impuesto la moda de las cabinas privadas lo cual propicia aún más el acceso a estas páginas.

En algunos países como Turquía, Cuba, China o Arabia Saudita, los cafés Internet están duramente regulados para prevenir que sus ciudadanos se comuniquen libremente y accedan a información del exterior sin restricciones. Se obliga a la aplicación de estrictos filtros y se realizan controles de censura gubernamental.

La masificación del servicio de Internet residencial ha mermado la existencia de este tipo de locales, provocando su paulatina disminución.

Según el local y la clientela que lleva asociada, pueden distinguirse varios tipos de cibercafés. Cada uno de ellos requiere un equipo informático diferente, así como distinto nivel de mantenimiento y conocimientos para poder gestionarlo.

Este tipo de cibersala se caracteriza por una clientela joven que acude para jugar en red con otros jugadores (normalmente conocidos o amigos) o bien solos (jugar con la computadora). Piden ordenadores potentes, debido a las exigencias de los juegos modernos, y una mayor renovación de equipos, ya que constantemente salen al mercado nuevos juegos que necesitan mayor potencia o calidad gráfica. Permiten una innovación constante en las técnicas.

Disponen de computadoras para poder navegar por Internet en un ambiente generalmente tranquilo y relajado.

Los equipos no tienen que ser tan potentes como en el caso anterior, pero deben configurarse adecuadamente para impedir un mal uso de estos, como instalación de programas que puedan causar daños al equipo.

Se aconseja no conectar más de ocho cámaras web para el uso de videoconferencia. Si se desea una buena calidad, es necesario contratar ADSL de, al menos, 2 Mbps. Si excede el número de ocho personas utilizando videoconferencia, al ocupar gran parte del ancho de banda de subida todos los accesos a Internet y la carga de las páginas web se volverán lentos.

Este tipo de local orienta el negocio a un público universitario o a extranjeros.
Es el complemento ideal a los locutorios, pues ambos compiten por el mismo público objetivo.
Los ordenadores son más baratos y permite tener a cargo del cibercafé a una persona menos calificada.

Este tipo de cibersala es el que está sustituyendo a los salones recreativos, donde se reúnen los jóvenes, que suele ser el público que mayor desgaste hace en el material informático.

Es recomendable el máximo nivel de protección posible.

Debido a la masificación del servicio de Internet residencial, se ha producido una considerable merma de usuarios de este tipo de locales, lo cual hace que los mismos disminuyan al punto de casi desaparecer paulatinamente.

Hoy día, los mayores usuarios de los cibercafés son los jóvenes que suelen reunirse en ellos para acceder a videojuegos en red. También son frecuentados por gente que tiene controlado el acceso a Internet en su trabajo y no tiene alternativas de consulta en sus hogares, por estudiantes y padres que requieren asesoría en búsquedas de información, por viajeros que encuentran en los cibercafés la posibilidad de tener una vía para comunicarse con sus familiares y amigos y por cualquier otra persona que requiera acceso a Internet y sus servicios de forma rápida y barata.

En algunos países, los cibercafés reúnen comunidades de amigos los fines de semana por la noche, como alternativa de ocio a la ingesta de alcohol, pues sus horarios liberales se suelen extender hasta altas horas de la noche.

Actualmente, con el gran auge de las denominadas redes sociales, se ha venido incrementando el uso de estos establecimientos, incluso por aquellas personas que no hacían uso del Internet con anterioridad, aprovechando las herramientas que estas disponen, como el poder enviar y recibir imágenes, vídeos, servicio de chat, incluso recibir noticias generadas al momento.

Aunque en muchos países el concepto de café propiamente dicho no es la constante, todavía hay muchos de estos negocios que permiten la venta y consumo de comida y bebida, incluido el café. Tal vez el nombre de café viene asociado a los antiguos cafés, donde la gente se reunía para intercambiar opiniones con sus amigos y conocidos, y como tal es solo una evolución natural favorecida por la tecnología. Muchos cibercafés se llaman así aunque no vendan ningún alimento ni tampoco ninguna bebida.

Para la administración y el control de un cibercafé se puede encontrar "software" libre o comercial, lo cual permite gestionar, administrar el tiempo de uso, manejar stock de productos, bloquear equipos y manejar la contabilidad. Además permite llevar una base de datos de clientes de los que asisten con mayor frecuencia, al recibir beneficios por cada promoción y hacer mercadotecnia de fidelización.



</doc>
<doc id="666" url="https://es.wikipedia.org/wiki?curid=666" title="Calamovilfa">
Calamovilfa

Calamovilfa es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de Norteamérica. Comprende 6 especies descritas y de estas, solo 5 aceptadas.

El género fue descrito por (A.Gray) Hack. ex Scribn. & Southw. y publicado en "The True Grasses" 113. 1890. La especie tipo es: "Calamovilfa brevipilis"
El nombre del género deriva del griego "kalamos" ( caña) y "Vilfa", un sinónimo de un género de la familia.
Número de la base del cromosoma, x = 10. 2n = 40. 
A continuación se brinda un listado de las especies del género "Calamovilfa" aceptadas hasta noviembre de 2014, ordenadas alfabéticamente. Para cada una se indica el nombre binomial seguido del autor, abreviado según las convenciones y usos. 



</doc>
<doc id="667" url="https://es.wikipedia.org/wiki?curid=667" title="Calderonella sylvatica">
Calderonella sylvatica

Calderonella es un género monotípico de plantas herbáceas perteneciente a familia de las poáceas. Su única especie: Calderonella sylvatica Soderstr. & H.F.Decker, es originaria de Panamá y Colombia.
El nombre del género fue otorgado en honor de C.E.Calderón, botánica argentina.



</doc>
<doc id="671" url="https://es.wikipedia.org/wiki?curid=671" title="Cathestechum">
Cathestechum

Cathestechum es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de Norteamérica en Estados Unidos, México y el Caribe.
Número de la base del cromosoma, x = 10. 2n = 60. 



</doc>
<doc id="672" url="https://es.wikipedia.org/wiki?curid=672" title="Cenchrus">
Cenchrus

Cenchrus es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de las regiones templadas y tropicales del globo.
Son plantas anuales o perennes, cespitosas o rizomatosas; plantas polígamas. Lígula una membrana ciliolada; láminas lineares, aplanadas. Inflorescencia una espiga terminal en zigzag de cipselas erizadas espinosas o más o menos recta de fascículos cerdosos desarticulándose como una unidad; espinas y cerdas cilíndricas o aplanadas, en general retrorsamente escabrosas, a veces antrorsamente escabrosas; cipselas erizadas y los fascículos con 1–7 espiguillas retenidas adentro permanentemente, las espiguillas ocultas casi completamente en las cipselas erizadas o visibles en los fascículos; espiguillas comprimidas dorsalmente, con 2 flósculos; glumas membranáceas, la inferior corta, la superior más de la 1/2 de la longitud de la espiguilla; lema inferior tan larga como la espiguilla o más corta; flósculo inferior estéril o estaminado, flósculo superior rígido; lodículas ausentes o 2; anteras 3; estilos 1 o 2. Fruto una cariopsis; embrión 1/3–9/10 la longitud de la cariopsis; hilo punteado o elíptico.
Son especies de malezas: "C. biflorus, C. brownii, C. ciliaris, C. echinatus, C. incertus, C. longispinus, C. myosuroides, C. pauciflorus, C. tribuloides". Forrajes cultivados: "C. ciliaris" (resistentes a la sequía, tolerante al pastoreo intensivo), "C. setiger". Importantes especies de pastos nativos: "C. biflorus, C. ciliaris, C. pennisetiformis, C. setigerus". 

El género fue descrito por Carlos Linneo y publicado en "Species Plantarum" 2: 1049. 1753. La especie tipo es: "Cenchrus echinatus" L.
El nombre del género deriva del griego "kegchros" (mijo, "Panicum miliaceum"). 

El número cromosómico básico del género es x = 9 Y 12, con números cromosómicos somáticos de 2n = 34, 35, 36, 40, 44, 45 y 68, ya que hay especies diploides y una serie poliploide. Cromosomas relativamente «pequeños».





</doc>
<doc id="673" url="https://es.wikipedia.org/wiki?curid=673" title="Centotheca">
Centotheca

Centotheca es un género de plantas de la familia de las Poáceas. Es originario de África tropical, Asia y Polinesia.
El nombre del género deriva del griego "centein" (pinchar) y "theke" (caja), refiriéndose a los pelos espinosos dentro de la espiguilla (en los lemmas superiores).

El número cromosómico básico del género es x = 12, con números cromosómicos somáticos de 2n = 24. 2 ploide.



</doc>
<doc id="674" url="https://es.wikipedia.org/wiki?curid=674" title="Centrochloa sungularis">
Centrochloa sungularis

Centrochloa es un género monotípico de plantas de la familia de las Poáceas. Su única especie, Centrochloa sungularis Swallen, es originaria de Brasil. 
Es una planta anual; cespitosa. Con culmos de unos 20-75 cm de altura; herbácea; no ramificada arriba. Los nodos de los culmos glabros. Hojas en su mayoría basales; no auriculadas. Las láminas se estrechan; persistente. La lígula es una membrana con flecos. Plantas bisexuales, con espiguillas bisexuales; con flores hermafroditas. Inflorescencia de las ramas principales espigadas (el raquis filiforme); digitadas a subdigitadas. Raquis ahuecados. La inflorescencia espateada; no comprende inflorescencias parciales y de los órganos foliares. Espiguilla fértil ejes con raquis muy delgados; persistentes. 
"Centrochloa sungularis" fue descrita por Jason Richard Swallen y publicado en "Journal of the Washington Academy of Sciences" 25(4): 192, f. A. 1935.



</doc>
<doc id="675" url="https://es.wikipedia.org/wiki?curid=675" title="Cephalostachyum">
Cephalostachyum

Cephalostachyum, es un género de plantas herbáceas perteneciente a la familia de las poáceas.Es originario de Madagascar, India, China e Indochina. Su hábitat son de montaña hasta las tierras bajas de los bosques.


El número cromosómico básico del género es x = 12, con números cromosómicos somáticos de 2n = 72. 6 ploide.

Se han excluido de este género las siguientes especies:



</doc>
<doc id="677" url="https://es.wikipedia.org/wiki?curid=677" title="Chamaeraphis hordeacea">
Chamaeraphis hordeacea

Chamaeraphis es un género monotípico de plantas herbáceas de la familia de las poáceas. Su única especie, Chamaeraphis hordeacea, es originaria de Australia.
Es una planta perenne. Con culmos de 25-60 cm de largo. La lígula es una membrana ciliolada.
Inflorescencia compuesta de racimos que aparecen a lo largo de un eje central; en un falso pico bilateral; teniendo 1 espiguilla. Eje de la inflorescencia central aplanado. Raquis de hojas caducas; terminando en una extensión estéril; como una extensión de cerdas. Espiguillas solitarias y las fértiles sésiles. 

El género fue descrito por Robert Brown y publicado en "Prodromus Florae Novae Hollandiae" 1: 194. 1810. 



</doc>
<doc id="678" url="https://es.wikipedia.org/wiki?curid=678" title="Chikusichloa">
Chikusichloa

Chevalierella, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de China y Ryukyu en Japón.

El número cromosómico básico del género es x = 12, con números cromosómicos somáticos de 2n = 24. diploide.



</doc>
<doc id="679" url="https://es.wikipedia.org/wiki?curid=679" title="Chloris (planta)">
Chloris (planta)

Chloris es un género de plantas con flor de la familia de las poáceas. Es originario de las regiones templadas y tropicales del globo.
Son plantas anuales o perennes, cespitosas o estoloníferas; tallos sólidos, glabros; plantas hermafroditas. Vainas carinadas; lígula una membrana generalmente ciliolada; láminas lineares, aplanadas o plegadas. Inflorescencia de uno o más verticilos de espigas unilaterales, raramente de una espiga, las espiguillas sésiles, adpresas o pectinadas en dos hileras sobre los lados inferiores del raquis; espiguillas comprimidas lateralmente, generalmente con un flósculo bisexual y uno o dos flósculos estériles o raramente estaminados; desarticulación arriba de las glumas; glumas subiguales o la inferior más corta que la superior, más cortas que el flósculo fértil, membranáceas, 1-nervias, carinadas, agudas o acuminadas; lema fértil membranácea o cartilaginosa, carinada, 3-nervia, emarginada o bífida, generalmente aristada desde el ápice o justo debajo del ápice, las nervaduras marginales generalmente pilosas, la quilla pilosa o glabra; callo piloso; raquilla prolongada por encima del flósculo fértil y con flósculos rudimentarios; lodículas, adnadas a la pálea; estambres; estilos. Fruto una cariopsis; embrión 1/3–2/3 la longitud de la cariopsis; hilo punteado.

El género fue descrito por Peter Olof Swartz y publicado en "Nova Genera et Species Plantarum seu Prodromus" 1, 25. 1788. La especie tipo es: "Chloris cruciata" (L.) Sw. 
El nombre del género deriva del griego "chloros" (verde), refiriéndose a las hojas; Como alternativa, el nombre de Chloris (La verde), en la mitología griega es la diosa de las flores.

El número cromosómico básico del género es x = 10, con números cromosómicos somáticos de 2n = 14, 20, 26, 30, 36, 40, 72, 80 y 100, ya que hay especies diploides y una serie poliploide. Cromosomas relativamente "pequeños".




</doc>
<doc id="681" url="https://es.wikipedia.org/wiki?curid=681" title="Chrysopogon">
Chrysopogon

Chrysopogon es un género de plantas herbáceas de la familia de las poáceas. Es originario de las regiones tropicales y subtropicales del globo.
Es una especie perenne matojosa de hasta 80 cm de altura, que crece con una precipitación anual entre 300 y 1200 mm. Se considera una gramínea pratense útil y se ha utilizado para sobresiembra de las praderas naturales pobres. No debe pastarse durante el año de establecimiento. Apetecible y de bastante buen rendimiento.
El nombre del género deriva del griego "chrysos" (de oro) y "pogon" (barba), aludiendo a pelos de color marrón dorado del callo de algunas especies.
Número de la base del cromosoma, x = 5 y 10. 2n = 20 y 40. Cromosomas "pequeños". 



</doc>
<doc id="682" url="https://es.wikipedia.org/wiki?curid=682" title="Chusquea">
Chusquea

Chusquea es un género de plantas de la familia de las gramíneas (Poaceae). Son unas 120 especies, la mayoría de montaña, nativos del sur de México al sur de Chile y Argentina. En los ecosistemas de páramo de los Andes se denomina bambú. Los tallos de estas especies son sólidos y no huecos.

Tallos sólidos, generalmente fistulosos con la edad, normalmente ramificándose en estado vegetativo. Vainas foliares del tallo sin aurículas; láminas articuladas con las vainas, pero generalmente erectas, no seudopecioladas, triangulares. Nudos en el 1/2 del tallo con una yema central más grande y 2-numerosas yemas subsidiarias subyacentes, más pequeñas, subiguales, independientes, consteladas, lineares o verticiladas, todas las yemas de 2 o raramente 3 tamaños, persistiendo este polimorfismo durante el desarrollo de las ramas. Inflorescencia generalmente una panícula, raramente un racimo paucifloro. Espiguillas teretes, o comprimidas lateral o dorsalmente; glumas 2; lemas estériles 2; flósculo bisexual 1; desarticulación por encima de las glumas y debajo de las lemas estériles; raquilla no prolongada más allá de la pálea; lodículas 3; estambres 3, los filamentos filiformes, libres, las anteras lineares; ovarios glabros; estigmas 2. Fruto una cariopsis.
El género fue descrito por Carl Sigismund Kunth y publicado en "Journal de Physique, de Chimie, d'Histoire Naturelle et des Arts" 95: 151. 1822. La especie tipo es: "Chusquea scandens" Kunth. 
La palabra chusque viene del muisca "chusquy", que según un manuscrito colonial de la Biblioteca Nacional de Colombia, significa "Caña ordinaria de la tierra". Al parecer el nombre científico fue asignado por José Celestino Mutis durante la Expedición Botánica.

El número cromosómico básico del género es x = 12, con números cromosómicos somáticos de 2n = 40, 44 y 48, ya que hay especies diploides y una serie poliploide.




</doc>
<doc id="683" url="https://es.wikipedia.org/wiki?curid=683" title="Cinna">
Cinna

Cinna es un género de plantas herbáceas de la familia de las poáceas. Es originario de las regiones templadas de Eurasia, América del Norte y América del Sur.
Son plantas perennes cespitosas. La lígula es una membrana; láminas lineares. Inflorescencia una panícula laxa, solitaria, terminal. Espiguillas comprimidas lateralmente, con 1 flósculo bisexual; desarticulación por debajo de las glumas; glumas un poco más largas o más cortas que el flósculo, membranáceas, 1-3-nervias; lema membranácea, 3-nervia, cortamente aristada por debajo del ápice; pálea tan larga como la lema, angostamente 2-carinada o 1-carinada; raquilla prolongada, oculta entre las quillas; estambres 1-2; estilos 2; ovario glabro. Fruto una cariopsis; hilo punteado; endospermo líquido o pastoso. 
El género fue descrito por Carlos Linneo y publicado en "Species Plantarum" 1: 5. 1753. La especie tipo es: "Cinna arundinacea" 
El nombre del género deriva de la palabra griega "cinna" que significa hierba.
Número de la base del cromosoma, x = 7. 2n = 28. 4 ploid. Cromosomas "grandes". 



</doc>
<doc id="685" url="https://es.wikipedia.org/wiki?curid=685" title="Coelachne">
Coelachne

Coelachne es un género de plantas herbáceas de la familia de las poáceas. Es originario de África tropical Asia y Australia.
El nombre del género deriva del griego "koilos" (hueco) y "achne" (paja, escala), refiriéndose a un ventricoso lemma inferior. 
Número de la base del cromosoma, x = 10. 2n = 40. 



</doc>
<doc id="686" url="https://es.wikipedia.org/wiki?curid=686" title="Coelorachis">
Coelorachis

Coelorachis es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de las regiones tropicales del globo.

El nombre del género deriva del griego "koilon" (cavidad) y "raquis" (eje), refiriéndose a su raquis. 

El número cromosómico básico del género es x = 9, con números cromosómicos somáticos de 2n = 18,36 y 54, ya que hay especies diploides y una serie poliploide.



</doc>
<doc id="687" url="https://es.wikipedia.org/wiki?curid=687" title="Colpodium">
Colpodium

Colpodium, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario del Hemisferio Norte templado a gran altitud. Comprende 70 especies descritas y de estas, solo 22 aceptadas.
El género fue descrito por Carl Bernhard von Trinius y publicado en "Fundamenta Agrostographiae" 119, pl. 7. 1820. 
El número cromosómico básico del género es x = 2, con números cromosómicos somáticos de 2n = 4, o 8 (en "Keniochloa") O 28 ("C. Colchicum, Paracolpodium"), ya que hay especies diploides y una serie poliploide. Cromosomas relativamente "grandes".
Colpodium: nombre genérico que deriva del griego "kolpos" (una bahía o golfo), refiriéndose a lemmas emarginados (?). 



</doc>
<doc id="688" url="https://es.wikipedia.org/wiki?curid=688" title="Cryptochloa">
Cryptochloa

Cryptochloa, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de la región del Caribe, el Amazonas y los Andes.
Son plantas perennes cespitosas; tallos con muchos nudos, simples, monomorfos o dimorfos; plantas monoicas. Pseudopecíolo corto; lígula membranácea, asimétrica, adnada a la aurícula de la vaina; láminas aplanadas, oblongas a ovadas, asimétricas, basalmente redondeadas. Inflorescencias racemiformes o paniculiformes, generalmente 1–8 surgiendo desde los nudos foliosos más superiores, a veces desde los nudos sin láminas más inferiores, inflorescencia terminal en general estrictamente estaminada, a veces bisexual, inflorescencias axilares usualmente bisexuales, rara vez estrictamente pistiladas; pedúnculos generalmente ocultos en las vainas, raramente exertos en inflorescencias terminales; pedicelos estaminados delgados, pedicelos pistilados engrosados en la punta; espiguillas unisexuales, dorsalmente comprimidas, solitarias o las estaminadas a veces pareadas, con 1 flósculo, espiguillas estaminadas en la base de la inflorescencia en inflorescencias bisexuales, más cortas que las espiguillas pistiladas, glumas ausentes, lema y pálea membranáceas, pálea 2-nervia, lodículas 3, estambres 2 o 3; espiguillas pistiladas en las puntas de las ramas más superiores en inflorescencias bisexuales, fusiformes, desarticulación por encima de las glumas, glumas 2, subiguales o iguales, más largas o tan largas como el flósculo, herbáceas, no largamente persistentes, flósculo dispuesto en y cayendo con un entrenudo de la raquilla alargado y engrosado, lema y pálea endurecidas, brillantes y con manchas cafés en la madurez, lodículas 3, estilo 1, estigmas 2. Fruto una cariopsis; hilo linear, tan largo como la cariopsis; embrión ca 1/6 de la longitud de la cariopsis.
El género fue descrito por Jason Richard Swallen y publicado en "Annals of the Missouri Botanical Garden" 29(4): 317. 1942. La especie tipo es: "Cryptochloa variana" Swallen
Número de la base del cromosoma, x = 10 y 11. 2n = 20 y 22. Cromosomas "pequeños".





</doc>
<doc id="689" url="https://es.wikipedia.org/wiki?curid=689" title="Cymbopogon">
Cymbopogon

Cymbopogon es un género de plantas de la familia Poaceae, con cerca de 55 especies originarias de las regiones cálidas y tropicales de Asia. Es un tipo de pasto perenne. Se conoce como hierba de limón en Panamá, limonaria o limoncillo en Colombia, paja cedrón en Bolivia, mal ojillo o malojillo en Venezuela, y zacate limón en Honduras, El Salvador, Costa Rica, Nicaragua y México. En República Dominicana se llama limoncillo, en el Noroeste Argentino se llama por cedrón pasto. En la parte occidental de Cuba se le denomina caña santa y en la oriental limoncillo o yerba de calentura. En el Paraguay se conoce como cedrón Kapi-í. Es también conocida como té de limón, pasto de limón, lemongrass o pasto citronella. En el Ecuador , Chile y el Perú es conocida como hierba luisa. No debe ser confundida con el cedrón o "Aloysa citridora".

Son plantas perennes cespitosas, polígamas con tallos sólidos. Hojas generalmente aromáticas con olor a limón cuando trituradas, una membrana ciliolada lígula, láminas lineares, aplanadas. Inflorescencia como un par de racimos cortos, los racimos agregados en una panícula compuesta, falsa, densa, espatácea, terminal, raquis articulado arriba del par basal de espiguillas, aplanado, las espiguillas pareadas, el par basal con las espiguillas similares, sin aristas, estaminadas, los otros pares de espiguillas con una espiguilla sésil bisexual, aristada, y la otra espiguilla pedicelada estaminada o estéril, las 2 más o menos de igual tamaño, las 2 espiguillas y 1 entrenudo del raquis caedizos como una unidad, o la pedicelada desarticulándose; espiguillas sésiles lanceoladas, con 2 flósculos, glumas iguales, coriáceas, ocultadas por los flósculos, la inferior aplanada, carinada lateralmente, los márgenes superiores agudamente inflexos, la superior navicular, carinada hacia el ápice, 1–3-nervia, flósculo inferior estéril, lema inferior enervia, hialina, ciliada, pálea inferior ausente, flósculo superior bisexual, lema superior hialina, profundamente 2-lobada, cortamente aristada desde el seno, pálea superior diminuta o ausente, lodículas 2, estambres 3, estilos 2; espiguillas pediceladas similares a las espiguillas sésiles pero la gluma inferior redondeada, herbácea, pedicelos libres, flósculo inferior ausente, flósculo superior estaminado, lema superior hialina, estambres 3. Fruto una cariopsis; hilo punteado.

Francisco Hernández en el siglo XVI relata el uso del zacate limón como antiespasmódico, antipalúdico, antitusígeno, carminativo, diaforético, ”dolor alcohólico”, estimulante y ”rubefaciente”. Es hasta el siglo XX que la Sociedad Mexicana de Historia Natural vuelve a mencionarla o recomendarla como antiespasmódico. Luis Cabrera poco después comenta que tiene efectos antiespasmódicos, aperitivos, eupépticos y sirve para la gastroenteritis, y finalmente la Sociedad Farmacéutica de México reitera su uso como antiespasmódico y eupéptico.

El pasto de limón se usa ampliamente en Asia como hierba, particularmente en Tailandia, Laos, Sri Lanka y en la cocina caribeña. Tiene un sabor y aroma similares al del limón y puede secarse, pulverizarse o usarse fresco. El tallo es difícil de ingerir, exceptuando la parte interna. Sin embargo, puede molerse finamente conservando el aceite aromático. El principal constituyente del aceite del pasto-limón es el citral.

Se usa comúnmente en infusiones de té, sopas y currys, lo mismo que en pescados y mariscos. Se usa más frecuentemente como té en los países africanos.

El pasto de limón del este de la India ("Cymbopogon flexuosus"), también conocido como pasto de cochin, pasto de Malabar, es originario de Camboya, India, Sri Lanka, Myanmar y Tailandia mientras que el pasto de limón del oeste ("Cymbopogon citratus") se presume tiene su origen en Malasia mientras ambos son usados invariablemente la especie "C. citratus" tiene un perfil más encaminado hacia la cocina. En la India la especie "C. citratus" es usada como hierba medicinal y en perfumes. El pasto citronella ("Cymbopogon nardus" y "Cymbopogon winterianus") es similar a las especies descritas arriba pero crece hasta una altura de 2 m y los tallos en la base son en color rojo.

Aquellas especies son usadas para la producción de aceite de citronela, el cual es usado en jabones, como repelente de mosquitos, insecticidas, y velas, también aromaterapia, la cual es famosa en Bintan, Indonesia. Los principios químicos activos de la citronella, geraniol y citronelol son antisépticos, de ahí su uso en desinfectantes caseros y jabones. Al lado de la producción de aceite, el pasto citronella tiene uso culinario como “té de limón” o “de Ceylan”. En el Perú se usan solo las hojas en infusión. En Paraguay forma parte de uno de los yuyos más populares para preparar la bebida denominada tereré.

El género fue descrito por Curt Polycarp Joachim Sprengel y publicado en "Plantarum Minus Cognitarum Pugillus" 2: 14. 1815. La especie tipo es: "Cymbopogon schoenanthus" 
Cymbopogon: nombre genérico que deriva del griego "kumbe" = (barco) y "pogon" = (barba), refiriéndose a las muchas aristas y espatas parecidas a un barco.
Tiene un número de cromosomas de: x = 5, o 10. 2n = 20, 22, 40, y 60.


Las partes aéreas de la planta contienen un aceite esencial en el que se han identificado los monoterpenos alcanfor, borneol, camfeno, cineal, citral, citronelal, citronelol, fenchona, geranial, geraniol, 6-metil hep-5-en-ona, limoneno, linalol, mentol, mentona, mirceno, neral, acetato de nerol, nerol, ocimeno, alfa-pineno, terpineol, terpinoleno y los sesquiterpenos alfa-oxobisabolona, beta-cadineno y humuleno. En las hojas se han detectado el beta-sitosterol y los triterpenos cimbopogenol, cimbopogona y cimbopogonol. 3

Varios estudios demuestran que el aceite esencial ejerce una actividad antibiótica contra las bacterias "Staphylococcus aureus", "Bacillus subtilis", "Escherichia coli", "Pseudomona aeruginosa", "Mycobacterium smegmatis", antimicótica contra los hongos "Candida albicans", "C. pseudotropicalis" y otros hongos; y antimutagénica.

El extracto acuoso de las hojas ejerció una acción hipotérmica en ratas al ser administrado por vía intraperitoneal a la dosis de 0.05g/kg. Este mismo extracto presentó una actividad antiinflamatoria y diurética en ratas por vía intragástrica y también una actividad inhibitoria de la movilidad intestinal en ratones al administrarse por vía intraperitoneal, efecto que fue reproducido en íleon de conejo con una fracción insaponificable de la hoja y en íleon de rata con un extracto acuoso de las hojas.

En el hombre, el extracto acuoso de la hoja provocó un estado de ansiedad en 16 voluntarios sanos, usando la prueba de Stroop Colour-word cuando fue administrado por vía oral a la dosis de 2-10g. El extracto en dosis de 4g persona presentó una actividad depresora del sistema nervioso central en adultos. El aceite esencial administrado en cápsulas a 22 voluntarios a la dosis de 140mg/día durante 60 días disminuyó ligeramente los niveles de colesterol de 8 individuos de manera significativa.

https://web.archive.org/web/20140503074207/http://www.medicinatradicionalmexicana.unam.mx/monografia.php?l=3&t=zacate%20lim%C3%B3n&id=7810



</doc>
<doc id="690" url="https://es.wikipedia.org/wiki?curid=690" title="Chaetium">
Chaetium

Chaetium es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario tanto de Norteamérica como de Sudamérica. Comprende 4 especies descritas y de estas, solo 3 aceptadas.

Tiene hábito perenne. Los culmos son erectos (1/1); de 3–7 y excepcionalmente 12 dm de largo. La lígula tiene una membrana ciliada, o un rizo de pelos. La inflorescencia está compuesta de racimos, a través de un eje central; erecta; unilateral. El raquis angular.
Son plantas perennes cespitosas; tallos fistulosos, 40–100 cm de alto, erectos o ascendentes, simples o ramificados; entrenudos comprimidos, glabros; nudos barbados; plantas hermafroditas. Vainas escasamente carinadas, glabras excepto en los márgenes; lígula una membrana ciliada 0.3–0.4 mm de largo, los cilios 1.2–2.1 mm de largo; láminas lineares, 10–30 cm de largo y 3–7 mm de ancho, papiloso-pilosas. Inflorescencias terminales y axilares; panículas 10–23 cm de largo y 1–2 cm de ancho; racimos erectos, adpresos; espiguillas pareadas, comprimidas dorsalmente, lanceoloides, aplanado-convexas, 8–10 mm de largo excluyendo las aristas, verdosas o purpúreas, con 2 flósculos; callo basal puntiagudo, 1.5–2.5 mm de largo, formado de la base de la gluma inferior y el entrenudo de la raquilla; desarticulación oblicua en la base del callo; glumas iguales, 6–10 mm de largo, más largas que los flósculos, herbáceas, 5–7-nervias, las aristas 20–35 mm de largo, flexuosas; flósculo inferior estéril; lema inferior 6–8 mm de largo, 3-nervia; pálea inferior ausente; flósculo superior bisexual; lema superior 6–8 mm de largo, lisa, atenuada o con una arista corta de hasta 2 mm de largo, los márgenes aplanados, cartácea; pálea superior atenuada en una arista corta; lodículas 2; estambres 3, las anteras ca 1.5 mm de largo; estilos 2. Fruto una cariopsis; embrión ca 1/2 la longitud de la cariopsis; hilo ca 3/10 la longitud de la cariopsis, linear-oblongo.
El género fue descrito por Christian Gottfried Daniel Nees von Esenbeck y publicado en "Flora Brasiliensis seu Enumeratio Plantarum" 2(1): 269–270. 1829. La especie tipo es: "Chaetium festucoides" Nees
Número de la base del cromosoma, x = Aparentemente 13. 2n = 26. 2 ploidias. 
A continuación se brinda un listado de las especies del género "Chaetium" aceptadas hasta abril de 2014, ordenadas alfabéticamente. Para cada una se indica el nombre binomial seguido del autor, abreviado según las convenciones y usos.



</doc>
<doc id="701" url="https://es.wikipedia.org/wiki?curid=701" title="Biología celular">
Biología celular

La biología celular (anteriormente citología, del griego "κύτος", que significa ‘célula’) es una disciplina académica que se encarga del estudio de las células en lo que respecta a las propiedades, estructura, funciones, orgánulos que contienen, su interacción con el ambiente y su ciclo vital.

Con la invención del microscopio óptico fue posible observar estructuras nunca antes vistas por el ser humano, las células. Esas estructuras se estudiaron detalladamente algo más tarde, con el empleo de diversas técnicas de tinción y de citoquímica y más tarde con la ayuda fundamental del microscopio electrónico.

La biología celular se centra en la comprensión del funcionamiento de los sistemas celulares, de cómo estas células se regulan y la comprensión del funcionamiento de sus estructuras. Una disciplina afín es la biología molecular.

La primera referencia al concepto de célula data del siglo XVII, cuando el inglés Robert Hooke utilizó este término, para referirse a los pequeños huecos poliédricos que constituían la estructura de ciertos tejidos vegetales como el corcho (y por su parecido con las habitaciones de los sacerdotes llamadas "celdas").

No obstante, hasta el siglo XIX no se desarrolla este concepto considerando su estructura interior. Es en este siglo, cuando se desarrolla la teoría celular , que reconoce la célula como la unidad básica de estructura y función de todos los seres vivos, idea que constituye desde entonces uno de los pilares de la biología moderna. 

Fue esta teoría celular la que impulsó en buena medida las investigaciones biológicas al terreno microscópico, pues las células no son visibles a simple vista. La unidad de medida utilizada es el micrómetro (μm) antes conocida como micra, existiendo células de entre 2 y 20 μm, aunque las neuronas pueden tener una longitud mayor. 

La investigación microscópica pronto daría lugar al descubrimiento de la estructura celular interna incluyendo el núcleo, los cromosomas, el aparato de Golgi, las mitocondrias y otros orgánulos celulares, así como la identificación de la relación existente entre la estructura y la función de los orgánulos celulares.
Ya en siglo XX, la introducción del microscopio electrónico reveló detalles de la megaestructura celular, y aparecieron la histoquímica y la citoquímica. También se descubrió la base material de la herencia, con los cromosomas y el ADN, y nació la citogenética.

La Biología Celular como tal, surgió como consecuencia de un cambio en la concepción del estudio de los organismos vivos, en tanto estos mostraban funciones que sobrepasaban lo estructural.
Es esencial conocer los procesos de la vida de la célula durante su ciclo celular, como son la nutrición, la respiración, la síntesis de componentes, los mecanismos de defensa, la división celular y la muerte celular.

La historia de la bioquímica como la conocemos hoy en día, viene del siglo XIX cuando una buena parte de la biología y de la química se orientaron a la creación de una nueva disciplina integradora: la química fisiológica hoy conocida como bioquímica.

Podemos entender la bioquímica como una disciplina científica integradora, que aborda el estudio de las biomoléculas y los biosistemas. Integra de esta forma las leyes químico-físicas y la evolución biológica que determinan a los biosistemas y a sus componentes.

La biología molecular implica la comprensión de las interacciones de los diferentes sistemas de la célula, lo que incluye muchas relaciones, entre ellas las del ADN con el ARN, la síntesis de proteínas, el metabolismo, y cómo todas esas interacciones son reguladas para conseguir un correcto funcionamiento de la célula.

La biología molecular tiene como objetivo el estudio, desde el punto de vista molecular, de los procesos que se desarrollan en la célula viva. Dos macromoléculas en particular son objeto de su estudio: el ADN y las Proteínas. Esta área específica de estudio está relacionada con otros campos de la Biología Celular, como son la Ingeniería genética y la bioquímica. 

El estudio mediante métodos físico-químicos de la materia viva y sus procesos biológicos, incluye varias disciplinas dentro del concepto general de Biología Molecular, ellas son: Bioquímica Estructural, Bioquímica Inorgánica, Bioquímica Metabólica y Enzimología, Fisiología Molecular, Biología Molecular y Química Física.

Para alcanzar sus objetivos, los biólogos celulares se ven obligados a estudiar los componentes de la célula a nivel molecular (biología molecular). 

Componentes principales del estudio celular: 





</doc>
<doc id="709" url="https://es.wikipedia.org/wiki?curid=709" title="Cráneo">
Cráneo

El cráneo (del griego: "κρανίον", "kranion" y del latín: "cranium") es parte del sistema óseo o sistema esquelético, es una caja ósea que protege de golpes y contiene al encéfalo principalmente. El cráneo humano está conformado por la articulación de 8 huesos, que forman una cavidad abierta y ovoide de espesor variable, con una capacidad aproximada de 1.450 ml (en adultos). En zoología, al cráneo osificado también se lo denomina osteocráneo.

El esqueleto de la cabeza, o macizo esquelético neo-facial, es el conjunto de los huesos del "cráneo" ("ossa cranii" PNA) y los huesos de la cara ("ossa faciei" PNA), conocido como "calavera" en términos coloquiales, aunque anatómicamente es la cabeza ósea, siendo el "cráneo" una parte de la cabeza. Es común que "cráneo" designe a la totalidad de la "cabeza ósea", lo cual es impropio en el estudio de la anatomía. Sin embargo, en otros ámbitos (embriología, biología, etc.) se considera el "cráneo" como sinónimo de "esqueleto de la cabeza".

La distinción entre "cráneo" y cara es muy clara: el cráneo aloja el encéfalo fundamentalmente el -"neurocráneo"-, mientras que la cara presta inserción a los músculos de la mímica y de la masticación y aloja algunos de los órganos de los sentidos.
El cráneo cumple una función muy importante, ya que se preocupa de contener todo el sistema nervioso central, con excepción de la médula.

El cráneo es el esqueleto de la cabeza y diversos huesos constituyen sus dos partes: el neurocráneo y esplacnocráneo. El neurocráneo es la caja ósea del encéfalo y sus cubiertas membranosas. En un adulto, está formado por una serie de ocho huesos: cuatro impares centrados en la línea media (frontal, etmoides, esfenoides y occipital) y dos series de pares bilaterales (temporal y parietal). Los huesos del denominado neurocráneo en conjunto conforman otras dos estructuras anatómicas: Los huesos frontal, parietales y occipital suelen conformar una estructura de techo parecido a una cúpula, denominada calvaria o bóveda craneal, mientras que el hueso esfenoides y temporales forman parte de la base del cráneo. 

El esplacnocráneo o viscerocráneo, también llamado esqueleto facial, constituye la parte anterior del cráneo y se compone de los huesos que rodean la boca (maxilares y mandíbula), la nariz/cavidad nasal y la mayor parte de las cavidades orbitarias. Este consta de 14 huesos irregulares: dos huesos impares centrados (mandíbula y vómer) y seis huesos pares bilaterales (maxilar, cornete nasal inferior, cigomático, palatino, nasal y lagrimal). Los maxilares y la mandíbula albergan los dientes; dicho de otra manera, proporcionan las cavidades y el hueso de sostén para los dientes maxilares y mandibulares. Los maxilares forman la mayor parte del esqueleto facial superior, fijado a la base del cráneo. La mandíbula forma el esqueleto facial inferior, siendo este de carácter móvil al articularse con la base del cráneo en las articulaciones temporomandibulares.

El cráneo, como cavidad, puede ser considerado desde el interior de esa cavidad como "endocráneo", o desde el exterior como "exocráneo". A su vez, en conjunto, se puede dividir mediante una sección horizontal que pase por la eminencia frontal media y por la protuberancia occipital externa, en dos porciones:
Esta división no es tan arbitraria. Parte del diferente origen embriológico de las estructuras óseas: osificación endocondral para los huesos de la base craneal, y osificación intramembranosa para los huesos de la calota.

La bóveda está formada por el frontal (parte vertical), los parietales, las escamas de los temporales y el occipital (parte superior). Está cubierta por el cuero cabelludo; los huesos se unen por unas articulaciones llamadas suturas: sutura coronal o frontoparietal, entre el frontal y las parietales, sutura sagital o interparietal, entre los dos parietales, y sutura lambdoidea o parietooccipital, entre el occipital y los parietales.

Las estructuras cefálicas craneales se originan a partir del mesénquima proveniente de las células de la "cresta neural" y el "mesodermo paraxial".
Los huesos que forman el cráneo no tienen un mismo origen, por ello se hace la diferencia entre las regiones de la bóveda y la base craneal.

Los huesos de la calota son huesos planos de revestimiento. Se generan por el proceso de osificación intramembranosa a partir de placas de tejido conjuntivo fibroso (mesénquima) que rodean el encéfalo. De esta forma, centrífugamente, se desarrollan (osifican) huesos membranosos planos. Al momento del nacimiento, los huesos de la calota no están fusionados ni totalmente osificados, dejando espacios interóseos cubiertos por tejido fibroso (suturas y fontanelas).

Los huesos de la base craneal se desarrollan por el proceso de "osificación endocondral" a partir del "condrocráneo", una estructura formada por varios núcleos cartilaginosos osteogénicos separados y extendidos por toda la región (condrocráneo precordal originado de la cresta neural, y condrocráneo cordal originado del mesodermo paraxial)

Al momento del nacimiento, los huesos planos del cráneo no están completamente osificados y se hallan separados entre sí por espacios ocupados por tejido conectivo fibroso derivado de la cresta neural que contribuirá en el futuro a la formación definitiva de los huesos y a su articulación (sinfibrosis). Estos espacios son las suturas metópica, coronal, sagital y lamdoidea. En aquellos sitios donde se articulan más de dos huesos, las suturas son amplias y forman las seis "fontanelas":

Las suturas y fontanelas tienen importancia capital durante el parto, ya que admiten una mecánica de superposición entre las placas óseas del cráneo ("modelado") que posibilita el paso de la cabeza fetal a través del canal de parto. Durante el puerperio, los huesos vuelven a su posición primitiva. Durante la niñez, la palpación de la fontanela anterior permite verificar la normalidad del desarrollo y osificación del cráneo así como también la presión intracraneana.

Las suturas y fontanelas tardan años en osificarse completamente y lograr la coaptación total entre las piezas óseas del cráneo. El crecimiento de los huesos de la bóveda que continúa hasta la adultez se hace a expensas del material fibroso de las suturas y fontanelas. Este mecanismo admite cierta complacencia de la caja craneal para el crecimiento del encéfalo y una adaptación acorde al desarrollo y crecimiento del macizo facial. La capacidad craneal completa se alcanza hacia los 6 años.

Las articulaciones de los huesos craneales son sinartrosis, articulaciones inmóviles que fijan las piezas óseas entre sí por medio de cartílago (sincondrosis) o de tejido conectivo fibroso (sinfibrosis).

Aquellos huesos que forman parte de la base craneal, desarrollados por osificación endocondral, se unen entre sí a través de sincondrosis. Y los huesos procedentes de la bóveda del cráneo (y los huesos de la cara también), desarrollados a partir de esbozos de tejido conjuntivo, se unen entre sí a través de sinfibrosis o suturas ("suturae" PNA).

Según la configuración de las superficies articulares implicadas en la unión ósea, hay cuatro tipos de suturas (sinfibrosis) en el cráneo:





</doc>
<doc id="710" url="https://es.wikipedia.org/wiki?curid=710" title="Cerebro">
Cerebro

El cerebro (del latín "cerebrum", con su raíz indoeuropea «ker», "cabeza, en lo alto de la cabeza" y «brum», "llevar"; teniendo el significado arcaico de "lo que lleva la cabeza") es un órgano que centraliza la actividad del sistema nervioso y existe en la mayor parte de los animales.
El cerebro se encuentra situado en la cabeza; por lo general, cerca de los principales órganos de los sentidos como la visión, la audición, el equilibrio, el gusto y el olfato. Corresponde, por tanto, al encéfalo de los humanos y otros vertebrados y se subdivide en cerebro anterior, medio y posterior. En otros animales, como los invertebrados bilaterales, se entiende como cerebro a una serie de ganglios alrededor del esófago en la parte más anterior del cuerpo, (véase protóstomos e hiponeuros) comprendidos por el protocerebro, deutocerebro y tritocerebro en artrópodos, ganglios cerebral, pleural y pedial en moluscos gasterópodos y masas supraesofágica y subesofágica en moluscos cefalópodos. También poseen cerebros muy arcaicos o simples bilaterales como platelmintos, nemátodos o hemicordados. Sin embargo, hay bilaterales que muestran muy pocos rasgos distintivos de cefalización como los bivalvos o briozoos. En algunas especies de invertebrados no existe un cerebro por carecer completamente de sistema nervioso, como los poríferos, los placozoos, los mesozoos, y otros, aunque poseen un sistema nervioso, carecen de rasgos definidos de centralización o cefalización al mostrar simetrías no bilaterales como los cnidarios, ctenóforos o equinodermos.
Desde un punto de vista evolutivo y biológico, la función del cerebro como órgano, es ejercer un control centralizado sobre los demás órganos del cuerpo. El cerebro actúa sobre el resto del organismo por la generación de patrones de actividad muscular o por la producción y secreción de sustancias químicas llamadas hormonas. Este control centralizado permite respuestas rápidas y coordinadas ante los cambios que se presenten en el medio ambiente. Algunos tipos básicos de respuesta tales como los reflejos pueden estar mediados por la médula espinal o los ganglios periféricos, pero un sofisticado control intencional de la conducta sobre la base de la información sensorial compleja requiere la capacidad de integrar la información de un cerebro centralizado.
El cerebro de los vertebrados es el órgano más complejo del cuerpo. En un humano típico, la corteza cerebral se estima que contiene 16 000 millones de neuronas y todo el encéfalo contiene 86 000 millones. Estas neuronas se comunican con otras a través de fibras largas de protoplasma llamadas axones, las cuales llevan trenes de impulsos eléctricos denominados potenciales de acción a partes distantes del cerebro o del resto del cuerpo. El punto de contacto entre las prolongaciones de dos neuronas que se comunican recibe el nombre de sinapsis.
Desde una perspectiva filosófica, lo que hace al cerebro especial en comparación con los otros órganos, es que forma la estructura física en donde se presenta el correlato material de las distintas actividades de la mente. 

Durante las primeras etapas de la psicología, se creyó que la mente debía separarse del cerebro. Sin embargo, posteriormente, los científicos realizaron experimentos que llegaron a determinar que la mente era un componente en el funcionamiento cerebral, por la expresión de ciertos comportamientos basados en su medio ambiente externo y el desarrollo de su organismo. Los mecanismos por los cuales la actividad cerebral da lugar a la conciencia y al pensamiento son muy difíciles de comprender: a pesar de los múltiples y rápidos avances científicos, mucho acerca de cómo funciona el cerebro sigue siendo un misterio. En la actualidad, las operaciones de las células cerebrales individuales son comprendidas con más detalle, pero la forma en que cooperan entre los conjuntos de millones ha sido muy difícil de descifrar. Asimismo, los enfoques más prometedores tratan el cerebro como una «computadora biológica», totalmente diferente en el mecanismo de las computadoras electrónicas, pero similar en el sentido que adquieren la información del mundo circundante, la almacenan y la procesan de múltiples formas.

Sin embargo, pese a ser uno de los órganos más estudiados, se han desarrollado una serie de conceptos erróneos que han llegado a ser asimilados por la sociedad como correctos; como es el caso del mito que dice, que los humanos solamente utilizamos un 10% del cerebro.
En este artículo se comparan las propiedades de los cerebros de toda la gama de especies animales, con una mayor atención en los vertebrados y el ser humano. Existe un artículo específico para el cerebro humano.

El cerebro es el mayor órgano del sistema nervioso central y forma parte del centro de control de todo el cuerpo. También es responsable del pensamiento, la memoria, las emociones, el habla y el lenguaje. También es el responsable de los reflejos y el control del cuerpo. Pesa entre 1.300 y 1.500 gramos. Eso representa entre el 0,8% y 2% de la masa corporal de una persona.

En los vertebrados el cerebro se encuentra ubicado en la cabeza, protegido por el cráneo y en cercanías de los aparatos sensoriales primarios de la visión, el oído, el olfato, el gusto y el sentido del equilibrio.
En los vertebrados el encéfalo se divide en tres partes: cerebro, cerebelo y tronco cerebral. En ocasiones se utiliza erróneamente el término "cerebro" como sinónimo de encéfalo, en realidad el cerebro solamente es una parte del encéfalo. 

Los cerebros son sumamente complejos. La complejidad de este órgano emerge por la naturaleza de la unidad que nutre su funcionamiento: la neurona. Estas se comunican entre sí por medio de largas fibras protoplasmáticas llamadas axones, que transmiten trenes de pulsos de señales denominados potenciales de acción a partes distantes del cerebro o del cuerpo depositándolas en células receptoras específicas.

Los cerebros controlan el comportamiento provocando la contracción de los músculos, o estimulando la secreción de sustancias químicas como algunas hormonas. Incluso los organismos unicelulares pueden ser capaces de obtener información de su medio ambiente y actuar en respuesta a ello.

Las esponjas que no poseen un sistema nervioso central, son capaces de coordinar las contracciones de sus cuerpos y hasta su locomoción.

Si se observa a simple vista un corte del cerebro pueden apreciarse dos zonas de aspecto diferente. Una de ellas de color más oscuro se llama sustancia gris y está formada por los cuerpos neuronales, la otra más clara se llama sustancia blanca y está constituida por los axones cubiertos de mielina que parten de las neuronas para transmitir el impulso nervioso. La sustancia blanca está formada por las vías por la que se transmite la información a distancia dentro del sistema nervioso, mientras que la sustancia gris se constituye por los cuerpos de las neuronas que es donde se generan los impulsos.

En la superficie del cerebro de los vertebrados se encuentra la corteza cerebral que está formada por sustancia gris, por debajo se sitúa una masa central de sustancia blanca que envuelve un conjunto de núcleos de sustancia gris situados en el centro del cerebro, entre los que se incluye el tálamo y los llamados ganglios basales o núcleos basales.

La neurona es la unidad básica sobre la que está construido el cerebro. Según estudios recientes un cerebro humano medio dispone de alrededor de 86.000.000.000 de neuronas. Una neurona no es más que una célula que se ha especializado en la transmisión de los impulsos nerviosos, consta de un cuerpo celular o soma, un gran número de pequeñas prolongaciones llamadas dendritas y una prolongación principal que puede ser muy larga y recibe el nombre de axón, el cual a su vez puede ramificarse en muchas ramas al final de su recorrido. El axón se forma en un engrosamiento del cuerpo celular y se extiende a distancias variables que oscilan entre algunos micrómetros y más de un metro en algunas neuronas de ciertas localizaciones. Las conexiones que se establecen entre dos neuronas reciben el nombre de sinapsis. Según el principio de conectividad específica establecido por Ramón y Cajal, las neuronas no se conectan entre sí aleatoriamente, sino que establecen conexiones específicas en determinados lugares con otras células nerviosas, por lo que la aparente maraña de ramificaciones que se observa cuando se mira a través del microscopio una muestra de tejido cerebral no es un conjunto de conexiones al azar, sino una red de contactos entre células perfectamente organizado que es la que hace posible el funcionamiento del sistema nervioso y todas las actividades cerebrales.

Cada neurona integra continuamente numerosos impulsos eléctricos que recibe a través de sus dendritas y emite una respuesta única a través de su axón. Existen neuronas sensitivas que captan la información procedente de los diferentes sentidos y neuronas motoras que emiten impulsos que generan los movimientos musculares voluntarios, pero la mayor parte de las que existen en el cerebro son interneuronas que forman parte de circuitos anatómicos muy precisos.

Un neurotransmisor es una sustancia química producida por las neuronas que se libera al espacio sináptico de una sinapsis química por la acción de un impulso nervioso o potencial de acción. Interacciona con un receptor específico en la neurona postsináptica donde produce una determinada respuesta que puede ser excitatoria o inhibitoria. Los neurotransmisores son un aspecto fundamental en la función del cerebro.

Existen diferentes sustancias que actúan como neurotransmisores, algunas de las más importantes son las siguientes: GABA, acrónimo de ácido g-aminobutírico, serotonina, acetilcolina, dopamina, noradrenalina y endorfina. Las vías dopaminérgicas, por ejemplo, son rutas de neuronas que funcionan con la dopamina como neurotransmisor. Estas vías son de gran interés en la función del cerebro y se ha comprobado que su alteración puede provocar diferentes enfermedades, entre ellas la enfermedad de Parkinsón.

El cerebro forma parte del sistema nervioso. El sistema nervioso se divide en dos partes: sistema nervioso central formado por el encéfalo y la médula espinal y el sistema nervioso periférico constituido por los nervios motores y sensitivos que parten del sistema nervioso central. El encéfalo humano se divide en tres partes: cerebro, cerebelo y tronco cerebral. De ellas el cerebro es la de mayor peso y volumen. 

El cerebro humano está dividido en dos hemisferios, uno derecho y otro izquierdo, separados por la cisura interhemisférica y comunicados mediante el cuerpo calloso. La superficie se denomina corteza cerebral y está formada por plegamientos denominados circunvoluciones constituidas de sustancia gris. Subyacente a la misma se encuentra la sustancia blanca. En zonas profundas existen áreas de sustancia gris conformando núcleos como el tálamo, el núcleo caudado y el hipotálamo. Cada hemisferio cerebral posee varias cisuras que dividen la corteza cerebral en lóbulos.

El cerebro humano posee en su interior 4 ventrículos cerebrales intercomunicados que están llenos de un líquido claro llamado líquido cefalorraquídeo.

Cada hemisferio posee varias cisuras que subdividen el córtex cerebral en lóbulos:

Aparte de estos cuatro lóbulos muy conocidos porque comparten los nombres de los cuatro huesos de la bóveda craneana, podemos encontrar un lóbulo más llamado lóbulo de la Ínsula, que no es visible desde el exterior. Este lóbulo se encuentra en la parte interna del cerebro; se puede observar abriendo la cisura de Silvio.

El tálamo está situado por encima del tronco del encéfalo, casi en el centro del cerebro. Mide alrededor de 3 cm de largo y está formado por materia gris es decir el soma de células neuronales. Cumple la función de estación de relevo de las señales nerviosas y centro de integración donde se procesan los impulsos sensoriales antes de continuar su recorrido hasta la corteza cerebral. También recibe señales que siguen la dirección opuesta y llegan al tálamo procedente de la corteza cerebral.

El hipotálamo es una pequeña región del cerebro formada por sustancia gris. Está situado inmediatamente debajo del tálamo. Tiene el tamaño aproximado de una almendra y desempeña importantes funciones, entre ellas enlazar el sistema nervioso con el sistema endocrino a través de la hipófisis.

Los ganglios basales en realidad deberían llamarse núcleos basales pues no son verdaderos ganglios. Son un conjunto de estructuras cerebrales formadas por sustancia gris que están situados debajo de la corteza y desempeñan importantes funciones, una de las principales es el control de los movimientos voluntarios, pero también intervienen en el procesamiento de la información sensorial y en aspectos relacionados con la memoria y las emociones. Están conectados con la corteza cerebral y funcionan con un alto grado de integración. Pueden diferenciarse los siguientes: 

El hipocampo es una estructura cerebral que desempeña importantes funciones en la memoria y la orientación espacial. Está formado por materia gris y procede del lóbulo temporal, aunque se ubica por debajo de la corteza cerebral. Debe su nombre a que su forma recuerda en cierto modo a la de un caballito de mar. El hipocampo forma parte del sistema límbico y es una de las pocas regiones del cerebro en la que se produce el fenómeno de la neurogénesis (formación de nuevas neuronas).

El cuerpo calloso es una importante estructura del cerebro que está formada por fibras que actúan como vía de comunicación entre el hemisferio cerebral derecho y el izquierdo, con la finalidad de que ambos funcionen de forma conjunta y complementaria.

La cápsula interna es un grueso conjunto de fibras nerviosas tanto ascendentes como descendentes que comunican la corteza con las regiones inferiores del sistema nervioso central, las fibras son de origen diverso, pero muchas de ellas transportan información motora o sensitiva. En su trayecto pasan cerca de la región del tálamo y los ganglios basales. La cápsula interna es una región muy sensible, cualquier lesión en esta zona daña numerosas fibras nerviosas y provoca en consecuencia déficits neurológicos graves.

El cerebro procesa la información sensorial, tanto la visual como la táctil, auditiva y olfatoria. Las áreas motoras controlan y coordinan el movimiento, mientras que las áreas de asociación son las responsables de funciones complejas como la memoria y el razonamiento. Los ganglios basales actúan en la coordinación del movimiento, mientras que el sistema límbico es responsable de las respuestas emocionales. Aunque ciertas zonas del cerebro se encargan de determinadas funciones, se trata de un sistema con alto grado de integración que se relaciona además con otras partes del encéfalo como el cerebelo encargado de coordinar secuencias complejas de movimientos iniciados por las áreas motoras y el tronco del encéfalo.

La función motora del cerebro se lleva a cabo principalmente a través de la vía piramidal o corticoespinal, un grupo de fibras nerviosas que parten de neuronas situadas en la corteza motora primaria situada en la parte posterior del lóbulo frontal y terminan en el asta anterior de la médula espinal, donde enlazan con una segunda neurona de la que parten axones que confluyen en los diferentes nervios motores que hacen posible el control voluntario de la musculatura de todo el cuerpo. La vía piramidal se cruza en la base del tronco del cerebro, en la llamada decusación de las pirámides, de tal forma que las fibras provenientes del hemisferio cerebral derecho controlan los músculos de la mitad izquierda y las del hemisferio cerebral izquierdo la mitad derecha. Esta vía es de gran importancia pues es la que permite realizar los movimientos necesarios para la mayor parte de las funciones vitales, entre ellas desplazarse, hablar, masticar, etc. Si se lesiona la vía piramidal se produce parálisis de los músculos correspondientes.

El dolor se define como una experiencia sensorial y emocional desagradable relacionada con un daño tisular real o potencial. Tiene la función de aviso o advertencia para informar de un peligro que se debe evitar, previniendo de está forma lesiones más graves.

La sensación de dolor se inicia en determinados receptores situados en los tejidos que reciben el nombre de nociceptores y son sensibles al daño tisular. Los impulsos nerviosos generados por estos receptores llegan a través de los nervios sensitivos hasta el asta posterior de la médula espinal, desde donde suben a través de un haz de fibras nerviosas llamado espino talámico hasta alcanzar el encéfalo. Llegan primero a la región del tálamo, desde donde alcanzan la corteza sensitiva del lóbulo temporal que es donde la señal se procesa y la sensación de dolor se hace consciente.

Existen diferentes enfermedades de origen congénito en las que las personas afectadas son incapaces de percibir el dolor. Este grupo de trastornos se conoce genéricamente como insensibilidad congénita al dolor, suele acompañarse de falta de sensibilidad a la temperatura y provoca importantes problemas de salud, entre ellos lesiones ósea o en la piel que pasan desapercibidas pues la persona no siente dolor alguno tras traumatismos graves y continua su actividad habitual sin percatarse de que ha sufrido una fractura ósea o una herida. 

En los lóbulos parietales se desarrolla el sistema emocional y el sistema valorativo. El sistema emocional –aunque compromete a todo el cerebro, y en retroalimentación, a todo el cuerpo del individuo– se ubica principalmente en el área bastante arcaica llamada sistema límbico, dentro del sistema límbico las dos amígdalas cerebrales, se focalizan las emociones básicas (temor, agresión, placer) que tenemos y que damos cuando algo o alguien interfiere en la actividad que esté haciendo en el exterior. Por otra parte está el sistema valorativo, este es la relación que existe entre los lóbulos prefrontales (que como su nombre lo indica está atrás de la frente) y las amígdalas cerebrales, esa relación "física" se llama hipocampo.

La gran mayoría de los procesos que permiten el lenguaje se llevan a cabo en diferentes áreas de asociación. Existen dos áreas bien identificadas, las cuales son consideradas vitales para la comunicación humana: el área de Wernicke y el área de Broca. Estas áreas están localizadas en el hemisferio dominante (que es el izquierdo en el 97% de las personas) y son consideradas las más importantes en cuanto a procesamiento de lenguaje. Esta es la razón por la cual el lenguaje es considerado como una función lateralizada. Sin embargo, el hemisferio no dominante también participa en el lenguaje, aunque existen cuestionamientos acerca del nivel de participación de las áreas localizadas en dicho hemisferio.

El área de Wernicke, se conoce así en honor al neurólogo que la describió por primera vez. Está especialmente desarrollada en el hemisferio dominante para el lenguaje, que, generalmente suele ser el lado izquierdo. El desarrollo de esta área permite alcanzar niveles altos de comprensión y procesar la mayor parte de las funciones intelectuales del cerebro. Se encarga de la decodificación de lo oído y de la preparación de posibles respuestas. Es importante para la comprensión de palabras y en los discursos significativos.

Da paso después al área de Broca, también conocida como el área motora de las palabras, que se conecta con el área de Wernicke mediante el fascículo longitudinal superior. Se ubica en la corteza prefrontal, en la parte anterior de la porción inferior de la corteza motora primaria, cercana a la fisura lateral (FL). En la mayoría de los casos, es dominante en el lado izquierdo del cerebro. Su función es permitir la realización de los patrones motores para la expresión de las palabras, articulando el lenguaje hablado y también el escrito. Es la responsable de la formación de las palabras en la que se activa el accionamiento de los músculos fonadores, es decir laríngeos, respiratorios y de la boca, para asegurar la producción de sonidos articulados, lo que tiene lugar en el área motora primaria, de donde parten las órdenes a los músculos fonadores. Además se conecta con el área motora suplementaria, que tiene relación con la iniciación del habla.

Aun cuando ambos hemisferios humanos son opuestos, no son la imagen geométrica invertida uno del otro. Desde un punto de vista puramente morfológico son asimétricos. Esta asimetría depende de una pauta de expresión génica también asimétrica durante el desarrollo embrionario del individuo y se ha comprobado que no es exclusiva de la especia humana, pues está presente, aunque en menor grado, en parientes cercanos en la filogenia al humano como puede ser el chimpancé.

El estudio de impresiones craneales de antepasados del género "Homo" tiene entre sus objetivos determinar la presencia o no de asimetría en el telencéfalo, puesto que es un rasgo de aumento de la especialización, de una capacidad cognitiva más compleja.

Las diferencias funcionales entre hemisferios son mínimas y solo en algunas pocas áreas se han podido encontrar diferencias en cuanto a funcionamiento, existiendo excepciones en personas que no se observaron diferencias. Se ha dicho que el lenguaje y la lógica (las áreas actualmente más conocidas especializadas en el lenguaje son la Broca y la de Wernicke, aunque al hacer un proceso lingüístico es probable que todo el cerebro esté involucrado —casi indudablemente las áreas de la memoria participan en el proceso del lenguaje—, las áreas de Broca y de Wernicke se encuentran en la mayoría de los individuos en el hemisferio izquierdo; por su parte las áreas más involucradas en la lógica y actividades intelectuales se ubican principalmente en el córtex prefrontal, teniendo quizás las áreas temporales izquierdas gran importancia para procesos de análisis y síntesis como los que permiten hacer cálculos (matemáticos) estas áreas dotan al individuo de mayor capacidad de adaptación al medio, pero con procesos de aprendizaje mucho más dilatados, y como tal más dependientes de sus progenitores durante la etapa de cría.

Se llama neurogénesis a la producción, diferenciación y migración de nuevas neuronas en el sistema nervioso. Hasta los años 60 del siglo XX se creía que era imposible que esto ocurriera en la vida adulta y se consideraba que las mismas neuronas que existían en el momento del nacimiento perduraban hasta la muerte sin incorporación de nuevas unidades. En la segunda mitad del siglo XX se publicaron varios estudios que contradecían este antiguo dogma de la biología. Actualmente está comprobado que en el cerebro humano y de los mamíferos existen dos áreas de neurogénesis adulta, el hipocampo y la zona ubicada por debajo de los ventrículos laterales del cerebro. Se ha observado que determinados procesos de aprendizaje dependientes del hipocampo como el aprendizaje espacial en un laberinto actúan como estimulantes del proceso de neurogénesis. 
Las células madre son las que dan origen a las nuevas neuronas, no obstante, la capacidad regenerativa del cerebro es muy escasa en comparación con otros tejidos del organismo.

La neuroplasticidad, es el proceso de modificación de la organización neuronal del encéfalo a resultas de la experiencia. El concepto se sustenta en la capacidad de modificación de la actividad de las neuronas, y como tal fue descrita por el neurocientífico polaco Jerzy Konorski. La capacidad de modificar el número de sinapsis, de conexiones neurona-neurona, o incluso del número de células, da lugar a la neuroplasticidad. Históricamente, la neurociencia concebía durante el siglo XX un esquema estático de las estructuras más antiguas del encéfalo así como de la neocorteza. No obstante, hoy día se sabe que las conexiones encefálicas varían a lo largo de la vida del adulto, así como es también posible la generación de nuevas neuronas en áreas relacionadas con la gestión de la memoria (hipocampo, giro dentado). Este dinamismo en algunas áreas del encéfalo del adulto responde a estímulos externos, e incluso alcanza a otras partes del encéfalo como el cerebelo.

Tres grupos de animales, con algunas excepciones, tienen cerebros notablemente complejos: los artrópodos (por ejemplo, los insectos y los crustáceos), los cefalópodos (pulpos, calamares y moluscos similares) y los craniados (vertebrados principalmente). El cerebro de los artrópodos y los cefalópodos surge desde un par de nervios paralelos que se extienden a lo largo del cuerpo del animal. El cerebro de los artrópodos tiene grandes "lóbulos ópticos" por detrás de cada ojo para el procesado visual y un cerebro central con tres divisiones.
En los insectos, el cerebro se puede dividir en cuatro partes: los "lóbulos ópticos", que localizados tras los ojos, procesan los estímulos visuales; el "protocerebro", que responde al olfato; el "deutocerebro", que recibe la información de los receptores táctiles de la cabeza y las antenas; y el "tritocerebro".

En los cefalópodos, el cerebro se divide en dos regiones separadas por el esófago del animal y conectadas por un par de lóbulos. Reciben el nombre de "masa supraesofágica" y "masa subesofágica".

El cerebro de los craniados se desarrolla desde la sección anterior de un único tubo nervioso dorsal, que más tarde se convierte en la médula espinal, luego la médula espinal (siempre evolutiva y filogenétiamente) habría veccionado (usando la terminología de Piaget o evolucionado complejificándose y transformándose sucesivamente en el puente de Varolio y el tronco encefálico; ya en los peces y, principalmente, en los tetrápodos primitivos (anfibios, reptiles) habría surgido el "cerebro límbico" (sistema límbico). Los craniados tienen el cerebro protegido por los huesos del neurocráneo. Los vertebrados se caracterizan por el aumento de la complejidad del córtex cerebral a medida que se sube por los árboles filogenético y evolutivo. El gran número de circunvoluciones que aparecen en el cerebro de los mamíferos es característico de animales con cerebros avanzados. Estas convoluciones surgieron de la evolución para proporcionar más área superficial (con más materia gris) al cerebro: el volumen se mantiene constante a la vez que aumenta el número de neuronas. Por ello, es la superficie, y no el volumen (absoluto ni relativo), lo que condiciona el nivel de inteligencia de una especie. Este es un error muy común que debe ser tenido en cuenta. No obstante, si comparásemos dos cerebros de la misma especie, podríamos aproximar que hay más posibilidades que el cerebro más grande de los dos tenga una mayor superficie, aunque tampoco esto es definitorio de la cualidad intelectiva cognitiva, sino que se considera como factor clave para mayores capacidades intelectivas y cognitivas a la "arquitectura" del cerebro: por ejemplo los "Homo neanderthalensis" podían tener cerebros tan voluminosos o más que los del "Homo sapiens" actual, pero la arquitectura cortical de sus cerebros estaba más dedicada a controlar sus fuertes musculaturas, mientras que en los Homo sapiens las áreas corticales más desarrolladas se ubican en las zonas dedicadas al lenguaje simbólico, y las áreas prefrontales y frontales -en especial del hemisferio izquierdo- en donde se realizan las síntesis que dan por resultado procesos elaborados de reflexión, cognición e intelección.






</doc>
<doc id="713" url="https://es.wikipedia.org/wiki?curid=713" title="Cerebelo">
Cerebelo

El cerebelo es una región del encéfalo cuya función principal es de integrar las vías sensitivas y las vías motoras. Existe una gran cantidad de haces nerviosos que conectan el cerebelo con otras estructuras encefálicas y con la médula espinal. El cerebelo integra toda la información recibida para precisar y controlar las órdenes que la corteza cerebral envía al aparato locomotor a través de las vías motoras. Es el regulador del temblor fisiológico. 

Por ello, lesiones a nivel del cerebelo no suelen causar parálisis pero sí desórdenes relacionados con la ejecución de movimientos precisos, mantenimiento del equilibrio, la postura y aprendizaje motor. Los primeros estudios realizados por fisiólogos en el siglo XVIII indicaban que aquellos pacientes con daño cerebelar mostraban problemas de coordinación motora y movimiento. Durante el siglo XIX comenzaron a realizarse los primeros experimentos funcionales, causando lesiones o ablaciones cerebelares en animales. Los fisiólogos observaban que tales lesiones generaban movimientos extraños y torpes, descoordinación y debilidad muscular. Estas observaciones y estudios llevaron a la conclusión de que el cerebelo era un órgano encargado del control de la motricidad. Sin embargo, las investigaciones modernas han mostrado que el cerebelo tiene un papel más amplio, estando así relacionado con ciertas funciones cognitivas como la atención y el procesamiento del lenguaje, la música, el aprendizaje y otros estímulos sensoriales temporales.

Fue descrito por primera vez por Herófilo en el siglo IV a. C.

El cerebelo es un órgano impar y medio, situado en la fosa craneal posterior, dorsal al tronco del encéfalo e inferior al lóbulo occipital y ocular.

Al igual que el resto del sistema nervioso central y la piel, el cerebelo deriva de la capa ectodérmica del disco germinativo trilaminar.

Durante las fases más tempranas del desarrollo embrionario, el tercio cefálico del tubo neural presenta tres dilataciones (vesículas encefálicas primarias) lo que nos permite dividirlo en tres segmentos distintos: prosencéfalo, mesencéfalo y rombencéfalo. El rombencéfalo es el segmento más caudal, y cuando el embrión tiene 5 semanas se divide en dos porciones: el metencéfalo, y el mielencéfalo. El metencéfalo es la porción más cefálica y dará lugar a la protuberancia (puente) y al cerebelo, mientras que del mielencéfalo se originará la médula oblongada (bulbo raquídeo). El límite entre estas dos porciones está marcado por la curvatura protuberencial.

Al igual que todas las estructuras que derivan del tubo neural, el metencéfalo está constituido por placas alares y basales separadas por el surco limitante. Las placas alares contienen núcleos sensitivos que se dividen en tres grupos: el grupo aferente somático lateral, el grupo aferente visceral especial y el grupo aferente visceral general. Las placas basales contienen núcleos motores que se dividen en tres grupos: el grupo eferente somático medial, el grupo eferente visceral especial y el grupo eferente visceral general.

Las porciones dorsolaterales de las placas alares se incurvan en sentido medial para formar los labios rómbicos. En la porción caudal del mesencéfalo, los labios rómbicos están muy separados, pero en la porción cefálica se aproximan a la línea media. Al ir profundizando el pliegue protuberencial, los labios rómbicos se comprimen en dirección cefalo-caudal y forman la placa cerebelosa. A las 12 semanas del desarrollo, en la placa cerebelosa se aprecia la existencia de tres porciones: el vermis, en la línea media, y dos hemisferios, a ambos lados. Al poco tiempo, una fisura transversal separa el nódulo del resto del vermis y los flóculos del resto de los hemisferios.

Inicialmente, la placa cerebelosa está compuesta por tres capas, que de profunda a superficial son: capa neuroepitelial, capa del manto y capa marginal. Aproximadamente a las 12 semanas del desarrollo, algunas células originadas en la capa neuroepitelial emigran hacia la zona más superficial de la capa marginal. Estas células conservan la capacidad de dividirse y empiezan a proliferar en la superficie donde acaban formando la capa granulosa externa. En el embrión de 6 meses, la capa granulosa externa comienza a diferenciarse en diversos tipos celulares que emigran hacia el interior para pasar entre las células de Purkinje y dar origen a la capa granular interna. La capa granulosa externa termina por quedarse sin células y da origen a la capa molecular. Las células en cesta y las células estrelladas provienen de células que proliferan en la sustancia blanca (capa marginal).

Los núcleos cerebelosos profundos, como el núcleo dentado, se sitúan en su posición definitiva antes del nacimiento mientras que la corteza del cerebelo alcanza su desarrollo completo después del nacimiento.


Contrario a la idea anatómica clásica; el cerebelo adulto no proviene únicamente del metencefalo. Los estudios de Hallonet y Nicole M. Le Douarin a principios de la década de los noventa mostraron que las células progenitoras del cerebelo provienen de la región caudal del mesensefalo y la rostral del metencéfalo. Para mostrarlo, crearon diferentes quimeras de pollo ("Gallus gullus") y codorniz ("Corurnir coturnir juponica"), con injertos de las regiones metencefalicas y mesensefálicas de interés. Debido a que las células de codorniz presentan un núcleo en interfase con heterocromatina condensada, estas células son fácilmente diferenciables de las células de pollo luego de una tinción de Feulgen (tiñe el DNA) (Ver enlace externo). Haciendo uso de esta metodología, Hallonet y Le Douarin mostraron que las células mediorostrales del cerebelo adulto provienen del área caudal del mesencéfalo, mientras que el resto de las células progenitoras del cerebelo tienen origen en el área rostral del metencefalo. Los autores hacen énfasis en el origen estrictamente metencefálico de las células de la capa granular externa (EGL), que dará lugar a las células granulares en etapas posteriores del desarrollo. Las demás células del cerebelo (células de Purkinje, por ejemplo) provienen de las vesículas mes- y metencefalicas.

Gao y Hatten querían mostar la potencialidad de las células progenitoras provenientes de la capa granular externa (EGL) y compararla con la potencialidad de las células progenitoras de la zona ventral (VZ). Para ello, aislaron células precursoras de estas zonas a partir de ratones E13, luego las implantaron en la capa granular externa de ratones postnatales y observaron los tipos celulares en los cuales se diferenciaban estas células. Se observó que las células progenitoras de la capa granular externa (EGL) eran unipotentes, produciendo únicamente células granulares. En contraste las células provenientes de la zona ventral se diferenciaron en neuronas de Purkinje, interneuronas, astroglia y células granulares, lo cual evidencia las restricciones que se dan durante el desarrollo dependiendo de los contextos espaciales y temporales en los cuales se desarrollan las células.


Una de las ventajas de la teoría evolutiva en la biología es la posibilidad de formulación de hipótesis en otros grupos de organismos a partir del conocimiento en un grupo particular. El cerebelo es ejemplo perfecto de lo anterior. Debido a la gran facilidad de obtener mutantes en organismos como Drosophila, muchos genes involucrados en la identidad de segmentos fueron identificados en la segunda mitad del siglo XX. Debido a que estos genes eran capaces de establecer la identidad antero-posterior de los segmentos en Drosophila, varios investigadores propusieron la hipótesis que los homólogos en mamíferos podrían controlar los patrones de desarrollo. Los genes candidatos eran En (Engrailed), wingless y genes Pax. Al buscar sus homólogos en vertebrados y analizar los mutantes se encontró una ruta muy fina del control de desarrollo espacial y temporal del cerebelo en ratones. 

Las mutaciones en el gen (-/-) generan un fenotipo que prácticamente no desarrolla cerebelo. Mientras que mutaciones en el gen En-2 generan un fenotipo menos severo, con daños en la formación de las estructuras foliares de los lóbulos cerebelosos. Mutantes condicionales para En-1 activados en el día E-9 cuya expresión de En-2 es normal, presentan fenotipos casi normales. Esto sugiere que En1 determina el ”Territorio” del cerebelo en etapas tempranas, mientras que En2 es requerido en estadios posteriores.Debido al efecto regulatorio de Wnt-1 (homólogo de wingless) y genes Pax sobre Engrailed, era predecible el fenotipo de mutantes para estos genes. Mutantes homocigotos de Wnt-1 mostraron la perdida completa del cerebelo, lo cual se correlaciona con la pérdida de expresión de En el “territorio cerebeloso”


La transición célula progenitora a neurona madura, implica una serie de cambios morfológicos y moleculares altamente regulada espacial y temporalmente. Estos cambios incluyen el arresto del ciclo mitótico, la formación de axón y dendritas, la expresión de proteínas específicas como proteínas canal, en algunos casos migraciones y finalmente el establecimiento de conectividad (sinapsis) con otras neuronas. A pesar de ser rutinas que incluyen la mayoría de estos procesos, distintos tipos celulares presentan sus programas en diferente orden. Por ejemplo, las células de Purkinje al igual que células del cortex cerebral migran justo después de salir del ciclo celular y forman conexiones axonales en etapas posteriores del desarrollo. Por el contrario, las células precursoras de células granulares inician el crecimiento axonal al salir del ciclo celular y posteriormente inician su migración a la capa interna (IGL).A continuación se muestran algunas características del desarrollo de las células granulares del cerebelo.


Los patrones de expresión génica durante el desarrollo de las células granulares, permite establecer cuatro etapas: La neurogenesis, el inicio de la diferenciación neuronal, el crecimiento axonal y migración y finalmente la formación de conexiones sinápticas. En la figura 1 se muestran los marcadores específicos de cada etapa.


El proceso de proliferación ocurre principalmente en la capa externa del EGL (oEGL) durante las tres primeras semanas postnatales en ratón. Los primeros estudios de proliferación in vitro mostraron que estas células tienen la capacidad proliferativa en ausencia de mitógenos, sugiriendo actividad autocrina en la regulación de la proliferación celular. Más recientemente se han mostrado algunas moléculas de señalización cuya relación con la proliferación es más clara. Wechsler y Scott de la universidad de Stanford mostraron la expresión de mensajeros de Shh en células de Purkinje a nivel somático y dendrítico, por su parte las células granulares expresaban el gen ptc (inhibidor de la ruta shh en ausencia de Shh) y los genes gli1/2 que codifican factores de transcripción corriente abajo en la cascada de señalización de Shh. Luego evaluaron el papel que juega Shh en la proliferación de las células granulares, encontrando que la presencia de este factor incrementa la proliferación de estas células 100 veces. Este efecto fue específico para células granulares (no se vieron incrementos significativos en la proliferación de células glia). Para dar validez biológica a los resultados in vitro, los investigadores inhibieron la actividad de Shh con la expresión de anticuerpos anti-Shh por parte de células de hibridoma inyectadas en los animales en el periodo postnatal inicial. Dichos experimentos causaron una notable disminución en el grosor de la capa granular externa (EGL), al igual que disminución en el número de células. Ello permite concluir el efecto causal de la señalización de Shh en el estadio proliferativo de las células granulares. La presencia de ptc2 en las células granulares es de relevancia, puesto que las células granulares con la ruta de señalización Shh activa no entran en la etapa de diferenciación celular, incluso mutantes para ptc generan meduloblastoma en ratones y en humanos. Por lo tanto, la actividad de Shh es esencial en etapas iniciales del desarrollo (proliferación) de las células granulares pero su inhibición y regulación posterior es necesaria para continuar el curso del desarrollo normal de estas células. Un artículo reciente sobre el tema, que habla sobre Shh y ATF5 en el control de la proliferación de células granulares puede ser consultado


Dando continuidad al proceso, las neuronas granulares deben terminar la proliferación celular inducida por agentes mitógenos como Shh. Sato y colaboradores mostraron el efecto antagónico de JASP1 sobre Shh a través de la modulación de la actividad de JNK.La activación de esta ruta de señalización por el factor de crecimiento fibroblástico FGF-2 produce una colocalización de JASP1 y las formas fosforiladas de JNK y ERK en la membrana celular, que posteriormente dará lugar a la inhibición de la actividad mitógena de Shh permitiendo salir del ciclo celular. Ello es evidenciado por un decrecimiento en la población de células positivas para el factor Ki67 (proliferación) y el aumento de células positivas p27-Kip1 (represor del ciclo celular) y BrdU.

Otro gen implicado en la interfase diferenciación-migración es el gen weaver. Mutantes para este gen tienen proliferación de las células precursoras granulares normal (GCPs), sin embargo estas células no pueden salir del ciclo celular y terminan muriendo. Estas células pueden expresar algunos marcadores neuronales como N-CAM, L1 y MAP-2, pero la expresión de genes tardíos como TAG-1 y astrotactina es eliminada. 


Las neuronas granulares inmaduras que inician la diferenciación celular, comienzan la formación de un axón con la forma característica de T (ubicado hacia lo que será la capa molecular). Este estadio del desarrollo es identificable por la presencia de TAG-1 en el axón en formación. Del otro extremo, se inician translocaciones sucesivas y discretas del núcleo; este proceso de migración desde el EGL hasta el IGL atravesando la capa de células de Purkinje (PCL) implica la interacción y contacto directo entre células gliales de Berman y las neuronas granuales. En 1988, a través de técnicas inmunológicas y de microscopía Edmonson y colaboradores descubrieron la proteína de membrana astrotactina (ASTN1), una glicoproteína de 100 kDa cuya función es estabilizar las uniones temporales entre la astroglia y las neuronas granulares. En este artículo se muestra como los mutantes weaver, mencionados en el apartado anterior no expresan esta proteína y paralelamente son incapaces de unirse a las células gliales de Bergman e iniciar la migración. 

Estudios recientes realizados por el grupo de la Dr.Hatten han demostrado la actividad no redundante de la ASTN2. Esta proteína fue descubierta a partir de análisis bioinformáticos de homología. Increíblemente (como la misma autora dice), esta proteína no es expuesta a la superficie celular como su homóloga ASTN1, y por lo tanto no puede tener una función directa en la adhesión neuron-glia. En una primera fase del estudio se mostró el control dinámico en la exocitosis endocitosis de vesículas con ASTN1, esta glicoproteína es exocitada en el área distal del proceso líder (proceso citoplasmático que define la dirección de migración) donde es requerido un punto de adhesión para aplicar las fuerzas que conducen la translocación somática. Una vez se ha dado este movimiento se requiere de ASTN1 en la nueva frontera de migración y la membrana con ASTN1 que se encuentra cerca al núcleo es endocitada para su posterior reciclaje. La ASTN2 interactúa físcamente con la ASTN1 y parece regular la cantidad de ASTN1 que es exportada a la membrana.

Además de las interacciones celulares glía-neurona, las células granulares deben establecer una polaridad que dé dirección a la migración y organizar los componentes motores que ejecutan el desplazamiento. Al respecto, Solecki y colaboradores han trabajado en el control de componentes citoesqueléticos en el proceso de migración. En primer lugar se ensambla una caja de microtúbulos alrededor del núcleo, ello es coordinado por el centrosoma. Los movimientos discretos del núcleo son precedidos por el avance del centrosoma en la dirección del proceso líder, lo cual es coordinado molecularmente por el complejo Par6 (actualmente se realizan estudios sobre GTPases que interactúan con el complejo Par6, que puedan contribuir en la explicación de la polaridad en la migración). Uno de los mecanismos moleculares encargados directamente en el movimiento es la activación motores actomiosínicos.


Terminada la migración, las neuronas se localizan en la capa granular interna, listas para el proceso que las convertirá en neuronas funcionales: Las conexiones sinápticas. Los axones con forma de T de la capa molecular dan origen a conexiones con las dendritas de las células de Purkinje, mientras que las fibras musgosas forman terminales nerviosas alrededor de los somas de las neuronas granulares (glomérulos sinápticos).Otro cambio que ocurre en la maduración de las células granulares es la expresión de la subunidad α6 del receptor GABA (Hay que recordar que la modulación electrofisiológica depende de los receptores canal activos, como el receptor GABA) y la expresión de la enzima deshidrogenasa de ácido glutámico (cataliza la descarboxilación del glutamato para sintetizar GABA). Piper y colaboradores ha identificado un factor de transcripción que gatilla la expresión de la subunidad α6 del receptor GABA en estas células, haciendo pensar que estos cambios en el desarrollo están controlados por cascadas divergentes (la activación de pocos factores de transcripción es responsable de un perfil de expresión genética muy distinto.En las figuras 2 y 3 se muestran cortes de cerebelo adulto, donde se puede identificar la capa granular, la capa decélulas de purkinje y la capa granular interna (IGL)después de la migración y establecimiento de conexiones sinápticas.

El cerebelo aparece en todos los vertebrados pero con diferente grado de desarrollo: muy reducido en peces, anfibios y aves, alcanza su máximo tamaño en los primates especialmente en el hombre.

El cerebelo se encuentra pegado a la pared posterior del tronco del encéfalo y está incluido dentro de un estuche osteofibroso -la celda cerebelosa o subtentorial- formado por una pared superior y otra inferior. La pared superior está constituida por una prolongación de la duramadre denominada tienda del cerebelo y la pared inferior la forman las fosas cerebelosas del hueso occipital recubiertas por la duramadre. Normalmente, el cerebelo de un varón adulto pesa unos 150 g y mide 10 cm de ancho, 5 cm de alto y 6 cm en sentido antero-posterior. En los niños la relación entre el volumen del cerebelo y del cerebro es de 1 a 20, mientras que en adultos es de 1 a 8.

El cerebelo está conformado por dos hemisferios separados por un vermis, tiene forma de cono truncado aplastado en sentido supero-inferior en el cual se pueden diferenciar tres caras: superior, inferior y anterior.

La cara superior tiene la forma de un tejido con dos vertientes laterales y está en contacto con la tienda del cerebelo. En la parte central, presenta una elevación alargada en sentido antero-posterior que recibe el nombre de vermis superior. A ambos lados del vermis superior se extienden dos superficie inclinadas y casi planas que constituyen las caras superiores de los hemisferios cerebelosos. La cara superior está separada de la cara inferior por el borde circunferencial del cerebelo. En una vista superior, el borde circunferencial presenta dos escotaduras: una anterior en relación con el tronco del encéfalo, y otra posterior en relación con la hoz del cerebelo. El borde circunferencial del cerebelo está recorrido longitudinalmente por una fisura profunda denominada fisura prima o surco primario.

La cara inferior está directamente apoyada sobre la duramadre que recubre las fosas cerebelosas. Muestra un amplio surco en la línea media denominado vallécula o cisura media que alberga la hoz del cerebelo y en cuyo fondo se encuentra el vermis inferior que es la continuación del superior. Lateralmente a la cisura media se localizan las caras inferiores de las hemisferios cerebelosos, que son convexas hacia abajo. En la parte más anterior y a ambos lados del vermis inferior, los hemisferios cerebelosos presentan una prominencia ovoidea denominada amígdala cerebelosa. Estas amígdalas guardan una estrecha relación con el bulbo raquídeo.

La cara anterior está íntimamente relacionada con la cara posterior del tronco del encéfalo y para poder verla es necesario seccionar los tres pares de pedúnculos que la unen a ella. Presenta una depresión central que se corresponde con el techo del [IV ventrículo] y está delimitada por los pedúnculos de ambos lados y por los velos medulares superior e inferior. Por encima de esta depresión asoma el extremo anterior del vermis superior o língula, y por debajo se ve el extremo anterior del vermis inferior o nódulo. A ambos lados del nódulo, y por debajo de los pedúnculos cerebelosos inferiores, hay unas prominencias denominadas flóculos. El nódulo y los folículos están unidos entre sí por el pedúnculo del floculo que, en parte, corre sobre el velo medular inferior.

Hay tres maneras diferentes de dividir el cerebelo: morfológicamente, filogenéticamente y funcionalmente.

Clásicamente se realiza una división morfológica que es meramente descriptiva de la superficie del cerebelo, y no tiene base funcional ni ontogénica ni ninguna aplicación en la práctica clínica.

La superficie del cerebelo se encuentra surcada por muchas fisuras transversales más o menos paralelas entre sí. Entre ellas hay dos que destacan por ser las más profundas y nos sirven para dividirlo en lóbulos. Una es la fisura prima o primaria que recorre la cara superior y la divide aproximadamente en dos mitades iguales, y la otra es la fisura posterolateral o dorsolateral que se localiza en la cara anterior en posición caudal respecto del nódulo y los flóculos.

Estas fisuras delimitan los tres lóbulos del cerebelo: el anterior, el posterior y el floculonodular. Cada uno de estos lóbulos incluye una porción que forma parte del vermis y otra que forma parte de los hemisferios cerebelosos. La porción del vermis que corresponde a cada lóbulo se subdivide en segmentos a los que, generalmente, se asocia un par de lobulillos situados en los hemisferios cerebelosos. La subdivisión dentro de cada uno de los lóbulos viene determinada por la existencia otras fisuras transversales de menor profundidad.

El lóbulo anterior se sitúa por delante de la fisura prima y abarca parte de la cara anterior y parte de la cara superior. Se subdivide en:




El lóbulo posterior se sitúa entre las fisuras prima y posterolateral y abarca parte de la cara superior y parte de la cara inferior. Se subdivide en:






El lóbulo floculonodular se sitúa por delante de la fisura posterolateral y como su propio nombre indica está formado por el nódulo (X) -que corresponde al vermis- y los flóculos (H X) -que corresponden a los hemisferios-, unidos por el pedúnculo del flóculo.

El término cuerpo del cerebelo se utiliza para denominar a la totalidad del cerebelo, a excepción del lóbulo floculonodular.

El vermis superior está constituido por la língula, el lobulillo central, el culmen, el declive y el folium. El vermis inferior está constituido por el túber, la pirámide, la úvula y el nódulo.

Algunos autores en vez de distinguir tres lóbulos distinguen cuatro: el anterior, el medio, el posterior y el floculonodular. La diferencia radica en que dividen al lóbulo posterior en dos mediante la fisura prepiramidal, de tal forma que por encima de ella se extiende el lóbulo medio y por debajo el lóbulo posterior.

Desde el punto de vista filogenético, el cerebelo puede dividirse en tres porciones: arquicerebelo, paleocerebelo y neocerebelo. Esta división es de gran interés porque cada una de las porciones posee cierta identidad funcional y clínica.

El arquicerebelo. Es la porción filogenéticamente más antigua y se corresponde con el lóbulo floculonodular. Surge durante el desarrollo filogenético al mismo tiempo que el aparato vestibular del oído interno. La mayoría de aferencias que recibe provienen de los núcleos vestibulares y se corresponde en gran medida con el vestíbulocerebelo. Tiene una función de equilibrio.

El paleocerebelo. Es más moderno que el arqueocerebelo y está integrado por la pirámide, la úvula, el lobulillo central con las alas, el culmen y el lobulillo cuadrangular. La mayoría de las aferencias que recibe provienen de la médula espinal y tiene cierta correspondencia con el espinocerebelo. Tiene una función de control postural.

El neocerebelo. Es la parte más moderna y está formado por la totalidad del lóbulo posterior a excepción de la pirámide y la úvula. La mayoría de las aferencias que recibe provienen de la corteza cerebral a través de los núcleos del puente y se identifica con el cerebrocerebelo. Tiene una función de coordinación motora (movimientos voluntarios).

Del mismo modo que la corteza somatosensitiva, la corteza motora, los ganglios basales, los núcleos rojos y la formación reticular poseen una representación topográfica de las diferentes partes del cuerpo, esto sucede también en el caso de la corteza cerebelosa. El tronco y el cuello así como las porciones proximales de las extremidades quedan situadas en la región perteneciente al vermis. En cambio, las regiones faciales y las porciones distales de las extremidades se localizan en las bandas paravermianas. Las porciones laterales de los hemisferios cerebelosos (cerebrocerebelo) al igual que el lóbulo floculonodular (vestibulocerebelo), no poseen una representación topográfica del cuerpo.

Estas representaciones topográficas reciben aferencias desde todas las porciones respectivas del cuerpo y también desde las áreas motoras correspondientes en la corteza cerebral y en el tronco del encéfalo. A su vez, devuelven señales motoras a las misma áreas respectivas de la corteza motora y también a las regiones topográficas oportunas del núcleo rojo y de la formación reticular en el tronco del encéfalo.

De una forma similar al cerebro, el cerebelo puede dividirse en sustancia gris y sustancia blanca. La sustancia gris se dispone en superficie, donde forma la corteza cerebelosa, y en el interior, donde constituye los núcleos profundos. La sustancia blanca se localiza en la parte interna, envolviendo por completo a los núcleos profundos.

La corteza cerebelosa tiene una superficie muy extensa, unos 500 cm² gracias a los numerosos pliegues o circunvoluciones ("folia cerebelli") predominantemente transversales que aumentan unas tres veces su área. Los abundantes surcos y fisuras le dan a la superficie vergosa y venosa un aspecto rugoso característico.

La corteza está conformada por multitud de unidades histofuncionales conocidas como laminillas cerebelosas. En un corte sagital de una circunvolución del cerebelo visto al microscopio, se puede observar que está integrada por multitud de microcircunvoluciones. Estas microcircunvoluciones son las laminillas cerebelosas, que están constituidas por una fina lámina de sustancia blanca recubierta de sustancia gris.

La sustancia gris periférica de la laminilla cerebelosa tiene un espesor de alrededor de 1 mm. Posee una estructura histológica, homogénea en todas sus regiones, constituida por tres capas en las que se distinguen siete tipos fundamentales de neuronas. Al igual que el resto del sistema nervioso, la corteza cerebelosa también posee células gliales y vasos sanguíneos.

En la corteza cerebelosa, de profundo a superficial, se puede distinguir las siguientes capas: capa de células granulares, capa media o de células de Purkinje y capa molecular o plexiforme.

La capa granular es la capa más profunda de la corteza cerebelosa y limita en su zona interna con la sustancia blanca. Debe su nombre a que en ella predominan un tipo de pequeñas neuronas intrínsecas denominadas granos o células granulares del cerebelo. Debido a las características tintoriales de los núcleos de estas células, la capa granular presenta un aspecto linfocitoide (basófilo), aunque de cuando en cuando se pueden apreciar unos pequeños espacios acelulares eosinófilos denominados islotes protoplásmicos. Tiene una anchura variable de 500 en la convexidad a 100 μm en el surco, siendo la capa de mayor espesor de la corteza cerebelosa.

La capa de las células de Purkinje está constituida por los somas de las células de Purkinje que se disponen en una formando una lámina monocelular. A pocos aumentos presenta una mayor densidad celular en la convexidad de la laminilla que en los surcos. Algunos autores no consideran que las células de Purkinje formen una capa definida y dividen la corteza cerebelosa solo en dos capas: granular y molecular.

La capa molecular recibe su nombre porque contiene principalmente prolongaciones celulares y pocos somas neuronales. Tiene un carácter tintorial eosinófilo (adquiere color rosáceo en los cortes teñidos con hematoxilina-eosina). Su espesor aproximado es de unos 300 a 400 μm y su superficie se halla cubierta por la piamadre.

Las neuronas de la corteza cerebelosa se clasifican en: neuronas principales o de proyección y las intrínsecas o interneuronas. Las principales son aquellas cuyos axones salen de la corteza para alcanzar los núcleos cerebelosos profundos o los núcleos vestibulares. Las intrínsecas son las que extienden sus axones exclusivamente por la corteza. También tenemos que tener en cuenta las fibras aferentes extrínsecas que llegan a la corteza, entre las que destacan las fibras musgosas y las trepadoras.

Las neuronas principales son las células de Purkinje cuya disposición, forma y tamaño son homogéneos en toda la corteza cerebelosa. Se ha calculado que en el cerebelo humano existen unos 30 millones de estas neuronas. Su soma tiene un diámetro de entre 40 y 80 μm. De la parte superior del cuerpo neuronal parte un grueso tronco dendrítico que se ramifica profusamente en ramas de primer, segundo y tercer orden, de forma que constituyen un denso árbol dendrítico característico de estas neuronas. Este árbol dendrítico se extiende por todo el espesor de la capa molecular, con la particularidad de que se arboriza prácticamente en un solo plano, perpendicular al eje transversal de la laminilla. De esta forma en secciones parasagitales se aprecia en toda su extensión las ramificaciones de estas neuronas, mientras que en secciones transversales se observa su arborización como unas pocas y estrechas ramas verticales. Las dendritas se hallan cubiertas de espinas, de modo que se ha calculado que cada célula de Purkinje puede tener de 30.000 a 60.000 espinas. De la parte inferior del soma se origina el axón que, cerca de su origen, se mieliniza, atraviesa la capa de células granulares y, tras emitir colaterales, ingresa en la sustancia blanca. Desde aquí los axones de las células de Purkinje se dirigen hacia los núcleos cerebelosos y vestibulares donde terminan. Las recurrentes axónicas vuelven a la capa de células de Purkinje en cuyas proximidades se arborizan formando los plexos supragangliónico e infragangliónico. Ultraestructuralmente, las células de Purkinje se caracterizan porque su soma muestra abundante retículo endoplásmico rugoso y un aparato de Golgi muy desarrollado. Tanto en el soma como en las dendritas y el axón aparecen frecuentemente cisternas membranosas aplanadas pertenecientes al retículo endoplásmico liso justo por debajo de las membrana (cisternas hipolemnales). Estas cisternas hipolemnales son características de este tipo celular, aunque puede hallarse algunas de ellas en otros tipos de neuronas de gran tamaño.

Las neuronas intrínsecas se distribuyen por las capas granular y molecular. En la capa granular se encuentran tres tipos de células: las células granulares, las grandes células estrelladas -células de Golgi y de Lugaro- y las células monodendríticas o monopolares en penacho. En la capa molecular se hallan las células estrelladas pequeñas -células estrelladas y células en cesta-.

Las células granulares o granos del cerebelo, son las neuronas de menor tamaño de todo el sistema nervioso humano y su soma mide de 5 a 8 μm de diámetro. Se hallan densamente empaquetadas en la capa granular. Son muy numerosas, calculándose que en el cerebelo humano hay unos 50.000 millones de estas neuronas. El soma no posee apenas grumos de Nissl y está ocupado casi por completo por el núcleo, que presenta cromatina densa, lo que provoca una gran cromofilia y es responsable del aspecto linfocitoide de la célula. Los cuerpos neuronales no están recubiertos de glía y se sitúan muy próximos entre sí pero sin presentar sinapsis. Del soma parten cuatro a seis dendritas cortas, de unos 30 μm de longitud, con un trayecto algo flexuoso y sin ramificaciones, que presentan en su interior neurotúbulos y neurofilamentos. Estas dendritas terminan en varias dilataciones que recuerdan a los dedos de una mano, que confluyen en los islotes protoplásmicos y mediante las cuales establece sinapsis con las fibras musgosas. Del soma, o de una de sus dendritas, parte el axón, amielínico en todo su trayecto, que asciende por la capa molecular siguiendo un trayecto ligeramente curvo. Una vez alcanzada la superficie de la capa molecular, el axón se ramifica en T dando origen a dos fibras denominadas fibras paralelas. Estas fibras paralelas llevan un trayecto transversal, es decir paralelo al eje de la laminilla y perpendicular a la arborización dendríticas de las células de Purkinje. Las fibras paralelas llegan a medir de 2 a 3 mm de longitud, lo que resulta extraordinario para una neurona con un soma tan pequeño. Normalmente, los granos más profundos son los que tienen los axones más gruesos y dan origen a las fibras paralelas más profundas. Mediante las fibras paralelas, las células granulares, hacen sinapsis "en passant" con las espinas dendríticas de las células de Purkinje, de forma que una sola células granular puede contactar con un número variable (50 a 100) de células de Purkinje y, a su vez, cada una de estas recibe impulsos de unas 200.000 a 300.000 fibras paralelas. Esta disposición recuerda a la de los postes y los cables de un tendido eléctrico. Además las fibras paralelas hacen también sinapsis "en passant" sobre las dendritas de las células de Golgi, las células en cesta y las estrelladas. Las células granulares reciben sus aferencias de las rosetas de las fibras musgosas y de los axones de las células de Golgi. Ambos tipos de terminales hacen sinapsis sobre las varicosidades digitiformes de las células granulares formando, en conjunto, lo que se denomina glomérulo cerebeloso.
Bajo el nombre de grandes células estrelladas se incluyen a todas aquellas neuronas, distintas de los granos y de las células monodendríticas en penacho, que se sitúan en la capa granular.

Las células de Golgi son de un tamaño algo menor a las células de Purkinje y su número es similar al de estas últimas neuronas. Su soma tiene forma estrellada y se halla preferentemente situado en la zona superficial de la capa de células granulares. Contiene abundantes grumos de Nissl y neurofibrillas, y un retículo endoplásmico liso y un aparto de Golgi casi tan ricos como los de la célula de Purkinje; en cambio, las cisternas hipolemnales son muy escasas. Presenta un núcleo escotado, con cromatina laxa y un prominente nucléolo excéntrico. Sus dendritas, en número de cuatro o cinco, parten en dirección horizontal o descendente, se incurvan y se dicotamizan adoptando en conjunto la forma de un ramillete no muy tupido, que se proyecta hacia la capa molecular. Las espinas dendríticas no son muy abundantes. A medida que nos alejamos del soma, las dendritas van disminuyen su contenido en orgánulos y en las regiones más distales solo hay haces de neurotúbulos y algo de retículo endoplásmico liso. A diferencia de la célula de Purkinje, el campo dendrítico de la célula de Golgi se dispone en las tres dimensiones y comprende un amplio territorio abarcando un área de unas 20 células de Purkinje. De la región basal de la célula o de uno de los troncos dendríticos principales parte un axón con forma de plexo ramificado, extraordinariamente denso, situado en la capa de células granulares. El plexo axónico de las células de Golgi presenta tres tipos básicos de arborización con una correspondencia funcional perfecta. En el primer tipo, el plexo axónico cubriría un campo similar al campo dendrítico; en el segundo tipo, el axón se extendería mucho más pero sin salirse de la laminilla; en el tercer tipo, se originan dos plexos, uno en la propia laminilla y otro en la vecina. El plexo axónico acaba en numerosos grupos de terminaciones arracimados que confluyen en los islotes protoplásmicos y hacen sinapsis con las dendritas de las células granulares. Las células de Golgi reciben sus aferencias de las fibras musgosas y las fibras trepadoras y, en menor proporción, de otras neuronas como las células granulares. Un tipo característico de sinapsis son las axo-somáticas formadas por una dilatación de las fibras musgosas que se incrusta en cuerpo de una célula de Golgi, quedando casi envuelta por su citoplasma.

Las células de Lugaro no son tan conocidas ni están tan estudiadas como otros tipos neuronales del cerebelo. Se caracterizan por tener un gran soma fusiforme localizado justo por debajo de la capa de células de Purkinje. Tienen largas dendritas opositopolares rectilíneas o en abanico, que se extienden siguiendo un plano transversal y cubriendo un campo que alberga 1 o 2 hileras completas de células de Purkinje. Su axón se bifurca en un amplio plexo arrosariado que se extiende desde la zona superior de la capa granular hasta la superficie de la capa molecular, dispuesto en un plano sagital.

Aparte de las células de Golgi y de Lugaro, hay otros tipos de células que también son grandes células estrelladas. Se trata de elementos aberrantes y, por lo tanto, muy infrecuentes y con escaso significado funcional. Son células de Golgi, células de Purkinje y neuronas de proyección de los núcleos profundos, en una situación ectópica. 

Las células monodendríticas en penacho son un nuevo tipo celular descrito recientemente. Se encuentran en la capa granular, presentan un soma esférico y un único tronco dendrítico que termina en una corta arborización en penacho.

Las células estrelladas pequeñas pueden ser superficiales (células estrelladas) o profundas (células en cesta).

Las células en cesta son un tipo especial de células estrelladas pequeñas a las que Cajal denominó "pequeñas estrelladas profundas". En el cerebelo humano, hay alrededor de 90 millones de células en cesta. Se caracterizan porque su soma tiene forma triangular o estrellada con unos 10 a 20 μm de diámetro y se sitúa en la mitad interna de la capa molecular justo por encima de las células de Purkinje. Tiene una núcleo lobulado y excéntrico, y su citoplasma posee unas pocos orgánulos concentradas en el polo opuesto al núcleo. Los grumos de Nissl y las cisternas hipolemnales son escasas, y el aparato de Golgi y el retículo endoplásmico liso están poco desarrollados. Sus dendritas pueden ser descendentes aunque lo normal es que asciendan hasta el tercio superior de la capa molecular, miden entre 100 y 200 μm de longitud, y se orientan en el mismo plano, aproximadamente, que las células de Purkinje. Las dendritas son rectilíneas, casi sin ramificaciones y con espinas, aunque mucho menos abundantes y más groseras que las de las células de Purkinje. Tienen abundantes neurotúbulos, neurofilamentos y retículo endoplásmico liso hasta en sus porciones más distales, y mitocondrias, retículo endoplásmico rugoso y aparato de Golgi en los principales tronco dendríticos. El axón, que puede alcanzar 1 mm de longitud, tras recorrer un trayecto horizontal en el plano sagital, aumenta de calibre, emite colaterales a la capa molecular y finaliza en una serie de terminales que rodean los somas de las células de Purkinje estableciendo numerosos contactos sinápticos. Estos terminales axónicos forman una especie de cesta -por lo que estas neuronas reciben su característico nombre- confluyendo sus extremos en la base del soma de la célula de Purkinje donde forman un pincel que rodea el segmento inicial del axón. Cada axón de una célula en cesta puede dar origen a unas diez cestas perisomáticas, mientras que varias células en cesta contribuyen a formar los nidos pericelulares de una célula de Purkinje. En contraposición a las otras neuronas del cerebelo, los campos axónicos de las células en cesta presentan una notable superposición. Las aferencias de las células en cesta provienen principalmente de las fibras trepadoras y paralelas, así como de células estrelladas, de colaterales del plexo supragangliónico de las células de Purkinje y de otras células en cesta.

Dentro de las células estrelladas se distinguen varios tipos diferentes, aunque su morfología general es esencialmente similar en todas ellas. Su soma es estrellado o poligonal y se sitúa en la parte externa de la capa molecular. Tiene un núcleo con cromatina laxa y un citoplasma con escasos orgánulos. Su axón, después de un tramo inicial de 5 a 6 μm de longitud, se ramifica cerca del soma formando un plexo que termina haciendo sinapsis sobre diferentes zonas de la célula de Purkinje y sobre otras interneuronas. Sus dendritas se originan de cinco o seis troncos principales y se ramifican en el plano transversal formando un plexo varicoso provisto de espinas que se extiende por la capa molecular recibiendo sinapsis de las fibras paralelas y trepadoras además de otras células estrelladas y de células en cesta. Además hay otras células estrelladas que son algo más grandes y presentan un aspecto muy similar al de las células en cesta llegando a participar en la formación de las cestas perisomáticas aunque sin formar parte del pincel.

Las fibras extrínsecas son los axones mielínicos aferentes que alcanzan la corteza cerebelosa desde otras regiones del sistema nervioso central. Las más importantes son las fibras musgosas y las trepadoras.

Las fibras musgosas son gruesas fibras mielínicas que proceden de numerosas áreas del sistema nervioso como son el ganglio y núcleos vestibulares, la médula espinal, la formación reticular y los núcleos del puente. A través de estas fibras el cerebelo recibe información procedente de, prácticamente, todo el sistema nervioso incluida la corteza cerebral. Entran principalmente por los pedúnculos cerebelosos medio y superior, y dan colaterales para los núcleos profundos, distribuyéndose a continuación por toda la corteza cerebelosa. Las fibras musgosas al llegar a la capa granular siguen un trayecto tortuoso y se dividen en varias ramas que presentan dilataciones arborizadas y varicosas parecidas al musgo y denominadas rosetas o rosáceas. Cada fibra musgosa da origen a unas 20 rosetas que se localizan tanto en el curso de la fibra como en sus terminaciones y bifurcaciones. Estas rosetas hacen sinapsis sobre las dilataciones digitiformes de las células granulares y los axones de las células de Golgi, formando los denominados glomérulos cerebelosos. Además hacen sinapsis con el soma de las células de Golgi.

Las fibras musgosas son gruesas, con abundantes neurotúbulos, neurofilamentos y mitocondrias. Están envueltas en una gruesa vaina de mielina en cuyos nodos de Ranvier se localizan las rosetas.

Las fibras trepadoras son los axones de las neuronas de proyección del núcleo olivar inferior desde donde penetran en el cerebelo por el pedúnculo inferior. Una única neurona del núcleo olivar inferior da origen a unas diez fibras trepadoras. Tienen menor diámetro que las musgosas. Al llegar al cerebelo, estas fibras dan colaterales para los núcleos profundos y luego se distribuyen por toda la corteza cerebelosa donde pierden la mielina. Penetran en la capa granular en línea recta y sin varicosidades dando una o dos colaterales. Alcanzar la capa de células de Purkinje donde cada fibra se superpone a varias células de Purkinje ascendiendo sobre ellas a la vez que se ramifica. Hay una fibra trepadora por cada 5 a 10 células de Purkinje que realiza unas 300 sinapsis con cada neurona. El destino de las colaterales de la capa granular son las dendritas y los somas de las células de Golgi.

Las fibras trepadoras en su porción más distal se hacen finas y amielínicas, con algunos neurofilamentos, pocas mitocondrias y abundantes sinapsis "en passant" con las dendritas de las células de Purkinje. También presentan unos botones muy densos y repletos de vesículas redondeadas que demuestran la existencia de sinapsis entre estas fibras y las dendritas de las células estrelladas y las células en cesta.

Además de las musgosas y las trepadoras, la corteza cerebelosa recibe otras fibras nerviosas aferentes entre las que destacan las procedentes de locus caeruleus, que son noradrenérgicas y se distribuyen por las tres capas, y las que se originan en los núcleos del rafe, que envían serotonina a la capa de células granulares y a la capa molecular.

En la corteza cerebelosa predominan los astrocitos protoplásmicos entre los que destaca un tipo peculiar de astrocito denominado glia de Bergmann. El soma de esta célula tiene forma irregular y se halla entre las células de Purkinje desde donde parten de dos a tres prolongaciones con gruesas excrecencias protoplásmicas que se extienden por toda la capa molecular y alcanzan la piamadre. Una vez alcanzada la piamadre se adosan a ella mediante unos ensanchamientos que forman la capa limitante de Cajal. Otro tipo especial de astrocitos son las células de Fañanás cuyos somas se sitúan en la capa molecular y sus expansiones no alcanzan la piamadre. Tanto las células de Fañanás como la glia de Bergmann no presentan ninguna pecularidad ultraestructural, expresando ambas positividad para el anticuerpo de la proteína gliofibrilar ácida (GFAP).

En la capa granular se pueden observar astrocitos protoplasmáticos que no aíslan todas las neuronas y que parecen formar círculos alrededor de los glomérulos cerebelosos. Así mismo existen oligodendrocitos en la capa molecular pero no en la granular.

En el interior de la sustancia blanca podemos encontrar 4 pares de núcleos de sustancia gris, que de medial a lateral son: el núcleo del fastigio (o del techo), el globoso, el emboliforme y el dentado. El emboliforme y el globoso está muy relacionados funcionalmente y en conjunto forman el núcleo interpuesto. Los núcleos vestibulares del bulbo raquídeo también funcionan en ciertos aspectos como si fueran núcleos cerebelosos profundos debido a sus conexiones directas con la corteza del lóbulo floculonodular.

El núcleo del fastigio es una masa gruesa con forma de cometa, ubicada casi en la línea media, justo por encima del techo del IV ventrículo del cual está separado por una delgada capa de sustancia blanca. El núcleo globoso es alargado en sentido anteroposterior y se sitúa entre el núcleo del fastigio y el emboliforme. El núcleo emboliforme tiene forma de coma, con la parte gruesa dirigida hacia delante y se sitúa junto al hilio del núcleo dentado.

El núcleo dentado es el de mayor tamaño y se ha calculado que tiene unas 250.000 neuronas. Es de color gris amarillento y tiene forma de bolsa con pliegues abierta hacia delante y hacia la línea media. La abertura se denomina hilio del núcleo dentado y por él salen la mayor parte de las fibras que forman el pedúnculo cerebeloso superior. En el núcleo dentado se distinguen al menos dos tipos de neuronas: las grandes o de proyección y las pequeñas o interneuronas. Pero los circuitos sinápticos de este núcleo no están claramente establecidos. Tanto las neuronas de proyección como las interneuronas tienen prolongaciones no muy numerosas, largas y poco ramificadas, que les dan un aspecto general estrellado.

El núcleo dentado, como el resto de los núcleos cerebelosos, además de recibir colaterales de fibras que desde otros centros nerviosos llegan al cerebelo, reciben los axones de las células de Purkinje. Cada uno de estos axones finaliza en un dilatado plexo terminal sobre unas 30 neuronas de los núcleos cerebelosos. Los axones de las neuronas de proyección se dirigen a través de los pedúnculos hacia centros nerviosos específicos. No hay conexiones directas de la corteza cerebelosa con el exterior, excepto por algunos axones de las células de Purkinje que alcanzan directamente los núcleos vestibulares.

En un corte sagital del cerebelo, la sustancia blanca adopta una disposición arborescente por lo que a veces se la conoce como árbol de la vida del cerebelo o "arbor vitae". Está formada por una masa voluminosa central, denominada cuerpo o centro medular, de la que parten prolongaciones hacia las circunvoluciones del cerebelo denominadas láminas blancas. El cuerpo medular se continúa hacia delante directamente con los pedúnculos, que también están constituidos de sustancia blanca.

Desde el punto de vista histológico, la sustancia blanca del cerebelo está constituida por axones junto con astrocitos fibrosos y abundantes oligodendrocitos productores de la envoltura mielínica. Los axones de la sustancia blanca son tanto fibras eferentes y aferentes como fibras intrínsecas que conectan diferentes áreas corticales entre sí. Las fibras aferentes de la corteza corresponden a axones de las células de Purkinje mientras que las de los núcleos profundos corresponden a axones de las neuronas de proyección de dichos núcleos. Las aferencias corresponden a las fibras musgosas, las trepadoras y las que provienen de los sistemas noradrenérgico y serotoninérgico. Entre las fibras intrínsecas o propias se distinguen dos tipos: las fibras comisurales y las arqueadas o de asociación. Las comisurales cruzan la línea media y conectan las mitades opuestas del cerebelo mientras que las arquedas conectan circunvaluciones cerebelosas adyacentes entre sí.
La sustancia gris (o materia gris) corresponde a aquellas zonas del sistema nervioso central de color grisáceo integradas principalmente por somas neuronales y dendritas carentes de mielina junto con células gliales (neuroglia). En la médula espinal se aprecia en su centro y hacia los laterales, en forma de mariposa o letra H, mientras que en el cerebro ocupa la zona externa, con excepción de los internos ganglios basales que sirven como estaciones de relevo. En el cerebro se dispone en su superficie y forma la corteza cerebral, que corresponde a la organización más compleja de todo el sistema nervioso.

Al cerebelo llegan aferencias de todas las vías motoras y de todas las sensitivas, incluyendo la olfatoria y de él parten eferencias para controlar todas las vías motoras descendentes. Las eferencias no suelen hacer sinapsis directamente sobre las motoneuronas de la vía final común excepto en las de los músculos extrínsecos del globo ocular. Las eferencias normalmente actúan sobre los núcleos motores del tronco del encéfalo. El número de fibras aferentes cerebelosas es más de 40 veces superior al de fibras eferentes. Todas las conexiones del cerebelo pasan por los pedúnculos.

A continuación se expondrán las principales conexiones que establece el cerebelo ordenadas siguiendo su división funcional. Hay que tener en cuenta que las fibras aferentes, al contrario que las eferentes, no terminan sobre la corteza cerebelosa siguiendo de manera estricta la división funcional.

Mayoritariamente provienen del sistema vestibular mediante dos tractos: el vestibulocerebeloso directo o de Edinger y el vestíbulocerebeloso indirecto. También recibe algunas fibras del tracto corticopónticocerebeloso que provienen de la corteza visual del lóbulo occipital (fibras occipitopónticocerebelosas).

El tracto vestibulocerebeloso directo o de Edinger está formado por los axones de las neuronas localizadas en el ganglio vestibular o de Scarpa, que llegan preferentemente al nódulo y algunas a la banda vermiana. No pasa por los núcleos vestibulares, no se decusa en su trayecto y entra directamente por el pedúnculo inferior. Transmite información sobre la posición de la cabeza y las aceleraciones lineales y angulares que sufre el cuerpo.

El tracto vestibulocerebeloso indirecto está formado por los axones de las neuronas asentadas en los núcleos vestibulares superior y medial, que van a terminar en los flóculos y, en menor medida, en la banda vermiana. No se decusa en su trayecto y entra por el pedúnculo inferior. Transmite información sobre la posición de la cabeza y las aceleraciones lineales y angulares que sufre el cuerpo.

Los principales tractos de fibras que parten del vestíbulocerebelo son: el cerebelovestibular, el floculooculomotor y el uncinado de Russell.

El tracto cerebelovestibular está formado por fibras directas y cruzadas que se origina en los flóculos y que salen del cerebelo por el pedúnculo inferior para alcanzar los núcleos vestibulares medial y lateral. Regula la actividad de los tractos vestibuloespinales medial y lateral.

El tracto floculoculomotor se origina en los flóculos, se decusa en pleno cerebelo, sale por el pedúnculo superior y asciende por el tronco del encéfalo hasta llegar al núcleo del nervio oculomotor (o motor ocular común). Controla los movimientos del globo ocular.

El tracto uncinado de Russell se origina en los flóculos, se cruza y se dirige cranealmente hacia el pedúnculo cerebeloso superior. Pero antes de alcanzar ese pedúnculo, cambia bruscamente de dirección formando una especie de gancho y termina saliendo por el inferior. Acaba en los núcleos vestibulares. En su trayecto en el cerebelo emite colaterales que salen por el pedúnculo superior y alcanzan los núcleos de los nervios motores oculares, la formación reticular y el hipotálamo. Controla los movimientos del globo ocular y la actividad de los tractos vestíbuloespinales.

Las aferencias del espinocerebelo proceden de tres zonas del neuroeje: la médula espinal, el bulbo raquídeo y el mesencéfalo.

A nivel de la médula espinal las aferencias llegan por medio de los tractos espinocerebelosos posterior y anterior. Estos tractos son capaces de transmitir impulsos nerviosos más rápido que cualquier otra vía del SNC alcanzando una velocidad de 120 m/s. Esta rapidez es necesaria para que llegue al cerebelo la información sobre los cambios ocurridos en los grupos musculares periféricos y poder coordinarlos a tiempo.

El tracto espinocerebeloso anterior (ventral) o de Gowers se origina en la médula, en neuronas que se asientan en la zona lateral de la base del asta posterior, entre los últimos segmentos lumbares y los sacrococcígeos. Algunas de sus fibras cruzan la comisura gris para ascender por el cordón lateral del lado contrario, donde se sitúa próximo a la superficie medular. Las pocas fibras que no se cruzan ascienden por el cordón lateral del mismo lado. Todas sus fibras atraviesan el bulbo y el puente, y llegan hasta la zona más caudal del mesencéfalo donde cambian bruscamente de dirección para entrar al cerebelo por el pedúnculo superior. Alcanza el vermis y las bandas paravermianas de ambos lados. Transmite información propioceptiva inconsciente y exterioceptiva de la extremidad inferior.

El tracto espinocerebeloso posterior (dorsal) o de Flechsing está formado por axones de neuronas cuyo soma se localiza en la columna torácica o núcleo de Stilling-Clarke. Asciende por el cordón lateral pegado a la superficie y justo por detrás del tracto espinocerebeloso anterior. Al alcanzar el bulbo penetra en el cerebelo por el pedúnculo inferior y llega hasta el vermis y la banda paravermiana del mismo lado de su origen. Transmite información propioceptiva inconsciente y exteroceptiva procedente del tronco y la extremidad inferior.

A nivel del bulbo raquídeo las aferencias llegan por medio de los tractos cuneocerebeloso, olivocerebeloso y reticulocerebeloso.

El tracto cuneocerebeloso está formado por los axones de las neuronas que asientan en el núcleo cuneiforme accesorio (fibras arqueadas externas posteriores). Asciende por el bulbo raquídeo sin decusarse y mezclado con el tracto espinocerebeloso posterior. Entra por el pedúnculo cerebeloso inferior y acaba en el vermis y en la banda paravermiana del mismo lado. Transmite la sensibilidad propioceptiva inconsciente y exteroceptiva de la mitad superior del cuerpo.

El tracto olivocerebeloso es la conexión más importante que se establece entre bulbo raquídeo y cerebelo. Está formado por axones de las neuronas del núcleo olivar inferior y de los núcleos olivares accesorios. Estos núcleos reciben información somatoestésica, visual y de la corteza cerebral además de recibir aferencias vestibulares y del propio cerebelo. Al poco de originarse, el tracto olivocerebeloso se decusa totalmente y entra en el cerebelo por el pedúnculo inferior. Termina proporcionando fibras trepadoras para toda la corteza cerebelosa. Transmite al cerebelo la información recibida por los núcleos olivares.

El tracto reticulocerebeloso está formado por axones de neuronas localizadas en la formación reticular bulbar y póntica. Parte de las fibras se cruzan y otra parte van directas. Entra por el pedúnculo cerebeloso inferior y alcanza principalmente el espinocerebelo aunque también manda algunas fibras para el cerebrocerebelo. Transmite información compleja, tanto de la periferia como de la corteza cerebral y otras partes del sistema nervioso central.

A nivel del mesencéfalo las aferencias llegan por medio de los tractos tectocerebeloso, trigeminocerebeloso y rubrocerebeloso.

El tracto tectocerebeloso está formado por los axones de las neuronas de los tubérculo cuadrigéminos superiores e inferiores. Entran en el cerebelo a través del pedúnculo superior del mismo lado y terminan en la parte media del vermis. Transmite información visual y acústica proveniente de la corteza cerebral.

El tracto trigeminocerebeloso está formado por axones de neuronas del núcleo mesencefálico del nervio trigémino que entran al cerebelo a través del pedúnculo superior sin decusarse por el camino. Terminan en el vermis y en la banda vermiana del mismo lado de su origen. Transmite información propioceptiva del macizo craneofacial.

El tracto rubrocerebeloso está formado por axones de neuronas asentadas la porción parvocelular del núcleo rojo que se decusan en su totalidad antes de alcanzar el cerebelo por el pedúnculo superior.

Las principales referencias que parten del espinocerebelo son: el tracto interpuestorreticular, el tracto interpuestoolivar, el tracto interpuestotectal y el tracto interpuestorrúbrico.

El tracto interpuestorreticular se origina en el núcleo interpuesto, sus fibras se decusan parcialmente y salen del cerebelo por los pedúnculos inferiores para alcanzar los núcleos de la formación reticular.

El tracto interpuestoolivar sale por el pedúnculo cerebeloso superior, se decusa en su totalidad a nivel del mesencéfalo y desciende por el tronco del encéfalo para alcanzar el núcleo olivar inferior.

El tracto interpuestotectal se decusa parcialmente antes de salir por el pedúnculo cerebeloso superior y ascender por el tronco del encéfalo hasta alcanzar los tubérculos cuadrigéminos superior e inferiores.

El tracto interpuestorrúbrico es la eferencia más importante del espinocerebelo y principal vía de descarga del núcleo interpuesto. Las fibras que lo conforman salen del cerebelo por el pedúnculo superior, se decusan en su totalidad en el mesencéfalo y alcanzan el núcleo rojo contralateral. Desde el núcleo rojo parten axones hacia el núcleo ventral intermedio del tálamo que, a su vez, envía axones para la corteza cerebral motora y sensorial. Controla la actividad de las vías motoras que descienden hasta la médula espinal.

Todas las aferencias que recibe el cerebrocerebelo forman parte del tracto corticoponticocerebeloso. Este tracto se origina en una amplia zona de la corteza cerebral que abarca los lóbulos frontal, parietal, occipital y temporal, y antes de entrar en el cerebelo hace sinapsis en los núcleos del puente.

La mayoría de las fibras que van desde la corteza hacia los núcleos del puente son colaterales de axones que se dirigen hacia otras zonas del encéfalo o hacia la médula espinal y cuyo cuerpo neuronal se sitúa en la capa V del cortex cerebral. Estas fibras se pueden dividir, según su origen, en: frontopónticas, parietopónticas, occipitopónticas y temporopónticas.

Las fibras frontopónticas se originan en las cortezas motora y premotora, y pasan por el brazo anterior de la cápsula interna. En el mesencéfalo, discurren por la base de los pedúnculos cerebrales medialmente al tracto corticonuclear. Terminan en los núcleos del puente más mediales.

Las fibras parietopónticas se originan en las áreas somatosensitivas primaria y secundaria y en áreas visuales. Pasan por el brazo posterior de la cápsula interna y luego por la base de los pedúnculos cereberales lateralmente al tracto corticoespinal. Terminan en los núcleos del puente más laterales.

Las fibras occipitopónticas se originan en áreas secundarias relacionadas con el procesamiento de estímulos visuales del movimiento (corriente magnocelular de la vía óptica). Pasan por la porción retrolenticular de la cápsula interna y luego por la base de los pedúnculos cereberales lateralmente al tracto corticoespinal. Terminan en los núcleos del puente más laterales.

Las fibras temporopónticas pasan por la porción sublenticular de la cápsula interna y a nivel del mesencéfalo se colocan lateralmente al tracto corticoespinal. Termina en los núcleos del puente más laterales.

Las fibras que van desde los núcleos del puente al cerebelo (fibras pontocerebelosas) siguen un trayecto horizontal por la protuberancia, se decusan y entran por el pedúnculo medio. Terminan en la corteza de los hemisferios y en el núcleo globoso.

La mayoría de las eferencias del cerebrocerebelo salen por el tracto dentadotalámico. Este tracto está formado por los axones de las neuronas localizadas en el núcleo dentado, que salen del cerebelo por el pedúnculo superior. Se decusan en la porción caudal del mesencéfalo (decusación de Wernekink) y terminan en el núcleo ventral intermedio del tálamo. Desde el tálamo parten fibras tálamocorticales que alcanzan las misma áreas de la corteza cerebral de las que partieron las aferencias corticoponticocerebelosas.

Existe un grupo de fibras denominadas dentadorrúbricas, que partiendo del núcleo dentado salen por el pedúnculo cerebeloso superior, se decusan y alcanzan el núcleo rojo contralateral.

El cerebelo, al igual que otras partes del SNC, recibe fibras de los sistemas neuroquímicos moduladores. Concretamente de dos de los sistemas monoaminérgicos: el noradrenégico y el serotoninérgico.

El sistema noradrenérgico manda el tracto caeruleocerebeloso desde el grupo A6 (que coincide con el locus caeruleus) hacia el cerebelo. Este tracto penetra por el pedúnculo superior y termina distribuido por todos los núcleos y la corteza. Sus fibras no se comportan como fibras musgosas ni como trepadoras sino como proyecciones difusas.

El tracto serotoninérgico cerebeloso se origina en los grupos B5 y B6, entra por el pedúnculo medio y termina distribuido por todos los núcleos y la corteza. Sus fibras acaban en proyecciones difusas.

El cerebelo se fija a la cara posterior del tronco del encéfalo mediante 3 pares de pedúnculos por los que discurren todas las fibras nerviosas que entran y salen de él. Hay dos pedúnculos inferiores, dos pedúnculos medios y dos pedúnculos superiores.

Los pedúnculos cerebelosos inferiores o cuerpos restiformes conectan el cerebelo con la parte superior del bulbo raquídeo. Entre ellos se extiende el velo medular inferior. Por ellos entran las fibras del tracto espinocerebeloso dorsal, las del tracto cuneocerebeloso, las de los tractos vestibulocerebelosos, las del tracto reticulocerebeloso y las fibras trepadoras provenientes del núcleo olivar inferior y accesorios (tracto olivocerebeloso). A través de ellos salen las fibras del tracto cerebelovestibular, las del tracto uncinado de Russell y las del tracto interpuestorreticular.

Los pedúnculos cerebelosos medios o pontinos conectan el cerebelo con la protuberancia o puente. Son los más grandes y están separados de los pedúnculos superiores por el surco interpeduncular. Constituyen las caras laterales de la protuberancia. Por ellos entran las fibras del tracto corticopontocerebeloso y las del tracto serotoninérgico cerebeloso. A través de ellos no salen fibras eferentes importantes.

Las fibras de los pedúnculos medios se organizan en tres fascículos: superior, inferior y profundo.

El fascículo superior, el más superficial, deriva de las fibras transversales superiores de la protuberancia. Se dirige dorsal y lateralmente, cruzando superficialmente a los otros dos fascículos. Se distribuye principalmente por los lobulillos de la cara inferior de los hemisferios cerebelosos y por las porciones adyacentes de la cara superior.

El fascículo inferior está constituido por las fibras transversales inferiores de la protuberancia. Pasa profundamente al fascículo superior y se continúa hacia atrás y hacia abajo más o menos paralelo a él. Se distribuye por los lobulillos de la cara inferior en las porciones cercanas al vermis.

El fascículo profundo incluye la mayor parte de las fibras transversas profundas de la protuberancia. En sus primeros tramos está cubierta por los fascículos inferior y superior, pero termina por cruzarse oblicuamente y aparece al lado medial del fascículo superior, de quien recibe un paquete de fibras. Sus fibras se disgregan y acaban en los lobulillos de la parte anterior del cara superior. Las fibras de este fascículo cubren a las del cuerpo restiforme.

Los pedúnculos cerebelosos superiores conectan el cerebelo con el mesencéfalo. Entre estos dos pedúnculos se extiende el velo medular superior. Por ellos entran las fibras del tracto espinocerebeloso ventral, las del tracto tectocerebeloso, las del tracto trigeminocerebeloso, las del tracto rubrocerebeloso y las del tracto caeruleocerebeloso. A través de ellos salen las fibras del tracto floculooculomotor, las del interpuestoolivar, las del interpuestorrúbrico, las del interpuestotectal, las del tracto dentadotalámico, las dentadorrúbricas y las colaterales del uncinado de Russell.

Hay tres pares de arterias principales que irrigan el cerebelo: las arterias cerebelosas superiores (SCA), las arterias cerebelosas inferoanteriores (AICA) y las arterias cerebelosas inferoposteriores (PICA).

Se origina de la arteria basilar justo por debajo del lugar donde esta se divide en sus dos ramas terminales. Se dirige lateralmente y hacia atrás contorneando el pedúnculo cerebeloso correspondiente, a la altura del surco pontomesencefálico. Pasa inmediatamente por debajo del nervio motor ocular común (III) y atraviesa la cisterna ambiens acompañando al nervio troclear (IV). Sus ramas terminales discurren por la piamadre, entre la tienda del cerebelo y la cara superior del cerebelo. Se anastomosa con las arterias cerebelosas inferiores. Irriga la corteza cerebelosa de la cara superior y los núcleos profundos, así como los pedúnculos cerebeloso superiores y medios.

Cuando contornea el mesencéfalo, la arteria cerebelosa superior da la arteria romboidal que sigue el pedúnculo cerebeloso superior y penetra en el interior del cerebelo para irrigar a los núcleos profundos. También da varias ramas colaterales que llegan hasta la glándula pineal, el velo medular superior y la tela coroidea del III ventrículo.

Se origina de la arteria basilar justo por encima del lugar donde esta se forma por la unión de las dos arterias vertebrales. Se dirige lateralmente y hacia atrás, contorneando la cara lateral del puente justo por debajo del origen aparente del nervio trigémino (V). Sigue su trayecto por el borde inferior del pedúnculo cerebeloso medio. Irriga la porción anterior de la cara inferior del cerebelo, así como los nervios facial (VII) y vestibulococlear (VIII). Sus ramas terminales se anastomosan con las de las arterias cerebelosas inferoposterior y superior.

En algunas personas, la arteria cerebelosa inferior emite la arteria laberíntica o auditiva interna (en otras personas la arteria laberíntica se origina en la arteria basilar). Esta rama acompaña al nervio vestíbulococlear (VIII) a través del conducto auditivo interno hasta alcanzar el oído medio.

Se origina de las arterias vertebrales justo por debajo del lugar donde estas se unen para formar la arteria basilar. Se dirige hacia atrás rodeando la parte superior del bulbo raquídeo y pasando entre el origen del nervio vago (X) y el nervio accesorio (XI). Sigue su trayecto sobre el pedúnculo cerebeloso inferior y cuando alcanza la cara inferior del cerebelo se divide en dos ramas terminales: una medial y otra lateral. La rama medial se continúa hacia atrás por la cisura media, entre los dos hemisferios cerebelosos. La rama lateral se distribuye por la superficie inferior de los hemisferios hasta llegar al borde circunferencial, donde se anastomosa con las arterias cerebelosas inferoanterior y superior.

Irriga la parte posterior de la cara inferior del cerebelo, el pedúnculo cerebeloso inferior, el núcleo ambiguo, el núcleo motor del nervio vago, el núcleo espinal del nervio trigémino, el núcleo solitario, los núcleos vestibulares y los núcleos cocleares.

Sus ramas colaterales más importantes son la rama coroidea del IV ventrículo y las ramas bulbares medial y lateral. La primera contribuye al plexo coroideo del IV ventrículo, y las otras dos irrigan el bulbo raquídeo y el pedúnculo cerebeloso inferior.

Las principales venas que drenan la sangre del cerebelo son: las venas superiores del cerebelo, la vena superior del vermis, la vena precentral del cerebelo, las venas inferiores del cerebelo, la vena inferior del vermis y las venas petrosas. Todas ellas terminan por enviar la sangre a senos venosos de la duramadre.

Las venas superiores del cerebelo recogen la sangre de la porción lateral de la cara superior de los hemisferios cerebelosos y normalmente desembocan en el seno transverso.

La vena superior del vermis recoge la sangre del vermis superior y desemboca en el seno recto a través de la vena cerebral interna o la vena cerebral magna (vena de Galeno).

La vena precentral del cerebelo recoge la sangre de la língula y del lobulillo central, y desemboca en la vena cerebral magna.

Las venas inferiores del cerebelo recogen la sangre de la porción lateral de la cara inferior de los hemisferios cerebelosos y desembocan en los senos transverso, occipital y petroso superior.

La vena inferior del vermis recoge la sangre del vermis inferior y desemboca directamente en el seno recto.

Las venas petrosas recogen la sangre de la región del flóculo y desembocan en el seno petroso inferior o en el superior.

sistematización de las caras del cerebelo

1 superior lóbulo occipital

2 anterior tallo cerebral

3 posterior protuberancia occipital interna bordes laterales 

4 inferior fosa cerebelosa

5 lingula espino talamico dorsal vía propioseptiva inconsciente del dolor brazos y piernas

En conjunto, las conexiones neuronales del cerebelo se pueden dividir en: axones aferentes, que transmiten la información de otras partes del SNC al cerebelo; circuitos cerebelosos intrínsecos -corticales y nucleares-, que integran y procesan la información; y axones eferentes, que transmiten la información procesada a otras partes del SNC.

Los axones o fibras aferentes alcanzan la corteza cerebelosa tras dar colaterales para los núcleos cerebelosos profundos o para los núcleos vestibulares. A su vez, la información es procesada en los circuitos intrínsecos de la corteza cerebelosa, y el resultado, en forma de impulsos nerviosos, es enviado por los axones de las células de Purkinje a los núcleos profundos. En estos núcleos la información también se procesa y de ellos parten las fibras eferentes del cerebelo tanto en dirección ascendente, hacia el tálamo y corteza, como descendente, hacia la médula espinal.

De esta forma el circuito funcional básico del cerebelo que constituido por dos arcos: uno principal o excitador, que pasa por los núcleos profundos, y otros secundario o inhibidor, que pasa por la corteza y regula al anterior. Este circuito se repite unas 30 millones de veces en todo el cerebelo y está formado por una sola célula de Purkinje y la neurona nuclear de proyección correspondiente más las interneuronas relacionadas con ellas.

El circuito funcional básico y los elementos celulares que lo conforman son idénticos en todas las partes del cerebelo, por este motivo se considera que la información se procesa de forma similar en todo el cerebelo.

El arco principal está constituido por las ramas colaterales de las fibras musgosas y trepadoras, que terminan en las neuronas de los núcleos profundos. Los axones de las neuronas de proyección de los núcleos profundos salen del cerebelo a través de los pedúnculos para terminar en diferentes núcleos del tronco del encéfalo y en el tálamo.

En los núcleos profundos se encuentran principalmente sinapsis axodendríticas y algunas axosomáticas, aunque también existen disposiciones más complejas como sinapsis en serie y tríadas. La sinapsis más frecuente es la sinapsis axodendrítica excitadora que se establece entre un terminal de las colaterales axónicas de las fibras musgosas o trepadoras -como elemento presináptico- y una dendrita de una neurona de proyección o una interneurona de los núcleos profundos -elemento postsináptico-. Las colaterales de las fibras musgosas y las fibras trepadoras usan como neurotransmisor principal el glutamato, aunque también pueden utilizar otros neurotransmisores (en espacial las fibras musgosas). Los circuitos sinápticos que se realizan entre las propias neuronas de los núcleos profundos son poco conocidos.

Desde el punto de vista funcional, los núcleos profundos del cerebelo poseen dos tipos básicos de neurona de proyección: unas neuronas gabaérgicas (inhibidoras) y pequeñas que mandan su axón hacia el núcleo olivar inferior, y otras neuronas glutaminérgicas (excitadoras) que mandan sus axones a otros centros nerviosos.

Las neuronas de proyección de los núcleos profundos en condiciones normales disparan permanentemente potenciales de acción a una frecuencia de más de 100 por segundo. Esta frecuencia puede modularse al alza o a la baja dependiendo de las señales excitadoras e inhibidoras que le lleguen a la neurona. Las señales excitadoras provienen principalmente de las colaterales axónicas de las fibras musgosas y trepadoras, mientras que las señales inhibidoras provienen de los axones de las células de Purkinje, que forman parte del arco secundario. El equilibrio entre estos dos efectos es ligeramente favorable a la excitación, lo que explica por qué la frecuencia de descargas de las neuronas de proyección se mantiene relativamente constante a un nivel moderado de estimulación continúa.

El arco secundario pasa a través de la corteza cerebelosa y está constituido en torno a una pieza neural fundamental: la célula de Purkinje. En la célula de Purkinje terminan dos tipos de circuitos: los circuitos excitadores o principales, que son los que la estimulan, y los circuitos inhibidores, formados por interneuronas inhibidoras. Finalmente, los axones de las células de Purkinje se proyectan sobre las neuronas de los núcleos cerebelosos y vestibulares, ejerciendo sobre ellos una acción inhibitoria mediante sinapsis gabaérgicas. De esta forma se modula y regula el arco principal excitador.

A todo esto hay que añadir que las terminaciones noradrenérgicas que llegan al cerebelo liberan un neurotransmisor de forma difusa que produce una hiperpolarización de las células de Purkinje.

Las células de Purkinje pueden ser estimuladas por dos vías distintas: mediante las fibras trepadoras (vía directa) o mediante las fibras musgosas (vía indirecta).

Las fibras trepadoras, al terminar sobre el soma y el árbol dendrítico de las células de Purkinje, producen una estimulación directa y muy específica mediante sinapsis tipo I de Gray que utilizan como neurotransmisor el glutamato. Al formar múltiples contactos con cada célula de Purkinje, una sola fibra trepadora produce una acción excitadora mucho más eficaz que las fibras musgosas.

Las fibras musgosas no actúan de forma directa sobre las células de Purkinje sino que lo hacen a través de unas interneuronas excitatorias, las células granulares. La presencia de interneuronas excitatorias es muy infrecuente en el sistema nervioso y es característica de la corteza cerebelosa. A nivel del glomérulo cerebeloso, las fibras musgosas hacen sinapsis tipo I de Gray (excitadoras) sobre las dendritas de las células granulares y los impulsos son vehiculados por las fibras paralelas hasta alcanzar las dendritas de las células de Purkinje. Las fibras paralelas presentan sinapsis que contienen vesículas esféricas con glutamato y conformación tipo I de Gray, lo que concuerda con su carácter excitador. En conjunto, las fibras musgosas actúan sobre las células de Purkinje con mucha convergencia y divergencia, estableciendo conexiones más inespecíficas que las fibras trepadoras.

Las células de Purkinje no cumplen el principio que dice que todos los potenciales de acción producidos por una neurona son iguales porque presenta dos tipos de potenciales de acción distintos dependiendo de la vía por la cual sean estimuladas. Si se estimulan de manera directa a través de las fibras trepadoras, generan una despolarización prolongada y un potencial de acción de pico complejo con una frecuencia de descarga de 3 o 4 herzios. Al ser estimuladas por la vía indirecta a través de las fibras musgosas generan un potencial de acción breve denominado pico sencillo, con una frecuencia de descarga de 100 a 200 herzios. Para generar un pico sencillo es necesaria la suma temporal y espacial de la estimulación producida por varias fibras paralelas. Todo esto demuestra que la información aportada por los dos tipos de fibras extrínsecas que llegan al cerebelo es diferente y es procesada de manera distinta.

Los circuitos inhibidores están constituidos por los tres tipos fundamentales de interneuronas inhibitorias: las células de Golgi, las células estrelladas y las células en cesta. Pueden actuar directamente sobre las células de Purkinje -como lo hacen las células estrelladas y las células en cesta- o indirectamente a través de las células granulares -como lo hacen las células de Golgi-. Todas estas interneuronas utilizan GABA como neurotransmisor inhibidor.

Las células estrelladas y las células en cesta son estimuladas por las fibras paralelas de los granos, que previamente han sido estimuladas por las fibras musgosas, y son las encargadas de modular la activación de las células de Purkinje por las fibras trepadoras produciendo un fenómeno de inhibición lateral. Esta inhibición lateral hace más precisa la señal que llega a las células de Purkinje de la misma manera que otros mecanismos de inhibición lateral acentúan el contraste de las señales en otros muchos circuitos neuronales de sistema nervioso.

Las células de Golgi reciben estímulos excitatorios de las fibras paralelas y, en menor cantidad, de las fibras trepadoras y musgosas. Actúan a nivel de los glomérulos cerebelosos haciendo sinapsis tipo II de Gray (inhibitaria) sobre las dendritas de las granos. Mediante estas sinpasis modulan la activación de las células granulares por las fibras musgosas y, por consiguiente, regulan la actividad de las células de Purkinje. De esta forma, las células de Golgi crean un circuito de retroalimentación negativa para las células granulares.

Clásicamente las lesiones del cerebelo se manifiestan clínicamente por:


Todos estos trastornos se observan mejor cuanto más rápidamente se ejecutan las maniobras. La adiadococinesia indica una dificultad o la imposibilidad para ejecutar movimientos alternativos rápidos (prueba de las marionetas).



etc.

La enfermedad o lesión de la totalidad o de una gran parte del cerebelo es lo que se conoce como síndrome cerebeloso. Las lesiones selectivas del cerebelo son extremadamente raras.

La causa más frecuente es el meduloblastoma del vermis en los niños.
El compromiso del lóbulo floculonodular produce signos y síntomas relacionados con el sistema vestibular. Dado que el vermis es único e influye sobre las estructuras de la línea
media, la descoordinación muscular afecta a la cabeza y el tronco, y no a las extremidades. Se produce una tendencia a la caída hacia delante o hacia atrás, así como dificultad para mantener la cabeza quieta y en posición erecta.
También puede haber dificultad para mantener el tronco erecto.

La causa de este síndrome puede ser un tumor o una isquemia en un hemisferio cerebeloso. En general, los síntomas y signos son unilaterales y afectan a los músculos ipsilaterales al hemisferio cerebeloso enfermo. Están alterados los movimientos de las extremidades, especialmente de los brazos y piernas, donde la hipermetría y la descomposición del movimiento son muy evidentes
A menudo, se produce oscilación y caída hacia el lado de la lesión. También son hallazgos frecuentes la disartria y el nistagmo.

Las etiología más frecuentes de síndromes cerebelosos son:





</doc>
<doc id="716" url="https://es.wikipedia.org/wiki?curid=716" title="Complejo">
Complejo

El término complejo puede designar a:






</doc>
<doc id="719" url="https://es.wikipedia.org/wiki?curid=719" title="Costa Rica">
Costa Rica

Costa Rica, oficialmente República de Costa Rica, es una nación soberana, organizada como una república presidencialista unitaria compuesta por 7 provincias. Ubicada en América Central, posee un territorio con un área total de 51 100km². Limita con Nicaragua al norte, el mar Caribe al este, Panamá al sureste y el océano Pacífico al oeste. En cuanto a los bordes marítimos, colinda con Panamá, Nicaragua, Colombia y Ecuador. Cuenta con 5 137 000 habitantes según su última proyección demográfica. Su capital, centro político y económico es San José, y su idioma oficial es el español.

Con una sólida y longeva democracia, de entre las más funcionales del mundo, y una eficaz capacidad de movilidad social que le permite alcanzar un elevado progreso general en relación a la distribución y tamaño de su economía, Costa Rica es el quincuagésimo país más rico del mundo según datos del Fondo Monetario Internacional y uno de los más estables de América, obteniendo resultados generalmente favorables en todos los . Aunado a esto, posee diversas políticas a la vanguardia para la protección del ambiente y es una nación desmilitarizada por voluntad propia desde 1948, manteniendo un elevado nivel de prosperidad de acuerdo al Instituto Legatum y siendo considerada la sociedad más feliz del planeta, durante más de una década, según New Economics Foundation.

De forma paralela, el país actualmente afronta importantes retos en su desarrollo relacionados con la pésima condición y capacidad de mejora en su infraestructura vial y de transporte, un elevado déficit fiscal, la sostenibilidad y eficacia de su estado benefactor, el estancamiento en la pobreza (la cual no ha bajado de 20% desde hace años ) y el desempleo, la tendencia a la alza en su delincuencia y desigualdad, y profundos problemas ecológicos relacionados con prácticas agrícolas nocivas, mala gestión de residuos y contaminación de cuerpos de agua.

El nombre Costa Rica para designar al territorio nacional apareció por primera vez en una cédula real fechada el 17 de diciembre de 1539 y enviada a la Audiencia de Panamá en 1543, en la que el rey español Carlos V otorgó un permiso especial a Diego Gutiérrez y Toledo para que realizase la conquista y colonización del territorio costarricense, que en adelante se denominó «Nuevo Cartago y Costa Rica».

Acerca del porqué de este término, se han planteado dos hipótesis. La primera y más difundida es que este se encuentra en el arribo colombino al país. Cristóbal Colón llegó al territorio costarricense el 25 de septiembre de 1502, en su cuarto viaje. Impresionado por la abundancia de la joyería y por los adornos de oro de los indígenas, Colón pensó que en estas tierras existían muchas riquezas. En los diarios escritos por Colón a los Reyes Católicos de España, describe en el territorio existía mucho oro, tema que recalcó en el documento 9, recopilado en el "Libro copiador de Colón", llamado "Carta de Jamaica". En dicho documento, Colón anota en su visita a la tierra de Cariay (actual Limón):

Luego, más adelante:

Y finalmente:

Este importante hecho pudo influir en la posterior atribución del nombre de "Costa Rica" entre los exploradores españoles que ingresaron al territorio años después, para diferenciarla de la región de Veragua. Esa fama sobre las supuestas riquezas del país fue lo que impulsó a los aventureros a emprender otras exploraciones y sirvió de polo de atracción para los colonizadores e inmigrantes europeos. Se cree que para esa época, el nombre de Costa Rica se había difundido entre los exploradores españoles para designar el hasta ese momento inexpugnable territorio, basándose en lo expresado por Colón acerca de él.

La segunda hipótesis fue propuesta por parte de Dionisio Cabal Antillón, cuya premisa era que Costa Rica era la castellanización de una palabra indígena de origen huetar (a la llegada de los españoles, el grupo autóctono más numeroso del país) con que este pueblo designaba al país o a un asentamiento importante dentro de él. Según Cabal, Costa Rica derivaría de un huetarismo (un vocablo indígena), posiblemente "Coquerrica", "Coquerrique" o "Cotaquerrique", y que los españoles simplificaron bajo el término "Costarrica", es decir, los españoles pronunciaban de esa forma, en su idioma, el nombre que los indígenas ya daban a su propio país, en forma similar como ocurrió con otros países americanos, como México (México-Tenochtitlan), Nicaragua (Nicarao) o Chile (Chili). Para sostener su hipótesis, Cabal sostenía que el caso de "Costarrica", con doble erre y en una sola palabra, era el mismo de otros topónimos huetares que se usan actualmente en el país: Tucurrique, Turrialba, Turrujal, Suerre, Siquirres, Curridabat, Aserrí, Ujarrás, y palabras de uso cotidiano como turruja, yigüirro, yurro, curré, etc. No obstante, esta hipótesis ha sido considerada poco probable por no tener ningún fundamento lingüístico ni histórico en la cual basarse.

La evidencia más antigua de ocupaciones humanas en Costa Rica se asocia a la llegada de grupos de cazadores-recolectores alrededor de 12.200 años, con el hallazgo en el cantón de Siquirres de 66 asentamientos humanos donde se recuperaron elementos funerarios, petroglifos, basamentos de viviendas, calzadas, herramientas de piedra, alfarería, cerámica, los cuales datan del Paleoindio y que corresponden a la etnia cabécar, constituyéndose en el sitio arqueológico más antiguo de Centroamérica. De 10 000 a 7000 añosa.C. datan antiguas evidencias arqueológicas (fabricación de herramientas de piedra) localizadas en el Valle de Turrialba, con presencia de puntas de lanza tipo clovis (norteamericana) y cola de pez (sudamericana). La agricultura incipiente aparece hacia 5000a.C., principalmente dada por tubérculos y raíces. Para el primer y segundo milenios a.C. ya existían comunidades agrícolas sedentarias, pequeñas y dispersas. Hacia 2000-3000 a.C., aparece el uso más antiguo que se conoce de la cerámica, con fragmentos de ollas, vasijas cilíndricas, platones, tecomates y otras formas de vasijas, decoradas con técnicas como incisos o acanaladuras, estampados y modelados.

Entre 300 a.C. y 500 d.C. hay un cambio de una organización tribal a una sociedad cacical, con la construcción de basamentos con cantos rodados, montículos, hornos, pozos de almacenamiento, y estatuaria. El maíz llega a consolidarse como el cultivo principal en algunas regiones, mientras que en otras se da un sistema mixto, además del uso de recursos costeros (pesca) y cacería. En este periodo aparece la producción y uso de artefactos de jade y otras piedras verdes, metates ceremoniales, remates de piedra para bastones y cerámicas especiales, se inicia el uso de objetos de metal (cobre y oro). Los llamados metates trípodes de panel colgante son una manifestación sobresaliente y única del arte precolombino costarricense, decorados con elementos animales y humanos. Su manufactura se inicia en la parte tardía de este periodo (0-500a.C.). Entre 300 y 800 d.C. aparecen los primeros cacicazgos complejos, con presencia de aldeas grandes y obras de infraestructura (basamentos, calzadas y montículos funerarios). Hay jerarquización de asentamientos, con aldeas principales y poblados secundarios, formación de linajes de poder hereditario y especialización de labores, con aparición de un cacique en la aldea principal y caciques secundarios en aldeas subordinadas.

A partir de 800 d.C. y hasta la llegada de los españoles en el siglo XVI, se presentó un incremento en el tamaño y complejidad del diseño interno de las aldeas, y las diferencias regionales se acentuaron. La presencia de numerosos cementerios, simples y complejos, obras de infraestructura masivas, diversidad de bienes domésticos y suntuarios, desarrollo de orfebrería, intercambio regional y conflictos entre cacicazgos por territorios y recursos son elementos característicos de esta época. La jerarquización social incluye individuos principales como el cacique y el chamán, y el pueblo común formado por artesanos y agricultores. El oro substituyó al jade como símbolo de rango, en especial en las regiones Central y Diquís. En la región del Valle del Díquis, se fabrican las esferas de piedra distintivas de la región, en el delta de los ríos Térraba y Sierpe, las cuales se postula que fueron utilizadas como símbolo de rango y marcadores territoriales. Otras obras de piedra incluyen figuras de bulto de formas humanas y animales, metates en forma de jaguar y estatuas antropomorfas. Las zonas sur y atlántica del país tuvieron influencia sudamericana, debido a la presencia de grupos que hablan lenguas chibchas. La actual provincia de Guanacaste se convirtió en la frontera sur de Mesoamérica con la llegada de los chorotegas para el periodo comprendido entre los años 900 al 1000 d.C.Los asentamientos humanos en la Costa Rica prehispánica cumplieron una función de puente cultural entre el Sur y el Norte del continente, y la orfebrería y la artesanía policromada en barro, tuvieron un amplio desarrollo y bellísimos resultados.

Cristóbal Colón llegó a la costa atlántica de Costa Rica el 25 de septiembre de 1502, en su cuarto viaje, visitando la isla Uvita (llamada Quiribrí por los indígenas y bautizada La Huerta por Colón), y el poblado de Cariay. Según los diarios escritos por Colón, en el territorio existía mucho oro, lo que impulsó a los aventureros a emprender otras exploraciones y sirvió de polo de atracción para los colonizadores. A las expediciones iniciales de Diego de Nicuesa y Alonso de Ojeda sobre el litoral atlántico, siguió la de Vasco Núñez de Balboa, quien descubrió el Océano Pacífico en 1513 luego de atravesar el istmo de Panamá. En 1519, Gaspar de Espinosa junto a Juan de Castañeda, Alonso Martín de Don Benito y Hernán Ponce de León descubrieron el golfo Dulce y el golfo de Nicoya. Gil González Dávila recorrió el litoral pacífico costarricense, arribó a Nicoya y continuó hacia Nicaragua, donde fue obsequiado ricamente por el cacique Nicarao.

Las riquezas encontradas por González Dávila hicieron que el gobernador de Panamá, Pedrarias Dávila, enviara una misión al mando de Francisco Hernández de Córdoba, quien bordeando el litoral pacífico desembarcó en el río Grande de Tárcoles y fundó la Villa de Bruselas en 1524, siendo esta la primera población colonial en territorio costarricense. En 1534 Felipe Gutiérrez obtuvo permiso para conquistar la Gobernación de Veragua y en 1538, Hernán Sánchez de Badajoz se convirtió en adelantado y mariscal de Costa Rica, fundó la efímera ciudad de Badajoz en Talamanca y el puerto de San Marcos. En 1540, Diego de Gutiérrez recorrió el litoral caribeño hasta llegar al río San Juan. Fundó las poblaciones de Santiago y San Francisco en el territorio de Cartago. Apresó luego a los caciques Camaquire y Cocorí para pedir recompensa (a pesar de que fue bien recibido por los aborígenes), y tras internarse en las llanuras de Santa Clara para salir a la Cordillera Central, fue emboscado y murió. Tras esto, no hubo más expediciones al país por un lapso de diez años.

En enero de 1561, Juan de Cavallón recorrió Nicoya y entró en el Valle Central, donde fundó Castillo de Garcimuñoz, la primera población del Valle Central. A Juan de Cavallón se le considera como el primer conquistador de Costa Rica, si bien no pudo ejercer un control absoluto de la población indígena y se vio envuelto en una lucha contra el cacique Garabito, rey de los huetares, para poder adquirir víveres. El reino de Garabito se extendía desde el río Virilla hasta las costas del Pacífico (Jacó y Tilarán) y desde la cordillera Volcánica Central hasta el río San Juan, y su influencia en el país era enorme, aun entre sus enemigos los chorotegas, que controlaban Nicoya. Garabito, símbolo de la resistencia huetar, no se enfrentó abiertamente a los españoles, sino que usó tácticas de guerrilla, con emboscadas e incursiones rápidas en los campamentos y las poblaciones españolas.

En 1562, Juan Vázquez de Coronado recorrió los cuatro costados del territorio costarricense y participó en dos expediciones, la primera, penetrando en Nicoya desde Nicaragua, hasta Garcimuñoz, luego Quepos y Coto, en el Pacífico Central. Fue nombrado alcalde mayor de Costa Rica y Nueva Cartago ese año. En la segunda expedición, recorrió la desembocadura del río Grande de Térraba hasta la cordillera de Talamanca, llegó a las llanuras del Caribe y luego a Garcimuñoz, la cual trasladó al Valle del Guarco en 1564 y renombró como Cartago. A Vázquez de Coronado se le conoce como el verdadero conquistador de Costa Rica por el papel pacificador que desempeñó al lado de los indígenas, ganándose su confianza y adhesión, empleando el diálogo en lugar de la violencia y entablando amistad entre españoles y aborígenes, además de que fue el que tuvo un mayor conocimiento del territorio costarricense. Más tarde, fue nombrado gobernador y adelantado de la Provincia de Costa Rica, pero no asumió el cargo al morir en el naufragio de su nave en 1565. Con su ausencia, los soldados españoles iniciaron la represión de los indígenas, que se alzaron en todo el país y sitiaron Cartago en 1566, por lo que se nombró a un nuevo gobernador, Perafán de Rivera, quien liberó Cartago en 1568, y un año después, sometió a encomienda a los indígenas y repartió la tierra entre los conquistadores, iniciando de este modo el periodo colonial.

Costa Rica fue desde 1574 la dependencia más austral de la Capitanía General de Guatemala, parte del Virreinato de Nueva España, situación en la que permaneció hasta su independencia. La lejanía de la ciudad de Guatemala, el reducido número de oficiales del gobierno y de representantes de la Iglesia, y su carencia de riquezas agrícolas o mineras, provocaron que se encontrara en total abandono por parte de las autoridades españolas, lo cual facilitó que se desarrollara con mucha mayor autonomía que otras provincias de Centroamérica. El interés relativo que mostraron los colonizadores hacia esta región modificó algunas de las situaciones características que se dieron en otras naciones, dotando a Costa Rica de algunas peculiaridades. Algunos estudiosos sostienen que parte de la idiosincrasia nacional se formó durante esta época colonial, en donde las privaciones de tipo material eran el común para todos y al no haber una fuerte mano de obra indígena y africana, tanto el Gobernador Provincial como el más humilde de los campesinos, esclavos y amerindios, tenían que velar cada cual por su sustento y por el de sus familias, creándose así una sociedad más igualitaria y menos regida por castas. Otros estudios demuestran que en la Costa Rica colonial, y sobre todo, a partir del siglo XVII, se empezó a cimentar una marcada diferenciación social, con una élite comercial y terrateniente que manejaba a antojo los hilos de la economía y la política interna.
Con el fin de concentrar a una población cada vez más dispersa, las autoridades civiles y eclesiásticas ordenaron la fundación de iglesias, oratorios y parroquias en el Valle Central: Villa Vieja (1707, actual Heredia); Villa Nueva de la Boca del Monte (1738, actual San José); Villa Hermosa (1782, actual Alajuela). En el Pacífico, Esparza (Costa Rica) fue la ciudad más importante, que se fue despoblando por los ataques piratas, por lo que mucha población se trasladó hacia el valle de Bagaces, dando lugar a la población de Cañas en 1751. Hacia la segunda mitad del siglo XVIII, se inició la actividad en el puerto de Puntarenas, principalmente para comerciar tabaco, pero no fue declarado oficialmente puerto hasta 1814. En Nicoya, la actividad ganadera con Nicaragua permitió una fuerte influencia de esa provincia en toda la región hasta el valle del Tempisque, la cual, sin embargo, contaba con una población dispersa, por lo que en 1769 se fundó una ermita en un importante cruce de caminos, que dio lugar a la ciudad de Liberia.
En 1812, siendo gobernador Tomás de Acosta y Hurtado de Mendoza, se nombra al presbítero Florencio del Castillo como diputado a las Cortes de Cádiz por la provincia de Costa Rica y el Partido de Nicoya, esto por la exigua población de ambas dependencias como para nombrar un representante cada uno. Durante este periodo, se introduce el café en el Valle Central, el cual será vital para el futuro desarrollo del estado luego de la independencia.

Costa Rica se independizó del Imperio español el 15 de septiembre de 1821, junto al resto de la Capitanía General de Guatemala. Después de recibir el pronunciamiento de la ciudad de León, establecido en el "Acta de los Nublados", el ayuntamiento de Cartago emitió el acta del 29 de octubre, declaró la independencia y, el 1 de diciembre, una Junta de Legados promulgó el Pacto de Concordia, la primera Constitución, en la que constituyó su propia forma de gobierno, a cargo de una Junta Superior Gubernativa, se reconocieron los derechos civiles de los habitantes, se abolió la esclavitud y se proclamó la libertad de comercio. Costa Rica formó parte de la República Federal de Centroamérica (1823), pero su participación (1823-1842) fue bastante marginal. Costa Rica buscó solucionar sus problemas por sí misma: para 1825, contaba con su propia moneda, Jefe de Estado, Asamblea Legislativa y Corte Suprema de Justicia. Entre 1825 y 1833, mientras la República Federal se debate en la anarquía política y la guerra civil (1826-1829), Costa Rica vive un periodo de relativa estabilidad política. El Estado ganó territorio con la anexión del Partido de Nicoya al país, el 25 de julio de 1824, pero también perdió con la ocupación de Bocas del Toro de 1836 por la República de la Nueva Granada.

El 3 de marzo de 1823, se formó el primer Congreso con diputados de las cuatro ciudades principales del Valle Central. Los localismos llevaron a un enfrentamiento por el poder entre las ciudades conservadoras de Cartago y Heredia, partidarias de unirse al Primer Imperio Mexicano, y las ciudades liberales y republicanas de San José y Alajuela. La batalla de Ochomogo (5 de abril de 1823) resultó en la victoria de los republicanos, dirigidos por Gregorio José Ramírez, y el traslado de la capital a San José. Ramírez devolvió el poder a los pocos días, y se nombró una nueva Junta Gubernativa que gobernó hasta 1824, año en que se nombró al primer Jefe de Estado, Juan Mora Fernández (1824-1833), josefino liberal con amplia experiencia administrativa, bajo cuyo gobierno se introdujo la imprenta, se estableció la primera casa de moneda y se reorganizó la Casa de Enseñanza de Santo Tomás, considerada la primera universidad de Costa Rica; se promulgó una nueva constitución, la "Ley Fundamental del Estado Libre de Costa Rica" (1825), así como la "Ley Aprilia", que daba autonomía al país dentro de la República Federal.

A Mora le sucedió José Rafael de Gallegos en 1833, en cuyo gobierno entró en vigencia la "Ley de la Ambulancia", que rotaba la capital del país entre las cuatro principales ciudades del Valle Central. Esta ley fue derogada por su sucesor, Braulio Carrillo (1835-1837, 1838-1842), lo que detonó la "Guerra de la Liga", la segunda guerra civil de Costa Rica, en la que la victoria josefina afianzó a la ciudad como capital. Carrillo impuso el orden del país: fortaleció las instituciones públicas, creó el pago de impuestos, impulsó el cultivo del café, promulgó la Ley Contra la Vagancia y el respeto a las autoridades. Derrotado por Manuel Aguilar Chacón en las elecciones de 1837, Carrillo lo derrocó en 1838. En su segundo mandato, estableció códigos en materia penal, civil y de procedimientos, fundamentales para el país, y promulgó el Decreto de Bases y Garantías, en el cual se proclamó Jefe Vitalicio. El 15 de noviembre de 1838 separó al país de manera definitiva de la República Federal y lo proclamó estado libre, soberano e independiente. En 1842, los adversarios políticos de Carrillo convocaron al país a Francisco Morazán, quien lo derrocó. Morazán intentó usar a Costa Rica como base militar para reconstruir la República Federal, disuelta en 1839, por lo que fue depuesto y fusilado en San José el 15 de septiembre de 1842. Le sucedió José María Alfaro (1842-1844, 1846-1847), y tras él Francisco María Oreamuno (1844-1846), primer Jefe de Estado electo por sufragio directo.

En 1847 fue elegido José María Castro Madriz (1847-1849, 1866-1868), doctor en leyes que llegó a ocupar la presidencia de los tres poderes de la República. Hombre culto, propició la educación de la mujer, la libertad de prensa y la gestión para crear la primera diócesis en Costa Rica en 1850, con Monseñor Anselmo Llorente y Lafuente como el primer obispo de Costa Rica. El 31 de agosto de 1848 promulgó una nueva constitución en la que declaró a Costa Rica como nación soberana e independiente de cualquier otro estado, nombrándola definitivamente como República de Costa Rica. El 29 de septiembre de 1848 dotó al país de la primera bandera tricolor, obra de su esposa, Pacífica Fernández, y las bases del actual escudo. La Constitución de 1848 estableció un Poder Ejecutivo fuerte frente al Legislativo, permitiendo al país agilidad para nombrar funcionarios públicos, acelerando la centralización del poder y abriendo camino al país para su conversión hacia un Estado moderno.

En 1849 ascendió al poder Juan Rafael Mora Porras (1849-1859), bajo cuya administración el país presentó un gran auge económico y social, secundario al establecimiento de un modelo agroexportador basado en el café, lo que permitió la formación y fortalecimiento de una oligarquía cafetalera de gran influencia sobre los asuntos del estado. La bonanza económica permitió que se importaron nuevas tecnologías, se abrieron vías de comunicación, se mejoraron los caminos y puertos, se fortaleció el poder central del Estado, eliminándose los localismos, se fortaleció y modernizó el ejército, se construyeron importantes obras de infraestructura pública como el Palacio Nacional, el Hospital San Juan de Dios, el edificio de la Universidad de Santo Tomás, la primera Facultad de Medicina, el primer banco nacional, el primer teatro, la Fábrica Nacional de Licores, se compuso la música del actual Himno Nacional, obra de Manuel María Gutiérrez Flores, y se delimitó la frontera con Nicaragua mediante el Tratado Cañas-Jerez.

La mayor amenaza a la estabilidad política y cultural del país se vivió durante el ascenso al poder en la vecina Nicaragua del filibustero estadounidense William Walker, quien arribó a Nicaragua durante la guerra civil de ese país y quien se hizo nombrar presidente, tras lo cual emitió un decreto de colonización y otro de esclavitud. Su presencia significaba un peligro para integridad territorial de Costa Rica, principalmente por sus pretensiones sobre la vía del tránsito por el río San Juan, por lo que el presidente Mora llamó a las armas al pueblo costarricense. El ejército marchó hacia Nicaragua el 4 de marzo, con 4000 efectivos bajo el mando de José Joaquín Mora Porras, hermano del presidente, y del general José María Cañas. A la victoria costarricense en la Batalla de Santa Rosa el 20 de marzo de 1856, siguió el enfrentamiento en Rivas, Nicaragua, el 11 de abril de 1856, batalla cruenta y larga, con grandes pérdidas para ambos bandos y muchas muestras de heroísmo: la más recordada es la del soldado alajuelense Juan Santamaría.

Tras la victoria de Rivas, el ejército costarricense se vio obligado a replegarse de la ciudad ante la aparición de la epidemia de cólera morbus, que terminó costando la vida a 10 000 costarricenses (el 10% de la población nacional en esa época), al dispersarse la infección con el regreso del ejército al país. No fue sino hasta 1857 cuando Costa Rica volvió a entrar en la guerra, ahora aliada con el resto de los ejércitos centroamericanos, con la decisiva Campaña de la Vía del Tránsito, en la cual toma el río San Juan para cortar la ruta de aprovisionamiento de los ejércitos filibusteros, obligando a Walker a rendirse el 1 de mayo de 1857. El epílogo de la Campaña Nacional fue sangriento: en 1859, Mora fue derrocado y exiliado por sus enemigos políticos. En 1860, intentó recuperar el poder por la fuerza. Derrotado en la batalla de La Angostura, fue fusilado, junto al general Cañas, en la ciudad de Puntarenas.

La caída de Mora trajo un periodo de inestabilidad política donde dominaron los militares, encabezados por los generales Máximo Blanco y Lorenzo Salazar, que colocan y quitan a los gobernantes de acuerdo a los intereses propios y de la oligarquía económica. A Mora le sucedieron José María Montealegre Fernández (1859-1863), Jesús Jiménez Zamora (1863-1866 y 1868-1870) y José María Castro Madriz (1866-1868). Durante este periodos se dan algunos hitos como la fundación del Banco Anglo Costarricense (1864), la creación de la primera línea de telégrafos (1866) y la declaración de la educación básica como gratuita y obligatoria. En 1868, Jiménez logró apartar del poder a los militares Blanco y Salazar, pero a su vez él sería derrocado en 1870 por Tomás Guardia Gutiérrez.

La transformación del estado inició bajo la dictadura de Tomás Guardia Gutiérrez (1870-1882), seguido de los gobiernos de Próspero Fernández (1882-1885) y Bernardo Soto (1885-1889), con la introducción del liberalismo, que se profundizó en todos los aspectos de la vida nacional, desde el económico hasta el cultural y educativo. Se expandió la administración pública, las instituciones maduraron y se consolidaron, y se configuró un círculo de políticos e intelectuales de orientación reformista, con el propósito de modernizar el Estado y la sociedad. Se promulgó una nueva Constitución en 1871, nuevos códigos penales, civiles y fiscales, se introdujo el matrimonio civil y el divorcio, se secularizaron los cementerios, se inauguró el registro de nacimientos y defunciones, se estableció la educación secular gratuita y obligatoria, se hicieron cambios en el sistema de salud y se abolió la pena de muerte. Las ideas liberales definieron el país en lo económico, social e institucional. El Estado se volvió garante y protector de los más preciados valores de la sociedad burguesa: la propiedad y la libertad, al mismo tiempo que se daba la separación entre Estado e Iglesia. En lo económico, se caracterizó por una economía agro-exportadora basada en el bicultivo café-banano. El país crece económicamente, se fundan bancos de capital nacional y se establece el colón como moneda nacional (1900). La construcción del ferrocarril al Atlántico (1870-1890) y del ferrocarril al Pacífico (1897-1910) permitieron el crecimiento demográfico y la diversificación cultural con la llegada de inmigrantes jamaiquinos, chinos, italianos y otros, así como el establecimiento en el país de la United Fruit Company, cuya presencia tendrá peso importante durante la primera mitad del siglo XX.

En 1889, un intento de desconocer la victoria electoral de José Joaquín Rodríguez Zeledón provocó el levantamiento popular del 7 de noviembre, considerado como el origen de las prácticas democráticas en Costa Rica. La democracia, tras las administraciones autoritarias de Rodríguez y Rafael Yglesias (1894-1902) experimentó avances importantes. El voto directo se introdujo en 1913 y el sufragio secreto entre 1925 y 1927, a pesar de lo cual existieron irregularidades en las urnas y pactos secretos. En 1917, el gobierno reformista del presidente Alfredo González Flores (1914-1917) fue derrocado por Federico Tinoco (1917-1919), cuya dictadura fue depuesta en 1919 tras un movimiento cívico, victoria que aceleró la decadencia del ejército, como se vio reflejado en la guerra de Coto contra Panamá en 1921.

La presidencia, durante las primeras cuatro décadas del siglo XX, fue dominada por Cleto González Víquez (1906-1910 y 1928-1932) y Ricardo Jiménez Oreamuno (1910-1914, 1924-1928 y 1932-1936). La crisis económica mundial de 1929 y la diversificación capitalista de la economía trajo consigo conflicto social, lo que dio cabida al nacimiento de partidos políticos no tradicionales de corte socialista, como el Partido Reformista (1923) de Jorge Volio y el Partido Comunista (1931) de Manuel Mora Valverde. El malestar social se vio reflejado especialmente con la huelga de brazos caídos de 1934 contra la United Fruit Company. La economía no empezó a recuperarse sino hasta 1936, con el financiamiento de gran cantidad de obras públicas durante el gobierno de León Cortés Castro (1936-1940), sin embargo, el estallido de la Segunda Guerra Mundial, con el cierre del mercado europeo, reorientó la exportación nacional hacia los Estados Unidos.

Las luchas de los trabajadores por mejorar sus condiciones de vida y laborales obtuvo su fruto en 1940, cuando el presidente Rafael Ángel Calderón Guardia emprendió un ambicioso proyecto de reformas sociales, con el apoyo de la Iglesia católica, en la figura de su arzobispo Víctor Manuel Sanabria Martínez, y del líder comunista Manuel Mora Valverde, lo que permitió la promulgación de las Garantías Sociales en la Constitución Política y el Código de Trabajo, así como la creación de la Universidad de Costa Rica, la Caja Costarricense del Seguro Social y la Orquesta Sinfónica Nacional. Además, se dio la firma el Tratado de límites Echandi Montero-Fernández Jaén, que delimita la frontera con Panamá.

La alianza entre Calderón y los comunistas significó una polarización de la política costarricense, puesto que, si bien sentó las bases del Estado de bienestar, agitó la inestabilidad política. Hubo también un creciente malestar entre un grupo de importantes capitalistas que adversaban las reformas sociales de Calderón. Los cambios geopolíticos tras 1945, con el fin de la Segunda Guerra Mundial y el inicio de la Guerra Fría, produjo un aumento del interés de los Estados Unidos en la región por la creciente presencia de guerrillas izquierdistas, pero Costa Rica fue uno de los pocos países del mundo donde la democracia no colapsó en la década de 1930, y donde el partido comunista permaneció legal y exitoso en las urnas, una excepción en Centroamérica. Existió, sin embargo, un deterioro en la confianza en el sistema electoral por prácticas fraudulentas, y la oposición anticalderonista acusó la victoria del progobiernista Teodoro Picado en 1944 como fraude. En un discurso radiofónico dado el 8 de julio de 1942, José Figueres Ferrer denunció actos irregulares y corruptelas por parte del gobierno, razón por la cual se le expulsó del país, al que volvió en 1944. Durante su exilio, Figueres ganó aliados que le permitieron la adquisición de armas y la formación de un ejército rebelde.

La Guerra Civil de Costa Rica fue desencadenada primordialmente por la nulidad de las elecciones de 1948, con el fin de defender la transparencia del sufragio frente a un supuesto fraude, aunque realmente las causas son diversas y complejas, con causas internas y externas que se venían gestando a lo largo de una década. Es la última guerra civil de la historia costarricense, y la última vez que se interrumpió el gobierno constitucional en la historia del país.

La campaña electoral de 1948 se efectuó bajo un clima de extrema polarización. Las elecciones, por primera vez en la historia dirigidas por un tribunal electoral, favorecieron claramente al candidato de la oposición Otilio Ulate Blanco, pero el bando perdedor denunció un supuesto fraude. El Congreso de la República, con diputados republicanos y comunistas como mayoría, fue convocado a sesiones extraordinarias el día 1 de marzo. Al final de una acalorada sesión, se declararon nulas las elecciones presidenciales, pero, paradójicamente, no las elecciones de diputados.

El 12 de marzo, Figueres se alzó en armas en su finca ""La Lucha"" al frente del Ejército de Liberación Nacional. En poco tiempo, sus fuerzas derrotaron al Ejército oficial y tomaron las ciudades de Cartago y San Isidro del General, preparándose para enfrentar a las milicias progubernamentales en San José, los llamados "mariachis". Puerto Limón fue tomado por asalto por la Legión del Caribe, que con anterioridad había bombardeado con avionetas puntos estratégicos de la ciudad. Los comunistas participaron en la lucha armada como aliados del gobierno de Picado y los calderonistas, pero se retiraron luego que Figueres y Manuel Mora acordaran un pacto para mantener las garantías sociales. El cuerpo diplomático del gobierno se reunió con Figueres en Cartago, quien pidió la rendición incondicional, pero Picado rechazó la propuesta. Una segunda reunión en la embajada de México logró la firma de un pacto para poner fin a las hostilidades. El 20 de abril de 1948, Picado entregó el poder a Santos León Herrera, y el 27 de abril, José Figueres entró victorioso a la ciudad capital. Calderón Guardia es exiliado en Nicaragua y luego en México. La Guerra civil se peleó entre el 12 de marzo y el 19 de abril de 1948, y se calcula que hubo unos 4000 muertos en todo el país.

Tras la guerra, se organizó una junta que asumió los poderes Ejecutivo y Legislativo hasta 1949, encabezada por José Figueres Ferrer. Durante los dos años siguientes, se inició un vasto programa de cambio institucional: se abolió el ejército (1 de diciembre de 1948), se nacionalizó la banca, se fortalecieron las reformas sociales, se integró el Tribunal Nacional Electoral, se concedió el derecho al sufragio a las mujeres y a la población afrocaribeña. La redacción de la actual Constitución Política en 1949 marcó el nacimiento de la Segunda República.

El panorama internacional estuvo enmarcado por la guerra fría, donde el país practicó una política internacional más activa, cercana a los Estados Unidos (ruptura con Cuba de 1961), pero a su vez, crítica con el apoyo de ese país a las dictaduras militares en América Latina. El panorama político interno también tuvo periodos de inestabilidad: intentos de invasión por parte de partidarios del expresidente Calderón Guardia en 1948 y 1955, así como un fallido intento de golpe de estado en 1949. Luego de 1950, hubo persecuciones y exilio para comunistas y calderonistas. Una amnistía general fue promulgada por gobierno de Mario Echandi Jiménez (1958-1962), pero aun así hubo conflictos por la concentración de las tierras agrícolas en pocas manos, así como un crecimiento de la migración rural hacia las ciudades. El 24 de abril de 1970, miles de estudiantes y trabajadores apedrearon la Asamblea Legislativa por el traspaso de tierras a la transnacional Alcoa, de carácter minero. Desde 1948, dos bandos oscilaron en el poder: el bando figuerista, representado por el Partido Liberación Nacional, y el bando calderonista representado por distintos partidos y coaliciones entre ellos el Partido Unión Nacional, Partido Unificación Nacional, Coalición Unidad y Partido Unidad Social Cristiana. A partir de la fundación del PUSC en 1983 se gestó el sistema bipartidista costarricense que imperó desde entonces hasta 2002, donde los dos partidos mayoritarios y oscilantes en el poder, el PLN y el PUSC, hegemonizaron la política nacional.

El país experimentó una fuerte crisis económica durante la década de 1980, motivada por múltiples factores tanto internos como externos. Esto produjo malestar social, con actos violentos en 1981, y protestas y huelgas en 1983 y 1985. El gobierno de Rodrigo Carazo (1978-1982) firmó dos acuerdos con el Fondo Monetario Internacional, pero sus recomendaciones, de ideología neoliberal, implicaban costos sociales inaceptables, por lo que se declaró una moratoria en el pago de la deuda y el gobierno rompió con el FMI. La deuda externa se incrementó pero el sistema político no colapsó. Luis Alberto Monge Álvarez (1982-1986) estabilizó la economía cambiando el modelo económico, a lo que se sumó la aprobación de la Iniciativa para la Cuenca del Caribe por parte de Estados Unidos, así como el aporte económico de la Agencia Internacional para el Desarrollo (AID), casi 1.300 millones de dólares entre 1982 y 1990. La intención de Ronald Reagan de militarizar el país para enfrentar la revolución sandinista en Nicaragua motivó que Monge declarara la Ley de Neutralidad Perpetua (1984), con un masivo apoyo popular, e hizo que el eje de política exterior de Óscar Arias Sánchez (1986-1990) fuese la pacificación de Centroamérica. La política militar de Reagan en Centroamérica fracasó y en 1987, Arias recibió el Premio Nobel de la Paz luego de la firma del Acuerdo de Esquipulas por los cinco presidentes de Centroamérica. Los cambios producidos con la desintegración de la Unión Soviética (1989) hicieron que Estados Unidos perdiera interés en Centroamérica, lo que llevó a una reducción en la ayuda estadounidense a Costa Rica, en parte compensado por el alza en la inversión extranjera directa, que alcanzó los 542 millones de dólares en 2004.

El modelo neoliberal se profundizó en los gobiernos subsiguientes (Óscar Arias, 1986-1990; Rafael Ángel Calderón Fournier, 1990-1994; José María Figueres Olsen, 1994-1998; Miguel Ángel Rodríguez, 1998-2002), con políticas de reducción del apoyo estatal a los productores agrícolas, la firma del Pacto Figueres-Calderón para acelerar el ajuste neoliberal y reducir las funciones del Estado, firma de programas de ajuste estructural con el Banco Mundial y el FMI, e intentos de privatizar activos estatales, como el ICE. Estos cambios causaron fuertes protestas por parte de campesinos y agricultores (1987), alza del empleo informal y la deserción estudiantil, y el aumento de la pobreza (27 a 32% entre 1990 y 1991, con una breve recuperación económica y reducción de la pobreza en 1994-2005); 89 paros y huelgas entre 1990 y 1993, protestas de la ciudadanía y vastas manifestaciones populares, como el movimiento nacional de protesta en 2000, con bloqueos de vías, paros laborales y actos de desobediencia civil que obligaron al gobierno a retirar dicho proyecto.

Para finales del siglo XX, Costa Rica se transformó económicamente: diversificó su economía con productos no tradicionales, la llegada de empresas de alta tecnológica encabezadas por la transnacional Intel (1996), la promoción del ecoturismo aprovechando la excepcional biodiversidad del país, y una economía basada en el sector servicios. Se fortalecieron los derechos civiles con la creación de la Sala Constitucional (1989) y la Defensoría de los Habitantes (1992), y el Estado acrecentó las oportunidades de segmentos específicos de la población, mientras que el giro hacia el mercado incrementó el número de entes privados en los campos de educación, salud y seguridad. En lo político, el ascenso del neoliberalismo desarticuló las vanguardias sociales y culturales del país, a lo que se sumó el desgaste del sindicalismo y las disputas internas de los partidos de izquierda, lo que significó un deterioro de los derechos laborales. Sin embargo, la impugnación del neoliberalismo resultó electoralmente ventajoso, lo que propició el ascenso de nuevos partidos políticos que tendrán un papel protagónico a inicios del siglo XXI.

La Costa Rica del siglo XXI plantea grandes desafíos para el pueblo costarricense y sus gobernantes, con cuestionamientos a su modelo democrático. Al desgaste del bipartidismo, potenciado por escándalos políticos y de corrupción, se ha sumado una larga crisis de gobernabilidad nacional, cuestionamientos a las políticas usadas en temas como ambiente, infraestructura, derechos humanos, seguridad ciudadana, el incremento del crimen organizado, problemas en servicios de salud, empleo, vivienda, y crisis en finanzas de instituciones públicas, lo que ha desembocado en el desencanto de los ciudadanos. Esto ha impulsado el surgimiento de nuevas organizaciones políticas, lo que se ve reflejado en una conformación multipartidaria de la Asamblea Legislativa, que ha sido la tónica desde 2002; es necesario acudir a una segunda ronda para elegir al presidente en las elecciones de 2002, 2014 y 2018, algo inédito hasta el momento; en 2006, Óscar Arias (2006-2010) accedió a su segundo mandato al ganar las elecciones por apenas un 1% de los votos válidos a su contendiente Ottón Solís del PAC, en una elección muy polarizada con alto abstencionismo, situación que se volverá a reflejar en el referéndum sobre el TLC con Estados Unidos (2008); en 2010, es electa por primera vez una mujer como mandataria (Laura Chinchilla Miranda, 2010-2014), y en 2014, por primera vez desde 1948, es elegido un gobernante proveniente de una fuerza política distinta al PLN o el PUSC (Luis Guillermo Solís del PAC, 2014-2018).
El país se vio afectado por la Crisis económica de 2008-2015. Además, ha visto un incremento en su déficit fiscal entre 2006 y 2018, lo que ha motivado la discusión sobre la aprobación de reformas fiscales, a la vez que se ha puesto en tela de juicio el gasto del gobierno por concepto de convenciones colectivas, altos salarios en instituciones estatales y pensiones de lujo. Durante el 2014 y 2015 el país incrementó sus lazos económicos con Europa, y logró un acuerdo con Alemania de cooperación en educación. En 2015 la FAO premió a Costa Rica por haber reducido el hambre, al reducirse de un 5% a la mitad según cálculos de Naciones Unidas. También se convirtió en el país con mayor expectativa de empleo de Latinoamérica según encuesta de la firma estadounidense Manpower.
En el plano internacional, un conflicto diplomático con Nicaragua por la incursión de militares nicaragüenses en la costarricense Isla Calero, debió resolverse en la Corte Internacional de Justicia, la cual reconoció en 2015 la soberanía de Costa Rica sobre la isla, y en 2018, la Corte definió los límites marinos de ambos países, en donde Costa Rica obtiene alrededor de 10 000 km² en el océano Pacífico y en el mar Caribe. En la misma resolución, la Corte calificó la incursión de tropas nicaragüenses como una violación a la soberanía costarricense, y determina que, por daños ambientales generados en la Isla Calero en 2010, Nicaragua debe pagar $378.890 como indemnización a Costa Rica.
Los derechos humanos fueron el tema principal durante las elecciones presidenciales de Costa Rica del 2018 cuando en enero de ese año, después de una consulta previamente realizada por el Gobierno de Costa Rica en mayo del 2016, la Corte Interamericana de Derechos Humanos, con sede en San José, indica que Costa Rica y el resto de naciones parte de la Corte IDH deben de garantizar el matrimonio igualitario entre parejas del mismo sexo para asegurar la protección de las mismas. Esta resolución divide al país, y marca el proceso de las elecciones, en donde los candidatos Fabricio Alvarado, del partido evangélico conservador Restauración Nacional, opuesto a la medida, y Carlos Alvarado, del oficialista Acción Ciudadana, quien aprueba la resolución de la Corte IDH, pasan a una segunda ronda electoral tras ninguno alcanzar el 40% de los votos necesarios. Finalmente, Carlos Alvarado es elegido como 48° presidente de Costa Rica luego de obtener la victoria en segunda ronda con el 60% de los votos válidos.

El 8 de agosto la Sala Constitucional IV de la Suprema Corte de Justicia declaró inconstitucional la prohibición del matrimonio igualitario y dictaminó un plazo de dieciocho meses (hasta 2020) para implementarlo legalmente.

Costa Rica está regida por la constitución política del 7 de noviembre de 1949, en la cual se establece un sistema presidencialista y un estado unitario.

Es ejercido por el Presidente de la República, quien es elegido por voto popular directo, secreto y universal durante un periodo de 4 años, entre sus facultades está el nombramiento de los presidentes de las Instituciones Autónomas, nombramiento o destitución de ministros y diplomáticos costarricenses, vetar leyes, firmar decretos, indulto de penas, asueto nacional y duelo o luto nacional. La Presidencia actual es ocupada por Carlos Alvarado Quesada, desde el 8 de mayo de 2018. Le acompañan dos vicepresidentes también elegidos por voto popular: Epsy Campbell Barr y Marvin Rodríguez Cordero, ambos también desde el 8 de mayo de 2018.

Recae en la Asamblea Legislativa de Costa Rica, la cual es un órgano unicameral con 57 diputados, encargado de aprobar, reformar o derogar leyes y decretos. Los legisladores son elegidos por voto popular directo, secreto y universal durante un período de 4 años y tienen carácter provincial. Su actual presidente es Eduardo Cruickshank.

Está conformado por la Corte Suprema de Justicia.

La Corte está compuesta por 22 magistrados electos por la Asamblea Legislativa durante un período de 8 años. Su actual presidente es el magistrado Fernando Cruz Castro.

Sobre la Corte Suprema de Justicia recae el nombramiento de los Magistrados que conforman el Tribunal Supremo de Elecciones.

En Costa Rica las elecciones generales (Presidente y Diputados a la Asamblea Legislativa) son realizadas cada cuatro años y las municipales (Alcaldes, Regidores, Síndicos, Concejales de Distrito e Intendentes, donde corresponda elegir está última figura) desde 2016 se realizan cada 4 años, pero sin coincidir con las elecciones generales (dos años después de estas), y son organizadas por el Tribunal Supremo de Elecciones de Costa Rica, creado en 1949 por la actual Constitución Política. El Tribunal Supremo de Elecciones es el Órgano Constitucional superior en materia electoral y por lo tanto responsable de la organización, dirección y vigilancia de los actos relativos al sufragio. Goza de independencia en el desempeño de su cometido.

Los partidos con representación parlamentaria en el país son (en orden alfabético): Acción Ciudadana (en el gobierno), Frente Amplio, Liberación Nacional, Integración Nacional, Republicano Social Cristiano, Restauración Nacional y Unidad Social Cristiana. En total hay 19 partidos a escala nacional, 18 provincial y 55 cantonal para un total de 161 partidos políticos inscritos.

El Democracy Index (índice de democracia) es la clasificación elaborada por la Unidad de Inteligencia de The Economist, a través de la cual se pretende determinar el rango de democracia en 167 países.

Costa Rica es —junto con Uruguay— el único país de Latinoamérica considerado como una democracia plena, ubicado entre los 20 mejores sistemas democráticos de todo el planeta y alcanzando elevadas puntuaciones en varios rubros del estudio, superiores a los de la mayoría de países de la Unión Europea.

Costa Rica es miembro fundador de la Organización de las Naciones Unidas, sostiene relaciones diplomáticas con casi todos los países de América y Europa. En tres ocasiones ha sido parte del Consejo de Seguridad de las Naciones Unidas: 1974-1975, 1997-1998 y 2008-2009. No es miembro del Parlamento Centroamericano.

Costa Rica enfrenta realidades muy distintas en las relaciones diplomáticas con sus dos países vecinos. La situación más complicada es la larga disputa que sostiene con Nicaragua por el derecho de navegación del Río San Juan. Con Panamá, ambos países tienen relaciones cordiales y amistosas, y una frontera abierta.

Las relaciones con Estados Unidos, principal socio comercial, datan de 1851. El país ha sido visitado por siete presidentes de Estados Unidos: Hoover (1928), Kennedy (1963), Johnson (1968), Reagan (1982), Bush (1989), Clinton (1997) y Obama (2013). Las relaciones con México datan de 1831 y con Brasil desde 1907. Las relaciones con Cuba estuvieron interrumpidas por cuestiones políticas entre 1961 y 2009. La embajada en La Habana fue reabierta en 2009, y en 2015 Luis Guillermo Solís se convirtió en el primer presidente en realizar una visita oficial a Cuba desde la ruptura de relaciones. El país debió enfrentar la crisis migratoria cubana de 2015-2016, lo que motivó su retiro político del SICA, aunque mantuvo sus responsabilidades comerciales y económicas.

Costa Rica no tiene ejército, pues este fue abolido el 1 de diciembre de 1948, abolición que fue perpetuada en el artículo 12 de la Constitución Política de 1949. Este mismo artículo contempla la formación de un ejército ya sea por convenio continental o para la defensa nacional, el cual siempre estará subordinado al poder civil. Costa Rica cuenta, desde 1983, con una Ley de Neutralidad que la inhibe de participar en conflictos bélicos de forma perpetua, activa y no armada, y en 2014 aprobó una Ley de Proclamación de la Paz como Derecho Humano y de Costa Rica como País Neutral, en la que establece que el país debe tomar una posición neutral en conflictos armados internacionales, además de que obliga al Estado a incluir dentro de sus programas educativos contenidos que cultiven la cultura de paz.

La seguridad ciudadana del país recae en el Ministerio de Seguridad Pública, el cual también es el encargado de la defensa de la soberanía nacional en caso de ser necesario, aunque la política exterior del país en caso de conflictos internacionales se ha orientado hacia el derecho internacional. El Ministerio de Seguridad Pública está dividido en varias Direcciones: Fuerza Pública, Servicio Nacional de Guardacostas, Vigilancia Aérea, Policía de Control de Drogas, Escuela Nacional de Policía, Armamento, Reserva y Servicios de Seguridad Privada. Costa Rica destina el 0.69 % del Producto Interno Bruto (29.240 millones de dólares), y el 0.03 % del presupuesto nacional, para la seguridad nacional.

La división territorial de Costa Rica comprende 7 provincias divididas en 82 cantones y estos a su vez, subdivididos en 485 distritos colegiados.

Costa Rica está localizada en el istmo de América Central, entre las latitudes 8° y 12°N, y las longitudes 82° y 86°O. Limita al este con el Mar Caribe y con el Océano Pacífico al oeste, con una extensión total de 1290 km en costas: 212 km en la costa caribeña y 1 016 km en la costa pacífica.

Costa Rica además limita al norte con Nicaragua (309 km de frontera) por el tratado Cañas-Jerez de 1858 y el Laudo Cleveland de 1888 y con Panamá al sureste (330 km de frontera) por el tratado Echandi-Fernández de 1941. En cuanto a los límites marítimos, Costa Rica limita con Ecuador en el Océano Pacífico por el tratado Gutiérrez-Terán de 1985, con Colombia en el Mar Caribe por el tratado Facio-Fernández de 1977 y en el Océano Pacífico por el tratado Gutiérrez-Lloreda de 1984 y con Panamá por el tratado Calderón-Ozores de 1980. En total, Costa Rica comprende 51 100 km más 589 000 km de aguas territoriales.

Costa Rica es un país muy montañoso y la mayor parte del territorio está formado por elevaciones de entre 900 y 1800 metros sobre el nivel del mar. Existen cuatro sistemas montañosos principales: la cordillera Volcánica de Guanacaste, la sierra Minera de Tilarán, la cordillera Volcánica Central y la cordillera de Talamanca, existen además cuatro sistemas montañosos secundarios: serranías de Nicoya ubicadas en la provincia de Guanacaste; fila Brunqueña o fila Costera que recorre la provincia de Puntarenas en forma paralela a la costa; y la sierra de Osa, en el Pacífico Sur, una serie de montañas que se extienden hasta Panamá, posee alturas entre los 600 y 1500 msnm. El punto más alto en el país es el cerro Chirripó (3820 msnm), el quinto pico más alto en América Central. El volcán más alto es el volcán Irazú (3432 msnm). El país posee cerca de 200 , de los cuales cinco se encuentran activos: Irazú, Poás, Arenal, Rincón de la Vieja y Turrialba. Los ciclos eruptivos más importantes en tiempos históricos los han protagonizado el Irazú (1963-1965), el Arenal (1968-2010), el Turrialba (2014-2019) y el Poás (2016-2019). Costa Rica es un país altamente sísmico: la mayoría de los terremotos son producidos por la interacción de las placas Cocos y Caribe, además del . Algunos de los han sido: terremoto de Santa Mónica o de Cartago (1910), Alajuela (1990), Limón (1991), Parrita (2004), Cinchona (2009) y Nicoya (2012).

El eje montañoso central del país produce áreas bajas hacia el Caribe norte y el Pacífico, originadas por acumulación de materiales sedimentarios. Las más extensas son las llanuras del Norte y el Caribe, surcadas por ríos largos y caudalosos que forman meandros. Toman distintos nombres según las zonas: Guatusos, San Carlos, Sarapiquí, Tortuguero, Santa Clara, Pacuare, Matina y Estrella, además de los valles de Sixaola y Talamanca. La llanura costera del Caribe tiene poca altitud y está parcialmente cubierta de selvas tropicales. Sus suelos se usan en plantaciones de banano y cacao, ganadería de carne y leche. El canal de Tortuguero, de 112 km de largo, comunica Moín con la frontera con Nicaragua. En el Pacífico norte, la llanura más grande, conocida como la pampa, se ubica en Guanacaste, donde alcanza 75 km de anchura. En el Pacífico central la llanura se estrecha por la presencia de la fila Brunqueña, y vuelve a ensancharse hasta 50 km en el Pacífico sur. La pampa guanacasteca se extiende desde la meseta de Santa Rosa hasta el golfo de Nicoya, y está conformada por planicies de aluviones y mantos de cenizas volcánicas. La franja central posee dos terrazas marinas, interrumpidas por algunos cerros no mayores de 100 m de altitud. El valle de Parrita se extiende hasta punta Uvita y está formado por material sedimentario y volcánico, procedente de la cordillera de Talamanca. El Pacífico sur posee una llanura peninsular en Osa, además de los valles de Diquís y Coto Brus, formados por materiales marinos, fluviales y volcánicos. Los terrenos del Pacífico son propicios para la agricultura, poseen mayor infraestructura turística y en sus costas se desarrolla la mayoría de la actividad pesquera del país.

En el centro del país se encuentra el Valle Central, una meseta tectónica caracterizada por su fertilidad y abundancia de fuentes hídricas, rodeada por montañas y volcanes, presenta un clima muy agradable, aquí reside la mayor parte de la población del país, aproximadamente 60% de los habitantes. La única provincia que no limita con la provincia de San José es la provincia de Guanacaste.

Costa Rica posee muchas islas. En el Caribe, se destacan las islas fluviales, como isla Calero, que es además la isla más grande del país con 151.6 km². Otra isla importante del Caribe es la isla Uvita, ubicada frente al puerto de Limón. En el océano Pacífico, se encuentran las islas del archipiélago del golfo de Nicoya, muchas de ellas habitadas (Chira, Venado, Caballo), con importancia para el turismo (San Lucas, Tortuga) y otras que constituyen reservas biológicas (Guayabo, Pájaros, Negritos). En bahía Salinas, se encuentra la isla Bolaños, que también es refugio de fauna silvestre. En el Pacífico sur, la isla del Caño tiene importancia ecológica y arqueológica. La Isla del Coco (24 km²) es la isla de Costa Rica más conocida a nivel internacional. Ubicada en el océano Pacífico, a 500 km del puerto de Puntarenas, se destaca por su distancia a la plataforma continental. Es de gran importancia por su biodiversidad y ha sido declarada Patrimonio de la Humanidad.

Cerca del 25 % del territorio nacional se encuentra protegido por el SINAC (Sistema Nacional de Áreas de Conservación), que supervisa todas las áreas protegidas del país. Costa Rica posee una de las mayores densidades de especies del mundo.

Costa Rica cuenta con una red hidrográfica extensa, la cual se encuentra distribuida en tres vertientes, delimitadas naturalmente por el sistema montañoso del país, y se clasifican según su desembocadura en vertientes norte, Caribe y Pacífico.

Los ríos que forman la vertiente norte desembocan en el lago de Nicaragua o en el río San Juan, son generalmente cortos y tienen un régimen torrencial. Algunos nacen en las faldas de los volcanes. Los ríos más importantes de esta vertiente son: Sapoá (32 km), Frío (70 km), Haciendas y San Carlos (125 km). A este se le unen los ríos Tenorio, Arenal, Coto y La Muerte, para formar el sistema hidrográfico que baña las llanuras de San Carlos y los Guatusos. El otro sistema hidrográfico de esta vertiente lo forman los ríos Sucio, Toro, Toro Amarillo, Blanco y Cuarto, que se unen para formar el río Sarapiquí (103 km), el cual es navegable en más de la mitad de su curso.

Los ríos de la vertiente del Caribe tienen un torrente constante durante todo el año, relacionado con las abundantes precipitaciones de la región atlántica. El más importante es el río Reventazón (110 km), el segundo más largo del país y el de mayor capacidad para la generación de energía hidroeléctrica de Centroamérica. Junto con el río Parismina (92 km) forma la tercera cuenca hidrográfica del país. El río Sixaola (76 km) forma límite natural con Panamá, nace en las faldas de la cordillera de Talamanca y es navegable. Se destacan también los ríos Colorado (navegable), Chirripó Norte (96 km), Tortuguero (85 km), Pacuare (133 km), Jiménez, Matina y Chirripó Atlántico (92 km) , Moín, Limón, Banano, Bananito y La Estrella (52 km).

Los ríos de la vertiente del Pacífico suelen ser tranquilos, con un caudal menos torrentoso, que nacen en las faldas de los volcanes. El río más importante es el río Tempisque, cuya longitud es de 144 km, es navegable, y forma un gran sistema hidrográfico compuesto por ríos como Liberia, Bolsón, Salto, Potrero, Piedras Blancas, Tenorio, Corobicí, Cañas y Lajas, lo que le da irrigación a la importante región agrícola de Guanacaste. En la península de Nicoya se destacan los ríos Morote, Nosara, Lajas y Bongo.

En la región del Pacífico Central se ubican los ríos Jesús María y Grande de Tárcoles, cuyas aguas provienen del Valle Central Occidental; sus afluentes son el río Grande y el Virilla, que tienen sus cuencas en las provincias de San José, Heredia y Alajuela. La cuenca del río Grande de Tárcoles (111 km) es muy importante para el país, dado que aporta toda la riqueza de sus recursos naturales. Tiene un área de 2.121 km², una precipitación de 2 456 mm y un caudal de 48 litros por segundo y kilómetro cuadrado. Los ríos Guacimal, Abangares, Aranjuez y Barranca nacen de la sierra de Tilarán y durante la estación seca su caudal merma.

Los ríos Parrita o Pirrís (82 km), Naranjo y Savegre depositan sus aguas en el Pacífico sur del país. El río Grande de Térraba (o "Díquis", su nombre aborigen), formado por los ríos General y Coto Brus, es el más extenso (186 km) y caudaloso del país. Solo es navegable en su curso inferior (22 km), cuando recorre una extensa zona sembrada de palma africana. Desemboca mediante un amplio delta cubierto de manglares en el Golfo Dulce.

Costa Rica se encuentra en la región del neotrópico, por lo que presenta un clima tropical sin grandes variaciones anuales en su temperatura y con dos estaciones bien definidas: la seca, desde principios de diciembre hasta finales de abril (llamada también verano), y la lluviosa, desde principios de mayo hasta finales de noviembre (llamada invierno). Sin embargo, la topografía del país es muy variada, con presencia de montañas, valles y llanuras en una extensión pequeña, lo que contribuye a la existencia de diferentes y muy heterogéneos paisajes climáticos a lo largo del territorio. Al estar ubicado en el hemisferio norte, el país se ve afectado durante los cambios de estación, durante los meses de diciembre y enero, cuando las temperaturas suelen descender mucho considerablemente por los vientos del norte en los lugares más altos del país. La duración del día se ve afectada durante los solsticios y equinoccios. La estructura montañosa del país determina el origen de tres grandes regiones climáticas con características diferenciadas: Atlántico, Pacífico y Central.

Con tan solo el 0.03 % de la superficie terrestre mundial, Costa Rica posee aproximadamente el 6 % de la biodiversidad del planeta. Para el año 2013 , un 52.4 % de la superficie continental total del país se encontraba cubierta de bosques y selvas, mostrando un incremento de un 12% en la recuperación de la cobertura boscosa en los últimos 17 años. Aproximadamente el 25% del territorio se encuentra protegido. Entre 2005 y 2010, el país presentó una reducción en su tasa de deforestación.

Costa Rica cuenta con una mayor superficie marítima que continental dado que la zona oceánica es de 589 000 km² aproximadamente, que incluye la Isla del Coco la cual está situada a unos 480 km al suroeste de la Península de Osa, en la costa del Océano Pacífico. Esta isla fue declarada Patrimonio Natural de la Humanidad por la UNESCO en el año 1997.

El país cuenta con más de 1000 especies de orquídeas, siendo Monteverde la región con mayor densidad de orquídeas del planeta. En total Costa Rica alberga a más de 10 000 especies de plantas. Además Costa Rica obtiene cerca $1 millón al año por la exportación de más de 900 000 crisálidas de 150 especies de mariposas, como la Morpho –una de las más grandes y de color azul intenso.

En este país hay un verdadero paraíso conocido por la diversidad de su fauna. Da cobijo a 232 especies de mamíferos, 838 especies de aves, 183 especies de anfibios, 258 especies de reptiles y 130 especies de peces de agua dulce. Entre las especies más sobresalientes que habitan el país están el puma, el jaguar, el venado, el mono, el coyote, el armadillo y varias especies de aves entre las que se destacan el quetzal, el yigüirro y el colibrí. En el país se hallan 14 de las 149 especies conocidas de ranas de cristal. Desde 1973, no se había encontrado ninguna nueva especie de rana traslúcida.Costa Rica es el primer país del continente americano en prohibir la cacería de animales por deporte.

Costa Rica posee una economía mixta, que ha sufrido una fuerte evolución, pasando de ser un país eminentemente agrícola a una economía de servicios. Según el Índice de Competitividad Global del Foro Económico Mundial, en el 2018 Costa Rica ocupó el cuarto lugar entre las mejores economías de América Latina y el Caribe, por detrás de Chile, México y Uruguay.

El turismo es la industria con mayor crecimiento y desde inicios de la década de 2000 genera más divisas que cualquiera de los principales productos agrícolas de exportación.

Son también de gran importancia las exportaciones agrícolas tradicionales del banano, el azúcar, el cacao y la piña; así como flores y minivegetales en los últimos años. Se destaca la producción de café costarricense de alta calidad y su exportación al mercado estadounidense en donde es muy apreciado. 

Asimismo, la producción de insumos médicos y quirúrgicos, de alta tecnología y de componentes electrónicos; así como el desarrollo de software, los servicios financieros y de atención y las operaciones de outsourcing, aportan cada vez más a la economía.

La economía del país creció un 8.8% en 2006, un 6.8% en 2007 y un escaso 3% en el 2008. Para finales del 2015, la actividad económica del país creció en un 4.3%, el mejor cierre de los últimos cuatro años. Para el 2014, el PIB interanual ascendió un 3,5% con respecto al de 2013. El país tiene la sexta tasa de inflación más alta del Hemisferio. Durante el 2015, el país experimentó deflación.

Según datos del Instituto Nacional de Estadística y Censos, el ingreso promedio por hogar fue de 1.034.362 colones mensuales en 2017 (aproximadamente US$1.815 mensuales). Mostró un crecimiento de 0,7% respecto al año 2016. La pobreza está estimada en 16.5%, lo que la ubica en el tercer lugar entre los países de América Latina con menos pobreza, según Cepal. Según la Encuesta Nacional de Hogares del Instituto Nacional de Estadística y Censos, dicho rubro alcanzaba en 2015 el 21.7%.

El turismo es la principal fuente de ingreso de divisas de la economía costarricense. El sector de turismo da empleo directo a 130.000 personas. Costa Rica ofrece actualmente 45 531 habitaciones. Con un ingreso anual de US$ 2636 millones en 2014, la industria turística de Costa Rica obtuvo un 30 % de los ingresos por turismo de región centroamericana, y se destaca como el destino más visitado del istmo, ya que en 2014 el país alcanzó un nuevo récord histórico de visitantes con 2 526 817 turistas. Para 2015, el número de visitantes creció en un 5.5 %, para 2 665 608 visitantes. Con alrededor de 500 visitantes por cada mil habitantes, Costa Rica tiene uno de los índices más altos de turistas per cápita de la Cuenca del Caribe. En la clasificación del Índice de Competitividad en Viajes y Turismo de 2017, Costa Rica alcanzó el lugar 38, siendo el cuarto clasificado entre países de América Latina.

La mayoría de los visitantes extranjeros proviene de los Estados Unidos y Canadá (67.5%) y de países de la Unión Europea (16%), lo que le permite recibir en media aproximadamente entre US$1300 y $1400 por visitante, valor por visita entre los más altos de América Latina. En 2005 el turismo contribuyó con un 8,1 % del PIB del país, y representó un 13,3 % de los empleos directos e indirectos. Desde inicios de los años 2000, el turismo genera para el país más ingreso de divisas que la exportación de banano y café juntos.

El país ofrece condiciones de sol, playa, selva, montaña, ciudad, aventura, ecoturismo, turismo rural comunitario, y presenta potencial en las áreas de deportes, bienestar y salud, moda y turismo de reuniones. El ecoturismo es extremadamente popular entre los turistas extranjeros que visitan la amplia cantidad de parques nacionales y áreas protegidas que existen por todo el país. Costa Rica fue uno de los pioneros en ecoturismo y es reconocido como uno de los pocos destinos internacionales con verdaderas opciones de turismo ecológico. 

La economía costarricense ha estado históricamente basada en la agricultura, y aún hoy sigue siendo un sector importante en la generación de divisas. El café se destaca por haber sido el motor económico del país durante buena parte su historia, constituyéndose junto al banano en la base del modelo agroexportador por lo menos hasta 1980. Actualmente es el tercer producto agrícola de exportación, detrás de la piña y el banano. Asociado al cultivo cafetalero, existe toda una industria paralela basada en la producción de café refinado (gourmet), coffee tours y producción de artesanías y productos alusivos al grano.

Asimismo, el país es el séptimo mayor productor de bananos a nivel mundial, con una exportación de 100 millones de cajas anuales (1.8 millones de toneladas métricas), lo que significó ventas en 2014 por 903 millones de dólares. Para el año 2014, la exportación bananera significó el 8 % del total de las exportaciones del país. Los principales destinos son la Unión Europea (48.5%) y los Estados Unidos (40.5 %). Además, Costa Rica es el principal exportador mundial de piña desde 2013, generando 27 000 empleos directos y 110 000 indirectos. 

Otros cultivos importantes son el cacao, que se destaca entre los mejores del mundo, la caña de azúcar, la palma africana, diversos tipos de flores y hortalizas.

El Ministerio de Obras Públicas y Transportes (MOPT) es la institución pública encargada de la construcción y el mantenimiento de la infraestructura vial, aeroportuaria y portuaria del país, la regulación del tránsito, la elaboración de mapas y cartas geográficas del país, y la atención de calamidades y desastres que afecten la infraestructura nacional. Esta institución fue creada en 1860 como Dirección General de Obras Públicas, adscrita a una de las Secretarías del Estado. A partir de 1870, pasó a formar la Cartera de Obras Públicas y en 1948, se convirtió en Ministerio de Obras Públicas, que luego pasó a llamarse Ministerio de Transportes (1963), y finalmente tomó su nombre actual en 1971.

El sistema ferroviario de Costa Rica cuenta con 278 km y está a cargo del Instituto Costarricense de Ferrocarriles (INCOFER). Costa Rica posee en total 151 aeropuertos y pistas de aterrizaje, de los cuales 36 cuentan con caminos pavimentados. De ellos, existen cuatro aeropuertos internacionales: el Aeropuerto Internacional Juan Santamaría, ubicado en la ciudad de Alajuela; el Aeropuerto Internacional Daniel Oduber Quirós, en la ciudad de Liberia, Guanacaste; el Aeropuerto Internacional de Limón, en esta provincia del Caribe; y el Aeropuerto Internacional Tobías Bolaños, en San José. El Juan Santamaría se considera el principal aeropuerto del país.

Los dos principales puertos de Costa Rica son Puerto Limón, en el Caribe, y Puerto Caldera, en el Pacífico, por donde se realizan las exportaciones e importaciones del país por vía marítima.

La constitución política de Costa Rica garantiza la libertad de expresión y de prensa en su artículo 29. Según la organización Reporteros sin Fronteras, Costa Rica es el país de América Latina mejor calificado en libertad de prensa, y en 2018 ocupó el décimo lugar a nivel mundial.

Costa Rica tiene una gran tradición periodística. Han existido publicaciones de todos los temas, tanto políticos, religiosos, como de protesta. El periodismo en Costa Rica como tal fue creado el 25 de noviembre de 1824 mediante el decreto 23 de la Colección de Leyes de ese año, durante el gobierno de Juan Mora Fernández, el cual hace una invitación a los ciudadanos a establecer periódicos manuscritos. La imprenta fue introducida a Costa Rica en 1830, importada por Miguel Carranza Fernández, y se llamó "La Paz". El primer medio impreso de Costa Rica fue creado por Joaquín Bernardo Calvo (a quien se considera como «"Padre del periodismo costarricense"») cuando el 4 de enero de 1833 circuló el primer número del semanario "El Noticioso Universal". En la actualidad circulan en el país varios diarios impresos, siendo los principales La Nación, La República, Semanario Universidad, The Tico Times y Diario Extra, que también cuentan con edición digital. Existen también algunos periódicos independientes de Internet, como CRHoy, La Prensa Libre, El Mundo CR, Nuestro País y AMPrensa.com.

La historia de la televisión de Costa Rica inicia el 9 de mayo de 1960 cuando el empresario René Picado y Carlos Manuel Reyes crean la primera compañía de televisión en el país denominada Televisora de Costa Rica (Teletica) o más popularmente conocida, Canal 7. Televisora de Costa Rica empezó a transmitir varias series compradas a los Estados Unidos tales como la Familia Monster, Mister Ed y otras más. Posteriormente iniciaron las transmisiones en vivo. La primera en estrenar esta modalidad fue la visita del entonces Presidente de los Estados Unidos, John F. Kennedy a Costa Rica en 1963. Otro recordatorio de las primeras transmisiones en vivo vía microonda y satélite fue la llegada del primer hombre a la luna el 20 de julio de 1969. En ese entonces 600 millones de personas vieron el espectáculo alrededor del mundo.

En la década de los 70 inició la transmisión de teleseries nacionales, lo que se extendió hasta la actualidad. Luego nace el primer noticiero en la historia de Costa Rica, denominado Telenoticias.

Hasta finales de la década de los 80 se inició la venta de transmisiones por cable, por lo que aumentaron las opciones de poder ver eventos deportivos, series de entretenimiento, cultura y programas educativos y hasta científicos.

En noviembre de 1994 aparece la empresa Representaciones Televisivas S.A., mayormente conocida como Repretel. Como inauguración adquieren el Canal 9, que pertenecía a Multivisión. Meses más tarde y ya para mayo de 1995, Repretel adquiere la frecuencia que durante varios años perteneció a Telecentro, el Canal 6.

En 1996, se adquirió el Canal 11 donde empezaría a transmitir programación destinada a los más pequeños y a público en general. En 2000, se devolvió a su concesionario el Canal 9 e inmediatamente se empezó a transmitir en el Canal 4 (que también pertenecía a Multivisión, cadena que años más tarde fue organizada de una sociedad cooperativa).

"Emisora Pública"

"Emisoras Privadas"

Costa Rica es uno de los países pioneros en América Latina en cuanto a radiodifusión. En 2012, se reportaron un total de 125 radioemisoras transmitiendo desde todo el territorio nacional, tanto en amplitud modulada como en frecuencia modulada, la mayoría de ellas agrupadas en la Cámara Nacional de Radiodifusión (CANARA). Existen desde grandes grupos radiofónicos como Central de Radios-Repretel, Cadena Radial Costarricense, Cadena de Emisoras Columbia, Grupo Radiofónico Omega y Prisa Radio-Multimedios Radio hasta pequeños radiodifusores independientes. Los programas tratan temas de la más diversa índole: deportivos, radionoticieros, musicales, juveniles, infantiles, religiosos, humorísticos, políticos y otros. Entre las principales radioemisoras del país pueden mencionarse: Monumental, Columbia, Sinfonola 90.3, Azul 99.9, Omega, Fides, Faro del Caribe, EXA, Los 40 Principales, Musical, WAO, La mejor FM, Zeta FM, Bahía Puntarenas, Bésame, 95.5 Jazz, Malpaís, 959, 103 FM, Radio U, Momentos Reloj, 106.3 Pura Vida, 94.7, Radio Dos, 91.5 Teletica Radio y muchas otras más.

En Costa Rica se explotan cinco fuentes de energía, en orden de importancia: hídrica, térmica, geotérmica, eólica y solar. En América Latina, Costa Rica se destaca como líder en la producción de energía renovable, debido principalmente a la producción de energía hidroeléctrica, según el informe "Líderes en energía limpia" de la organización World Wildlife Fund (WWF).

La primera planta hidroeléctrica del país, Aranjuez y ubicada en el centro de San José, entró en operación en 1884, convirtiendo en esa época a la capital costarricense en la tercera ciudad del mundo iluminada por energía eléctrica, luego de Nueva York y París. A partir de ese acontecimiento se continuaron construyendo diversas obras de generación eléctrica en varios lugares del país, como producto de iniciativas de las municipalidades y de empresarios privados, tanto nacionales como extranjeros.

La producción de energía eléctrica en Costa Rica está a cargo del Instituto Costarricense de Electricidad (ICE, 1949), empresa estatal que también brinda el servicio de telecomunicaciones. Entre las funciones del ICE están:


El desarrollo de la cobertura eléctrica del país a través de los años y como consecuencia de la obra realizada por el ICE, y otras empresas de distribución eléctrica, ha tenido un desarrollo vertiginoso en las tres áreas que componen un sistema eléctrico: Generación, Transmisión y Distribución. Cuando se creó el Instituto aproximadamente el 15% del territorio nacional continental tenía cobertura eléctrica. Cincuenta y un años después, en el año 2000, ese porcentaje llegaba a un 94.4%. Para 2009, el porcentaje de cobertura energética alcanza el 98.6% del país, un porcentaje comparable al de países desarrollados. En 2014, el servicio eléctrico costarricense fue catalogado como el segundo mejor de América Latina, superado únicamente por Uruguay, según un estudio realizado por la Federación Interamericana de la Industria de la Construcción (FICC).

En 2015, Costa Rica cerró el año con un 99% de generación de energía eléctrica a partir de fuentes renovables, sin utilizar hidrocarburos. Esto le valió un reconocimiento al país en el marco de la XXI Conferencia sobre Cambio Climático (COP21) celebrada en París, al ser iluminada la emblemática Torre Eiffel con el lema ""100 % Pura Vida"", en alusión a Costa Rica, como parte de la campaña ""One Heart, One Tree"". El 16 de septiembre de 2016, el ICE puso en total funcionamiento el Proyecto Hidroeléctrico Reventazón, la planta hidroeléctrica más grande de Centroamérica, la mayor obra de infraestructura construida en el país en toda su historia y la segunda construcción en tamaño en la región después del Canal de Panamá, con una capacidad instalada de 305.5 megavatios y una inversión de US$1.400 millones. Costa Rica es el tercer país de América con mayor producción de energía geotérmica, después de Estados Unidos y México.

Según el Instituto Nacional de Estadística y Censos la población de Costa Rica en junio de 2019 alcanzaba los 5.057.000 habitantes.

En cuanto a su crecimiento, la misma lo hace anualmente al alto ritmo de 1,4%. Esto se debe a una natalidad moderadamente alta, 16,5 por cada 1000 y a una mortalidad baja, 4,8 (5) por cada 1000 habitantes, sin embargo ha variado 5 milésimas más esa cantidad del 2017 al 2018.

La población costarricense es multiétnica y pluricultural, originada en el mestizaje amplio e interminable entre sus pobladores nativos (pertenecientes a las áreas Mesoamérica e Intermedia), diversos grupos europeos, sefardíes de origen mediterráneo, asiáticos y africanos. Actualmente en el país habitan más de 4,85 millones de descendientes de españoles (blancos o mestizos), de 250.000 a 500.000 descendientes de italianos, 400.000 afrodescendientes, y más de 60.000 descendientes de chinos, entre otros; que según el último censo realizado en 2011 se autoperciben como un 83 % de blancos y mestizos, 6,72 % de mulatos, 2,42 % de indígenas, 2 % de negros y 5,95 % de otros grupos. 

Costa Rica es el país latinoamericano con mayor cantidad de inmigrantes porcentualmente. Según la Organización Internacional para las Migraciones hacia 2017 viven 414 148 extranjeros en territorio costarricense, lo que roza un 9% de la población total. Esta cifra se incrementa hasta un 12 o 13%, de acuerdo a un estudio llevado a cabo por la Universidad de Costa Rica a mediados de 2017.

En el país predomina la entrada de inmigrantes económicos y de refugiados políticos. Actualmente las comunidades extranjeras más grandes son las provenientes de Nicaragua (70,9 %), Colombia (5 %), El Salvador (3,3 %) y Estados Unidos (3 %). Hay un 17,8 % que corresponde a otras más de 150 nacionalidades, con comunidades importantes de panameños (12 000), cubanos, venezolanos (5000 c/u), hondureños y mexicanos (3000 c/u). Desde hace varios años hay un movimiento migratorio destacable de canadienses, europeos (especialmente españoles, británicos, alemanes, suizos, suecos e italianos) y asiáticos (chinos, taiwaneses y japoneses), que se afincan en el país atraídos por la estabilidad general.

Además, Costa Rica ostenta la tasa de emigración más baja de América Central, con 143 465 costarricenses viviendo en el extranjero que representan el 2,8 % de la población.

Existe una gran variedad idiomática en el país, debido a que constitucionalmente se reconoce como oficial al idioma español, pero además se hablan 5 lenguas autóctonas, a saber, el maleku, cabécar, bribri, guaymí y bocotá. De forma paralela, en la zona caribeña del país se practica mekatelyu o inglés criollo limonense, que se podría definir como una combinación de "patois" (inglés criollo jamaiquino) con ""tico"" (español costarricense). Este nombre es una suerte de onomatopeya formada a partir de la pronunciación de la frase "May I tell you" en esta variante del inglés. Un dialecto anglófono distinto se habla en Monteverde donde predomina el inglés cuáquero.

Por otro lado en el sur-este del país, en San Vito y otras comunidades de la región, se encuentran numerosas colonias de habla italiana de mayoría sarda y siciliana. Sin embargo, el paso del tiempo y la inmigración costarricense hacia el sur han creado un dialecto híbrido local que mezcla el castellano con lenguas itálicas. Incluso, en las escuelas públicas de esta zona el italiano estándar es impartido como asignatura obligatoria y puede elegirse para rendir en el bachillerato. Del otro lado del territorio, en la zona norte, se encuentran numerosas colonias de alemanes y menonitas de ascendencia alemana que practican el idioma alemán y —estos últimos— un dialecto derivado del alemán antiguo llamado platzdutch. También es destacable la presencia del idioma chino, en sus variantes del mandarín y cantonés que se hablan en el país por la numerosa comunidad de inmigrantes chinos y sus descendientes desde el siglo XIX.

Otros idiomas que tienen presencia importante en Costa Rica son: el francés, que constituye una asignatura impartida obligatoriamente en varias escuelas y en el Tercer Ciclo de la Educación Básica, sin contar que es hablado por la numerosa comunidad francesa del país y sus descendientes y es promovido por decenas de instituciones culturales, el idioma árabe también tiene presencia debido a la inmigración libanesa y de otros países levantinos, el hebreo también es utilizado por alrededor de 1000 israelíes que viven en el país y por la extensa comunidad judeo-costarricense, finalmente el idioma portugués cuenta con la presencia de 1500 brasileños que habitan en el país y es un idioma que cada vez es más promovido e impartido en diversas escuelas y colegios.

Según el artículo 75 de Constitución, la religión oficial del país es la católica, lo que convierte a Costa Rica en el único estado confesional de América. El mismo artículo también reconoce la libertad de culto. El catolicismo es la religión predominante entre la población, producto de la herencia española, y muchas tradiciones religiosas católicas se siguen celebrando en la actualidad en el país, como la Semana Santa o la tradicional romería del 2 de agosto a Cartago para visitar a la Virgen de los Ángeles, patrona de la nación. Sin embargo, su dominio ha venido disminuyendo con los años, pasando de un 86% en 1996, a un 62% en 2017. El estudio más reciente realizado en el país, efectuado por el Instituto de Estudios Sociales de Población (IDESPO) de la Universidad Nacional en 2018, reportó que un 52.5% de la población se declaró católica practicante, confirmando una tendencia en la reducción del catolicismo que se viene dando tanto en Costa Rica como en América Latina desde los años 1990. Por el contrario, los protestantes de las distintas denominaciones han venido creciendo. El protestantismo prácticamente se duplicó entre 1996 y 2013, pasando de un 9% a un 21%, respectivamente, representando en la actualidad un 27.1% de la población según la encuesta de IDESPO.

Las personas sin religión pasaron de un 5% a un 11% entre 1996 y 2013 según datos de Latinobarómetro, mientras que la encuesta de IDESPO reportó un 2.4% declarándose como ateo o agnóstico, sin embargo, la encuesta de IDESPO también reveló que un 16.5% de los costarricenses cree en un dios, deidad o fuerza superior pero no practica ninguna religión. Esta diferencia radica en que la encuesta de IDESPO es la primera que estableció una diferenciación entre creyentes sin religión y agnósticos, algo que no se había realizado en encuestas anteriores, que incluían a ambos grupos en la misma categoría.

En Costa Rica existen 8 etnias indígenas. Muchas de ellas preservan sus creencias religiosas hasta la actualidad, particularmente los bribris y cabécares. Producto de la inmigración de diversos pueblos a lo largo de su historia, existen en Costa Rica otras religiones: el budismo suma unas 100.000 personas para el 2%, hay unos 3500 profesantes de la fe judía, unos 500 musulmanes, así como comunidades más modestas de cristianos ortodoxos, hinduistas, bahais, jainistas, neopaganos (mayormente wiccanos y asatruar), rastafaris, taoístas, sijs, y luciferinos.

El sistema de salud está a cargo de diversas instituciones estatales: el Ministerio de Salud Pública, la Caja Costarricense de Seguro Social (CCSS), el Instituto Costarricense de Acueductos y Alcantarillados (AyA) y el Instituto Nacional de Seguros. Costa Rica invirtió en salud, durante el 2014, un 9.3 % de su PIB. Esto lo ubica entre los doce países del mundo que más invierte en salud y el que más invierte a nivel de América Latina, según la OCDE. El sistema de seguridad social del país, a cargo de la CCSS, es de tipo universal.

Costa Rica es el país con la mayor esperanza de vida de América Latina (80 años), y en él se encuentra una de las cinco zonas azules de longevidad del planeta, única de Latinoamérica: la península de Nicoya. En 2015 reportó una tasa bruta de natalidad de 14.86 y una tasa de mortalidad de 4.35. La tasa de fecundidad anual oscila en 1.76, unos 80.000 partos por año, de los cuales un 13 % ocurre en adolescentes. Desde 2011, la tasa de mortalidad infantil ha venido descendiendo continuamente hasta alcanzar niveles históricos en 2015: 7.8 muertes por cada 1000 nacidos vivos. La razón de mortalidad materna fue de 2.65 por cada 10.000 nacimientos en 2015. El aborto en Costa Rica únicamente es terapéutico, cuando corre riesgo la salud de la persona gestante, aunque están en continuo debate diversas propuestas para cambiar su estatus: desde una norma técnica hasta la despenalización. 

Según datos de la FAO, Costa Rica posee el porcentaje de desnutrición más bajo de Centroamérica (5.6%), por debajo del promedio para América Latina, pero también muestra un incremento importante en los niveles de sobrepeso y obesidad. El país presenta una disminución progresiva en los índices de mortalidad por enfermedades infecciosas, de la reproducción, perinatales y por deficiencias nutricionales, mientras registra causas de mortalidad general similares a las de los países desarrollados: enfermedades cardiovasculares, tumores y traumatismos. La tasa de incidencia de diabetes fue de 10.5 % en 2014. A 2016, el país registraba alrededor de 10.000 personas que viven con VIH/SIDA.

El sistema de salud de Costa Rica ha sido catalogado en varias ocasiones como uno de los mejores en el mundo por diversas instituciones y organizaciones internacionales, aunque desde hace décadas surgen incógnitas relacionadas con su difícil mantenimiento, corrupción interna y saturación de servicios.

La enseñanza general básica, que abarca desde el nivel preescolar hasta la educación diversificada en secundaria, es obligatoria, gratuita y costeada por la nación. Esta es supervisada por el Ministerio de Educación Pública. Según su Constitución Política, el PIB asignado a educación corresponde al 8%, lo que equivalió, en 2017, a una inversión de 2.6 billones de colones (352 millones de dólares), siendo el país latinoamericano que más invierte en educación según la OCDE. 

El país presentó una tasa de alfabetización del 97% a 2016. La tasa neta de escolaridad en educación primaria fue del 93.1% en el 2018, mientras que en secundaria rondó el 75%. En 2018, la matrícula en educación preescolar alcanzó un histórico 80% luego de permanecer en un 60% por más de una década. La matrícula en secundaria pasó del 81% al 123% en educación diversificada y 123 a 132% en el tercer ciclo, entre 2011 y 2018. En 2017, el 28% de la población entre 25 y 34 años poseía un título de educación superior. 

Costa Rica posee 5 universidades públicas, que cuentan con el estatus constitucional de instituciones autónomas, siendo la principal la Universidad de Costa Rica (1940). Para el 2018, se determinó que existen en el país al menos 1341 ofertas educativas superiores, correspondiendo el 20% a carreras de Educación, el 16% a Ciencias Económicas y el 14% a Ciencias de la Salud y Ciencias Sociales.

La cultura de Costa Rica se manifiesta como la mezcla y cohesión sincrética de las diversas costumbres de sus habitantes, originadas en intensas inmigraciones, mestizaje y convivencia pluricultural que tiene el país como puente natural y crisol étnico. Dicha abundancia única y distintiva, que se conserva a nivel familiar, cantonal y nacional, permite definir las raíces del ser costarricense mediante todo tipo de tradiciones, folclor, usos culinarios, música, bailes, creencias y supersticiones, así como un lenguaje popular característico.

La identidad nacional es el mosaico de características que definen la personalidad "tica", originado en un cúmulo amplio de vivencias, costumbres, triunfos, hitos, leyendas y estructuras sociopolíticas que hacen a la población costarricense identificarse como tal, sentir arraigo por su entorno y mostrarse orgullosa de su origen.

En las expresiones artísticas del país se puede reconocer un hilo conductor a través de más de 8.000 años, identificable en sus mitos, héroes, leyendas y expresiones populares. En el país se practican multitud de técnicas y estilos, y hay diversidad en las temáticas y uso de materiales. El arte costarricense se ha establecido de acuerdo a los comportamientos sociales y demandas estéticas de determinadas épocas históricas por las que ha atravesado la sociedad costarricense, recibiendo primeramente impulso del Estado, como la fundación de la Academia Nacional de Bellas Artes (1897), y la Universidad de Costa Rica (1940), y a partir de 1980, también por parte de la empresa privada.

La escultura cuenta con sólidas raíces ancestrales que se remontan a la época prehispánica, cuyo epítome son las esferas de piedra por su síntesis formal. Los maestros imagineros coloniales produjeron gran cantidad de arte religioso, y de sus talleres surgieron los primeros escultores laicos educados en la tradición académica europea. La pintura surge a finales del siglo XIX y principios del siglo XX, principalmente entre la alta sociedad y con influencia de pintores extranjeros establecidos en el país, como Emil Span, Tomás Povedano y Aquiles Bigot. A partir de 1930, una nueva generación de artistas creará una identidad artística nacional, con un arte más vernáculo, en la pintura, con un estilo criollo con influencias de las vanguardias europeas, pero centrado en el paisaje rural y la cultura autóctona, en especial la casa de adobes como icono cultural del país, y consolidándose una escuela de acuarela, y en la escultura, obras cercanas a las raíces indígenas, con el uso de materiales autóctonos como la piedra volcánica en lugar del mármol. A partir de allí, los artistas nacionales explorarán nuevos estilos como el muralismo (1940-1950), el abstraccionismo (1960-1970), el arte figurativo (1970-1980), hasta alcanzar la libertad creativa.

Entre los grandes artistas plásticos costarricenses, destacan Juan Ramón Bonilla, Juan Rafael Chacón, Ezequiel Jiménez Rojas, Enrique Echandi, Francisco Zúñiga, Teodorico Quirós, Manuel de la Cruz, Francisco Amighetti, Juan Manuel Sánchez, Néstor Zeledón Varela, Fausto Pacheco, Luisa González de Sáenz, Max Jiménez, Margarita Bertheau, César Valverde, Rafael Ángel García, Lola Fernández, Néstor Zeledón Guzmán, Hernán González Gutiérrez, Ana Griselda Hine, Luis Daell, Rafa Fernández, Gonzalo Morales Sáurez, Leonel González Chavarría, Isidro Con Wong, Rafael Sáenz Rodríguez, Ibo Bonilla Oconitrillo y Jorge Jiménez Deredia.

La arquitectura de Costa Rica responde a las necesidades que ha tenido el país a lo largo de su historia, siendo un puente biológico y cultural, recibiendo múltiples influencias externas de todo el mundo y desarrollándose a través de un hilo conductor milenario que inicia con sus pobladores nativos y continúa en la actualidad través de su entorno como un país activo en vías de desarrollo.

El cine llegó a Costa Rica en 1897 y la producción cinematográfica de Costa Rica se inició en 1913. Esta siempre ha sido modesta, predominando la producción de documentales, cortometrajes y la industria publicitaria. La creación de largometrajes ha sido escasa, pero durante el siglo XXI se ha ido incrementando con la producción independiente, que enfrenta un mercado pequeño, falta de financiamiento con altos costos de producción, apatía del público y la competencia del cine comercial de otras latitudes. Se mantiene una constante producción de documentales, muchos de ellos bajo encargo de organizaciones no gubernamentales; y una industria publicitaria vasta que emplea a un buen número de profesionales del audiovisual. En Costa Rica es usual que se realicen campañas publicitarias para el resto de América Central; y muchas producciones estadounidenses, europeas y sudamericanas utilizan al país como locación. Una única universidad privada oferta la carrera de cine. La Universidad de Costa Rica gradúa productores audiovisuales por medio de la Escuela de Ciencias de la Comunicación Colectiva; y el Instituto Nacional de Aprendizaje posee el Centro de Imagen, una escuela técnica para vídeo y televisión.

El ente encargado de velar por el desarrollo del cine costarricense es el Centro Costarricense de Producción Cinematográfica, adscrito al Ministerio de Cultura. Su intervención se da principalmente en coproducciones con productores independientes. En el país se realiza el Costa Rica Festival Internacional de Cine (antigua Muestra de Cine y Vídeo Costarricense), que reúne la producción nacional en ficción, documental y videoarte.

El cine tiene un lugar importante en las preferencias del costarricense, con una elevada asistencia a las salas de cine. Con un público mayoritariamente habituado a observar las grandes producciones holliwodenses, en el panorama cultural costarricense también existe un público acostumbrado a ver cine de autor, cine alternativo, auspiciado principalmente por Cine Magaly que se dedica a la exhibición de cine independiente y películas galardonadas de todas partes del mundo. Cine Magaly es sede de festivales de cine y otros eventos culturales. El cine costarricense cuenta también con la Sala Garbo, especializada en cine alternativo y de autor, y por el esfuerzo efectuado en algunas universidades, centros culturales periféricos e incluso centros de alquiler de videos por proponer tales producciones dentro del marco de series temáticas (gay, latinoamericano, de derechos humanos, de autor, etcétera).

En el campo de las artes escénicas, la actividad teatral del país es muy dinámica, posee una compañía estatal (la Compañía Nacional de Teatro) y la disciplina se enseña profesionalmente en dos universidades estatales, en varios institutos privados y en el Taller Nacional de Teatro. Funcionan catorce salas de teatro independiente, tres salas estatales y algunas regionales, en las que se puede ver desde teatro clásico hasta vodevil, lo que ha generado un público con un gusto de teatro variado. Es frecuente que haya grupos teatro de aficionados en algunas comunidades. Aunque en Costa Rica existe teatro desde el siglo XIX, es con la fundación del Teatro Universitario y el Teatro Arlequín (1950), la fundación de la Compañía Nacional de Teatro y la Escuela de Artes Dramáticas, y gracias sobre todo al aporte de reconocidos actores y directores argentinos y chilenos, quienes abandonaron sus países en la década de los años setenta, huyendo de los regímenes militares que se habían instaurado en el cono sur, que la actividad teatral costarricense experimentó un periodo de auge en la década de 1970 y se propició una sólida formación profesional. "El Teatro Carpa", "la familia Catania", el "Grupo Zurco" y "el Teatro del Ángel" fueron los principales exponentes durante las décadas comprendidas entre mediados de los años 1970 y fines de 1980. Además, surgieron tres importantes dramaturgos costarricenses en la segunda mitad del siglo XX: Alberto Cañas, Daniel Gallegos y Samuel Rovinski. La dramaturgia actual está orientada hacia la indagación de los hechos del pasado como una búsqueda de explicar el desencanto social actual, con la representación de obras que plasman la marginación social de la clase obrera o de la mujer, así como los conflictos sociales, con dramaturgos como Juan Fernando Cerdas, Rubén Pagura, Lupe Pérez, Leda Cavallini, Melvin Méndez, Arnoldo Ramos, Linda Berrón, Jorge Arroyo y Ana Istarú.

Posee también patrocinio estatal, sin embargo no ha logrado los niveles de alcance popular del teatro. Una de sus máximas ejecutantes ha sido la maestra Cristina Gigirey; a nivel internacional, el grupo independiente ""Losdenmedium"", dirigido por Jimmy Ortiz alcanzó cierta trascendencia en la década de los noventa con bailarines de reconocida trayectoria como Doris Campbell, Florencia Chaves, David Calderón, Rodolfo Seas, Daniel Marenco y Andrea Catania entre otros. El Festival de Jóvenes Coreógrafos reúne a muchas de las grandes figuras de la danza nacional y a menudo las coreografías son de gran calidad. Sin embargo, igual que en el teatro, las obras originales se caracterizan por un reduccionismo temático (que buscan disimular cambiando el título de la obra) y homogeneización de la expresión corporal, todo lo cual contribuye a dar un sentimiento de monotonía. Esto es resentido por el público y el apoyo a este arte se ve limitado.

El ballet clásico en Costa Rica no cuenta aún con una representación nacional pero sí existen academias de ballet que proyectan este arte al escenario. El Ballet Juvenil Costarricense, la Academia Superior de Ballet Clásico Ruso, el Ballet Atelier, Danza Libre, Magnificat, entre otros, son las principales organizaciones de ballet en el país, llevando a cabo un amplio montaje en escena de obras clásicas como "La bella durmiente del bosque, El lago de los cisnes, La sirenita, Coppélia, Don Quijote, La bayadera, El pájaro de fuego, Sueño de una noche de verano" entre otras obras. Se cuenta con proyecciones a nivel internacional en festivales y encuentros de ballet a nivel mundial. La Escuela de Danza de la Universidad Nacional es una de las principales instituciones de enseñanza de esta disciplina en el país con programas de grado y posgrado y cursos libres de formación.

El Teatro Nacional cierra su temporada de todos los años presentando el ballet clásico "El Cascanueces", el cual ha sido un éxito en taquilla y nivel de los bailarines de ballet clásico costarricenses. Se ha presentado en diciembre desde el 2004 y ha contado con la participación de las primeras figuras del Ballet Nacional de Cuba, American Ballet Theatre y Saint Petersburg Classic Ballet Theatre. Ha sido dirigido en 4 ocasiones por el cubano Pedro Martín Boza, director del Ballet Juvenil Costarricense, grupo que fue el primero en encargarse con gran éxito de inaugurar la obra en el país. Ha sido dirigido también por María Amalia Pendones y Patricia Carreras; y la norteamericana Peggy Willis.

La Orquesta Sinfónica Nacional de Costa Rica, fundada en 1940, es una de las orquestas más reconocidas de América. Su actual director es el estadounidense , desde 2013. Esta orquesta se encuentra a la base de un encomiable proyecto cultural nacido a principios de los años setenta por iniciativa del entonces Ministro de Cultura, Guido Sáenz, que consistió en la creación de la Orquesta Sinfónica Juvenil, para lo cual el entonces Presidente, José Figueres Ferrer, invitó al país al director de orquesta estadounidense, Gerald Brown.

Dentro del marco de la música clásica cabe mencionar, además, el Coro Sinfónico Nacional, la Compañía Lírica Nacional y la Dirección General de Bandas. Un importante centro de educación musical media lo constituye el Conservatorio Castella, institución secundaría única en su tipo en América Central. De este conservatorio se ha egresado la gran mayoría de músicos en el país desde su fundación en 1953, entre ellos Eddie Mora Bermúdez, Allen Torres Castillo, Francisco Piedra Vargas, Fidel Gamboa Goldemberg, entre otros.

Dentro del área de la música rock se encuentran bandas como Gandhi, Evolución y Café con Leche. En esta última se destacó el músico José Capmany Ulacia, considerado el padre del rock nacional. En el ámbito del heavy metal, se encuentran bandas como Acero, Kronos, Höwler y Heresy. Por lo general tienen influencias de los subgéneros thrash metal y groove metal, de la NWOBHM, y de bandas como Iron Maiden, Metallica o Megadeth.

Tradicionalmente, la música tropical ocupa un lugar privilegiado en los gustos del costarricense por su afición al baile. Los ritmos de la salsa, el merengue y el reggae se escuchan en muchos de los rincones más inopinados de este país. Muchos grupos han desfilado por las tarimas nacionales, entre ellos, "Los Brillanticos", "La Selección", "Orquesta Explosión". Es interesante notar que hay músicos de la Orquesta Sinfónica Nacional de Costa Rica que también tocan ya sea en la escena alternativa o en grupos de música tropical, con lo que el círculo es completo, muestra de amplia apertura a todos los temas y estilos.

Entre los ritmos locales, se destacan el swing criollo, danza muy popular en el país, y la música conocida como "chiqui-chiqui", que tuvo su auge en la década de 1980 con grupos musicales como "Los Hicsos", "Taboga", "Sus Diamantes", "La Banda", "La Pandilla", "Marfil", "La Nota", "Jaque Mate", "Manantial", "Papel y Lápiz", "Pura Vida", "Los Alegrísimos" y otros.

Es importante destacar, que con el paso del tiempo, el acceso a nuevas tecnologías y el hecho de que Costa Rica se ha posicionado como destino en Centroamérica en las giras mundiales de muchas bandas, cantantes contemporáneos y sede de importantes festivales, el gusto musical del público costarricense se ha diversificado abarcando géneros tan diversos como heavy metal, punk rock, funk, ska, rock alternativo, música independiente, reggae, dancehall, diversos tipos de música electrónica, tanto en español e inglés como en otros idiomas, con propuestas musicales provenientes de América, Europa, Asia y Oceanía, que ha llegado a ocupar un lugar privilegiado dentro del gusto de los costarricenses. Por su posición estratégica en el continente y su estabilidad, el país ha contado con visitas tan diversas como Depeche Mode, Iron Maiden, Pearl Jam, Maroon 5, Björk, The Smashing Pumpkins, Shakira, Lady Gaga, Slayer, The Flaming Lips, Major Lazer, Red Hot Chili Peppers, Paul McCartney, Roger Waters entre cientos de bandas, cantantes y otros "performances" musicales en vivo. Esto se traduce en una diversificación del gusto musical del público costarricense en los últimos años, dando paso a una escena musical local cada vez más grande que empieza a obtener proyección internacional, sin contar aún con mayor apoyo de los gobiernos de las últimas décadas.

La literatura costarricense surge a partir de finales del siglo XIX e inicios del siglo XX. Predomina al inicio el costumbrismo con autores como Aquileo Echeverría y Manuel González Zeledón, que conviven con el modernismo de poetas como Roberto Brenes Mesén y Lisímaco Chavarría. Las primeras vanguardias literarias aparecen en 1900 con la creación de la revista Repertorio Americano, de Joaquín García Monge, autor de la primera novela costarricense, "El Moto" (1900). La década de 1940 es prolífica en escritores principalmente realistas, de gran trascendencia para la literatura nacional, con temas enfocados en lo social: Carmen Lyra, Carlos Luis Fallas, Joaquín Gutiérrez, Fabián Dobles, Yolanda Oreamuno y Carlos Salazar Herrera. En la literatura costarricense se destacan sus poetas (Isaac Felipe Azofeifa, Julián Marchena, Jorge Debravo, Eunice Odio y Julieta Dobles, entre otros), sus historiadores (Cleto González Víquez, Rafael Obregón Loría, Ricardo Fernández Guardia y Carlos Meléndez Chaverri) y sus ensayistas (Moisés Vincenzi, León Pacheco, Rodrigo Facio, Carlos Monge Alfaro, etc). En la década de 1960, una nueva generación de escritores introduce la temática urbana en la literatura nacional: Alberto Cañas, José León Sánchez y Carmen Naranjo. Generaciones más recientes han dedicado su obra a expresar su desencanto por el sistema: Rafael Ángel Herra, Fernando Contreras, Fernando Durán, Tatiana Lobo, y Anacristina Rossi. El siglo XXI introduce temáticas como la exploración de temas complejos de la sociedad actual (feminisimo, aborto, suicidio, pedofilia, explotación sexual, drogas), la literatura de ciencia ficción, fantasía épica, novela negra y terror, con autores como Catalina Murillo Valverde, Mirta González Suárez, Alí Víquez, entre otros.

La gastronomía criolla costarricense es una cocina mestiza, de intenso carácter tropical y mediterráneo, que cuenta con la influencia culinaria aborigen, europea, sefardí y africana, más el fuerte aporte gastronómico de cientos de inmigrantes que han arribado de manera masiva al territorio como españoles, italianos, afrocaribeños, chinos, alemanes, franceses y árabes, así como de todas partes de América.

Es así que platos insignes de este gran sincretismo alimentario —y de la cocina costarricense en sí— son el gallo pinto (popular desayuno que combina el frijol aborigen y el arroz asiático introducido por los españoles, con otros productos europeos como embutidos y lácteos, mientras que su combinación y preparación es fruto de la cocina africana), el casado (almuerzo tradicional donde se integran el arroz euroasiático con el frijol precolombino y la pasta italiana, acompañado de picadillo andaluz, algún tipo de carne a elegir, plátano frito afrocaribeño y ensalada), la olla de carne (herencia directa del guiso ibérico de olla podrida, con verduras autóctonas como el tacaco), los picadillos (guisos de origen andaluz que mezclan carne, chorizo o pollo con verduras, especias y achiote), la pasta fresca y seca (espagueti, lasaña, ñoquis, canelones, ravioles), las empanadas (especie de pastel relleno, puede ser dulce o salado) y diversos arroces (con cerdo, con pollo, con mariscos, paella, con palmito, con atún, cantonés, con almendras); así como incontables embutidos, sopas, guisos, lácteos y tosteles. Mientras que en época navideña son tradicionales los tamales (plato indígena de maíz mestizado con aceitunas mediterráneas, cerdo europeo y arroz asiático), la pierna de cerdo, el rompope, el queque navideño, el roscón y el panetón.

Entre los postres se encuentran las cajetas (turrones a base de leche, frutas, café, chocolate), queques (seco, volcado con frutas, tres leches, tradicional con dulce de leche), helados (de sorbetera, napolitano, nieves y de diversos sabores), atoles (chocolate, avena, maicena), mazamorra, arroz con leche, budín de pan, granizados y ensalada de frutas. Asimismo, las bebidas más típicas del país son los frescos naturales y fríos, con agua o leche. También gozan de gran popularidad el café costarricense, el aguadulce, el chocolate con leche, y en cuanto a las bebidas alcohólicas; el guaro, el ron, la cerveza y las mistelas.

La música folclórica de Costa Rica es producto de la interacción de varios grupos culturales, desde la música indígena y las tradiciones europeas, hasta los ritmos afroantillanos. Sus ritmos musicales se combinan con otras expresiones culturales, como la danza, las bombas y retahílas, la vestimenta tradicional y los instrumentos musicales. Esta música y sus ritmos suelen asociarse a los días festivos cívicos, religiosos y populares. Entre los géneros musicales más reconocibles, se destacan el punto guanacasteco, el baile nacional; el tambito, popular en el Valle Central y Guanacaste; el calipso limonense, ritmo afroantillano declarado patrimonio nacional; y el aire nacional, en el que se han compuesto algunas canciones consideradas himnos nacionales, como "Caña dulce" y "Guaria morada". También se adoptaron ritmos musicales de otros países, que se fusionaron con estilos locales para dar lugar a nuevas expresiones musicales: la mazurca, la polka, el vals, el pasillo, el corrido, la balada, el bolero, etc. Muchos instrumentos musicales son herencia del pasado precolombino, la colonia española y la inmigración afroantillana: la marimba, el quijongo, de legado africano, que tiene dos tradiciones distintas en las provincias de Guanacaste y Limón, de herencia colonial en la primera y producto de la inmigración caribeña en el caso de la segunda; instrumentos de origen indígena como las ocarinas; instrumentos europeos como la guitarra, la mandolina, el piano, el acordeón, etc. Entre los compositores, músicos y letristas más importantes de la música folclórica costarricense pueden citarse a Héctor Zúñiga Rovira, Jesús Bonilla, Mario Chacón Segura, Walter Ferguson, Aníbal Reni, Manuel Monestel, Lorenzo «"Lencho"» Salazar, Carlos Guzmán Bermúdez, entre muchos otros.

Los trajes típicos son parte fundamental de las actividades cívicas y folclóricas de cada pueblo del país. Se utilizan en actos conmemorativos, fiestas patrias y actos culturales como las celebraciones de la Independencia, la Anexión del Partido de Nicoya o el 11 de abril. Cada región y provincia del país tiene su propio traje de gala o de trabajo, con folclor e historia detrás, con detalles específicos, como el traje indígena de la etnia ngöbe o el que corresponde a la provincia de Limón, que revela influencia afroantillana y británica. Varios factores que influyen en la manera de vestir: condiciones geográficas, climáticas, económicas y sociales. El traje típico más conocido es el que recuerda al campesino del Valle Central, en el cual la mujer utiliza una blusa blanca y una vistosa y larga falda de vuelos amplios y vivos colores, se peina con una trenza, maquillada y con flores en la oreja, mientras que el hombre lleva camisa blanca, pantalón blanco u obscuro, un sombrero de ala pequeña conocido como "chonete", machete al cinto, y el pañuelo, llevando en la mano o anudado al cuello o a la cintura, que puede ser rojo o azul. Tanto la falda como el pañuelo llevan decorados que recuerdan a las carretas típicas pintadas.

Las leyendas costarricenses han sido transmitidas de forma oral desde épocas precolombinas y coloniales. Dentro del sistema de creencias indígenas, las más estudiadas corresponden a la cosmovisión y mitologías de los pueblos indígenas que persisten hasta la actualidad, como bribris, cabécares y malékus. Posterior a la llegada de los españoles y afroantillanos, la mitología costarricense se ha enriquecido con el sincretismo cultural que conlleva el contacto de las creencias de estos grupos étnicos. Las leyendas de Costa Rica se han clasificado en tres grupos: leyendas de la tierra, propias de alguna zona geográfica específica del país (leyendas de los volcanes, leyendas indígenas, etc); leyendas de la religión y leyendas de la magia, que son historias de espectros fantasmagóricos y que tienen una función moralizadora (la Llorona, la Cegua, el Cadejos, el padre sin cabeza, la carreta sin bueyes, etc). Como en otros países del mundo, el pueblo costarricense cuenta con su propio sistema de supersticiones, como la creencia en brujerías, talismanes u objetos mágicos, conocidos como agüizotes (del náhuatl «ahuitzotl»), así como la práctica de magia blanca y magia negra, y la creencia en santos populares.
Entre las tradiciones costarricenses, se destacan festividades que combinan la influencia indígena con la española, festividades religiosas, festividades cívicas y festividades populares. Existen algunas celebraciones que reflejan la herencia indígena, siendo una de las más importantes el juego de los diablitos de Boruca en Rey Curré, que se celebra los fines de año en Buenos Aires de Puntarenas. Algunas fiestas religiosas reflejan sincretismos entre las creencias indígenas y la tradición católica, como por ejemplo la Danza de la Yegüita en Nicoya, que se celebra en honor a la Virgen de Guadalupe. Para el pueblo que profesa la religión católica, es importante la romería a Cartago para visitar a La Negrita. Otra celebración destacada es la del Cristo Negro de Esquipulas en Alajuelita y Santa Cruz. La reunión de Nochebuena el 24 de diciembre y las procesiones religiosas de Semana Santa son otras festividades con trasfondo religioso que se celebran tradicionalmente en el país.

Mientras que, de las celebraciones de índole folclórico y popular, se destacan las fiestas de Zapote y las fiestas de Palmares. Cada pueblo cuenta con sus propias festividades locales, con algún factor distintivo propio de cada comunidad, por ejemplo, la Fiesta del Tamal en Aserrí, la Chicharronada en Puriscal, la Carrera de las Mulas en Parrita, la Fiesta del Boyero en Escazú, los Carnavales de Puntarenas o de Limón, etc. Estas fiestas tiene en común actividades como la mascarada tradicional costarricense, las cimarrona, los topes y cabalgatas, los carnavales, las corridas de toros ""a la tica"", la monta de toros, las carreras de cintas y los turnos. Otras celebraciones de importancia a nivel nacional son el Día de la Madre (15 de agosto); el desfile de los faroles el 14 de septiembre (víspera del Día de la Independencia); la decoración de carretas típicas, las pulperías de pueblo, etc.

Las expresiones costarricenses son un conjunto de palabras, dichos y maneras de hablar propias de la jerga popular del tico, que permiten afianzar el sentimiento colectivo de costarriqueñidad.

Actualmente Costa Rica tiene dos elementos inscritos como Patrimonio de la Humanidad: la tradición del boyeo y las carretas como Patrimonio Cultural Inmaterial, y cuatro sitios arqueológicos (Finca 6, Batambal, El Silencio y Grijalba-2) que contienen esferas precolombinas de piedra.

La tradicional carreta de bueyes pintada de forma de forma característica es el tipo de artesanía más famoso de Costa Rica. Desde mediados del siglo XIX, las carretas de bueyes eran utilizadas para transportar el grano de café desde el Valle Central de Costa Rica, en las montañas, a Puntarenas, en la costa del Pacífico, de donde se exportaba al exterior, en un viaje plagado de peligros que podía durar entre 10 y 15 días. Durante el siglo XIX y gran parte del siglo XX, el café fue el principal motor del desarrollo económico nacional. Las carretas de bueyes se caracterizan por ruedas en forma de disco, sin radios, para avanzar en medio del fango sin atascarse. En muchos casos, las carretas de bueyes eran el único medio de transporte de una familia y simbolizaban su estatuto social.

La tradición de pintar y engalanar las carretas comenzó a principios del siglo XX. Su arte se caracteriza por la decoración con figuras geométricas y puntas de estrella. Originalmente, cada región de Costa Rica tenía su propio diseño, lo que permitía identificar el origen del boyero por los motivos pintados en las ruedas. A principios del siglo XX, flores, rostros y paisajes en miniatura empezaron aparecer al lado de los motivos que representaban estrellas puntiagudas. Se organizaron concursos anuales para premiar a los artistas más creativos, costumbre que perdura hoy día. Los segundos domingos del mes de marzo, se celebra en el cantón de Escazú el Día Nacional del Boyero y la Carreta costarricenses.

La tradición del boyeo y la carreta típica costarricense es de la Unesco desde el 24 de noviembre de 2005. La carreta es uno de los símbolos patrios de Costa Rica, dado que representa la cultura de la paz y el trabajo del costarricense, la humildad, la paciencia, el sacrificio, y la constancia en el afán por alcanzar los objetivos trazados del pueblo costarricense.

En Costa Rica se han identificado 45 yacimientos de esferas de piedra, uno en el Pacífico norte, seis en la región Central, y el resto en el Pacífico sur, específicamente en la subregión Diquís, donde se han documentado unas 200 esferas, cuyo tamaño varía desde unos pocos centímetros hasta 2.5 metros, con pesos entre varios kilos hasta 30 toneladas, y que se caracterizan por la depuración en sus técnicas de producción. Dichas esferas comenzaron a elaborarse alrededor de 300 a 800 años de nuestra era, siendo los sitios más antiguos Bolas de Buenos Aires de Puntarenas y Piedra Pintada, en San Vito de Coto Brus. La región de Palmar Sur es un importante sitio arqueológico donde se hallan estas esferas. Su fabricación continuó hasta el periodo de los cacicazgos tardíos, en especial en el área del delta formado por los ríos Térraba y Sierpe, y se consideran distintivas de la llamada cultura del Diquís, y en la actualidad, son símbolos de identidad nacional. El 23 de junio de 2014, cuatro sitios arqueológicos costarricenses que contienen esferas de piedra, ubicados en la zona sur del país, fueron declarados Patrimonio de la Humanidad por la UNESCO, y el 16 de julio de 2014 fueron declaradas símbolo nacional de Costa Rica. Particularidades como la falta de información que date tanto de la época precolombina y colonial y que haya dado noticia de estas esferas, como la perfección de su forma, el gran tamaño de las mismas, la no presencia del material utilizado en la zona donde fueron ubicadas originalmente, la escabrosa topografía de la región, la falta de evidencia de las herramientas utilizadas en su elaboración y la especialización de sus técnicas de fabricación; han llevado a lanzar teorías sobre su existencia, la mayoría sin asidero científico que les de sustento. En esta línea especulativa, se postula que son símbolos de poder, que tienen connotaciones religiosas, que están relacionadas con el ciclo agrícola, que son representaciones mitológicas de los astros y las constelaciones, que forman parte de calendarios astronómicos, que se utilizaron en la navegación, hasta significados mágicos y esotéricos.

La fabricación de objetos de cerámica en la provincia de Guanacaste, cuyo origen se remonta a la época precolombina, es patrimonio cultural inmaterial de esta provincia desde el 14 de agosto de 2013 mediante decreto ejecutivo N°37824-C. La tradición fue iniciada por las culturas precolombinas que habitaron esta región, e incluso fue utilizada como preciado bien de intercambio comercial con otras regiones de Mesoamérica, Sudamérica y dentro del territorio nacional. Se le considera el arte más representativo de la cultura de la Gran Nicoya. Estos objetos son piezas artesanales de cerámica policromada con motivos chorotegas, confeccionadas a mano con elementos naturales que se mezclan con agua y arcilla para obtener sus colores característicos.

Las técnicas para la fabricación de la cerámica nicoyana han persistido hasta la actualidad y son utilizadas por los artesanos guanacastecos para elaborar las piezas modernas, que son consideradas las únicas artesanías con registro de denominación de origen protegida en América Central desde 2017. Esta tradición milenaria se mantiene vigente principalmente en las comunidades guanacastecas de Guaitil de Santa Cruz, y en las poblaciones nicoyanas de Las Pozas y San Vicente.

El desarrollo científico y tecnológico en Costa Rica ha representado un importante aporte a la identidad nacional. Muchos de los proyectos de investigación científica y tecnológica costarricense se realizan en las universidades nacionales, principalmente en la Universidad de Costa Rica y el Instituto Tecnológico de Costa Rica, así como en diversas entidades privadas que desarrollan alta tecnología, en gran parte para la exportación. A nivel estatal, el país cuenta con el Ministerio de Ciencia, Tecnología y Telecomunicaciones (MICITT), y un Consejo Nacional para Investigaciones Científicas y Tecnológicas que fomenta la investigación científica. De acuerdo con Scopus, Costa Rica se ubica en el lugar 92 a nivel mundial en materia de publicaciones científicas, ocupando el undécimo lugar en América Latina y el primero en América Central. Entre las disciplinas científicas, se han desarrollado fuertemente la biología y muchas de sus ramas, como la zoología, la botánica, la ecología, la etología, la conservación, así como la genética, la nanotecnología y la biotecnología.

El deporte más popular en el país es el fútbol, seguido del atletismo y el ciclismo. Costa Rica participó por primera vez en los Juegos Olímpicos en 1936, y ha asistido a las justas de verano de forma ininterrumpida desde 1964. Los más grandes logros del deporte costarricense han sido en la natación con 4 medallas olímpicas: una medalla de oro (Juegos Olímpicos de Atlanta 1996) y dos medallas de bronce (Juegos Olímpicos de Sídney 2000), obtenidas por Claudia Poll Ahrens, y una medalla de plata (Juegos Olímpicos de Seúl 1988) obtenida por Sylvia Poll Ahrens.

El fútbol a nivel profesional está regulado por la Federación Costarricense de Fútbol. La Selección de fútbol de Costa Rica ha asistido a cinco Copas del Mundo: Italia 1990, Corea-Japón 2002, Alemania 2006, Brasil 2014, su participación más destacada (octava), y Rusia 2018. La Selección Sub-20 ha clasificado a 7 Copas del Mundo, con su mejor posición en Egipto 2009 (cuarta). La Selección Sub-17 ha logrado clasificarse a 9 Copas del Mundo y la Selección Olímpica lo ha hecho en 3 oportunidades. La Selección femenina de fútbol de Costa Rica es una de las potencias en el área. Ha logrado clasificarse a 5 mundiales (4 menores y uno mayor): Nueva Zelanda 2008, Alemania 2010, Costa Rica 2014, en la que fue sede, Canadá 2014 y Canadá 2015. 

El deporte más practicado en el país es el atletismo. Entre los atletas históricos de Costa Rica, se destacan Rafael Ángel Pérez Córdoba (fondo), José Luis Molina Núñez (fondo), Zoila Rosa Steward (velocidad), Nery Brenes (velocidad), Gabriela Traña (fondo), Leonardo Chacón (triatlón), Roberto Sawyers (lanzamiento de martillo), Sharolyn Scott (velocidad), Ana María Porras (heptatlón), César Lizano (fondo) y Andrea Vargas (velocidad). Costa Rica fue sede del Campeonato Clasificatorio para el Mundial de Atletismo del 2015.

El ciclismo se practica en las modalidades de ruta, pista, ciclismo de montaña y BMX, en categoría masculina y femenina, a nivel amateur y profesional. La FECOCI organiza anualmente la Vuelta a Costa Rica en el mes de diciembre, la competición más importante del país en esta disciplina. Existen otras competencias destacadas como la Vuelta Femenina a Costa Rica, Vuelta Internacional a Higuito, Vuelta a Guanacaste, la Copa Cross Country AMPM y la Ruta de los Conquistadores; esta última, una dura prueba de ciclismo de montaña que cruza el país de costa a costa por caminos rurales atravesando ríos y montañas, considerada una de las pruebas ciclísticas más duras del planeta. Costa Rica ha dado importantes ciclistas como Andrey Amador Bikkazakova, Evangelista Chavarría, Carlos Alvarado, Juan de Dios Castillo, José Manuel Soto Delgado, Andrés Brenes, Henry Raabe, Federico Ramírez, Juan Carlos Rojas, Joseph Chavarría, Román Urbina, José Adrián Bonilla, Luis Morera, Gregory Brenes, y otros.
La figura cumbre del boxeo costarricense es Jesús "Tuzo" Portuguez, miembro de la Galería Costarricense del Deporte, del Comité Mundial de Boxeo y del Salón de la Fama del Consejo Mundial de Boxeo. Entre los boxeadores históricos destacados se encuentran Isaac Marín, Álvaro Rojas Delgado, Orlando Hernández Bonilla y Humberto Aranda. En 2015, Bryan "Tiquito" Vázquez fue campeón mundial interino en las 130 libras de la AMB. En la rama femenina, Costa Rica ha obtenido campeonatos mundiales con Hanna Gabriel (147 y 154 libras de la OMB, 2009 y 2010), y Yokasta Valle (102 y 105 libras de la FIB, 2016 y 2019 respectivamente). A nivel del boxeo aficionado, David "Medallita" Jiménez obtuvo medalla de bronce en el Campeonato Mundial de Boxeo Aficionado Kazajistán 2013, la única presea en su tipo para el país en esta disciplina.
El principal recinto para la práctica del béisbol es el Estadio Antonio Escarré, seguido por el Estadio Big Boy de Limón y el Parque Metropolitano La Sabana. Los equipos que han ganado más campeonatos son las novenas de Limón (7), U.I.A (7) y Glidden (4). El pelotero histórico más destacado es Donald Hawling Shaw, lanzador limonense quien jugara en forma profesional en Estados Unidos con Dodgers (1950) y Buffalo, y considerado el mejor beisbolista de la historia nacional.

En el ajedrez, el país ha contado con dos grandes maestros internacionales: Alejandro Ramírez Álvarez y Bernal González Acosta. En Costa Rica se celebra la Copa del Café, un torneo de tenis grado 1 jugado cada mes de enero en San José. Es el Torneo Júnior de la ITF más antiguo en América Latina y cada año recibe entradas por parte de más de 40 países.

El surf ha mostrado importante crecimiento. Costa Rica se proclamó campeón mundial de este deporte al ganar los World Surfing Games 2015 de la Asociación Internacional de Surf. Surfistas destacados de Costa Rica son Noe Mar McGonagle (campeón mundial masculino 2015), Leilani McGonagle y Carlos "«Cali»" Muñoz. Costa Rica fue sede de este mismo torneo en 2016, disputado en Jacó, en el cual logró la medalla de bronce por equipos.












</doc>
<doc id="721" url="https://es.wikipedia.org/wiki?curid=721" title="Contrato de compraventa">
Contrato de compraventa

La compraventa (en latín "emptio venditio") es un contrato consensual, bilateral, oneroso y típico mediante el cual un sujeto –denominado vendedor– se obliga a transferir la propiedad sobre un bien a favor de otro sujeto –denominado comprador– a cambio de que este último le pague un precio en dinero. Es decir, es un contrato cuya causa es la transmisión del derecho de propiedad.

Este contrato es el que tiene mayor importancia entre los de su clase porque se trata del contrato tipo traslativo de dominio y, además, porque constituye la principal forma moderna de adquisición de riqueza; es decir, tanto en su función jurídica como económica, debe merecer un estudio especial.

Como contrato tipo de los translativos de dominio, aplicaremos sus reglas principales a la permuta; sufrirán estas modificaciones esenciales en la donación; también recurriremos a la compraventa para explicar ciertas especialidades del mutuo, de la sociedad, de la transacción y de la renta vitalicia.

Por otra parte, la compraventa constituye el medio primordial de adquirir el dominio. Las formas de adquisición del dominio están representada por el contrato, la herencia, la prescripción, la ocupación, la accesión, la adjudicación y la ley. El contrato es en el derecho moderno la forma principal de adquirir la propiedad dentro de los contratos translativos de dominio.

La compraventa en el derecho latino moderno, que deriva del Código Napoleón, es un contrato translativo de dominio, que se define como el contrato por virtud del cual una parte, llamada vendedor, transmite la propiedad de una cosa o de un derecho a otra, llamada comprador, mediante el pago de un precio cierto y en dinero.

Un contrato de compraventa es un contrato:
Exactamente correcto 





En el derecho romano es el convenio de cambiar una cosa (res) que se entregara al comprador (emptor) por una cantidad de dinero (pretium) que se pagara al vendedor (venditor). De hecho la forma más primitiva de compraventa consistió en el trueque de cosa y precio. Un tipo paradigmático de negocio de "toma y daca". Este se realizaba con el simple intercambio manual para la compra de res nec manicipi y adoptaba la forma solemne de manicipatio para la compra de res manicipi.
En el derecho griego exigía para la perfección de la compraventa, el pago del precio o la forma escrita; en la compra de género futuro el pago anticipado era préstamo de dinero. esto repercutió en la jurisprudencia romana, quien llegó admitir la perfección del contrato por el simple consentimiento, presentándose como simultáneas las obligaciones de entregar y pagar.
Gayo, en su obra Institutas, define el contrato de compraventa - emtio venditio - como un contrato consensual por el que una persona denominada vendedor (venditor), se obliga a transmitir la libre y pacificaposesión y el disfrute útil (habere licere) que tiene sobre una cosa (merx), a otra persona denominada comprador (emptor), a cambio de una cantidad cierta de dinero (pretium).

La compra venta resulta ser el medio más eficaz y práctico por el cual se intercambia la riqueza. La aparición de la moneda trajo como consecuencia el nacimiento del contrato de compra venta, precisando con mayor realismo el valor económico de las contraprestaciones, ya que la concepción primitiva de los valores al ganado o a la pecunia, daba a los contratantes una aproximación en las contraprestaciones, pero no con la misma precisión de la moneda merced de los valores fraccionarios.

Definida como la obligación del vendedor a entregar una cosa determinada y la obligación del comprador a pagar un precio cierto, en dinero o signo que lo represente.

Definida como la compraventa de bienes o muebles para revenderlas, bien en la misma forma que se compraron o bien en otra diferente, con ánimo de lucrarse en la reventa. Y realizar la venta exitosa y no encontrar engaños en lo que se muestra

Es aquel contrato en el cual las partes, mediante un pacto expreso, modifican el efecto traslativo de dominio, haciéndolo depender del cumplimiento de una obligación por parte del comprador.

Es aquel contrato en el cual mediante pacto expreso de las partes acuerdan que el comprador cubrirá de manera total o parcial en cierto tiempo el precio de la cosa mediante entregas parciales en cierto tiempo.
Si se pactara que la totalidad del precio se pagara en cierto tiempo en una sola exhibición no sería compraventa en abonos, sino un contrato de compraventa con el pago del precio diferido, por lo que la condición para que exista dicho contrato es que existan pagos en parcialidades.

Contrato en el cual las partes pactan que el comprador se obliga a no vender el bien adquirido como consecuencia de la compraventa, sin antes darle preferencia al vendedor para volver a adquirirlo en igualdad de condiciones en que pudiera adquirirlo un tercero...

Es aquel contrato en el cual las partes pactan que el comprador no pueda vender a determinada persona el bien que ha adquirido y es objeto del contrato. No debe confundirse con el hecho de pactar no vender a persona alguna, pues el contrato seria nulo.

Es aquel contrato en el cual las partes pactan la entrega de cosas futuras a cambio de un precio cierto y en dinero, por lo que el comprador corre el riesgo de que esas cosas nunca lleguen a existir y aun así este deba cubrir el precio.

Contrato en el cual las partes pactan la entrega de cosas futuras y el pago del precio subordinándolas a la condición suspensiva de que la cosa llegue o no a existir.

Es equiparada por la ley a una enajenación forzada. Técnicamente no es un contrato porque no existe acuerdo de voluntades entre el propietario del bien y el deudor del crédito que da origen al procedimiento judicial, ni mucho menos el mejor postor. Por regla general, dentro del procedimiento judicial, el propietario no acude a esta audiencia, y aunque lo hiciera y manifestará su inconformidad con el procedimiento su consentimiento es irrelevante para la enajenación del bien.

Por otra parte, tampoco es una compraventa ya que ninguna de las partes, ni el juez de conocimiento, fijan el precio en que se remata la cosa, tampoco los peritos que la valuarán, ya que ellos señalarán un valor y precio estimado que servirá de base para su estimación y la legalidad de sus posturas. En todo caso, el mejor postor será el que fije el precio al ofrecer una cantidad a cambio de la cosa, que debe ser cuando menos las mismas, que sea considerada como postura legal. En síntesis, si no haya acuerdo de voluntades entre el propietario y el adquirente, ni respecto a la transmisión del bien, ni respecto el precio, no puede hablarse de contrato de compraventa.

Los elementos personales de este contrato se conforman por dos partes: el comprador y el vendedor.

Los elementos reales son dos: precio y cosa.

Como parte de la prestación que debe dar el comprador, el precio debe tener las siguientes características:

Pueden ser objeto del contrato de compraventa todas las cosas y derechos que reúnan los siguientes requisitos: 

Los contratos de compraventa tienen dos vertientes:











El Código Civil y Comercial Colombiano lo tratan en sus artículos: 

Artículo 1849 C.C. Concepto de compraventa:
La compraventa es un contrato en que una de las partes se obliga a dar una cosa y la otra a pagarla en dinero. Aquélla se dice vender y ésta comprar. El dinero que el comprador da por la cosa vendida se llama precio.

Artículo 905 C.Com. Definición de compraventa
La compraventa es un contrato en que una de las partes se obliga a trasmitir la propiedad de una cosa y la otra a pagarla en dinero. El dinero que el comprador da por la cosa vendida se llama precio.

El Código Civil y Comercial Argentino lo trata dentro del Libro III acerca de los "Derechos Personales", en el Título IV acerca de los "Contratos en particular", le dedica el primer capítulo del mencionado título al contrato de compraventa, este a su vez se divide en 8 secciones: 


En el Derecho español la "Compraventa con reserva de dominio", el traspaso posesorio es realizado en el momento de la perfección del contrato, quedando la transmisión de la propiedad supeditada al cumplimiento de una determinada condición.

La "compraventa a plazos" se realiza la transmisión de posesión y propiedad de un bien mueble, corporal, identificable y no consumible, naciendo, por el lado del adquirente, la obligación de realizar el pago fraccionado en un determinado número de cuotas periódicas. Esta modalidad, debido a su trascendencia en la economía contemporánea, ha sido regulada específicamente por la Ley 28/1998.

La "compraventa con pacto de retroventa" (retracto convencional), en caso de bienes inmuebles, mientras el pacto de retro no haya sido inscrito en el Registro de la Propiedad no vincula al tercero que adquiera la propiedad procedente del comprador.

La "compraventa ad gustum" de acuerdo con el artículo 1115 del Código Civil, no puede tratarse de una condición puramente potestativa, prohibida por dicho artículo. Finalmente, una vez haya resultado positiva la prueba, el comprador pierde la facultad de desistir de la compraventa.

Los artículos 2248 al 2322 del Código Civil Federal regulan el contrato de compraventa.

En el Derecho mexicano la compraventa es perfecta y obligatoria para las partes, por regla general, cuando se ha convenido sobre la cosa y su precio, aunque la cosa no haya sido entregada ni el precio satisfecho.

El "pacto de retroventa" está expresamente prohibido, así como la promesa de venta de un bien raíz que haya sido objeto de una compraventa entre los mismos contratantes

Los artículos 1793 al 1896 del Código Civil de Chile regulan el contrato de compra-venta.

Los artículos 1597 a 1686 del Código Civil regulan la compraventa civil, y los artículos 964 al 994 del Código de Comercio regulan la compraventa mercantil.

Se encuentra regulada de los artículos 170 al 1879 del Código Civil Decreto 107. Se entiende que por el contrato de compraventa el vendedor transfiere la propiedad de una cosa y se compromete a entregarla, y el comprador se obliga a pagar el precio en dinero.




</doc>
<doc id="726" url="https://es.wikipedia.org/wiki?curid=726" title="Go">
Go

El go (chino simplificado: 围棋, chino tradicional: 圍棋, pinyin: "wéiqí"; japonés: 囲碁 "igo", coreano: 바둑 "baduk") es un juego de tablero de estrategia para dos personas. Se originó en China hace más de 4000 años. Fue considerado una de las cuatro artes esenciales de la antigüedad China. Los textos más antiguos que hacen referencia al go son las analectas de Confucio.

El objetivo del juego, cuya traducción aproximada es "juego de rodear", es controlar una cantidad de territorio mayor a la del oponente. Para controlar un área, debe rodearse con las piedras. Gana el jugador que controla la mayor cantidad de territorio al finalizar la partida.

El juego consiste en colocar las piedras en las intersecciones del tablero. Antes de comenzar se asigna un color a cada jugador. Las negras inician la partida y una vez colocada una piedra, no se puede volver a mover. Se puede capturar una piedra o un conjunto de piedras y eliminarlas del tablero si están completamente rodeadas por piedras de otro color. Existen diferentes conjuntos de reglas, pero todas coinciden en los aspectos generales y las diferencias no afectan significativamente la estrategia ni el desarrollo del juego salvo en situaciones excepcionales.

A pesar de la aparente simplicidad de las reglas, requiere de una estrategia bastante compleja.

La dimensión de los tableros puede ser de 7×7, 9×9, 13×13 y 19×19. La dificultad aumenta con el número de interseciciones. El tablero más común es el de 19×19. Originalmente se jugaba en tableros de 17×17 pero para cuando el juego llegó a Corea y Japón en los siglos V y VII d.C., los tableros de 19×19 ya se habían generalizado.

El go es muy popular en Asia Oriental, pero también ha ganado cierta popularidad en otras partes del mundo. Llegó a Europa a través de Japón, por lo cual se conoce principalmente como go, del japonés "igo". A mediados de 2008, había más de 40 millones de jugadores de go en el mundo, la gran mayoría en Asia. La Federación Internacional de Go cuenta con 74 países miembros.

La referencia escrita más antigua que se conoce del juego es el Zuo Zhuan (siglo IV a. C), que hace referencia a un evento histórico del año 548 a. C. También se le menciona en el Libro XVII de las Analectas de Confucio y en dos libros escritos por Mencio. En todos aquellos trabajos, el juego se menciona como yì (). Hoy se le conoce en China como weiqi (), literalmente el «juego de tablero de envolvimiento».

Originalmente el go se jugaba sobre una cuadrícula de 17×17, pero durante la Dinastía Tang (618-907) se impuso el uso de la cuadrícula de 19×19 . La leyenda "oficial" atribuye el origen del Weiqi al Emperador Yao (2337-2258 a. C.), quien solicitó a su consejero Shun que diseñara un juego que enseñara disciplina, concentración y equilibrio a su hijo Dazhu, quien se supone era desjuiciado. Otras teorías sugieren que el juego fue inventado por generales y jefes del ejército chino, quienes usaban piedras para señalar posiciones de ataque en mapas, o que los elementos usados actualmente para el juego fueron alguna vez usados para realizar lecturas de la suerte.

En China, fue considerado el juego preferido por la aristocracia, mientras que el xiangqi era el juego de las masas. El go era considerado una de las "Cuatro Artes Tradicionales" de los eruditos chinos, junto con la caligrafía, la pintura y la interpretación del instrumento musical guqin.

El "go" ("weiqi") fue introducido en Corea entre los siglos V y VII d. C. y se volvió popular entre las clases altas del país. En Corea al juego se le denomina baduk (hangul: ), que evolucionó en una variante llamada Sunjang baduk en el siglo XVI. El Sunjang baduk fue la versión que más se jugó en el país hasta finales del siglo XIX.

Probablemente el go se introdujo a Japón en el siglo VI d. C. donde recibe el nombre o . De acuerdo a los registros de la dinastía Sui, el go era uno de los tres principales pasatiempos de los japoneses del siglo VII, los otros eran el backgammon y las apuestas. Es posible que esta información llegara a través del embajador japonés en la capital del reino. Lo que hace posible que haya llegado durante el siglo VI o antes. Es probable que llegara a Japón a través de Corea.

Aunque se considera que el aristócrata Kibi no Makibi introdujo el juego en Japón. Kibi había sido comisionado por la hija del emperador Komu para regresar con lo mejor de la dinastía Tang, sin embargo se sabe que ya existía desde antes como pasatiempo para los monjes budistas. Aún así el prestigio del go aumentó tras el regreso de Kibi no Makibi.
El juego se volvió popular en la Corte Imperial Japonesa en el siglo VIII, y entre el público general en el siglo XIII. En 1603, Tokugawa Ieyasu volvió a establecer el gobierno nacional unificado de Japón. Ese mismo año, designó al mejor jugador japonés de ese entonces, un monje budista llamado Nikkai, en el puesto de Godokoro (Ministro de go). Nikkai tomó el nombre Honinbo Sansa y fundó la escuela Hon'inbō. Se fundaron otras escuelas poco después. Esos institutos de go, oficialmente reconocidos y subsidiados, contribuyeron enormemente a desarrollar el nivel de juego, e introdujeron el sistema de clasificación de jugadores Los jugadores de las cuatro escuelas competían en los juegos anuales de palacio, que se realizaban en presencia del shōgun.
A pesar de su elevada popularidad en el este de Asia, el juego se ha introducido muy lentamente en el resto del mundo, a diferencia de otros juegos de origen asiático como el ajedrez. Aunque existen algunas menciones al juego en la literatura occidental a partir del siglo XVI, el go no empezó a volverse popular hasta finales del siglo XIX, cuando el científico alemán Oskar Korschelt escribió un tratado sobre el go. A comienzos del siglo XX, el go empezó a expandirse en el imperio alemán y el imperio austrohúngaro. En 1905, Edward Lasker aprendió a jugarlo mientras se encontraba en Berlín. Cuando se trasladó a Nueva York, Lasker fundó el New York Go Club junto a Arthur Smith (entre otros), que conocía el juego y había publicado el libro "The Game of Go" en 1908. El libro de Lasker, "Go and Go-moku" (1934) ayudó a popularizar el juego en Estados Unidos. En 1935 fue fundada la Asociación Americana de Go. Dos años después, en 1937, se creó la Asociación Alemana de Go.

La Segunda Guerra Mundial detuvo temporalmente la expansión del juego. Durante la mayor parte del siglo XX, la Asociación de go de Japón jugó un papel vital en la propagación del juego fuera de Asia al publicar la revista "Go Review" en los años 1960, establecer Centros de go en Estados Unidos, Europa y Sudamérica, y enviar maestros profesionales a realizar giras por diversas naciones occidentales.

En 1996, los astronautas Daniel T. Barry y Koichi Wakata se convirtieron en las primeras personas en jugar al go en el espacio. Ambos fueron condecorados con rangos de dan honoríficos por la Asociación de Go de Japón.

En 2008, la Federación Internacional de Go tenía un total de 71 países miembros. Se ha afirmado que 1 de cada 222 personas en el mundo juega al go.

En Japón el juego se ha revitalizado recientemente gracias a la influencia del manga y anime "Hikaru no Go".

El go es un juego de estrategia en que dos jugadores (adversarios) luchan con el objetivo de lograr controlar un mayor territorio que el oponente. Mientras el juego progresa, cada jugador coloca piedras en el tablero, tratando de formar territorios. Las áreas se disputan en una lucha entre las piedras opuestas (blancas y negras), en general muy compleja, cuyo resultado puede ser la expansión, reducción o pérdida del área en cuestión.

Las partidas de go generalmente están llenas de posibilidades de "cambios", en los que la ganancia de un jugador en una parte del tablero puede ser una desventaja en otra parte. Inversamente, la pérdida en una parte del tablero puede compensarse o mitigarse con una ganancia en otra parte. Es frecuente que un jugador pueda forzosamente ganar una ventaja local con el resultado de una pérdida a mayor escala. Todas estas posibilidades de "cambios" constituyen mucha de la complejidad estratégica del go. La mayoría de las jugadas puede tener numerosas ventajas e inconvenientes de naturaleza sutil.

Un principio básico del Go es que las piedras deben tener al menos una "libertad" (Chino: 氣) para quedarse en el tablero. Una "libertad" es un "punto" abierto (intersección) próximo a una piedra. Una libertad encerrada se llama "ojo" (眼; "eye" en inglés), y un grupo de piedras con al menos dos ojos separados se dice que está incondicionalmente "vivo". Dicho grupo no puede ser capturado, incluso si está rodeado por piedras enemigas. Piedras "muertas" son piedras que están rodeadas y forman un grupo con una mala forma (uno o ningún ojo), y por tanto no pueden resistir una eventual captura.

La estrategia general del go es expandir el territorio de uno cuando sea posible, atacar los puntos débiles del oponente (grupos que posiblemente puedan matarse), y siempre ser consciente del "estado de vida" de los propios grupos. Las libertades de los grupos son contables. Situaciones donde dos grupos opuestos deben capturarse el uno al otro para poder vivir se llaman "carrera de captura" ('semeai' [攻め合い] en Japonés). En una carrera de captura, el grupo con mayores libertades (y/o mejor forma) terminará siendo capaz de capturar las piezas del oponente. Las carreras de capturas y las cuestiones de vida y muerte son ejemplos de la complejidad y desafíos del Go.

La partida termina cuando ambos jugadores pasan, y lo hacen cuando no hay más jugadas rentables por hacer. Entonces, se puntúa: el jugador con el mayor número de puntos controlados (rodeados), teniendo en cuenta el número de piedras capturadas y el komi, gana la partida. Las partidas también pueden ganarse por resignación, por ejemplo cuando un jugador ha perdido un gran número de piedras.

En la apertura de la partida, los jugadores típicamente establecen posiciones (o "bases") en las esquinas o alrededor de los bordes del tablero. Estas bases rápidamente ayudan a desarrollar fuertes formas que puedan tener opciones de "vivir" (imposibilidad de ser capturadas y removidas del tablero) y establecer formaciones para territorios potenciales. Los jugadores usualmente empiezan por las esquinas, puesto que es más eficiente para dar vida a un grupo, y establecer territorio con la ayuda de los dos bordes del tablero. Las formaciones ya establecidas en las esquinas durante la apertura se llaman "joseki" (japonés: 定石) or "jungsuk" (en coreano) y son generalmente estudiadas independientemente.

Los puntos neutros (“dame”, japonés: 駄目) son puntos que se encuentran entre las paredes límites de las blancas y negras, y por tanto son considerados de valor nulo para cualquiera de los partes. "Seki" (chino: 共活) son pares de grupos blancos y negros mutuamente vivos, en los cuales ninguno tiene dos ojos. "Ko" (chino y japonés: 劫) es una situación en la que las piedras pueden ser capturadas y recapturadas, repitiendo en cada captura la posición anterior. Esto implicaría una repetición infinita de la posición, sin ningún progreso en la partida. Por eso, las reglas del Go prohíben esta repetición infinita de la posición, forzando a realizar otra jugada antes de la recaptura que origina al ko. Esta compleja lucha se denomina "Pelea de ko" (o "luchas de ko"); algunas pueden decidir la vida de largos grupos, mientras que otras solo pueden valer uno o dos puntos. Algunas peleas de ko se denominan "pincin kos" cuando solo una parte tiene mucho que perder.

Jugar con otros usualmente requiere conocer el nivel de cada jugador, indicado por su ranking (30kyu→1kyu|1dan→6dan|1dan pro→9dan pro). Si hay diferencia de ranking se pueden utilizar «Hándicap»: las negras pueden colocar dos o más piedras en el tablero para compensar el nivel de las blancas. Hay diferentes tipos de reglas (japonesas, chinas, AGA, etc.), prácticamente equivalentes, salvo por ciertos casos específicos.

El go se juega sobre un tablero (碁盤 "goban" en japonés) que es una cuadrícula de 19 líneas verticales por 19 horizontales, formando 361 intersecciones. Los novatos suelen jugar en tableros más pequeños de 13×13 o de 9×9, sin otros cambios en las reglas de juego. Sobre las intersecciones se colocan alternativamente las piedras (碁石 "go ishi"), que son negras o blancas.

El tablero de juego —comúnmente llamado goban, por su nombre en japonés— mide generalmente entre 45 y 48 cm de largo y 42 y 44 cm de ancho. Los tableros chinos son ligeramente más grandes, debido a que las piedras chinas son también ligeramente más grandes. El tablero no es cuadrado: hay una relación 15:14 entre largo y ancho. Hay dos tipos principales de tablero: el de mesa, similar al utilizado en otros juegos como el ajedrez, y el de suelo, que se ubica directamente en el suelo ya que dispone de su propia base.

Usualmente, un juego completo de piedras de go ("goishi") está formado por 181 piedras negras y 180 blancas. Dado que una cuadrícula de 19×19 tiene 361 puntos, hay suficientes piedras para cubrir todo el tablero. Hay una piedra negra de más porque ese jugador mueve primero. Hay dos tipos principales de piedras: convexas, en las que uno de los lados es plano, y biconvexas, en las que ambos lados tienen una curvatura similar. Cada tipo tiene ventajas y desventajas: las piedras convexas, puestas con el lado plano hacia el tablero, son menos proclives a moverse de posición si se mueve el tablero. Adicionalmente, durante el análisis posterior al juego, los jugadores pueden probar variaciones poniendo las piedras al revés, lo que facilita recordar las posiciones originales. Por otro lado, al finalizar la partida es más fácil retirar del tablero las piedras biconvexas.

Las piedras tradicionales japonesas son biconvexas, fabricadas de concha de almeja (blancas) y pizarra (negras). La pizarra clásica utilizada se extraía en la Prefectura de Wakayama y las conchas eran de almejas Hamaguri. Sin embargo, debido a la escasez de este tipo de almeja en Japón, frecuentemente las piedras se fabrican con conchas cultivadas en México. Históricamente, las piedras más valiosas eran de jade, y generalmente se ofrecían como regalo al emperador.

En China, normalmente se juega con piedras convexas fabricadas mediante la sinterización de un compuesto llamado "yunzi". El material proviene de la provincia de Yunnan, y su composición exacta es secreta. El material es apreciado por sus colores, su agradable sonido al tocar el tablero y su bajo costo, comparado con otros materiales como pizarra y concha.

Las piedras tradicionales se fabrican de modo que las negras sean ligeramente más grandes que las blancas, para contrarrestar una ilusión óptica creada por el contraste de colores que hace que las blancas se vean sobre el tablero ligeramente más grandes que las negras.

Los tazones para las piedras tienen forma de esfera achatada. Generalmente, la tapa se voltea al inicio de cada juego, para poner allí las piedras capturadas durante la partida. Los tazones chinos son ligeramente más grandes y redondeados, un estilo conocido en Japón como "Go Seigen". Los tazones japoneses "Kitani" tienden a tener una forma más cercana a lo que es una copa de brandy.

Generalmente, los tazones están hechos de madera torneada. La madera de palisandro es el material tradicional para hacer los tazones japoneses, pero es muy cara; la madera de azufaifo Chino es un sustituto común. Otros materiales comunes para la fabricación de tazones chinos incluyen madera lacada, cerámica, piedra y paja tejida o ratán.

Los nombres dados a los estilos de tazón, "Go Seigen" y "Kitani", rinden homenaje a dos jugadores profesionales de go del siglo XX, de nacionalidad china y japonesa, respectivamente, a quienes se hace referencia como los "padres del go moderno".

La forma tradicional de colocar una piedra de go es primero tomar una del tazón, agarrándola entre los dedos índice y medio, con el dedo del medio arriba, y luego colocándola directamente en el lugar de intersección deseado. También se puede colocar una piedra en el tablero y luego deslizarla a su posición bajo apropiadas circunstancias (si no se mueve ninguna otra piedra). Se considera respetuoso hacia las blancas que el negro coloque la primera piedra en el extremo superior derecho. (Por la simetría, esto no afecta el resultado del juego.)

Se considera de mala educación golpear con los dedos el propio tazón con piedras sin jugar, puesto que el sonido puede molestar al otro oponente. Tampoco se recomienda golpear piedras con otras piedras, en el tablero, en la mesa o el piso. De todos modos, se permite enfatizar ciertas jugadas al golpear el tablero con la piedra más fuerte de lo normal, produciendo entonces un agudo "clack" en el tablero.

La duración de una partida de go puede controlarse mediante un reloj de juego. Formalmente, la regulación de la duración se introdujo en partidas profesionales en la década de 1920 y fue polémica. Los aplazamientos comenzaron a regularse en la década de 1930. Los torneos de go utilizan diversos sistemas de control de tiempo. Todos los sistemas establecen un único período de tiempo principal para cada jugador, pero varían en los protocolos para las continuaciones (en el "tiempo suplementario") cuando un jugador ha terminado su tiempo permitido. El sistema de tiempo más usado es el denominado "byoyomi". Las partidas profesionales de go cuentan con personas que regulan el tiempo para que los jugadores no deban estar presionando sus propios relojes.

Dos variantes del sistema byoyomi muy usadas son:

Las partidas de go se registran usando un simple sistema de coordenadas. Es comparable a la notación algebraica en ajedrez, excepto que en go las piedras no se mueven y por tanto requieren solo una coordenada por turno. Los sistemas de coordenadas incluyen los numéricos (por ej. punto 4-4), híbridos (K3) y puramente alfabéticos. El "Smart Game Format" (archivos de extensión «.sgf») usa un sistema alfabético, pero la mayoría de editores representa el tablero con coordenadas híbridas, reduciendo así la confusión. La palabra japonesa "kifu" se refiere a veces a un registro de partida.

En go, el rango de un jugador indica su habilidad en el juego. Tradicionalmente, los rangos se miden mediante un sistema de grados de "kyu" y "dan", un sistema que también ha sido adoptado por diversas artes marciales. Recientemente, se han adaptado al go sistemas matemáticos de calificación similares al Elo. Frecuentemente, estos sistemas ofrecen un mecanismo para calcular el equivalente en "kyu" o "dan".

Los grados de kyu (abreviados "k") se consideran grados de estudiante. Su número disminuye a medida que el nivel de juego aumenta, de modo que el 1. kyu es el grado de kyu más alto. Los grados de dan (abreviados "d") se consideran grados de maestro, y se incrementan de 1. a 7.º dan. Los jugadores profesionales obtienen un grado especial de dan, el profesional (abreviado "p"), cuyo máximo es el 9.º dan.

Cuando se juegan partidas con hándicap, un grado de diferencia equivale a una piedra de ventaja para el jugador más débil. Para los grados profesionales, tres grados de diferencia equivalen a una piedra.

Los reglamentos que se aplican durante torneos y competiciones de go tienen que manejar factores que puedan influir el desarrollo de las partidas, pero que no forman parte del reglamento del juego. Las reglas pueden diferir entre eventos. Algunas de las reglas que pueden influir el juego son los puntos de compensación ("komi"), el uso del hándicap y el control del tiempo.

Generalmente, los torneos de go utilizan sistemas de clasificación como el McMahon,el suizo, el de liga y el de eliminación directa. Particularmente, combinan varios sistemas, como los de liga y eliminación directa.

Los reglamentos de torneo también pueden tener en cuenta detalles como los puntos de compensación y la aplicación de la regla del "superko". Los puntos de compensación, llamados "komi", compensan al segundo jugador por la ventaja del movimiento inicial de su oponente. Los torneos comúnmente utilizan una compensación de entre 5 y 8 puntos, más medio punto adicional para prevenir empates. El "superko", por su parte, evita que una partida pueda quedar atrapada indefinidamente en un mismo patrón de jugadas. La regla del "ko" impide que un movimiento haga que el tablero vuelva a la posición inmediatamente anterior. Sin embargo, existen posiciones más complejas que causan que una posición se repita después de varias jugadas. Para prevenirlo, la regla del "ko" puede ampliarse para que impida repetir "cualquier" posición previa.

A pesar de que el juego se desarrolló en China, la creación de las cuatro escuelas de go por Tokugawa Ieyasu al iniciar el siglo XVII trasladó el foco principal del mundo del go a Japón. El patrocinio del Estado, el permiso dado a los jugadores de dedicarse a tiempo completo a estudiar el juego y las duras competiciones entre casas incrementaron significativamente el nivel de juego. Durante este período, el mejor jugador de su generación recibía el prestigioso título de meijin (maestro) y el puesto de Godokoro (ministro de go). Además, tres jugadores recibieron el título de kisei (santo del go): Dosaku, Jowa y Shusaku, todos ellos de la casa Honinbo.

Tras el fin del shogunato Tokugawa y el periodo de restauración Meiji, las casas de go desaparecieron lentamente. En 1924, se constituyó la Asociación Japonesa de Go. Frecuentemente, los jugadores de este periodo se enfrentaban en competiciones patrocinadas por periódicos, de 2 a 10 partidas cada una. Destacan Go Seigen, que obtuvo una puntuación del 80%, y Minoru Kitani, que dominó las partidas a principios de los años 1930. Ambos jugadores son también reconocidos por su trabajo en la nueva teoría de apertura de Go (Shinfuseki).

Durante gran parte del siglo XX, el go siguió dominado por jugadores entrenados en Japón. Entre los nombres notables están Eio Sakata, Rin Kaiho (nacido en China), Masao Kato, Koichi Kobayashi y Cho Chikun (nacido Cho ch'i-hun, Corea del sur). Usualmente, los mejores jugadores chinos y coreanos se desplazaban a Japón, porque el nivel de juego era alto y el financiamiento mejor. Uno de los primeros jugadores coreanos en hacerlo fue Cho Namchul, que estudió con el Kitani Dojo 1937-1944. Después de su retorno a Corea, se creó el Hanguk Kiwon (Asociación coreana de Baduk), lo que mejoró significativamente el nivel de juego en Corea en la segunda mitad del siglo XX. En China, el juego declinó durante la Revolución Cultural (1966-1976), pero pronto se recuperó en el último cuarto del siglo XX, dando lugar a jugadores chinos como Niew Weiping y Ma Xiaochun, a la par de sus contemporáneos japoneses y coreanos. La Asociación China de Weiqui (hoy en día parte de China Qiyuan) se creó en 1962. Los grados profesionales "dan" empezaron a usarse en 1982.
A partir del 1989, con la llegada de los títulos internacionales mayores, se pudo comparar con mayor precisión el nivel de los jugadores de diferentes países. Jugadores coreanos tales como Lee Chang-ho, Cho Hunhyun, Lee Sedol y Yoo Changhyuk dominaron internacionalmente el go y ganaron muchos títulos. Muchos jugadores chinos también ascendieron a la cima en el go internacional, especialmente Ma Xiaochun, Chang Hao y Gu Li.

Históricamente, como en la mayoría de los deportes y juegos, han jugado al go más hombres que mujeres. Existen torneos especiales para mujeres, pero solo recientemente los hombres y las mujeres compiten juntos en los niveles más altos. De todas formas, la creación de un nuevo torneo abierto y el ascenso de fuertes jugadoras, más notablemente Rui Naiwei, han destacado la fuerza y la competitividad de las jugadoras emergentes.

El nivel en otros países ha sido tradicionalmente mucho más bajo, excepto por algunos jugadores que obtuvieron preparación profesional entrenando en Asia. Alrededor del 1970, Hasta el siglo XX, el conocimiento del juego ha sido escaso. Un jugador famoso de la década de 1920 fue Edward Lasker. No fue hasta la década de 1950 que unos pocos jugadores occidentales se tomaron el juego como algo más que un pasatiempo interesante. En 1978, Manfred Wimmer se convirtió en el primer occidental en recibir un certificado de jugador profesional de la asociación profesional de go de Asia. En el 2000, el estadounidense Michael Redmond finalmente obtuvo el máximo ranking 9 dan otorgado por la asociación de go de Asia. En el 2008, solo nueve jugadores no asiáticos de go habían alcanzado el estatus profesional de asociaciones asiáticas.

Al inicio de la partida el tablero está vacío. Las Negras juegan primero. Después ambos jugadores juegan alternadamente. Una jugada consiste en poner una piedra en una intersección vacía. En general, el jugador más fuerte juega con blancas, puesto que las negras tiene la ventaja de iniciar primero.

Las intersecciones 4D, 4J, 4P, 10D, 10P, 16D, 16J y 16P se llaman "hoshi" (estrella). La central 10J se llama "tengen" (origen del cielo).

Cuando un jugador hace una jugada que priva de su última libertad a una piedra o formación del oponente, debe sacar las piedras rodeadas del tablero y guardarlas separadamente hasta el final de la partida.

Las piedras o formaciones que solo poseen una libertad (y por tanto pueden ser capturadas a la siguiente jugada) se dicen que están en atari. Las piedras capturadas se denominan "prisioneras", y es importante tenerlas en cuenta puesto que cuentan al realizar la puntuación de territorio al final de la partida.

En los ejemplos se usará, siempre que sea posible, un tamaño de tablero reducido de 9x9, usado normalmente para enseñar a los principiantes.
Las figuras de arriba muestran el antes y el después de las capturas. Se pueden ver ejemplos de captura en el centro del tablero, en los bordes y en la esquina. El jugador que capturó se queda con las piedras del adversario para contabilizarlas al final de la partida.

No se permite hacer una jugada ocupando una posición libre en el interior de una formación enemiga (suicidio), a no ser que esta jugada capture piedras enemigas.

Si la formación que uno quiere capturar está rodeada, entonces se puede jugar en su interior, si al hacerlo gana dicha formación.
Se denomina ko a una formación especial en la que blancas y negras pueden capturase unas a otras indefinidamente. Esto daría lugar a una repetición infinita de la posición en el tablero, sin ningún progreso en la partida. La regla del ko existe para prohibir este ciclo infinito.

La regla del ko establece que si un jugador captura una piedra en situación de ko ("infinitud" en japonés), el otro jugador no puede recapturar inmediatamente. Ha de hacer otra jugada antes de recapturar. Esto permite que la recaptura tenga lugar sin que haya una repetición de la posición anterior. Se trata de evitar que las posiciones de las piedras en el tablero sean idénticas en dos turnos diferentes.

La regla del ko es un componente importante de la estrategia y táctica del Go. La lucha por las formaciones en un ko se llaman "peleas de ko" ("ko fight" en inglés). Esta lucha puede decidir la pérdida de una o dos piedras, o la de un territorio de más de 20 puntos.

Por ello, el estudio del ko es fundamental en el Go.

En lugar de poner una piedra, un jugador puede pasar (perder un turno). Pasar un turno casi nunca es buena idea, puesto que da al oponente una jugada libre.

Solo es bueno pasar de turno al final de la partida, cuando los territorios ya están definidos y no hay jugadas por hacer. Cuando los dos jugadores pasan consecutivamente, se acaba la partida.

Puesto que las negras tienen la ventaja de realizar la primera jugada, en el siglo XX se introdujo la idea de otorgar alguna compensación a las blancas. Esta ventaja se denomina komi. En las reglas japonesas, consiste en dar 7,5 puntos de compensación a las blancas (el número de puntos varía según el tipo de reglas). Si hay una piedra (ranking) de diferencia en el nivel de los jugadores, el jugador más fuerte toma las blancas, y solo puede recibir 0,5 puntos de komi, para impedir un posible empate ("jigo"). En un juego con dos o más piedras de hándicap, las blancas también podrían obtener 0,5 puntos de komi para impedir el empate, pero es más común que no haya komi en dichos casos.

Aunque no se menciona en las reglas del go (al menos no en las más simples), el concepto de "vida" de un grupo de piedras es necesario para la comprensión práctica del juego.

Cuando un grupo de piedras está casi rodeado y no tiene opciones de conectarse con otras piedras amigas, el estatus de tal grupo es "vivo", "muerto" o "inestable". Un grupo de piedras se dice que está vivo si no puede ser capturado, incluso si el oponente mueve primero. Al revés, un grupo de piedras se dice que está muerto si no puede evitar ser capturado, incluso si mueve primero a quién pertenece el grupo. En cualquier otro caso, el grupo se dice que es inestable: en tal situación, el jugador que mueve primero podría ser capaz de hacer vivir el grupo (si es suyo), o de matarlo (si es del oponente).

Para vivir, un grupo debe ser capaz de crear al menos dos ojos si es amenazado. Un ojo es un punto vacío que está rodeado por piedras propias; el oponente no puede jugar en el ojo puesto que violaría la regla del suicidio. Si existen dos ojos, el oponente nunca puede capturar el grupo de piedras, puesto que este siempre tendrá al menos dos libertades. Un ojo no es suficiente para vivir, ya que un punto que normalmente sería imposible jugar por el suicidio, puede jugarse si las piedras que rodean al ojo están completamente rodeadas por piedras enemigas. En el diagrama de "Ejemplos de ojos", todos los puntos con círculos son ojos. Los dos grupos de piedras negras en la esquina superior están vivos, pero el punto vacío rodeado "A" no es un ojo. Blanco puede jugar ahí y tomar una piedra negra. Este punto se suele llamar "falso ojo".

Hay una excepción al requerimiento de que un grupo debe tener dos ojos para vivir: se llama seki (o vida mutua). Cuando dos grupos opuestos están adyacentes y comparten libertades, la situación puede llegar a una posición en la que ninguno de los jugadores desea mover primero, porque al hacerlo permitiría ser capturado por el oponente. Por tanto, en estas situaciones las piedras de ambos jugadores permanecen en el tablero en un estado de vida mutua o "seki". Ningún jugador recibe punto alguno por dicho grupo (estos puntos son "neutros"), pero al menos se mantiene vivos, en vez de ser capturados.

Seki puede ocurrir en muchas formas. Las más simples son: (1) cada jugador tiene un grupo sin ojos y comparten dos libertades, y (2) cada jugador tiene un grupo con un ojo y comparten una libertad más. En el diagrama "Ejemplo de seki (vida mutua)", los puntos con círculos son libertades compartidas por los grupos blancos y negros. Ningún jugador quiere jugar en uno de los puntos con círculos, porque al hacerlo permitiría ser capturado por el oponente. Todos los otros grupos en este ejemplo, tanto blancos como negros, están vivos con al menos dos ojos. Seki es inusual, pero puede resultar de un intento de un jugador por invadir y matar a un grupo de otro jugador.

Una vez finalizada la partida, se retiran las piedras muertas de cada bando añadiéndolas a las capturadas. Se conocen como piedras muertas a aquellas que se encuentran rodeadas y en grupos con mala forma -con 1 o ningún ojo-, por lo que no podrían resistir una eventual captura. Entonces, los jugadores cuentan los puntos de cada territorio, teniendo en cuenta las piedras prisioneras y las muertas, y se decide quién ganó la partida.

Cada jugador recibe un punto por cada piedra de su color que haya sobre el tablero, más un punto por cada intersección vacía dentro de su territorio. Quien obtenga mayor puntuación gana. En caso de empate, ganarían las Blancas en compensación por haber comenzado la partida después de las Negras.

Cada jugador recibe un punto por cada intersección vacía dentro de su territorio, más un punto por cada piedra capturada al enemigo. Quien obtenga mayor puntuación gana. En caso de empate, ganarían las blancas en compensación por haber comenzado la partida después de las negras.

En la figura se puede ver un ejemplo de final de partida. Durante la partida las negras capturaron 1 piedra blanca. Las blancas capturaron 2 piedras a las Negras. Ya se han retirado las piedras muertas de ambos bandos, que fueron 6 negras y 9 blancas. Luego las negras han hecho 1 + 9 = 10 prisioneros; las blancas han hecho 2 + 6 = 8 prisioneros. En la figura de la derecha se han marcado con un cuadrado los puntos de territorio de las blancas y con un círculo los de las negras. Las dos intersecciones marcadas con un triángulo son las intersecciones comunes y no se cuentan.
Los puntos por territorio para las negras son 22 y para las blancas 15. Las blancas reciben una compensación de 5.5 puntos llamada komi.

La puntuación total para las negras será: 22 + 10 = 32. La puntuación total para las blancas será: 15 + 8 + 5.5 = 28.5. Así pues ganan las negras por 3.5 puntos.

En go, la táctica está relacionada con la lucha inmediata entre piedras, la captura y rescate de piedras, vida y muerte, y otras cuestiones localizadas en partes específicas del tablero. Cuestiones más generales, no limitadas a una sola parte del tablero, se contemplan como "estrategia", y se tratan en la sección siguiente.

Hay muchas construcciones tácticas con el objetivo de capturar piedras. Estas son algunas de las primeras cosas que aprende un jugador después de las reglas. Reconocer las posibilidades de que las piedras pueden ser capturadas utilizando estas técnicas es un avance importante.
La técnica más básica es la "escalera" (""ladder"" en inglés). Para capturar piedras en una escalera, un jugador utiliza amenazas de capturas, llamadas "atari"—forzar al oponente hacia un patrón de zigzag, como se muestra en el diagrama. Salvo que el patrón vaya hacia piedras amigas, las piedras de la escalera no pueden evitar ser capturadas. Los jugadores experimentados reconocen la futilidad de continuar el patrón y juegan en cualquier otro lado. La presencia de una escalera en el tablero da a un jugador la opción de jugar una piedra en el camino de la escalera, amenazando entonces con rescatar sus piedras y forzando una respuesta. Esta jugada se llama "quiebro de escalera" ("ladder breaker" en inglés) y puede ser un movimiento estratégico fuerte. En el diagrama, Negro tiene la opción de jugar un quiebro de escalera.
Otra técnica para capturar piedras es la llamada "malla" (""net"" en inglés), también conocida por su nombre japonés "geta". Este movimiento rodea las piedras en una forma tal que previene su escape en cualquier dirección. Un ejemplo se da en el diagrama de la izquierda. En general, es mejor capturar piedras en una malla que en una escalera, porque una malla no depende de la condición de que no haya piedras opuestas en el camino, ni permite al oponente jugar la estrategia del quiebro de escalera.
Una tercera técnica de captura de piedras se conoce como "snapback". ("utte-gaeshi" en japonés) En un snapback, un jugador permite que una piedra sola sea capturada, y luego inmediatamente juega en el punto ocupado por la piedra capturada. Al hacerlo, el jugador captura un mayor grupo de piedras oponentes. Un ejemplo se puede ver en el diagrama. Como sucede con la escalera, un jugador experimentado no juega dicha secuencia, reconociendo la futilidad de capturar sólo para ser capturado inmediatamente.

Una de las habilidades más importantes, requerida para un fuerte juego táctico, es la habilidad de anticipar lo que podría suceder en el tablero.

Anticipar incluye considerar los movimientos posibles por jugar, las posibles respuestas a cada jugada, y las subsecuentes posibilidades después de cada respuesta. Algunos de los mejores jugadores pueden anticipar hasta 40 movimientos, incluso en posiciones complicadas.

Como se explica en las reglas, las formaciones de piedras que no pueden ser capturadas se dice que están vivas, mientras que las piedras en una posición que no pueden evitar ser capturadas se dice que están muertas. Gran parte del material práctico de los jugadores viene en forma de problemas de vida o muerte, también conocidos como "tsumego". En estos problemas, se desafía a los jugadores a encontrar la secuencia vital de movimiento que mata a un grupo oponente o salva un grupo propio. Los Tsumego son considerados una forma excelente de entrenar la habilidad de los jugadores a anticipar. Los hay para todos los niveles, algunos incluso desafiando a los mejores jugadores.

En situaciones donde se aplica la regla de ko, puede ocurrir una lucha de ko. Si el jugador al que le está prohibido capturar piensa que la captura es importante, ya que previene que sea capturado un grupo grande de piedras (por ejemplo), el jugador puede jugar una "amenaza de ko". Este es un movimiento en cualquier parte del tablero que amenaza una gran ganancia si el oponente no responde. Si el oponente sí responde a la amenaza de ko, la situación en el tablero cambia y la prohibición de captura de ko ya no se aplica. Por tanto, el jugador que hizo la amenaza puede recapturar el ko. Su oponente entonces se encuentra en la misma situación que él anteriormente, y puede realizar una amenaza de ko también, o conceder el ko y simplemente jugar en cualquier otro lado. Si un jugador concede el ko, ya sea porque no piensa que es importante o porque no hay movimientos que puedan funcionar como amenaza de ko, entonces ha perdido el ko, y su oponente puede dar fin a la batalla conectando el ko.

En lugar de responder a una amenaza de ko, un jugador podría también elegir "ignorar" la amenaza y conectar el ko. De esa forma gana el ko, pero con un coste. La elección de cuándo responder a una amenaza de ko y cuándo ignorarla es un tema sutil, que requiere del jugador considerar muchos factores, incluyendo cuánto se gana conectando, cuánto se pierde si no se responde, cuántas posibilidades de amenazas de ko les quedan a cada jugador, cuál es el orden óptimo de jugarlas, y cuántos son los puntos ganados o perdidos en cada de una de las amenazas que quedan.

El go es, ante todo, un juego de territorio. El jugador que consigue más territorio gana la partida. Sin embargo, aunque parezca fácil, lograr este objetivo no es nada trivial. El go posee una rica y compleja estrategia, cuya verdadera dimensión solo puede ser apreciada mediante su estudio profundo. Sin embargo, existen unos "principios" útiles que pueden guiar al principiante en los caminos de cómo construir efectivamente territorios. Una partida de go se divide, al igual que en ajedrez, en la apertura, medio juego y final.
En el go se "progresa" al ir consiguiendo territorios. Por tanto, la apertura se caracteriza por la toma eficiente de puntos estratégicos que delinean territorios. Por ello, importa entender, antes que nada, dónde es más eficiente realizarlos. En el diagrama de la izquierda se muestran tres territorios de 9 puntos cada uno; uno en la esquina, otro en el borde y otro en el centro. Como se aprecia, el territorio de la esquina (círculos) es el más eficiente, pues utiliza solo 6 piedras (aprovechando las dos esquinas); el territorio en el borde (triángulos) utiliza 9 piedras (aprovechando un lado); y el territorio del centro (cuadrados) utiliza 12 piedras. Por lo tanto, los territorios en la esquinas son más eficientes.

Por esta razón, en la apertura las esquinas son las ubicaciones más importantes del tablero, y por ello es usual que las piedras se coloquen allí. Sin embargo, esto no implica que uno deba simplemente focalizarse en una sola esquina; la idea de la apertura es delinear los territorios de la forma más eficiente. Si nuestras primeras jugadas fuesen todas alrededor de una única esquina, estaríamos dejando todo el resto del tablero para el oponente. Incluso si con esto logramos establecer irrevocablemente nuestro territorio en la esquina, después será muy difícil invadir otro lugar en el tablero, dado que el oponente ya ha tenido tiempo de colocar alguna piedra fuera de la esquina.

En el diagrama de la derecha se puede apreciar esto a través de las primeras jugadas de la apertura. Las negras han gastado todas sus jugadas en reforzar la esquina superior derecha, mientras que las blancas se han distribuido a lo largo de todas las demás esquinas, sin focalizarse en ninguna particular. Las blancas están mejor. Esto establece un importante principio: jugar las piedras una al lado del otro, en la apertura, no es una buena estrategia ya que solo aumenta nuestro territorio en un punto. Lo mejor es jugar al principio alrededor de los "puntos estrellas" del tablero, solidificando dichas áreas y delineando potenciales territorios. Esto permite un juego que abarca más y que es mejor a largo plazo, donde los verdaderos territorios se debatirán en la lucha por los mismos en el "medio juego".

La secuencia de movimientos usualmente jugados durante la apertura se conoce como joseki. Elegir el joseki adecuado y que a la vez resulte bueno globalmente es el desafío de los buenos jugadores. En general, se aconseja mantener un balance entre la estabilidad y la influencia; cuál de las dos es más importante es, sobre todo, un criterio de cada jugador.

El medio juego es lo que sigue inmediatamente a la apertura. En esta etapa de la partida la verdadera lucha por los territorios empieza, por lo que la táctica también juega un papel importante. En el medio juego es importante entender acerca de cómo invadir o defender un grupo, y cómo reducir el territorio de una formación o realizar una base.

Para defenderse o atacar un territorio debemos entender la estabilidad de una formación de piedras. Ligado a esto está relacionado el término de base, o un territorio dónde se puedan hacer dos ojos, ya que entonces está incondicionalmente vivo.

Si queremos formar una base es necesario extender una formación de piedras. En el diagrama, las negras juegan 1, amenazando acorralar a la piedra blanca solitaria. Para salvarla y crear un territorio en el borde, es necesario "extenderse". Por ello las blancas juegan 2, un salto de "dos-puntos"; este tipo de movimiento se conoce como "extensión". Con esta jugada las blancas se extienden, previenen ser rodeadas, y pueden realizar un territorio con dos ojos. Cualquier otro salto, por ejemplo de "tres-puntos" o "un-punto" sería o muy separado o muy corto. Si se tuviesen dos piedras juntas, se podría hacer un salto de "tres-puntos". Un proverbio del go dice que: "Si una, salta dos; si dos, salta tres".

En la lucha del medio juego están presentes la "invasión" y "reducción" de territorio. La invasión se da en áreas del oponente donde el territorio no está completamente definido. Es una estrategia arriesgada, pero si se hace bien puede ser muy fructífera. Para que la invasión tenga éxito es necesario que la formación resultante tenga dos ojos para que viva. Por ello, los conceptos de invasión y de base (extensión) están muy ligados.

La "reducción" es similar a la invasión, salvo que menos profunda. Su objetivo principal no es crear un propio territorio dentro del área enemiga, sino más bien reducir al mínimo el área del enemigo. El resultado de la reducción, pese a no crear un territorio, sí crea una barrera (que reduce el territorio enemigo) que puede influir en la formación de las piedras, habitualmente en el centro del tablero. Este "poder" de la barrera creada puede ser muy útil en luchas posteriores por territorio. La reducción también previene la expansión de otros potenciales territorios del enemigo.

La etapa final del go es dónde acaban de definirse los territorios. Después del medio juego, los territorios están bastante definidos, pero no consolidados. Por esto, el final es importante y requiere de gran atención. Un pequeño descuido puede significar la pérdida de un territorio que, habiéndolo luchado en el medio juego, lo dimos por nuestro.

En el ejemplo del diagrama, en la posición original las piedras marcadas no han sido jugadas y el territorio en la esquina superior izquierda aún no está del todo definido. Dependiendo de a quién le toca jugar, puede haber una gran diferencia en el territorio resultante. Si juegan las blancas, entonces el movimiento de la piedra con un cuadrado es una gran jugada, puesto que extiende el territorio de las blancas hacía la esquina, quitando así varios puntos que podrían haber sido de las negras. Si en cambio jugasen las negras, entonces deben prevenirse y jugar la piedra con un círculo, asegurándose el territorio de la esquina como suyo.

En el final suelen aparecer términos claves en el go, como los son el gote y el sente. Ambos están relacionados con la ganancia y pérdida de iniciativa en la partida. La palabra japonesa "sente" (先手) está relacionada con la iniciativa o la oportunidad de jugar en cualquier lugar de tablero; el énfasis de que una jugada sea "sente" se da porque obliga al oponente a responder, puesto que si no sale perdiendo. Esta situación del oponente, cuya "única jugada" está en responder (y no jugar libremente) se denota por la palabra japonesa "gote" (後手).

Como se vio en el ejemplo anterior, en general en la etapa final del go el tamaño de muchos territorios depende de quién juega primero. Es decir, quien tiene la iniciativa y puede jugar un movimiento sente. Reconocerlos y saber cuándo jugarlos es una importante habilidad en el final.

A lo largo de una partida de go, algunos aspectos básicos útiles a tener en cuenta son:


En términos de la teoría de juegos, go es un juego de suma cero, de información perfecta, juego de estrategia determinista, colocándolo en la misma clase que el ajedrez, las damas y el reversi. Sin embargo, difiere de estos en la forma en que se juega. Aunque las reglas son simples, la estrategia del go es extremadamente compleja.

El go enfatiza la importancia del equilibrio entre múltiples facetas del juego, cada una con sus propias tensiones internas. Por ejemplo, para asegurar un área en el tablero, es bueno realizar jugadas próximas entre sí. Por otra parte, para cubrir la mayor área, uno debe expandirse, dejando quizás debilidades que pueden ser explotadas. Jugar muy "bajo" (próximo a los bordes) no asegura un territorio suficiente e influyente, pero jugando muy "alto" (lejos de los bordes) permite invasiones del oponente.

Se ha dicho que el go es el juego más complejo del mundo, debido al vasto número de variaciones en una partida individual. Su gran tablero (19x19) y la falta de restricciones permiten amplitud de visiones en la estrategia y en la expresión individual de los jugadores. Decisiones en una parte del tablero pueden influir en una parte distante, aparentemente no relacionada. Jugadas tempranas en una partida puede moldear la naturaleza del conflicto cien jugadas después.

La complejidad como juego del go es tal que describir siquiera estrategias elementales podría abarcar muchos libros introductorios. De hecho, estimaciones numéricas muestran que el número de posibles partidas de go excede en mucho el número de átomos en el universo observable.

Investigaciones en los finales del go por John Horton Conway llevaron a la invención de los números surreales. El go también ha contribuido a desarrollar la teoría de juegos combinatoria.

El go significa un desafío desalentador para los programadores. Hasta 2011, los mejores programas de go solo podían alcanzar el nivel de amateur dan. En tableros pequeños de 9×9, la computadora se desempeña mejor, y actualmente algunos programas ganan fracciones de partidas de 9×9 contra algunos jugadores profesionales. Muchos en el campo de la inteligencia artificial consideran que el go requiere más elementos que los que imitan el pensamiento humano en el ajedrez.
Las razones por las que los programas de computadores no jugaron a go al nivel de profesionales dan hasta el año 2016 incluyen:



Como ilustración, el mayor hándicap da normalmente 9 piedras al oponente más débil. No fue hasta agosto de 2008 cuando una computadora ganó una partida ante un jugador de nivel profesional con este hándicap. Fue el programa Mogo el que obtuvo su primera victoria en una partida de exhibición jugada durante el congreso US Go.

En enero de 2016, el programa de inteligencia artificial de Google, "AlphaGo", ganó cinco a cero al triple campeón de Europa de Go Fan Hui (2p).

En marzo de 2016, AlphaGo se enfrenta con el coreano Lee Sedol. El resultado de los cinco enfrentamientos de cinco es de cuatro victorias para AlphaGo a una para Lee Sedol.

En octubre de 2017, DeepMind anunció una versión significativamente más fuerte llamada AlphaGo Zero que superó a la versión anterior AlphaGo Master (una versión mejorada de AlphaGo Lee) por 100-0.
Demostrado que el aprendizaje supervisado de las versiones anteriores de AlphaGo, cómo juegan los humanos, era irrelevante.
Para ello ha ejecutado 30 millones de partidas contra sí misma, durante 42 días, usando una sola red de neuronas con 44 capas de convolución.

Algunas páginas para aprender a jugar Go son las siguientes


Se pueden encontrar videos en español, con partidas comentadas y las reglas para aprender a jugar al go:
https://www.youtube.com/goenbilbao
https://www.youtube.com/gogoratugo

Aparte de la literatura técnica, el go y su estrategia han sido el tema de varios trabajos de ficción.


El go ha aparecido numerosas veces en el manga y anime. Algunas de las más relevantes son:



El go también ha aparecido en distintas películas. Algunas de las más conocidas son:

La empresa Atari se nombra en referencia al término en el go.

Un artículo de 2004 publicado por Fernand Gobet, de Voogt & Retschitzki, muestra que se ha investigado relativamente poco acerca de la psicología del go, comparado con otros juegos de mesa tradicionales como el ajedrez o el mancala. Investigaciones en el go computacional han mostrado que el conocimiento y el reconocimiento de patrones son más importantes en el go que en cualquier otro juego de estrategia, como el ajedrez. Un estudio sobre los efectos de jugar al go en la edad ha mostrado que el decaimiento mental es más leve para jugadores potentes que para jugadores débiles. De acuerdo a un análisis de Gobet y sus colaboradores, el patrón de actividad cerebral observado con técnicas tales como PET e IRMf no muestra grandes diferencias entre el go y el ajedrez. Por otro lado, un estudio realizado por Xiangchuan Chen et al. mostró una mayor activación en el hemisferio derecho entre los jugadores de go que entre los de ajedrez. Alguna evidencia sugiere una correlación entre jugar juegos de mesa y una reducción del riesgo a contraer la enfermedad de Alzheimer y la demencia.

El go empieza con un tablero vacío. Está focalizado en construir desde cero (de la nada a algo) con múltiples y simultáneas luchas, dando lugar a una victoria por puntos. El ajedrez es más táctico que estratégico, porque la estrategia predominante consiste en matar una pieza individual (el rey). Esta comparación se ha aplicado recientemente a la historia militar y política, en el libro de Scott Boorman "The Protracted Game" (1969), que explora la estrategia del Partido Comunista de China en la Guerra Civil China a través de la óptica del go.

Se ha dibujado una comparación similar entre el go, el ajedrez y el backgammon, quizás los tres juegos antiguos con mayor popularidad mundial. El backgammon es una disputa del tipo "hombre vs. destino", en la que el azar juega un rol determinante en el resultado. El ajedrez, con filas de soldados marchando hacia adelante para capturarse uno al otro, encarna el conflicto "hombre vs. hombre". Dado que el sistema de hándicap del go indica a los jugadores su situación respecto a otros jugadores, un jugador honesto puede esperar perder cerca de la mitad de sus partidas; por tanto, el go puede verse como la búsqueda de la autosuperación, "hombre vs. sí mismo".




Libros introductorios

Interés histórico

Problemas y ejercicios

Generales

Otros juegos de tablero abstractos

Campeonatos


</doc>
<doc id="727" url="https://es.wikipedia.org/wiki?curid=727" title="Atari-Go">
Atari-Go

El Atari-Go o Capture Go es una variante del Go. Fue creada por Yasuda Yasutoshi, un jugador octavo dan profesional con gran actividad en la enseñanza del Go a niños y adultos.

La variante se juega igual que el Go original, pero, a diferencia de este, el primer jugador en realizar una captura es quien gana. Debido a esto, no es necesario explicar reglas como el "ko" o conceptos como vida y muerte. 

Esta modalidad del juego tiene la ventaja de que las reglas pueden ser aprendidas en pocos minutos, y que dos principiantes pueden jugarla sin complicaciones, de modo que un maestro del juego puede instruir grupos de varias personas simultáneamente. Luego de que el principiante ha obtenido un cierto nivel de juego, se puede empezar a enseñarle paulatinamente las demás reglas del Go completo. 


</doc>
<doc id="728" url="https://es.wikipedia.org/wiki?curid=728" title="Clasificación de los jugadores de go">
Clasificación de los jugadores de go

Tradicionalmente, las clasificaciones de jugadores de "go" se han medido utilizando un sistema de "kyū" (級） y "dan"（段) (ambos con significando ‘nivel’ o ‘grado’).

En China, Japón y Corea, según la tradición, para clasificar las categorías de los jugadores profesionales y aficionados de "go", existen dos graduaciones distintas: la inferior (los "kyū") y la superior (los "dan").

Los "kyū" y los "dan" amateur (este último no presente en China) se utilizan para clasificar a los jugadores amateur, mientras que los "dan" profesionales se utilizan para clasificar a los jugadores que se dedican profesionalmente al Go y participan en los torneos de profesionales.

Los "kyū" van desde 30 "kyū" (el más bajo) hasta 1 "kyū" (el más alto).

En los "dan", tanto en los amateur como profesionales, es a la inversa que en los "kyū". El nivel más bajo es 1 "dan", hasta llegar a 9 "dan". Por lo tanto, un jugador que es 1 "kyū" pasaría a ser 1 "dan" si continúa su progresión. 

En China se practica el sistema abierto: no distingue a profesionales o aficionados, usando el mismo criterio para otorgar los títulos. No así en Japón, cuya academia trata de forma más rigurosa a los profesionales.

En algunos países europeos, como Alemania, se han unificado las dos categorías en una sola, compuesta de 18 grados en total.


</doc>
<doc id="729" url="https://es.wikipedia.org/wiki?curid=729" title="Candela">
Candela

La candela (símbolo: cd) es la unidad básica del Sistema Internacional que mide la intensidad luminosa. Se define como la intensidad luminosa en una dirección dada, de una fuente que emite una radiación monocromática de frecuencia 540×10 hercios y de la cual la intensidad radiada en esa dirección es 1/683 vatios por estereorradián.

Esta cantidad es equivalente a la que, en 1948 en la Conferencia General de Pesas y Medidas, se definió como una sexagésima parte de la luz emitida por un centímetro cuadrado de platino puro en estado sólido a la temperatura de su punto de fusión (2046 K).

Dado que resulta un poco más complejo evaluarlas teóricamente, la valoración de las intensidades medidas en candela son señaladas a través de algunos ejemplos que podemos observar:

Para ser de utilidad práctica, la candela debe poder indicar la intensidad luminosa de una fuente de luz de cualquier color. Por ejemplo, en la realización de un panel luminoso donde se ubican dispositivos que emiten luces de diversos colores se debe tener una medida que indique cómo son percibidos por el ojo humano.

Ya que el ojo humano no posee la misma sensibilidad a los diversos colores, puesto que es máxima para el color verde, y siempre más baja para el rojo y violeta , este problema es similar a aquel de la percepción de los sonidos donde se encuentra establecida una curva que define la sensibilidad del oído humano a las diversas frecuencias denominada curva isofónica.

Para la percepción de los colores, ha sido definido un estándar de sensibilidad para un ojo normal en una curva obtenida con una prueba sobre muestras o ejemplos representativos de la población humana. La curva obtenida es generalmente llamada formula_1 y tiene un valor de longitud de onda de aproximadamente 555 nm descendiendo hasta cero sea hacia longitudes de onda más cortas luego del color violeta o, por el contrario, hacia longitudes de onda mayores que la del color rojo.



</doc>
<doc id="731" url="https://es.wikipedia.org/wiki?curid=731" title="Lorenzo Cerdá">
Lorenzo Cerdá

Lorenzo Cerdá Bisbal (n. Pollensa, Mallorca; 1862 - f. Palma de Mallorca; 1956) fue un pintor español.

Pintó el paisaje mallorquín, sobre todo la zona norte de Mallorca utilizando un alto sentido académico. Estudió en La Academia de Bellas Artes de Palma de Mallorca y en La Academia de Bellas Artes de San Fernando (Madrid). También amplió estudios en Roma (1885-1886), donde fue discípulo de Joaquín Sorolla y Mariano Benlliure. Obras suyas de esta época son: "Cap de vell" (1884) y "Home nu" (1886), las dos expuestas en el palacio del Consell Insular de Mallorca. En Italia también pinta la obra premiada en la Exposición Universal de Barcelona y titulada "Foners balears" (museo de Edimburgo). 

Fue un autor de estilo realista y academicista, muy influido por Joaquín Sorolla.



</doc>
<doc id="734" url="https://es.wikipedia.org/wiki?curid=734" title="Constitución">
Constitución

Una Constitución (del latín "constitutio, -ōnis") es un texto codificado de carácter jurídico-político, surgido de un poder constituyente, que tiene el propósito de constituir la separación de poderes, definiendo y creando los poderes constituidos (legislativo, ejecutivo y judicial), que anteriormente estaban unidos o entremezclados, define sus respectivos controles y equilibrios ("checks and balances"), además es la ley fundamental de un Estado, con rango superior al resto de las normas jurídicas, fundamentando (según el normativismo) todo el ordenamiento jurídico, incluye el régimen de los derechos y libertades de los ciudadanos, también delimitando los poderes e instituciones de la organización política. 

En la actualidad también se tiene como costumbre adicionar normas ajenas a la regulación del poder político, dependiendo de la ideología, tales como los fundamentos del sistema económico. La constitución no constituye al estado o la nación, debido a que ambos ya son hechos anteriores constituidos. 
En ciencia política los estados que tienen separación de poderes se la añade el término «constitucional» a su forma de estado (como es el caso de la monarquía constitucional o la república constitucional). En el uso cotidiano del término, se llama Constitución a todas las leyes supremas de los Estados aunque no cuenten con separación de poderes. También se usa como sinónimo carta magna, en referencia a un tratado de paz de 1215 que ha influido profusamente en el "common law" anglosajón.
La constitución, como toda norma jurídica, puede definirse tanto desde el punto de vista formal como desde el punto de vista material. Desde el punto de vista material, la constitución es el conjunto de reglas fundamentales que se aplican al ejercicio del poder estatal. Desde el punto de vista formal, la constitución se define a partir de los órganos y procedimientos que intervienen en su adopción, derivándose así una de sus características principales, la supremacía sobre cualquier otra norma del ordenamiento jurídico.

Una constitución material se define de acuerdo con sus reglas y contenido, en lugar de un texto o documentos específicos. Por lo tanto, una constitución material es un conjunto de reglas que se pueden agrupar en un solo documento, pero no necesariamente. Estas reglas generalmente se clasifican en una o más de las siguientes categorías:

La mayoría, si no todos, los estados tienen una constitución material, en el sentido de que tienen reglas que organizan y gobiernan sus instituciones políticas.

Según su reformabilidad, las Constituciones se clasifican en rígidas y flexibles. Las Constituciones rígidas son aquellas que requieren de un procedimiento especial y complejo para su reformabilidad; es decir, los procedimientos para la creación, reforma o adición de las leyes constitucionales es distinto y más complejo que los procedimientos de las leyes ordinarias.

Son aquellas constituciones que para modificarse establecen un procedimiento más agravado que el procedimiento legislativo ordinario. Según el grado de complejidad del mismo se denominarán rígidas o semirrígidas.

En la práctica, las Constituciones escritas son también constituciones rígidas; es decir, cuando en un Estado encontramos que existe Constitución escrita, descubrimos que esta tiene un procedimiento más complejo de reforma o adición que el procedimiento para la creación, reforma o adición de una ley ordinaria.
Se modifican mediante el procedimiento legislativo ordinario, lo que significa que una ley del parlamento puede cambiarlas en cualquier momento.

Las Constituciones se diferencian también en función de su origen político: pueden ser creadas por contrato entre varias partes, por imposición de un grupo a otro, por decisión soberana, etc.

Las Constituciones otorgadas se dice que corresponden tradicionalmente a un Estado monárquico, donde el propio soberano es quien precisamente otorga; es decir, son aquellas en las cuales el monarca, en su carácter de titular de la soberanía, las otorga al pueblo. En este caso, se parte de las siguientes premisas: 


Hay Constituciones que son impuestas por el Parlamento al monarca, refiriéndose al Parlamento en sentido amplio, con lo que se alude a la representación de las fuerzas políticas de la sociedad de un Estado, de los grupos reales de poder en un Estado que se configuran en un órgano denominado Parlamento. En este tipo de Constitución, es la representación de la sociedad la que le impone una serie de notas, determinaciones o de cartas políticas al rey, y este las tiene que aceptar. Por lo tanto, en el caso de las Constituciones impuestas existe una participación activa de la representación de la sociedad en las decisiones políticas fundamentales.

En las Constituciones pactadas, la primera idea que se tiene es el consenso. Nadie las otorga en forma unilateral, ni tampoco las impone debido a que si son impuestas y no se pactan, carecerían de un marco de legitimidad. Estas Constituciones son multilaterales, ya que todo lo que se pacte implica la voluntad de dos o más agentes; por lo tanto, son contractuales y se dice que parten de la teoría del pacto social. Así, se puede pactar entre comarcas, entre provincias, entre fracciones revolucionarias, etc.
Las constituciones pactadas o contractuales implican


Así, aun tratándose de una monarquía, cuando se pacta los gobernados dejan de ser súbditos y se consagran como un pueblo soberano.

Son aquellas elegidas por el pueblo de un Estado, la cual por lo general se dan a conocer por una asamblea y se reafirman con la votación máxima de la población en un proceso electoral. Por lo tanto, no es que la sociedad pacte con los detentadores del poder público, sino que la Constitución surge de las necesidades sociales y de la fuerza popular.

El poder constituyente: Sieyes reconoce que todos los ciudadanos tienen derecho a establecer su gobierno. Pero este gobierno o el manejo de los intereses generales de la comunidad es un trabajo humano y es de primordial importancia ya que la voluntad de todos y cada uno cuestiona el bienestar y la libertad de todos.

Como resultado, sin un mandato expreso, los legisladores no deben tocar esta gran jurisdicción del Estado que uno llama la constitución.

La primacía de la nación: Sieyes -a diferencia de Rousseau- ""se sostiene por lo racional y lo construido. El estado social, en relación con el estado de naturaleza, perfecciona y ennoblece al hombre. Se extiende y protege la libertad. Defiende y garantiza la igualdad de derechos"".

Para él, las verdaderas relaciones de una constitución política son «con la nación que permanece» en lugar de «la generación que pasa; con las necesidades de la naturaleza humana, común a todos, en lugar de las diferencias individuales». En "¿Qué es el Tercer Estado?" Sieyes proclama: «Considerado en forma aislada, el poder de los ciudadanos sería nulo, reside solo en el todo».

La nación no se crea a sí misma, existe; es una ley natural, pero necesita una organización política y administrativa, o en palabras de Sieyès de un establecimiento público, es decir, un conjunto de medios formados por personas y cosas, destinados a darse cuenta de los fines sociales:

El control de constitucionalidad trata de los mecanismos de revisión de la adecuación de las leyes y de los actos del Estado. 



Sería anacrónico entender como constituciones modernas los sistemas políticos de la Antigüedad griega (democracia ateniense), la obra de sus legisladores (logógrafos) o los estudios legislativos de sus filósofos (Aristóteles, "Athenaion politeia" -cuyo título se traduce habitualmente como "Constitución de los atenienses"-). Lo mismo puede decirse de los fueros locales o estamentales que se redactaron en la Europa medieval o de la Carta de Medina (Mahoma, año 2 de la Hégira -622 d. C.-)

Ya en el contexto de las revoluciones burguesas de la Edad Moderna, algún texto ha sido retrospectivamente calificado de "constitución", como la "Ley Perpetua del Reino de Castilla" que intentaron promulgar los comuneros de la Junta de Ávila (1520). Con mayor trascendencia, el régimen político establecido en los Países Bajos sublevados desde finales del siglo XVI incorpora algunas características propias de un sistema constitucional, aunque los textos que generó (Pacificación de Gante -1576-, Unión de Utrecht -1579-) no pueden considerarse "constituciones". El régimen político inglés establecido paulatinamente desde finales del siglo XVII ("Revolución Gloriosa" y "Bill of Rights" de 1689 -los "Tratados sobre el gobierno civil" de John Locke se publican ese mismo año-) se basa en el derecho consuetudinario, y se define como un régimen constitucional sin constitución escrita cuyos precedentes pueden remontarse a la "Carta Magna" de 1215, que en sí misma no puede calificarse de "constitución", pero es considerada coloquialmente como sinónimo.

Consecuencia de la crítica ilustrada a los sistemas políticos del Antiguo Régimen (la monarquía absoluta o autoritaria), las constituciones actuales comienzan con los proyectos para Córcega y para Polonia que redactó Jean-Jacques Rousseau en 1755 y 1771-1772 respectivamente; y, ya como documento que efectivamente entró en vigor, la "Constitución de Estados Unidos" (17 de septiembre de 1787, cuyo antecedente directo es la del Estado de Virginia, 1776), estableciendo los límites de los poderes gubernamentales, y que en sus primeras enmiendas ("Bill of Rights" de 15 de diciembre de 1791) protege los derechos y libertades fundamentales.

La "Declaración de los Derechos del Hombre y del Ciudadano" aprobada por la Asamblea Nacional Constituyente francesa (26 de agosto de 1789), documento precursor de los derechos humanos, menciona explícitamente en su artículo XVI el concepto de constitución: «Una sociedad en la que la garantía de los derechos no está asegurada, ni la separación de poderes determinada, no tiene Constitución». El siglo XIX supuso un desarrollo constante de esta idea de constitución, de división de poderes y de establecimiento del derecho moderno como hoy lo conocemos. Así, con el liberalismo, las constituciones se concretan y desarrollan ("Ustawa Rządowa" o constitución polaca del 3 de mayo de 1791, "Constitución francesa" del 3 de septiembre de 1791, "Constitución Política de la Monarquía Española" del 19 de marzo de 1812, etc.); diseñando sistemas políticos muy diversos ("Acta constitucional del pueblo francés" o "Constitución del año I" -1793, una constitución republicana y democrática, que reconocía el derecho al trabajo y a la insurrección, y que no llegó a entrar en vigor-, "Carta constitucional" francesa de 1814 -una "carta otorgada" que no reconocía la soberanía nacional-).

Más allá de los derechos civiles y políticos, la introducción de la llamada "segunda generación de derechos" (los derechos sociales) comienza en las constituciones surgidas en el primer tercio del siglo XX ("Constitución Política de los Estados Unidos Mexicanos" de 31 de enero de 1917, "Constitución soviética" de 10 de julio de 1918 -precedida por una "Declaración de derechos del pueblo trabajador y explotado"-, Constitución de Weimar de 11 de noviembre de 1919, Constitución española de 1931). Simultáneamente se promulgaron las constituciones o pseudo-constituciones fascistas ("Carta del Lavoro" italiana de 1927, "Ley Habilitante" alemana de 1933, "Leyes Fundamentales" españolas de 1938-1967), caracterizadas por no imponer límites al poder del gobernante.

El siguiente hito fundamental fue la Segunda Guerra Mundial, tras la cual se produjo la "Declaración Universal de los Derechos Humanos" (10 de diciembre de 1948), cuya ratificación por los distintos Estados le otorgó cierto valor constitucional que algunas constituciones explícitamente reconocen.

La norma fundamental no solo es una norma que controla y estructura el poder y sus manifestaciones en una sociedad, sino que además es la norma que reconoce los derechos que el Estado advierte en todas las personas. La Constitución no otorga los derechos, como tampoco lo hacen las múltiples declaraciones que internacionalmente se han pronunciado sobre el tema. Los derechos humanos son precedentes a cualquier Estado y superiores a cualquier expresión de poder que este tenga.

Hasta el día de hoy el proceso demostró un desarrollo, gracias al cual el modelo inicial del sujeto poderoso y violento pasó al pueblo soberano y superior en sus derechos a cualquier expresión del Estado. Hoy el sujeto poderoso no es una persona, sino que es una entelequia creada por el pueblo y ocupada por él según las normas que este mismo estableció a través de una Constitución.

El punto más novedoso de este desarrollo se da con la certeza de que la mera declaración de derechos no hace a estos invulnerables a cualquier violación o intento de violación por parte tanto del Estado como de otras personas. En ese sentido el desarrollo del Constitucionalismo moderno se dedica al estudio de procedimientos que aseguren una adecuada protección a los derechos reconocidos. Algunos de estos procedimientos tienen un gran desarrollo histórico y teórico (como el "habeas corpus" que data del siglo XIII) y otros son aún novedosos y tienen poco desarrollo (como el "habeas data" y la acción de cumplimiento).






</doc>
<doc id="738" url="https://es.wikipedia.org/wiki?curid=738" title="Clipper (lenguaje de programación)">
Clipper (lenguaje de programación)

Clipper es un lenguaje de programación procedural e imperativo creado en 1985 por Nantucket Corporation y vendido posteriormente a Computer Associates, la que lo comercializó como CA-Clipper. En un principio Clipper se creó como un compilador para el sistema gestor intérprete
de bases de datos dBase III (de hecho las "versiones estacionales" de Nantucket incluían una etiqueta que lo indicaba así), pero con el tiempo el producto evolucionó y maduró, convirtiéndose en un lenguaje compilado más poderoso que el original, no solo por sus propias implementaciones sino también por las ampliaciones desarrolladas por terceros en C, Ensamblador y Pascal, de los que fue heredando características. Esto lo convirtió en la herramienta líder de desarrollo de aplicaciones de bases de datos relacionales bajo sistema operativo MS-DOS, sobre todo programas de gestión, contabilidad y facturación, agendas comerciales y programas de testificación.

A diferencia de otros lenguajes xBase, Clipper nunca contó con un modo intérprete, similar al de dBase. Sus utilidades para manejo de base de datos, tales como la de creación de tablas (DBU), se entregaban con el código fuente escrito en Clipper e incluido, el usuario podía adaptarlas a sus necesidades si quería. Se aportaban también muchas rutinas escritas en C y Ensamblador a las que el usuario podía acudir, incluso ampliar y crear bibliotecas de pre-enlace completas.

Clipper trabaja en modo compilador puro generando un código objeto binario; el paquete proveía también un enlazador (RTLINK o DLINK, y BLINKER) que con el módulo objeto y las bibliotecas de pre-enlace generaba un módulo ejecutable directo. Esto último le otorgaba a las aplicaciones Clipper una velocidad que otros manejadores de bases de datos no poseían, y, como desventaja, la necesidad de recompilar y enlazar nuevamente cada vez que se corregía algún error en el código fuente (la depuración era lenta).

Posee características que fueron muy atractivas para su época y su entorno de trabajo (DOS), tales como: manejo propio de memoria virtual (RAM en disco); manejo de memoria extendida, las aplicaciones podían superar la barrera de los 640Kb de RAM impuesta por MS-DOS; rutinas y bibliotecas pueden cargarse solo cuando son necesarias y se descargan de RAM cuando ya no hacen falta (enlace y overlays dinámicos); la cantidad de registros por tablas estaba solo limitada a la capacidad del disco (con máximo 1024 columnas); gran robustez en las aplicaciones, particularmente en las diseñadas para cliente-servidor (red LAN), etc.

Si bien no poseía prácticamente funciones de cálculo tales como las trigonométricas, que otros lenguajes como FoxPro sí incorporaron; el usuario las podía fácilmente elaborar en C y utilizarlas como cualquier otra función de biblioteca propia del paquete, ventaja que devenía de que el compilador Clipper y muchas de sus bibliotecas estaban casi completamente desarrolladas en C.

Su forma, administración, almacenamiento e intercambio de pantallas era sencillo, efectivo y veloz; lo que otorgaba buen dinamismo a las aplicaciones desarrolladas con Clipper.

El lenguaje en sí era poderoso, contando con una gran cantidad de sentencias, funciones, administración de memoria y variables que permitían al programador desarrollos muy flexibles y eficientes; en forma razonablemente rápida. También el paquete incluía un completo "manual en línea navegable", que se cargaba en memoria RAM, a requerimiento del programador, y se accedía por la sola presión de un par de teclas.

En su larga época dorada ha sido, probablemente, el lenguaje gestor de bases de datos relacionales de "bajo y mediano porte" más utilizado en el mundo. Aún hoy existen muchos desarrolladores Clipper (algunos agrupados comunidades y con foros en Internet), que elaboran aplicaciones, incluso estilo Windows, usando sus propias bibliotecas gráficas escritas en C y Ensamblador.

Las primeras versiones se denominan "versiones estacionales" por hacer referencia a una estación del año en sus nombres oficiales. Todas ellas se nominaban como compiladores dBase. Estas fueron:


Clipper 5.0 supone un salto cualitativo del lenguaje, aunque comienza mal. Dada la creciente popularidad (Summer 87 ha sido utilizada hasta el año 2000 como herramienta habitual de desarrollo; en Telefónica se utilizó ampliamente hasta al menos 2008 en aplicaciones importantes), se decide centrarse más en ampliar el lenguaje que en ser un "compilador mejorado" de dBase. Se implementan así los pseudoobjetos y otras mejoras... pero el producto se lanza con numerosos errores de software que hacen que el público objetivo se retraiga y siga usando la versión Summer87, mucho más estable. La 5.01 corrige muchos de los problemas, pero no será hasta la 5.2 que se produzca el vuelque masivo de los desarrolladores.

Las versiones 5 de Nantucket son:




La multinacional americana Computer Associates adquiere Nantucket y se lanza a mejorar el producto afianzando las características heredadas de C, en particular el tipo de datos code-block (literalmente "bloque de código", un híbrido entre las macros de dBase, la evaluación de cadenas de caracteres y los punteros de funciones). Otra de las mejoras procedentes de la versión 5.0 es el sistema "Replaceable Database Drivers" (RDD o drivers reemplazables de base de datos), que permiten con una sola sentencia cambiar entre diferentes normas de base de datos. La aparición de la versión 5.2, con una carrera frenética de subversiones (con mejoras y corrección de errores) hasta la 5.2c, que marca el comienzo de la migración masiva de quienes todavía permanecían en Summer'87. Deviene así la versión de Clipper más usada de la historia. Contrariamente, su sucesora, 5.3, pese a implementar mejoras, cae en un error de bulto, al no tener en cuenta la compatibilidad con al menos las más populares bibliotecas de Clipper (tanto comerciales como freeware), y por consumir excesivos recursos de DOS.

Computer Associates decide abandonar Clipper ante la pujanza de Microsoft Windows, y vuelca parte del desarrollo de Clipper (el "proyecto Aspen" de Nantucket) a su nueva herramienta CA-Visual Objects, que se presenta casi a la vez que Clipper 5.3 Pero el abandono de la sintaxis xBase y el no proveer una herramienta de migración adecuada, unido al alto precio del producto (que además debía competir con otros productos de la propia casa, uno de ellos basado en BASIC), hace que el grueso de programadores Clipper opten por permanecer en las versiones 5.2/5.3 con bibliotecas de terceros como FiveWin, y DGE, o migren a herramientas xBase como Visual FoxPro a medida que el mercado DOS va reduciéndose.

El 22 de abril de 2002 Computer Associates y GrafX Software anuncian que han alcanzado un acuerdo de licenciamiento, marketing y desarrollo de dos de sus lenguajes de desarrollo: CA-Clipper y CA-Visual Objects. 

Una de las principales características que ayudó al éxito de Clipper fue la posibilidad de expandir el lenguaje con rutinas en C y ensamblador. Varias de ellas, como CodeBase o Apollo son RDDs. Con la aparición de Windows se desarrollaron varias de ellas para portar las aplicaciones Clipper a Windows. La más popular es la española FiveWin, empleada en los productos líderes de contabilidad en España.

Además, el uso de linkers alternativos permitieron mejorar el rendimiento del ejecutable generado. El más aclamado fue Blinker, que añade un extensor de DOS con modo protegido (es utilizado con numerosos lenguajes y compiladores). Añadió soporte para compilar programas y bibliotecas para Windows.

En la actualidad el lenguaje Clipper está siendo activamente implementado y extendido por varios proyectos y vendedores. Entre los proyectos de software libre podemos destacar Clip, Harbour y xHarbour. Entre los compiladores comerciales "XBase++" y "Visual FlagShip". Y Otros productos como "MEDIATOR" y "Eagle1 y Condor1" que le brindan la posibilidad de conectarse a Servidores de bases de datos relacionales como MS-SQL, MySQL y Oracle. 

XBase++ ha sido llamado el Compilador Clipper de 32 bits, siendo actualmente el líder en innovaciones e incorporaciones al lenguaje. Ya se han vendido más de 25.000 copias del compilador y lo usan desde desarrolladores solitarios hasta grandes empresas como Hewlett-Packard o el Gobierno de Canadá.

Varias de esas implementaciones son portables gracias a su desarrollo en C (DOS, Windows, GNU/Linux (32 y 64 bits), Unix (32 y 64 bits), y Mac OS X), soportando varias extensiones del lenguaje ; cuentan con varias extensiones del lenguaje, y varios Replaceable Database Drivers (RDD) que soportan los formatos más populares de tablas, como DBF, DBTNTX, DBFCDX (FoxPro y Comix), MachSix (Apollo), SQL, y más. Todas estas nuevas implementaciones mantienen la completa compatibilidad con la sintaxis estándar xBase, a la vez que ofrecen programación orientada a objetos y sintaxis orientada al destino como SQLExecute().

Actualmente hay una versión libre, el Proyecto Harbour que tiene como objetivo original ser 100% compatible con la versión 5.2 (La más popular de las versiones de Clipper), también se han añadido nuevas características como soporte para SQL a través de SQLite.
Harbour está disponible para múltiples plataformas, incluyendo no solo MS-DOS y Windows, sino también a GNU/Linux, OS/2 y otras.
En el mes de julio de 2011 se anunció oficialmente el lanzamiento de la versión 3.0, la cual ha sido la última publicada y se descuenta que el proyecto ha sido abandonado.

En 2005, los newsgroups de Usenet relativos a Clipper comp.lang.clipper y comp.lang.clipper.visual-objects seguían activos.

Un sencillo Hola Mundo:
Una máscara simple de entrada de base de datos:
Función que invierte una cadena de caracteres (Clipper 5.2):
CLEAR
ACCEPT "Teclee una cadena para invertirla: " TO cTexto
? cTexto
? "invertido es: "
? invStr( cTexto )
FUNCTION invStr( __cTexto )
RETURN cNew


</doc>
<doc id="739" url="https://es.wikipedia.org/wiki?curid=739" title="Solenostemon">
Solenostemon

Los cóleos son plantas perennes oriundas de África y Asia tropical. En las clasificaciones actuales constituyen un género taxonómico, el género Solenostemon. En otro tiempo se clasificaba a los cóleos en el género "Coleus", taxón hoy en día abandonado, aunque algunos autores lo consideran todavía como un sinónimo del género "Plectranthus".

Muchos de los cultivares de la especie del sudeste asiático "Solenostemon scutellarioides" han sido seleccionados por el colorido y los marcados contrastes de sus hojas variegadas, que pueden ser verdes, rosas, amarillas, marrones y rojas. Estas plantas vegetan bien en suelos húmedos y bien drenados, con una altura que varía entre 0,5 y 1 metro, aunque algunas pueden alcanzar los 2 metros. Se cultivan principalmente como ornamentales. Toleran el calor aunque prefieren una ubicación sombreada en zonas subtropicales. En regiones de climas más fríos son normalmente cultivadas como anuales, ya que no son resistentes.

Las pequeñas flores púrpuras surgen en el ápice de los tallos.

Aunque la mayoría de estas plantas se cultivan para uso en jardinería, son recomendables para hacer experimentos, gracias a su fácil cultivo y rápido crecimiento. Entre los muchos experimentos que se pueden hacer con esta planta, están: hacer injertos con la misma y estudiar sus coloridas células.

Entre las especies más populares utilizadas en jardinería se encuentra el "Solenostemon blumei".










</doc>
<doc id="743" url="https://es.wikipedia.org/wiki?curid=743" title="Cinetosis">
Cinetosis

La cinetosis o mal de movimiento es un trastorno debido a que existe un desacuerdo entre el movimiento percibido visualmente y el sentido de movimiento del sistema vestibular producidos por la aceleración y desaceleración lineal y angular repetitivas producidas por automóviles, aviones, trenes o por mar.
También lo pueden producir algunos videojuegos pero en menor proporción.
Otra de sus manifestaciones es el síndrome de adaptación espacial.

Sus síntomas más comunes son mareos, fatiga, pérdida de equilibrio, debilidad, dolor de cabeza, alucinaciones y náuseas. Si el movimiento que produce náuseas continúa, el afectado comenzará a vomitar. Los vómitos generalmente no aliviarán la debilidad ni las náuseas, por lo que la persona puede seguir vomitando hasta que se trate su causa.
Las alucinaciones pueden llegar a ser fuertes por lo que lo mejor es mantener la calma y sentarse hasta que se pase el efecto.

Sus causas son la estimulación excesiva del aparato vestibular por el movimiento como causa primaria. La susceptibilidad individual es muy variable. Los estímulos visuales como un horizonte en movimiento, la mala ventilación debida a humos, monóxido de carbono o vapor, en muchas circunstancias asociados al vehículo de transporte y los factores emocionales como el miedo o la ansiedad, actúan junto con el movimiento para precipitar un ataque, otra explicación de los síntomas es que el oído está íntimamente relacionado con el sistema nervioso central y con el nervio vago, que influye en las actividades del corazón, tráquea, glándulas salivares y estómago. La susceptibilidad a la enfermedad de los viajes es sumamente individual, algunos individuos nunca experimentan ningún síntoma.

Como remedio, cuando no se puede evitar el movimiento, las personas susceptibles pueden reducir la exposición colocándose en la zona de menor movimiento del vehículo. Lo mejor es la posición en decúbito supino o semirrecostado con la cabeza bien apoyada. Se debe evitar la lectura. El mantenimiento del eje de visión con un ángulo de 45 grados por encima del horizonte reduce la susceptibilidad. Para algunas personas es útil evitar fijar la vista sobre las olas u otros objetos en movimiento. En los barcos es importante que el camarote esté bien ventilado, así como salir a la cubierta para respirar aire fresco. El exceso de alcohol o comida antes del viaje o durante el mismo aumenta la probabilidad de cinetosis. Se deben consumir cantidades pequeñas de líquidos y comidas sencillas con frecuencia durante un viaje prolongado, aunque si se trata de un viaje corto en avión es preciso evitar los líquidos y sólidos y especialmente las bebidas alcohólicas y las que contienen gases.

Existe la posibilidad de disminuir los síntomas fijando la mirada en un objeto que no esté en movimiento. A otras personas les funciona cerrar los ojos durante el viaje e intentar relajarse o incluso dormirse. También dormir es una solución para algunas personas para eliminar los síntomas tras un ataque.



</doc>
<doc id="744" url="https://es.wikipedia.org/wiki?curid=744" title="Calathea">
Calathea

Calathea es un género de plantas de la familia de las Marantaceae. Nativa de América tropical, principalmente de Brasil y Perú, muchas de las especies son populares como plantas hogareñas o de ornato, algunas conocidas como "de la pregaria" o "cebra". Comprende 689 especies descritas, y de éstas, solo 287 están aceptadas.
Son plantas herbáceas, rizomatosas, que en estado espontáneo pueden alcanzar un metro de altura, mientras que si son cultivadas no rebasan los 50-60 cm. Tiene hojas muy hermosas, con extraordinario colorido en alternancia de verdes, rosas y púrpuras, en varias tonalidades y matices.

Se utilizan como plantas de interior: en las regiones tropicales se convierten en espléndidas plantas de jardín.

Necesitan luz difusa, nada de sol directo, no soportan temperaturas inferiores a los 18 °C y necesitan ambientes húmedos.

Para su cultivo en maceta requiere de 3 partes de tierra de hojarasca, 3 de tierra de jardín, 3 de turba y 1 parte de arena. El posible cambio de maceta se realiza a finales del invierno. Su multiplicación se hace por división de las plantas al comienzo de la primavera.

El género fue descrito por Georg Friedrich Wilhelm Meyer y publicado en "Primitiae Florae Essequeboensis" . . . 6–7. 1818. 
"Calathea albertii"
"Calathea allouia"
"Calathea capitata" - achira de monte en Perú
"Calathea crocata"
"Calathea galdamesiana"
"Calathea lancifolia"
"Calathea loeseneri"
"Calathea louisae"
"Calathea lutea"
"Calathea macrosepala K. Schum"
"Calathea majestica"
"Calathea makoyana"
"Calathea orbifolia"
"Calathea ornata"
"Calathea roseopicta"
"Calathea rufibarba"
"Calathea tiroi"
"Calathea undulata"
"Calathea warscewiczii"
"Calathea zebrina"


</doc>
<doc id="746" url="https://es.wikipedia.org/wiki?curid=746" title="Ciudad">
Ciudad

Una ciudad es un asentamiento de población con atribuciones y funciones políticas, administrativas, económicas y religiosas, a diferencia de los núcleos rurales que carecen de ellas, total o parcialmente. Esto tiene su reflejo material en la presencia de edificios específicos y en su configuración urbanística.

Una ciudad es un espacio urbano con alta densidad de población, en la que predomina el comercio, la industria y los servicios. Se diferencia de otras entidades urbanas por diversos criterios, entre los que se incluyen población, densidad poblacional o estatuto legal, aunque su distinción varía entre países. La población de una ciudad puede variar entre unas pocas centenas de habitantes hasta decenas de millones de habitantes. Las ciudades son las áreas más densamente pobladas del mundo, por ejemplo São Paulo con sus cerca de 20 millones de habitantes tiene una densidad poblacional aproximadamente 7160 habitantes por kilómetro cuadrado, mientras que todo Brasil posee poco más de 22 hab./km².

El término ciudad suele utilizarse para designar una determinada entidad político-administrativa urbanizada. Sin embargo, la palabra también se usa para describir un área de urbanización contigua (que puede abarcar diversas entidades administrativas). Por ejemplo, la ciudad de Londres propiamente dicha tiene aproximadamente 8 millones de habitantes. No obstante, cuando alguien se refiere a la ciudad de Londres, suele referirse a su área metropolitana, es decir, al conjunto de su área urbanizada, la cual tiene aproximadamente 15 millones de habitantes. Otro claro ejemplo es la ciudad mexicana de Monterrey, que tiene un área metropolitana formada por 11 municipios, y cuyos puntos están distribuidos por toda el área metropolitana, a la cual popularmente se le conoce como Monterrey. La ciudad de México y su zona metropolitana con más de 20 millones de habitantes es otro ejemplo. También podría usarse como ejemplo la confusión que se crea cuando se habla del Área Metropolitana de Buenos Aires, ya que el Gran Buenos Aires junto con la Ciudad Autónoma de Buenos Aires son confundidos como una sola ciudad, “Buenos Aires”, pero el Gran Buenos Aires es parte de la Provincia de Buenos Aires, y la ciudad de Buenos Aires es la capital de la República Argentina y son gobernadas por distintas instituciones, además la Ciudad Autónoma de Buenos Aires tiene capacidades de autogobierno casi idénticas a las de una provincia argentina. Tokio, muchas veces descrita incorrectamente como una ciudad, es en realidad una provincia de Japón, formada por 23 barrios diferentes. Santiago de Chile es una ciudad conformada por 40 municipios, todo el conjunto en general se le conoce como "Gran Santiago".

La Conferencia Europea de Estadística de Praga, celebrada en 1966, propuso, sin aceptación, considerar como ciudades las aglomeraciones de más de 10 000 habitantes y las de entre 2000 y 10 000 habitantes siempre que la población dedicada a la agricultura no excediera del 25 % sobre el total. A partir de 10 000 habitantes, todas las aglomeraciones se consideran ciudades, siempre que estos se encuentren concentrados, generalmente en edificaciones colectivas y en altura, y se dediquen fundamentalmente a actividades de los sectores secundario y terciario (industria, comercio y servicios). Esta definición ha quedado en desuso, por lo que, a falta de una regla global, cada país ha creado sus propias reglas adaptadas a sus características particulares.

El concepto político de ciudad se aplica principalmente a conglomerados urbanos con entidad de capitalidad y mayor importancia en la región y que asume los poderes del Estado o nación. Será la "ciudad capitalina", pero por extensión se aplica la denominación a cualquiera entidad administrativa con alguna autonomía a nivel de municipio, siendo las demás denominaciones, como pueblo, genéricas y optativas.

En el concepto religioso, tanto en la Alta Edad Media como en otros periodos como el Renacimiento y anteriormente al siglo XII, solo era ciudad la que dentro de sus murallas tuviera una catedral donde un obispo ostentase su propia cátedra; ya que en el pasado las catedrales eran también centros docentes. En algunos países europeos como Francia o España, durante la Edad Media y la Inquisición, dentro del concepto político solo fue considerada ciudad como tal la que tuviese su propia catedral o que fuese sede de una arquidiócesis, llegándose a dar el caso de que en una misma ciudad con más de una arquidiócesis se construyese más de una catedral, en dedicación a cada patrón.

Es pues una definición administrativa del estado político, región geográfica o comunidad autónoma, que tienen una ciudad central y pueblos o ciudades menores. La geografía urbana y la sociología urbana estudian ambos aspectos desde el punto de vista de la geografía humana y la sociología con la ecología humana. Asimismo, la ecología urbana estudia la ciudad como un ecosistema y analiza los flujos de materia y energía entre la ciudad y su entorno.

La definición de lo que se entiende bajo el concepto de «ciudad» no solo varía según las específicas leyes o reglamentos de cada país, sino también conforme a las distintas apreciaciones de cada especialista. Algunas de estas interpretaciones son:

El "Diccionario de la Academia Francesa" ("Dictionnaire de l'Académie française"), desde la edición de 1694 hasta la de 1835, definió a la ciudad ("ville") como «la reunión de muchas casas dispuestas en calles y encerradas dentro de un recinto común que suele ser de muros y fosos». Pero esta definición está muy anticuada. Hoy en día, el "Tesoro de la lengua francesa" ("Trésor de la langue française") dice: «conglomerado relativamente grande cuyos habitantes tienen actividades profesionales diversificadas, especialmente en el sector terciario».

El "Diccionario de la lengua española", de la Real Academia Española, define a la ciudad como un «conjunto de edificios y calles, regidos por un ayuntamiento, cuya población densa y numerosa se dedica por lo común a actividades no agrícolas».

En la organización política del territorio, en las que los diversos núcleos poblacionales tenían diferentes privilegios, el título de ciudad se le daba a algunos de ellos y les otorgaba mayores preferencias que a las villas. En el mismo sentido que las villas, que solían obedecer al fuero común otorgado por el rey (usualmente era su fundador), al contrario de las anteiglesias o aldeas (núcleos de población bajo la jurisdicción de un Señor), el estatus de ciudad era el reconocimiento de algún hecho singular en el que la población había participado activamente.

En los nomenclátores de España, existen los títulos de «ciudad» y «villa», tratándose de calificaciones otorgadas o tradicionalmente reconocidas. Ambas categorías corresponden, mayoritariamente, a entidades urbanas. Así, según el "Diccionario de la lengua española de la Real Academia Española", una de las acepciones de ciudad es:

La diferenciación entre ciudad y villa no guarda ninguna relación con el tamaño o importancia actual de la entidad. Por ejemplo, la villa de Madrid es la capital de España y supera en número de habitantes a la ciudad de Barcelona.

La Oficina Nacional de Estadísticas de Cuba reconoce como ciudad a las poblaciones urbanas de más de 20 000 habitantes, mientras que las poblaciones de carácter urbano entre 2000 y 20 000 habitantes son catalogados como pueblos. Dentro de las ciudades se distinguen tres categorías:
Esta clasificación es independiente del estatus político-administrativo de la ciudad, no obstante 13 de las 15 capitales provinciales son ciudades de primer orden y 72 de los 168 municipios del país tienen como cabecera una ciudad.

Durante la Colonia y el primer siglo de la República, la calidad de ciudad en Chile solo se podía obtener mediante decreto, fuera este real o gubernamental, y solo para el caso de aquellas localidades que previamente hubieran sido catalogadas de villa. Luego esta forma de catalogar a las ciudades fue dejada en desuso.

Actualmente, la calificación de ciudad es realizada, para efectos puramente estadísticos, por el Instituto Nacional de Estadísticas, que establece que son ciudades las localidades urbanas cuya población es igual o superior a los cinco mil (5000) habitantes. También, clasifica las ciudades en:

La historia de las ciudades del mundo es en general larga, dado que las primeras ciudades habrían surgido entre quince a cinco mil años atrás, como asentamientos permanentes poco complejos. Las sociedades sedentarias que viven en ciudades son frecuentemente llamadas civilizaciones. La rama de la historia y del urbanismo encargada del estudio de las ciudades y del proceso de urbanización es la historia urbana. Las primeras ciudades verdaderas son a veces consideradas aquellos grandes asentamientos permanentes donde sus habitantes ya no eran los simples dueños de las áreas cercanas al asentamiento, sino que pasaron a trabajar en ocupaciones más especializadas en la ciudad, donde el comercio, la provisión de alimentos y el poder fueron centralizados.

Usando esta definición, las primeras ciudades conocidas aparecieron en Mesopotamia (Ur, por ejemplo), a lo largo del río Nilo, en el valle del Indo y en China, entre aproximadamente siete a cinco mil años atrás, siendo generalmente resultado del crecimiento de pequeñas aldeas y/o de la fusión de pequeños asentamientos. Antes de esta época, los asentamientos raramente alcanzaron algún tamaño significativo, aunque hay casos excepcionales como Jericó, Çatal Höyük y Mehrgarh. Harappa y Mohenjo-Daro, ambas ciudades del valle del Indo, eran las más populosas de estas antiguas ciudades, con una población conjunta estimada entre 100 y 150 mil habitantes.

El crecimiento de los imperios antiguos y medievales coadyuvó en la aparición, en el mar Mediterráneo, de grandes ciudades capitales y sedes de la administración provincial, como Babilonia, Roma, Antioquía, Alejandría, Cartago, Seleucia del Tigris, Pataliputra (localizada en la actual India), Chang'an (localizada en la actual República Popular de China), Constantinopla (actual Estambul), y, posterior y sucesivamente, diversas ciudades chinas e indias. Roma contaba con más de un millón de habitantes en el siglo I a. C., siendo considerada por muchos como la única ciudad a superar esta marca hasta el inicio de la Revolución industrial. En la antigua Roma se denominaba ciudad "(cívitas)" a la zona habitada por ciudadanos "(cívis)", los cuales eran aquellos que poseían derechos ciudadanos, independientemente de su actividad (fuera la industria, la agricultura o los servicios). Otros grandes centros administrativos, comerciales, industriales y ceremoniales emergieron en otras áreas, siendo considerada Bagdad como la primera ciudad en batir la marca del millón de habitantes, que ostentaba Roma.

Durante la Edad Media en Europa, una ciudad era tanto una entidad político-administrativa como una agrupación de casas. En la España medieval y del Renacimiento, una ciudad era la población que no tenía señor y era regida directamente por el rey. Tenía el privilegio de enviar procuradores a las cortes para negociar las tasas y gabelas que le pudieran ser impuestas, a cambio de fueros. Esta calificación de ciudad era independiente del tamaño, así, Madrid, capital de España desde 1561, no era ciudad sino villa, estatus que aún conserva. Algunas ciudades, excepcionalmente, tales como Venecia, Génova o Lübeck, se convirtieron en ciudades-estados poderosas, tomando en ocasiones el control de las tierras próximas o estableciendo extensos imperios marítimos. Tal fenómeno no se limitó solamente a Europa, sino que se dieron casos como el de Sakai, que poseía un considerable grado de autonomía en el Japón medieval. En Europa se consideraban las ciudades más importantes de esta época Venecia, Róterdam, Florencia y Lisboa, las cuales crecieron todas al alero de sus puertos y un rol importante en el intercambio comercial.

A medida que las ciudades-estados situadas en los litorales del Mediterráneo y del mar Báltico comenzaban a desaparecer a partir del siglo XVI, las grandes capitales europeas se beneficiaron del incremento del comercio que surgió fruto de la colonización de América y el establecimiento de una economía transatlántica. Hacia finales del siglo XVIII, Londres se había convertido en la mayor ciudad del mundo, con una población que se aproximaba al millón de habitantes, con París, Bagdad, Pekín, Estambul y Kioto como otras grandes ciudades. Pero fue el inicio de la Revolución Industrial y el crecimiento de la industria moderna, a fines del siglo XVIII, lo que permitió la urbanización masiva y el surgimiento de nuevas grandes ciudades, primeramente en Europa, y luego en otras regiones, a medida que las nuevas oportunidades generadas en las ciudades hicieran que un gran número de emigrantes provenientes de comunidades rurales se instalasen en áreas urbanas.

Actualmente, las grandes ciudades son mucho mayores y más populosas que en tiempos pasados. Un ejemplo es París, que en 1400, tenía 225 mil habitantes en 8 km² de área. Hoy en día, la ciudad cuenta con 2,2 millones de habitantes y 105 km², y su región metropolitana posee más de 11,2 millones de habitantes y 14 518 km² de área.

En Estados Unidos y en Canadá, el padrón más común de las vías públicas es el plan hipodámico o de damero, esto es, arterias viales corriendo paralelas entre sí, con otras calles paralelas cortándolas perpendicularmente. Este sistema fue también usado por millares de años en China, y por los españoles al fundar ciudades en América. En Europa, dado que la mayoría de las ciudades no se planificaron de antemano, su sistema de vías públicas, calle y avenidas se extienden desorganizadamente por la ciudad. Muchas de las murallas que cercaban las antiguas ciudades europeas dieron lugar a modernas vías públicas de alta capacidad.

Comúnmente, las grandes ciudades poseen un distrito financiero, donde se localizan instituciones financieras, sedes de grandes compañías y centros comerciales. Personas de todas partes de la ciudad (así como de ciudades vecinas) acuden a este centro financiero a trabajar diariamente. Este generalmente es pequeño en área, pero puede albergar hasta decenas de miles de puestos de trabajo, gracias a la existencia de los rascacielos. La región de la ciudad de Londres propiamente dicha, a modo de ejemplo, centro financiero de la región metropolitana de Londres, posee 2900 km² y 8,6 millones de habitantes permanentes, y más de 300 mil personas de otros lugares de la región metropolitana van a la "City" a trabajar cada día.

La administración de las ciudades corresponde a distintas instituciones, dependiendo de cada país. Entre las denominaciones más corrientes que se emplean para designar al órgano administrativo de una ciudad se encuentran municipalidad, ayuntamiento y prefectura. Estas organizaciones son responsables por la planificación de la ciudad, y de acuerdo a las competencias dadas por las respectivas legislaciones nacionales, pueden encargarse de la administración del sistema de transporte público, del sistema escolar y de bibliotecas públicas, de policía y de bomberos. La administración de una ciudad está encabezada generalmente por un alcalde o presidente municipal y/o un concejo, todos elegidos por votación popular (en regímenes democráticos). Habitualmente está a cargo de velar por los intereses de sus conciudadanos, representándolos ante la autoridad jerárquica mayor, además de impulsar políticas locales para mejorar su calidad de vida, como programas de salud o deporte, y combatir contra la delincuencia, entre otras diversas tareas. Su presupuesto proviene por lo general de fondos nacionales y de ciertos ingresos propios, como permisos de comercio, edificación o impuestos específicos.

Algunas grandes ciudades suelen subdividirse administrativamente en comunas, barrios, distritos, delegaciones o pedanías.

Actualmente, la economía de las ciudades es general y altamente diversificada, variando entre ciudades. Ya que la economía urbana nunca se basa solamente en un determinado sector económico, varias ciudades dependen principalmente de un único, o de algunos pocos, sectores económicos. Algunas ciudades, sin embargo, aún dependen mucho de la agricultura y la ganadería, tales como Saskatoon. La economía de las grandes ciudades tiende a ser más diversificada, mas esto no siempre sucede. En las ciudades de mayor tamaño, la industria manufacturera es casi siempre una de las principales fuentes de ingresos, generando miles de empleos, aunque la industria ya no es actualmente la mayor actividad económica de las ciudades, traspasando esta posición al sector servicios. En varias grandes ciudades, miles de personas trabajan diariamente en oficinas e instituciones financieras. Urbes como Nueva York, Tokio, Londres, París y Hong Kong son grandes polos financieros, donde esta actividad es la principal fuente de ingresos de la ciudad. En otras ciudades, como Roma, Quebec y Foz do Iguazú, dependen enormemente del turismo. Diversas ciudades poseen una economía altamente diversificada, es decir, donde todos los sectores tienen aproximadamente la misma importancia, por lo que están menos vulnerables a recesiones económicas en comparación a aquellas ciudades que dependen de un sector económico en particular.

Una metrópolis es un gran centro poblacional, que consiste en una gran ciudad central (a veces, dos o más) y su zona adyacente de influencia, constituida por otras ciudades y/o localidades menores y relativamente próximas. Generalmente, las metrópolis forman conurbaciones, formando una única área urbana. Por ejemplo, la Ciudad de México es una ciudad central, y con Naucalpan, Ecatepec de Morelos, Tultitlán y otras localidades adyacentes juntas forman una conurbación, conocida como "Zona Metropolitana de la Ciudad de México".

Sin embargo, una metrópolis no necesita estar obligatoriamente formada por una única área urbanizada contigua, pudiendo designarse como metrópolis la unión de dos o más áreas urbanizadas intercaladas con áreas rurales. Las ciudades que forman una metrópolis tienen un alto grado de integración entre sí. Una región formada por diversas metrópolis localizadas próximas entre sí son conocidas como megalópolis. Actualmente, las metrópolis más populosas del mundo, que poseen entre 10 y 40 millones de habitantes, son Tokio, Ciudad de México, Seúl, Nueva York, Buenos Aires y São Paulo.

Según cálculos fidedignos, el crecimiento de la población urbana es asombroso, pues supera el millón de personas semanalmente. Más de doscientas ciudades de los países en vías de desarrollo sobrepasan el millón de habitantes, y hay unas veinte metrópolis con más de diez millones de residentes. Además, no se prevé que aminore el aumento. De acuerdo con un informe del Instituto Worldwatch, la ciudad nigeriana de Lagos, “tendrá 25.000.000 de habitantes en el año 2015, con lo que la decimotercera ciudad más grande del mundo pasará a ser la tercera”.

Una ciudad global es un gran centro bancario, comercial, financiero, político e industrial. El término "ciudad global" (que no debe ser confundido con megalópolis o con megaciudad) fue inventado por la socióloga Saskia Sassen en 1991. La expresión "megaciudad" se refiere a una gran área urbana, mientras que una ciudad global se distingue por su gran influencia a nivel regional, nacional e internacional. Las ciudades globales, según Sassen, tienen más características semejantes entre sí que con otras ciudades de su mismo país.

La noción de ciudad global visualiza a la ciudad como un contenedor donde habilidades y recursos están concentrados. Cuanto más una ciudad es capaz de concentrar habilidades y recursos, más próspera y poderosa es, volviéndose suficientemente poderosa para influenciar lo que ocurre alrededor del mundo. Críticos de esta noción alegan la ambigüedad de la expresión "poder". En una ciudad global, "poder" se refiere primariamente poder económico y/o político y, por lo tanto, puede no incluir ciudades que son poderosas en otros sentidos. Por ejemplo, ciudades como Roma o Jerusalén son poderosas en términos históricos y religiosos.

En 2000, los líderes municipales firmaron la Carta de Aalborg, una iniciativa comunitaria conocida como la Agenda 21, que promovía el desarrollo sostenible de las ciudades bajo principios medioambientales. Las principales directrices orientadoras de las actuaciones que debían emprenderse estaban centradas en los ciclos de los recursos naturales en las ciudades, la calidad del medio ambiente urbano y la planificación territorial y urbanística. Sobre la calidad del medio ambiente urbano en concreto, las actuaciones debían encaminarse a mejorar y proteger el entorno natural urbano con una planificación de los usos del suelo adecuada, la creación de parques, zonas verdes y de uso social y la recuperación de espacios de interés. Esta Agenda 21 surge ante la preocupación por la masiva urbanización mundial. Se espera que dentro de 35 años el número de población que vive en las ciudades se multiplique por tres, lo cual hace necesario un cambio que garantice la calidad de vida de las ciudades futuras.
Tradicionalmente los parques han cumplido tres funciones, satisfacer las demandas recreativas y de ocio de los ciudadanos, mejorar la calidad de vida y ayudar a estructurar el entramado urbano. Estas funciones son de vital importancia para la vida de la población dentro de las ciudades, sin embargo aunque la consideración social de estas zonas verdes es muy positiva, es necesaria una mayor implicación por parte de todos los actores, (instituciones gubernamentales, ciudadanos y medios de comunicación) para conseguir que los parques contribuyan de forma eficaz a la sostenibilidad de las ciudades.

La diferencia entre pueblo, villa y ciudad se entiende de diverso modo en distintas partes del mundo. Algunos idiomas tienen una distinción tripartita, como los idiomas ibéricos portugués y español, por ejemplo: "vila" (villa), "povo" (pueblo),y "cidade" (ciudad).
Otros tienen distinción bipartita, como el francés: "village" (pueblo/villa) y "ville" (ciudad).

Aunque en el mundo hispanohablante, no hay consenso universal sobre las distinciones exactas. El término se puede usar para villas dotadas de posición de ciudad, o para una villa que ejerce control sobre otras villas vecinas. No obstante, los términos se suelen entender así en el mundo hispanohablante:

Aldea: núcleo muy pequeño de población sin ningún personal de la administración trabajando ni a tiempo completo ni parcial.
Villa: núcleo pequeño de población, con algún personal de la administración trabajando, pero sin Ayuntamiento propio.
Pueblo: núcleo de población, con Ayuntamiento propio y otro personal de la administración.
Ciudad: núcleos, con Ayuntamiento propio y con personal para casi la totalidad de todos los servicios administrativos.

La definición es borrosa, porque históricamente la administración se entendía de manera religiosa, es decir: en la aldea había ermitas no atenidas, en las villas había parroquias con curas compartidos con otras villas, en los pueblos había curas permanentes y en las ciudades había autoridades religiosas (obispo). Al laicizarse la hispanidad esta definición histórica ha perdido sentido y el uso de los términos es ahora un tanto ambiguo.

Aunque "ciudad" puede referirse a una aglomeración, suburbios y satélites incluso, no se aplica a una conurbación (grupo) de áreas urbanas distintas, ni a un área metropolitana.




</doc>
<doc id="747" url="https://es.wikipedia.org/wiki?curid=747" title="Catedral de León">
Catedral de León

La catedral de Santa María de Regla de León es un templo de culto católico, sede episcopal de la diócesis de León, España, consagrada bajo la advocación de la Virgen María. Fue el primer monumento declarado en España mediante Real Orden de 28 de agosto del año 1844 (confirmada por Real Orden el 24 de septiembre del año 1845).

Iniciada en el siglo , es una de las grandes obras del estilo gótico, de influencia francesa. Conocida con el sobrenombre de "Pulchra leonina", que significa 'Bella Leonesa', se encuentra en pleno Camino de Santiago.

La catedral de León se conoce sobre todo por llevar al extremo la «desmaterialización» del arte gótico, es decir, la reducción de los muros a su mínima expresión para ser sustituidos por vitrales coloreados, constituyendo una de las mayores colecciones de vidrieras medievales del mundo.

Originariamente, en la actual ubicación de la catedral, la Legio VII Gemina había construido unas termas, con un tamaño superior al del actual edificio. Durante la gran restauración del edificio que se llevó a cabo en el siglo fueron descubiertos sus restos bajo la catedral, y en el año 1996 se exploraron otros junto a la fachada sur. Poco queda de estas primitivas edificaciones, apenas algunos vestigios de mosaicos, tégulas y cerámicas, hoy expuestas en el museo catedralicio. Otros, como el hipocausto, permanecen aún bajo el solar catedralicio.

Durante la reconquista cristiana, las antiguas termas romanas fueron convertidas en palacio real. En el año 916 el rey Ordoño II, que hacía pocos meses había ocupado el trono de León, venció a los árabes en la batalla de San Esteban de Gormaz. Como señal de agradecimiento a Dios por la victoria, cedió su palacio para construir la primera catedral. Bajo el episcopado de Fruminio II, fue transformado el edificio en lugar sagrado. El templo estaba custodiado y regido por monjes de la orden de San Benito, y es muy probable que su estructura fuera muy similar a la de tantos otros existentes durante la mozarabía leonesa. Siguiendo la tradición cristiana de enterrar dentro de los templos a quienes encarnaban la autoridad "venida de Dios", aquella sencilla catedral muy pronto se vio enriquecida con los restos del rey Ordoño II, fallecido en Zamora en el año 924.

Hablan las crónicas del paso de Almanzor por estas tierras a finales del siglo , devastando la ciudad y destruyendo sus templos. No obstante, parece que los daños ocasionados en la fábrica de la catedral debieron de ser inmediatamente reparados, ya que el año 999 era coronado en ella, en un acontecimiento lleno de esplendor, el rey Alfonso V. Tras una sucesión de revueltas políticas y de duras empresas bélicas, hacia el año 1067 el estado de la catedral era de suma pobreza. Ello conmovería al rey Fernando I de León, quién, después de trasladar los restos de San Isidoro de Sevilla a León, «se volcó en favores a la misma». Con este rey se inició una época pacífica, cosechando grandes triunfos en la expansión del reino cristiano. Era el momento del florecimiento del arte Románico.

Con la ayuda de la infanta Urraca de Zamora, hermana del rey, se inicia la construcción de una segunda catedral, acorde con las aspiraciones de la ciudad, y de estilo románico. Ocupaba la sede episcopal Pelayo II. Cuando el arquitecto Demetrio de los Ríos, entre los años 1884 y 1888 excavó el subsuelo de la catedral para reponer el pavimento y cimentar los pilares, encontró parte de los muros y fábrica de aquella segunda catedral. A través del plano que él mismo dibujó, podemos apreciar como se configuraba todo dentro de la gótica: era de ladrillo y mampostería, con tres naves rematadas en ábsides semicirculares, dedicado el central a Santa María, como en la iglesia anterior. También se construyó un claustro en el lado norte. Esta nueva iglesia tuvo unas dimensiones considerables, midiendo 60 metros de longitud y 40 metros de anchura máxima. Aunque toda ella estuviese ejecutada dentro de las corrientes internacionales del románico, contemplando lo que ha pervivido de su estatutaria podemos averiguar que tenía su carácter autóctono, utilizándose aún el arco de herradura, al menos como forma decorativa. Fue consagrada el 10 de noviembre del año 1073 durante el reinado de Alfonso VI. Es de suponer que en ella trabajasen los mismos canteros que estaban construyendo la Basílica de San Isidoro de León.

Esta catedral se mantuvo en pie hasta finales del siglo siguiente. Cuando accede al trono el último rey privativo de León, Alfonso IX, se asiste en la ciudad y en el reino a un importante cambio social, de creatividad artística y desarrollo cultural.

La construcción de la tercera catedral se inicia hacia el año 1205, pero los problemas constructivos de los cimientos hicieron que pronto las obras quedaran paralizadas, y no se reemprendiera la tarea hasta el año 1255, bajo el pontificado del obispo Martín Fernández y el apoyo del rey Alfonso X de Castilla, siendo esta nueva catedral de estilo enteramente gótico.

El arquitecto de la catedral parece ser que fue el maestro Enrique, seguramente natural de Francia, y que ya había trabajado anteriormente en la catedral de Burgos. Es evidente que conocía la forma arquitectónica gótica de la isla de Francia. Falleció en el año 1277 y fue sustituido por el español Juan Pérez. En el año 1289 fallecía también el obispo Martín Fernández, cuando la cabecera del templo ya estaba abierta al culto. La estructura fundamental de la catedral se finaliza pronto, en el año 1302 abriendo el obispo Gonzalo Osorio la totalidad de la iglesia a los fieles, aunque en el siglo aún se terminaría el claustro y la torre norte, y la torre sur no se finalizó hasta la segunda mitad del siglo . Esta prontitud en las obras le da una gran unidad de estilo arquitectónico.
La catedral de León, se inspira en la planta de la catedral de Reims (aunque de menor superficie), que bien pudo conocer el maestro Enrique. Al igual que la mayoría de catedrales francesas, la de León está construida con un módulo geométrico basado en el triángulo ("ad triangulum"), cuyos miembros se relacionan con la raíz cuadrada de 3, al que responden la totalidad de sus partes y del todo. Este aspecto, como la planta, los alzados, y los repertorios decorativos y simbólicos convierten esta catedral en un auténtico edificio transpirenaico, alejado de la corriente hispánica, que le ha merecido los calificativos de «la más francesa de las catedrales españolas» o el de "Pulchra Leonina", pues si sus rasgos en planta se relacionan con el gótico champaniense, parte de sus alzados están estrechamente ligados con los de la catedral de Saint Denis, dentro ya de la corriente "radiante" que se observa en Francia a partir de 1230 y de hecho puede ser considerado como caso único de edificio completamente concebido y construido dentro del Gótico radiante fuera de Francia durante el siglo XIII. Geográficamente tampoco es ajena a aquel mundo, pues aunque levantada en la vieja capital de los reyes leoneses, la ciudad era uno de los hitos más importantes del Camino de Santiago, también llamado Camino Francés. La composición arquitectónica de las portadas de la fachada oeste del templo parece inspirarse en los portales de los cruceros de la catedral de Chartres, mientras que para la peculiar cuestión del emplazamiento de las torres, separadas de la nave central y fuera de las laterales, se han propuesto antecedentes como las fachadas-pantalla de las catedrales góticas inglesas, la solución de los cruceros de la catedral de Saint Denis o más locales, como la fachada oeste de la catedral de Santiago de Compostela antes de sus importantes reformas barrocas.

También influencia francesa es el desarrollo del presbiterio, con la idea inicial de colocar allí el coro según la costumbre de aquellos. Individual leonés es la ubicación del claustro y la no continuidad de las cinco naves de la cabecera en el cuerpo del templo, que se reducen a tres.

Como rasgo característico más importante, goza la catedral leonesa de alcanzar el "summum" lumínico de todas las catedrales, con un espacio inmenso de vidrieras al reducirse la estructura pétrea de sustentamiento al mínimo posible, llegando a superar así técnicamente a las mismas catedrales francesas.

El problema fue que gran parte del solar se asienta sobre restos romanos, hipocaustos del siglo , lo que dificultó la buena cimentación de los pilares. La acumulación de humedades y la filtración de aguas ocasionó graves inconvenientes a los maestros. Por otra parte, la mayoría de los sillares de la catedral son de piedra de mediocre calidad, de tipo calizo, con escasa resistencia ante los agentes atmosféricos. Además, la sutilidad de su estilo es un desafío a la materia; los numerosos soportes son sumamente frágiles, las líneas se reducen a una depuración total, de modo que varios arquitectos de la época pusieron en duda que tal proyecto pudiera mantenerse en pie. Esta estructura casi inverosímil junto con la mala calidad de la piedra y la pobre cimentación, provocaron que desde el siglo sufriera constantes intervenciones y restauraciones, convirtiendo al templo en el paradigma europeo de intervenciones de transformación, restauración y conservación.

Sobre la puerta de san Juan, por el interior, cuelga un pellejo, a modo de quilla, que la tradición leonesa ha identificado siempre como un «topo maligno». Según cuenta la leyenda, el topo destrozaba lo construido a lo largo del día durante la noche en los primeros momentos de la magna obra del templo. Impacientándose los leoneses porque la obra de la prometida catedral no avanzaba, decidieron acabar con aquel ser maligno que no dejaba avanzar los trabajos: algunos de ellos lo esperaron durante la noche y acabaron con él a garrotazos. En recuerdo de aquel acontecimiento y en agradecimiento a la Virgen María, titular del templo, la piel del animal fue colgada en el interior de la catedral, sobre la citada puerta, en la fachada oeste.

La realidad que esconde la leyenda es que las obras de la catedral de León se encontraron con numerosos problemas de cimentación, sobre un terreno muy inestable que, ya por entonces, había acogido muchas y diversas construcciones. Por su parte, lo que hoy podemos contemplar en la penumbra sobre la ya mencionada puerta del templo catedralicio demostró durante los años 90 ser en realidad un caparazón de tortuga laúd, cuyo origen aún es incierto, aunque se presupone que se trataría de la ofrenda realizada por algún hombre de poder a la catedral, insertándose tal elemento en la antigua tradición (clave en la constitución de numerosos museos) del coleccionismo de Antigüedades y Rarezas.

La extrema fragilidad del edificio dio problemas muy pronto. En el siglo , la construcción de torrecillas huecas por el maestro Justín en la zona sur (la «silla de la reina») y en la zona norte («la limona») mejoró los empujes de los arbotantes hacia la cabecera, pero la frágil estructura siguió con problemas. Por entonces, el maestro Justín terminó la torre sur en estilo gótico flamígero. También se construyeron los remates triangulares de los hastiales norte y sur. En los últimos años del siglo , se lleva también a cabo la construcción de la Librería (actual capilla de Santiago) por Juan de Badajoz padre e hijo, en estilo gótico flamígero. También el coro es obra de este siglo, así como las pinturas de Nicolás Francés y el retablo.

A principios del siglo , Juan de Badajoz el Mozo construyó el remate del hastial occidental en estilo plateresco, excesivamente pesado y alto. También construyó una sacristía plateresca para la catedral, en el lado sur-este, y rehízo las bóvedas del claustro. En el interior del edificio destacó la construcción del trascoro.

En el siglo se reanudaron los problemas. En el año 1631 se derrumbó parte de la bóveda central del crucero. El cabildo recurrió a Juan de Naveda, arquitecto de Felipe IV de España, quien cubrió el crucero con una gran cúpula, rompiendo los contrarrestos del sistema gótico, tan distintos de los del barroco. El excesivo peso provocaría el desplazamiento de las cargas radiales hacia el hastial sur ante la debilidad de los arcos torales y al fallar también los cimientos. La linterna se cerró provisionalmente en el año 1651 pero a finales del siglo ya se advertían algunas fallas en la cúpula que desviaba su eje hacia el sur. El quebrado hastial sur tuvo que ser reedificado por Conde Martínez en el año 1694, sustituyendo el hastial gótico por una espadaña barroca.

Quiso poner remedio a estos desastres Joaquín de Churriguera levantando cuatro grandes pináculos alrededor de la cúpula y sobre los pilares del crucero, a principios del siglo , pero las consecuencias de esta intervención serían nefastas. Por León fueron desfilando grandes arquitectos, como Giacomo de Pavía, mientras los males seguían agravándose. El terremoto de Lisboa del año 1755 conmovió a todo el edificio, afectando de manera especial a los es y a las vidrieras. Se abrieron grandes grietas en la fachada sur, por lo que fue necesario cegar el triforio, desmontar el rosetón, y sustituirlo por una ventana doble geminada.

En el año 1830 aumentaron los desprendimientos de piedras en el hastial sur y, para salvarlo, Fernando Sánchez Pertejo tuvo que reforzar los contrafuertes de toda la fachada.

En el año 1844, el mal estado del edificio hizo que el Estado tuviera que acudir a su reparación, fecha en que la catedral de León fue declarada Monumento Nacional, el primero del patrimonio español. En el año 1849 el jesuíta P. Ibáñez diseñó y colocó un nuevo rosetón para el hastial sur.

Poco después, el cabildo temió un desenlace fatal cuando en el año 1857 comenzaron nuevamente a caer piedras del crucero y la nave central, cundiendo el temor de una ruina total de la catedral, que se extendió por España y por toda Europa. Intervino entonces la Real Academia de Bellas Artes de San Fernando, y el gobierno encargó las obras a Matías Laviña en el año 1859. Sin embargo, este desconocía el funcionamiento del edificio gótico al tener una formación clasicista. Desmontó la cúpula de media naranja y los cuatro pináculos que la flanqueaban debido a su excesivo peso, y prosiguió desmontando el crucero y toda la fachada sur. Pero el peligro de un total hundimiento se hacía más inminente. Las críticas que provocaron sus decisiones le llevaron a la muerte en el año 1868.

A su muerte se responsabilizó de las obras Andrés Hernández Callejo, quien pretendía seguir desmontando el edificio, cuando fue cesado en el cargo. En el año 1868 se encargó la obra a Juan de Madrazo, amigo de Viollet-le-Duc, el gran restaurador francés, sin duda el mejor restaurador de España y buen conocedor del gótico francés, que aplicaba las teorías de aquel. Para contener el deterioro del edificio y, al mismo tiempo, proceder a su reconstrucción, Juan de Madrazo proyectó su admirable sistema de encimbrado de las bóvedas altas. El encimbrado fue una complejísima trabazón de carpintería, que sirvió para sostener todos los empujes del templo mientras se procedía a la reconstrucción de toda la fachada sur y del crucero desmontados. Además, Madrazo modificó notablemente la disposición de las bóvedas, y volvió a rehacer desde la arcada la fachada sur, inspirándose en la norte, incluyendo el perdido rosetón. El nuevo hastial triangular fue también inspirado por el existente en la fachada norte. En general, planeó todo el templo tal y como lo encontramos hoy. El objetivo era conseguir la "Pulchra Leonina", es decir la catedral en su estado primigenio de gótico puro, eliminando todo aquello que alterara esa pureza. El momento más importante tuvo lugar en 1878, cuando se retiraron las cimbras y el edificio resistió inmutable. Los equilibrios del gótico se habían repuesto. Pero el carácter progresista de Juan de Madrazo le hizo tomar partido en los graves momentos sociales que sacudían entonces España (el Sexenio Democrático o Revolucionario), enfrentándose con el cabildo, el obispo y la sociedad conservadora leonesa, quienes le acusaron de masón, protestante y anticatólico, declarándose él mismo como deísta o ateo. Cuando estaba edificando el hastial sur y después de haber sostenido toda la catedral con su asombroso encimbrado de madera que provocó visitas de técnicos de toda Europa, fue destituido en el año 1879, falleciendo de los disgustos pocos meses después.

A Juan de Madrazo le sucedió en el cargo Demetrio de los Ríos en el año 1880. Purista, como el anterior, continuó dando a la catedral el aspecto gótico primitivo, según su pensamiento racionalista, y desmontó el hastial occidental plateresco, que había sido hecho por Juan López de Rojas y Juan de Badajoz el Mozo en el siglo , sustituyéndolo por un diseño neogótico análogo al recién construido en la fachada sur. También terminaría de reconstruir las bóvedas del crucero y de la nave central, además de introducir algunos nuevos diseños, perfiles y motivos ornamentales neogóticos en diversas partes del edificio.

A su muerte fue nombrado arquitecto de la catedral Juan Bautista Lázaro, que concluyó los trabajos de restauración arquitectónica en la mayor parte del edificio. En el año 1895 emprendió la ardua tarea de recomponer las vidrieras. Estas llevaban varios años desmontadas y almacenadas, con grave deterioro. Fue ayudado por su colaborador, Juan Crisóstomo Torbado. Se reabrió un taller de vidrieras al estilo medieval para su restauración y composición de otras nuevas. Se decidió también aislar la catedral de su entorno urbano más próximo para resaltar su monumentalidad, lo que acarreó la desaparición de varias dependencias anexas, y su conexión con el Palacio Episcopal a través de la Puerta del Obispo.

Finalmente, concluida la restauración, en el año 1901 la catedral fue reabierta al culto. Ya no era un edificio en peligro, sino que había recobrado el esplendor del gótico, principalmente con la sustitución de los hastiales oeste y sur y la eliminación de la cúpula barroquizante. En aras de mantener el delicado equilibrio, nunca se permitió elevar flecha alguna sobre el crucero, pese a que hubo proyecto al respecto. Hoy la catedral de León es el monumento gótico más armónico de España.

La gran restauración decimonónica verificó la recuperación de la estabilidad de un edificio que había arrastrado graves problemas en su estructura a lo largo de los varios siglos de su existencia. Puede afirmarse que esta restauración fue una de las más complejas y arriesgadas realizadas en Europa en el siglo Los minuciosos cálculos sobre la estabilidad de bóvedas, los portentosos sistemas de carpintería armados a gran altura y los sistemas de cantería puestos en práctica para la reparación y reconstrucción de bóvedas de la catedral de León sirvieron de modelo para la restauración posterior de otras grandes catedrales españolas, como la de Sevilla o Burgos. Pero también fueron referencia imprescindible en toda Europa para restaurar edificios que décadas más tarde resultarían dañados seriamente en su esqueleto estructural como consecuencia de catástrofes bélicas. El elevado mérito de estos trabajos fue reconocido en su momento, pues en el año 1881, Juan de Madrazo recibió a título póstumo la Medalla de Oro en la Exposición Nacional de Bellas Artes por sus proyectos de restauración de la catedral de León.

Los trabajos de restauración continuaron levemente en el siglo , sobre todo en las primeras décadas. En el año 1911 Manuel Cárdenas destruiría lamentablemente la Puerta del Obispo, edificio civil que unía la catedral con el Palacio Episcopal. En el año1930, Juan Crisóstomo Torbado terminaría la verja exterior iniciada en 1794, cerrando todo el atrio. Este arquitecto acometería después la restauración del claustro.

En el año 1963 el arquitecto Luis Menéndez Pidal reharía la rosa calada del remate triangular del hastial sur, imitando el del norte.

El 27 de mayo del año 1966, un incendio motivado por la caída de una chispa de un rayo arrasó toda la techumbre de las naves altas, aunque por suerte las consecuencias no fueron graves gracias a la intervención del maestro Andrés Seoane la techumbre se pudo reparar.

En las últimas décadas se ha estado trabajando con gran intensidad en el refuerzo de las estructuras y en el tratamiento y limpieza de la piedra con las más novedosas técnicas, en un esfuerzo por conservar para la Humanidad esta maravilla arquitectónica.

Desde el año 2009 también se están llevando a cabo la restauración y consolidación de las vidrieras, usando las más modernas técnicas. Se usan vidrios de protección para cerrar los vanos y un acristalamiento isotérmico para proteger y conservar la vidriera de los efectos atmosféricos, así como mallas metálicas protectoras exteriores. La financiación ha sido llevada a cabo por el ministerio de Cultura del Gobierno de España y por la consejería de Cultura de la Junta de Castilla y León, mediante el proyecto cultural «Catedral de León, el sueño de la luz». Este proyecto permitió la visita guiada para conocer la restauración de las vidrieras. No se prevé que los trabajos de restauración concluyan antes del año 2020, siendo los últimos dos años de la misma los que más oculten la fachada principal.

En la arquitectura gótica se generaliza el uso de los arcos apuntados (o arcos ojivales) y la bóveda de crucería concentrando así los empujes en puntos determinados y no en todo el muro, que permiten hacer catedrales más esbeltas (por una parte, el arco puede alargarse sin ampliar su ancho como ocurría en el románico y reduce los empujes haciendo cubiertas más ligeras, lo que permite abrir los muros). Desaparece la tribuna románica y los empujes laterales que esta resolvía se envían a los arbotantes, arcos que transmiten el empuje de la cubierta a los contrafuertes exteriores, que solían estar rematados con pináculos. Las grandes vidrieras son una muestra del interés del gótico por comunicarse con el pueblo. Así mismo, la sensación de verticalidad se corresponde a la idea del Jerusalén celeste, en comparación contra la sensación de acogimiento y seguridad a los fieles creada en el románico. Este tipo de construcciones solían tener un número impar de naves (3 ó 5) sustentadas por una bóveda de crucería cuatripartita, sexpartita, de terceletes, de abanico o estrellada.
La fachada principal se estructuraba generalmente en tres vanos abocinados, constituidos por arquivoltas y jambas y enmarcados en un gablete, una galería de reyes del Antiguo Testamento, un gran rosetón (situado en la nave central), un andito (espacio mediante el cual se accede a la fachada para realizar posibles reformas) y por dos torres de características diferentes (rematadas o no con un pináculo en forma de flecha).

La planta es casi una réplica de la catedral de Reims aunque en formato algo menor. Tiene unas dimensiones de 90 m de longitud, 30 m de alto y 29 m de ancho. Dividida en tres naves, de la entrada al transepto, y cinco naves del transepto al altar mayor. La catedral presenta "macrocefalia", es decir una cabecera de mayor tamaño que lo común (el ancho del transepto en este caso) y que le resta algo de profundidad y perspectiva pero a cambio le brinda mayor espacio para los fieles (debido a estar en el Camino de Santiago su afluencia era mayor). Las naves de la catedral de León se cubren con bóveda de crucería cuatripartita en tramos rectangulares. El crucero lo hace con una bóveda cuatripartita, que sustituyó a la cúpula barroca del siglo en las obra acometidas a finales del siglo con el fin de guardar coherencia con el resto de la construcción. En sus muros presenta 125 ventanales, con 1.800 m² de vidrieras policromadas de origen medieval, siendo consideradas de las mejores del mundo en su género. De ellas, destacan el gran rosetón central situado en el pórtico central, entre las dos torres de aguja, así como las de la Capilla Mayor, el transepto norte y la Capilla de Santiago. 

La fachada occidental es la principal de la catedral, y por la que normalmente se accede al templo. Consta de un triple pórtico ojival similar al de la catedral de Reims. En las jambas, arquivoltas, tímpanos y parteluces de las portadas se desarrolla un trabajo escultórico de destacado papel en el gótico español, actuando de filtro de la influencia francesa. Encima del pórtico, se sitúa el gran rosetón central, con vidrieras de finales del siglo . El hastial triangular neogótico actual fue construido por Demetrio de los Ríos a finales del siglo durante su restauración, siendo desmontado el anterior plateresco del siglo .

La fachada se encuentra flanqueada por dos torres góticas de 65 y 68 metros respectivamente. El hecho que las torres sean diferentes, en forma y altura, responde a momentos diferentes de la construcción y es bastante típico del gótico. La torre norte o de las campanas fue iniciada en el siglo y terminada en el siglo , siendo más sobria y maciza, terminada en una aguja cerrada. La torre sur o del reloj se inició también en el siglo , pero no fue concluida hasta finales del siglo , momento en el que el maestro Justín le dio remate. Su estilo es gótico flamígero, con una aguja calada, estando menos acorde con el resto del edificio que su compañera. Las torres de la catedral de León presentan la particularidad de estar adosadas a las naves laterales, en lugar de surgir de ellas. Esto permite la curiosa vista de los arbotantes de las naves laterales desde la fachada occidental.

Realizadas en la segunda mitad del siglo , las riquezas de las portadas de la catedral de León la convierten en el máximo exponente de la escultura gótica española. El triple pórtico occidental se encuentra dedicado en los laterales a San Francisco y a San Juan Bautista, mientras la portada principal representa el Juicio Final. La influencia francesa se evidencia en esta portada principal, realizada en torno al año 1270. De esta, destacan los personajes de las jambas y Nuestra Señora La Blanca en el parteluz, hoy sustituida por una copia ejecutada por Andrés Seoane. Las figuras presentan el naturalismo propio del gótico que se impone sobre el simbolismo y hieratismo románicos. El trabajo de los pliegues en los ropajes, la expresión e individualización de los rostros y la sensación de movimiento son las principales características. El modelo de Nuestra Señora La Blanca o Virgen Blanca destaca por su humanidad, conseguida en gran parte, por la sonrisa que recuerda al Ángel de la Anunciación de la catedral de Reims.

La portada izquierda, o "de San Juan", en el tímpano se muestra el Ciclo de la Natividad de Jesús: Visitación, Nacimiento, Adoración de los Pastores, Herodes, Epifanía y Matanza de los Inocentes, con un gran sentido de la narratividad. En las Arquivoltas aparece una alusión del árbol de Jesé, en relación con la genealogía de Cristo e historias relacionadas con la vida de San Juan Bautista, dando el nombre a la puerta.Entre esta portada y la siguiente, aparece el "locus apellationis", columna ante la que se administraba justicia en el Reino de León, según normas que se remontan al Fuero Juzgo y al leonés del año 1020, y que provoca la asimilación de Alfonso X, patrocinador del templo, con la figura del rey Salomón, representado al fondo sobre la columna.

La portada central, "de la Virgen Blanca" o del Juicio Final, está presidida por el Cristo Juez que muestra sus estigmas, mientras los ángeles portan los instrumentos de su martirio y la Virgen y San Juan se arrodillan como principales intercesores. Bajo el y en las arquivoltas se desarrolla el juicio en el que san Miguel pesa las almas (psicostasis) y separa a los bienaventurados que van al paraíso (incluidos Alfonso X o San Francisco, que aparecen con varios instrumentos de música), de los condenados que sufren los tormentos del infierno, siendo devorados por demonios o siendo introducidos en calderas hirviendo. En las arquivoltas aparecen escenas de la resurrección de los santos. El parteluz de ambas puertas es presidido por una reproducción de la "Virgen Blanca" con el Niño en las manos (la original está guardada en el interior del templo, en la capilla del mismo nombre), siendo la escultura más representativa de la catedral y una de las de más calidad del gótico español. Junto a las puertas, aparecen esculturas de santos, evangelistas y protagonistas del Antiguo Testamento. Destaca la escultura de Santiago (reconocida por la concha de su gorro suyo pedestal está gastado, según la tradición, por las manos de los peregrinos a su paso por León de camino hacia Santiago de Compostela.

La portada de la derecha, o "de san Francisco", está dedicada a la Virgen, relatando el tímpano la muerte y coronación de María y portando las arquivoltas varios concejos de ángeles, y en el exterior, las cinco vírgenes prudentes frente a las cinco necias. Las jambas albergan figuras de varios profetas de distinta cronología.

Se extiende hacia Puerta Obispo, orientada al sur del templo. Fue la zona que más sufrió los problemas constructivos de la catedral, llegando a ser reconstruida en varias ocasiones. En el siglo , el triforio fue cegado, y se colocó un hastial barroco en forma de espadaña, y el siglo , tras el terremoto de Lisboa, su rosetón fue desmontado y sustituido por una ventana doble barroca. Afortunadamente, durante las grandes restauraciones de finales del siglo , Matías Laviña proyectó la fachada actual, construyendo un nuevo rosetón, triforio y hastial neogóticos, imitando la fachada norte, que nunca fue retocada. A la derecha del gran rosetón encontramos la torrecilla denominada "silla de la reina", realizada en el siglo por el maestro Justín, y que servía para recoger los empujes de los arbotantes hacia la cabecera.
La portada sur también cuenta con tres pórticos, al estilo de las catedrales góticas francesas, realizados entre 1265 y 1275.

La portada izquierda es la denominada «de la muerte», por la configuración que acompaña al tamizado heráldico de Castilla y León. No posee decoración en el tímpano, solo en las arquivoltas y jambas. El nombre de Puerta de la Muerte procede de una figura de un esqueleto con alas, colocado en época posterior en una de las ménsulas.

La central, llamada "del sarmental" es muy similar a la puerta del sarmental de la catedral de Burgos, representando a Cristo sedente como Pantocrátor mostrando el Libro de la Ley, y rodeado del tetramorfos: El toro (Lucas), el águila (san Juan), el león (San Marcos) y el hombre (San Mateo). A sus lados aparecen evangelistas sentados, escribiendo sobre pupitres. Ángeles y ancianos del Apocalipsis con instrumentos musicales adornan las arquivoltas. El parteluz está ocupado por una estatua de San Froilán.

La portada derecha, llamada "de san Froilán", muestra en su tímpano escenas de la vida del santo, su muerte y el traslado de sus reliquias a la catedral leonesa. Las arquivoltas están decoradas con ángeles. Esta portada tuvo gran importancia en la Edad Media pues por ella entraba el obispo, al estar situado enfrente el Palacio Episcopal. Además, también era empleada por todos los peregrinos que entraban por una puerta de la muralla cerca de la catedral para visitar los restos del santo. Actualmente se encuentra cegada.

La fachada norte de la catedral se encuentra poco visible debido a que se encuentra justo encima del claustro, lo que impide su visibilidad total a no ser desde el mismo o desde las alturas de la ciudad. Consta de un hastial con triforio presidido por un gran rosetón con vidrieras de finales del siglo . El remate del hastial triangular y su rosa calada es del siglo , y fue usado como modelo para la reconstrucción de los hastiales de los lados sur y oeste en las restauraciones de finales del siglo . A la izquierda de la fachada está otra torrecilla, "la limona", realizada en el siglo para recoger los empujes de los arbotantes hacia la cabecera. Esta fachada no ha sido retocada por las grandes restauraciones, permaneciendo inalterada desde su construcción. 

En su origen constaba, como en los casos anteriores, de una portada triple, pero no da al exterior del templo, sino que está cubierta por la estancia de acceso al claustro. 

La portada izquierda fue cegada y desapareció con la construcción del claustro. 

La portada central, llamada "de la Virgen del dado" se llevó a cabo en la última década del siglo y aún conserva la mayoría de la policromía, del siglo . En el tímpano, un Cristo bendice desde la almendra mística que sujetan los ángeles y flanquean los Evangelistas. En el parteluz aparece la escultura de la "Virgen del dado", llamada así por la leyenda sobre un soldado que arrojó sus dados a causa de una adversidad en el juego, hacia el rostro de Niño, que sangró milagrosamente (frente a ella se representa el milagro en una vidriera). En las jambas aparecen Pablo, Pedro, Santiago, Mateo y la Anunciación a María. 

La portada derecha se usa como entrada y salida a la estancia que da salida al claustro. El tímpano no es escultórico, sino que posee una pintura gótica de la Virgen con el Niño. En las arquivoltas hay figuras vegetales. También conserva la policromía.

La catedral de León cuenta con tres naves y un transepto. La nave central mide 90 metros de largo y 30 de altura, mientras que las dos naves laterales miden 15 metros de altura, y se unen a través de la girola. Desde el exterior, son perfectamente visibles todos los contrafuertes, los arbotantes y los pináculos a lo largo de las naves, que sirven para desviar los empujes del edificio al exterior y poder perforar las naves con grandes ventanales. También existen gran cantidad de gárgolas, con forma de animales o monstruos míticos, que servía para escupir al exterior el agua que caía del tejado a través de los arbotantes.
De cara al exterior lo más importante es el reemplazo de los muros de piedra por los vanos con vidrieras. Las dos naves laterales están perforadas con vanos desde la fachada occidental hasta el transepto, constituyendo los ventanales de la parte baja del edificio. La nave principal está perforada con grandes ventanales que recorren todo el edificio, incluido el transepto y el ábside, dando lugar al claristorio. Justo debajo está el triforio calado que también recorre el edificio. Esta aligeración de los muros es lo que da lugar a la "desmaterialización" del arte gótico. Algunas volutas y adornos que se pueden observar en la piedra entre los ventanales de la nave principal son obra de las restauraciones de finales del siglo .

El ábside es la parte más antigua de la catedral, debido a que las iglesias comenzaban a construirse por la cabecera para poder realizarse culto antes de la conclusión definitiva del templo. La vista de la catedral desde el ábside es una de las más impresionantes, porque es en esta zona donde los arbotantes alcanzan su mayor espectacularidad. Los arbotantes desvían los empujes hacia el exterior permitiendo aligerar los muros, que pueden ser perforados por grandes ventanales. Durante gran parte del año, los pináculos del ábside de la catedral leonesa son usados como posadero y nidos por numerosas cigüeñas. Algunas de ellas pasan el invierno en León.

En cuanto a su alzado interior, la catedral sigue con el modelo francés en tres pisos o registros. El primero es el de los arcos formeros apuntados con pilares fasciculares, cuyos baquetones se insertan en los nervios de las bóvedas creando un eje que marca la verticalidad del interior. El segundo piso cuenta con un triforio y el tercero es el claristorio, o conjunto de vidrieras.

En León, también según la costumbre extranjera, estaba situado originalmente en la cabecera, delante del altar mayor. En el año 1746 finalmente fue trasladado al centro de la nave mayor. El arquitecto Cárdenas en el año 1915 abriría el gran arco central con unos enormes cristales que recuperan, en parte, la vieja perspectiva de la nave central y la vista del altar mayor.

La sillería actual, quizá sustituta de una anterior, fue realizada entre los años 1461 y 1481 en estilo gótico en madera de nogal. Se trata de una de las sillerías más antiguas de España. A un primer maestre Enrique, carpintero que planearía el trabajo, sucedieron Juan de Malinas y el maestro Copín, imagineros de las principales tallas (testeros y respaldos). Un riquísimo elenco de motivos congrega a los habituales personajes del Antiguo Testamento y a los santos en los lugares más visibles, contraponiéndolos a varias figuras profanas de tono burlesco y costumbrista, a la moda nórdica. Los autores no se recataron en utilizar figuras de clérigos para satirizar vicios, en mordaces escenas que llegan a veces a la obscenidad.

La tradición organística en la seo leonesa se remonta a la Edad Media, como es habitual en las catedrales europeas. Heredero de esta larga historia es el actual órgano monumental, que se dispone en las cuatro tribunas que se hallan sobre los costados del coro. Este instrumento fue construido por la empresa de organería Johannes Klais de Bonn (Alemania) e inaugurado el 21 de septiembre de 2013. La concepción y disposición sonora del órgano son obra del compositor y organista francés Jean Guillou, mientras que el diseño de las fachadas de tubos se debe al artista leonés Paco Chamorro Pascual. Los tubos correspondientes a los teclados manuales I y II se sitúan en las dos cajas enfrentadas del este del coro, mientras que entre las dos del lado oeste se encuentra repartida la división expresiva (que se toca desde el III Teclado) y los tubos del IV y V Teclado. El órgano tiene un total de 64 registros (incluidos once registros por transmisión) repartidos en cinco teclados manuales y uno de pedal. Las transmisiones de notas y registros son eléctricas.

Su disposición sonora de registros es la siguiente:

Primeramente un antecoro que miraba a los fieles desde el altar clausurado e inaccesible, fue trazado por Juan de Badajoz el Mozo en estilo plateresco. La obra se remataría entre los años 1560 y 1590 en plena contrarreforma, elevando su envergadura con imágenes laterales y un gran arco triunfal. Esteban Jordán labró los cuatro relieves de alabastro con escenas de la Anunciación, Nacimiento y Adoración.

El altar mayor o capilla mayor de la catedral está actualmente ocupado por un retablo neogótico montado por Juan Bautista Lázaro con cinco tablas procedentes del retablo perdido realizado por Nicolás Francés a mediados del siglo , y otras de procedencia diversa (Palanquinos, etc).
Representan la vida de san Froilán, el traslado del cuerpo de Santiago y la Presentación de la Virgen, en una mezcla de estilo gótico internacional y gótico flamenco. Destacan las tablas laterales, en especial el Descendimiento de la izquierda.

En el altar, el arca de san Froilán, obra maestra del platero Enrique de Arfe (1519-1522) con algunas modificaciones barrocas. El altar está cerrado por una verja de estilo plateresco, obra de Juan de Badajoz el Mozo.

Alrededor de la parte trasera del altar mayor se encuentra la girola o deambulatorio. En el caso de la catedral de León, como muchas otras del Camino de Santiago, la girola tiene un gran espacio, para permitir el paso de abundantes peregrinos y evitar aglomeraciones. Consta de nueve capillas de forma hexagonal, dispuestas alrededor del deambulatorio, y delimitadas por rejas. Cada una de estas capillas está presidida por un ventanal doble con vidrieras. 

Dispuesta en la nave sur, justo después del crucero, e inmediatamente antes de llegar a la girola. Alberga el sepulcro del obispo Rodrigo Álvarez y posee un pequeño altar.
También anterior a la girola, está centrada en torno a un retablo renacentista con calvario, obra de Juan de Valmaseda (1524).
Se encuentra vacía, solamente permite el paso al edificio de la sacristía, anexo a la catedral, y obra plateresca de 
Juan de Badajoz el Mozo.
Conserva pinturas murales del siglo .

Guarda la imagen original de la Virgen Blanca desde el año 1954. A la izquierda de la escultura se sitúa el sepulcro de la condesa Sancha Muñiz, y a la derecha el de Alfonso de Valencia, hijo del infante Juan de Castilla el de Tarifa y nieto de Alfonso X, ambas del siglo . Frente a esta capilla, y justamente detrás del altar mayor, se sitúa el sepulcro de Ordoño II obra del siglo reformada con un tímpano en el siglo . De este momento son también los dos murales que miran hacia la girola, con temas de la piedad y Ecce-homo, obra de Nicolás Francés, que también pintó un mural del Juicio Final en el muro occidental, obra que fue picada a principios de siglo por sus desnudos. 

Está presidida por una virgen gótica del siglo y guarda tablas hispano-flamencas. Dentro se encuentra el sepulcro de san Alvito.

Consta de un pequeño altar y de una maqueta de un nacimiento, de estilo Gótico flamenco del siglo . Dicho retablo fue elegido para ilustrar los décimos de lotería del Sorteo Extraordinario de Navidad del año 2016.

Ya se encuentra fuera de la girola, en la nave norte. Por aquí se accede a la antigua Librería o Capilla de Santiago (o la de la Virgen del Camino) y a la de san Andrés, por el magnífico arco, obras respectivas de Juan de Badajoz padre e hijo. La obra se inició en los años finales del siglo y terminó en el año 1504. Destacan los cuatro ventanales con vidrieras.

Situada cerca del brazo norte del crucero, posee una talla de la escuela de Gregorio Fernández. En el muro, una pintura mural de Nicolás Francés del año 1459, que representa el "Martirio de san Sebastián", recientemente restaurada.

La catedral de León fue diseñada sin claustro, pero finalmente se levantó entre finales del siglo y principios del siglo . Esta construcción cubrió la entrada norte que, desde entonces, quedó resguardada. Este claustro tiene una planta cuadrada de 30 metros de lado, posee seis intercolumnios por lado y 24 pilares en total, respondiendo a las proporciones establecidas en los claustros cistercienses. A su alrededor se agrupan diversas dependencias, entre ellas, el actual Museo Catedralicio. 

Los arcos apuntados y capiteles del muro interior presentan escenas bíblicas y de la vida cotidiana, mostrando de nuevo el diálogo de lo divino y lo humano, típico del gótico. Los murales entre las arcadas del claustro fueron pintados con los episodios de la vida de Cristo por Nicolás Francés en los años 60 del siglo , aunque algunos de ellos son obra posterior de Lorenzo de Ávila y otros.

A comienzos del siglo , Juan de Badajoz el Mozo rehízo las bóvedas del claustro. Aprovechó los lienzos y los arcos formeros y montó 28 bóvedas de crucería complicadas y decoradas. Filacterias y medallones presentan un complejo programa iconográfico, vinculado con la Virgen de Regla. Bajo las bóvedas puede observarse una completísima colección de sepulcros que revelan las etapas de la actividad escultórica catedralicia, pero siendo en su mayoría obras tanto del siglo como del siglo .

En el centro del patio y por diversos lugares del claustro se conservan restos de los hastiales oeste y sur, que fueron desmontados durante las restauraciones del siglo por los arquitectos "purificadores" de la catedral.

En el claustro y en la iglesia propiamente dicha, se pueden encontrar multitud de enterramientos, algunos procedentes de la vieja catedral románica, y por tanto, anteriores al actual edificio.

Uno de los más valiosos es el de don Rodrigo, obispo leonés muerto en el año 1232. Está situado en el lado sur, en la capilla del Carmen, cerca de la girola. El yacente está rodeado por clérigos que ofician el funeral y un grupo de gente que llora. En el tímpano está representado un calvario. En la parte frontal del sarcófago, los criados del finado reparten pan a un grupo de pobres. Este modelo de sepulcro tuvo un éxito extraordinario, pues de él se hicieron dos copias en la misma catedral y se extendió fuera de León.

Pese a ser una de las imitaciones del de don Rodrigo, la obra maestra de la escultura funeraria es el sepulcro de don Martín "el Zamorano", muerto en el año 1242, llamado así por haber sido obispo de Zamora. Está situado en el lado norte del crucero, junto a la entrada al claustro. Está atribuido al Maestro de la Virgen Blanca, de una calidad excepcional, alcanzado un nivel de expresión y detalle de los rostros difícilmente superable. La iconografía general es la misma que el de don Rodrigo, aunque desgraciadamente el calvario está perdido por la corrosión de la piedra, conservándose solo la figura de san Juan.

El sepulcro del brazo sur, simétrico al anterior, también es del maestro de la Virgen Blanca. No se sabe con seguridad quien está enterrado allí, tal vez pertenezca a Munio Álvarez o a su sucesor, Martín Fernández, obispo iniciador de la catedral. En favor de esta hipótesis está la imagen de san Martín en uno de los tres tímpanos. Los otros están ocupados por una crucifixión y una flagelación. El deterioro de este sepulcro es grande, sobre todo en la parte baja, motivado por las corrientes de agua en los cimientos de esa parte del templo, debido a las calderas de las antiguas termas romanas en el subsuelo.

Ordoño II, rey de León al que se debe la construcción de la primera catedral leonesa en el siglo , tiene una abigarrada sepultura en el trasaltar, justo detrás del altar mayor y frente a la capilla de la Virgen Blanca. Aunque la mayor parte de la obra es del siglo , el yacente es del siglo , contemporáneo a la catedral. Tiene aspecto sereno y una postura equivocada pues alguno de sus rasgos parece más propio de una figura erguida que yacente.

En la capilla de la Virgen Blanca, se encuentran otros dos sepulcros, a ambos lados de la escultura de la Virgen. Uno de ellos contiene los restos del infante Alfonso de Valencia, hijo del infante Juan de Castilla el de Tarifa y nieto de Alfonso X. En el otro, se encuentra la condesa Sancha Muñiz, ambos del siglo . En los dos, la forma es la de un sarcófago con la escultura del yacente encima, ambos empotrados en la pared de la capilla.

También destacan los variados enterramientos de obispos y personajes leoneses destacados del claustro, de entre los siglos y .

En la catedral de León, la pintura, quizás desplazada por la grandiosidad de la vidriera, no aparece hasta el siglo . En esta centuria surgen diversos maestros que configuran un variado muestrario.

La más antigua es la del trasaltar que representa una piedad con influencia italiana. Muy cerca de ella hay un mutilado ecce-homo, pintura bella a excepción del Cristo, pintado para sustituir al antiguo por la puerta que se abrió en su lugar.

En el brazo norte del crucero existe una tabla pintada al temple sobre el martirio de san Erasmo. Los personajes ofrecen aspecto apacible pese a la brutalidad de la escena.

De todos los maestros que trabajaron en León durante el siglo , Nicolás Francés destaca principalmente, hasta el punto de caracterizar la pintura catedralicia. Debió de nacer en Francia, (de ahí su nombre) pero vino joven a León, y aquí trabajó hasta su muerte en 1468. 

Una de sus obras más importantes fue el retablo del altar mayor, lugar donde permaneció trescientos años, hasta que en el año 1741 fue desarmado para ser sustituido por otro barroco, de descomunal tamaño, diseñado por Narciso Tomé, autor del transparente de la catedral de Toledo, y elaborado por su primo Gavilán Tomé y el suegro de éste, José de Sierra. A finales del siglo , los restauradores, viendo que la obra barroca atentaba tanto contra la estética del templo como contra los elementos estructurales de la cabecera, lo desmontaron y enviaron a la iglesia de los Padres capuchinos. En su lugar, fue colocado en el altar mayor uno compuesto de las tablas del retablo de Nicolás Francés que aún se conservaban, y se añadieron otras suyas de Palanquinos y de la iglesia de Nuestra Señora del Mercado de León, sobre todo las seis pequeñas pinturas con escenas de la vida de la Virgen. De la iglesia de Palanquinos son las dos tablas de la parte central con los seis apóstoles: Andrés, Juan y Pablo por un lado y Pedro, Santiago y Tomás por otro. Las conservadas están dedicadas a la Virgen, a San Froilán y a Santiago. 

Otra obra fundamental suya son las pinturas murales que decoran gran parte de las paredes del Claustro. Fueron realizados hacia la década 1460, y de los veintinueve existentes, más de veinte pertenecen a la mano de Nicolás Francés. Representan escenas de la vida de la Virgen y de Cristo, con dibujo ágil y una gran expresividad. La exposición a los agentes meteorológicos en el Claustro había deteriorado su policromía, por lo que tuvieron que ser restauradas a finales del siglo por Juan Crisóstomo Torbado, recuperando su pintura original. También destaca la pintura mural del martirio de san Sebastián, recientemente restaurada, en la capilla de Santa Teresa.

La catedral de León es conocida sobre todo por su conjunto de vidrieras, quizás el más importante del mundo junto con el de la catedral de Chartres. Conservadas la mayoría de las originales, hecho extraño en catedrales de esta época, fueron construidas entre los siglos y . La técnica de la vidriera tiene su origen, según se cree, en la cultura musulmana. De ella fue tomada por el arte cristiano, que los utilizó desde el siglo para alcanzar su auge dos siglos más tarde. En el siglo entró en una total decadencia, y más tarde, con la pérdida de interés por lo medieval, los vitrales fueron eliminados de muchos templos. Esto, junto con la fragilidad propia del vidrio, es la causa de que se conserven tan pocas colecciones

En su época de esplendor no se tenían como meros elementos decorativos, sino que eran una parte fundamental de edificio. La técnica del arbotante permitía prácticamente eliminar los muros como elemento de sostén, por lo que se podían perforar para abrir grandes ventanales, que recubiertos con vidrieras, daban al templo una mágica apariencia. En este sentido, la catedral de León fue uno de los edificios que más metros cuadrados dedicó a la creación de vanos para vidrieras en proporción a su tamaño. En metros cuadrados se distribuyen de la siguiente manera: 464 metros en su parte baja, 282 en el triforio y 1.018 en la zona superior, lo que hace un total de al menos 1.764 metros cuadrados de superficie, según las mediciones de Demetrio de los Ríos. El conjunto cuenta con 134 ventanales y 3 grandes rosetones. Esta enorme superficie cobra mayor importancia debido a las dimensiones contenidas de la catedral. 

No obstante, parece ser que entre los siglos y fueron tapiados parte de los ventanales inferiores (de las que solo se conservaron las partes superiores y las rosas originales) y del triforio para dar mayor consistencia al edificio por sus problemas constructivos, si bien estas zonas acristaladas fueron recuperadas en las restauraciones de finales del siglo por Demetrio de los Ríos y Juan Bautista Lázaro con la creación de otras nuevas, usando la técnica constructiva medieval. 
Pese a todo, se conserva el programa iconográfico original, el cual estaba pensado de forma tripartita, en función del pensamiento de la sociedad medieval. Los ventanales altos o claristorio, constan de escenas bíblicas, representando el cielo. Los ventanales medios o triforio, constan de escudos nobiliarios y eclesiásticos, representando a la nobleza. Los ventanales inferiores de las naves laterales, constan de representaciones vegetales, representando la tierra, y de tareas mundanas, representado a los hombres "pecheros".

En las ventanas altas o claristorio, hay distinta temática en función de si es el lado norte o el sur. Los ventanales del lado norte, que reciben menos luz, tienen tonos más fríos y su temática del Antiguo Testamento destaca que aún no han conocido la luz Cristo. Los del lado sur, más luminosos, representan el Nuevo Testamento y presentan colores más cálidos. 

El triforio de la catedral de León también está perforado con ventanales exteriores que incluyen vidrieras, tapiadas durante la Edad Moderna y recuperadas durante las restauraciones del siglo , momento en el que se crearon nuevas vidrieras que respetaban el conjunto iconográfico original, en este caso, todas las vidrieras del triforio son representaciones de escudos reales y nobiliarios, además de ciudades españolas.

Se puede clasificar el estilo de las vidrieras en función de su momento constructivo. Habría tres épocas principales:


Se cree que una vidriera, llamada "la cacería", no fue hecha para el edificio y procede de un palacio real. Puede verse en el muro norte de la nave central. Es el quinto gran ventanal superior comenzando a contar por los pies de la iglesia. Su nombre alude a los distintos jinetes y hombres armados dispuestos para la caza que en ella puede verse. Además, contiene otras escenas que representan algunas ciencias, entre ellas la de la alquimia, ciencia medieval con la que se creía relacionada con los constructores de catedrales.

Junto con la cacería, otras vidrieras del siglo , más o menos restauradas, se diseminan por los ventanales, destacando las rosas situadas en los ventanales de las capillas del ábside. Destaca también aquí, por su originalidad, la vidriera de Simón el Mago, de temática profana. También destacan las apariciones de reyes en los ventanales, sobre todo Alfonso X el Sabio, en cuyo reinado se inicia la catedral.

De espectacular efecto por sus brillantes tonos azules son los ventanales más altos de la nave central en el lado norte, sobre todo encima del coro, realizadas a principios del siglo . 
El gran rosetón occidental se realizó a finales del siglo , y fue bastante restaurado a finales del siglo . En su centro, aparece la Virgen con el Niño, rodeado por doce ángeles a su vez sucedidos por motivos ornamentales. 

Frente a este gran rosetón occidental, al otro lado de la iglesia, en el centro del ábside, se alza en "árbol de Jesé", realizado en la misma época, representando el árbol genealógico de Cristo. 

El rosetón del lado norte es del mismo período, aunque con añadidos del siglo . En el centro aparece Cristo, rodeado por doce formas de rayos de luz, a su vez rodeadas por doce músicos. 

El rosetón del sur fue realizado en su totalidad a finales del siglo , debido a la pérdida del anterior con motivo de los cambios de fachada. Es una copia de los motivos temáticos del lado norte, esta vez con la Virgen presidiendo el centro rodeado por los rayos de luz y los músicos.

A partir del siglo , el arte de la vidriera empezó a decaer, y se transformó en poco más que pintura sobre vidrio. Las grandes restauraciones del siglo , dejando de lado cierta falta de rigor, tuvieron el valor de resucitar temporalmente la técnica medieval de su elaboración, hasta entonces caída en el olvido.

Es importante también el efecto día en las vidrieras, teniendo en cuenta que en sol sale por el este (vidrieras del ábside), ilumina las naves (durante el día) y se pone por el oeste (rosetón occidental), por tanto la incidencia de la luz en los ventanales va a variar según en momento del día. El brillo de las vidrieras puede cambiar también en función de la meteorología, entrando más cantidad de luz en los días más luminosos y menos en los más oscuros o nublados.

Si las vidrieras son objetos signos de ser admirados como unidades aisladas, su efecto en conjunto con la arquitectura ojival hace de esta catedral una de las construcciones más espirituales jamás diseñada.

La actual catedral gótica, al haber sido construida en el mismo emplazamiento que las antiguas termas romanas, iglesias primitivas e incluso una catedral románica, posee en el subsuelo importantes restos arqueológicos de las anteriores construcciones. En el interior de la llamada cripta arqueológica de Puerta Obispo, situada justo frente a la fachada sur de la catedral, se albergan importantes restos del campamento de la Legio VII Gemina, que fueron hallados en las excavaciones arqueológicas del año 1996 durante la peatonalización del entorno de la catedral. 
La "porta principalis sinistra" era una puerta romana de acceso al campamento de la Legio VII y fue construida con grandes sillares de piedra caliza y flanqueada por ambos extremos con dos torres rectangulares, de 12,80 metros de longitud por 5 metros de anchura, de las cuales solo los restos de la situada más al norte se muestra en la cripta, mientras que la contraria subyace bajo la calle. Esta entrada doble permitía el acceso al campamento desde el este. El tránsito se realizada mediante sendos pasajes en cuyos extremos se abrían arcos de medio punto. Los exteriores se cerraban mediante puertas de madera de doble hoja. Una construcción similar, la "porta principalis dextra", existió al extremo contrario del campamento, donde hoy se ubica el Palacio de los Guzmanes. Ambas se unirían por medio de la "via principalis", cuyo trazado hereda la actual calle Ancha. A finales del siglo o comienzos del , se realizan reformas en la puerta, como la clausura de uno de los vanos. Durante toda la Edad Media, la puerta siguió funcionando como acceso a la ciudad, pese a la desaparición de las termas y la construcción de diferentes iglesias en la zona.

A mediados del siglo , con la construcción de la catedral gótica, se erige una puerta de estilo gótico civil levantada sobre el trazado de la muralla romana bajoimperial. Se trataba de una “puerta-puente”, con un macizo de sillería en su tramo inferior y dos crujías en su parte superior. Presentaba una vistosa galería jalonada de ventanillas geminadas, ya que su función era la de permitir el tránsito desde el palacio episcopal a la catedral. A finales del siglo esta puerta ya se había clausurado, lo que motivó la apertura del llamado “paso de carruajes” anexo al palacio episcopal y en uso hasta la demolición en los años 1910-1911 de todas las construcciones que ocupaban la zona, debido a la búsqueda de un aislacionismo monumental de la catedral de otras construcciones anexas.

El primer edificio que aparecía en el interior del campamento, al cruzar las puertas y a la derecha de la "vía pricipalis" eran las grandes termas interiores de la Legio VII Gemina. De sus grandes proporciones da idea el hecho de que se extendían sobre buena parte del solar que ocupa actualmente la catedral e incluso buena parte de la actual plaza de Regla. Según el arqueólogo García Marcos, si las termas leonesas son similares a las de Britania y Germania, existe la suposición de que contarían con una basílica "thermarum" o zona dedicada a ejercicios gimnásticos -con una, dos o tres naves-, en sustitución de la palestra (al aire libre). La basílica thermarum estaría situada en la zona sur de la catedral (entre el costado sur y la verja). A diferencia de otros hipocaustos romanos, el localizado en el costado septentrional del templo gótico tiene bóvedas de arista y muros enteramente de ladrillo y no de piedra. Ángel Morillo, profesor titular de la Universidad Complutense de Madrid, y uno de los mayores expertos en arqueología romana de España, descubrió en la cripta de Puerta Obispo piezas relacionadas con el culto a Mercurio. Con seguridad, el complejo termal sobre el que se asienta la catedral tendría en su día un ninfeo o santuario de culto a las ninfas y otras divinidades curativas.

Los primeros restos de esta construcción fueron descubiertos a mediados del siglo bajo la catedral con motivo de las obras de restauración que se efectuaban en el templo. Bajo el pórtico de la fachada principal sabemos de la existencia de tres espacios sobre hipocausto, según dibujos que realizó el arquitecto autor del hallazgo, Demetrio de los Ríos. 

Al complejo termal se asocian los restos de las letrinas que se albergan en la cripta arqueológica, que ocupaban una de las esquinas de los baños. Esta zona experimentó profundas reformas a lo largo de su dilatada existencia. En un primer momento existió un depósito de agua de forma rectangular, al que corresponden los muros de hormigón hidráulico del exterior y el pavimento de pequeños ladrillos puestos en espiga (opus spicatum). Reformas posteriores motivaron la construcción de muros de mampostería y ladrillo, configurando unas letrinas de las que conservamos los canales por los que discurrían las aguas residuales, acometiendo a una pequeña cloaca encargada de evacuarlas al exterior del campamento. 

Como fecha inicial de las termas se ha propuesto el siglo d.C., tomando como apoyo una inscripción incisa dedicada al emperador Antonino Pio (138-161 d.C.) en un ladrillo que apareció en el lugar. Las excavaciones en Puerta Obispo, además de afirmar esta datación, han determinado que el conjunto termal aún estaba en uso en el siglo . Para su construcción se aprovechó una estructura precedente, posiblemente una piscina o un depósito. También han permitido establecer dos límites (este y sur) de esta imponente construcción.

La envergadura y calidad constructiva de las termas legionenses hicieron que el rey Ordoño I de Asturias situara en ellas su palacio, tras arrebatarles la ciudad a los musulmanes en el año 856. Más tarde, en 916 fueron donadas por el rey Ordoño II de León para la sede de la iglesia episcopal de Santa María, utilizándose una parte del antiguo edificio romano como panteón real, hasta que esta función se trasladó en tiempos de Ramiro II de León a Palat de Rey.

Asimismo, existen más restos de las termas romanas y de la antigua catedral románica bajo la actual catedral. En la zona norte está la llamada cripta de Menéndez Pidal, descubierta por este historiador y no abierta al público, que correspondería al caldarium de las termas, utilizada como cámara funeraria en la Edad Media, donde se conservan cinco sepulcros.

"La Catedral de Cristal" es un oratorio profano compuesto por el músico leonés Igor Escudero a partir del libreto de Pedro G. Trapiello, escritor, periodista y guionista leonés. Su argumento hace un repaso por la historia del edificio, que funciona como palimpsesto de las culturas y tradiciones musicales que vivieron bajo la sombra de sus muros. El oratorio fue estrenado en el Auditorio Ciudad de León el 1 de marzo del año 2014.



</doc>
<doc id="751" url="https://es.wikipedia.org/wiki?curid=751" title="Ocultismo">
Ocultismo

El ocultismo, las ciencias ocultas o las artes ocultas es el estudio de diversos conocimientos y prácticas misteriosas de carácter dogmático, como la magia, la alquimia (como disciplina espiritual y filosófica), la adivinación, etc. que desde la antigüedad pretenden estudiar los secretos del universo.

La palabra española «ocultismo» deriva de la voz latina "occultus", que significa ‘oculto, clandestino, escondido, secreto’, y que proviene de "occulere" (‘ocultar’).

Lo oculto es todo aquello que no tiene explicación, cuyo conocimiento no está a disposición de los no iniciados.
En el lenguaje común, tanto en inglés como en español, lo oculto se refiere al conocimiento de lo paranormal e inexplicable, en oposición del conocimiento de lo medible y explicable, usualmente referido como ciencia.
El término a veces se utiliza para designar el conocimiento que está destinado a ciertas personas y que debe permanecer fuera de la vista de aquellos no iniciados en el tema. Para muchos estudiosos ocultistas, lo oculto es simplemente el estudio de la realidad espiritual subyacente y más profunda que va más allá de la razón pura y de las ciencias del conocimiento de lo sensible y físico.
Los términos esotérico y arcano tienen un significado similar y en muchos contextos los tres términos son intercambiables.

El ocultismo también se refiere a cierto tipo de organizaciones u órdenes, sus enseñanzas y prácticas y a las corrientes literarias y de filosofía espiritual, presentes e históricas, relacionadas con este tema.

El ocultismo es el estudio de las artes, prácticas o ciencias ocultas como
la magia,
la alquimia,
la percepción extrasensorial,
la astrología,
el espiritismo y
la adivinación, entre otras.

Entre los ejemplos más importantes de ciencias ocultas desarrollados en la Antigüedad, se pueden mencionar las siguientes:


La interpretación del ocultismo y sus conceptos puede encontrarse en las estructuras de creencias de ciertas filosofías y religiones como
el gnosticismo,
el hermetismo,
la teosofía,
la wicca,
el thelema,
el satanismo,
el neopaganismo o ―en realidad―
cualquier religión.
El historiador británico Nicholas Goodrick-Clarke ofrece una definición más amplia:

La historia del Ocultismo es muy antigua, no se conoce a ciencia cierta su origen, pues muchas culturas alrededor del mundo tuvieron prácticas misteriosas y esotéricas desde el inicio de la humanidad. Históricamente ha tenido relaciones estrechas con ciertas religiones o doctrinas puesto que su concepto se ha confundido. Los ocultistas divulgan la noción de un solo Dios, un solo poder y energía. Esta energía, Dios, es el dador, el recibidor y el mismo don en sí.

En el Renacimiento, el filósofo, médico, astrónomo, abogado, teólogo y mago alemán Cornelio Agrippa (1486-1535) realizó una obra ocultista y esotérica con influencias judeocristianas y cabalísticas. En su obra principal, "De occulta philosophia libri tres" (1531), recogió todo el conocimiento medieval sobre magia, astrología, alquimia, medicina y filosofía natural y lo respaldó teóricamente. Erudito de fama y protegido por distintas casas reinantes o nobles, fue amigo de gran parte de los filósofos y grandes figuras de su tiempo.

También en el ocultismo se ha mencionado a Leonardo Da Vinci, personaje que se vio involucrado por abrir cuerpos y realizar cosas que en este tiempo no debían ser realizadas.

A mediados del siglo XIX el ocultismo tuvo un período de gran apogeo en Francia, de la mano del cabalista Eliphas Lévi, quien produjo una decena de obras esotéricas de importancia, entre las que se destaca "Dogma y ritual de alta magia".

Poco tiempo después (1875), Helena Blavatsky y Henry Olcott fundan la Sociedad Teosófica en Nueva York para el estudio de los fenómenos inexplicados de la naturaleza y de las religiones comparadas. Tras la publicación de "Isis sin velo" y "La Doctrina Secreta", el ocultismo se difundió rápidamente por todo Occidente, reapareciendo escuelas de la rosacruz, herméticas y de magia, relacionadas con la masonería.

Tras la muerte de Helena Blavatsky, el movimiento teosófico se fracturó y surgieron varios grupos, de la mano de Annie Besant, William Judge y posteriormente Rudolf Steiner (fundador de la antroposofía) y Alice Bailey.

La Escuela Arcana de Alice Bailey puede ser considerada el más inmediato antepasado del fenómeno de la Nueva Era y la supuesta canalización de entidades que enseñan las doctrinas del nuevo tiempo, como los maestros ascendidos, entre ellos Djwhal Khul. Del tronco teosófico también surgen otras sectas como Nueva Acrópolis.

Aparecieron grupos de tendencia masónica, como la Fraternidad Rosacruz (de Max Heindel),
la Fraternidad Rosacruz Antigua (de Arnold Krumm-Heller) y
AMORC (sigla de la Antigua y Mística Orden Rosae Crucis, de Harvey Spencer Lewis).

El moderno movimiento de la magia ritual se inspiró en las doctrinas del grupo esotérico Orden Hermética de la Aurora Dorada, el cual fue fundado por MacGregor Mathers, quien afirmaba que estaba continuando el legado de una orden rosacruz alemana conocida como Orden de la Rosa Oro Rubí y de la Cruz de Oro. Actualmente la magia ritual pone un particular énfasis en los continuadores del británico Aleister Crowley.

Dentro del movimiento denominado nueva era (o "new age"), se ha desarrollado el interés por el conjunto de diversas creencias y de prácticas que no necesariamente excluye el interés por las anteriores. Las ideas reformuladas por sus partidarios suelen relacionarse con la medicina holística, las medicinas alternativas o tradicionales y el misticismo. También se incluyen numerosas ciencias ocultas y pseudociencias, perspectivas generales sobre historia, religión, espiritualidad, estilos de vida y ciertos tipos de música. Algunos de los temas relacionados son:




</doc>
<doc id="752" url="https://es.wikipedia.org/wiki?curid=752" title="Ciencias sociales">
Ciencias sociales

Las ciencias sociales son las ramas de la ciencia relacionadas con la sociedad y el comportamiento humano. Se las distingue de las ciencias naturales y de las ciencias formales. Además, es una denominación genérica para las disciplinas o campos del saber que analizan y tratan distintos aspectos de las relaciones sociales y los grupos de personas que componen la sociedad. Estas se ocupan tanto de sus manifestaciones materiales como de las inmateriales. Otras denominaciones confluyentes o diferenciadas, según la intención de quien las utiliza, son las de ciencias humanas, humanidades o letras. También se utilizan distintas combinaciones de esos términos, como la de "ciencias humanas y sociales". 

Las ciencias sociales estudian el origen del comportamiento individual y colectivo, buscando comprender y explicar regularidades y particularidades que se expresan en el conjunto de las instituciones humanas.

La historia de las ciencias sociales tiene sus raíces en la filosofía antigua. En la Edad Antigua no existía diferencia entre las matemáticas y el estudio de la historia, la poesía o la política. Durante la Edad Media, la civilización islámica hizo importantes contribuciones a las ciencias sociales. Esta unidad de ciencia como restos descriptivos y razonamiento deductivo de axiomas crearon un marco científico.

La Ilustración vio una revolución la cual denominada filosofía natural, con la cual se modificó el marco básico por el cual los individuos entendían lo que era " Científico. En algunos sectores, el avance Reciente de los estudios matemáticos presumía una realidad independiente del observador y que funcionaba por sus propias normas. Las ciencias sociales provienen de la Filosofía Moral de la época y estuvieron influenciadas por la Era de las revoluciones, tales como la Revolución industrial y la Revolución francesa. Las ciencias sociales desarrolladas a partir de las ciencias (experimentales y aplicadas) o el conocimiento de base sistemático o prácticas prescriptivas, relaciones con el progreso social de un grupo de entidades interactuantes.

Los inicios de las ciencias sociales en el siglo XVIII están reflejados en la Enciclopedia de Diderot, con artículos de Rousseau y otros enciclopedistas. El crecimiento de las ciencias sociales también fue mostrado en otras enciclopedias especializadas. En el período moderno, el término "ciencias sociales" fue inicialmente utilizada como un campo conceptual distinto. Las ciencias sociales fue influenciada por el positivismo, centrado en el conocimiento basado en la experiencia real; la especulación metafísica fue eliminada. Auguste Comte usó el término "ciencia social" para describir el campo, tomado de las ideas de Charles Fourier; Comte también se refiere al campo de la "física social".

Después de este período, hubo cinco vías de desarrollo que surgieron en las ciencias sociales, influenciadas por Comte y otros campos. Una de ellas fue la investigación social, por la cual se llevaron a cabo grandes muestras estadísticas en varias partes de Estados Unidos y Europa. Otro camino fue iniciado por Émile Durkheim, quien estudió los "hechos sociales", y por Vilfredo Pareto que introdujo ideas metateóricas y teorías individuales. Un tercer camino, desarrollado por Max Weber, surgió de la dicotomía metodológica, en la cual se identificaba y entendía el fenómeno social. La cuarta ruta se fundamentó en la economía, desarrolló y promovió el conocimiento económico como el propio de una ciencia dura. La última vía fue la correlación de conocimiento y valores sociales; Weber demandó firmemente esta distinción. En esta vía, la teoría (descripción) y la prescripción eran discusiones formales no superpuestas de un tema.

Para el inicio del siglo XX, la filosofía ilustrada había sido desafiada en varios frentes. Después de la utilización de las teorías clásicas desde el final de la revolución científica, diversos campos sustituyeron los estudios matemáticos por estudios experimentales y analizaron ecuaciones para construir una estructura teórica. El desarrollo de subcampos de las ciencias sociales se hizo muy cuantitativa en la metodología. Por el contrario, el carácter inter y transdisciplinar de la investigación científica en el comportamiento humano y los factores sociales y ambientales que la afectaban hizo que muchas de las ciencias naturales se interesaran en algunos aspectos de la metodología de las ciencias sociales. Ejemplos de una frontera borrosa incluyen disciplinas emergentes, como la medicina, sociobiología, neuropsicología, bioeconomía e historia y sociología de la ciencia. Cada vez más, la investigación cuantitativa y los métodos cualitativos están siendo integrados en el estudio de la acción humana y sus implicancias y consecuencias. En la primera mitad del siglo XX, la estadística se convirtió en una disciplina independiente de matemática aplicada, por lo que los métodos estadísticos cobraron mayor confiabilidad.

En el período contemporáneo, Karl Popper y Talcott Parsons influyeron en la promoción de las ciencias sociales. Los investigadores continúan en busca de un consenso unificado sobre qué metodología tendrá el poder y refinamiento de conectar una "gran teoría" propuesta" con las múltiples teorías de medio rango que siguen proveyendo con considerable éxito marcos utilizables para bases de datos masivos y crecientes (véase consiliencia); sin embargo, en la actualidad, los distintos ámbitos de las ciencias sociales evolucionan en una variedad de formas, aumentando el conocimiento general de la sociedad. Las ciencias sociales en el futuro previsible estarán compuestas de diferentes áreas de investigación de campo.

Las ciencias sociales presentan problemas metodológicos y epistemológicos propios, diferentes de los que aparecen en las ciencias naturales. Sin embargo, en ciencias sociales históricamente ha existido mayor discusión respecto a qué constituye genuinamente una ciencia social y qué no. De hecho, algunos estudios o disciplinas sociales, si bien involucran razonamientos y discusión racional, no se consideran propiamente ciencias sociales. 

Las ciencias sociales buscan, desde sus inicios, llegar a una etapa verdaderamente científica, logrando cierta independencia respecto del método prevaleciente en la filosofía. En esta coexisten posturas opuestas respecto de algún aspecto de la realidad, mientras que en las ciencias exactas, ello no es posible. De ahí que las ramas humanistas de la ciencia deberían tratar de imitar, al menos en este aspecto, a las ciencias exactas. William James expresaba, a finales del siglo XIX: «Una serie de meros hechos, pequeños diálogos y altercados sobre opiniones; parcas clasificaciones y generalizaciones en un plano meramente descriptivo….pero ni una sola ley como la que nos proporciona la física; ni una sola proposición de la cual pueda deducirse casualmente consecuencia alguna…. Esto no es ciencia, es solamente un proyecto de ciencia».

Recordemos que toda ciencia debe establecer descripciones objetivas basadas en aspectos observables, y por tanto verificables, de la realidad. Las leyes que la han de constituir consistirán en vínculos causales existentes entre las variables intervinientes en la descripción. Además, el conocimiento deberá estar organizado en una forma axiomática, en forma similar a la ética establecida por Baruch de Spinoza. Tal tipo de organización no garantiza la veracidad de una descripción, sino que constituirá un requisito necesario para que las ciencias sociales adquieran el carácter científico que tanto se busca.

Mario Bunge escribió: «De los investigadores científicos se espera que se guíen por el método científico, que se reduce a la siguiente sucesión de pasos: conocimiento previo, problema, candidato a la solución (hipótesis, diseño experimental o técnica), prueba, evaluación del candidato, revisión final de uno u otro candidato a la solución, examinando el procedimiento, el conocimiento previo e incluso el problema».

«La verificación de las proposiciones consiste en someterlas a prueba para comprobar su coherencia y su verdad, la que a menudo resulta ser solo aproximada. Esa prueba puede ser conceptual, empírica o ambas cosas. Ningún elemento, excepto las convenciones y las fórmulas matemáticas, se considera exento de las pruebas empíricas. Tampoco hay ciencia alguna sin éstas, o ninguna en que estén ausentes la búsqueda y la utilización de pautas».

«Según lo estimo, la descripción sumaria antes mencionada es válida para todas las ciencias, independientemente de las diferencias de objetos, técnicas especiales o grados de progreso. Se ajusta a las ciencias sociales como la sociología, lo mismo que a las ciencias biosociales como la psicología o a la antropología, y a las naturales como la biología. Si una disciplina no emplea el método científico o si no busca o utiliza regularidades, es protocientífica, no científica o pseudocientífica».

En la actualidad, existen críticas a la creciente especialización y escasa intercomunicación entre las ciencias sociales. Esto iría en menoscabo de un análisis global de la sociedad (ver Wallerstein 1996).

En la "Encyclopedia of Sociology" (Borgata y Mantgomery 2000), estudia este tema: La Sociología está poco relacionada con la Psicología social, con la Historia social, con la Geografía humana, con la Política pero debería estarlo más; si está más relacionada con la Antropología cultural, con la Ecología humana, con la Demografía, con el Urbanismo, con la Estadística y con la Filosofía. Dichas relaciones no son en su totalidad, sino en partes o sectores de cada disciplina. La Sociología es la asignatura más abierta a otras aportaciones del resto de las Ciencias sociales y esto lo deducen de las recopilaciones de índices de citaciones en artículos y libros.

El proceso es que en las zonas fronterizas se van creando híbridos y esto es lo que les da coherencia a las necesarias interdependencias o prestaciones. La escasa intercomunicación entre disciplinas aún es más manifiesta entre científicos sociales de los diferentes países, que citan solamente a los de su entorno cultural, o su propio país, y principalmente a los clásicos, cuando de hecho los grupos latinoamericano, europeo y japonés, supera en bibliografía al grupo inglés americano. La transdisciplinariedad es la apuesta que emerge de esta insuficiencia de las disciplinas y lo interdisciplinar.

En general, existe un acuerdo no tan razonable sobre qué disciplinas deben ser consideradas parte de las ciencias sociales y también de las ciencias naturales, aunque la división tradicional entre ambas es dudosa en el caso de algunas. Por ejemplo, si bien la lingüística había sido considerada casi universalmente una ciencia social, el enfoque moderno iniciado con la gramática generativa de Noam Chomsky sugiere que la lingüística no trata tanto de la interacción social sino que debe ser vista como una parte de la psicología o la biología evolutiva, ya que en el funcionamiento de las lenguas y en su evolución temporal la conciencia de los hablantes o sus representaciones psicológicas no parecen desempeñar ningún papel. Por eso mismo, algunos autores han llegado a considerar que las lenguas son un objeto natural que se genera espontáneamente y no por la intención deliberada de los seres humanos.

Existe un conjunto de Tecnologías sociales, a veces llamadas impropiamente «ciencias sociales aplicadas», que hacen un uso importante de desarrollos de las ciencias sociales propiamente dichas y de otras tecnologías sociales, para tratar de ordenar o mejorar procesos organizativos o enseñanza. Estas disciplinas científicas utilizan el conocimiento de las ciencias sociales, y a su vez desarrollan conocimiento científico propio utilizando para esto el método científico; es decir su conocimiento es científico, desarrollan conocimiento científico, pero no es ciencia ya que, el fin que persiguen es aplicar el conocimiento a la realidad por medio de la Técnica para transformarla y no obtener conocimiento en sí mismo por el mero hecho de conocer la realidad:


La relación de estas disciplinas con las ciencias sociales es similar a la que existe entre la ingeniería o Medicina y las ciencias naturales. Si bien la ingeniería hace uso de métodos objetivos y puede servirse de experimentación guiada por el método científico, su objetivo primordial no es adquirir nuevos conocimientos o investigar problemas científicos, sino encontrar la mejor manera de aprovechar principios y conocimientos científicos para resolver problemas prácticos.

Estas disciplinas son eminentemente técnicas, pueden ser científicas, es decir basadas en ciencias y tecnologías, sobre todo esta última aplicada a distintos ámbitos, situaciones y objetos de estudio.


La mayoría de universidades ofrecen grados en campos de las ciencias sociales. La Licenciatura en Ciencias Sociales es un grado dirigido a las ciencias sociales en particular. Es a menudo más flexible y profundo que otros grados que incluyen materias de ciencias sociales.

En los Estados Unidos, una universidad puede ofrecer a un estudiante que estudia un campo de las ciencias sociales un grado de Bachelor of Arts, particularmente si el campo está entre uno de las tradicionales artes liberales como la historia, o un BSc: grado de Bachelor of Science como aquellos proporcionados por la London School of Economics, ya que las ciencias sociales constituyen una de las dos principales ramas de la ciencia (siendo la otra las ciencias naturales). Además, algunas instituciones tienen grados para una ciencia social particular, como el grado de Bachelor of Economics, aunque estos grados especializados son relativamente raros en los Estados Unidos.




</doc>
<doc id="754" url="https://es.wikipedia.org/wiki?curid=754" title="Cámara">
Cámara

Cámara o camarilla puede referirse a:













</doc>
<doc id="755" url="https://es.wikipedia.org/wiki?curid=755" title="Comunidad autónoma">
Comunidad autónoma

Una comunidad autónoma (C. A.) es una entidad territorial española que, dentro del ordenamiento jurídico constitucional, está dotada de autonomía, con instituciones y representantes propios y determinadas competencias legislativas, ejecutivas y administrativas.

La promulgación de la Constitución de 1978, que recoge el derecho de autonomía de las nacionalidades y regiones que forman el Estado, supuso un cambio de 180 grados con respecto al régimen anterior, que se basaba en planes centralizados tradicionales. Esto daba respuesta a un problema que había surgido repetidamente en la historia de España como resultado de las diferentes identidades sobre las que se ha construido la unidad de España.

Tras la ratificación de la carta magna, y como resultado de la implementación de los principios contenidos en el Título VIII, en el curso de unos pocos años se ha completado el proceso de instauración de las 16 comunidades autónomas y de la comunidad foral, mediante la aprobación de sus Estatutos de Autonomía, aunque solo cuatro de ellos —Cataluña, País Vasco, Galicia y Andalucía— han sido refrendados por sus ciudadanos. Han sido también dotadas de su propio órgano de gobierno e instituciones representativas. Hay que destacar que el proceso que ofrece la Constitución española no obliga ni a las regiones ni a las nacionalidades, sino que es, en teoría, un derecho para ellas. No obstante, en la práctica, los pactos autonómicos obligaron a las provincias a formar comunidades autónomas. De hecho, sirva como ejemplo que, en marzo de 1983, la única provincia que no formaba parte de ninguna comunidad, Segovia, fue obligada por decreto del Gobierno a integrarse en Castilla y León «por motivos de interés nacional».

El 31 de julio de 1981, Leopoldo Calvo-Sotelo, presidente del Gobierno, y Felipe González, líder de la oposición, acuerdan los primeros pactos autonómicos, que prevén un mapa de 17 autonomías, con las mismas instituciones pero con distintas competencias. En 1995, se actualizó con los Segundos Pactos Autonómicos, firmado entre el entonces presidente, Felipe González, y el líder de la oposición, José María Aznar, por el cual se crean las dos ciudades autónomas, Ceuta y Melilla. Fruto de estos acuerdos, en 1995 se dará por cerrado el «mapa de las autonomías» a nuevas remodelaciones o ampliaciones.

Desde 2003, y para fines estadísticos, basadas en las normativas europeas y fijadas por la Eurostat, se encuentran las unidades NUTS en vigor en la Unión Europea. Las 17 comunidades autónomas españolas se encuentran clasificadas en los niveles NUTS 2.

El artículo 152.1 de la Constitución establece la organización institucional básica de aquellas comunidades autónomas que accedieron a la autonomía por la denominada «vía rápida», esto es, País Vasco, Cataluña, Galicia y Andalucía. No obstante, dicha organización institucional básica ha sido la que, mediante sus respectivos Estatutos de Autonomía, han asumido todas las comunidades autónomas, con independencia de su vía de acceso a la autonomía política.

Así, en la actualidad, los órganos básicos comunes a todas las comunidades autónomas son una Asamblea Legislativa, elegida por sufragio universal; un Consejo de Gobierno, con funciones ejecutivas; y un Presidente de la comunidad autónoma, elegido por la Asamblea Legislativa de entre sus miembros, que ostenta la más alta representación de la Comunidad.

La asamblea es el parlamento autonómico unicameral, que en las distintas comunidades se denomina de diferentes formas:

El sistema de elección de los miembros es por sufragio universal, siguiendo el mismo régimen de incompatibilidad e inelegibilidad que las Cortes Generales. Las elecciones se celebran el último domingo de mayo cada cuatro años, en todas las comunidades excepto en:

Las comunidades autónomas gozan de potestad legislativa, la cual reside en su asamblea. Además de otras funciones: presupuestarias, control del ejecutivo autonómico, elección del gobierno, del Presidente del ejecutivo, participación en las reformas de la Constitución, control de la constitucionalidad de Leyes y disposiciones con fuerza de Ley, participación en la composición del Senado.

Una vez celebradas las elecciones y constituida la asamblea legislativa, su presidente propone a uno o varios candidatos, según las distintas comunidades. Estos candidatos se someten a votación, resultando elegido el que sea apoyado por mayoría absoluta en primera votación y si ninguno la obtuviese, en segunda por mayoría simple.

Si no obtuviese el apoyo se repetirán las votaciones con distintas candidaturas, hasta que si pasados 2 meses desde la primera votación ninguno de los candidatos hubiese obtenido el apoyo, se disolverá la cámara, y se volverán a convocar elecciones.

Dirección del consejo de gobierno y suprema representación de la Comunidad, representación del Estado en la comunidad autónoma. Promulga y ordena la publicación de las Leyes y del nombramiento del poder judicial en la comunidad.

Es el máximo órgano ejecutivo colegiado de la Comunidad autónoma. Sus funciones son la administración civil, ejecución de las leyes y reglamentación. Los miembros responden ante el Tribunal Superior de Justicia, tanto en el ámbito civil, como penal. Están sometidos a control político a través de la cuestión de confianza y la moción de censura. Está presidido por el presidente de la Comunidad Autónoma, que nombra y cesa a sus miembros.

Las Comunidades pueden crear sus propios Tribunales de Cuentas, Defensores del Pueblo (en Aragón es la figura del Justicia de Aragón quien cumple con ese deber además de ser la tercera autoridad junto al Presidente de la Diputación General de Aragón y al presidente de las Cortes de Aragón) y otros organismos para su buen funcionamiento.

La Constitución española enumera los poderes y competencias potenciales de las comunidades autónomas, también las áreas que el gobierno central mantiene como competencias exclusivas o compartidas. Generalmente, las comunidades autónomas tienen las competencias sobre la educación, el sistema de salud, cultura y lengua, patrimonio, asistencia social, ordenación del territorio y urbanismo, protección del medio ambiente, transporte público, agricultura, etc. Las competencias transferidas dependen de los estatutos de autonomía y pueden variar considerablemente; por ejemplo, el País Vasco y Navarra disponen de un poder y autonomía fiscal más extensivos, mientras que los servicios básicos de las ciudades autónomas de Ceuta y Melilla son generalmente provistos por el gobierno central, debido a su localización y circunstancias.

Las comunidades autónomas de régimen común (todas las CC.AA excepto País Vasco y Navarra, que tienen un régimen foral) comparten la gestión de impuestos con el gobierno central, por ejemplo recogiendo sus propios impuestos patrimoniales y sucesiones, recogiendo una parte del impuesto sobre la renta (IRPF), mientras que el 50% de los fondos generados por el IVA y el impuesto especial sobre el alcohol son cedidos a las CCAA por el gobierno central. La división territorial de España se basa en el principio de equidad, pues existen medidas para garantizar un nivel similar de financiación para los servicios públicos de las distintas comunidades, a pesar de las diferencias económicas entre las CC.AA.

Lista de comunidades y ciudades autónomas españolas ordenadas por población (datos del padrón municipal de habitantes, INE a fecha 1 de enero de 2018).

A continuación encontramos la lista de comunidades autónomas españolas ordenadas por renta per cápita (2018). Es de gran importancia conocer la renta per cápita de una comunidad autónoma, pues de ello depende la cantidad de fondos europeos que recibe una región. Las regiones «menos desarrolladas», cuyo PIB per cápita es inferior al 75 % de la media de la UE, siguen siendo la principal prioridad de la Política de Cohesión (Objetivo 1). Las regiones «en transición», cuyo PIB per cápita está situado entre el 75 % y el 90 % de la media de la UE, serán el Objetivo 2. En el caso de España, en 2010 tan solo Extremadura se encontraba en dicha situación.

La puesta en funcionamiento de las Comunidades Autónomas determinó la necesidad de crear un sistema de financiación que garantizara la obtención de los recursos necesarios para ejercer las competencias que estas nuevas administraciones territoriales iban asumiendo. La Constitución de 1978 determinaba los aspectos básicos de la financiación autonómica en sus artículos 156 a 158. En ellos se establecen estos principios:

Sin embargo, estos principios comunes no determinan la existencia de un modelo único. Por el contrario, el reconocimiento de los derechos históricos por la Constitución (Disposición Adicional Primera) se tradujo en un sistema de Convenio y Concierto aplicables exclusivamente a los territorios forales de Navarra y el País Vasco, mientras que el resto de las Comunidades se rige por el régimen común, cuyas bases quedan establecidas en la Ley Orgánica 8/1980, de 22 de septiembre, de Financiación de las Comunidades Autónomas (LOFCA).

El Estatuto de Autonomía para el País Vasco vino a reconocer el régimen de Concierto Económico para la financiación de sus territorios históricos (Álava, Guipúzcoa y Vizcaya) configurándose un sistema propio y característico. El primer Concierto Económico fue aprobado por la ley 12/1981, de 13 de mayo y su validez se extendía hasta el 31 de diciembre de 2001. La Ley 12/2002, de 23 de mayo, aprobó, con carácter indefinido, el Concierto Económico vigente, estableciendo que cada cinco años se aprobará una ley de metodología del cupo del quinquenio.

La distribución de la renta por Comunidades Autónomas arroja datos paradójicos. La Comunidad de Madrid, que es una de las mayores en cuanto a PIB, es la peor parada si dividimos los presupuestos de la Comunidad entre el número de habitantes, lo que llamamos "ingresos por persona". Ordenadas de mayor a menor, las diferencias entre las "Comunidades ricas" y las "Comunidades pobres" son de prácticamente el 100%. En primer lugar tenemos a Navarra, con 6 255€ por persona; en segundo lugar, el País Vasco, con 5 054€ por persona; frente a ellas, Melilla es el farolillo rojo, con 3 014€ por persona; la ciudad autónoma de Ceuta dispone de 3 138€ por persona, y la Comunidad de Madrid, paradójicamente una de las de mayor PIB de España, apenas cuenta con 3 110€ por persona. 

Los diecisiete presidentes y los dos alcaldes-presidentes autonómicos actuales de España son:




</doc>
<doc id="756" url="https://es.wikipedia.org/wiki?curid=756" title="Cartografía">
Cartografía

La cartografía (del griego χάρτης, "chartēs" = mapa y γραφειν, "graphein" = escrito) es la ciencia aplicada que se encarga de reunir, realizar y analizar medidas y datos de regiones de la Tierra, para representarlas gráficamente con diferentes dimensiones lineales —escala reducida. Por extensión, también se denomina cartografía a un conjunto de documentos territoriales referidos a un ámbito concreto de estudio. La Asociación Cartográfica Internacional define la cartografía como la disciplina relacionada con la concepción, producción, diseminación y estudio de mapas.

Al ser la Tierra esférica, o más bien geoide, lo cual es una derivación del término "esférico", ha de valerse de un sistema de proyecciones para pasar de la esfera al plano. El problema es aún mayor, pues en realidad la forma de la Tierra no es exactamente esférica, su forma es más achatada en los polos, que en la zona ecuatorial. A esta figura se le denomina geoide.

Pero además de representar los contornos de las cosas, las superficies y los ángulos, se ocupa también de representar la información que aparece sobre el mapa, según se considere qué es relevante y qué no. Esto, normalmente, depende de lo que se quiera representar en el mapa y de la escala.

Actualmente estas representaciones cartográficas se pueden realizar con programas de informática llamados SIG, en los que tiene georreferencia desde un árbol y su ubicación, hasta una ciudad entera incluyendo sus edificios, calles, plazas, puentes, jurisdicciones, etc.

Amberes fue el centro de la cartografía en la segunda mitad del siglo XVI, cuando la ciudad era el principal puerto del imperio español con acceso al Mar del Norte; con el declive del imperio español durante el reinado de Felipe III, y la política ejercida por los gobernadores españoles sobre los flamencos protestantes, gran parte de estos dejaron los Países Bajos españoles (la actual Bélgica) y pasaron a trabajar en los Países Bajos rebeldes: la "República de las Provincias Unidas de los Países Bajos", determinando así que en la primera mitad del siglo XVII fuese Ámsterdam la principal fuente de cartografía moderna, luego el impulso pasaría a Francia, hasta mediados del siglo XVIII, y de allí en adelante a Gran Bretaña, así como a los Estados Unidos a partir del siglo XIX.

La cartografía en la época de la Web 2.0 se ha extendido hasta Internet, propiciando el surgimiento del contenido creado por el usuario. Este término implica que existan mapas creados de la manera tradicional - mediante contribuciones de varios cartógrafos individuales - o con información aportada por el público. En la actualidad, son numerosos los portales que permiten visualizar y consultar mapas de casi todo el mundo.

El mapa conocido más antiguo es una cuestión polémica, porque la definición de "mapa" no es unívoca y porque para la creación de mapas se utilizaron diversos materiales. Existe una pintura mural, que puede representar la antigua ciudad de Çatalhöyük, en Anatolia (conocida previamente como Huyuk o Çatal Hüyük), datada en el VII milenio a. C. Otros mapas conocidos del mundo antiguo incluyen a la civilización minoica: la «Casa del almirante» es una pintura mural datada en 1.600 a. C., en la que se observa una comunidad costera en perspectiva oblicua. También hay un mapa grabado de la Sagrada Ciudad de Babilonia de Nippur, del período Kassita, (Siglo XIV a. C. - Siglo XII a. C.)

En la antigua Grecia y el Imperio romano se crearon mapas, como el de Anaximandro en el Siglo VI a. C. o el mapamundi de Claudio Ptolomeo, que es un mapa del mundo conocido "(Ecúmene)" por la sociedad occidental en el Siglo II d. C. En el siglo VIII, los eruditos árabes tradujeron los trabajos de los geógrafos griegos al árabe.

En la antigua China, los códigos geográficos datan del siglo V. Los mapas chinos más viejos son del Estado de Qin y se datan en el siglo IV, durante los Reinos Combatientes. En el libro del "Xin Yi Xiang Fa Yao", publicado en 1092 por el científico chino Su Song, hay una carta astronómica con una proyección cilíndrica similar a la actual y, al parecer, inventado por separado, a la Proyección de Mercator. Aunque este método de cálculo parece haber existido en China incluso antes de esta publicación y, científicamente, el significado más grande de las Cartas astronómicas de Su Song, es que representan los mapas impresos existentes más antiguos conocidos.

Los primeros signos de la cartografía india incluyen pinturas legendarias; mapas de localizaciones descritas en epopeyas hindúes como el Rāmāyana. Las tradiciones cartográficas hindúes también situaron la localización de la Estrella Polar, así como otras constelaciones.

Mapamundi es el término general usado para describir a los mapas europeos del Mundo Medieval. Aproximadamente 1.100 mapamundis sobrevivieron a la Edad Media. De estos, 900 son ilustraciones manuscritas y el resto existe como documentos independientes (Woodward, P. 286).

El geógrafo árabe, Muhammad al-Idrisi, elaboró su mapa, la "Tabula Rogeriana", en 1154, incorporando el África conocida, el océano Índico y el Extremo Oriente conocido, compilando la información de los comerciantes y exploradores árabes y la heredada de los geógrafos clásicos para crear el mapa más exacto del mundo en su tiempo y durante los siguientes tres siglos.

En la Era de los descubrimientos, del siglo XV al XVII, los cartógrafos europeos copiaron mapas antiguos (algunos datados muchos siglos atrás) y dibujaron sus propios mapas basados en las observaciones de los exploradores aunque con nuevas técnicas. La invención de la brújula, el telescopio y el desarrollo de la agrimensura les dieron mayor exactitud. En 1492, Martin Behaim, un cartógrafo alemán, hizo el primer globo terráqueo, el "Erdapfel".

Johannes Werner estudió y perfeccionó los sistemas de proyección de los mapas, desarrollando la proyección cordiforme. En 1507, Martin Waldseemüller elaboró un globo del mundo y un gran mapamundi mural distribuido en 12 hojas ("Universalis Cosmographia"), siendo el primer mapa en aplicar el nombre de «América» a las tierras recién descubiertas por los europeos y el primero en presentar este continente separado del asiático. El cartógrafo portugués, Diego Ribero, fue el autor del primer planisferio conocido con un Ecuador terrestre graduado (1527). El cartógrafo italiano Bautista Agnese elaboró por lo menos 71 atlas manuscritos de las cartas marinas.

Debido a las dificultades inherentes en la cartografía, fabricantes de mapas copiaron con frecuencia el material de trabajos anteriores sin mencionar al cartógrafo original. Por ejemplo, uno de los mapas antiguos más famosos de Norteamérica, vulgarmente conocido como el “Mapa Castor”, publicado en 1715 de Herman Moll, es una reproducción exacta de un trabajo en 1698 de Nicolás De Fer. De Fer había copiado a su vez las imágenes impresas en libros de Louis Hennepin, publicados en 1697, y François Du Creux, en 1664. Por los años 1700, los fabricantes de mapas comenzaron a darle crédito al autor original imprimiendo la frase “Según [el cartógrafo original]”.

En México, la cartografía tiene sus propias características. Si bien se inscribe en el contexto del pensamiento cartográfico de occidente su origen se encuentra en las formas de expresión empleadas por los antiguos pobladores de Mesoamérica para representar el conocimiento geográfico.

Los cambios en la producción de mapas corren paralelos a los cambios producidos en la tecnología. El salto más grande se produjo a partir de la Edad Media cuando se inventan instrumentos como el cuadrante y la brújula, que permiten medir los ángulos respecto a la estrella polar y el Sol. Estos instrumentos, permitieron determinar la latitud para finalmente plasmarla en los mapas.

En las llamadas "cartas planas", las latitudes observadas y las direcciones magnéticas se representan directamente en el mapa, con una escala constante, como si la Tierra fuese plana.

En la cartografía, la tecnología ha cambiado continuamente para resolver las demandas de nuevas generaciones de fabricantes de mapas y de lectores de mapas. Los primeros mapas fueron elaborados manualmente con plumas sobre pergaminos; por lo tanto, variaban en calidad y su distribución fue muy limitada. La introducción de dispositivos magnéticos, tales como la brújula permitían la creación de mapas de diferentes escalas más exactos y más fáciles de almacenar y manipular.

Los avances en dispositivos mecánicos tales como la imprenta, el cuadrante y el nonio, utilizados para que la producción en masa de mapas y la capacidad de hacer reproducciones más exactas de datos. La tecnología óptica, como el telescopio, el sextante y otros dispositivos, permitían examinar de forma más exacta la Tierra y aumentaron la capacidad de los creadores de mapas y navegantes para encontrar su latitud midiendo ángulos con la Estrella Polar de noche o al mediodía.

Avances en tecnología fotoquímica, tales como la litógráficos y la procesos fotomecánicos, han tenido en cuenta la creación de mapas que tienen detalles finos, no se tuercen en su forma y resistentes a la humedad y el desgaste. Esto también eliminó la necesidad del grabado, que en un futuro acortó el tiempo que toma para hacer y para reproducir mapas.

Avances en tecnología electrónica en el Siglo XX condujeron a otra revolución en la cartografía. Disponiendo de una lista de computadores y sus avances por ejemplo monitores, los trazadores, las impresoras, los escáneres (remotos y de documentos) y los trazadores estéreos analíticos, junto con los programas de computadora para la visualización, el proceso de imagen, el análisis espacial, y la gerencia de la base de datos, lo hicieron accesible al pueblo y han ampliado grandemente la fabricación de mapas. La capacidad espaciales localizar variables sobre mapas existentes y se crearon nuevas aplicaciones para los mapas y nuevas industrias de exploración y para explotar estos potenciales. El uso de técnicas actuales como la fotografía por satélite, ha facilitado en los últimos tiempos, la elaboración de mapas cartográficos de forma más precisa. Esto tiene unas consecuencias inmediatas para las demás ciencias y estudios que dependen de la cartografía para desarrollarse.

Además, en la actualidad podemos desarrollar mapas en 3D usando softwares destinados a  esto, lo que lleva un paso más allá la visualización de estos mapas.

Actualmente la mayoría de los mapas de calidad comercial se hacen usando software que figuran tres tipos principales: Diseño asistido por ordenador (DAO), Sistema de Información Geográfica (SIG) y software de ilustración especializada. La información espacial se puede almacenar en la base de datos, de que puede ser extraída en demanda. Estas herramientas conducen cada vez a mapas más dinámicos e interactivos pudiendo ser manipulados digitalmente.

De acuerdo a mapas básicos, el campo de la cartografía, se puede dividir o separar en dos categorías generales: la Cartografía general y la Cartografía temática. La Cartografía general implica esos mapas que se construyen para una audiencia general y contengan así una variedad de características. Los mapas generales exhiben muchas referencias y los sistemas de localización se producen a menudo en series. Por ejemplo, los mapas topográficos de escala 1:24,000 de la United States Geological Survey (USGS) es un estándar con respecto a los mapas canadienses de escala de 1:50,000. El gobierno de Reino Unido produce los clásicos "Ordnance Survey" mapas de 1:63,360 (1 pulgada por milla) del Reino Unido entero junto con una gama de mapas más grandes y escala muy pequeña correlacionados a gran detalle.

La Cartografía temática implica los mapas de temas geográficos específicos, orientados hacia las audiencias específicas. Un par de ejemplos puede ser el mapa del punto demostrar la producción del maíz en Indiana o un mapa sombreado del área de los condados de Ohio, dividido en clases numéricas. Mientras que el volumen de datos geográficos han evolucionado enormemente durante el siglo pasado, la cartografía temática ha llegado a ser cada vez más útil y necesaria para interpretar datos espaciales, culturales y sociales. Por ejemplo las redes sociales se mapean georeferencialmente, también se hacen mapas que muestren distancia entre personas (en número de vínculos o pasos que los separan). La línea del tiempo también puede considerarse un mapa o carta. A partir de su uso en la navegación se han perfeccionado técnicas que son recuperadas para guiar la navegación web. En sociología y comunicación, el oficio del cartógrafo también es citado como estrategia para sostener el rumbo en un mundo fluido.

El mapa del deporte de orientación combina la cartografía general y temática, diseñada para una comunidad de usuario muy específica. El elemento temático más prominente está sombreado, eso indica grados de dificultad del recorrido debido a la vegetación. La vegetación en sí mismo no es identificada, clasificándose simplemente por la dificultad (“lucha”) que él presenta.

La geometrización atravesó gran parte de la cultura visual de la ciencias modernas , afecto a las imágenes propiamente dichas y también a los modos de ver. El hombre creó métodos, que hacían que fenómenos que no podían ser conocidos, sino por medio del sentido del tacto, del gusto o del olfato, ahora podían ser visualizados, esa racionalización de la mirada reposaba en las producciones de los clásicos de la modernidad, y los mapas de la modernidad no resultaban ajenos a esa mirada, representando isomorfismos y proporcionalidad, inventando una nueva geometría de la representaciones geográficas. Esa proporcionalidad es la que busca definir a la miniatura geográfica. El filósofo francés Gastón Bachelard sostenía que, las miniaturas son objetos faltos de provistos de una objetividad psicológica real, y que miniaturizar el mundo, implicaba poseerlo. 

El mapa topográfico se trata sobre todo de la descripción topográfica de un lugar (zona provincial, región, un país o el mundo), incluyendo (especialmente en el Siglo XX) el uso de líneas de isolíneas para demostrar la altimetría (hipsometría) o batimetría del relieve. El relieve terrestre en la cartografía se puede demostrar en una variedad de maneras. En estos mapas se utilizan colores, símbolos y diferentes tipos de trazos para diferentes tipos de paisajes y relieves como montañas, valles, llanuras, lagos, depresiones del terreno y muchas características más. Además, se suelen agregar también diferentes tipos de señalizaciones que refieren a construcciones hechas por el hombre, como por ejemplo: vías de transporte, zonas de producción energética o diferentes tipos de cultivos.

El mapa topológico es un tipo muy general de mapa o plano. Desatiende a menudo la escala y el detalle en el interés de la claridad de la información emparentada. El mapa del Metro de Caracas es un ejemplo. Sin embargo el mapa utilizado preserva poco de realidad. Varía la escala constantemente y precipitadamente, y las direcciones de los contornos casuales. Los únicos rasgos importantes del mapa son la ubicación fácil de las estaciones y travesías a lo largo de pistas y si una estación o una travesía está del norte o sur del Río Guaire. Satisfacen todos los deseos típicos que un pasajero requiere informarse, satisfaciendo el propósito cartográfico.




</doc>
<doc id="757" url="https://es.wikipedia.org/wiki?curid=757" title="Compositor">
Compositor

Un compositor es aquella persona «que hace composiciones musicales». Es quien sabe escribir composiciones musicales según las normas artísticas, donde básicamente organiza una serie de sonidos teniendo como base los parámetros de la teoría. Existen el compositor dramático, que compone óperas y el compositor sinfónico, que compone música sinfónica. 

El compositor es aquel que inventa música trabajando los sonidos de forma imaginativa con el fin de poder crear su propia música, es quien tiene la capacidad de hablar a través de los sonidos. El hablar podría decirse que es el equivalente a improvisar en música y escribir sería el equivalente a componer. El compositor es quien construye la música. Elabora un producto musical que luego puede ser interpretado en distintas ocasiones también por otras personas y permite ser fijado en un soporte, por ejemplo, grabado. 

Esto se debe a que la música es una forma artística efímera que necesita ser fijada de alguna manera. 

Por eso el compositor, aunque normalmente usa la improvisación, la mayoría de las veces deja un registro escrito de sus composiciones.

El compositor no es solo un artista, es también un artesano y un inventor. Aunque la historia de la música nos muestra algunos genios, la idea de que los compositores son tocados por la varita mágica de la inspiración es un mito. 

El compositor, cuya materia prima son los sonidos y es quien decide qué instrumentos utilizar, debe aprender primero las técnicas de la composición y luego trabajar duramente en su oficio, practicar, equivocarse y borrar, probar nuevas experiencias y así va aprendiendo por ensayo y error de sus propias experiencias. La improvisación y la escucha son importantes, el compositor va escuchando lo que crea y entonces decide continuar o modificarlo. 

Generalmente se admite la opinión de que los sonidos se corresponden con ideas y que el compositor está plasmando sus ideas sobre el arte, la música o la vida, a través de su estilo y de sus composiciones. Las ideas musicales van cambiando según el momento en que viva el compositor. Los intentos por etiquetar a los compositores por estilo o por periodo histórico se debe al deseo del espectador de agrupar elementos que considera similares. 

Hablar de un compositor clásico o un compositor moderno, de un compositor barroco o un compositor renacentista, reside en el convencimiento de que todos los demás van a interpretarlo de la misma manera. Cuanto más atrás en el tiempo vivió el compositor estudiado, más fácil es etiquetarlo porque el que escucha o el que lo estudia conoce poco sobre esa época y resulta más sencillo considerar muchos años en un solo bloque. Mientras que cuando el compositor se acerca más a nuestro periodo histórico, podemos conocerlo mejor y notar las diferencias sutiles y el conocimiento de estos detalles ayudan a no mezclarlo todo.

Se sabe muy poco de los compositores de las primeras eras. Entre los pocos de los que tenemos noticia se encuentra Mesomedes de Creta, un compositor griego de comienzos del siglo II. Los primeros intentos de fijar la altura de los sonidos tuvieron lugar en el siglo IX. Primitivamente la música se conservaba por tradición oral y no se escribía. Del siglo IX nos quedan referencias de la fama de Ziryab, cuyas innovaciones musicales tuvieron también una fuerte influencia en Hispania.

A pesar de que siempre existieron compositores de música, no tenemos registro de sus nombres hasta la época medieval, cuando en el siglo XI aparecieron los trovadores, que eran músicos y poetas, es decir, escribían la letra y componían la música. Se llamaban ministriles a los encargados de acompañar a los trovadores mediante instrumentos musicales.

Uno de los primeros trovadores de los que tenemos referencia en Europa es Guillermo de Poitiers, duque de Aquitania, quien vivió entre 1089 y 1127. Los trovadores del norte de Francia se llamaban troveros y los alemanes "Minnesänger", que significa cantores de amor. En Alemania existían los maestros cantores. Entre los más conocidos figuran Hans Sachs y Enrique von Meissen.

Entre el siglo XII y el siglo XIII aparecieron Magister Alberto, Leonino o Roberto de Sabilon, todos ellos compositores de Ars antiqua. A partir de Perotino comenzó a usarse el motete primitivo. A partir del siglo XIII surge el Ars nova. Sus representantes más conocidos fueron Guillermo de Machaut, Giovanni da Firenze (fl. 1340–50), quien también aparece como Giovanni da Cascia, Jovannes de Cascia, Johannes de Florentia o Maestro Giovanni da Firenze Jacopo da Bologna y Francesco Landino.

Debido al humanismo y a la creciente importancia del individuo, a partir del Renacimiento cobraron mayor importancia los compositores por sí mismos. Tenemos registro de la fama de Giovanni Pierluigi da Palestrina, el más célebre compositor de polifonía religiosa italiana de esa época. También era conocido Orazio Vecchi, el compositor de la comedia madrigalesca "Amfiparnasso", el organista Claudio Merulo, Giovanni Gabrieli y Andrea Gabrieli, compositores de la Basílica de San Marcos en Venecia.

En España se recuerda a Cristóbal de Morales, Francisco Guerrero y Tomás Luis de Victoria. En Inglaterra, durante el siglo XVI, William Byrd, Orlando Gibbons, John Bull y Thomas Morley. En los Países Bajos, Orlando di Lasso, Josquin des Pres, Adrian Willaert y Philippe de Monte.

A partir de 1500 ya los compositores se hacen famosos y sus nombres perduran sin problema en la historia, las composiciones no se consideran grupales, sino producto del genio de un solo individuo.

A lo largo de la historia de la música han existido algunos compositores célebres por legarnos una extensa obra musical de gran calidad, a continuación una lista con algunos de ellos.

•Antonio Vivaldi
•Johann Sebastian Bach
•Wolfgang Amadeus Mozart
•Ludwig van Beethoven
•Franz Liszt
•Franz Schubert
•Frédéric Chopin
•Sergey Rachmaninoff
•Piotr Ilich Tchaikovsky
•Maurice Ravel
•Gustav Holst
•Edvard Grieg
•Claude Debussy
•Erik Satie
•Dmitri Shostakovich
•Charles Valentin Alkan
•Camille Saint-Saëns
•Niccolò Paganini
•Alexander Scriabin
•Serguey Prokofiev
•Carl Orff
•Antonin Dvorak Richard Wagner Arnold Schönberg



</doc>
<doc id="758" url="https://es.wikipedia.org/wiki?curid=758" title="Cultura">
Cultura

Cultura (del latín "cultūra") es un término que tiene muchos significados interrelacionados. Por ejemplo, en 1952, Alfred Kroeber y Clyde Kluckhohn recopilaron una lista de 164 definiciones de "cultura" en "Cultura: una reseña crítica de conceptos y definiciones"; y han clasificado más de 250 distintas.
En el uso cotidiano, la palabra "cultura" se emplea para dos conceptos diferentes: 

Cuando el término surgió en Europa, entre los siglos XVIII y XIX, se refería a un proceso de cultivación o mejora, como en la agricultura u horticultura. En el siglo XIX, pasó primero a referirse al mejoramiento o refinamiento de lo individual, especialmente a través de la educación, y luego al logro de las aspiraciones o ideales nacionales. A mediados del siglo XIX, algunos científicos utilizaron el término «cultura» para referirse a la capacidad humana universal. Para el antipositivista y sociólogo alemán Georg Simmel, la cultura se refería a «la cultivación de los individuos a través de la injerencia de formas externas que han sido objetificadas en el transcurso de la historia».

En el siglo XX, la «cultura» surgió como un concepto central de la antropología, abarcando todos los fenómenos humanos que no son el total resultado de la genética. Específicamente, el término «cultura» en la antropología americana tiene dos significados: (1) la evolucionada capacidad humana de clasificar y representar las experiencias con símbolos y actuar de forma imaginativa y creativa; y (2) las distintas maneras en que la gente vive en diferentes partes del mundo, clasificando y representando sus experiencias y actuando creativamente. Después de la Segunda Guerra Mundial, el término se volvió importante, aunque con diferentes significados, en otras disciplinas como estudios culturales, psicología organizacional, sociología de la cultura y estudios gerenciales.

Algunos etólogos han hablado de «cultura» para referirse a costumbres, actividades o comportamientos transmitidos de una generación a otra en grupos de animales por imitación consciente de dichos comportamientos.
Las creencias y prácticas de una cultura determinada pueden ser ejercidas como mecanismos de control que limitan la conducta social. La cultura se asocia con la libertad, ya que es el vehículo entre el conocimiento y nuevas formas de conciencia que permiten una desestabilización en la hegemonía. Además puede reconocerse como conjuntos o modos de vida y costumbres de una época o grupo social. El término cultura puede alcanzar extensión y usos diversos, como diversidad cultural, objeto del conocimiento empírico, y la diferencia cultural. 

Otros conceptos de cultura:



La etimología del concepto moderno “cultura” tiene un origen antiguo. En varias lenguas europeas, la palabra “cultura” está basada en el término latino utilizado por Cicerón, en su "Tusculanae Disputationes", quien escribió acerca de una cultivación del alma o "“cultura animi”", para entonces utilizando una metáfora agrícola para describir el desarrollo de un alma filosófica, que fue comprendida teleológicamente como uno de los ideales más altos posibles para el desarrollo humano. Samuel Pufendorf llevó esta metáfora a un concepto moderno, con un significado similar, pero ya sin asumir que la filosofía es la perfección natural del hombre. Para este autor, los significados de cultura, que muchos escritores posteriores retoman, “se refieren a todas las formas en la que los humanos comienzan a superar su barbarismo original y, a través de artificios, se vuelven completamente humanos”.

Como lo describe Velkley: El término “cultura”, que originalmente significaba la cultivación del alma o la mente, adquiere la mayoría de sus posteriores significados en los escritos de los pensadores alemanes del siglo XVIII, quienes en varios niveles desarrollaron la crítica de Rousseau al liberalismo moderno y la Ilustración. Además, un contraste entre “cultura” y “civilización” está usualmente implícito por estos autores, aun cuando no lo expresen así. Dos significados primarios de cultura surgen de este período: cultura como un espíritu folclórico con una identidad única, y cultura como la cultivación de la espiritualidad o la individualidad libre. El primer significado es predominante dentro de nuestro uso actual del término “cultura”, pero el segundo juega todavía un importante rol en lo que creemos debería lograr la cultura, como la “expresión” plena del ser único y “auténtico”.

El término cultura proviene del latín "cultus" que a su vez deriva de la voz "colere" que significa cuidado del campo o del ganado. Hacia el siglo XIII, el término se empleaba para designar una parcela cultivada, y tres siglos más tarde había cambiado su sentido como estado de una cosa, al de la acción: el cultivo de la tierra o el cuidado del ganado (Cuche, 1999: 10), aproximadamente en el sentido en que se emplea en el español de nuestros días en vocablos como agricultura, apicultura, piscicultura y otros. Por la mitad del siglo XVI, el término adquiere una connotación metafórica, como el cultivo de cualquier facultad. De cualquier manera, la acepción figurativa de cultura no se extenderá hasta el siglo XVII, cuando también aparece en ciertos textos académicos.

El Siglo de las Luces (siglo XVIII) es la época en que el sentido figurado del término como “cultivo del espíritu” se impone en amplios campos académicos. Por ejemplo, el "Dictionnaire de l'Académie Française" de 1718. Y aunque la "Enciclopedia" lo incluye solo en su sentido restringido de cultivo de tierras, no desconoce el sentido figurado, que aparece en los artículos dedicados a la literatura, la pintura, la filosofía y las ciencias. Con el paso del tiempo, como cultura se entenderá la formación de la mente. Es decir, se convierte nuevamente en una palabra que designa un estado, aunque en esta ocasión es el estado de la mente humana, y no el estado de las parcelas.

La clásica oposición entre cultura y naturaleza también tiene sus raíces en esta época. En 1798, el "Dictionnaire" incluye una acepción de cultura en que se estigmatiza el “espíritu natural”. Para muchos de los pensadores de la época, como Jean Jacques Rousseau, la cultura es un fenómeno distintivo de los seres humanos, que los coloca en una posición diferente a la del resto de animales. La cultura es el conjunto de los conocimientos y saberes acumulados por la humanidad a lo largo de sus milenios de historia. En tanto una característica universal (el vocablo), se emplea en número singular, puesto que se encuentra en todas las sociedades sin distinción de etnias, ubicación geográfica o momento histórico.

También es en el contexto de la Ilustración cuando surge otra de las clásicas oposiciones en que se involucra a la cultura, esta vez, como sinónimo de la civilización. Esta palabra aparece por primera vez en la lengua francesa del siglo XVIII, y con ella se significaba la refinación de las costumbres. Civilización es un término relacionado con la idea de progreso. Según esto, la civilización es un estado de la Humanidad en el cual la ignorancia ha sido abatida y las costumbres y relaciones sociales se hallan en su más elevada expresión. La civilización no es un proceso terminado, es constante, e implica el perfeccionamiento progresivo de las leyes, las formas de gobierno, el conocimiento. Como la cultura, también es un proceso universal que incluye a todos los pueblos, incluso a los más atrasados en la línea de la evolución social. Desde luego, los parámetros con los que se medía si una sociedad era más civilizada o más salvaje eran los de su propia sociedad. En los albores del siglo XIX, ambos términos, cultura y civilización eran empleados casi de modo indistinto, sobre todo en francés e inglés (Thompson, 2002: 186).

Es necesario señalar que no todos los intelectuales franceses emplearon el término. Rousseau y Voltaire se mostraron reticentes a esta concepción progresista de la historia. Intentaron proponer una versión más relativista de la historia, aunque sin éxito, pues la corriente dominante era la de los progresistas. No fue en Francia, sino en Alemania donde las posturas relativistas ganaron mayor prestigio. El término "Kultur" en sentido figurado aparece en Alemania hacia el siglo XVII -aproximadamente con la misma connotación que en francés. Para el siglo XVIII goza de gran prestigio entre los pensadores burgueses alemanes. Esto se debió a que fue empleado para denostar a los aristócratas, a los que acusaban de tratar de imitar las maneras "“civilizadas”" de la corte francesa. Por ejemplo, Immanuel Kant apuntaba que “nos cultivamos por medio del arte y de la ciencia, nos civilizamos [al adquirir] buenos modales y refinamientos sociales” (Thompson, 2002: 187). Por lo tanto, en Alemania el término civilización fue equiparado con los valores cortesanos, calificados de superficiales y pretenciosos. En sentido contrario, la cultura se identificó con los valores profundos y originales de la burguesía (Cuche, 1999:13).

En el proceso de crítica social, el acento en la dicotomía cultura/civilización se traslada de las diferencias entre estratos sociales a las diferencias nacionales. Mientras Francia era el escenario de una de las revoluciones burguesas más importantes de la historia, Alemania estaba fragmentada en múltiples Estados. Por ello, una de las tareas que se habían propuesto los pensadores alemanes era la unificación política. La unidad nacional pasaba también por la reivindicación de las especificidades nacionales, que el universalismo de los pensadores franceses pretendía borrar en nombre de la civilización. Ya en 1774, Johann Gottfried Herder proclamaba que el genio de cada pueblo ("Volksgeist") se inclinaba siempre por la diversidad cultural, la riqueza humana y en contra del universalismo. Por ello, el orgullo nacional radicaba en la cultura, a través de la que cada pueblo debía cumplir un destino específico. La cultura, como la entendía Herder, era la expresión de la humanidad diversa, y no excluía la posibilidad de comunicación entre los pueblos.

Durante el siglo XIX, en Alemania el término cultura evoluciona bajo la influencia del nacionalismo. Mientras tanto, en Francia, el concepto se amplió para incluir no solo el desarrollo intelectual del individuo, sino el de la humanidad en su conjunto. De aquí, el sentido francés de la palabra presenta una continuidad con el de civilización: no obstante la influencia alemana, persiste la idea de que más allá de las diferencias entre “cultura alemana” y “cultura francesa” (por poner un ejemplo), hay algo que las unifica a todas: la cultura humana.

Para efecto de las ciencias sociales, las primeras acepciones de cultura fueron construidas a finales del siglo XIX. Por esta época, la sociología y la antropología eran disciplinas relativamente nuevas, y la pauta en el debate sobre el tema que aquí nos ocupa la llevaba la filosofía. Los primeros sociólogos, como Émile Durkheim, rechazaban el uso del término. Hay que recordar que en su perspectiva, la ciencia de la sociedad debía abordar problemas relacionados con la estructura social. Si bien es opinión generalizada que Karl Marx dejó de lado a la cultura, ello se ve refutado por las mismas obras del autor, sosteniendo que las relaciones sociales de producción (la organización que adoptan los seres humanos para el trabajo y la distribución social de sus frutos) constituyen la base de la superestructura jurídico-política e ideológica, pero en ningún caso un aspecto secundario de la sociedad. No es concebible una relación social de producción sin reglas de conducta, sin discursos de legitimación, sin prácticas de poder, sin costumbres y hábitos permanentes de comportamiento, sin objetos valorados tanto por la clase dominante como por la clase dominada. El desvelo de las obras juveniles de Marx, tanto de La ideología alemana (1845-1846) en 1932 por la célebre edición del Instituto Marx-Engels de la URSS bajo dirección de David Riazanov, como de los Manuscritos económicos y filosóficos (1844) posibilitó que varios partidarios de sus propuestas teóricas desarrollaran una teoría de la cultura marxista (véase más adelante).

El significado de cultura generalmente es relacionado con la antropología. Una de las ramas más importantes de esta disciplina social se encarga precisamente del estudio comparativo de la cultura. Quizá por la centralidad que la palabra tiene en la teoría de la antropología, el término ha sido desarrollado de diversas maneras, que suponen el uso de una metodología analítica basada en premisas que en ocasiones distan mucho las unas de las otras. Fue Franz Boas, frente a esta empresa etnocentrista, quien opera el gran cambio epistemológico en la antropología. A partir de Boas, padre del relativismo cultural, el antropólogo se hace traductor, pudiendo entrar en la cosmovisión del estudiado y entender el mundo de sus significaciones. Ese mundo de la significación, irá conduciendo, lentamente, hacia un concepto que no es antropológico, y es el de la construcción social del sentido. Ante este aumento de la sensibilidad vinculada con las cuestiones del lenguaje, se desarrollarán disciplinas nuevas, vinculadas con el mundo de la significación humana y del lenguaje, que completarán la idea de la cultura entendida desde el mundo de la significación.

De acuerdo con la Declaración Universal sobre la Diversidad Cultural de la UNESCO "la cultura debe ser considerada como el conjunto de los rasgos distintivos espirituales y materiales, intelectuales y afectivos que caracterizan a una sociedad o a un grupo social y que abarca, además de las artes y las letras, los modos de vida, las maneras de vivir juntos, los sistemas de valores, las tradiciones y las creencias" 

Los etnólogos y antropólogos británicos y estadounidenses de las postrimerías del siglo XIX retomaron el debate sobre el contenido de "cultura". Estos autores tenían casi siempre una formación profesional en derecho, pero estaban particularmente interesados en el funcionamiento de las sociedades exóticas con las que Occidente se encontraba en ese momento. En la opinión de estos pioneros de la etnología y la antropología social (como Bachoffen, McLennan, Maine y Morgan), la cultura es el resultado del devenir histórico de la sociedad. Pero la historia de la humanidad en estos escritores era fuertemente deudora de las teorías ilustradas de la civilización, y sobre todo, del darwinismo social de Spencer.

Como señala Thompson (2002:190), la definición descriptiva de cultura se encontraba presente en esos primeros autores de la antropología decimonónica. El interés principal en la obra de estos autores (que abordaba problemáticas tan disímbolas como el origen de la familia y el matriarcado, y las supervivencias de culturas antiquísimas en la civilización occidental de su tiempo) era la búsqueda de los motivos que llevaban a los pueblos a comportarse de tal o cual modo. En esas exploraciones, meditarente, o entre la tecnología y el resto del sistema social.

Uno de los más importantes etnógrafos de la época fue Gustav Klemm. En los diez tomos de su obra "Allgemeine Kulturgeschichte der Menschheit" (1843-1852) intentó mostrar el desarrollo gradual de la humanidad por medio del análisis de la tecnología, costumbres, arte, herramientas, prácticas religiosas. Una obra monumental, pues incluía ejemplos etnográficos de pueblos de todo el mundo. El trabajo de Klemm habría de tener eco en sus contemporáneos, empeñados en definir el campo de una disciplina científica que estaba naciendo. Unos veinte años más tarde, en 1871, Edward B. Tylor publicó en "Primitive Culture" una de las definiciones más ampliamente aceptadas de cultura. Según Tylor, la cultura es:

De esta suerte, uno de los principales aportes de Tylor fue la elevación de la cultura como materia de estudio sistemático. A pesar de este notable avance conceptual, la propuesta de Tylor adolecía de dos grandes debilidades. Por un lado, sacó del concepto su énfasis humanista al convertir a la cultura en objeto de ciencia. Por el otro, su procedimiento analítico era demasiado descriptivo. En el texto citado arriba, Tylor plantea que “un primer paso para el estudio de la civilización consiste en diseccionarla en detalles, y clasificar éstos en los grupos adecuados” (Tylor, 1995:33). Según esta premisa, la mera recopilación de los “detalles” permitiría el conocimiento de una cultura. Una vez conocida, sería posible clasificarla en una graduación de más a menos civilizada, premisa que heredó de los darwinistas sociales.

La propuesta teórica de Tylor fue retomada y reelaborada posteriormente, tanto en Gran Bretaña como en Estados Unidos. En este último país, la antropología evolucionaba hacia una posición relativista, representada en primera instancia por Franz Boas. Esta posición representaba un rompimiento con las ideas anteriores sobre la evolución cultural, en especial las propuestas por los autores británicos y el estadounidense Lewis Henry Morgan. Para este último, contra quien Boas dirigió sus críticas en uno de sus pocos textos teóricos, el proceso de la evolución social humana (tecnología, relaciones sociales y cultura) podía ser equiparado con el proceso de crecimiento de un individuo de la especie. Por lo tanto, Morgan comparaba el salvajismo con la “infancia de la especie humana”, y la civilización, con la madurez. Boas fue sumamente duro con las propuestas de Morgan y el resto de los antropólogos evolucionistas contemporáneos. A lo que sus autores llamaban “teorías” sobre la evolución de la sociedad, Boas las calificó de “puras conjeturas” sobre el ordenamiento histórico de “fenómenos observados conforme a principios admitidos [de antemano]” (1964:184).

La crítica de Boas en contra de los evolucionistas es un eco de la perspectiva de los filósofos alemanes como Herder y Wilhelm Dilthey. El núcleo de la propuesta radica en su inclinación a considerar la cultura como un fenómeno plural. En otras palabras, más que hablar de cultura, Boas hablaba de "culturas". Para la mayor parte de los antropólogos y etnólogos adscritos a la escuela culturalista estadounidense, el estado del arte etnográfico al principio del siglo XX no permitía la conformación de una teoría general sobre la evolución de las culturas. Por lo tanto, la labor más importante de los estudiosos del fenómeno debía ser la documentación etnográfica. De hecho, Boas escribió muy pocos textos teóricos, en comparación con sus monografías sobre los pueblos indígenas de la costa pacífica de América del Norte.

Los antropólogos formados por Robin Reid hubieron de heredar muchas de las premisas de su maestro. Entre otros casos notables, están el de Ruth Benedict. En su obra "Patterns of culture" (1939), Benedict señala que cada cultura es un todo comprensible solo en sus propios términos y constituye una suerte de matriz que da sentido a la actuación de los individuos en una sociedad. Alfred Kroeber, retomando la oposición entre cultura y naturaleza, también señalaba que las culturas son fenómenos "sui generis" pero, en sentido estricto, eran de una categoría exterior a la naturaleza. Por lo tanto, según Kroeber, el estudio de las culturas debía salirse del dominio de las ciencias naturales y encarar a las primeras como lo que eran: fenómenos superorgánicos. Melville Herskovits y Clyde Kluckhohn retomaron de Tylor su definición cientificista del estudio de la cultura. Para el primero, también la recolección de rasgos definitorios de las culturas permitiría su clasificación. Aunque, en este caso, la clasificación no se realizaba en sentido diacrónico, sino espacial-geográfico que habría de permitir el conocimiento de las relaciones entre los diferentes pueblos asentados en un área cultural. Kluckhonn, por su parte, resume en su texto "Antropología" la mayor parte de los postulados vistos en esta sección, y reclama el dominio de lo cultural como el campo específico de la actividad antropológica.

Por su parte Javier Rosendo describe la cultura como el conjunto de rasgos que caracterizan a una región o grupo de personas, con respecto al resto, que puede ir cambiando de acuerdo a la época en la cual se vive. Estos rasgos pueden abarcar la danza, tradiciones, arte, vestuario y religión.

La característica más peculiar del concepto funcionalista de cultura se refiere precisamente a la función social de la misma. El supuesto básico es que todos los elementos de una sociedad (entre los que la cultura es uno más) existen porque son necesarios. Esta perspectiva ha sido desarrollada tanto en antropología como en sociología aunque, sin duda, sus primeras características fueron delineadas involuntariamente por Émile Durkheim. Este sociólogo francés muy pocas veces empleó el término como unidad analítica principal de su disciplina. En su libro "Las reglas del método sociológico" (1895), plantea que la sociedad está compuesta por entidades que tienen una función específica, integradas en un sistema análogo al de los seres vivos, donde cada órgano está especializado en el cumplimiento de una función vital. Del mismo modo en que los órganos de un cuerpo son susceptibles a la enfermedad, las instituciones y costumbres, las creencias y las relaciones sociales también pueden caer en un estado de anomia. Durkheim y sus seguidores, sin embargo, no se ocupan exclusiva ni principalmente de la cultura como objeto de estudio, sino de hechos sociales. A pesar de ellos, sus propuestas analíticas fueron retomadas por autores conspicuos de la antropología social británica y la sociología de la cultura de Estados Unidos.

Más tarde, el polaco Bronislaw Malinowski retomó tanto la descripción de cultura de Tylor como algunos de los planteamientos de Durkheim relativos a la función social. Para Malinowski, la cultura podía ser entendida como una «realidad "sui generis"» que debía estudiarse como tal (en sus propios términos). En la categoría de cultura incluía artefactos, bienes, procesos técnicos, ideas, hábitos y valores heredados (Thompson, 2002: 193). También consideraba que la estructura social podía ser entendida análogamente a los organismos vivos pero, a diferencia de Durkheim, Malinowski tenía una tendencia más holística. Malinowski creía que todos los elementos de la cultura poseían una función que les daba sentido y hacía posible su existencia. Pero esta función no era dada únicamente por lo social, sino por la historia del grupo y el entorno geográfico, entre muchos otros elementos. El reflejo más claro de este pensamiento aplicado al análisis teórico fue el libro "Los argonautas del Pacífico Occidental" (1922), una extensa y detallada monografía sobre las distintas esferas de la cultura de los isleños trobriandeses, un pueblo que habitaba en las islas Trobriand, al oriente de Nueva Guinea.

Años más tarde, Alfred Reginald Radcliffe-Brown, también antropólogo británico, retomaría algunas de las propuestas de Malinowski, y muy especialmente las que se referían a la función social. Radcliffe-Brown rechazaba que el campo de análisis de la antropología fuera la cultura, más bien se encargaba del estudio de la estructura social, un entramado de relaciones entre las personas de un grupo. Sin embargo, también analizó aquellas categorías que habían sido descritas con anterioridad por Malinowski y Tylor, siguiendo siempre el principio del análisis científico de la sociedad. En su libro "Estructura y función en la sociedad primitiva" (1975) Radcliffe-Brown establece que la función más importante de las creencias y prácticas sociales es la del mantenimiento del orden social, el equilibrio en las relaciones y la trascendencia del grupo en el tiempo. Sus propuestas fueron retomadas más tarde por muchos de sus alumnos, especialmente por Edward Evan Evans-Pritchard etnógrafo de los nuer y los azande, pueblos del centro de África. En ambos trabajos etnográficos, la función reguladora de las creencias y prácticas sociales está presente en el análisis de esas sociedades, a la primera de las cuales, Evans-Pritchard llamó “anarquía ordenada”.

Los orígenes de las concepciones simbólicas de cultura se remontan a Leslie White, antropólogo estadounidense formado en la tradición culturalista de Boas. A pesar de que en su libro "La ciencia de la cultura" afirma, en un principio, que esta es «el nombre de un tipo preciso o clase de fenómenos, es decir, las cosas y los sucesos que dependen del ejercicio de una habilidad mental, exclusiva de la especie humana, que hemos llamado 'simbolizante'», en el transcurso de su texto, White irá abandonando la idea de la cultura como símbolos para orientarse hacia una perspectiva ecológica.

El estructuralismo es una corriente más o menos extendida en las ciencias sociales. Sus orígenes se remontan a Ferdinand de Saussure, lingüista, quien propuso "grosso modo" que la lengua es un sistema de signos. Tras su "conversión" a la antropología (tal como la llama en "Tristes trópicos"), Claude Lévi-Strauss –influido por Roman Jakobson– habría de retomar este concepto para el estudio de los hechos de interés antropológico, entre los que la cultura era solo uno más. De acuerdo con Lévi-Strauss, la cultura es básicamente un sistema de signos producidos por la actividad simbólica de la mente humana (tesis que comparte con White).

En "Antropología estructural" (1958) Lévi-Strauss irá definiendo las relaciones que existen entre los signos y símbolos del sistema, y su función en la sociedad, sin prestar demasiada atención a este último punto. En resumen, se puede decir que en la teoría estructuralista, la cultura es un mensaje que puede ser decodificado tanto en sus contenidos, como en sus reglas. El mensaje de la cultura habla de la concepción del grupo social que la crea, habla de sus relaciones internas y externas. En "El pensamiento salvaje" (1962), Lévi-Strauss apunta que todos los símbolos y signos de que está hecha la cultura son productos de la misma capacidad simbólica que poseen todas las mentes humanas. Esta capacidad, básicamente consiste en la clasificación de las cosas del mundo en grupos, a los que se atribuyen ciertas cargas semánticas. No existe un grupo de símbolos o signos (campo semántico) que no tenga uno complementario. Los signos y sus significados pueden ser asociados por metáfora (como en el caso de las palabras) o metonimia (como en el caso de los emblemas de la realeza) a fenómenos significativos para el grupo creador del sistema cultural. Las asociaciones simbólicas no necesariamente son las mismas en todas las culturas. Por ejemplo, mientras en la cultura occidental, el rojo es el color del amor, en Mesoamérica es el de la muerte.

Según la propuesta estructuralista, las culturas de los pueblos “primitivos” y “civilizados” están hechas de la misma materia y, por tanto, los sistemas del conocimiento del mundo exterior dominantes en cada uno —magia en los primeros, ciencia en los segundos—– no son radicalmente diferentes. Aunque son varias las distinciones que se pueden establecer entre culturas primitivas y modernas: una de las más importantes es el modo en que manipulan los elementos del sistema. En tanto que la magia improvisa, la ciencia procede sobre la base del método científico. El uso del método científico no quiere decir —según Lévi-Strauss— que las culturas donde la ciencia es dominante sean superiores, o que aquellas donde la magia juega un papel fundamental sean menos rigurosas o metódicas en su manera de conocer el mundo. Simplemente, son de índole distinta unas de otras, pero la posibilidad de comprensión entre ambos tipos de culturas radica básicamente en una facultad universal del género humano.

En la perspectiva estructuralista, el papel de la historia en la conformación de la cultura de una sociedad no es tan importante. Lo fundamental es llegar a dilucidar las reglas que subyacen en la articulación de los símbolos en una cultura, y observar la manera en que estos dotan de sentido la actuación de una sociedad. En varios textos, Lévi-Strauss y sus seguidores (como Edmund Leach) parecen insinuar, como Ruth Benedict, que la cultura es una suerte de patrón que pertenece a todo el grupo social pero no se encuentra en nadie en particular. Esta idea también fue retomada del concepto de lenguaje propuesto por Saussure.

La antropología simbólica es una rama de las ciencias sociales cuyo desarrollo se relaciona con la crítica al estructuralismo lévi-straussiano. Uno de los principales exponentes de esta corriente es Clifford Geertz. Comparte con el estructuralismo francés la tesis de la cultura como un sistema de símbolos pero, a diferencia de Lévi-Strauss, Geertz señala que no es posible para los investigadores el conocimiento de sus contenidos:

Bajo la premisa anterior, Geertz y la mayor parte de los antropólogos simbólicos ponen en duda la autoridad de la etnografía. Señalan que a lo que pueden limitarse los antropólogos es a hacer “interpretaciones plausibles” del significado de la trama simbólica que es la cultura, a partir de la descripción densa de la mayor cantidad de puntos de vista que sea posible conocer respecto a un mismo suceso. En otro sentido, los simbólicos no creen que todos los elementos de la trama cultural posean el mismo sentido para todos los miembros de una sociedad. Más bien creen que pueden ser interpretados de modos diferentes, dependiendo, ya de la posición que ocupen en la estructura social, ya de condicionamientos sociales y psíquicos anteriores, o bien, del mismo contexto.

Tal como se señaló anteriormente, Karl Marx a pesar de la opinión generalizada, puso atención en el análisis de las cuestiones culturales, específicamente en su relación con el resto de la estructura social. Según la propuesta teórica de Marx, el dominio de lo cultural (constituido sobre todo por la ideología) es un reflejo de las relaciones sociales de producción, es decir, de la organización que adoptan los seres humanos frente a la actividad económica. La gran aportación del marxismo en el análisis de la cultura es que esta es entendida como el producto de las relaciones de producción, como un fenómeno que no está desligado del modo de producción de una sociedad. Asimismo, la considera como uno de los medios por los cuales se reproducen las relaciones sociales de producción, que permiten la permanencia en el tiempo de las condiciones de desigualdad entre las clases.

En sus interpretaciones más simplistas, la definición de la ideología en Marx ha dado lugar a una tendencia a explicar las creencias y el comportamiento social en función de las relaciones que se establecen entre quienes dominan el sistema económico y sus subalternos. Sin embargo, son muchas las posturas donde la relación entre la base económica y la superestructura cultural es analizada en enfoques más amplios. Por ejemplo, Antonio Gramsci llama la atención a la hegemonía, un proceso por medio del cual, un grupo dominante se legitima ante los dominados, y estos terminan por ver natural y asumir como deseable la dominación. Louis Althusser propuso que el ámbito de la ideología (el principal componente de la cultura) es un reflejo de los intereses de la élite, y que a través de los aparatos ideológicos del Estado se reproducen en el tiempo.

Así mismo, Michel Foucault –en el conocido debate de noviembre de 1971 en Holanda con Noam Chomsky– respondiendo la pregunta de que si la sociedad capitalista era democrática, además de contestar negativamente –argumentando que una sociedad democrática se basa en el efectivo ejercicio del poder por una población que no esté dividida u ordenada jerárquicamente en clases– sostiene que, de manera general, todos los sistemas de enseñanza –los cuales aparecen simplemente como transmisores de conocimientos aparentemente neutrales–, están hechos para mantener a cierta clase social en el poder, y excluir de los instrumentos de poder a otras clases sociales.

Si bien el estudio de la cultura nació como una inquietud por el cambio de las sociedades a lo largo del tiempo, el desprestigio en el que cayeron los primeros autores de la antropología fue un terreno fértil para que arraigaran en la reflexión sobre la cultura las concepciones ahistóricas. Salvo los marxistas, interesados en el proceso revolucionario hacia el socialismo, el resto de las disciplinas sociales no prestaron mayor atención al problema de la evolución cultural.

Para introducir las definiciones neoevolucionistas de cultura, es necesario recordar que los evolucionistas sociales de finales del siglo XIX (representados, entre otros, por Tylor), pensaban que las sociedades “primitivas” de su época eran residuos de antiguas formas culturales, por las que necesariamente habría pasado la civilización de Occidente antes de llegar a ser lo que era en ese momento. Como se indicó antes, Boas y sus discípulos echaron por tierra estos argumentos, señalando que nada probaba la veracidad de estas suposiciones. Sin embargo, en Estados Unidos, hacia la década de 1940 tuvo lugar un nuevo viraje del enfoque temporal de la antropología. Este nuevo rumbo es el neoevolucionista, interesado entre otras cosas, por el cambio socio-cultural y las relaciones entre cultura y medio ambiente.

Según el neoevolucionismo, la cultura es el producto de las relaciones históricas entre un grupo humano y su medio ambiente. De esta manera se pueden resumir las definiciones de cultura propuestas por Leslie White (1992) y Julian Steward (1992), quienes encabezaron la corriente neoevolucionista en su nacimiento. El énfasis de la nueva corriente antropológica se movió del funcionamiento de la cultura a su carácter dinámico. Este cambio de paradigma representa una clara oposición al funcionalismo estructuralista, interesado en el funcionamiento actual de la sociedad; y el culturalismo, que aplazaba el análisis histórico para un momento en que los datos etnográficos lo permitieran.

Tanto Steward como White concuerdan en que la cultura es solo uno de los ámbitos de la vida social. Para White, la cultura no es un fenómeno que deba entenderse en sus propios términos, como proponían los culturalistas. El aprovechamiento energético es el motor de las transformaciones culturales: estimula la transformación de la tecnología disponible, tendiendo siempre a mejorar. Así, la cultura está determinada por la forma en la que el grupo humano aprovecha su entorno. Este aprovechamiento se traduce a su vez en energía. El desarrollo de la cultura de un grupo es proporcional la cantidad de energía que la tecnología disponible le permite aprovechar. La tecnología determina las relaciones sociales y esencialmente la división del trabajo como una prístina forma de organización. A su vez, la estructura social y la división del trabajo se reflejan en el sistema de creencias del grupo, que formula conceptos que le permiten comprender el entorno que le rodea. Una modificación en la tecnología y la cantidad de energía aprovechada se traduce, por tanto, en modificaciones en todo el conjunto.

Steward, por su parte, retomaba de Kroeber la concepción de la cultura como un hecho que se encontraba por encima y fuera de la naturaleza. Sin embargo, Steward sostenía que había un diálogo entre ambos dominios. Opinaba que la cultura es un fenómeno o capacidad del ser humano que le permite adaptarse a su medio biológico. Uno de los principales conceptos en su obra es el de evolución. Steward planteaba que la cultura sigue un proceso de evolución multilineal (es decir, no todas las culturas pasan de un estado salvaje a la barbarie, y de ahí a la civilización), y que este proceso se basa en el desarrollo de tipos culturales derivados de las adaptaciones culturales al medio físico de una sociedad. Steward introduce en las ciencias sociales el término de ecología, señalando con él: el análisis de las relaciones existentes entre todos los organismos que comparten un mismo nicho ecológico.

Dentro del tipo de ideas introducidas por White y Steward, cabe señalar el materialismo cultural propugnado por Marvin Harris y otros antropólogos estadounidenses. Esta corriente puede ser asimilada a una forma de ecofuncionalismo en el que se encajan ciertas divisiones introducidas por Marx. Para el materialismo cultural, entender la evolución cultural y la configuración de las sociedades depende básicamente de condiciones materiales, tecnológicas e infraestructurales. El materialismo cultural establece una triple división entre grupos de conceptos que atiende a su relación causal. Esos grupos se llaman: "infraestructura" (modo de producción, tecnología, condiciones geográficas, etc.), "estructura" (modo de organización social, estructura jerárquica, etc.) y "supraestructura" (valores religiosos y morales, creaciones artísticas, leyes, etc.).

Había por lo menos una gran distancia conceptual entre la propuesta de White y de Steward. El primero se inclinaba por el estudio de la cultura como fenómeno total, en tanto que el segundo se mantenía más proclive al relativismo. Por ello, entre las limitaciones que tuvieron que superar sus sucesores estuvo la de concatenar ambas posturas, para unificar la teoría de los estudios de la ecología cultural. De esta suerte, Marshall Sahlins propuso que la evolución cultural sigue dos direcciones. Por un lado, crea diversidad “a través de una modificación de adaptación: las nuevas formas se diferencian de las viejas. Por otra parte, la evolución genera progreso: las formas superiores surgen de las inferiores y las sobrepasan”.

La idea de que la cultura se transforma siguiendo dos líneas simultáneas fue desarrollada por Darcy Ribeiro, que introdujo el concepto de proceso civilizatorio para comprender las transformaciones de la cultura.

Con el tiempo, el neoevolucionismo sirvió como una de las principales bisagras entre las ciencias sociales y las ciencias naturales, especialmente como puente con la biología y la ecología. De hecho, su propia vocación como enfoque holístico le ha convertido en una de las corrientes más interdisciplinarias de las disciplinas que estudian la humanidad. A partir de la década de 1960, la ecología entró en una relación muy estrecha con los estudios culturales de corte evolutivo. Los biólogos habían descubierto que los seres humanos no son los únicos animales que poseen cultura: se habían encontrado indicios de ella entre algunos cetáceos, pero especialmente entre los primates. Roy Rappaport introdujo en la discusión de lo social la idea de que la cultura forma parte de la misma biología del ser humano, y que la evolución misma del ser humano se debe a la presencia de la cultura. Señalaba que:

Los nuevos descubrimientos en la etología (ciencia que estudia el comportamiento de los animales) animaron a muchos biólogos a intervenir en el debate sociológico de la cultura. Algunos de ellos buscaban establecer relaciones entre la cultura humana y las formas primitivas de cultura observadas, por ejemplo, entre los macacos de Japón. Uno de los ejemplos más conocidos es el de Sherwood Washburn, profesor de antropología de la Universidad de California. Al frente de un equipo multidisciplinario, emprendió la tarea de buscar cuáles eran los orígenes de la cultura humana. Como primera parte de su proyecto, analizó el comportamiento social de los primates superiores. En segundo lugar, suponiendo que los bosquimanos !kung eran los últimos reductos de las formas más primitivas de cultura humana, procedió al estudio de su cultura. La tercera etapa del programa de Washburn (en el que colaboraron Richard Lee e Irven de Vore, y que se prolongó durante la primera mitad de los años sesenta) fue proceder a la comparación de los resultados de ambas investigaciones, y especuló sobre esta base acerca de la importancia de la cacería en la construcción de la sociedad y la cultura.

Esta hipótesis fue presentada en un congreso llamado "Man, the Hunter", realizado en la Universidad de Chicago en 1966. Fuera porque la investigación se apoyaba en premisas sobre la evolución cultural que fueron desechadas desde los tiempos de Boas, o porque era una tesis que negaba la importancia de la mujer en la construcción de la cultura, la tesis de Washburn, Lee y De Vore no fue bien recibida.

Esta definición, atiende a la característica principal de la cultura, que es una obra estrictamente de creación humana, a diferencia de los procesos que realiza la naturaleza, por ejemplo, el movimiento de la tierra, las estaciones del año, los ritos de apareamiento de las especies, las mareas e incluso la conducta de las abejas que hacen sus panales, elaboran miel, se orientan para encontrar el camino de regreso pero, que a pesar de eso, no constituyen una cultura, pues todas las abejas del mundo hacen exactamente lo mismo, de manera mecánica, y no pueden cambiar nada. Exactamente lo contrario ocurren en el caso de las obras, ideas y actos humanos, ya que estos transforman o se agregan a la naturaleza, por ejemplo, el diseño de una casa, la receta de un dulce de miel o de chocolate, la elaboración de un plano, la simple idea de las relaciones matemáticas, son cultura y sin la creación humana no existirían por obra de la naturaleza.

En 1998, Jesús Mosterín publicó su libro ¡Vivan los animales!, donde explica qué es la cultura:

Una interesante definición de cultura fue formulada por Mat Fric: «Cultura es toda actividad que contribuye al desarrollo de la inteligencia».

La definición clásica de cultura en la Iglesia católica se encuentra en el concilio Vaticano II:

En la definición destacan dos aspectos: el poner al individuo al centro, siendo la cultura un producto del hombre y al servicio del hombre; y el conjugar la formación de cada persona a través de la cultura, con la contribución específica de una comunidad al progreso de la humanidad. Este concepto de cultura es la base para explicar el proceso de la inculturación o inserción de la Iglesia católica en una cultura y expresión del cristianismo en una nueva modalidad y culturalidad.

El concepto científico de cultura hizo uso desde el principio de ideas de la teoría de la información, de la noción de meme introducida por Richard Dawkins, de los métodos matemáticos desarrolladas en la genética de poblaciones por autores como Luigi Luca Cavalli-Sforza y de los avances en la compresión del cerebro y del aprendizaje. Diversos antropólogos, como William Durham, y filósofos, como Daniel Dennett y Jesús Mosterín, han contribuido decisivamente al desarrollo de la concepción científica de la cultura. Mosterín define la cultura como la información transmitida por aprendizaje social entre animales de la misma especie. Como tal, se contrapone a la naturaleza, es decir, a la información transmitida genéticamente. Si los memes son las unidades o trozos elementales de información adquirida, la cultura actual de un individuo en un momento determinado sería el conjunto de los memes presentes en el cerebro de ese individuo en ese momento. A su vez, la noción vaga de cultura de un grupo social es analizada por Mosterín en varias nociones precisas distintas, definidas todas ellas en función de los memes presentes en los cerebros de los miembros del grupo.

La industria cultural la define la UNESCO como aquella que produce y distribuye bienes o servicios culturales que, «considerados desde el punto de vista de su calidad, utilización o finalidad específicas, encarnan o transmiten expresiones culturales, independientemente del valor comercial que puedan tener. Las actividades culturales pueden constituir una finalidad de por sí, o contribuir a la producción de bienes y servicios culturales».

La importante aportación de la psicología humanista de, por ejemplo, Erik Erikson con una teoría psicosocial para explicar los componentes socioculturales del desarrollo personal.


Así, el ser humano tiene la facultad de enseñar al animal, desde el momento en que es capaz de entender su rudimentario aparato de gestos y sonidos, llevando a cabo nuevos actos de comunicación; pero los animales no pueden hacer algo parecido con nosotros. De ellos podemos aprender por la observación, como objetos, pero no mediante el intercambio cultural, es decir, como sujetos.

La cultura se clasifica, respecto a sus definiciones, de la siguiente manera:

La cultura puede también ser clasificada del siguiente modo:





La cultura forma todo lo que implica transformación y seguir un modelo de vida.
Los elementos de la cultura se dividen en:

a) Materiales: Son todos los objetos, en su estado natural o transformados por el trabajo humano, que un grupo esté en condiciones de aprovechar en un momento dado de su devenir histórico: tierra, materias primas, fuentes de energía, herramientas, utensilios, productos naturales y manufacturados, etcétera.

b) De organización: Son las formas de relación social sistematizadas, a través de las cuales se hace posible la participación de los miembros del grupo cuya intervención es necesaria para cumplir la acción. La magnitud y otras características demográficas de la población son datos importantes que deben tomarse en cuenta al estudiar los elementos de organización de cualquier sociedad o grupo.

c) De conocimiento: Son las experiencias asimiladas y sistematizadas que se elaboran, es decir los conocimientos, las ideas y las creencias que se acumulan y trasmiten de generación a generación y en el marco de las cuales se generan o incorporan nuevos conocimientos.

d) De conducta: Son los comportamientos o las pautas de conducta comunes a un grupo humano.

e) Simbólicos: Son los diferentes códigos que permiten la comunicación necesaria entre los participantes en los diversos momentos de una acción. El código fundamental es el lenguaje, pero hay otros sistemas simbólicos significativos que también deben ser compartidos para que sean posibles ciertas acciones y resulten eficaces.

f) Emotivos: que también pueden llamarse subjetivos. Son las representaciones colectivas, las creencias y los valores integrados que motivan a la participación y/o la aceptación de las acciones: la subjetividad como un elemento cultural indispensable.

Dentro de toda cultura hay dos elementos a tener en cuenta:

Los cambios culturales: son los cambios a lo largo del tiempo de todos o algunos de los elementos culturales de una sociedad (o una parte de la misma).

La cultura está basada en todos nosotros.




Un ejemplo claro de este arte hilemorfista actual es Lotty Rosenfeld, una artista visual chilena, adscrita al neo-vanguardismo. Quien por medio de sus obras modifica la realidad, llevando su arte a las calles y transformando la relación convencional que existe entre materia y forma. Tal es "“Un millar de cruces sobre el pavimento”" Que fue una obra hecha en la época de la dictadura militar chilena en la que Rosenfeld dibujaba una cruz utilizando las líneas de señalización de las calles y colocándose una línea transversal por cada desaparecido que había de la dictadura, haciendo una contra información a los medios tradicionales de ese momento que negaban la realidad en la que estaba consumido el país, y que no daban cifras o estadísticas de los desaparecidos que había.

Un elemento esencial de la cultura es el lenguaje, hay conceptos culturales dentro de los diferentes sistemas en los que encontramos características específicas de gramática y léxico. Por tanto, incluir la lingüística en el estudio de las culturas antiguas y contemporáneas es indispensable. El análisis de la cultura desde la lingüística en los últimos sesenta años ha evolucionado desde el pensamiento estructuralista hasta la variación cultural. Los lingüistas también han desarrollado una investigación sobre la comunicación intercultural, y recientemente han creado conceptos nuevos tales como multilingüismo y multiculturalismo para definir nuevos fenómenos culturales.



</doc>
<doc id="759" url="https://es.wikipedia.org/wiki?curid=759" title="Ciencias aplicadas">
Ciencias aplicadas

Las ciencias aplicadas utilizan el conocimiento científico de una o varias ramas de la ciencia para resolver problemas prácticos. Los campos de la ingeniería, por ejemplo, se acercan a lo que es la ciencia aplicada. Estas áreas prácticas del saber son vitales para el desarrollo de la tecnología. Su utilización en campos industriales se refiere generalmente desarrollo y uso.

Es decir la ciencia aplicada es un cuerpo de conocimiento en el que la investigación y el descubrimiento tienen su orientación directa a la práctica; esto son las ciencias que proporcionan el desarrollo de nuevas tecnologías, a saber: los algoritmos de acción para obtener el producto deseado. Aplicada es la disciplina científica que aplica el conocimiento científico existente para desarrollar sistemas más prácticos aplicados, como la tecnología o la invención.

Su concepto opuesto es el de ciencia fundamental, la investigación científica que se realiza para aumentar el conocimiento, sin fin práctico inmediato.

Las ciencias aplicadas son orientadas a la práctica del conocimiento obtenido en las ciencias fundamentales; estos conocimientos sirven directamente a las necesidades de la sociedad. Como resultado, proporciona una amplia gama de funcionamiento de las ciencias aplicadas. Debido al desarrollo de las disciplinas de las ciencias naturales en la ciencia fundamental, surge una colección de nuevos datos e información que permite ver, predecir y en algunos casos explicar y entender los fenómenos en el mundo, en particular, la ciencia aplicada puede aplicar la ciencia formal, como las estadísticas, las matemáticas y la medicina, lo que condujo a la formación de tales disciplinas como estadística aplicada, matemáticas aplicadas, medicina aplicada, etc. Junto con esto, el curso de formación y desarrollo de áreas relacionadas de la ciencia aplicada fue históricamente determinado, como, por ejemplo, psicología aplicada, ética aplicada, biomecánica aplicada, hasta la educación preescolar aplicada.

Existe una división tradicional de las ciencias en ciencias fundamentales y ciencias aplicadas. Las ciencias fundamentales investigan las leyes fundamentales que rigen el comportamiento y la interacción de las estructuras básicas de la naturaleza. Las investigaciones de las ciencias fundamentales se encuentran al borde entre lo conocido y lo inesperado, y conducen al descubrimiento científico. Por contraste, las ciencias aplicadas aplican los resultados de las ciencias fundamentales a la solución de problemas sociales y prácticos.

El propósito de la investigación fundamental es el conocimiento, es decir la representación objetiva y racional de la realidad, mientras que el propósito de la investigación aplicada es el conocimiento instrumental efectivo sobre un fragmento de realidad, diseñado para resolver un problema práctico específico.

Para la investigación fundamental la verdad del conocimiento sobre el mundo es el valor más alto; para la investigación aplicada el valor más alto es la efectividad tecnológica de la información sobre el mundo que no siempre coincide con su verdad, en el caso de la ciencia fundamental las perspectivas y el progreso de la investigación están determinados principalmente por la tarea de identificar e introducir racionalmente nuevas características del mundo, aún desconocidas. En la ciencia aplicada el curso de la investigación está determinado por la necesidad de resolver problemas tecnológicos específicos, por esta razón la novedad del conocimiento sobre el mundo en sí mismo aparece como un subproducto de la búsqueda de estas soluciones. El conocimiento obtenido en el marco de la investigación aplicada se fija, en primer lugar, como un medio para resolver un problema práctico local; el conocimiento a menudo se presenta en formas que no implican un uso cognitivo directo adicional, pero que tiene una aplicación práctica directa, por ejemplo, instrucción, metodología, receta tecnológica, etc.



</doc>
<doc id="760" url="https://es.wikipedia.org/wiki?curid=760" title="Clasificación">
Clasificación

Clasificación puede referirse a:


















</doc>
<doc id="762" url="https://es.wikipedia.org/wiki?curid=762" title="Clase">
Clase

Clase puede referirse a:













</doc>
<doc id="763" url="https://es.wikipedia.org/wiki?curid=763" title="Clasificación Unesco">
Clasificación Unesco

La clasificación Unesco (Nomenclatura Internacional de la Unesco para los campos de Ciencia y Tecnología), creada por dicho organismo, es un sistema de clasificación del conocimiento ampliamente usado en la ordenación de proyectos de investigación y de las tesis doctorales.

Los apartados se diferencian por niveles según el nivel de detalle en campos, disciplinas y subdisciplinas, que son codificados con dos, cuatro y seis dígitos respectivamente.


En el nivel de dos dígitos se clasifican los siguientes campos:




</doc>
<doc id="764" url="https://es.wikipedia.org/wiki?curid=764" title="Clasificación Unesco de 4 dígitos">
Clasificación Unesco de 4 dígitos

Ver también: Clasificación UNESCO y Clasificación UNESCO 6 dígitos.



</doc>
