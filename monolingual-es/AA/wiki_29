<doc id="5269" url="https://es.wikipedia.org/wiki?curid=5269" title="Chapapote">
Chapapote

El chapopote, también conocido por otros localismos como "chapapote", "pichi", "chapote", "fuel", "galipote", o galipó (o galipot) es un sinónimo utilizado para asfalto. La palabra proviene del náhuatl y es de uso corriente en México y otros países americanos; donde se ha usado desde la antigüedad para referirse al petróleo crudo, tanto al que llegaba a las costas por filtraciones naturales desde el subsuelo marino, así como de las filtraciones en tierra, de donde era tomado para diversos usos. Parece ser que en otras latitudes retoman el término deformado como chapapote para referirse específicamente a los derrames de petróleo en el océano.

El derrame provocado por el buque monocasco "Prestige" (13 de noviembre de 2002) desembocó en una gran marea negra en Galicia, que afectó también al resto de la costa norte de España (Asturias, Cantabria y País Vasco) y al sudoeste de Francia. El desastre ecológico movilizó a miles de voluntarios para limpiar las costas y dio lugar a la creación del movimiento "Nunca Máis".

Desde entonces, se populariza el término chapapote para referirse erróneamente al petróleo o aceite mineral soltado accidentalmente por los barcos hacia la costa, en ocasiones procedente de sus tanques de combustible o maquinaria, o por vertidos accidentales. 
Pero resulta igualmente confuso hablar de "chapapote" para referirse a los vertidos efectuados en cualquier costa, por dicho carácter de localismo. Así, en el País Vasco se hablaría de galipó o galipot.




</doc>
<doc id="5273" url="https://es.wikipedia.org/wiki?curid=5273" title="Cámara oscura">
Cámara oscura

La cámara oscura (literalmente "habitación oscura") es un instrumento óptico que permite obtener una proyección plana de una imagen externa sobre la zona interior de su superficie. Constituyó uno de los dispositivos ancestrales que condujeron al desarrollo de la fotografía. Los aparatos fotográficos actuales heredaron la palabra cámara de las antiguas cámaras oscuras. Consiste en una caja cerrada y un pequeño agujero por el que entra una mínima cantidad de luz que proyecta en la pared opuesta la imagen del exterior. Si se dota con papel fotográfico se convierte en una cámara fotográfica estenopeica.
Originalmente, consistía en una sala cerrada cuya única fuente de luz era un pequeño orificio practicado en uno de los muros, por donde entraban los rayos luminosos reflejando los objetos del exterior en una de sus paredes. El orificio funciona como una lente convergente y proyecta, en la pared opuesta, la imagen del exterior invertida tanto vertical como horizontalmente.

El nombre "cámara oscura" (del latín "camera obscura") fue acuñado por Johannes Kepler en su tratado "Ad Vitellionem Paralipomena" de 1604. En él expone el funcionamiento de la cámara oscura, que servirá para desarrollar el invento del telescopio.

A su vez, el concepto de "cámara" en óptica lo introdujo por primera vez el físico y matemático musulmán Alhacén, إبن الهيثم. Este erudito árabe, nacido en Basora en 965, escribió el primer tratado óptico en el que demostraba que las teorías griegas sobre los rayos luminosos no tenían fundamento y eran erróneas. Así, en su libro argumentó que los rayos luminosos van de los objetos al ojo que los observa y no al revés, como habían afirmado los griegos Aristóteles y Euclides. Fue el primero en describir los principios de la "cámara oscura", del árabe, قمرة, debe leerse "comra", construyendo un cajón oscuro con un pequeño orificio en una de sus paredes que, al ser atravesado por un rayo de luz, proyectaba invertida la imagen del objeto exterior. Sistema precursor de las modernas cámaras fotográficas.

Aunque nada es seguro, existen teorías que afirman que ciertos dibujos de Pintura rupestre podían estar inspirados en los efectos producidos por la cámara oscura. Se cree que las distorsiones en ciertas formas de los animales en el arte prehistórico podían estar inspiradas en las distorsiones producidas en la superficie cuando la imagen proyectada no reproducía los ángulos de forma exacta. Así mismo, también se ha llegado a creer que las proyecciones de la cámara oscura pudieron haber jugado un papel en las estructuras Neolíticas.

Gnomons perforados proyectando imágenes del sol fueron descritas en los escritos del chino Zhoubi Suanjing (1046 A.C. - 256 A.C) La localización de este círculo de luz podía medir la hora del día y el año. En las culturas árabe y Europea esta invención fue atribuida mucho después al astrónomo y matemático Ibn Yunus D.C.

Se cree que observaciones antiguas de dioses y espíritus, especialmente en templos de rezo, pudieron haber se llevado a cabo a partir de proyecciones de cámara oscura.

Los primeros escritos recogidos de la cámara oscura se encuentran en los escritos chinos Mozi datados en el siglo IV A.C., atribuidos y llamados así por Mozi, un filósofo chino y fundador del Moísmo. En estos escritos explicaba cómo la imagen invertida en un "punto de recogida" o "casa del tesoro" se invertía a partir de un punto intersectado que recogía los rayos de luz. La luz procedente de una persona iluminada estaría por una parte escondida bajo el agujero y por otra en la parte superior de la imagen. Los rayos de la cabeza (o parte superior) estarían por un lado escondidos arriba (justo encima del agujero) y por otro en el lado inferior de la imagen. Esta es una descripción temprana de la cámara oscura: no hay más ejemplos conocidos datados antes del siglo XI.

El filósofo griego Aristóteles (384-322 A.C.) en el siglo IV a.C, o posiblemente un seguidor de sus ideas, tocaron la temática de la cámara oscura en su obra ... . De él se conserva una descripción del aparato y del fenómeno que le daba sentido: 

"Los rayos del sol que penetran en una caja cerrada a través de un pequeño orificio sin forma determinada practicado en una de sus paredes forman una imagen en la pared opuesta cuyo tamaño aumenta al aumentar la distancia entre la pared en la que se ha practicado el orificio y la pared opuesta en la que se proyecta la imagen". La observación posterior de este fenómeno dio origen a las teorías de Alhacén.

Muchos filósofos y científicos occidentales se planteaban esta cuestión antes de que se aceptase la idea de que las formas circulares descritas en este "problema" eran en realidad proyecciones en imágenes circulares del sol. Aunque una imagen proyectada tenga la imagen de la apertura cuando la fuente de luz, apertura o plano de proyección estén juntos, la imagen proyectada tendrá la forma de la fuente de luz cuando estén muy separadas.

Se ha atribuido a Euclides la mención del fenómeno de la cámara oscura como una demostración de que la luz viaja en líneas derechas en su obra Ópticas No obstante, en las traducciones populares no se encuentra nada que pueda identificarse con la cámara oscura. 

Ignacio Danti añadió una descripción de la cámara oscura en su traducción anotada de 1573.

En el siglo IV el académico griego Teón de Alejandría observó que: "La luz de las velas pasando a través de un agujero creará un punto iluminado en una pantalla que estará directamente alineado con la apertura y el centro de la vela."

En el siglo VI, el arquitecto y matemático Griego bizantino Antemio de Trales (Más famoso por ser el co-árquitecto de Santa Sofía), experimentó con los efectos relacionados con la cámara oscura. Antemio tenía una concepción sofisticada de las ópticas, como pudo demostrarse en el diagrama de rayos luz que construyó en el año 555 D.C.

En el siglo IX, Al-Kindi DEMOSTRÓ QUE "La luz de la parte derecha de una llama pasará a través de la apertura y terminará en el lado izquierdo de la pantalla, mientras que la luz procedente de la parte izquierda de la llama pasará por la apertura y terminará proyectada en el lado derecho de la pantalla."

En el siglo X Yu Chao-Lung supuestamente proyectó imágenes de modelos pagoda a través de un pequeño agujero sobre una pantalla para estudiar las direcciones y divergencias de los rayos de luz.

El físico árabe Alhacén (965-1039) explicó en su Libro de Óptica (1027) que los rayos de luz viajaban en líneas rectas y se distinguían por el cuerpo que reflejaba esos rayos y escribió:

"La evidencia de que la luz y el color no se mezclan en el aire o en cuerpos trasparentes se encuentra en el hecho de que, cuando numerosas velas están en diferentes localizaciones dentro de una misma zona y, cuando todas ellas miran a una ventana que se abre en un hueco oscuro y cuando hay una parez blanca o cuerpo opaco en la oscuridad mirando a esa ventana, las luces de esas velas aparecen individualmente sobre ese cuerpo o pared en función del número de velas; y cada una de esas luces (o puntos de luz) aparece directamente en la vela opuesta sobre una línea directa que pasa a través de la ventana. Además, si una de las velas está tapada, solo la luz opuesta del candado se extingue, pero si esa oscuridad es elevada, la luz volverá."

Él describió la "cámara oscura" e hizo numerosos experimentos con pequeños agujeros y luz pasando a través de ellos. Los experimentos consistían en tres velas en fila y observar los efectos en la pared después de situar una separación entre las velas y la pared.

"La imagen del sol en el momento de un eclipse, salvo cuando es total, demuestra que cuando su luz pasa a través de un agujero estrecho y redondo y se emite en un plano opuesto al agujero adquiere la forma de hoz de luna. La imagen del son muestra esta peculiaridad solo cuando esta hoz es muy pequeña. Cuando el agujero se agranda, la imagen cambia, y el cambio aumenta con la anchura añadida. Cuando la apertura es muy grande, la imagen de hoz desaparece, y la luz aparecerá redonda cuando el agujero es redondo, cuadrado si el agujero es cuadrado y si la forma de la apertura es irregular, la luz en la pared adquirirá esa misma forma, siempre y cuando el agujero sea ancho y el plano en el cual se proyecta la luz sea paralelo a este."

Alhacén también analizó los rayos de luz solar y concluyó que creaban una forma cónica en el punto en el que coincidían en el agujero, formando una forma cónica opuesta a la primera en la pared opuesta dentro del cuarto oscuro. A él se le atribuye haber dicho sobre la cámara oscura "Nosotros no inventamos esto". Sus libros sobre las ópticas fueron influyentes en Europa a partir de las traducciones al latín desde el año 1200. Entre la gente a la que inspiró se encuentran Witelo, John Peckham, Roger Bacon, Leonardo da Vinci, René Descartes y Johannes Kepler.

En el libro "Dream Pool Essays" el científico chino Shen Kuo (1031-1095) de la Dinastía Song comparó el punto focal de un espejo ustorio cóncavo y el "recogido" agujero del fenómeno de la cámara oscura con un remo en un escálamo para explicar cómo se invertían las imágenes:

"Cuando un pájaro vuela en el aire, su sombra se mueve alrededor del suelo en la misma dirección. Pero si su imagen es recogida (cómo un cinturón que se aprieta) sobre un agujero pequeño en una ventana, entonces la sombra se mueve en dirección opuesta a la del pájaro.[...] Este es el mismo principio que sigue el espejo ustorio. Este espejo tiene una superficie cóncava y reflecta un dedo para dar una imagen vertical si el objeto está muy cerca, pero si el dedo se mueve más y más lejos llega un punto donde la imagen desaparece y después de eso la imagen se proyecta invertida. El punto donde la imagen desaparece es como el agujero de la ventana. Por ello el remo se arregla y el escálamo en algún punto de su mitad, constituyendo, cuando este se mueve, un tipo de "cintura" y el manejo del remo será siempre la posición inversa del final (La cual está en el agua)."

Shen Kuo también respondió a una afirmación de Duan Chengshi en "Miscellaneous Morsels from Youyang " (Diversos bocados de Youyang) escrito alrededor del año 840 que decía que la imagen invertida de una pagoda China frente una orilla se invertía porque estaba reflejada por el mar: "Esto es un sinsentido. Es un principio normal que la imagen se invierta después de pasar a través de un agujero pequeño."

El estadístico y filósofo escolástico Roberto Grosseteste (c. 1175 - 9 october 1253) comentó sobre la cámara oscura.

En el siglo XIII, el filósofo inglés y franciscano Roger Bacon conocía ya el fenómeno de la cámara oscura aunque, probablemente, hasta el siglo XV, no se le dio aplicación práctica como instrumento auxiliar para el dibujo. Falsamente afirmó en su "De Multiplicatione Specerium" (1267) que la imagen proyectada a través de una apertura cuadrada era redonda porque la luz viajaría en ondas esféricas y por tanto asumiría su forma natural tras pasar a través del agujero. También se le atribuye un manuscrito que aconsejaba estudiar los eclipses solares con cuidado observando los rayos pasando a través de agujeros redondos y estudiando los puntos de luz que formaban en las superficies.

La imagen de una cámara oscura con tres entradas de luz ha sido también atribuida a Bacon, pero la fuente de esta atribución no ha sido dada. Una imagen muy similar se encontró en el Ars magna lucis et umbrae de Atanasio Kircher (1646).

El fraile, teólogo, físico, matemático y filósofo Witelo escribió sobre la cámara oscura en su obra "Perspectiva" (1270-1278), la cual estaba principalmente basada en la obra de Ibn al-Haytham.

El arzobispo y escolar John Peckham (1230-1292) escribió sobre la cámara oscura en su obra "Tractatus de Perspectiva" (1269-1277) y "Perspectiva communis ("1277-79), argumentando falsamente que la luz crea gradualmente la forma circular después de pasar sobre una apertura. Sus escritos estaban influenciados por Roger Bacon.

A finales del siglo XIII, ArnaU de Villa Nova utilizó una cámara oscura para proyectar actuaciones en directo por entretenimiento.

El astrónomo francés Guillaume de Saint-Cloud sugirió en su obra "Almanach Planetarum" que la excentricidad del sol podía determinarse con la cámara oscura de forma inversamente proporcional entre distancias y los aparentes diámetros solares y el apogeo y perigeo.

Kamal al-Din al-Farisi (1267-1319) describió en su obra de 1309 "Kitab Tanqih al-Manazir (La revisión de las ópticas)" cómo experimentaba con una esfera de cristal llena de agua en una cámara oscura con una apertura controlada y descubrió que los colores del arcoíris eran un fenómeno de la descomposición de la luz.

El filósofo, matemático, físico, astrónomo y astrólogo judeofrancés Levi ben Gershon (1288-1344) hizo numerosas observaciones astronómicas utilizando una cámara oscura con una Vara de Jacob, describiendo métodos para medir diámetros angulares del son, la luna y los planetas brillantes de Venus y Júpiter. Determinó también la excentricidad del sol basada en sus observaciones de los solsticios de verano e invierno en 1334. Levi también notó cómo el tamaño de la apertura determinaban el tamaño de la imagen proyectada. Escribió sobre sus descubrimientos en Ebreo en su tratado "Sefer Milhamot Ha-Shem (Las Guerras del Señor)" Libro V capítulos 5 y 9.

La primera descripción completa e ilustrada sobre el funcionamiento de la cámara oscura, aparece en los manuscritos de Leonardo da Vinci, el polímata italiano (1452-1519). Familiar con el trabajo de Alhazen en traducción Latina y tras un estudio extenso de la visión óptica y humana, escribió la más antigua descripción de la cámara oscura conocida en escritura especular en un cuaderno en 1502, más adelante publicada en la colección Códice Atlántico (Traducida del latín).

"Si la fachada de un edificio, o lugar, o paisaje está iluminada por el sol y un pequeño agujero se encuentra en la pared de un cuarto en un edificio frente a esto, que no esté directamente iluminado por el sol, todos los objetos iluminados por el sol enviarán sus imágenes a través de esta apertura y aparecerá, del revés, en la pared frente al agujero.

Estas imágenes se grabarán en un papel blanco, si se sitúa verticalmente en el cuarto no lejos de esa apertura, se observarán los objetos mencionados anteriormente en este papel con sus colores y formas naturales, pero aparecerán en un tamaño reducido y del revés, debido al cruce de los rayos en la apertura de la pared. Si estas imágenes naciesen en un sitio iluminado por el sol, aparecerían en el papel exactamente como son. El papel debe ser muy delgado y debe ser visto por detrás."

Esta descripción, sin embargo, se desconocería hasta que Venturi las publicase en 1797.

Da Vinci estaba claramente interesado en la cámara oscura: a lo largo de los años dibujó alrededor de 270 diagramas de la cámara oscura en sus cuadernos. Experimentó sistemáticamente con varias formas y tamaños de aperturas y con múltiples aperturas (1,2,3,4,8,16,24,28 y 32). Él comparó el funcionamiento del ojo con aquel de la cámara oscura y parecía especialmente interesado en la capacidad de demostrar principios básicos de las ópticas: la inversión de las imágenes a través de un agujero o pupila, la no interferencia de imágenes y el hecho de que las imágenes eran "todas en todas y todas en cada parte".

El último dibujo publicado conocido de una cámara oscura se encontró en el libro "De Radio Astronomica et Geometrica" del físico, matemático y creador de instrumentos Regnier Gemma Frisius, en el cual describió e ilustró cómo utilizaba la cámara oscura para estudiar los eclipses solares el 24 de enero de 1544.

El polímata italiano Gerolamo Cardano describió utilizar un disco de cristal - probablemente Lente bifocal - en una cámara oscura en su libro "De subtilitate, vol. I, Libri IV." Sugirió utilizarlas para observar "qué tiene lugar en la calle cuando el sol brilla" y aconsejó utilizar una hoja de papel muy blanca como pantalla de proyección para que los colores fuesen realistas.

El matemático y astrónomo siciliano Francesco Maurolico (1494-1575) respondió el problema de Aristóteles sobre cómo la luz del sol que brilla a través de agujeros rectangulares puede formar puntos redondos de luz o puntos de tamaño creciente durante un eclipse en su tratado "Photismi de lumini et umbra" (1521-1554). No obstante esta obra no fue publicada hasta 1611, hasta que Johannes Kepler ya había publicado descubrimientos similares por su cuenta.

El polímata italiano Giovanni Battista della Porta describió la cámara oscura, la cual llamaba "obscurum cubiculum", en 1558 fue la primera edición de su serie de libros "Magia Naturalis." Sugirió la utilización de un espejo convexo para proyectar la imagen en un papel y utilizar esto como dibujo de ayuda. Della Porta comparó el ojo humano con la cámara oscura: "La imagen se muestra en el ojo a través del globo ocular cómo lo es aquí a través de la ventana". La popularidad de los libros de Della Porta ayudó a expandir el conocimiento de la cámara oscura.

En su obra de 1567 "La Practica della Perspectiva Venetian" el nóbel Daniele Barbaro (1513-1570) describió utilizar la cámara oscura con lentes binoculares como dibujos de ayuda y subrayó que la imagen era más vívida si las lentes estaban cubiertas tanto como para dejar circunferencias en el medio.

En su influyente y meticulosamente anotada edición latina de las obras de Al-Haytam y Witelo "Opticae thesauru" (1572) el matemático alemán Friedrich Risner propuso una ayuda de dibujo de cámara oscura portable, una pequeño cobertizo de madera ligero con lentes en cada una de las cuatro paredes que proyectaría las imágenes de estas en un cubo de papel situado en el medio de este cobertizo. La construcción se llevaría a cabo con dos polos de madera. Una preparación similar fue ilustrada en la obra de Atanasio Kircher de 1645 "Ars Magna Lucis Et Umbrae."

Alrededor de 1575, el cura dominicoitaliano, matemático y cosmógrafo Ignacio Danti diseñó un estilo de cámara oscura y una línea meridiana para la Basílica de Santa María Novella, más adelante hizo construir un gnomon gigantesco en la Basílica de San Petronio en Bologna. El gnomon se utilizó para el estudio de los movimientos del sol durante el año y ayudó a determinar el nuevo calendario gregoriano en el cual Dante formó parte de su elaboración al estar dentro de la comisión nombrada por Gregorio XIII e instituida en 1582.

En su obra de 1585 "Diversarum Spectulationum Mathematicarum", el matemático veneciano Giambattista Benedetti propuso la utilización de un espejo en un ángulo de 45 grados para proyectar la imagen vertical. Esto hacía que la imagen estuviese revertida, pero se volvería una práctica común en las siguientes cajas de cámaras oscuras.

Giambattista della Porta añadió un "cristal lenticular" o lentes binoculares a la descripción de la cámara oscura en la segunda edición de "Magia Naturallis de 1589." También describió el uso de la cámara oscura para proyectar escenas de caza, banquetes, batallas, partidas o cualquier cosa deseada en las calles. Árboles, bosques, ríos, montañas "Todo ello está hecho tanto por el Arte, madera, o cualquier otra materia" podría contratarse en un plano en la luz del sol en el otro lado de la pared de la cámara oscura. Niños pequeños y animales (Por ejemplo renos hechos de madera, osos salvajes, rinocerontes, elefantes y leones) podían hacer el set. "Deben aparecer por grados, como saliendo de sus cuevas, sobre el plano: El cazador debe venir con sus mástiles, redes, flechas y otras necesidades que puedan representar la caza: dejad que haya cuernos, cornetas, trompetas sonando: aquellos que estén en la sala verán árboles, animales, caras de cazadores, y todos los demás tan llanamente que no sabrán identificar lo que es cierto de lo que son ilusiones: Los dibujos de espadas brillarán dentro del agujero, lo cual hará que la gente esté casi asustada." Della Porta afirmó haber mostrado estos espectáculos de forma usual a sus amigos. Estos lo admiraban mucho y pocas veces llegaban a ser convencidos por las explicaciones de Della Porta de que lo que habían visto era un truco óptico.

El primer uso del término "Cámara oscura" se encuentra en el libro "Ad Vitellionem Paralipomena" del matemático, astrónomo y astrólogo alemán Johannes Kepler. Kepler descubrió la utilización de la cámara oscura recreando su principio con un libro reemplazando un libro brillante y enviando hilos desde sus bordes a través de una apertura en una mesa sobre el suelo donde los hilos recreaban la forma del libro. Él también pudo darse cuenta de que las imágenes estaban "pintadas" de forma invertida y revertida en la retina del ojo y se figuró que esto de alguna forma estaba corregido por el cerebro. En 1607 Kepler estudió el sol en su cámara oscura y observó una mancha solar, pero pensó que era Mercurio transitando el sol. En su obra de 1611 "Dioptrice" Kepler describió cómo la imagen proyectada de la cámara oscura puede ser mejorada y revertida con una lente. Se cree que más adelante él utilizó un telescopio con tres lentes para revertir la imagen en la cámara oscura. 

Cuando los alemanes David Fabricius y Johannes Fabricius (padre e hijo) estudiaron manchas solares con una cámara oscura, después de darse cuenta que observar el sol con el telescopio directamente podía ser dañino para la vista. Se cree que ellos combinaron el telescopio con la cámara oscura creando el telescopio de cámara oscura.

En 1612 el matemático italiano Benedetto Castelli escribió a su mentor, el astrónomo, físico, ingeniero, filósofo y matemático italiano Galileo Galilei sobre la proyección de imágenes del sol a través de un telescopio (Inventado en 1608) para estudiar las recientemente descubiertas manchas solares. Galilei escribió sobre la técnica de Castelli al cura jesuíta, físico y astrónomo alemán Christoph Scheiner.

Desde 1612 hasta al menos 1630 Cristoph Scheiner continuaría estudiando las manchas solares y construyendo nuevos sistemas de proyección telescópica solar. Llamaría a estos sistemas "Heliotropii Telioscopi", más adelante conocidas como Helioscopios. Para los estudios del helioscopio Scheiner construyó una caja alrededor del final de observación/proyección del telescopio, el cual puede considerarse la más antigua versión de una cámara oscura tipo caja conocida. Scheiner también construyó una cámara oscura portable.

En su obra de 1613 "Opticorum Libri Sex" el jesuíta, matemático, físico y arquitecto belga François d'Aguilon describió cómo algunos charlatanes quitaban el dinero de la gente afirmando que conocían nigromancia y que alzarían los espectros del demonio desde el infierno para enseñarselos a la audiencia dentro de un cuarto oscuro. La imagen de un asistente con la máscara del demonio se proyectaba a través de lentes en el cuarto oscura, asustando a los espectadores analfabetos. 

Sobre 1620 Kepler utilizaba una tienda de cámara oscura portable con un telescopio modificado para dibujar los paisajes. Esto podía ser dado la vuelta para capturar los alrededores por partes.

Se cree que el inventor holandés Cornelius Drebbel construyó una cámara oscura tipo caja que corregía la inversión de la imagen proyectada. En 1622 vendió una al poeta, compositor y diplomático holandés Constantijn Huygens, el cual solía utilizarla para pintar y la recomendaría a sus amigos artistas. Huygens escribió lo siguiente a sus padres:

"Tengo en casa el otro instrumento de Drebbel, el cual crea efectos admirables en la pintura a partir del reflejo en una habitación oscura; es imposible para mi expresaros esta belleza con palabras; toda la pintura está muerta en comparación, aquí esta la vida misma o algo más elevado si alguien pudiese articularlo así. La figura, el contorno y los movimientos se unen de forma natural en un estilo extremadamente placentero".

El Orientalista, matemático, inventor, poeta y librero alemán Daniel Schwenter escribió en su libro publicado en 1636 "Deliciae Physico-Mathematicae" sobre un instrumento que un hombre de Pappenheim le había mostrado, que permitía el movimiento de lentes para proyectar más de una escena a través de la cámara oscura. 

En su obra de 1637 "Dioptrique" el filósofo, matemático y científico francés René Descartes sugirió colocar un ojo de un hombre muerto recientemente (si no había uno de estas características disponible se utilizaba el ojo de un zorro) en una apertura en un cuarto oscuro y quitar la piel hacia atrás hasta que uno pudiese ver la imagen invertida formada en la retina.

El filósofo, matemático y astrónomo jesuita italiano Mario Bettinus escribió sobre la creación de una cámara oscura con 12 agujeros en su obra "Apiria Universae Philosophiae Mathematicae" (1642). Cuando un soldado se situaba frente a la cámara, un ejército de 12 personas haciendo el mismo movimiento serían proyectadas también. 

El matemático francés Minim Friar y el pintor Jean-François Niceron (1613-1646) escribieron sobre la cámara oscura con lentes convexas. Explicó cómo la cámara oscura podía llegar a utilizarse por pintores a fin de adquirir la perspectiva perfecta en sus trabajos. También se quejaba de cómo los charlatanes abusaban de la cámara oscura para burlarse de los espectadores y hacer que estos creyeran que las proyecciones eran magia o ciencia oculta. Estos escritos se publicaron en una versión de "La Perspective Curieuse" (1652).

Fue utilizada antiguamente como ayuda para el dibujo. La imagen, proyectada sobre papel u otro soporte, podía servir de pauta para dibujar sobre ella. Posteriormente, cuando se descubrieron los materiales fotosensibles, la cámara oscura se convirtió en cámara fotográfica estenopeica (la que usa un simple orificio como objetivo). 

Estas cámaras estaban muy limitadas por el compromiso necesario al establecer el diámetro de la abertura: suficientemente reducido para que la imagen tuviera una definición aceptable; suficientemente grande para que el tiempo de exposición no fuera demasiado largo.

El uso de la cámara oscura supuso un gran impulso para idear la manera de producir imágenes permanentes y automáticas. Puede ser considerado como lo que proporcionó las bases de lo que hoy conocemos como la fotografía.

La cámara oscura, si bien fue creada en respuesta a las necesidades de pintores y científicos, en la antigüedad fue conocida como "caja mágica" y fue estrechamente relacionada con un animal fantástico: el unicornio. Se han encontrado distintos escritos y esbozos que describen la cámara obscura, pero el orificio y el efecto de producción de imágenes que la caracterizan solo podía darse si se perforaba la "caja" con el cuerno del unicornio.

Desde el siglo IV, magos y alquimistas investigaros fenómenos relacionados con la luz y las imágenes. Fata Morgana, hechicera de la corte y hermana de Arturo, celosa del prestigio de Merlín consiguió robar secretos del mago para intentar utilizarlos, entre ellos, se encontró el siguiente escrito: «(...) El ojo de la caja mágica tendrá que ser perforado con un cuerno de unicornio; de no ser así, resultará completamente inefectiva. (...)». Esta creencia perduró hasta el siglo XI y se creía que los unicornios se extinguieron por el uso que se le daba a sus cuernos a fin de utilizarlos tal y como se ha descrito anteriormente. Con Merlín aparece la primera referencia a este animal y su participación en el "arte de aprender imágenes". 

Tzung Ching Pung, alquimista del siglo VI, hizo esta otra referencia: «(...) Para conseguir bellas y delicadas reproducciones, tanto de bosques como lagos, así como cualquier otra cosa en general, es necesario disponer del cuerno del unicornio de Ycung-*Kuo (...)». 

Abdel-el-Kamir no describe la cámara obscura como su contemporáneo Merlín. No obstante, da una receta de cómo preparar una emulsión sensible a la luz; esta es, la película fotográfica. Es hasta el siglo XI, con el alquimista Adojuhr, que se utiliza por primera vez la cámara oscura (cámara mágica según él) con una emulsión extraordinariamente sensible, que le permitió imprimir imágenes en movimiento mancando de lente.

Las alusiones al unicornio de Merlín y Tzung Ching Pung son vagas; en el de Adojuhr, sucede lo contrario, ya que hace una detallada y minuciosa descripción de este animal. Además, también señala la utilidad del cuerno de las diferentes especies para la perforación del "objetivo" de las cajas mágicas. Una transcripción de Adojuhr es: «(...)Se toma un cuerno de unicornio, se aguza por la punta y con él se practica un pequeño orificio sobre cualquier superficie refulgente. Por este orificio podrán hacerse pasar, comprimiendo su esencia, toda clase de personas, objetos y lugares, los mismos que tendrán que ser guardados cuidadosamente en una caja de cartón donde permanecerán por toda la eternidad, para ser sacados cuando alguien los necesite(...)». 

Otra de las funciones que también se le llegaron a dar a esta caja mágica es la de "captar espíritus malignos" y buscar la forma de exterminarlos, en las representaciones del alquimista se ve más claramente el anterior. Se creía que existían distintas especies de unicornios y cada uno se usaba de una forma distinta en las cajas mágicas. 

Algunas cámaras oscuras fueron construidas como atracciones turísticas, pero cada vez se conservan menos de estas. Algunos ejemplos se pueen encontrar en Grahamstown (Sudáfrica), en la Torre de Tavira de Cádiz (España) y a Dumfries y Edimburgo (Escocia). 




</doc>
<doc id="5274" url="https://es.wikipedia.org/wiki?curid=5274" title="Película fotográfica">
Película fotográfica

La película fotográfica es una superficie transparente, en la mayoría de los casos flexible, compuesta en su inicio de celuloide, pero en la actualidad de acetato de celulosa u otros plásticos como el poliéster, recubierta de una delgada capa de emulsión fotográfica, formada por gelatina en la que se introduce una sustancia sensible a la luz, como el bromuro de plata. Las más modernas capas fotosensibles son de haluros de plata, con un tamaño variable de partícula (granularidad) que afecta a la sensibilidad de la película y las características de la imagen final. Cuando esta emulsión es sometida a una exposición controlada de luz u otro tipo de rayos -generalmente a través de un conjunto de lentes (objetivo)-, la imagen queda impresa en la película de forma muy tenue, recibiendo el nombre de imagen latente. Para obtener una imagen inalterable en futuras exposiciones a la luz -la imagen fotográfica o instantánea fotográfica-, se le aplican a la película una serie de procesos químicos, en un proceso llamado revelado fotográfico, que amplifica la imagen existente y estabiliza la imagen. La película fotográfica es, a su vez, la base para el proceso fotográfico conocido como fotografía química, el proceso convencional para la creación de imágenes, antecesor a la fotografía digital.

La fotografía en blanco y negro usa una sola capa de emulsión con haluros de plata, mientras que las películas en color usan como mínimo tres capas.

Quizá una de las decisiones más críticas a la hora de hacer una fotografía sea la elección de la película. ¿Cómo saber cual, de entre todas las películas disponibles, es la que mejor se adaptará a nuestras necesidades? Para responder a esta pregunta debemos conocer las características de las distintas emulsiones fotosensibles disponibles en el mercado, y para qué fines han sido desarrolladas.

Una primera división de las películas fotográficas se hace en función del tipo de emulsión, y por tanto del tipo de imagen que se obtiene tras su revelado. Según este criterio las películas pueden ser:


Las películas en color (tanto negativas como diapositivas) presentan una característica adicional, ésta es, su equilibrado al blanco.

Lo que nosotros llamamos luz blanca no es más que el resultado de la adición de las diferentes longitudes de onda que forman el espectro visible. La luz del sol no produce el mismo blanco que la luz de una vela. Esta última, debido a su temperatura, tiene mayor cantidad de radiación en la banda del rojo por lo que el resultado es una luz más cálida. En realidad, es nuestro cerebro el que interpreta una luz determinada como blanca independientemente de que su origen sea el azul del cielo, un fluorescente o una lámpara de tungsteno. Pero de una forma objetiva cada una de estas fuentes tiene una temperatura de color diferente, que se expresa en kelvins (K).

La emulsión fotográfica no es capaz de interpretar los colores tal y como lo hace nuestro cerebro, por lo que en ella se impresionará la combinación de colores que exista en realidad producto de las longitudes de onda que refleje o emita cada cuerpo. Por ello, si la fuente de luz blanca es el cielo azul, la fotografía tenderá a quedar azulada, mientras que si la fuente de luz es una lámpara quedará anaranjada. Para evitar este efecto, las películas en color se equilibran a una temperatura de color específica, esto es, se calibran para un blanco determinado a partir del cual obtendremos toda la gama tonal.

Es evidente que producir películas para cada una de las diferentes fuentes de iluminación posibles no sería posible ni rentable, por lo que la industria fotográfica lo ha simplificado a los dos tipos de iluminación más frecuentes:

Películas de luz de día equilibradas a 5.600 K que es la temperatura media de la luz solar al mediodía.

Películas de luz de tungsteno, esto es, para iluminación con lámparas fotográficas, equilibradas a 3.200 K si es de tipo A y a 3.400 K si es de tipo B. En general este es el tipo de película que se empleaba cuando se realiza fotografía en interiores sin flash y la fuente de luz es una bombilla incandescente.

Para el resto de las situaciones fotográficas se pueden utilizar los siguientes filtros correctores del color, dependiendo de la película que se vaya a emplear:

Película luz día con iluminación de tungsteno Filtro 80A
Película luz día con iluminación halógena Filtro 80B
Película luz día con iluminación fluorescente Filtro Magenta
Película luz de tungsteno B con luz día Filtro 85B

Las emulsiones fotográficas también se pueden clasificar de acuerdo a un nuevo parámetro. Este es la sensibilidad. La sensibilidad de una película fotográfica es la cantidad de luz con la que su emulsión fotosensible reacciona. Algunas marcas fotográficas hablan de E.I., esto es Exposure Index o Índice de Exposición.

El índice de exposición o sensibilidad de una película se indica mediante una escala de sensibilidad fotográfica. Existen diferentes escalas: ASA, DIN, ISO o GOST(escala soviética actualmente en desuso).

El que una emulsión sea más o menos sensible depende del tamaño de los granos de haluros fotosensibles. De modo que cuando el tamaño de los granos es grande, mayor es el área que ocupa cada partícula, por lo que una menor cantidad de fotones que incida contra la emulsión será suficiente para producir la imagen latente, con lo el resultado será un tiempo de exposición más breve.

Las películas se clasifican en función de su sensibilidad de la siguiente manera:

de sensibilidad baja (o películas lentas): desde ISO 6 hasta ISO 64. Poseen un grano extremadamente fino y una escala tonal muy amplia. Permiten hacer grandes ampliaciones sin que el grano sea perceptible. Estas películas se emplean cuando se requiere un gran detalle en la imagen, con objetos estáticos y cuando hay buena iluminación o cuando son posibles largas exposiciones con trípode. Películas ultra lentas, habitualmente ISO 6 son aquellas empleadas para copiado por contacto.

De sensibilidad media: desde ISO 100 hasta ISO 200. Suelen considerase como sensibilidades todo-terreno, por lo que son las más usadas. Tienen una amplia escala tonal y permiten ampliaciones de hasta 30 cm x 40 cm con grano apenas perceptible.

de sensibilidad alta (o películas rápidas): desde ISO 400 hasta ISO 3200. Presentan un bajo contraste. El grano es grueso y evidente en las ampliaciones por lo que la imagen pierde definición. Se utilizan en fotografías de acción donde se requiera congelar el movimiento o en situaciones de escasa iluminación.

Podemos afirmar como regla general que cuanta mayor sensibilidad tenga la película más bajo será su contraste, pero existen casos especiales donde la emulsión ha sido diseñada para propósitos específicos donde tanto la forma en la que producen densidades como el color de la base de celuloide afectan el contraste final de la copia ( en el caso de las películas de bajo contraste, pueden poseer soportes amarillos y alcanzar bajas densidades máximas, las de alto contraste soportes azules y alcanzar altas densidades máximas).

Otro concepto importante asociado a las películas es la latitud de exposición, o lo que es lo mismo, el margen de error en la exposición que permite una emulsión, dando resultados aceptables.
Las películas más sensibles tienen mayor latitud que las menos sensibles. En cuanto a las diferentes emulsiones las que presentan mayor latitud son las películas negativas de blanco y negro (hasta 2 diafragmas en algunos casos), seguidas por las negativas de color (1 diafragma como máximo), siendo las de menor latitud las diapositivas con apenas 1/2 e incluso 1/3 diafragma. El resultado es que, usando diapositiva el fotógrafo tiene un control muy exhaustivo de la saturación de color con los mandos de la cámara sin necesidad de entrar en el cuarto oscuro, pero cualquier error en el cálculo de la exposición echará a perder la toma. Por el contrario el uso de película negativa o de B/N permite una cierta flexibilidad en el cálculo de la exposición en la toma.

Al elegir una película es fundamental saber qué utilidad le vamos a dar a la imagen que obtengamos. En muchos casos, y ante una situación luminosa perfectamente controlada, este será el factor discriminatorio primordial a la hora de seleccionar una emulsión.

La película negativa o de B/N nos servirá, básicamente, para obtener un positivo en papel, útil para la ilustración de trabajos y publicaciones. La principal ventaja del negativo en B/N es su facilidad de procesado en un laboratorio casero, lo que se traduce en su inmediatez. Por otro lado, durante el positivado (y sin contar con un laboratorio profesional) se pueden realizar gran cantidad de manipulaciones, desde encuadres selectivos hasta eliminación de fondos, lo que permite obtener imágenes de enorme impacto visual.

De un negativo en color se pueden obtener copias en papel tanto en color como en B/N, también es posible obtener diapositivas, pero es un proceso costoso y no siempre de una calidad satisfactoria. Su revelado es relativamente sencillo, sin embargo, el positivado en un laboratorio casero es mucho menos cómodo, pues al hecho de tener que trabajar casi en completa oscuridad hay que añadir que las más mínimas variaciones en las temperaturas de los líquidos de revelado producen graves alteraciones del color, tanto en el negativo como en el positivo.

La diapositiva permite su proyección, por lo que es útil en conferencias y clases, permite obtener de ella copias en papel de alta calidad tanto en color como en B/N, se pueden hacer duplicados de ellas fácilmente, y es el material fotográfico que requieren las imprentas para realizar reproducciones en color de calidad. Su revelado no presenta especiales dificultades, siempre y cuando tengamos un control riguroso de la temperatura. Por último, se podría añadir que son fáciles de archivar y de transportar.




</doc>
<doc id="5276" url="https://es.wikipedia.org/wiki?curid=5276" title="Escalada clásica">
Escalada clásica

La escalada clásica o tradicional es una modalidad de escalada en la que se hace especial hincapié en los métodos tradicionales de aseguramiento; utilizando como puntos de anclaje sistemas no fijos (empotradores, clavos, nudos, etc.), que se emplazan en grietas o agujeros naturales, y luego se recuperan. Podemos decir que en esta modalidad el escalador recupera sus anclajes al contrario de otras que tienen sistemas de anclaje fijados en la pared previamente a la escalada (propio de la escalada deportiva).
Se incluye así ahora en la categoría "Área de Escalada" en el artículo Montañismo en lugar del antiguo "Escalada en zonas de roca" anterior por oposición a escalada deportiva y porque por definición toda escalada que no sea en hielo es en zonas de roca.


</doc>
<doc id="5277" url="https://es.wikipedia.org/wiki?curid=5277" title="1968">
1968

1968 () fue un año bisiesto comenzado en lunes según el calendario gregoriano. Fue declarado Año Internacional de los Derechos Humanos por la Organización de las Naciones Unidas.




















































</doc>
<doc id="5278" url="https://es.wikipedia.org/wiki?curid=5278" title="1966">
1966

1966 () fue un año común comenzado en sábado según el calendario gregoriano.




















































</doc>
<doc id="5279" url="https://es.wikipedia.org/wiki?curid=5279" title="1965">
1965

1965 () fue un año normal comenzado en viernes según el calendario gregoriano. También fue declarado «Año Internacional de la Cooperación» por la Organización de las Naciones Unidas.










































Atletismo
Automovilismo
Hockey sobre patines
Ciclismo
Fútbol
Rugby





</doc>
<doc id="5280" url="https://es.wikipedia.org/wiki?curid=5280" title="1964">
1964

1964 () fue un año bisiesto comenzando en miércoles según el calendario gregoriano.












































</doc>
<doc id="5281" url="https://es.wikipedia.org/wiki?curid=5281" title="1963">
1963

1963 () fue un año normal comenzado en martes según el calendario gregoriano.











15 de noviembre: Adolfo López Mateos, presidente de México, designa a Gustavo Díaz Ordaz como candidato presidencial del Partido Revolucionario Institucional.
Luis Echeverría es nombrado Secretario de Gobernación.






































Joseph Losey estrena su película "The Servant" ("El sirviente"), interpretada por: Dirk Bogarde (Hugo Barrett), Sarah Miles (Vera), Wendy Craig (Susan), James Fox (Tony) y Catherine Lacey (Lady Mounset), entre otros.

En 1963 también se estrena "Il Gattopardo" ("El gatopardo"), dirigida por Luschino Visconti y protagonizada por Claudia Cardinale.

En el género de cine histórico y de aventuras, en 1963 destaca la película "Jason and the Argonauts" ("Jasón y los argonautas"), interpretada, entre otros, por Todd Armstrong (Jasón) y Laurence Naismith (Argos).




</doc>
<doc id="5282" url="https://es.wikipedia.org/wiki?curid=5282" title="1962">
1962

1962 () fue un año normal comenzado en lunes según el calendario gregoriano.



















La empresa estadounidense Corning desarrolla Chemcor, un material vítreo de gran resistencia, precursor del actual "gorilla glass" (2008).



Todas las fechas pertenecen a los estrenos oficiales de sus países de origen, salvo que se indique lo contrario.






























</doc>
<doc id="5283" url="https://es.wikipedia.org/wiki?curid=5283" title="1961">
1961

1961 () fue un año normal comenzado en domingo en el calendario gregoriano. También fue declarado como el Año Internacional de la Investigación Médica y de la Salud por la Organización de las Naciones Unidas.



































</doc>
<doc id="5284" url="https://es.wikipedia.org/wiki?curid=5284" title="1960">
1960

1960 () fue un en el calendario gregoriano. Fue conocido como el año de África (debido a las independencias logradas por la mayoría de naciones de este continente).





























</doc>
<doc id="5285" url="https://es.wikipedia.org/wiki?curid=5285" title="Médicos Sin Fronteras">
Médicos Sin Fronteras

Médicos Sin Fronteras (Médecins Sans Frontières, MSF) es una organización médica y humanitaria internacional que aporta su ayuda a las víctimas de desastres naturales o humanos y de conflictos armados, sin ninguna discriminación de raza, sexo, religión, filosofía o política. Esta organización recibió el Premio Nobel de la paz en 1999.

"Médecins Sans Frontières" (en francés) fue fundada en Francia en 1971 por un grupo de médicos y periodistas, entre ellos Bernard Kouchner y Jacques Mabit.

Algunos médicos eran testigos del genocidio de la minoría Ibo, pues trabajaban en el Comité Internacional de la Cruz Roja (CICR). Este grupo se sentía frustrado ante la obligación de guardar silencio que exigía el CICR (Comité Internacional de la Cruz Roja) a sus miembros, sobre lo visto y hecho en Biafra.

El otro grupo de médicos acababa de llegar de socorrer a las víctimas de las inundaciones que asolaron Pakistán Oriental (actual Bangladés).

Se dieron cuenta de que, una vez finalizado el proceso de descolonización, el escenario internacional estaba en fase de transformación y se imponía adaptar la ayuda humanitaria a las nuevas necesidades. A partir de ese momento, atender a las víctimas no sería suficiente: habría que denunciar las violaciones de los derechos humanos, crear corrientes de opinión a través de los medios de comunicación y profesionalizar la ayuda.

Actualmente, cuenta con más de seis millones de socios, 3600 profesionales internacionales y 37.800 trabajadores locales en 462 programas humanitarios en 72 países.
Previene y trata algunas enfermedades como la tuberculosis, el sida y la meningitis.

Según el reporte de actividades de 2010 de la organización, La organización cuenta con cinco centros operacionales: en París, Bruselas, Ámsterdam, Barcelona y Ginebra, además de 14 oficinas de país, situadas en Europa, Estados Unidos, Japón y Australia, que apoyan a los centros principales.

Según el reporte anual de Médicos Sin Fronteras de 2017, los proyectos más grandes según gastos fueron República Democrática del Congo (101 millones de Euros), Sudán del Sur (74,3 millones de Euros), Yemen (61,5 millones de Euros), República Centroafricana (57,8 millones de Euros) e Irak (57,6 millones de Euros). 

Existen varias oficinas adicionales, de entre las cuales destacan la Oficina Internacional en Ginebra, las Oficinas de Enlace en Nueva York y en Bruselas, la Campaña de Acceso a Medicinas de Ginebra, el Almacén y la Oficina de Recaudación de Fondos en los Emiratos Árabes Unidos y asociaciones de la sociedad civil en África y en América Latina.

MSF basa su estrategia en la obtención de fondos que aporten estabilidad, que procedan de fuentes diversas y que estén comprometidas con los principios humanitarios. Esto se refleja en una estructura de fondos en la que predominan los de origen privado frente a los públicos, y cuyo máximo exponente son las aportaciones directas de socios y colaboradores. 

En 2017 el 96% de los fondos provenían de fondos privados y un 2% de fondos públicos. Los ingresos de la organización en 2014 fueron de 1.280,3 millones de euros, de los cuales un 89% (1.141,7 millones de euros) provenía de financiación privada; en 2015 los ingresos fueron de 1.443,8 millones de euros, de los cuales un 92% (1.332,1 millones de euros) provenía de financiación privada. Las operaciones en el año 2010 se llevaron a cabo en 61 países, con 2.769 voluntarios internacionales (7.086 salidas) y 31.052 empleados nacionales.

El 17 de junio de 2016 Médicos sin Fronteras renunció a los Fondos de la UE y sus Estados miembros en protesta por la dañina política migratoria que se plasma en el Acuerdo UE-Turquía para externalizar el control migratorio de personas.
El trabajo de MSF está guiado por la ética médica y por los siguientes principios de la acción humanitaria.

Humanidad: todas las personas tienen derecho a ser asistidas en su hora de mayor necesidad.

Independencia: ningún poder político, económico o religioso dicta las acciones de Médicos Sin Fronteras.

Imparcialidad: Médicos sin fronteras no pregunta por etnia, la religión o la ideología de los pacientes.

Neutralidad: MSF no toma partido por nadie, solo por las personas que necesitan ayuda.

Ética médica: la obligación como médicos es asistir sin causar daño.

Además, MSF se compromete con los siguientes principios y valores:

Proporcionalidad: la asistencia debe responde equilibradamente a las necesidades que detectan.

Profesionalidad: compromiso con la calidad y la eficacia de la ayuda.

Proximidad: proporciona asistencia directa, sin intermediarios.

Compromiso individual: cada integrante de MSF asume una responsabilidad personal con su trabajo y acepta los riesgos que entraña.

Sin ánimo de lucro: todos los fondos recibidos se destinan a acción médica, testimonio, administración y captación de recursos.

Control de la ayuda: tanto el trabajo y como las finanzas están sometidos a estrictas supervisiones internas y externas.

Transparencia: cuentas e informes financieros de MSF son públicos.

Médicos Sin Fronteras MSF fue galardonada con el Premio Nobel de la Paz en 1999. El entonces presidente de MSF, Dr. James Orbinski, pronunció el discurso del Premio Nobel de la Paz en nombre de la organización. En la apertura discute las condiciones de las víctimas del genocidio de Ruanda.


Los captadores de socios de Médicos Sin Fronteras en España han denunciado condiciones de explotación laboral. Estas incluyen cláusulas de productividad por las que se puede despedir sin indemnización a los captadores si no consiguen 24 altas de socios al mes. Según Comisiones Obreras este «es un objetivo casi imposible». Esta práctica lleva a una alta rotación que reduce la sindicación. CCOO y CNT llevaron en 2017 esta práctica ante los tribunales. El caso fue desestimado en el Tribunal Superior de Justicia de Galicia por un defecto de forma y está actualmente pendiente en el Tribunal Supremo.



</doc>
<doc id="5287" url="https://es.wikipedia.org/wiki?curid=5287" title="Amnistía Internacional">
Amnistía Internacional

Amnistía Internacional (AI, en el original inglés Amnesty International), es un movimiento global  presente en más de 150 países que trabaja para que sean reconocidos y respetados los derechos humanos, reconocidos en la Declaración Universal de los Derechos Humanos aprobada en 1948 y en otros tratados internacionales como los Pactos Internacionales de Derechos Humanos. Cuenta con más de siete millones de miembros y simpatizantes. Su objetivo es ""realizar labores de investigación y emprender acciones para impedir y poner fin a los abusos graves contra los derechos civiles, políticos, sociales, culturales y económicos"" y pedir justicia para aquellos cuyos derechos han sido violados.

En el campo de las organizaciones internacionales de derechos humanos, AI es una de las que tiene un historial más largo, la de mayor reconocimiento y, según muchos ""es la que establece la referencia en esta área en general"".

Desde su fundación, AI trabaja para llamar la atención de la sociedad sobre los abusos contra los derechos humanos, y hace campaña por el cumplimiento de las normas internacionales. Procura movilizar a la opinión pública para presionar a los gobiernos que toleran esos abusos.

Amnistía Internacional es un organismo creado en Londres el 1 de octubre de 1962, tras la publicación del artículo «The Forgotten Prisoners» en "The Observer", el 28 de mayo de 1961, escrito por el abogado Peter Benenson. Según el historiador Edward Peters, la idea de fundar la asociación surgió de la lectura en 1960 de una crónica periodística que relataba la detención y encarcelamiento de dos estudiantes portugueses por haber brindado por la libertad bajo la dictadura salazarista, aunque esto se ha demostrado luego ser un mito . «Desesperando de la eficacia de la protesta individual y nacional, Benenson, con sus colegas Louis Blom-Cooper y Erik Baker, y los miembros del grupo abogados de Justice, fundada en 1957 para exigir el cumplimiento de la Declaración de las Naciones Unidas de 1948, decidió formar una organización cuyos miembros, como individuos, tratasen de lograr la liberación de los encarcelados por sus opiniones, cuidar de que tales presos recibiesen un trato justo, desarrollar el derecho de asilo y ayudar a los refugiados a encontrar trabajo, e instar a la creación de un mecanismo internacional efectivo para asegurar la libertad de opinión y expresión. Benenson y sus colaboradores llegaron a la conclusión de que el medio más efectivo para alcanzar esos fines era la publicidad».
El primer informe oficial de Amnistía Internacional se refirió a la situación de las prisiones en la Sudáfrica del apartheid, se publicó en 1965, un año después de la celebración de un juicio que había develado el uso sistemático de la tortura por la policía sudafricana durante los interrogatorios a los miembros de la mayoría negra detenidos o presos. Algunos murieron como resultado de los golpes, de las descargas eléctricas o de los azotes con el "sjambok" (látigo hecho de piel de rinoceronte) que recibían. Ese año, la organización fue reconocida por la ONU, el Tribunal Europeo de Derechos Humanos, la Cruz Roja Internacional, la Comisión Internacional de Juristas y otras asociaciones de derechos humanos, además de conseguir el rango de organismo consultivo del Consejo de Europa.

También en 1965 se publicaron informes sobre el Portugal salazarista y sobre la Rumanía comunista. Al año siguiente publicó un informe sobre el régimen racista de Rodesia.

En 1968, se reunió en Estocolmo la Asamblea Internacional de AI, que adoptó como uno de sus principales fines el cumplimiento del artículo 5 de la Declaración Universal de Derechos Humanos de la ONU de 1948: ""Nadie será sometido a tortura o a tratamiento o castigo cruel, inhumano o degradante"". El motivo fueron las alegaciones presentadas por la sección sueca sobre las torturas perpetradas por el régimen de los coroneles en Grecia, instaurado un año antes mediante un golpe de estado. De inmediato, AI publicó dos informes sobre el uso de la tortura por el nuevo régimen griego. Ese año, tuvo como resultado la expulsión de Grecia del Consejo de Europa por haber violado nueve de los artículos de la Convención Europea de Derechos Humanos aprobada en 1950. Tras la caída del régimen de los coroneles en 1974, AI publicó el detallado y documentado informe "La tortura en Grecia: el Primer Juicio a los Torturadores" (1975). Pudo realizarse gracias a la colaboración del nuevo gobierno griego. Según el historiador de la tortura Edward Peters, es ""una de las obras clásicas sobre la documentación y las técnicas de tortura de fines del siglo XX"".

En 1972, AI inició la Campaña para la Abolición de la Tortura que incluyó un informe publicado en 1973 sobre la tortura desde 1962, lo que desató las protestas de los estados mencionados allí que, por ejemplo, impidieron que AI pudiera utilizar los locales de la UNESCO en París para celebrar su planeada conferencia sobre la tortura. 

Ese año, AI también publicó un informe sobre la tortura por el nuevo régimen del general Pinochet instaurado en Chile tras el golpe de estado militar del 11 de septiembre. A este informe le había precedido otro sobre la tortura en Brasil (1972), también bajo una dictadura militar, y al que siguieron otros sobre la tortura en Irán (1976), Nicaragua (1976), Argentina (1976) e Irak (1981).

En 1977 AI recibió el Premio Nobel de la Paz por su «campaña contra la tortura» y en 1978 el Premio de Derechos Humanos de las Naciones Unidas.

En la década de 1980, algunos gobiernos incrementaron sus críticas a Amnistía Internacional. La Unión Soviética alegó que AI espiaba; el gobierno de Marruecos la acusó de defender delincuentes y las dictaduras argentina y chilena prohibieron la difusión de su informe anual 1983.

Durante los '80, AI continuó su campaña contra la tortura y en defensa de los presos de conciencia. También se abordaron nuevos problemas, como las ejecuciones extrajudiciales, transferencias de personal entre el ejército y la policía, homicidios políticos y desapariciones.

Hacia el final de la década creció la preocupación de AI por el creciente número de refugiados en el mundo. Muchos lo eran a causa de guerras y hambrunas pero, tal y como exige el mandato de AI, la organización concentró sus esfuerzos en ayudar a los que habían sido obligados a huir a causa de violaciones de los derechos humanos. Se pedía a los gobiernos que en lugar de aumentar las restricciones a la entrada de solicitantes de asilo actuaran para evitar las violaciones de derechos humanos que les forzaban al exilio.

Además de una segunda campaña contra la tortura, durante la primera mitad de la década hubo dos importantes acontecimientos musicales pensados para que las generaciones más jóvenes conocieran mejor Amnistía y los derechos humanos. En 1986 el tour Conspiración de la Esperanza realizó una serie de conciertos en Estados Unidos y en 1988, para el 40º aniversario de la Declaración Universal de los Derechos Humanos, AI organizó el tour mundial "¡Derechos Humanos Ya!" A lo largo de seis semanas algunos de los músicos y grupos más famosos del momento actuaron en conciertos en los cinco continentes.

En 1998 recibió la primera edición del Premio Internacional Jaime Brunet de la Universidad Pública de Navarra a la promoción de los Derechos Humanos.

Los principales objetivos de la Amnistía Internacional son:

AI tiene entre sus objetivos la defensa a las personas migrantes, solicitantes de asilo, refugiadas, desplazadas o víctimas de trata, aumentando su protección legal y física, garantizando que no se les niega su derecho a la educación, a la salud o a la vivienda. Considera a estas personas población vulnerable.

AI ha realizado una serie de investigaciones sobre la temática de refugiados y migración y ha dedicado a este tema acciones y campañas prioritarias.

Los datos hablan por sí solos y reflejan la magnitud de la situación de las personas migrantes, refugiadas o demandantes de asilo en el mundo:


En su mayoría, son africanos en cuyos países hay situaciones económicas difíciles (inestabilidad política) o existen campamentos de refugiados con alto índice de población. La situación de la población en Oriente Próximo y algunos países de África subsahariana (entre otros, Eritrea), es tal que los que deciden migrar no lo hacen por motivos económicos, sino por buscar refugio ante la situación de inestabilidad política o guerras en sus lugares de origen. Según AI, estos últimos deberían estar amparadas por la Convención de Ginebra.

El último estudio publicado de AI, titulado "El Coste humano de la fortaleza Europa", ponía de manifiesto que, en 2013, el 48 % de todas las entradas irregulares y el 63 % de todas las irregulares por mar a Europa eran de personas de Siria, Eritrea, Afganistán y Somalia, países asolados por conflictos.

Ante esta situación, AI denuncia el esfuerzo de los países de la Unión Europea por cerrarles fronteras. Entre 2007 y 2013, se han gastado dos mil millones de euros en proteger sus fronteras externas, frente a los 700 millones destinados a mejorar la situación de personas refugiadas y solicitantes de asilo. Así, Grecia ha construido una valla de 10,5km a lo largo de su frontera terrestre de 203km con Turquía y ha desplegado a casi 2000 guardias costeros desde el verano de 2012. Y España ha invertido 300 millones de euros en proteger sus fronteras, frente a los nueve millones destinados a proteger a las personas.

Ante las dificultades impuestas por los países europeos, los migrantes se ven obligados a intentar rutas cada vez más peligrosas por el mar Egeo y Mediterráneo, viéndose atrapados en países como Libia, Marruecos, Ucrania y Turquía. En algunos de estos países viven en la indigencia, carecen de los derechos sociales y económicos y sufren torturas o violencia.

AI se ha centrado y se centrara entre 2012 y 2015 en mejorar la situación de las personas migrantes, refugiadas de asilo en las regiones siguientes:

Para ello, AI se esforzará en facilitar a los migrantes el acceso a información sobre sus derechos, conseguir que los trabajadores migrantes estén protegidos contra la explotación de los agentes no estatales, ajustar las leyes, políticas y prácticas sobre inmigración a las obligaciones internacionales en materia de derechos humanos, abordando las prácticas abusivas de detención, proteger a las personas refugiadas y solicitantes de asilo contra la devolución a países donde estén expuestas a sufrir graves violaciones de los derechos humanos, buscar soluciones duraderas para las personas refugiadas que no puedan regresar a su país, captar apoyos entre responsables clave de la toma de decisiones en gobiernos, organismos regionales y organizaciones internacionales e influir en ellos para que aprueben políticas que protejan a las personas refugiadas y migrantes y colaborar con organizaciones locales para crear redes de apoyo a las personas refugiadas y migrantes.

AI efectúa su acción por medio de:

Se financia por medio de donaciones de sus socios y no solicita ni recibe subvenciones de gobiernos. Busca mantener su imparcialidad de esta forma. Según la web de AI, este tipo de financiación le "permite mantener su total independencia de cualquier gobierno, ideología política, interés económico o religión". Por ejemplo, en 2012 la Sección Española de AI recibió más del 90 % de sus ingresos de sus socios y afiliados.

Entre otras actividades para su financiación, en 2012 AI publicó un álbum recopilatorio de versiones de Bob Dylan, incluyendo a Miley Cyrus, Adele, Kesha, entre otros.

AI es un movimiento de personas, basado en membresía mundial voluntaria, que está representada por entidades de membresía (Secciones y Estructuras) y membresía internacional.

La Asamblea Global es el máximo órgano de decisiones de AI. Está formada por representantes de las entidades de membresía y de la membresía internacional. La Asamblea Global elige a la Junta Directiva Internacional.

La Junta Directiva Internacional (JDI) de AI proporciona orientación global para el cumplimiento de la visión y misión del movimiento, y para que se respeten sus políticas y normas globales.

La gestión diaria de AI está a cargo del Secretariado Internacional (SI), encabezado por un secretario o secretaria general, bajo la dirección de la Junta Directiva Internacional. El Secretariado Internacional de AI tiene las siguientes sedes en todo el mundo:


En alianza con IANSA e Intermón Oxfam, en 2003 se lanzó la campaña Armas Bajo Control. Pedía un tratado que garantizara que los Estados dejan de transferir material militar y policial a lugares donde es probable que sea utilizado para cometer abusos graves contra los derechos humanos. En concreto, recogió un millón de fotografías de rostros que pedían normas internacionales para el comercio de armas pequeñas, causantes de 2.000 muertes al día. Este Tratado del Comercio de Armas fue adoptado por la ONU en 2013. Numerosos países se están adhiriendo a él.

En 2009, AI lanzó la campaña global «Exige Dignidad», cuyo objetivo principal es poner fin a los abusos contra los derechos humanos que mantienen a las personas en la pobreza. En palabras de Irene Khan: «Hace casi 50 años, Amnistía Internacional nació para pedir la liberación de los presos de conciencia. Hoy "exigimos dignidad” también para los presos de la pobreza, para que puedan cambiar sus vidas».

Comienza en 2014 y trata los derechos sexuales y reproductivos: elegir a tu pareja, elegir si tener hijos, educación sexual, salud reproductiva y no sufrir violencia intrafamiliar. Se denuncian los casos más graves de violación de estos derechos: en el Magreb un violador puede evitar su condena si se casa con la víctima, en Nepal las mujeres sufren prolapso uterino porque paren muy jóvenes y no hay sanidad materna, en El Salvador el aborto está prohibido aunque haya peligro para la vida de la madre.

A pesar de que la tortura está prohibida desde 1984, en 2014 se lanza esta campaña porque aún ocurren torturas en más de 141 países. «En todo el mundo hay gobiernos hipócritas ante la tortura: la prohiben en la ley, pero la facilitan en la práctica», afirmó Salil Shetty, secretario general de AI. Lo que se pide para evitarlas es acabar con la impunidad del torturador (como ocurre en Filipinas), que las confesiones obtenidas bajo tortura no sean aceptadas por los tribunales (como pasa en Nigeria) y que dejen de realizar desapariciones forzadas (como en México).

En 2010, Amnistía Internacional publicó un informe sobre la sanidad en Corea del Norte. Al poco tiempo, fue criticado por la Organización Mundial de la Salud que afirmó que el sistema sanitario norcoreano podría ser «la envidia de muchos países desarrollados» y que era «poco científico» porque se basaba en casos «anecdóticos», algunos sucedidos hace años, por lo no que no refleja las mejoras logradas por proyectos llevados a cabo en los últimos años, algunos con la participación de Corea del Sur. Sin embargo, el corresponsal de la BBC en Suiza, donde está la sede de la OMS, informó que se cree que las «diversas agencias de las Naciones Unidas [que] llevan a cabo proyectos de ayuda en Corea del Norte... son reticentes a criticar abiertamente el régimen por miedo a poner en peligro su trabajo allí».

Por otro lado, AI ha elaborado diversos informes sobre el régimen de Corea del Norte, en los que se habla de varios crímenes contra la humanidad, el último de 1995. Fue respondido por un portavoz del régimen con la siguiente afirmación: «Amnistía Internacional no ha llegado jamás a Corea [del Norte]. Es más, no han pisado nunca el territorio coreano [del norte], porque si usted insulta a mi familia no le voy a invitar a que venga a mi casa. Por lo tanto, Amnistía Internacional se ha cerrado las puertas por hacer de maquinaria propagandística».

En 2007, la Iglesia católica manifestó que retiraba su apoyo económico a Amnistía Internacional por entender que esta organización apoyaba el aborto inducido. Esta declaración fue reacción a la decisión del Consejo Internacional de AI de defender el derecho al aborto de las mujeres en los casos de violación, riesgo para la salud de la madre e incesto.

Sobre su posición sobre el aborto, Amnistía Internacional indicó que:
Sin embargo, en 2018 AI sí promueve el aborto como un derecho no solo en los casos de violación o riesgo para la salud.

En cualquier caso, desde la Iglesia católica se entiende que esto contradice lo expuesto en los comunicados en los que tratan el aborto como un derecho y en que se critican las leyes que restringen el acceso a una plena salud sexual y reproductiva.

Una de las críticas a AI sobre su informe anual es que incluye violaciones de los derechos humanos cometidas en países donde la organización no tiene presencia, sección ni afiliados, a partir de informaciones periodísticas obtenidas en otros países, a menudo enfrentados políticamente. En particular, el Gobierno de Cuba rechaza los informes de AI aduciendo falta de rigor y fiabilidad en las acusaciones de violación de los derechos humanos allí, señalando que tal información es falaz y sesgada. La organización responde que estas acusaciones no tienen en cuenta los métodos de trabajo de AI, que van más allá de presencia física permanente en los países en cuestión. Según la organización, estos métodos se emplean para investigar «de manera independiente e imparcial, tanto situaciones generalizadas como casos concretos».

Ricardo García Damborenea, político español que fue secretario general del Partido Socialista de Euskadi-Euskadiko Ezkerra (PSOE) y que fue condenado por el caso GAL en 1998, criticó a AI en 1985 diciendo: «No he visto a AI preocuparse por los derechos de una sola víctima del terrorismo […] sin embargo, son capaces de alborotar a redoble de tambor si a un terrorista que tiene catorce muertos a la espalda un guardia civil le da una bofetada». En su web Amnistía Internacional declara: «Desde AI se ha pedido en todo momento que no haya impunidad para los graves abusos cometidos por ETA, pero también que se investiguen las violaciones de derechos humanos cometidas por las fuerzas de seguridad, tal y como recomienda el relator de Naciones Unidas contra la tortura».

Se ha acusado a AI de ocuparse con preferencia de los derechos humanos de los delincuentes. AI ha respondido: «Amnistía Internacional afirma, de acuerdo con el derecho internacional, que nadie (comprendidos los terroristas) puede ser sometido a penas crueles, inhumanas o degradantes, entre las que AI incluye la pena de muerte. De lo anterior se desprende que Amnistía Internacional, naturalmente, no ha pedido jamás la liberación de terroristas».




</doc>
<doc id="5292" url="https://es.wikipedia.org/wiki?curid=5292" title="1958">
1958

1958 () fue un año normal comenzado en miércoles según el calendario gregoriano.

















Creación de ARPANET en E.U.A































</doc>
<doc id="5293" url="https://es.wikipedia.org/wiki?curid=5293" title="1957">
1957

1957 () fue un año normal comenzado en martes en el calendario gregoriano.
















También la dirección de la Federación Internacional de Voleibol tuvo ocasión en presentar un torneo en Sofia en el año 1957, durante la sesión del Comité Olímpico Internacional.


















</doc>
<doc id="5298" url="https://es.wikipedia.org/wiki?curid=5298" title="Potencial eléctrico">
Potencial eléctrico

El potencial eléctrico en un punto, es el trabajo a realizar por unidad de carga para mover dicha carga dentro de un campo electrostático desde el punto de referencia hasta el punto considerado, ignorando el componente irrotacional del campo eléctrico. Dicho de otra forma, es el trabajo que debe realizar una fuerza externa para traer una carga positiva unitaria "q" desde el punto de referencia hasta el punto considerado, en contra de la fuerza eléctrica y a velocidad constante. Aritméticamente se expresa como el cociente:
El potencial eléctrico solo se puede definir unívocamente para un campo estático producido por cargas que ocupan una región finita del espacio. Para cargas en movimiento debe recurrirse a los potenciales de Liénard-Wiechert para representar un campo electromagnético que además incorpore el efecto de retardo, ya que las perturbaciones del campo eléctrico no se pueden propagar más rápido que la velocidad de la luz.

Si se considera que las cargas están fuera de dicho campo, la carga no cuenta con energía y el potencial eléctrico equivale al trabajo necesario para llevar la carga desde el exterior del campo hasta el punto considerado. La unidad del Sistema Internacional 
es el voltio (V).

Todos los puntos de un campo eléctrico que tienen el mismo potencial forman una superficie equipotencial. Una forma alternativa de ver al potencial eléctrico es que a diferencia de la energía potencial eléctrica o electrostática, él caracteriza solo una región del espacio sin tomar en cuenta la carga que se coloca ahí.

Considérese una carga eléctrica puntual formula_1 en presencia de un campo eléctrico formula_2. La carga experimentará una fuerza eléctrica:

Esta fuerza realizará un trabajo para trasladar la carga o elemento de un punto A a otro B, de tal forma que para producir un pequeño desplazamiento formula_3 la fuerza eléctrica hará un trabajo diferencial formula_4 expresado como:

Por lo tanto, integrando la expresión se obtiene el trabajo total realizado por el campo eléctrico:

Un caso particular de la fórmula anterior, es el del caso de un campo eléctrico creado por una carga puntual estática "Q". Sea una carga puntual formula_1 que recorre una determinada trayectoria A - B en las inmediaciones de una carga formula_6 tal y como muestra la figura 1. Siendo formula_7 el desplazamiento infinitesimal de la carga formula_1 en la dirección radial, el trabajo diferencial formula_4 se puede expresar así:

Para calcular el trabajo total, se integra entre la posición inicial A, distante formula_10 de la carga formula_6 y la posición final B, distante formula_12 de la carga formula_6:

En la expresión , formula_14 es la permitividad del vacío; de dicha expresión se concluye que el trabajo formula_15 no depende de la trayectoria seguida por la partícula, solo depende de la posición inicial y final, lo cual implica que la fuerza eléctrica formula_16 es una fuerza conservativa. Por lo tanto se puede definir una energía potencial que permite calcular el trabajo más fácilmente:

El trabajo realizado por la fuerza eléctrica para desplazar una partícula entre A y B será:

Usualmente, el nivel cero de energía potencial se suele establecer en el infinito, es decir, si y solo si formula_17 (esto tiene que ver con la elección de la constante de integración en la fórmula del potencial).

Considérese una carga de prueba positiva formula_18 en presencia de un campo eléctrico y que se traslada desde el punto A al punto B conservándose siempre en equilibrio. Si se mide el trabajo que debe hacer el agente que mueve la carga, la "diferencia de potencial eléctrico" se define como:

formula_19

El trabajo formula_20 puede ser positivo, negativo o nulo. En estos casos el potencial eléctrico en B será respectivamente mayor, menor o igual que el potencial eléctrico en A. La unidad en el SI para la diferencia de potencial que se deduce de la ecuación anterior es Joule/Coulomb y se representa mediante una nueva unidad, el voltio, esto es: 1 voltio = 1 joule/coulomb.

Un electronvoltio ("eV") es la energía adquirida para un electrón al moverse a través de una diferencia de potencial de 1 V, 1 eV = 1,6x10 J. Algunas veces se necesitan unidades mayores de energía, y se usan los kiloelectronvoltios (keV), megaelectronvoltios (MeV) y los gigaelectronvoltios (GeV). (1 keV=10 eV, 1 MeV = 10 eV, y 1 GeV = 10 eV).

Aplicando esta definición a la teoría de circuitos y desde un punto de vista más intuitivo, se puede decir que el potencial eléctrico en un punto de un circuito representa la energía que posee cada unidad de carga al paso por dicho punto. Así, si dicha unidad de carga recorre un circuito constituyendóse en corriente eléctrica, ésta irá perdiendo su energía (potencial o tensión) a medida que atraviesa los diferentes componentes del mismo. Obviamente, la energía perdida por cada unidad de carga se manifestará como trabajo realizado en dicho circuito (calentamiento en una resistencia, luz en una lámpara, movimiento en un motor, etc.). Por el contrario, esta energía perdida se recupera al paso por fuentes generadoras de tensión. Es conveniente distinguir entre potencial eléctrico en un punto (energía por unidad de carga situada en ese punto) y corriente eléctrica (número de cargas que atraviesan dicho punto por segundo). 

Usualmente se escoge el punto A a una gran distancia (en rigor el infinito) de toda carga y el potencial eléctrico formula_21 a esta distancia infinita recibe arbitrariamente el valor cero. Esto permite definir "el potencial eléctrico en un punto" poniendo formula_22 y eliminando los índices:

formula_23

siendo formula_24 el trabajo que debe hacer un agente exterior para mover la carga de prueba formula_18 desde el infinito al punto en cuestión.

Obsérvese que la igualdad planteada depende de que se da arbitrariamente el valor cero al potencial formula_21 en la posición de referencia (el infinito) el cual hubiera podido escogerse de cualquier otro valor así como también se hubiera podido seleccionar cualquier otro punto de referencia.

También es de hacer notar que según la expresión que define el potencial eléctrico en un punto, el potencial en un punto cercano a una carga positiva aislada es positivo porque debe hacerse trabajo positivo mediante un agente exterior para llevar al punto una carga de prueba (positiva) desde el infinito. Similarmente, el potencial cerca de una carga negativa aislada es negativo porque un agente exterior debe ejercer una fuerza (trabajo negativo en este caso) para sostener a la carga de prueba (positiva) cuando esta (la carga positiva) viene desde el infinito.

Por último, el potencial eléctrico queda definido como un escalar porque formula_24 y formula_18 son escalares.

Tanto formula_20 como formula_30 son independientes de la trayectoria que se siga al mover la carga de prueba desde el punto A hasta el punto B. Si no fuera así, el punto B no tendría un potencial eléctrico único con respecto al punto A y el concepto de potencial sería de utilidad restringida.

Es posible demostrar que las diferencias de potencial son independientes de la trayectoria para el caso especial representado en la figura. Para mayor simplicidad se han escogido los puntos A y B en una recta radial.

Una carga de prueba puede trasladarse desde A hacia B siguiendo la trayectoria I sobre una recta radial o la trayectoria II completamente arbitraria.

La trayectoria II puede considerarse equivalente a una trayectoria quebrada formada por secciones de arco y secciones radiales alternadas. Puesto que estas secciones se pueden hacer tan pequeñas como se desee, la trayectoria quebrada puede aproximarse a la trayectoria II tanto como se quiera. En la trayectoria II el agente externo hace trabajo solamente "a lo largo de las secciones radiales", porque a lo largo de los arcos, la fuerza formula_31 y el corrimiento formula_32 son perpendiculares y en tales casos formula_33 es nulo. La suma del trabajo hecho en los segmentos radiales que constituyen la trayectoria II es el mismo que el trabajo efectuado en la trayectoria I, porque cada trayectoria está compuesta del mismo conjunto de segmentos radiales. Como la trayectoria II es arbitraria, se ha demostrado que el trabajo realizado es el mismo para todas las trayectorias que unen A con B.

Aun cuando esta prueba solo es válida para el caso especial ilustrado en la figura, la diferencia de potencial es independiente de la trayectoria para dos puntos cualesquiera en cualquier campo eléctrico. Se desprende de ello el carácter conservativo de la interacción electrostática el cual está asociado a la naturaleza central de las fuerzas electrostáticas.

Para un par de placas paralelas en las cuales se cumple que formula_34, donde d es la distancia entre las placas paralelas y E es el campo eléctrico constante en la región entre las placas.

Sean A y B dos puntos situados en un campo eléctrico uniforme, estando A a una distancia d de B en la dirección del campo, tal como muestra la figura. 

Considérese una carga de prueba positiva q moviéndose sin aceleración, por efecto de algún agente externo, siguiendo la recta que une A con B. 

La fuerza eléctrica sobre la carga será qE y apunta hacia abajo. Para mover la carga en la forma descrita arriba, se debe contrarrestar esa fuerza aplicando una fuerza externa F de la misma magnitud pero dirigida hacia arriba. El trabajo formula_24 realizado por el agente que proporciona esta fuerza es:

formula_36

Teniendo en cuenta que: 

formula_37 

sustituyendo se obtiene:

formula_38 

Esta ecuación muestra la relación entre la diferencia de potencial y la intensidad de campo en un caso sencillo especial. 

El punto B tiene un potencial más elevado que el A. Esto es razonable porque un agente exterior tendría que hacer trabajo positivo para mover la carga de prueba de A hacia B.

En el caso más general de un campo eléctrico no uniforme, este ejerce una fuerza sobre la carga de prueba, tal como se ve en la figura. Para evitar que la carga acelere, debe aplicarse una fuerza que sea exactamente igual para todas las posiciones del cuerpo de prueba.

Si el agente externo hace que el cuerpo de prueba se mueva siguiendo un corrimiento formula_39 a lo largo de la trayectoria de A a B, el elemento de trabajo desarrollado por el agente externo es formula_40. Para obtener el trabajo total formula_20 hecho por el agente externo al mover la carga de A a B, se suman las contribuciones al trabajo de todos los segmentos infinitesimales en que se ha dividido la trayectoria. Así se obtiene:

formula_42

Como formula_37, al sustituir en esta expresión, se obtiene que formula_44

Si se toma el punto A infinitamente alejado, y si el potencial formula_21 al infinito toma el valor de cero, esta ecuación da el potencial en el punto B, o bien, eliminando el subíndice B, 

formula_46

Estas dos ecuaciones permiten calcular la diferencia de potencial entre dos puntos cualesquiera si se conoce formula_47.

El potencial eléctrico suele definirse a través del campo eléctrico a partir del teorema del trabajo de la física.

donde E es el campo eléctrico vectorial generado por una distribución de carga eléctrica. Esta definición muestra que estrictamente el potencial eléctrico no está definido sino tan solo sus variaciones entre puntos del espacio. Por lo tanto, en condiciones de campo eléctrico nulo el potencial asociado es constante. Suele considerarse sin embargo que el potencial eléctrico en un punto infinitamente alejado de las cargas eléctricas es cero por lo que la ecuación anterior puede escribirse:
^{\infty} \mathbf{E}(\mathbf{r}) \cdot d\mathbf{r}</math>
En términos de energía potencial el potencial en un punto "r" es igual a la energía potencial entre la carga "Q":

El potencial eléctrico según Coulomb, también puede calcularse a partir de la definición de energía potencial de una distribución de cargas en reposo:

donde formula_48 es un volumen que contiene la región del espacio que contiene las cargas (se asume que dicha región es acotada en el espacio).

Considérense los puntos A y B y una carga puntual formula_49 situada en el origen, tal como muestra la figura. Consideremos que una carga de prueba, q, se mueve desde A hasta B. Si, por fijar ideas, formula_50, según se muestra, formula_47 apunta a la derecha y formula_52, que siempre está en la dirección del movimiento, apunta hacia el origen. Por consiguiente:

formula_53

Ahora bien, al moverse la carga una trayectoria hacia el origen, el módulo del desplazamiento infinitesimal formula_54 es igual a la disminución de la distancia r al origen, es decir, formula_55. Así pues:

formula_56

Por lo cual:

formula_57

Combinando esta expresión con la de E para una carga puntual se obtiene:

formula_58

Escogiendo el punto de referencia A en el infinito, esto es, haciendo que formula_59, considerando que formula_60 en ese sitio y eliminando el subíndice B, se obtiene:

Esta ecuación muestra claramente que las superficies equipotenciales para una carga puntual aislada son esferas concéntricas a la carga puntual.

El potencial en un punto P debido a dos cargas es la suma de los potenciales debido a cada carga individual en dicho punto.

formula_61

Siendo formula_62 y formula_63 las distancias entre las cargas formula_64 y formula_65 y el punto P respectivamente.

El potencial en un punto cualquier debido a un grupo de cargas punto se obtiene calculando el potencial formula_66 debido a cada carga, como si las otras cargas no existieran, y sumando las cantidades así obtenidas, o sea:
formula_67

siendo formula_68 el valor de la enésima carga y formula_69 la distancia de la misma al punto en cuestión. La suma que se efectúa es una suma algebraica y no una suma vectorial. En esto estriba la ventaja de cálculo del potencial sobre la de intensidad del campo eléctrico.
Las superficies equipotenciales cortan perpendicularmente a las líneas de campo. En el gráfico se representa la intersección de las superficies equipotenciales con el plano XY.

La ecuación de las líneas equipotenciales es: 

formula_70

Si la distribución de carga es continua y no una colección de puntos, la suma debe reemplazarse por una integral:
formula_71

siendo "dq" un elemento diferencial de la distribución de carga, "r" su distancia al punto en el cual se calcula "V" y "dV" el potencial que "dq" produce en ese punto.

Un plano infinito con densidad de carga de superficie formula_72 crea un campo eléctrico saliente en la dirección perpendicular al plano de valor constante 

formula_73

Si "x" es la dirección perpendicular al plano y éste se encuentra en "x=0" el potencial eléctrico en todo punto x es igual a:

formula_74

Donde se ha considerado como condición de contorno "V(x)=0" en "x=0"

Sea Q la carga total almacenada en la esfera conductora. Por tratarse de un material conductor las cargas están situadas en la superficie de la esfera siendo neutro su interior.

Potencial en el exterior de la corteza:
El potencial en el exterior de la corteza es equivalente al creado por una carga puntual de carga "Q" en el centro de la esfera.

formula_75

donde formula_76 es la distancia entre el centro de la corteza y el punto en el que medimos el potencial eléctrico.
Potencial en el interior de la corteza:
El campo eléctrico en el interior de una esfera conductora es cero, de modo que el potencial permanece constante al valor que alcanza en su superficie.

formula_77

Donde formula_78 es el radio de la esfera.





</doc>
<doc id="5299" url="https://es.wikipedia.org/wiki?curid=5299" title="Snowboard">
Snowboard

El snowboard, snowboarding, tabla sobre nieve, tabla de nieve, tabla neval o incluso tablanieve o nevotabla, es un deporte extremo de invierno, en el que se utiliza una tabla para deslizarse sobre una pendiente cubierta por nieve. El equipo básico para practicarlo son la mencionada tabla, las fijaciones y las botas. Se incorporó al programa de los Juegos Olímpicos de Invierno en 2015.

Las primeras evidencias de este deporte se remontan a principios del 1910, cuando la gente quería fijar sus pies a un tablón de madera contrachapada mediante cuerdas de pesca y riendas de caballos para poder bajar por las pendientes de las montañas nevadas.

La tabla de nieve empezó en el 1965 cuando Sherman Poppen, un ingeniero de Muskegon, Michigan, inventó un juguete para sus hijas juntando dos esquíes y atándolos a una cuerda con la cual tener control sobre la tabla. Lo llamó "snurfer" -una fusión de dos voces: "snow" (nieve) y "surf"- y fue tan popular que Sherman Poppen vendió la licencia a una empresa, Brunswick Corporation, que consiguió vender cerca de un millón de "snurfers" durante la siguiente década, más de medio millón solo en 1966.

A principios de la década del 1970, Poppen organizó competiciones de snurfing en la estación de esquí de Míchigan que atrajo a aficionados de todo el país. Uno de ellos fue Tom Sims, un amante del skateboarding, que fabricó un tabla de nieve en la escuela cuando cursaba octavo grado en Haddonfield, New Jersey, allá por el año 1960. Su idea fue tapizar la parte superior de un trozo de madera y fijar una chapa de aluminio en la parte inferior que, posteriormente, a mediados de los años 1970 produjo y comercializó. Al mismo tiempo, Dimitrije Milovich, un entusiasta del surf que utilizaba las bandejas de la cafetería de la universidad para deslizarse por la nieve, construyó una tabla de snowboard llamada "Winterstick" inspirada en las tablas de surf convencionales. Varios artículos sobre las tablas Winterstick en revistas importantes del país ayudaron a publicitar este deporte tan joven.

En 1977, Jake Burton Carpenter, un joven de Vermont que llevaba desde los 14 años practicando el snurfing, impresionó a todos los asistentes de una competición de snurfing en Míchigan con las fijaciones que él mismo se había fabricado para fijar sus pies a la tabla. El mismo año, Jake Burton fundó Burton Snowboards en Londonderry, Vermont. Las tablas de nieve estaban hechas de tablones de madera flexibles y con fijaciones de esquí acuático. Al principio, muy pocas personas compraron sus tablas, sin embargo, con el tiempo Burton se convertiría en la mayor empresa de tablanieves del mundo.

Las primeras competiciones de carreras en ofrecer premios en metálico fueron en el Campeonato Nacional de Snurfing, celebrado en el Parque Estatal de Muskegon, Míchigan. Jake Burton quiso competir con una tabla de las suyas y algunos participantes protestaron ya que consideraban que no era una tabla de snurfer, no obstante, algunos participantes abogaron para que Jake Burton pudiera competir con su tabla de nieve. La solución resultante fue la creación de otro "Open" o división en la que Jake Burton ganó al ser el único participante. Esta, está considerada como la primera competición de este deporte y que, además, ofrecía un premio en metálico.

El snowboard se hizo más popular al final de la década de los 70 y los 80, los pioneros como Dimitrije Milovich, Jake Burton, Tom Sims y Chuck Barfoot (fundador de Gnu Snowboard) llegaron con nuevos diseños, mecanismos y equipamientos relacionados que poco a poco se han ido desarrollando en el mundo del snowboard y que hoy en día se conocen.

La primera Carrera Nacional de Tablanieve de Estados Unidos se celebró en 1982 en Suicide Six, Vermont. El primer Campeonato Mundial de Mediotubo se celebró en 1983, Soda Springs, California. La primera Copa del Mundo se celebró en 1985 en Zürs, Austria.

En 1990 se fundó la Federación Internacional de Tablanieve (ISF) para regular las competiciones de forma universal. Además, la Asociación de Tablanieve de los EE.UU. (USASA) regula y organiza competiciones de snowboard en la actualidad. Hoy en día, estos eventos de tablanieve de alto nivel como los X Games de invierno, el Air &Style, US Open, Juegos Olímpicos, entre otros, son emitidos y seguidos mundialmente.
Inicialmente, las estaciones de esquí adoptaron a el snowboard de una forma más lenta que el público. Durante muchos año, se creó una confrontación entre esquiadores y snowboarders. Al principio muchas estaciones de esquí denegaban el acceso a los tablistas, después se les pedía un nivel mínimo antes de subir al telesilla, ya que, existía el pensamiento de que los tablistas más novatos dejaban las pistas "limpias" de nieve, es decir, se llevaban toda la nieve y dejaban las pistas en mal estado. En 1985 solo el 7% de las estaciones de esquí de los Estados Unidos, datos muy similares a Europa, permitía el acceso a los practicantes de tablanieve. Posteriormente, con el progreso tanto en la técnica y el equipamiento, poco a poco el snowboard fue más aceptada. En 1990, la gran mayoría de estaciones de esquí tenía pistas separadas para el snowboard. Actualmente, un 97 % de todas las estaciones de esquí en EE. UU y Europa permiten el snowboard y tienen zonas específicas para el estilo libre.

Existen varias modalidades de competición con el snowboard:

A lo largo de la historia de la tablanieve algunos deportistas se han convertido en leyendas, quedando su apellido como denominación para el truco o acrobacia inventados o su estilo de practicar tablanieve. Tom Sims, pionero competidor y en producir tablas a gran escala, Craig Kelly (+) revolucionó el estilo al llamado estilo libre, el posicionamiento sobre la tabla y las fijaciones a esta, desde finales de los 80, su fallecimiento sepultado en una avalancha le encumbró a lo más alto de este deporte. Terje Håkonsen, incorporó en el "mediotubo", las rotaciones en eje vertical y horizontal de modo simultáneo entrando marcha atrás, y todas las evoluciones siguen usando su apellido; Haakon, Haakon 540º, Haakon 720º, Haakon 1080º, y así sucesivamente, Michael Michalchuck incorporó hace más de 10 años, también en mediotubo, el doble mortal hacia atrás, abriendo esta nueva vía a todos los deportes extremos. Serge Vitelli es otra leyenda, por dar nombre a los giros inclinados rozando cara o espalda.

Una correcta ejecución del snowboard requiere años de aprendizaje y un inicio temprano en el mismo. Sin embargo, se ha registrado un caso tan anecdótico como extraordinario. Se trata de un adulto que tomó contacto con este deporte por primera vez ya entrado en su tercera década de vida, y sin haber visto nunca antes la nieve. Este deportista excepcional, ha logrado impresionantes resultados en la ejecución de las piruetas, mientras desciende de las nevadas montañas de Sapporo. 
Aunque de momento no se registran otros casos similares, el chico de la tinaja, como coloquialmente se le conoce sin que esté claro el origen de este apodo, demuestra que es posible una iniciación tardía y satisfactoria en el snowboard.




</doc>
<doc id="5300" url="https://es.wikipedia.org/wiki?curid=5300" title="Surf">
Surf

El surf es un deporte acuático que consiste en realizar amplios giros y maniobras aprovechando la fuerza de una ola, sobre una tabla.

Se tiene constancia de la presencia del surf desde hace más de 500 años en las islas de Polinesia. El explorador inglés James Cook llegó a Hawái en 1778, donde conoció el Bodysurfing (similar a este deporte pero sin tabla) y el Bodyboard.

Por otro lado en el norte de Perú, los habitantes locales dejaron evidencias que muestran personas practicando dicho deporte. Los huacos son cerámicas preincaicas y en uno de ellos se muestra claramente a un hombre sobre algo similar a un trozo de madera deslizándose sobre las olas. Esto sitúa los orígenes de este deporte en América del Sur, pero fueron los polinesios los que siglos más tarde llevaron el gusto por el surf hasta lugares como Hawái.

Tiempo después, las culturas autóctonas fueron reprimidas y el surf cayó en decadencia. Además James Cook fue asesinado por los nativos. Fue entonces en el siglo XX cuando el surf se recuperó y con la llegada de turistas y militares estadounidenses en Hawái y la fama del hawaiano olímpico Duke Kahanamoku, el surf empezó a popularizarse en las costas de California y Australia, extendiéndose luego a otros países.

Esto ocurrió durante los años 50 y 60. Entonces las tablas se tallaban en madera maciza volviendo al surf una práctica muy sencilla debido a la facilidad de elaborar las tablas. Más tarde fue evolucionando hasta convertirse en un deporte completo. Así fueron creados acrobacias, movimientos, diversos diseños y materiales que permitieran masificar el surf volviéndolo multidisciplinario y polivalente. 

El surf se popularizó en los años 60 en muchos continentes. Pasando a ser practicado en casi todo el mundo. Entre los destinos más solicitados por los viajeros practicantes están Australia y el Sudeste Asiático. El surf es un deporte importante también en Latinoamérica especialmente Perú, Chile, México o Brasil, con una gran cantidad de playas aptas para este deporte.

Actualmente el surf de competición está basado en:


Dentro del surf "de tabla", o surf propiamente dicho, existen 3 categorías básicas dependiendo del tamaño y tipo de tabla:
También se definen categorías en cuanto al tipo de olas

Es interesante saber dónde se originó el surf
Aunque nadie sabe seguro cuando y dónde se inicia el Surf, se tiene conocimiento que en las Islas Sándwich (Hawái), en las que desembarcó James Cook en 1.776, pudo observar que los nativos vivían y estaban obsesionados con la práctica de este deporte. Se sabe también que en el Norte de Perú, las culturas locales dejaron indicios que muestran a hombres remontando las olas en artilugios que pueden ser los precursores de las actuales tablas de surf. Existen cerámicas pre-incaicas que avalan esta teoría (Huacos) y en alguna de ellas se muestran hombres sobre maderos o algo similar deslizándose sobre las olas, seguramente lo anterior al Caballito de Totora. Esto seguramente indica que el surf en sus primeras formas pudo haberse iniciado en esta zona de América del Sur. 
Como en todos los deportes, posee movimientos básicos, intermedios y avanzados, que son los siguientes:, 


Maniobras de LongBoard:

La dificultad de este deporte radica tanto en la velocidad como en el tamaño y la forma de las olas.
Las olas adecuadas para ser navegadas al estilo del surf son aquellas que evolucionan y rompen desarrollando la "pared" y la espuma progresivamente hacia la derecha o hacia la izquierda. Si las olas rompen sobre una superficie de roca y con poca profundidad serán más adecuadas para la práctica del bodyboard.
Para identificar las condiciones adecuadas para la práctica del surf se utiliza la descripción de diversos elementos o partes de la ola:

Se nombran varios tipos de ola en atención a su forma:, 

Se clasifican según su tamaño (en el surf mayoritario):

Para la medición del oleaje se usa internacionalmente en náutica la Escala Douglas.

El tamaño de las olas en surf se mide dependiendo de la cultura donde nos encontremos:

El comportamiento de las olas varía mucho según el fondo sobre el que rompan:
La idoneidad de uno u otro fondo depende de cada lugar:
Y la ola de Lloret única en las Islas Canarias, en la Playa de las Canteras.

El surf es un deporte muy recurrente en Latinoamérica, y especialmente en países como Perú, Puerto Rico, Brasil y Costa Rica, El Salvador, seguidos de Argentina, Uruguay, Venezuela, Chile y en menor medida: Ecuador, México, Panamá, etc. Perú fue sede del Segundo Campeonato de Surf (ISF) en el año 1965, que hizo hincapié en la conocida playa peruana de Punta Rocas. De este resultó campeón el peruano Felipe Pomar.

En Perú es un deporte muy practicado, gracias a sus buenas olas en todo el litoral nacional, la principal representante del surf peruano es la campeona mundial Sofía Mulanovich. La selección peruana de surf, por otro lado, ha conseguido múltiples galardones, incluyendo varios campeonatos mundiales en diferentes categorías.

En México existe una variedad de spots repartidos por distintas zonas en donde te ofrecen increíbles olas de todo tipo, los más destacados están en Oaxaca, Ensenada, Punta mita, Mazatlán Sayulita, Baja California, Michoacán y Pascuales Colima.

En Puerto Rico, se practica el surf en las costas de Rincón y Aguadilla, también en la isla de Culebra. La formación de las olas (especialmente en el área de Rincón) es bastante parecida a las olas en Hawái.

En Venezuela, se practica sobre todo en las costas del Mar Caribe, cuyas condiciones naturales son muy favorables (gran diversidad de playas, buen oleaje y clima agradable casi todo el año), sobre todo en la Isla de Margarita y los estados Vargas, Aragua y Sucre.

En Argentina el surf pasa de ser un deporte particular a ser masivamente conocido, especialmente en la temporada de verano, donde las playas argentinas más elegidas para la práctica de surf son Mar del Plata, Miramar, Necochea y el Balneario El Cóndor, Bajada de Picoto y el Espigon. Balneario El Cóndor queda a solo 30 km de la ciudad de Viedma, capital de río negro. Bajada de Picoto se encuentra a unos 2 km de balneario el cóndor. El Espigón es una playa ubicada en la provincia de Río Negro, a 45 km de la ciudad de Viedma. En esta playa se realizan actividades como la pesca junto al surf y el "bodyboard". El tamaño de las olas no es constante pero llegan a sobrepasar los tres metros en días de gran oleaje
Hace más de 15 años que se comenzó con la práctica del surf en estas costas pero nunca se sobrepasan de veinte tablas en el mar, porque estos deportes están empezando a hacerse más populares entre los habitantes de la comarca Viedma-Patagones, y muy rara vez llegan surfista de otros lugares a disfrutar de las olas.

Mar del Plata, Miramar y Necochea son las zonas que más olas reciben de las marejadas del sur. El punto inicial de esta zona de buenas olas es el cabo Corriente en mar del plata, lugar geográfico, y uno de los puntos más expuestos de la costa, en ese punto las rompientes apuntan al este, desde ahí hacia el sur las playas van rotando hasta en Necochea apuntar prácticamente al Sur. Entre estos 2 puntos hay aproximadamente 150 km, por lo que buscar una rompiente con el viento adecuado es un viaje que se puede realizar en el día. En invierno luego de una sudestada las olas pueden llegar a los 2,5 metros de cara.

En Chile, debido a la baja temperatura del mar, el surf se practica mayoritariamente desde la VIII región al norte, así como en Isla de Pascua. Una buena opción para los surfistas de centro de Chile es la localidad de Pichilemu, donde el último tiempo el surf se ha hecho un deporte muy popular. Sin embargo, este deporte -cada vez más masivo dentro del país- se practica mayormente en el norte de Chile, consolidando a Arica como gran sede gracias a sus condiciones climáticas. Por ejemplo el "Campeonato Internacional de Surf Arica Big Buey" y el WCT "Rip Curl Pro Search Arica Chile". Los mejores meses para la práctica del surf en Chile son los de invierno, ya que las tormentas que son frecuentes en el sur del país, producen grandes olas que azotan al norte del litoral algunos días más tarde.

En España el surf comenzó entre 1963 y 1965 en diferentes puntos de la península, casi al mismo tiempo y espontáneamente, sin tener contacto unos núcleos con otros: Asturias (1963), Cádiz (1964), Guipúzcoa (1964), Vizcaya (1964), Cantabria (1965) y en Canarias(1968).

En Málaga, pionera del Mediterráneo español, el surf comenzó en 1970 a través de la figura de Pepe Almoguera, en el barrio marinero de Pedregalejo, cuando se fabricó su propia tabla. Desde entonces, otros jóvenes del barrio se les fueron uniendo, como Francisco Soria, Javier Gabernet, Rafael García, Carlos Sauco, Pepi Almoguera, los hermanos Toño y Paco Gutiérrez Espejo, Joaquín Fernández-de las Alas, y otros varios que formaron en 1974 el primer club activo de surf del Mediterráneo español: el Málaga Surfing Club.

La revista Airberlin Magazin publicó un amplio reportaje el pasado mes de septiembre de 2012 en el que calificaba a Las Palmas de Gran Canaria como el Hawái del Atlántico

En el Principado de Asturias existe una gran tradición surfera, las playas del Principado tiene unas condiciones óptimas para la práctica de este deporte, concretamente en las localidades costeras Tapia de Casariego, Navia, Luarca, San Juan de La Arena, Salinas, Luanco, Candas, Gijón, Villaviciosa (Rodiles), Colunga, Ribadesella o Llanes.

Cantabria, de renombre internacional son conocidos Liencres, Somo, Loredo, Los Locos, El Brusco, Santa Marina, Ajo, San Vicente de la Barquera, Langre, Berria, Sardinero, Noja y Laredo. La costa de Cantabria se divide en 11 zonas con 36 sitios para surfear. Todos ellos son spots con gran atracción sobre la población Europea. Sin embargo, tal vez sean las playas de Somo y Loredo las más visitadas por su cercanía con Santander y fácil accesibilidad.

En el País Vasco la cultura de surf está muy enraizada, sus playas son el escenario perfecto y las posibilidades son muchas y variadas, en función del nivel. Destacamos cuatro grandes zonas: zona Sopelana, zona Mundaka, zona Zarautz; y zona San Sebastián.

En Galicia, es la playa de Pantín, situada a 20 km de Ferrol la playa insignia para la práctica de este deporte. Destaca también la playa de Doniños (Ferrol), Río Sieira (Porto do Son), playa de Montalvo y playa de A Lanzada (Sanxenxo), playa de Razo (Carballo) y playa de Patos (Nigrán).

El surf en Portugal se identifica por la calidad de sus olas. Gracias a su situación las costas de Portugal disfrutan de unas potentes olas durante la mayor parte del año. La costa está plagada de playas, bahías escondidas y surf spots. Las mejores mareas llegan a Nazaré y Peniche, al norte de Lisboa, una de las olas más consistente en Europa y es famosa por su tubo "Supertubos". Otras zonas populares para practicar el surf en Portugal es Ericeira, entre Lisboa y Peniche, y la Costa de Caparica, entre Almada y el Parque natural de la Arrábida. En el 1 de noviembre de 2011, el surfista hawaiano, Garrett McNamara, surfó una ola de 23,77 metros en la playa del Norte, en Nazaré, y en ese año batió el récord mundial del Guinness.

Francia es uno de los destinos míticos del surf en Europa. Se puede realizar la práctica de este deporte en playas como Lacanau, Hossegor, Anglet, Biarritz y Guethary famosas en la costa suroeste de Francia, entre Burdeos y la frontera española. Olas excepcionales, una tradición surfera asentada, un mar templado en verano y el famoso "Art de vivre" francés.

En el Reino Unido, Cornualles es un paraíso del surf situado en la punta del extremo oeste de Inglaterra y Newquay su capital, el corazón de la zona surfera. Las dos playas más populares para la práctica del surf son Croyde Bay en North Devon y Fistral Beach en Cornualles. El Reino Unido se divide en cuatro zonas de olas: La Costa Sur, Suroeste, Este y Escocia.

El surf en Irlanda se centra prácticamente en la localidad de Bundoran, capital Irlandesa del surf, en el Condado de Donegal, en la costa noroeste de la isla. Considerada como la Capital del Surf de Irlanda, está situado en la costa sur de la Bahía de Donegal y se ha convertido en un lugar de surf mundialmente famoso por la calidad de sus olas, y lugar donde se celebran muchas competiciones internacionales de Surf.

En el siglo XXI el surf ha aumentado mucho su ritmo de progresión, día a día se exploran nuevas técnicas: aéreos, maniobras en general, variantes del surf (stand up paddle), olas de gran altura, competiciones. Por su gran popularidad, se incluyó en el programa de los Juegos Olímpicos a partir de Tokio 2020.

Condición física : Adaptarse a un medio ajeno y en movimiento como el mar hace que el esfuerzo físico sea intenso y se quemen muchas calorías. Surfear reduce el tejido adiposo (en mujeres en las caderas y hombres en la barriga) y aumenta nuestra capacidad aeróbica. Mejora también aspectos como la resistencia, flexibilidad, coordinación, equilibrio y fondo respiratorio.

Actividad cardiaca: La práctica del surf beneficia el funcionamiento del corazón debido a la mejora del flujo sanguíneo, el buen estado de los lípidos sanguíneos y la presión arterial, fortaleciendo el sistema nervioso, relajándonos y ayudando a un bienestar también mental.

Actividad muscular: Practicando Surf refuerzas toda tu musculatura, sobre todo en la zona media del cuerpo, pues ahí se encuentran la mayor parte de los músculos que utilizas mientras maniobras sobre la tabla. Pero por supuesto también tus piernas, brazos y espalda se verán reforzados mientras remas y surfeas. El aumento de masa muscular y fuerza ayudará también a prevenir lesiones. Sobre todo en los brazos.

Actividad social: A través de la práctica del surf conocerás a muchos otros apasionados de este deporte. Los lazos entre surfistas son grandes y es una enorme comunidad en todo el mundo. Podrás hacer nuevos amigos con una pasión común, viajar y conocer nuevas olas y nuevas culturas.





</doc>
<doc id="5301" url="https://es.wikipedia.org/wiki?curid=5301" title="Panamá">
Panamá

Panamá, oficialmente República de Panamá, es un país ubicado en el sureste de América Central y norte de América del Sur. Su capital es la Ciudad de Panamá. Limita al norte con el mar Caribe, al sur con el océano Pacífico, al este con Colombia y al oeste con Costa Rica. Tiene una extensión de 75 475 km². Localizado en el istmo del mismo nombre, franja que une a América del Sur con América Central, su territorio montañoso solamente es interrumpido por la cuenca del Canal de Panamá, la vía interoceánica que une al océano Atlántico con el océano Pacífico, es considerado también un país transcontinental. Su condición de país de tránsito lo convirtió tempranamente en un punto de encuentro de culturas provenientes de todo el mundo. El país es el escenario geográfico del canal, obra que facilita la comunicación entre las costas de los océanos Atlántico y Pacífico y que influye significativamente en el comercio mundial. Su población actual en 2019 es de 4 170 607 habitantes.

Políticamente, su territorio está constituido por 10 provincias y por cinco comarcas indígenas desde 2014. De acuerdo con la Constitución Política de la República de Panamá, el español es la lengua oficial del país y todos los panameños tienen el deber de conocerla y el derecho a usarla. En 2006, era la lengua materna del 93,1 % de los panameños. Otras lenguas, también panameñas, son reconocidas en el sistema educativo en diversas comunidades donde es mayoritaria la población indígena.

Por su posición geográfica actualmente ofrece al mundo una amplia plataforma de servicios marítimos, comerciales, inmobiliarios y financieros, entre ellos la Zona Libre de Colón, la zona franca más grande del continente y la segunda del mundo.

Con una población superior a los cuatro millones de habitantes, tiene una posición privilegiada en varias clasificaciones de crecimiento y desarrollo de América Latina, como el índice de desarrollo humano 2018, quinto en América Latina). Panamá es el tercer país más competitivo de América Latina según el Foro Económico Mundial, pero también es el país latinoamericano con mayor crecimiento económico.
en términos absolutos, es decir, sin tener en cuenta la distribución de la riqueza

La República de Panamá recibe su nombre por la ciudad de Nuestra Señora de la Asunción de Panamá, donde se celebró el cabildo y se estableció la jurisdicción de la Real Audiencia de Panamá, la cual comprendía en su totalidad el istmo de Panamá. El nombre de Panamá prevaleció durante la pertenencia del territorio a los virreinatos de Perú y Nueva Granada, durante el periodo colonial español, y finalmente fue oficializado tras la firma del Acta de Separación de Panamá de Colombia. La etimología de la palabra Panamá es de origen indígena, probablemente de la lengua cueva.

Algunos historiadores atribuyen el nombre al majestuoso árbol llamado localmente panamá, de frondosa sombra y muy común en el área, bajo el cual se reunían familias aborígenes.

Con respecto a Ciudad de Panamá, cerca del sitio de fundación de la ciudad por Pedrarias, se encontraban pequeños asentamientos de pescadores llamados panamá, como lo indica en una carta, lo que para algunos autores pudo ser el motivo para bautizar a la ciudad con dicho nombre. Otros significados son: en lengua indígena significa "más allá" y era el nombre del cacique de esta región.

Antes de la llegada de los europeos, los territorios de Panamá estaban habitados por diversas etnias que estaban organizadas en cacicazgos, estos pueblos tenían en común que sus diversos lenguajes provenían de una familia idiomática conocida hoy como Lenguas chibchenses del grupo ístmicos. No obstante, estos pueblos formaban grupos diversos por lo que no constituían una unidad política unificada. 

A raíz de las crónicas españolas del siglo XVI, se ha podido determinar la extensión de los pueblos que existían en Panamá al momento de la conquista de América. Además de las crónicas, la historiografía panameña e internacional ha utilizado el lenguaje, la orfebrería y demás aspectos culturales, para lograr establecer el área de influencia territorial de las distintas naciones indígenas.

Entre las naciones indígenas que habitaban el istmo destacaban los Cuevas, cuyo territorio comprendía el suroeste de Panamá, tanto en sus costas caribeña y pacífica, incluyendo el Darién. Se ha fijado el río Atrato como el límite este de los dominios de dicho pueblo indígena, mientras, por el oeste, el territorio se extendería hasta Chame en el Pacífico y Quebore (Río Indio) en el Caribe.

Los cacicazgos Cuevas más importantes eran los de Pocorosa, Comagre y Careta; cuyos territorios ocupaban la actual comarca de Guna Yala. En el litoral pacífico destacaba el cacicazgo de Chochama que ocupaba una buena parte del golfo de San Miguel, al igual que la costa comprendida entre Chimán y la bahía de Chame, el archipiélago de Las Perlas y las islas de Chepillo, Taboga, Taboguilla y Otoque.

No obstante en la parte oriental del istmo existieron pueblos que no hablaban el Cueva. Entre ellos destacan los Chuchures que, provenientes de Honduras, se asentaron en Nombre de Dios. Por otro lado “los de Birú”, fueron reportados por Pascual de Andagoya y ubicados por Romoli en la cuenca alta del río Tucutí, y los de ‘Quarequa’ o ‘Careca’ que «habían venido conquistando de hacia las espaldas del Darién».

En la parte central del istmo vivían un número plural de naciones indígenas que no compartían lenguaje ni características fenotípicas comunes. Las crónicas españolas apuntan que estos cacicazgos mantenían constantes enfrentamientos bélicos entre ellos por el control territorial.

Entre los señoríos hallados por los españoles durante la conquista, en territorios de la actual provincia de Coclé, se menciona a Periquete, Totonaga, Taracuru, Penonomé. Sin embargo, se ha establecido que en Natá residía el cacique Acherse que comandaba todos esos territorios.

En el área geográfica de la actual península de Azuero se ha documentado los cacicazgos de Escoria, Usagaña, Quema, Guararé, Pocrí y París. No obstante se conoce que el cacique Cutatara de París había dominado mediante la guerra al resto de los cacicazgos vecinos.

En la parte central de la actual provincia de Veraguas estaba establecido el cacicazgo de Tabraba; hacia el norte se encontraba el cacicazgo de Urracá que se encontraba en el área de la actual Santa Fe; con el avance de la colonización este territorio albergó una importante resistencia indígena.

En las actuales provincias de Bocas del Toro y Chiriquí, se desarrollaron un número plural de tribus indígenas entre las que destacan los guaymíes, dorasques y dolegas. Estas tribus se encontraban dispersas tanto en las costas del Océano Pacífico y el Mar Caribe; como en la Cordillera Central.

El istmo de Panamá fue visitado por primera vez por los conquistadores españoles durante la expedición del escribano de Triana, Rodrigo de Bastidas, en el año 1501. Bastidas navegó la costa caribeña de la actual provincia de Colón y las islas del archipiélago de la Comarca de San Blas.
En su cuarto viaje, Colón llegó a la costa atlántica del istmo. El 2 de noviembre, llegó a una preciosa bahía en la actual provincia de Colón, a la que bautizó como el nombre de "Portobelo" o Puerto Bello.

El Reino de Tierra Firme, dentro del cual estaba el istmo de Panamá, fue dividido entre Diego de Nicuesa, quien obtuvo la gobernación de Castilla de Oro, que iba desde el río Atrato en el golfo de Urabá hasta el cabo Gracias a Dios y Alonso de Ojeda, la de Nueva Andalucía, desde el Río Atrato al Cabo de la Vela.

Santa María la Antigua del Darién fue la primera ciudad fundada por los españoles con permanencia, en la Tierra firme del continente americano.
Fue fundada por Vasco Núñez de Balboa en el 1510.
Santa María la Antigua del Darién fue la capital del territorio de Castilla de Oro hasta la fundación de la Ciudad de Panamá por Pedrarias Dávila en 1519. Pocos años después del traslado de la capital a Ciudad de Panamá, Santa María la Antigua del Darién, fue abandonada y en el año 1524 la ciudad fue asaltada y quemada por los indígenas.

En 1513, Vasco Núñez de Balboa emprende la conquista de los territorios de los caciques Careta, Ponca y Comagre, donde escucha por primera vez de la existencia de otro mar por parte de Indígena Panquiaco, hijo mayor de Comagre, donde se relataba de un reino al sur de población tan rica que utilizaban vajillas y utensilios en oro para comer y beber.

La noticia inesperada de un nuevo mar lleno de riquezas fue tomada muy en cuenta por Vasco Núñez de Balboa, quien organiza una expedición que parte de Santa María La Antigua el 1 de septiembre de 1513. El día 25 de septiembre, Núñez de Balboa se adelanta al resto de lo expedición y se interna en la cordillera del río Chucunaque, y antes del mediodía logra llegar a la cima de la cordillera desde donde logra ver en el horizonte las aguas del nuevo mar.

Cuando la expedición llega a las playas, Núñez de Balboa levantó sus manos, en una estaba su espada y en la otra un estandarte de la Virgen María, entró a las aguas hasta el nivel de las rodillas y tomó posesión del Mar del Sur en nombre de los soberanos de Castilla.

Vasco Núñez de Balboa bautizó al golfo donde llegó la expedición como San Miguel, porque fue descubierto el día de San Miguel Arcángel, 29 de septiembre y al nuevo mar como Mar del Sur por el recorrido que tomó la exploración por el istmo rumbo al sur. Este hecho es considerado por la Historia de Panamá, como el capítulo más importante de la conquista después del descubrimiento de América.

En Panamá, se han bautizado a parques y avenidas con el nombre de Vasco Núñez de Balboa. En Ciudad de Panamá, frente a las costas se erige un monumento dedicado a su memoria y a la hazaña del descubrimiento del Mar del Sur. En su honor se ha bautizado la moneda oficial de la República de Panamá con la denominación de Balboa, apareciendo su rostro en el anverso de algunas monedas. Asimismo, el principal puerto en el Pacífico del canal de Panamá y el distrito que abarca el archipiélago de las Perlas, también llevan su nombre.

La máxima condecoración otorgada por el Gobierno de la República de Panamá a personajes destacados y sobresalientes es la Orden de Vasco Núñez de Balboa en sus diferentes grados. Cabe resaltar que Panamá es el único país de América Latina que honra la memoria de un conquistador español a tan alto nivel.

La Ciudad de Panamá fue fundada el 15 de agosto de 1519 por Pedro Arias Dávila, conocido como Pedrarias, siendo la primera ciudad española en las costas del Mar del Sur u Océano Pacífico y la más antigua de tierra firme que existe hasta nuestros días como ciudad. Su fundación reemplazó a las anteriores ciudades de Santa María la Antigua del Darién y Acla, convirtiéndose en la capital de Castilla del Oro. El 15 de septiembre de 1521, recibió mediante Real Cédula el título de Ciudad y un Escudo de Armas conferido por Carlos I de España así como su lema oficial que se mantiene hasta la actualidad Muy Noble y Muy Leal Ciudad de Panamá. La Ciudad de Panamá se convirtió en el punto de partida para la exploración y conquista del Perú y ruta de tránsito para los cargamentos de oro y riquezas provenientes de todo el litoral pacífico del continente americano que se enviaban a España.

En 1671 la ciudad es atacada por las fuerzas del pirata galés Henry Morgan con intenciones de saquearla. Por medidas de seguridad y de la población y los bienes, el Capitán General de Tierra Firme, Juan Alonso Pérez de Guzmán ordena evacuar la ciudad y volar los depósitos de pólvora provocando un gigantesco incendio que destruyó totalmente la ciudad. Las ruinas de la antigua ciudad todavía se mantienen incluyendo la torre de su catedral y son una atracción turística conocida como el Conjunto monumental histórico de Panamá Viejo, reconocida como patrimonio de la humanidad.

La Ciudad de Panamá fue reconstruida en 1673, ubicada a 8km al sudoeste de la ciudad original a las faldas del cerro Ancón, conocida hoy en día como el Casco Antiguo de la ciudad.

El 15 de agosto de 1519, Pedrarias Dávila funda Nuestra Señora de la Asunción de Panamá a orillas del océano Pacífico, que aparte de responder a las instrucciones dadas por el Rey Fernando de erigir poblados, se transformó en el centro de la actividad del descubrimiento y obtención de riquezas, con la partida de expediciones hacia el istmo de Centroamérica y el Perú.
Simultáneamente a la fundación de Panamá, Pedrarias envía a su lugarteniente Diego de Albítez a repoblar Nombre de Dios en el océano Atlántico, sitio que había sido descubierto por Cristóbal Colón y ocupado con algunas chozas de paja por Nicuesa en 1510. Entre ambos puertos, se estableció el Camino Real de Nombre de Dios, una ruta en tierra firme que atravesaba el istmo de Panamá para el transporte de mercancías y metales preciosos entre ambos océanos.

Gaspar de Espinosa en compañía del piloto Juan de Castañeda parten en julio de 1519 con una expedición que visitaría las tierras de los caciques París, Escoria y Chagres, haciendo un reconocimiento de la costa septentrional del Mar del Sur, a bordo de los navíos de Balboa, el San Cristóbal y el Santa María de Buena Esperanza. En Punta Burica desembarca dispuesto a emprender su viaje de regreso a Panamá por tierra, mientras Juan de Castañeda continuaba la navegación hacia el norte hasta alcanzar el golfo de Nicoya en Costa Rica. En su camino de retorno Espinosa fue apresando indígenas con la finalidad de llevarlos a Panamá para ser repartidos en encomiendas. En 1520, Gaspar de Espinosa establece el asiento de Natá, en territorios fértiles convirtiéndose rápidamente en un centro agrícola y de frontera con Veragua. Pedrarias declara la fundación de Natá el 20 de mayo de 1522, la cual fue atacada por los indígenas dirigidos por el poderoso cacique Urracá, quien agrupó en torno suyo a los pueblos de las regiones de Chiriquí y Veraguas, creando una oposición al avance español en el área por casi una década. En 1531 muere el gran jefe indio Urracá.

Pedrarias, interesado en encontrar un estrecho marino que comunicara ambos mares, se dedicó a organizar una serie de expediciones como la de Gil González Dávila y Andrés Niño que navegaron y desembarcaron en la actual Costa Rica y luego en Nicaragua. Gracias a los indígenas González Dávila conoció la existencia de dos grandes lagos, Nicaragua y Managua, pensando erróneamente que se trataba de un estrecho entre los mares.

Otra expedición organizada por Pedrarias fue la del capitán Francisco Hernández de Córdoba, acompañado por Gabriel de Rojas, Francisco Campañón y Hernando de Soto, que partió a fines de 1523, con la misión de fundar poblaciones a lo largo de toda la tierra visitada por Gil González y Andrés Niño. Hernández de Córdoba visitó parte de Costa Rica y en 1524 fundó el asiento de Bruselas próximo a la actual Puntarenas, a orillas del lago Cocibolca fundó la ciudad de Granada y al norte del lago Managua erigió el asiento de León.

En 1523, Hernán Cortés había concluido la conquista del Imperio azteca y con el propósito de encontrar un paso o estrecho entre los dos mares, envió a Pedro de Alvarado con destino a Guatemala y a Cristóbal de Olid con dirección a la actual Honduras, creando una situación de rencillas con Pedrarias.

Hacia 1526 tanto las exploraciones enviadas por Pedrarias desde Panamá como las de Cortés desde México habían demostrado que el tan ansiado estrecho de mar no existía en Centroamérica. Para entonces ya se habían cumplido seis años desde que Fernando de Magallanes el 28 de noviembre de 1520 descubriera en el extremo meridional del continente el estrecho de los Patagones que hoy lleva su nombre.
El 20 de mayo de 1524, Pedrarias autoriza la expedición de Francisco Pizarro, Diego de Almagro y el sacerdote Hernando de Luque, la cual parte el 14 de noviembre desde Panamá hacia la conquista del Perú.

El río Chagres representó para las autoridades españolas una posibilidad de servir como parte de una ruta transístmica navegable. Con este propósito, en 1527 el Gobernador Pedro de los Ríos instruyó a Hernando de la Serna, Miguel de la Cuesta y Pedro Corso para que hicieran exploraciones en el río Chagres, los cuales determinaron que era favorable para ser utilizado en una vía para comunicar ambos mares.

En 1529, Álvaro de Saavedra Cerón fue el primero en proponer la construcción de un canal interoceánico por el Istmo de Panamá, pero en 1533 Gaspar de Espinosa le escribe al rey Carlos I de España señalándole que el Río Chagres podría hacerse navegable a un costo muy bajo, siendo la ruta más útil del mundo, afirmando que un canal para la navegación puede ser excavado. Por órdenes de la Corona española se hicieron otras exploraciones en el río Chagres durante las Gobernaciones de Antonio de la Gama y Francisco de Barrionuevo sin resultados alentadores.

El Camino Real de Nombre de Dios era casi intransitable en época de estación lluviosa por lo que se pensó en una nueva ruta. En 1536 se autorizó a la Municipalidad de Panamá a construir un almacén en Venta Cruz o Cruces a orillas del río Chagres, a siete leguas de la ciudad de Panamá (equivalente a 7 horas de marcha, a pie). Ante las deplorables condiciones en que se encontraba el Camino Real de Nombre de Dios, en 1569 el Virrey del Perú, Francisco de Toledo, ordenó construir otro camino que pasara por Cruces, el cual fue llamado Camino Real de Cruces. La mayor parte del antiguo pueblo de Cruces se encuentra bajo las aguas del lago Gatún en el canal de Panamá, aunque una parte de los restos de su antiguo emplazamiento son todavía visibles en el lugar de Venta de Cruces.

Como resultado de las exploraciones en Centroamérica y el Perú, se produce un despoblamiento de los principales asentamientos en el istmo. Esta situación es mencionada por Pedro Cieza de León en 1535, en una descripción de Ciudad de Panamá donde indica que habiendo muerto los antiguos conquistadores, los nuevos pobladores no pensaban en habitar Panamá más tiempo del necesario para hacerse ricos, sin miras a colonizar y establecerse en el istmo. Panamá dejó de ser el habitual centro de exploraciones, descubrimientos y conquista para convertirse en el sitio de paso de metales preciosos y productos americanos con destino a Europa, y a la vez de centro de comercio de manufacturas europeas con las que el Imperio español abastecía a los mercados de las Indias Occidentales. La función de ruta de tránsito fue el papel que asumió el territorio panameño durante poco más de dos siglos en la época colonial española.
Las ferias realizadas en la costa atlántica del istmo de Panamá, primero en Nombre de Dios en 1544 y a partir de 1597 en Portobelo, tenían como objetivo primordial abastecer de artículos europeos los mercados americanos y enviar con destino a España los metales preciosos procedentes del Perú. La importancia de este evento de intercambio comercial se pone de manifiesto en los datos suministrados que indican que entre 1531 y 1660, de todo el oro que ingresó a España procedente del Nuevo Mundo, el 60% cruzó por el Istmo de Panamá. La última feria se realizó en Portobelo en 1737.

La introducción de los negros en condición de esclavos provenientes de Senegal y el Congo, ofreció resistencia como antes lo hizo el indio, con levantamientos y ataques al Camino Real de Cruces, por parte de los negros cimarrones como Felipillo y Bayano. La convivencia entre blancos criollos, indios y negros trajo una mezcla de razas en el istmo.

Mediante la Real Cédula de 30 de febrero de 1535, se crea la Real Audiencia de Panamá y se confunde en la Jefatura de Panamá, los títulos de Gobernador, Capitán General, y Presidente de la Real Audiencia, creándose un Gobierno Central para el llamado Reino de Tierra Firme, cuya jurisdicción comprendía la Gobernación de Veragua y la Provincia de Castilla de Oro hasta Buenaventura y el río Atrato.

El Reino de Tierra firme en un principio constituyó un gobierno Autónomo de los virreinatos existentes en el siglo XVI, posteriormente sería puesto bajo la jurisdicción de la Nueva España, Virreinato del Perú y finalmente sería extinguido con la creación de la Comandancia General de Tierra Firme en 1751, bajo la jurisdicción del Virreinato de la Nueva Granada.

Varias Reales Cédulas del emperador Carlos I configuraron el territorio:



Durante los siglos XVI y XVII, Panamá fue blanco de constantes ataques por parte de piratas, corsarios, filibusteros y bucaneros, como Francis Drake quien sufriría una derrota definitiva frente a tropas españolas en 1596, y Henry Morgan, así como algunos intentos escoceses de colonizar el Darién, en territorios denominados por ellos como Nueva Caledonia.

Para 1746 las flotas del Mar del Sur utilizaban la ruta del cabo de Hornos, que aunque era más larga en distancia, resultaba ser más segura. En 1753 se permitió a los barcos de registro utilizar el puerto de Buenos Aires y con las reformas de Carlos III en 1764 se comienza a abrir al comercio los puertos de España y las Indias, lo cual significó para el Istmo la postración económica. Los campos adquieren importancia económica debilitando la vida urbana.

La necesidad de implantar la economización de los gastos del gobierno de Tierra Firme y de poner fin al estado de agitación en que mantenía al país la intemperancia de los Oidores, determinaron la extinción definitiva del Tribunal de la Audiencia de Panamá por Cédula del 20 de junio de 1751 y por ende del Reino de Tierra Firme. Los Territorios quedaron regidos por un gobierno netamente militar, a cargo de don Manuel Montiano, con dependencia de la autoridad del Virrey de la Nueva Granada y, en lo contencioso, de la Real Audiencia de Santafé. La sede de panamá siguió siendo sufragánea del Arzobispado de Lima. y se crea entonces La Comandancia General de Tierra Firme, cuyos límites se conservaron desde el Atrato hasta los linderos de la Capitanía de Guatemala, incluía las provincias de Panamá, Darién, Veraguas y Portobelo.

Los movimientos separatistas transforman al istmo en sitio de exportador de ejércitos realistas, pues la situación de España y sus virreinatos se había agravado y los movimientos conducían a las guerras separatistas.

La independencia de las 13 Colonias del Reino de Gran Bretaña en 1776 para constituirse en EE. UU., acrecientan los movimientos independentistas de España por parte de varios panameños, que propugnaban un régimen de libertades comerciales y civiles, contra el desgastado régimen monárquico. En 1812 se establece el Virreinato del Istmo de Panamá, como respuesta al contrabando y restableciendo el comercio por el istmo.

La invasión napoleónica a España y las victorias de Simón Bolívar en Boyacá debilitan el poder de la corona española en América, empobreciendo el comercio en el istmo. En 1815, Simón Bolívar en su profética carta de Jamaica habla de la asociación de los estados del Istmo de Panamá hasta Guatemala en una sola nación, la cual es vista con admiración por los panameños.

El movimiento panameño de independencia de la Corona Española se inicia el 10 de noviembre de 1821 con los eventos del Primer Grito de Independencia en la Villa de Los Santos por Rufina Alfaro y Segundo Villareal, el cual contó con el respaldo de otras poblaciones como Natá, Penonomé, Ocú y Parita.

El ejército realista de la Ciudad de Panamá estaba al mando del Coronel José de Fábrega, criollo oriundo de Veraguas, lo cual fue aprovechado por los istmeños, obteniendo la complicidad del Coronel Fábrega, las sociedades patrióticas y el clero, que contribuyó económicamente al movimiento. El 28 de noviembre, el Ayuntamiento convocó a Cabildo Abierto y en acto solemne, en presencia de las autoridades militares, civiles y eclesiásticas, se declararon rotos los vínculos que ataban al Istmo de Panamá con España. Entre los personajes ilustres se encontraban José Higinio Durán y Martell, Obispo de Panamá, Carlos de Icaza Arosemena, Mariano Arosemena, Juan de Herrera, Narciso de Urriola, José de Alba, Gregorio Gómez, Manuel María Ayala, Antonio Planas, Juan Pío Victorias, Antonio Bermejo, Gaspar Arosemena y Casimiro del Bal.

El 30 de noviembre de 1821 las fragatas de guerra Prueba y Venganza llegan a la bahía de Panamá acompañadas a buscar al resto de las tropas españolas. Los capitanes españoles José de Villegas y Joaquín de Soroa firman un tratado de paz con el General José de Fábrega (ascendido a General y nombrado jefe civil y militar del istmo por Simón Bolívar) el 4 de enero de 1822, entre la monarquía española y los patriotas donde acuerdan la no agresión a los territorios del istmo y la retirada de las tropas y todos los barcos de la Corona Española de la nueva nación istmeña.

La falta de presupuesto, el poco armamento militar con el que se contaba y la inseguridad de ser reconquistados por España, pone en peligro el seguir con la aventura independentista del istmo, por lo que se proponen la unión con algunas de las nuevas naciones americanas, entre ellas los vecinos de la unión centroamericana y la nación del Perú que había sido el principal socio comercial del Istmo en la época Virreinal.

Sin embargo, los patriotas panameños admirando el liderazgo y la visión de Simón Bolívar, y por la previa pertenencia del istmo al Virreinato de Nueva Granada, toman la medida de unirse voluntariamente a la Gran Colombia.

Declarada su independencia de España, el 28 de noviembre de 1821, los gobernantes de Panamá toman la decisión de unirse voluntariamente a la República de Colombia, la cual estaba conformada por las actuales Colombia, Ecuador, Venezuela y Panamá y que en su máximo periodo de expansión incluyó territorios de las actuales Costa Rica, Nicaragua y Perú.

La falta de presupuesto, el poco armamento militar con el que se contaba y la inseguridad de ser reconquistados por España, pone en peligro el seguir con la aventura independentista del istmo, por lo que se propone la unión con algunas de los nuevos Estados Americanos, entre ellos los vecinos de la unión centroamericana y la nación del Perú, que había sido el principal socio comercial del Istmo en la época colonial. Pero finalmente, mediante la aprobación de los próceres, se decide la unión voluntaria a la República de Colombia de Simón Bolívar, unión que fue motivada debido a la gran admiración y liderazgo de Bolívar en las campañas independentistas del Sur de América.

El Congreso Anfictiónico de junio de 1826, bajo el ideal de Simón Bolívar, reúne en la ciudad de Panamá a representantes de los nuevos países del continente americano como Argentina, Bolivia, Brasil, Centroamérica, Estados Unidos, Colombia, Chile, México y Perú, como una confederación en defensa del continente contra posibles acciones de la Liga de la Santa Alianza conformada por las potencias europeas y sus reclamaciones de territorios perdidos en América.


En 1826, mismo año en que se celebró en la capital istmeña el famoso Congreso internacional Bolivariano, Panamá rechazó la constitución bolivariana. Más este notable acontecimiento, no fue óbice para que en ese año se produjera la primera tentativa de separación de la Gran Colombia. Sucede que el congreso colombiano hacía caso omiso de las solicitudes de franquicias comerciales para el istmo, lo cual frustraba las aspiraciones panameñas. En consecuencia, surgió un movimiento separatista para convertir a Panamá en un país hanseático, bajo la protección del Reino Unido y los Estados Unidos. El movimiento fue, sin embargo, reprimido por los militares colombianos destacados en el istmo.


En 1830 ocurre un movimiento que intentó la separación de Panamá de la Gran Colombia, la cual atravesaba por un caos político debido a que Venezuela y Ecuador tomaron la decisión de separarse de la confederación, el general Antonio José de Sucre había sido asesinado y Bolívar desistió del gobierno. El general José Domingo Espinar, Comandante Militar del Istmo, declara la separación de Panamá el 26 de septiembre de 1830, al no estar de acuerdo con la inestabilidad del gobierno de Joaquín Mosquera, sucesor de Bolívar. Espinar le ofrece a Bolívar el gobierno del istmo, para que luchara por la adhesión de los demás países de la confederación, sin embargo Bolívar se encontraba enfermo y declina el ofrecimiento, pidiéndole a Espinar que reintegrara el Istmo de nuevo a la Gran Colombia. Panamá fue reintegrada a la confederación el 11 de diciembre de 1830, demostrando la posibilidad de ser un Estado independiente de la Gran Colombia.

El general Fábrega no apoyaba la decisión de reintegro del istmo por parte de Espinar y se marcha hacia Veraguas, dejando a cargo del control militar de la ciudad de Panamá al coronel Juan Eligio Alzuru. Los enemigos de Espinar convencen a Alzuru de aprisionarlo y enviarlo al destierro. Con la idea de proclamarse dictador, Alzuru busca apoyo en el pueblo panameño y su sentido nacionalista, dando como resultado la Separación de Panamá del 9 de julio de 1831. Alzuru se convirtió en un dictador y pierde el apoyo de la población panameña. La llegada al istmo del coronel Tomás Herrera, en cooperación con Fábrega y demás panameños ilustres, Alzuru es apresado y fusilado. Meses después, la nación del istmo se vuelve une a la Nueva Granada. Sin embargo, la falta del liderazgo de Simón Bolívar, deja ver entre los panameños que formar parte de la República de la Nueva Granada era innecesario, naciendo así sociedades y partidos con ideales separatistas en Panamá.

Separados de la Gran Colombia, los departamentos que conformaban las regiones del norte y sur, surgieron dos nuevos países denominados Estado de Venezuela y Estado del Ecuador.

Las Provincias que geográficamente ocupaban la parte central de la desintegrada Gran Colombia, que en ese entonces comprendía los antiguos departamentos de Boyacá, Cauca, Cundinamarca, Magdalena e Istmo decidieron formar un nuevo Estado.

Mediante el Convenio de Apulo (llevado a cabo el 28 de abril de 1831), el general Rafael Urdaneta, último presidente de la Gran Colombia, entregó el mando a Domingo Caicedo (3 de mayo de 1831). El nombre provisorio adoptado por la república granadina a partir de ese momento, fue proclamado como Estado de Nueva Granada.

El 29 de febrero de 1832 la Convención Nacional, conformada por los representantes de las provincias de Antioquía, Barbacoas, Bogotá, Cartagena, Mompós, Neiva, Pamplona, Panamá, Pasto, Popayán, Socorro, Tunja, Vélez y Veraguas, sanciona una nueva constitución por medio de la cual el país se denominaba República de la Nueva Granada.


El Estado del Istmo fue una antigua república independiente que abarcaba el istmo de Panamá; se constituyó el 18 de noviembre de 1840, separándose de la República de la Nueva Granada.

A excepción de la separación definitiva en 1903, sería el más exitoso de los intentos de
separación que tendría Panamá. Su único jefe de Estado fue el General Tomás Herrera. La independencia no fue reconocida por la Nueva Granada, aunque sí fue reconocida internacionalmente por Costa Rica.

La guerra granadina de 1839 al mando de general José María Obando, quien 10 años atrás asesinara a Sucre, lanzó a la región a un conflicto armado, al cual los habitantes del istmo se sentían ajenos y preferían evitar. Desistiendo de entrar a la guerra, se creó una junta popular reunida en la ciudad de Panamá el 18 de noviembre de 1840, para declarar la separación de Panamá de la Nueva Granada por segunda vez, bajo el nombre del Estado del Istmo. Encabezado por el Coronel Tomás Herrera, se redacta la primera constitución panameña, se organiza la economía y las instituciones políticas de la nación. Costa Rica y EE. UU. reconocieron al nuevo país. Tras meses de negociación el gobierno de Bogotá logra convencer al Coronel Herrera de reintegrar al istmo bajo el acuerdo de no emprender castigo contra los secesionistas istmeños. Haciendo caso omiso a lo acordado, una vez reintegrado el istmo, el Coronel Herrera es desterrado y borrado del escalafón militar.

Al reintegrarse el istmo de Panamá a la Nueva Granada en 1841, las autoridades neogranadinas contemplaron la idea de negociar con el Reino Unido, Francia y EE. UU., garantías para que la Nueva Granada mantuviera el control y soberanía sobre el Istmo de Panamá y sus habitantes. Con ese propósito, el Ministro de Relaciones Exteriores de la Nueva Granada, Manuel María Mallarino y el encargado de los negocios estadounidenses Benjamin Bidlack, firman el 12 de diciembre de 1846 el tratado Mallarino-Bidlack, en donde la Nueva Granada le solicitaba a EE. UU. que le garantizara la posesión y soberanía del Istmo de Panamá, ofreciéndole a cambio ventajas para el transporte a través del territorio panameño de sus mercancías, correos y pasajeros. Asimismo, los Estados Unidos se compromete a garantizar la neutralidad del istmo y el libre tránsito entre los océanos Pacífico y Atlántico, produciéndose la entrada del ejército estadounidense en territorio panameño y abriendo la puerta al intervencionismo en Panamá. Con este tratado se inician formalmente las relaciones entre Panamá y los Estados Unidos, trayendo como consecuencia un retraso de la separación del Istmo de Panamá de la Nueva Granada, al impedir movimientos de emancipación durante la segunda mitad del siglo XIX.


En 1850 el general José Domingo Espinar y E. A. Teller, editor del periódico "Panamá Echo", llevan a cabo una revolución la madrugada del 29 de septiembre, que termina con la segunda separación de Panamá de la Nueva Granada. Obaldía, gobernador del Istmo, no estaba de acuerdo con esta separación ya que veía al istmo todavía no preparado para asumir el control de su destino, convenciendo de desistir y reintegrar nuevamente al istmo.

La estructura centralista que se venía implantando en la República de la Nueva Granada después de la disolución de la Gran Colombia y que fue ratificada por la constitución de 1843, fue rápidamente afectada por los sentimientos separatistas de las diferentes regiones del país, particularmente en aquellas muy lejanas de la capital como fueron las que se encontraban en el istmo de Panamá, las cuales demandaban autonomía interna.

Justo Arosemena, estadista elegido representante del Istmo ante el Congreso Granadino, considerado el principal teórico de la nacionalidad panameña, logró el 27 de febrero de 1855 que se incorporase a la constitución, por medio de un Acto Legislativo, la creación del Estado Federal de Panamá, el primer estado federal dentro de la Nueva Granada: Panamá.

La fiebre del oro en California, produjo la migración de viajeros de todo el mundo por diversas rutas, convirtiendo a Panamá como la vía más corta y factible entre el este y el oeste del continente americano, haciendo retomar la idea de la construcción de vías de comunicación como canales y ferrocarriles para el paso de mercancías y pasajeros. Los derechos para la construcción y administración de la obra por parte de los Estados Unidos en territorio panameño fueron negociados por el gobierno de Bogotá a través del Convenio Paredes-Stephens. El 28 de enero de 1855 se inaugura el ferrocarril de Panamá por parte del presidente de la Nueva Granada, el panameño José de Obaldía, como una de las obras de ingeniería más importantes de esa época que atravesaba el istmo. Bajo el liderazgo de William J. Aspinwall, John L. Stephens y James L. Baldwin, se completa la construcción del ferrocarril, demostrando un gran valor y resistencia a los intensos trabajos y lucha contra las enfermedades.

El 15 de abril de 1856 ocurrió una serie de hechos violentos entre panameños y estadounidenses conocidos como el incidente de la "tajada de sandía". El estadounidense Jack Olivier, decide comprarle al panameño José Manuel Luna una tajada de sandía, la cual se comió y por la que se negó a pagar un real o 5 centavos de dólar. Esto generó una discusión que finalizó cuando Olivier sacó un arma y disparó, escapando luego del lugar. Esto provocó una pelea entre panameños y estadounidenses, donde se termina por incendiar las instalaciones del ferrocarril, provocando que los soldados estadounidenses reprimieran a la población panameña, con un saldo de 16 muertos estadounidenses y 2 muertos panameños. El gobierno de Estados Unidos acusó a la Policía de Nueva Granada de haberse puesto de parte de los panameños y permitirles asaltar y saquear propiedades estadounidenses, indicando la incapacidad de mantener el orden y suministrar protección adecuada para el tránsito estadounidense por Panamá.

El 19 de septiembre de ese año, el ejército estadounidense desembarca un destacamento militar para la protección de la estación de ferrocarril y restablecer el orden en la Ciudad de Panamá. Esta ocupación es considerada el primer caso de intervención armada en Panamá por parte del gobierno estadounidense, con el motivo de garantizar la neutralidad y el libre tránsito a través del istmo. El 10 de septiembre de 1857 el gobierno neogranadino acepta su culpabilidad y firma el Tratado Herrán-Cass, pagando una indemnización de US$ 412 394 (dólares estadounidenses en oro), por los daños causados por los panameños.

Mediante la Constitución Política de Colombia de 1863, Panamá se convirtió en Estado Soberano bajo el esquema que constituyó a los Estados Unidos de Colombia en una nación federalista.
El 5 de julio de 1874 se funda la "Compagnie Universelle du Canal Interocéanique" por parte del conde De Lesseps, con el propósito de construir un canal a nivel por Panamá. Los franceses iniciaron los trabajos en enero de 1881, pero los grandes gastos y el poco control existente, sumado al desconocimiento de la forma de transmisión de enfermedades en la región como la fiebre amarilla y la malaria se convirtieron en el principal obstáculo para la construcción del canal. Entre los trabajadores altamente calificados que llegaron al istmo para la construcción del canal por parte de Francia se encontraba el ingeniero francés Phillipe Bunau-Varilla, graduado de la École Polytechnique y de la École de Ponts et Chaussées, que a la edad de 27 años es designado Jefe Interino de la Compañía del Canal.
La "Compagnie Universelle du Canal de Panama" fue intervenida y liquidada el 15 de septiembre de 1889. Como causas probables para explicar el fracaso se indican una mala administración, corrupción, alta mortalidad por enfermedades tropicales y la no aceptación por parte del Conde de Lesseps de no cambiar el proyecto de canal a nivel por uno de esclusas, como alternativa y recomendación de ingeniería para poder concluir la obra. En esfuerzos desesperados por salvar los dineros de la compañía, se autoriza a vender activos y derechos en el istmo a los Estados Unidos, por parte de Bunau-Varilla. La aventura francesa en el istmo duró diez años a un costo aproximado de 1.400 millones de francos y una pérdida de vidas humanas cercana a los 20 000 muertos.

Con la Constitución Política de Colombia de 1886, Panamá se convirtió en departamento de la República de Colombia, sometido a la autoridad directa del Gobierno, y administrado con arreglo a leyes especiales. Entre 1899 y 1902 se desata la Guerra de los Mil Días entre liberales y conservadores, convirtiendo al istmo en un sangriento campo de batalla donde muere gran parte de la juventud panameña, como lo reflejan las batallas del puente de Calidonia en julio de 1900 y la de Aguadulce en febrero de 1901. El 22 de noviembre de 1902 conservadores y liberales firmaron en el barco de guerra estadounidense "Wisconsin", el pacto llamado la "Paz del Wisconsin", donde se da por terminado el conflicto. En noviembre de 1902 es capturado Victoriano Lorenzo, con el argumento de que no compartía el acuerdo de paz y que tomaría de nuevo las armas. El gobierno colombiano, temeroso de que el guerrillero panameño fuera puesto en libertad decide condenarlo a muerte presentándolo como un delincuente común. El 15 de mayo de 1903 es ejecutado en la ciudad de Panamá el caudillo liberal Victoriano Lorenzo. Su cadáver nunca fue entregado a sus familiares y amigos.

En enero de 1903 se firma el Tratado Herrán-Hay entre Estados Unidos y Colombia para finalizar la construcción del canal por territorio panameño, el cual luego no fue ratificado por el senado colombiano el 12 de agosto.

Si bien es cierto que la independencia panameña de España fue un movimiento ajeno a la revolución bolivariana, la unión voluntaria del istmo a la Gran Colombia, en busca de un mejor futuro bajo el liderazgo de Simón Bolívar, fue una decisión tomada por los istmeños en 1821, la cual estuvo marcada por las situaciones adversas vividas en las diferentes repúblicas colombianas como guerras civiles, enfrentamientos políticos y una mala situación económica.

Luego de 17 intentos de separación y 4 separaciones declaradas con un posterior reintegro a Colombia, el fracaso de la construcción del canal por parte de los franceses, la Guerra de los Mil Días trasladada a territorio panameño, el fusilamiento del caudillo liberal Victoriano Lorenzo, el rechazo del senado colombiano al tratado Herrán-Hay para la construcción del canal interoceánico por parte de los Estados Unidos sirven de detonante para un nuevo movimiento separatista liderado por José Agustín Arango, Manuel Amador Guerrero, Carlos Constantino Arosemena, General Nicanor A. De Obarrio, Ricardo Arias, Federico Boyd, Tomás Arias y Manuel Espinosa Batista.

Según algunos historiadores,

el político istmeño José Agustín Arango conspiró en secreto con inversionistas de Wall Street la preparación del movimiento separatista y conformó una junta revolucionaria clandestina destinada a separar el istmo de la soberanía colombiana, y así poder negociar directamente con Estados Unidos la construcción del canal interoceánico por Panamá, ya que los Estados Unidos exploraba la posibilidad de la construcción de la vía entre Nicaragua y Costa Rica. Por su parte, Manuel Amador Guerrero viajó en secreto a los Estados Unidos en busca de apoyo para el plan. Asimismo, el movimiento obtuvo en Panamá el respaldo de importantes jefes liberales y el apoyo del comandante militar Esteban Huertas, acordándose la puesta en marcha del plan separatista para un día no definido del mes de noviembre de 1903.

Los insistentes rumores sobre un movimiento en Ciudad de Panamá, hicieron que Colombia movilizara al Batallón Tiradores desde Barranquilla, con instrucciones para reemplazar al Gobernador José Domingo de Obaldía y al General Esteban Huertas, quienes ya no gozaban de confianza por parte del gobierno de Bogotá.

La mañana del 3 de noviembre de 1903, desembarca en Ciudad de Colón el Batallón Tiradores, al mando de los generales Juan B. Tovar y Ramón G. Amaya. El contingente armado debió ser transportado hacia la Ciudad de Panamá a través del Ferrocarril de Panamá, quienes actuaron en complicidad con el movimiento separatista. Sin embargo los generales y altos oficiales accedieron a transportarse a la ciudad de Panamá sin sus tropas.

Una vez llegados a Ciudad de Panamá, Tovar, Amaya y sus oficiales fueron arrestados por órdenes del general Esteban Huertas, quien comandaba el Batallón Colombia, de cuya jefatura pretendían reemplazar.

La decisión del general Huertas de apoyar el movimiento separatista y arrestar a los generales colombianos dependió del apoyo que le brinda el general Domingo Díaz quien junto al pueblo del arrabal de Santa Ana tomaron las armas, formando un ejército de más de mil panameños listos a defender la separación. La flota naval anclada en la bahía de Panamá se rindió sin oponer resistencia.

En Colón quedó la tropa del Batallón Tiradores bajo el mando del coronel Eliseo Torres, quienes fueron sometidos por las fuerzas separatistas y obligados a zarpar del istmo rumbo a Colombia.

La Ciudad de Panamá se encontraba conmocionada y en los barrios se escuchaban los gritos de celebración y festejo de la naciente República de Panamá. En la noche del 3 de noviembre de 1903 el Consejo Municipal de la ciudad de Panamá, presidido por Demetrio H. Brid se reunió bajo la voluntad del pueblo de ser libre y de establecer un gobierno propio, independiente, y soberano, sin la subordinación de Colombia, bajo el nombre de República de Panamá, decisión que halló inmediatamente respaldo en el resto del país en los subsigientes días en poblados como Natá de los Caballeros, Chitré, Soná, Santiago de Veraguas entre otros. Ante el surgimiento de un gobierno de facto, Demetrio H. Brid se convirtió entonces en el primer Presidente de facto de la República de Panamá.
El Consejo Municipal de Panamá, por intermedio de su Presidente, estableció el 4 de noviembre a una Junta Provisional de Gobierno, quedando conformada por José Agustín Arango (Presidente), Federico Boyd y Tomás Arias, quienes ejercieron funciones hasta el 19 de febrero de 1904 cuando la Convención Nacional Constituyente designó a Manuel Amador Guerrero como primer Presidente Constitucional de la República de Panamá.

Hubo varios intentos por parte del gobierno colombiano para revertir la separación del istmo, desde reuniones de alto nivel entre representantes de Bogotá y Ciudad de Panamá, ofrecimientos políticos como la aprobación del tratado del canal que había sido rechazado y el traslado de la capital de Colombia a Ciudad de Panamá, así como un fracasado intento de invasión militar a través de las selvas del Darién y hasta la invocación del tratado Mallarino-Bidlack que exigía a los Estados Unidos someter militarmente al pueblo panameño a fin de restablecer una soberanía colombiana sobre el istmo. Sin embargo la decisión para los panameños ya estaba tomada y la República de Panamá fue rápidamente reconocida por las naciones latinoamericanas, los Estados Unidos y las potencias europeas.

El 30 de marzo de 1922, el Congreso de Estados Unidos ratificó el tratado Thompson-Urrutia, tratado que fue firmado en 1914, que concedía a Colombia una indemnización por 25 millones de dólares, con el propósito de "eliminar todas las desavenencias producidas por los acontecimientos políticos ocurridos en Panamá en 1903", además de otorgarle a Colombia el derecho a tránsito gratuito por el canal para buques de guerra y tropas. A raíz de dicho tratado se produce el intercambio de embajadores, Nicolás Victoria Jaén por Panamá y Guillermo Valencia por Colombia, lo que marca el inicio de relaciones diplomáticas y el reconocimiento de ambos países.

Una vez declarada la Separación de Panamá de Colombia, el nuevo gobierno por medio de su embajador plenipotenciario Philippe-Jean Bunau-Varilla, logra la firma de un tratado para la construcción de un canal interoceánico por el istmo con el gobierno de Estados Unidos de América. El Tratado Hay-Bunau Varilla permitió la construcción de la vía que había quedado inconclusa por el grupo francés de Ferdinand de Lesseps y el gobierno de Colombia. La sorprendente obra de ingeniería fue terminada en 1914 utilizando tecnología avanzada para la época como motores eléctricos con sistemas de reducción para mover las compuertas de las esclusas, sistemas de vías de ferrocarril para movilizar las toneladas de material excavado y la construcción del lago Gatún, el lago artificial más grande del mundo hasta esa época. Algunos aspectos en salud pública resultaron de relevancia ya que se consideraron como uno de los obstáculos que motivaron el fracaso de la empresa francesa. El saneamiento y fumigación de las áreas, así como la reconstrucción de los acueductos y alcantarillados de las ciudades de Panamá y Colón fueron decisivos.
Los tratados del canal concedían la administración de una franja de terreno de 10 millas de ancho a lo largo de la vía interoceánica al gobierno de los Estados Unidos, que aun cuando se reconocía la soberanía de Panamá generó situaciones de conflicto entre ambas naciones en décadas siguientes.

Las controversias políticas surgidas por la interpretación de los tratados, eran consideradas como una amenaza a la soberanía panameña y acentuaban las diferencias entre las autoridades del Istmo y las de la Zona del Canal. En 1914, el presidente Belisario Porras plantea por primera vez la necesidad de un nuevo tratado sobre el Canal de Panamá.

El Tratado Arias-Roosevelt de 1936, firmado por los presidentes Harmodio Arias Madrid de Panamá y Franklin Delano Roosevelt de Estados Unidos, anula el principio de la intervención militar estadounidense en los asuntos internos del estado panameño, cambiando el concepto jurídico de país protegido por Estados Unidos para garantizar su independencia.

En 1948 se crea la Zona Libre de Colón como una institución autónoma del estado panameño, por el Presidente Enrique A. Jiménez, a través de una zona franca que aprovecha la posición geográfica, los recursos portuarios y el canal como paso de rutas navieras mundiales.
La firma del Tratado Remón-Eisenhower de 1955, entre los presidentes José Antonio Remón Cantera de Panamá y Dwight David Eisenhower de Estados Unidos, le otorga nuevas ventajas económicas y el pago de arriendos a Panamá por el canal.
El Puente de las Américas, la estructura sobre el Canal de Panamá que une por vía terrestre el istmo, es inaugurado el 12 de octubre de 1962.

El 9 de enero de 1964, estudiantes del Instituto Nacional lideran un movimiento que reclama la izada de la bandera panameña junto a la estadounidense en la zona del canal, según los acuerdos Chiari-Kennedy de 1962, terminando en disturbios estudiantiles y enfrentamientos con la población civil. Como medida para controlar la situación, el gobernador de la Zona del Canal autoriza al ejército estadounidense quien abre fuego contra civiles panameños dejando un saldo de 21 muertos y más de 300 heridos. El Presidente de Panamá Roberto F. Chiari, en una situación sin precedentes en el continente americano, rompe relaciones diplomáticas con los Estados Unidos de América y declara el no reinicio de las mismas hasta que se acordara abrir negociaciones para un nuevo tratado. En abril de ese año, ambas naciones reasumen relaciones diplomáticas y el presidente estadounidense Lyndon Johnson accede a iniciar conversaciones con el propósito de eliminar las causas de conflicto entre ambas naciones.

En 1965, Panamá y los Estados Unidos firmaron la Declaración Robles-Johnson, entre los presidentes Marco Aurelio Robles de Panamá y Lyndon Johnson de Estados Unidos, en los cuales se tocaron temas como la administración del canal, la exploración para un canal a nivel por una nueva ruta, y la defensa de la vía acuática.
El 11 de octubre de 1968, a solo unos días de haber asumido la presidencia Arnulfo Arias Madrid, los mandos medios de la Guardia Nacional, liderados por Boris Martínez dan un golpe de estado, en el comunicado oficial los golpistas señalaron que: "el intento por violar la voluntad popular en las elecciones legislativas, así como la integración ilegal del Tribunal Electoral, los había llevado a adoptar la decisión de asumir el poder por medio de un gobierno provisional que preparara el retorno al orden democrático". estableciendo el inicio de una dictadura militar que trajo consigo exilios, asesinatos, desapariciones y corrupción al país. Un año después asume el mando de la Guardia Nacional el General de Brigada Omar Torrijos. En 1972 el gobierno militar del General Torrijos emite una nueva Constitución Política en la cual se le reconoce como líder del proceso revolucionario del 11 de octubre y jefe del estado panameño. En 1977 el general Torrijos en calidad de jefe de Estado de Panamá y el presidente de EE. UU., Jimmy Carter, firman los Tratados Torrijos-Carter que establecen la entrega de la administración del Canal de Panamá y el cierre de todas las bases militares estadounidenses en el territorio de Panamá. 

Torrijos implementó una política populista, con la inauguración de escuelas y la creación de empleo, la redistribución de tierras agrícolas (que fue la medida más popular de su gobierno). Las reformas fueron acompañadas de un importante programa de obras públicas. También se enfrenta a las multinacionales norteamericanas, que exigen aumentos salariales para los trabajadores y la redistribución de 180 000 hectáreas de tierras no cultivadas. En febrero de 1974, siguiendo el modelo de la OPEP para el petróleo, intentó formar la Unión de Países Exportadores de Banano con los demás Estados centroamericanos para responder a la influencia de estas multinacionales, pero no obtuvo su apoyo. Su política promueve el surgimiento de una clase media y la representación de las comunidades indígenas. En 1981 muere el General Torrijos en un accidente aéreo.
En agosto 1983 asciende a comandante en jefe de la Guardia Nacional el General de Cuatro Estrellas, Manuel Antonio Noriega, quien transforma la institución armada en las Fuerzas de Defensa de Panamá. Durante los siguientes años, el país es bloqueado económicamente por EE.UU. y cae en una recesión económica y social, pues sufre una contracción del PIB por dos años seguidos (1987: -1,8), (1988: -13,3). En mayo de 1989, por instrucciones del General Noriega, son anulados los resultados electorales para elecciones presidenciales, suspendiendo en septiembre la Constitución y asumiendo el control de la nación panameña en calidad de jefe del Gabinete de guerra, declarando a Panamá en estado de guerra con EE. UU. Según fuentes nacionales panameñas, instituciones sociales, organismos gubernamentales y sociedad civil, existen estimaciones de que hubo casi unas 3000 víctimas fatales entre soldados de las Fuerzas de Defensa de Panamá y población civil

La Comisión para la Defensa de los Derechos Humanos en Centroamérica (CODEHUCA) estimó entre 2500 y 3000 civiles muertos y la Comisión para la Defensa de los Derechos Humanos en Panamá (CONADEHUPA) estimó en 3500 el número de muertos civiles.

El 20 de diciembre de 1989 el ejército de EE. UU. invadió Panamá, capturando al general Noriega quien fue llevado ante los tribunales estadounidenses, marcando el fin de la dictadura militar en Panamá.
Guillermo Endara Galimany, ganador de las elecciones de 1989 mismas que fueran anuladas por el General Noriega, asume el cargo de presidente y restablece el orden constitucional. El 10 de febrero de 1990, el gobierno del presidente Endara emitió un decreto ejecutivo en el que reorganizaba la fuerza policial. De acuerdo con el decreto ejecutivo, las Fuerzas de Defensa de Panamá quedaban abolidas con efecto retroactivo al 22 de diciembre de 1989 y en su lugar se creaban una Policía Nacional (PN), un Servicio Marítimo Nacional (SMN), un Servicio Aéreo Nacional (SAN) y un Servicio de Protección Institucional (SPI), más tarde el 15 de noviembre de 1992 se celebra en referéndum de reformas constitucionales entre las que figuraba la abolición del Ejército, el cual fue rechazado por casi el 60% de los votos, tiempo después, la antigua "Asamblea Legislativa" aprueba la abolición del ejército, que lleva a la nación por primera vez desde 1968 a un proceso electoral transparente en 1994, donde gana el candidato de oposición Ernesto Pérez Balladares.

Mireya Moscoso, viuda del expresidente Arnulfo Arias, gana las elecciones en 1999, convirtiéndose en la primera mujer que preside el gobierno panameño. El 31 de diciembre de 1999, 22 años después de la firma del tratado Torrijos- Carter, la República de Panamá asume el control total del Canal de Panamá.

En mayo de 2004 gana las elecciones Martín Torrijos Espino, hijo del general Omar Torrijos. Ocupó el cargo desde septiembre del mismo año, hasta el 30 de junio de 2009.

El día uno de julio de 2009 el empresario Ricardo Martinelli toma posesión de la administración del gobierno hasta el año 2014.
En febrero de 2011, el vicepresidente y excanciller, Juan Carlos Varela anunció que la extradición de Manuel Antonio Noriega, era un hecho. Por su parte el Departamento de Justicia de Estados Unidos comunicó no tener inconvenientes para que Noriega regresara al país, en donde respondería a la justicia por varios delitos cometidos.

En las elecciones presidenciales del 4 de mayo de 2014, fue elegido como presidente a Juan Carlos Varela y como vicepresidente a Isabel Saint Malo, por parte de la alianza "El Pueblo Primero" conformado por el Partido Panameñista y el Partido Popular, con el 39,1% de los votos (724 762 votos). Varela sucedió a Martinelli como presidente de Panamá el 1º de julio de 2014.

La República de Panamá es un Estado Independiente y Soberano, asentado en un territorio propio, en donde se observan y respetan los derechos individuales y sociales establecidos en la Constitución Política. La voluntad de las mayorías, está representada por el libre sufragio.

El Poder Público emana del pueblo y se ejerce por medio de tres Órganos: Legislativo; Ejecutivo y Judicial, armonizados en la separación, unidos en la cooperación y limitados por el clásico sistema de frenos y contrapesos.

Se encuentran tres organizaciones independientes cuyas responsabilidades están claramente definidas en la Constitución Política:

La Constitución Política de la República de Panamá, reformada por los Actos Reformatorios de 1978 y por el Acto Constitucional de 1983, presenta un gobierno unitario, republicano, democrático y representativo.

Formado por el Presidente de la República, el Vicepresidente de la República y los Ministros de Estado, que conforman el Consejo de Gabinete.

El Presidente será elegido por sufragio universal directo, por un período de cinco años, de igual manera será elegido el vicepresidente (Título VI, Capítulo 1, Constitución Política de la República de Panamá).

Le corresponde administrar justicia en forma permanente, gratuita y expedita en nombre de la República y conforme a la ley.

El Órgano Judicial de Panamá está constituido por la Corte Suprema de Justicia, los Tribunales y los Juzgados que la Ley establezca, según la Constitución Política de la República de Panamá (Título VII, Capítulo 1).

Para los efectos jurisdiccionales en lo judicial, se ha dividido el territorio de la República de Panamá en cuatro distritos judiciales. Estos se dividirán en Circuitos Judiciales.

El primer Distrito Judicial comprenderá las Provincias de Panamá, Provincia de Colón, Provincia de Darién y la Comarca Guna Yala; el segundo Distrito Judicial estará formado por las Provincias de Cocle y Provincia de Veraguas; el tercer Distrito Judicial por las Provincias de Chiriquí y Provincia de Bocas del Toro; y el Cuarto Distrito Judicial por las Provincias de Herrera y Provincia de Los Santos.

Mantiene distintas jurisdicciones judiciales penales, civiles, de comercio, de familia, contencioso administrativo, marítima entre otras, teniendo más notoriedad y aceptación ciudadana la Jurisdicción Penal desde la entrada en vigencia del Sistema Penal Acusatorio que implicó la reorganización de la Justicia en sus distintos ámbitos.

Está constituido por una corporación denominada Asamblea Nacional de Panamá (anteriormente llamada "Asamblea Legislativa") y tiene como actividad principal la expedición de leyes.

La Asamblea Nacional estará conformada por los Diputados (anteriormente llamados "legisladores"), escogidos mediante postulación partidista y votación popular directa para ocupar el cargo por un período de 5 años (Título V, Capítulo 1, Constitución Política de la República de Panamá.)

Tras la invasión de Panamá en 1989, Panamá no tiene ejército. En cambio estableció una fuerza pública. Las Fuerzas Públicas de Panamá están compuestas por la Policía Nacional, el Servicio Nacional de Fronteras, el Servicio Nacional Aeronaval y el Servicio Nacional de Migración, el Servicio el Protección Institucional no esta adscrito al Ministerio de Seguridad Pública, sino que este esta adscrito al Ministerio de la Presidencia, pero aun así, se le toma en cuenta dentro de las Fuerzas Públicas.

Hasta el 31 de diciembre de 2013 la división política de la República de Panamá comprendía , o municipios, 5 comarcas indígenas de nivel provincial y de los cuales dos son comarcales. El 1 de enero de 2014 se creó la provincia de Panamá Oeste que se segregó de la provincia de Panamá y se compone de los distritos que se ubican al oeste del canal de Panamá.

La República de Panamá es una franja ístmica con una superficie total de 75 517 km², y 2 210 km² de superficie de aguas territoriales, totalizando 77 630 km².

El país se localiza en América Central entre los paralelos 7° 11' y 9° 37' de latitud norte.

Las máximas alturas son el Volcán Barú con 3475 m, el cerro Fábrega con 3375 m, el Itamut con 3280 m y el Echandi con 3163 m.
Sus islas principales son Coiba con 493 km², Del Rey con 234 km² y Cebaco con 80 km².
Los lagos más grandes son Gatún con 423,15 km², el Bayano con 185,43 km², y el Alajuela con 57 km².
Sus ríos más importantes son el Chucunaque con 231 km, Tuira con 230 km, Bayano con 206 km, Santa María con 173 km y su río más importante por su impacto en la economía es el Chagres de 125 km, vital para el funcionamiento del Canal de Panamá.


Limita al norte con el Mar Caribe, al sur con el Océano Pacífico, al este con la República de Colombia y al oeste con la República de Costa Rica.


Panamá cuenta con costas tanto en el océano Atlántico como en el Pacífico.

Los principales núcleos urbanos del país se asientan sobre la costa pacífica, dado que presenta condiciones climáticas más favorables para el establecimiento de la población, así como mayor cantidad de golfos, bahías y sectores para el desarrollo de actividades humanas. En la costa pacífica es muy notorio el fenómeno de las mareas, tanto en el Golfo de Panamá como en el Golfo de Chiriquí, donde es posible apreciar durante la marea baja el retiro del mar en algunos sectores de hasta 70 kilómetros, dejando al descubierto extensas superficies fangosas. 

La costa del mar Caribe es de contornos más regulares aunque más abrupta, con acantilados y playas estrechas y cubierta en su mayor parte por una tupida selva tropical. El fenómeno de las mareas es menos notorio en esta costa y en algunos sectores presenta abundantes playas de arena blanca y gran cantidad de islas, islotes y cayos.

En general tiene un clima tropical, muy caluroso durante todo el año en las costas y tierras bajas, modificándose hacia el interior a medida que se gana altitud, siendo las temperaturas agradablemente frescas hacia los 1000 msnm y frías por encima de 2000 msnm. Las precipitaciones son por lo general altas, con diferencias entre la vertiente del Caribe (3000mm/año en promedio) donde prácticamente no existe estación seca, y la vertiente del Pacífico, que presenta una estación seca muy marcada de diciembre a marzo (1500 mm/año en promedio). Ciertas condiciones locales de exposición, corrientes oceánicas, dirección de los vientos y ubicación a barlovento o sotavento, hacen variar el patrón de precipitación en algunas localidades del país, indistintamente de su ubicación en alguna vertiente, por ejemplo, en algunos puntos de la península de Azuero, en el Pacífico, la precipitación es inferior a 900 mm/año y en algunos puntos de la ciudad de Panamá y la isla de Coiba, también en el Pacífico, supera los 2000 mm/año. En las montañas del interior las precipitaciones son muy altas, registrándose valores superiores a 5000 mm/año.

Los huracanes no constituyen una amenaza para el país por encontrarse al sur de su zona de influencia (sobre los 10º de Latitud Norte)

El anticiclón semipermanente del Atlántico Norte, afecta sensiblemente las condiciones climáticas del país, ya que desde este sistema se generan los vientos alisios del nordeste que en las capas bajas de la atmósfera llegan al país, determinando sensiblemente el clima de la República.

Existe una zona de confluencia de los vientos alisios de ambos hemisferios (norte y sur) que afecta el clima de los lugares que caen bajo su influencia y que para Panamá tiene particular importancia: la Zona de Convergencia Intertropical (ZCIT), la cual se mueve siguiendo el movimiento aparente del sol a través del año. Esta migración norte-sur de la ZCIT produce las dos estaciones (seca y lluviosa) características de la mayor parte del territorio.


Panamá está ubicada en la zona intertropical próxima al ecuador terrestre.

Es una franja de tierra angosta orientada de Este a Oeste y bañada en sus costas por el mar Caribe y el océano Pacífico.

Uno de los factores básicos en la definición del clima es la orografía, ya que el relieve no solo afecta el régimen térmico produciendo disminución de la temperatura del aire con la elevación, sino que afecta la circulación atmosférica de la región y modifica el régimen pluviométrico general.

Las grandes masas oceánicas del Atlántico y Pacífico son las principales fuentes del alto contenido de humedad característico del país, y debido a lo angosto de la franja que separa estos océanos, el clima refleja una gran influencia marítima. La interacción océano-atmósfera determina en gran medida las propiedades de calor y humedad de las masas de aire que circulan sobre los océanos. Las corrientes marinas están vinculadas estrechamente a la rotación de la tierra y a los vientos.

Es el país más meridional de la región, es también uno de los más biodiversos. Por su cercanía a América del Sur, alberga varias especies sudamericanas como el poncho o capibara (el roedor más grande del mundo), el oso frontino u oso de anteojos, y el guacamayo azul/amarillo (Ara ararauna). Esta causa hace que Panamá posea una mayor biodiversidad que otros países de la región, como El Salvador o Belice, y un mayor número de especies en algunos géneros. Es el primer país de la región en peces (1497, las vecinas Costa Rica y Nicaragua albergan 1254 y 1176 especies respectivamente), en aves (957 especies) y en mamíferos (229 especies, incluyendo el mayor número de primates de la región) con notables endemismos como el mono aullador de la isla de Coiba (Alouatta coibensis), el agutí o ñeque también de Coiba (Dasyprocta coibae) o el perezoso pigmeo de la isla Escudo de Veraguas (Bradypus pygmaeus). Posee 10 115 especies de plantas, de reptiles 229 especies y de anfibios 179.
Según el Índice de Actuación Medioambiental (EPI), Panamá es uno de los países que presentan un adecuado control de la contaminación ambiental.

La mayor parte del territorio panameño está formado por tierras bajas (un 89%). A este grupo pertenecen; las tierras bajas y llanuras del sur, las colinas y llanuras del Istmo Central, las depresiones orientales, las tierras bajas y las llanuras del norte. Estas tierras están constituidas por rocas ígneas, metamórficas y sedimentarias. El 10% son tierras altas. La mayor parte de la población panameña habita en tierra caliente baja. A este grupo pertenecen: Volcán Barú, la Cordillera Central, el Arco Oriental del Norte, el Arco Oriental del Sur, Macizos y Cadenas Volcánicas del Sur.

Su hidrografía está representada por numerosos ríos y lagos. Las características comunes de los cauces de la vertiente del Caribe es que son cortos, ya que nacen de montañas próximas al mar, y permiten generar energía eléctrica. Los de la vertiente del Pacífico son de mayor longitud, muchos de ellos navegables, pues recorren una distancia larga para llegar al océano. Los principales ríos son el Chagres, el Changuinola, el Chucunaque, el Majagua, el Teribe, el San San, el Sixaola y el Tuira.

A través de los años la economía panameña y su sistema bancario han sido conocidos internacionalmente como uno de los más sólidos del continente, un componente importante para esta solidez económica, ha sido el estable crecimiento del PIB, que en promedio avanzó un 6,3% desde mediados de la década de 1990 hasta comienzos de la década de 2010,
además de no haber sufrido contracciones desde 1988.

Según el ranking mundial de competitividad del Foro Económico Mundial, Panamá es después de Chile la economía más competitiva, y consolida su posición como la máxima de América Central. Aparte, según datos del Banco Mundial, Panamá tiene el PIB per cápita más alto de la región centroamericana siendo aproximadamente de 16.993.82 para el 2013, superando el PIB per cápita de países como México, Venezuela, Brasil y Perú PPA.
Según diversos organismos financieros la economía panameña es considerada de "ingresos altos".

El modelo económico liberal, impuesto durante la década de 1990, ha permitido al país ser de los más globalizados de América Latina durante varios años.
Es una economía totalmente dolarizada y sin banco central. La política económica de Panamá se basa en el sector terciario, siendo uno de los países más precoces en utilizar esta política. Este sector representa el 75% de su producto interno bruto, sin embargo ha existido un aumento significativo del sector industrial y de construcción.
Su moneda oficial es el Balboa, el cual es equivalente al dólar estadounidense que circula legalmente en todo su territorio desde (1904).

Durante 2009 Panamá exportó, $16 209 millones de dólares, según la CEPAL, lo que lo convierte en el principal exportador de América Central y el décimo a nivel latinoamericano.

Debido al crecimiento sostenido del PIB ocurrido durante los últimos años, organismos como el FMI proyectan que para 2030 el país alcance los US$ 35 000 dólares per cápita PPA
acercándose algo más al umbral de ingreso de las economías desarrolladas, situado en torno a los US$ 20 000 per cápita PPA.

El país está clasificado en la categoría de "grado de inversión" por parte de las empresas calificadoras de riesgo: "Standard & Poor's", "Moody's" y "Fitch Ratings".
El Índice de Libertad Económica de Panamá en el 2012 fue de 65,2 Puntos, ubicándolo en el puesto mundial número 55.

El turismo representa una de las principales actividades del país. Las principales áreas del turismo en Panamá se centran en el turismo de negocios, playas y comercio.
La mayor parte de los turistas provienen de Estados Unidos de América, Canadá, Europa, América Central y América del Sur.
Anualmente el turismo genera ganancias aproximadas a 1400 millones de USD.
Esta cifra ha aumentado rápidamente desde que en 2004 arribó el "turista millón". 2011 cerró con la llegada del turista 2 millones.

Panamá recibió en el año 2013 cerca de 1,527,228 turistas en el aeropuerto de Tocumen. En Panamá un turista, en promedio, gasta entre 365-385 USD por día, siendo el gasto turístico más elevado de América Central, mientras que el promedio de estadía turística en Panamá oscila entre 6 y 7 días.

Durante el año 2011 Panamá recibió más de 2 millones de turistas, con un crecimiento del 18% con respecto al 2010. La revista New York Times colocó a Panamá como el mejor lugar para visitar durante el 2012 ya que este país vive un gran momento económico, luego de haber recuperado hace 12 años el control del Canal. Para el diario el sello distintivo del país es la vía interoceánica y su ampliación, que debe finalizar en el 2014, bajo una inversión de miles de millones de dólares.
También destacan la construcción del Waldorf Astoria Panama, el primer hotel Waldorf Astoria en América Latina, que abrió en marzo de 2013; el Trump Ocean Club, que fue inaugurado en el 2010, y el BioMuseo, un centro de historia natural que abrió sus puertas el 2 de octubre del año 2014; así como el Casco Antiguo de la ciudad, declarado por la UNESCO Patrimonio de la Humanidad en 1997, y el archipiélago de Bocas del Toro, que se ha convertido en una parada popular de mochileros.

Según el reporte de la Contraloría General de la República, Migración y Estadísticas de la Autoridad de Turismo de Panamá, el turismo aportó un 8% al crecimiento del PIB en 2016 y aumentó en un 9% las plazas de empleo en el país siendo los visitantes de América del Sur los que más ingresaron por el Aeropuerto de Tocumen en el período de enero a octubre 2016. 

La principal obra de infraestructura del país es el canal de Panamá, empezado a construir en 1869 por un consorcio francés.
El país invirtió 5250 millones de dólares en la ampliación de esta vía.
En el siglo XIX también se destacó la construcción del ferrocarril transístmico, que facilitó la comunicación interoceánica.
La Línea 1 del Metro de Panamá, que une la gran estación de autobuses de Albrook con San Isidro en la zona norte de la ciudad de Panamá, se inauguró el 5 de abril de 2014, siendo así el primer metro de Centro América. Actualmente cuenta con dos líneas en funcionamiento.

En 1974 fue creado el "Instituto Nacional de Telecomunicaciones" (INTEL), el cual tenía la tarea de planificar, dirigir, mejorar, extender y proveer los servicios de telecomunicaciones. Con la "Ley 5 de 1995", se reforma este ente gubernamental. El proceso termina en 1997 con la privatización al venderse el 49% de sus acciones a "Cable & Wireless" (C&W), compañía de origen británico. El resto de las acciones se reparte entre el Gobierno (49%), y los empleados (2%). Este reparto otorgó a C&W el régimen de exclusividad hasta el 2 de enero de 2003. Panamá cuenta con cuatro sistemas de fibra óptica que se extienden por todo el país. Estas redes submarinas dan al país cuatro conexiones internacionales por alta banda ancha, rutas troncales de fibra óptica, reemplazando el sistema de alimentación mediante enlace por satélite.

Durante 1995 se introdujeron modificaciones en la "Ley 17 de 1991", con el fin de someter a licitación pública la banda "A" de telefonía móvil. En enero de 1996, el gobierno adjudica la banda "A" de telefonía celular a la compañía "BCS de Panamá" (Bellsouth), luego vendida a Telefónica opera bajo el nombre de "Tigo Panamá". En 1997"C&W" crea una subsidiaria para la telefonía móvil llamada "(C&W móvil)". Una década más tarde, en 2008, dos compañías más ganarían licitaciones para ofrecer este servicio:
Conformando así cuatro compañías que ofrecen el servicio de telefonía móvil.
Mientras que el promedio de abonados por cada 100 habitantes al servicio de telefonía celular en América Latina y el Caribe fue 106,9, en los países en desarrollo 77,8 y en el mundo 85,7, en Panamá 203,9 según dicho Informe. La proporción de Panamá es equivalente a reconocer que cada habitante en promedio contó con dos celulares o teléfonos móviles.

Las primeras conexiones de internet comienzan en junio de 1994, mediante RedHUCyT ("Red Hemisférica Interuniversitaria de Información Científica y Tecnológica") y PANNet, ("Red Académica y de Investigación Nacional").
Sin embargo, el salto definitivo a una utilización masiva de este medio se hace gracias la creación de Intered Panamá en 1995, a raíz de la Ley de Reestructuración de INTEL.
El servicio de internet es ofrecido por diversas compañías de nivel nacional e internacional, como:
entre otras.

También es ofrecido el servicio de internet público, conocido como "Red Nacional de Internet" en áreas como parques públicos, colegios, hospitales y bibliotecas de todo el país.

Para comienzos de la década de 2000, existían cerca de 45 000 usuarios, los cuales representaban una penetración del 1,5%.
Para inicios de la década de 2010, se estima existen unos 959 900 usuarios, los cuales representan un crecimiento del 2033% y alcanzan una penetración del 43%.
Según la Unión Internacional de Telecomunicaciones, Panamá es el país con mayor número de suscriptores a internet de América Central.

Panamá es la economía con mayor penetración de Internet en América Central, ya que el 42,7% de la población contó con acceso a Internet según un Informe preparado por la Unión Internacional de Telecomunicaciones con datos de 2011, y el tercero de Latinoamérica, superado solo por Chile (53,9%) y Brasil (45%). Panamá ocupó el puesto 70 entre 177 países
evaluados y cuenta con una mayor proporción de usuarios conectados a Internet que la media de América Latina y el Caribe (38,8%), países en desarrollo (24,4%) y el mundo (32,5%), según el mismo informe.

Panamá cuenta con siete diarios de circulación nacional: La Prensa, Mi Diario, La Estrella de Panamá, El Siglo, Crítica, Día a Día, Panamá América., P.M. y Metro libre. El primer diario del istmo fue publicado por Mariano Arosemena en 1820.

En Panamá existen en todo el país 184 frecuencias de radiodifusión comercial autorizadas por la Autoridad de los Servicios Públicos de la República de Panamá en la banda FM y 101 en la banda de AM.
La primera estación de radio AM comercial panameña autorizada, inició el 25 de diciembre de 1934.

En 2009 Panamá adoptó los estándares europeos de DVB-T (Digital Video
Broadcasting) para la televisión digital terrestre.

Existen más de diez canales de televisión que ofrecen el servicio en televisión abierta para todo el país. Entre estos: "TVN," "RPC," "FETV, NEX, TV Max, Telemetro, Mas 23, SerTV, Viva," "Oye TV, Plus Canal 35 y" "Hosanna Visión."

Además de otros canales que solo brindan el servicio en televisión por suscripción. Estos son: Cable Onda Sports, Eco TV, BTV Panamá, COS FC, 

El primer canal de televisión abierta en Panamá fue el canal de las fuerzas armadas de los Estados Unidos SCN Canal 8, que comenzó a transmitir el 6 de mayo de 1956, y el primer canal panameño fue RPC lanzado en marzo de 1960, opera en el canal 4 de la ciudad de Panamá.
Ya en 1961 la ABC de origen estadounidense en conjunto con empresarios panameños, fundan TVN, la cual opera en el canal 2 de Panamá, además de su señal en formato HDTV.
El 13 de octubre de 1981, inicia transmisión Telemetro como un canal especializado en películas. Durante sus inicios transmitía de 4:00 a.m. a 10:00 p.m. Opera en la frecuencia VHF 13, a partir de 2009 transmite 24 horas y de 2011 transmite en formato HDTV. El 31 de octubre de 2012, inicia transmisiones NEX, siendo este un canal con programación Generalista. 

En la actualidad existen 3 conglomerados de medios televisivos, siendo "Corporación Medcom" el más grande con 6 canales. Otro conglomerado importante en el país es TVN Media, que cuenta con 2 canales y "Compañía Digital de Televisión" cuenta con 3 canales. También se encuentra el "Sistema Estatal de Radio y Televisión" la cual opera el canal estatal panameño (SerTV).

En el año 2016, Panamá tiene una población de 4 015 813 habitantes, convirtiéndose en uno de los países menos poblado del continente americano.
Entre 1950 y 2010 la población pasó de 839 000 habitantes a los 4,1 millones de habitantes.
Más del 70% de los panameños habita en áreas urbanas y la mitad habita en la ciudad de Panamá y zonas conurbadas. Por su parte el 30% de la población tiene menos de 14 años, el 63,6% tiene entre 15 y 64, y el 6,4% tiene más de 64 años.

Está clasificado dentro de los países que poseen desarrollo Humano alto, con una puntuación de 0,795 (2019), que lo ubica en el puesto 58 de 169 países, siendo el país con mejor desarrollo humano en América Central y el cuarto en América Latina.
La tasa de alfabetismo alcanzó en 2014 un 97,5%. Por su parte la tasa de escolarización es del 96,4%, con un máximo de 97,1% en la provincia de Colón y un mínimo de 86,8 en la Comarca de Guna Yala.

Panamá es uno de los países étnicamente más diversos del mundo. Su población está compuesta por mestizos, mulatos, negros, blancos, indígenas y de diversos orígenes nacionales: chinos, hindúes, españoles, estadounidenses, colombianos, italianos, argentinos, griegos, franceses, árabes,judíos, nicaragüenses, costarricenses, hondureños, mexicanos, venezolanos, antillanos, dominicanos, chilenos, entre otros. Además habitan siete grupos indígenas.
En lo relativo a la distribución étnica, el 70% de los panameños son mestizos y blancos, el 15% son mulatos y negros, el 6% indígenas y el 9% asiáticos, estos últimos en su mayoría de ascendencia china.

Hasta principios del siglo XX, la sociedad panameña estaba dividida en clases alta y baja inamovibles. Pero desde la década de 1970, el país ha logrado crear predominantes clases medias.
Sin embargo en Panamá persiste la pobreza y la desigualdad social, pese a una notable reducción de la pobreza y desempleo en los últimos años y al alto índice de desarrollo humano logrado en las últimas dos décadas.
Para 2017, la pobreza cubrió a un 14% de la población, ubicándose por debajo de la media latinoamericana de un 31,4%.

Aunque el país es mayoritariamente monolingüe en idioma español, que es la lengua nacional y oficial del país, se hablan numerosas lenguas indígenas. Además del español, se reconocen como idiomas oficiales indígenas al ngäbe, buglé, kuna, emberá, wounaan, naso tjerdi y bri bri. Los inmigrantes, además del idioma español, hablan sus propios idiomas. El árabe es hablado por las colonias sirias y libanesas en la provincia de Colón. El inglés es la lengua extranjera de mayor uso y demanda, y es hablada por afroantillanos de la costa atlántica del país y en Ciudad de Panamá. También son importantes las colonias chinas, italianas, griegas, alemanas, suizas y francesas.

Por su diversidad cultural, en el país se practica una amplia gama de religiones; sin embargo, desde aproximadamente 30 años no se conoce con precisión la cantidad de los asiduos a cada grupo debido a que la Contraloría General de la República de Panamá ha obviado preguntar en los tres últimos censos, es decir, en 30 años, la religión que profesa cada habitante del país.

Las cifras que se manejan hasta ahora indican que la religión católica es la que predomina, seguida de los evangélicos, los Adventistas, los Testigos de Jehová y los Mormones.
Según una encuesta realizada en junio de 2017 por Pew Forum, se reflejan los siguientes datos:

Es común encontrar religiones orientales como el judaísmo, el budismo, el hinduismo, el islam, la Fe Bahá'í, entre otras.

La Constitución Nacional establece que "es libre la profesión de todas las religiones así como el ejercicio de todos los cultos, sin otra limitación que el respeto a la moral cristiana y el orden público". 

El 31 de julio de 2016 Panamá es anunciada como sede de la XXXIV edición de la Jornada Mundial de la Juventud 2019 del 22 al 27 de enero de 2019 en la Ciudad de Panamá.

La salud panameña es regida por el "Ministerio de Salud de Panamá". Por su parte el sistema de salud público es administrado por dos entidades distintas:
Los hospitales y los centros de atención primario que son administrados por el "MINSA" reciben fondos de partidas presupuestarias del estado.

En 1941, durante el gobierno de Arnulfo Arias Madrid se crea la Caja de Seguro Social. Esta etapa se caracteriza por una creciente participación del Estado en los problemas de Salud Pública, pero sin una planificación adecuada y con la consecuente duplicidad de actividades y servicios.
En 1969, durante el gobierno de José María Pinilla (régimen militar) se crea el "Ministerio de Salud".
Durante las décadas de 1960, 1970 y 1980, el sistema de salud panameño tuvo un desarrollo considerable, sobre la base del Ministerio de Salud y la Caja de Seguro Social, a los que se fue agregando un sector privado.

Este crecimiento se hizo a partir de un gasto estatal elevado, especialmente durante la década de 1970, cuando el gasto en salud se mantuvo sobre el 10% de un presupuesto nacional, lo que produjo el aumento gasto por persona entre 1970 (34,7 Dólares) y 1980 (57 Dólares).
A finales de la década de 1980, se detuvo esta tendencia tanto por la "crisis económica de 1982-1984" como por la crisis del "Estado de 1988-1989", que hicieron caer el gasto social notablemente. Como resultado, en 1988 el gasto por persona se había reducido a (solo 18,4 Dólares).
Durante las últimas dos décadas el sistema de salud ha sido criticado por muchos debido a la duplicidad de funciones y a la ineficiencia. A pesar de ello, la salud es accesible a casi todas las personas del área urbana y área rural no indígena.

Para 2007 La esperanza de vida en Panamá alcanzó los 76 años según la OMS, y en 2009 aumentó a los 77 años, en 2018 fue de 78,4 años y solamente es superado en la región por Costa Rica, con 80,2 años, y Chile que tiene un promedio de vida de 79,9 años. para 2007 fue de 75,8 años según datos de la ONU.
Según la OMS las panameñas tienen una esperanza de 78 años, la cual es la tercera mejor en América Latina. Por su parte los panameños tienen una esperanza de vida de 74 años, la segunda más alta de América Latina.

Se espera que durante el siglo XXI la población panameña sufra un proceso de envejecimiento, con el aumento de la población de la tercera edad. En el siguiente tabla se muestra este proceso, cifras en millones de personas.

El sistema educativo panameño se basa en la "Ley orgánica de educación", promulgada en 1946.
El sistema educativo panameño está estructurado en cuatro niveles de enseñanza: preescolar, primaria, secundaria y terciaria o universitaria. Los niños de 4 a 5 años de edad pueden acceder a la educación preescolar. La educación primaria o básica está dirigida a niños de 6 a 11 años de edad, mientras que la educación secundaria se divide en dos etapas: premedia (para jóvenes de 12 a 14 años de edad) y media (para jóvenes de 15 a 17 años de edad).

Los niveles primario y secundario del sistema educativo son regidos por el Ministerio de Educación de Panamá (Meduca), mientras que la educación superior está comandada universidades estatales: la Universidad de Panamá, Universidad Autónoma de Chiriquí, Universidad Tecnológica de Panamá, Universidad Marítima Internacional de Panamá y la Universidad Especializada de Las Américas. Los programas de estudios de las restantes universidades son fiscalizados y aprobados por estas dos instituciones. El país cuenta con una tasa bruta de matriculación de 79,7%, lo que lo convierte en el líder de la región centroamericana. Según el nuevo método de cálculo su índice de educación promedió en 2010 un 0,718.

El decreto ejecutivo 141, de 1997, crea las regiones educativas escolares que establece el modelo de organización administrativa descentralizada para las direcciones regionales.
Las cuales son:

En Panamá existen siete culturas indígenas que practican costumbres ancestrales. También hay museos, sitios arqueológicos y tres ciudades históricas coloniales con fortalezas, iglesias y conventos que datan de los siglos XVI al XIX.

El folclore varía en cada región y está representado por el traje típico, la pollera, la comida y platos tradicionales, así como la música y el baile.

Los carnavales son una de las fiestas principales de Panamá, especialmente el carnaval de Panamá el carnaval de Las Tablas y el carnaval de Chitré, en la capital de la provincia de Los Santos y Herrera.

Arte y costumbre popular tienen un escenario de encuentro en las famosas ferias y festivales azuerenses, tomando como referencia de que en esa región (La península de Azuero) es donde más se dan manifestaciones típicas y se guarda celosamente las tradiciones. Entre los más famosos están, la celebración del Corpus Christi, una de las principales fiestas del folklore panameño, el desfile de Mil Polleras, etc.

El béisbol es el deporte más practicado y es considerado el "deporte nacional". Varios peloteros panameños han actuado en la Major League Baseball estadounidense, como Humberto Robinson, primer panameño en debutar en la MLB; Rod Carew, miembro del salón de la fama, Mariano Rivera y Carlos " Calicho" Ruiz que se retiró en su provincia. El baloncesto tiene también una gran popularidad, sobre todo en la ciudad de Panamá y Colón; varias figuras han destacado, como Rolando Blackman. El boxeo es otra de las disciplinas más practicadas en todo el territorio, entre las leyendas de este deporte son Alfonso Teófilo Brown (Panamá Al Brown) y Roberto Durán. 

El fútbol, que durante gran parte del siglo XX fue un deporte minoritario, ha crecido notablemente y se ha vuelto muy popular debido a los éxitos de jugadores históricos como Luis Ernesto Tapia, Armando Dely Valdés, Rommel Fernández, Julio César Dely Valdés, Jaime Penedo, Román Torres, Felipe Baloy, Blas Pérez, Luis Tejada; en la actualidad se destacan Luis Mejía, Michael Murillo, Alberto Quintero, Gabriel Torres y Harold Cummings. Además, la selección nacional ha logrado el mayor éxito de su historia al lograr clasificarse como equipo participante en la Copa Mundial de Fútbol de 2018,en el que debutó ante Bélgica el 18 de junio. El atletismo es otra modalidad popular en la ciudad de Panamá y en la provincia de Colón; el único medallista de oro olímpico panameño (Pekín 2008) es nativo de esa provincia; Irving Saladino, uno de los mejores saltadores de longitud del mundo. Otro atleta destacado es Alonso Edward, subcampeón de los 200 metros en los campeonatos del mundo de Berlín 2009, y es el poseedor del récord continental de dicha prueba.

El estadio nacional del país adoptó el nombre del futbolista Rommel Fernández cuando falleció en España en 1993 víctima de un accidente de tráfico.


La clasificación se muestra en el orden de la posición del índice de Panamá con respecto la clasificación de los países evaluados en cada categoría. El año mostrado junto con el indicador refleja la fecha de los datos utilizados en la evaluación según lo reporta cada fuente, y no necesariamente corresponde al año de publicación.




</doc>
<doc id="5304" url="https://es.wikipedia.org/wiki?curid=5304" title="Presidente">
Presidente

Presidente (del latín "praesĭdere", «sentarse al frente»), por lo general, es la designación utilizada para identificar a la persona que dirige una reunión, una sesión de trabajo o una asamblea.

Actualmente, el término aislado se refiere al funcionario público, electo para un período determinado, que ostenta el poder ejecutivo de un Estado o de una región. Esta fórmula fue incorporada por primera vez en la Constitución de los Estados Unidos y actualmente se utiliza de manera generalizada. Al igual que otros sustantivos como vidente, paciente, sirviente, etc., su forma femenina puede determinarse anteponiéndole el artículo femenino correspondiente («la vidente», «la paciente», «la sirviente», «la presidente»), o bien, utilizando directamente la forma femenina presidenta, recogida por la Real Academia Española. Además, sirve para identificar a quien preside determinado órgano público colegiado, como los son en vía de ejemplo, los presidentes respectivos del Congreso, Senado, Cámara de Diputados, Corte Suprema, Tribunal Constitucional y otros. También existe la palabra vicepresidente, que es un funcionario (de carrera o de facto) de gobierno, o un hombre o mujer de negocios, con un rango inferior al del presidente en la jerarquía organizacional.

De la misma manera y por extensión se denomina también, en algunos países, al máximo directivo de empresas privadas, universidades, cámaras empresariales, asociaciones e instituciones de diversa índole, públicas o privadas.

En los países con régimen de gobierno republicano, el jefe de Estado o gobierno recibe el título de "presidente". Normalmente, los jefes de Estado son elegidos por el pueblo, el congreso o parlamento por un período inferior a una década (4 a 8 años, según el país).

Los presidentes se pueden distinguir según sus funciones y poderes de la siguiente forma:



En algunos países con monarquía parlamentaria, el primer ministro recibe el título de presidente del Gobierno. Ese es el caso de España.

África

América

Asia

Europa

Oceanía




</doc>
<doc id="5305" url="https://es.wikipedia.org/wiki?curid=5305" title="Theodore Roosevelt">
Theodore Roosevelt

Theodore Roosevelt /ˈθiːəˌdɔːr ˈroʊzəvɛlt/ (Nueva York, 27 de octubre de 1858-Oyster Bay, Nueva York; 6 de enero de 1919) fue el presidente de los Estados Unidos (1901-1909).

Es recordado por su personalidad exuberante, su amplitud de intereses y logros, su personalidad de "cowboy", su masculinidad y su liderazgo del Movimiento Progresista, un período de activismo social y reforma en EE. UU. entre 1890 y 1920 destinado a acabar con la corrupción. Fue líder del Partido Republicano y fundador del efímero Partido Progresista de 1912. Antes de acceder a la presidencia ocupó cargos en la administración local, estatal y federal. Los logros de Roosevelt como naturalista, explorador, cazador, escritor y soldado contribuyen tanto a su fama como sus cargos políticos.

Nacido en el seno de una acaudalada familia, Theodore Roosevelt fue un niño enfermizo y débil que sufrió asma y salió poco de casa, donde se quedaba aprendiendo historia natural. Para compensar su debilidad física, desarrolló una vida intensa. Estudió en casa y acudió a la Universidad de Harvard, donde practicó el boxeo y desarrolló interés por los asuntos navales. En 1881 fue elegido para la Asamblea del Estado de Nueva York como su miembro más joven. Su primer libro de Historia, "La Guerra Naval de 1812" (1882), le otorgó fama como historiador serio. Tras unos años trabajando en un rancho de ganado en las Dakotas, Roosevelt retornó a la ciudad de Nueva York y se ganó fama luchando contra la corrupción policial. La Guerra Hispano-Estadounidense estalló cuando Roosevelt estaba dirigiendo el Departamento de la Armada, cargo al que renunció de inmediato para liderar en Cuba un pequeño regimiento conocido como Rough Riders, que obtuvo una nominación para la Medalla de Honor y que le fue entregada de forma póstuma en 2001. Tras la guerra volvió a Nueva York y fue nombrado gobernador en una reñida elección. En el plazo de dos años fue elegido vicepresidente de los Estados Unidos.

En 1901 el presidente William McKinley fue asesinado y lo sucedió Roosevelt, que entonces contaba 42 años y se convertía así en el presidente más joven de la historia de los Estados Unidos y el primero desde 1865 que no había luchado en la Guerra de Secesión. Roosevelt trató de virar el Partido Republicano hacia el progresismo, incluyendo la lucha contra los monopolios y la regulación de las empresas. Acuñó la frase "Square Deal" para describir su política interna, haciendo hincapié en que el ciudadano de a pie tendría su justa parte bajo sus políticas. Como amante de la naturaleza, promovió la conservación ambiental. En el escenario internacional las políticas de Roosevelt estuvieron caracterizadas por la doctrina del Gran Garrote (Big Stick). Promovió la terminación del Canal de Panamá, envió la Gran Flota Blanca a circunnavegar el mundo para demostrar el poder de su nación y negoció el fin de la Guerra Ruso-Japonesa, por lo que fue galardonado con el Premio Nobel de la Paz, convirtiéndose así en el primer estadounidense en ganar un premio Nobel.

Roosevelt declinó presentarse a la reelección en 1908. Tras dejar el cargo se embarcó en un safari por África y un tour por Europa. A su retorno a los EE. UU. se enfrentó con el nuevo presidente William Howard Taft. En 1912 intentó arrebatarle la nominación republicana a Taft; como no lo consiguió, fundó el Partido Progresista. En las siguientes elecciones, Roosevelt consiguió ser el único candidato de un tercer partido en quedar en segundo lugar en unas elecciones presidenciales en los Estados Unidos, batiendo a Taft pero perdiendo contra Woodrow Wilson. Tras las elecciones, se embarcó en una gran expedición a Sudamérica, donde el río por el que navegó recibió su nombre. Durante este viaje enfermó de malaria, lo que deterioró su salud. Murió pocos años después, a la edad de 60. Roosevelt ha sido considerado por los historiadores como uno de los mejores presidentes de los Estados Unidos.

Sus padres, Theodore Roosevelt Sr y Martha Bulloch, procedían de familias aristocráticas de origen neerlandés y vivían holgadamente gracias a los ingresos proporcionados por su empresa de importación y exportación. Durante su adolescencia sufrió asma. Esto provocó posiblemente su obsesión por el ejercicio y la vida sana. Profesaba la fe calvinista.

En 1880, finaliza sus estudios de Historia en la Universidad Harvard y se casa con Alice Hathaway Lee, hija de un banquero. Empieza a estudiar Derecho, pero lo abandona al ser elegido para la Asamblea del Estado de Nueva York de 1882 a 1884 por el Partido Republicano. En 1884, fruto de su matrimonio con Alice, nace su primera hija. Alice murió dos días después de dar a luz producto de una insuficiencia renal, que su embarazo había ocultado. La madre de Roosevelt, Mittie Theodore Roosevelt, había muerto unas 11 horas antes, ese mismo día, en la misma casa, de fiebre tifoidea. Roosevelt dejó a su hija al cuidado de su hermana Anna en la ciudad de Nueva York. En su diario, escribió una gran "X" en la página y, a continuación, «La luz se ha ido de mi vida».

Nunca más habló de su esposa ni escribió nada en su autobiografía u otros textos. Theodore se retira a una granja de Dakota del Norte para olvidar esas tragedias.

Durante dos años vivirá como un cowboy estadounidense. «No se puede soñar una vida más atractiva para un joven con buena salud que la de un rancho en esa época. Es una vida verdaderamente agradable y sana; me enseñó a ser independiente, tenaz y a adoptar decisiones con rapidez... Aprecié este tipo de vida real y completamente». Este período fue muy importante para alcanzar la madurez: «Nunca hubiera podido llegar a presidente sin la experiencia adquirida en Dakota del Norte».

En 1886, regresa a Nueva York y reinicia su carrera política, escribe tres libros y se vuelve a casar, con Edith Kermit Carow. El presidente Benjamin Harrison lo nombra miembro de una comisión sobre los funcionarios federales. Dirige a continuación la prefectura de policía de Nueva York en 1895. En 1897, el presidente William McKinley lo nombra secretario adjunto para la Armada, puesto desde el que prepara la Guerra contra España. Roosevelt actuó astutamente: acusó a España de la destrucción del acorazado "Maine" en Cuba, sin ninguna prueba. Apoyado por el magnate de la prensa William Randolph Hearst, puso a la Armada en estado de alerta sin autorización del presidente McKinley.

Al estallar en 1898 la guerra contra España, se alista a la cabeza de un regimiento de caballería, los "Rough Riders" ('Duros jinetes'), lo que le permite ganarse una desproporcionada reputación de héroe. Roosevelt fue nominado para recibir la Medalla de Honor, la máxima condecoración militar estadounidense, pero la petición fue desestimada en varias ocasiones. En 2001, Roosevelt recibió la medalla a título póstumo, siendo el único presidente en recibirla.

Después de la guerra reanuda su carrera política en el estado de Nueva York, del que es elegido gobernador en ese mismo año. Consigue enfrentarse a los dirigentes del Partido Republicano luchando contra la corrupción, y estos, para deshacerse de él, lo proponen como candidato a la vicepresidencia, un puesto de escasa relevancia.

































Al finalizar su segundo mandato, Roosevelt, fiel a sus compromisos, no se vuelve a presentar. Viaja a África para realizar un safari y regresa después de haber comprado más de animales.

En política exterior, Theodore Roosevelt abogó por el expansionismo estadounidense, pasando a controlar las posesiones españolas en el Caribe y en el océano Pacífico. Instigó una revuelta en Panamá para conseguir la separación de ese país que con anterioridad se había unido a la Gran Colombia en 1822 y décadas después se convirtió en un departamento autónomo de Colombia. El objetivo de dicha insurrección era construir el canal y que quedaría bajo control de los Estados Unidos. Roosevelt, ferviente defensor de la Marina, opinaba que el paso a través del istmo de Panamá era fundamental para poder crear una marina fuerte y cohesionada.

Durante el período de su presidencia, el ejército de los Estados Unidos estableció en 1903 en Cuba la base de Guantánamo, según lo convenido en el Tratado cubano-estadounidense, con unas condiciones tan férreas que ni siquiera el gobierno comunista de Fidel Castro pudo lograr su devolución al país. También intervino en la República Dominicana en 1904 y ocupó Cuba en 1906.

Roosevelt instituye un corolario a la doctrina del presidente James Monroe al afirmar que los Estados Unidos debían intervenir para defender sus intereses en el conjunto del mundo. Intervino personalmente en el arbitraje del conflicto entre Francia y Alemania sobre Marruecos y en el que se produjo entre Rusia y Japón, lo que le sirvió para obtener el Premio Nobel de la Paz.

Fue un detractor de las intervenciones humanitarias. Respecto a los desmanes cometidos por los belgas en el Congo Belga, Roosevelt reiteró su apoyo al rey Leopoldo II, que Estados Unidos le había conferido durante el mandato de Chester A. Arthur, diciendo en 1906 que "era una literal imposibilidad física intervenir" y llamó "imbécil" a la campaña favorable a la intervención.

Roosevelt era partidario de un fuerte poder federal, capaz de regular la actividad económica del país. Atacó a las grandes empresas privadas y su política económica estaba contra el monopolio, a las que acusaba de obtener enormes beneficios en detrimento de los consumidores, e inicia procedimientos contra los grandes capitalistas del ferrocarril, del petróleo y de la industria agroalimentaria. El inicio formal de esta cruzada contra los "trusts" industriales se producen en un largo discurso de más de 30 páginas que pronuncia en la Cámara de Representantes. 

Theodore Roosevelt se compromete a conseguir que se respete el acta Sherman. También interviene para arbitrar el conflicto entre los mineros en huelga y la patronal; este hecho permite que consigan una jornada de ocho horas y unos salarios más justos para los trabajadores, lo que se llamó un «acuerdo equitativo». Roosevelt pertenece a la corriente progresista y algunos conflictos, como el que le enfrentó al banquero J. P. Morgan, le dieron esa reputación en la historia.

Roosevelt es el primer presidente que se preocupa de modo efectivo por la conservación de los espacios naturales y por la fauna. Creó las bases del sistema de Parques Nacionales, de Monumentos Nacionales y de Bosques Nacionales así como de las Reservas Naturales, haciendo pasar estos terrenos al control federal. Del mismo modo, en 1902, el "National Reclamation Act" (o "Newlands Act") daba al gobernador federal los poderes supremos para la construcción de presas o para los proyectos de irrigación. Se crea una nueva agencia federal, el "Reclamation Service" para colaborar con los científicos. La gestión del agua pasa a control federal, lo que es especialmente relevante en la parte oeste del territorio. En total fueron más de un millón de km² los que pasaron a ser controlados y protegidos por el gobierno federal. Durante su mandato se crearon los parques de Crater Lake, Wind Cave y Mesa Verde.

1912


Tras esta derrota Roosevelt siguió llevando una vida aventurera que le llevó a diversos lugares del mundo, destacándose entre sus viajes la Expedición científica Roosevelt-Rondon, gracias a la cual consiguió que un río, anteriormente explorado y bautizado por el explorador brasileño Cândido Rondon como Río de la Duda, y luego por él junto a este en Brasil entre 1913-1914, hoy lleve en su honor el nombre de río Roosevelt tal y como fue rebautizado desde entonces.

Al estallar la Primera Guerra Mundial (1914-1918), Roosevelt se pronunció en favor del apoyo a Gran Bretaña (lo que el presidente Wilson no hizo hasta 1917).

Ya como político de la oposición, apoyó los esfuerzos de Tomáš Masaryk para crear una Checoslovaquia independiente. En agosto de 1918, cuando distribuyó el Premio Nobel de la Paz en metálico, envió una suma simbólica de 1.000 dólares a Rusia a los legionarios checoslovacos y en una ceremonia en Nueva York, dijo: "Los checoslovacos heroicos deben formar una comunidad independiente". 

1918

1919


Theodore Roosevelt está considerado por los estadounidenses como uno de sus presidentes más importantes debido en gran parte a que impulsó muchas leyes progresistas. Roosevelt inauguró el 18 de marzo de 1911 una presa cerca de Phoenix, en el estado de Arizona, que lleva su nombre y que aún hoy en día es la presa de mayor tamaño de los Estados Unidos. Es uno de los cuatro presidentes esculpidos en el granito del Monte Rushmore junto a George Washington, Thomas Jefferson y Abraham Lincoln. El portaaviones de propulsión nuclear de la Marina de los Estados Unidos CVN-71 lleva su nombre, "USS Theodore Roosevelt" y además cuenta con un monumento nacional en la Isla Theodore Roosevelt en Washington D.C.

Inclusive una cita de la plataforma de Partido Progresista de 1912 de Roosevelt fue citada como un epigrama por Julian Assange, fundador de WikiLeaks, en su manifiesto de 2006: «Detrás del gobierno aparente se asienta entronizado un gobierno invisible que no debe lealtad ni reconoce responsabilidad alguna hacia el pueblo. Destruir este gobierno invisible y torpedear la nefasta alianza entre los negocios corruptos y la política corrupta es la primera tarea de los estadistas de la época».





</doc>
<doc id="5306" url="https://es.wikipedia.org/wiki?curid=5306" title="Historia de Panamá">
Historia de Panamá

La historia de Panamá abarca desde la llegada de sus primeros habitantes hasta la actualidad. Su historia se divide en cuatro épocas: precolombina, colonial, unión a Colombia y republicana. También se encuentran periodos como la conquista española, la Independencia de Panamá del Imperio Español, la Separación de Panamá de Colombia, la Dictadura Militar en Panamá, y el Regreso a la Democracia.

Antes de la llegada de los europeos, las tierras de Panamá estaban habitadas básicamente por pueblos chibchenses que hablaban lenguas chibchas del grupo ístmico. Estos pueblos formaban grupos diversos por lo que no constituían una unidad política unificada. 

A raíz de las crónicas españolas del siglo XVI, se ha podido determinar la extensión de los pueblos que existían en Panamá al momento de la conquista de América. Además de las crónicas, la historiografía panameña e internacional ha utilizado el lenguaje, la orfebrería y demás aspectos culturales, para lograr establecer el área de influencia territorial de las distintas naciones indígenas.

Entre las naciones indígenas que habitaban el istmo destacaban los Cuevas, cuyo territorio comprendía el suroeste de Panamá, tanto en sus costas caribeña y pacífica, incluyendo el Darién. Se ha fijado el río Atrato como el límite este de los dominios de dicho pueblo indígena, mientras, por el oeste, el territorio se extendería hasta Chame en el Pacífico y Quebore (Río Indio) en el Caribe.

Los cacicazgos Cuevas más importantes eran los de Pocorosa, Comagre y Careta; cuyos territorios ocupaban la actual comarca de Guna Yala. En el litoral pacífico destacaba el cacicazgo de Chochama que ocupaba una buena parte del golfo de San Miguel, al igual que la costa comprendida entre Chimán y la bahía de Chame, el archipiélago de Las Perlas y las islas de Chepillo, Taboga, Taboguilla y Otoque.

No obstante en la parte oriental del istmo existieron pueblos que no hablaban el Cueva. Entre ellos destacan los Chuchures que, provenientes de Honduras, se asentaron en Nombre de Dios. Por otro lado “los de Birú”, fueron reportados por Pascual de Andagoya y ubicados por Romoli en la cuenca alta del río Tucutí, y los de ‘Quarequa’ o ‘Careca’ que «habían venido conquistando de hacia las espaldas del Darién».

En la parte central del istmo vivían un número plural de naciones indígenas que no compartían lenguaje ni características fenotípicas comunes. Las crónicas españolas apuntan que estos cacicazgos mantenían constantes enfrentamientos bélicos entre ellos por el control territorial.

Entre los señoríos hallados por los españoles durante la conquista, en territorios de la actual provincia de Coclé, se menciona a Periquete, Totonaga, Taracuru, Penonomé. Sin embargo, se ha establecido que en Natá residía el cacique Acherse que comandaba todos esos territorios.

En el área geográfica de la actual península de Azuero se ha documentado los cacicazgos de Escoria, Usagaña, Quema, Guararé, Pocrí y París. No obstante se conoce que el cacique Cutatara de París había dominado mediante la guerra al resto de los cacicazgos vecinos.

En la parte central de la actual provincia de Veraguas estaba establecido el cacicazgo de Tabraba; hacia el norte se encontraba el cacicazgo de Urracá que se encontraba en el área de la actual Santa Fe; con el avance de la colonización este territorio albergó una importante resistencia indígena.

En las actuales provincias de Bocas del Toro y Chiriquí, se desarrollaron un número plural de tribus indígenas entre las que destacan los guaymíes, dorasques y dolegas. Estas tribus se encontraban dispersas tanto en las costas del Océano Pacífico y el Mar Caribe; como en la Cordillera Central.

Cristóbal Colón fue el primer explorador español en alcanzar tierra firme americana, en su tercer viaje, pero el primero en arribar al territorio panameño correspondió a Rodrigo Galván de Bastidas, natural de la ciudad de Sevilla. 

Como Colón había sido apresado en su tercer viaje, los Reyes Católicos eliminaron la exclusividad de la empresa para él (las Capitulaciones de Santa Fe). Por esta razón, Bastidas solicitó licencia para explorar. En 1501, el sevillano recorrió las costas de Venezuela y el norte de Colombia, hasta el Golfo de Urabá. Posteriormente bordeó la costa del istmo panameño, y llegó casi hasta el emplazamiento actual del canal de Panamá.

En este trayecto, los exploradores no fundaron ningún pueblo ni se adentraron en el territorio. Por medio de trueques con los pueblos indígenas, Bastidas acumuló oro y palo de Brasil (apreciado por su madera y como fuente de un tinte). A diferencia de otros conquistadores, Bastidas se dedicó antes a comerciar que a saquear las riquezas o a esclavizar a los indígenas.

Las embarcaciones de Bastidas se vieron atacadas por el molusco llamado broma. Este es un molusco de unos 20 cm de longitud, que excava galerías en las maderas sumergidas en agua de mar, como los cascos de los barcos y los muelles. Por la razón anterior, varias naves de Bastidas naufragaron camino a la isla La Española, y perdió gran parte de sus riquezas.

Al llegar a dicho territorio, el gobernador Francisco de Bobadilla enjuició a Bastidas por haber violado la prohibición que tenía de arribar a la isla. En su contrato con la Corona española, Bastidas se había comprometido a lo anterior y a no negociar con los indígenas. Ambas cosas fueron incumplidas. Bobadilla envió a Bastidas hacia España, donde se le siguió juicio, pero fue absuelto. Posteriormente, incluso se le reconoció una pensión vitalicia sobre las riquezas extraídas de Urabá.

Bastidas realizó otros viajes de exploración. En 1525, fundó Santa Marta, en territorio de la actual Colombia, primer poblado duradero en la región.

Como gobernador de esa ciudad, enfrentó una sublevación, debido en parte a su actitud de comerciar con los pueblos autóctonos, en lugar de saquear sus riquezas. Fue herido, y marchó a La Española a recuperarse, pero los vientos lo llevaron a Cuba, donde finalmente murió en 1527.

El 10 de octubre de 1502, en su cuarto viaje, Cristóbal Colón llegó a la costa atlántica del istmo, en las actuales provincias de Bocas Del Toro y Veraguas. El 2 de noviembre, llegó a una bahía en la actual provincia de Colón, a la que bautizó como el nombre de "Portobelo" o Puerto Bello.

Santa María la Antigua del Darién fue la primera ciudad fundada por los españoles en la Tierra Firme del continente americano, situada en la Provincia de Darién, en la región de la actual frontera entre Panamá y Colombia, en territorio colombiano.

Fue fundada por Vasco Nuñez de Balboa en 1510, en los territorios del Cacique Cémaco. Al encontrar una fuerte resistencia por parte de los indígenas del área, los españoles ofrecieron a la Virgen de la Antigua venerada en Sevilla que de salir triunfantes en la batalla darían su nombre a la población. Cémaco fue vencido y en septiembre de 1510, cumpliendo con la promesa hecha, la ciudad fue bautizada con el nombre de Santa María de la Antigua del Darién.

Se constituyó un gobierno municipal, y se realizó en ella el primer cabildo abierto en el continente americano, designando a Vasco Núñez de Balboa como alcalde. En dicha ciudad, también se construyó la primera iglesia de Tierra Firme, sobre el sitio de la vivienda de Cémaco, y fue la primera sede episcopal del continente. Los cabildos eran instituciones democráticas, pioneras en todo el mundo en esta práctica.

Santa María la Antigua del Darién fue la capital del territorio de Castilla de Oro hasta la fundación de Panamá por Pedrarias Dávila en 1519. Pedrarias ordenó el traslado de la capital de Castilla del Oro, de personas, ganado y municiones a la nueva Panamá a orillas del Mar del Sur u Océano Pacífico. Pocos años después Santa María La Antigua del Darién fue abandonada y en 1524 la ciudad fue asaltada y quemada por los indígenas.

En 1513, Vasco Núñez de Balboa emprende la conquista de los territorios de los caciques Careta, Ponca y Comagre, donde escucha por primera vez de la existencia de otro mar por parte de Panquiaco, hijo mayor de Comagre, donde se relataba de un reino al sur de población tan rica que utilizaban vajillas y utensilios en oro para comer y beber.

La noticia inesperada de un nuevo mar lleno de riquezas fue tomada muy en cuenta por Vasco Núñez de Balboa, quien organiza una expedición que parte de Santa María La Antigua el 1 de septiembre de 1513. El día 25 de septiembre, Núñez de Balboa se adelanta al resto de la expedición y se interna en la cordillera del río Chucunaque, y antes del mediodía logra llegar a la cima de la cordillera desde donde logra ver en el horizonte las aguas del nuevo mar.

Cuando la expedición llega a las playas, Núñez de Balboa levantó sus manos, en una estaba su espada y en la otra un estandarte de la Virgen María, entró a las aguas hasta el nivel de las rodillas y tomó posesión del Mar del Sur en nombre de los soberanos de Castilla.

Núñez de Balboa bautizó al golfo donde llegó la expedición como San Miguel, porque fue descubierto el 29 de septiembre, día de San Miguel Arcángel, y al nuevo mar como Mar del Sur por el recorrido que tomó la exploración por el istmo rumbo al sur. Este hecho es considerado por la historia de Panamá como el capítulo más importante de la conquista después del descubrimiento de América.

En Panamá se han bautizado parques y avenidas con el nombre de Vasco Núñez de Balboa. En la ciudad de Panamá, frente a las costas se erige un impresionante monumento dedicado a su memoria y a la hazaña del descubrimiento del Mar del Sur. En su honor se ha bautizado la moneda oficial del país con la denominación de balboa, apareciendo su rostro en el anverso de algunas monedas. Así mismo, el principal puerto en el Pacífico del Canal de Panamá y el distrito que abarca el archipiélago de las Perlas, también llevan su nombre. La máxima condecoración otorgada por el Estado panameño a personajes destacados y sobresalientes es la Orden Vasco Núñez de Balboa en sus diferentes grados.

La ciudad de Panamá fue fundada el 15 de agosto de 1519 por Pedro Arias Dávila, siendo la primera ciudad española en las costas del Mar del Sur u Océano Pacífico y la más antigua de Tierra Firme que existe hasta nuestros días como ciudad. Su fundación reemplazó a las anteriores ciudades de Santa María la Antigua del Darién y Acla, convirtiéndose en la capital de Castilla del Oro. El 15 de septiembre de 1521 recibió, mediante real cédula, el título de ciudad y un escudo de armas conferido por Carlos V de España. 

La ciudad de Panamá se convirtió en el punto de partida para la exploración y conquista del Perú y ruta de tránsito para los cargamentos de oro y riquezas provenientes de todo el litoral pacífico del continente americano que se enviaban a España. En 1671 la ciudad es atacada por las fuerzas del pirata galés Henry Morgan con intenciones de saquearla. Por medidas de seguridad, de la población y los bienes, el Capitán General de Tierra Firme, Juan Pérez de Guzmán ordena evacuar la ciudad y volar los depósitos de pólvora provocando un gigantesco incendio que la destruyó totalmente. Las ruinas de la antigua ciudad todavía se mantienen incluyendo la torre de su catedral y son una atracción turística conocida como el conjunto monumental histórico de Panamá la Vieja, reconocida como patrimonio de la humanidad por la Unesco. La ciudad de Panamá fue reconstruida en 1673 en una nueva localización a 2 km al oeste-suroeste de la ciudad original a las faldas del cerro Ancón, conocida actualmente como el Casco Viejo de la ciudad.

En 1821, luego de la independencia de Panamá de España y su unión voluntaria a la Gran Colombia de Simón Bolívar, la ciudad de Panamá pasa de capital de Castilla del Oro y el ducado de Veraguas, a ser la capital del departamento del Istmo. La unión a Colombia se llevó a cabo con intenciones autónomas que Colombia nunca aceptó. En 1830, 1831 y 1832, Panamá se intentó separar de Colombia, pero la insistencia de Bolívar primero, y la razón de las armas luego, reunificaron los territorios. Dentro de las 6 guerras civiles habidas en Colombia durante el siglo XIX, la ocurrida a mediados de siglo ocasiona la separación de Panamá en 1840, adoptando el nombre de Estado del Istmo, por un año. 

La fiebre del oro en California, en 1848, convirtió nuevamente al istmo como la ruta de viajeros que cruzaban camino a la costa occidental de Norteamérica, devolviéndole el auge comercial a la ciudad. En 1855 empezó operaciones el ferrocarril de Panamá, la primera vía férrea transoceánica desde la Ciudad de Panamá en el Pacífico hasta la costa atlántica del istmo. 

En 1868 ocurrió otra revuelta popular; y finalmente el 12 de agosto de 1903 el Senado Colombiano reunido en Congreso, rechazó el Tratado Herrán-Hay para construir un canal por los Estados Unidos, por considerar que menoscababa su soberanía. La razón real del rechazo era dejar caer la concesión hecha a la Compañía francesa del Canal, que vencía hacia febrero de 1904, y así asumir la propiedad de sus haberes, y renegociar el tratado estipulando que los 40 millones de dólares que irían a la compañía irían al Tesoro de Colombia. Los panameños se organizan y declaran la separación el 3 de noviembre de 1903. Estados Unidos reconoce a los tres días el nuevo estado, e impide con su armada la acción de Colombia para restablecer la autoridad central. 

La República de Panamá declara su separación de Colombia y la ciudad de Panamá se convierte en la capital de la nueva nación. Con los trabajos de construcción del canal de Panamá se mejoró la infraestructura de la ciudad en aspectos como sanidad, la erradicación de la fiebre amarilla y la malaria, la reconstrucción de calles y alcantarillado, así como la introducción del primer sistema de agua potable. 

Durante la Segunda Guerra Mundial, la construcción de bases militares y la presencia de gran cantidad de militares y personal civil estadounidenses trajeron nuevos niveles de prosperidad y comercio a la ciudad. También los alemanes (nazis) tenían como un punto especial atacar el canal de Panamá. Incluso se han encontrado bases aéreas nazis que tenían como dirección al canal de Panamá, también se encontraron submarinos hundidos en el canal por acorazados norteamericanos.

Durante los años de 1970 y 1980, la ciudad de Panamá se convirtió en uno de los centros bancarios más fuertes del mundo a la par de Nueva York, y el centro financiero y de seguros más poderoso de toda América Latina. El 20 de diciembre de 1989, el ejército de EE. UU. invade la Ciudad de Panamá con el propósito de capturar al general Manuel Antonio Noriega, comandante en jefe de la Fuerzas de Defensa y último dictador militar de la República de Panamá, quien era acusado de narcotráfico en tribunales norteamericanos. Como resultado de esa acción militar, el barrio del Chorrillo, donde se encontraba la comandancia de las Fuerzas de Defensa de Panamá, fue destruido en gran parte. 

En la actualidad, la ciudad de Panamá, que incluye los distritos de Panamá y San Miguelito principalmente, así como otros distritos y corregimientos cercanos, supera los 1,2 millones de habitantes, en una de las ciudades más avanzadas y cosmopolitas del continente americano, con numerosas atracciones turísticas y vacacionales, hoteles y restaurantes de clase mundial, casinos y centros comerciales o malls internacionales, centros nocturnos y recreativos, el centro bancario internacional, el centro de seguros y reaseguros, y sus imponentes edificios y rascacielos, algunos de ellos entre los más altos de América Latina y el mundo. El desarrollo megaportuario, la bolsa de valores, de diamantes y las transacciones inmobiliarias son la tónica del inicio del Siglo XXI, siendo considerado el país y su capital como uno de los mejores países para vivir.

El 15 de agosto de 1519, Pedro Arias Dávila funda Nuestra Señora Asunción de Panamá a orillas del océano Pacífico, que aparte de responder a las instrucciones dadas por el Rey Fernando de erigir poblados, se transformó en el centro de la actividad del descubrimiento y obtención de riquezas, con la partida de expediciones hacia el istmo de Centroamérica y el Perú.

Simultáneamente a la fundación de Panamá, Pedrarias envía a su lugarteniente Diego de Albítez a repoblar Nombre de Dios en el océano Atlántico, sitio que había sido descubierto por Cristóbal Colón y ocupado con algunas chozas de paja por Nicuesa en 1510. Entre ambos puertos, se estableció el Camino Real, una ruta en tierra firme que atravesaba el Istmo de Panamá para el transporte de mercancías y metales preciosos entre ambos océanos.

Gaspar de Espinosa en compañía del piloto Juan de Castañeda parten en julio de 1519 con una expedición que visitaría las tierras de los caciques Paris, Escoria y Chagres, haciendo un reconocimiento de la costa septentrional del Mar del Sur, a bordo de los navíos de Balboa, el San Cristóbal y el Santa María de Buena Esperanza. En punta Burica desembarca dispuesto a emprender su viaje de regreso a Panamá por tierra, mientras Juan de Castañeda continuaba la navegación hacia el norte hasta alcanzar el golfo de Nicoya en Costa Rica. En su camino de retorno Espinosa fue apresando indígenas con la finalidad de llevarlos a Panamá para ser repartidos en encomiendas. En 1520, Gaspar de Espinosa establece el asiento de Natá, en territorios fértiles convirtiéndose rápidamente en un centro agrícola y de frontera con Veragua. Pedrarias declara la fundación de Natá el 20 de mayo de 1522, la cual fue atacada por los indígenas dirigidos por el poderoso cacique Urracá, quien agrupó en torno suyo a los pueblos de las regiones de Chiriquí y Veraguas, creando una oposición al avance español en el área por casi una década. En 1531 muere el gran jefe indio Urracá.

Pedrarias, interesado en encontrar un estrecho marino que comunicara ambos mares, se dedicó a organizar una serie de expediciones como la de Gil González Dávila y Andrés Niño que navegaron y desembarcaron en la actual Costa Rica y luego en Nicaragua. Gracias a los indígenas González Dávila conoció la existencia de dos grandes lagos, Nicaragua y Managua, pensando erróneamente que se trataba de un estrecho entre los mares.

Otra expedición organizada por Pedrarias fue la del capitán Francisco Hernández de Córdoba, acompañado por Gabriel de Rojas, Francisco Campañón y Hernando de Soto, que partió a fines de 1523, con la misión de fundar poblaciones a lo largo de toda la tierra visitada por Gil González y Andrés Niño. Hernández de Córdoba visitó parte de Costa Rica y en 1524 fundó el asiento de Bruselas próximo a la actual Puntarenas, a orillas del lago Cocibolca fundó la ciudad de Granada y al norte del lago Managua erigió el asiento de León.

En 1523, Hernán Cortés había concluido la conquista del Imperio azteca y con el propósito de encontrar un paso o estrecho entre los dos mares, envió a Pedro de Alvarado con destino a Guatemala y a Cristóbal de Olid con dirección a la actual Honduras, creando una situación de rencillas con Pedrarias.

Hacia 1526 tanto las exploraciones enviadas por Pedrarias desde Panamá como las de Cortés desde México habían demostrado que el tan ansiado estrecho de mar no existía en Centroamérica. Para entonces ya se habían cumplido seis años desde que Fernando de Magallanes el 28 de noviembre de 1520 descubriera en el extremo meridional del continente el estrecho de los Patagones que hoy lleva su nombre.

El 20 de mayo de 1524, Pedrarias autoriza la expedición de Francisco Pizarro, Diego de Almagro y el sacerdote Hernando de Luque, la cual parte el 14 de noviembre desde Panamá hacia la conquista del Perú.

Como resultado de las exploraciones en América Central y Perú, se produce un despoblamiento de los principales asentamientos en el istmo. Esta situación es mencionada por Pedro Cieza de León en 1535, en una descripción de la ciudad de Panamá donde indica que habiendo muerto los antiguos conquistadores, los nuevos pobladores no pensaban en habitar Panamá más tiempo del necesario para hacerse ricos, sin miras a colonizar y establecerse en el istmo. Panamá dejó de ser el habitual centro de exploraciones, descubrimientos y conquista para convertirse en el sitio de paso de metales preciosos y productos americanos con destino a Europa, y a la vez de centro de comercio de manufacturas europeas con las que el Imperio español abastecía a los mercados de las Indias Occidentales. La función de ruta de tránsito fue el papel que asumió el territorio panameño durante poco más de dos siglos en la época colonial española.
Las ferias realizadas en la costa atlántica del istmo de Panamá, primero en Nombre de Dios en 1544 y a partir de 1597 en Portobelo, tenían como objetivo primordial abastecer de artículos europeos los mercados americanos y enviar con destino a España los metales preciosos procedentes del Perú. La importancia de este evento de intercambio comercial se pone de manifiesto en los datos suministrados que indican que entre 1531 y 1660, de todo el oro que ingresó a España procedente del Nuevo Mundo, el 60% cruzó por el Istmo de Panamá. La última feria se realizó en Portobelo en 1737.

El camino real era casi intransitable en época de estación lluviosa por lo que se pensó en una nueva ruta. En 1536 se autorizó a la Municipalidad de Panamá a construir un almacén en Venta Cruz o Cruces a orillas del río Chagres, a siete millas de la ciudad de Panamá. Ante las deplorables condiciones en que se encontraba el camino real, en 1569 el Virrey del Perú, Francisco de Toledo, ordenó construir otro camino que pasara por Cruces, el cual fue llamado camino de cruces. El sitio del antiguo pueblo de Cruces se encuentra bajo las aguas del Lago Gatún en el Canal de Panamá.

El río Chagres representó para las autoridades españolas una posibilidad de servir como parte de una ruta transístmica navegable. Con este propósito, en 1527 el Gobernador Pedro de los Ríos instruyó a Hernando de la Serna, Miguel de la Cuesta y Pedro Corso para que hicieran exploraciones en el río Chagres, los cuales determinaron que era favorable para ser utilizado en una vía para comunicar ambos mares.

En 1529, Álvaro de Saavedra Cerón fue el primero en proponer la construcción de un canal interoceánico por el Istmo de Panamá, pero en 1533 Gaspar de Espinosa le escribe al rey Carlos I de España señalándole que el río Chagres podría hacerse navegable a un costo muy bajo, siendo la ruta más útil del mundo, afirmando que un canal para la navegación puede ser excavado. Por órdenes de la Corona española se hicieron otras exploraciones en el río Chagres durante las Gobernaciones de Antonio de la Gama y Francisco de Barrionuevo sin resultados alentadores.

Fue creada mediante Real Cédula del 26 de febrero de 1538 por Carlos I y fue la tercera Audiencia del continente. En ella se incluían las provincias de Tierra Firme (Castilla de Oro y Veragua), todos los territorios que comprenden desde el Estrecho de Magallanes hasta el golfo de Fonseca (las provincias del Río de la Plata, Chile, Perú, la gobernación de Cartagena y Nicaragua).
La introducción de los negros en condición de esclavos provenientes de Senegal y el ex-reino del Congo, ofreció resistencia como antes lo hizo el indio, con levantamientos y ataques al Camino de Cruces, por parte de los negros cimarrones como Felipillo y Bayano. La convivencia entre blancos criollos, indios y negros trajo una mezcla de razas en el istmo.

Durante los siglos XVI y XVII, Panamá fue blanco de constantes ataques por parte de Inglaterra mediante piratas, corsarios, filibusteros y bucaneros, asesinos racistas que masacraban poblaciones y hacían esclavos, como Francis Drake y Henry Morgan, así como algunos intentos escoceses de colonizar el Darién, en territorios denominados por ellos como Nueva Caledonia.

Para 1746 las flotas del Mar del Sur utilizaban la ruta del Cabo de Hornos, que aunque era más larga en distancia, resultaba ser más segura. En 1753 se permitió a los barcos de registro utilizar el puerto de Buenos Aires y con las reformas de Carlos III en 1764 se comienza a abrir al comercio los puertos de España y las Indias, lo cual significó para el Istmo la postración económica. Los campos adquieren importancia económica debilitando la vida urbana.

Los movimientos separatistas nacidos a la sombra de Inglaterra y Francia, transforman al istmo en sitio de exportador de ejércitos realistas, pues la situación de España y sus colonias se había agravado y los movimientos conducían a las guerras separatistas.

La independencia de las 13 Colonias de Inglaterra en 1776 para constituirse en EE. UU., acrecientan el tema de los movimientos independentistas de España por parte de varios panameños, que propugnaban por un régimen de libertades comerciales y civiles, contra el desgastado régimen monárquico. En 1812 se establece el Virreinato del Istmo de Panamá, como respuesta al contrabando y restableciendo el comercio por el istmo. 

La invasión napoleónica a España y las victorias de Simón Bolívar en Boyacá debilitan el poder de la corona española en América, empobreciendo el comercio en el istmo. En 1815, Simón Bolívar en su profética carta de Jamaica habla de la asociación de los estados del istmo de Panamá hasta Guatemala en una sola nación, la cual es vista con admiración por los criollos (españoles o sus descendientes) en cuyas manos tenían todo el poder económico.

El movimiento panameño de independencia de la Corona Española se inicia el 10 de noviembre de 1821 con los eventos del Primer Grito de Independencia en la Villa de Los Santos por Rufina Alfaro, el cual contó con el respaldo de otras ciudades como Natá, Penonomé, Ocú y Parita.

El ejército realista de la Ciudad de Panamá estaba al mando del general José de Fábrega, criollo oriundo de Panamá, lo cual fue aprovechado por los istmeños, obteniendo la complicidad del General Fábrega, las sociedades patrióticas y el clero, que contribuyó económicamente al movimiento. El 28 de noviembre, el Ayuntamiento convocó a Cabildo Abierto y en acto solemne, en presencia de las autoridades militares, civiles y eclesiásticas, se declararon rotos los vínculos que ataban al Istmo de Panamá con España. Entre los personajes ilustres se encontraban José Higinio Durán y Martell, Obispo de Panamá, Carlos de Icaza Arosemena, Mariano Arosemena, Juan de Herrera, Narciso de Urriola, José de Alba, Gregorio Gómez, Manuel María Ayala, Antonio Planas, Juan Pío Victorias, Antonio Bermejo, Gaspar Arosemena y Casimiro del Bal.

El 30 de noviembre de 1821 las fragatas de guerra Prueba y Venganza llegan a la Bahía de Panamá acompañadas a buscar al resto de las tropas españolas. Los capitanes españoles José de Villegas y Joaquín de Soroa firman un tratado de paz con el Coronel José de Fábrega el 4 de enero de 1822, entre la monarquía española y los patriotas donde acuerdan la no agresión a los territorios del istmo y la retirada de las tropas y todos los barcos de la Corona Española de la nueva nación istmeña.

La falta de presupuesto, el poco armamento militar con el que se contaba y la inseguridad de ser reconquistados por España, pone en peligro el seguir con la aventura independentista del istmo, por lo que se proponen la unión con algunas de las nuevas naciones americanas, entre ellas los vecinos de la unión centroamericana y la nación del Perú que había sido el principal socio comercial del istmo en la época colonial.

Sin embargo, los patriotas panameños, admirando el liderazgo y la visión de Simón Bolívar, toman la decisión de unirse voluntariamente a la República de Colombia o Gran Colombia.

Hacia 1810 los territorios correspondientes a la Real Audiencia de Panamá estaban conformados por Castilla del Oro y el Ducado de Veragua (de la familia Colón). Al declarar su unión voluntaria a la Gran Colombia de Simón Bolívar (Cundinamarca, Venezuela y Quito), fue dividida en dos provincias: la de Panamá (que comprendía la ciudad de Panamá, el Darién, las costas del golfo de Urabá en el Caribe y el Chocó) y la de Veraguas (que extendía desde los territorios centrales del Istmo, la ciudad de Natá de los Caballeros, parte de la actual Costa Rica como Burica en el Pacífico, la costa del golfo de los Mosquitos hasta la frontera de la actual Nicaragua y las varias islas en el Caribe, como el archipiélago de San Andrés y Providencia, frente a las costas de Nicaragua). Esta situación no fue tomada con agrado por los habitantes del istmo, generando en el futuro situaciones de distanciamiento con el gobierno colombiano y movimientos separatistas.

El Congreso Anfictiónico de junio de 1826, bajo el ideal de Simón Bolívar, reúne en Ciudad de Panamá a representantes de los nuevos países del continente americano como Centroamérica, la Gran Colombia, México y Perú, como una confederación en defensa del continente contra posibles acciones de la Liga de la Santa Alianza conformada por las potencias europeas y sus reclamaciones de territorios perdidos en América.

En 1830 se produce la Primera Separación de Panamá de Colombia. La Gran Colombia atravesaba por un caos político debido a que Venezuela y Ecuador tomaron la decisión de separase de la confederación, Sucre había sido asesinado y Bolívar desistió del gobierno. El general José Domingo Espinar, Comandante Militar del Istmo, declara la separación de Panamá el 26 de septiembre de 1830, al no estar de acuerdo con la inestabilidad del gobierno de Joaquín Mosquera, sucesor de Bolívar. Espinar le ofrece a Bolívar el gobierno del Istmo, para que luchara por la adhesión de los demás países de la confederación, sin embargo Bolívar se encontraba enfermo y declina el ofrecimiento, pidiéndole a Espinar que reintegrara el Istmo de nuevo a la Gran Colombia. Panamá fue reintegrada a la confederación el 11 de diciembre de 1830, insinuando la posibilidad de una nación independiente de la Gran Colombia.

El general Fábrega no apoyaba la decisión de reintegro del istmo por parte de Espinar y se marcha hacia Veraguas, dejando a cargo del control militar de la Ciudad de Panamá al coronel Juan Eligio Alzuru. Los enemigos de Espinar convencen a Alzuru de aprisionarlo y enviarlo al destierro. Con la idea de proclamarse dictador, Alzuru busca apoyo en el pueblo panameño y su sentido nacionalista, dando como resultado la Segunda Separación de Panamá de Colombia el 9 de julio de 1831. Alzuru se convirtió en un dictador y pierde el apoyo de la población panameña. La llegada al istmo del Coronel Tomás Herrera, en cooperación con Fábrega y demás panameños ilustres, Alzuru es apresado y fusilado. Meses después, la nación del istmo se vuelve a unir a Colombia, con el desencanto de estar unido a un país en decadencia, con la extinción de la Gran Colombia, ya que Venezuela y Ecuador eran países independientes, y la falta del liderazgo de Simón Bolívar, dejando ver entre los panameños que formar parte de la República de la Nueva Granada era innecesario, naciendo así sociedades y partidos con ideales separatistas en Panamá.

La guerra granadina de 1839 al mando de general José María Obando, lanzó a la región a un conflicto armado, al cual los habitantes del istmo se sentían ajenos y preferían evitar. Desistiendo de entrar a la guerra, se creó una junta popular reunida en la Ciudad de Panamá el 18 de noviembre de 1840, para declarar la separación de Panamá de Colombia por tercera vez, bajo el nombre del Estado del Istmo. Encabezado por el coronel Tomás Herrera, se redacta la primera constitución panameña, se organiza la economía y las instituciones políticas de la nación. Costa Rica y EE. UU. reconocieron al nuevo país. Tras meses de negociación el gobierno de Bogotá logra convencer al Coronel Herrera de reintegrar al istmo bajo el acuerdo de no emprender castigo contra los secesionistas istmeños. Haciendo caso omiso a lo acordado, una vez reintegrado el istmo, el Coronel Herrera es desterrado y borrado del escalafón militar.

Al reintegrarse el istmo de Panamá a la Nueva Granada en 1841, las autoridades neogranadinas entrevieron que Inglaterra tenía intenciones de tomar posesión de alguna región panameña por donde se pudieran unir las dos costas por algún medio de comunicación, cercenando el territorio neogranadino. Pruebas de esa apreciación eran los enclaves ingleses en Centroamérica (Belice y costa de los Miskitos); entonces buscó la protección de EE.UU. para que salvaguardara la soberanía neogranadina en Panamá, ofreciéndole, a cambio, importantes privilegios en esa parte del istmo. Con ese propósito, el Ministro de Relaciones Exteriores de la Nueva Granada, Manuel María Mallarino y el encargado de los negocios estadounidenses Benjamin Bidlack firman, el 12 de diciembre de 1846, el tratado Mallarino-Bidlack, en donde EE.UU. garantiza la soberanía neogranadina en Panamá, y la Nueva Granada concede a EE.UU. el privilegio de usar el istmo para la construcción de vías de comunicación entre las dos costas. Asimismo, los Estados Unidos se comprometen a garantizar la neutralidad del istmo y el libre tránsito entre los océanos Pacífico y Atlántico, produciéndose la entrada del ejército estadounidense en territorio panameño y abriendo la puerta al intervencionismo norteamericano en Panamá. Una de las consecuencias de este tratado es el desaliento de los panameños en el deseo de separarse de la Nueva Granada, durante la segunda mitad del siglo XIX, viendo tropas norteamericana acantonadas en su territorio dispuestas a "garantizar el orden".

En 1850 el general José Domingo Espinar y el dr. E. A. Teller editor del periódico "Panama Echo", llevan a cabo una revolución la madrugada del 29 de septiembre, que termina con la Cuarta Separación de Panamá de Colombia. Obaldía, gobernador del Istmo, no estaba de acuerdo con esta separación ya que veía al istmo todavía no preparado para asumir el control de su destino, convenciendo de desistir y reintegrar nuevamente al istmo.

La fiebre del oro en California, produjo la migración de viajeros de todo el mundo por diversas rutas, convirtiendo a Panamá como la vía más corta y factible entre el este y el oeste del continente americano, haciendo retomar la idea de la construcción de vías de comunicación como canales y ferrocarriles para el paso de mercancías y pasajeros. Los derechos para la construcción y administración de la obra por parte de los Estados Unidos en territorio panameño fueron negociados por el gobierno de Bogotá a través del Convenio Paredes-Stephens. El 28 de enero de 1855 se inaugura el Ferrocarril de Panamá por parte del presidente de la Nueva Granada, el panameño José de Obaldía. Una de las obras de ingeniería más importantes de esa época, que atravesaba el istmo, y convertía a la Ciudad de Panamá en la primera gran metrópoli que tuvo Colombia. Bajo el liderazgo de William J. Aspinwall, John L. Stephens y James L. Baldwin, se completa la construcción del ferrocarril, demostrando un gran valor y resistencia a los intensos trabajos y lucha contra las enfermedades.

Justo Arosemena, estadista elegido representante del istmo ante el Congreso Granandino, logró el 27 de febrero de 1855 que se incorporase a la constitución, por medio de un Acto Legislativo, la creación Estado Federal de Panamá.

El 15 de abril de 1856 ocurrieron una serie de hechos violentos entre panameños y estadounidenses conocidos como "el incidente de la tajada de sandía". El estadounidense Jack Olivier, decide comprarle al panameño José Manuel Luna una tajada de sandía, la cual se comió y por la que se negó a pagar un real o 5 centavos de dólar. Esto generó una discusión que finalizó cuando Olivier saca un arma y dispara, escapando luego del lugar. Esto provocó una pelea entre panameños y estadounidenses, donde se termina por incendiar las instalaciones del ferrocarril, provocando que los soldados estadounidenses reprimieran a la población panameña, con un saldo de 16 muertos estadounidenses y 2 muertos panameños. El gobierno de Estados Unidos acusó a la policía de Nueva Granada de haberse puesto de parte de los panameños y permitirles asaltar y saquear propiedades estadounidenses, indicando la incapacidad de mantener el orden y suministrar protección adecuada para el tránsito estadounidense por Panamá.

El 19 de septiembre de ese año, el ejército estadounidense desembarca un destacamento militar para la protección de la estación de ferrocarril y restablecer el orden en la Ciudad de Panamá. Esta ocupación es considerada el primer caso de intervención armada en Panamá por parte del gobierno estadounidense, con el motivo de garantizar la neutralidad y el libre tránsito a través del istmo. El 10 de septiembre de 1857 el gobierno granadino acepta su culpabilidad y firma el Tratado Herrán-Cass, pagando una indemnización de US$ 412.394 (dólares estadounidenses en oro), por los daños causados por los panameños.

El 5 de julio de 1874 se funda la "Compagnie Universelle du Canal Interocéanique" por parte del conde De Lesseps, con el propósito de construir un canal a nivel por Panamá. Los franceses iniciaron los trabajos en enero de 1881, pero los grandes gastos y el poco control existente, sumado al desconocimiento de la forma de transmisión de enfermedades en la región como la fiebre amarilla y la malaria se convirtieron en el principal obstáculo para la construcción del canal. Entre los trabajadores altamente calificados que llegaron al istmo para la construcción del canal por parte de Francia se encontraba el ingeniero francés Phillipe Bunau-Varilla, graduado de la École Polytechnique y de la École de Ponts et Chaussées, que a la edad de 27 años es designado Jefe Interino de la Compañía del Canal.

La "Compagnie Universelle du Canal de Panamá" fue intervenida y liquidada el 15 de septiembre de 1889. Como causas probables para explicar el fracaso se indican una mala administración, corrupción, alta mortalidad por enfermedades tropicales y la no aceptación por parte del Conde de Lesseps de no cambiar el proyecto de canal a nivel por uno de esclusas, como alternativa y recomendación de ingeniería para poder concluir la obra. En esfuerzos desesperados por salvar los dineros de la compañía, se autoriza a vender activos y derechos en el istmo a los Estados Unidos, por parte de Bunau-Varilla. La aventura francesa en el istmo duró diez años a un costo aproximado de 1.400 millones de francos y una pérdida de vidas humanas cercana a los 20.000 muertos.

Entre 1899 y 1902 se desata la Guerra de los Mil Días entre liberales y conservadores, convirtiendo al istmo en un sangriento campo de batalla donde muere gran parte de la juventud panameña, como lo reflejan las batallas del puente de Calidonia en julio de 1900 y la Aguadulce en febrero de 1901. El 22 de noviembre de 1902 conservadores y liberales firmaron en el barco de guerra estadounidense "Wisconsin", el pacto llamado la Paz del Wisconsin, donde se da por terminado el conflicto. En noviembre de 1902 es capturado Victoriano Lorenzo, con el argumento de que no compartía el acuerdo de paz y que tomaría de nuevo las armas. El gobierno colombiano, temeroso de que el guerrillero panameño fuera puesto en libertad, decide condenarlo a muerte presentándolo como un delincuente común. El 15 de mayo de 1903 el caudillo liberal es ejecutado en la Ciudad de Panamá. Su cadáver nunca fue entregado a sus familiares y amigos.

En enero de 1903 se firma el Tratado Herrán-Hay entre Estados Unidos y Colombia para finalizar la construcción del canal por territorio panameño, el cual luego no fue ratificado por el senado colombiano el 12 de agosto, aduciendo que la cláusula que concedía soberanía a EE.UU. sobre el canal y una franja a lado y lado, era inaceptable.

Si bien es cierto que la independencia de Panamá de España fue un movimiento ajeno a la revolución liderada por Bolívar, la unión voluntaria de la Nación del Istmo a Colombia, en busca de un mejor futuro bajo el liderazgo de Simón Bolívar, fue una decisión tomada por los istmeños en 1821, la cual estuvo marcada por las situaciones adversas vividas en las diferentes repúblicas colombianas como enfrentamientos sociales, decisiones políticas desatinadas y una mala situación económica que no presentaba una salida al empobrecimiento al que había sido sometida la nación del istmo.

Luego de 17 intentos de separación y 4 separaciones declaradas con un posterior reintegro de la unión con Colombia, el fracaso de la construcción del canal por parte de los franceses, la Guerra de los Mil Días librada en territorio panameño, el fusilamiento del caudillo liberal Victoriano Lorenzo, el rechazo del senado colombiano al tratado Herrán-Hay para la construcción del canal interoceánico por parte de los Estados Unidos sirven de detonante para un nuevo movimiento separatista liderado por líderes José Agustín Arango, Manuel Amador Guerrero, Carlos Constantino Arosemena, General Nicanor A. De Obarrio, Ricardo Arias, Federico Boyd, Tomás Arias y Manuel Espinosa Batista.

José Agustín Arango, prominente ciudadano y político istmeño, trabajó en secreto la preparación del movimiento separatista y conformó una junta revolucionaria clandestina destinada a separar el istmo de la soberanía colombiana, y así poder negociar directamente con Estados Unidos la construcción del canal interoceánico por Panamá, ya que los Estados Unidos exploraba la posibilidad de la construcción de la vía entre Nicaragua y Costa Rica. Por su parte, Manuel Amador Guerrero viajó en secreto a los Estados Unidos en busca de apoyo para el plan. Así mismo, el movimiento obtuvo en Panamá el respaldo de importantes jefes liberales y el apoyo del comandante militar Esteban Huertas, acordándose la puesta en marcha del plan separatista para un día no definido del mes de noviembre de 1903.

Los insistentes rumores sobre un movimiento en ciudad de Panamá, hicieron que Colombia movilizara al Batallón Tiradores desde Barranquilla, con instrucciones para reemplazar al gobernador José Domingo de Obaldía y al general Esteban Huertas, quienes ya no gozaban de confianza por parte del gobierno de Bogotá.

La mañana del 3 de noviembre de 1903 desembarca en Colón el Batallón Tiradores, al mando de los generales Juan B. Tovar y Ramón G. Amaya. El contingente armado debió ser transportado hacia Ciudad de Panamá, pero fueron comunicados de contratiempos, por parte de las autoridades del Ferrocarril de Panamá, quienes actuaron en complicidad con el movimiento separatista. Sin embargo los generales y altos oficiales accedieron a transportarse a la Ciudad de Panamá sin sus tropas.

Una vez llegados a Ciudad de Panamá, Tovar, Amaya y sus oficiales fueron arrestados por órdenes del general Esteban Huertas, quien comandaba el selecto Batallón Colombia, cuya jefatura pretendían reemplazar.

La decisión del general Huertas de apoyar el movimiento separatista y arrestar a los generales colombianos dependió del apoyo que le brinda el general Domingo Díaz quien junto al pueblo del arrabal de Santa Ana tomaron las armas, formando un ejército de más de mil panameños listos a defender la patria. La flota naval anclada en la bahía de Panamá se rindió sin oponer resistencia.

En la Ciudad de Colón quedó la tropa del Batallón Tiradores bajo el mando del coronel Eliseo Torres, quienes fueron sometidos por las fuerzas separatistas y obligados a zarpar del Istmo rumbo a Colombia.

Toda la Ciudad de Panamá se encontraba conmocionada y en todos los barrios se escuchaban los gritos de celebración y festejo a la naciente República de Panamá. La tarde del 3 de noviembre de 1903 el Consejo Municipal de la Ciudad de Panamá presidido por Demetrio H. Brid se reunió bajo la voluntad del pueblo de ser libre y de establecer un Gobierno propio, independiente, y soberano, sin la subordinación de Colombia, bajo el nombre de República de Panamá, decisión que halló inmediatamente respaldo en el resto del país. 

El Consejo Municipal de Panamá establece el 4 de noviembre una Junta Provisional de Gobierno conformada por José Agustín Arango, Federico Boyd y Tomás Arias, la cual ejerció funciones hasta febrero de 1904 cuando la Convención Nacional Constituyente designa a Manuel Amador Guerrero como primer Presidente Constitucional de la República de Panamá.

Hubo varios intentos por parte del gobierno colombiano para revertir la separación del istmo, desde reuniones de alto nivel entre representantes de Bogotá y Panamá, ofrecimientos políticos como la aprobación del tratado del canal que había sido rechazado y el traslado de la capital de Colombia a Ciudad de Panamá, así como un fracasado intento de invasión militar a través de las selvas del Darién y hasta la invocación del tratado Mallarino-Bidlack que exigía a los Estados Unidos someter militarmente al pueblo panameño a fin de restablecer una soberanía colombiana sobre la nación del Istmo. Sin embargo la decisión para los panameños ya estaba tomada y la República de Panamá fue rápidamente reconocida por las naciones latinoamericanas, los Estados Unidos y las potencias europeas.

El 30 de marzo de 1922, el Congreso de Estados Unidos ratificó el tratado Thompson-Urrutia, que concedía a Colombia una indemnización por 25 millones de dólares, con el propósito de "eliminar todas las desavenencias producidas por los acontecimientos políticos ocurridos en Panamá en 1903", además de otorgarle a Colombia el derecho a tránsito gratuito por el Canal para buques de guerra y tropas. A raíz de dicho tratado se produce el intercambio de embajadores, Nicolás Victoria Jaén por Panamá y Guillermo Valencia por Colombia, lo que marca el inicio de relaciones diplomáticas y el reconocimiento de ambos países.

Una vez declarada la Separación de Panamá de Colombia, el nuevo gobierno por medio de su embajador plenipotenciario Philippe-Jean Bunau-Varilla, logra la firma de un tratado para la construcción de un canal interoceánico por el istmo con el gobierno de los Estados Unidos de América. El Tratado Hay-Bunau Varilla permitió la construcción de la vía que había quedado inconclusa por el grupo francés de Ferdinand de Lesseps y el gobierno de Colombia. La sorprendente obra de ingeniería fue terminada en 1914 utilizando tecnología avanzada para la época como motores eléctricos con sistemas de reducción para mover las compuertas de las esclusas, sistemas de vías de ferrocarril para movilizar las toneladas de material excavado y la construcción del lago Gatún, el lago artificial más grande del mundo hasta esa época. Algunos aspectos en salud pública resultaron de relevancia ya que se consideraron como uno de los obstáculos que motivaron el fracaso de la empresa francesa. El saneamiento y fumigación de las áreas, así como la reconstrucción de los acueductos y alcantarillados de las ciudades de Panamá y Colón fueron decisivos.

Los tratados del canal concedían la administración de una franja de terreno de 10 millas de ancho a lo largo de la vía interoceánica al gobierno de los Estados Unidos, que aun cuando se reconocía la soberanía de Panamá generó situaciones de conflicto entre ambas naciones en décadas siguientes.

Las controversias políticas surgidas por la interpretación de los tratados, eran consideradas como una amenaza a la soberanía panameña y acentuaban las diferencias entre las autoridades del Istmo y las de la Zona del Canal. En 1914, el Presidente Belisario Porras plantea por primera vez la necesidad de un nuevo tratado sobre el Canal de Panamá.

El tratado Arias-Roosevelt de 1936, firmado por los presidentes Harmodio Arias Madrid de Panamá y Franklin Delano Roosevelt de Estados Unidos, anula el principio de la intervención militar norteamericana en los asuntos internos del estado panameño, cambiando el concepto jurídico de país protegido por Estados Unidos para garantizar su independencia.

En 1948 se crea la Zona Libre de Colón como una institución autónoma del estado panameño, por el Presidente Enrique A. Jiménez, a través de una zona franca que aprovecha la posición geográfica, los recursos portuarios y el canal como paso de rutas navieras mundiales.
La firma del Tratado Remón-Eisenhower de 1955, entre los presidentes José Antonio Remón Cantera de Panamá y Dwight David Eisenhower de Estados Unidos, le otorga nuevas ventajas económicas y el pago de arriendos a Panamá por el canal.

El Puente de las Américas, la estructura sobre el Canal de Panamá que une por vía terrestre el istmo, es inaugurado el 12 de octubre de 1962.

El 9 de enero de 1964, estudiantes del Instituto Nacional lideran un movimiento que reclama la izada de la bandera panameña junto a la estadounidense en la zona del canal, según los acuerdos Chiari-Kennedy de 1962, terminando en disturbios estudiantiles y enfrentamientos con la población civil. Como medida para controlar la situación, el gobernador de la Zona del Canal autoriza al ejército estadounidense quien abre fuego contra civiles panameños dejando un saldo de 22 muertos y más de 400 heridos. El Presidente de Panamá Roberto F. Chiari, en una situación sin precedentes en el continente americano, rompe relaciones diplomáticas con los Estados Unidos de América y declara el no reinicio de las mismas hasta que se acordara abrir negociaciones para un nuevo tratado. En abril de ese año, ambas naciones reasumen relaciones diplomáticas y el presidente estadounidense Lyndon Johnson accede a iniciar conversaciones con el propósito de eliminar las causas de conflicto entre ambas naciones.

En 1965, Panamá y Estados Unidos firmaron la Declaración Robles-Johnson, entre los presidentes Marco Aurelio Robles de Panamá y Lyndon Johnson de Estados Unidos, en los cuales se tocaron temas como la administración del canal, la exploración para un canal a nivel por una nueva ruta, y la defensa de la vía acuática.

El 11 de octubre de 1968, a sólo unos días de haber asumido la presidencia Arnulfo Arias Madrid, los mandos medios de la Guardia Nacional, liderados por Boris Martínez, secundado por el Coronel Omar Torrijos Herrera dan un golpe estado, en el comunicado oficial los golpistas señalaron que el intento por violar la voluntad popular en las elecciones legislativas, así como la integración ilegal del Tribunal Electoral, los había llevado a adoptar la decisión de asumir el poder por medio de un gobierno provisional que preparara el retorno al orden democrático. estableciendo el inicio de una dictadura militar en el país que duró 21 años, bajo 4 regímenes distintos que fueron, Junta Militar (1968 a 1969), Omar Torrijos llamado también "El Proceso Revolucionario" (1969 a 1981), Rubén Darío Paredes(1981 a 1983) y Manuel Antonio Noriega (1983 a 1989). Bajos estos 4 regímenes ocurrieron exilios y desapariciones, como también movimientos armados a favor de Arnulfo Arias Madrid en Piedra Candela en la Provincia de Chiriquí y Huacas del Ige en la Provincia de Coclé que fueron derrotados por la Guardia Nacional, dando como resultados perdidas humanas en ambos bandos. En 1972 el gobierno militar del General Torrijos emite una nueva constitución política (Sigue vigente bajo las reformas de 1983 y 2003) en la cual se le reconoce como líder del proceso revolucionario del 11 de octubre y jefe del estado panameño. 

Torrijos implementó una política populista, con la inauguración de escuelas y la creación de empleo, la redistribución de tierras agrícolas (que fue la medida más popular de su gobierno). Las reformas fueron acompañadas de un importante programa de obras públicas. También se enfrenta a las multinacionales norteamericanas, que exigen aumentos salariales para los trabajadores y la redistribución de 180.000 hectáreas de tierras no cultivadas. En febrero de 1974, siguiendo el modelo de la OPEP para el petróleo, intentó formar la Unión de Países Exportadores de Banano con los demás Estados centroamericanos para responder a la influencia de estas multinacionales, pero no obtuvo su apoyo. Su política promueve el surgimiento de una clase media y la representación de las comunidades indígenas. En 1981 muere el General Torrijos en un accidente aéreo. 

Luego de la muerte de Omar Torrijos se establece un relevo generacional dentro de la Guardia Nacional, en el cual figuraban los siguientes nombres: General Rubén Darío Paredes, Roberto Díaz Herrera y Manuel Antonio Noriega.
Paredes asume bajo como General en Jefe de la Guardia Nacional durante poco tiempo. El régimen cerró diarios de publicación masiva y censuró toda publicación contra el gobierno de turno. El General Paredes tenía aspiraciones políticas, en especial, el ser presidente de Panamá, y se lanzó confiado del apoyo de los militares junto a Noriega en las elecciones de mayo de 1984, y traspasó el mando de las Fuerzas Armadas a Manuel Antonio Noriega; teniendo un total descalabro y siendo pasado a retiro
En agosto de 1983 asciende a comandante en jefe de la Guardia Nacional el General de Cuatro Estrellas, Manuel Antonio Noriega, que transforma la institución armada en las Fuerzas de Defensa de Panamá. El General Noriega fue acusado de narcotraficante, de corrupción y fraude electoral de 1984 por el doctor Hugo Spadafora, quien fue asesinado, por su segundo al mando Coronel Roberto Díaz Herrera, provocando protestas y manifestaciones por parte de la población panameña, que fueron reprimidas brutalmente por las Fuerzas de Defensa. Durante los siguientes años, el país cae en una recesión económica y social, cuando el Índice de Desarrollo Humano pasa de 0,769 en 1985 a 0,765 en 1990; se sufre una contracción del PIB por dos años seguidos (1987: -1.8), (1988: -13.3). Más tarde en mayo de 1989, por instrucciones del General Noriega son anulados los resultados electorales para elecciones presidenciales, suspendiendo en septiembre la constitución y asumiendo el control de la nación panameña en calidad de jefe del gabinete de guerra, declarando a Panamá en estado de guerra con EE. UU.

El 20 de diciembre de 1989 el ejército de EE. UU. invadió Panamá. El 3 de enero de 1990, al cabo de dos semanas de asedio en la Nunciatura, Noriega se entregó a las tropas estadounidenses y 12 de enero el Pentágono dio por concluida la Operación Causa Justa. Noriega fue llevado ante los tribunales estadounidenses acusado de narcotráfico, y marcando el fin de la dictadura militar en Panamá.

Durante la invasión, en la base militar de Howard, bajo control estadounidense en esa época, prestó como juramento Guillermo Endara Galimany, Ricardo Arias y Guillermo Ford como presidente, vicepresidente primero y vicepresidente segundo de la República, respectivamente. Endara habría sido el ganador de las elecciones anteriores del 7 de mayo de 1989, abolidas por el régimen militar de Manuel Antonio Noriega y debido a las cuales, el 1 de septiembre de 1989, toma posesión como presidente de la República, Francisco Rodríguez Poveda, miembro del Partido Revolucionario Democrático.

El 27 de diciembre de 1989, un nuevo escrutinio por el Tribunal Electoral de las actas correspondientes al 83,1% de las mesas electorales atribuyó a Endara el 62,5% de los sufragios frente al 24,9% de Carlos Alberto Duque Jaén, sustentado por la progubernamental Coalición de la Liberación Nacional (COLINA), integrada por el Partido Revolucionario Democrático, el Partido Liberal, el Partido Laboral Nacional, y otros. Tras esta certificación, el dirigente arnulfista fue proclamado presidente y su toma de posesión fue validada con carácter retroactivo. El abogado se inscribió como el postulante de la Alianza Democrática de Oposición Civilista (ADOC), que reunía a tres formaciones del centro-derecha y el nacionalismo moderado y antimilitar: el Partido Liberal Auténtico (PLA), de Arnulfo Escalona Ríos; el PDC, de Ricardo Arias Calderón y el MOLIRENA, de Guillermo Ford Boyd; los disidentes legitimistas del PPA y el pequeño Partido de Acción Popular (PAPA) cerraron filas también con Endara. La ADOC había obtenido 51 de los 67 escaños de la Asamblea, 27 de estos, del Partido Demócrata Cristiano.

El 10 de febrero de 1990, el gobierno del Presidente Endara emitió un decreto ejecutivo en el que reorganizaba la fuerza policial. De acuerdo con el decreto ejecutivo, las Fuerzas de Defensa de Panamá quedaban abolidas con efecto retroactivo al 22 de diciembre de 1989 y en su lugar se creaban una Policía Nacional (PN), un Servicio Marítimo Nacional (SMN), un Servicio Aéreo Nacional (SAN) y un Servicio de Protección Institucional (SPI), más tarde el 15 de noviembre 1992 se celebra en referéndum de reformas constitucionales entre las que figuraba la abolición del Ejército, el cual fue rechazado por casi el 60% de los votos, tiempo después la antigua 'asamblea legislativa' aprueba la abolición del ejército, que lleva a la nación por primera vez desde 1968 a un proceso electoral transparente en 1994, donde gana el candidato de oposición Ernesto Pérez Balladares, que por medio de una combinación de alianzas a lo interno del PRD logra retomar el poder político perdido en 1989, ganando con sólo el 33% de los votos debido a la inexistencia de una segunda vuelta electoral en el país y al hecho de que existían 7 candidatos para la presidencia. 

La gestión de gobierno 1994 a 1999 se destacó por una reforma intensa del Estado Panameño, empezada por Endara y continuada por Pérez Balladares. Estas reformas abarcaban, entre varias, las privatizaciones de instituciones prestadoras de servicios públicos (nacionalizadas por el mismo partido en la década de los 60) de energía (IRHE) y telecomunicaciones (INTEL), las empresas de juegos de azar, los puertos de Cristóbal y Manzanillo en la costa atlántica y de Balboa en el pacífico, de la cementera estatal y del ferrocarril transístmico, entre otros, un programa de ajuste económico y una reforma laboral que abarata el proceso de despido de un trabajador en favor de los empleadores. 

Esta serie de tendencias de corte neoliberal, si bien se alejan del sentido socialdemócrata del partido, favoreciendo el individualismo y no la igualdad en la distribución de la riqueza, también establece un punto de inflexión para el surgimiento de diversas teorías políticas dentro del PRD y la posterior democratización interna de la mayoría de los colectivos políticos del país. 

Se proponen también una serie de cambios constitucionales, entre los que se incluía la propuesta para que el Presidente de la República de Panamá tuviera la opción del pueblo lo reelegiera a un segundo mandato inmediato. En el Referéndum realizado posteriormente, el 63,8% de la población votó en contra de la propuesta, el rechazo fue considerado como un voto castigo al gobierno del presidente Ernesto Pérez Balladares.

Mireya Moscoso, viuda del expresidente Arnulfo Arias, gana las elecciones en 1999, convirtiéndose en la primera mujer que preside el gobierno panameño. El 31 de diciembre de 1999, en fiel cumplimiento de los tratados Torrijos-Carter, la República de Panamá asume el control total del canal de Panamá.

En mayo del 2004 gana las elecciones el Licenciado Martín Torrijos Espino, hijo del General Omar Torrijos. Ocupó el cargo desde septiembre del mismo año, hasta el 30 de junio de 2009.

Ricardo Martinelli, empresario millonario que ganó las elecciones en mayo de 2009 con un 61 % de la aceptación en contra de Balbina Herrera y Guillermo Endara, tomando posesión de la administración del gobierno desde el día uno de julio de 2009 hasta el día Primero de julio de 2014.

Laurentino Cortizo es el actual Presidente de la República de Panamá. Él ganó las Elecciones generales en Panamá de 2019 con 33,35 % de los votos. Sus principales rivales, Rómulo Roux y Ricardo Lombana, tuvieron 30.99 % y 18.78 % respectivamente. Esta fue la primera vez en la historia de Panamá que un candidato independiente obtiene tal cantidad de los votos.



</doc>
<doc id="5307" url="https://es.wikipedia.org/wiki?curid=5307" title="Galvanómetro">
Galvanómetro

Un galvanómetro es un instrumento que se usa para detectar y medir la corriente eléctrica. Se trata de un transductor analógico electromecánico que produce una deformación de rotación en una aguja o puntero en respuesta a la corriente eléctrica que fluye a través de su bobina. Este término se ha ampliado para incluir los usos del mismo dispositivo en equipos de grabación, posicionamiento y servomecanismos.

Es capaz de detectar la presencia de pequeñas corrientes en un circuito cerrado, y puede ser adaptado, mediante su calibración, para medir su magnitud. Su principio de operación (bobina móvil e imán fijo) se conoce como mecanismo de D'Arsonval, en honor al científico que lo desarrolló. Este consiste en una bobina normalmente rectangular, por la cual circula la corriente que se quiere medir. Esta bobina está suspendida dentro del campo magnético asociado a un imán permanente, según su eje vertical, de forma tal que el ángulo de giro de dicha bobina es proporcional a la corriente que la atraviesa. La inmensa mayoría de los instrumentos indicadores de aguja empleados en instrumentos analógicos se basan en el principio de operación explicado, utilizándose una bobina suspendida dentro del campo asociado a un imán permanente. Los métodos de suspensión empleados varían, lo cual determina la sensibilidad del instrumento. Así, cuando la suspensión se logra mediante una cinta metálica tensa, puede obtenerse deflexión a plena escala con solo 2 μA, pero el instrumento resulta extremadamente frágil, mientras que el sistema de "joyas y pivotes", semejante al empleado en relojería, permite obtener un instrumento más robusto pero menos sensible que el anterior, en el que típicamente se obtiene deflexión a plena escala, con 50 μA.

La desviación de las agujas de una brújula magnética mediante la corriente en un alambre fue descrita por primera vez por Hans Oersted en 1820. Los primeros galvanómetros fueron descritos por Johann Schweigger en la Universidad de Halle el 16 de septiembre de ese año. El físico francés André-Marie Ampère también contribuyó a su desarrollo. Los primeros diseños aumentaron el efecto del campo magnético debido a la corriente mediante el uso de múltiples vueltas de alambre; estos instrumentos fueron denominados "multiplicadores" debido a esta característica de diseño común. El término "galvanómetro", de uso común desde 1836, se deriva del apellido del investigador italiano Luigi Galvani, que descubrió que la corriente eléctrica podía hacer mover la pata de una rana. 
Originalmente, los galvanómetros se basaron en el campo magnético terrestre para proporcionar la fuerza para restablecer la aguja de la brújula; estos se denominaron galvanómetros "tangentes" y debían ser orientados, según el campo magnético terrestre, antes de su uso. Más tarde, los instrumentos del tipo "estático" usaron imanes en oposición, lo que los hizo independientes del campo magnético de la Tierra y podían funcionar en cualquier orientación. La forma más sensible, el galvanómetro de Thompson o de espejo, fue inventado por William Thomson (Lord Kelvin). En lugar de tener una aguja, utilizaba diminutos imanes unidos a un pequeño espejo ligero, suspendido por un hilo. Se basaba en la desviación de un haz de luz muy magnificado debido a corrientes pequeñas. Alternativamente, la deflexión de los imanes suspendidos se podía observar directamente a través de un microscopio. 

La capacidad de medir cuantitativamente el voltaje y la corriente en los galvanómetros permitió al físico Georg Ohm formular la Ley de Ohm, que establece que el voltaje a través de un conductor es directamente proporcional a la corriente que pasa a través de él. 

El primer galvanómetro de imán móvil tenía la desventaja de ser afectado por cualquier imán u objeto de hierro colocado en su cercanía, y la desviación de su aguja no era proporcionalmente lineal a la corriente. En 1882, Jacques-Arsène d'Arsonval desarrolló un dispositivo con un imán estático permanente y una bobina de alambre en movimiento, suspendida por resortes en espiral. El campo magnético concentrado y la delicada suspensión hacían de éste un instrumento sensible que podía ser montado en cualquier posición. En 1888, Edward Weston desarrolló una forma comercial de este instrumento, que se convirtió en un componente estándar en los equipos eléctricos. Este diseño es casi universalmente utilizado en medidores de veleta móvil actualmente.

Todos los tipos de galvanómetros contienen básicamente todos estos elementos:

Según el mecanismo interno, los galvanómetros pueden ser de imán móvil o de cuadro móvil.

En un galvanómetro de imán móvil la aguja indicadora está asociada a un imán que se encuentra situado en el interior de una bobina por la que circula la corriente que se trata de medir y que crea un campo magnético que, dependiendo del sentido de la misma, produce una atracción o repulsión del imán proporcional a la intensidad de dicha corriente.

En el galvanómetro de cuadro móvil o bobina móvil, el efecto es similar, difiriendo únicamente en que en este caso la aguja indicadora está asociada a una pequeña bobina, por la que circula la corriente a medir y que se encuentra en el seno del campo magnético producido por un imán fijo.

En el diagrama de la derecha está representado un galvanómetro de cuadro móvil en el que, en rojo, se aprecia la bobina o cuadro móvil y en verde el resorte que hace que la aguja indicadora vuelva a la posición de reposo una vez que cesa el paso de corriente.

En el caso de los galvanómetros térmicos, lo que se pone de manifiesto es el alargamiento producido al calentarse, por el "Efecto Joule", al paso de la corriente, un hilo muy delgado arrollado a un cilindro solidario con la aguja indicadora. Lógicamente el mayor o menor alargamiento es proporcional a la intensidad de la corriente.



</doc>
<doc id="5309" url="https://es.wikipedia.org/wiki?curid=5309" title="Varsovia">
Varsovia

Varsovia (en polaco: Warszawa, en Alfabeto Fonético Internacional: es la ciudad más grande de Polonia y la capital del país desde 1596. Es también la sede del presidente de la República, del Parlamento y del resto de las autoridades centrales. Cuenta con una población de 1 745 000 habitantes (en 2014), lo que la convierte en la séptima ciudad más poblada de la Unión Europea, y que posee unos 3 101 000 habitantes en su área metropolitana.

La historia de la ciudad se remonta a finales del siglo XIII. En ese momento era un pequeño pueblo de pescadores. En 1569, el rey Segismundo III transfirió su corte junto con la capital polaca de Cracovia a Varsovia. Una vez descrito como el «París del Norte», Varsovia fue considerada una de las ciudades más hermosas del mundo hasta la Segunda Guerra Mundial. Bombardeada al comienzo de la invasión alemana en 1939, la ciudad resistió. Las deportaciones de la población judía a los campos de concentración provocaron el Levantamiento del gueto de Varsovia en 1943 y la destrucción del gueto después de un mes de lucha. Una revuelta general en Varsovia entre agosto y octubre de 1944 llevó a una mayor devastación. Varsovia adquirió el nuevo título de «Ciudad Fénix» debido a su larga historia y reconstrucción completa después de la Segunda Guerra Mundial, que había dejado en ruinas más del 85 % de los edificios.

Varsovia es la sede de Frontex, la guardia de fronteras europea y la agencia costera. Varsovia es también una de las ciudades metropolitanas más dinámicas de Europa. En 2012, la Unidad de inteligencia de The Economist clasificó a Varsovia como la 32ª ciudad más habitable del mundo. En 2017, la ciudad ocupó el 4º lugar en la categoría "Amigable para los negocios" y 8a en "Capital humano y estilo de vida". También se ha clasificado como una de las ciudades más habitables de Europa Central y Oriental y es uno de los principales centros económico-financieros y culturales de Europa.

El Centro histórico de Varsovia fue declarado Patrimonio de la Humanidad por la Unesco en 1980. Es la parte más antigua de la ciudad y también es la principal atracción turística con la Columna de Segismundo, la Barbacana y el Castillo Real.

Varsovia es conocida por varios tratados como: el Pacto de Varsovia, la Convención de Varsovia y el Tratado de Varsovia.

El nombre Warszawa viene del posesivo del nombre Warsz ―es decir, Warszewa o Warszowa―.
Según la etimología popular, el nombre viene de un pescador pobre llamado Wars y su mujer, una sirena llamada Sawa.
Desde la segunda mitad del siglo XVII, el emblema de la ciudad es dicha sirena con una espada y un escudo en sus manos, y representa a la criatura que, según la leyenda, ordenó fundar la ciudad.

Hacia fines del siglo X e inicios del XI, existió un pequeño asentamiento comercial llamado Antiguo Bródno en los límites de Varsovia.
Esta pequeña población compitió comercialmente con Kamion y Jazdów, dos villas cercanas.
Se estima que Kamion fue fundada alrededor del año 1065. El primer registro histórico de Jazdów corresponde a julio de 1262, cuando la villa fue arrasada por los lituanos.
El duque Bolesław II trasladó entonces a la población de Jazdów dos millas al norte, donde se encontraba un pequeño pueblo pescador llamado Warszowa, y construyó un castillo.
Una capilla de madera fue construida cerca de la fortificación, pero fue quemada por los lituanos.
En el lugar se inició entonces la construcción de una iglesia de ladrillos, que se convertiría en la Catedral de San Juan. Alrededor del 1300, la ciudad recibió su acta municipal, la cual más tarde se perdió.
En 1339 la ciudad quedó bajo la jurisdicción de un bailío, y desde 1376 fue administrada por un ayuntamiento.
Para el final del siglo, la ciudad ya contaba con una doble muralla defensiva.

La mayor cantidad de información sobre la ciudad en sus inicios está contenida en el caso de la corte contra los Caballeros Teutónicos, que tuvo lugar en la Catedral de San Juan en 1339. Durante el siglo XIV la economía de Varsovia se basaba en las artesanías y el comercio. En 1350, se fundó la iglesia y el monasterio agustino. En 1411, la Princesa Anna Mazowiecka ordenó la construcción de la Iglesia de la Asunción de la Virgen María.

En 1413, Janusz I convirtió a Varsovia en la capital oficial del Ducado de Mazovia, reemplazando a Czersk. La población estimada en esta fecha era de 4500 habitantes.
De inmediato se inició la reconstrucción del Castillo, las murallas y el Ayuntamiento. Además, desde 1408, se había iniciado la expansión de la ciudad hacia el norte del sitio original, quedando dividida la ciudad en el pueblo nuevo y el pueblo viejo. El pueblo nuevo (Nowe Miasto) contaba con su propia acta municipal y sus propias leyes. El objetivo de esto era regular la presencia de nuevos asentamientos a las afueras de la ciudad, habitados principalmente por judíos.

Los ciudadanos, que en aquella época tenían la misma nacionalidad, estaban marcados por una gran disparidad en sus estatus financieros. Esta diferenciación y los contrastes del desarrollo social resultaron en 1525 en la primera revuelta de los pobres de Varsovia contra los ricos y la autoridad que los gobernaba.

En 1526, con la extinción de la dinastía ducal mazoica, el rey Segismundo I el Viejo entró a la ciudad, y después de que el gobierno local jurara lealtad al rey polaco, la ciudad, junto con su provincia, fue anexionada al Reino de Polonia. La incorparación al reino significó una aceleración en el desarrollo de la ciudad, que se convirtió en la principal del mismo.
En 1529, Varsovia sirvió de sede al Sejm por primera vez, aunque sin contar con carácter de sede permanente. Ya en el pasado, las reuniones de la Dieta mazoica se realizaban en la Iglesia de San Martín.

En 1526, con la llegada del rey polaco, se decretó la intolerancia para con los judíos, en una ordenanza municipal.
El primer registro de judíos en Varsovia data de 1414, y se conoce que en 1483 habían sido expulsados a las afueras de la ciudad, regresando poco después.
Tras el impacto de conocer el regreso de los judíos en Varsovia, Varsovia se levanta en armas, pero el decreto de 1526 tuvo poco efecto, y el gueto judío siguió creciendo en el centro de la ciudad. El rey tras el haberse enamorado de una joven judía en [(1570)] suspende el ataque a los judíos y les permite el paso. A pesar de los continuos roces entre los patricios, gremios y judíos, la rápida expansión de la economía permitió la convivencia de estos grupos.
Se estima que Varsovia inició el siglo XIV con 4500 habitantes, ascendido su población a 20 000 personas al finalizar el siglo.
En 1544, la ciudad vieja fue dañada por un incendio.

En 1571 se firmó la unión de Lublin, donde se declaraba la República de las Dos Naciones y se convenía el realizar siempre las reuniones del Sejm en Varsovia. Desde 1573, las elecciones reales se celebraron en la ciudad. Estas elecciones significaban el ingreso temporal en la ciudad de unos 50 000 o 100 000 nobles armados, y se incrementaba la criminalidad. Ese mismo año se concluyó el primer puente permanente sobre el río Vístula, que se estaba construyendo desde 1568 por órdenes de Segismundo II. Dicho puente fue destruido por una inundación en 1603.

En 1573, Varsovia le dio su nombre a la Confederación de Varsovia, estableciendo formalmente la libertad religiosa en la República. Durante el reinado de Segismundo II, el Castillo Real fue remodelado por Giovanni Battista di Quadro, y lo convirtió en un palacio renacentista.

En 1595, un incendio dañó el Castillo Real de Wawel, ubicado en Cracovia. El rey Segismundo III decidió trasladar la corte a Varsovia un año después, debido a su localización central entre las capitales de la Mancomunidad Polaco-Lituana, Cracovia y Vilna respectivamente, y su cercanía al puerto de Danzig, que siempre estaba bajo amenaza sueca.
El arquitecto real, Santa Gucci, inició entonces la remodelación del castillo, y al mismo tiempo se dio inicio a un nuevo período de prosperidad para la ciudad.
No obstante, Varsovia sufrió algunos embates de la naturaleza a inicios del siglo XVII. En 1602, un huracán destruyó la torre de la catedral, y en 1607, un incendio arrasó la plaza de la ciudad vieja.

En 1611, para conmemorar una victoria polaca en Smolensko y la captura del Zar Basilio IV, el rey y la corte se mudaron definitivamente para Varsovia, que se convirtió en la capital polaca. No obstante, los trabajos de remodelación no estaban terminados, y continuaron durante 20 años más. En los años siguientes, la ciudad se expandió por los suburbios. Varios distritos privados e independientes fueron establecidos, propiedad de la aristocracia y la burguesía, que eran gobernados bajo sus propias leyes. Estos fueron ocupados por artesanos y comerciantes. El desarrollo de la ciudad se detuvo con la llegada de una ola de invasiones suecas. Entre 1624 y 1625, y 1652 y 1653, Varsovia fue azotada por la plaga. Tres veces entre 1655 y 1658 la ciudad estuvo bajo sitio, y las tres veces fue tomada y cayó víctima del pillaje de las fuerzas suecas, brandenburguesas y transilvanas. Muchos libros, objetos históricos y obras de arte fueron robados por los invasores. Sin embargo, con la derrota de los turcos en la batalla de Kahlenberg en 1683, durante el reinado de Juan III Sobieski, la ciudad recuperó su antigua prosperidad.

En 1700, la Gran Guerra del Norte estalló y la ciudad fue tomada varias veces. El 12 de mayo de 1702, Varsovia fue capturada por las tropas suecas del rey Carlos XII. En 1704, después de la huida del rey polaco Augusto II, el Sejm nombró rey a Estanislao I Leszczynski, aliado de los suecos. Augusto forjó una alianza con Rusia en Narva, en el verano de 1704, y entró en guerra, intentando tomar la ciudad el 21 de julio de 1705, para impedir la coronación de Estanislao, sin éxito. El 21 de octubre de ese año, el ejército ruso-sajón sitió la ciudad. En 1707, en una paz virtual dado el tratado de paz entre Augusto II y Carlos XII, las tropas aliadas rusas entraron en Varsovia. Tras dos meses, las fuerzas rusas abandonaron la ciudad. Varias veces durante esa guerra la ciudad fue obligada a pagar altas contribuciones.

Al iniciarse la Guerra de Sucesión Polaca, la ciudad de nuevo empezó a retroceder económica y culturalmente.
En 1740, Estanislao Konarski estableció el Collegium Nobilium, una escuela para hijos de nobles, predecesora de la Universidad de Varsovia. Siete años después, Józef Andrzej Załuski y su hermano Andrzej Stanisław Załuski abrieron la primera biblioteca pública polaca. Desde 1742, una Comisión Urbana al mando del Mariscal Franciszek Bieliński inició la pavimentación de las calles varsovianas, así como la construcción de drenajes y puentes peatonales sobre quebradas. Sin embargo, grandes secciones de la ciudad continuaron creciendo fuera del control municipal.
Gracias a los esfuerzos del Alcalde de la ciudad vieja de Varsovia, Jan Dekert, en 1767 la ciudad fue administrada bajo un solo municipio, dividido en siete distritos. Además, en 1791, se ampliaron las libertades de los burgueses.

La segunda mitad del siglo XVIII y la primera del XIX marcaron una nueva y característica etapa en el desarrollo de la ciudad. Varsovia se convirtió en la principal ciudad practicante del capitalismo temprano. Durante el reinado de Estanislao II Poniatowski, la Ilustración llegó a Polonia, siendo Varsovia el centro cultural, económico, político y comercial de la nación.
En 1765 se abrió el Cuerpo de Cadetes, el primer colegio laico de la ciudad y poco después se instaló el Comité Nacional de Educación, el primer Ministerio de Educación del mundo. El teatro y las imprentas florecieron.
El crecimiento de la actividad política, el desarrollo de ideas progresistas, cambios políticos y económicos ejercieron en conjunto un impacto en la formación de la ciudad. En efecto, la aparición de la banca, las empresas manufactureras, y otras empresas creó una base económica firme, y se empezaron a realizar experimentos de planificación urbana.
La composición de la población de Varsovia se alteró durante la Ilustración. Se desarrollaron fábricas, el número de trabajadores se incrementó, la clase de los mercaderes, industriales y banqueros se expandió. Al mismo tiempo hubo una fuerte migración rural hacia las ciudades. En 1792, Varsovia tenía 115 000 habitantes, comparados con los 24 000 de 1754. Estos cambios trajeron consigo el desarrollo de la industria de la construcción. Nuevas residencias nobiliarias fueron asentadas, la clase media construyó sus propias casas las cuales demostraban una marcada diferenciación social. Las residencias de los representantes del estrato más rico, los grandes mercaderes y banqueros acompañados de los magnates. Un nuevo tipo de ciudad se desarrolló, con viviendas construidas respondiendo a las necesidades y gustos de la burguesía. Todas ellas estuvieron marcadas por un estilo anticuado.

Luego de la primera partición de Polonia, se proclamó en Varsovia una constitución parlamentaria el 3 de mayo de 1791, la primera constitución europea. Sin embargo, estos avances civiles polacos fueron oscurecidos por la llegada de una segunda partición polaca. En 1794, durante la revuelta de Kościuszko, los varsovianos apoyaron a Tadeusz Kościuszko atacando a las fuerzas rusas estacionadas en Varsovia, derrotándolas. Los prusianos acudieron a ayudar a Rusia y sitiaron Varsovia, pero la falta de artillería pesada y nuevos alzamientos polacos en la retaguardia condenaron al fracaso el sitio. La llegada del Conde Aleksandr Suvórov con refuerzos catalizó la derrota polaca, y el 4 de noviembre las tropas rusas asaltaron el suburbio varsoviano de Praga, masacrando a unas 10 000 personas.
El 5 de noviembre, los polacos en Varsovia se rindieron y esto conllevó a una Tercera Partición en 1795, la cual disolvió a Polonia como Estado y rebajó a Varsovia al nivel de un pueblo provincial, integrado al Reino de Prusia.

En 1806, los rusos fueron expulsados de Varsovia, que fue ocupada por el ejército de Napoleón Bonaparte. Siguiendo los términos del "Tratado de Tilsit", Varsovia se convirtió en la capital del Gran Ducado de Varsovia. No obstante, la caída del Primer Imperio Francés también ocasionó el fin del Ducado y de este breve período de reavivamiento político y cultural en la ciudad.

Tras el Congreso de Viena en 1815, Varsovia se convirtió en la sede del Congreso de Polonia, una monarquía constitucional en unión personal con el Imperio ruso. La Real Universidad de Varsovia fue establecida en 1816.

En 1830 se inició el Levantamiento de Noviembre. Aunque las fuerzas polacas cosecharon algunas victorias, la revuelta fue derrotada y Varsovia fue asaltada por Rusia. La autonomía que Varsovia con la que contaba en el Congreso de Polonia se esfumó, estableciéndose en su lugar un gobierno militar.
Sin embargo, esto no evitó que la ciudad continuase creciendo, e industrias textiles y metalúrgicas prosperaron.
Entre 1840 y 1848 se construyó el primer enlace ferroviario con Varsovia, comunicándola con Viena. Entre 1851 y 1855 fue construido el primer sistema de agua potable. Esta obra fue diseñada por Enrico Marconi, quien también creó una escultura de una sirena, uno de los iconos de la ciudad.

En octubre de 1860, bombas fétidas fueron arrojadas en el Gran Teatro de Varsovia, donde se encontraba presente el Zar Alejandro II y el emperador Francisco José I.
El 27 de febrero de 1861, una muchedumbre polaca que protestaba contra la administración zarista fue aplacada por las tropas rusas, muriendo cinco personas.
En 1863 se inició el Levantamiento de Enero, cuya derrota trajo como consecuencia inmediata la disolución del Congreso de Polonia y la anexión oficial al Imperio ruso. Se intensificó la rusificación de Polonia, quedando la administración pública y los colegios bajo control ruso.

Varsovia vivió un período de florecimiento a fines del siglo XIX bajo el alcalde Sokrates Starynkiewicz (entre 1875 y 1892), un general de origen ruso designado por el zar Alejandro III. Bajo su administración, Varsovia renovó su sistema de agua potable e inició la construcción del sistema de desagüe, que fueron diseñados y ejecutados por el ingeniero británico William Lindley y su hijo William Heerlein Lindley. Además, se levantaron nuevas iglesias y se restauraron las antiguas. En 1884, se construyó un nuevo cementerio en Bródno y se puso en marcha el sistema telefónico. En 1881 se inauguró un sistema de transporte público basado en caballos, y se proyectaron nuevas calles y veredas. La iluminación de gas también fue mejorada.

Hacia 1903, en Varsovia vivían . A pesar de 1905-1907 se incrementaron las actividades revolucionarias clandestinas, la censura de la prensa fue aligerada.
Se permitió de nuevo el establecimiento de escuelas polacas e instituciones culturales, y se dio inicio a una nueva etapa de reavivamiento cultural. En 1907 también apareció el tranvía eléctrico.

En 1915, durante la Primera Guerra Mundial, Varsovia fue ocupada por el II Imperio alemán. Con la derrota rusa y alemana, los socialdemócratas alemanes liberaron a Józef Piłsudski, quien llegó a la capital polaca el 10 de noviembre de 1918. Al día siguiente proclamó la Segunda República Polaca, y Varsovia recuperó su estatus de capital nacional. Durante el curso de la Guerra Polaco-Bolchevique de 1920, los soviéticos lograron hacer retroceder a los polacos hasta Varsovia; sin embargo, la Batalla de Varsovia, que tuvo lugar en las afueras del este de la ciudad, fue ganada por los polacos, y el Ejército Rojo fue expulsado del país, en un desenlace inesperado popularmente conocido como el milagro del Vístula.
Polonia detuvo a las fuerzas bolcheviques, y con este episodio la "expansión del socialismo" hacia el este de Europa fue pospuesta hasta el final de la Segunda Guerra Mundial.
La profunda enemistad entre rusos y polacos llevó a algunos episodios lamentables, como la demolición de la catedral ortodoxa de San Alejandro Nevsky, situada en lo que hoy es la gran explanada de Plac Piłsudskiego.

El Vizconde d'Abernon, miembro de la Misión interaliada a Polonia, declaró después:

Por su parte, Vladímir Lenin declaró dos meses después de la batalla:

En 1925, Varsovia alcanzó el millón de habitantes. A pesar de la Gran Depresión, nuevas industrias como la automotriz y la aeronáutica se desarrollaron. Varsovia continuó siendo un centro cultural, y desde 1927 se empezó a celebrar el Concurso Internacional de Pianistas Frédéric Chopin, y desde 1937, el Concurso Internacional de Violinistas Henryk Wieniawski.

El comienzo de la Segunda Guerra Mundial dio inicio a una de las experiencias más traumáticas de esta ciudad. Durante el asedio de Varsovia, unas 10 000 personas murieron y más de 50 000 fueron heridas.
Los alemanes realizaron un saqueo cultural de la ciudad, y muchos habitantes fueron enviados a campos de trabajo o campos de concentración.
Los invasores establecieron también a la población judía en un gueto, conocido como el gueto de Varsovia. Miles murieron de hambre, enfermedades y hacinamiento antes de empezar a ser enviados a campos de la muerte, donde destaca el de Treblinka, desde finales de 1941.
Cuando se conoció la noticia del destino final de los judíos polacos, se inició el levantamiento del gueto de Varsovia, que duró casi un mes. Tropas alemanes al mando de Jürgen Stroop pusieron fin a la resistencia judía, y luego de destruir la Gran Sinagoga, símbolo de la Varsovia judía, reanudaron las deportaciones a Treblinka. El Dr. Ludwig Fischer sería nombrado Gobernador del Distrito de Varsovia, el cual llevó a cabo una administración completamente compuesta por alemanes.
Los judíos se vieron totalmente aislados en Varsovia para poder hacer frente al régimen nazi ya que esta masacre se ocultó al resto del mundo. Los historiadores relatan distintos casos de complicidad, como cuando en 1944 los miembros de la Cruz Roja Alemana guiaron a visitantes de la Cruz Roja Internacional a través del campo de concentración de Theresienstadt, por un "paseo" predeterminado que escondía los horrores del exterminio.

La ciudad sufriría aún mayor destrucción en el año siguiente. Coincidiendo con el acercamiento del Ejército Rojo a Varsovia, el Ejército clandestino polaco inició un nuevo alzamiento contra los alemanes.
Se estima que entre 150 000 y 180 000 personas murieron durante el conflicto. En total, se cree que entre 600 000 y 800 000 varsovianos murieron en la Segunda Guerra Mundial.
Un 30 % de la ciudad fue destruida en la lucha, pero tras finalizar la guerra casi toda la ciudad sería destruida. Anteriormente, tanto Hitler como Himmler habían expresado su deseo de destruir la capital polaca, siendo el sistema ferroviario la única estructura que sobrevivió, debido a que fue usado para el transporte de la tropas alemanas. Al finalizar la ocupación alemana el Castillo Real fue destruido, las principales bibliotecas incendiadas, junto con museos, iglesias, palacios y otros edificios culturales. Varsovia, que una vez había sido conocida como "la París del Norte", perdió cerca del 80 % de sus edificios.

En enero de 1945, los soviéticos entraron en Varsovia, y el 1 de febrero fue proclamada la República Popular Polaca. De inmediato se creó una oficina de reconstrucción urbana. Grandes proyectos de viviendas prefabricadas fueron erigidos en Varsovia para hacer frente a la escasez inmobiliaria, junto con otros edificios típicos de una ciudad del Bloque del Este, como el Palacio de la Cultura y la Ciencia.
La ciudad reasumió su rol como capital de Polonia y el centro de la política y la vida económica del país. Muchas de las calles, edificios e iglesias históricas fueron restauradas a su forma original. En 1989, el Barrio histórico de Varsovia fue inscrito en la lista de Patrimonio de la Humanidad de la UNESCO.
El 14 de mayo de 1955 se firmó el Pacto de Varsovia que serviría como organización militar de la Unión Soviética y sus países satélites, hasta su desaparición tras la caída del Muro de Berlín, en 1989.

Las visitas del papa Juan Pablo II a su país natal en 1979 y en 1983 brindaron apoyo al incipiente Movimiento Solidaridad, e impulsó el crecimiento del fervor anticomunista.
En 1979, menos de un año después de convertirse en papa, Juan Pablo II celebró una misa en la Plaza de la Victoria (o plaza Piłsudski) en Varsovia y concluyó su sermón: «Dejad que el Espíritu descienda y renueve la cara de esta tierra».
Estas palabras tuvieron un gran impacto en los ciudadanos polacos, que lo entendieron como un incentivo para los cambios democráticos.

En 1995, se inauguró el Metro de Varsovia.
Con la entrada de Polonia en la Unión Europea en 2004, Varsovia experimentó el mayor crecimiento económico en su historia.

Está situada en el centro del país, en la región del voivodato de Mazovia (es también su capital), a las orillas del río Wisła (Vístula), a unos 100 metros sobre el nivel del mar, a 350 kilómetros de los montes Cárpatos y a 523 kilómetros de Berlín.
Varsovia se encuentra en dos formas geomorfológicas principales: la meseta morrena llana y el valle del Vístula con su asimétrico diseño de bancales diferentes.
El río Vístula es el eje de Varsovia, que divide la ciudad en dos partes, izquierda y derecha. La parte izquierda se encuentra en la meseta de morrena (entre 10-25 metros sobre el nivel del Vístula) y sobre los bancales del mismo río (máx. 6.5 m por encima del nivel del Vístula). El elemento importante del relieve en esta parte de Varsovia es el borde de la meseta morrena, conocida como la Escarpa de Varsovia. Esta es de 20-25 m de altura en el casco antiguo y en el distrito central y cerca de 10 m en el norte y el sur de Varsovia. Pasa por la ciudad y juega un papel importante como atracción de Varsovia.

La llana meseta de la morrena tiene unos pocos estanques naturales y artificiales, y también grupos de pozos de barro. El diseño de los bancales del Vístula son asimétricos. El lado izquierdo consiste principalmente en dos niveles: el más alto un antiguo bancal inundable y el más bajo en el bancal llano inundable. El bancal inundable contemporáneo tiene aún valles y depresiones visibles con sistemas de abastecimiento de agua procedente de antiguos cauces del Vístula. Se componen de todavía bastantes arroyos y lagos naturales, así como el diseño de las zanjas de drenaje. El lado derecho de Varsovia tiene diferentes modelos de formas geomorfológicas.

Los inviernos son fríos y los veranos son frescos. La temperatura promedio es de –2.4 °C en enero y 19.1 °C en julio. A menudo pueden llegar a temperaturas de 30 °C en el verano. El promedio anual de lluvias es de 500 milímetros de precipitación y el mes más lluvioso es julio. La primavera y el otoño suelen ser temporadas agradables.

Los espacios verdes suponen una cuarta parte de la superficie de Varsovia, incluyendo una amplia gama de estructuras afines; desde pequeños parques en los vecindarios, espacios verdes a lo largo de las calles y en los patios; hasta grandes parques históricos, áreas de conservación de la naturaleza y bosques urbanos en la franja de la ciudad.

Hay 82 parques en la ciudad, que cubren en total el 8 % de su área.
Los más antiguos, forman parte de palacios muy representativos de la ciudad como; el jardín del palacio Krasiński, el parque Łazienki, el parque del palacio Wilanów, el parque del palacio Królikarnia y el Jardín Sajón (poseedor de un área de 15.5 hectáreas) que solía ser un jardín real, en el cual hay más de cien especies diferentes de árboles y cuyas avenidas son un lugar para pasear y relajarse.

El parque Łazienki data de la década de 1780. Dentro de su área central pueden verse aún viejos árboles que datan de aquel período: ginkgo, nogales negros, avellanos turcos, etc. Complementado por sus bancos, alfombras de flores, un charco con patos y un patio para niños, el parque Łazienki es un popular destino de los varsovianos a la hora de dar un paseo. El parque cubre un área de 76 hectáreas. El carácter único y la historia del parque está reflejada en su arquitectura: pabellones, esculturas, puentes, cascadas, charcas, y en su vegetación: especies vernáculas y foráneas de árboles y arbustos. Lo que hace a este parque diferente a otros espacios verdes de Varsovia es la presencia de pavos reales y faisanes, que pueden ser vistos caminando libremente, y de carpas reales en el estanque.

El parque del palacio Wilanów data de la segunda mitad del siglo XVII. Este cubre un área de 43 hectáreas. Su área central de estilo francés, corresponde al antiguo estilo barroco de las formas del palacio. La sección oriental del parque, la más cercana al palacio, es un jardín de dos niveles con una terraza enfrentada al estanque.

El parque que rodea al palacio Królikarnia, está situado en la vieja escarpa del río Vístula, tiene veredas que corren sobre unos niveles profundos en los barrancos a ambos lados del palacio.
Otros espacios verdes en la ciudad incluyen el Jardín Botánico y el jardín de la Biblioteca Universitaria, los cuales poseen extensas colecciones botánicas de plantas extrañas, tanto endémicas como foráneas, mientras que un invernadero en el New Orangerie presenta plantas subtropicales de todas las partes del mundo.
La flora de la ciudad se compone de una gran variedad de especies. Su riqueza se debe en gran parte, a la localización de Varsovia en los bordes de grandes regiones florales que comprenden sustanciales proporciones de áreas acotadas a la actividad humana (bosques naturales, pantanos a la laguna del Vístula) así como tierras arables, prados y bosques. El bosque de Bielany, localizado en los límites de Varsovia, es la parte restante del primitivo bosque de Masovia. Su reserva natural se conecta con el bosque de Kampinos.
Es hogar de una rica fauna y flora. Dentro del bosque hay tres senderos para bicicletas o recorrer a pie.

A 15 km de Varsovia, el ambiente del río Vístula cambia sorprendentemente el entorno y constituye un ecosistema perfectamente conservado, con un hábitat de animales entre los que figuran, entre otros, la nutria, el castor, y cientos de especies de pájaros.

El zoológico de Varsovia cubre un área de 40 hectáreas, con alrededor de 5000 animales que representan a unas 500 especies. Aunque fue creado oficialmente en 1929, su origen está en los cotos privados del siglo XVII a menudo abiertos al público.

Varsovia es un "powiat" (‘condado’), y está dividido en un total de 18 distritos urbanos, conocidos en polaco como "dzielnica" (mapa), cada cual con su propio cuerpo administrativo. Cada uno de estos distritos esta a su vez formado por distintos barrios que carecen de estatus legal o administrativo. Varsovia cuenta con dos barrios históricos que componen el corazón de esta ciudad. Estos son conocidos como el Casco antiguo (Stare Miasto) y la Ciudad Nueva (Nowe Miasto), ambos en el distrito de Śródmieście.

Históricamente, Varsovia ha sido el destino de la emigración interna y extranjera, más concretamente de toda Europa. Durante unos 300 años, fue conocida como "La antigua París" o "La segunda París". Polonia tenía un 20 % de la población nacida en el extranjero o judía. Demográficamente Varsovia era la ciudad más cosmopolita del país y antes de la guerra la población judía alcanzaba la cifra de 350 000, lo cual constituía alrededor del 30 % de la población que albergaba la ciudad.

En 2006 su población se estimaba en 1.8 millones de habitantes, y unos 3.101 millones de residentes en el área metropolitana.

Varsovia, especialmente su centro (Śródmieście), es hogar no solo de numerosas instituciones y agencias gubernamentales, sino también de muchas compañías nacionales e internacionales. En 2006, 304 016 compañías estaban registradas en la ciudad. La participación financiera de inversores extranjeros en la ciudad estaba estimada en 2002 en cerca de 650 millones de euros. Varsovia produce el 12 % del ingreso nacional el cual, per cápita, está estimado en alrededor del 290 % del promedio polaco. El GDP (PPP) nominal per cápita en Varsovia era en 2005 de 25 500 euros. Este fue uno de los más veloces crecimientos económicos, con un crecimiento del 6.5 % en 2007 y 6.1 % en el primer trimestre de 2008. Al mismo tiempo la tasa de desempleo es mínima.

La tributación de la misma ciudad produce alrededor de 8 741 millones de złotys polacos en concepto de impuestos y ganancias directas del gobierno.

Varsovia, junto con Fráncfort del Meno, Londres, Madrid, Barcelona, París, Moscú, Bruselas y Milán es una de las ciudades con edificios más altos de Europa. Once de los , de los cuales nueve son edificios de oficinas, están localizados en Varsovia. La estructura más alta es el Palacio de la Cultura y la Ciencia, que es a su vez el séptimo edificio más alto de la Unión Europea.

Los rascacielos del área financiera de Varsovia se encuentran numerados de mayor a menor altura en la .

El primer mercado de valores de Varsovia fue establecida en 1817 y continuó en funcionamiento hasta la Segunda Guerra Mundial. Fue restablecido en abril de 1991, sucediendo al fin del gobierno comunista de la posguerra y la reintroducción de una economía de libre mercado. Hoy, la Bolsa de valores es, de acuerdo a muchos indicadores, el mercado más grande de la región. Es actualmente el mayor mercado de valores de la ciudad, con más de 300 compañías adheridas. Desde 1991 hasta 2000, esta institución estaba localizada, irónicamente, en lo que antes habían sido los cuarteles generales del Partido Comunista Polaco, el Partido de los Trabajadores Unidos Polacos. La capitalización obtenida fue de 440.92 millones de dólares (el 28 de diciembre de 2007). La Bolsa de Valores ofrece dinero en efectivo y productos derivados bajo un mismo techo. La ciudad es actualmente considerada como uno de los puntos de negocios más atractivos en Europa.

Durante la reconstrucción de Varsovia tras la Segunda Guerra Mundial, las autoridades comunistas decidieron que la ciudad se convertiría en el mayor centro industrial del país. Reputadas fábricas de gran tamaño fueron construidas en la ciudad o en sus límites inmediatos. La mayor de ellas era la Huta Warszawa, trabajos siderúrgicos y dos fábricas de autos. Hoy, la Arcelor Warszawa Molino de Acero (formalmente Huta Warszawa) es la única gran fábrica que queda. La empresa automotriz Fabryka Samochodów Osobowych, produce la mayoría de los automóviles de exportación. El número de empresas públicas continúa decreciendo mientras aumenta el número de compañías operadas con capitales extranjeros. Los mayores inversores foráneos son Daewoo, Coca-Cola Amatil y Metro AG. Varsovia tiene la mayor concentración de industrias dedicadas a la electrónica y la alta tecnología en Polonia y el crecimiento del mercado alimentario promueve perfectamente el desarrollo de la industria procesadora de alimentos.

Como capital de Polonia Varsovia es centro político del país. Todas las agencias estatales están localizadas aquí, incluidos el Sejm (Parlamento de la República de Polonia), o la Oficina del presidente y la Corte Suprema. La ciudad y su área metropolitana están representadas en el Sejm por 31 diputados de 460. Es la sede del presidente de Polonia y la legislación nacional.

Antes de la firma del "Acta de Varsovia" "(Ustawa warszawska)", el 27 de octubre de 2002, Varsovia era una asociación municipal de 11 distritos, el distrito Centrum era la cabecera, estaba dividido en 7 subdistritos y rodeado por otros 10 distritos. Los distritos de Varsovia eran independientes en una gran medida: administraban sus propios presupuestos, confeccionaban sus propias cuentas de sus políticas de inversión y tenían consejos y juntas propias. La ciudad en su totalidad era gobernada por un alcalde, y la administración de la ciudad entera se ocupaba de tareas que traspasaban los límites municipales, tales como las comunicaciones, los caminos, el agua, el sistema de alcantarillado o la promoción de la ciudad. Aparte de los distritos y la asociación municipal, estaba el condado de Varsovia, que gobernaba sobre las escuelas y las instituciones culturales, era responsable de la sanidad, construcción e inspección comercial.

La situación cambió con la firma del Acta de Varsovia, referente a la estructura de la ciudad. Los distritos antiguos y los del distrito Centrum se transformaron en unidades auxiliares de distritos de la ciudad de Varsovia.

El sistema electoral cambió también, transformándose en elección directa. El primer alcalde fue Lech Kaczyński, profesor de derecho, antiguo ministro de justicia y expresidente de la Oficina Central de Auditorías (NIK). En su campaña electoral, Kaczyński anunció transformaciones más excesivas en la gestión de Varsovia que otros candidatos.

La autoridad legislativa en la flamante estructura fue conferida al Ayuntamiento de Varsovia (llamado "Rada Miasta", Consejo de la Ciudad), reducido a 60 concejales. Los miembros del consejo son elegidos de manera directa cada cuatro años. Como la mayoría de las cámaras legislativas, la Rada Miasta se divide en comités encargados de diversas funciones de gobierno. Los proyectos de ley se aprueban por mayoría simple y se envían al alcalde (denominado presidente de Varsovia) que será el encargado de promulgarlos. Si el presidente veta un proyecto de ley, el Consejo tiene 30 días para superar ese veto por una mayoría del 66 %.

Cada uno de los 18 distritos tiene su propio consejo ("Rada dzielnicy"). Sus funciones son ayudar al presidente y al Consejo de la Ciudad, además de supervisar distintas compañías municipales, patrimonio y escuelas. El dirigente de cada consejo de distrito recibe el título de burgomaestre ("burmistrz") y es elegido por los miembros del mismo de entre las candidaturas propuestas por el presidente de Varsovia.

Al presidente de Varsovia y a la Oficina de la Ciudad de Varsovia le fueron confiados las tareas concernientes a la ciudad en general y la coordinación del trabajo de los distritos. Como antes de la reforma se mantuvieron el papel de los distritos de servicio a la población en forma local como en los caminos locales, las escuelas, los jardines de infantes, la expedición de licencias de conducir, el registro de residentes, etc. Sin embargo sus poderes derivan ahora del Ayuntamiento y del presidente de Varsovia, y sus presupuestos y políticas financieras deben estar acordes a los del resto de la ciudad.

Los palacios, iglesias y mansiones de Varsovia presentan una gran riqueza de color y de detalles arquitectónicos. Los edificios son representativos de casi todos los estilos y períodos de la arquitectura europea. La ciudad tiene maravillosos ejemplos de la arquitectura gótica, renacentista, barroca y neoclásica, todos los cuales están localizados a pocos pasos del centro de la ciudad. La arquitectura gótica está representada en las majestuosas iglesias, pero también en las fortificaciones. Las construcciones más significativas son la catedral de San Juan Bautista (del siglo XIV), la iglesia de Santa María (1411), actual catedral; una casa en la ciudad de la familia Burbach (siglo XIV), la torre de la pólvora y el Castillo real Curia Maior (1407-1410). Los ejemplos más notables del estilo renacentista son la casa Barczyko (1562), un edificio llamado "El Negro" (comienzos del siglo XVII) y los conventillos Salwator (1632). Los ejemplos más interesantes de la arquitectura manierista son el Castillo real y la iglesia jesuita (1609-1626) en el barrio viejo. A su vez las primeras estructuras de estilo barroco son la iglesia de San Jacinto (1603-1629) y la columna Zygmunt (en honor al rey Segismundo III), de 1644.

La construcción se centró, sobre todo, en numerosos palacios de nobles e iglesias durante las últimas décadas del siglo XVII. Algunos de los mejores ejemplos de esta corriente son el palacio Krasiński (1677-1683), el palacio Wilanów (1677-1696) y la iglesia de San Casimiro (1677-1692). Los ejemplos más impresionantes de la arquitectura rococó son el palacio Czapski (1713-1718), el Palacio bajo los Cuatro Vientos (1730) y la Iglesia Visitacionista (fachada 1728-1761). El estilo neoclásico en Varsovia puede ser descrito por su simplicidad y por sus formas geométricas combinadas con una gran inspiración en la cultura romana. Algunos de los mejores ejemplos del neoclásico son el palacio Łazienki o el Palacio sobre el Agua (reconstruido entre 1775 y 1795) ambos ubicados en el parque Łazienki, Królikarnia (1782-1786), la Iglesia Carmelista (fachada 1761-1783) y la evangelista Iglesia de la Santísima Trinidad (1777-1782). El crecimiento económico durante los primeros años del Congreso de Polonia trajo consigo un efímero auge de la arquitectura. El revive neoclásico afectó a todos los aspectos de la arquitectura, los más notables son el Gran Teatro (1825-1833) y las construcciones sobre la Plaza de la Banca (1825-1828). De esta época es también la Ciudadela de Varsovia, una fortaleza construida por orden del zar Nicolás I de Rusia.

Ejemplos excepcionales de la arquitectura burguesa de los últimos períodos (como el mencionado palacio Kronenberg y el edificio de la Compañía de seguros Rosja) no fueron restaurados por la autoridad burguesa después de la guerra. Algunos, en cambio, fueron reconstruidos por un estilo realista de corte socialista (como la Orquesta Filarmónica de Varsovia, edificio originalmente inspirado en la Ópera Garnier de París). A pesar de esto la Universidad de Politécnica de Varsovia, construida entre 1899 y 1902 es lo más interesante de la arquitectura de finales del siglo XIX. Las autoridades del gobierno municipal de Varsovia, reconstruyeron el palacio Brühl y existe una iniciativa para reconstruir el palacio Sajón, las más distintivas edificaciones de la preguerra en Varsovia.

Algunos notables ejemplos de la arquitectura contemporánea son el Estadio del 10° aniversario que solía ser el mercado al aire libre más grande de Europa y la Plaza de la Constitución con su monumental arquitectura de realismo socialista. El más polémico de esta época es el Palacio de la Cultura y la Ciencia (1952-1955), un enorme edificio de arquitectura realista rusa, el cual constituye un legado del comunismo ruso al pueblo de Polonia, situación que incomoda a muchos ciudadanos y al propio ministro de relaciones exteriores, quien se pronunció a favor de la demolición de este magno edificio. Fue un regalo directo de Stalin para el pueblo polaco en 1955 con una altura de 230.7 metros, siendo el segundo rascacielos más alto de Varsovia (en 2012 será el cuarto) y uno de los más altos de Europa. Su estilo imita a la universidad rusa y alberga el centro de cultura y arte de la ciudad. En contraposición a la polémica, en 2007 fue declarado Patrimonio Nacional y símbolo de la ciudad.

Símbolos de la arquitectura moderna en Varsovia son el Edificio Metropolitano de Oficinas en la Plaza Piłsudski, construido por el reconocido arquitecto Norman Foster; y la Biblioteca Universitaria de Varsovia (BUW) construida por Marek Budzyński y Zbigniew Badowski, característica por un jardín en su techo y una vista del río Vístula. La oficina Rondo 1, obra de Skidmore, Owings y Merrill; y las Terrazas Doradas, consistentes en siete domos superpuestos y un centro comercial, son también ejemplos de este estilo.

Entre los sitios destacables para ver en Varsovia:

Varsovia cuenta con gran cantidad de museos y galerías de arte, siendo el más destacado el Museo Nacional, el Museo de la Aviación Polaca, la galería de arte Zachęta, el Centro de Arte Contemporáneo, Museo del Ejército Polaco. El mayor de ellos, el Museo Nacional de Varsovia tienen numerosas sedes, ubicadas en distintas partes de Varsovia, en particular en el Castillo Real y el palacio Wilanów. El Museo del Alzamiento de Varsovia se inauguró en 2004.

Varsovia acoge algunos de las más importantes instituciones de educación superior en Polonia. Es hogar de las 4 mayores universidades polacas y de 62 escuelas de educación superior más pequeñas. El número total de estudiantes en todos los grados de educación en Varsovia es de casi 500 000 (o el 29.2 % de la población en 2002). El número de estudiantes universitarios es de, aproximadamente, 280 000. La mayoría de las más reputadas universidades son públicas, pero en los años recientes ha habido también un aumento en el número de universidades privadas.

La Universidad de Varsovia fue fundada en 1816, cuando las particiones de Polonia separaron a Varsovia del más antiguo y más influyente centro académico de Polonia, en Cracovia. La Universidad Tecnológica de Varsovia es la segunda escuela superior de tecnología en el país, y uno de las mayores de Europa Central y Occidental, empleando 2000 profesores. Otras instituciones de educación superior incluyen la Universidad de Medicina de Varsovia, la mayor escuela de medicina en Polonia y una de las más prestigiosas, la Universidad de la Defensa Nacional, la mayor institución académica militar de Polonia, la academia musical Fryderyk Chopin, la mayor y más antigua escuela de música de Polonia, y una de las mayores de Europa, la escuela de Ciencias Económicas de Varsovia, la más antigua y más renovada escuela de economía en el país, y finalmente la Universidad de Ciencias de la Vida, la mayor universidad agrícola fundada en 1818

Varsovia posee numerosas bibliotecas, muchas de las cuales poseen vastas colecciones de documentos históricos. La biblioteca más importante, en términos de colecciones de documentos históricos, es la Biblioteca Nacional de Varsovia. Esta alberga 8.2 millones de volúmenes en su colección, formada en 1928 se ve a sí misma como una sucesora de la Biblioteca Załuski, la mayor de Polonia y una de las primeras y mayores bibliotecas del mundo.

Otra biblioteca importante es la biblioteca universitaria, fundada en 1816 es hogar de cerca de 2 millones de libros. El edificio fue diseñado por los arquitectos Marek Budzyński y Zbigniew Badowski y abierto el 15 de diciembre de 1999 Está rodeado de espacios verdes. Su jardín fue diseñado por Irena Bajersjka e inaugurado el 12 de junio de 2002. Es uno de los mayores y más bellos jardines de cubierta en Europa y un área de más de 10 000 m², con plantas cubriendo 5111 m² de su superficie. Como jardín de la universidad, este abierto al público todos los días.

Varsovia fue candidata para ser la Capital Europea de la Cultura en el 2016

La solicitud titulada "Varsovia - Nuevas Energías para Europa" tuvo que competir contra otras cuatro ciudades candidatas de Polonia, país elegido junto a España para que una de sus ciudades fuera representante en 2016 de la capitalidad cultural europea, la cual fue adjudicada finalmente a las ciudades de Breslavia en Polonia y San Sebastián en España.

Con el título de Capital Europea de la Cultura 2016, Varsovia tuvo la oportunidad de convertirse en una ciudad conocida por la transición de "la miseria urbana" en la era del "renacimiento urbano". Varsovia quería renacer como una metrópoli verdaderamente contemporánea, que encuentra soluciones innovadoras a sus problemas y que desarrolla novedosas soluciones para la regeneración de la ciudad y el arte en los espacios públicos.
Los temas principales de la candidatura de Varsovia fueron:
Vístula: río de posibilidades.
Ciudad de Talentos.
Varsovia en construcción.

Desde 1833 hasta el estallido de la Segunda Guerra Mundial, la "Plac Teatralny" (Plaza del Teatro) fue el foco cultural del país y el hogar de varios teatros. El edificio principal albergó el Gran Teatro de 1833 a 1834, el Teatro Rozmaitości de 1836 a 1924 y luego el Teatro Nacional, el Teatro Reduta de 1919 a 1924, y de 1928 a 1939 el Teatro Nowy, el cual organizaba producciones de teatro poético contemporáneo, incluyendo aquellas dirigidas por Leon Schiller.

Cerca de allí, en el Ogród Saski (Jardín Sajon), el Teatro de Verano estuvo en operación desde 1870 hasta 1939, y durante el período de entreguerras. El teatro comprendía también Momus, el más importe de los cabarets literarios de Varsovia, y el teatro musical Melodram, de Leon Schiller. El Teatro Wojciech Bogusławski (1922-1926) fue el mejor ejemplo del Teatro monumental Polaco. Desde mediados de los años 1930, el edificio del Gran Teatro acogió al Instituto Estatal de Arte Dramático, la primera academia de arte administrado por el Estado de Polonia, con un Departamento de Actuación y un Departamento de Dirección de Etapas.

La Plac Teatralny y sus alrededores fueron la sede de numerosos festivales, celebraciones de feriados estatales, bailes de carnaval y conciertos.

Varsovia es el hogar de 30 de los principales teatros del país, que se expanden por toda la ciudad, incluidos el Teatro Nacional (fundado en 1765) y el Gran Teatro, establecido en 1778.

Varsovia atrae a su vez a numerosos directores y productores jóvenes y vanguardistas que suman a la cultura teatral de la ciudad. Su producción puede ser vista sobre todo en los teatros más pequeños y Casas de la Cultura (Domy Kultury), sobre todo fuera del centro. Varsovia acoge a las Reuniones Teatrales Internacionales.

Gracias a sus numerosos sitios musicales, incluyendo el Teatro Wielki o Gran Teatro, sede de la Ópera Nacional Polaca, la Cámara de la ópera, el Salón Filarmónico nacional y el Teatro Nacional, así como también los teatros musicales de Roma y Buffo y el Salón de Congresos en el Palacio de la Cultura y la Ciencia, Varsovia es sede de numerosos eventos y festivales. Algunos de los que poseen más prestigio son: La Competición Internacional de Piano «Frédéric Chopin», el Festival Internacional de Música Contemporánea Otoño de Varsovia, el Jazz Jamboree, las Jornadas estivales de Jazz de Varsovia, la competición internacional vocal Stanisław Moniuszko, y el festival de música antigua.

En Varsovia tiene su sede y su temporada de conciertos la Orquesta Filarmónica Nacional de Varsovia.

Los eventos de Varsovia durante la guerra dejaron profundos hoyos en las colecciones históricas de la ciudad.
Bien a pesar de que una considerable cantidad de tesoros se había amontonado en la ciudad sin imaginar los desastres que ocurrirían en 1939, es verdad también que un gran número de colecciones de palacios y museos en la periferia del país fueron enviados a Varsovia en la época en la que como capital era considerada un espacio más salvo para estos en lugar de algunos remotos castillos en los límites de Polonia.
Así pues, las pérdidas fueron muy importantes.

No obstante, Varsovia cuenta aún con maravillosos museos. Como interesantes ejemplos de exposiciones los más notables son: el primer Museo de Pósteres del mundo poseyendo una de las mayores colecciones de arte póster del mundo, el Museo de Caza y Equitación y el Museo Ferroviario. De entre los 60 museos de Varsovia, uno de los más prestigiosos es el Museo Nacional, con una amplia colección de obras cuyo origen varía en tiempo desde la antigüedad hasta la época actual, así como una de las mejores colecciones de pinturas en el país, y el Museo del Ejército Polaco, cuya obra retrata la historia de las armas.

Las colecciones del palacio Łazienki y el palacio Wilanów (ambos edificios en buena forma a pesar de la guerra) son excelentes, así como las del Castillo Real. El Palacio de Natolin, la antigua residencia rural del duque de Czartoryski posee un parque interior accesible a los turistas.

Poseedor de la colección de arte privada más grande de Polonia, el Museo de Colecciones Carroll Porczyński disponen de trabajos de variados artistas tales como Peter Paul Rubens, Francisco Goya, John Constable, Pierre-Auguste Renoir, Vincent van Gogh y Salvador Dalí, entre otras

Un bello homenaje a la caída de Varsovia y la historia de Polonia puede ser encontrado en el Museo del Alzamiento de Varsovia y en el Museo de la masacre de Katyń, los cuales preservan la memoria del crimen. La prisión de Pawiak, que durante la guerra fue un centro de detención de las SS de siniestra fama, funciona actualmente como un centro memorial de las víctimas del nazismo.
El Museo de la Independencia acoge una sentimental y patriótica parafernalia relacionada con estas fatídicas épocas, así como también algunas invaluables colecciones de arte. Datado de 1936, el Museo Histórico de Varsovia contiene 60 salas que acogen una exhibición permanente de la historia de Varsovia desde sus orígenes hasta la actualidad.

El Real Castillo Ujazdów, del siglo XVII es sede del Centro de Arte Contemporáneo, con exhibiciones tanto permanentes como temporales, conciertos, shows y talleres creativos. La Galería nacional de arte Zachęta es el sitio de exhibiciones más antiguo de Varsovia, con una estrecha tradición que data de mitad del siglo XIX. La galería organiza muestras de arte moderno por artistas tanto polacos como internacionales y promueve el arte de muchas otras formas.

La ciudad posee a su vez algunos maravillosas curiosidades tales como el Museo de la Caricatura (el único de su tipo en el mundo) y un Museo de la Motorización, que posee, entre su fuertemente variadas propuesta, los autos clásicos de 1930 utilizados por Elvis Presley y Marilyn Monroe.

Varsovia es el centro mediático de Polonia. Telewizja Polska, la mayor emisora pública de Polonia tiene su sede central en la calle Woronicza en Varsovia. Hay a su vez numerosas estaciones de radio y T.V, tanto locales como nacionales, localizada en Varsovia, como la sucursal TVN de Polonia, Polsat, TV4 Polonia, TV Puls, Canal+ Polonia, Cyfra+ y MTV Polonia.

Desde mayo de 1661, el primer periódico polaco, el "Merkuriusz Polski Ordynaryjny" (‘Mercurio ordinario de Polonia’), fue impreso en Varsovia. La ciudad es también la capital de la impresión de toda Polonia con una amplia variedad de revistas nacionales y extranjeras que expresan distintas opiniones, siendo los periódicos nacionales son muy competitivos. "Rzeczpospolita", "Gazeta Wyborcza", "Dziennik Polska-Europa-Świat" son los grandes diarios de Polonia a nivel nacional, y tienen sus oficinas centrales en Varsovia.

Varsovia posee también una creciente industria de películas y televisión. La ciudad alberga numerosas compañías y estudios. Entre las compañías cinematográficas figuran TOR, Czołówka, Zebra y Kadr, la cual se halla detrás de varias producciones internacionales de películas.

En un futuro cercano la nueva Film City en Nowe Miasto, localizada a 80 km de Varsovia, se convertirá en el centro de la producción polaca de filmes y la coproducción internacional.
Será también el estudio de alta tecnología más grande de Europa. Los primeros proyectos filmados aquí serán dos filmes sobre la Revuelta de Varsovia.

Desde la Segunda Guerra Mundial, Varsovia ha sido el centro de producción de films más importante de Polonia. La ciudad también ha aparecido en numerosas películas, tanto polacas como extranjeras, por ejemplo: "Kanał" y "Korczak", por Andrzej Wajda, "El Decálogo" por Krzysztof Kieślowski, como también la ganadora del Oscar en 2002 "El Pianista", obra de Roman Polański.

El 9 de abril de 2008, la presidenta de Varsovia Hanna Gronkiewicz-Waltz obtuvo del alcalde de Stuttgart, Wolfgang Schuster, una placa conmemorativa en virtud de que Polonia fuese la capital europea del deporte en 2008.

El Estadio Nacional, un estadio de fútbol, fue construido en Varsovia en el sitio del antiguo y dilapidado Estadio del 10° aniversario.
El estadio nacional fue la sede del partido de apertura, el resto de los partidos del grupo 2, un partido de los cuartos de finales y un partido de semifinal de la UEFA Euro 2012, que organizaron en conjunto Polonia y Ucrania.

Existen muchos otros centros deportivos en la ciudad. Muchos de los más vistos son las piscinas de nataciones y salones deportivos, muchos de ellos construidos por la municipalidad en los últimos años.

El mejor de los centros de natación de la ciudad está en el parque Warszawianka, 4 km al sur del centro sobre la calle Merliniego, donde existe una pileta de talle olímpico así como dispositivos acuáticos y un área para los niños.
El equipo del fútbol con más éxitos es Legia Warszawa. En 1994 jugó en los cuartos de final de la Liga de Campeones de la UEFA. Equipo del ejército, con fanáticos en todo el país, juego en el Estadio del Ejército Polaco, justo al sudeste del centro en la calle Łazienkowska. Su mayor rival es el Polonia de Varsovia, campeones de la Liga en 2000, su sede está en la calle Konwiktorska, 10' a pie al norte del Barrio histórico.

Otras actividades son el tenis, el squash, los deportes acuáticos, las actividades hípicas, el ciclismo, la escalada o bien el fitness, practicable en muchos clubes. Cerca del centro de la ciudad hay campos de golf, piletas de natación, acuaparques, ríos artificiales y dispositivos y piscinas infantiles. Otros clubes de la ciudad son el Warsaw Eagles, un equipo de fútbol americano y uno de los mejores clubes de Polonia.


Aunque muchas calles y avenidas fueron ampliadas en los años 1950 y se construyeron nuevas vías de circulación, la ciudad presenta numerosos problemas de tráfico.
El transporte público se extiende por toda la ciudad, y está conformado principalmente por autobuses, tranvías y un sistema de metro.

La ciudad carece de un efectivo anillo periférico, por lo que la mayor parte del tráfico que se dirige de un distrito a otro suele atravesar el centro de la ciudad.
Los varsovianos con vehículos deben afrontar congestionamientos viales, carencia de puestos de estacionamiento y trabajos de mantenimiento municipal diariamente. En la actualidad, se ha planificado la construcción de vías que no crucen el centro varsoviano, y se estima que para el 2014 esté listo un sistema de tres anillos que rodeen la ciudad. Las autoridades municipales también han estudiado la posibilidad de restringir el tráfico de vehículos privados a ciertas partes de la ciudad.
El alto afluencia de vehículos no es la única causa del tráfico. Se estima que entre el 15 y el 20 % del movimiento de vehículos se debe a la búsqueda de un lugar para estacionarlo.

Uno de los sistemas en construcción recibe el nombre de Obwodnica Etapowa Warszawy, y tendrá una longitud de 10 km, iniciando en el centro de la ciudad, y cruzando dos puentes nuevos. Para el 2005, la ciudad contaba con 800 000 vehículos, dando una relación de un vehículo por cada 2.5 habitantes. Otro sistema nuevo va a ser parte de la autopista A-2, que a su vez será parte de la E30 europea, y de la carretera S-7. Esta vía atravesará el sur de la ciudad mediante un túnel ubicado en el distrito varsoviano Ursynów, ubicado al sur. Se espera que la construcción esté finalizada antes del 2012.

La autopista E30 conecta Berlín con Varsovia, un trayecto de 5 horas, y con Brest en Bielorrusia. La E77 comunica con Gdansk al norte, y Cracovia al sur. La E67 dirige a Breslavia, al suroeste.

Las condiciones de los 2600 km de calles de Varsovia no son óptimas. Para el 2004, el promedio de vida de los amortiguadores europeos era de 30 000 a 40 000 km, mientras que en Varsovia se reducía a entre 15 000 y 20 000 km.
La ZDM es la oficina responsable de las calles de Varsovia, y están obligadas a pagar indemnizaciones si se demuestra que la suspensión de un carro sufrió daños debido al mal estado de las vías.
Varsovia tiene un aeropuerto internacional, el Aeropuerto de Varsovia-Frederic Chopin (normalmente denominado aeropuerto Okęcie), situado a solo 10 kilómetros del centro de la ciudad.
Con alrededor de 100 vuelos internacionales y domésticos al día y con más de 9.27 millones de pasajeros en 2007 es, con mucho, el principal aeropuerto de Polonia.
Inmediatamente adyacente a la terminal principal, el complejo de la Terminal 1, está la terminal Etiuda, que sirve rutas de vuelo de bajo coste.
Una nueva Terminal 2 se abrió al público en marzo de 2008, a fin de aliviar el hacinamiento habitual y para ampliar la capacidad del aeropuerto en otros 6 millones de pasajeros más. La Terminal 2 sirve vuelos nacionales e internacionales operados únicamente por compañías aéreas de Star Alliance. La Terminal 2 fue construida por una empresa española.

Desde julio 2012 Varsovia tiene un segundo aeropuerto en Modlin, 35 kilómetros al norte del centro de la ciudad. En el segundo Aeropuerto de Varsovia-Modlin, operando sobre todo con compañías aéreas de bajo coste y en futuro con CARGO.

También hay planes a largo plazo para construir un nuevo aeropuerto internacional. Su ubicación aún no se ha decidido.

El transporte público incluye el sistema de autobuses, de tranvías, de metro y de tren ligero para rutas inter-urbanas y otro para rutas extra-urbanas. Todos estos sistemas son controlados por la Autoridad Municipal de Transporte, excepto el tren extra-urbano, o regional, que es operado por Szybka Kolej Miejska Sp. z o.o. y Koleje Mazowieckie (Ferrocarriles Mazovianos).
Hay tres rutas turísticas: "T", un antiguo tranvía que funciona entre los meses de julio y agosto; el autobús "100" que circula los fines de semana (es el único autobús de dos pisos propiedad de la ciudad) y el "180", un servicio regular de autobuses que sigue la "Ruta Real" de la guerra desde el Cementerio del Norte, cerca del casco antiguo de la ciudad, pasando por las vías más importantes de la ciudad como Krakowskie Przedmiescie, Nowy Świat y Aleje Ujazdowskie, y finalizando en el palacio de Wilanow.

Durante los años 1990, la popularidad del transporte público cayó drásticamente, llegando a ser utilizado por el 64 % de los varsovianos en 1998. En los años setenta, el transporte público había alcanzado su mayor pico, 90 % de uso. Para contrarrestar esta tendencia, las autoridades empezaron a renovar la flota de tranvías y buses, la mayoría con 30 años de servicio.
Además, se inició la construcción de la segunda línea del metro. En 2009, el 67 % de los varsovianos usa este tipo de transporte para movilizarse por la ciudad.

Las autoridades han empezado a construir carriles para ciclistas, pero los varsovianos se han mostrado reticentes a usarlos, y en invierno es habitual ver estos carriles vacíos.

El servicio de autobuses cubre toda la ciudad, con aproximadamente 170 rutas de un total de 2603 kilómetros de longitud y con unos 1600 vehículos. Entre la medianoche y las 5 de la mañana, la ciudad y sus suburbios son atendidos por líneas nocturnas. El mismo billete, con la inscripción "ZTM Varsovia" es válido para todos los medios de transporte municipales, incluyendo las líneas de metro de la ciudad y sus alrededores.
Los billetes pueden adquirirse en los quioscos, pero se pueden comprar a los conductores de tranvías y autobuses.

La primera línea de tranvía se inauguró en Varsovia el 11 de diciembre de 1866.
El último tranvía propulsado mediante caballos completó su recorrido el 26 de marzo de 1908.
En el período de entreguerras (la Primera y Segunda Guerra Mundial), la red de tranvías se amplió considerablemente. Después de la invasión alemana en septiembre de 1939 el servicio se detuvo durante aproximadamente tres meses debido a daños causados por el conflicto. Volvió a prestar servicio en 1940.
Un año más tarde, en 1941, se introdujeron los actuales colores de los coches (amarillo y rojo, los colores de la Bandera de Varsovia. Anteriormente, los tranvías fueron pintados de color blanco y rojo, o totalmente rojo).

Durante el levantamiento de Varsovia, el sistema de tranvía fue destruido. La primera línea de tranvía se reabrió el 20 de junio de 1945. Tras la Segunda Guerra Mundial, la red de tranvía en Varsovia sufrió un rápido desarrollo.
Las pistas llegaron a todos las puntos principales de la ciudad. Sin embargo, en los años 1960, la política oficial de las autoridades soviéticas en Polonia y la promoción del uso de petróleo soviético, provocaron que se compraran más autobuses y la red de tranvías se vio notablemente reducida.

Actualmente, la empresa Tramwaje Warszawskie dispone de 863 coches. Alrededor de veinte líneas discurren a través de la ciudad con líneas adicionales disponibles en ocasiones especiales (como el día de todos los Santos). La progresiva aproximación a la Unión Europea también ha facilitado subvenciones que se han destinado en la mejora de las infraestructuras y los vehículos de la red de tranvías.

Los planes para la construcción del Metro de Varsovia (Metro warszawskie, en polaco) se remontan a 1925.
La Gran Depresión fue la causa de que se pospusiera indefinidamente. Los estudios sobre el proyecto de metro se reactivaron en 1938, pero, esta vez, la Segunda Guerra Mundial fue la culpable de un nuevo aplazamiento.
A partir de 1955 comenzó nuevamente a considerarse la posibilidad de una red de metro superficial. Sin embargo, la fase de planificación se llevó a un ritmo muy lento y la situación económica impidió que los sucesivos gobiernos comunistas pudieran llevar a cabo los planes de ejecución del metro.
Ya en 1985, el programa fue aprobado por el gobierno y se construyeron los túneles.
La falta de fondos, la mala planificación, y la tediosa burocracia hacía que el trabajo avanzara muy lentamente, a una velocidad no superior a los 2 metros al día. Tras 70 años, el metro se inauguró en 1995 con un total de 11 estaciones.
La línea cuenta actualmente con 21 estaciones a lo largo de sus 23 kilómetros aproximados de vías.
Inicialmente, todos los trenes fueron de construcción rusa. En 1998, 108 nuevos vagones fueron encargados a la corporación francesa Alstom.
El metro de Varsovia cuenta con una sola línea, que está siendo ampliada por el extremo norte. La estación situada más hacia el norte se llama Metro Młociny, y atravesando el centro de la ciudad llega hasta Ursynów, un distrito ubicado al sur. Este sistema subterráneo recibe el 14 % de los usuarios de transporte público de la ciudad. Para disminuir el uso del vehículo, se han establecido estaciones con estacionamientos de vehículos, la cuales han sido un éxito.
La segunda línea atravesará Varsovia de oeste a este, luego de cruzar el Vístula girará hacia el norte, y aunque su conclusión está planteada para 2012 (año en que se celebrará la Eurocopa en Polonia), el estado actual del proyecto hace poco probable que pueda estar terminada para entonces.

La ciudad cuenta con una amplia red de taxis que puedes tomar directamente en la calle. El servicio de taxi también está disponible en el aeropuerto y en las estaciones principales. Los precios del taxi son bastante asequibles en comparación con otras capitales europeas.

La principal estación de tren es Warszawa Centralna, cuyo estructura está ligada al del transporte subterráneo. También hay otras cinco estaciones de ferrocarril y un número menor de estaciones de trenes de cercanías.

En 1845 se inauguró la línea Varsovia-Viena, el primer ferrocarril de Varsovia.

La principal línea de ferrocarril que cruza la ciudad lo hace mediante un túnel (el "średnicowy", construido en 1933) de aproximadamente 2.3 kilómetros de largo que pasa por el centro de la ciudad.
Es parte de una línea este-oeste, conectando las estaciones Warszawa Zachodnia, Warszawa Centralna y Warszawa Wschodnia por medio del túnel y el puente ferroviario sobre el río Vístula.

El primer hospital en Varsovia fue establecido en 1353 por el duque Siemowit III y su esposa Eufemia y llamado el Espíritu Santo "intra muros".
En 1571 el famoso Wojciech Oczko, un autor de extensos tratados de balneología y sifidología se convirtió en doctor del hospital.
Su sede se localizaba primeramente en las calles Piwna, Przyrynek y Konwiktorska, pero fue barbáricamente destruida durante el Sitio de Varsovia de 1939.

La Universidad de Médica de Varsovia, la mayor escuela de medicina de Polonia, posee 16 hospitales afiliados incluyendo el hospital clínico más grande de Polonia, el Hospital Educativo Central Público sobre la calle Banacha, donde los estudiantes son entrenados en casi todos los campos de la medicina.

Varsovia alberga al Instituto de Salud Memorial de Niños, el hospital de más alta referencia de Polonia, como también un activo centro de educación e investigación.
Este fue fundado por polacos residentes en Polonia e inaugurado en 1968.
En un enorme complejo de construcciones de diseño nuevo, con el equipamiento más actualizado, posee un grupo de autoridades en pediatría y sus colaboradores. Actualmente, el hospital cubre un área de 20 hectáreas y emplea casi 2000 personas convirtiéndose en el centro pediátrico más grande de Polonia.
Sus fondos provienen del Estado, de seguros de salud y de otros recursos.

El Instituto de Oncología Maria Skłodowska-Curie es unas de las instituciones oncológicas más grandes y modernas de Europa.
Su sección clínica está localizada en un edificio de 10 pisos con 700 camas, 10 quirófanos, una unidad de terapia intensiva, varios departamentos de diagnósticos y una clínica para pacientes ambulatorios.
Cada piso forma un departamento separado con sus propios pabellones de cirugía, radioterapia y quimioterapia. Cada departamento provee la gama completa de tratamiento combinado en una rama particular.

A pesar de que los sistemas de salud en Polonia son gratuitos para las personas cubiertas con seguro de salud, a veces estos son lentos. Para aquellos que desean evitar las colas de los hospitales públicos, hay muchos centros médicos privados y hospitales en Varsovia.

Las ciudades hermanadas con Varsovia son:
Además, la ciudad cuenta con acuerdos de cooperación con:




</doc>
<doc id="5318" url="https://es.wikipedia.org/wiki?curid=5318" title="Premio Princesa de Asturias de Ciencias Sociales">
Premio Princesa de Asturias de Ciencias Sociales

Los Premios Princesa de Asturias de Ciencias Sociales (Premios Príncipe de Asturias de Ciencias Sociales hasta 2014) son concedidos, desde 1981, a la persona, grupo de personas o institución cuya labor creadora o de investigación en los campos de la Antropología, Derecho, Economía, Geografía, Historia, Psicología, Sociología y demás Ciencias Sociales represente una contribución relevante al desarrollo de las mismas en beneficio de la Humanidad.




</doc>
<doc id="5319" url="https://es.wikipedia.org/wiki?curid=5319" title="Premio Princesa de Asturias de la Concordia">
Premio Princesa de Asturias de la Concordia

El Premio Princesa de Asturias de la Concordia (Premio Príncipe de Asturias de la Concordia hasta 2014) es concedido, desde 1986, a aquella persona o personas, o institución cuya labor haya contribuido de forma ejemplar y relevante al entendimiento y a la convivencia en paz entre los seres humanos, a la lucha contra la injusticia, la pobreza, la enfermedad, la ignorancia o a la defensa de la libertad, o que haya abierto nuevos horizontes al conocimiento o se haya destacado, también de manera extraordinaria, en la conservación y protección del patrimonio de la humanidad.




</doc>
<doc id="5320" url="https://es.wikipedia.org/wiki?curid=5320" title="Premio Princesa de Asturias de Cooperación Internacional">
Premio Princesa de Asturias de Cooperación Internacional

Los Premios Princesa de Asturias de Cooperación Internacional (hasta 2014 recibían el nombre de Premios Príncipe de Asturias de Cooperación Internacional) se conceden desde 1981 (la entonces Fundación Príncipe de Asturias cambió su nombre por el de Fundación Princesa de Asturias) a la persona, personas o institución cuya labor haya contribuido de forma ejemplar y relevante al mutuo conocimiento, al progreso o a la fraternidad entre los pueblos.




</doc>
<doc id="5321" url="https://es.wikipedia.org/wiki?curid=5321" title="Premio Princesa de Asturias de Investigación Científica y Técnica">
Premio Princesa de Asturias de Investigación Científica y Técnica

Los Premios Princesa de Asturias de Investigación Científica y Técnica (llamados Premios Príncipe de Asturias de Investigación Científica y Técnica hasta el 2014) se conceden desde 1981 a la persona cuyos descubrimientos o labor de investigación representen una contribución relevante para el progreso de la humanidad en los campos de las matemáticas, la física, la química y la biología, así como en las técnicas y las tecnologías relacionadas con ellas.



</doc>
<doc id="5323" url="https://es.wikipedia.org/wiki?curid=5323" title="Esgrima">
Esgrima

La esgrima, conocida también como esgrima deportiva para diferenciarla de la esgrima histórica, es un deporte de combate en el que se enfrentan dos contrincantes debidamente protegidos que deben intentar tocarse con un arma blanca, en función de la cual se diferencian tres modalidades: sable, espada y florete. Su definición es "arte de defensa y ataque con una espada, florete o un arma similar". La esgrima moderna es un deporte de entretenimiento y competición, pero sigue las reglas y técnicas que se desarrollaron en su origen para un manejo eficiente de la espada en los duelos. La esgrima artística es una modalidad actual que incorpora elementos de la esgrima histórica junto a los elementos artísticos del espectáculo.

La palabra procede del verbo "esgrimir", y este a su vez del verbo germánico "skermjan", que significa reparar o proteger. Los contrincantes reciben el nombre de tiradores. Cuando un tirador es tocado por el arma del rival, el tirador que toca al rival recibe un punto. 

A finales del siglo XVI comienzan a ver la luz en Europa distintos manuales de la disciplina. Esta acaba de instituirse como deporte a finales del siglo XIX, cuando las armas blancas ya no se destinan a la defensa personal. Se adopta entonces la lengua francesa en la terminología del reglamento. La esgrima está presente en la primera edición de los Juegos Olímpicos modernos, aunque solo en categoría masculina. Se incorpora la categoría femenina en 1924.

Como deporte, se postula en España que se habría originado en ese país con la espada ropera, arma que forma parte del vestuario o indumento caballeresco. Las espadas denominadas roperas pertenecen a una época en la que aún no existía el concepto de práctica recreativa de la esgrima. El uso de las armas modernas de esgrima surge a finales del renacimiento simultáneamente en toda Europa. 

Muchos datos erróneos han llevado a difundir el mito de que la esgrima es el único deporte olímpico de origen español. La realidad es que ésta ya era una disciplina olímpica en los Primeros Juegos de la Era Moderna de 1896, donde no hubo ningún participante español; un único esgrimidor español se incluye entre los 156 participantes de los Juegos de 1900, aunque no jugó ningún papel relevante, y lo hizo a título personal ya que el Comité Olímpico Español no fue fundado hasta 1905. En todo caso, las normas se negociaban entre la federación francesa y la italiana (no se tenía en cuenta a los españoles); de hecho, un desacuerdo entre franceses e italianos fue lo que provocó que la esgrima fuese suprimida del programa de los Juegos Olímpicos de Roma en 1908.

Tanto ingleses como franceses, españoles, italianos y hasta alemanes se disputan el origen de la esgrima moderna. En la zona germánica se constata la tratadística desde finales del siglo XIII con la obra anónima conocida como "Royal Armouries Ms. I.33" a la que le siguen otros escritos que indican la existencia de una tradición fuertemente asentada cuyo máximo representante sería Johannes Liechtenauer. 

En Italia el primer tratado conservado es obra de Fiore dei Liberi, del año 1409, aproximadamente, manuscrito conocido en español como Flor de Batallas ("Flos Duellatorum in armis, sine armis, equester, pedestre"). También Inglaterra conserva escritos como el "Manuscrito Harley", conservado en el British Museum, datable en torno a 1430: un texto anónimo rimado, indicador de una incipiente escuela inglesa de esgrima de salón. 

Por su parte, en Francia, la bibliografía se inicia apenas unos años después, a mediados del siglo XV, con "Le Jeu de l'hache d'armes", asimismo anónimo. 

Con todo, las técnicas y ejercitación con la espada se pierden en el tiempo, pero lo cierto es que la esgrima tal y como la conocemos ahora está fuertemente ligada a la implantación de las armas de fuego. Llegó un momento en el que las armaduras dejaron de tener sentido, dando paso a espadas más ligeras, y a esto habría que unir la cultura de la defensa del honor entre otros matices contextuales, dando paso posteriormente a la esgrima deportiva. Pero en el siglo XIX no fueron los españoles los que dieron forma a este deporte si no los italianos y franceses. En todo caso, acabó por incluirse en los juegos olímpicos de 1896. 
La "esgrima germánica" es el arte de combate que comprende las técnicas de empleo de la espada larga a dos manos ("Langschwert") enseñadas en el Sacro Imperio Romano Germánico entre los siglos XIV y XVII, tal y como se describen en el "Fechtbücher" ("manuales de combate").

Esgrima italiana es un término que se emplea para describir el estoque y la técnica que los italianos popularizaron en Europa, principalmente en Inglaterra y Francia. El origen del sistema de combate se suele fijar en 1409, fecha del tratado italiano más antiguo del que se tiene conocimiento, y se extiende hasta 1900, en la etapa de la esgrima clásica.

Aunque las armas y los fines para los que se usaban cambiaron radicalmente durante esos cinco siglos, algunas características han permanecido constantes en la escuela italiana. Algunas de ellas son la preferencia por determinadas guardias, la especial atención al "tempo" y muchas de las acciones defensivas.

En la actualidad, el estilo se preserva tanto en Italia como en el resto del mundo. En Italia, escuelas oficiales de esgrima como la "Accademia Nazionale" ofrecen maestrías, tanto en esgrima histórica como en esgrima moderna, que se adhieren a los principios de la técnica italiana. También se practica la esgrima italiana en instituciones en el extranjero, como la Universidad Estatal de San José, en California, Estados Unidos.

En España hombres y mujeres lo practicaban y en el siglo XV aparecen los primeros tratados que establecen las pautas para el ejercicio de esta actividad.

Como práctica de combate de armas blancas, se origina en España con la famosa espada ropera, es decir, arma que formaba parte del vestuario o indumento caballeresco. Hombres y mujeres la practicaban y como testimonio, se sabe que la Princesa de Éboli, bella pero tuerta, pudo haber perdido el ojo en desgraciado accidente causado por su maestro de esgrima.

Los primeros tratados de la esgrima se encontraron en España a través de Jaime Pons (Jaume Ponç (esgrima)) con la "La verdadera esgrima y el arte de esgrimir" (desaparecido pero publicado en 1472), Pedro de la Torre con "El manejo de las armas de combate" (1473), y Jerónimo Sánchez de Carranza y "Filosofía de las armas y de su destreza" (1582), en la que se desarrolla un sistema de esgrima llamado "Verdadera Destreza", método global de lucha con armas blancas con un fuerte componente matemático, filosófico y geométrico; fruto de la educación renacentista de la época. Con la desaparición del duelo en el último tercio del siglo XIX, aparecen también las reglas propias de cada una de las armas de la esgrima moderna. Desde ese momento, las tres seguirán una evolución paralela.

A pesar de la historia del deporte en el país, el único deportista que ha obtenido una medalla en los Juegos Olímpicos es José Luis Abajo que obtuvo la de bronce en espada en Pekín 2008.

Hay que aclarar que una "espada ropera" es tan sólo la denominación que recibía toda espada que, en España, se llevara sujeta al cinto como complemento de la ropa o traje de vestir. Al menos es esa la teoría más aceptada. De cualquier modo, el término no identifica un tipo concreto de espada, por el contrario era aplicable incluso a espadas de tipo militar portadas por militares que usaban de ropa civil. Los modelos de espadas para duelo utilizados en la esgrima del siglo XVII aparecen en diferentes territorios europeos y sólo uno de ellos puede ser considerado como una espada de origen español, aquel que monta una guarnición de taza y gavilanes salomónicos, amplios o largos. Los modelos de la época que utilizaban guarniciones de lazo descienden de las espadas militares de uso de punta y corte (spada da lato) italianas, originarias del siglo XVI y que proceden por lo tanto de Italia. la identificación de toda espada correspondiente a modelos propios de la esgrima europea del siglo XVII como de "origen español" es una confusión habitual, sobre todo en las poblaciones anglosajonas.

Con la desaparición del duelo en el último tercio del siglo XIX, aparecen también las reglas propias de cada una de las armas de la esgrima moderna. Desde ese momento, las tres seguirán una evolución paralela.

Los Juegos Olímpicos de Atenas de 1896, los primeros de la era moderna, fueron iniciativa del barón Pierre de Coubertin. Él mismo esgrimista, incluyó competiciones de florete y sable, ambos en categoría masculina individual. La espada se introduciría en los Juegos siguientes, los de París, 1900. El sable y florete por equipos llegaría en los Juegos Olímpicos de San Luis de 1904. Los primeros Campeonatos del Mundo de Esgrima se celebraron en Londres en 1956. El florete femenino apareció a nivel individual en 1924 en los Juegos Olímpicos de París y por equipos en 1932 en los de Los Ángeles.

En 1913 nace la Federación Internacional de Esgrima, tras empezar a constituirse federaciones nacionales a partir de 1906. Esta Federación Internacional será quien conste como organizadora de las grandes competiciones y la responsable del Reglamento Internacional para estas pruebas.

Desde entonces se han introducido numerosos cambios, entre ellos la irrupción de la tecnología que permite el registro electrónico de los tocados con la ayuda de un aparato señalizador y la mejora en la seguridad de los materiales, tanto de la indumentaria protectora como de las armas, que hacen de la esgrima actual un deporte en el que los accidentes son prácticamente inexistentes.

En la esgrima moderna se usan tres armas: el florete, la espada, y el sable, hechas de acero templado.
La longitud mínima permitida de la hoja para florete y espada es de 90 cm y en el caso del sable de 88 cm, siendo la longitud máxima del arma de 110 cm para las dos primeras y de 105 cm para el sable.
El peso máximo autorizado debe de ser inferior a 500 g en el florete y sable y de 750 g en la espada.

Arma desarrollada durante el siglo XVII como arma ligera de entrenamiento para combate.

Desarrollada como arma de práctica y deportiva, el florete es considerada el arma básica. Es ligera y flexible y se usa para conseguir tocados embistiendo con su punta roma. La hoja es rectangular en sección transversal. El área válida de tocado para los floretistas es el torso y la barbilla de la careta, resultando por lo tanto "no válido" el tocado en las extremidades o la cabeza. Los tocados se hacen únicamente de punta igual que con la espada, sin el filo y contrafilo como en el caso del sable.

Los tocados se registran gracias a un peto metalizado, que se une a la red de registro de tocados mediante un pasante especial.

Además, es un arma de convención, es decir, se asigna prioridad a los ataques, no existiendo en ningún caso un tocado doble.

La espada moderna deriva del espadín francés, el cual a su vez procede de las espadas de duelo del siglo XVII. Como el florete, es un arma de estocada, pero tiene una cazoleta, releganche o protección de mano más grande, además de ser más pesada y de tener una construcción más rígida. La sección de su hoja es triangular. El área válida de ataque es todo el cuerpo.

Los duelos de espada son los más realistas, pues se asemejan más a la esgrima clásica, no tiene reglas de convención y solo cuenta el orden cronológico entre un tocado y otro, pudiendo existir los tocados dobles.

El sable moderno deriva del arma que usaban los soldados de caballería. Tiene un protector en forma de cuenco, que se curva bajo la mano, y una hoja rectangular en sección transversal. Los tocados o puntos se pueden conseguir embistiendo con la punta o golpeando con el filo o con el contrafilo. Se considera blanco válido el torso, la cabeza y los brazos.

Al igual que el florete es un arma de convención, en la que se asigna prioridad a los ataques, y no existen tocados dobles.

Los asaltos (es decir, los "acometimiento(s) que se hace(n) metiendo el pie derecho y la espada al mismo tiempo", según la definición del "Diccionario de la lengua española") de sable son los más rápidos y ágiles en esgrima, por lo que requieren una buena forma física.

El material requerido para su realización se divide en dos grupos el primer grupo, o material personal, es aquel que el esgrimista pose y por lo tanto es de su propiedad, normalmente en este grupo se hallan los siguientes materiales:


El segundo grupo es el que ya ha sido proporcionado por la organización del evento:




Estas son las instrucciones generales, pero, dependiendo del arma que se esté utilizando la posición del arma y del brazo varían un poco. Mientras en espada el antebrazo se encuentra en posición horizontal, en florete, la punta del arma apunta ligeramente hacia arriba (ya que el brazo no es zona de blanco válido y no hay necesidad de protegerlo).


Con la práctica, el pie derecho se elevará apenas unos centímetros del suelo, pero durante el aprendizaje se exageran los movimientos.



Es el ataque básico y sirve de "catapulta" para otros ataques. También sirve para mantener a distancia al contrincante.

Desde la posición de guardia, estirar el brazo armado apuntando al hombro del contrincante. En caso de practicar ante un espejo, se debe apuntar al hombro del brazo armado del reflejo.

Se hace un fondo (véase más abajo) y al volver a la guardia se permanece con el brazo armado por encima de la cabeza, el arma apuntando hacia abajo y las piernas más juntas que en la guardia.

pierna izquierda.



La mano armada ha de ir siempre protegida con un guante en las tres armas. La mano desarmada no deberá, nunca, bajo ninguna condición, tocar el arma del contrincante y sólo podrá tocar el arma propia si el combate está detenido y por razones técnicas (punta del arma floja en caso de espada, curvatura anormal de la hoja, pasante suelto).

March con appel, realizado con los pies seguido del a fondo.






</doc>
<doc id="5327" url="https://es.wikipedia.org/wiki?curid=5327" title="Bádminton">
Bádminton

El bádminton es un deporte de raqueta en el que se enfrentan dos jugadores (individuales) o dos parejas (dobles) situadas en las mitades opuestas de una pista rectangular dividida por una red.

A diferencia de otros deportes de raqueta, en el bádminton no se juega con pelota, sino con un volante.

Los jugadores deben golpear con sus raquetas el volante para que este cruce la pista por encima de la red y caiga en el sector oponente. El punto finaliza cuando el volante toca el suelo, después de sobrepasar la red.

El bádminton es, desde los Juegos Olímpicos de Barcelona (1992), un deporte olímpico en cinco modalidades: individuales masculino y femenino, dobles masculino y femenino y dobles mixto. En esta última, la pareja está compuesta por un hombre y una mujer. Este deporte está fuertemente dominado por los deportistas asiáticos: China, Indonesia y Corea del Sur consiguieron 28 medallas de oro sobre 29.

El actual juego de bádminton surgió en Asia, más específicamente, en la India, donde recibía el nombre de 'Poona' ciudad ubicada en el estado indio occidental del Maharashtra y lugar donde se jugaba originalmente. Algunos oficiales del ejército británico observaron el juego en la India y lo llevaron a Inglaterra en 1875. Allí, el duque de Beaufort se interesó en el juego, y ya que se practicaba con regularidad en su finca campestre de Gloucestershire, conocida como "Badminton House", este nombre continuó asociado con el juego.
El juego se implantó en los Estados Unidos en 1890 y también fue introducido en Canadá. La Asociación Nacional de Bádminton de los Estados Unidos fue creada en 1895. En esa época se unificaron las reglas. El primer torneo para varones de toda Inglaterra se celebró en 1899, y el primero para damas en 1900. La Asociación Canadiense de Bádminton fue fundada en 1931 y la Asociación Norteamericana de Bádminton en 1936. Los primeros campeonatos norteamericanos se celebraron en 1937, en Chicago.

Las dimensiones del campo de juego son de 13,4 m de longitud por 5,18 m de ancho en individuales y de 13,40 m de longitud por 6,10 m de ancho para los encuentros dobles (normalmente en un partido). La red tiene 1,55 m de altura. Las líneas son de cuatro centímetros de ancho, preferentemente amarillas, que forman parte de la superficie de juego –por ello están trazadas hacia su interior–. También la línea mediana se traza repartiendo su ancho entre las dos zonas de saque formando parte de cada zona de saque.

Las raquetas de bádminton profesional son más flojas, con un peso de entre 75 y 90 gramos, mientras que las raquetas de principiantes son de unos 100 o 115 gramos (sin cordaje). Están compuestas por fibra de carbono junto con una gran variedad de otros cuantos materiales. La fibra de carbono tiene una excelente resistencia en proporción a su peso (iguala al acero en dureza), y da una excelente transferencia de energía cinética. En los inicios de este deporte las raquetas estaban hechas de madera, y posteriormente fueron fabricándose de materiales más ligeros como el aluminio.

Un volante, pluma, plumilla, mosca o gallito tiene plumas, y es el proyectil utilizado en bádminton. Tiene una forma cónica abierta: el cono puede estar formado por dieciséis plumas insertadas alrededor de una base de corcho semiesférica cubierto de una capa fina de cuero.

El volante debe pesar entre 4,74 y 5,5 g, tiene 16 plumas de 6 cm de longitud que están fijadas a una base de corcho de 25 a 28 mm de diámetro que tiene forma esférica en la zona de golpeo.

Las zapatillas de bádminton son muy ligeras, con suela de goma que permite al jugador moverse con facilidad.

Comparadas con las zapatillas de correr, las de bádminton tienen un pequeño soporte lateral. Un gran soporte lateral es útil para actividades en las que el movimiento lateral es indeseable e inesperado; en cambio, en el bádminton requiere de potentes movimientos laterales y la camiseta y los pantalones cortos son elásticos para poder estirarse.

En cada set, los jugadores puntúan, siempre que ganen el punto que estaban disputando (esto difiere del antiguo sistema, en el que sólo se puntuaba en el marcador al conseguir el punto disputado con el servicio). El partido consta de 3 sets, y se lo adjudica el jugador que consiga vencer en dos de ellos, sin necesidad de disputarse el tercero si ya se han conseguido los dos primeros.
En el saque inicial, el jugador que sirve y el que recibe deben situarse en diagonales opuestas de la zona de servicio. El servidor debe golpear el volante por debajo de la cintura para que este aterrice en la zona de servicio del rival. El jugador que está sirviendo no debe cometer faltas, las cuales son: golpear el volante con la raqueta sobre la cintura; golpear la parte de las plumas con la raqueta antes de tocar el corcho; pisar cualquiera de las líneas demarcadas en la cancha o levantar alguno de los dos pies antes de golpear el volante; ubicar la parte superior de la raqueta (cordaje) hacia arriba y golpear el volante con ésta en esa posición; realizar un amague de forma evidente con la raqueta, simulando un golpe al volante, lo que se denomina doble golpe.

El sintonismo ofrece una amplia variedad de golpes básicos, lo que requiere un alto nivel de control de los jugadores para ejecutarlos de forma efectiva. Existen gran variedad de golpes con la empuñadura de la raqueta de derechas como de revés.
El revés es el costado contrario con el que sujeta la raqueta: para un diestro la zona izquierda, y para un zurdo la derecha.

En la zona delantera y media de la pista, la mayoría de golpes pueden ser ejecutados con la misma efectividad tanto de derecha como de revés; pero en la zona del fondo, los jugadores intentarán realizar la mayoría de golpes de derecha. El golpeo de revés tiene dos principales inconvenientes: en primer lugar, debe poner su mano derecha al otro lado del cuerpo y golpear, restringiendo así su visión del rival y eficacia en el momento del golpeo. En segundo lugar, el golpe alto de revés no puede ser tan potente como el de derecha, ya que la acción de golpeo está limitada por la articulación del hombro. El globo de revés es considerado el golpe básico más difícil de efectuar, debido a que requiere de una técnica precisa para que el volante pueda cruzar toda la pista y llegar al fondo contrario. Por la misma razón los remates de revés tienden a tener menos potencia que los de derecha.

La elección del golpe depende de múltiples factores: potencia del volante, dirección, posición del jugador en la cancha, etc., y corresponde también, a la táctica que va a utilizar este durante el juego.

En la mitad de medio campo, un volante alto normalmente será golpeado con un remate. Existen también remates en salto, que permiten a los jugadores un mayor ángulo para picar el volante hacia abajo.

Si el volante se encuentra a una altura inferior a la de la red, se puede realizar un "push" o "block", empujándolo suavemente. En caso de que se encuentre a una altura igual a la de la red existe el golpe plano, también llamado "drive".

En la zona del fondo de la pista, los jugadores intentan golpear el volante siempre por encima de su cabeza. Esto les permite realizar globos, levantadas (arriba al fondo de la pista rival), o remates (picando el volante hacia abajo), clear (golpe sobre el hombro con proyección al fondo de la cancha del oponente), y dejadas (golpe suave para que caiga en la zona de la red).

Para defender un remate, existen tres opciones básicas posibles: levantar, bloquear el volante para realizar una dejada, o realizar un tenso o "drive". En individuales, bloquear es la respuesta más común. Por contrario, en dobles, levantar es la opción más segura aunque permite que los oponentes sigan atacando. "Blocks" y tensos son golpes de contra ataques, para tomar la iniciativa del punto, pero que pueden ser interceptados por el compañero del jugador que ha rematado. Muchos jugadores utilizan la defensa de revés, tanto si el volante va a la zona de derechas como a la de revés, ya que el revés es más efectivo para devolver remates que van dirigidos al cuerpo.

El saque presenta su particular variedad de golpes. A diferencia del tenis, el servicio está restringido por las reglas de juego, por lo que se debe golpear el volante por debajo de la cintura. El servidor puede escoger entre un servicio corto, un servicio largo al fondo, o un servicio tenso o un servicio de defensa.

Faltas: las siguientes acciones implican la pérdida de la acción de juego que se esté disputando:

Enviar el volante fuera del campo de juego.
Realizar el servicio fuera del área de saque que nos corresponde.
Realizar el servicio con el volante por encima de la cintura.
Enviar el servicio (saque) fuera del área de saque contraria.
Golpear el volante en el campo contrario y/o tocar con nuestra raqueta la red.
Tocar la red o los postes que la soportan durante el juego.
Si el volante se queda enganchado en la red o en la raqueta de uno de los jugadores.
Golpear el volante más de una vez de manera consecutiva el mismo jugador (o su compañero, en caso de dobles).

Después de los Mundiales y los Juegos Olímpicos, el torneo más prestigioso es el All England, que se disputa anualmente en Birmingham desde 1899. Hasta la instauración del Campeonato del Mundo en 1977, el torneo inglés era considerado el más importante a nivel mundial.




</doc>
<doc id="5328" url="https://es.wikipedia.org/wiki?curid=5328" title="Balonmano">
Balonmano

El balonmano, handball o hándbol es un deporte de pelota en el que se enfrentan dos equipos, se caracteriza por transportar la pelota con las manos. Cada equipo se compone de siete jugadores (seis de campo y un portero), pudiendo el equipo contar con otros siete jugadores (o menos, o ninguno) reservas que pueden intercambiarse en cualquier momento con sus compañeros. Se juega en un campo rectangular, con una portería a cada lado del campo. El objetivo del juego es desplazar una pelota a través del campo, valiéndose fundamentalmente de las manos, para intentar introducirla dentro de la portería contraria, acción que se denomina gol. El equipo que marque más goles al concluir el partido, que consta de dos partes de treinta minutos, es el que resulta ganador, pudiendo darse también el empate.

Cuando quedan empate, se hacen los penaltis (goles desde la línea de 7 metros).

Han sido numerosos los juegos de pelota que han utilizado las manos a lo largo de la historia; no obstante, el balonmano moderno es relativamente reciente, pues sus primeras reglamentaciones se remontan a los últimos años del sigloXIX y la normalización definitiva de las mismas no llegó hasta 1926, año en que se uniformizaron las reglas para el juego entre equipos de once jugadores y al aire libre, el denominado balonmano a 11. Dicha modalidad llegó a participar en los Juegos Olímpicos de Berlín 1936, pero con el paso de los años, el balonmano comenzó a practicarse en pista cubierta, lo que hizo que el número de jugadores se redujera a siete. Pese a que durante un tiempo convivieron el balonmano a once y a siete, solo este último pervivió, debutando como deporte olímpico en los Juegos Olímpicos de Múnich 1972.

El balonmano se juega siguiendo una serie de reglas, llamadas oficialmente "Reglas de juego," que son modificadas cada cuatro años. Este deporte se practica con una pelota esférica, donde dos equipos de siete jugadores cada uno (seis jugadores «de campo» y un guardameta) compiten por encajar la misma en la portería rival, marcando así un gol. El equipo que más goles haya marcado al final del partido es el ganador; si ambos equipos marcan la misma cantidad de goles, entonces se declara un empate.

La regla principal es que los jugadores, excepto los guardametas, no pueden tocar intencionalmente la pelota con sus pies durante el juego.

En un juego típico, los jugadores intentan llevar la pelota valiéndose del control individual de la misma, o de pases a compañeros, hasta las cercanías de la portería rival, defendida por un guardameta. Una vez allí, tratarán de introducir la pelota en la portería contraria mediante lanzamientos. Los jugadores rivales intentan recuperar el control de la pelota interceptando los pases, quitándole la pelota al jugador que la lleva o bloqueando los disparos con sus brazos y manos. El contacto físico entre jugadores es continuo, pero está sujeto a una serie de restricciones. El juego fluye libremente y se detiene solo cuando el árbitro lo decide.

Es un deporte que con el tiempo ha potenciado el juego de ataque, desarrollándose reglas que limitan el tiempo de posesión del balón de un equipo si este no logra lanzar a portería.

Las reglas no especifican ninguna otra posición de los jugadores aparte de la del guardameta, pero con el paso del tiempo se han desarrollado una serie de posiciones en el resto del campo. A grandes rasgos, se identifican cinco posiciones de juego: pivote, lateral, extremo, central y guardameta. A su vez, algunas de estas posiciones (lateral y extremo) se subdividen en los lados del campo en que los jugadores se desempeñan la mayor parte del tiempo. Así, por ejemplo pueden existir un extremo derecho y un lateral izquierdo. Los seis jugadores de campo pueden distribuirse en cualquier combinación y aunque los jugadores suelen mantenerse durante la mayoría del tiempo en una posición, hay pocas restricciones acerca de su movimiento en el campo. El esquema de los jugadores en el terreno de juego se denomina formación del equipo, algo que, junto con la táctica, depende del entrenador.

El guardameta es el único jugador que, dentro del área, puede dar los pasos que quiera con la pelota en las manos sin necesidad de hacerla botar. Debe ir identificado de un color distinto en su equipación al del resto de jugadores, y es el único que puede tocar la pelota con sus piernas, aunque solo con intención defensiva (como detener un disparo). Fuera de dicha área debe comportarse como cualquier otro jugador del campo.

Es el jugador de primera línea situado entre ambos laterales, que dentro de la cancha 

Los extremos se colocan uno a cada lado de los laterales. Suelen ser jugadores rápidos, ágiles, poco pesados y con gran capacidad de salto. Aprovechan al máximo el terreno de juego para abrir las defensas y generar huecos. Comienzan las jugadas de ataque estático desde su posición. Pueden convertirse en una fuente constante de goles cuando se juega contra defensas abiertas (como el 3-2-1).

Los laterales se sitúan uno a cada lado del central. Suelen ser jugadores altos y corpulentos con un potente lanzamiento. Se utilizan para romper defensas cerradas desde la línea de 9 metros. Son los que asisten en la mayoría de ocasiones a los extremos por su proximidad.

Finalmente, el pivote es el encargado de internarse en la defensa rival y abrir huecos. Son jugadores robustos, que funcionan bien en el cuerpo a cuerpo. Sus movimientos dejan paso libre a los laterales, pero también se convierten en goleadores cuando reciben un pase y tienen la oportunidad de girarse con velocidad hacia la portería.

Para establecer los orígenes del balonmano los investigadores tratan de buscar similitudes y puntos de contacto con juegos propios de los griegos y los romanos. Parece lógico pensar que la agilidad del hombre con sus manos pudo llevarle ya en las primeras civilizaciones conocidas a utilizarlas para sus juegos. Sin embargo, el balonmano, tal y como se entiende ahora, es un deporte realmente muy joven, del primer cuarto del sigloXX.

En cualquier caso, también es cierto que en la antigua Grecia existió el «juego de urania», en el que se usaba un balón de medidas parecida a una manzana que debía ser sostenido en el aire. En uno de los libros fundamentales de la literatura clásica, la "Odisea," Homero habla de este juego y explica cómo dos de sus protagonistas lanzaban la pelota al aire en dirección a las nubes y la cogían saltando, antes de que sus pies volvieran a pisar el suelo. Algunas escenas de este tipo de diversión fueron halladas en la muralla de Atenas en 1926.

Posteriormente, también entre los romanos el médico Claudio Galeno había aconsejado a sus enfermos la práctica del "harpastum," una modalidad que se realizaba con una pelota y con las manos. Aquello aconteció alrededor de los años Mucho más adelante, ya en la Edad Media, el trovador Walter Von der Vogelwide describió asimismo el «juego de la pelota», que consistía en atrapar el balón en vuelo de una forma parecida a como se lo pasan ahora los jugadores de balonmano. Era practicado principalmente en la Corte y los trovadores lo bautizaron como el «primer juego de verano». De todos modos, era una práctica deportiva no estructurada, sin ningún tipo de reglamento ni de normas.

El balonmano se desarrolló a partir de una serie de juegos similares, que estuvieron en vigor al comienzo del sigloXX, practicados en el centro y norte de Europa. En 1926 se estableció el Reglamento Internacional de Balonmano; en 1928 se fundó la Federación Internacional Amateur de Balonmano por once países durante los IX Juegos Olímpicos de Verano. Este organismo más tarde se convirtió en la actual Federación Internacional de Balonmano (IHF).

En la primera parte del sigloXX, el balonmano fue jugado en el estilo de once contra once (balonmano a 11), que se practicaba al aire libre en campos de fútbol y, de hecho, esta versión del juego sigue siendo practicada por personas en países como Austria y Alemania.

A medida que la popularidad del balonmano comienza a aumentar en toda Europa, empiezan a estudiarse nuevas modificaciones en el norte de Europa, debido a su clima más frío. La necesidad de practicar el balonmano en interior se hizo evidente. En su modalidad de interior, este deporte se transformó en un juego más rápido y vistoso, que ayudó a que el resto de Europa empezara a practicarlo.

En 1954, la IHF organiza el primer Campeonato del Mundo Masculino, convirtiéndose Suecia en campeona. Tres años más tarde, Checoslovaquia ganó el primer Mundial de Balonmano Femenino. Los escandinavos, junto con Alemania y la antigua Unión Soviética, fueron las potencias en el mundo del balonmano. Esto ha ido cambiando durante las últimas décadas, debido a que la popularidad de este deporte ha aumentado en el resto de países europeos (con excepción del Reino Unido), así como en el Norte de África, principalmente por la influencia francesa.

El balonmano de interior y al aire libre gozaron de la misma popularidad hasta finales de la década de 1960. En 1965 el Comité Olímpico Internacional aprobó la modalidad de interior para que se practicara en losJuegos Olímpicos y con el nombre de «balonmano», el cual ahora se refiere exclusivamente al balonmano a siete. Siendo su primera participación en categoría masculina en los Juegos Olímpicos de Múnich 1972 y en categoría femenina en los Juegos Olímpicos de Montreal 1976.

El balonmano es ampliamente practicado en Europa, pero aún no ha conseguido ganar popularidad en el resto del mundo: aún cuenta como un deporte minoritario y de escasa relevancia en los países de habla inglesa, en América (donde últimamente países como Brasil y Argentina han mejorado su nivel competitivo), África y Asia (continentes donde solamente es practicado profesionalmente en algunos países árabes, y en el caso del balonmano femenino por Corea del Sur y Angola). Los equipos de estos países compiten regularmente en los campeonatos mundiales y en los torneos Olímpicos, pero sin entrar en la clasificación de las mejores naciones del mundo.

El terreno de juego es un rectángulo de 40m (metros) de largo por 20m de ancho, dividido en dos partes iguales, en la cual podemos encontrar un área de portería en cada una.

La portería está situada en la zona central de cada línea exterior de portería. Las porterías estarán firmemente fijadas al suelo o a las paredes que están detrás de ellas para mayor seguridad. Sus medidas son de 2m de alto por 3m de ancho, pintada a dos colores con franjas de 2dm (decímetros) y el ancho de los postes y el larguero es de 8cm (centímetros), medida que coincide con el ancho de la línea de gol. Dicha portería se encuentra dentro de un área de 74,5m² (metros cuadrados), trazada a partir de dos cuartos de círculo, con centro en cada uno de los postes y radio de 6m, unidos por una línea paralela a la línea de gol.

Todas las líneas del terreno forman parte de la superficie que delimitan, midiendo las líneas de gol 8cm de ancho entre los postes de la portería mientras que las otras líneas serán de 5cm.

La línea de golpe franco es una línea discontinua; se marca a 3m por fuera de la línea del área de portería. Tanto los segmentos de la línea como los espacios entre ellos medirán 15cm y la línea de 7m será de 1m de largo y estará pintada directamente frente a la portería. Será paralela a la línea de gol y se situará a una distancia de 7m de ella. La línea de limitación del portero (utilizada solo para penaltis) será de 15cm de longitud y se traza directamente delante de la portería, se sitúa a una distancia de 4m de ella.

La línea de cambio (un segmento de la línea de banda) para cada equipo se extiende desde la línea central a un punto situado a una distancia de 4,5m de ella. Este punto final de la línea de cambio está delimitado por una línea que es paralela a la línea central, extendiéndose 15cm hacia dentro de la línea de banda y 15cm hacia fuera de ella.

Es un rectángulo de 40m de largo y 20m de ancho, que consta de dos áreas de portería (véase: Regla 1:4 y Regla 6) y un área de juego. Las líneas más largas se llaman líneas de banda y las más cortas son llamadas líneas de gol (entre los postes de la portería) o línea exterior de portería.

El juego consta de un balón de cuero o de material sintético. Se utilizan 3 tamaños:

El tamaño y peso de las pelotas para «minibalonmano» (para niños menores de 8años) no se encuentran fijadas en las reglas de la IHF. El tamaño no oficial de la pelota de mini-balonmano es de 48cm.

A partir de la categoría juvenil, se permite el uso de resina. La resina se utiliza debido al tamaño y al peso que el balón adquiere a partir de esta categoría, puesto que se hace realmente complicado sostener el balón a las grandes velocidades y fuerzas con las que se mueve.

El balón estará fabricado de piel o material sintético. Tiene que ser esférico. La superficie no debe ser brillante ni resbaladizo.

Antes de iniciarse el juego, los dos equipos deben firmar la plantilla de jugadores, declarando así estar en condiciones legales de poder jugar el partido.

Se hace una entrada al unísono, desde mitad de cancha hacia el centro, cada equipo a un costado de la línea central. Se saludan los jugadores, a los árbitros y se hace el sorteo, el cual generalmente consiste en elegir al azar una mano del árbitro donde hay una moneda o el silbato del mismo. El ganador puede elegir entre sacar de mitad de cancha o pedir que arco desea defender en el primer tiempo.

Los jugadores se posicionan, el árbitro hace una seña a la mesa de control para centrar la atención y así se puede dar la orden de iniciar el juego.

Un equipo está compuesto por un máxima de 7 jugadores cada club
deberán estar presentes en el terreno de juego, simultáneamente, un máximo de siete jugadores. El resto de los jugadores son reservas.

La duración del partido es de 60minutos, divididos en 2 periodos de 30minutos cada uno. El resultado puede ser de victoria para cada uno de los equipos, o empate. Para los equipos de jóvenes entre 12 y 16años es de dos tiempos de 25minutos, y, para la edad comprendida entre los 8 y los 12años, de dos tiempos de 20minutos. En todos los casos, el descanso será de 10minutos.

Si el partido está empatado al final de la duración normal del encuentro y las reglas de la competición requieren el desempate, se juega una prórroga tras 5minutos de descanso para determinar un ganador. El periodo de prórroga consiste en dos tiempos de 5minutos cada uno con un minuto de descanso entre ambos.

Si tras el primer periodo de la prórroga continúa el empate se disputa un segundo periodo de prórroga después de 5minutos de descanso. Esta segunda prórroga también consiste en dos tiempos de 5minutos con 1minuto de descanso.

Si aun así el partido continúa empatado, el ganador se determinará según las reglas de esa competición en particular. En el caso de que se decida por lanzamientos de 7m (metros), se disputaría al mejor de 5 lanzamientos de 7m; de persistir el empate se seguiría lanzando hasta proclamar al ganador.

En este deporte está permitido el contacto «de cara», es decir, pecho con pecho, usando las manos con brazos semiflexionados, agarrar, a fin de obstruir el ataque del equipo rival, pero no está permitido los empujones, sean del tipo que sean. Estas faltas se sancionan con golpe franco.

La amonestación solo puede ser mostrada una vez a cada jugador (siendo el máximo 3 por equipo) y se le mostrará cuando el jugador muestre una conducta antirreglamentaria, se exceda en el contacto con el jugador rival o tenga un comportamiento antideportivo.

La forma correcta de amonestación es enseñar la tarjeta amarilla para que la vea el jugador, el anotador y el público.

El jugador excluido no podrá jugar durante 2minutos, y su puesto quedará libre hasta que vuelva al terreno de juego. Si un jugador es excluido tres veces en un partido, da lugar a su descalificación inmediata. El árbitro la usará en caso de que cometa infracciones de forma reiterada, repita su comportamiento antideportivo o cuando el jugador no ponga el balón en el suelo cuando se pita una falta en contra de su equipo.

La forma correcta de excluir es mostrar el puño cerrado con el dedo índice y corazón levantados.

El jugador deberá abandonar el terreno de juego para el resto del partido, jugando su equipo durante 2minutos con uno menos y entrando otro jugador en su lugar cuando el tiempo se haya cumplido. También puede ser descalificado un componente del banquillo, ya sea suplente o entrenador cumpliéndose esta con la salida de un jugador de campo. Un jugador es descalificado cuando comete una infracción muy grave contra el rival, su actitud antideportiva continua, acumula tres exclusiones, comete algún tipo de agresión o entra en el terreno de juego sin tener que estar en él.

La descalificación es mostrada por el árbitro enseñándole la tarjeta roja al jugador.

Un nuevo matiz aparece en el reglamento, es el de las acciones de sabotaje en el último minuto del partido; aunque se debe dar unos condicionantes como el resultado igualado, y debe ser acciones que eviten una última posibilidad de gol o que eviten que se ejecute un saque o lanzamiento en los últimos instantes. En estos casos también se sancionará con descalificación directa.

El balonmano playa contiene grandes similitudes con el balonmano tradicional. Participan dos equipos de cuatro jugadores cada uno, siendo uno de ellos el portero. Se juega en un campo de unos 27 por 12m (metros), el cual está cubierto íntegramente por arena. Cada partido consta de dos tiempos de 10minutos cada uno, y el resultado es contabilizado independientemente, si se logra ganar los dos tiempos, se logra un 2-0, pero, en caso de que cada equipo ganase un periodo, el partido se decide con el sistema de «un jugador contra el portero». Los golpes francos deben sacarse justamente en el lugar donde se cometieron, teniendo que estar los jugadores a 1m del lanzador. Si un jugador es excluido, este no podrá volver a entrar hasta que su equipo haya recuperado la posesión del balón, en caso de descalificación, este jugador no podrá volver a entrar y será reemplazado por otro cuando su equipo vuelva a recuperar la posesión. Para los cambios, los jugadores de ambos equipos se situarán en el exterior de la misma línea de banda, cada uno en la parte correspondiente a su campo, permaneciendo sentados y podrán cambiarse tantas veces como quieran.

La competición más importante en la actualidad es el Mundial de Balonmano Playa, que se disputa bajo el mandato de la IHF (Federación Internacional de Balonmano).

El minibalonmano se juega entre dos equipos de cinco jugadores cada uno, siendo uno de ellos el portero, aunque este debe ser sustituido en cada periodo. Además, al ser este un juego dirigido para niños, todos ellos deben participar en algunos de los cuatro tiempos. Se juegan cuatro tiempos de 10minutos cada uno, teniendo 6minutos de descanso entre tiempos y 2minutos entre periodos (2 tiempos = 1 periodo).

Cada encuentro se juega sobre una superficie de material sólido de unos 20 por 13m (metros), además de ser reducidas otras distancias del área. La portería debe ser rebajada a 1,6m en caso de ser benjamín, o a 1,8m si es alevín. Desde línea de meta hasta el área hay 5m, y el punto de penalti se hallará a 6m.

El balón utilizado por los niños depende de la categoría de estos: 44cm (centímetros) de diámetro para benjamines, y 48cm para alevines. En la defensa no podrán ser utilizadas las mixtas (defensas independientes a un jugador), y no podrá ser utilizada ninguna sustancia en la sujeción del balón. El resultado final solo podrá oscilar entre 0-0, 0-1, 1-0, 1-1, 2-0 y 0-2; ya que cada periodo es independiente y se le da un punto al equipo ganador.

Este tipo de deporte no tiene representaciones internacionales, ya que es practicado solo para la enseñanza del balonmano común entre los niños varones y niñas de los distintos clubes.

El ente rector del balonmano a nivel internacional es la Federación Internacional de Balonmano (más conocida por sus siglas en inglés: IHF), con sede en Basilea, Suiza.
Debido al constante crecimiento de la IHF, se han creado a lo largo de la historia cinco "federaciones" regionales, cuyos objetivos son similares a los de la IHF. Las mismas están encargadas de coordinar todos los aspectos del deporte en cada región.

A continuación se detallan los nombres completos traducidos al español, las siglas en su idioma oficial y la zona de influencia de cada una de las cinco federaciones:

A su vez, dentro de cada federación hay asociaciones de balonmano, las cuales representan a un país y, en algunas ocasiones, un territorio o estado no reconocido internacionalmente. Salvo casos excepcionales, hay una sola asociación por país o territorio, y en caso de existir más de una, sólo una puede estar afiliada a su federación. En algunos casos, la asociación principal del país tiene afiliadas otras "sub-asociaciones" para ayudar en la organización del balonmano. Cada asociación organiza el balonmano de su país independientemente de su federación, pero en algunos casos, por ejemplo para clasificar clubes a torneos internacionales, dichos clubes deben estar avalados por la asociación ante la federación.

Aparte de los intentos aislados en 1936, el balonmano moderno se introdujo en el programa olímpico en los Juegos de Múnich de 1972. Yugoslavia venció en la categoría masculina, mientras que Rusia ganó el oro en la categoría femenina. Estas dos naciones dominan las listas de éxitos internacionales desde entonces. La primera copa del mundo se realizó en 1938 en modalidad de grupo con 4 participantes (Alemania, Austria, Suecia y Dinamarca), proclamándose campeón Alemania al ganar en sus 3 partidos. Con la profesionalización de algunos campeonatos nacionales en Europa occidental y la desintegración de Rusia y Yugoslavia, Francia (campeón olímpico en , triple campeón mundial en 1995, 2001, 2009 y campeón de Europa en 2006 y 2010) o, más recientemente, España (campeón mundial en 2005) también fueron capaces de ganar grandes títulos. En los campeonatos del mundo de 2007, Alemania, que jugó en su terreno, se coronó campeón del mundo frente a Polonia con una victoria por 29-24.

El balonmano internacional está dominado por los países europeos (tanto en hombres como en mujeres). La única excepción notable es Corea del Sur, cuyo equipo femenino fue: doble campeón olímpico ( y ), tres veces finalista y campeón del mundo en 1995. Túnez, Egipto y Argelia entre los hombres y Angola entre las mujeres están entre los países no europeos que regularmente clasifican para las finales de grandes torneos internacionales.


A nivel de clubes, la cita más importante es la Liga de Campeones (Champions League), antigua Copa de Europa, que enfrenta a los grandes clubes europeos masculinos desde 1956 y femeninos desde 1960. Otros continentes poseen competiciones similares, como la Liga de Campeones de África que se creó en 1979.


Los principales campeonatos nacionales se disputan en Alemania, España, Dinamarca y Francia, donde los jugadores compiten en una situación profesional. Debido a ser el lugar de origen de este deporte y gracias a su infraestructura deportiva y financiera, la liga alemana es la más poderosa de las cuatro. El balonmano no es lo suficientemente popular en el plano mundial por lo cual los salarios de los jugadores profesionales son inferiores en comparación con otros deportes.


Además de estas ligas, hay competiciones eliminatorias en estos países: Copa Alemana, Copa del Rey de España y la Copa de Francia. Rusia, los países nórdicos y los de la antigua Yugoslavia también ofrecen competiciones de alto nivel.

Las grandes ligas de balonmano femenino se disputan en Dinamarca, Francia y Alemania.




</doc>
<doc id="5329" url="https://es.wikipedia.org/wiki?curid=5329" title="Ciclismo">
Ciclismo

El ciclismo es un deporte en el que se utiliza una bicicleta para recorrer Circuitos al Aire Libre o en Pista Cubierta y que engloba diferentes especialidades como las que se mencionan a continuación:

El ciclismo de competición es un deporte en el que se utilizan distintos tipos de bicicletas. Hay varias modalidades o disciplinas en el ciclismo de competición como ciclismo en carretera, ciclismo en pista, ciclismo de montaña, trial, ciclocrós y BMX y dentro de ellas varias especialidades. El ciclismo de competición es reconocido como un deporte olímpico. La Unión Ciclista Internacional es el organismo gobernante mundial para el ciclismo y eventos internacionales de ciclismo de competición.

Se caracteriza por disputarse sobre asfalto aunque en determinadas pruebas se circule por caminos no asfaltados. Dentro del ciclismo en carretera existen las siguientes pruebas:
1 Prueba en línea de un día. Las pruebas de este tipo de mayor éxito se denominan clásicas y dentro ellas destacan los llamados monumentos del ciclismo.
2 Prueba por etapas. Se disputan en un mínimo de dos días con una clasificación por tiempos. Se compone de etapas en línea y etapas contrarreloj. Destacan Vuelta a España, Giro de Italia y la más prestigiosa el Tour de Francia. El objetivo es terminar todas las etapas en el menor tiempo posible.

Se caracteriza por disputarse en un velódromo y con bicicletas de pista, que son bicicletas de carretera modificadas. Hay varios tipos de pruebas entre los cuales existen:






Modalidad ciclista, nacida a mediados del siglo XX que consiste en realizar un determinado número de vueltas a un circuito con tramos de asfalto, caminos y prados y con una serie de obstáculos (naturales o artificiales) que deban obligar al corredor a bajarse de la bicicleta para sortearlos.
La principal característica es la utilización de bicicletas de carretera, aunque con algunas diferencias, como neumáticos más anchos para mejorar la tracción sobre tierra y barro o la utilización de pedales de bicicleta de montaña, entre otros. Es frecuente en la preparación invernal de algunos de los profesionales de ruta.

Es una modalidad de ciclismo derivada de los triales de motocicleta. El objetivo es intentar llegar sin velocidad y, tratando de realizar el mínimo número de apoyos con los pies, desde el suelo a la cima de un obstáculo como un vehículo, un barril, un pasamano, rocas, etc.
Existen diferentes categorías según el número de pulgadas de las ruedas:

Esta se sub-divide en dos categorías:

En el "bici motocross" (conocido por sus siglas BMX, y por el término en español 'bicicrós') existen dos modalidades: BMX RACE . El primero se practica en circuitos con curvas y obstáculos; actualmente es deporte olímpico, donde el último campeón en Londres 2012 fue el letón Māris Štrombergs en la rama masculina y la rama femenina la colombiana Mariana Pajón mientras que el último campeón mundial fue el francés Joris Doudet. El segundo consiste en hacer trucos sobre la bicicleta.

Freestyle

Se practica con una bicicleta BMX de, por lo general, aro 20. Este deporte consiste en hacer trucos sobre la bicicleta. Existen dos tipos de freestyle:

<nowiki>*</nowiki> Freestyle urbano: consiste en hacer trucos en la calle, plazas, escaleras, etc.

<nowiki>*</nowiki> Freestyle en rampa: este se practica sobre un circuito previamente hecho.

Es la práctica del ciclismo sin ánimo competitivo, usando la bicicleta como medio de ejercicio físico, diversión, transporte o turismo. Se realizan viajes cortos durante el día, o viajes más largos que pueden durar días, semanas e incluso meses; en esta modalidad se viaja llevando consigo los elementos necesarios para sobrevivir, aunque unos cargan con la casa a cuestas (tienda campaña) y otros prefieren pernoctar en hostales, albergues, etc. Es bastante común que se realice en solitario. Un gran proyecto para incentivar el cicloturismo en Europa es EuroVelo.

A pesar de que por su denominación no se considere ciclismo competitivo existen «pruebas» o rutas organizadas en las que algunos de los participantes compiten entre sí como en la marcha cicloturista Quebrantahuesos y en la Treparriscos, más liviana que la Quebrantahuesos pero aun así es bastante exigente, y que ambas tienen los puntos de salida y llegada en Sabiñánigo (Huesca-España), la marcha marcha Perico Delgado por los puertos de montaña de Guadarrama con salida y llegada en Segovia, lugar de nacimiento del ciclista que da nombra a la marcha o en las pruebas de «ultramaratón ciclista» ("randonneur"), entre otras, pero en ellas se presupone que hay que ser totalmente autónomo sin asistencias, al contrario que en el ciclismo en ruta que está todo mucho más controlado. El Cruce del Lagarto es considerado uno de los eventos de resistencia más duros de Panamá. Consiste en atravesar el Istmo de Panamá desde el océano Atlántico al Pacífico, transitando por caminos Off-Road, y atravesando el gran Lago Gatún (en cayucos) todo esto en un solo día. Es uno de los eventos más respetados en Panamá y pone a prueba la fuerza mental, la resistencia y habilidades de manejo de las bicicletas.

El ciclismo urbano no es necesariamente un deporte, aunque favorece la salud de quien lo practica. Consiste en la utilización de la bicicleta como medio de transporte urbano, ya sea al trabajo, de compras, para hacer gestiones o de ocio; se trata por tanto de distancias cortas o medias recorridas en medio urbano y sus alrededores. Sus seguidores son, junto con los cicloturistas, los que viven la bicicleta como medio de transporte. Ciudades con excelente infraestructura ciclista son Ámsterdam en Holanda, Copenhague en Dinamarca entre otras. España y Argentina, que tradicionalmente no ocupaban posiciones relevantes en el ámbito del ciclismo urbano, van mejorando paulatinamente. Barcelona fue reconocida en 2011 como la tercera mejor ciudad para el ciclismo urbano del mundo según el ranking elaborado por The Copenhagenize Index y Sevilla mereció en 2013 la cuarta posición mundial y primera de España. Ambas ciudades, junto a Buenos Aires siguen manteniendo posiciones de liderazgo en la clasificación de 2015, situándose entre las quince mejores del mundo. Bogotá, Colombia estuvo en su día posicionada como la tercera ciudad del mundo más amigable al ciclista, detrás de Ámsterdam (1) y Copenhague (2).
Hace unos años eran un lunar en la piel de la ciudad, situación por la que calificaban como moda. Hoy son una alternativa ante la vorágine motora que condena a las ciudades como a sus pobladores, quienes sucumben cada vez más ante las presiones del trabajo.

Aunque la bicicleta aún demanda el espacio público que se merece por las bondades que genera, como el ser un vehículo limpio capaz de ayudar a la forma física de quien la conduce.
Así que el número de usuarios -entre los que se destacan estudiantes, trabajadores, amas de casa y hasta los tamaleros de cada mañana- hacen pensar que al menos esa sociedad víctima de su trajín cotidiano comienza a recuperar su espacio-tiempo al montar sobre dos ruedas.
Y es que la iniciativa de estos jinetes posmodernos por recuperar las calles ha ocasionado que confluyan en eventos semanales llamados ‘rodadas’, carreras sin fin competitivo pero con el común de pasar un buen momento sobre la bicicleta. Y las redes sociales tienen tanto que ver en esto, facilitado a la programación como propagación de más ciclistas urbanos.
La respuesta de estos ha llevado a los gobiernos estatales como municipales a auspiciar sus eventos como poner al alcance de cualquiera, bicicletas.
En Distrito Federal el programa denominado Smart-Bike amplió su horario de atención hace par de meses. Mientras que en ciudades como Puebla, ha llevado a la creación de más bici-estaciones.
Tal vez, en algunos años en ambas urbes se comience a considerar en establecer un día sin camiones y autos a favor de la bicicleta como ya se piensa en Londres, Inglaterra.

Durante las últimas décadas se han ido perfeccionando las técnicas de entrenamiento y nutrición ciclista facilitando todo tipo de recursos a los aficionados para poder avanzar en su preparación.

Todos los inventos humanos son el resultado de intentar satisfacer una necesidad. Aunque, a veces, la falta de ingenio o la falta de tecnología, puede no permitirnos una determinada satisfacción.
También se dan casos en los que los inventos aparecen como evolución de lo que inicialmente era un divertimento intelectual.
La bicicleta no empezará a desarrollarse como tal hasta finales del siglo XVIII.

La primera prueba ciclista de la historia a modo competitivo registrada se disputó el 31 de mayo de 1868 en un pequeño circuito de 1200 metros en el parque de Saint-Cloud, a las afueras de París, en la que participaron 7 ciclistas y fue ganada por el expatriado británico James Moore con una bicicleta de madera de piñón fijo y ruedas de hierro.

Un año después se disputó la primera carrera propiamente dicha, concretamente el 7 de noviembre de 1869, entre París y Rouen. En ella participaron un centenar de ciclistas con el objetivo de culminar o ganar la prueba consistente en 123 km y finalmente la lograron acabar 33. De nuevo el británico James Moore ganó la prueba con un tiempo de 10 h y 45 min. La intención de los organizadores fue demostrar que la bicicleta valía como medio de transporte para largas distancias.

Las primeras asociaciones ciclistas se crearon en Florencia (Italia) el 15 de enero de 1870 y en Holanda en 1871 y posteriormente en Gran Bretaña y en España (Sociedad Velocipedista Madrileña y el Club Velocipédico de Cádiz) en 1878 pero fueron asociaciones humildes de pequeños clubs. La primera asociación nacional fue la francesa en 1881 que creó el primer campeonato francés de ciclismo.

En 1892 se creó la Asociación Internacional de Ciclistas, en Londres siendo la primera asociación internacional de ciclismo. Pero divergencias entre los países que la formaban produjo que el 14 de abril de 1900 se crease la Unión Ciclista Internacional, actual organismo rector, fundado en París. Los integraron las federaciones nacionales de Francia, Bélgica, Estados Unidos, Italia y Suiza. En España el primer organismo ciclista nacional fue la Unión Velocipédica Española creada en 1895.

Estas asociaciones se basaban prácticamente en el ciclismo en pista y ciclismo en ruta ya que apenas existían otras modalidades. Sin haber ningún tipo de especialización ya que los corredores disputaban indistintamente ambas disciplinas desde los 333 metros de pista hasta los más de 100 kilómetros de la ruta. Sin embargo, se puede decir que el ciclismo en pista cogió cierta ventaja al organizarse su primer mundial en 1895 ya que al disputarse en un velódromo se podía controlar mejor aparte de poder cobrar entrada.

En el ciclismo en pista la primera carrera se considera los Seis Días de Londres creados en 1878, y en 1895 se efectuó el primer Campeonato Mundial de dicha disciplina contando con pruebas de velocidad y medio fondo.

Entre 1890 y 1900 nacieron grandes pruebas de ciclismo en ruta, que con el paso de los años se han convertido en "monumentos", algunas hoy todavía existentes como la Lieja-Bastogne-Lieja, la París-Roubaix...

En España las primeras pruebas estatales surgirían de un colectivo de fabricantes de bicicletas de Éibar durante la República. Desde 1932 a 1935 se celebró la Éibar-Madrid-Éibar en 4 etapas, antesala de la Vuelta a España. Si bien anteriormente ya se habían disputado carreras en pequeños clubs, siendo oficialmente los más antiguos la Volta a Cataluña (1911) y la Clásica de Ordizia (1922) debido a la influencia francesa al estar próximo a dicha frontera.

En América la primera carrera registrada fue la Vuelta Ciclista del Uruguay cuya primera edición fue en 1939.

En 1965, bajo la presión del Comité Olímpico Internacional, la UCI (Unión Ciclista Internacional) se dividió en la "Federación Internacional Amateur de Ciclismo" (FIAC) y la "Federación Internacional de Ciclismo Profesional" (FICP), coordinando ambas instituciones. La amateur se fijó en Roma, la profesional en Luxemburgo, y la UCI en Ginebra.

La Federación Amateur era la más extensa de ambas organizaciones, con 127 miembros por los cinco continentes. Era dominada por los países del este europeo, que eran básicamente amateurs. Además, representaba al ciclismo en los Juegos Olímpicos, y solo competían contra los miembros de la Federación Profesional en raras ocasiones.

En 1992, la UCI unificó a la FIAC y la FICP, fusionándose dentro de la UCI. La organización conjunta se trasladó a Lausana.

El ciclismo forma parte del programa olímpico desde la primera edición moderna de los Juegos Olímpicos de Atenas en 1896, cuando se celebraron 5 pruebas de pista (velocidad, sprint, 12 horas pista, 10.000 m y 100 km) y 1 prueba de ruta (87 km).

En los solo se disputaron pruebas en la disciplina en ruta, única vez que ocurrió tal circunstancia.

Hasta los Juegos de Los Ángeles 1984 la participación fue solamente masculina. Las mujeres empezaron a participar en las pruebas de ruta en dichas olimpiadas y en las pruebas de pista en los Juegos de Seúl 1988.

En las Olimpiadas de Atlanta de 1996 participaron por primera vez los ciclistas profesionales y se introdujo la modalidad de ciclismo de montaña.

En los Juegos Olímpicos de Pekín 2008 se agregó otra modalidad de esta disciplina, «BMX SX» (BMX Supercross), esta modalidad descendiente del BMX incorpora nuevas dificultades como una rampa de salida con mayor inclinación y saltos de mayor envergadura, con considerables velocidades.

Todas las pruebas olímpicas de ciclismo han sido de velocidad, nunca hubo eventos acrobáticos o de trial.




</doc>
<doc id="5330" url="https://es.wikipedia.org/wiki?curid=5330" title="Velódromo">
Velódromo

Un velódromo es una pista artificial de forma de rectángulo redondeado, con las curvas peraltadas, donde se disputan competiciones de ciclismo en pista. La superficie suele ser de madera, aunque también las hay de cemento y compuestos sintéticos. Los velódromos olímpicos y para campeonatos del mundo deben tener un perímetro de entre 250. Par otros eventos oficiales deben medir entre 133 y 500 metros, de modo que la suma de vueltas o medias vueltas permita completar 1000 metros exactos.
Las bicicletas utilizadas en velódromos suelen ser de piñón fijo, sin frenos y sin rueda libre (es decir, no se puede parar de pedalear sin parar la bicicleta). Esto ayuda a maximizar la velocidad, reducir el peso y evitar las frenadas bruscas.

La elevación en las curvas, denominada peralte, permite a los ciclistas mantener sus bicicletas relativamente perpendiculares a la superficie, cuando se conduce a gran velocidad. La velocidad aproximada de una bicicleta en la curva puede superar los 80 km/h. El peralte intenta hacer coincidir la inclinación natural de la bicicleta en la curva. De esta manera se logra que la inercia o fuerza centrífuga sea en todo momento casi perpendicular a la pista.

Los ciclistas, no obstante, no viajan siempre a la máxima velocidad. En ciertas carreras de equipo (como la Madison) algunos ciclistas van más lentamente. Por esta razón, el peralte tiende a ser de 10 a 15 grados menor a lo que se prevé necesario para eliminar la fuerza centrífuga. Además las rectas están ligeramente peraltadas para reducir los cambios en la inclinación. Todas estas modificaciones hacen que la pista sea utilizable en un amplio rango de velocidades y permita a los ciclistas dar las curvas sin bruscas modificaciones en la dirección, mejorando su rendimiento.


</doc>
<doc id="5332" url="https://es.wikipedia.org/wiki?curid=5332" title="1916">
1916

1916 () fue un año bisiesto comenzando en sábado según el calendario gregoriano.
















okio`klkolk., ko.lok.lko.lko



























</doc>
<doc id="5333" url="https://es.wikipedia.org/wiki?curid=5333" title="1946">
1946

1946 () fue un según el calendario gregoriano.































</doc>
<doc id="5336" url="https://es.wikipedia.org/wiki?curid=5336" title="Sarampión">
Sarampión

El sarampión es una enfermedad infecciosa exantemática como la rubeola o la varicela, bastante frecuente, especialmente en niños, causada por un virus, específicamente de la familia paramyxoviridae del género "Morbillivirus". Se caracteriza por las típicas manchas en la piel de color rojo (exantema), así como la fiebre y un estado general debilitado. Si se presentan complicaciones, el sarampión puede causar inflamación en los pulmones y en el cerebro que amenazan la vida del paciente.

El período de incubación del sarampión suele durar de 4 a 12 días, durante los cuales no hay síntomas. Las personas infectadas siguen siendo contagiosas desde la aparición de los primeros síntomas hasta los 3 a 5 días posteriores a la aparición del sarpullido.

El diagnóstico se hace a través del cuadro clínico y la detección de anticuerpos en la sangre. No existe terapia específica para el tratamiento de la enfermedad; sin embargo, se puede prevenir mediante la administración de la vacuna contra el sarampión. En el pasado, la vacuna triple vírica (también conocida como SPR) ha reducido el número de infecciones. En la mayoría de los países, la enfermedad es de declaración obligatoria a las autoridades de salud social.

En 1998, la Asamblea Mundial de la Salud estableció el objetivo de la eliminación del sarampión endémico de la Región Europea en el 2007, para poder certificar su eliminación antes del 2010.

El ser humano es el único huésped del virus del sarampión, un virus de alrededor de 120-140 nanómetros con un ARN monocatenario, miembro de la familia de los paramixovirus (género Morbillivirus).
En la superficie del virus del sarampión se encuentran dos glicoproteínas: la hemaglutinina o proteína H y la proteína de fusión o proteína F, formando una matriz de proteínas superficiales. Las proteínas H y F son las proteínas responsables de la fusión del virus con la célula huésped y la inclusión dentro de este. Los receptores de la célula humana son el CD150 o SLAM y en menor medida el CD46. La vacuna produce en el individuo anticuerpos dirigidos contra las proteínas de la superficie del virus del sarampión, en particular, contra la proteína H.

La OMS ha reportado 23 genotipos o variantes genéticas, agrupados en ocho serotipos (A-H). La tasa de mutación de los genomas es comparativamente baja, por lo que las zonas geográficas de origen viral de la infección pueden ser reconstruidas con relativa facilidad. En Europa Central, por ejemplo, se han localizado los genotipos C2, D6 y D7. Los brotes de sarampión en Suiza y Baviera 2006/2007, por su parte, fueron causadas por el genotipo D5 proveniente de Tailandia o Camboya. Esto permitió la detección de una infección en cadena, de Suiza a Baviera y de allí a Austria y Hannover. Además, por razón que en determinadas regiones geográficas sólo hay un serotipo estable, la combinación de elementos provenientes de la superficie del patógeno, permite la fabricación de una buena vacuna para la región en donde se encuentre.

El virus es muy sensible a factores externos tales como temperaturas elevadas, la radiación ultravioleta (luz) y, debido a su envoltura vírica, a muchos desinfectantes como, por ejemplo, los que contienen 1% de hipoclorito de sodio, 70% de etanol, glutaraldehído y formaldehído. En el medio ambiente, puede ser infeccioso durante periodos de hasta dos horas.

La transmisión del virus del sarampión ocurre por contacto toser o por gotitas infectadas provenientes de alguien enfermo, quien permanece infeccioso tres a cinco días antes de la aparición de las erupciones hasta cuatro días después. El virus penetra en las células epiteliales de la mucosa de las vías respiratorias altas, como la orofaringe o, con menos frecuencia en la conjuntiva de los ojos. El virus llega al tejido linfoide y reticuloendotelial local en menos de 48 horas: amígdalas, adenoides, timo, bazo, etcétera y al resto de las vías respiratorias altas, donde se reproduce originando una viremia inicial asintomática durante los primeros 4 días del contagio. Esto es por lo general acompañada de una breve aparición del virus en la sangre. Después de unos 5-7 días hay una segunda viremia, con la consiguiente infección de la piel y las vías respiratorias. Al décimo día del contagio se inicia la respuesta inmune del huésped y la producción del interferón, que disminuyen progresivamente la viremia, y aparece la erupción con el exantema característico y otros síntomas, como tos y bronquitis aguda, que definen el período exantemático de la enfermedad.

A través de la invasión del virus en los linfocitos T y un aumento de los niveles de sustancias mensajeras como las citoquinas, en particular, interleucina-4, se instala una debilidad inmune temporal del cuerpo. Durante esa fase, de aproximadamente cuatro a seis semanas, pueden aparecer infecciones secundarias.

El organismo se defiende sobre todo con una inmunidad de tipo celular: los linfocitos T citotóxicos y las células asesinas naturales. Los pacientes con inmunidad reducida, sobre la base de un debilitamiento de esta parte del sistema inmune, tienen un alto riesgo de infección por sarampión grave. Sin embargo, se ha demostrado que un sistema inmune debilitado, que abarca el área del sistema inmune humoral y no el celular, no conduce a un mayor riesgo de enfermedad. Con el inicio de las erupciones, aparecen anticuerpos, primero de la clase IgM y posteriormente de la clase IgG.
El periodo de incubación es de aproximadamente 4-12 días (durante los cuales no hay síntomas). El primer síntoma suele ser la aparición de fiebre alta, por lo menos tres días, tos, coriza (nariz moqueante) y conjuntivitis (ojos rojos). La fiebre puede alcanzar los 40°C (104°F). Las manchas de Koplik que aparecen dentro de la boca son patognomónicas (su aparición diagnostica la enfermedad), pero son efímeras: desaparecen en unas 24 horas.

Otro síntoma es el exantema, que aparece tres o cuatro días después de comenzar la fiebre. Se trata de una erupción cutánea de color rojizo que desaparece al presionar con el dedo. El característico exantema del sarampión se describe como una erupción generalizada, maculopapular, que comienza 2-3 días después de la aparición de la fiebre y de la sintomatología catarral. Aparece primero detrás de las orejas, se extiende luego progresivamente a la frente, mejillas, cuello, pecho, espalda, extremidades superiores, abdomen y, por último, a las extremidades inferiores, por lo que se dice que el brote sigue una dirección de cabeza a pies, con discreto picor. Al tercer día, el brote palidece; al cuarto, se vuelve de color pardusco, ya no se borra con la presión y la piel tiende a descamarse; desaparece en el mismo orden que apareció. Por esa razón, se suele decir que el sarpullido se "mancha", cambiando de color de rojo a café oscuro, antes de desaparecer.

La erupción y la fiebre desaparecen gradualmente durante el séptimo y décimo día, desapareciendo los últimos rastros de las erupciones generalmente a los 14 días, con descamación ostensible. 

El diagnóstico clínico del sarampión requiere una historia de fiebre de por lo menos tres días consecutivos con al menos uno de los otros tres síntomas. La observación de las manchas de Koplik es también diagnóstica del sarampión.

Alternativamente, el diagnóstico del sarampión por medio del laboratorio se puede hacer mediante la confirmación de anticuerpos IgM frente al sarampión, o el aislamiento del ARN del virus del sarampión en especímenes respiratorios. En casos de infección de sarampión después de una falla de la vacuna secundaria, los anticuerpos IgM podrían no estar presentes. En tales casos, la confirmación serológica puede establecerse mediante la detección de un aumento de los anticuerpos IgG por la técnica de inmunoanálisis ELISA o fijación del complemento.

El contacto con otros pacientes que presentan sarampión aumenta la evidencia epidemiológica al diagnóstico.

No hay un tratamiento específico o terapia antiviral para el sarampión. La mayor parte de los pacientes se recuperarán sin complicaciones con descanso y tratamiento de ayuda.

Algunos pacientes desarrollarán neumonía como una secuela del sarampión. Histológicamente, una sola célula puede encontrarse en la región paracortical de los ganglios linfáticos hiperplásicos de pacientes afectados por este cuadro. Esta célula, conocida como la célula Warthin-Finkeldey, es una gigante multinucleótica con citoplasma eosinofílico e inclusionesk nucleares. Aquellas personas que hayan padecido una infección activa de sarampión o que hayan recibido la vacuna frente al mismo adquieren inmunidad permanente contra dicha afección.

Se debe establecer el diagnóstico diferencial entre el sarampión y la fiebre de Zika.

El sarampión es un virus de transmisión aérea altamente contagioso, el cual se propaga primordialmente a través del sistema respiratorio. El virus es transmitido en secreciones respiratorias, y puede ser pasado de persona a persona vía gotitas de saliva (gotas de Flügge) que contienen partículas del virus, como las producidas por un paciente con tos. Una vez que la transmisión ocurre, el virus infecta las células epiteliales de su nuevo huésped, y pueden replicarse en el tracto urinario, el sistema linfático, la conjuntiva, los vasos sanguíneos y el sistema nervioso central.

Las complicaciones con el sarampión son relativamente comunes, que van desde la habitual y poco grave diarrea, a la neumonía, encefalitis, ulceración córnea que llevan a abrasión córnea. Las complicaciones son generalmente más severas en los adultos que se contagian por el virus.

El porcentaje de casos mortales es de aproximadamente una muerte por cada mil casos. En los países en desarrollo con altos grados de malnutrición y servicios sanitarios pobres, donde el sarampión es más común, la cantidad de fallecimientos es de un 10 por ciento, aproximadamente. En pacientes immunodeprimidos, el porcentaje aumenta hasta aproximadamente un 30 por ciento.

Una complicación rara, pero de extrema gravedad es la denominada panencefalitis esclerosante subaguda (PEES) cuya incidencia es de 7/1000 casos de sarampión. Aunque en países desarrollados es mínima y se diagnostican muy pocos casos al año, suele aparecer unos 7 años después del sarampión y es más prevalente en niños que se afectaron por primera vez antes de los 2 años. Ocurre cuando un virus defectivo, es decir cuya síntesis de proteína M está disminuida, sobrevive en las células del cerebro y actúa como virus lento. Sus síntomas son, cambios de personalidad, cambios del comportamiento y la memoria, seguidos de contracciones bruscas fasciculadas, así como ceguera. Usualmente, es fatal.

El sarampión es una enfermedad infecciosa significativa porque, aunque la tasa de complicaciones no es alta, la enfermedad en sí misma es tan infecciosa que el gran número de personas que sufrirían complicaciones en un brote entre las personas no-inmunes saturaría rápidamente los recursos hospitalarios disponibles. Si las tasas de vacunación caen, el número de personas no-inmunes en una comunidad aumentan, por tanto, el riesgo de un brote de sarampión aumenta.

En los países desarrollados, la mayor parte de los niños están inmunizados contra el sarampión a la edad de 12 meses, generalmente como parte de la vacuna triplevírica SPR (sarampión, paperas y rubéola). La vacunación no se aplica antes ya que los niños menores de 12 meses retienen inmunoglobulinas anti-sarampiónicas (anticuerpos) trasmitidos de la madre durante el embarazo. Un refuerzo de la vacuna se debe recibir entre los cuatro y los cinco años. Las tasas de vacunación han sido suficientemente altas para hacer al sarampión relativamente poco común. Incluso un solo caso en un dormitorio universitario, o escenario similar, genera un programa local de vacunación, en caso de que cualquiera de las personas expuestas no sean inmunes.

Las poblaciones no vacunadas enfrentan el riesgo constante de la enfermedad. Después de que las tasas de vacunación bajaron en el norte de Nigeria a principios de los años 2000 debido a objeciones políticas y religiosas, el número de casos aumentó significativamente, y cientos de niños murieron. En el 2005, un brote de sarampión en Indiana fue atribuido a niños cuyos padres se negaron a la vacunación. A principio de los años 2000, la controversia de la vacuna SPR en el Reino Unido con referencia a un lazo potencial entre la vacuna combinada SPR y el autismo provocó un regreso de las "fiestas de sarampión", en las que los padres infectan a los niños con sarampión de manera deliberada para reforzar la inmunidad del niño sin una inyección. Esta práctica presenta muchos riesgos a la salud del niño, y ha sido desaconsejado por las autoridades de salud pública. La evidencia científica no provee apoyo para la hipótesis de que la SPR sea una causa del autismo. Las tasas decayentes de inmunización en el Reino Unido son la causa probable de un aumento significativo en los casos de sarampión, presentando un aumento constante en el número de casos.

De acuerdo con la Organización Mundial de la Salud (OMS), el sarampión es la primera causa de muerte infantil prevenible por vacunación.

A nivel mundial, la tasa de mortalidad ha sido significativamente reducida por los signatarios de la Iniciativa Sarampión: la Cruz Roja Americana, los Centros para el Control y Prevención de Enfermedades de los Estados Unidos (CDC), la Fundación de las Naciones Unidas, Unicef y la Organización Mundial de la Salud (OMS). Globalmente, las muertes por sarampión han bajado en 60%, desde aproximadamente 873 000 muertes en 1999 hasta 345 000 en el 2005. África es la región que ha mostrado el mayor avance, con una reducción de las muertes anuales por sarampión del 75 por ciento en sólo cinco años, desde unas 506 000 hasta unas 126 000.

El comunicado de prensa lanzado en conjunto por la Iniciativa Sarampión arroja luz sobre otro beneficio de la lucha contra el sarampión: "Las campañas de vacunación contra el sarampión están contribuyendo a la reducción de las muertes infantiles por otras causas. Se han convertido un canal para la entrega de otros implementos salvavidas, tales como redes para las camas para proteger contra la malaria o paludismo, medicina desparasitante y suplementos de vitamina A. Combinar la inmunización contra el sarampión con otros suplementos de salud es una contribución al logro del Objetivo del Milenio #4: una reducción de dos tercios en las muertes infantiles entre 1990 y 2015."

Una vez contraída y curada la enfermedad, el cuerpo adquiere inmunidad permanente.

En 2007, Japón se convirtió en un nido para el sarampión. Japón sufrió de un número récord de casos, y un número de universidades y otras instituciones en el país cerraron en un intento de contener el brote.

En la década de 1990, los gobiernos americanos, junto con la Organización Panamericana de la Salud, lanzaron un plan para erradicar las tres enfermedades para los que sirve la SPR —sarampión, paperas y rubéola— de la región.

El sarampión endémico ha sido eliminado de Norte, Centro y Sudamérica; el último caso endémico en la región se reportó el 12 de noviembre del 2002. En el 2016, el continente americano fue el primero —y el único, hasta ahora— del mundo que había eliminado la enfermedad. 

En agosto del año 2010, se reportaron casos de sarampión en Argentina, en la provincia de Buenos Aires y en la Ciudad Autónoma de Buenos Aires, los cuales se presume que se contagiaron por personas que asistieron a la Copa Mundial de Fútbol de Sudáfrica 2010.

Durante el 2018, reapareció el sarampión en Argentina, tras 18 años de erradicada la enfermedad, en el 2000: el primero fue un caso autóctono en el país, en la ciudad de Buenos Aires, y se extendió a la provincia homónima.

En agosto del 2018, en Ecuador, tras más de ocho años de haber erradicado el sarampión vernáculo en el país, reaparecelió con 19 casos, todos ellos importados por inmigrantes.

En agosto y septiembre del 2011, se confirmaron siete casos en Barranquilla, Colombia, luego de muchos años sin aparecer brotes de la enfermedad. El gobierno colombiano inició un plan de vacunación de ocho millones de dosis en las principales ciudades de la costa y en Bogotá. Según declaraciones del gobierno, se debió al tránsito de extranjeros, a raíz de la Copa Mundial Sub 20 de la FIFA Colombia 2011.
Aunque las organizaciones más pequeñas han propuesto una erradicación global del sarampión, de las paperas y de la rubéola, aún no hay planes serios, al menos, hasta la erradicación mundial de la poliomielitis.

A fines de diciembre del 2014, comenzó un brote de sarampión en los Estados Unidos; se estima que tuvo lugar cuando cinco personas enfermaron después de visitar el parque Disneyland. La empresa Disney informó que al menos cinco empleados enfermaron de sarampión. Esta epidemia es la peor que ha ocurrido en 15 años y, según los medios, parece empeorar. Si bien la enfermedad había sido erradicada de los Estados Unidos en el 2000 según el Centro de Control y Prevención de Enfermedades de los Estados Unidos, desde el brote ocurrido en Disneylandia en diciembre del 2014 se diagnosticaron 644 casos en 27 estados de ese país, lo que provocó que el entonces presidente, Barack Obama, solicitara a la población que vacunara a sus hijos. Se reportaron dos casos en Argentina en el 2018, los cuales aún están bajo investigación.

Desde que la vacunación empezó a practicarse a finales del siglo XVIII, sus oponentes han afirmado que las vacunas no funcionan, que son o pueden ser peligrosas, que en su lugar debería hacerse énfasis en la higiene personal o que las vacunaciones obligatorias violan derechos individuales o principios religiosos.

Es difícil asegurarlo, pero la reaparición de casos de sarampión en países desarrollados como los Estados Unidos, Italia, Reino Unido e Irlanda hacen sospechar que la falta de vacunación impulsada en los años 2000 por los grupos antivacunas ha favorecido esta situación. 

El sarampión volvió a aparecer en el 2019 en los Estados Unidos. Desde enero, 22 estados en el país sumaron un total de 695 casos de sarampión, una enfermedad que se creía erradicada hace casi más de dos décadas, cuando surgió un brote de más de 30 000 casos. A partir de ahí, se impulsó la vacunación a todas las personas.

La aparición del sarampión ha sido atribuida, a que las personas no se vacunaban. La enfermedad se está propagando en los Estados Unidos y alrededores. Los datos del brote de sarampión se han recogido en: Nueva York, Washington, Nueva Jersey, California, Michigan.

La gente que no admite la vacunación de esta enfermedad pone en riesgo su salud y la de todos los que les rodean, pues según Unicef, el sarampión es más contagioso que el ébola, que la tuberculosis o que la gripe. El contagio de esta enfermedad es muy feroz, pues puede aparecer hasta dos horas después de mantener el contacto con una persona infectada. Además, esta enfermedad es tan contagiosa que se propaga por el aire e infecta el sistema respiratorio. Si el sarampión se suma a otros daños de la salud humana, como la desnutrición, puede acabar con la vida de estos o de bebés sin vacunar. Al producirse el contagio, no hay un tratamiento específico para la cura, por lo que la vacunación es esencial para salvar a las persona de esta enfermedad, sobre todo a los niños.

Los cinco países con más casos en contra de la vacunación del sarampión entre el 2010 y el 2017 han sido: Estados Unidos, con una cifra de 2 593 000 personas; 608 000, en Francia; Reino Unido, con 527 000 personas; Argentina, con 438 000, e Italia, con 435 000 personas.

En el 2019, la Organización Mundial de la Salud catalogó a estos grupos radicalizados como una de las principales amenazas a la salud mundial.

En el 2020, surgieron brotes de sarampión en México.


</doc>
<doc id="5339" url="https://es.wikipedia.org/wiki?curid=5339" title="Protocolo de oficina de correo">
Protocolo de oficina de correo

En informática se utiliza el Post Office Protocol (POP3, "Protocolo de Oficina de Correo" o "Protocolo de Oficina Postal") en clientes locales de correo para obtener los mensajes de correo electrónico almacenados en un servidor remoto, denominado Servidor POP. Es un protocolo de nivel de aplicación en el Modelo OSI.

Las versiones del protocolo POP, informalmente conocido como POP1 (RFC 918) y POP2, (RFC 937) se han quedado obsoletas debido a las últimas versiones de POP3. En general cuando se hace referencia al término POP, se refiere a "POP3" dentro del contexto de protocolos de correo electrónico.

POP3 está diseñado para recibir correo, que en algunos casos no es para enviarlo; esto le permite a los usuarios con conexiones intermitentes o muy lentas (tales como las conexiones por módem), descargar su correo electrónico mientras tienen conexión y revisarlo posteriormente incluso estando desconectados. Cabe mencionar que aunque algunos clientes de correo incluyen la opción de "dejar los mensajes en el servidor", el funcionamiento general es: un cliente que utilice POP3 se conecta, obtiene todos los mensajes, los almacena en la computadora del usuario como mensajes nuevos, los elimina del servidor y finalmente se desconecta. En contraste, el protocolo IMAP permite los modos de operación "conectado" y "desconectado".

Los clientes de correo electrónico que utilizan IMAP dejan por lo general los mensajes en el servidor hasta que el usuario los elimina directamente. Esto y otros factores hacen que la operación de IMAP permita a múltiples clientes acceder al mismo buzón de correo.
La mayoría de los clientes de correo electrónico soportan POP3 ó IMAP; sin embargo, solo unos cuantos proveedores de internet ofrecen IMAP como valor agregado de sus servicios.

Los clientes que utilizan la opción "dejar mensajes en el servidor" por lo general utilizan la orden UIDL (Unique IDentification Listing). La mayoría de las órdenes de POP3 identifican los mensajes dependiendo de su número ordinal del servidor de correo. Esto genera problemas al momento que un cliente pretende dejar los mensajes en el servidor, ya que los mensajes con número cambian de una conexión al servidor a otra. Por ejemplo un buzón de correo contenía 5 mensajes en la última conexión, después otro cliente elimina el mensaje número 3, si se vuelve a iniciar otra conexión, ya el número que tiene el mensaje 4 pasará a ser 3, y el mensaje 5 pasará a ser número 4 y la dirección de estos dos mensajes cambiara. El UIDL proporciona un mecanismo que evita los problemas de numeración. El servidor le asigna una cadena de caracteres única y permanente al mensaje. Cuando un cliente de correo compatible con POP3 se conecta al servidor utiliza la orden UIDL para obtener el mapeo del identificador de mensaje. De esta manera el cliente puede utilizar ese mapeo para determinar qué mensajes hay que descargar y cuáles hay que guardar al momento de la descarga.

Al igual que otros viejos protocolos de internet, POP3 utilizaba un mecanismo de firmado sin cifrado. La transmisión de contraseñas de POP3 en texto plano aún se da. En la actualidad POP3 cuenta con diversos métodos de autenticación que ofrecen una diversa gama de niveles de protección contra los accesos ilegales al buzón de correo de los usuarios. Uno de estos es APOP, el cual utiliza funciones MD5 para evitar los ataques de contraseñas. Mozilla, Eudora, Novell Evolution así como Mozilla Thunderbird implementan funciones APP.

Para establecer una conexión a un servidor POP, el cliente de correo abre una conexión TCP en el puerto 110 del servidor. Cuando la conexión se ha establecido, el servidor POP envía al cliente POP y después las dos máquinas se envían entre sí otras órdenes y respuestas que se especifican en el protocolo. Como parte de esta comunicación, al cliente POP se le pide que se autentifique (Estado de autenticación), donde el nombre de usuario y la contraseña del usuario se envían al servidor POP. Si la autenticación es correcta, el cliente POP pasa al Estado de transacción, en este estado se pueden utilizar órdenes LIST, RETR y DELE para mostrar, descargar y eliminar mensajes del servidor, respectivamente. Los mensajes definidos para su eliminación no se quitan realmente del servidor hasta que el cliente POP envía la orden QUIT para terminar la sesión. En ese momento, el servidor POP pasa al Estado de actualización, fase en la que se eliminan los mensajes marcados y se limpian todos los recursos restantes de la sesión.

Es posible conectarse manualmente al servidor POP3 haciendo Telnet al puerto 110.
Es muy útil cuando envían un mensaje con un fichero muy largo que no se quiere recibir.


La ventaja con otros protocolos es que entre servidor-cliente no se tienen que enviar tantas órdenes para la comunicación entre ellos. El protocolo POP también funciona adecuadamente si no se utiliza una conexión constante a Internet o a la red que contiene el servidor de correo.



</doc>
<doc id="5340" url="https://es.wikipedia.org/wiki?curid=5340" title="Tenis">
Tenis

El tenis, también llamado tenis de campo, es un deporte de raqueta practicado sobre una pista rectangular (compuesta por distintas superficies, las cuales pueden ser cemento, tierra o césped), delimitada por líneas y dividida por una red.
Se disputa entre dos jugadores (individuales) o entre dos parejas (dobles). El objetivo del juego es lanzar una pelota golpeándola con la raqueta de modo que rebote en la otra cancha pasando la red dentro de los límites permitidos del campo del rival, procurando que este no pueda devolverla para conseguir un segundo rebote en el suelo y por ende un punto.

La palabra española «tenis» proviene del inglés " «tennis»", que a su vez tiene su origen en el francés " «tenez»".
Cuando el jugador de tenis (llamado «tenista») ponía la pelota en juego, exclamaba " «¡tenez!»" (‘¡ahí va!’, en francés).

Las primeras referencias del tenis tienen lugar en Alemania, nombrado "«Palmenspiel»" (‘juego de palmas’), dado que al principio se golpeaba la pelota con la mano. Más tarde se empezaron a utilizar palos de golf.

El tenis original se jugaba en pistas de hierba natural.

Se originó en Europa a finales del siglo XVIII y se expandió en un principio por los países angloparlantes, especialmente entre sus clases altas. En la actualidad el tenis se ha universalizado, y es jugado en casi todos los países del mundo. Desde 1926, con la creación del primer "tour", es un deporte profesional. Es además un deporte olímpico desde los Juegos Olímpicos de Atenas 1896.

=Reglas de tenis=

El tenis se juega en una [cancha para tenis] (llamada «pista» en España) de forma rectangular, de 23,77 metros (78 pies) de longitud por 8,23 m (27 pies) de ancho. Para el partido de dobles la cancha será de 10,97 m (36 pies) de ancho.

Las líneas que limitan los extremos de la pista se denominan líneas de fondo y las líneas que limitan los costados de la pista se denominan líneas laterales. A cada lado de la red y paralela a ella, se trazan dos líneas entre las líneas laterales a una distancia de 6,40 m a partir de la red.

Estas líneas se llaman líneas de saque o de servicio. A cada lado de la red, el área entre la línea de servicio y la red queda dividida por una línea central de servicio en dos partes iguales llamadas cuadros de servicio. La línea central de servicio se traza paralelamente a las líneas laterales de individuales y equidistante a ellas.

Cada línea de fondo se divide en dos por una marca central de 10 cm de longitud, que se traza dentro de la pista y es paralela a las líneas laterales de individuales. La línea central de servicio y la marca central son de 5 cm de anchura. Las otras líneas de la pista son de entre 2,5 y 5 cm de anchura, excepto las líneas de fondo que pueden ser de hasta 10 cm de anchura. Todas las medidas de la pista se toman por la parte exterior de las líneas. Todas las líneas de la pista tienen que ser del mismo color para que contrasten claramente con el color de la superficie.

El tenis puede ser practicado en distintas superficies; ya conocemos la primera en que se comenzó a jugar el tenis, hierba natural. Existen también otras que con el paso del tiempo se han ido popularizando, como son las pistas duras, tierra batida, tenis quick... Estas últimas son elegidas para la apertura de clubes, ya que son las más económicas. Actualmente no figura ninguna competición profesional que se realiza en dicha superficie.

Las pistas de tenis que tienen un mayor coste económico en su mantenido son las de tierra batida, seguida de hierba natural.

La pista está dividida en su mitad por una red suspendida de una cuerda o un cable metálico, cuyos extremos están fijados a la parte superior de dos postes o pasan sobre la parte superior de dos postes a una altura de 1,07 metros. La red está totalmente extendida, de manera que llena completamente el espacio entre los dos postes de la red, y la malla es de un entramado lo suficientemente pequeño para que no pase la pelota de tenis. La altura en el centro de la red es de 0,914 m, en donde está sostenida mediante una faja. Hay una banda cubriendo la cuerda o el campo metálico y la parte superior de la red. La faja y la banda son blancas por todas partes. El diámetro máximo de la cuerda o cable metálico es de 8 mm. La anchura máxima de la faja es de 5 cm. La banda es de entre 8 y 10,35 cm de anchura a cada lado.

Un partido de tenis está compuesto por parciales ("sets" en inglés). El primero en ganar un número determinado de parciales es el ganador. Cada parcial está integrado por juegos. En cada juego hay un jugador que saca, el cual se va alternando. A su vez, los juegos están compuestos de puntos, que son 15, 30, 40 y el punto de juego.

La cuenta de los puntos es bastante particular: cuando un jugador gana su primer punto, su tanteador es 15, cuando gana 2 puntos, 30, y cuando gana 3 puntos, 40. Por ejemplo, si el sacador de ese juego lleva ganados 3 puntos y el receptor 1 punto, el marcador es de "40-15". Siempre se nombra en primer lugar la puntuación del sacador. Cuando ambos jugadores empatan a 40, se dice que hay "deuce" o «iguales». El primer jugador o equipo que gane un punto después del "deuce", logra una «ventaja» y, en caso de ganar el siguiente punto, se lleva el juego. De lo contrario se vuelve a estar en "deuce" hasta que se logre la diferencia de dos puntos. El primer jugador que supera los 40 puntos gana el juego.

El primero en ganar 6 juegos, con una diferencia mínima de 2 con respecto a su rival, es el ganador del set; en caso de que ninguno de los dos jugadores o equipos tenga una ventaja de dos juegos al llegar a seis (6-5), se juega un juego más para conseguir la diferencia de 2 juegos (7-5). De darse el empate (6-6), se jugará un juego de «desempate» o "tie-break".

Si el reglamento del torneo establece un tope de juego, o sea si hay un empate entre dos jugadores en un set, entonces habrá que jugar un juego especial denominado "tie-break", «juego decisivo» o «desempate», en el cual el resultado se decide mediante puntos correlativos (uno-cero, dos-cero, tres-cero, etc.) hasta llegar a 7 tantos, con diferencia de 2. Si se llega a 7 puntos sin diferencia de 2 (por ejemplo: 7-6), el juego se prolongará hasta que uno de los dos jugadores obtenga dicha diferencia y consiga la victoria. La anotación de un set que se ha decidido en el "tie break" será 7-6, acompañada abreviadamente por el número de puntos obtenidos por el perdedor del mismo entre paréntesis. P. ej. si el jugador perdió el juego decisivo por 7-3, la anotación del set será: 7-6 (3).

La inusual y exclusiva forma de anotar el tanteo en tenis (y otros deportes de raqueta inspirados en él) proviene del sistema sexagesimal. Al parecer, antiguamente el tanteo de cada juego se llevaba con un reloj y por cada punto obtenido se movía la aguja un cuarto de vuelta. Así, con el primer punto la aguja se desplazaba al 15, con el segundo al 30, con el tercero al 45 y con el cuarto se cerraba el círculo y se concluía el juego. Con el tiempo y por economía del lenguaje, el parcial "45" se convirtió en "40", dando origen al actual modo de llevar el tanteo: 15, 30, 40 y juego.

Históricamente, esa puntuación de 15-30-40-juego y luego seis juegos para un set viene de la astronomía antigua en la que se usaba un Sextante para medir la elevación del sol. El sextante se divide en 4 partes (15º-30º-45º-60º), y es la sexta parte de una circunferencia de 360º (6 juegos = 1 Set = 360º). La puntuación corresponde por tanto a dichas mediciones que eran en esa época tan usuales como para nosotros el sistema decimal. Luego el 45º, que en inglés es Forty Five, se dejó en forty para comodidad del árbitro.

El tenis es un deporte que requiere que los jugadores dominen técnicas como son: golpes, empuñaduras, efectos, posiciones corporales y desplazamientos corporales, además de necesitar resistencia física para aguantar peloteos largos o fuertes. Durante el partido se utilizan muchos tipos de golpes, cada uno con sus respectivas técnicas; los golpes son: el saque, la derecha ("forehand"), el revés, el globo, la volea, el slice, la dejada y el remate ("smash").

El "saque" es el golpe más importante del tenis, ya que este da comienzo al punto, y su correcta aplicación puede permitir a la persona que saca quedar en una posición de ventaja tras la devolución o bien lograr un "saque ganador" o "ace" (punto ganado sin que el rival impacte la pelota), o que tras el impacto del adversario la pelota no llegue a pasar la red o esta se vaya fuera de los límites de los ejes (en cuyo caso no se denomina "ace", sino "saque ganador"). Al tener buen saque, el tenista aprende a acabar mejor los golpes efectuados sin que la pelota toque suelo y pudiendo dificultarle al contrincante marcarle un punto después de que le hagan una cortada.

El segundo saque suele realizarse buscando mayor seguridad en el resultado. Para ello suelen hacerse saques liftados, cortados o con "kick" (que es lo mismo que liftado) para provocar la mayor dificultad al rival, ya que esos saques suelen ser peligrosos al cambiar la dirección de la pelota o la rapidez después del bote. Uno de los cambios de cómo se hace cada saque es que el cortado, intenta hacer que la pelota corra por las cuerdas de un lado al otro de la raqueta en posición vertical, y el liftado es igual pero en forma horizontal.

Entre los mejores sacadores de la denominada Era Abierta se encuentran los croatas Goran Ivanicevic e Ivo Karlovic, los estadounidenses Pete Sampras, Andy Roddick y John Isner, el neerlandés Richard Krajicek, el canadiense Milos Raonic y el suizo Roger Federer.

El "drive" o "derecha" es el golpe básico. Consiste en golpear la pelota después del bote, de forma directa, del mismo lado del brazo hábil del jugador. Para la mayoría de los jugadores es el arma principal para ganar un punto y el de mayor control. 
Para realizar un correcto "forehand", se debe estar perfilado a la pelota; en el caso de un diestro, el golpe empieza en el lado derecho del cuerpo, continuando a través del mismo hasta impactar la pelota y terminando en la parte izquierda del cuerpo. 
El impacto debe darse en la zona comprendida entre hombro y cadera, y el movimiento se realiza de abajo arriba. Una vez que la pelota impacta en la raqueta, el tenista pasa el brazo derecho adelante cerrando el golpe. En el momento que llega la pelota en altura, el tenista toma la decisión de dar un golpe potente o cruzarla a algún lado. Es el golpe más fácil de aprender, al ser también el más natural.
Hay un error común en países de habla hispana de llamar "drive" al forehand o derecha. En los diccionarios de tenis en inglés "drive" es el golpe que se hace desde el fondo de la cancha con potencia, luego del bote. Por lo tanto hay "drive" de derecha y "drive" de revés.
Entre los mejores golpeadores de derecha ya sea por potencia, precisión, o ambas, se encuentran Pete Sampras, Roger Federer, Ivan Lendl, Juan Martín del Potro y Fernando González.

La "volea" o "golpe de aire" es el golpe que se realiza antes que la pelota rebote en el suelo. Es ejecutado normalmente cerca de la red para definir un punto. A su vez podemos encontrarnos dos tipos de voleas, profundas y cortas. Cada una de ellas dependerá del lugar en el que se produzca el bote de la pelotaː las voleas cortas se producen cuando la pelota bota a la altura del cuadro de saque y las profundas, cuando el bote se produce detrás del cuadro de saque. Debido a la mayor cercanía del jugador al contrincante, es un golpe que requiere ser realizado con gran velocidad y reflejo. La raqueta debe encontrarse en todo momento al frente y alto. El golpe se realiza llevando adelante el pie opuesto al lado donde se va a impactar la pelota, simultáneamente con el perfilado del cuerpo, de modo que la raqueta pueda hacer un breve movimiento atrás para impactar la pelota adelante y de arriba abajo, aprovechando la fuerza que la propia pelota trae, en lo posible sin aplicar energía extra y sin flexionar la muñeca. El golpe que se utiliza para llegar a la red en una jugada se denomina "approach" según la trayectoria del golpe se realizará sin dificultad. Entre los mejores voleadores de la historia se encuentran Stefan Edberg, John McEnroe, Boris Becker, Patrick Rafter, Pete Sampras, Roger Federer, Pat Cash y Radek Stepanek.

El "revés" es el golpe al lado opuesto al "forehand" o derecha. A pesar de ser un golpe de mecánica natural, suele ser uno de los que más cuesta llegar a dominar cuando se empieza a jugar al tenis. Es muy importante la posición del cuerpo, que debe ser colocado de perfil, utilizándose como técnica para ello bajar el hombro para apuntarlo en dirección a la red, mientras que el brazo derecho en los diestros e izquierdo en los zurdos, pasa sin ser flexionado por debajo del mentón, para ubicarse atrás antes de retornar para impactar la pelota, siempre delante del cuerpo, al igual que en el "forehand" o derecha, que el peso del cuerpo se traslade de atrás adelante en el momento de impactar la pelota.

Décadas atrás, el golpe de revés se enseñaba a impactarlo tomando la raqueta con una sola mano (unos grandes exponentes de esta técnica fueron Ivan Lendl, Gustavo Kuerten, Ken Rosewall, Guillermo Vilas, Gastón Gaudio, Stefan Edberg, Pete Sampras y Boris Becker. En la actualidad lo son Stanislas Wawrinka, Roger Federer y Richard Gasquet. Hoy en día el revés a dos manos está ganando cada vez más terreno: jugadores como Rafael Nadal, Juan Martín del Potro, Novak Djokovic y los ya retirados David Nalbandián y André Agassi hacen uso de este golpe. Vale la pena recordar a Jimmy Connors y Björn Borg, cuyos golpes de revés a dos manos inspiraron la popularización que actualmente tiene esta forma de golpeo.

La "dejada" o "drop shot" (del inglés "drop", ‘dejar caer’) es un golpe en el que se le resta potencia a la pelota con la intención de que caiga lo más cerca posible de la red, del lado contrario. Se realiza habitualmente de revés o "backhand", aunque es posible hacerlo también de derecha o "forehand". La preparación del golpe es similar a la preparación del "forehand" (o revés), debiendo hacerse en el último momento, para sorprender al contrincante, que espera un tiro al fondo. Al momento del impacto, en lugar de realizarse el "swing" amplio, la raqueta debe caer de manera perpendicular a la pelota, con un giro de muñeca para producir el efecto de "goteo" que hará a la pelota caer y pasar bien la red.

Se utiliza generalmente cuando el tenista rival se encuentra muy por detrás del fleje del fondo de la pista, y no es un golpe que se deba usar con mucha regularidad, ya que el objetivo es sorprender al rival.

Resulta vital que el golpe sea bajo y corto, para así evitar que el contrincante llegue a la pelota antes del segundo bote, porque de lo contrario le quedará un golpe fácil cerca de la red. Simultáneamente, el jugador puede acercarse a la red para prevenir una "contradejada".

La "contradejada" es la respuesta a una dejada, a la que el jugador llega poco antes del segundo rebote. Como habitualmente la pelota se encuentra muy baja y cerca de la red, no es posible recurrir a un golpe potente, por lo cual, el jugador solo tiene la opción de dar un golpe suave sobre el fondo, es decir, una nueva dejada de respuesta, esta vez hecha desde cerca de la red.

El "smash" o "remate" es un golpe que se efectúa sobre la cabeza con un movimiento similar al saque. En general se puede golpear con gran fuerza de manera relativamente segura y es a menudo un tiro definitorio. La mayoría son realizados cerca de la red o a mitad de la pista antes del pique de la pelota. Suele ser la respuesta a un globo realizado por el oponente que no tuvo la suficiente altura. También puede hacerse desde la línea de base tras el pique, aunque es menos definitorio. Es un golpe alto, realizado de arriba abajo, antes de que la pelota bote, o después de que lo haga, pero únicamente en caso de que este lleve una parábola más vertical que horizontal. Para que sea efectivo, es indispensable que sea muy potente y que no dé oportunidad de respuesta al contrario, ya que se trata siempre de un golpe de definición. Se realiza cuando la pelota viene muy alta, a la altura del brazo extendido del jugador.

El golpe se prepara perfilando el cuerpo, llevando la raqueta atrás y colocándola detrás de la nuca, mientras la mano libre apunta a lo alto, hacia la pelota. En el momento del impacto, el pie trasero pasa adelante, al mismo tiempo que la raqueta sale de atrás del cuerpo en un movimiento similar al del saque. Al momento de impactar la pelota, la muñeca debe flexionarse abajo, terminando el golpe de manera parecida al saque. La pelota tiene que rebotar antes de que el contrario la devuelva.

El "globo" es un golpe sencillo que se utiliza para pasar la pelota por encima del jugador contrario. Se ejecuta tanto de "derecha" como de revés. Incluso existe (su uso no es tan frecuente) la volea globeada. Su ejecución consiste en impactar arriba la pelota (a diferencia de las demás ejecuciones que se hacen adelante); con esto se logra pasar a un jugador que está parado en la zona de la volea o bien hacer un juego defensivo de fondo.

El desarrollo de la técnica ha llevado a aplicar con diferentes modos de golpe, diferentes efectos sobre la pelota, de tal modo que dichos efectos y las consiguientes variaciones de trayectoria y velocidad dificulten la devolución del rival. Los mismos se obtienen con diferentes empuñaduras y formas de impactar la pelota.

Cada uno de los golpes, con sus respectivas trayectorias según el efecto aplicado sobre la pelota y sus respectivos tipos de bote contra la superficie, se explican a continuación.

El "golpe liftado" o con "top spin" (literalmente en inglés «efecto desde arriba») se ejecuta mediante la aplicación de una trayectoria de la raqueta anterior y posterior al golpeo ("swing") de abajo arriba. Antes del impacto con la pelota, la cabeza de la raqueta está por debajo de la trayectoria de la misma y, tras el impacto, el "swing" finaliza por encima de esa altura. Este golpe impone a la pelota un efecto de rotación adelante, que hace que tras el bote, esta salga despedida arriba y adelante, obligando al rival a golpear bien a una altura superior a la normal —muchas veces por encima del hombro—, lo cual le impide ejecutar un golpe agresivo, o bien le obliga a «atacar» la pelota en su trayecto ascendente tras el bote, lo que supone un mayor riesgo de error.

Este tipo de golpe es el más usado, pues tiene, además, la ventaja para el jugador que lo ejecuta de ofrecerle un «margen de seguridad» más amplio, puesto que la trayectoria que se le impone a la pelota es más elíptica y esta pasa a una mayor altura sobre la red que con el golpeo plano o el cortado.

El motivo por el cual logramos imprimir una velocidad angular es la fricción que existe (durante un período muy pequeño) entre la pelota y las cuerdas de la raqueta, estas últimas colisionan casi tangencialmente respecto de la pelota. Esto último es lo que produce un momento de la fuerza sobre la pelota, la cual pasa a tener energía cinética de traslación más energía cinética de rotación. Dicha velocidad angular supone una fricción aerodinámica, de modo que la pelota va empujando (por su parte posterior) constantemente el aire que se le presenta como obstáculo arriba (la fluidodinámica de Bernouilli demuestra este suceso), por la tercera ley de Newton (acción-reacción), la pelota recibe la misma respuesta por parte del viento pero de sentido contrario. Por esto mismo, la pelota adoptará una trayectoria parabólica mucho más pronunciada que en el caso de una colisión plana con las cuerdas.

Finalmente, la velocidad angular que aún conserve la pelota al colisionar con la superficie, se verá reducida considerablemente por la fricción estática con el suelo, haciendo una fuerza atrás, y de nuevo por la tercera ley de Newton, el suelo se la devolverá en sentido pitido.

El "cortado" (en inglés " «slice»", ‘cortar en rebanadas’, o " «backspin»", ‘rotar atrás’) es el efecto inverso al liftado: la pelota adquiere una rotación «atrás» que la lleva a adoptar una trayectoria más baja al botar, obligando al contrario a tener que impactarla más bajo. El efecto se obtiene impactando a la pelota desde arriba y estirando el brazo como si se atravesara la pelota y se la siguiera en su recorrido. Esto hace que la pelota rote de arriba abajo vista desde atrás (nótese que esto no cambia el sentido de rotación de la pelota si el oponente puso efecto liftado a la suya). Debido al efecto Magnus, que en este caso imprime una fuerza neta a la pelota dirigida arriba, un golpe cortado hace que la pelota parezca «flotar» y vuele más lenta. La menor velocidad de la pelota hace que uno de los usos del cortado sea para tener más tiempo de volver a ponerse en posición o acercarse a la red.

El "golpe plano" es aquel que se realiza sin imprimirle ningún efecto a la pelota. En general es muy efectivo cuando se realiza desde una altura mayor a la red, de arriba abajo.

Los golpes que pueden ejecutarse de esta manera son el "forehand" o derecha, el revés, el saque y la volea (esta debe ejecutarse de manera plana «siempre», exceptuando las veces que la volea se quiera muy suave, es decir, haciendo un "drop shot" de volea.

En la actualidad los jugadores están ordenados en un tipo de clasificación llamado «Sistema de entradas» ("Entry Ranking"), en el cual se suman los puntos obtenidos por los jugadores en las últimas 52 semanas, o sea, aproximadamente un año. Por lo tanto, al finalizar cada semana, se le restan a cada jugador los puntos obtenidos en esa misma semana del año anterior, y se le suman los ganados en la semana actual.

La ATP (Asociación de Tenistas Profesionales) es el organismo directivo del circuito masculino de tenis profesional a nivel mundial. El circuito tiene 66 torneos en 32 países que reparten entre 20 millones y 0,325 millones de dólares en premios. La ATP también organiza los Torneos Challenger, donde muchos de los jóvenes jugadores ganan sus primeros partidos y torneos. Cada año se organizan alrededor de 90 eventos a nivel mundial y sus premios en metálico varían entre 50 000 y 125 000 dólares.

Los puntos se consiguen en función de la categoría del torneo y la posición resultante en él. Los torneos más valorados son los cuatro Grand Slams, seguido de la Barclays ATP World Tour Finals y las ATP Master Series. La siguiente tabla resume las puntuaciones otorgadas en 2009:

El nuevo sistema de puntución otorga estos puntos:

1) Grand Slams: Australian Open, Roland Garros, Wimbledon, US Open...

Puntuación: 2000 puntos (ganador) 1200 (finalista), 720 (semifinalista) 360 (Cuartofinalista) 180 (octavofinalista) 90 (3ª ronda) 45 (2ª ronda) 10 (1ª ronda)

2) ATP Serie1000: Indian Wells, Miami, MonteCarlo (no es obligatorio jugarlo), Roma, Madrid, Canadá, Cincinnati, Shanghái, París.

Puntuación: 1000 puntos (ganador) 600 (finalista), 360 (semifinalista) 180 (Cuartofinalista) 90 (octavofinalista) 45 (2ª ronda) 10 (1ª ronda). En Miami e Indian Wells al haber una ronda más la 1ª ronda son 10, la 2ª 25 y la 3ª 45.

3) ATP Serie500: Róterdam, Dubai, Acapulco, Memphis, Barcelona, Hamburgo, Washington, Beijing, Tokio, Basilea, Valencia.

Puntuación: 500 puntos (ganador), 300 (finalista) 180 (semifinalista) 90 (Cuartofinalista) 45 (octavofinalista) 0 (1ª ronda)

Sería obligatorio para los jugadores top jugar cuatro de estos torneos.

4) ATP Serie250: EJ. Viña del Mar

Puntuación: 250 (ganador), 150 (finalista), 90 (Semifinalista), 45 (Cuartofinalista) 20 (octavofinalista) 0 (1ª ronda).

La copa Davis repartirá puntos como un torneo ATP Serie250.

El ranking resultará de la suma de los siguientes torneos: los cuatro Grand Slam, ocho de los Masters 1000, cuatro mejores torneos 500 y dos mejores 250 o challengers.

El suizo Roger Federer fue el número uno del mundo desde el 3 de febrero de 2004 hasta el 17 de agosto de 2008, estableciendo un récord de 237 semanas consecutivas al frente del ATP Ranking.
El español Rafael Nadal, que escoltó al suizo durante 160 semanas desde el 25 de julio de 2005, cuando ocupó el número dos, se convirtió en el número uno del mundo desde el 18 de agosto de 2008 hasta el 5 de julio de 2009, fecha en que Roger Federer volvió a ocupar el primer lugar tras ganar su sexto Wimbledon en 2009.

Rafael Nadal recuperó el número uno tras ganar el trofeo de Roland Garros de 2010 a Robin Soderling quedando Roger Federer en el número dos. Después del US Open 2010, Roger Federer pasa a ocupar el puesto tres y Novak Đoković sube hasta el puesto número dos. Nadal se mantuvo en el primer lugar hasta Wimbledon de 2011, en donde Djokovic derrotó en la final a Rafa y le arrebató el número uno del ranking mundial. Después de ganar la final contra Andy Murray en Wimbledon 2012, Federer recupera el número uno después de dos años, superando a Pete Sampras en semanas como número uno. Después de noviembre de 2012, Novak Đoković vuelve a ocupar el primer lugar hasta finales del año 2013, donde Rafael Nadal volvió a ser número uno. Posteriormente en julio de 2014 Novak Đoković recuperó el número uno luego de disputarse la final de Wimbledon con Roger Federer,quien se quedó con el número tres, dejando a Rafael Nadal en segundo lugar. 

Desde enero de 2000 existe una clasificación paralela, llamada Carrera de Campeones (ATP Champions Race), en la cual se suman los puntos conseguidos en los torneos del año en curso, sin contar puntos del año anterior, y se utilizaba para completar la lista de clasificados a la ATP World Tour Finals (los ganadores de los cuatro torneos de Grand Slam se clasifican automáticamente).

Para aquellos tenistas ubicados en el Top 30, se suman los puntos obtenidos en los cuatro torneos de Grand Slam, los nueve torneos de la Serie Masters y cinco torneos adicionales, incluido la participación en Copa Davis de aquellos jugadores que participen en el Grupo Mundial o en los Play Off. En caso de no haber actuado en algún torneo de Grand Slam o de la Serie Masters, el jugador sumaba los puntos obtenidos en algún torneo de la Serie Internacional.

La Carrera de Campeones refleja el desempeño de los jugadores en el año que está en curso. A final de temporada las clasificaciones de ambos rankings son muy similares especialmente en los primeros puestos, encontrándose algunas diferencias ya que los torneos Challenger y Future no suman puntos en la Carrera de Campeones pero si en el Ranking ATP.

Desde 2009 cambia el nombre para la clasificación de dobles, denominándose "ATP Doubles Team Rankings".

La WTA (Asociación Femenina de Tenis) (en inglés, "Women's Tennis Association"), es la organización que rige los torneos y el circuito profesional del tenis femenino a nivel mundial. A modo comparativo, la WTA es al tenis femenino lo que la ATP al tenis masculino. La Asociación organiza el calendario y designa las sedes oficiales de los torneos del circuito femenino, también llamado como WTA Tour.

En 2005 la Asociación Femenina de Tenis cambió el nombre del WTA Tour por el de The Sony Ericsson WTA Tour debido a un contrato firmado de patrocinio con la firma nipona-sueca de teléfonos móviles y accesorios, Sony Ericsson.

Martina Navratilova resalta por ser una de las mejores tenistas femeninas de la historia, llegando a acumular 128.550 puntos en el sistema de clasificación histórica hasta su retiro en 2006. Martina superó las marcas de Chris Evert y Steffi Graf quienes compitieron hasta 1989 y 1999 respectivamente.
Otras tenistas destacadas en el ranking femenino en la historia han sido
Margaret Smith Court,
Billie Jean King,
Helen Wills Moody,
Suzanne Lenglen,
Evonne Goolagong,
Serena Williams,
Martina Hingis y
Mónica Seles.

Las principales marcas del tenis son:










 es importante hacer deporte

-->

</doc>
<doc id="5341" url="https://es.wikipedia.org/wiki?curid=5341" title="Tenis de mesa">
Tenis de mesa

El tenis de mesa (también conocido popularmente como ping-pong) es un deporte de raqueta que se disputa entre dos jugadores o dos parejas (dobles). Es un deporte olímpico desde Seúl 1988, y el deporte con mayor número de practicantes, con 40 millones de jugadores compitiendo en todo el mundo. Según un estudio realizado por la NASA, es el deporte más complicado que un ser humano puede practicar a nivel profesional. Diversos estudios han demostrado que la práctica de este deporte mejora, entre otras, la capacidad y el tiempo de reacción, la coordinación ojo-mano, la concentración y la memoria.

La regulación a nivel mundial de este deporte corre a cargo de la Federación Internacional de Tenis de Mesa ("ITTF", por sus siglas en inglés), que agrupa a más de 200 organizaciones nacionales y 33 millones de federados a todos los niveles de competición, desde torneos de clubs hasta los campeonatos del mundo, que se celebran anualmente desde 1926 y bianualmente desde 1957, o el World Tour, un conjunto de torneos organizados por la ITTF que se celebran en todos los continentes y que reúne a los profesionales del más alto nivel.

Nació en la década de 1870 en Inglaterra como una derivación del tenis. La historia de este deporte está marcada por una serie de evoluciones técnicas, como la naturaleza de los revestimientos de las raquetas, aumento del tamaño de la pelota, la reducción del número de tantos por juego o la introducción y posterior prohibición del uso de pegamentos rápidos, evoluciones que condujeron a innovaciones en el estilo de juego, como la utilización del agarre de la raqueta con estilo asiático o «de lapicero» (originalmente por los húngaros y posteriormente por los asiáticos), y en las tácticas empleadas, como la aparición del "topspin" a finales de los años 1980. El tenis de mesa moderno permite una gran variedad de sistemas de juego, tanto ofensivos como defensivos.

Aunque a menudo se asocia el tenis de mesa con los países asiáticos, está ampliamente aceptado que este deporte nació en el último cuarto del siglo XIX en Inglaterra como una derivación del tenis.Es posible que jugadores de tenis ante la adversa climatología inventaran una especie de tenis en miniatura utilizando una mesa de billar o de comedor, en un club de tenis, y dividiéndola en dos campos con libros o simultáneamente con una cuerda. Como pelotas servirían algunos de los muchos modelos existentes para juegos infantiles, o incluso tapones de corcho convenientemente adaptados. Las raquetas serían tapas de cajas de puros o bates infantiles. Indudablemente se mezcla la leyenda con la realidad. Por esta versión se inclinan Gerald Gurney y Ron Crayden, dos profundos estudiosos en la historia del tenis de mesa. Los estudiantes universitarios adoptaron rápidamente el entonces juego de salón en toda Inglaterra. En 1884 la firma F. H. Ayres Ltd. (Frederick Henry Ayres) ya comercializaba un juego de tenis de salón en miniatura. El británico James Devonshire patenta, el 9 de octubre de 1885, su «Table Tennis», la primera vez de la que se tiene conocimiento en utilizar el término «tenis de mesa». En julio de 1890, el industrial de Yorkshire David Forster, patentó un juego de mesa para sala, el cual consistía únicamente en una mesa rodeada con una especie de valla para mantener la pelota dentro de unos límites. No existen evidencias de su comercialización.

En 1891, John Jaques, fabricante de artículos deportivos, patentó un juego llamado "Gossima", el cual no tuvo aceptación. Ese mismo año Charles Barter, de Gloucestershire, registró una patente con pelotas de corcho, y en fechas cercanas James Gibb, atleta famoso y fundador de la Amateur Athletic Association, improvisó un material que consistía en una red fija a dos postes y sobre una superficie de madera elevada del suelo, inventando un juego de 21 puntos y con pelotas de goma. Gibb encontró en América pequeñas pelotas de celuloide, introduciéndolas en el juego con un éxito inmediato. James Gibb sugirió el nombre de "Ping Pong" a la firma John Jaques Ltd., la cual registró el nombre. El nombre viene por el sonido de "ping" que hacía la pelota de celuloide al impactar con las raquetas recubiertas en pergamino y el sonido "pong" al contacto de la pelota con la mesa. Estas raquetas de pergamino tenían un mango de 45 cm de longitud.
Ya en 1901 se celebraron en Inglaterra torneos de ping-pong con participación de hasta 300 jugadores y con premios en metálico por importe de hasta 25 libras. En este año se constituye en Inglaterra la Asociación de Ping Pong, la cual contaba con unos 500 jugadores pertenecientes a 39 clubes distribuidos por todo el país. En estas fechas iniciales el servicio se hacía directamente por encima de la red, como el tenis, teniendo una altura variable de 17 cm y de 17,5 cm. Los juegos de dobles eran designados por el nombre de «juego a cuatro manos». En Branthem Essex se producía, según una información de la época, toneladas de pelotas de celuloide a la semana (2,5 millones de unidades aproximadamente) y se distribuían por todo el mundo.

En 1902 comenzó a publicarse la primera revista sobre este deporte, la "The Table Tennis and Pastimes Pioneer", que tenía una periodicidad semanal y que se enorgullecía ese mismo año de haber alcanzado la cifra de 20 000 lectores. También en 1902 se habían editado en Inglaterra y en EE. UU. unos 20 libros con instrucciones del juego. Los principales jugadores ingleses de la época, que desempeñarían un gran papel en la evolución del tenis de mesa mundial fueron A. Parker, P. Bronfield, P. E. Warden, G. J. Ross, J. J. Payme, J. Thompson, E. C. Goode y A. T. Finney; y el primer punteado cubierto de caucho o goma fue patentado por Frank Bryan en 1901 y vendido bajo el nombre de "Atropo". Este tipo de raqueta fue adoptado casi universalmente durante muchos años. Salió también la raqueta de aluminio, garantizando gran rapidez, pero era muy cara y no se vendía. Ayres y G. G. Bussey fabricaron raquetas acordonadas, como las de tenis, en miniatura. Eran de fabricación muy esmerada y utilizaban cordones muy tensos y de gran calidad, pero tenían el inconveniente de que no ofrecían buen control sobre la pelota y fueron prohibidas en muchos torneos, probablemente porque no producía ruido alguno. Las primitivas pelotas de celuloide eran excesivamente ligeras y además tenían la desventaja de que, como se fabricaban en dos partes que luego se unían, la junta producía un bote muy inconsistente. En 1900 Jaques Ltd. fabricaba una pelota de celuloide sin costura y normalizada en tamaño y forma. Las pelotas fueron adquiriendo dureza y además incrementaron el tamaño, circunstancia que facilitaba un juego rápido. Los accesorios para jugar, excepto la raqueta, se vendían en estuches fabricados principalmente en Inglaterra y en Estados Unidos.

En 1922 ya se conocía el nuevo deporte en gran parte de Europa y la India, estando regulado en varios países y jugándose campeonatos asiduamente. En el año 1926 se funda la Asociación Inglesa de Tenis de Mesa con nuevas reglas y estatutos, eligiéndose como presidente a Ivor Montagu y como secretario a Bill Pope. Cuando se fundó esta asociación, tanto Montagu como Pope emprendieron la tarea de organizar el I Campeonato del mundo en Londres, que tuvo un gran éxito y se resolvió económicamente en este año 1926 con 300 libras de pérdidas. La idea del campeonato mencionado surgió con motivo de un torneo internacional organizado en Berlín por el doctor Lehmann, participando alemanes, austríacos, húngaros e ingleses. En este torneo se habló de la necesidad de constituir una Federación Internacional de Tenis de Mesa de forma provisional y la organización del I Campeonato del Mundo y de un Congreso, ambos en Londres. Celebrado el Congreso se constituye oficialmente la Federación Internacional de Tenis de Mesa (ITTF), nombrándose presidente a Ivor Montagu y secretario a Bill Pope, el cual lo sería hasta su muerte prematura en 1950.
En este primer campeonato del mundo se dona por "lady" Swaythling, madre de Montagu, la Copa que lleva su nombre para ser disputada por equipos masculinos. Participan Hungría, Austria, Inglaterra, India, País de Gales, Checoslovaquia y Alemania. En principio la denominación no iba a ser la de Campeonato del Mundo, pero la participación de ocho jugadores indios, residentes en realidad en Inglaterra, condujo a los organizadores a darle este nombre. La participación femenina fue muy baja, pues se redujo a 14 jugadoras: 11 inglesas, 2 austriacas y 1 húngara. Estos primeros Campeonatos vieron el triunfo total de los representantes de Hungría, los cuales se alzaron con todos los títulos. El campeonato ya se jugó en mesas con las dimensiones actuales, aunque la altura de la red era de 17,5 cm. La Organización de este primer Campeonato recomendaba a los jugadores el no usar prendas de color blanco, pero no impedía que la vestimenta fuera poco deportiva, pues los hombres utilizaban pantalones largos, camisas de manga larga y además chalecos, e incluso en algunos casos corbatas, y las mujeres faldas largas y vestidos normales de calle. La primera decisión del Congreso de Londres fue la de intentar unificar las reglas que entonces imperaban en el Tenis de Mesa. En el I Campeonato del Mundo se había jugado en equipos a 21 tantos cada juego y al mejor de tres juegos, y en individuales al mejor de cinco juegos. La ITTF hace oficiales para 1927 dos sistemas diferentes: el sistema de contar hasta 21 tantos en cada juego y que era defendido por los ingleses, y el sistema de tenis de campo en sets de seis juegos, sistema preconizado por húngaros, austriacos y alemanes. En enero de 1928, durante los campeonatos del mundo celebrados en Estocolmo, fue tomada la decisión de unificar el sistema y contar hasta 21 tantos.

En sus inicios, el tenis de mesa está dominado por los países del Bloque del Este europeo, especialmente por Hungría y Checoslovaquia. Con jugadores como el gran Viktor Barna, Hungría consigue la medalla de oro en los campeonatos del mundo por equipos entre 1926 y 1931, entre 1933 y 1935 y en 1938, 1949 y 1952 (entre 1940 y 1946 no se disputaron).

El primer país asiático en frenar el dominio europeo fue Japón, que dominó los mundiales entre 1952 y 1957. Este dominio fue el reflejo de una incorporación técnica aportada por este país: la espuma. Al colocar una fina espuma entre la madera de la raqueta y la goma, se hicieron posibles efectos inéditos con las raquetas clásicas. Con la irrupción de los japoneses en este deporte y la incorporación de nuevas técnicas y materiales, empezó una nueva época de la historia del tenis de mesa.
En 1959, en los campeonatos del mundo de Dortmund, el jugador chino Rong Guotuan ganó la medalla de oro individual, convirtiéndose en el primer deportista de ese país en conquistar un título mundial en cualquier deporte. La supremacía de China comienza en los años 1960 y se mantiene hasta nuestros días. China es actualmente la mayor potencia en tenis de mesa: desde que este deporte entró en el programa olímpico, y hasta las olimpiadas de Río de Janeiro 2016, el tenis de mesa distribuyó 32 medallas de oro, de las cuales China obtuvo 28. Esta supremacía solamente ha sido interrumpida por casos aislados como el húngaro Tibor Klampar en 1979 y sobre todo por los suecos en los años 1990, especialmente con Jan-Ove Waldner y Jörgen Persson.

A principios de los años 1970 se inició un intercambio de partidas de tenis de mesa entre jugadores chinos y estadounidenses al que se conoció en los medios de comunicación como la «Diplomacia del ping-pong» y que tuvo unas importantes implicaciones políticas. Este suceso marcó el comienzo del deshielo en las relaciones entre la China comunista y los Estados Unidos, además de que abrió el camino para la histórica visita al país asiático realizada en 1972 por el entonces presidente Richard Nixon. Esta histórica relación se vio reflejada en la popular y oscarizada película "Forrest Gump".

En pleno dominio chino de este deporte, pocos países pudieron competir con el gigante asiático, a excepción de la llamada «escuela sueca» que, mediante innovadores métodos de entrenamiento consiguió hacerse con el campeonato del mundo por equipos en 1989, 1991, 1993 y 2000, con jugadores como Jörgen Persson o Peter Karlsson, pero sobre todo con Jan-Ove Waldner. En 1982, con menos de 17 años, Waldner fue subcampeón de Europa absoluto, y en 1983 subcampeón del mundo por equipos, en 1989 conquistó su primer campeonato del mundo individual y por equipos y en 1992 fue campeón olímpico individual en Barcelona. En 1997, ya superados los 30 años, consiguió su segundo campeonato del mundo individual, en el 2000 su cuarto campeonato del mundo por equipos y la medalla de plata individual en los Juegos Olímpicos de Sídney 2000, y en los de Atenas 2004, ya con 39 años, consiguió ser semifinalista en individuales.

Con un juego ofensivo muy vistoso, Waldner aumentó la popularidad de este deporte a lo largo de una carrera anormalmente extensa en el tenis de mesa al más alto nivel. En 2003 fue incluido en el Salón de la Fama del Tenis de Mesa.

La adquisición del estatus olímpico es fundamental para el desarrollo y expansión de un deporte. Las propuestas para la inclusión del tenis de mesa como deporte olímpico se inician en 1931, pero no fue hasta 1977 cuando el director técnico del Comité Olímpico Internacional, Harry Banks, comunicó que el COI, en su 79.ª sesión celebrada en Praga, había acordado reconocer al tenis de mesa como deporte olímpico. En la 84.ª sesión del COI, en septiembre de 1981, se acordó la inclusión de este deporte en los Juegos, no obstante, el programa de la Olimpiada de 1984 en Los Ángeles no se pudo alterar, con lo que solo pudo ser deporte de exhibición y hubo que esperar hasta los Juegos de Seúl de 1988 para que al fin figurara en el programa oficial de los juegos.

La ITTF establece la reglamentación oficial del tenis de mesa a nivel mundial. De acuerdo con esta reglamentación se indican, de manera resumida, algunas de estas normas:

De acuerdo con las normas establecidas por la ITTF, la superficie superior de la mesa, conocida como superficie de juego, será rectangular, con una longitud de 2,74 m y una anchura de 1,525 m, y estará situada en un plano horizontal a 76 cm del suelo.

La superficie de juego puede ser de cualquier material y proporcionará un bote uniforme de unos 23 cm al dejar caer sobre ella una pelota reglamentaria desde una altura de 30 cm. El color debe ser oscuro, uniforme y mate, con una línea lateral blanca de 2 cm de anchura a lo largo de cada borde de 2,74 m, y una línea de fondo blanca de 2 cm de anchura a lo largo de cada borde de 1,525 m. Estará dividida en dos campos iguales por una red vertical paralela a las líneas de fondo y será continua en toda el área de cada campo.

Para dobles, cada campo estará dividido en dos medios campos iguales por una línea central blanca de 3 mm de anchura y paralela a las líneas laterales.

La pelota es esférica, tiene un diámetro de 40 mm y un peso de 2,7 g. Será de celuloide o de un material plástico similar. La ITTF autoriza únicamente pelotas de color naranja o blanco y de tono mate. Los estampados de las marcas pueden variar ampliamente, dependiendo del fabricante. Para el año 2015, la ITTF aprobó 74 modelos de pelotas para su utilización en competiciones.

El reglamento inicial de la ITTF de diciembre de 1926 establecía que la pelota debería tener una circunferencia de entre 4,5 y 4,75 pulgadas (aproximadamente entre 36 y 38 mm de diámetro). Tras los Juegos Olímpicos de Sídney 2000, a partir de octubre de ese mismo año y con el fin de disminuir la velocidad de juego y hacerlo más atractivo para los espectadores y las transmisiones por televisión, la ITTF incrementó el diámetro de la pelota de 38 a 40 mm.

Para golpear la pelota se emplea una raqueta, que puede ser de cualquier tamaño, forma o peso, aunque la hoja deberá ser plana y rígida y, como mínimo, el 85 % de su grosor será de madera natural. La hoja puede estar reforzada en su interior con una capa adhesiva de un material fibroso como fibra de carbono, fibra de vidrio o papel prensado, pero sin sobrepasar el 7,5 % del grosor total o 0,35 mm, siempre la dimensión inferior.

El lado de la hoja usado para golpear la pelota estará cubierto, bien con goma de picos normal con los picos hacia fuera y un grosor total no superior a 2 mm, o bien con goma sándwich con los picos hacia dentro o hacia fuera y un grosor total no superior a 4 mm. La superficie del recubrimiento de los lados de la hoja, o la de un lado si éste queda sin cubrir, será mate, de color rojo vivo por un lado y negro por el otro.

Los partidos pueden ser individuales o dobles. Después de cada 2 tantos anotados, el receptor o pareja receptora pasará a ser el servidor o pareja servidora, y así hasta el final del juego, a menos que ambos jugadores o parejas hayan anotado 10 tantos o esté en vigor la regla de aceleración. En estos últimos casos, el orden del servicio y de la recepción será el mismo, pero cada jugador servirá tan solo un tanto alternativamente.

En cada juego de un partido de dobles, la pareja que tiene el derecho a servir en primer lugar elegirá cuál de los dos jugadores lo hará primero, y en el primer juego de un partido la pareja receptora decidirá cuál de los dos jugadores recibirá primero; en los siguientes juegos del partido, una vez elegido el primer servidor, el primer receptor será el jugador que le servía en el juego anterior; en cada cambio de servicio, el anterior receptor pasará a ser servidor, y el compañero del anterior servidor pasará a ser receptor.

El jugador o pareja que sirva primero en un juego recibirá en primer lugar en el siguiente juego del partido. En el último juego posible de un partido de dobles, la pareja receptora cambiará su orden de recepción cuando la primera de las parejas anote 5 tantos. El jugador o pareja que comienza un juego en un lado de la mesa comenzará el siguiente juego del partido en el otro lado, y en el último juego posible de un partido, los jugadores o parejas cambiarán de lado después de que el primero de los jugadores o parejas anote 5 tantos.
En los partidos de dobles los jugadores de la pareja tendrán que golpear alternativamente a la pelota (uno primero y otro después). En dobles el saque se realizará cruzado siempre desde el lado derecho del jugador que saca hacia el lado derecho del jugador del equipo contrario incluyéndose el rebote en la línea central como válido.

Ganará un juego el jugador o pareja que primero alcance 11 tantos, excepto cuando ambos jugadores o parejas consigan 10 tantos; en este caso, ganará el juego el primer jugador o pareja que posteriormente obtenga 2 tantos de diferencia (por ejemplo: 12-10). Un partido se disputará al mejor de cualquier número impar de juegos; el número de juegos por partido varía dependiendo de la competición.

Salvo que se hayan anotado ya 18 tantos o más, si un juego no ha finalizado tras 10 minutos de duración (o en cualquier momento a petición de ambos jugadores o parejas) entrará en vigor la regla de aceleración.

A partir de ese momento, cada jugador servirá un tanto por turno hasta el final del juego, y si el jugador o pareja receptora hace 13 devoluciones correctas en una jugada, se anotará el tanto. Una vez que la regla haya entrado en vigor, se mantendrá hasta el final del partido.

El servicio comenzará con la pelota descansando libremente sobre la palma abierta e inmóvil de la mano libre del servidor. Después, el servidor lanzará la pelota hacia arriba lo más verticalmente posible, sin imprimirle efecto, de manera que se eleve al menos 16 cm tras salir de la palma de la mano libre y luego caiga sin tocar nada antes de ser golpeada. Cuando la pelota está descendiendo, el servidor la golpeará de forma que toque primero su campo y después toque directamente el campo del receptor; en dobles, la pelota tocará sucesivamente el medio campo derecho del servidor y del receptor.

Desde el comienzo del servicio hasta que es golpeada, la pelota estará por encima de la superficie de juego y por detrás de la línea de fondo del servidor, y no será escondida al receptor por el servidor o su compañero de dobles ni por nada de lo que ellos vistan o lleven. Tan pronto como la pelota haya sido lanzada, el brazo y la mano libres del servidor se quitarán del espacio existente entre la pelota y la red.

La jugada será anulada si en el servicio la pelota toca el conjunto de la red, siempre y cuando, por lo demás, el servicio sea correcto. También será un tanto nulo si se efectúa el servicio cuando el receptor o pareja receptora no están preparados, siempre que ni el receptor ni su compañero intenten golpear la pelota.

Desde la primera normativa instaurada por la ITTF en 1926, la reglamentación de este deporte ha experimentado numerosos cambios.

Así, por ejemplo, para evitar situaciones como las acontecidas en los campeonatos del mundo de 1936 en Praga, donde dos jugadores tardaron más de dos horas en finalizar un solo punto, o la de un partido entre el jugador francés Michel Haguenauer y el rumano Marin Goldberger que duró más de siete horas y media, en 1948 se instauró para las competiciones internacionales la regla de aceleración, y en 1963 se incorporó al reglamento.
En el año 2000, y con el fin de disminuir la velocidad de juego y hacerlo más atractivo para los espectadores y las transmisiones por televisión, la ITTF incrementó el diámetro de la pelota de 38 a 40 mm. Al año siguiente la ITTF disminuyó el número de tantos necesarios para ganar un juego de 21 a 11 y la rotación en el servicio se redujo de cinco tantos a dos. En las competiciones de nivel nacional es necesario ganar tres juegos de 11 tantos para ganar un partido, frente a los dos juegos de 21 tantos que se utilizaba con la reglamentación anterior, y a nivel internacional, es necesario ganar cuatro juegos, en lugar de los tres de veintiún puntos anteriores. Este cambio hizo que todos los puntos fueran importantes, haciendo el juego más emocionante ya desde el primer tanto.

En 2003, la reglamentación del servicio se complementa con la prohibición de ocultar la pelota durante el servicio; tan pronto como la pelota haya sido lanzada, el brazo y la mano libres del servidor se quitarán del espacio existente entre la pelota y la red. Anteriormente era posible situar el brazo que no sostiene la raqueta delante de la pelota durante el servicio, ocultando al jugador que restaba el impacto de la raqueta con la pelota. Esta modificación permite percibir con mucho más detalle los servicios, incluso por parte de los espectadores, y promover un juego basado en el intercambio de golpes en lugar del basado en los servicios.

Desde septiembre de 2008 se prohibió la utilización de colas con disolventes orgánicos volátiles (SOV). La utilización de ese tipo de colas aumentaba la velocidad, pero los disolventes utilizados para su producción eran peligrosos para la salud.

La vestimenta normal de juego consistirá normalmente en un polo de manga corta o sin mangas, un pantalón corto o una falda o vestido deportivo de una pieza, calcetines y zapatillas de deporte; durante el juego no podrán usarse otras prendas, como parte o la totalidad de un chándal, a no ser que el juez árbitro lo permita. No está permitido utilizar prendas del mismo color de la pelota de juego utilizada.

Las prendas utilizadas pueden llevar números o rótulos en la espalda de la camiseta para identificar a un jugador, su Asociación o su club, y publicidad. Las marcas o ribetes situados en la parte frontal o lateral de una prenda de juego, así como los objetos que lleve un jugador, como por ejemplo joyas, no serán demasiado llamativos ni emitirán un brillo reflectante que deslumbre al oponente, y no se pueden utilizar prendas con dibujos o rótulos que pudieran resultar ofensivos o desacreditar el juego.


Las principales competiciones internacionales de tenis de mesa son:




En la siguiente tabla se muestran los jugadores —hombres y mujeres— que han conseguido al menos cinco victorias individuales en sus participaciones en dichas competiciones:
A nivel continental se desarrollan competiciones como el Campeonato Latinoamericano, los Juegos Panamericanos, el Campeonato Norteamericano, los Juegos Asiáticos o el Campeonato de Europa. También se disputan importantes ligas continentales por clubes, como la Champions League, que se viene celebrando entre clubes de toda Europa desde 1998.

A los jugadores federados se les asigna una clasificación que refleja su nivel de juego, especialmente para su distribución en diferentes categorías en las competiciones o torneos. El sistema de clasificación es diferente en cada país; para las competiciones internacionales, la ITTF publica un "ranking" cada mes, basado en los resultados de los jugadores en las competiciones oficiales.





</doc>
<doc id="5343" url="https://es.wikipedia.org/wiki?curid=5343" title="Reino Unido">
Reino Unido

El Reino Unido (), oficialmente el Reino Unido de Gran Bretaña e Irlanda del Norte (), o de forma abreviada RU () es un país soberano e insular ubicado al noroeste de la Europa Continental. Su territorio está formado geográficamente por la isla de Gran Bretaña, el noreste de la isla de Irlanda y pequeñas islas adyacentes. Desde la independencia de la República de Irlanda, Irlanda del Norte ha sido la única parte del país con una frontera terrestre, hasta la inauguración del Eurotúnel que une por tierra a la isla de Gran Bretaña con Francia y las tierras continentales europeas. Gran Bretaña limita al norte y al oeste con el océano Atlántico, al este con el mar del Norte, al sur con el canal de la Mancha y al oeste con el mar de Irlanda.

El Reino Unido es un Estado unitario comprendido por cuatro naciones constitutivas: Escocia, Gales, Inglaterra e Irlanda del Norte. Es gobernado mediante un sistema parlamentario con sede de gobierno y capitalidad en Londres, pero con tres administraciones nacionales descentralizadas en Edimburgo, Cardiff y Belfast, las capitales de Escocia, Gales e Irlanda del Norte, respectivamente. Es una monarquía parlamentaria, siendo Isabel II la jefa de Estado. Coloquial y erróneamente se denomina Gran Bretaña e Inglaterra, consecuencia del mayor peso de ambos (territorio y reino respectivamente) dentro del Estado. Las dependencias de la Corona de las islas del Canal —Jersey y Guernsey— y la Isla de Man no forman parte del Reino Unido, si bien el Gobierno británico es responsable de su defensa y las relaciones internacionales.

El Reino Unido tiene catorce territorios de ultramar, todos ellos vestigios de lo que fue el Imperio británico, que en su territorio internacional llegó a alcanzar y a abarcar cerca de una quinta parte de la superficie terrestre mundial. Isabel II continúa estando a la cabeza de la Mancomunidad de Naciones y siendo jefe de Estado de cada uno de los Reinos de la Mancomunidad.

Es un país desarrollado que por su volumen neto de producto interno bruto es la quinta economía mundial. Fue el primer país industrializado del mundo y la principal potencia mundial durante el y el comienzo del (1815-1945), pero el costo económico de las dos guerras mundiales y el declive de su imperio en la segunda parte del disminuyeron su papel en las relaciones internacionales. Sin embargo, aún mantiene una significativa influencia económica, cultural, militar y política, y es una potencia nuclear. Fue miembro de la Unión Europea entre 1973 y 2020. Es uno de los cinco miembros permanentes del Consejo de Seguridad de Naciones Unidas con derecho a veto, miembro del G7, el G-20, la OTAN, la OCDE, la UKUSA, la Mancomunidad de Naciones y la Common Travel Area.

El nombre oficial del país es Reino Unido de Gran Bretaña e Irlanda del Norte (en inglés: United Kingdom of Great Britain and Northern Ireland), siendo Reino Unido y RU las formas abreviadas más utilizadas en español. El nombre fue propuesto por primera vez en el Acta de Unión de 1707, en la que los reinos de Inglaterra y Gales decidieron constituir un nuevo reino junto con Escocia, que tendría el nombre de Reino Unido de Gran Bretaña. Más tarde, con el Acta de Unión de 1800 la isla de Irlanda pasó a formar parte del país, por lo que el nombre cambió a Reino Unido de Gran Bretaña e Irlanda. En 1927, cuando la República de Irlanda obtuvo su independencia, el país obtuvo su nombre actual Reino Unido de Gran Bretaña e Irlanda del Norte.

Es denominado frecuentemente por el nombre de la isla que comprende la mayor parte de su territorio, Gran Bretaña, o también, por extensión, por el nombre de uno de sus países constituyentes, Inglaterra. El gentilicio del Reino Unido, así como el de la isla de Gran Bretaña es "británico", aunque también, por extensión, se suele usar en el habla corriente el gentilicio "inglés".

Aunque el Reino Unido, como Estado soberano, es un país, Inglaterra, Escocia, Gales, y en menor medida, Irlanda del Norte, también se consideran como "los países", a pesar de que no son Estados soberanos. La página web del primer ministro británico ha utilizado la expresión "países dentro de un país" para describir al Reino Unido.

Algunos resúmenes estadísticos también se refieren a los países de Inglaterra, Escocia y Gales como "regiones", mientras que a Irlanda del Norte se le conoce como "provincia".

Los primeros asentamientos por seres humanos anatómicamente modernos en el actual territorio del Reino Unido se produjo en oleadas hace aproximadamente 30 000 años. Se cree que, hacia fines del período prehistórico de la región, la población pertenecía a la cultura de los celta insulares, que comprende a los britanos y a la Irlanda gaélica. La conquista romana, iniciada en el año 43 sometió al sur de la isla a ser una provincia del imperio por cuatro siglos. A esto, le siguió una serie de invasiones encabezadas por distintos pueblos germánicos —anglos, sajones y jutos—, que redujo el área británica hacia lo que iba erigirse como el actual territorio de Gales, Cornualles y el histórico Reino de Strathclyde. La mayor parte de la región colonizada por los anglosajones se unificó en el Reino de Inglaterra en el . Al mismo tiempo, los gaélico-hablantes en el noroeste de Bretaña —con conexiones hacia el nordeste de Irlanda y tradicionalmente se supone que han migrado desde allí en el — se unieron con los pictos para crear el denominado Reino de Escocia en el .

En 1066, los normandos invadieron Inglaterra desde Francia y después de su conquista, tomaron el poder de grandes partes de Gales, Irlanda y fueron invitados a establecerse en Escocia, introduciendo al feudalismo de cada país el modelo norteño-francés y la cultura normanda. La élite normanda influenció en gran medida, pero fue asimilada con cada una de las culturas locales. Por consiguiente, los reyes medievales ingleses conquistaron Gales y realizaron un intento fallido para anexar a Escocia a su territorio. Tras la Declaración de Arbroath, Escocia mantuvo su estatus soberano, a pesar de las constantes tensiones con Inglaterra. Los monarcas ingleses, debido a la herencia que poseían sobre vastos territorios en Francia y por las reclamaciones a la corona francesa, mantuvieron varios conflictos en Francia, siendo el más notable de ellos la Guerra de los Cien Años. En ella, Escocia se alió con Francia y finalizó en 1453, con la retirada inglesa de tierras francesas.

La Edad Moderna estuvo marcada por conflictos religiosos en torno a la reforma protestante, donde se produjo a partir de allí la introducción de las iglesias protestantes estatales en cada país. Gales fue incorporado totalmente al Reino de Inglaterra, e Irlanda fue constituido como reino en unión personal con la corona inglesa. Dentro del actual territorio norirlandés, las tierras de la nobleza católica gaélica independiente fueron confiscadas y dadas a los colonos protestantes de Inglaterra y Escocia.

En 1603, Jacobo VI de Escocia heredó la corona de Inglaterra e Irlanda, lo cual unió a los tres reinos y se trasladó su corte desde Edimburgo a Londres; no obstante, cada país seguía siendo una entidad política independiente, al mismo tiempo que conservaban sus instituciones políticas, legales y religiosas separadas.

A mediados del , los tres reinos estuvieron involucrados en una serie de guerras —incluyendo la Guerra Civil Inglesa— que desencadenaron en el derrocamiento temporal de la monarquía y el establecimiento de una república unitaria de la Mancomunidad de Inglaterra, Escocia e Irlanda. Durante los siglos y , se reportaron actos de piratería (corsario) de la flota británica, atacando y robando buques de las costas europeas y caribeñas.

Pese a restauración de la monarquía en 1660, el interregno aseguró, tras la Revolución gloriosa (1688) y la Declaración de Derechos de 1689 (en inglés, "Bill of Rights") y la Ley de Derecho, que a diferencia de los demás países europeos, el absolutismo real no prevalecería, y que un profesado como católico jamás podría acceder al trono. La constitución británica se desarrollaría sobre la base de una monarquía constitucional y un sistema parlamentario. Con la fundación de la Royal Society en 1660, el estudio de la ciencia aumentó notablemente. Durante este período, particularmente en Inglaterra, el desarrollo de la armada inglesa —dentro del contexto de la denominada «era de los descubrimientos») condujo a la adquisición y liquidación de colonias de ultramar, particularmente en América del Norte.

El 1 de mayo de 1707, se creó el Reino de Gran Bretaña por medio de la unión política celebrada entre el Reino de Inglaterra (del que formaba parte Gales) y el Reino de Escocia. Este evento fue el resultado del Tratado de Unión firmado el 22 de julio de 1706 y ratificado por los parlamentos inglés y escocés para crear el Acta de Unión de 1707. Casi un siglo después, el Reino de Irlanda, bajo el dominio inglés desde 1691, se unió con el Reino de Gran Bretaña para formar el Reino Unido de Gran Bretaña e Irlanda, según lo estipulado en el Acta de Unión de 1800. Aunque Inglaterra y Escocia habían sido Estados separados antes de 1707, habían permanecido en una unión personal desde 1603, cuando se llevó a cabo la Unión de las Coronas.

En su primer siglo de existencia, el país desempeñó un papel importante en el desarrollo de las ideas occidentales sobre el sistema parlamentario, además de que realizó contribuciones significativas a la literatura, las artes y la ciencia. La Revolución Industrial, liderada por el Reino Unido, transformó al país y dio sustento al creciente Imperio británico. Durante este tiempo, al igual que otras potencias, estuvo involucrado en la explotación colonial, incluyendo el comercio de esclavos en el Atlántico, aunque con la aprobación de la Ley de esclavos en 1807, el país fue uno de los pioneros en la lucha contra la esclavitud.

Después de la derrota de Napoleón Bonaparte en las Guerras Napoleónicas, la nación emergió como la principal potencia naval y económica del y continuó siendo una potencia eminente hasta el . La capital, Londres, fue la ciudad más grande del mundo desde 1831 hasta 1925. El Imperio británico alcanzó su máxima extensión en 1921, cuando después de la Primera Guerra Mundial, la Sociedad de Naciones le otorgó el mandato sobre las antiguas colonias alemanas y posesiones otomanas, las últimas como parte de la partición del Imperio otomano. Un año más tarde, se creó la Compañía de Radiodifusión Británica (British Broadcasting Company), que posteriormente se convirtió en la British Broadcasting Corporation (BBC), la primera radiodifusora a gran escala de todo el mundo.

En 1921, los conflictos internos en Irlanda sobre las demandas para un gobierno autónomo irlandés, finalmente condujeron a la partición de la isla. Al mismo tiempo, la victoria del partido Sinn Féin en las elecciones generales de 1918, seguida por una guerra de independencia, llevaron a la creación del Estado Libre Irlandés; Irlanda del Norte optó por seguir formando parte del Reino Unido. Como resultado, en 1927 el nombre formal del Reino Unido de la Gran Bretaña e Irlanda cambió a su nombre actual, el Reino Unido de Gran Bretaña e Irlanda del Norte. La Gran Depresión, estalló en un momento en el que el país todavía estaba lejos de recuperarse de los efectos de la Primera Guerra Mundial.

El Reino Unido formó parte con Estados Unidos, la Unión Soviética y Francia de entre los aliados de la Segunda Guerra Mundial. Tras la derrota de sus aliados europeos en el primer año de la guerra, el ejército británico continuó la lucha contra Alemania en una campaña aérea conocida como la batalla de Inglaterra. Después de la victoria, el país fue una de las tres grandes potencias que se reunieron para planificar el mundo de la posguerra. La Segunda Guerra Mundial dejó la economía nacional dañada. Sin embargo, gracias a la ayuda del plan Marshall y a los costosos préstamos obtenidos de los Estados Unidos y Canadá, la nación comenzó el camino de la recuperación.

Los años inmediatos a la posguerra vieron el establecimiento del Estado del bienestar, incluyendo uno de los primeros y más grandes servicios de salud pública del mundo. Los cambios en la política del gobierno también atrajeron a personas de toda la Mancomunidad, naciendo un Estado multicultural. A pesar de que los nuevos límites del papel político británico fueron confirmados por la Crisis de Suez de 1956, la propagación internacional del idioma inglés significó la influencia permanente de su literatura y su cultura, mientras que desde la década de 1960, su cultura popular también comenzó a tener gran influencia en el extranjero.

Tras un período de desaceleración económica mundial y los conflictos industriales de la década de 1970, el siguiente decenio vio la sustancial afluencia de ingresos obtenidos por la venta del petróleo del mar del Norte y el crecimiento económico. El mandato de Margaret Thatcher marcó un cambio significativo en la dirección del consenso político y económico de la posguerra; un camino que desde 1997 siguieron los gobiernos laboristas de Tony Blair y Gordon Brown. En 1982, hubo una breve guerra contra Argentina en las Malvinas que concluyó con victoria británica. En los años 80 hubo varias tragedias en estadios de fútbol provocadas, entre otros motivos por el apogeo del fenómeno hooligan, como la Tragedia de Heysel, la Tragedia de Valley Parade y la Tragedia de Hillsborough. En 1988, la plataforma petrolífera Piper Alpha, situada en el Mar del Norte, explotó y murieron 167 personas. Ese mismo año sucedió el atentado terrorista más sangriento cometido en Europa, cuando una bomba estalló en el interior del vuelo 103 de Pan Am y mató a 270 personas.
El Reino Unido fue uno de los doce miembros fundadores de la Unión Europea en su inicio en 1992 con la firma del Tratado de Maastricht. Con anterioridad, desde 1973 había sido miembro de la precursora de la Unión Europea, la Comunidad Económica Europea (CEE). El fin del vio cambios importantes en el gobierno británico, con el establecimiento de las administraciones descentralizadas conferidas para Irlanda del Norte, Escocia y Gales.

El 16 de septiembre de 1992 se produjo el episodio llamado "miércoles negro" cuando unos especuladores financieros, entre otros, George Soros, apostaron contra la libra esterlina provocando unas perdidas multimillonarias al estado inglés, el colapso del Banco de Inglaterra y obligando a este a retirarse del Mecanismo Europeo de Cambio de divisas.

En 1997 Reino Unido transfiere la soberanía de Hong Kong a China. Ese mismo año la muerte de Diana de Gales en un accidente automovilístico conmociona a todo el país. En 1998, tras casi dos años de negociaciones, se firmó el acuerdo de Viernes Santo Para dicho acuerdo actuó como mediador el entonces presidente estadounidense Bill Clinton, consumándose el proceso de paz en Irlanda del Norte y alto el fuego del grupo terrorista IRA, poniendo fin al conflicto de Irlanda del Norte (llamado por los ingleses "The Troubles" es decir, ["Los Problemas"]).
La política exterior durante el gobierno de Tony Blair (1997-2007) fue de un estrecho alineamiento con los Estados Unidos. Tras la participación del Reino Unido en la Operación Libertad Duradera en Afganistán iniciada en 2001, Blair tomo parte de la cumbre de las Azores en 2003 donde se adoptó la decisión de lanzar un ultimátum de 24 horas al régimen iraquí encabezado por Saddam Hussein para su desarme. Este ultimátum finalmente desembocó en la invasión de Irak ("Operación Libertad Iraquí") en 2003.

El terrorismo islámico golpeó Londres el 7 de julio de 2005 provocando 56 muertos y más de 700 heridos, el día siguiente de que Londres fuera la sede elegida para albergar los Juegos Olímpicos de Londres 2012.

La crisis financiera de 2008 afectó severamente la economía británica. Dos años después, los laboristas de Gordon Brown pierden las elecciones y asciende el gobierno conservador encabezado por David Cameron, que introdujo nuevas medidas de austeridad destinadas a hacer frente a los déficits públicos sustanciales que se dieron durante el período de crisis. En 2014, el Gobierno escocés celebró un referéndum para la independencia de Escocia en septiembre de ese año, siendo rechazada la propuesta de independencia con un 55 % de los votos. El 9 de septiembre del año 2015, la reina Isabel II se convirtió en la monarca con más tiempo de reinado en el país, habiendo superado así a su propia tatarabuela, la reina Victoria I.

En junio de 2016 se celebró un referéndum sobre la permanencia del Reino Unido en la Unión Europea con un 51.9 % de votos a favor de dejar la entidad europea, proceso que podría demandar hasta dos años y que inició oficialmente el 29 de marzo de 2017. Como parte de la coalición antiyihadista en la guerra contra el Estado Islámico, el Reino Unido volvió a ser golpeado ese año por el terrorismo en ciudades como Londres y Mánchester.

El 31 de enero de 2020 se hizo efectivo el "Brexit", la salida del Reino Unido de la Unión Europea.

El Reino Unido es una monarquía parlamentaria cuya jefa de Estado es Isabel II. Asimismo, es la jefa de Estado de los otros quince países de la Mancomunidad de Naciones, situando al Reino Unido en una unión personal con aquellas naciones. La reina tiene la soberanía sobre las dependencias de la Corona, la isla de Man y los bailiazgos de Jersey y Guernsey. Estos no forman parte del Reino Unido, aunque el Gobierno británico gestiona sus relaciones exteriores y la defensa, además de que el parlamento tiene autoridad para legislar en su nombre.

El Reino Unido no tiene un documento que sirva como constitución totalmente definida, algo que solo ocurre en otros dos países del mundo, Israel y Nueva Zelanda. La constitución del Reino Unido, por lo tanto, consiste principalmente en una colección de diferentes fuentes escritas, incluyendo leyes, estatutos, jurisprudencias y tratados internacionales. Como no hay ninguna diferencia técnica entre los estatutos ordinarios y la "ley constitucional", el parlamento puede realizar una "reforma constitucional" por el simple hecho de aprobar una ley, y en consecuencia, tiene el poder para cambiar o suprimir casi cualquier elemento escrito o no escrito de la constitución. Sin embargo, existen ciertas limitaciones para la aprobación de las leyes, por ejemplo, ninguna legislatura puede crear leyes que no se puedan cambiar en un futuro.

El Reino Unido cuenta con un gobierno parlamentario, basado en el sistema Westminster, el cual ha sido emulado alrededor del mundo, uno de los legados del Imperio británico. El parlamento del Reino Unido, que se reúne en el Palacio de Westminster tiene dos cámaras: la Cámara de los Comunes (elegida por el pueblo) y la Cámara de los Lores. Cualquier ley aprobada por el parlamento requiere el consentimiento real para convertirse en ley. El hecho de que el parlamento descentralizado en Escocia y las asambleas en Irlanda del Norte y Gales no sean órganos soberanos y puedan ser abolidos por el parlamento británico, hace que este último sea el órgano legislativo más importante en el país.

El puesto del jefe de gobierno del Reino Unido, el primer ministro, lo ocupa el miembro del parlamento que obtiene la mayoría de votos en la Cámara de los Comunes, por lo general es el líder del partido político con más asientos en dicha cámara. El primer ministro y el gabinete son nombrados por el monarca para formar el "Gobierno de Su Majestad", aunque el primer ministro elige al Consejo de Ministros, y por convención, el monarca respeta su elección.

Tradicionalmente, el gabinete se conforma de miembros del mismo partido del primer ministro de ambas cámaras legislativas, en su mayoría de la Cámara de los Comunes. El poder ejecutivo es ejercido por el primer ministro y el gabinete, quienes hacen su juramento delante del rey, para formar parte del Consejo Privado, de tal modo que se convierten en Ministros de la Corona. En las elecciones de 2010, el líder del Partido Conservador, David Cameron, puso fin a los trece años del mandato laborista y asumió el papel de primer ministro. Cameron pudo repetir este éxito en las elecciones generales de 2015, en donde el Partido Conservador obtuvo mayoría absoluta.

Las elecciones generales son convocadas por el monarca. Aunque no existe ningún plazo mínimo para ocupar un puesto en el parlamento, la Ley del Parlamento de 1911 exige que se debe llamar a una nueva elección dentro del plazo de cinco años después de las elecciones anteriores. Anteriormente, para las elecciones a la Cámara de los Comunes, el territorio nacional se dividía en 646 distritos electorales, con 529 en Inglaterra, 18 en Irlanda del Norte, 59 en Escocia y 40 en Gales; este número aumentó a 650 en las elecciones generales del 2010. Cada distrito electoral elige a un miembro del parlamento por mayoría simple.

El Partido Conservador, el Partido Laborista y el Partido Nacional Escocés (el cual se presenta sólo en Escocia), son los principales partidos políticos; en las elecciones generales de 2015 ganaron 619 de los 650 escaños disponibles en la Cámara de los Comunes. La mayoría de los escaños restantes fueron ganados por partidos que, al igual que el Partido Nacional Escocés, solo compiten en una parte del país, como el Partido de Gales (solo en Gales), el Partido Unionista Democrático, el Partido Socialdemócrata y Laborista, el Partido Unionista del Ulster y el Sinn Féin (solo en Irlanda del Norte, aunque el Sinn Féin también compite en las elecciones en Irlanda), además de los Liberal Demócratas (los cuales se presentan a nivel nacional y obtuvieron 8 escaños). Para las elecciones al Parlamento Europeo, el Reino Unido tiene actualmente 72 diputados elegidos por voto en bloque. Las dudas sobre la verdadera soberanía de cada nación constitutiva surgieron tras la adhesión del Reino Unido a la Unión Europea.

El país no tiene un sistema jurídico único, ya que fue creado por la unión política de los países anteriormente independientes y el artículo 19 del Tratado de la Unión de 1707 garantiza la existencia por separado del sistema legal escocés. Hoy en día, el país tiene tres diferentes sistemas jurídicos: el derecho de Inglaterra, el derecho de Irlanda del Norte y la ley escocesa. En octubre de 2009, los recientes cambios constitucionales trajeron consigo la creación de una nueva Corte Suprema para asumir las funciones de apelación de la Comisión de Apelación de la Cámara de los Lores. El Comité Judicial del Consejo Privado es el tribunal de apelación más alto para varios países independientes de la Mancomunidad, los territorios de ultramar y las dependencias de la Corona británica.

El Reino Unido pertenece a varias organizaciones internacionales como lo son la Organización de las Naciones Unidas, la Mancomunidad de Naciones, el G-8, el G-7, el G-20, la Organización del Tratado del Atlántico Norte, la Organización para la Cooperación y el Desarrollo Económico, la Organización Mundial del Comercio, el Consejo de Europa, la Organización para la Seguridad y la Cooperación en Europa. Además es uno de los miembros permanentes del Consejo de Seguridad de Naciones Unidas con derecho de veto.
Este abandono definitivamente la Unión Europea el 31 de enero de 2020, lo que lo convirtió en frontera exterior de la misma. 
Este proceso, conocido popularmente como Brexit, estaba previsto que culminara en mayo de 2019. Sin embargo, debido a sucesivas ampliaciones a lo largo del año, por falta de acuerdo en el Parlamento Británico, fue efectiva a inicios de 2020. 

La alianza más notable entre el Reino Unido con otro país es su "relación especial" con los Estados Unidos, aunque también mantiene relaciones estrechas con varios miembros de la Unión Europea, de la OTAN, de la Mancomunidad y con otros países poderosos como Japón. La presencia global y la influencia británica se amplifican aún más a través de sus relaciones comerciales, su ayuda oficial al desarrollo y sus fuerzas armadas, que mantienen cerca de ochenta instalaciones militares y otras implementaciones alrededor del mundo.

El Ejército, la Marina Real y la Real Fuerza Aérea británica se conocen colectivamente como las Fuerzas Armadas británicas. Las tres fuerzas son administradas por el Ministerio de Defensa y controladas por el Consejo de Defensa, presidido por el Secretario de Estado para la Defensa. Las tropas británicas son unas de las que cuentan con un mejor entrenamiento, además de ser las más avanzadas tecnológicamente. Según diversas fuentes, incluyendo el Ministerio de Defensa, el Reino Unido tiene el tercer o cuarto presupuesto más alto para gastos militares a nivel internacional, a pesar de contar solo con el 25.º ejército más grande en términos de personal. Actualmente, el gasto total en defensa representa el 2,5 % del PIB.

La Marina Real es una flota de agua azul, una de las tres que sobreviven, junto con la Marina Nacional francesa y la Armada de los Estados Unidos. El 3 de julio de 2008, el Ministerio de Defensa firmó varios acuerdos con un valor de 3,2 millones de £ para construir dos nuevos portaaviones. El Reino Unido es uno de los de cinco países (junto con Estados Unidos, China, Rusia y Francia) que puede estar en posesión de armas nucleares, utilizando un submarino de clase Vanguard, que cuenta con el sistema de misiles balísticos de Trident II D5.

Entre las principales funciones de las Fuerzas Armadas británicas se encuentran la protección y defensa del Reino Unido y sus territorios de ultramar, la promoción de los intereses de seguridad global y el apoyo a los esfuerzos internacionales por mantener la paz. Además, son participantes activos y regulares en la OTAN, la ONU y en otros organismos internacionales que buscan la resolución pacífica de los conflictos. Existen varias guarniciones de ultramar e instalaciones del ejército británico alrededor del mundo, principalmente en la Isla Ascensión, Belice, Brunéi, Canadá, Diego García, las Islas Malvinas/Falkland, Alemania, Gibraltar, Kenia, Chipre y Catar.

En 2010, el Ejército Británico reportó que contaba con 197 840 militantes. Aparte, están los cuerpos de las Fuerzas Especiales de Reino Unido, las Fuerzas de Reserva y las Fuerzas de Auxilio Real. Con esto, la cifra de soldados se eleva a 435 500, incluyendo al personal activo y de reserva. A pesar de las capacidades militares del Reino Unido, una política reciente sobre cuestiones de defensa asume que "las operaciones más exigentes" podrían llevarse a cabo como parte de una coalición. Dejando a un lado la intervención en Sierra Leona, las operaciones en Bosnia y Herzegovina, Kosovo, Afganistán e Irak pueden ser tomadas como precedentes de esta política. De hecho, la última guerra en la que el Ejército Británico luchó por su propia cuenta fue durante la guerra de las Malvinas en 1982, en la que derrotó al Ejército Argentino.

La organización territorial del Reino Unido es compleja y muy variada, ya que cada país constituyente tiene su propio sistema de demarcación geográfica y administrativa con orígenes anteriores a la unión entre ellos. En consecuencia, no hay «ninguna unidad administrativa en común entre los integrantes del Reino Unido». Hasta el se realizaron pocos cambios a estas administraciones, pero desde entonces ha habido una evolución constante de su papel y función. El cambio no ocurrió de manera uniforme en las naciones constitutivas, y la devolución del poder sobre la administración local a Escocia, Gales e Irlanda del Norte, hace que sea poco probable que los cambios administrativos futuros sean uniformes.

La organización del gobierno local en Inglaterra es compleja, debido a que la distribución de funciones varía de acuerdo a las disposiciones locales. La legislación local se lleva a cabo por el Parlamento británico y el gobierno del Reino Unido, porque Inglaterra no cuenta con un parlamento descentralizado. El nivel superior de las subdivisiones de Inglaterra son las nueve oficinas regionales de gobierno. Desde 2000, la región de Londres cuenta con una asamblea electa y con un alcalde, después del gran apoyo dado a dicha propuesta en el referéndum de Londres de 1998. Se pretendía que las otras regiones también contaran con su propia asamblea regional, pero el rechazo a esta idea en un referéndum realizado en 2004 en la región Nordeste de Inglaterra detuvo la reforma. Por debajo del nivel de la región, Londres se conforma por 32 municipios y el resto de Inglaterra tiene consejos de distrito y diputaciones o autoridades unitarias. Los concejales son elegidos por sufragio directo, mediante voto sencillo o por bloque.

El gobierno local de Escocia se divide en 32 áreas de consejos, que tienen una amplia variación tanto en tamaño como en población. Las ciudades de Glasgow, Edimburgo, Aberdeen y Dundee son áreas de consejo especiales, así como el área de consejo de Highland, que incluye una tercera parte de la superficie de Escocia, pero solo poco más de 200 000 personas. El poder conferido a las autoridades locales es administrado por los concejales elegidos, que son actualmente 1222. Las elecciones se llevan a cabo por voto único transferible, mediante elecciones en bloque de tres o cuatro concejales. Cada Consejo elige a un Administrador o un Coordinador General para presidir las reuniones del Consejo y para actuar como el representante de la zona. Los concejales están sujetos a un código de conducta impuesto por la Comisión de Normas para Escocia. La organización representante de los funcionarios locales es la Convención de Autoridades Locales Escocesas (COSLA).

Desde 1973, el gobierno local en Irlanda del Norte se organiza en 26 consejos de distrito, en donde se celebran elecciones de voto único transferible, para elegir representantes con poderes limitados a servicios, como ser la recolección de residuos y el mantenimiento de parques y lugares públicos. Sin embargo, el 13 de marzo de 2008, el poder ejecutivo propuso la creación de once consejos nuevos para reemplazar el sistema actual y las próximas elecciones locales se postergarán hasta el 2011 para facilitar este proceso.

Por último, el gobierno local en Gales consta de 22 autoridades unitarias, incluyendo las ciudades de Cardiff, Swansea y Newport, que son autoridades unitarias independientes. Las elecciones se celebran cada cuatro años por sufragio directo. La Asociación del Gobierno Local de Gales representa a los intereses de las autoridades locales galesas.

Los territorios británicos de ultramar son catorce territorios dependientes del Reino Unido, pero que no conforman parte de él. Principalmente, se trata de pequeñas islas poco pobladas que representan los vestigios del antiguo Imperio británico. Juntos, representan un área que supera los 1 728 000 km² y una población de aproximadamente 260 000 personas. Sin embargo, el más extenso de ellos (1 709 400 km², equivalente al 98,9% de los territorios de ultramar) es el Territorio Antártico Británico, que solo es reconocido por otros cuatro países, mientras que la mayoría de los signatarios del Tratado Antártico. no reconocen soberanía británica sobre ese territorio y lo tratan apenas como una reclamación británica, mientras que otros dos países firmantes, Chile y Argentina, tienen sus propias reclamaciones. El territorio antártico reivindicado por Reino Unido se superpone parcialmente con el área reclamada por Chile (Territorio Chileno Antártico) y totalmente con la reclamada por Argentina (Antártida Argentina), al punto que este desacuerdo llevó a tensiónes diplomáticas, presiones e incidentes (como el de Isla Decepsión o el de Bahía Esperanza) en años anteriores a la firma del tratado, que pospone la resolución del asunto.. 

Las dependencias de la Corona británica son tres territorios semidependientes del monarca del Reino Unido, pero que tampoco forman parte del país. A diferencia de los territorios de ultramar, la legislación y otros asuntos de interés local corresponden a una asamblea legislativa local; además, los tratados internacionales y las normas de carácter nacional solo son aplicadas si esta asamblea las aprueba. Estas dependencias ocupan cerca de 779 km² y tienen una población de más de 235 700 habitantes.

El Reino Unido tiene 244 820 km² de superficie. que comprenden la isla de Gran Bretaña y la parte nororiental de la isla de Irlanda (Irlanda del Norte) y otras islas más pequeñas. El país se encuentra entre el océano Atlántico y el mar del Norte, a 35 kilómetros de la costa noroeste de Francia, de la que se encuentra separado por el canal de la Mancha.

Gran Bretaña se ubica entre las latitudes 49° y 59° N (las islas Shetland se extienden casi a los 61° N) y las longitudes 8° O a 2° E. El observatorio de Greenwich, en Londres, es el punto de definición para el meridiano de Greenwich. Cuando se mide directamente de norte a sur, Gran Bretaña mide poco más de 1100 kilómetros de longitud y poco menos de 500 kilómetros en su parte más ancha. Sin embargo, la mayor distancia entre dos puntos en la isla es de 1350 kilómetros entre el final de la tierra en Cornualles (cerca de Penzance) y John o' Groats en Caithness (cerca de Thurso). Irlanda del Norte comparte una frontera de tierra de 443 km con la República de Irlanda.

Inglaterra acapara poco más de la mitad de la superficie total del Reino Unido, con 130 410 kilómetros cuadrados de superficie. La mayor parte del país consiste de tierras bajas, con un poco de terreno montañoso en la zona noroeste, donde se encuentra la línea de Tees-Exe, entre las montañas de Cumbria y los montes Peninos. La montaña más alta de la región es Scafell Pike (978 msnm) y se ubica dentro de esta zona. Los principales ríos y estuarios de Inglaterra son el Támesis, el Severn y el Humber.

Escocia representa menos de un tercio del área total del Reino Unido, cubriendo 78 772 kilómetros cuadrados; esta cifra incluye las casi ochocientos islas, que en su mayoría se encuentran al oeste y al norte de Gran Bretaña, destacando las Hébridas, las islas Orcadas y las islas Shetland. La topografía de Escocia se distingue por la falla de las Highlands, que atraviesa el territorio escocés de Helensburgh a Stonehaven. La falla separa las dos principales regiones escocesas: las tierras altas del norte y oeste y las tierras bajas del sur y este. La región montañosa contiene la mayoría de las montañas de Escocia, incluyendo el Ben Nevis, que con sus 1343 msnm, es el punto más alto en las islas británicas. Las tierras bajas, especialmente la franja estrecha de tierra entre el fiordo de Clyde y el fiordo de Forth conocida como el "Cinturón Central", son más planas y en ellas se encuentra la mayoría de las comunidades escocesas, incluyendo Glasgow, la ciudad más grande de la región, y Edimburgo, la capital y centro político del país.

Gales ocupa menos de una décima parte del total del área del Reino Unido, cubriendo solo 20 758 kilómetros cuadrados. Gales es principalmente montañosa, aunque la zona sur es menos montañosa que el norte y el centro. Por eso, las principales zonas industriales están en Gales del Sur, formadas por las ciudades costeras de Cardiff, Swansea y Newport. Las montañas más altas son las Snowdonia, donde se encuentra el pico más alto de la región: el Snowdon con 1085 msnm. Las catorce (o quince) montañas más altas de Gales sobrepasan los y se conocen comúnmente como las "Gales 3000's". Hay varias islas que se extienden delante de los más de 1200 km de costa, la más grande de ellas es Anglesey (Ynys Môn), ubicada al noroeste del país.

Irlanda del Norte acapara solo 14 160 kilómetros cuadrados y su territorio es en su mayoría montañoso. Se encuentra separado de la isla británica por el mar de Irlanda y el canal del Norte. El pico más alto de esta región es el Slieve Donard con 849 msnm, localizado en los montes de Mourne. En Irlanda del Norte se encuentra el Lough Neagh, que con sus cerca de 388 kilómetros cuadrados, es el cuerpo de agua más grande en el Reino Unido.

El Reino Unido tiene un clima templado y un clima oceánico con abundantes lluvias durante todo el año. La temperatura varía con las estaciones, pero rara vez cae por debajo de -10 °C, o se eleva por encima de los 35 °C. El viento predominante proviene del suroeste, trayendo consigo el clima húmedo y cálido desde el océano Atlántico. La parte oriental se encuentra más protegida de este viento y por lo tanto tiene un clima más seco. Las corrientes atlánticas, calentadas por la corriente del Golfo, hacen que los inviernos no sean tan severos, especialmente en el oeste, donde los inviernos son húmedos. Los veranos son más cálidos en el sureste de Inglaterra, siendo la parte más cercana al continente europeo, y más frescos conforme se avanza hacia el norte. Las nevadas ocurren durante el invierno y la primavera, aunque las nevadas intensas rara vez caen en las tierras bajas.

En el Reino Unido, como resultado del cambio climático, hay una tendencia hacia inviernos más cálidos y veranos más cálidos, el nivel del mar en la costa británica aumenta aproximadamente 3 mm cada año y hay signos de un cambio en los patrones de precipitación. Los científicos del clima esperan que las olas de calor, como las de 2003, se conviertan en la norma en la década de 2040 como resultado de la crisis climática. Los cálculos del modelo de 2019 muestran que Londres sería reubicado en otra zona climática si ocurre el escenario RCP4.5. El clima en Londres en 2050 se parece más al clima anterior en Barcelona que al clima anterior en Londres. Incluso los fenómenos meteorológicos extremos son cada vez más frecuentes e intensos. Se ha demostrado que las inundaciones en Inglaterra 2013-2014 se remontan al cambio climático provocado por el hombre.

En la mayoría de Gran Bretaña hay un clima templado que recibe altos niveles de precipitaciones y niveles medios de insolación. Hacia el norte, el clima se hace más frío y los bosques de coníferas sustituyen en gran medida a las especies caducifolias de los bosques del sur.

Hay algunas variaciones en el clima británico, con algunas áreas con condiciones subárticas tal como ocurre en las Tierras Altas de Escocia y Teesdale, e incluso subtropical en las islas Sorlingas. Los cambios estacionales que se producen en todo el archipiélago condicionan a las plantas que deben hacer frente a los cambios en los niveles de luz solar, precipitación y temperatura, así como el riesgo de nieve y las heladas durante el invierno.

Dentro de la isla existen varios ecosistemas como los bosques templados, pantanos, marismas, etc. El roble, el olmo, el haya, el fresno, el pino y el abedul son algunos de los árboles más comunes dentro de los bosques británicos. Anteriormente, las islas británicas se encontraban repletas de bosques de árboles caducifolios y coníferas, pero para la década de 2000, solamente cerca del 10 % del territorio nacional se encontraba cubierto por bosques, concentrándose en el noreste de Escocia y en el sureste de Inglaterra, debido en gran parte a la tala descontrolada y al crecimiento urbano. El área que rodea a las zonas urbanas está cubierta principalmente por pastos y plantas con flores
La isla de Gran Bretaña, junto con el resto del archipiélago conocido como las islas británicas, alberga una fauna típica de clima templado oceánico, poco diversa si se compara a nivel mundial y similar a la de otros países de Europa del Norte.

Entre los mamíferos que más abundan en el país se incluyen los zorros, conejos, ciervos, erizos, ratones, comadrejas y musarañas. Como otras islas ubicadas en latitudes similares, son escasos los ejemplares de reptiles y anfibios. En todo el territorio nacional se han descubierto más de 21 000 especies de insectos y cerca de 230 especies de aves, algunas de las cuales están amenazadas por la caza y la destrucción de su hábitat. Los principales ríos británicos, como el río Támesis, son la principal fuente de agua para la fauna de los ecosistemas locales, a la vez de que son el hábitat de varias especies de peces y aves acuáticas.

La biodiversidad disminuyó severamente durante la última glaciación, y en poco tiempo (en términos geológicos) se separó del continente por la formación del canal de la Mancha.

El hombre ha perseguido a las especies de mayor tamaño que interferían con sus actividades (el lobo, el oso pardo y el jabalí) hasta provocar su extinción en la isla, aunque por supuesto siguen existiendo las formas domesticadas como el perro y el cerdo. El jabalí se volvió a introducir posteriormente.

Desde mediados del , Gran Bretaña ha sufrido una gran industrialización y aumento de urbanización. Un estudio de DEFRA publicado en 2006 sugirió que 100 especies de animales se han extinguido en el Reino Unido durante el , lo que supone cerca de 100 veces la tasa de extinción de fondo. Esto ha tenido un gran impacto en las poblaciones de animales autóctonos, particularmente en los paseriformes, siendo cada vez más escasas. La pérdida de hábitat ha afectado principalmente a las especies de mamíferos de mayor tamaño. Sin embargo, algunas especies se han adaptado al entorno urbano en expansión, en particular el zorro, la rata, y otros animales como la paloma torcaz.

La economía del Reino Unido se compone (en orden descendente de tamaño) de las economías de Inglaterra, Escocia, Gales e Irlanda del Norte. Basado en las tasas de cambio del mercado, el Reino Unido es la del mundo y la segunda más grande en Europa después de Alemania, por delante de Francia.

La Revolución Industrial se inició en el Reino Unido, en un proceso donde se dio una gran concentración de las industrias pesadas en todo el país, como la construcción naval, la extracción del carbón, la producción de acero y la industria textil. La extensión del Imperio creó un mercado exterior enorme para los productos británicos, permitiendo que la nación dominara el comercio internacional en el . Más tarde, como le sucedió a otras economías industrializadas, junto con el declive económico después de las dos guerras mundiales, el Reino Unido comenzó a perder su ventaja competitiva y la industria pesada disminuyó. Aunque la fabricación sigue siendo una parte importante de la economía, en 2003 solo representaba una sexta parte de los ingresos del país.

La industria automovilística es una parte importante del sector manufacturero, aunque ha disminuido con el colapso del MG Rover Group y actualmente la mayor parte de la industria es propiedad extranjera. La producción de aviones civiles y de defensa es liderada por BAE Systems, el mayor contratista de defensa en el mundo, y por la firma europea EADS, el propietario del Airbus. Rolls-Royce tiene una parte importante del mercado mundial de motores aeroespaciales. La industria química y farmacéutica son importantes en el Reino Unido, ya que las compañías británicas de GlaxoSmithKline y AstraZeneca son la segunda y sexta empresa farmacéutica más grandes del mundo, respectivamente.

Sin embargo, durante las últimas décadas el sector terciario aumentó considerablemente y ahora produce cerca del 73 % del PIB. El sector de servicios está dominado por los servicios financieros, especialmente bancos y aseguradoras. Esto hace a Londres el centro financiero más grande del mundo, ya que aquí se encuentran las sedes de la Bolsa de Londres, el London International Financial Futures and Options Exchange y el Lloyd's of London; además de ser el líder de los tres "centros de comando" de la economía mundial (junto con Nueva York y Tokio). Además, cuenta con la mayor concentración de sucursales de bancos extranjeros en el mundo. En la última década, un centro financiero rival de Londres ha crecido en la zona de Docklands, donde el HSBC, el banco más grande del mundo, y el Barclays reubicaron sus sedes. Muchas empresas multinacionales que no son de propiedad británica han elegido Londres como el lugar para su sede europea o extranjera: un ejemplo es la firma estadounidense de servicios financieros Citigroup. La capital de Escocia, Edimburgo, también es uno de los grandes centros financieros de Europa y es la sede del Royal Bank of Scotland Group, uno de los bancos más importantes del mundo.

El turismo es muy importante para la economía británica. Con los más de 27 millones de turistas que arribaron al país en 2004, el Reino Unido está clasificado como el sexto destino turístico más importante en el mundo. Londres, por un margen considerable, es la ciudad más visitada en el mundo con 15,6 millones de visitantes en 2006, por delante de Bangkok (10,4 millones de visitantes) y de París (9,7 millones). Las industrias creativas aportaron el 7 % del PIB de 2005 y crecieron a una tasa promedio anual del 6 % entre 1997 y 2005.

En julio de 2007, el Reino Unido tenía una deuda pública del 35,5 % del PIB. Esta cifra aumentó a 56,8 % del PIB en julio de 2009. La moneda nacional es la libra esterlina, representada con el símbolo £. El Banco de Inglaterra es el banco central, responsable de la emisión de moneda, aunque los bancos de Escocia e Irlanda del Norte tienen derecho a emitir sus propios billetes. La libra esterlina también se utiliza como moneda de reserva por otros gobiernos e instituciones y es la tercera moneda con mayor cantidad de reservas, después del dólar estadounidense y del euro. El Reino Unido decidió no participar en el lanzamiento del euro como moneda, y el anterior primer ministro británico, Gordon Brown, ha descartado la adopción del euro en un futuro cercano, argumentando que la decisión de no unirse al proyecto había sido la mejor opción para el país y para Europa. El anterior gobierno de Tony Blair se comprometió a celebrar un referéndum público para decidir si el país realizaría las "cinco pruebas económicas". En 2005, más de la mitad de los británicos (55 %) estaban en contra de la adopción del euro como moneda, mientras que solo el 30 % estaban a favor.

El 23 de enero de 2009, cifras de la Oficina Nacional de Estadísticas mostraron que la economía británica estaba oficialmente en recesión por primera vez desde 1991. Se informó que fue en el último trimestre del 2008 cuando la economía cayó en una recesión que fue acompañada por el creciente desempleo, el cual aumentó de 5,2 % en mayo de 2008 a 7,6 % en mayo de 2009. La tasa de desempleo para adultos entre 18 a 24 años, aumentó de 11,9 % a 17,3 % en el mismo periodo.

La línea de pobreza relativa en el Reino Unido se define comúnmente por debajo del 60 % del ingreso promedio. Entre 2007 y 2008, el 13,5 millones de personas, o sea, el 22 % de la población, vivían por debajo de esta línea. Se trata de uno de los niveles de pobreza relativa más altos entre los miembros de la Unión Europea. Después de tomar en cuenta los costos de la vivienda, se demostró que en el mismo lapso 4 millones de niños, 31 % del total, vivían en hogares que estaban por debajo de la línea de pobreza. Esto representa una disminución de 400 000 niños comparado con el periodo entre 1998 y 1999.

Entre 2007 y 2015, el Reino Unido registró la mayor disminución de los salarios reales (ajustados por inflación) de todos los países avanzados, al mismo nivel que Grecia (-10,4%). El Reino Unido tiene la mayor desigualdad de ingresos entre los países de la OCDE y las mayores disparidades regionales de Europa. La parte de los ingresos capturada por el 1% más rico se ha duplicado en los últimos 30 años, pasando de alrededor del 4% a más del 8,5% del producto interno bruto (PIB) en 2018. 

Las principales carreteras británicas forman una red de 46 904 kilómetros, de los cuales más de 3520 kilómetros son autopistas. Además, hay cerca de 213 750 kilómetros de caminos pavimentados. La red ferroviaria, con 16 116 kilómetros en Gran Bretaña y 303 kilómetros en Irlanda del Norte, diariamente transporta más de 18 000 trenes de pasajeros y 1000 trenes de mercancías. Las redes ferroviarias urbanas están bien desarrolladas en Londres y otras ciudades importantes. Llegaron a existir más de 48 000 km de vías férreas en todo el país, sin embargo, la mayoría se redujo entre 1955 y 1975, en gran parte después de un informe del asesor de gobierno Richard Beeching a mediados de la década de 1960 (conocido como el hacha de Beeching). Actualmente se consideran nuevos planes para construir nuevas líneas de alta velocidad para el año 2025.

La Agencia de Carreteras es la agencia ejecutiva responsable de los caminos y autopistas en Inglaterra, aparte de la empresa privada M6 Toll. El Departamento de Transporte afirma que la congestión vehicular es uno de los más graves problemas en materia de transporte y que si no se controla, para el año 2025 podría costarle a Inglaterra más de 22 000 millones de libras esterlinas. De acuerdo con el "Informe Eddington" de 2006 realizado por el gobierno inglés, la congestión está en peligro de perjudicar la economía, a menos que se contrarreste con el cobro de peajes y la expansión de la red de transporte.

Las vías y medios de transporte de Escocia son responsabilidad del Departamento de Transportes del gobierno local, siendo Transportes Escocia la agencia gubernamental encargada del mantenimiento de las carreteras y redes ferroviarias del país. La red de ferrocarriles de Escocia tiene alrededor de 340 estaciones y 3000 kilómetros de vías y transporta a más de 62 millones de pasajeros cada año. En 2008, el gobierno escocés estableció planes de inversión para los próximos 20 años, con prioridades para incluir un nuevo puente en la carretera de Forth y la electrificación de la red ferroviaria.

El aeropuerto de Londres-Heathrow, situado a 24 km al oeste de la capital, es el aeropuerto más concurrido del Reino Unido y tiene el mayor nivel de tráfico de pasajeros internacionales en el mundo. Entre octubre de 2009 y septiembre de 2010, los aeropuertos británicos recibieron a 211.4 millones de pasajeros. Asimismo, es la base de operaciones de aerolíneas como British Airways, Virgin Atlantic y bmi.

La televisión es el principal medio de comunicación en el Reino Unido. Las principales cadenas de carácter nacional son: BBC One, BBC Two, ITV1, Channel 4 y Five. En Gales, S4C es el principal canal en idioma galés.

La BBC es la principal compañía emisora de carácter público del Reino Unido y la más grande y antigua emisora del mundo. Opera varios canales de televisión y estaciones de radio en el país y en el extranjero. El servicio de televisión internacional de la BBC, BBC World News, se retransmite en todo el mundo y el servicio de radio internacional, BBC World Service, emite en treinta y tres idiomas.

En cuanto a la radio, el principal servicio es BBC Radio que cuenta con diez estaciones nacionales, entre las que se encuentran las dos con mayor popularidad: BBC Radio 1 y BBC Radio 2; y cerca de cuarenta estaciones regionales. Además existen servicios en otros idiomas dentro de las fronteras británicas, como BBC Radio Cymru en galés y BBC Radio nan Gàidheal en gaélico escocés; algunos programas de BBC Radio Ulster son emitidos en irlandés para la población norirlandesa. Existen además cientos de estaciones privadas de carácter local.

Internet es otro de los medios de comunicación más importantes en el país, además de que ha tenido un gran aumento desde la última década, de tal modo que con 41 817 847 de usuarios, es el con la mayor cantidad de internautas en el mundo. El dominio de Internet para el Reino Unido es .uk. El sitio web más popular con terminación ".uk" es la versión británica de Google, seguido por la página de la BBC.

"The Sun" es el periódico de mayor circulación nacional, con 3,1 millones de ejemplares diarios, acaparando aproximadamente un cuarto del mercado. Su publicación hermana, "News of the World" era el periódico semanal de mayor circulación, cancelado tras un escándalo de escuchas ilegales, que se centraba normalmente en historias de celebridades. "The Daily Telegraph", un periódico de derecha, es considerado el periódico de "calidad" más vendido en el país. "The Guardian" es otro periódico de "calidad", aunque de tendencia más liberal; "Financial Times" es el principal diario financiero del país, caracterizado por imprimirse en hojas color salmón.

Impreso desde 1737, "The News Letter" de Belfast, es el periódico en inglés más antiguo aún en circulación. Uno de sus competidores norirlandeses, "The Irish News", ha sido calificado como el mejor periódico regional del Reino Unido en varias ocasiones. Además de los periódicos, algunas publicaciones británicas cuentan con circulación internacional, entre los que destacan las revistas "The Economist" y "Nature".

El consumo de energía eléctrica en el país asciende a 345 800 millones de kWh anuales, siendo el de electricidad en el mundo. Sin embargo, produce 1,54 millones de barriles de petróleo diarios y 69,9 millones de m³ de gas natural anuales. Actualmente, la mayor parte de la energía eléctrica proviene de fuentes no renovables, principalmente del carbón y petróleo. Esto ha hecho que el gobierno comience a implementar medidas para reducir la dependencia de los combustibles fósiles en materia de producción de energía y se pretende que para 2020 el 40 % de la electricidad provenga de fuentes de energía alternativas como la solar, la eólica y la mareomotriz.

El Reino Unido tiene una pequeña reserva de carbón, junto con reservas importantes, pero en continua disminución, de gas natural y petróleo. Se han identificado unos 400 millones de toneladas de carbón en el país. En 2004, el consumo de carbón total (incluyendo las importaciones) fue de 61 millones de toneladas, permitiendo al país ser autosuficiente en carbón por apenas 6,5 años, aunque con los niveles de la extracción actual, el periodo aumenta a 20 años. Una alternativa a la generación de energía eléctrica con carbón es la gasificación del carbón subterráneo (GCS). La GCS es un sistema que inyecta vapor y oxígeno dentro de un pozo, donde se extrae gas del carbón y empuja la mezcla de gases a la superficie (un método de extracción de carbón con emisiones de carbono potencialmente bajas). Tras la identificación de áreas terrestres que tienen el potencial para la GCS, las reservas de gas se calculan entre 7 mil millones y 16 mil millones de toneladas. Basado en el consumo de carbón actual en el país, estos volúmenes representan reservas que podrían durar entre 200 y 400 años.

La educación en el Reino Unido es una cuestión descentralizada, ya que cada nación constituyente tiene su propio sistema de educación. La educación en Inglaterra es responsabilidad de la Secretaría de Estado para los Niños, Escuelas y Familias, aunque la administración y financiación de las escuelas estatales corresponden a las autoridades locales. La universalidad en la educación en Inglaterra y Gales fue introducida en 1870 para la educación primaria y en 1900 para la educación secundaria. Actualmente, la educación es obligatoria de los cinco a dieciocho años de edad. La mayoría de los niños son educados en escuelas del sector estatal, solo una pequeña porción estudia en escuelas especiales, principalmente por motivos de habilidades académicas. Las escuelas del Estado que tienen permitido seleccionar a los alumnos de acuerdo a su inteligencia y habilidad académica pueden lograr resultados comparables a las escuelas privadas más selectivas: en 2006, de las diez escuelas de mejor rendimiento académico, dos eran escuelas estatales de gramática. A pesar de una caída en las cifras reales, la proporción de niños en Inglaterra que asisten a escuelas privadas ha aumentado en más de 7 %. Sin embargo, más de la mitad de los estudiantes de las principales universidades, Cambridge y Oxford, asistió a las escuelas estatales. Inglaterra tiene algunas de las mejores universidades a nivel internacional; la Universidad de Cambridge, la Universidad de Oxford, el Imperial College London y la University College de Londres están clasificadas dentro de las diez mejores del mundo. Según la TIMSS (Tendencias en el Estudio Internacional de Matemáticas y Ciencias), los alumnos en Inglaterra son los séptimos mejores en matemáticas y los sextos en ciencias. Los resultados sitúan a los alumnos ingleses por delante de otros países europeos, incluyendo Alemania y los países escandinavos.

La educación en Escocia es responsabilidad de la Secretaría de Educación y Aprendizaje, con la administración y financiación de las escuelas estatales a cargo de las autoridades locales. Dos organismos públicos no departamentales tienen un papel clave en la educación escocesa: la Autoridad Escocesa de Calificaciones y Aprendizaje y Enseñanza de Escocia. La educación se volvió obligatoria en Escocia en 1496. La proporción de niños que asisten a escuelas privadas es apenas del 4 %, aunque ha ido aumentando lentamente en los últimos años. Los estudiantes escoceses que asisten a universidades de Escocia no pagan colegiaturas ni los cursos para realizar algún posgrado, ya que todas estas cuotas fueron abolidas en 2001. La aportación monetaria a las universidades por parte de los alumnos egresados fue abolida en 2008.

La educación en Irlanda del Norte es administrada por el Ministerio de Educación y el Ministerio de Empleo y Aprendizaje, aunque a nivel local es responsabilidad de cinco juntas de educación, que cubren áreas geográficas determinadas. El Consejo para el Plan de Estudios, Exámenes y Evaluaciones (CCEA) es el organismo encargado de asesorar al gobierno sobre lo que debe enseñarse en las escuelas norirlandesas, el seguimiento de normas y la adjudicación de títulos.

La Asamblea Nacional de Gales tiene la responsabilidad de la educación en este país. Un número significativo de estudiantes galeses aprende, ya sea totalmente o en gran medida, en el idioma galés; las lecciones en galés son obligatorias para todos los alumnos hasta la edad de 16 años. Hay planes para aumentar el número de escuelas de educación media que imparten clases en galés, como parte de la política para lograr un Gales totalmente bilingüe.

Un estudio publicado en diciembre de 2019 por la asociación "The Equality Trust" revela que sumando las fortunas de las cinco familias más ricas del Reino Unido —por un total de 46.000 millones de euros— obtenemos la suma que poseen los 13 millones de personas más pobres del país. En términos más generales, el 1 % más rico de los británicos posee por sí solo tanto dinero como el 80 % de la población total.

Entre 2017 y 2018, la tasa de pobreza en el país aumentó del 22,1 % al 23,2 %, el mayor incremento desde 1988. Se cree que el aumento de la inflación y los recortes presupuestarios del Gobierno conservador para 2015, en particular en lo que respecta a las asignaciones familiares y los subsidios de vivienda, son las principales causas. Cuatro millones de británicos viven con menos de la mitad de la línea de pobreza y 1,5 millones no pueden cubrir sus necesidades básicas.

Cada diez años se efectúa un censo simultáneamente en todas las regiones del Reino Unido. La Oficina Nacional de Estadísticas es la responsable de la recopilación de datos para Inglaterra y Gales, mientras que en Escocia e Irlanda del Norte los responsables de llevar a cabo los censos son la Oficina de Registro General y la Agencia de Estadísticas e Investigación, respectivamente.

En el más reciente censo realizado en 2001, el total de la población del Reino Unido fue de 58 789 194 personas, la tercera más grande en la Unión Europea, la quinta más grande en la Mancomunidad y la . A mediados de 2008, se estimó que había crecido a los 61 383 000 habitantes. En 2008, el crecimiento natural de la población superó la migración neta como el principal contribuyente al crecimiento de la población, la primera vez que ocurre desde 1998. Entre 2001 y 2008, la población aumentó en una tasa media anual del 0,5 %. Esto se compara con el 0,3 % anual en el período de 1991 a 2001 y al 0,2 % en la década de 1981 a 1991. Publicado en 2008, la estimación de la población de 2007 reveló que, por primera vez, el Reino Unido era hogar de más personas en edad de jubilación que de niños menores de 16 años.

A mediados de 2008, del total de unos 61 millones de británicos, la población de Inglaterra se estimó en 51 383 000 habitantes. De esta forma, Inglaterra es uno de los países más densamente poblados del mundo con 383 habitantes por kilómetro cuadrado, con una concentración particular en Londres y en el sureste del país. Las estimaciones de ese mismo periodo ponen la población de Escocia en 5,17 millones, de Gales en 2,99 millones y de Irlanda del Norte en 1,78 millones, con mucho menor densidad de población que Inglaterra. En comparación con los 383 habitantes ingleses por kilómetro cuadrado, las cifras correspondientes fueron 142 h/km² en Gales, 125 h/km² para Irlanda del Norte y solo 65 h/km² para Escocia. Irlanda del Norte tenía la población de más rápido crecimiento en términos de porcentaje de todos los cuatro países constituyentes del Reino Unido.

Ese mismo año, la tasa de fertilidad promedio en todo el Reino Unido fue de 1,96 hijos por mujer. Mientras que una creciente tasa de natalidad contribuye al crecimiento de la población actual, aún permanece considerablemente por debajo del "baby boom" de 1964, donde cada mujer tenía en promedio 2,95 hijos, pero superior al récord más bajo en 2001, de 1,63 hijos por mujer. En 2008, Escocia tenía la tasa de fecundidad más baja con solo 1,8 niños por mujer, mientras que Irlanda del Norte tuvo la más alta con 2,11 niños.

El Reino Unido no tiene un idioma oficial, pero el más predominante es el inglés, una lengua germánica occidental que desciende del anglosajón, que cuenta con un gran número de préstamos del nórdico antiguo, del francés normando y del latín. Debido en gran medida a la expansión del Imperio británico, el idioma inglés se esparció por el mundo y se convirtió en el idioma internacional de los negocios, así como la segunda lengua más divulgada en el mundo.

El escocés ("Lallans"), una lengua emparentada con el inglés que también desciende del inglés medio hablado en el noreste de Inglaterra, es reconocido a nivel europeo. También hay cuatro lenguas celtas en uso: el galés, el irlandés, el gaélico escocés y el córnico. En el censo de 2001, más de una quinta parte de la población de Gales dijo que sabía hablar galés (21%), Además, se estima que cerca de 200 000 galesoparlantes viven en Inglaterra.

El censo de 2001, en Irlanda del Norte se demostró que 167 487 personas (10,4 % de la población) tenían "cierto conocimiento del irlandés", casi exclusivamente en la población católica y nacionalista del país. Más de 92 000 personas en Escocia (justo por debajo del 2 % de la población) poseían algún entendimiento de la lengua gaélica, incluyendo el 72 % de los habitantes de las Hébridas Exteriores. Está aumentando el número de escuelas que enseñan en galés, gaélico escocés e irlandés. Estos idiomas también son hablados por pequeños grupos alrededor del mundo; en Nueva Escocia, Canadá se habla irlandés, mientras que existe una población que habla galés en la Patagonia argentina.

Generalmente, es obligatorio para los alumnos británicos estudiar un segundo idioma en algún momento de su trayectoria escolar: a la edad de 14 años en Inglaterra, y hasta la edad de 16 en Escocia. El francés y el alemán son los dos idiomas más estudiados en Inglaterra y Escocia. En Gales, todos los alumnos de 16 años deben haber aprendido el galés como segunda lengua.

En el Acta de Unión de 1707, que llevó a la formación del Reino Unido, se aseguró que el protestantismo seguiría existiendo, así como un vínculo entre la Iglesia y el Estado que permanece hasta el . De esta forma, el cristianismo es la religión con más seguidores, seguida por el islam, el hinduismo, el sijismo y el judaísmo, según los datos obtenidos en el censo de 2001.

En el mismo censo el 71,6 % de los encuestados dijo que el cristianismo era su religión, aunque encuestas que emplean una pregunta "más específica" tienden a encontrar proporciones menores; tal es el caso del "Estudio de Tearfund de 2007", el cual reveló que el 53 % se identificaron como cristianos, y del "Estudio británico de Actitudes Sociales de 2007", que encontró que era casi un 47,5 %. Sin embargo, el "Estudio de Tearfund" demostró que solo uno de cada diez británicos realmente asistía a la iglesia semanalmente.

El "Estudio británico de Actitudes Sociales de 2007", que abarca a Inglaterra, Gales y Escocia, pero no a Irlanda del Norte, indicó que 20,87 % de la población eran parte de la Iglesia de Inglaterra, 10,25 % cristianos sin denominación, 9,01 % católicos, 2,81 % presbiterianos (Iglesia de Escocia), 1,88 % metodistas, 0,88 % bautistas y 2,11 % cristianos de otro tipo. Entre otras religiones, los musulmanes ocupaban el 3,30 %, los hinduistas el 1,37 %, los judíos el 0,43 %, los sijistas el 0,37 % y los adeptos a otras religiones el 0,35 %. Una gran proporción afirmó no tener ninguna religión (45,67 %).

En el censo de 2001, 9,1 millones de personas (15 % de la población) afirmaron ser ateos, con más de 4,3 millones de personas (7 %) que no indicaron una preferencia religiosa en específico. Existe una disparidad entre las cifras para aquellos que se identifican con una religión en particular y para aquellos que proclaman la creencia en un dios: una encuesta del "Eurobarómetro" realizada en 2005 mostró que el 38 % de los encuestados cree que "hay un dios", 40 % cree que "hay algún tipo de espíritu o fuerza vital" y 20 % dijo que "no creo que exista algún tipo de espíritu, dios o fuerza vital". El druidismo es desde 2010 reconocido como una de las religiones oficiales de Reino Unido y como una de las más antiguas del país.

Al igual que la educación, la asistencia médica es un asunto descentralizado, por lo que Inglaterra, Irlanda del Norte, Escocia y Gales cuentan con su propio sistema de atención de la salud, junto con terapias alternativas, holísticas y complementarias. El National Health Service (NHS) (Servicio Nacional de Salud) es el organismo encargado de brindar asistencia médica a todos los residentes permanentes del Reino Unido de manera gratuita. En 2000, la Organización Mundial de la Salud situó al National Health Service como el decimoquinto mejor en Europa y el decimoctavo en el mundo.

Además del National Health Service, existen varios organismos encargados del cuidado de la salud que son administrados por el gobierno, como el Consejo Médico General y el Consejo de Obstetricia y Enfermería, mientras que otros corresponden a la iniciativa privada, como los Colegios Reales. Sin embargo, la política y la administración del National Health Service corresponden a cada nación constitutiva. Cada National Health Service tiene diferentes políticas y prioridades, resultando en grandes contrastes entre uno y otro.

Desde 1979, los gastos del servicio médico han aumentado significativamente, acercándose al gasto promedio de la Unión Europea. El Reino Unido gasta alrededor de un 8,4 % de su PIB en el cuidado de la salud, lo que está un 0,5 % por debajo del promedio de la Organización para la Cooperación y el Desarrollo Económico y alrededor de un 1 % por debajo del promedio de la Unión Europea.

La cultura del Reino Unido, también llamada "cultura británica", puede ser descrita como el legado de la historia de un país insular desarrollado, una gran potencia y también como el resultado de la unión política de cuatro países, cada uno conservando sus elementos distintivos de las tradiciones, costumbres y simbolismos. Como resultado del dominio del Imperio británico, la influencia de la cultura británica se puede observar en el idioma, las tradiciones, las costumbres y los sistemas jurídicos de muchas de sus antiguas colonias, como Canadá, Australia, Nueva Zelanda, India y los Estados Unidos.

El arte y la cultura han sido influenciados históricamente por la ideología occidental. Desde la expansión del Imperio británico, la experiencia del poder militar, político y económico llevó a una técnica, gusto y sensibilidad únicos de los artistas del Reino Unido. Los británicos usaban su arte "para ilustrar sus conocimientos y liderar el mundo natural", mientras que los colonos de América del Norte, Australasia y Sudáfrica "se embarcaron hacia la búsqueda de una expresión artística distintiva y apropiada para su identidad nacional". El imperio estuvo "en el centro, más que en los márgenes, de la historia del arte británico", y las artes visuales de la época victoriana han sido fundamentales para la construcción, celebración y expresión de la identidad británica.

El arte del Reino Unido abarca todas las manifestaciones artísticas realizadas desde la fundación del país hasta la actualidad. Sin embargo, gran parte del denominado arte británico proviene de antes de 1707, siendo Stonehenge la manifestación artística más antigua en el país, ya que data del año 2500 a. C. Desde entonces, el arte en el territorio comprendido por el Reino Unido se fue desarrollando con el paso de los siglos, y para la época de la unión de las cuatro naciones, cada una ya contaba con una tradición artística definida.

La época de mayor auge para las artes británicas fue durante el Imperio, cuando el Reino Unido se ubicó a la cabeza de varios movimientos artísticos en los que además de representar momentos históricos, bíblicos y mitológicos, plasmaron momentos de la vida cotidiana que podían trascender en el arte. Además, gracias a la expansión imperial los artistas pudieron tomar influencias de las culturas de los países bajo el dominio británico, tales como India, Estados Unidos, etc., al mismo tiempo que las obras británicas dejaban su huella y legado dentro de los artistas de las colonias. Durante el , el arte británico comenzó a expandirse a las corrientes del arte moderno y contemporáneo, como el posimpresionismo, el cubismo y el impresionismo.

Actualmente, existen varias instituciones artísticas en el Reino Unido, de las cuales han surgido varios movimientos artísticos y artistas destacados dentro de su campo. Entre estas se encuentran la Royal Academy, el Royal College of Art, la Royal Society of Arts y la galería Tate. Además, dentro de sus fronteras también se ubican varios museos y galerías de prestigio internacional, como el Museo Británico, la National Gallery de Londres, la Galería Nacional de Escocia, el Museo de Ciencias de Londres o el Museo de Yorkshire, entre otros.

La arquitectura británica se caracteriza por la combinación ecléctica de distintos estilos arquitectónicos, variando desde aquellos que se encontraban antes de la creación del país, como la arquitectura romana, hasta la arquitectura contemporánea del . Irlanda del Norte, Escocia y Gales desarrollaron estilos arquitectónicos únicos y jugaron papeles importantes en la historia de la arquitectura mundial. Aunque existen estructuras prehistóricas y clásicas en las islas Británicas, la historia de la arquitectura británica comienza con las primeras iglesias anglosajonas, construidas poco después de la llegada de Agustín de Canterbury a Gran Bretaña en el año 597. Desde el , la arquitectura normanda se esparció en Gran Bretaña e Irlanda, en forma de castillos e iglesias para ayudar a imponer la autoridad normanda en sus dominios. La arquitectura gótica inglesa, que floreció entre 1189 y 1520, fue traída desde Francia, pero rápidamente desarrolló sus propias características.

Por todo el país, la arquitectura medieval secular se desarrolló en forma de castillos, la mayoría de ellos se encuentra cerca de la frontera entre Inglaterra y Escocia, y datan del , la época de las guerras de independencia de Escocia. La invención de las armas de fuego y el cañón hicieron a los castillos inútiles y el renacimiento inglés dio paso al desarrollo de nuevos estilos artísticos para la arquitectura nacional: el estilo Tudor, el barroco inglés y el palladianismo. La arquitectura georgiana y neoclásica avanzaron después de la Ilustración Escocesa y a partir de la década de 1930 aparecieron varios estilos modernistas. Sin embargo, la lucha por la conservación de las antiguas estructuras y la resistencia de los movimientos tradicionalistas ha cobrado fuerza, además de ser apoyados por figuras públicas como Carlos de Gales.

El Reino Unido fue una fuerte influencia en el desarrollo del cine, con los Estudios Ealing que reclaman el título de ser los estudios más antiguos en el mundo. A pesar de una historia de producciones importantes y exitosas, esta industria se caracteriza por un debate en curso sobre su identidad y las influencias del cine estadounidense y europeo. El mercado británico es muy pequeño para que la industria cinematográfica británica pueda producir exitosamente "blockbusters" al estilo de Hollywood por un período sostenido. En comparación con la estadounidense, la industria cinematográfica británica no ha sido capaz de producir éxitos comerciales a nivel internacional; por lo que mantiene una actitud compleja y dividida hacia Hollywood. No obstante, cabe destacar que ocho de las diez de todos los tiempos tienen alguna dimensión británica, sea histórica, cultural o creativa: "Titanic", dos episodios de "El Señor de los Anillos", dos de la trilogía de los "Piratas del Caribe" y tres películas de la saga de "Harry Potter".

La literatura británica se refiere a la literatura asociada con el Reino Unido, la isla de Man y las islas del Canal, así como a la literatura de Inglaterra, Gales y Escocia antes de la formación del país. La mayor parte de las obras de la literatura británica fue escrita en el idioma inglés. El Reino Unido publica cerca de 206 000 libros cada año, convirtiéndolo en el mayor editor de libros en el mundo. La capital de Escocia, Edimburgo, fue declarada como "Ciudad de Literatura" por la UNESCO.

El poeta y dramaturgo inglés William Shakespeare es ampliamente considerado como el mayor dramaturgo de todos los tiempos. Entre los escritores en inglés más reconocidos se encuentran Geoffrey Chaucer (), Thomas Malory (), Thomas More () y John Milton (). A Samuel Richardson, escritor del , se le atribuye la invención de la novela epistolar, además de Daniel Defoe el creador de Robinson Crusoe. En el , siguieron más representantes de la literatura británica: la innovadora Jane Austen, la novelista gótica Mary Shelley, el escritor de cuentos para niños Lewis Carroll, las hermanas Emily, Charlotte y Anne Brontë, el activista social Charles Dickens, el naturalista Thomas Hardy, el poeta visionario William Blake, el poeta romántico William Wordsworth y "sir" Arthur Conan Doyle creador de Sherlock Holmes.

Los escritores más famosos del incluyen al novelista de ciencia ficción H. G. Wells, los escritores de clásicos infantiles Rudyard Kipling y A. A. Milne, el controvertido D. H. Lawrence, la modernista Virginia Woolf, la satírica Evelyn Waugh, el novelista George Orwell, el popular novelista Graham Greene, la novelista policíaca Agatha Christie, el creador de James Bond Ian Fleming, los escritores de fantasía J. R. R. Tolkien, C. S. Lewis y más recientemente J. K. Rowling; así como los poetas Ted Hughes y John Betjeman.

Desde su fundación, el Reino Unido ha estado a la cabeza de los avances científicos y tecnológicos, así como en la investigación y desarrollo. La Royal Society es la sociedad científica más antigua del Reino Unido, y una de las más antiguas en el mundo. Durante sus más de 300 años de historia, se ha encargado de promover, proteger y divulgar el conocimiento y las ciencias en el país y en el mundo entero. Dentro de esta sociedad participaron varios científicos que contribuyeron al avance de sus respectivas áreas de conocimiento; entre estos se encuentran: Robert Boyle, John Wallis, Isaac Newton, Robert Hooke, Thomas Willis, entre otros. El Consejo de Facilidades para la Ciencia y Tecnología es otro de los organismos encargados de promover y dar apoyo a las investigaciones científicas en el país. Durante los años 2008 y 2009, este consejo invirtió más de 1200 millones de dólares estadounidenses para brindar recursos a varios institutos y sociedades científicas británicas. Con respecto a la investigación biomédica, uno de los grandes avances en este país ha sido la secuenciación del genoma de 10 000 personas británicas para conocer las variantes genéticas raras y de baja frecuencia implicadas en la salud y la enfermedad.

Como país líder de la Revolución Industrial, los inventores del Reino Unido le brindaron al mundo varias innovaciones, principalmente en el campo de la textilería, la maquinaria de vapor, los ferrocarriles y la ingeniería. Dentro de este periodo destacan los inventores George Stephenson, James Watt y Robert Stephenson. Desde entonces, los inventos e inventores británicos han destacado y sido numerosos. Entre estos nuevos innovadores se encuentran Alan Turing, Alexander Graham Bell, John Logie Baird, Frank Whittle, Charles Babbage, Alexander Fleming, entre muchos otros. En 2007, el Reino Unido contaba con 79 855 patentes en vigor, el con mayor número de ellas. La inversión de las empresas del Reino Unido en tecnología y ciencias fue de 9700 millones de USD entre 2010-2015.

El Reino Unido es famoso por la tradición del "empirismo británico", una rama de la filosofía del conocimiento que indica que el único conocimiento válido es aquel que se comprueba por la experiencia; y de la "Filosofía escocesa", que a veces se denomina el "la escuela escocesa del sentido común". Los filósofos más famosos del empirismo británico son: John Locke, George Berkeley y David Hume, mientras que Dugald Stewart, Thomas Reid y William Hamilton fueron los principales exponentes de la escuela escocesa del sentido común. Gran Bretaña también es notable por una teoría de la filosofía moral, el utilitarismo, usado por primera vez por Jeremy Bentham y posteriormente por John Stuart Mill, en su obra homónima "Utilitarismo". Otros eminentes filósofos del Reino Unido y de los Estados que lo precedieron incluyen a Duns Scoto, John Lilburne, Mary Wollstonecraft, Francis Bacon, Adam Smith, Thomas Hobbes, Guillermo de Ockham, Bertrand Russell y Alfred Jules Ayer.

Existen varios estilos musicales bastante populares en el Reino Unido, desde la música folclórica de Inglaterra, Irlanda, Escocia y Gales, hasta el heavy metal. Entre los compositores británicos de música clásica más notables se encuentran: William Byrd, Henry Purcell, Edward Elgar, Gustav Holst, Arthur Sullivan (más conocido por trabajar con el libretista W. S. Gilbert), Ralph Vaughan Williams y Benjamin Britten, pionero de la ópera moderna británica. Peter Maxwell Davies es uno de los compositores vivos más destacados en el país y el actual maestro de música de la reina. También aquí se encuentran varias orquestas sinfónicas y coros de renombre internacional, como la Orquesta Sinfónica de la BBC y el Coro de la Sinfónica de Londres. El compositor barroco Georg Friedrich Händel, aunque nació en Alemania, obtuvo la ciudadanía británica y algunas de sus mejores obras, como "El Mesías", fueron escritas en inglés.

Los británicos más prominentes que han influenciado la música popular de los últimos cincuenta años incluyen a The Beatles, Queen, Elton John, Bee Gees, Led Zeppelin, Oasis, Pink Floyd y The Rolling Stones, todos ellos con ventas que superan los doscientos millones de discos en todo el mundo. Asimismo, The Beatles tienen el musicales, con más de mil millones de discos vendidos a nivel internacional. Un gran número de ciudades británicas son conocidas por su escena musical: estadísticamente, los artistas de Liverpool son los que tienen más éxito en la lista "UK Singles Chart". Por su lado, la contribución de Glasgow a la escena musical fue reconocida en 2008, cuando fue nombrada por la UNESCO como "Ciudad de la Música", título que comparte con Bolonia, Sevilla y Gante.

Históricamente, la gastronomía del Reino Unido ha sido etiquetada como «platos desabridos hechos con ingredientes de baja calidad, mezclados con salsas simples para acentuar el sabor, en vez de disfrazarlo.» Sin embargo, la cocina británica ha absorbido la influencia cultural de los inmigrantes establecidos en el país, produciendo varios platillos híbridos, como el pollo tikka masala, considerado «el verdadero platillo nacional británico».

Los platos tradicionales de la cocina británica incluyen el "fish and chips", el "Sunday roast", el "steak and kidney pie" y el "bangers and mash". La gastronomía del Reino Unido tiene múltiples variantes nacionales y regionales, como son las gastronomías propias de Inglaterra, Escocia y Gales, las cuales han desarrollado su propios platillos regionales, tales como el queso Cheshire, el "Yorkshire pudding" y el pastel galés. Como en otros países occidentales, el consumo de comida rápida es muy amplio, lo que ha ocasionado un problema de salud pública tan grave como el que sufre Estados Unidos.

El té es la bebida más popular en el país y de hecho también es una de las tradiciones gastronómicas más conocidas de la cocina del Reino Unido. Originada durante el , la "tea time" (literalmente, «la hora del té», pero mejor traducido como «la hora de la merienda»), no es exclusivamente para consumir té, sino que es una de las comidas centrales de los británicos, similar a una merienda o incluso la cena. El "tea break" y el "tea sandwich" son dos variaciones de esta comida.

El deporte es un elemento clave de la cultura británica. Gran cantidad de deportes fueron creados en el Reino Unido, incluyendo el fútbol, el rugby, el tenis y el golf, siendo el primero el deporte más popular en el país. Internacionalmente, Inglaterra, Escocia, Gales e Irlanda del Norte compiten separadamente en la mayoría de los deportes colectivos (aunque Irlanda del Norte en muchos deportes, como es el caso del rugby o el golf, continua unida al resto de la isla de Irlanda), así como en los Juegos de la Mancomunidad. Sin embargo, en algunos deportes el Reino Unido participa como un único equipo, como en el baloncesto.

En los Juegos Olímpicos, el Reino Unido también participa como un único equipo, representado por el comité olímpico nacional del Reino Unido, la British Olympic Association. El país ha participado en cada una de las ediciones de los Juegos Olímpicos de la era moderna y ha sido anfitrión de tres, de las ediciones de 1908, en donde se ubicó en el primer lugar del medallero, de 1948, de 2012, realizados en Londres.

Se afirma que el críquet se inventó en Inglaterra (aunque investigaciones recientes sugieren que en realidad originó en Bélgica) y la selección inglesa, controlada por la Junta de Críquet de Inglaterra y Gales, es el único equipo nacional del Reino Unido con estatus de test críquet. Los miembros de la selección son de nacionalidad galesa e inglesa, a diferencia de las selecciones de otros deportes como el fútbol y el rugby. Algunos norirlandeses y escoceses han jugado para la selección inglesa, debido a que sus respectivas selecciones no cuentan con estatus de test críquet. Todas las naciones constitutivas han competido en la Copa Mundial de Críquet, con Inglaterra llegando a la final en más de tres ocasiones.

Como en otros deportes colectivos, en el rugby Inglaterra, Escocia, Gales e Irlanda del Norte compiten como países separados en las diversas competencias internacionales, pero con la diferencia de que Irlanda del Norte lo hace en conjunto con la República de Irlanda, por lo que existe una selección de rugby de Irlanda que representa a toda la isla. Sin embargo, cada cuatro años los Leones británico-irlandeses, equipo compuesto por jugadores de todo el Reino Unido más Irlanda, hacen una gira por distintas partes del mundo. Mientras la selección de rugby de Inglaterra logró el campeonato de la Copa Mundial de Rugby de 2003, la mejor actuación de Gales ha sido un tercer lugar, Escocia un cuarto lugar e Irlanda ha alcanzado a llegar a los cuartos de final.

Una variante del rugby, el rugby league, también conocido como rugby a 13, se practica en todo el país, pero en el norte de Inglaterra (el lugar donde se originó) es el deporte más importante en muchas áreas, en especial en Yorkshire, Cumbria y Lancashire, aunque también tiene presencia en Londres y Gales del Sur. Anteriormente una selección del Reino Unido representaba al país en competiciones internacionales, pero desde 2008 cada nación cuenta con su propia selección de rugby league. En 2013, el Reino Unido será la sede de la Copa Mundial de Rugby League por quinta ocasión.

El tenis se inventó en la ciudad de Birmingham entre los años 1859 y 1865. Desde 1877, cada verano se efectúa en Wimbledon (Londres) el Campeonato de Wimbledon, que es el tercer Grand Slam del año. A nivel de logros, el Reino Unido ha alcanzado la Copa Davis en 10 ocasiones, siendo la última la alcanzada en el año 2015, y ha alcanzado el subcampeonato de la Fed Cup en cuatro ocasiones.

El golf es el sexto deporte más popular del país, en términos de participación. Aunque The Royal and Ancient Golf Club of St Andrews, en Escocia, es la cuna de este deporte, el campo de golf más antiguo del mundo es el Musselburgh Links' Old Golf Course. El shinty (o "camanachd") es un deporte muy popular en la región escocesa de Highlands, a veces atrayendo a miles de espectadores de toda la nación, especialmente para ver la final del principal torneo, la Copa Camanachd.

En cuanto al automovilismo, el Reino Unido es uno de los países con mayor participación en este deporte, ya que la mayoría de los equipos de Fórmula 1 tienen su base en el país y los conductores británicos han ganado más títulos en conjunto que ningún otro. En el Circuito de Silverstone se organiza anualmente el Gran Premio de Gran Bretaña, válido para la Fórmula 1. Otros eventos automovilísticos que se organizan en el país son el Campeonato británico de Turismos y una fecha del Campeonato Mundial de Rally. Asimismo, el Reino Unido es el hogar de varios de los principales equipos de Fórmula 1, destacándose entre ellos los múltiples campeones de constructores y pilotos McLaren, Williams F1, Lotus F1 y Red Bull Racing. En el caso de esta última, a pesar del origen austríaco de la marca de bebidas propietaria del equipo, posee sus cuarteles generales en el Reino Unido, debido a la adquisición que hiciera de la franquicia del exequipo Jaguar F1 para poder ingresar al Campeonato Mundial de Fórmula 1.

Otros deportes populares a escala nacional incluyen las carreras de caballos y el hockey sobre césped. Particularmente en Irlanda del Norte, sobre todo dentro de la población católica, son muy populares el fútbol gaélico y el "hurling", ambos regidos por la Asociación Atlética Gaélica.

El fútbol tiene sus orígenes en el Reino Unido, además de que fue en este país donde se formalizó y estandarizó, convirtiéndose en el deporte más popular. Cada uno de los países constituyentes posee su propia asociación de fútbol, selección nacional y sistema de ligas independiente, aunque algunos clubes compiten fuera de sus países de origen debido a razones históricas o logísticas.

La cuestión por la que Inglaterra, Gales, Escocia e Irlanda del Norte pueden disputar las competiciones internacionales por separado, y no sucede lo mismo con otras regiones europeas, es por un motivo histórico. En el momento en el que la FIFA fue creada, en 1904, ya existía la Asociación de Fútbol de Inglaterra (Football Association, 1863), la Asociación de Fútbol de Escocia (Scottish Football Association, 1873), la Asociación de Fútbol de Gales (Football Association of Wales, 1876) y la Asociación Irlandesa de Fútbol (Irish Football Association, 1880), cuyas selecciones ya habían disputado partidos internacionales y contaban con sus propias competiciones domésticas. Por eso, en cuanto la FIFA —así como la UEFA, más de cuarenta años después— solicitó a esas federaciones que se afiliaran, éstas aceptaron, pero siempre y cuando se mantuvieran intactos sus estatutos, cada uno por separado.

Distinta es la situación durante los Juegos Olímpicos. El COI dejó en claro desde su fundación, en 1894, que sólo iba a permitir la participación de estados soberanos. Existen otros deportes en los que ingleses, escoceses, galeses y norirlandeses van por separado, todos ellos de enorme tradición, y que por supuesto no son olímpicos (hablamos por ejemplo del cricket o el rugby).
Es por este motivo que la isla ha disputado a través de una selección unificada los Juegos Olímpicos realizados entre 1908 y 1972 (bajo el nombre oficial de Gran Bretaña e Irlanda del Norte) y el de 2012, ocasión para la cual se conformó una selección olímpica compuesta mayoritariamente por futbolistas ingleses y algunos galeses, aunque sin escoceses ni norirlandeses.

El sistema de ligas de Inglaterra incluye cientos de ligas interconectadas con miles de divisiones. La máxima categoría, la Premier League, es la liga de fútbol con mayor audiencia en el mundo. Bajo la Premier League, está la Football League, que consiste en tres divisiones, y luego la Football Conference, que consiste en una división nacional y dos divisiones regionales. Los equipos ingleses han obtenido buenos resultados en las competiciones europeas, incluyendo los que han ganado la Copa de Europa/Liga de Campeones de la UEFA: Liverpool en seis ocasiones, Manchester United en tres ocasiones, Nottingham Forest en dos y Chelsea con Aston Villa en una. En total, los clubes de Inglaterra han ganado internacionales de la UEFA. El principal coliseo deportivo de Inglaterra es el Estadio de Wembley, donde juega de local la selección inglesa de fútbol, que cuenta con una capacidad para 90 000 personas.

El sistema de ligas de Escocia tiene dos ligas nacionales: La Premier League de Escocia, máxima categoría, y la Football League de Escocia, que tiene tres divisiones. Un club de Inglaterra, el Berwick Rangers, compite en el sistema escocés de fútbol. Los equipos más importantes de Escocia son el Celtic Football Club y el Rangers Football Club, ambos de Glasgow: el Celtic se proclamó campeón de la Copa de Europa, actual Champions, en 1967, siendo el primer equipo británico en hacerlo, y el Rangers fue campeón de la Recopa de Europa en 1972. Además, el Aberdeen también fue campeón de la Recopa y de la Supercopa de Europa en 1983. La selección escocesa de fútbol juega de la mayoría de las veces de local en Hampden Park.

El sistema de ligas de Gales se compone de la Welsh Premier League y varias ligas regionales. El equipo de la Welsh Premier League, The New Saints, juega sus encuentros de local en Oswestry, ciudad fronteriza de Inglaterra, mientras tanto, algunos equipos de Gales como el Cardiff City, el Swansea City y el Wrexham, entre otros, compiten bajo el sistema de ligas de Inglaterra. El Millenium Stadium de Cardiff es el estadio en el que juega de local la selección de fútbol de Gales.

El sistema de ligas de Irlanda del Norte incluye la IFA Premiership, que es la máxima división. Un equipo de Irlanda del Norte, el Derry City, compite fuera de las fronteras del Reino Unido, en el fútbol de la República de Irlanda. La selección de fútbol de Irlanda del Norte juega sus partidos de local en el Windsor Park de Belfast.





</doc>
<doc id="5345" url="https://es.wikipedia.org/wiki?curid=5345" title="Atentados del 11 de septiembre de 2001">
Atentados del 11 de septiembre de 2001

Los atentados del 11 de septiembre de 2001, también llamados «9/11» «11/9» «11S» y «11-S», fueron una serie de cuatro atentados terroristas suicidas cometidos la mañana del martes 11 de septiembre de 2001 en los Estados Unidos por la red yihadista Al Qaeda que, mediante el secuestro de aviones comerciales para ser impactados contra diversos objetivos, causaron la muerte de 2.996 personas, incluidos los diecinueve terroristas, la desaparición de veinticuatro víctimas, y más de seis mil heridos. A su vez, se registró la destrucción de todo el complejo de edificios del World Trade Center, más notablemente las Torres Gemelas de Nueva York y graves daños en el edificio del Pentágono, sede del Departamento de Defensa de los Estados Unidos. Este episodio precedió a la guerra de Afganistán y a la adopción por parte del Gobierno estadounidense y sus aliados de la política denominada «guerra contra el terrorismo».

Los terroristas, divididos en cuatro grupos de secuestradores, cada uno de ellos con un piloto que se encargaría de pilotar el avión una vez reducida la tripulación de la cabina. El vuelo 11 de American Airlines y el vuelo 175 de United Airlines fueron los primeros en ser secuestrados y ambos fueron estrellados contra las Torres Gemelas del World Trade Center, el primero contra la Torre Norte y el segundo contra la Torre Sur, lo que provocaría que ambos rascacielos se derrumbaran en las dos horas siguientes.

El tercer avión secuestrado pertenecía al vuelo 77 de American Airlines y fue empleado para ser impactado contra la fachada oeste del Pentágono, en Virginia. El cuarto avión, perteneciente al vuelo 93 de United Airlines, tenía como objetivo el Capitolio de los Estados Unidos ubicado en la ciudad de Washington D.C., pero se estrelló en campo abierto cerca de Shanksville, Pensilvania, aproximadamente 208 kilómetros antes de llegar al Capitolio, tras perder el control en cabina como consecuencia del enfrentamiento de los pasajeros y tripulantes contra el comando terrorista.

Los atentados, que fueron condenados inmediatamente como «horrendos ataques terroristas» por el Consejo de Seguridad de Naciones Unidas, se caracterizaron por el empleo de aviones comerciales como armamento, provocando una reacción de temor generalizado en todo el mundo y particularmente en los países occidentales, que alteró desde entonces las políticas internacionales de seguridad aérea.

El atentado ha dado lugar a numerosas teorías conspirativas sobre el 11-S.

En la "Carta a Estados Unidos" de Osama bin Laden de noviembre de 2002, el líder terrorista declaró explícitamente los motivos de al-Qaeda para sus ataques, culpando a la "alianza de sionistas y sus colaboradores", en una clara referencia a los Estados Unidos. Los principales puntos de sus acusaciones eran el apoyo militar estadounidense a Israel y la presencia de tropas estadounidenses en Arabia Saudí.

Estados Unidos ya había sufrido una serie de atentados provocados por el terrorismo islámico en las décadas anteriores. Es el caso del Atentado contra los cuarteles en Beirut en 1983 que mató a más de doscientos soldados estadounidenses y cincuenta y ocho soldados franceses. En 1993, el atentado del World Trade Center, provocado por una furgoneta bomba en los cimientos de una de las torres, mató a seis personas. Los atentados terroristas a las embajadas estadounidenses en 1998 en Kenia y Tanzania causaron la muerte de 213 personas, incluidas doce estadounidenses. Y en el año 2000 el atentado contra el USS Cole, en el cual se utilizó una lancha bomba suicida, mató a 17 marinos estadounidenses.

La idea de ataques suicidas con aviones de pasajeros secuestrados vino de Jálid Sheij Mohámed, quien se la presentó por primera vez a Osama bin Laden en 1996, tras fracasar un gran proyecto similar abortado por la policía filipina en 1995 denominado operación Bojinka. En 1999 un grupo de musulmanes radicalizados que vivían en Hamburgo (Alemania) y a los que se apodó posteriormente como "célula de Hamburgo» viajaron a Afganistán a recibir formación para luchar contra los rusos en la segunda guerra chechena. En ese momento Bin Laden los captó y en los siguientes meses financió su entrenamiento a fin de convencerlos de realizar operaciones suicidas con aviones para chocarlos contra edificios emblemáticos de EE.UU. Bin Laden se inspiró en parte en el vuelo 990 de Egyptair en 1999, en el cual el piloto se suicidó y estrelló el avión en el mar.

El plan original de los atentados del 11 de septiembre de 2001 era secuestrar doce aviones, once de los cuales serían estrellados contra los siguientes edificios:


Posteriormente, debido a la cantidad de objetivos señalados, se consideró una operación inabarcable y se redujeron los objetivos de once edificios a cinco: las dos Torres Gemelas (que representaban la economía capitalista estadounidense y ya habían sufrido un atentado en 1993); el Pentágono (que representaba el poder militar); el Capitolio (que representaba el poder legislativo) y la Casa Blanca (que representa el poder ejecutivo). Sin embargo, el quinto avión nunca fue secuestrado porque el piloto suicida que lo iba a dirigir (Zacarias Moussaoui) fue detenido fortuitamente por el FBI el 16 de agosto de 2001 por cargos de inmigración irregular.

Alrededor de tres semanas antes de los ataques, los objetivos fueron asignados a cuatro equipos. El Capitolio tuvo como nombre en clave «La Facultad de Derecho». El Pentágono se denominó "La Facultad de Bellas Artes". El código del World Trade Center fue "La Facultad de Urbanismo".

La idea de secuestrar simultáneamente varios aviones no era nueva. En septiembre de 1970 sucedieron los secuestros de Dawson's Field cuando miembros del Frente Popular para la Liberación de Palestina secuestraron, en pocos días, cuatro aviones comerciales (más un 5.º intento de secuestro que fracasó) y los desviaron a Jordania y Egipto. Los rehenes fueron liberados días después y los aviones explotados intencionadamente. En el cine ya se había tratado la idea del choque de un avión contra un edificio en la película inglesa The Medusa Touch (1978) y en uno de los capítulos de la serie Los pistoleros solitarios (emitido el 4 de marzo de 2001) se secuestraba un Boeing 727 para estrellarlo contra una de las Torres Gemelas. Los autores de la masacre de la Escuela Secundaria de Columbine (1999) también planearon el secuestro y colisión de un avión en Nueva York. En Nueva York ya se había producido, 56 años antes, el impacto de un avión contra un rascacielos: el choque del B-25 contra el Empire State Building en 1945, con catorce víctimas fatales.

Cuatro aviones con 265 pasajeros fueron secuestrados mientras volaban hacia California desde el Aeropuerto Internacional Logan de Boston, el Aeropuerto Internacional Washington-Dulles y el Aeropuerto Internacional Libertad de Newark. Los cuatro aviones tenían como destino el estado de California, los tres primeros hacia Los Ángeles y el último a San Francisco, por lo que sus depósitos de combustible iban llenos con unos 91.000 litros (unos 65.455 kg). Los dos primeros aviones impactaron contra las Torres Gemelas del World Trade Center, el tercero contra el Pentágono, en el Condado de Arlington, cerca de Washington D.C., y el cuarto en campo abierto en Shanksville (Pensilvania).

Fueron revelados testimonios desde los propios aviones, en los cuales los secuestradores habían tomado el control de estos usando simples navajas con las que mataron a azafatas de vuelo y al menos a un piloto o pasajero. Según las investigaciones de la Comisión del 11-S, se tiene también constancia de que usaron algún tipo de aerosol para retener a los pasajeros en la cabina de Primera Clase. Asimismo, se amenazó con la presencia de una bomba en tres de los aviones, pero no en el vuelo que impactó en El Pentágono. Según las conclusiones de esta comisión, los avisos de bomba eran probablemente falsos.
En el cuarto avión, el vuelo 93 de United Airlines, la caja negra reveló que los pasajeros, después de enterarse de que el resto de aviones habían sido estrellados deliberadamente, trataron de retomar el control del aparato. Los secuestradores reaccionaron moviendo el avión en un fallido intento para someter a los pasajeros. De acuerdo con la grabación 9-1-1, uno de los pasajeros, Todd Beamer, pidió a la persona con quien hablaba por teléfono que rezara con él y al finalizar simplemente dijo «"Let's roll»". Poco después, el avión se estrelló en un campo cercano a Shanksville, en Pensilvania, a las 10:03 a.m. hora local. Existe un debate acerca del momento exacto en que el avión chocó contra el suelo, ya que los registros sísmicos marcan el impacto a las 10:06 a.m. Posteriormente, el terrorista de Al Qaeda capturado, Jálid Sheij Mohámed, dijo que el vuelo 93 tenía como objetivo el Capitolio de los Estados Unidos.

La exclamación póstuma de Beamer comenzó a ser ampliamente usada en los Estados Unidos después de los ataques. Neil Young compuso una canción con ese título como tributo a las víctimas. Por su parte, la viuda de Beamer patentó la frase como marca registrada.

Los atentados extendieron la confusión en todo el país. A lo largo del día se sucedió la publicación de todo tipo de informes y noticias contradictorias sin confirmar. Una de las más recurrentes fue la de que había estallado un coche bomba en la sede central del Departamento de Estado de los Estados Unidos en Washington D.C. Esta falsa noticia pasó por las agencias de noticias y llegó a ser publicada por varios periódicos ese mismo día. Otro informe, difundido por la agencia Associated Press, afirmaba que el vuelo 1989 de la compañía Delta Air Lines, un Boeing 757, había sido secuestrado también. La noticia resultó ser también un error: el avión había sido considerado por unos instantes en riesgo de secuestro, pero finalmente respondió a los controladores aéreos y aterrizó con normalidad en el aeropuerto de Cleveland, Ohio.


Las muertes se contaron por miles, pereciendo exactamente 2996 personas, incluyendo 265 muertos en los cuatro aviones estrellados (ninguno de sus ocupantes sobrevivió); 2606 en Nueva York, tanto dentro de las Torres Gemelas como en la base de las mismas, y 125 muertos dentro del edificio del Pentágono. Entre las víctimas se contaban 343 bomberos del departamento de bomberos de Nueva York, 23 policías del departamento de policía de la ciudad y 37 policías de la autoridad portuaria de Nueva York y Nueva Jersey. En 2013 todavía permanecían 24 personas más entre la lista de desaparecidos.

El banco de inversiones Cantor Fitzgerald, que estaba ubicado en los pisos 101 a 105 de la Torre Norte del World Trade Center, perdió 658 empleados, una cantidad considerablemente mayor a la de otros empleadores. Howard Lutnick presidente de Cantor Fitzgerald, se salvó de morir en los ataques ya que esa mañana llevó a su hijo a su primer día de clases, pero perdió en los atentados a su hermano menor Gary de 36 años, que trabajaba en Cantor en la planta 104.

Marsh & McLennan una compañía de seguros que estaba ubicada inmediatamente debajo de Cantor Fitzgerald en los pisos 93 a 100 en la Torre Norte, perdió 358 empleados y Aon Corporation en la Torre Sur perdió 175 empleados que también fueron asesinados. El Instituto Nacional de Estándares y Tecnología (NIST) estimó que unos 17.400 civiles estaban en el complejo del World Trade Center en el momento de los ataques. La Autoridad Portuaria de Nueva York y Nueva Jersey sugiere que 14.154 personas estaban en las Torres Gemelas a las 8:46 a.m., cuando el primer avión, el vuelo 11 de American Airlines golpeó a la Torre Norte. La mayoría de las personas por debajo de las zonas de impacto evacuaron los edificios de manera segura.

Semanas después del los ataques, se estimó que el número de muertos era de más de 6,000, más del doble del número de muertes finalmente confirmadas. La ciudad de Nueva York solo pudo identificar los restos de alrededor de 1.600 víctimas del World Trade Center. La oficina del servicio médico forense recolectó «alrededor de 10,000 fragmentos de huesos y tejidos no identificados que no se pueden comparar con la lista de muertos». En 2006, trabajadores que se estaban preparando para demoler el edificio dañado del Deutsche Bank, encontraron en la azotea del edificio fragmentos de huesos. En 2010, un equipo de antropólogos y arqueólogos buscaron restos humanos y artículos personales en el relleno sanitario Fresh Kills, donde se recuperaron 72 restos humanos más, con lo que en total fueron encontrados 1,845. El análisis de ADN continúa en un intento de identificar víctimas adicionales. Los restos se encuentran almacenados en el Memorial Park, fuera de las instalaciones del médico forense de la ciudad de Nueva York. Se esperaba que los restos de las víctimas fueran trasladados en 2013 a un depósito detrás de una pared en el museo del 11 de septiembre. En julio de 2011, un equipo de científicos de la Oficina del Médico Forense todavía estaba tratando de identificar restos, con la esperanza de que una tecnología mejorada les permita identificar a otras víctimas. El 7 de agosto de 2017, la víctima 1.641 fue identificada como resultado de la tecnología de ADN recientemente disponible, y la víctima 1.642 fue identificada el 26 de julio de 2018. Hasta la fecha y a casi 20 años de los atentados aun quedan por identificar los restos de 1.111 víctimas.

Según las cifras presentadas por el Departamento de Salud en enero de 2002, 139 latinoamericanos estuvieron entre los muertos del atentado terrorista de Al-Qaeda contra las Torres Gemelas, representando un 16% del total. De estos, 25 eran nacionales de la República Dominicana, 21 de Argentina, 18 de Colombia, 13 de Venezuela, 11 de Ecuador, 7 de El Salvador, 6 de Cuba, 3 de Bolivia, 3 de Brasil y 2 de Chile. En otros sitios, se habla de 15 muertos de México, así como otros de Honduras, Jamaica, Perú, Paraguay, Uruguay y Guyana.

Los atentados supusieron el ataque terrorista de mayor importancia contra los Estados Unidos, al superar al atentado de Oklahoma City cometido por los terroristas de ultraderecha Timothy McVeigh y Terry Nichols, que causó 168 muertos, y los ataques llevados a cabo por células de Al-Qaeda en 1998 contra las embajadas estadounidenses en Kenia y Tanzania.

Según la Comisión del 11-S, aproximadamente 16 000 personas se encontraban en las zonas de impacto del complejo del World Trade Center al momento de los ataques. La gran mayoría de ellos sobrevivió gracias a las labores de evacuación antes del derrumbe de las torres.

La española Alicia Esteve se hizo pasar por superviviente del atentado. Adoptó una identidad falsa (Tania Head) e incluso llegó a ser presidenta de la Red de Supervivientes de la catástrofe del World Trade Center. Gracias al diario The New York Times, se descubrió su fraude; y gracias al diario español La Vanguardia se reveló su verdadera identidad.

Tres edificios del complejo del World Trade Center se derrumbaron debido a fallos estructurales el día de los ataques. La Torre Sur cayó a las 9:59 (GMT-4 hora local de Nueva York), tras estar en llamas durante 56 minutos en un fuego causado por el impacto del vuelo 175 de United Airlines a las 9:03. La Torre Norte cayó a las 10:28, tras estar en llamas aproximadamente 102 minutos en un fuego causado por el impacto del vuelo 11 de American Airlines a las 8:46. Un tercer edificio, el World Trade Center 7 (WTC 7), se derrumbó a las 17:20, al parecer tras haber sido seriamente dañado por los escombros de las Torres Gemelas al caer, junto con una serie de incendios. Numerosos edificios adyacentes al complejo también sufrieron daños sustanciales, se incendiaron y tuvieron que ser demolidos. El edificio del "Deutsche Bank" fue la única estructura grande que sufrió daños e incendios en la zona cero que al 2006 aún no había sido totalmente demolida. La demolición se llevó a cabo en febrero de 2011.

Una investigación técnica federal del edificio y de seguridad de los derrumbes de la Torres Gemelas y el WTC 7 fue realizada por el "National Institute of Standards and Technology" (NIST) del Departamento de Comercio de los Estados Unidos. Los objetivos de esta investigación, que tomó en cuenta la construcción del edificio, los materiales usados, y las condiciones técnicas que contribuyeron al derrumbe, se dieron por cumplidos el 6 de abril de 2005. La investigación estableció una serie de bases para:
El informe concluye que la protección contra incendios de las infraestructuras de acero de las Torres Gemelas salió desprendida con el impacto inicial de los aviones y que, si esto no hubiera ocurrido, las torres probablemente habrían permanecido erguidas. Los incendios debilitaron las cerchas que sostenían los pisos, e hicieron que los pisos se combaran. A su vez, los pisos al combarse, tiraron de las columnas de acero exteriores hasta el punto que las columnas exteriores se inclinaron hacia el interior. Con los daños a las columnas principales, las columnas exteriores torcidas no pudieron soportar el peso de los edificios, produciéndose el derrumbe. Además, el informe afirma que los huecos de las escaleras de las torres no fueron reforzados adecuadamente para proporcionar una salida de emergencia para las personas que se encontraban por encima de las zonas de impacto. El NIST declaró que el informe final sobre el derrumbe del WTC 7 aparecería en un informe separado.

Aparte del derrumbe de las Torres Gemelas y el WTC 7, otros 23 edificios fueron dañados. Actualmente al área ocupada por los restos materiales de las Torres Gemelas se la conoce como "Zona Cero" y esta construido en el lugar el National September 11 Memorial & Museum un memorial y museo que contiene dos piscinas de agua, en los cimientos que ocupaban antiguamente las Torres Gemelas, que tiene a sus alrededores paneles de bronce con los nombres de las víctimas que murieron en los atentados del 26 de febrero de 1993 y del 11 de septiembre de 2001 y El nuevo edificio que reemplaza a las desaparecidas Torres Gemelas llamado One World Trade Center fue inaugurado oficialmente el 3 de noviembre de 2014.

Aparte de las dos Torres Gemelas, de 110 plantas cada una, cinco edificios del World Trade Center resultaron destruidos o seriamente dañados, entre ellos el WTC 7 y el hotel Marriott, cuatro estaciones del metro de Nueva York y la iglesia cristiana ortodoxa de San Nicolás. En total, en Manhattan 32 edificios sufrieron daños. Más tarde, el Deutsche Bank Building situado en Liberty Street y Borough of Manhattan Community College's Fiterman Hall en el 30 de West Broadway tuvieron que ser demolidos debido al estado en que quedaron, que los hacía inhabitables. Actualmente, están a la espera de ser reconstruidos. Varios equipos de comunicaciones también sufrieron daños. Sin ir más lejos, la antena de telecomunicaciones de la Torre Norte cayó con su derrumbe, mientras que otras antenas de radio de torres colindantes resultaron también gravemente dañadas.

En el condado de Arlington, una porción del Pentágono fue gravemente dañada por el fuego y el impacto del avión. Al cabo de un rato, una sección entera del edificio se derrumbó.

19 hombres árabes embarcaron en los cuatro aviones, cinco en cada uno, excepto el vuelo 93 de United Airlines, que tuvo cuatro secuestradores. De los atacantes, 15 eran de Arabia Saudita, dos eran de los Emiratos Árabes Unidos, uno era de Egipto, y uno de Líbano. En general, eran gente con estudios y de familias acomodadas.

La lista completa es:

En el vuelo 11 de American Airlines:

En el vuelo 175 de United Airlines:

En el vuelo 77 de American Airlines:

En el vuelo 93 de United Airlines:

Veintisiete miembros de la organización terrorista Al-Qaeda, trataron de entrar a los Estados Unidos para formar parte de los atentados, pero muchos de ellos fueron arrestados o sus visas para acceder al país fueron denegadas. Finalmente, solo 19 miembros de Al-Qaeda tuvieron éxito en los trámites de sus visas y fueron los que participaron en los ataques. Los otros ocho son llamados a menudo «el vigésimo secuestrador»:

El 3 de mayo de 2006, un jurado federal rechazó la pena de muerte para los acusados y los condenó a seis cadenas perpetuas en prisión sin libertad condicional.</p

En su juicio, el agente del FBI, Greg Jones, testificó que con anterioridad a los ataques ya había avisado a su supervisor Michael Maltbie, de que «evitara que Zacarias Moussaoui estrellara un avión contra el World Trade Center». Maltbie se había negado a actuar en 70 peticiones de otro agente, Harry Samit, para poder buscar en el ordenador de Moussaoui.

Otros miembros de Al-Qaeda que intentaron participar pero no lo lograron fueron Saeed al-Ghamdi (no confundir con el secuestrador del mismo nombre que sí intervino), Mushabib al-Hamlan, Zakariyah Essabar, Ali Abdul Aziz Ali, y Tawfiq bin Attash. Según el Informe de la Comisión del 11-S, Khalid Sheikh Mohammed, autor intelectual del ataque, quería echar al menos a un miembro del equipo (Khalid al-Mihdhar) pero Osama bin Laden se opuso.

Una semana después del 11-S, el 18 de septiembre, comenzaron una serie de atentados terroristas utilizando carbunco, una bacteria mortal. Durante el transcurso de varias semanas, hasta el 9 de octubre, los terroristas utilizaron el correo para exponer el carbunco a periodistas, políticos y empleados civiles en Nueva York, Nueva Jersey, Washington D.C. y Florida. Un total de 22 personas fueron contaminadas con carbunco, de las cuales cinco murieron.

Estos ataques acentuaron la inseguridad ciudadana y el clima de terror producidos por los atentados del 11 de septiembre.

Los autores de los ataques nunca pudieron ser identificados. El vicepresidente de EE.UU., Dick Cheney, afirmó que no le sorprendería encontrar a Osama bin Laden detrás de estos atentados y sostuvo que:
Si bien los organismos de seguridad de Estados Unidos no pudieron identificar a los terroristas el Procurador General John Ashcroft mencionó al Dr. Steven Hatfill como una «persona de interés» potencialmente relacionada con los mismos, aunque no se le levantaron cargos.

Más adelante se demostró que las esporas provenían de un laboratorio del ejército de Estados Unidos.

Los ataques tuvieron un impacto significativo en los mercados estadounidenses y mundiales. La Reserva Federal redujo temporalmente sus contactos con bancos por la falta del equipo perdido en el distrito financiero de Nueva York. En horas, se recuperó el control sobre el suministro de dinero, con la consecuente liquidez para los bancos. Los índices bursátiles New York Stock Exchange (NYSE), American Stock Exchange y NASDAQ no abrieron el 11 de septiembre y permanecieron cerrados hasta las 15:30 del 17 de ese mismo mes. Los sistemas del NYSE no fueron dañados por el ataque, pero los daños en las redes telefónicas del sistema financiero del World Trade Center impidieron que funcionara.

Cuando los mercados reabrieron el 17 de septiembre de 2001, tras el mayor parón desde la Gran Depresión, el índice Dow Jones Industrial Average cayó 684 puntos (7,1 %), hasta 8920, en su mayor caída en un solo día. Al final de la semana, el Dow Jones había perdido 1369,7 puntos (14,3 %), su mayor caída en una semana. Desde entonces Wall Street permanece protegido contra un atentado terrorista.
La economía del Bajo Manhattan, tercer distrito económico de Estados Unidos, quedó devastada. El 30% del suelo de oficinas (2,7 millones de m³), muchos de ellos de clase A, fue destruido o dañado. El edificio del Deutsche Bank, vecino de las Torres Gemelas tuvo que ser cerrado por los daños y demolido. La electricidad, teléfono y gas fueron cortados. Se restringió la entrada de personas en el Soho y Bajo Manhattan. El traslado de muchos de los puestos de trabajo ubicados anteriormente aquí, hacia Midtown y Nueva Jersey se aceleró. Varias opiniones afirman que los ingresos fiscales de la zona no se recuperarán.

La reconstrucción se ha enfrentado a la falta de acuerdo sobre las prioridades. Por ejemplo, el alcalde Bloomberg hizo de la candidatura de Nueva York para los Juegos Olímpicos de 2012 el eje de su plan de desarrollo 2002-2005, mientras que el gobernador Pataki delegó en la Corporación para el Desarrollo del Bajo Manhattan, duramente criticada por los escasos logros obtenidos con los amplios fondos recibidos. En los solares de los edificios colindantes (WTC 7) se comenzó a construir un nuevo complejo de oficinas en 2006. El One World Trade Center se terminó en el año 2014 y alcanza 541 m de altura, lo que le convirtió en el edificio más alto de la ciudad de Nueva York. Tres torres más se construyeron en la zona este del World Trade Center, las cuales fueron terminadas entre los años 2007 y 2012.

Las pérdidas del sector aéreo fueron significativas: el espacio aéreo estadounidense permaneció cerrado durante varios días por primera vez en su historia, y en varios países como Canadá. Tras su reapertura, las compañías aéreas sufrieron una disminución de su tráfico. Se estima que el negocio perdió un 20% de su tamaño, y los problemas financieros de las compañías aéreas estadounidenses se agravaron, dando lugar a una crisis económica.

Tras los atentados del 11 de septiembre de 2001, Estados Unidos apostó por la desregulación de los mercados, las bajadas de impuestos y de tipos de interés y la expansión del crédito, lo cual causó una burbuja inmobiliaria en las denominadas hipotecas subprime. A eso había que sumar los gastos multimillonarios en la guerra de Afganistán y la guerra de Irak que pudieron costar desde 2 billones de dólares hasta 6 billones en total. La burbuja finalmente empezó a desmoronarse en agosto de 2007 y colapsó de forma brutal en septiembre de 2008 cuando quebró el banco Lehman Brothers.

La economía estadounidense entró en una fase de recesión desde 2001 como resultado de la inseguridad y la desconfianza creciente en la seguridad del mundo occidental después de una década de crecimiento prácticamente ininterrumpido, a pesar de que la actividad económica ya había mostrado señales de agotamiento desde 1998, efecto de la crisis asiática, con la pérdida de más de un millón de empleos en el sector industrial entre los años 1999 y 2000.

Los ataques terroristas agravaron la situación al reducirse fuertemente el consumo como consecuencia del estado de psicosis de la población, que evitaba visitar sitios concurridos o viajar. El sector aéreo fue uno de los más afectados, pues la demanda de vuelos comerciales se redujo drásticamente, debido sobre todo al temor de que se repitieran las acciones terroristas, y también a la resistencia del público a someterse a las medidas rigurosas de seguridad en los aeropuertos. En un intento por aliviar esta situación, el Congreso aprobó un paquete financiero de 15 000 millones de dólares para el sector aéreo, en tanto que el gobierno de Bush adelantó un recorte adicional de los impuestos para revitalizar el consumo; esta medida tuvo efectos negativos en el presupuesto, ya de por si mermado por los gastos de la guerra.

Las miles de toneladas de escombros tóxicos resultado de la caída de las Torres Gemelas están compuestos por: un 50 % de material no fibroso y escombros de construcción; un 41 % de vidrio y fibra; un 9,2 % de celulosa y un 0,8 % de asbesto, plomo y mercurio. Además se liberaron niveles sin precedentes de dioxinas e hidrocarburos policíclicos aromáticos en los fuegos que ardieron durante los tres meses siguientes. Esto causó varias enfermedades en los equipos de rescate y reconstrucción que trabajaron en la zona cero, incluyendo la muerte del agente James Zadroga. Los efectos se han extendido también a la salud de los habitantes del Bajo Manhattan y la cercana Chinatown. Según una especulación científica, la exposición a varios productos tóxicos y los contaminantes del aire circundante a las Torres tras el derrumbe del WTC podría tener efectos negativos en el desarrollo fetal.

Debido a este riesgo potencial, un notable centro de salud de niños está actualmente analizando a los hijos de madres que estaban embarazadas durante el derrumbe del WTC y que vivían o trabajaban cerca de las torres. El personal de este estudio evalúa a los niños usando test psicológicos cada año y entrevista a las madres cada seis meses. El propósito del estudio es determinar si hay diferencias significativas en el desarrollo y la salud de los niños de las madres que estuvieron expuestas a los productos tóxicos, frente a niños cuyas madres no estuvieron expuestas a la contaminación.

En mayo de 2007, el máximo responsable forense de Nueva York, Charls F. Hirst admitió que la muerte de una abogada se debió a la exposición a la nube tóxica, lo que constituyó el primer reconocimiento oficial de una muerte como consecuencia del polvo tras la caída de las Torres Gemelas. Declarando que: ""Casi con toda certeza, más allá de una duda razonable, la exposición al polvo del World Trade Center contribuyó a la muerte de Dunn-Jones"". Un total de 7.300 trabajadores de la "zona cero" presentaron denuncia y reclaman compensaciones a la ciudad por la exposición y manipulación de las sustancias tóxicas de las Torres.

El FBI, trabajando junto el Departamento de Justicia de los Estados Unidos, identificó a 19 secuestradores fallecidos en apenas 72 horas. Pocos habían tratado de ocultar sus nombres o tarjetas de crédito, y eran casi los únicos pasajeros de origen árabe en los vuelos. Así, el FBI pudo determinar sus nombres y en muchos casos detalles, como la fecha de nacimiento, las residencias conocidas o posibles, el estado del visado, y la identidad específica de los sospechosos pilotos. El FBI publicó fotos de los 19 secuestradores, junto con la información sobre las posibles nacionalidades y sus apodos.

Las pesquisas del gobierno de los Estados Unidos incluyeron la operación del FBI PENTTBOM, la mayor de la historia con más de 7000 agentes involucrados. Los resultados de esta determinaron que al-Qaeda y Osama bin Laden tenían la responsabilidad de los atentados. A idéntica conclusión llegaron los estudios encargados por el gobierno británico. Su declaración de una guerra santa contra los Estados Unidos, y una fatwa firmada por Bin Laden y otros llamando a matar a civiles estadounidenses en 1998 desde Afganistán, son consideradas por muchos como evidencia de su motivación para cometer estos actos.

El 16 de septiembre de 2001, Bin Laden negó cualquier participación en los atentados leyendo un comunicado que fue emitido por el canal de satélite catarí Al Jazeera y posteriormente emitido en numerosas cadenas estadounidenses:
Sin embargo, en noviembre de 2001, las fuerzas de los Estados Unidos encontraron una cinta de video casera de una casa destruida en Jalalabad, Afganistán, en donde Osama bin Laden habla con Khaled al-Harbi. En varias secciones de la cinta, como en el párrafo citado a continuación, Bin Laden reconoce haber planeado los ataques:
El 27 de diciembre de 2001, se difundió otro vídeo de bin Laden en el que afirma: 

Poco antes de las elecciones presidenciales de Estados Unidos de 2004, en un comunicado por vídeo, bin Laden reconoció públicamente la responsabilidad de al-Qaeda en los atentados de Estados Unidos, y admitió su implicación directa en los ataques. Dijo que los atentados se llevaron a cabo porque: ...somos gente libre que no acepta injusticias, y queremos recuperar la libertad de nuestra nación. 

En una cinta de audio transmitida en Al Jazeera el 21 de mayo de 2006, bin Laden dijo que dirigió personalmente a los 19 secuestradores. Otro video obtenido por Al Jazeera en septiembre de 2006 muestra Osama bin Laden con Ramzi Binalshibh, así como a dos secuestradores, Hamza al-Ghamdi y Wail al-Shehri, haciendo preparaciones para los atentados.

La Comisión Nacional sobre los Ataques Terroristas contra Estados Unidos fue formada por el gobierno de los Estados Unidos y es habitualmente conocida como Comisión 11-S. Publicó su informe el 22 de julio de 2004, concluyendo que los atentados estuvieron concebidos y llevados a cabo por miembros de al-Qaeda. En el informe de la Comisión se señala que:
El 11 de septiembre de 2007, bin Laden emitió otro comunicado en el que decía:
""Califico de héroes a los pilotos de los aviones"" 

Alrededor de 1 200 extranjeros han sido arrestados y encarcelados en secreto en relación con la investigación de los ataques del 11 de septiembre, aunque el gobierno no ha divulgado el número exacto.

Los métodos utilizados por el Estado para investigar y detener sospechosos han sido severamente criticados por organizaciones de derechos humanos como Human Rights Watch y jefes de gobierno como la canciller alemana Angela Merkel.

Hasta el momento, el gobierno de Estados Unidos no ha hallado a ninguno de los partícipes de la conspiración que realizaron las operaciones en tierra.

El 26 de septiembre de 2005, la Audiencia Nacional de España dirigida por el juez Baltasar Garzón condenó a Abu Dahdah a 27 años de prisión por conspiración en los atentados del 11-S y por ser parte de la organización terrorista Al Qaeda. Al mismo tiempo, otros 17 miembros de Al Qaeda fueron condenados a penas de entre 6 y 12 años. El 16 de febrero de 2006, el Tribunal Supremo rebajó la pena a Abu Dahdah a 12 años porque consideró que su participación en la conspiración no estaba probada.

Según las conclusiones de las investigaciones oficiales del gobierno estadounidenses, los ataques cumplían con la intención declarada de al-Qaeda, expresada en la fatwa de 1998 de Osama bin Laden, Ayman al-Zawahiri, Abu-Yasir Rifa'i Ahmad Taha, Shaykh Mir Hamzah, y Fazlur Rahman (emir del Movimiento Yihadista de Bangladés, Fazlur Rahman).

La carta en la que se listan los tres "crímenes y pecados" cometidos por los estadounidenses en opinión de sus autores contenía los siguientes motivos de los ataques:
En la misma carta se estableció que los Estados Unidos:
La Primera Guerra del Golfo, el posterior embargo sobre Irak, y el bombardeo de este país por Estados Unidos son citadas en la carta de 1998 como prueba de esas alegaciones. Para desaprobación de musulmanes moderados, la fatwa cita textos islámicos como exhortación de la acción violenta contra militares y ciudadanos estadounidenses hasta que los agravios alegados se solucionen: estableciendo que "los ulemas a lo largo de la historia han estado de acuerdo en que la Yihad es un deber individual si los enemigos destruyen los países musulmanes."

Unas declaraciones de Al Qaeda grabadas tras el 11 de septiembre confirmaron las suposiciones estadounidenses sobre la autoría. En un vídeo de 2004, aparentemente reconociendo la responsabilidad de los ataques, Bin Laden afirmó que la Guerra del Líbano de 1982, de la que considera responsable a los Estados Unidos, le impulsó a desarrollar los atentados. En el vídeo, también hizo saber que, con ellos, quería "restaurar la libertad de nuestra nación" para "castigar al agresor" e infligir daños en la economía estadounidense. Declaró que uno de los objetivos de su guerra santa era "desangrar Estados Unidos hasta la bancarrota." Bin Laden dijo también:
El informe de la Comisión del 11S determina que la animosidad contra los Estados Unidos de Khalid Shaikh Mohammed, principal arquitecto de los ataques, procedía «no de sus experiencias como estudiante, sino de su violento desacuerdo con la política exterior estadounidense en favor de Israel». Los mismos motivos se han imputado a los dos pilotos suicidas que estrellaron los aviones en el World Trade Center: Mohamed Atta, quien fue descrito por Ralph Bodenstein (compañero suyo de trabajo y viajes) como «principalmente imbuido por la protección de los Estados Unidos a las políticas israelíes en la región». Marwan al-Shehhi se dice que explicó su estado de ánimo con las palabras «¿cómo puede la gente reír cuando hay personas muriendo en Palestina?»

En contraste con estas conclusiones, la administración Bush redujo los motivos del ataque al "odio a la libertad y la democracia, ejemplificados por los Estados Unidos".

Según el experto antiterrorista Richard A. Clarke, los conflictos internos en el mundo musulmán son la causa de los atentados del 11 de septiembre. Específicamente, Bin Laden y otros residentes de Arabia Saudí y Egipto creen que la mayoría de los gobiernos de Oriente Medio son apóstatas, que no siguen su modelo de piedad islámica, dado que ninguno es un califato. Inspirados por el teólogo egipcio Sayyid Qutb, Bin Laden y sus seguidores sostienen que es un deber para los musulmanes el establecer un califato en Oriente Medio.

Partiendo de esas creencias, Bin Laden diseñó un plan para establecer este califato, comenzando por un ataque a los Estados Unidos. Esto les obligaría a aumentar la presión militar y económica sobre Oriente Medio, uniendo así a todos los musulmanes. La oleada religiosa popular llevaría a los musulmanes conservadores a tomar el control.

De acuerdo con Michale Doran, esta meta queda demostrada por el frecuente uso de "espectacular" por Bin Laden en sus declaraciones. De acuerdo a su hipótesis, Bin Laden esperaba provocar una reacción visceral y emotiva de los Estados Unidos, con el fin de asegurarse una contrarrespuesta por los ciudadanos árabes.

En las horas siguientes a los ataques, se inició una operación de búsqueda y rescate a gran escala con más de 350 perros especialmente entrenados. Solo se logró rescatar con vida a 20 sobrevivientes malheridos bajo los escombros del World Trade Center y en las semanas posteriores se hizo evidente que ya no se iban a hallar más.

La recuperación de cadáveres llevó meses. Simplemente el apagar todos los fuegos que ardían entre los escombros se demoró semanas, mientras que el desescombro completo no terminó hasta mayo de 2002. Se instalaron miradores provisionales para observar el trabajo de los equipos, que fueron retirados el 30 de mayo de 2002.

Asimismo, se iniciaron muchas recogidas de fondos para ayudar a los sobrevivientes de los atentados y a los familiares de los fallecidos. Una vez cumplido el plazo para pedir las indemnizaciones (11 de septiembre de 2003) 2833 personas habían recibido el pago.

Los atentados del 11 de septiembre tuvieron un efecto abrumador sobre la población. Los cuerpos y fuerzas de seguridad (conocidos como "first responders") que intervinieron en las labores de rescate y auxilio, especialmente los bomberos, fueron aclamados como héroes. Policías y miembros de equipos de rescate de todo el país se concentraron en Nueva York para la recuperación de cuerpos. Las donaciones de sangre experimentaron un auge.

Otra respuesta aparentemente patriótica fue el aumento del racismo y odio contra las personas de origen árabe.
Otros grupos originarios de Oriente Medio fueron frecuentemente confundidos con árabes y fueron víctimas de esta xenofobia, particularmente los sijs, que tienen la tradición de llevar turbantes, signo que en Occidente se suele asociar al Islam.
Balbir Singh Sodhi fue asesinado por un disparo el 15 de septiembre, confundido con un musulmán.
Al menos otras ocho personas sufrieron la misma suerte.

Políticamente, la población respaldó masivamente al gobierno en su labor antiterrorista. Así, el índice de aprobación del presidente George W. Bush alcanzó el 86 %. El 20 de septiembre, el presidente habló ante la nación y la sesión conjunta del Congreso de los Estados Unidos, explicando los sucesos del día, la actuación de su gobierno en los 9 días transcurridos y sus planes de respuesta. El alcalde de Nueva York Rudy Giuliani fue aclamado tanto en Nueva York como en todo el país por su reacción a la catástrofe terrorista.

Tras los ataques, se registraron las huellas de 80000 árabes y musulmanes bajo la Alien Registration Act de 1940. De ellos, 8000 fueron entrevistados y 5000 extranjeros fueron detenidos bajo la resolución conjunta del Congreso de los Estados Unidos 107-40, que autorizó el uso de fuerza militar para detener y prevenir el terrorismo internacional en los Estados Unidos.

A causa de los atentados, la opinión pública se centró sobre todo en materia de seguridad nacional, e incluso se creó una nueva agencia federal a nivel de gabinete, el Departamento de Seguridad Nacional de los Estados Unidos, reorganizando así la lucha antiterrorista.

Asimismo se aprobó la Ley Patriótica , suspendiendo y limitando algunas libertades y derechos constitucionales con el fin de aumentar la seguridad interna de los Estados Unidos. Esta medida ha sido duramente criticada por defensores de los derechos civiles, que ven en ella una violación de la privacidad de los ciudadanos, además de una relajación del control judicial sobre los cuerpos de inteligencia.

El 11-S fue también el argumento utilizado por el gobierno de Bush para iniciar una nueva operación de la Agencia de Seguridad Nacional con el objetivo de registrar las comunicaciones de ciudadanos estadounidenses con el extranjero.

Los cambios en la vida cotidiana de la población y la exigencia de un compromiso directo con la seguridad han sido considerables. En cada medio de transporte se han colocado carteles y altavoces que repiten la consigna «If you see something, say something» («Si ves algo, di algo»).

La Comisión Nacional sobre los Atentados Terroristas contra los Estados Unidos (en inglés National Commission on Terrorist Attacks Upon the United States y más vulgarmente la Comisión del 11-S), presidida por el exgobernador de Nueva Jersey Thomas Kean, fue formada a finales de 2002 para preparar un informe completo de los atentados y de las circunstancias con ellas relacionadas, incluyendo desde la preparación a la respuesta inmediata de las autoridades estadounidenses. Dicho informe fue publicado finalmente el 22 de julio de 2004.

Los ataques tuvieron ramificaciones globales. Gobiernos, asociaciones y medios de comunicación lo condenaron en todo el mundo. Especialmente famoso fue el titular del periódico francés "Le Monde": "Nous sommes tous Américains" (Somos todos americanos), en referencia a Estados Unidos.

Tras los atentados, la administración Bush declaró la llamada guerra contra el terrorismo, con los objetivos de llevar a Osama bin Laden y Al-Qaeda a la justicia y prevenir la acción de redes terroristas anti-estadounidenses. Estos objetivos se conseguirían a través de sanciones económicas y militares contra estados percibidos como protectores de terroristas y aumentando la vigilancia e inteligencia global.

Aproximadamente un mes después de los ataques, los Estados Unidos de América, con la colaboración de una coalición internacional, invadió Afganistán, cuyo gobierno había dado apoyo a fuerzas de Al-Qaeda. Particularmente importante fue el apoyo del gobierno pakistaní, que tras los atentados se alineó con Estados Unidos, cediéndole bases para la guerra en Afganistán y arrestando a más de 600 sospechosos de colaborar con al-Qaeda.

Tras el 11-S, numerosos gobiernos aprobaron leyes antiterroristas o endurecieron las ya existentes, particularmente de cara al terrorismo islámico. Entre ellos estuvieron el Reino Unido, España, la India, Australia, Francia, Alemania, Indonesia, China, Canadá, Rusia, Pakistán, Jordania, Mauricio, Uganda y Zimbabue. Una consecuencia de dichas medidas fue la congelación de cuentas bancarias asociadas a Al-Qaeda.

Los servicios de seguridad e inteligencia de varios países (Italia, Malasia, Indonesia, Filipinas...) arrestaron tras los atentados a personas relacionadas con varias células de Al-Qaeda. Dichas medidas han sido objeto de críticas varias, que las ven como un atentado a las libertades individuales, como un recorte de derechos y, en general, como un aumento de la injerencia del Estado en la intimidad de los ciudadanos.

Particularmente conocido es el campo de detención de Guantánamo, base estadounidense en Cuba, donde se encuentran numerosos prisioneros capturados como "combatientes ilegales". Dicho centro, criticado por Amnistía Internacional, la Unión Europea, la ONU y numerosas organizaciones más, ha sido reiteradamente denunciado como una violación de los Derechos Humanos.

En México, el Presidente de la República, Vicente Fox Quesada declaró la cancelación total de los festejos patrios del 15 de septiembre correspondientes al día de la Independencia Nacional; también rechazó rotundamente los ataques terroristas de los cuales Estados Unidos era blanco y manifestó su apoyo al presidente George W. Bush.

Por su parte, los gobiernos de Guatemala y Chile declararon luto nacional en solidaridad a los familiares de las víctimas. 

En Roma, Italia, el Santo Padre Juan Pablo II ofreció una oración en memoria de las víctimas del atentado. 
El primer paso dado por EE. UU. en la Guerra contra el Terrorismo fue la invasión de Afganistán el 7 de octubre de 2001 por fuerzas de la OTAN y la Alianza del Norte con apoyo de las Naciones Unidas, ante la negativa del gobernante régimen talibán de entregar a Osama bin Laden, que supuestamente se había refugiado en ese país.

El 13 de noviembre de 2001, la capital Kabul fue tomada por la Alianza del Norte y el gobierno quedó en manos de E.E.U.U/OTAN y la Alianza del Norte. Desde entonces, Al-Qaeda y los talibanes se han unido y reorganizado como guerrilla insurgente.

Casi diez años después de los atentados, el lunes 2 de mayo de 2011, Osama bin Laden fue asesinado por tropas de élite estadounidenses en Abbottabad, Pakistán.

El segundo paso de la Guerra contra el Terrorismo de EE.UU. fue la invasión de Irak el 20 de marzo de 2003. Esta acción militar fue realizada por Estados Unidos y Gran Bretaña sin autorización de las Naciones Unidas. Además España, Italia y otros países, se aliaron con EE.UU. en esta acción y enviaron ayuda humanitaria a la zona. Estados Unidos sostuvo que la invasión era indispensable debido a que Irak poseía armas de destrucción masiva ocultas. La invasión desencadenó una guerra, con cientos de muertos, y causó el derrocamiento del gobierno encabezado por Saddam Hussein el 9 de abril de 2003. Una vez controlado el país, no se encontraron armas de destrucción masiva. Estados Unidos sostuvo entonces que la razón de la invasión se debía a que existían informaciones de los servicios de inteligencia que permitían suponer que Saddam Hussein mantenía relaciones secretas con Al-Qaeda. Recientes informes indican que nunca hubo una relación de Hussein con Al-Qaeda, y el presidente Bush trató de relacionar a Irak con la guerra contra el Terrorismo.

Desde entonces, varios grupos iraquíes opositores a la invasión han organizado un movimiento de resistencia que se ha mostrado muy activo en la realización de ataques contra objetivos militares. Paralelamente, después de la invasión, Al Qaeda también se ha podido instalar en Irak, en donde realiza fundamentalmente atentados de naturaleza terrorista.

Al día de hoy, las consecuencias continúan al haberse detonado una guerra civil sectaria "no declarada", que tiene como consecuencia la muerte de más de 34 000 civiles (solamente en 2006, según la ONU) y según cifras de Acnur, hay 1,7 millones de iraquíes desplazados internamente y otros dos millones que han huido a países vecinos. Además, a junio de 2007 las bajas del ejército de los Estados Unidos ascienden a más de 4000 caídos.

En los días siguientes a los ataques terroristas del 11 de septiembre de 2001 se realizaron varias vigilias y homenajes a las víctimas alrededor del mundo. Muchas personas colocaron fotografías de las víctimas y desaparecidos en la Zona Cero. Un testigo declaró que "no era capaz de olvidar las caras de las víctimas inocentes que fueron asesinadas. Sus fotos están en todas partes, en las cabinas telefónicas, semáforos, paredes de estaciones de metro. Todo me recuerda a un enorme funeral, con gente callada, llorando y triste, pero también muy amable. Antes, Nueva York me hacía sentir frío; ahora la gente se acerca para ayudarse unos a otros".

El 20 de septiembre de 2001 se publicó una canción llamada El último adiós, escrita por Emilio Estefan Jr. y Gian Marco a manera de homenaje a las víctimas, en la que se reunieron más de 60 artistas en señal de alianza; entre los que participaron en este homenaje destacan: Ricky Martin, Alejandro Sanz, Thalía, Gloria Estefan, Juan Luis Guerra, Celia Cruz, Olga Tañón, Jennifer Lopez, Paulina Rubio, Alicia Villarreal, José José, Shakira, Lucía Méndez, John Secada, OV7, Kumbia Kings, Álvaro Torres, Ana Bárbara, Los Temerarios, Los Tigres del Norte, Lupillo Rivera, Andy Garcia, Yuri, Soraya, Marco Antonio Solís, Carlos Vives, Ana Gabriel, Carlos Ponce, Patricia Manterola, Pilar Montenegro, Christina Aguilera, Chayanne, Beto Cuevas, A.B. Quintanilla hermano de la fallecida cantante Selena, etcétera. La totalidad de las ganancias recaudadas por la disquera que distribuyó la canción y el disco Sony Music fueron donadas junto con la manufactura de los primeros cien mil discos de esta canción a American Red Cross y a United Way para ayudar a las familias de las víctimas de los ataques.

Uno de los primeros memoriales fue el "Tribute in Light", la instalación de ochenta y ocho luces de búsqueda en el sitio donde se encontraban las Torres Gemelas, que proyectaba dos columnas verticales de luz hacia el cielo. En Nueva York, se llevó a cabo un concurso para diseñar el memorial más apropiado para el lugar. El diseño ganador, "Reflecting Absence", que ahora se llama National September 11 Memorial & Museum, fue elegido en agosto de 2006 e inaugurado el 11 de septiembre de 2011, en el décimo aniversario de los ataques y consiste en un par de piscinas reflectoras donde estaban las Torres Gemelas, rodeadas de paneles de bronce con los nombres de las víctimas que murieron en los atentados del 11 de septiembre de 2001 y del 26 de febrero de 1993, así como un museo ubicado en el subterráneo del memorial, que exhibe objetos encontrados entre los escombros de las Torres Gemelas tras los ataques, así como objetos personales usados en vida por las víctimas. El nuevo edificio que reemplaza a las desaparecidas Torres Gemelas llamado One World Trade Center fue inaugurado oficialmente el 3 de noviembre de 2014.
En el séptimo aniversario de los ataques, el 11 de septiembre de 2008, se completó la construcción y se abrió al público el Pentagon Memorial. Consiste en un parque con 184 bancos que representan el número de víctimas que tuvo el atentado en el lugar donde el avión impactó en el Pentágono. Cuando el edificio fue reparado, a finales de 2001 e inicios de 2002, se incluyeron una capilla privada y un memorial interno, localizados en el punto donde se estrelló el vuelo 77 de American Airlines.

En Shanksville, Pensilvania se construyó el Memorial Nacional al Vuelo 93 que incluye un círculo de árboles que rodean la zona donde se estrelló el avión del vuelo 93 de United Airlines, con cuarenta carillones que llevan escritos los nombres de las víctimas. También se construyó un memorial de forma temporal a 457 metros del choque.
Los bomberos de la ciudad de Nueva York donaron un memorial al Departamento de Bomberos de Shanksville. Se trata de una cruz hecha de acero del World Trade Center, sobre una plataforma con la forma del Pentágono. Fue instalado frente a la central de bomberos el 25 de agosto de 2008.

En muchos otros lugares se están construyendo memoriales permanentes en honor a las víctimas y apoyos económicos a sus familiares, numerosas organizaciones y figuras públicas han creado varios programas de becas y fundaciones para recaudar fondos.

En cada aniversario del 11 de septiembre, en la ciudad de Nueva York, en el National September 11 Memorial & Museum, lugar donde se encontraban las Torres Gemelas del World Trade Center, se leen los nombres de las víctimas que murieron en los ataques con música fúnebre de fondo. El presidente de los Estados Unidos, por su parte, asiste a un servicio conmemorativo en el Pentágono. En Shanksville, Pensilvania, se llevan a cabo servicios más pequeños, a los que suele asistir la Primera Dama.

La fundación Wikimedia abrió también un wiki dedicado a los atentados, que fue cerrado el 15 de septiembre de 2006.
También se han realizado muchas películas y documentales sobre los ataques del 11 de septiembre de 2001, así como conciertos homenajes en memoria de las víctimas que murieron.

Desde que se produjeron los atentados han surgido varias teorías a las que se suele agrupar bajo la denominación de "teorías conspirativas", que sostienen que las conclusiones alcanzadas en la investigación oficial no resultan consistentes con los hechos.

En general, en estas teorías se habla de la posibilidad de que en realidad fue un misil lo que impacto en el Pentágono; que las Torres Gemelas del World Trade Center y la Torre N.º 7 del World Trade Center se derrumbaron por cargas explosivas, en una demolición controlada hecha a control remoto; que el vuelo 93 de United Airlines fue derribado por un caza estadounidense y no por el enfrentamiento entre los pasajeros y los terroristas, etc. Por lo general, estos autores afirman haber encontrado incongruencias que ponen en duda toda la versión gubernamental. Algunas de las supuestas inconsistencias que los críticos mencionan serían el hecho de que, en teoría, era imposible que un avión pudiera acercarse al Pentágono sin accionar las defensas antiaéreas o que el FBI hubiese localizado el pasaporte intacto de uno de los terroristas dentro de los restos humeantes del World Trade Center. Otras incongruencias están basadas en las irregularidades económicas acaecidas, antes, durante y después de los atentados.

En cuanto a los autores, algunas de estas teorías sostienen que algunos miembros del gobierno de los Estados Unidos ya conocían los planes de Al-Qaeda de atacar al World Trade Center pero no hicieron nada para evitarlo. Otras llegan incluso a acusar directamente al propio gobierno de Estados Unidos de planear y ejecutar los atentados.

Entre los principales opositores a la versión dada por el gobierno estadounidense se encuentra el periodista francés y director de la web de izquierda Red Voltaire Thierry Meyssan, quien escribió un libro titulado "La gran impostura". En su trabajo, Meyssan exhibe una serie de razones y argumentos por los que, según él, no es posible dar por cierta la versión gubernamental.

Otro de los más acérrimos críticos es el profesor estadounidense David Ray Griffin, autor del libro "Desenmascarando el 11-S","" donde hace un análisis punto por punto de los hechos ocurridos el 11 de septiembre de 2001. Griffin afirma haber encontrado al menos 115 fallos lógicos graves en la versión oficial de los atentados.

Dos películas basadas en estos atentados fueron estrenadas en el año 2006:

Aunque fue anterior a los hechos, el episodio piloto de la serie Los pistoleros solitarios describia un complot de los servicios secretos del Gobierno de los Estados Unidos para estrellar un Boeing 727 contra una de las Torres Gemelas. El fin que perseguía el Gobierno era que pretendía culpar a un país extranjero del hecho, con el propósito de obtener un "casus belli" ("motivo para declaración de guerra") y sacar beneficio de la venta de armas. Este episodio fue estrenado el 4 de marzo de 2001, poco más de seis meses antes de los atentados en Nueva York.


De la National Commission on Terrorist Attacks Upon the United States (Comisión Nacional sobre Ataques Terroristas contra los Estados Unidos):








</doc>
<doc id="5347" url="https://es.wikipedia.org/wiki?curid=5347" title="Wikcionario">
Wikcionario

El Wikcionario (contracción de "wiki" y "diccionario"; en inglés, Wiktionary) es un proyecto de diccionario libre de la Fundación Wikimedia, que contiene definiciones, traducciones, etimologías, sinónimos y pronunciaciones de palabras en múltiples idiomas.

Al igual que Wikipedia, Wikcionario está basado en la tecnología wiki, utiliza el "software" MediaWiki y su contenido está protegido por las licencias libres GFDL y CC BY-SA.

El primer Wikcionario fue la versión en inglés, creada por Brion Vibber el 12 de diciembre de 2002, a la cual le siguieron poco después las versiones en francés y en polaco.

El 1 de mayo de 2004, Tim Starling comenzó "Wikcionarios" para todas las lenguas en que existía Wikipedia, 143 en total, incluido el español. El número de versiones ascendía a 171 idiomas en noviembre de 2019.

Wiktionary fue puesto en línea el 12 de diciembre de 2002. El 28 de marzo de 2004, siendo los primeros idiomas no-inglés, se iniciaron a crear Wikcionarios en francés y polaco. Desde entonces se han empezado a usar Wiktionaries en muchos otros idiomas. Wikcionario fue alojado en un nombre de dominio temporal hasta el 1 de mayo de 2004, cuando cambió al nombre de dominio actual. 

Otro de estos bots, "ThirdPersBot", fue responsable de la adición de un número de personas que no habrían recibido sus propias entradas en los diccionarios estándares. Por ejemplo, definió "smoulders" como la "tercera persona singular simple forma presente de smolder". 

De las 648.970 definiciones el Wiktionary inglés proporciona para 501.171 palabras inglesas, 217.850 son forma de definiciones de esta clase. Es ligeramente menor que la de los principales diccionarios monolingües de impresión. 

El Wiktionary Inglés no se basa en los bots en la medida en que algunas otras ediciones. La Wikipedia, por ejemplo, importó grandes secciones del Proyecto de Diccionario Vietnamita Libre (FVDP, por sus siglas en inglés), que proporciona diccionarios bilingües de contenido gratuito desde y hacia el vietnamita.



</doc>
<doc id="5349" url="https://es.wikipedia.org/wiki?curid=5349" title="Medio ambiente natural">
Medio ambiente natural

El medio ambiente, medioambiente o entorno natural abarca todos los seres vivos y no vivos que interaccionan naturalmente, lo que significa que en este caso no es artificial. El término se aplica con mayor frecuencia a la Tierra o algunas partes de la Tierra. Este entorno abarca la interacción de todas las especies vivas, el clima, y los recursos naturales que afectan la supervivencia humana y la actividad económica. Se pueden distinguir como componentes del medio ambiente:


En contraste con el entorno natural es el ambiente construido. En áreas donde el hombre ha transformado fundamentalmente paisajes como los entornos urbanos y la conversión de tierras agrícolas, el entorno natural se modifica enormemente en un entorno humano simplificado. Incluso los actos que parecen menos extremos, como la construcción de una choza de barro o un sistema fotovoltaico en el desierto, el entorno modificado se convierte en uno artificial. Aunque muchos animales construyen cosas para proporcionar un mejor ambiente para ellos mismos, no son humanos, por lo tanto, las presas de castores, y las obras de las termitas, termiteros o montículos, se consideran naturales. 

Las personas rara vez encuentran ambientes "absolutamente naturales" en la Tierra, y la naturalidad generalmente varía en un continuo, desde el 100 % natural en un extremo hasta el 0% natural en el otro. Más precisamente, podemos considerar los diferentes aspectos o componentes de un entorno, y ver que su grado de naturalidad no es uniforme. Si, por ejemplo, en un campo agrícola, la composición mineralógica y la estructura de su suelo son similares a las de un suelo de bosque no perturbado, pero la estructura es bastante diferente. 

El término medio ambiente se usa a menudo como sinónimo de hábitat, por ejemplo, cuando se dice que el ambiente natural de las jirafas es la sabana. 

Según el Programa de las Naciones Unidas para el Medio Ambiente (PNUMA) se usa más comúnmente en referencia al ambiente "natural", o la suma de todos los componentes vivos y los abióticos que rodean a un organismo, o grupo de organismos. El "medio ambiente natural" comprende componentes físicos tales como aire, temperatura, relieve, suelos y cuerpos de agua así como componentes vivos: plantas, animales y microorganismos. También existe el "medio ambiente construido", que comprende todos los elementos y los procesos hechos por el hombre. En términos macroscópicos se suele considerar al medioambiente como un sector, una región o un todo (escala global). En cada uno de esos niveles o alcances de estudio hay una interacción entre el aire, del agua o del suelo como agentes abióticos y de toda una gran variedad de organismos animales y vegetales, con distinto nivel de organización celular, como integrantes del mundo biótico.

Las ciencias de la Tierra generalmente reconocen las siguientes cuatro esferas como componentes de los sistemas que conforman la totalidad del medio ambiente natural: 


Estas esferas se corresponden al conjunto de las rocas, las aguas, la atmósfera y la vida, respectivamente. 

Algunos científicos incluyen, como parte de las esferas de la Tierra, la criosfera (correspondiente al hielo) como una porción distinta de la hidrosfera, así como la pedosfera (correspondiente al suelo) como una esfera activa y entremezclada de las cuatro anteriores. 

Las ciencias de la Tierra (también conocidas como geociencias o ciencias geológicas) engloban todas las ciencias relacionadas con el estudio directo del planeta Tierra como tal. Hay diferentes disciplinas en ciencias de la tierra, como geografía física, geología, paleontología, geofísica, climatología, oceanografía o geodesia, ente otras. Estas disciplinas utilizan la física, la química, la biología, la geocronología y las matemáticas para desarrollar una comprensión cualitativa y cuantitativa de las principales áreas o "esferas" de la Tierra. 

La corteza terrestre, o litosfera, es la superficie sólida más externa del planeta y es química y mecánicamente diferente del manto subyacente. Es la capa de roca de la Tierra con la que interaccionan la vida y los seres humanos. Se ha generado en gran medida por procesos ígneos en los que el magma se enfría y se solidifica para formar roca sólida. Debajo de la litosfera se encuentra el manto que se calienta por la descomposición de loselementos radiactivos. El manto, aunque sólido, se encuentra en un estado de convección reológica. Este proceso de convección hace que las placas litosféricas se muevan, aunque lentamente. El proceso resultante se conoce como tectónica de placas. Los volcanes resultan principalmente de la fusión del material de la corteza subducida o del manto ascendente en las cordilleras medioocéanicas y las plumas del manto. 

La mayor parte del agua se encuentra en uno u otro tipo de cuerpo de agua natural. 

Un océano es un cuerpo importante de agua salina y un componente de la hidrosfera. Aproximadamente el 71 % de la superficie de la Tierra (un área de unos 362 millones de kilómetros cuadrados) está cubierta por el océano, una masa de agua continua que normalmente se divide en varios océanos principales y mares más pequeños. Más de la mitad de esta área tiene más de 3000 metros (9800 pies) de profundidad. La salinidad oceánica promedio es de alrededor de 35 partes por mil (ppt) (3.5 %), y casi toda el agua de mar tiene una salinidad en el rango de 30 a 38ppt. Aunque generalmente reconocidas como varios océanos ""separados"", estas aguas comprenden un cuerpo global e interconectado de agua salada a menudo conocido como el océano mundial o el océano global. Los fondos marinos profundos son más de la mitad de la superficie de la Tierra, y se encuentran entre los entornos naturales menos modificados. Las principales divisiones oceánicas están definidas en parte por los continentes, varios archipiélagos y otros criterios: estas divisiones son (en orden descendente de tamaño) el océano Pacífico, el océano Atlántico, el océano Índico, el océano Antártico y el océano Ártico.

Un río es un curso de agua natural, generalmente de agua dulce, que fluye hacia un océano, un lago, un mar u otro río. Unos pocos ríos simplemente fluyen hacia el suelo y se secan completamente antes de llegar a otro cuerpo de agua. 

El agua en un río está generalmente en un canal, formado por un lecho de un arroyo entre las orillas. En los ríos más grandes también hay una llanura de inundación más amplia formada por aguas que cubren el canal. Las llanuras de inundación pueden ser muy anchas en relación con el tamaño del canal del río. Los ríos son parte del ciclo hidrológico. El agua dentro de un río generalmente se recolecta de la precipitación a través de la escorrentía superficial, la recarga de aguas subterráneas, los manantiales y la liberación de agua almacenada en glaciares y paquetes de nieve.

Los ríos pequeños también se pueden denominar con otros nombres, como arroyo. Su corriente está confinada dentro de un cauce y un banco. Las corrientes desempeñan un importante papel de corredor en la conexión de hábitats fragmentados y por lo tanto en la conservación de la biodiversidad. El estudio de arroyos y cursos de agua en general se conoce como "hidrología superficial."

Un lago (del latín "lacus") es una característica del terreno, un cuerpo de agua que se localiza en el fondo de la cuenca. Un cuerpo de agua se considera un lago cuando está en el interior, no es parte de un océano, y es más grande y más profundo que un estanque. 

Los lagos naturales de la Tierra se encuentran generalmente en áreas montañosas, zonas de ruptura y áreas con glaciaciones recientes o en curso. Otros lagos se encuentran en las cuencas endorreicas o a lo largo de los cursos de los ríos maduros. En algunas partes del mundo, hay muchos lagos debido a los caóticos patrones de drenaje que quedaron de la última Edad de Hielo. Todos los lagos son temporales en escalas de tiempo geológicas, ya que se llenarán lentamente con sedimentos o se derramarán fuera de la cuenca que los contiene.

Un estanque es un cuerpo de agua estancada, ya sea natural o hecha por el hombre, que generalmente es más pequeña que un lago. Una gran variedad de cuerpos de agua hechos por el hombre se clasifican como estanques, incluidos los jardines acuáticos diseñados para la ornamentación estética, los estanques de peces diseñados para la cría de peces comerciales y los estanques solares diseñados para almacenar energía térmica. Los estanques y lagos se distinguen de los arroyos por su velocidad actual. Mientras que las corrientes en los arroyos son fácilmente observables, los estanques y lagos poseen micro-corrientes térmicas y corrientes moderadas por el viento. Estas características distinguen un estanque de muchas otras características del terreno acuático, como las piscinas de arroyos y las pozas de mareas.

Los humanos impactan el agua de diferentes maneras, como la modificación de los ríos (a través de represas y canalizaciones de arroyos), la urbanización y la deforestación. Estos afectan los niveles del lago, las condiciones del agua subterránea, la contaminación del agua, la contaminación térmica y la contaminación marina. Los humanos modifican los ríos utilizando la manipulación directa de canales. Están construyendo represas y embalses y manipulando la dirección de los ríos y el camino del agua. Las represas son buenas para los humanos, algunas comunidades necesitan los reservorios para sobrevivir. Sin embargo, los embalses y represas pueden afectar negativamente el medio ambiente y la vida silvestre. Las presas detienen la migración de los peces y el movimiento de los organismos río abajo. La urbanización afecta al medio ambiente debido a la deforestación y al cambio en los niveles de los lagos, las condiciones de las aguas subterráneas, etc. La deforestación y la urbanización van de la mano. La deforestación puede causar inundaciones, disminución del flujo de la corriente y cambios en la vegetación de la ribera. El cambio en la vegetación ocurre porque cuando los árboles no pueden obtener el agua adecuada, comienzan a deteriorarse, lo que lleva a una disminución del suministro de alimentos para la vida silvestre en un área.

La atmósfera de la Tierra sirve como un factor clave para sostener el ecosistema planetario. La delgada capa de gases que envuelve a la Tierra se mantiene en su lugar por la gravedad del planeta. El aire seco consta de 78 % de nitrógeno, 21 % de oxígeno, 1 % de argón y otros gases inertes, como el dióxido de carbono. Los gases restantes a menudo se denominan gases traza, entre los cuales se encuentran los gases de efecto invernadero, como el vapor de agua, el dióxido de carbono, el metano, el óxido nitroso y el ozono. El aire filtrado incluye trazas de muchos otros compuestos químicos. El aire también contiene una cantidad variable de vapor de agua y suspensiones de gotitas de agua y cristales de hielo vistos como nubes. Muchas sustancias naturales pueden estar presentes en pequeñas cantidades en una muestra de aire sin filtrar, incluyendo polvo, polen y esporas, rocío marino, cenizas volcánicas y meteoroides. Varios contaminantes industriales también pueden estar presentes, tales como cloro (primario o en compuestos), compuestos de flúor, mercurio y azufre tales como dióxido de azufre [SO] 

La capa de ozono de la atmósfera de la Tierra juega un papel importante en el agotamiento de la cantidad de radiación ultravioleta (UV) que llega a la superficie. Como el ADN se daña fácilmente con la luz UV, esto sirve para proteger la vida en la superficie. La atmósfera también retiene el calor durante la noche, lo que reduce las temperaturas diarias extremas. 

La atmósfera de la Tierra se puede dividir en cinco capas principales. Estas capas están determinadas principalmente por si la temperatura aumenta o disminuye con la altitud. De mayor a menor, estas capas son: 



Dentro de las cinco capas principales determinadas por la temperatura hay varias capas determinadas por otras propiedades. 


Los peligros del calentamiento global están siendo estudiados cada vez más por un amplio consorcio mundial de científicos. Estos científicos están cada vez más preocupados por los posibles efectos a largo plazo del calentamiento global en nuestro entorno natural y en el planeta. De particular preocupación son el cambio climático y el calentamiento global causados por las emisiones antropogénicas o hechas por el hombre, de gases de efecto invernadero, principalmente el dióxido de carbono, pueden actuar interactivamente y tener efectos adversos sobre el planeta, su entorno natural y la existencia de los seres humanos. Está claro que el planeta se está calentando, y se está calentando rápidamente. Esto se debe al efecto invernadero, causado por los gases de efecto invernadero, que atrapan el calor dentro de la atmósfera de la Tierra debido a su estructura molecular más compleja que les permite vibrar y, a su vez, atrapar el calor y liberarlo hacia la Tierra. Este calentamiento también es responsable de la extinción de los hábitats naturales, lo que a su vez conduce a una reducción de la población de vida silvestre. El informe más reciente del Panel Intergubernamental sobre el Cambio Climático (el grupo de los principales científicos del clima en el mundo) concluyó que la tierra se calentará entre 2,7 y casi 11 grados Fahrenheit (1,5 a 6 grados Celsius) entre 1990 y 2100. Los esfuerzos se han centrado cada vez más en la mitigación de los gases de efecto invernadero que están causando cambios climáticos, en el desarrollo de estrategias adaptativas al calentamiento global, para ayudar a los seres humanos, otras especies animales, vegetales, ecosistemas, regiones y naciones a adaptarse a los efectos del calentamiento global. Algunos ejemplos de colaboración reciente para abordar el cambio climático y el calentamiento global incluyen: 

Un desafío significativamente profundo es identificar la dinámica ambiental natural en contraste con los cambios ambientales que no están dentro de las variaciones naturales. Una solución común es adaptar una vista estática que deje de lado las variaciones naturales. Metodológicamente, esta visión podría defenderse cuando se observan procesos que cambian lentamente y series temporales cortas, mientras que el problema surge cuando los procesos rápidos se vuelven esenciales en el objeto del estudio. 

El clima analiza las estadísticas de temperatura, humedad, presión atmosférica, viento, lluvia, conteo de partículas atmosféricas y otros elementos meteorológicos en una región determinada durante largos períodos de tiempo. El tiempo atmosférico, por otro lado, es la condición actual de estos mismos elementos durante períodos de hasta dos semanas. 

Los climas se pueden clasificar de acuerdo con el promedio y los rangos típicos de diferentes variables, generalmente temperatura y precipitación. El esquema de clasificación más utilizado es el desarrollado originalmente por Wladimir Köppen. El sistema Thornthwaite, en uso desde 1948, utiliza la evapotranspiración, así como información sobre la temperatura y la precipitación para estudiar la diversidad de las especies animales y los posibles impactos de los cambios climáticos.

El tiempo atmosférico es un conjunto de todos los fenómenos que ocurren en un área atmosférica determinada en un momento dado. La mayoría de los fenómenos meteorológicos ocurren en la troposfera, justo debajo de la estratosfera. El clima se refiere, en general, a la temperatura diaria y la actividad de precipitación, mientras que el clima es el término para las condiciones atmosféricas promedio durante períodos de tiempo más largos. Cuando se usa sin calificación, se entiende por "clima" el clima de la Tierra. 

El clima ocurre debido a las diferencias de densidad (temperatura y humedad) entre un lugar y otro. Estas diferencias pueden ocurrir debido al ángulo del sol en cualquier lugar en particular, que varía según la latitud desde los trópicos. El fuerte contraste de temperatura entre el aire polar y el tropical da lugar a la corriente en chorro. Los sistemas meteorológicos en las latitudes medias, como los ciclones extratropicales, son causados por la inestabilidad del flujo de chorro. Debido a que el eje de la Tierra está inclinado con respecto a su plano orbital, la luz solar incide en diferentes ángulos en diferentes épocas del año. En la superficie terrestre, las temperaturas suelen oscilar entre ± 40 °C (100 °F a –40 °F) anualmente. Durante miles de años, los cambios en la órbita de la Tierra han afectado la cantidad y distribución de energía solar recibida por la Tierra e influyen en el clima a largo plazo 

Las diferencias de temperatura en la superficie a su vez causan diferencias de presión. Las altitudes más altas son más frías que las bajas debido a las diferencias en el calentamiento por compresión. El pronóstico del tiempo es la aplicación de la ciencia y la tecnología para predecir el estado de la atmósfera para un tiempo futuro y una ubicación determinada. La atmósfera es un sistema caótico, y pequeños cambios en una parte del sistema pueden crecer para tener grandes efectos en el sistema en su conjunto. Los intentos humanos para controlar el clima se han producido a lo largo de la historia humana, y hay evidencia de que la actividad humana civilizada, como la agricultura y la industria, ha modificado inadvertidamente los patrones climáticos.

La evidencia sugiere que la vida en la Tierra ha existido durante unos 3700 millones de años. Todas las formas de vida conocidas comparten mecanismos moleculares fundamentales, y con base en estas observaciones, las teorías sobre el origen de la vida intentan encontrar un mecanismo que explique la formación de un organismo primordial de una sola célula, del cual se origina toda la vida. Hay muchas hipótesis diferentes con respecto a la ruta que se podría haber tomado desde moléculas orgánicas simples a través de la vida precelular hasta las protocélulas y el metabolismo. 

Aunque no existe un acuerdo universal sobre la definición de vida, los científicos generalmente aceptan que la manifestación biológica de la vida se caracteriza por la organización, el metabolismo, el crecimiento, la adaptación, la respuesta a los estímulos y la reproducción. También se puede decir que la vida es simplemente el estado característico de los organismos. En biología, la ciencia de los organismos vivos, la "vida" es la condición que distingue a los organismos activos de la materia inorgánica, incluida la capacidad de crecimiento, la actividad funcional y el cambio continuo que precede a la muerte. 

Se puede encontrar una variedad diversa de organismos vivos (formas de vida) en la biosfera en la Tierra, y las propiedades comunes a estos organismos (plantas, animales, hongos, protistas, arqueas y bacterias) son una forma celular basada en carbono y agua, con organización compleja e información genética hereditaria. Los organismos vivos tienen un metabolismo, mantienen la homeostasis, poseen la capacidad de crecer, responden a estímulos, se reproducen y, a través de la selección natural, se adaptan a su entorno en generaciones sucesivas. Los organismos vivos más complejos pueden comunicarse a través de diversos medios.

Un ecosistema (que a veces se usa con el mismo significado de medio ambiente) es una unidad natural que consta de todas las plantas, animales y microorganismos (factores bióticos) en un área que funciona junto con todos los factores físicos (abióticos) no vivos del medio ambiente. 

En el concepto de ecosistema es fundamental la idea de que los organismos vivos están continuamente comprometidos en un conjunto de relaciones altamente interrelacionadas con todos los demás elementos que constituyen el ambiente biofísico en el que existen. Eugene Odum, uno de los fundadores de la ciencia de la ecología, declaró: 

""Cualquier unidad que incluya a todos los organismos (es decir, la "comunidad") en un área determinada que interactúa con el entorno físico para que un flujo de energía conduzca claramente a la estructura trófica definida, la diversidad biótica y los ciclos de materiales (es decir, el intercambio de materiales entre partes vivas y no vivas) dentro del sistema es un ecosistema"". 

El concepto de ecosistema humano se basa entonces en la deconstrucción de la dicotomía humano/naturaleza, y la premisa emergente de que todas las especies están integradas ecológicamente entre sí, así como con los constituyentes abióticos de su biotopo. 

Un mayor número o variedad de especies o la biodiversidad de un ecosistema puede contribuir a una mayor capacidad de recuperación del mismo, porque hay más especies presentes en un lugar para responder al cambio y, por lo tanto, "absorber" o reducir sus efectos. Esto reduce el efecto antes de que la estructura del ecosistema se cambie fundamentalmente a un estado diferente. Este no es el caso universalmente y no existe una relación comprobada entre la diversidad de especies de un ecosistema y su capacidad para proporcionar bienes y servicios a un nivel sostenible. 

El término ecosistema también puede referirse a entornos creados por el hombre, como los ecosistemas humanos y los ecosistemas influenciados por el hombre, y puede describir cualquier situación en la que exista una relación entre los organismos vivos y su entorno. Hoy en día, existen menos áreas en la superficie de la tierra libres de contacto humano, aunque algunas áreas naturales o salvajes continúan existiendo sin ninguna forma de intervención humana.

Los biomas son terminológicamente similares al concepto de los ecosistemas, y son desde el punto de vista climático áreas definidas geográficamente de condiciones climáticas ecológicamente similares en la Tierra, tales como comunidades de plantas, animales y organismos del suelo, a menudo referidas "como" ecosistemas. Los biomas se definen sobre la base de factores como las estructuras de las plantas (como árboles, arbustos y pastos), los tipos de hojas (como la hoja ancha y la hoja de la hoja), el espaciamiento de las plantas (bosques, bosques, sabanas) y el clima. A diferencia de las ecozonas, los biomas no están definidos por similitudes genéticas, taxonómicas o históricas. Los biomas a menudo se identifican con patrones particulares de sucesión ecológica y vegetación culminante.

Los ciclos biogeoquímicos globales son críticos para la vida, especialmente los de agua, oxígeno, carbono, nitrógeno y fósforo. 


La naturaleza salvaje generalmente se define como un entorno natural en la Tierra que no ha sido modificado significativamente por la actividad humana. La Fundación "WILD" entra en más detalles, definiendo la vida silvestre como:«"Las áreas naturales silvestres más intactas y sin perturbaciones que quedan en nuestro planeta: esos últimos lugares verdaderamente silvestres que los humanos no controlan y no se han desarrollado con carreteras, tuberías u otra infraestructura industrial»". Las áreas silvestres y los parques protegidos se consideran importantes para la supervivencia de ciertas especies, estudios ecológicos, conservación, soledad y recreación. El desierto es profundamente valorado por razones culturales, espirituales, morales y estéticas. Algunos escritores de la naturaleza creen que las áreas silvestres son vitales para el espíritu humano y la creatividad. 

La palabra "salvaje", deriva de la noción de salvajismo; en otras palabras, lo que no es controlable por los humanos. La etimología de la palabra "wild" proviene de los "wildeornes del" inglés "antiguo", que a su vez se deriva de "wildeor que" significa "bestia" salvaje ("wild + deor" = bestia, venado). Desde este punto de vista, es la naturaleza salvaje de un lugar lo que lo convierte en un desierto. La mera presencia o actividad de las personas no descalifica a un área de ser "desierto". Muchos ecosistemas que están, o han sido, habitados o influenciados por actividades de personas aún pueden considerarse "salvajes". Esta forma de ver el desierto incluye áreas dentro de las cuales los procesos naturales operan sin una interferencia humana muy notable. 

La vida silvestre incluye todas las plantas, animales y otros organismos no domesticados. La domesticación de especies de plantas y animales silvestres para beneficio humano ha ocurrido muchas veces en todo el planeta y tiene un gran impacto en el medio ambiente, tanto positivo como negativo. La vida silvestre se puede encontrar en todos los ecosistemas. Los desiertos, las selvas tropicales, las llanuras y otras áreas, incluidos los sitios urbanos más desarrollados, tienen formas distintas de vida silvestre. Mientras que el término en la cultura popular usualmente se refiere a animales que no están afectados por factores humanos civilizados, la mayoría de los científicos están de acuerdo en que la vida silvestre en todo el mundo está (ahora) afectada por las actividades humanas. 

Es la comprensión común del "entorno natural lo" que subyace en el ecologismo: un amplio movimiento político, social y filosófico que aboga por diversas acciones y políticas con el fin de proteger lo que la naturaleza permanece en el entorno natural, o restaurar o ampliar el papel de la naturaleza en este ambiente. Si bien la verdadera naturaleza salvaje es cada vez más rara, "la" naturaleza "salvaje" (p. ej., Bosques no administrados, pastizales sin cultivar, vida silvestre, flores silvestres) se puede encontrar en muchos lugares previamente habitados por humanos. 

Los objetivos en beneficio de las personas y los sistemas naturales, expresados comúnmente por los científicos ambientales y ambientalistas incluyen: 


En algunas culturas, el término entorno carece de significado porque no hay separación entre las personas y lo que ellos ven como el mundo natural o su entorno. Específicamente en los Estados Unidos, muchas culturas nativas no reconocen el "ambiente", o se ven a sí mismas como ambientalistas.

El 5 de junio, se celebra globalmente el Día Mundial del Medio Ambiente. Este fue establecido por la Asamblea General de las Naciones Unidas en 1972. Es uno de los medios importantes por los cuales la Organización de las Naciones Unidas estimula la sensibilización mundial acerca del entorno.



</doc>
<doc id="5350" url="https://es.wikipedia.org/wiki?curid=5350" title="Contaminación acústica">
Contaminación acústica

Se llama contaminación acústica, contaminación sónica o contaminación sonora al exceso de sonido que altera las condiciones normales del ambiente en una determinada zona. Si bien el ruido no se acumula, traslada o perdura en el tiempo como las otras contaminaciones, también puede causar grandes daños en la calidad de vida de las personas si no se controla bien o adecuadamente.

El término «contaminación acústica» hace referencia al ruido (entendido como sonido excesivo y molesto), provocado por las actividades humanas (tráfico, industrias, locales de ocio, aviones, barcos, entre otros) que produce efectos negativos sobre la salud auditiva, física y mental de los seres vivos.

Este término está estrechamente relacionado con el ruido debido a que esta se da cuando el ruido es considerado como un contaminante, es decir, un sonido molesto que puede producir efectos nocivos fisiológicos y psicológicos para una persona o grupo de personas.

Las principales causas de la contaminación acústica son aquellas relacionadas con las actividades humanas como el transporte, la construcción de edificios, obras públicas y las industrias, entre otras.

Se ha dicho por organismos internacionales, que se corre el riesgo de una disminución importante en la capacidad auditiva, así como la posibilidad de trastornos que van desde lo psicológico (paranoia, perversión) hasta lo fisiológico por la excesiva exposición a la contaminación sónica.

Un informe de la Organización Mundial de la Salud (OMS), considera los 70 dB (A), como el límite superior deseable. En España, se establece como nivel de confort acústico los 55 dBA. Por encima de este nivel, el sonido resulta pernicioso para el descanso y la comunicación. Según estudios de la Unión Europea (2005): «80 millones de personas están expuestas diariamente a niveles de ruido ambiental superiores a 65 dBA y otros 170 millones, lo están a niveles entre 55-65 dBA».

Para medir el impacto del ruido ambiental (contaminación acústica) se utilizan varios indicadores que están en continuo desarrollo, a partir de Lp:

El nivel de presión sonora se define como 20 veces la relación logarítmica de la presión sonora eficaz respecto a una presión de referencia p, de valor p = 2,0×10 N/m² = 20 μPa, obtenida mediante una ponderación normalizada de frecuencias y una ponderación exponencial normalizada de tiempos.

Si no se mencionan explícitamente, debe sobreentenderse que se trata de la ponderación temporal FAST y de la ponderación de frecuencias A, adoptando la siguiente nomenclatura LpA.

Se define como el nivel de presión que contiene la energía promedio de un ruido fluctuante para el mismo periodo de tiempo.

El SEL es el nivel LEQ de un ruido de 1 segundo de duración. El SEL se utiliza para medir el número de ocasiones en que se superan los niveles de ruido tolerado en sitios específicos: barrios residenciales, hospitales, escuelas, etc.

Es el más alto nivel de presión sonora continuo equivalente ponderado A, en decibelios, determinado sobre un intervalo temporal de 1 segundo (LAeq,1) registrado en el periodo temporal de evaluación.

El dB es la unidad que se utiliza para medir la intensidad del sonido y otras magnitudes físicas. Es la décima parte de un belio B, unidad que recibe su nombre por Graham Bell.

Es el nivel de presión sonora continuo equivalente ponderado A, corregido por el tipo de fuente de ruido (tráfico o industrial), por el carácter del ruido (impulsivo, tonal) y por el período considerado (nocturno, vespertino, fin de semana). LKeq, T = LAeq, T +Ki

El LDN mide el nivel de ruido Leq que se produce en 24 horas. Al calcular el ruido nocturno, como no debe haber, se penaliza con 10 dBA a los ruidos que se producen entre las 10 de la noche y las 7 de la mañana. La Organización Mundial de la Salud (OMS) establece que los niveles de ruido no deben exceder los 50 decibeles (dB) durante el día y los 45 dB por la noche.

El sistema auditivo se resiente ante una exposición prolongada a la fuente de un sonido, aunque esta sea de bajo nivel.

El efecto auditivo provocado por el ruido ambiental se llama "socioacusia". Cuando una persona se expone de forma prolongada a un nivel de sonido excesivo, nota un silbido constante en el oído, esta es una señal de alerta. Inicialmente, los daños producidos por una exposición prolongada no son permanentes, sobre los 10 días desaparecen. Sin embargo, si la exposición a la fuente no cesa, las lesiones serán definitivas. La audición se irá perdiendo, hasta convertirse en sordera.

No solo el ruido prolongado es perjudicial, un sonido repentino de 160 dBA, como el de una explosión o un disparo, pueden llegar a perforar el tímpano o causar otras lesiones irreversibles. Citando puntualmente las afecciones auditivas que produce el ruido tenemos: Desplazamiento Temporal y Permanente del umbral de audición.

Consiste en una elevación del umbral producida por la presencia de un ruido, existiendo recuperación total al cabo de un período, siempre y cuando no se repita la exposición al mismo. Se produce habitualmente durante la primera hora de exposición al ruido. Está puede causar dilatación de pupilas, fatiga, dolor de cabeza, etc.

Es el mismo efecto TTS pero agravado por el paso del tiempo y la exposición al ruido. Cuando alguien se somete a numerosos TTS y durante largos períodos (varios años), la recuperación del umbral va siendo cada vez más lenta y dificultosa, hasta volverse irreversible.

El desplazamiento permanente del umbral de audición esta directamente vinculado con la presbiacucia (pérdida de la sensibilidad auditiva debida a los efectos de la edad).

La sordera producida por el desplazamiento permanente del umbral de audición afecta a ambos oídos y con idéntica intensidad.

La inteligibilidad de la comunicación se reduce debido al ruido de fondo. El oído es un transductor y no discrimina entre fuentes de ruido, la separación e identificación de las fuentes sonoras se da en el cerebro. Como ya es sabido, la voz humana produce sonido en el rango de 100 a 10 000 Hz, pero la información verbal se encuentra en el rango de los 200 a 6000 Hz. La banda de frecuencia determinada para la inteligibilidad de la palabra, es decir entender palabra y frase, está entre 500 y 2500 Hz. La interferencia en la comunicación oral durante las actividades laborales puede provocar accidentes causados por la incapacidad de oír llamados de advertencia u otras indicaciones. En oficinas como en escuelas y hogares, la interferencia en la conversación constituye una importante fuente de molestias.

Con el paso de los años, la contaminación acústica se ha convertido en un problema para la salud. Es por ello, que la industria ha aumentado sus esfuerzos para disminuir la emisión de ruido en fuentes específicas. Una opción para facilitar esta determinación de ruido en dichas fuentes, es localizando el punto donde se genera mayor cantidad de energía sonora. La contaminación acústica, además de afectar al oído, puede provocar efectos psicológicos negativos y otros efectos fisiopatológicos.

Por supuesto, el ruido y sus efectos negativos no auditivos sobre el comportamiento y la salud mental y física dependen de las características personales, al parecer el estrés generado por el ruido se modula en función de cada individuo y de cada situación.



Todos los efectos psicológicos están íntimamente relacionados, por ejemplo:

Entre otros efectos no auditivos tenemos:

El ruido produce dificultades para conciliar el sueño y despierta a quienes están dormidos. El sueño es una actividad que ocupa un tercio de nuestras vidas y nos permite descansar, ordenar y proyectar nuestro consciente. El sueño está constituido por dos tipos: el sueño clásico profundo (no REM —etapa de sueño profundo—, el que a su vez se divide en cuatro fases distintas), y por otro lado está el sueño paradójico (REM). Se ha demostrado que sonidos del orden de aproximadamente 60 dBA, reducen la profundidad del sueño, acrecentándose dicha disminución a medida que crece la amplitud de la banda de frecuencias, las cuales pueden despertar al individuo, dependiendo de la fase del sueño en que se encuentre y de la naturaleza del ruido. Es importante tener en cuenta que estímulos débiles sorpresivos también pueden perturbar el sueño.

El ruido produce alteraciones en la conducta momentáneas, las cuales consisten en agresividad o mostrar un individuo con un mayor grado de desinterés o irritabilidad. Estas alteraciones, que generalmente son pasajeras, se producen a consecuencia de un ruido que provoca inquietud, inseguridad o miedo en algunos casos.

En aquellas tareas en donde se utiliza la memoria se ha demostrado que existe un mayor rendimiento en aquellos individuos que no están sometidos al ruido, debido a que este produce crecimiento en la activación del sujeto y esto en relación con el rendimiento en cierto tipo de tareas, produce una sobre activación traducida en el descenso del rendimiento. El ruido hace que la articulación en una tarea de repaso sea más lenta, especialmente cuando se tratan palabras desconocidas o de mayor longitud, es decir, en condiciones de ruido, el individuo se desgasta psicológicamente para mantener su nivel de rendimiento.

Por supuesto que todos los efectos son directamente proporcional al tiempo de exposición de la persona.

El ruido hace que la atención no se localice en una actividad específica, haciendo que esta se pierda en otros. Perdiendo así la concentración de la actividad.

Se ha observado que las madres embarazadas que han estado desde comienzos de su embarazo en zonas muy ruidosas, tienen niños que no sufren alteraciones, pero si la exposición ocurre después de los cinco o seis meses de gestación, después del parto los niños no soportan el ruido, lloran cuando lo sienten, y al nacer tienen un tamaño inferior al normal. Además son más propensos a desarrollar problemas auditivos.

El ruido repercute negativamente sobre el aprendizaje y la salud de los niños. Cuando los niños son educados en ambientes ruidosos, estos pierden su capacidad de atender señales acústicas, sufren perturbaciones en su capacidad de escuchar, así como un retraso en el aprendizaje de la lectura y la comunicación verbal. Todos estos factores favorecen el aislamiento del niño, haciéndolo poco sociable.

Hace varios años en las normativas de protección del ambiente no se consideraba el contaminante al ruido, pero pese a que la industrialización y en sí ciudades y países han ido creciendo y evolucionando, en todos los países del mundo se han elaborado normas y estatutos que se encargan de la protección del medio ambiente contra el exceso de ruido. Los esfuerzos más serios de las comunidades internacionales se traducen en la profundización de los estudios sobre causas y origen (fuentes), deterioro y políticas de prevención y control de la contaminación sonora.

En Bolivia, su reglamentación se ha basado en los estatutos de los organismos internacionales, incluyendo disposiciones de defensa y preservación de los recursos. En el 92 se dicta la ley 1333 general del Medio Ambiente, moderna normativa que incluye la EIA con inclusión de disposiciones de defensa y preservación de los recursos naturales.

En relación con el control del ruido ambiental, en Chile, se ha avanzado regulando las fuentes fijas como industrias, talleres, bares, etc., con el Decreto Supremo n.º 146 de 1997 del Ministerio Secretaría General de la Presidencia y las fuentes móviles más ruidosas, como los buses de locomoción colectiva, con el Decreto Supremo n.º 129 de 2002 del Ministerio de Transportes y Telecomunicaciones. Además, el 15 de septiembre de 1999 se aprueba el reglamento sobre condiciones sanitarias y ambientales básicas en los lugares de trabajo que en su Título IV, Párrafo III, Artículos 70 al 82, regula la exposición al ruido en el trabajo.

En Ecuador no se ha determinado normativa específica a la contaminación sonora. En algunos decretos generales de protección del ambiente se han hecho alusiones pequeñas a este tipo de contaminación.

En la ciudad de Quito se emitió la ordenanza metropolitana 123 el 5 de julio de 2004 denominada La ordenanza para la prevención y control de la contaminación por ruido, sustitutiva del capítulo II para el control del ruido, del título V del libro segundo del código.


El ruido en las ciudades es un problema que se aborda desde muy variadas posiciones en España. Más que una cuestión de salud, suele tratarse como un problema político e incluso ético. Numerosas encuestas e informes de expertos, señalan el ruido de las actividades de ocio (música callejera, conciertos, botellones), y no otros ruidos, como uno de los principales causantes de la contaminación acústica.

La música alta, el botellón o los pubs y discotecas aglutinan el mayor número de críticas por parte de los ciudadanos y políticos de los centros urbanos españoles, como causantes del ruido que impide llevar una vida más saludable a las personas. En este sentido, el jefe de Servicio de Información Geográfica del Instituto de Cartografía de Andalucía, Antonio Fajardo de la Fuente, culpaba en un artículo de la revista "Amigos de los Museos", a los jóvenes que hacían botellón y a las motocicletas con escape libre, de la excesiva contaminación acústica que había en el municipio sevillano de Osuna.

Sin embargo, hay estudios que demuestran que hay otros elementos que pueden generar más ruido que los bares, locales de fiestas, concentraciones callejeras, etc. De esta forma, los coches y las motocicletas causan el 47 % del ruido que se genera en las ciudades españolas, por solo el 6 % que generan los peatones o el 2,2 % que producen los perros.

Otros estudios concluyen que los taladradores de las obras o el paso de los aviones por encima de los edificios, generan hasta 130 decibelios (dB) (el umbral del dolor está en 140 según la OMS), mientras que el ruido de discotecas es de 110 dB y el de una conversación en la calle, de 50 db de media.

Con esto, se concluye que, pese al pensamiento generalizado en muchas capas de la población, no son los jóvenes ni las actividades de ocio los principales causantes de la contaminación acústica en las ciudades españolas. A pesar de esto, las normativas y leyes se empeñan en limitar el ruido en estos ámbitos antes que en otros más ruidosos.

En 1976 Venezuela establece la Ley Orgánica del Ambiente la cual promulga los principios rectores para la conservación, defensa y mejoramiento del ambiente en beneficio de la calidad de vida. En el artículo 88 de esta ley, impone pena de arresto "a quienes dentro de parques nacionales, monumentos nacionales, reservas o refugios de fauna silvestre: Inc. 2: Utilicen radiorreceptores, fonógrafos o cualquier instrumento que produzca ruido que por su intensidad, frecuencia o duración fuesen capaces de causar daño o perturbar la calma y tranquilidad de esos lugares. Inc. 10: Perturbar conscientemente a los animales por medio de gritos, ruidos, proyecciones de piedras, derrumbes provocados o cualquier otro medio". El artículo 101 establece que quien, contraviniendo las disposiciones legales dictadas por autoridad competente, produzca o permita la producción de ruidos que por intensidad, frecuencia o duración fuesen capaces de causar daño o malestar a las personas, será sancionado con arresto de 15 a 30 años y multa de 15 a 30 días de salario mínimo. Si el ruido es producido en zonas o bajo condiciones capaces de aumentar el daño y malestar de las personas, la pena será aumentada al doble.

Bolivia, Colombia, Perú, Ecuador y Venezuela firmaron en Cartagena de Indias el Acuerdo Acta de Barahona" con fecha 5 de diciembre de 1991, creando un Comité Ambiental Andino con base en la primera reunión de actividades nacionales del medio ambiente celebrada en Caracas en agosto de 1991. Su objeto fue centralizar los esfuerzos sobre conservación del medio y disminución de contaminación a nivel regional, nacional y municipal en la zona, sin que hasta el presente, conforme a informes diplomáticos, el mismo se haya puesto en práctica.

Con el fin de erradicar y atenuar un poco los efectos del exceso de ruido en las diferentes partes del planeta, muchos especialistas en el tema han planteado algunos métodos para estos: en algunos casos se habla de la elaboración de un mapa acústico, en el cual se encierran medidas y análisis de los diferentes niveles sonoros de diversos puntos de la ciudad, haciendo énfasis en el sonido provocado por el tráfico sin olvidar otro tipo de emisores de ruido.

Constituye uno de los métodos más eficientes y a la vez económicos. Se trata de los denominados tapones auditivos (o "conchas acústicas"), que tienen la capacidad de reducir el ruido en casi 20 dB, lo cual permite que la persona que los usa pueda ubicarse en ambientes muy ruidosos sin ningún problema. Muy usado por los operarios y demás trabajadores de algunas industrias ruidosas.

Su utilización consiste en ubicarlos en lugares estratégicos, de forma que puedan cumplir con su función eliminando aquellos componentes de ruido que no deseamos escuchar. Entre los materiales que se usan tenemos: resonadores fibrosos, porosos o reactivos, fibra de vidrio y poliuretano de célula. La función principal de estos materiales es la de atrapar ondas sonoras y posteriormente transformar la energía aerodinámica en energía termodinámica o calor. A la hora de seccionar el material adecuado, de acuerdo a la aplicación requerida, debe tenerse en cuenta el coeficiente de absorción sonora del material, la cual es un dato que debe brindar el fabricante.

Su función principal es la de evitar la transmisión de ruido de un lado a otro de su cuerpo físico. Su mayor utilidad se encuentra en áreas con un alto nivel de ruido. Su desempeño se basa en la eliminación de propagación de ondas y contaminación sonora de áreas contiguas de producción. En este caso, la selección de una barrera acústica determinada se basa en el coeficiente de transmisión de sonido, traducido en la cantidad de potencia sonora que la barrera puede contener. Una barrera acústica es una especie de cortina transparente de vinil o poliuretano de célula abierta. También se usan paneles metálicos con altos índices de absorción.

Los aislamientos se hacen en secciones industriales ruidosas,su función básica es la de disipar la energía mecánica asociada con las vibraciones. Su foco de acción se concentra en zonas rígidas de la maquinaria en cuestión, los cuales son los puntos donde se generan vibraciones y donde se promueven el colapso de ondas sonoras. En la actualidad, muchos fabricantes de maquinaria ruidosa desde secadores hasta refrigeradores, han adoptado medidas de este tipo, conscientes del gran perjuicio que puede causar a la salud humana.

Pese a su gran capacidad de controlar niveles muy altos de ruido por medio del aislamiento de la fuente emisora del mismo, del resto de la fuerza laboral, son poco utilizadas en la industria. Estas casetas permiten que maquinarias industriales emisoras de un alto nivel de ruido desempeñen su función bajo niveles de ruido tolerables.

La reducción del ruido se debe llevar a cabo siguiendo la secuencia de medidas a tomar que se muestra a continuación, ordenadas de mayor a menor eficacia y de un aspecto colectivo a uno individual:




</doc>
<doc id="5352" url="https://es.wikipedia.org/wiki?curid=5352" title="Diola">
Diola

Los diola o jola en grafía wólof son un grupo étnico que se encuentra en el actual Senegal (donde son predominantes en la región de Casamance), Gambia y Guinea-Bisáu. Hay un gran número de ellos en la costa atlántica entre la ribera sur del río Gambia, la región de la Casamance en Senegal y la región norte de Guinea-Bisáu. Se cree que los diola precedieron a los mande y fula en la costa ribereña de Senegambia y podrían haber migrado hacia Casamance antes del siglo XIII. El idioma diola se diferencia del dioula (dyoula) del pueblo mande de Gambia, del alto Níger y de las tierras altas Kong de Burkina Faso. Existen diversas lenguas jola todas ellas clasificadas dentro de las lenguas bak dentro de la rama atlántica occidental de las lenguas Níger-Congo.

Los diola empezaron a establecerse en el territorio que actualmente ocupan desde comienzos del s. XVI. Algunos fundaron sus propios pueblos independientes; otros escogieron establecer en pueblos preexistentes donde vivieron a menudo en áreas separadas, una práctica que todavía es común hoy.

La región de Casamance es la región con mayor concentración. Está separada del resto de Senegal por Gambia y el río Gambia. Los sentimientos secesionistas de Casamance han existido desde los tiempos coloniales durante los que los diolas se resistieron a la influencia francesa. Tradicionalmente, las personas de Casamance han permanecido apartadas de otras partes de Senegal. La separación geográfica y política por el río Gambia y la colonia británica de Gambia les ayudó a mantener su propio idioma y cultura pero también fue un inconveniente para su incorporación al resto de Senegal. Las diferencias son enormes en muchos aspectos: idiomáticas, culturales, religiosas (mientras que en el resto del país más del 80 por ciento de la población es musulmana, los diolas y otros pueblos de Casamance han mantenido su religión tradicional o el cristianismo. Después de la Independencia de Senegal, en 1960, los nuevos gobernantes del Estado, a través de su centralismo copia del estado francés repitió las prácticas coloniales. El tal "colonialismo interior" producía desigualdades socio-económicas excesivas entre los grupos étnicos y, como resultado, las gentes perjudicadas consideran que su región está dominada políticamente, y económicamente se consideran explotados por el colonizador interior, eso es, por su gobierno. 

La mayor parte de sus ingresos agrícolas y turísticos de la región se dirige a Dakar, la capital del país. Su sentido de abandono por el gobierno en términos de infraestructura, educación, y el desarrollo económico es aún más manifiesto cuando se comparan con el desarrollo de la vecina Gambia. 

Cuando el presidente de Gambia Dawda Kairaba Jawara vio amenazado su gobierno en 1981, el presidente de Senegal Abdou Diouf envió tropas en su socorro y como consecuencia ambos países firmaron la creación de la "Confederación de Senegambia" el 12 de diciembre de 1981 (entrada en vigor el 1 de febrero de 1982). Pero el acuerdo se disolvió el 30 de septiembre de 1989 debido a sus diferencias políticas (Diouf pretendía convertirse en presidente de la Confederación), y por el descontento de Gambia con lo que consideraban una injusta política de precios. El fracaso de la confederación fue negativa para Casamance porque ellos preferían comerciar con Banjul (la capital de Gambia) más que con Dakar (la capital de Senegal). Casamance y Gambia compartieron ambos una experiencia común: la dominación por parte del Estado de Senegal. Además, la proximidad geográfica con Banjul hacía más fácil el transporte y menos caro. Así, la desintegración de la confederación de Senegambia empeoró la situación económica de Casamance. 

La fase actual de movimiento secesionista en Casamance empezó en 1982, cuando el MFDC, dirigido por el pueblo diola organizó una marcha pacífica para exigir la secesión del estado senegalés. El gobierno ahogó la protesta arrestando a los líderes. Desde entonces, el gobierno ha empleado la fuerza como respuesta a las demandas políticas de Casamance. 

Son centenares los muertos reconocidos y miles las personas que han tenido que abandonar sus hogares huyendo de los ataques armados del ejército senegalés desde 1982. La situación turbulenta actual de la región de Casamance puede ser atribuida fundamentalmente a los agravios políticos y económicos que han sufrido y solo secundariamente como resultado de una hostilidad étnica o de celos de los diolas hacia los wólof (el grupo étnico dominante del país: 36 %).

Los diolas exigen que se resuelva dos situaciones que consideran injustas: 

La primera, el abandono económico desde la Independencia y la explotación de sus tierras por parte del gobierno central, dándose el caso de que en los años 80, para lograr una mayor productividad de las ricas tierras de Casamance, muchos pequeños agricultores que mantenían una agricultura de subsistencia fueron expropiados para más adelante transferir dichas tierras a colonos del norte (es decir, Wolofs, Serers, y Toucouleurs). 

Y en segundo lugar, el enfrentamiento debido al desprecio de su gobierno hacia sus diferencias étnicas, lingüísticas y religiosas. El pueblo diola no hablan wólof, el idioma principal de la nación, o francés, el idioma del gobierno de Senegal. Son despreciados por sus creencias tradicionales o cristianas. 

Es por esto, por lo que los diolas, piden que se les permita explotar sus propios recursos económicos terminando con el colonialismo interior y se acabe con el abandono en términos de infraestructura y educación. 

Durante los años noventa, continuaron los ataques intensos del ejército a pesar del cese el fuego acordado en 1993. Solo en 1995, el gobierno fue acusados de la muerte de centenares de personas. La región se ha calmó desde finales de 1995 cuando el MFDC pidió un nuevo alto el fuego. Las negociaciones entre el gobierno y los rebeldes empezaron a principios de 1996. Los rebeldes están más interesados en lograr un tratamiento igual y en el desarrollo de su región que en independencia completa. Les gustaría que el gobierno de Senegal volviera a poner en Casamance algunas de las ganancias que recibe de los recursos de la región. Las negociaciones actuales pueden resultar en una paz duradera en el Casamance, pero el aislamiento geográfico de la región del resto del país y las diferencias culturales de las personas de la región puede continuar causando tensiones entre Casamance y el norte. 

Casamance se ve favorecido con 2 a 3 veces más lluvia que el norte de Senegal. Mientras la parte norte es una inmenso sabana amenazada de una constante y rápida desertización, Casamance disfruta de arbolado y tierras fecundas buenas para la agricultura. La región produce la mayoría de la comida del país (incluso la mitad del arroz del país, algodón y maíz) produciendo tanto para el uso nacional como para la exportación. Cuenta además con los principales recursos turísticos de Senegal. 

Los diola viven en clanes, y el clan es el aspecto más importante de sus vidas. Las personas son furiosamente fieles a sus clanes y los defienden orgullosamente. Transmiten su historia y creencias a través de las tradiciones orales, canciones y danzas. Los hombres y mujeres viven en casas separadas hechas de barro o cemento; los hombres en casas de la redondas y las mujeres en viviendas rectangulares. El padre es la cabeza de la familia, y las herencias se pasan de los padres a sus hijos. Los varones más viejos poseen la mayoría del poder e influencia. Tienen una esperanza de vida de 45 años, siendo la mitad de la población menor de 15 años. Hay diferentes clanes desde los diola kalunae hasta los diola fogni asentados en Senegal.

Aproximadamente un 20 % mantiene la religión tradicional, un 50 % la cristiana y el 30 % la musulmana.



</doc>
<doc id="5357" url="https://es.wikipedia.org/wiki?curid=5357" title="Córdoba (Colombia)">
Córdoba (Colombia)

Córdoba es uno de los 32 departamentos que, junto con Bogotá, Distrito Capital, forman la República de Colombia. Su capital es Montería. Está ubicado al norte del país, en la región Caribe, limitando al norte con el mar Caribe (océano Atlántico), al este con los departamentos de Sucre y Bolívar, y al sur y oeste con el departamento de Antioquia. Con 1 710 000 habitantes en 2015, es el octavo departamento más poblado, por detrás de Antioquia, Valle del Cauca, Cundinamarca, Atlántico, Bolívar, Santander y Nariño. Fue creado por medio de la Ley 9 del 18 de diciembre de 1951, reglamentada el 18 de junio de 1952.

La historia del departamento se construye a partir de los registros del Archivo General de la Nación, del Archivo General de Indias, de crónicas dejadas por los españoles, por la tradición oral y por investigaciones realizadas recientemente. El nombre del departamento fue tomado del general José María Córdova, como un homenaje al prócer de la independencia por su importante participación en la libertad de Colombia.

Va desde la aparición de los primeros pueblos que cruzaron por el río Sinú procedentes de Norteamérica hace más de 6000 años, hasta 1501 aproximadamente, fecha en la que arribó al actual departamento de Córdoba la primera expedición española. En esta etapa los Zenúes fueron los señores de estos vastos territorios y desarrollaron una de las más prósperas culturas de América.

En opinión de algunos investigadores, los Zenúes alcanzaron el formativo superior. Sin embargo, por la destrucción y saqueo de sus tumbas a la llegada de los españoles, no es posible dar por hecho las mencionadas opiniones. El descubrimiento arqueológico de San Jacinto en enero de 1992 ha aportado nuevos elementos de juicio para esclarecer la controversia.

Abarca el período comprendido entre 1500 hasta la emancipación de la corona española, en las dos primeras décadas del siglo XIX. En estos tres siglos los españoles, fundaron ciudades, impusieron un nuevo régimen económico, político, administrativo y religioso, mezclándose con ellos como lo demuestra la tipología racial existente en la región.

El litoral cordobés fue reconocido por Rodrigo de Bastidas en 1501, quien arribó a la bahía de Cispatá y descubrió las bocas del río Sinú y las islas Fuerte y Tortuguilla; posteriormente llegaron Alonso de Ojeda, Francisco Pizarro y Martín Fernández de Enciso, quien se internó por el río Sinú hacia el interior, en busca de riquezas. Estos conquistadores iniciaron la fundación de poblaciones como Chimá (1573), San Andrés de Sotavento (1600), Los Córdobas (1621) y Momil (1693), entre otras. Durante este período Córdoba perteneció a la Provincia de Cartagena.

Antonio de la Torre y Miranda realizó varias expediciones al territorio cordobés por encomienda del gobernador de Cartagena, Juan de Torrezar Díaz Pimienta. La primera la inició en 1774 con la fundación y refundación de las poblaciones situadas en la zona de influencia de los ríos Sinú y San Jorge. En 1775 fueron fundados Chinú y Sahagún. En 1776, Momil, Lorica, San Bernardo del Viento, Ciénaga de Oro, San Antero y Chimá. En 1777, Montería, San Carlos, San Pelayo y Purísima. En sus expediciones, Antonio de la Torre y Miranda fomentó la cría de animales vacunos y domésticos, enseñó como preparar sementeras y cultivar algodón y maíz en forma técnica.

Comprende desde los años del grito de independencia (1810-1819) hasta nuestros días y tiene al año de 1952 como referencia en la identidad del departamento, ya que este fue el año en que se creó como entidad territorial independiente. Esto implicó el establecimiento de una etapa pre-segregacional y posterior a ella, una etapa post-segregacional luego de la escisión del departamento de Bolívar.

Con las consolidación y creación del departamento de Córdoba por Ley 9 del 18 de diciembre de 1951 y reglamentada el 18 de junio de 1952, el Departamento adquiere autonomía regional lo que le provoca un notable desarrollo. Comienza así la etapa Posegregacional que se extiende hasta nuestros días.

En marzo de 2019, la Defensoría del Pueblo informó sobre el incremento de los desplazamientos masivos ocurridos en días al sur del departamento de Córdoba, en los que más de 3000 personas fueron afectadas, de los cuales 181 son menores de edad.

El departamento de Córdoba está situado en la parte noroccidental de Colombia sobre la extensa Llanura del Caribe (132 000 km²) a los 7° 22’ y 9° 26’ de latitud norte y a los 74° 47’ y 76° 30’ de longitud al oeste de Greenwech. Tiene una superficie de 23 980 km², que en términos de extensión es similar a la de Cerdeña.

Hace parte de la región Caribe colombiana junto con los departamentos de Sucre, Cesar, Magdalena, San Andrés y Providencia, Bolívar, Atlántico y Guajira. Tiene una extensión de 23 980 km², limita por el norte con el Mar Caribe, por el oeste, sur y suroriente con Antioquia y al este con Bolívar y Sucre.

La geografía de Córdoba presenta dos zonas fácilmente diferenciables: una plana y otra montañosa, que es la que limita con Antioquia.

Representa aproximadamente el 60% de la superficie total del departamento y está formada por la gran llanura del Caribe. Esta zona posee elevaciones que no superan los 100 m s. n. m. y alberga los valles aluviales de los ríos Sinú, San Jorge y el área costera. La mayor parte de los municipios están en esta zona, donde la actividad agroeconómica es intensa.

La costa cordobesa se extiende desde la punta de Arboletes en límites con Antioquia hasta Punta de Piedra en límites con Sucre, sobre el golfo de Morrosquillo, recorriendo los municipios de Los Córdobas, Puerto Escondido, San Bernardo del Viento, Moñitos y San Antero. En total son 124 km de costa y 6 km en promedio de anchura. Las corrientes fluviales en la costa son pocas, pero se pueden mencionar los ríos Canalete y Mangle.

Esta zona no surgió sino a fines de la década de 1950 cuando luchas entre campesinos y hacendados de la región aledaña a la desembocadura del Sinú modificaron su curso. Cuando el río cambió su desembocadura de Cispatá por la de Boca de Tinajones, aquella se salinizó formándose un ecosistema de estuario y el naciente delta permitió el depósito de muchas especies y control del Sinú. Se calcula que la extensión de esta zona es de 130 km² y se ubica en los municipios de San Bernardo del Viento, San Antero y Lorica, incluyendo ambos deltas y los caños del Lobo, Salado, Sicará y las ciénagas de Garzal, Corozo y Ostional.

Las ciénagas más importantes son, entre las muchas que se ubican en el departamento, las siguientes:




Está conformada por ramificaciones de la cordillera occidental. Cuando el sistema andino llega al Nudo de Paramillo se trifurca y penetra al departamento así: al occidente la serranía de Abibe, que más al norte se bifurca tomando los nombres de El Águila y Las Palomas. Por el centro penetra la serranía de San Jerónimo, y por el oriente la serranía de Ayapel.




La hidrografía es muy rica y variada. A lo largo y ancho de sus ríos y mar, logra crear un ecosistema lleno de peces, cangrejos, camarones, etc., que se aprovechan en las labores culinarias y comerciales.

El sistema hidrográfico de Córdoba está conformado por el valle del Sinú, que abarca 1 207 000 hectáreas, y recoge los afluentes del sur del departamento; la zona del valle del San Jorge, que abarca 965 000 hectáreas en el sureste del departamento, y canaliza las aguas de la Ciénaga de Ayapel hacia la depresión momposina; y la zona de los ríos Canalete y Mangle, ubicada al noroeste del departamento. En todo el departamento hay 846 km de ríos principales y más del doble de afluentes y otros cauces. Existen también 110 000 hectáreas de ciénagas y una apreciable cantidad de aguas subterráneas -que según la Corporación Autónoma Regional de los Valles del Sinú y San Jorge, CVS- no cuantificadas en su totalidad.

Los principales ríos, el Sinú y el San Jorge, nacen en el Nudo de Paramillo, y corren paralelamente en sus primeros tramos, separados únicamente por la serranía de San Jerónimo.




Córdoba recibe vientos del sistema pacífico, vientos alisios del sudeste y noreste, también las brisas marinas del Caribe. La lluvia media anual va desde los 1000mm en el bajo Sinú y la costa, hasta los 4000 mm al sur. La mitad del territorio recibe un promedio anual que va desde los 1400 y 1800 horas de luz/año, un 40% está entre 1800 y 2200 horas de luz/año y en ciertos municipios como Sahagún, Chinú y en la depresión momposina, este promedio aumenta y puede variar desde los 2200 a los 2600 horas luz/año en promedio.

Debido al escaso promedio de altitud que tiene el territorio cordobés, la zona inferior de la atmósfera, llamada troposfera, presenta una alta temperatura de aire que en promedio es de 32°c.

Sobre la base de las precipitaciones y temperaturas -según la clasificación de climas de Wladimir Koppen- la mayor parte del territorio está en la zona tropical lluviosa (A) ya que su temperatura supera los 20°c y las precipitaciones están por encima de los 750 mm anuales. Dentro de esta misma zona tropical lluviosa, se presenta hacia el sur un clima muy húmedo de selva ecuatorial con lluvias durante todo el año (Af). Hacia la parte media y baja del Sinú y del San Jorge, se da un clima húmedo durante todo el año pero con períodos menos lluviosos (Am). La parte baja del Sinú, excepto en la desembocadura y a la altura de los municipios de Ciénaga de Oro, Sahagún, Chimá, Chinú, Lorica y Purísima hay un clima de Sabana, periódicamente húmedo y con lluvias cenitales (Aw). En la desembocadura del Sinú hay clima seco de baja latitud (B), de tipo BSwh o clima de sabana xerófilo cálido, con lluvias cenitales. El piso térmico es cálido.

El suelo cordobés es muy rico y variado hecho que se explica, entre otras cosas, por su zona de montañas y colinas, de litoral, planicie fluviolacustre y los de planicie aluvial y de piedemonte.


En Córdoba se ubica el Parque nacional natural Paramillo, estrella hidrográfica del departamento.

Políticamente el departamento de Córdoba está dividido en 30 municipios; 5 de los cuales pertenecen a la zona costanera, 16 a la zona o cuenca del Sinú, y 9 a la del San Jorge. Posee 308 corregimientos, 210 caseríos y seis inspecciones de policía.

La raza es producto del cruce entre los Zenúes que habitaron en el departamento en la época precolombina, los negros traídos del África durante la colonia, en su capital árabes inmigrantes especialmente de Líbano y Siria, y los colonizadores hispanos. Cada grupo aportó elementos genéticos, históricos y folclóricos. La raza mestiza se encuentra en mayor proporción en el medio y bajo Sinú, donde la mezcla con inmigrantes sirio-libaneses es apreciable. Negros hacia la zona costera e indígenas para el alto Sinú y San Jorge donde también están concentrados grupos de mulatos (negro y blanco) y zambo (negro e indio).


La economía regional se sostiene sobre dos pilares fuertes y propios para el terreno: ganadería y agricultura. La ganadería es el primer renglón económico del departamento, por lo que grandes extensiones de tierra han desplazado la agricultura tradicional para dar paso a haciendas ganaderas. La agricultura está representada por cultivos de arroz, maíz, ñame, yuca, ajonjolí, plátano, caña de azúcar, algodón, sorgo, cacao y coco. El sector industrial minero se concentra en la producción de ferroníquel en "Cerro Matoso" (municipio de Montelíbano) y la explotación de carbón mineral en el municipio de Puerto Libertador. Además la explotación de la madera se ha convertido en el segundo producto de exportación de Córdoba. Algunas minas que se explotan en la parte sur del departamento son de carácter ilegal, las cuales financian a grupos al margen de la ley.

Los servicios y el comercio se localizan principalmente en la capital.

Representa el 8% del total del territorio. Se estima que unas 170 000 hectáreas están dedicadas a cultivos semestrales, anuales y permanentes. Los principales productos son el maíz, algodón, arroz, ñame, yuca, plátano, coco, sorgo, ajonjolí, etc.

Se practica especialmente en las sabanas del departamento. Montería, sede anual del Reinado Nacional de la Ganadería, es la capital ganadera de Colombia. Se crían tipos vacunos como el Cebú, Pardo Suizo, Holstein y el muy cordobés Romo Sinuano.

Los pastos son de planicie y de colina. Los primeros están en el bajo Sinú y San Jorge. Predominan en esta zona el Pará o admirable, resistente a las inundaciones. En los sitios no inundables se dan los pastos de Guinea que junto con el Pará, fueron traídos de Brasil y Venezuela en 1875. Los segundos son pastos poco alimenticios en épocas de sequía. En las colinas bajas crece la guinea y el Puntero en las partes altas.

La industria pesquera, minera, hidroeléctrica, maderera y manufacturera son renglones de singular importancia dentro de la economía departamental. El yacimiento de ferroníquel de Cerromatoso ubicado en un cerro aislado de 269 m s. n. m. a 22 km de Montelíbano, fue descubierto en 1956 por la Richmond Petroleum, subsidiaria de la Standard Oil Company. El gobierno concedió a la Richmond un contrato de concesión, distinguido con el N° 866 del 30 de marzo de 1963, el cual fue modificado en sus términos mediante contrato adicional del 22 de julio de 1970, dicho contrato permitió la entrada del gobierno nacional como inversionista a través del IFI. En 1979 ingresa como socio la empresa holandesa Billiton (desde el 2001 BHP Billiton) y se constituye Cerromatoso S.A.


La cocina cordobesa es muy variada y elaborada; emplea productos de raigambre indígena como el maíz y la yuca que se han complementado con otros ingredientes como la berenjena y la almendra de los árabes, y el arroz, el plátano y el ñame de las culturas africana y asiática. Estos alimentos, junto con el pescado, la carne de res y de cerdo, conforman la esencia de la cocina de Córdoba.

Cuando los ríos Sinú, San Jorge y Cauca empiezan a bajar de cauce como consecuencia del verano, se produce el fenómeno de “la subienda”. Es la invasión anual de millones de peces que se conocen con el nombre científico de prochilodus reticulatus Magdalenae y con el popular de bocachico.

El consumo de bocachico no se limita única y exclusivamente a los habitantes de la región sinuana, también es despachado a las sabanas de Córdoba, Sucre, Bolívar y a otros departamentos de la costa y el interior del país. Con él se preparan diversos platos, desde el bocachico ahumado hasta el sancocho, plato típico de Córdoba.

Platos Típicos:

La cultura del departamento de Córdoba está representada por la música de bandas folclóricas y el porro, así como el fandango y las corralejas. Estas se realizan en la mayoría de los municipios del departamento.


De acuerdo a una nota publicada en el periódico El Tiempo, existe una confusión sobre la manera correcta de escribir el apellido del prócer José María Córdova y por ende, del departamento, el cual se escribe con B, a pesar de ser un homenaje al militar neogranadino, aunque de manera temporal, el departamento fue llamado Córdova, pero le fue devuelto el nombre original dispuesto en la ley de creación del departamento.

Esta confusión se debe a que los primeros españoles de apellido Córdoba llegaron de Andalucía a la región de Antioquia, entre el XVII y XVIII y que por esta misma razón, el prócer fue bautizado con el apellido Córdoba, pero en su carrera militar, decidió cambiar la B de su apellido por la V de la victoria.




</doc>
<doc id="5358" url="https://es.wikipedia.org/wiki?curid=5358" title="Kitesurf">
Kitesurf

El kitesurf o kitesurfing (llamado también a veces kiteboarding, o flysurfing) e incluso se ha propuesto la adaptación tablacometa, es un deporte de deslizamiento que consiste en el uso de una cometa de tracción ("kite", del inglés), que tira del deportista ("kitesurfista") por cuatro o cinco líneas, dos fijas a la barra (de dirección), y las dos o tres restantes (de potencia) pasan por el centro de la barra y se sujetan al cuerpo mediante un arnés, permitiendo deslizarse sobre el agua mediante una tabla (bidireccional o twintip, tabla de surf, tabla de race o hydrofoil).

Se pueden practicar varias modalidades; saltos y maniobras ("estilo libre"), regatas entre boyas ("race") y "surf" en olas ("surfkite").

El equipo básico de "kitesurf" se compone de:

Y opcionalmente puede incluir elementos de seguridad y comodidad como:

También sirve contar con herramientas que permitan consultar la velocidad y la dirección del viento.

No se recomienda la práctica del kite con viento de tierra ("off-shore"), porque si hubiera algún tipo de percance, la cometa no nos dejaría volver a la orilla. En España existen sitios específicos donde con viento de tierra donde sí se puede navegar, como es el levante en la playa de Valdevaqueros en Tarifa, Cádiz y la playa de Sotavento en Jandía, Fuerteventura. Lo ideal son vientos siempre de dirección del mar, llamados On Shore o los que viene en dirección paralelo a la orilla llamados Side Shore. Según la dirección del viento elegiremos una playa u otra, en la provincia de Cádiz hay playas para todos los vientos.

El equipo básico contiene distintos elementos de seguridad. El tándem cometa-barra es el que más elementos contiene. En caso de que sople un viento demasiado fuerte con el que la cometa se pueda descontrolar y pueda arrastrar, la unión al arnés llamada "chickenloop" tiene una anilla de seguridad que permite soltar la cometa del cuerpo. Es sólo entonces cuando actúa la quinta línea, opcional, que evita que la cometa se aleje y se extravíe.

A través de la barra pasan unas líneas, dando un margen para que la barra pueda pegarse más al cuerpo o alejarse, esta acción influye ligeramente en la cometa haciéndola más o menos sensible al viento (captando más o menos viento), es por sí una medida de seguridad, ya que se puede regular la cometa cuando vienen rachas fuertes de viento. Las cometas de dos líneas no tienen este sistema, hoy en día imprescindible (por eso están en desuso).

Aunque la práctica de este deporte de manera extendida es muy reciente, se tiene conocimiento que desde el siglo , en China e Indonesia, se usaban cometas para arrastrar pequeñas embarcaciones. A principios de siglo , el inventor británico George Pocock patentó un sistema de tracción con cometas para carros y embarcaciones. Realizó varias pruebas y batió varios récords. Sus barcos podían navegar en rumbos a menos de 90 grados contra la dirección del viento. En noviembre de 1903, el inventor norteamericano Samuel Cody atravesó en Canal de la Mancha navegando con cometas. En 1970, el inglés Peter Powel inventó la cometa de dos líneas, y construyó una cometa en forma de delta con la que navegó en pequeños botes. No es, sin embargo, hasta 1977 cuando Gijsbertus Adrianus Panhuise patenta un sistema de navegación sobre una tabla de "surf" traccionada por una especie de paracaídas, convirtiéndose así en el padre de este deporte.

En Indonesia es una cultura y un arte, los diseños son amplios y variados, en estas zonas es dónde se encuentra la industria de kiteboarding.

El 26 de agosto de 2007, Gisela Pulido se convirtió por cuarta vez campeona del mundo, y en su primera temporada en el circuito profesional, a falta de dos pruebas para la finalización del campeonato.

Otro referente en España es Abel Lago, proclamándose en el 2007 campeón del mundo en la modalidad de olas en el Kiteboard Pro World Tour. Y en el 2008, subcampeón en la misma modalidad.

También cabe destacar como protagonista del "kitesurf" en España a Alex Pastor (actualmente el primero del ranking PKRA en la modalidad de "Freestyle"). Y a Álvaro Onieva, dueño de la marca de tablas Ride Clash que estuvo varios años en el top 3 del Campeonato del mundo (PKRA).

Existen varios tipos de cometas según el modo de navegación que se quiera realizar y atendiendo a esto existen una serie de tipos de cometas especializadas para cada caso. Entre los tipos existentes tenemos el tipo «Delta», híbrida, «Bow», tipo «C» y «Foil»:

La tabla es un elemento que debemos tener muy en cuenta si queremos progresar rápidamente en los inicios del deporte.

Las tablas más utilizadas son las TwinTip, que son tablas exclusivas y creadas únicamente para el KiteSurf. Son tablas bidireccionales, lo que quiere decir que su navegación será igual en ambos sentidos. Solamente necesitamos cambiar el rumbo para ir en un sentido u otro.

Otras tablas muy utilizadas en diferentes disciplinas del kitesurf es la tabla direccional o tabla de Surf. Esta en cambio es una tabla que solamente navega en una dirección, teniendo que modificar la posición de los pies para cambiar el rumbo.



</doc>
<doc id="5360" url="https://es.wikipedia.org/wiki?curid=5360" title="Convenio de Berna para la Protección de las Obras Literarias y Artísticas">
Convenio de Berna para la Protección de las Obras Literarias y Artísticas

El Convenio de Berna para la Protección de las Obras Literarias y Artísticas, más conocido como el Convenio de Berna, Convención de Berna, CBERPOLA o Tratado de Berna, es un tratado internacional sobre la protección de los derechos de autor sobre obras literarias y artísticas. Su primer texto fue firmado el 9 de septiembre de 1886, en Berna (Suiza). Ha sido completado y revisado en varias ocasiones, siendo enmendado por última vez el 28 de septiembre de 1979. 

La Convención de Berna se apoya en tres principios básicos y contiene una serie de disposiciones que determinan la protección mínima de obras literarias y artísticas que se concede al autor, además de las disposiciones especiales disponibles para los países en desarrollo que tuvieran interés en aplicarlos. Hasta marzo de 2018, 176 estados son parte del Convenio.

La Convención de Berna requiere que sus partes traten los derechos de autor de las obras de autores de otras partes de la convención (conocidos como miembros de la "Unión de Berna" ) al menos, así como los de sus propios ciudadanos. Por ejemplo, la ley de derechos de autor francesa se aplica a todo lo publicado o realizado en Francia, independientemente de dónde se creó originalmente.

Además de establecer un sistema de igualdad de trato que armonizara el derecho de autor entre las partes, el acuerdo también requería que los estados miembros establecieran normas mínimas estrictas para la ley de derechos de autor.

Los derechos de autor bajo el Convenio de Berna deben ser automáticos; Está prohibido exigir el registro formal. Sin embargo, cuando Estados Unidos se unió a la Convención el 1 de marzo de 1989,  continuó haciendo que los daños legales y los honorarios de abogados solo estuvieran disponibles para obras registradas.

Sin embargo, "Moberg v Leygues" (una decisión de 2009 de un Tribunal de Distrito Federal de Delaware) sostuvo que se supone que las protecciones de la Convención de Berna son esencialmente "sin fricción", lo que significa que no se pueden imponer requisitos de registro en un trabajo de un país miembro diferente de Berna. Esto significa que los países miembros de Berna pueden exigir que las obras originarias de su propio país estén registradas y / o depositadas, pero no pueden exigir estas formalidades de obras de otros países miembros de Berna. 

Los tres principios básicos son los siguientes:


En cuanto a las obras, la protección debe incluir "todas las producciones en el dominio literario, científico y de artes plásticas, cualquiera que pueda ser su modalidad o forma de expresión" (artículo 2). Los siguientes derechos figuran entre los que deben ser reconocidos como derechos exclusivos de autorización: los derechos de traducir, de hacer adaptaciones y arreglos de la obra; de interpretar en público obras dramáticas, dramático-musicales y musicales; de recitar en público obras literarias; de comunicar al público la interpretación de esos trabajos; de difundirlos; de reproducirlos en cualquier modalidad o forma; de usar las obras como base para un trabajo audiovisual; y de reproducir, distribuir, interpretar en público o comunicar al público esa obra audiovisual.

La convención abarca también los "derechos morales", es decir, el derecho de reclamar la autoría de la obra y el derecho de oponerse a cualquier mutilación, deformación u otra modificación de la misma, o bien, de otras acciones que dañan la obra y podrían ser perjudiciales para el honor o el prestigio del autor.

En cuanto a la vigencia de la protección, la regla general dispone que se deberá conceder protección, como mínimo, hasta que concluya un periodo de 50 años a partir de la muerte del autor.

Por "Obras literarias y artísticas" se entienden todas las producciones en el campo literario, científico y artístico, cualquiera que sea el modo o forma de expresión, tales como los libros, folletos y otros escritos; las conferencias, alocuciones, sermones y otras obras de la misma naturaleza; las obras dramáticas o dramático-musicales; las obras coreográficas y las pantomimas; las composiciones musicales con o sin letra; las obras cinematográficas, a las cuales se asimilan las obras expresadas por procedimiento análogo a la cinematografía; las obras de dibujo, pintura, arquitectura, escultura, grabado, litografía; las obras fotográficas a las cuales se asimilan las expresadas por procedimiento análogo a la fotografía; las obras de artes aplicadas; las ilustraciones, mapas, planos, croquis y obras plásticas relativos a la geografía, a la topografía, a la arquitectura o a las ciencias.

Los elementos esenciales del convenio de Berna son:


La Convención de Berna se desarrolló a instancias de Victor Hugo  de la Asociación Littéraire et Artistique Internationale .  Por lo tanto, fue influenciado por el " derecho del autor " francés ( "droit d'auteur" ), que contrasta con el concepto anglosajón de "derecho de autor" que solo se ocupaba de las preocupaciones económicas.  Según la Convención, los derechos de autor de las obras creativas.están vigentes automáticamente desde su creación sin ser afirmados o declarados. Un autor no necesita "registrarse" o "solicitar" un derecho de autor en países adheridos a la Convención. Tan pronto como una obra es "fija", es decir, escrita o grabada en algún medio físico, su autor tiene derecho automáticamente a todos los derechos de autor de la obra y a cualquier obra derivada , a menos y hasta que el autor los niegue explícitamente o hasta que el copyright expira Los autores extranjeros tienen los mismos derechos y privilegios para el material con derechos de autor que los autores nacionales en cualquier país que ratificó la Convención.

Antes de la Convención de Berna, la legislación sobre derechos de autor seguía siendo descoordinada a nivel internacional.  Entonces, por ejemplo, un trabajo publicado en Gran Bretaña por un ciudadano británico estaría cubierto por derechos de autor allí, pero cualquier persona en Francia podría copiarlo y venderlo. El editor holandés Albertus Willem Sijthoff , quien saltó a la fama en el comercio de libros traducidos, escribió a la reina Wilhelmina de los Países Bajos en 1899 en oposición a la convención por la preocupación de que sus restricciones internacionales sofocarían la industria gráfica holandesa. 

El Convenio de Berna siguió los pasos del Convenio de París para la Protección de la Propiedad Industrial de 1883, que de la misma manera había creado un marco para la integración internacional de los otros tipos de propiedad intelectual: patentes, marcas y diseños industriales . 

Al igual que el Convenio de París, el Convenio de Berna creó una oficina para manejar tareas administrativas. En 1893, estas dos pequeñas oficinas se fusionaron y se convirtieron en las Oficinas Internacionales Unidas para la Protección de la Propiedad Intelectual (mejor conocidas por su acrónimo francés BIRPI), situadas en Berna.  En 1960, BIRPI se mudó a Ginebra , para estar más cerca de las Naciones Unidas y otras organizaciones internacionales en esa ciudad.  En 1967 se convirtió en la Organización Mundial de la Propiedad Intelectual (OMPI), y en 1974 se convirtió en una organización dentro de las Naciones Unidas. 

La Convención de Berna fue completada en París en 1886, revisada en Berlín en 1908, completada en Berna en 1914, revisada en Roma en 1928, en Bruselas en 1948, en Estocolmo en 1967 y en París en 1971, y fue enmendada en 1979. 

El Tratado de Derecho de Autor de la Organización Mundial de la Propiedad Intelectual fue adoptado en 1996 para abordar los problemas planteados por la tecnología de la información e Internet, que no fueron abordados por el Convenio de Berna. 





</doc>
<doc id="5361" url="https://es.wikipedia.org/wiki?curid=5361" title="Corriente eléctrica">
Corriente eléctrica

La corriente eléctrica es el flujo de carga eléctrica que recorre un material. Se debe al movimiento de las cargas (normalmente electrones) en el interior del mismo. Al caudal de corriente (cantidad de carga por unidad de tiempo) se le denomina intensidad de corriente eléctrica (representada comúnmente con la letra I). En el Sistema Internacional de Unidades se expresa en culombios por segundo (C/s), unidad que se denomina amperio (A). Una corriente eléctrica, puesto que se trata de un movimiento de cargas, produce un campo magnético, un fenómeno que puede aprovecharse en el electroimán.

El instrumento usado para medir la intensidad de la corriente eléctrica es el galvanómetro que, calibrado en amperios, se llama amperímetro, colocado en serie con el conductor por el que circula la corriente que se desea medir.

Históricamente, la corriente eléctrica se definió como un flujo de cargas positivas (+) y se fijó el sentido convencional de circulación de la corriente, como un flujo de cargas desde el polo positivo al negativo. Sin embargo posteriormente se observó, gracias al efecto Hall, que en los metales los portadores de carga son negativos, electrones, los cuales fluyen en sentido contrario al convencional.
En conclusión, el sentido convencional y el real son ciertos en tanto que los electrones como protones fluyen desde el polo negativo hasta llegar al positivo (sentido real), cosa que no contradice que dicho movimiento se inicia al lado del polo positivo donde el primer electrón se ve atraído por dicho polo creando un hueco para ser cubierto por otro electrón del siguiente átomo y así sucesivamente hasta llegar al polo negativo (sentido convencional). Es decir la corriente eléctrica es el paso de electrones desde el polo negativo al positivo comenzando dicha progresión en el polo positivo.

En el siglo XVIII cuando se hicieron los primeros experimentos con electricidad, solo se disponía de carga eléctrica generada por frotamiento (electricidad estática) o por inducción. Se logró (por primera vez, en 1800) tener un movimiento constante de carga cuando el físico italiano Alessandro Volta inventó la primera pila eléctrica.

Un material conductor posee gran cantidad de electrones libres, por lo que es posible el paso de la electricidad a través del mismo. Los electrones libres, aunque existen en el material, no se puede decir que pertenezcan a algún átomo determinado. 

Una corriente de electricidad existe en un lugar cuando una carga neta se transporta desde ese lugar a otro en dicha región. Supongamos que la carga se mueve a través de un alambre. Si la carga "q" se transporta a través de una sección transversal dada del alambre, en un tiempo "t", entonces la intensidad de corriente "I", a través del alambre es:

Aquí "q" está dada en culombios, "t" en segundos, e "I" en amperios. Por lo cual, la equivalencia es:

Una característica de los electrones libres es que, incluso sin aplicarles un campo eléctrico desde afuera, se mueven a través del objeto de forma aleatoria debido a la energía calórica. En el caso de que no hayan aplicado ningún campo eléctrico, cumplen con la regla de que la media de estos movimientos aleatorios dentro del objeto es igual a cero. Esto es: dado un plano irreal trazado a través del objeto, si sumamos las cargas (electrones) que atraviesan dicho plano en un sentido, y sustraemos las cargas que lo recorren en sentido inverso, estas cantidades se anulan.

Cuando se aplica una fuente de tensión externa (como, por ejemplo, una batería) a los extremos de un material conductor, se está aplicando un campo eléctrico sobre los electrones libres. Este campo provoca el movimiento de los mismos en dirección al terminal positivo del material (los electrones son atraídos [tomados] por el terminal positivo y rechazados [inyectados] por el negativo). Es decir, los electrones libres son los portadores de la corriente eléctrica en los materiales conductores. 

Si la intensidad es constante en el tiempo, se dice que la corriente es continua; en caso contrario, se llama variable. Si no se produce almacenamiento ni disminución de carga en ningún punto del conductor, la corriente es estacionaria.

Para obtener una corriente de 1 amperio, es necesario que 1 culombio de carga eléctrica por segundo esté atravesando un plano imaginario trazado en el material conductor.

El valor "I" de la intensidad instantánea será:

Si la intensidad permanece constante, en cuyo caso se denota "I", utilizando incrementos finitos de tiempo se puede definir como:

Si la intensidad es variable la fórmula anterior da el valor medio de la intensidad en el intervalo de tiempo considerado.

Según la ley de Ohm, la intensidad de la corriente es igual a la tensión de la fuente dividido por la resistencia que oponen los cuerpos:

Haciendo referencia a la potencia, la intensidad equivale a la raíz cuadrada de la potencia dividida por la resistencia. En un circuito que contenga varios generadores y receptores, la intensidad es igual a:

donde formula_3 es el sumatorio de las fuerzas electromotrices del circuito, formula_4 es la suma de todas las fuerzas contraelectromotrices, formula_5 es la resistencia equivalente del circuito, formula_6 es la suma de las resistencias internas de los generadores y formula_7 es el sumatorio de las resistencias internas de los receptores.

Intensidad de corriente en un elemento de volumen:
formula_8 , donde encontramos n como el número de cargas portadoras por unidad de volumen dV; q refiriéndose a la carga del portador; v la velocidad del portador y finalmente dS como el área de la sección del elemento de volumen de conductor.

La corriente eléctrica es el flujo de portadores de carga eléctrica, normalmente a través de un cable metálico o cualquier otro conductor eléctrico, debido a la diferencia de potencial creada por un generador de corriente. La ecuación que la describe en electromagnetismo es:

Donde formula_10 es la densidad de corriente de conducción, formula_11 es el vector perpendicular al diferencial de superficie, formula_12 es el vector unitario normal a la superficie, y formula_13 es el diferencial de superficie.

La carga eléctrica puede desplazarse cuando esté en un objeto y este es movido, como el electróforo. Un objeto se carga o se descarga eléctricamente cuando hay movimiento de carga en su interior.

Se denomina corriente continua o corriente directa (CC en español, en inglés DC, de "direct current") al flujo de cargas eléctricas que no cambia de sentido con el tiempo. La corriente eléctrica a través de un material se establece entre dos puntos de distinto potencial. Cuando hay corriente continua, los terminales de mayor y menor potencial no se intercambian entre sí. Es errónea la identificación de la corriente continua con la corriente constante (ninguna lo es, ni siquiera la suministrada por una batería). Es continua toda corriente cuyo sentido de circulación es siempre el mismo, independientemente de su valor absoluto.

Su descubrimiento se remonta a la invención de la primera pila voltaica por parte del conde y científico italiano Alessandro Volta. No fue hasta los trabajos de Edison sobre la generación de electricidad, en las postrimerías del siglo XIX, cuando la corriente continua comenzó a emplearse para la transmisión de la energía eléctrica. Ya en el siglo XX este uso decayó en favor de la corriente alterna, que presenta menores pérdidas en la transmisión a largas distancias, si bien se conserva en la conexión de redes eléctricas de diferentes frecuencias y en la transmisión a través de cables submarinos.

Desde 2008 se está extendiendo el uso de generadores de corriente continua a partir de células fotoeléctricas que permiten aprovechar la energía solar.

Cuando es necesario disponer de corriente continua para el funcionamiento de aparatos electrónicos, se puede transformar la corriente alterna de la red de suministro eléctrico mediante un proceso, denominado rectificación, que se realiza con unos dispositivos llamados rectificadores, basados en el empleo de diodos semiconductores o tiristores (antiguamente, también de tubos de vacío).

Se denomina corriente alterna (simbolizada CA en español y AC en inglés, de "alternating current") a la corriente eléctrica en la que la magnitud y dirección varían cíclicamente. La forma de onda de la corriente alterna más comúnmente utilizada es la de una onda senoidal. En el uso coloquial, «corriente alterna» se refiere a la forma en la cual la electricidad llega a los hogares y a las empresas. 

El sistema usado hoy en día fue ideado fundamentalmente por Nikola Tesla, y la distribución de la corriente alterna fue comercializada por George Westinghouse. Otros que contribuyeron al desarrollo y mejora de este sistema fueron Lucien Gaulard, John Gibbs y entre los años 1881 y 1889. La corriente alterna superó las limitaciones que aparecían al emplear la corriente continua (CC), la cual constituye un sistema ineficiente para la distribución de energía a gran escala debido a problemas en la transmisión de potencia. 

La razón del amplio uso de la corriente alterna, que minimiza los problemas de trasmisión de potencia, viene determinada por su facilidad de transformación, cualidad de la que carece la corriente continua. La energía eléctrica trasmitida viene dada por el producto de la tensión, la intensidad y el tiempo. Dado que la sección de los conductores de las líneas de transporte de energía eléctrica depende de la intensidad, se puede, mediante un transformador, modificar la tensión hasta altos valores (alta tensión), disminuyendo en igual proporción la intensidad de corriente. Esto permite que los conductores sean de menor sección y, por tanto, de menor costo; además, minimiza las pérdidas por efecto Joule, que dependen del cuadrado de la intensidad. Una vez en el punto de consumo o en sus cercanías, la tensión puede ser de nuevo reducido para permitir su uso industrial o doméstico de forma cómoda y segura.

Las frecuencias empleadas en las redes de distribución son 50 y 60 Hz. El valor .

Se denomina corriente trifásica al conjunto de tres corrientes alternas de igual frecuencia, amplitud y valor eficaz que presentan una diferencia de fase entre ellas de 120°, y están dadas en un orden determinado. Cada una de las corrientes que forman el sistema se designa con el nombre de fase.

La generación trifásica de energía eléctrica es más común que la monofásica y proporciona un uso más eficiente de los conductores. La utilización de electricidad en forma trifásica es mayoritaria para transportar y distribuir energía eléctrica y para su utilización industrial, incluyendo el accionamiento de motores. Las corrientes trifásicas se generan mediante alternadores dotados de tres bobinas o grupos de bobinas, arrolladas en un sistema dispuesto a 120 grados eléctricos entre cada fase. 

Los conductores de los tres electroimanes pueden conectarse en estrella o en triángulo. En la disposición en estrella cada bobina se conecta a una fase en un extremo y a un conductor común en el otro, denominado "neutro". Si el sistema está equilibrado, la suma de las corrientes de línea es nula, con lo que el transporte puede ser efectuado usando solamente tres cables. En la disposición en triángulo o delta cada bobina se conecta entre dos hilos de fase, de forma que un extremo de cada bobina está conectado con otro extremo de otra bobina.

El sistema trifásico presenta una serie de ventajas tales como la economía de sus líneas de transporte de energía (hilos más finos que en una línea monofásica equivalente) y de los transformadores utilizados, así como su elevado rendimiento de los receptores, especialmente motores, a los que la línea trifásica alimenta con potencia constante y no pulsada, como en el caso de la línea monofásica.

Tesla fue el inventor que descubrió el principio del campo magnético rotatorio en 1882, el cual es la base de la maquinaria de corriente alterna. Él inventó el sistema de motores y generadores de corriente alterna polifásica que da energía al planeta.

Se denomina corriente monofásica a la que se obtiene de tomar una fase de la corriente trifásica y un cable neutro. En España y demás países que utilizan valores similares para la generación y trasmisión de energía eléctrica, este tipo de corriente facilita una tensión de 230 voltios, lo que la hace apropiada para que puedan funcionar adecuadamente la mayoría de electrodomésticos y luminarias que hay en las viviendas.

Desde el centro de transformación más cercano hasta las viviendas se disponen cuatro hilos: un neutro (N) y tres fases (R, S y T). Si la tensión entre dos fases cualesquiera (tensión de línea) es de 400 voltios, entre una fase y el neutro es de 230 voltios. En cada vivienda entra el neutro y una de las fases, conectándose varias viviendas a cada una de las fases y al neutro; esto se llama corriente monofásica. Si en una vivienda hay instalados aparatos de potencia eléctrica alta (aire acondicionado, motores, etc., o si es un taller o una empresa industrial) habitualmente se les suministra directamente corriente trifásica que ofrece una tensión de 400 voltios.

Se denomina corriente eléctrica estacionaria, a la corriente eléctrica que se produce en un conductor de forma que la densidad de carga ρ de cada punto del conductor es constante, es decir que se cumple que:




</doc>
<doc id="5364" url="https://es.wikipedia.org/wiki?curid=5364" title="Rocinante">
Rocinante

Rocinante es el nombre del caballo de Don Quijote en el famoso libro de Miguel de Cervantes Don Quijote de la Mancha, ""cuatro días se le pasaron en imaginar que nombre le pondría... y así después de muchos nombres que formó borró y quitó, añadió, deshizo y tornó a hacer en su memoria e imaginación, al fin le vino a llamar Rocinante, nombre a su parecer alto, sonoro y significativo de lo que había sido cuando fue rocín, antes de lo que ahora era, que era antes y primero de todos los rocines del mundo"". 

Así pues, antes de lo que ahora era, piel y huesos, fue rocín que Don Quijote aún seguía viendo como ""mejor montura que los famosos Babieca del Cid y Bucéfalo de Alejandro Magno"".

"Rocín" en español significa un caballo de trabajo o un caballo de baja calidad, pero también puede significar un hombre analfabeto o áspero. Hay palabras similares en inglés ( "rouncey" ), francés ( "roussin o roncin; rosse" ), portugués ( "rocim" ) e italiano ( "ronzino" ). La etimología es incierta.

El nombre es un juego de palabras complejo. En español , "ante" tiene varios significados y puede funcionar como una palabra independiente y como un sufijo. Un significado es "antes" o "previamente". Otro es "delante de". Como sufijo, "-ante" en español es adverbial; "rocinante se" refiere a funcionar como, o ser, un "rocín" . "Rocinante", entonces, sigue el patrón de Cervantes usando palabras ambiguas y multivalentes, lo cual es común en toda la novela.

El nombre de Rocinante, entonces, significa su cambio de estado desde el "viejo fastidio" de antes hasta el corcel "más destacado".  Como Cervantes describe la elección del nombre de Don Quijote: "nombre a su parecer alto, sonoro y significativo de lo que había sido cuando era rocín, antes de lo que era ahora, qué era antes y primero de todos los rocines del mundo"  - "un nombre, para su pensamiento, elevado, sonoro y significativo de su condición como hack antes de convertirse en lo que ahora era, el primero y más importante de todos los hacks del mundo".

En el capítulo 1, Cervantes describe el cuidadoso nombramiento de Don Quijote de su corcel:Pasaron cuatro días pensando qué nombre darle, porque (como se dijo a sí mismo) no era correcto que un caballo perteneciente a un caballero tan famoso, y uno con tantos méritos propios, no tuviera un nombre distintivo, y se esforzó por adaptarlo para indicar lo que había sido antes de pertenecer a un caballero andante, y lo que entonces era "





</doc>
<doc id="5365" url="https://es.wikipedia.org/wiki?curid=5365" title="Bucéfalo">
Bucéfalo

Bucéfalo (en griego, Βουκέφαλος o Βουκεφάλας, de βούς (bous), "buey, toro" y κεφαλή (kephalē), "cabeza", por lo que su significado es "cabeza de buey" o "cabeza de toro") es el nombre del caballo de Alejandro Magno, y posiblemente el caballo más famoso de la Antigüedad. 

Su nombre significa en griego "Cabeza de buey", apodo que al parecer recibió el animal por el aspecto redondeado de su cara y la considerable anchura de su frente, donde además resplandecía una mancha blanca en forma de estrella. Plinio el Viejo y Pseudo Calístenes dicen, en cambio, que esta mancha representaba precisamente una cabeza de toro y que estaba en su espalda. 

Plutarco relata que Bucéfalo fue comprado por trece talentos por el rey Filipo II de Macedonia a un tesalio llamado Filonico. Fue entonces cuando, según narra la leyenda, el caballo comenzó a mostrarse tosco y salvaje, relinchando y lanzando coces por doquier, sin que nadie lograra apaciguarlo. Sólo el joven Alejandro logró montar al caballo, y se dio cuenta de que el caballo recelaba de su propia sombra. Alejandro giró la cabeza del caballo hacia el sol, cegándole y subiéndose de un solo brinco al caballo, momento que haría pronunciar a su padre la célebre frase: "Hijo, búscate un reino que sea igual a tu grandeza, porque Macedonia es pequeña para ti." Se dice que desde entonces Bucéfalo sólo se dejaba montar por Alejandro.

Frente a esta descripción de la doma de Bucéfalo realizada por Plutarco, el texto del Pseudo Calístenes sobre la vida del conquistador griego da una versión distinta. Allí, se refiere que Bucéfalo era un caballo de hermosa figura, pero dominado por un furor salvaje que lo llevaba al extremo de la antropofagia motivado quizás por la creencia de que era descendiente de una de las Yeguas de Diomedes, por lo que Filipo decidió construirle una jaula de hierro a donde echaría a todos aquellos que desobedecieran sus leyes. El Oráculo de Delfos dijo a Filipo que sería rey de todo el mundo habitado aquel que pudiera montar a Bucéfalo y cruzar la ciudad de Pela. Cuando, con 15 años, Alejandro descubrió la caballeriza del animal y se acercó al caballo, éste extendió sus patas delanteras y relinchó suavemente, como si le reconociera como su amo, y el joven príncipe pudo sacarlo sin ayuda de los criados y cabalgar con él por la ciudad, dominado por una completa docilidad.

En otra versión narrada por Diodoro Sículo, el caballo había sido un regalo de Demarato de Corinto.

Acompañó a Alejandro por toda su campaña en Asia contra el Imperio Aqueménida, hasta que murió a los 30 años durante o después de la batalla del Hidaspes, librada por el ejército macedonio en el año 326 a. C contra el ejército del rey indio Poros. Aunque hay quienes piensan que murió en la propia batalla, esto es cuando menos dudoso, ya que otros creen que murió de agotamiento y de viejo en el lugar donde Alejandro fundó, en su honor, la ciudad de Alejandría Bucéfala. Se cree que este sitio está localizado frente al moderno pueblo de Jhelum, en la provincia del Panyab, al noreste del actual Pakistán.


</doc>
<doc id="5366" url="https://es.wikipedia.org/wiki?curid=5366" title="Transistor">
Transistor

El transistor es un dispositivo electrónico semiconductor utilizado para entregar una señal de salida en respuesta a una señal de entrada. Cumple funciones de amplificador, oscilador, conmutador o rectificador. El término «transistor» es la contracción en inglés de "transfer resistor" («resistor de transferencia»). Actualmente se encuentra prácticamente en todos los aparatos electrónicos de uso diario tales como radios, televisores, reproductores de audio y video, relojes de cuarzo, computadoras, lámparas fluorescentes, tomógrafos, teléfonos celulares, aunque casi siempre dentro de los llamados circuitos integrados.

El físico austro-húngaro Julius Edgar Lilienfeld solicitó en Canadá en el año 1925 una patente para lo que él denominó "un método y un aparato para controlar corrientes eléctricas" y que se considera el antecesor de los actuales transistores de efecto campo, ya que estaba destinado a ser un reemplazo de estado sólido del triodo. Lilienfeld también solicitó patentes en los Estados Unidos en los años 1926 y 1928. Sin embargo, Lilienfeld no publicó ningún artículo de investigación sobre sus dispositivos ni sus patentes citan algún ejemplo específico de un prototipo de trabajo. Debido a que la producción de materiales semiconductores de alta calidad no estaba disponible por entonces, las ideas de Lilienfeld sobre amplificadores de estado sólido no encontraron un uso práctico en los años 1920 y 1930, aunque acabara de construir un dispositivo de este tipo.

En 1934, el inventor alemán Oskar Heil patentó en Alemania y Gran Bretaña un dispositivo similar. Cuatro años después, los también alemanes Robert Pohl y Rudolf Hilsch efectuaron experimentos en la Universidad de Göttingen, con cristales de bromuro de potasio, usando tres electrodos, con los cuales lograron la amplificación de señales de 1Hz, pero sus investigaciones no condujeron a usos prácticos.Mientras tanto, la experimentación en los Laboratorios Bell con rectificadores a base de óxido de cobre y las explicaciones sobre rectificadores a base de semiconductores por parte del alemán Walter Schottky y del inglés Nevill Mott, llevaron a pensar en 1938 a William Shockley que era posible lograr la construcción de amplificadores a base de semiconductores, en lugar de tubos de vacío.

Desde el 17 de noviembre de 1947 hasta el 23 de diciembre de 1947, los físicos estadounidenses John Bardeen y Walter Houser Brattain de los Laboratorios Bellllevaron a cabo diversos experimentos y observaron que cuando dos contactos puntuales de oro eran aplicados a un cristal de germanio, se produjo una señal con una potencia de salida mayor que la de entrada. El líder del "Grupo de Física del Estado Sólido" William Shockley vio el potencial de este hecho y, en los siguientes meses, trabajó para ampliar en gran medida el conocimiento de los semiconductores. El término "transistor" fue sugerido por el ingeniero estadounidense John R. Pierce, basándose en dispositivos semiconductores ya conocidos entonces, como el termistor y el varistor y basándose en la propiedad de "transrresistencia" que mostraba el dispositivo. Según una biografía de John Bardeen, Shockley había propuesto que la primera patente para un transistor de los Laboratorios Bell debía estar basado en el efecto de campo y que él fuera nombrado como el inventor. Habiendo redescubierto las patentes de Lilienfeld que quedaron en el olvido años atrás, los abogados de los Laboratorios Bell desaconsejaron la propuesta de Shockley porque la idea de un transistor de efecto de campo no era nueva. En su lugar, lo que Bardeen, Brattain y Shockley inventaron en 1943 fue el primer transistor de contacto de punto, cuya primera patente solicitaron los dos primeros nombrados, el 17 de junio de 1946,a la cual siguieron otras patentes acerca de aplicaciones de este dispositivo. En reconocimiento a éste logro, Shockley, Bardeen y Brattain fueron galardonados conjuntamente con el Premio Nobel de Física de 1956 "por sus investigaciones sobre semiconductores y su descubrimiento del efecto transistor".

En 1948, el transistor de contacto fue inventado independientemente por los físicos alemanes Herbert Mataré y Heinrich Welker, mientras trabajaban en la Compagnie des Freins et Signaux, una subsidiaria francesa de la estadounidense Westinghouse. Mataré tenía experiencia previa en el desarrollo de rectificadores de cristal de silicio y de germanio mientras trabajaba con Welker en el desarrollo de un radar alemán durante la Segunda Guerra Mundial. Usando este conocimiento, él comenzó a investigar el fenómeno de la "interferencia" que había observado en los rectificadores de germanio durante la guerra. En junio de 1948, Mataré produjo resultados consistentes y reproducibles utilizando muestras de germanio producidas por Welker, similares a lo que Bardeen y Brattain habían logrado anteriormente en diciembre de 1947. Al darse cuenta de que los científicos de Laboratorios Bell ya habían inventado el transistor antes que ellos, la empresa se apresuró a poner en producción su dispositivo llamado "transistron" para su uso en la red telefónica de Francia.El 26 de junio de 1948, Wiliam Shockley solicitó la patente del transistor bipolar de unión y el 24 de agosto de 1951 solicitó la primera patente de un transistor de efecto de campo, tal como se declaró en ese documento, en el que se mencionó la estructura que ahora posee. Al año siguiente, George Clement Dacey e Ian Ross, de los Laboratorios Bell, tuvieron éxito al fabricar este dispositivo,cuya nueva patente fue solicitada el 31 de octubre de 1952. Meses antes, el 9 de mayo de ese año, el ingeniero Sidney Darlington solicitó la patente del arreglo de dos transistores conocido actualmente como transistor Darlington.

El primer transistor de alta frecuencia fue el transistor de barrera de superficie de germanio desarrollado por los estadounidenses John Tiley y Richard Williams de Philco Corporation en 1953,capaz de operar con señales de hasta 60MHz. Para fabricarlo, se usó un procedimiento creado por los ya mencionados inventores mediante el cual eran grabadas depresiones en una base de germanio tipo N de ambos lados con chorros de sulfato de indio hasta que tuviera unas diez milésimas de pulgada de espesor. El Indio electroplateado en las depresiones formó el colector y el emisor.El primer receptor de radio para automóviles que fue producido en 1955 por Chrysler y Philco; usó estos transistores en sus circuitos y también fueron los primeros adecuados para las computadoras de alta velocidad de esa época.

El primer transistor de silicio operativo fue desarrollado en los Laboratorios Bell en enero 1954 por el químico Morris Tanenbaum.El 20 de junio de 1955, Tanenbaum y Calvin Fuller, solicitaron una patente para un procedimiento inventado por ambos para producir dispositivos semiconductores. El primer transistor de silicio comercial fue producido por Texas Instruments en 1954 gracias al trabajo del experto Gordon Teal quien había trabajado previamente en los Laboratorios Bell en el crecimiento de cristales de alta pureza. El primer transistor MOSFET fue construido por el coreano-estadounidense Dawon Kahng y el egipcio Martin Atalla, ambos ingenieros de los Laboratorios Bell, en 1960.

El transistor consta de tres partes dopadas artificialmente (contaminadas con materiales específicos en cantidades específicas) que forman dos uniones bipolares: el emisor que emite portadores, el colector que los recibe o recolecta y la tercera, que está intercalada entre las dos primeras, modula el paso de dichos portadores (base). A diferencia de las válvulas, el transistor es un dispositivo controlado por corriente y del que se obtiene corriente amplificada. En el diseño de circuitos a los transistores se les considera un elemento activo,a diferencia de los resistores, condensadores e inductores que son elementos pasivos.

De manera simplificada, la corriente que circula por el "colector" es función amplificada de la que se inyecta en el "emisor", pero el transistor solo gradúa la corriente que circula a través de sí mismo, si desde una fuente de corriente continua se alimenta la "base" para que circule la carga por el "colector", según el tipo de circuito que se utilice. El factor de amplificación o ganancia logrado entre corriente de colector y corriente de base, se denomina Beta del transistor. Otros parámetros a tener en cuenta y que son particulares de cada tipo de transistor son: Tensiones de ruptura de Colector Emisor, de Base Emisor, de Colector Base, Potencia Máxima, disipación de calor, frecuencia de trabajo, y varias tablas donde se grafican los distintos parámetros tales como corriente de base, tensión Colector Emisor, tensión Base Emisor, corriente de Emisor, etc. Los tres tipos de esquemas(configuraciones) básicos para utilización analógica de los transistores son emisor común, colector común y base común.

Modelos posteriores al transistor descrito, el transistor bipolar (transistores FET, MOSFET, JFET, CMOS, VMOS, etc.) no utiliza la corriente que se inyecta en el terminal de "base" para modular la corriente de emisor o colector, sino la tensión presente en el terminal de puerta y gradúa la conductancia del canal entre los terminales de Fuente y Drenaje. Cuando la conductancia es nula y el canal se encuentra estrangulado, por efecto de la tensión aplicada entre Compuerta y Fuente, es el campo eléctrico presente en el canal el responsable de impulsar los electrones desde la fuente al drenaje. De este modo, la corriente de salida en la carga conectada al Drenaje (D) será función amplificada de la tensión presente entre la compuerta y la fuente, de manera análoga al funcionamiento del triodo.

Los transistores de efecto de campo son los que han permitido la integración a gran escala disponible hoy en día; para tener una idea aproximada pueden fabricarse varios cientos de miles de transistores interconectados, por centímetro cuadrado y en varias capas superpuestas.

Llamado también "transistor de punta de contacto", fue el primer transistor capaz de obtener ganancia, inventado en 1947 por John Bardeen y Walter Brattain. Consta de una base de germanio, semiconductor para entonces mejor conocido que la combinación cobre-óxido de cobre, sobre la que se apoyan, muy juntas, dos puntas metálicas que constituyen el emisor y el colector. La corriente de base es capaz de modular la resistencia que se «ve» en el colector, de ahí el nombre de "transfer resistor". Se basa en efectos de superficie, poco conocidos en su día. Es difícil de fabricar (las puntas se ajustaban a mano), frágil (un golpe podía desplazar las puntas) y ruidoso. Sin embargo convivió con el transistor de unión debido a su mayor ancho de banda. En la actualidad ha desaparecido.

El transistor de unión bipolar (o BJT, por sus siglas del inglés "bipolar junction transistor") se fabrica sobre un monocristal de material semiconductor como el germanio, el silicio o el arseniuro de galio, cuyas cualidades son intermedias entre las de un conductor eléctrico y las de un aislante. Sobre el sustrato de cristal se contaminan en forma muy controlada tres zonas sucesivas, N-P-N o P-N-P, dando lugar a dos uniones PN.

Las zonas N (en las que abundan portadores de carga Negativa) se obtienen contaminando el sustrato con átomos de elementos "donantes" de electrones, como el arsénico o el fósforo; mientras que las zonas P (donde se generan portadores de carga Positiva o «huecos») se logran contaminando con átomos "aceptadores" de electrones, como el indio, el aluminio o el galio.

La tres zonas contaminadas, dan como resultado transistores PNP o NPN, donde la letra intermedia siempre corresponde a la región de la base, y las otras dos al emisor y al colector que, si bien son del mismo tipo y de signo contrario a la base, tienen diferente contaminación entre ellas (por lo general, el emisor está mucho más contaminado que el colector).

El mecanismo que representa el comportamiento semiconductor dependerá de dichas contaminaciones, de la geometría asociada y del tipo de tecnología de contaminación (difusión gaseosa, epitaxial, etc.) y del comportamiento cuántico de la unión.

El transistor de efecto de campo de unión (JFET), fue el primer transistor de efecto de campo en la práctica. Lo forma una barra de material semiconductor de silicio de tipo N o P. En los terminales de la barra se establece un contacto óhmico, tenemos así un transistor de efecto de campo tipo N de la forma más básica. Si se difunden dos regiones P en una barra de material N y se conectan externamente entre sí, se producirá una puerta. A uno de estos contactos le llamaremos surtidor y al otro drenador. Aplicando tensión positiva entre el drenador y el surtidor y conectando la puerta al surtidor, estableceremos una corriente, a la que llamaremos corriente de drenador con polarización cero. Con un potencial negativo de puerta al que llamamos tensión de estrangulamiento, cesa la conducción en el canal.

El transistor de efecto de campo, o FET por sus siglas en inglés, que controla la corriente en función de una tensión; tienen alta impedancia de entrada.

Los fototransistores son sensibles a la radiación electromagnética en frecuencias cercanas a la de la luz visible; debido a esto su flujo de corriente puede ser regulado por medio de la luz incidente. Un fototransistor es, en esencia, lo mismo que un transistor normal, solo que puede trabajar de 2 maneras diferentes:

Con el desarrollo tecnológico y evolución de la electrónica, la capacidad de los dispositivos semiconductores para soportar cada vez mayores niveles de tensión y corriente ha permitido su uso en aplicaciones de potencia. Es así como actualmente los transistores son empleados en conversores estáticos de potencia, controles para motores y llaves de alta potencia (principalmente inversores), aunque su principal uso está basado en la amplificación de corriente dentro de un circuito cerrado.

Los primeros transistores bipolares de unión se fabricaron con germanio (Ge). Los transistores de Silicio (Si) actualmente predominan, pero ciertas versiones avanzadas de microondas y de alto rendimiento ahora emplean el compuesto semiconductor de arseniuro de galio (GaAs) y la aleación semiconductora de silicio-germanio (SiGe). El material semiconductor a base de un elemento (Ge y Si) se describe como elemental.

Los parámetros en bruto de los materiales semiconductores más comunes utilizados para fabricar transistores se dan en la tabla adjunta; estos parámetros variarán con el aumento de la temperatura, el campo eléctrico, nivel de impurezas, la tensión, y otros factores diversos.

La tensión directa de unión es la tensión aplicada a la unión emisor-base de un transistor bipolar de unión con el fin de hacer que la base conduzca a una corriente específica. La corriente aumenta de manera exponencial a medida que aumenta la tensión en directa de la unión. Los valores indicados en la tabla son las típicos para una corriente de 1mA (los mismos valores se aplican a los diodos semiconductores). Cuanto más bajo es la tensión de la unión en directa, mejor, ya que esto significa que se requiere menos energía para colocar en conducción al transistor. La tensión de unión en directa para una corriente dada disminuye con el aumento de la temperatura. Para una unión de silicio típica, el cambio es de –2.1mV/°C. En algunos circuitos deben usarse elementos compensadores especiales (sensistores) para compensar tales cambios.

La densidad de los portadores móviles en el canal de un MOSFET es una función del campo eléctrico que forma el canal y de varios otros fenómenos tales como el nivel de impurezas en el canal. Algunas impurezas, llamadas dopantes, se introducen deliberadamente en la fabricación de un MOSFET, para controlar su comportamiento eléctrico.

Las columnas de movilidad de electrones y movilidad de huecos de la tabla muestran la velocidad media con que los electrones y los huecos se difunden a través del material semiconductor con un campo eléctrico de 1 voltio por metro, aplicado a través del material. En general, mientras más alta sea la movilidad electrónica, el transistor puede funcionar más rápido. La tabla indica que el germanio es un material mejor que el silicio a este respecto. Sin embargo, el germanio tiene cuatro grandes deficiencias en comparación con el silicio y arseniuro de galio:
Debido a que la movilidad de los electrones es más alta que la movilidad de los huecos para todos los materiales semiconductores, un transistor bipolar n-p-n dado tiende a ser más rápido que un transistor equivalente p-n-p. El arseniuro de galio tiene el valor más alto de movilidad de electrones de los tres semiconductores. Es por esta razón que se utiliza en aplicaciones de alta frecuencia. Un transistor FET de desarrollo relativamente reciente, el transistor de alta movilidad de electrones (HEMT), tiene una heteroestructura (unión entre diferentes materiales semiconductores) de arseniuro de galio-aluminio (AlGaAs)-arseniuro de galio (GaAs), que tiene el doble de la movilidad de los electrones que una unión de barrera GaAs-metal. Debido a su alta velocidad y bajo nivel de ruido, los HEMT se utilizan en los receptores de satélite que trabajan a frecuencias en torno a los 12GHz. Los HEMT basados en nitruro de galio y nitruro de galio aluminio (AlGaN/GaN HEMT) proporcionan una movilidad de los electrones aún mayor y se están desarrollando para diversas aplicaciones.

Los valores de la columna de Máximo valor de temperatura de la unión han sido tomados a partir de las hojas de datos de varios fabricantes. Esta temperatura no debe ser excedida o el transistor puede dañarse.

Los datos de la fila Al-Si de la tabla se refieren a los diodos de barrera de metal-semiconductor de alta velocidad (de aluminio-silicio), conocidos comúnmente como diodos Schottky. Esto está incluido en la tabla, ya que algunos transistor IGFET de potencia de silicio tienen un diodo Schottky inverso "parásito" formado entre la fuente y el drenaje como parte del proceso de fabricación. Este diodo puede ser una molestia, pero a veces se utiliza en el circuito del cual forma parte.

El comportamiento del transistor se puede ver como dos diodos (Modelo de Ebers-Moll), uno entre base y emisor, polarizado en directo y otro diodo entre base y colector, polarizado en inverso. Esto quiere decir que entre base y emisor tendremos una tensión igual a la tensión directa de un diodo, es decir 0,6 a 0,8V para un transistor de silicio y unos 0,4 para el germanio.

Lo interesante del dispositivo es que en el colector tendremos una corriente proporcional a la corriente de base: I = β I, es decir, ganancia de corriente cuando β>1. Para transistores normales de señal, β varía entre 100 y 300. Existen tres configuraciones para el amplificador transistorizado: emisor común, base común y colector común.

La señal se aplica a la base del transistor y se extrae por el colector. El emisor se conecta al punto de tierra (masa) que será común, tanto de la señal de entrada como para la de salida. En esta configuración, existe ganancia tanto de tensión como de corriente. Para lograr la estabilización de la etapa ante las variaciones de la señal, se dispone de una resistencia de emisor, (R y para frecuencias bajas, la impedancia de salida se aproxima a R. La ganancia de tensión se expresa:

formula_1 

El signo negativo, indica que la señal de salida está invertida con respecto a la señal de entrada.

Si el emisor está conectado directamente a masa, la ganancia queda expresada de la siguiente forma:

formula_2 

Como la base está conectada al emisor por un diodo polarizado en directo, entre ellos se puede suponer que existe una tensión constante, denominada formula_3 y que el valor de la ganancia (β) es constante. Del gráfico adjunto, se deduce que la tensión de emisor es: 

formula_4

Y la corriente de emisor: 

formula_5.

La corriente de emisor es igual a la de colector más la de base: 

formula_6

Despejando la corriente de colector: 

formula_7

La tensión de salida, que es la de colector se calcula así: 

formula_8

Como β » 1, se puede aproximar:

formula_9 

y, entonces es posible calcular la tensión de colector como: 

formula_10

La parte entre paréntesis es constante (no depende de la señal de entrada), y la restante expresa la señal de salida. El signo negativo indica que la señal de salida está desfasada 180º respecto a la de entrada.

Finalmente, la ganancia es expresada como: 

formula_11

La corriente de entrada, formula_12, si formula_13 puede expresarse como sigue:

formula_14

Suponiendo que formula_15, podemos escribir:

formula_16

Al dividir la tensión y corriente en la base, la impedancia o resistencia de entrada queda como: 

formula_17

Para tener en cuenta la influencia de frecuencia se deben utilizar modelos de transistor más elaborados. Es muy frecuente usar el modelo en pi.

Recta de carga

Esta recta se traza sobre las curvas características de un transistor que proporciona el fabricante.
Los puntos para el trazado de la misma son: formula_18 
y la tensión de la fuente de alimentación formula_19 

En los extremos de la misma, se observan las zonas de corte y de saturación, que tienen utilidad cuando el transistor actúa como interruptor. Conmutará entre ambos estados de acuerdo a la polarización de la base.

La elección del punto Q, es fundamental para una correcta polarización. Un criterio extendido es el de adoptar formula_20, si el circuito no posee formula_21. De contar con formula_21 como es el caso del circuito a considerar, el valor de formula_23 se medirá desde el colector a masa.

El punto Q, se mantiene estático mientras la base del transistor no reciba una señal.

Ejercicio

Procederemos a determinar los valores de formula_24

Datos: formula_25

formula_26

formula_27

Esta aproximación se admite porque formula_28

formula_29

formula_30

Para que el circuito opere en una zona de eficacia, la corriente a través del divisor de voltaje 
formula_31 y formula_32, debe ser mucho mayor que la corriente de base; como mínimo en una relación 10:1

formula_33 

formula_34 utilizando el valor de formula_35 obtenido anteriormente
formula_36 formula_37
formula_38 

La resistencia dinámica del diodo en la juntura del emisor formula_39, se calcula tomando el valor del voltaje térmico en la misma, y está dado por: formula_40

Con este valor, se procede a calcular la ganancia de voltaje de la etapa; formula_41
No se toma en cuenta formula_42 ya que el emisor se encuentra a nivel de masa para la señal por medio de formula_43, que en el esquema se muestra como formula_44; entonces, la impedancia de salida formula_45, toma el valor de formula_46 si el transistor no tiene carga. Si se considera la carga formula_47, formula_45 se determina por formula_49 considerando que formula_47 tiene el valor formula_51, formula_52 

Al considerar laformula_47, la ganancia de tensión se ve modificada: formula_54

La impedancia de entrada en la base del transistor para el ejemplo, está dada por formula_55

Mientras que la impedancia de entrada a la etapa, se determina: formula_56

La reactancia de los capacitores no se ha tenido en cuenta en los cálculos, porque se han elegido de una capacidad tal, que su reactancia formula_57 en las frecuencias de señales empleadas.

La señal se aplica al emisor del transistor y se extrae por el colector. La base se conecta a las masas tanto de la señal de entrada como a la de salida. En esta configuración se tiene ganancia solo de tensión. La impedancia de entrada es baja y la ganancia de corriente algo menor que uno, debido a que parte de la corriente de emisor sale por la base. Si añadimos una resistencia de emisor, que puede ser la propia impedancia de salida de la fuente de señal, un análisis similar al realizado en el caso de emisor común, da como resultado que la ganancia aproximada es:

La base común se suele utilizar para adaptar fuentes de señal de baja impedancia de salida como, por ejemplo, micrófonos dinámicos.

La señal se aplica a la base del transistor y se extrae por el emisor. El colector se conecta a las masas tanto de la señal de entrada como a la de salida. En esta configuración se tiene ganancia de corriente, pero no de tensión que es ligeramente inferior a la unidad. La impedancia de entrada es alta, aproximadamente β+1 veces la impedancia de carga. Además, la impedancia de salida es baja, aproximadamente β veces menor que la de la fuente de señal.

Antes de la aparición del transistor, eran usadas las válvulas termoiónicas. Las válvulas tienen características eléctricas similares a la de los transistores de efecto campo (FET): la corriente que los atraviesa depende de la tensión en el terminal llamado rejilla. Las razones por las que el transistor reemplazó a la válvula termoiónica son varias:
Como ejemplo de todos estos inconvenientes se puede citar a la primera computadora digital, llamada ENIAC, la cual pesaba más de treinta toneladas y consumía 200 kilovatios, suficientes para alimentar una pequeña ciudad, a causa de sus aproximadamente 18 000 válvulas, de las cuales algunas se quemaban cada día, necesitando una logística y una organización importantes para mantener este equipo en funcionamiento.
El transistor bipolar reemplazó progresivamente a la válvula termoiónica durante la década de 1950, pero no del todo. En efecto, durante los años 1960, algunos fabricantes siguieron utilizando válvulas termoiónicas en equipos de radio de gama alta, como Collins y Drake; luego el transistor desplazó a la válvula de los transmisores pero no del todo en los amplificadores de radiofrecuencia. Otros fabricantes de instrumentos eléctricos musicales como Fender, siguieron utilizando válvulas en sus amplificadores de audio para guitarras eléctricas. Las razones de la supervivencia de las válvulas termoiónicas son varias:





</doc>
<doc id="5367" url="https://es.wikipedia.org/wiki?curid=5367" title="1956">
1956

1956 () fue un según el calendario gregoriano.

















































</doc>
<doc id="5371" url="https://es.wikipedia.org/wiki?curid=5371" title="Prescripción">
Prescripción

El término prescripción puede referirse:



</doc>
<doc id="5372" url="https://es.wikipedia.org/wiki?curid=5372" title="Benito Pérez Galdós">
Benito Pérez Galdós

Benito Pérez Galdós (Las Palmas de Gran Canaria, -Madrid, ) fue un novelista, dramaturgo, cronista y político español.

Se le considera uno de los mejores representantes de la novela realista del siglo no solo en España y un narrador capital en la historia de la literatura en lengua española, hasta el punto de ser propuesto por varios especialistas y estudiosos de su obra como el mayor novelista español después de Cervantes.

Transformó el panorama novelesco español de la época, apartándose de la corriente romántica en pos del naturalismo y aportando a la narrativa una gran expresividad y hondura psicológica. En palabras de Max Aub, Pérez Galdós, como Lope de Vega, asumió el espectáculo del pueblo llano y con «su intuición serena, profunda y total de la realidad», se lo devolvió, como Cervantes, rehecho, «artísticamente transformado». De ahí que «desde Lope ningún escritor fue tan popular, ninguno tan universal desde Cervantes». Fue académico de la Real Academia Española desde 1897.

Llegó a ser propuesto al Premio Nobel de Literatura en 1912 pero su radical anticlericalismo provocó que fuera asediado y boicoteado con éxito por los sectores más conservadores de la sociedad española representados en el catolicismo tradicionalista, que no reconocían su valor intelectual y literario.

Tuvo gran afición a la política, aunque el mismo no se consideraba un político. Sus comienzos políticos fueron liberales para abrazar después un republicanismo moderado y posteriormente el socialismo de la mano de Pablo Iglesias Posse. En sus inicios liberales se afilió al Partido Progresista de Sagasta y en 1886 logró ser diputado por Guayama (Puerto Rico) en las Cortes. A comienzos del siglo XX ingresó en el Partido Republicano y en las legislaturas de 1907 y 1910 fue diputado a Cortes por Madrid por la Conjunción Republicano Socialista; en 1914 es elegido diputado por Las Palmas.

Galdós —bautizado como Benito María de los Dolores— fue el décimo hijo de un coronel del ejército, Sebastián Pérez Macías, natural del municipio de Valsequillo de Gran Canaria, que había formado parte del batallón de voluntarios conocido como "La Granadera Canaria" que luchó en la Guerra de la Independencia y de Dolores Galdós Medina, natural de Las Palmas de Gran Canaria aunque de origen guipuzcoano, mujer de ‘fuerte carácter’ –según la describía su hijo, el propio escritor–, e hija de Domingo Galdós Alcorta, un funcionario de la Audiencia de Canarias, natural de Azcoitia. Era hermano del militar Ignacio Pérez Galdós, Capitán General de Canarias entre 1900 y 1905.

Siendo aún niño su padre le aficionó a los relatos históricos contándole pasajes y anécdotas vividos en la guerra de la Independencia, en la que, como militar, había participado. En 1852 ingresó en el Colegio de San Agustín, en el barrio de Vegueta de Las Palmas de Gran Canaria (isla de Gran Canaria), con una pedagogía avanzada para la época, en los años en que empezaban a divulgarse por España las polémicas teorías darwinistas, polémicas que algunos críticos han rastreado en obras como "Doña Perfecta".

Galdós, que ya había empezado a colaborar en la prensa local con poesías satíricas, ensayos y algunos cuentos, obtuvo el título de bachiller en Artes en 1862, en el Instituto de La Laguna (Tenerife), donde había destacado por su facilidad para el dibujo y su buena memoria. La llegada de una prima suya, Sisita, al entorno familiar isleño, trastornó emocionalmente al joven Galdós, circunstancia que se ha considerado posible origen de la decisión final de Mamá Dolores de enviarle a Madrid a estudiar Derecho.

Llegó a Madrid en septiembre de 1862, se matriculó en la universidad y tuvo por profesores a Fernando de Castro, Francisco de Paula Canalejas, Adolfo Camús, Valeriano Fernández y Francisco Chacón Oviedo. En la universidad conoció al fundador de la Institución Libre de Enseñanza, Francisco Giner de los Ríos, que le alentó a escribir y le hizo sentir curiosidad por el krausismo, filosofía que se deja sentir en sus primeras obras. Frecuentó los teatros y la «Tertulia Canaria» en Madrid, formando tertulia con otros escritores paisanos suyos (Nicolás Estévanez, José Plácido Sansón, etcétera). También acudía a leer al Ateneo a los principales narradores europeos en inglés y francés. Fue en esa institución donde conoció a Leopoldo Alas, Clarín, durante una conferencia del crítico y novelista asturiano, en lo que sería el comienzo de una larga amistad. Al parecer fue alumno disperso y perezoso, faltando a clase a menudo:

En 1865 asistió a la terrible Noche de San Daniel, cuyos sucesos le impresionaron vivamente:

Asiduo de los teatros, le impresionó en especial la obra "Venganza catalana", de Antonio García Gutiérrez. Los cronistas y biógrafos recogen que ese mismo año empezó a escribir como redactor meritorio en los periódicos "La Nación" y "El Debate", así como en la "Revista del Movimiento Intelectual de Europa". Al año siguiente y en calidad de periodista, asistió al pronunciamiento de los sargentos del cuartel de San Gil.

En 1867 hizo su primer viaje al extranjero, como corresponsal en París, para dar cuenta de la Exposición Universal. Volvió con las obras de Balzac y de Dickens y tradujo de este, a partir de una versión francesa, su obra más cervantina, "Los papeles póstumos del Club Pickwick", que se publicó por entregas en "La Nación". Toda esta actividad supone su inasistencia a las clases de Derecho y le borran definitivamente de la matrícula en 1868. En ese mismo año se produce la llamada revolución de 1868, en que cae la reina Isabel II, precisamente cuando regresaba de su segundo viaje a París y volvía de Francia a Canarias en barco vía Barcelona; en la escala que el navío hizo en Alicante se bajó del vapor en la capital alicantina y llegó así a tiempo a Madrid para ver la entrada de los generales Francisco Serrano y Prim. El año siguiente se dedicó a hacer crónicas periodísticas sobre la elaboración de la nueva Constitución.

En 1869 vivía en el barrio de Salamanca, en la calle Serrano número 8, con su familia, y leía con pasión a Balzac mientras formaba parte de la redacción de "Las Cortes". Al año siguiente (1870), gracias a la ayuda económica de su cuñada, publicó su primera novela, "La Fontana de Oro", escrita entre 1867 y 1868 y que, aun con los defectos de toda obra primeriza, sirve de umbral al magno trabajo que como cronista de España desarrolló luego en los Episodios nacionales.

"La sombra", publicada en 1871, había ido apareciendo por entregas a partir de noviembre de 1870, en la "Revista de España", dirigida por José Luis Albareda y más tarde por el propio Galdós entre febrero de 1872 y noviembre de 1873; en ese mismo año (1871), también de la mano de Albareda, entrará en la redacción de "El Debate" y durante su veraneo en Santander conoció al novelista José María de Pereda. En 1873 se alía con el ingeniero tinerfeño Miguel Honorio de la Cámara y Cruz (1840-1930), propietario entonces de "La Guirnalda", en la que colabora desde enero con una serie de “Biografías de damas célebres españolas” entre otros artículos.

En 1873 Galdós comenzó a publicar los "Episodios nacionales" (título que le sugirió su amigo José Luis Albareda), una magna crónica del siglo que recogía la memoria histórica de los españoles a través de su vida íntima y cotidiana, y de su contacto con los hechos de la historia nacional que marcaron el destino colectivo del país. Una obra compuesta por 46 episodios, en cinco series de diez novelas cada una (con la salvedad de la última serie, que quedó inconclusa), que arranca con la batalla de Trafalgar y llega hasta la Restauración borbónica en España.

La primera serie (1873-1875) trata de la guerra de la Independencia (1808-1814) y tiene por protagonista a Gabriel Araceli, «que se dio a conocer como pillete de playa y terminó su existencia histórica como caballeroso y valiente oficial del ejército español».

La segunda serie (1875-1879) recoge las luchas entre absolutistas y liberales hasta la muerte de Fernando VII en 1833. Su protagonista es el liberal Salvador Monsalud, que encarna, en gran parte, las ideas de Galdós y en quien «prevalece sobre lo heroico lo político, signo característico de aquellos turbados tiempos».

Después de un paréntesis de veinte años, y tras recuperar los derechos sobre sus obras que detentaba su editor, con quien mantuvo un pleito interminable, Galdós continuó con la tercera serie, dedicada a la primera guerra carlista (1833-1840).

La cuarta serie (1902-1907) se desarrolla entre la Revolución de 1848 y la caída de Isabel II en 1868. La quinta (1907-1912), incompleta, acaba con la restauración de Alfonso XII.

Este conjunto novelístico constituye una de las obras más importantes de la literatura española de todos los tiempos y marcó una cota casi inalcanzable en la evolución de la novela histórica española. El punto de vista adoptado es vario y multiforme (se inicia desde la perspectiva de un joven que mientras lucha por su amada se ve envuelto en los hechos más importantes de su época); la perspectiva del propio autor varía desde el aliento épico de la primera serie hasta el amargo escepticismo final, pasando por la postura radical de tendencia socialista-anarquista de las series tercera y cuarta.

Para conocer bien España, el escritor se dedicó a recorrerla en coches de ferrocarril de tercera clase, conviviendo con el pueblo miserable y hospedándose en posadas y hostales «de mala muerte».

Benito Pérez Galdós solía llevar una vida cómoda, viviendo primero con dos de sus hermanas y luego en casa de su sobrino, José Hurtado de Mendoza.

En la ciudad, se levantaba con el sol y escribía regularmente hasta las diez de la mañana a lápiz, porque la pluma le hacía perder el tiempo. Después salía a pasear por Madrid a espiar conversaciones ajenas (de ahí la enorme frescura y variedad de sus diálogos) y a observar detalles para sus novelas. No bebía, pero fumaba sin cesar cigarros de hoja. A primera tarde leía en español, inglés o francés; prefería los clásicos ingleses, castellanos y griegos, en particular Shakespeare, Dickens, Cervantes, Lope de Vega y Eurípides, a los que se conocía al dedillo. En su madurez empezó a frecuentar a León Tolstói. Después volvía a sus paseos, salvo que hubiera un concierto, pues adoraba la música y durante mucho tiempo hizo crítica musical. Se acostaba temprano y casi nunca iba al teatro. Cada trimestre acuñaba un volumen de trescientas páginas.

Desde la óptica de un Ramón Pérez de Ayala Galdós era descuidado en el vestir, usando tonos sombríos para pasar desapercibido. En invierno era habitual verle llevando enrollada al cuello una bufanda de lana blanca, con un cabo colgando del pecho y otro a la espalda, un puro a medio fumar en la mano y, ya sentado, completaba la estampa tópica su perro alsaciano junto a él. Tenía por costumbre llevar el pelo cortado «al rape» y, al parecer, padecía fuertes migrañas.

Desde su llegada a Madrid, una de las mayores aficiones de Galdós eran las visitas al viejo Ateneo de la calle de la Montera, donde tuvo oportunidad de hacer amistad con intelectuales y políticos de todas las tendencias, incluidos personajes tan ajenos a su ideología y sensibilidad como Marcelino Menéndez Pelayo, Antonio Cánovas del Castillo o Francisco Silvela. También frecuentaba las tertulias del Café de la Iberia, la Cervecería Inglesa y del viejo Café de Levante. A partir de 1872, Galdós se aficionó a pasar los tórridos veranos madrileños en Santander (Cantabria), entorno con el que llegaría a identificarse hasta el punto de comprar una casa en El Sardinero, la animada «finca de San Quintín». Pero el auge del naturalismo en Francia y sus lecturas del mismo empezaron a afectar sus ideas narrativas y en 1881 dio un notable giro a su producción novelística al publicar "La desheredada", como observaría su amigo y crítico literario Leopoldo Alas, Clarín:

Con "La desheredada" abandona el género de la novela de tesis y abre el ciclo de las "Novelas españolas contemporáneas" (1881-1889) que —en su mayoría— describen la sociedad madrileña en la segunda mitad del siglo XIX. A partir de entonces comparecen ampliamente bajo perspectivas naturalistas los elementos novelescos más caros a Galdós: la locura generosa y abnegada, la debilidad sentimental femenina, el egoísmo masculino, la exploración de la inquietud romántica y, a su lado, el análisis de la dureza pragmática. Los personajes ya no serán de una pieza y sus sueños o las contradicciones de su pensamiento ocuparán largo trecho, como sucede en "El amigo Manso" (1882), intensa novelización de una renuncia amorosa narrada por un personaje cuya crisis de existencia parece anticipar a los muy posteriores de Miguel de Unamuno. Asimismo, como en "La comedia humana" de Balzac, los personajes de unas novelas empiezan a aparecer en otras.

La carrera parlamentaria de Galdós comienza, de un modo un tanto rocambolesco, cuando en 1886 y habiéndose aproximado el escritor al Partido Liberal, su amistad con Sagasta le llevó a ingresar en el Congreso como diputado por Guayama (Puerto Rico). El escritor nunca llegaría a visitar su circunscripción antillana, pero su obligada asistencia a las Cortes —donde, tímido por naturaleza, apenas despegaría los labios— le sirvió de nuevo e insólito observatorio desde el que analizar lo que luego titularía como «la sociedad española como materia novelable».
Más tarde en las elecciones generales de España de 1910 se presentaría como líder de Conjunción Republicano-Socialista, formada por partidos republicanos y el PSOE, en que dicha coalición obtendría un 10,3% de votos.

En su producción novelística, todavía dentro del ciclo de las "Novelas españolas contemporáneas", inicia una segunda fase en que tras publicar "Realidad" en 1889, la lectura de León Tolstoy le hace abandonar el influjo del naturalismo e inclinarse por el espiritualismo, publicando entre 1891 y 1897 diez novelas en esta nueva estética: "Ángel Guerra" (1891), "Tristana" (1892), "La loca de la casa" (1892), "Torquemada en la cruz" (1893), "Torquemada en el Purgatorio" (1894), "Torquemada y San Pedro" (1895), "Nazarín" (1895), "Halma" (1895), "Misericordia" (1897) y "El abuelo" (1897).

La vocación teatral de Galdós fue muy temprana y como él mismo escribió en sus "Memorias de un desmemoriado" ya de estudiante hizo sus pinitos como dramaturgo: «Si mis días se me iban en “flanear” por las calles, invertía parte de las noches en emborronar dramas y comedias». Empezó con "Quién mal hace, bien no espere" (1861) y el drama histórico "La expulsión de los moriscos" (1865), que no se han conservado, y siguió con la alta comedia "Un joven de provecho" (1867), de edición póstuma; pero abandonó esa vocación muy pronto para entregarse por completo a la novela, hasta que el 15 de marzo de 1892 se estrenó en el Teatro de la Comedia de Madrid la primera obra madura de la producción teatral de Galdós: "Realidad". El autor recordaría luego esa noche en sus "Memorias" como «solemne, inolvidable para mí». El éxito de la obra, y la buena disposición de la Guerrero, les llevaría a estrenar en los primeros días de 1893 la versión teatral de "La loca de la casa" (que como novela había pasado casi inadvertida). Pero su confirmación como autor de éxito y crítica se la dio "La de San Quintín", estrenada el 27 de enero de 1894; su cuarta obra llevada a las tablas, tras el fracaso de la adaptación del episodio "Gerona".

Pero el estreno más recordado de Galdós (junto con el posterior de "Casandra" en 1910) fue quizá el de su "Electra", el 30 de enero de 1901, por lo que supuso de oportuno «alegato contra los poderes de la Iglesia y contra las órdenes religiosas que la servían» en un momento histórico en el que en España, tras los avances liberales del periodo 1868-1873, crecía de nuevo la influencia de los intereses políticos del Vaticano. Aquella bofetada, que para asombro del propio Galdós fue mucho más sonora de lo que él había esperado, encendería la mecha de una conspiración ultramontana, que al cabo de los años se llevaría una desproporcionada, triste y muy poco cristiana revancha: impedir que el genio literario de Galdós fuera reconocido con el Premio Nobel de Literatura.

En general, el teatro de Galdós no tuvo sino un éxito discreto; abominaba con todas sus fuerzas de la rutina de empresarios, actores y espectadores que no aceptaban sus obras demasiado extensas y de numerosos personajes, sus tendencias al simbolismo, sus exigencias de decorados y elementos ambientales (como demuestra el airado prólogo que antepuso a la edición de "Los condenados", 1894), aunque tuvo poderosos defensores que se esforzaron en llevar sus ideas dramáticas a las tablas, como Emilio Mario.

Su primer intento resultó muy revelador sobre lo que buscaba en escena: convirtió una novela epistolar sobre el tema del adulterio, "La incógnita" (1888-1889) en novela dialogada y luego en drama, en los dos casos bajo el título de "Realidad" (1889 y 1892, respectivamente), queriendo que la voz y el diálogo expresaran directamente la confusión y el dolor de un "ménage à trois" donde todos sufren y conservan, de un modo u otro, su dignidad. Algunas de sus piezas se resienten de su origen narrativo, aunque muchas de ellas provienen de novelas dialogadas. Sus dramas contienen reflexiones regeneracionistas sobre el valor redentor del trabajo y del dinero, sobre la necesidad de una aristocracia espiritual, sobre la grandeza del arrepentimiento y sobre la función estimulante y mediadora de la mujer en la vida social: "La loca de la casa" (1893), "La de San Quintín" (1894), "Mariucha" (1903), "El abuelo" (1904), "Amor y ciencia" (1905), "Alceste" (1914). Sus dos grandes éxitos fueron el escándalo anticlerical de "Electra" (1901) y el político de "Casandra" (1910).

Por fin, el 7 de febrero de 1897, y pese a las oposiciones de los sectores conservadores del país —y en especial de los "neos" (neocatólicos)—, Galdós fue elegido miembro de la Real Academia Española.

Un laudo arbitral de 1897 independizó a Galdós de su primer editor, Miguel Honorio de la Cámara, y se dividió todo en dos partes, de lo que resultó que Galdós, en veinte años de gestión conjunta, había recibido unas 80000 pesetas más de lo que le correspondía. Después se averiguó que De la Cámara no había sido del todo legal respecto al número y fecha de las ediciones de sus obras; lo cierto es que a Galdós le dejó un déficit de 100000 pesetas. Sin embargo, quedó en su propiedad el cincuenta por ciento del fondo de sus libros que quedaba en espera de venta, 60000 ejemplares en total. Para librarse de ellos abrió el escritor una casa editorial con el nombre de Obras de Pérez Galdós en la calle de Hortaleza (número 132 bajo). Los dos primeros títulos que puso en el mercado fueron "Doña Perfecta" y "El abuelo". Continuó esta actividad editorial hasta 1904, año en que, cansado, firmó un contrato con la Editorial Hernando.

La vida sentimental de Galdós, que el escritor conservó celosamente en secreto, tardó en ser estudiada con cierto método. Hubo que esperar a que en 1948, el hispanista lituano establecido en Estados Unidos, Chonon Berkowitz, publicase su estudio biográfico titulado "Pérez Galdós. Spanish Liberal Crusader (1843-1920)".

Todos los críticos coinciden en la esterilidad biográfica de sus "Memorias de un desmemoriado" (Galdós poseía una memoria portentosa), escrita en forma de diario de viajes, y no se sabe si para desalentar empeños biográficos ulteriores.

Galdós permaneció soltero hasta su muerte. Algunos amigos y contemporáneos dejaron noticia de su debilidad por las relaciones con profesionales, aunque no se ha podido demostrar cuánto haya de mito y exageración en ello. Se le conoce una hija natural, María Galdós Cobián, nacida en 1891 de Lorenza Cobián. La lista de pasiones amorosas más o menos carnales se puede complementar con los nombres de la actriz meritoria Concha (Ruth) Morell y con la novelista Emilia Pardo Bazán. Una dilatada colección de estudios intentando desentrañar las relaciones claras de los rumores, permiten añadir a estas tres mujeres mencionadas una variopinta lista en la que figuran los nombres de la actriz Carmen Cobeña; la poetisa y narradora Sofía Casanova que estrenó en el Teatro Español su comedia "La Madeja" (con dirección artística del propio Galdós); la actriz Anna Judic; la cantante Marcella Sembrich; la artista Elisa Cobun; la actriz Concha Catalá, que trabajó en la compañía de Rosario Pino; y la viuda Teodosia Gandarias Landete, su último y algo más que platónico amor.

Al hilo de estos temas, la escritora y pintora Margarita Nelken, en su artículo titulado «El aniversario de Galdós/intimidades y recuerdos», y publicado en el diario "El Sol" del 4 de enero de 1923, comentaba la afición de Galdós por rodearse de «mujeres jóvenes que pusieran risas y se ponía más achacoso para que le mimásemos más».

En el último periodo de su vida, Galdós repartió su tiempo entre los compromisos políticos y la actividad como dramaturgo. Sus últimos años estuvieron marcados de modo progresivo por la pérdida de la visión y las consecuencias de sus descuidos económicos y tendencia a endeudarse de forma continua, aspectos íntimos que el entonces joven periodista Ramón Pérez de Ayala, aprovechándose de su interesada amistad con el viejo escritor, recogió más tarde en sus "Divagaciones literarias":

Como parte de las fuerzas políticas republicanas, Madrid eligió a Galdós representante en las Cortes de 1907. En 1909 presidió, junto a Pablo Iglesias, la coalición republicano-socialista, si bien Galdós, que «no se sentía político», se apartó pronto de las luchas «por el acta y la farsa» dirigiendo sus ya menguadas energías a la novela y al teatro.

Paralelamente, el habilidoso instinto político del conde de Romanones, urdía encuentros del joven rey Alfonso XIII con el popular escritor que le situaban en un contexto ambiguo. Con todo, en 1914 Galdós, enfermo y ciego, presentó y ganó su candidatura como diputado republicano por Las Palmas de Gran Canaria. Coincidía ello con la promoción, en marzo de 1914, de una Junta Nacional de Homenaje a Galdós, formada por personalidades de la talla y catadura de Eduardo Dato (jefe del Gobierno), el capitán general Miguel Primo de Rivera, el banquero Gustavo Baüer (representante de Rothschild en España), Melquiades Álvarez, jefe de los reformistas, o el duque de Alba, además de escritores consagrados como Jacinto Benavente, Mariano de Cavia y José de Echegaray. No figuraban en dicha junta políticos como Antonio Maura o Lerroux, y por razones antagónicas: la Iglesia y los socialistas.

En el aspecto literario, puede anotarse que su admiración por la obra de León Tolstói se trasluce en cierto espiritualismo en sus últimos escritos y, en esa misma línea rusa, no pudo disimular cierto pesimismo por el destino de España, como se percibe en las páginas de uno de sus últimos Episodios nacionales, "Cánovas" (1912), al que pertenece este párrafo:

El 20 de enero de 1919 se descubrió en el parque del Retiro de Madrid una escultura erigida por suscripción pública. Por razón de su ceguera, Galdós pidió ser alzado para palpar la obra y lloró emocionado al comprobar la fidelidad de la obra que un joven y casi novel Victorio Macho había esculpido sin cobrar su trabajo. Un año más tarde, Benito Pérez Galdós, cronista de España por designación del pueblo soberano, murió en su casa de la calle Hilarión Eslava de Madrid, en la madrugada del 4 de enero de 1920. El día de su entierro, unos 30000 ciudadanos acompañaron su ataúd hasta el cementerio de la Almudena (zona antigua, cuartel 2B, manzana 3, letra A).

Es habitual leer, en la abundante bibliografía y otros documentos que sobre la figura de Galdós se han producido, que el escritor murió pobre y olvidado. Es asunto debatido, pero sea como fuere José Ortega y Gasset denunció públicamente el olvido oficial, institucional y político, del autor, en una encendida necrológica publicada en el diario "El Sol" el 5 de enero de 1920, y que comenzaba así: «La España oficial, fría, seca y protocolaria, ha estado ausente en la unánime demostración de pena provocada por la muerte de Galdós. La visita del ministro de Instrucción Pública no basta... Son otros los que han faltado... El pueblo, con su fina y certera perspicacia, ha advertido esa ausencia... Sabe que se le ha muerto el más alto y peregrino de sus príncipes». Frente a esa falta de pasión, Ortega pronostica que la prensa de los días sucesivos se hará eco de la emoción y del dolor general. Por su parte, Unamuno en idéntica fecha escribía que, leyendo su obra, «nos daremos cuenta del bochorno que pesa sobre la España en que él ha muerto».

Según la prensa del momento, uno de los primeros en presentarse en la casa mortuoria fue, efectivamente, Natalio Rivas, ministro de Instrucción Pública, además de políticos como Alejandro Lerroux (siempre atento a la simbología de lo público) o la condesa y amiga íntima del finado, Emilia Pardo Bazán. Poco después llegó el torero Machaquito y una interminable procesión de amigos, conocidos y personalidades varias. El desfile aumentaría en forma progresiva cuando desde las once de la noche del mismo día de su muerte quedó instalada la capilla ardiente en el Patio de Cristales del Ayuntamiento de Madrid. Allí acudieron el jefe del Gobierno y cinco de sus miembros junto con «cientos de miles de ciudadanos». También ese mismo día 4, el ministro Rivas puso a la firma del rey un Decreto «estableciendo honores y distinciones», entre las que se incluían que el entierro fuese costeado por el Estado y la asistencia de las Reales Academias, Universidades, Ateneo y Centros de Enseñanza y Cultura, además de otros funcionarios ministeriales. El Senado, por su parte, celebró una sesión para acordar el pésame de la institución y su asistencia oficial al sepelio. Se publicó una esquela mortuoria dándoles el pésame a los familiares (la hija de Galdós y su marido, su hermana Manuela, ausente en Las Palmas de Gran Canaria, el albacea Alcaín...).

En señal de duelo, esa noche del 4 de enero se cerraron todos los teatros de Madrid con el cartel de "No hay función". En la prensa madrileña y nacional, algunos diarios como el conservador "La Época" publicaron números extraordinarios glosando la imagen del escritor canario fallecido.

El lunes 5 de enero de 1920, rodeando el féretro la Guardia Municipal, de gala, y cubierto por coronas de flores, partió el entierro de Benito Pérez Galdós. Los periódicos hablaron luego de que 30 000 personas habían pasado por la capilla ardiente y de que unas 20 000 formaron cortejo extraoficial hasta el cementerio. Aunque en esa época no era costumbre que las mujeres acudieran a los entierros, en aquella ocasión abrió la excepción la actriz Catalina Bárcena, y en cuanto el duelo oficial se retiró, a la altura de la Puerta de Alcalá, progresivamente fueron acudiendo las otras mujeres de Madrid: las menestralas, las obreras, las madres de familia de las clases populares. El abuelo que contaba historias que ellas podían entender y sentir, el hermano escritor que las había inmortalizado con muy diversos nombres y sentimientos, emprendía aquella fría tarde su último viaje.

De entre las numerosas ediciones puede destacarse la preparada por la Cátedra Pérez Galdós, espacio científico creado por la ULPGC y el Cabildo Insular de Gran Canaria que desde 2005 ha publicado el texto crítico de las "Obras completas" en varias series: una de 24 volúmenes entre 2005 y 2011 con las novelas, y cuatro años después, otra con la producción dramática (cuatro tomos entre 2009 y 2012). En 2013 recogieron en un solo volumen los cuentos.

Galdós, poseedor de una memoria privilegiada y una formación autodidacta sustentada por su curiosidad incansable, su capacidad de observación y su pasión por la lectura, acuñó un estilo narrativo personal con las siguientes características:

Numerosos estudios críticos han destacado la habilidad de Galdós en su construcción de personajes femeninos; en este sentido y además de los títulos citados, cabría añadir las mujeres protagonistas de "Gloria" (Gloria Lantigua); "La de Bringas" (Rosalía Pipaón); "Tormento" (Amparo); "La desheredada" (Isidora Rufete); "La familia de León Roch" (María Egipcíaca); "Marianela"; o la Benina de "Misericordia".

De la vasta obra literaria e histórica acometida por Benito Pérez Galdós, la crítica del mundo occidental ha coincidido en destacar novelas de resonancia mundial como las siguientes:




Primera serie

Segunda serie

Tercera serie

Cuarta serie

Quinta serie

Episodios Nacionales para niños

En sus inicios Galdós comenzó a escribir cuentos. A lo largo de toda su carrera literaria publicó múltiples relatos cortos en diversos periódicos y revistas literarias de la época. Algunos de los cuentos más destacados son los siguientes:

Galdós fue casi tan fecundo periodista como narrador y desde mucho antes, ya en su etapa canaria. Fundó en 1862 el periódico "La Antorcha"; colaboró en "El Ómnibus" (1862), "La Nación" (1865-1868), "Revista del Movimiento Intelectual de Europa" (1865-1867), "Las Cortes" (1869), "La Ilustración de Madrid" (1871), "El Debate" (1871), "Revista de España" (1870-1873 y 1876) y "La Guirnalda" (1873-1876) y "La Prensa" de Buenos Aires (1885) y, con artículos sueltos, en "Vida Nueva" (1898), "Electra" (1901), "Heraldo de Madrid" (1901), "Alma Española" (1903), "La República de las Letras" (1905), "España Nueva" (1909), "Revista Mensual Tyflofila" (1916), "Ideas y Figuras" (1918) y "La Humanidad" (1919). Según Carmen Bravo Villasante, están menos investigadas sus colaboraciones en "El Día", "La Esfera", "La Diana", "El Imparcial", "El Motín", "El País", "El Progreso Agrícola Pecuario", "El Sol", "La Tertulia" de Santander y "El Tribuno" de Las Palmas de Gran Canaria

La aportación más importante al conocimiento de la obra inédita de Galdós la hizo el argentino Alberto Ghiraldo, con la publicación en 1923 de los nueve volúmenes de las "Obras inéditas", en la editorial Renacimiento de Madrid. A partir de este texto (volúmenes VI y VII), Rafael Reig prologó la edición en 2003 de "El crimen de la calle Fuencarral"." El crimen del cura Galeote", un turbio asunto muy popular en el verano de 1888, que inició una oleada de amarillismo en la prensa que alcanzaría su auge hacia 1898, coincidiendo con la guerra de Cuba. En opinión de Reig, estos relatos, extraídos de crónicas enviadas al diario argentino "La Prensa", son comparables al estilo de Dashiell Hammett y dan noticia de un Galdós pionero en el género policíaco apenas frecuentado hasta entonces en la literatura española.

En 1979, el hispanista Alan E. Smith localizó entre manuscritos guardados en la Biblioteca Nacional de Madrid un fragmento extenso de novela que, reconstruida en gran parte, se publicó en 1983 con el título de "Rosalía". Por el estilo parece una novela fallida del «ciclo espiritualista» del segundo periodo de la novelística galdosiana.

Galdós es considerado por muchos especialistas como uno de los mejores novelistas en castellano después de Cervantes. Así parece avalarlo su obra, con cerca de 100 novelas, casi 30 obras de teatro, y una colección importante de cuentos, artículos y ensayos. También se le considera maestro indiscutible del Realismo en España y del naturalismo del siglo . Su valía ha sido reconocida por muy diversos creadores, como, entre otros muchos: Luis Buñuel o Max Aub en España, o Carlos Fuentes, Rómulo Gallegos o Sergio Pitol en Hispanoamérica. De entre los reconocimientos de reputados hispanistas, pudiera citarse por ejemplo esta reflexión de Hayward Keniston:

Como le ocurriría —aunque en menor grado— a su contemporáneo y amigo íntimo Leopoldo Alas Clarín, Galdós fue asediado y boicoteado por los sectores más conservadores de la sociedad española, ajenos a su valor intelectual y literario.

Diversos estudiosos de la obra galdosiana y su proyección social, coinciden en que ese sabotaje colectivo de un sector de la población española, aunque con una cabeza bien definida, se debió, como apunta Casalduero, a su honestidad como hombre y como escritor, y a sus ideas anticlericales, que provocaron que el catolicismo tradicionalista, muy poderoso en España y siguiendo algunos aspectos de la política de los Reyes Católicos, le tuviese en el punto de mira hasta su muerte, y aun después de ella; dicho con las palabras de Rosana Torres «el dedo que Galdós puso en la llaga de sus contemporáneos, y lo ha arrastrado hasta la misma herida que más de un siglo después aún no se ha cerrado: la del enfrentamiento entre la ilustración y el oscurantismo, entre la razón y el fanatismo, entre la ciencia y la religión...» 

Cuando en 1912, Galdós fue propuesto para el Nobel de literatura, «el elemento oficial y reaccionario» (incluyendo la propia Real Academia Española y la prensa tradicionalista católica), vio la oportunidad de vengar por fin las ofensas que, desde su sensibilidad y obcecación, suponía —por «su serenidad y sinceridad»— la persona de Galdós y su obra. Las «conjuras», en forma de campaña nacional e internacional, impidieron que le dieran el premio no solo en esa ocasión de 1912, sino también en 1913 y en una tercera convocatoria en 1915 (cuya propuesta en esa ocasión había partido de una mayoría de miembros de la propia Academia sueca, que como comenta Ortiz-Armengol fueron ninguneados sin mayores explicaciones), consiguiendo desvirtuar una suscripción pública en favor de Galdós.
En 1922, siete años más tarde, la Academia Sueca le concedió el Premio Nobel de literatura al dramaturgo español Jacinto Benavente (ya antes también lo había recibido Echegaray, aunque compartido). Es probable que tal gesto intentara ser una compensación política, pero como también ocurrió con otros grandes maestros de la literatura como Tolstoi, Ibsen, Emile Zola o Strindberg, vetados por el sesgo conservador en el seno de la propia Academia en Estocolmo, la obra de Galdós, «una de las tres o cuatro figuras máximas de la literatura española», fue apartada del Premio Nobel «por la ciega hostilidad de adversarios políticos a quienes la saña transformó en enemigos suyos y de la gloria de su país».

Varias son las interpretaciones en piedra que diferentes escultores en distintas épocas han hecho de la personalidad e imagen del escritor canario. De todas ellas quizá sea la más emotiva la que se conserva en el parque del Retiro de Madrid, en el paseo de Fernán Núñez, esculpida por un joven Victorio Macho e inaugurada en 1919 en presencia del propio Galdós. Otros homenajes en piedra —sin seguir un orden cronológico— son:

Una escultura, la segunda del escritor esculpida por Victorio Macho, hecha en piedra caliza 1922, originalmente frente al océano y conservada luego en la Casa-Museo Pérez Galdós en Las Palmas, en un prudente acto de traición al escultor castellano cuyo deseo, en sus propias palabras, fue: «... yo sueño que 'mi Galdós' llegue a confundirse con el paisaje y parezca una roca...»"

De 1969 es la escultura de Pablo Serrano instalada en la plaza de La Feria, también en Las Palmas. Y de 1991, en esa misma capital de Gran Canaria, otro Galdós yacente en piedra, en un escorzo que copia el esculpido por Serrano, encargado a Manuel Bethencourt y que se encuentra desde el 21 de febrero de 2008 ante el Teatro Pérez Galdós, pero que antes estuvo en la estación de "guaguas" de San Telmo. También en Las Palmas están: el busto colocado en el Parque Doramas, obra de Teo Mesa del año 2000, y un Galdós en bronce, de tamaño natural, sentado leyendo en un banco de la plaza que lleva su nombre en la barriada de Alfredo Schamann.

Instalado desde el 24 de mayo de 2012 en la avenida del Cabildo del municipio de Telde, otro busto, acordado por el pleno del Ayuntamiento en 1911, se hizo realidad un siglo después, con ayuda del Cabildo de Gran Canaria. Y al otro lado del Atlántico, un busto en piedra blanca de Córdoba, obra del escultor Erminio Blotta, instalado el 10 de mayo de 1943 en el parque Independencia de Rosario, Argentina. El monumento tenía una placa en bronce, en la que podía leerse: «Benito Pérez Galdós, 1843-1920. Homenaje de los españoles republicanos a la ciudad de Rosario en conmemoración del centenario del ilustre escritor. Rosario, 10 mayo MCMXLIII»... y que fue robada en fecha ignota. También en Sudamérica, en Caracas, en la plaza Galdós de la Avenida las Acacias se encuentra la escultura realizada en 1975 por el canario-venezolano Juan Jaén Díaz.

Y volviendo a la península ibérica, de 1998 es el bronce realizado por el escultor Santiago de Santiago y sito en una esquina del Parque de Mesones en el Sardinero de Santander.








</doc>
<doc id="5373" url="https://es.wikipedia.org/wiki?curid=5373" title="Farmacéutico">
Farmacéutico

El farmacéutico o químico farmacéutico o boticario es el profesional con habilidades integrales en salud, fabricación de medicamentos, control de calidad, desarrollo e investigación de los mismos. Además el farmacéutico tiene conocimientos de toxicología, legislación (que rige los productos medicamentosos y sanitarios), tecnología farmacéutica y salud pública, entre otros.

En la antigüedad, el farmacéutico elaboraba medicamentos a partir de principios activos extraídos directamente de animales, plantas y, en menor medida, minerales. La utilizacion de principios activos directamente de la naturaleza, hacia que, en muchos casos se administraran dosis erróneas (por exceso o por defecto) de sustancias activas lo que provocaba que el efecto fuera demasiado elevado (aparición de efectos adversos) o no fuera suficiente (sin correlación entre el principio activo y la enfermedad a tratar).

Actualmente la mayoría de medicamentos son elaborados de manera semisintética en laboratorio a partir de los principios activos de la naturaleza (tomando como modelo las sustancias activas anteriormente descritas, se modifican para que sean más eficaces, tengan utilidad terapéutica o tengan menores efectos adversos (como transformar el veneno del Tejo en el medicamento quimioterápico Paclitaxel) o sintética, creando los medicamentos para que encajen perfectamente en los receptores sobre los que van a adherirse o que actúen específicamente en las rutas metabólicas y que no se pueden conseguir en la naturaleza. Un ejemplo de fármacos sintéticos son los biotecnológicos: proteínas terapéuticas, anticuerpos monoclonales, etc.

Los farmacéuticos en algunos países, sobre todo en Latinoamérica, pueden ser llamados Químicos Farmacéuticos o Químicos Farmacéuticos Biólogos (Q.F.B.). Pero esta denominación puede inducir a error, ya que en algunos países se diferencian los licenciados en Farmacia de los licenciados en química medicinal (o Ciencias Farmacéuticas/ Química farmacéutica) 

En Brasil y Chile, se dicta la carrera de Química y Farmacia que al cabo de un plan de estudios de pregrado de seis años, otorga el título de Químico Farmacéutico. 

En Venezuela la carrera de Farmacia tiene una duración de 5 años y el título con el que egresan es de Farmacéutico. En la Universidad Central de Venezuela existe numerosas especializaciones de postgrado de Farmacia, entre ellas están: Toxicología, Farmacia Comunitaria, Farmacología, etc. 

En Argentina, la carrera posee un título final de Farmacéutico y tiene una duración estimada de 6 años. Se dicta en la Universidad de Buenos Aires como también en casas de estudios privadas como la Universidad de Belgrano y la Universidad Kennedy. Las incumbencias del Farmacéutico en Argentina son: ser Director Técnico responsable del funcionamiento de la oficina de Farmacia, sea esta privada o de carácter oficial definida por la legislación vigente, así como la Industria Farmacéutica y Cosmética; establecer las especificaciones técnicas, higiénicas y de la seguridad que deben reunir los ambientes en los que se realicen los procesos tecnológicos, en el ámbito oficial o privado, hospitalario o industrial destinados a la preparación de medicamentos y otros productos farmacéuticos, alimentos dietéticos, cosméticos, productos alimenticios y otros relacionados con la salud; integrar el personal técnico de producción, control y desarrollo en Farmacias, Industrias Farmacéuticas, Alimentarias y Cosméticas y laboratorios o institutos relacionados o vinculados con las mismas; extraer, aislar, reconocer, identificar y conservar fármacos y nutrientes naturales de origen animal, vegetal y mineral; sintetizar drogas, preparar y dispensar medicamentos destinados a la curación, alivio, prevención o diagnóstico de las enfermedades de los seres vivos; controlar la calidad en lo relacionado con la producción de medicamentos, alimentos y cosméticos, en cuanto a las materias primas, productos intermedios y finales en su aspecto físico, químico, biológico y/o farmacológico; ejercer la dirección de laboratorios de análisis de drogas y medicamentos; realizar estudios farmacológicos, efectuados en sistemas biológicos aislados o en seres vivos; actuar como asesor, consultor y perito, desempeñándose como Director Técnico en cargos, funciones y comisiones que entiendan en problemas que requieran el conocimiento científico o técnico que emane de la posesión del título de Farmacéutico; intervenir en la redacción del Formulario Nacional, de la Farmacopea y de los Códigos y Reglamentos Alimentarios y realizar las funciones paramédicas autorizadas por la legislación sanitaria (Primeros auxilios, inyecciones, etc.). 

En España, el Farmacéutico adquiere el Grado de Farmacia en la universidad tras 5 años. Una vez que tiene el grado ya está capacitado para trabajar como farmacéutico en oficina de farmacia, sin embargo, actualmente se requiere un máster de especialización para trabajar en industria (tanto farmacéutica, como alimentaria) o en ensayos clínicos. Para acceder a la farmacia hospitalaria, se requiere superar la prueba nacional de acceso a farmacéutico interno residente (FIR) y cursar 4 años como farmacéutico residente en un hospital de España. Para acceder a puestos de regulación legislativa o a farmacéutico del estado se requiere aprobar una oposición estatal. 

En México, por ejemplo debido razones históricas y sociales existe 14 denominaciones de farmacia, siendo los más importantes el de licenciatura en farmacia, el de químico farmacéutico biólogo, los cuales tienen duración de 5 años, posteriormente al terminar pueden acceder a trabajos del área hospitalaria, análisis clínicos e industrias farmacéutica y alimentaria, además de participar en desarrollo e investigaciones multidisciplinarias, en áreas de especialización y de posgrados, es el único país donde existe todas estas denominaciones de esta carrera, con un enfoque tanto químico, como farmacológico y biológico.

Los licenciados en Química Farmacéutica en Italia (Ciencias Farmacéuticas, en Estados Unidos) no pueden ejercer la práctica de la farmacia, por lo que tienen otros campos de actividad que incluyen desde la industria, el análisis y control de calidad hasta la investigación y desarrollo de medicamentos. Para ejercer de farmacéutico en estos dos países, se exige el nivel académico de doctor en farmacia o el título de Pharm.D., respectivamente.

A pesar de ello, y para evitar confusión en la mayoría de países del planeta la titulación en Farmacia es equivalente a la de Ciencias Farmacéuticas y denota la licenciatura necesaria para ejercer la profesión de farmacéutico, ya que a nivel mundial existen 4 grados de farmacia, el licenciado en farmacia, el químico farmacéutico, el maestro en farmacia y el doctor en farmacia (el maestro y el doctor se refiera a títulos de grado y no de posgrados).

Los farmacéuticos estudian durante la carrera materias como Química Analítica, Química Orgánica, Química Inorgánica, Biología celular, Biología molecular, Álgebra, Cálculo, Técnicas Instrumentales, Bioquímica, Anatomía, Botánica, Microbiología, Parasitología, Física, Bioestadística, Fisiología, Patología, Química Física, como asignaturas que proporcionan una base químico-física-biomédica, y Química Farmacéutica, Farmacología, Farmacognosia, Tecnología farmacéutica, Tecnología cosmética, Fisiopatología, Inmunología, Biofarmacia, Biotecnología, Farmacoquímica, Farmacocinética, Farmacia Clínica, Atención farmacéutica, Farmacovigilancia, Toxicología, Salud Pública, Análisis Clínicos, Bromatología, Control de calidad, Marketing farmacéutico, Gestión y legislación farmacéuticas como asignaturas de las ciencias farmacéuticas y de base para la práctica farmacéutica.

La especialidad más extendida, al menos en la cultura popular, de un farmacéutico es la titularidad de una oficina de farmacia. Para acceder a ella, en España se tienen dos formas: presentar a un concurso público o adquirir una licencia por parte de otro titular, lo que convierte la compraventa de farmacia en una transacción entre particulares. La "Oficina de Farmacia" es un establecimiento privado de interés público y para determinar el precio de esa licencia de apertura de una farmacia, se tendrán en cuenta múltiples factores pero, principalmente, dependerá de la facturación de la botica.

El farmacéutico lleva a cabo la atención farmacéutica al paciente que implica el seguimiento farmacoterapéutico que comprende primero, el acto en sí de la dispensación, el control e indicación de las tomas, la información hacia el paciente, despejar dudas del paciente, el control de las posibles interacciones farmacológicas y la correcta conservación de los medicamentos.

El farmacéutico, en su oficina de farmacia, elabora medicamentos en dosis adaptadas a niños o a patologías concretas, estas preparaciones son conocidas como fórmulas magistrales y oficinales, cumpliendo una función social cuando no disponen de estos medicamentos en forma industrializada. La dispensación (a diferencia de la venta, que implica mercancía) de medicamentos puede ser:



Otro cometido de un farmacéutico en una oficina de farmacia es la de aconsejar y vigilar a los pacientes sobre los posibles reacciones adversas a medicamentos, interacciones entre los mismos, y enseñarle la mejor forma de poder aprovechar al máximo los beneficios del medicamento y, en general, dudas sobre ellos de acuerdo a todo tipo de terapia. Si lo cree conveniente, el farmacéutico puede derivar a la persona a un médico.

Recientemente, se aboga por la práctica de la atención farmacéutica como el principal cometido de los farmacéuticos comunitarios.

Aparte de las citadas recetas también se venden otros productos de parafarmacia como productos de cosmética, alimentos especiales, productos de higiene personal, ortopedia, etc. Popularmente a la oficina de farmacia se le suele llamar simplemente farmacia y tradicionalmente se le llama botica. Una oficina de farmacia puede albergar un laboratorio de análisis clínicos o uno de elaboración de productos medicinales mediante las fórmulas magistrales o preparados oficinales.

La oficina de farmacia es el lugar donde el farmacéutico comunitario desenvuelve su labor profesional. Las oficinas de farmacia pueden ser propiedad de un farmacéutico, o en algunos países propiedad de una cadena de farmacias o empresarios. En cualquier caso, en una oficina de farmacia siempre ha de haber un farmacéutico titulado en todo momento, bien titular o empleado, entre los que encontramos distintas categorías: regente, sustituto, adjunto o facultativo. Estas categorías son beneficiarias del llamado "plus facultativo". Sin embargo, en la botica también se encuentra personal auxiliar, que ayudan al farmacéutico en la dispensación y recepción de pedidos, pero que ya no percibiría este complemento salarial. Poco a poco se van introduciendo los técnicos en farmacia.
El personal cumple las siguientes funciones:

En América Latina, la farmacia no puede existir sin el Farmacéutico, quien debe preparar medicamentos, controlar y supervisar la dispensación de medicamentos, no siempre atiende público, el que atiende público es un Auxiliar de Farmacia.

Pero en la actualidad la mayoría de los medicamentos son preparados en la Industria, aplicando la tecnología farmacéutica más sofisticada como tanques, mezcladores y más instalaciones industriales para elaborar lotes de distintas formas farmacéuticas, sin mencionar el uso de sistemas de control de calidad, aseguramiento de la calidad y de administración que permitan fabricarlos en serie, de la mejor calidad y económicamente viables siguiendo las GMP (Buenas Prácticas de Manufactura).

En casi todos los países, los farmacéuticos hospitalarios son farmacéuticos que han estudiado y realizado prácticas profesionales por un período de 1 a 5 años como postgrado. Esta especialización les permite realizar funciones clínicas y técnicas que normalmente no se esperan de los licenciados en farmacia. Un farmacéutico de hospital bien formado puede de hecho actuar, y esta es la realidad en muchos hospitales, como farmacéutico clínico (al mismo nivel de competencia, sino mayor ya que están más preparados para tareas de planificación y científico-técnicas, que los farmacólogos clínicos que generalmente suelen ser licenciados en medicina con escasa formación en ciencias farmacéuticas). Según algunos, esto explica porque la evolución y desarrollo de la farmacia hospitalaria y clínica ha sido mucho mayor que la farmacología clínica (restringida solo a médicos). No obstante, existen áreas donde la colaboración entre médicos formados en farmacología clínica y farmacéuticos de hospital (farmacéuticos clínicos) puede ser muy fructífera como por ejemplo: los ensayos clínicos, la farmacoeconomía, la farmacovigilancia y la evaluación de tecnologías sanitarias entre otras.

La investigación, desarrollo, elaboración y control de formas de dosificación de los medicamentos a gran escala son otros de los principales cometidos de los farmacéuticos.

Actualmente esta labor se desarrolla en la Industria Farmacéutica y Biotecnológica. Para ello, según los diferentes países, los farmacéuticos están más o menos preparados y por ello realizan breves o extensos programas del postgrado para realizar estas funciones.

Aparte del diseño de formas de dosificación y la elaboración y control de medicamentos, los farmacéuticos pueden desarrollar multitud de funciones específicas en la industria farmacéutica (técnicos comerciales, jefes de marketing de productos, monitores de ensayos clínicos, farmacólogos, químicos farmacéuticos, bioquímicos, especialistas de registros farmacéuticos, relaciones institucionales, farmacoeconomía, información médica, asesores médicos, etc, etc...).

La investigación y desarrollo de nuevos fármacos es un sector en auge hoy día. La necesidad de buscar remedio a miles de enfermedades es uno de los objetivos prioritarios y los farmacéuticos se encargan de investigar tanto el uso y seguridad de utilización de principios activos (naturales, semisinteticos o sintéticos) así como establecer la dosis correcta y comprobar que son eficaces y seguros. Para ello se valen de la colaboración de múltiples especialistas en ciencias farmacéuticas y biomédicas (licenciados en ciencias experimentales, de la salud y de la vida) y a la utilización de ensayos clínicos.

Farmacia es una profesión muy amplia. Lo que más conoce la población es la farmacia comunitaria que está regida por un farmacéutico quien tiene a su cargo la dirección técnica y científica del establecimiento; además puede preparar ciertos medicamentos ya sea por orden médica, generalmente el dermatólogo, o de formulación propia. En muchos países con el título de licenciado es suficiente.

Sin embargo, en muchos países para ciertas actividades es necesaria una especialización reglada del farmacéutico (por ejemplo, el sistema FIR, Farmacéutico interno residente de España) o bien a través de maestrías o doctorados:






</doc>
<doc id="5374" url="https://es.wikipedia.org/wiki?curid=5374" title="Farmacia">
Farmacia

La farmacia (del griego "φάρμακον /fármakon/", 'medicamento, veneno, tóxico') es la ciencia y práctica de la preparación, conservación, presentación y dispensación de medicamentos; también es el lugar donde se preparan, dispensan y venden los productos medicinales. Esta definición es la más universal y clásica, que se solapa con el concepto de Farmacia Galénica (Galeno fue un médico griego del siglo II, experto en preparar medicamentos). 

Antes del siglo XX y principios del mismo, la formulación y preparación de medicamentos se hacía por un solo farmacéutico o con el maestro farmacéutico. A partir del siglo XX, la elaboración de los medicamentos corre a cargo de la moderna industria farmacéutica, si bien siguen siendo farmacéuticos los que coordinan e investigan la formulación y preparación de medicamentos en las grandes empresas farmacéuticas. 

Recientemente se considera también práctica de las farmacias aconsejar al paciente en lo que se refiere a su medicación y asesorar a los médicos u otros profesionales sobre los medicamentos y su utilización (farmacia clínica y atención farmacéutica). 

Los farmacéuticos también colaboran en grupos de investigación con los químicos, bioquímicos, biólogos e ingenieros para descubrir y desarrollar compuestos químicos (y biológicos) con valor terapéutico. Además debido a las nuevas regulaciones internacionales en materia de higiene y salud públicas (OMS/ ICH), cada vez con más frecuencia se solicita su consejo en temas de salud pública.

La historia de la farmacia como ciencia independiente es relativamente joven. Los orígenes de la historiografía farmacéutica se remontan al primer tercio del s. XIX, que es cuando aparecen las primeras historiografías, que si bien no toca todos los aspectos de la historia farmacéutica, son el punto de partida para el definitivo arranque de esta ciencia.

Hasta el nacimiento de la farmacia como ciencia independiente, existe una evolución histórica, desde la antigüedad clásica hasta nuestros días que marca el curso de esta ciencia, siempre relacionada con la medicina.

La farmacia se ha desarrollado a partir de varias ciencias como la química orgánica, la bioquímica, la fisiología, la botánica, la biología celular y la biología molecular. En sus orígenes la práctica médica y la farmacéutica estaban fusionadas. Luego se separaron y divergieron. Actualmente son complementarias, no se entiende una medicina sin farmacia y no tiene sentido una farmacia sin medicina. Así, la farmacia es, en verdad, una reunión de múltiples disciplinas de la ciencia, y se puede dividir en dos ramas principales:

Ciencias Farmacéuticas y Práctica Farmacéutica.


La farmacología y toxicología, en algunos entornos y quizás por razones históricas, se consideran como ciencias separadas de las ciencias farmacéuticas, en cualquier caso actualmente son básicas en la formación de los graduados en Farmacia. Las facultades de Medicina suelen tener también programas de farmacología en la formación de sus graduados. La farmacología clínica es, en algunos países (USA y Holanda son excepciones), una disciplina exclusiva para graduados en Medicina, sin embargo la farmacocinética clínica es una disciplina donde los graduados en Farmacia en algunos casos han contribuido a la misma de forma importante en términos académicos y en su aplicación industrialy en otros supone una parte de la práctica habitual de la Farmacia Hospitalaria.

En los últimos años también se habla del uso de terapia génica como otra forma de remedio contra muchas nuevas enfermedades por lo cual también cobra interés entre los farmacéuticos todo lo relacionado con la biotecnología farmacéutica.

La botica es el lugar o establecimiento donde un farmacéutico ejerce la farmacia comunitaria o proporciona servicio sanitario a un paciente ofreciéndole consejo, dispensándole medicamentos fruto de este consejo o por receta del médico y otros productos de parafarmacia como productos de cosmética, alimentos especiales, productos de higiene personal, ortopedia, etc. Popularmente a la oficina de farmacia se le suele llamar simplemente farmacia y tradicionalmente se le llama botica. Una oficina de farmacia puede albergar un laboratorio de análisis clínicos o uno de elaboración de productos medicinales mediante las fórmulas magistrales o preparados oficinales.

En España, las oficinas de farmacia las regula cada Comunidad Autónoma a través de Leyes de Ordenación Farmacéutica . En ellas se establecen los requisitos que deben cumplir los locales donde se pueden abrir las farmacias, el precio de las licencias y concesiones de farmacia, así como otros aspectos como la distancia entre boticas, con lo que se pretende mantener la libre competencia. 

La oficina de farmacia es el lugar donde el farmacéutico comunitario desenvuelve su labor profesional. Las oficinas de farmacia pueden ser propiedad de un farmacéutico, o en algunos países propiedad de una cadena de farmacias o empresarios. En cualquier caso, en una oficina de farmacia siempre ha de haber un farmacéutico titulado en todo momento, bien titular o empleado. En cuestión de categorías, en España la ley distingue entre farmacéuticos:
Mientras que por farmacéutico adjunto se entiende que trabaja «conjuntamente» con el o los farmacéuticos propietarios o regentes, en el caso de los sustitutos se entiende que actúan «en vez de». En España el número de farmacéuticos adjuntos necesarios está regulado por las Comunidades Autónomas y varía de unas a otras.

Pero en la farmacia también se encuentra personal auxiliar, que ayuda al farmacéutico en la dispensación y recepción de pedidos. Poco a poco se van introduciendo los técnicos en farmacia.

El personal cumple las siguientes funciones:

En Latinoamérica la farmacia no puede existir sin el químico farmacéutico, quien debe preparar medicamentos, controlar y supervisar la dispensación de medicamentos, no siempre atiende público, el que atiende público es un idóneo o técnico en farmacia.

Pero en la actualidad la mayoría de los medicamentos son preparados masivamente en una fábrica, aplicando la tecnología más sofisticada como tanques, mezcladores y más instalaciones industriales para elaborar enormes lotes de distintas formas farmacéuticas, sin mencionar el uso de sistemas de control de calidad y de administración que permitan fabricarlos en serie, de la mejor calidad y económicamente viables.

Las farmacias españolas suelen contar con contenedores específicos, denominados Puntos SIGRE, en los que los ciudadanos pueden depositar los envases vacíos y los restos de medicamentos, bien al finalizar un tratamiento o cada vez que se revise el botiquín para retirar aquellos que estén caducados, en mal estado de conservación o ya no se necesiten.

De esta manera, el farmacéutico desempeña una importante labor de asesoramiento con los pacientes en todo lo referente al correcto cierre del ciclo de vida de los medicamentos, aconsejando sobre la adecuada manera de desprenderse de los mismos, sin dañar al medio ambiente y evitando la automedicación incontrolada. Así mismo, mediante la custodia del contenedor, también garantiza que los restos de medicamentos o envases depositados en el Punto SIGRE no puedan ser extraídos ni manipulados, con el consiguiente riesgo que esto entrañaría.

La farmacia está representada por muchos símbolos. Los más comunes en Argentina, España y Francia son la Copa de Higía, la cruz griega verde o la cruz pateada, este último especialmente en los luminosos de las oficinas de farmacia. También existen otros como el mortero y la maza, el carácter de receta, ("recipere"), medidas cónicas, caduceos, Vara de Esculapio o una "A" roja gótica y estilizada en el caso de Alemania. La "A" proviene de Apotheke, vocablo germano de Farmacia. .

En Chile se utiliza habitualmente el símbolo Rx (Recipe) o Rp, que es la traducción latina, para encabezar las recetas; ambos corresponden con la expresión latina de la entrega del producto curativo.

Los servicios de farmacia hospitalaria, en España, son, por ley, servicios generales clínicos. Sus funciones fueron descritas por la legislación. Jerárquicamente suelen depender de la dirección médica del hospital al igual que los servicios de análisis clínicos, Microbiología o Medicina Nuclear entre otros. En resumen, son responsables de la adquisición, conservación, dispensación y elaboración de medicamentos así como de la selección y evaluación de medicamentos, la información farmacoterapéutica, las actividades de farmacocinética clínica, de farmacovigilancia, el control de productos en fase de investigación clínica y la realización de estudios de utilización de medicamentos. Son responsables de coordinar las comisiones de farmacia y terapéutica de los hospitales y de elaborar y mantener las guías o formularios farmacoterapéuticos. Es decir, cumplen funciones de gestión, logísticas, y clínicas tanto con fines asistenciales, docentes como de investigación.

Recientemente destaca su involucración en el seguimiento y control de tratamientos farmacológicos tanto de pacientes hospitalizados como ambulatorios (atención farmacéutica y farmacia clínica), la elaboración y control de preparaciones parenterales (agentes antineoplásicos, antibióticos y nutrición parenteral) y la automatización de los procesos de dispensación individualizada de los medicamentos a los pacientes ingresados (distribución en dosis unitarias).

Para trabajar en los servicios de farmacia hospitalaria, en España y en muchos países europeos, se exige al licenciado en farmacia además un postgrado que es el título de especialista en farmacia hospitalaria. Este título oficial se consigue superando una prueba nacional de selección para elegir hospital y cuatro años de residencia remunerada (es conocido como el FIR, farmacéutico interno residente). Durante los cuatro años de residencia el farmacéutico adquiere todos los conocimientos y habilidades para ejercer la especialidad. En España el sistema MIR-FIR surgió como adaptación del sistema americano de formación de médicos en los años 60-70 y uno de los hospitales pioneros fue el Hospital Marqués de Valdecilla de Santander. Hoy en día nadie duda de las bondades de dicho sistema de formación por la calidad y excelencia de la misma.

Existen farmacéuticos especialistas en farmacia hospitalaria que además han conseguido diplomas acreditativos de superespecializaciones en Oncología farmacéutica, Farmacocinética clínica, Farmacoeconomía, Farmacoepidemiología y Nutrición Parenteral y Enteral (no oficiales en España pero avalados por instituciones académicas norteamericanas y españolas). Además, muchos farmacéuticos especialistas trabajan a tiempo completo en actividades como farmacocinética clínica, atención farmacéutica en distintas especialidades médicas, seguimiento nutricional, información y documentación farmacoterapéutica, farmacovigilancia y farmacoepidemiología, y educación e información a pacientes ambulatorios entre otras.

En un servicio de farmacia de un hospital de nivel terciario de unas 600 camas, típicamente hay unos 5-6 farmacéuticos especialistas y unos 4-8 farmacéuticos residentes. Además, suele haber estudiantes de 5º de farmacia realizando su estancia de prácticas tuteladas. Aparte de técnicos y/o enfermeros y personal auxiliar. En algunos grandes hospitales (más de 1500 camas) el número de farmacéuticos especialistas puede ser en torno a 15.

Los medicamentos constituyen la tecnología médica más utilizada, disponiendo de una gran variedad de estos fabricados por la industria farmacéutica para la terapéutica, prevención y/o rehabilitación, dispuestos para su distribución, almacenamiento, expendio y dispensación en los establecimientos. La clasificación se establece en la Ley General de Salud en los artículos 224 y 226, de acuerdo a su preparación, naturaleza, venta y suministro; y en la Organización Mundial de la Salud (OMS) por su efecto terapéutico.







Secretaría de Salud.


Esta prescripción se deberá retener por el establecimiento que la surta en la tercera ocasión; el médico tratante determinará, el número de presentaciones del mismo producto y contenido de las mismas que se puedan adquirir en cada ocasión.



Nota: El registro sanitario para la comercialización de medicamentos sólo puede ser otorgado por la Secretaría de Salud. La clave de registro será única, no se podrá aplicar la misma a dos productos que se diferencien ya sea en su denominación genérica o distintiva o en su formulación. El titular de un registro, no tendrá la posesión de dos registros que ostenten el mismo fármaco, forma farmacéutica o formulación.

Los elementos que constituyen la clave de registro son: un número consecutivo asignado por la autoridad sanitaria, la sigla que indica el tipo de medicamento, M para alopático, P para herbolario
Y para homeopático, el año en que fue autorizado y las siglas SSA.
Las etiquetas de los medicamentos indican además de lo anterior la fracción a la cual pertenecen, como se muestra en el siguiente ejemplo:
Reg. No.0310M2009 SSA IV.



</doc>
<doc id="5381" url="https://es.wikipedia.org/wiki?curid=5381" title="Especialidad farmacéutica">
Especialidad farmacéutica

Especialidad farmacéutica es el antiguo nombre que se le daba en la "Ley del Medicamento" de 1990 de España al medicamento de composición e información definidas, de forma farmacéutica y dosificación determinada, preparado para su uso medicinal inmediato, dispuesto y acondicionado para su dispensación al público, con denominación, embalaje, envase y etiquetado uniformes y al que la autoridad farmacéutica otorgue autorización sanitaria e inscriba en el Registro de especialidades farmacéuticas. Este término se ha sustituido casi por completo con la nueva ley de garantías y uso racional del medicamento y productos sanitarios (LGRUM) de 2006 con el término de medicamento y sólo se mantiene para la denominación del medicamento genérico.

La Especialidad Farmacéutica Genérica (EFG), también conocidos como medicamento genérico, es la especialidad con la misma forma farmacéutica e igual composición cualitativa y cuantitativa en sustancias medicinales que otra especialidad de referencia (bioequivalencia), cuyo perfil de eficacia y seguridad esté suficientemente establecido por su continuado uso clínico.

Cada número de registro se referirá únicamente a una composición, una forma farmacéutica, una dosis por unidad de administración y una presentación para la venta.

Para obtener autorización sanitaria, una especialidad farmacéutica deberá satisfacer las siguientes condiciones: 






</doc>
<doc id="5382" url="https://es.wikipedia.org/wiki?curid=5382" title="Opus Dei">
Opus Dei

La prelatura de la Santa Cruz y del Opus Dei (en latín: "Praelatura Sanctae Crucis et Operis Dei"), conocida simplemente como Opus Dei, es una jurisdicción de alcance mundial perteneciente a la Iglesia católica. Fue fundada el 2 de octubre de 1928 por Josemaría Escrivá de Balaguer, sacerdote español canonizado en 2002 por Juan Pablo II. Fue erigida como prelatura personal el 28 de noviembre de 1982 mediante la constitución apostólica "Ut sit" del papa Juan Pablo II. También es denominado la Obra, ya que el término latino «Opus Dei» significa «obra de Dios».

Las prelaturas personales se regula en los artículos 294 a 297 del Código de Derecho Canónico, que establecen que la prelatura debe estar gobernada por un prelado y compuesta por sacerdotes que forman el clero propio de la prelatura y por fieles laicos.

El Opus Dei, fundado en 1928, fue aprobado por primera vez en 1941 por el obispo de Madrid (España), Leopoldo Eijo y Garay. En 1950 la Santa Sede lo aprobó como Instituto Secular, rigiéndose por sus propios estatutos y dependiendo de la Congregación de Religiosos. Tras solicitarlo, fue erigida como prelatura personal (es decir, no territorial) el 28 de noviembre de 1982 por el papa Juan Pablo II, siendo la única existente hasta la actualidad. La prelatura depende de la Congregación para los Obispos.

De acuerdo con la propia organización, la misión del Opus Dei consiste en fomentar la conciencia de la llamada universal a la santidad en la vida ordinaria.

Según datos de la prelatura a 2017 el Opus Dei cuenta con 2083 sacerdotes y un total de 92 600 miembros aproximadamente. El 57% de los miembros del Opus Dei son mujeres y cerca del 90% reside en Europa y América.

El patrimonio de la prelatura estaba estimado en 2005 en alrededor de 2.800 millones de dólares estadounidenses, según un estudio de John Allen., cifra que también se encuentra en otras referencias.

El Opus Dei ha recibido reconocimiento y apoyo de los papas, de diversas autoridades católicas y de otras personalidades.

El Opus Dei fue fundado, originalmente sin este nombre, por el sacerdote español Josemaría Escrivá de Balaguer el 2 de octubre de 1928.En 1930, fundó la sección femenina del Opus Dei.

En 1933 se abrió la Academia DYA, el primer centro de la institución, donde se impartieron clases de Derecho y Arquitectura, para al año siguiente convertirse en residencia universitaria.

Hacia 1935/36, en la "Academia DYA", los miembros del Opus Dei comenzaron a practicar algunas costumbres que el fundador concibió como medios para alcanzar los fines de la institución y que pasarían a ser signos distintivos de la futura Obra, entre las que se encuentran la corrección fraterna, las visitas a los pobres de la Virgen, las catequesis o el llamado "plan de vida", que incluye actos de piedad como la misa diaria, comunión, rezo del ángelus, visita al Santísimo, lectura espiritual, rezo del Santo Rosario y penitencia.

Durante la guerra civil española, en la que se desata la persecución religiosa, Josemaría Escrivá se ve obligado a refugiarse en diversos lugares. En 1937, Escrivá y otros miembros del Opus Dei abandonan la zona "republicana" cruzando los Pirineos por Andorra y llegan a Francia, desde donde regresan a España, a la zona dominada por los sublevados, donde la Iglesia no era perseguida. La contienda hace suspender los proyectos del fundador del Opus Dei de extender la labor apostólica a otros países.

Tras la guerra civil, se inició en España la dictadura de Francisco Franco que, después de la persecución religiosa sufrida por la Iglesia católica, contó con el apoyo de buena parte de la jerarquía. Terminada la guerra, Josemaría Escrivá regresó a Madrid, y comenzó a expandir la labor del Opus Dei por otras ciudades de España. El inicio de la Segunda Guerra Mundial impidió los intentos de expandir el Opus Dei a escala internacional.

En 1941 fue aprobado como "Pía Unión" por el obispo de Madrid, Leopoldo Eijo y Garay, pues desde la fecha de su fundación en 1928 el Opus Dei había estado sin reconocimiento jurídico por parte de la Iglesia católica. Esta figura estaba englobada en las asociaciones de fieles, y no suponía un cambio de estado para sus miembros.

El 14 de febrero de 1943, Josemaría Escrivá encontró una solución jurídica que permitió la ordenación de sacerdotes dentro del Opus Dei, la Sociedad Sacerdotal de la Santa Cruz. Esto se ve reflejado un año después, el 25 de junio de 1944, cuando fue reconocida jurídicamente como "sociedad de vida en común sin votos públicos" por el obispo de Madrid, quien ordenó a los primeros sacerdotes del Opus Dei: Álvaro del Portillo, José María Hernández Garnica y José Luis Múzquiz. Esta "sociedad sacerdotal" está formada por algunos miembros varones del Opus Dei que se preparan para ser sacerdotes, y por los que se van ordenando. La figura de "sociedad de vida en común" pertenecía al estado de perfección, y sus miembros clérigos emitían los correspondientes votos de castidad, pobreza y obediencia.

Tras la Segunda Guerra Mundial el fundador del Opus Dei se trasladó a vivir a Roma al darse cuenta de que si quería expandir sus enseñanzas alrededor del mundo, debía establecer la sede del Opus Dei en esa ciudad. En los años siguientes viajó por Europa para preparar el establecimiento del Opus Dei en diversos países.

En 1946 empezó la labor del Opus Dei en Portugal, Italia, Inglaterra, Irlanda y Francia.

A partir de su establecimiento en Roma, se comenzaron a fundar nuevos centros de enseñanza del Opus Dei, entre los que cabe destacar el "Colegio Romano de la Santa Cruz" (fundado en 1948 y actualmente uno de los dos seminarios de la prelatura), por el que pasarán a partir de entonces cientos de miembros "numerarios" del Opus Dei, que recibirán una formación espiritual y pastoral al tiempo que realizan estudios en diversos ateneos pontificios romanos. Con esos estudios, gran parte de dichos numerarios se preparan para el sacerdocio.

En 1947 el Opus Dei recibió la aprobación provisional por parte de la Santa Sede como instituto secular de derecho pontificio. La aprobación definitiva le fue otorgada en 1950. Al instituto pertenecen hombres y mujeres laicos y sacerdotes, tanto los que provienen de los laicos del instituto y que se ordenan para servir a éste, como los sacerdotes diocesanos que continúan dependiendo de sus respectivos obispos.

Desde 1949 el fundador impulsó desde Roma la expansión del Opus Dei por todo el mundo. Antes de acabar ese año, irán los primeros miembros a Estados Unidos y México. Cada año se fueron sumando nuevos países.

En 1950 se empezó en Chile y Argentina. En 1951 fueron los primeros a Venezuela y Colombia. En 1952 se comenzó en Alemania; en 1953 tocó el turno a Perú y Guatemala; en 1954 se inició la labor en Ecuador; en 1956, en Suiza y Uruguay; en 1957 se dieron los primeros pasos en Austria, Brasil y Canadá; en 1958 se fue a El Salvador, Kenia y Japón; en 1959 a Costa Rica. En 1960 a Holanda.

En 1952 comienzan las actividades del Estudio General de Navarra, en Pamplona, que con el tiempo se convertiría en la Universidad de Navarra, con sedes en las ciudades de Pamplona, San Sebastián, Barcelona y Madrid.

En 1953 se fundó en Roma el "Colegio Romano de Santa María", dirigido a numerarias, que es el equivalente del "Colegio Romano de la Santa Cruz", con las mismas funciones que éste, exceptuando la preparación para el sacerdocio, pues la Iglesia no lo permite.

El 26 de junio de 1975 Josemaría Escrivá falleció en Roma. En ese momento pertenecían al Opus Dei unas 60 000 personas de 80 nacionalidades.

En Huesca (España) se inauguró el 7 de julio de 1975 el actual Santuario de Torreciudad, un antiguo proyecto de su fundador que databa de 1960. El 15 de septiembre del mismo año, Álvaro del Portillo fue elegido para suceder al fundador.

El 28 de noviembre de 1982 Juan Pablo II erige al Opus Dei como la primera prelatura personal de la Iglesia católica y nombró prelado a Álvaro del Portillo, al que en 1991 conferiría la ordenación episcopal. Intrínsecamente unida a la prelatura está la Sociedad Sacerdotal de la Santa Cruz, asociación de sacerdotes a la que pertenecen los sacerdotes de la prelatura y aquellos sacerdotes diocesanos que lo deseen (y que no dejan de depender en todo de sus respectivos obispos).

El 15 de octubre de 1984 fue fundado en Roma el "Centro Académico Romano de la Santa Cruz", que posteriormente la Santa Sede lo elevó al rango de Universidad Pontificia, el 9 de enero de 1990, pasando a ser la actual Pontificia Universidad de la Santa Cruz.

En 1994 falleció Álvaro del Portillo, siendo elegido como su sucesor Javier Echevarría, que fue ordenado obispo en 1995. Echevarría fue el prelado hasta su fallecimiento en 2016.

El 23 de enero de 2017 el papa Francisco nombró prelado del Opus Dei a Fernando Ocáriz Braña tras confirmar la elección realizada por el "Tercer Congreso Electivo", celebrado tras el fallecimiento del anterior prelado. En la votación, participaron 194 miembros de la prelatura.

Tras el fallecimiento de Josemaría Escrivá la Santa Sede recibió miles de cartas -entre ellas, las de un tercio del episcopado mundial- solicitando la urgente apertura del proceso de beatificación y canonización. Finalmente, su causa se introdujo en 1981 y el 17 de mayo de 1992, Juan Pablo II beatificó a Josemaría Escrivá de Balaguer y el 6 de octubre de 2002, fue canonizado por dicho papa.

El proceso de canonización de Escrivá gozó del apoyo de destacadas figuras de la jerarquía eclesiástica, pero estuvo también marcado por la polémica y la oposición; según algunos, por ejemplo, fue inusualmente rápido.

Entre las voces positivas se encuentran, por ejemplo, el arzobispo de París, que en 1979 afirmó que si "la Iglesia reconociese la santidad de Monseñor Escrivá (…), el mundo entero obtendría un gran beneficio", o el del cardenal František, arzobispo de Praga, que dijo pocos meses después de su fallecimiento: "su muerte ha sellado una ejemplar vida cristiana y sacerdotal, modelo para la Iglesia". García Lahiguera, arzobispo de Valencia, que trató a Escrivá durante más de 40 años, dijo que "contemplando su vida" se podía decir que "Josemaría Escrivá de Balaguer y Albás era un santo", y el cardenal Ángel Suquía afirmó en la clausura del proceso de virtudes (paso previo a la canonización) que tenía la "segura esperanza" de que su canonización serviría "para despertar y promover deseos y propósitos de santidad".

Hay abiertas otras causas de canonización de fieles de la prelatura del Opus Dei:


El "Opus Dei" fue fundado como "..."camino de santificación dirigido a toda clase de personas"", lo que resultaba novedoso, pues en aquella época era común pensar que sólo los religiosos podían ser santos.

Según explicaba el propio Josemaría Escrivá, la finalidad del Opus Dei es ""contribuir a que haya en medio del mundo hombres y mujeres de todas las razas y condiciones sociales que procuren amar y servir a Dios y a los demás hombres en y a través de su trabajo"". Para su Fundador, la actividad principal del Opus Dei es dar formación a sus miembros y a la gente que quiere recibirla, hasta el punto de que a veces resumía el papel del Opus Dei como "una gran catequesis".

Se presenta aquí un resumen de las enseñanzas de Escrivá de Balaguer, el mensaje oficial del Opus Dei:






Según Escrivá, el fundamento de la vida cristiana es una conciencia personal de la filiación divina. "La alegría viene de saberse hijos de Dios," dice Josemaría. El Opus Dei, dice, es "un ascetismo sonriente".

La espiritualidad de la institución se recoge, en gran medida, en la obra de Escrivá de Balaguer “Camino”, una serie de 999 puntos de meditación para orientar a los fieles.

La idea de la llamada universal a la santidad fue predicada por San Agustín y por San Francisco de Sales, que sin embargo daban énfasis a la liturgia y las oraciones. ""Escrivá es más radical... Para él, es el mismo trabajo material lo que debe transformarse en oración y santidad"", según reflejó el cardenal Luciani, que posteriormente sería papa con el nombre de Juan Pablo I.

Las premisas del mensaje del Opus Dei que todos los cristianos pueden y deben ser santos son las siguientes: los cristianos creen que:

Con el Espíritu Santo residiendo en un cristiano que está dispuesto a aprender, el espíritu humano que se creó para amar, dijo Escrivá, está llevado a través de un "plano inclinado", que empieza con la repetición ferviente de oraciones cortas y entonces " se deja paso a la intimidad divina, en un mirar a Dios sin descanso y sin cansancio...". Así, uno de sus enseñanzas favoritas es el mandato bíblico que todos deben amar a Dios con todo el corazón, alma, poder y mente, un amor que no se reserve nada, un amor que los padres deben transmitir todo el día a sus niños (Deut 6:4-9), y que Cristo llamó "el mandamiento más grande" (Mt 22:37-40). Y también Escrivá apunta al mandamiento nuevo de Jesús: Amar unos a otros como yo os amé.

La prelatura está formada tanto por presbíteros y diáconos del clero secular, como de fieles laicos, hombres y mujeres, gobernados por un Prelado.

Anteriormente a ser erigida como prelatura personal, ya en 1947 obtuvo la aprobación de la Santa Sede como Instituto Secular de Derecho Pontificio, siendo aprobados unos estatutos en 1950. Escrivá solicitó la conversión en prelatura personal en 1962, y no fue sino hasta el papado de Juan Pablo II, el cual finalmente concedió esta petición.

La constitución apostólica ""Ut Sit"" erigió al Opus Dei como "prelatura personal de la Iglesia católica" el 28 de noviembre de 1982. Según Juan Pablo II "se vio con claridad que tal figura jurídica se adaptaba perfectamente al Opus Dei", "teniendo presente la naturaleza teológica y genuina de la Institución."

Como prelatura personal, su clero está sometido directamente a la jurisdicción y a la autoridad del prelado del Opus Dei, y este a su vez, a la del papa, por tanto no está sometido ni a la jurisdicción, ni a la autoridad de los obispos diocesanos. Esto le ha dado amplia independencia dentro de la Iglesia católica para ejercer su apostolado, pues, a diferencia de las diócesis, que tienen una jurisdicción territorial, las prelaturas personales —como los ordinariatos militares— se encargan de "personas" en cuanto a algunos objetivos particulares sin tener en cuenta donde viven. En cuanto a los laicos del Opus Dei, ya que no son diferentes de otros católicos, "continúan bajo la jurisdicción del obispo diocesano," en las palabras de "Ut Sit". Estas estructuras seculares son muy diferentes de las órdenes religiosas o las congregaciones.

Según críticos al Opus Dei como Juan José Tamayo-Acosta, teólogo y profesor de la Universidad Carlos III de Madrid, Hans Küng, Leonardo Boff, Jesús Cardenal, Michael Walsh (exjesuita) y Kenneth Woodward, periodista de "Newsweek", el Opus Dei con esta categoría jurídica se convirtió de facto en una "Iglesia dentro de la Iglesia", debido a su gran independencia dentro de la misma por no estar sometida a la jurisdicción directa de las diócesis territoriales.

Por el contrario, desde el Opus Dei se señala: "Ninguna parte de la Iglesia constituye “una iglesia dentro de la Iglesia”, sino justamente lo contrario: cada parte promueve vínculos de comunión respecto a toda la Iglesia. (...) La legítima autonomía del Opus Dei para llevar a cabo su misión eclesial, como por lo demás la autonomía que en diversos grados es propia de todo fiel y de cualquier realidad eclesial, es siempre autonomía en la comunión con la Iglesia universal y el Romano Pontífice, y con las Iglesias particulares y los obispos diocesanos. En este sentido, el Opus Dei, en su actual configuración como prelatura, goza de la autonomía propia de los entes de la constitución jerárquica de la Iglesia (cuya cabeza es un sujeto con potestad episcopal), que es distinta de la autonomía propia de los entes de estructura asociativa".

La prelatura personal de la Santa Cruz y del Opus Dei ha sido gobernada por cuatro sacerdotes, llamados también prelados, desde que Juan Pablo II, elevó al Opus Dei a la calidad de prelatura personal.

La curia de la prelatura personal tiene su sede en Roma, en donde el prelado -único designado por el papa y único cargo vitalicio- está acompañado de tres vicarios: auxiliar, general y secretario central. El prelado cuenta con la colaboración de la "Asesoría Central" —consejo de mujeres— y del "Consejo General" —de hombres—. Los vicarios son designados por el prelado, quien también erige o modifica las circunscripciones de la prelatura en diversas partes del mundo. Estas son de tres tipos -con personalidad jurídica- según su grado de desarrollo: regiones, cuasi regiones y delegaciones dependientes del prelado. Al frente de las dos primeras, el prelado designa por quinquenios a "vicarios regionales", y al frente de las terceras a "vicarios delegados". Los vicarios regionales están asesorados por la "Asesoría Regional" para mujeres y por la "Comisión Regional" para hombres. Algunas regiones están subdivididas en delegaciones dependientes del vicario regional, las que, lo mismo que las dependientes directamente del prelado, tienen al frente un vicario delegado asesorado por una asesoría para mujeres y una comisión para hombres.

A nivel local, en algunas ciudades los vicarios regionales o delegados erigen "centros de la prelatura", que son las estructuras de base. Los centros son de mujeres o de hombres y cada uno tiene a su frente un laico director o directora que preside un "consejo local", y con al menos otros dos fieles de la prelatura. Para la atención espiritual de los fieles de cada centro el prelado designa un sacerdote.

El Opus Dei tiene establecidos centros en 64 países y en Puerto Rico, Hong Kong, Macao y Taiwán, y se compone de 49 circunscripciones: 

El Opus Dei es una prelatura personal formada por presbíteros, diáconos y laicos, a cuyo frente se encuentra un prelado. Por último, la Sociedad Sacerdotal de la Santa Cruz es una asociación sacerdotal intrínsecamente unida a la prelatura a la que pueden pertenecer los sacerdotes diocesanos. Cuando se dice que una persona "pertenece al Opus Dei", se quiere decir que se encuentra en alguna de esas categorías: los sacerdotes de la prelatura, los laicos que se dedican a sus obras apostólicas y los sacerdotes diocesanos de la Sociedad Sacerdotal de la Santa Cruz.

A su vez, dentro de cada grupo existen varios subtipos:

Por último, ambas instituciones (la prelatura y la Sociedad Sacerdotal) admiten Cooperadores (de cualquier tipo la primera, sólo sacerdotes diocesanos la segunda), que sin pertenecer a ellas les prestan ayuda de forma estable, con sus limosnas, sus oraciones o su trabajo.

Representan menos del 2% del total de sus miembros, lo que da al Opus Dei una naturaleza fundamentalmente laica dentro de la Iglesia católica. Proceden de los numerarios y agregados laicos del Opus Dei. Principalmente, atienden a los miembros laicos y trabajan en las labores apostólicas. Los principales cargos de gobierno en la prelatura (prelado, vicarios regionales y vicarios delegados) suelen estar ocupados por miembros de esta categoría.

Los sacerdotes que forman el clero de la prelatura, fueron llamados por el prelado a hacerse sacerdotes, y aceptaron esa llamada libremente. Realizan sus estudios sacerdotales en centros o en seminarios del Opus Dei (no en seminarios diocesanos), y el Opus Dei se responsabiliza de su sustento (alojamiento, ropa, etc.).

Los sacerdotes numerarios y agregados viven como los laicos numerarios y agregados, respectivamente: los sacerdotes numerarios en centros de la prelatura, y los sacerdotes agregados con su familia, en residencias, solos, etc. Varios sacerdotes numerarios han sido ordenados obispos por el papa.

Como ya se ha señalado, suponen la inmensa mayoría de los miembros del Opus Dei (más del 97%). Existen varios tipos de miembros laicos en la Prelatura del Opus Dei: supernumerarios, numerarios, agregados y numerarias auxiliares. Las diferencias entre ellos consisten principalmente en si viven el celibato o no y si viven en centros de la prelatura o no. Una y otra cosa determinan la disponibilidad de los miembros para ayudar en las actividades apostólicas de la Prelatura.


Son los más numerosos, representando actualmente cerca del 70% del total de miembros. Los supernumerarios no tienen compromiso de celibato (es decir, pueden casarse), viven y trabajan donde consideran oportuno, y buscan la santificación con su vida ordinaria, además de tener un "plan de vida" espiritual con diversos medios de formación y prácticas de piedad. Debido a su profesión y obligaciones familiares, los supernumerarios no poseen tanta disponibilidad como los numerarios y agregados, pero suelen colaborar económicamente con el Opus Dei u ofrecer apoyo según las circunstancias se lo permitan. No ocupan cargos directivos.

Aunque a veces se hable en masculino, en todos los grupos hay varones y mujeres: numerarios y numerarias, etc. Ambas secciones (masculina y femenina) son, en cuanto a lo apostólico, independientes (distintos centros y distintas labores apostólicas). Sin embargo, la sección femenina, mediante algunas de sus supernumerarias, agregadas y auxiliares, se ocupa a menudo de la administración (labores de limpieza, cocina y servicio doméstico) de los centros de las secciones masculinas. Durante estas labores de servicio doméstico se coordina para evitar la coincidencia presencial o contacto visual entre los miembros de ambas secciones (masculina y femenina). En este sentido, la sección masculina no es operativamente independiente de la femenina. Al mismo tiempo, la labor apostólica de la sección femenina depende del trabajo pastoral de los sacerdotes numerarios pertenecientes a la sección masculina.

Son miembros con compromiso de celibato. Son aproximadamente un 10% de los miembros del Opus Dei. Procuran santificar su profesión o labor, sus relaciones sociales y su vida familiar, haciendo apostolado y estando disponibles para atender encargos apostólicos que en sus centros del Opus Dei puedan encomendarles. Los agregados se diferencian de los numerarios en que no viven en centros del Opus Dei: los agregados, igual que sus conciudadanos, viven en su domicilio particular en el lugar que les resulte más conveniente. Hay quienes viven solos, quienes viven con sus padres o hermanos, y quienes viven junto con otros agregados en residencias establecidas al efecto. Por motivos de dedicación profesional o laboral, u otros motivos familiares o personales, no están tan disponibles como un numerario para ocupar cargos de gobierno, pero también están implicados en la labor apostólica de la Obra, ofreciendo dirección espiritual y formación a otros miembros y a otras personas que participan en la labor apostólica del Opus Dei. Reciben formación en filosofía y teología e incluso algunos se ordenan como sacerdotes de la Prelatura. 

Comprenden aproximadamente el 20% de los miembros. Los numerarios y numerarias son miembros con compromiso de celibato que, generalmente, viven en un centro del Opus Dei. Pueden, en principio, ejercer una profesión civil, pero han de estar dispuestos a renunciar a su ejercicio si la Prelatura se lo solicita para ejercer otra función dentro de la organización.

Son los primeros responsables de la formación de los demás miembros del Opus Dei, y suelen desempeñar los cargos directivos. Reciben una formación filosófica y teológica que, a lo largo de su vida, es comparable a la recibida por los sacerdotes en los seminarios.

Son numerarias que se dedican en exclusiva al trabajo del hogar, para que los centros del Opus Dei sean hogares de familia. Al ser esa su tarea profesional, no desempeñan cargos directivos.

En los estatutos del Opus Dei se dice acerca de las numerarias auxiliares:

La Sociedad Sacerdotal de la Santa Cruz es una asociación de clérigos (sacerdotes), intrínsecamente unida a la prelatura personal del Opus Dei. Pertenecen a ella los presbíteros y diáconos diocesanos que lo desean y los sacerdotes (agregados y numerarios) del clero de la prelatura. Forman parte de ella unos 1900 sacerdotes diocesanos y los 2100 sacerdotes de la prelatura (año 2017). El prelado del Opus Dei es el presidente de la Sociedad.

Los cooperadores del Opus Dei no son miembros de la prelatura, pero colaboran de distintas formas con ésta (oraciones, limosna, trabajo). Para ser Cooperador no es necesario ser cristiano sino, tan sólo, tener deseos de colaborar con las actividades o fines del Opus Dei.

Los cooperadores pueden participar en las actividades educativas y de formación del Opus Dei. Asimismo, las comunidades religiosas pueden ser cooperadoras de la Prelatura. Actualmente existen cientos de estas comunidades que cooperan mediante sus oraciones por el Opus Dei y sus apostolados.

Uno de los encargos que tiene la Sección femenina del Opus Dei es el de ocuparse de las labores domésticas en los Centros de la Prelatura, tanto de varones como de mujeres.

Para casi todas las numerarias auxiliares y para algunas numerarias, las tareas de mantenimiento de los Centros constituye su trabajo profesional y, por lo tanto, donde deben buscar su propia santificación.

Cuando se trata de un Centro de varones, la separación entre los varones y las mujeres es total. De ordinario debe haber doble puerta entre la casa de las auxiliares y los residentes. No suele haber ningún tipo de relación entre las auxiliares y los residentes de los centros, hasta el punto de que es habitual que no conozcan los nombres ni mantengan conversaciones. Las entradas de los numerarios y las auxiliares son siempre distintas, incluso se procura que estén en calles distintas, para que las personas que viven en una y otra casa no se vean al salir y entrar. 

John Allen, aporta cuatro razones por las que se vive esta separación: primero, porque así lo estableció Escrivá. Es parte del carisma fundacional; segundo, en términos de formación doctrinal y espiritual, hay algunas ventajas en tener a hombres y mujeres separados. Las mujeres pueden hablar con tranquilidad de su maternidad, sin que estén los hombres presentes; mientras que estos charlan más comodamente de deportes. Además, la separación permite cierta claridad administrativa, cada sección se ocupa de las actividades de su sección; tercero, los numerarios, son personas que se han comprometido en vivir el celibato, por lo que se puede justificar cierto grado de prudencia en su trato con personas del otro sexo. Evidentemente, como personas normales van a estar en contacto con personas del sexo opuesto a lo largo de la jornada, pero al menos, en sus dependencias y en su vida espiritual privada, tener un cierto grado de separación evita situaciones en las que podrían comprometer su compromiso; y finalmente, el cuarto motivo se centra en una razón histórica: las autoridades eclesiásticas, en especial el Vaticano, desde siempre ha dudado de la posibilidad de un solo cuerpo dentro de la Iglésia católica en la que hombres y mujeres compartieran la misma actividad apostólica y una idéntica vocación. Con los franciscanos y dominicos, por ejemplo, concurrieron comunidades de hombres y mujeres que trataban de vivir el espíritu de san Francisco de Asís y santo Domingo de Guzmán, pero permaneciendo tanto institucional como juridicamente separadas la una de la otra. Ese miedo tradicional provenía del pensamiento de que mezclar ambos sexos en la misma comunidad podría provocar cierta promiscuidad. Pasado el tiempo, Escrivá se vio en la tesitura de que el Opus Dei se dividiera en dos realidades separadas, una para hombres, otra para mujeres, con lo que la unidad orgánica prevista por él se viera destruida. En ese contexto, la única solución viable para aplacar las preocupaciones del Vaticano era levantar un muro de separación, que acabara con cualquier sospecha de promiscuidad.

Para pertenecer al Opus Dei se requiere solicitarlo libremente. La incorporación formal a la Prelatura se realiza mediante una convención bilateral que estipula los compromisos mutuamente asumidos por el interesado y por la propia Prelatura.

El vínculo de los fieles con la Prelatura se establece mediante una declaración de naturaleza moral entre la persona que desea pedir la admisión (previamente solicitada por carta al prelado) y un representante del prelado, ante un testigo. Entre la solicitud por carta de la admisión al prelado y la incorporación jurídica definitiva del aspirante median al menos seis años y medio, a lo largo de los cuales, el aspirante debe renovar su intención anualmente. En caso de no hacerlo, desaparecen las obligaciones mutuas, no devolviéndose en ningún caso las donaciones de bienes o dinero ni compensaciones por el trabajo realizado hasta ese momento.

El vínculo con la Prelatura cesa al terminar el plazo de vigencia del contrato, o antes, si la Prelatura así lo considera o si el interesado lo desea, solicitando dispensa al Prelado. En este último caso, para que se produzca dicho cese del vínculo entre la persona y la Prelatura, el interesado ha de hacer constar su deseo de abandonar voluntariamente la Prelatura. Por lo general, esta voluntad se manifiesta por escrito mediante una carta dirigida al prelado del Opus Dei. Dicha carta se hace llegar al prelado, a quien corresponde otorgar la dispensa de los deberes contraídos. No es necesario que esa carta exponga motivos: basta con que conste una voluntad libre, clara y explícita, de no seguir adelante. La confirmación del cese del vínculo entre la persona y la Prelatura se transmite al interesado, intentando aclarar cualquier aspecto de su nueva situación y ofrecerle, si lo desea, una ayuda espiritual adecuada a sus circunstancias. Generalmente todo eso se desarrolla con cierta celeridad tras la petición de cese del vínculo. Frecuentemente las personas que han abandonado el Opus Dei desean seguir como cooperadores. Pasado un tiempo, puede ocurrir que una persona desee volver y sea admitida como supernumerario, previa autorización del Prelado. En el hipotético caso de que el interesado no solicitara la dispensa, obviamente actuaría ilegitimamente.

Legalmente, por su propia voluntad y en cualquier momento, cualquiera puede abandonar el Opus Dei sin que exista obligación legal alguna de permanencia, pues el compromiso contractual es de índole únicamente moral. En ciertos casos, los bienes donados o testados podrían recuperarse.

Según el fundador del Opus Dei, un cristiano se hace santo a través de dos elementos imprescindibles: la lucha personal por alcanzar el ideal cristiano y la gracia y misericordia de Dios. Para alcanzar el ideal cristiano de "aprender a amar", existen unos medios de santificación. En el Opus Dei, dichos medios se pueden resumir en cuatro aspectos: 1) "vida interior": la vida de contemplación a la que Jesucristo llamó "la única necesaria"; 2) "trabajo": Escrivá defendió que el trabajo no es un castigo de Dios, sino un medio para santificarse y santificar a los demás; 3) "Apostolado": el cristiano no puede reservarse el mensaje recibido para sí mismo, sino que debe comunicarlo a los demás; 4) "formación doctrinal": conocimiento de la doctrina de la Iglesia Católica, que se ve como "religión del Logos" ("logos" =Verbo, razón o conocimiento). Así dice Escrivá que el cristiano tiene que tener "la piedad de los niños y la doctrina segura de los teólogos".

Los medios de formación personal son la charla fraterna o confidencia (que es lo que, en el resto de la Iglesia, se conoce como dirección espiritual propiamente dicha) y la corrección fraterna ("Catecismo del Opus Dei", n 200). El objetivo es ayudar a los fieles a mejorar en su vida interior y en otros aspectos de su vida personal.

Los medios de formación colectiva son: los Círculos Breves o los Círculos de Estudios, los retiros mensuales, los cursos de retiro espiritual, los cursos anuales y las convivencias, las "collationes" mensuales; además de otras clases o charlas, convivencias especiales, etc. ("Catecismo del Opus Dei". n. 201). En ellos se busca profundizar en el conocimiento de la doctrina de la Iglesia y del espíritu del Opus Dei.

La Dirección espiritual es parte importante dentro de la formación que reciben los miembros del Opus Dei. La dirección se brinda mediante la "charla fraterna", que nació como una conversación personal con san Josemaría para un acompañamiento espiritual sobre el espíritu y costumbres de la Obra y comunicar la intimidad, y se busca identificar el propio espíritu con el espíritu de la Obra y mejorar la actividad apostólica personal. Al aumentar el número, pasó a llevarse a cabo semanal o quincenalmente con el director/ra del Centro, u otros miembros; y con los sacerdotes de la Prelatura, sobre todo en la confesión. También forma parte de la dirección espiritual la corrección fraterna. 

En el Opus Dei se practica, al igual que en la Iglesia Católica, la "corrección fraterna", a la que se concede gran importancia como medio de ayudar a los demás a mejorar. Estas correcciones se pueden hacer a todos, incluidos sacerdotes y Directores.

En el caso del Opus Dei, antes de hacer la corrección fraterna se debe consultar al director/a del corregido, y después de hecha, informarlo. Según algunos críticos, esto equivale a delatar al hermano ante los superiores. Según el Opus Dei, se hace para evitar que una persona reciba la misma corrección fraterna varias veces, o que se haga una corrección fraterna que no resulte prudente; no para que el superior conozca los defectos del corregido.

Se requiere un equilibrio para que sea positiva esta costumbre: 1) Exagerando un control sobre "lo correcto" se hace difícil la amistad. 2) Es una costumbre evangélica, manifestación de caridad, si se dicen las cosas que ayuden a mejorar, y también lo bueno de las personas, y será malo corregir sin visión de conjunto, si no se dice lo bueno.

La confesión o sacramento de la penitencia es considerada en el Opus Dei, al igual que en el resto de la Iglesia Católica, como un medio básico para avanzar en el proceso de identificación con Cristo o de santificación. En el caso del Opus Dei sus miembros suelen recibir este sacramento periódicamente cada semana. También es una práctica extendida en el Opus Dei acudir siempre a la confesión con el mismo confesor. Sin embargo, cuentan con total libertad, en especial si es urgente, de acudir con cualquier sacerdote que tenga las licencias para administrar el sacramento de la Penitencia.

John Allen, que muestra una imagen positiva del Opus Dei, lo describe como ""la fuerza más polémica de la Iglesia católica"". La gran mayoría de los obispos y todos los papas aprecian la acción pastoral del Opus Dei. No faltan teólogos que lo consideran signo de contradicción o fuente de controversia. Algunos exmiembros se han mostrado también críticos después de abandonar el Opus Dei.

Entre las críticas se encuentran:













También se critica la actividad personal del fundador y, por ejemplo, se señala que en 1968 Josemaría Escrivá de Balaguer solicita al Gobierno franquista de España que le nombren Marqués de Peralta (título nobiliario que no le correspondía por linaje familiar, pero que le fue concedido ese mismo año). Cuatro años más tarde, y sin haberlo utilizado, cedería este título a su hermano. Un estudio del historiador Ricardo de la Cierva demostró, mediante documentos de su investigación, que la concesión de este nombramiento había sido irregular. La solicitud -según de la Cierva- habría estado motivada por el deseo del Fundador de hacer algo por su familia, que tanto había sufrido, y por estar sinceramente convencido de que le amparaba el derecho a esa reivindicación.

El apoyo prácticamente unánime de la Iglesia al mensaje central de Josemaría Escrivá contrasta con el silencio frente a las novedades que el Opus Dei y su Fundador introducen en lo referente a la vida espiritual: no existe ninguna intervención de dignatarios eclesiásticos en favor de los novedosos modos ascéticos introducidos por el Opus Dei. Ni el hecho de que la dirección espiritual sea llevada principalmente por laicos, ni su dependencia de la estructura de gobierno de la Prelatura, ni la obligación de los miembros de permitir que sus superiores conozcan su intimidad, ni que en la corrección fraterna vaya incluido el deber de informar al superior de los defectos del hermano, nunca han recibido la aprobación ni el rechazo por parte de las autoridades católicas. Sin embargo, algunos responsables católicos han reaccionado con preocupación frente a las denuncias recibidas contra supuestos abusos cometidos por el Opus Dei

El portavoz del Opus Dei en el Reino Unido, Jack Valero, niega todas las acusaciones en contra de la "Obra", aunque admite que algunos pueden haber cometido errores.

En cuanto a las denuncias de exmiembros, Valero explica que le duele que se hayan ido en malos términos y hablen mal del Opus Dei, pero también destaca los casos de personas que abandonaron el grupo y mantienen una buena relación con él. No obstante, aclara que no pone en duda la credibilidad de las personas que cuentan sus malas experiencias.

Sobre las críticas de algunos exmiembros, John L. Allen, Jr. dice que mucho de lo que dicen los críticos lo contradicen muchos otros exmiembros, por el elevado número de miembros presentes y por las personas que participan en las actividades del Opus Dei.

El núcleo del mensaje que transmite el Opus Dei ha sido alabado por distintas personalidades eclesiásticas. Tanto la llamada universal a la santidad y al apostolado como la importancia santificadora del trabajo profesional aparecen en discursos e intervenciones de Obispos, Cardenales y teólogos, incluso en varios documentos de la Iglesia relacionados con el Opus Dei, señalando la novedad de su mensaje. 

Albino Luciani, futuro papa Juan Pablo I escribía en julio de 1978 que la gran aportación del Opus Dei consistía en el desarrollo de una verdadera espiritualidad laical, que resumía en la imagen: ""fe y trabajo hecho con competencia para Escrivá caminan tomados del brazo: son las dos alas de la santidad"". Juan Pablo II dijo que ""el Opus Dei anticipó la teología del estado laical que es una nota característica de la Iglesia del Concilio y después del Concilio"" y describió su fin como ""un gran ideal"". Benedicto XVI, tres años antes de ser papa, cuando dirigía la Congregación para la Doctrina de la Fe dijo que la vida y mensaje de Escrivá son ""un mensaje de grandísima importancia... que lleva a superar la gran tentación de nuestro tiempo —la ficción de que después del 'Big Bang' Dios se retiró de la historia"".
A través de la enseñanza del valor santificador del trabajo, la gente ordinaria ya tiene una ""genuina espiritualidad laical"" para hacerse santos. Según el Cardenal José Saraiva Martins, la "gran originalidad" del mensaje del Opus Dei está en "proclamar sistemáticamente" que:


El papa Benedicto XVI, antes de ocupar dicho cargo, señaló que Escrivá presenta "un Cristo en que el poder y majestad de Dios se hace presente a través de cosas humanas, sencillas y ordinarias". Esperando como un Padre Misericordioso en el Sacramento de Reconciliación y realmente presente en el pan eucarístico, Cristo se hace "totalmente disponible" para alimentar al cristiano de forma que llegue a ser "una sola cosa con él". Con el regalo de esta "divinización" en la gracia, "un nuevo principio de energía," y con el apoyo de "la familia de Cristo", la Iglesia, y un director espiritual bueno, la difícil tarea de ser santo, "es también fácil", dice Escrivá. Y agrega: "Está a nuestro alcance".

La santidad se rehúye, según Ratzinger (2002), porque hay ""un concepto equivocado de la santidad… que estaría reservada para algunos 'grandes'... que son muy diferentes a nosotros, normales pecadores. Pero es una concepción errónea que ha sido corregida precisamente por Josemaría Escrivá"". El santo tiene virtud heroica porque “"ha estado disponible para dejar que Dios actuara. Ser santo no es otra cosa que hablar con Dios como un amigo habla con el amigo, el Único que puede hacer realmente que este mundo sea bueno y feliz".”

Una de las acusaciones frecuentes contra el Opus Dei es calificar a esta institución como una secta religiosa, o que sin serlo hace uso de diversos métodos sectarios.

Los laicistas afirman que la laicidad es un principio indisociable de la democracia, porque las creencias religiosas no son un dogma que deba imponerse a nadie ni convertirse en leyes. La Iglesia (y con ella el Opus Dei) reconoce que "la laicidad, entendida como autonomía de la esfera civil y política de la esfera religiosa y eclesiástica –nunca de la esfera moral–, es un valor adquirido y reconocido por la Iglesia, y pertenece al patrimonio de civilización alcanzado".

Por parte del Opus Dei y de la jerarquía católica se recalca que no es correcto llamar secta a una Prelatura de la Iglesia Católica y que una secta es una organización no reconocida y el Opus Dei sí que está reconocido por la Iglesia.

En el informe de la Asamblea Nacional Francesa sobre las sectas no se menciona al Opus Dei.

Los miembros del Opus Dei se caracterizan por su discreción según sus defensores o por su secretismo según sus detractores. Su fundador explicaba que ""la manera más fácil de entender el Opus Dei es pensar en la vida de los primeros cristianos. Ellos vivían a fondo su vocación cristiana; buscaban seriamente la perfección a la que estaban llamados por el hecho, sencillo y sublime del Bautismo. No se distinguían exteriormente de los demás ciudadanos"."

Como tantas otras organizaciones, el Opus Dei no pone en conocimiento público quién es o deja de ser miembro de la organización, esta información es de carácter privado y deja a la libre elección de cada miembro el reconocimiento de este hecho.

Hasta 1950, el Opus Dei no tuvo un estatuto jurídico pleno dentro de la Iglesia, con la primera constitución. El artículo 191, modificado en una revisión de los estatutos en los años ochenta cuando el Opus Dei fue nombrado Prelatura Personal, en la constitución original rezaba: «"Los miembros numerarios y supernumerarios sepan bien que deberán observar siempre un prudente silencio sobre los nombres de otros asociados y que no deberán revelar nunca a nadie que aquellos pertenecen al Opus Dei"». Aquella falta de publicidad dio una imagen de secretismo que continúa hasta la actualidad, pese a ser públicos los estatutos y constituciones del Opus Dei.

Esto ha tendido a crear la sospecha que el Opus Dei funciona como una sociedad secreta y, hasta entrados los años 1980, ha sido prácticamente imposible, no ya por la gente común, sino incluso por los clérigos y, según algunos , por muchos de los miembros conocer íntegramente las Constituciones y reglamentos de la asociación.

Basándose en los reportajes de España, en los años 40, el Superior General de la Sociedad de Jesús, Wlodimir Ledochowski (1866-1942), dijo a la Santa Sede que consideraba al Opus Dei como "muy peligroso para la Iglesia de España". Y le achacó el tener "un carácter secreto" además de que había "señales de una inclinación para dominar el mundo a través de una forma de sociedad secreta cristiana". Según Andrés Vázquez de Prada, miembro del Opus Dei (1997), Peter Berglar (1994), los periodistas católicos Vittorio Messori (1997) y John Allen (2005) esta controversia inicial, que procedía de círculos eclesiásticos muy respetados (la "oposición de los buenos", según Escrivá) será la primera raíz de las acusaciones posteriores a lo largo y ancho del mundo: que es una sociedad secreta, peligrosa e inclinada al poder y al dinero. Estas acusaciones han sido rebatidas tanto por el fundador como por sus sucesores. 

A este respecto, el Parlamento italiano investigó al Opus Dei en 1986 y concluyó que no era una sociedad secreta. Los Tribunales alemanes, por su parte, han indicado que el Opus Dei no está autorizado a publicar listas, pues la pertenencia es un asunto que forma parte de la esfera privada que se debe respetar.

En la labor de enseñar su mensaje, el Opus Dei encontró controversias y rechazos por parte de numerosos detractores, incluidos algunos obispos. El Cardenal Julián Herranz, miembro del Opus Dei, dijo que "Opus Dei fue víctima de la cristianofobia".

El papa Emérito Benedicto XVI, cuando era cardenal dijo que el Opus Dei es "la unión sorprendente de absoluta fidelidad a la tradición y fe de la Iglesia, y la apertura incondicional a todos los retos de este mundo". Sin embargo, el Opus Dei ha sido criticado por promover una visión demasiado ortodoxa (preconciliar) de la fe católica. Los críticos dicen que el Opus Dei logró acercarse más a la cúpula de la Iglesia católica gracias al papa Juan Pablo II, para lograr convertirse en una "iglesia dentro de la iglesia", siendo empleada como una "fuerza de choque" por la necesidad de llevar a cabo una "nueva evangelización" con principios ultraconservadores y reaccionarios. De otra parte, sus partidarios dicen que este término "conservador" está mal aplicado a nociones religiosas, morales e intelectuales. Sin embargo, otros dicen que el término es lo bastante amplio como para aludir a actitudes de conservadurismo en general, no exclusivamente en el campo político.

El segundo prelado, Javier Echevarría, dijo en una ocasión que "si se emplea la palabra "conservador" fuera del contexto político, se podría decir que toda la Iglesia es "conservadora", porque conserva y transmite el Evangelio de Cristo, los sacramentos, el tesoro de la vida de los santos, sus obras de caridad. Por razones análogas, toda la Iglesia es "progresista", porque mira al futuro, cree en los jóvenes, no busca privilegios, está cerca de los pobres y de los necesitados. O sea, el Opus Dei es conservador y progresista como lo es toda la Iglesia, ni más ni menos".

Escrivá también dice que "La religión es la más grande rebelión de hombres que no quieren vivir como bestias".

En los años 1950 y 1960, el jefe de Estado y dictador español Francisco Franco designó a varios miembros del "Opus Dei" como ministros y altos cargos dentro del régimen. Estos ministros, conocidos entonces como los ""tecnócratas"", generalmente son reconocidos por haber introducido en la dictadura de Franco una ideología capitalista-liberal, modernizando también la economía española que contrastó con las influencias falangistas, carlistas y militares anteriores. Este hecho hizo que en su momento se propagase la idea del apoyo del "Opus Dei" al régimen de Franco y viceversa. El historiador e hispanista inglés Paul Preston afirma (1993) que Franco los designó como ministros por su habilidad técnica y no por su pertenencia al "Opus Dei".

Sobre la acusación de que el "Opus Dei" fue una especie de partido político en el gobierno de Franco, Messori dice que ésta es una "leyenda negra" que la Falange española y algunos clérigos han propagado y alegan que el régimen franquista persiguió igualmente a algunos miembros del Opus Dei. No obstante, según el historiador Ricardo de la Cierva: "La equiparación de miembros del Opus Dei en el poder de Franco y en la oposición es falsa. Estaban en su inmensa mayoría con el poder; iniciaron una corriente de oposición muy minoritaria entre ellos mismos ya muy al final del régimen, por medio del profesor Calvo Serer, que durante décadas había sido un ardiente partidario de Franco y su régimen como Antonio Fontán y Rafael Calvo Serer.

En tiempos más recientes, durante la etapa del gobierno del español Partido Popular (1996-2004), algunos miembros del Opus Dei, como Federico Trillo o Isabel Tocino, fueron designados ministros por el entonces líder de ese partido, José María Aznar. De la misma forma, el exfiscal general del Estado Jesús Cardenal es miembro de la prelatura. Otro miembro que también ocupó un alto cargo fue Juan Cotino como director general de la Policía Nacional española. Dentro del nacionalismo vasco, sosteniendo una postura ideológica contraria a los antes mencionados, Rafael Larreina de Eusko Alkartasuna, parlamentario en el Congreso de los Diputados, pertenece al Opus Dei.

En cualquier caso, John Allen constata que, si bien el Opus Dei, desde el punto de vista institucional, "no tiene una postura política oficial", hay pocas dudas de que muchos de sus miembros son políticamente conservadores al igual que la mayoría de los católicos españoles dentro de la dinámica que mantienen Partido Popular y Partido Socialista Obrero Español en España a finales del siglo XX e inicios del XXI.

También se da hoy en día cierta presencia de algunos de sus miembros y simpatizantes en élites financieras y políticas, sobre todo en las de tendencia católica conservadora. Ha recibido el apoyo de diversos líderes políticos y empresariales<noinclude> como Lech Wałęsa de Polonia, Corazón Aquino de Filipinas, Ruth Kelly del Reino Unido, Raymond Barre de Francia, Charles Malik, expresidente de la Asamblea General de la Organización de las Naciones Unidas; éstos son algunos de los personajes que consideran como positiva la influencia del Opus Dei en el mundo.

Los miembros del Opus Dei remarcan que la institución tiene una finalidad únicamente espiritual y que cada miembro asume sus responsabilidades profesionales en el mundo de la política o los negocios, sin hacer partícipes de ellas a los demás miembros y menos aún a la institución. Escrivá decía que los fieles del Opus Dei podían tener la postura política que quisieran, siempre y cuando no entrara en contradicción con la doctrina católica.

Las posiciones opuestas se reflejan en cómo se interprete el punto 353 del libro "Camino" de Escrivá:

Alberto Moncada, un exmiembro crítico, sugirió que quizás la presunta búsqueda de influencia del Opus Dei en la sociedad se canalice a través de sus colegios y universidades.

Los críticos dicen también que los miembros del Opus Dei no serían libres en materias políticas, ya que seguirían una ideología de tipo "nacional-católico" y, según éstos, los miembros del Opus Dei estarían en la derecha política, impulsando una influencia conservadora en el mundo, promoviendo las políticas más tradicionales de la Iglesia católica. De acuerdo con los portavoces de "Opus Dei", esto no probaría la relación del Opus Dei con la política, sino la actividad política de algunos de sus miembros.

Según datos del Anuario Pontificiocorrespondiente al año 2017, pertenecen a la Prelatura un total de 93.203 fieles. En cuanto a la evolución del número de miembros, el Opus Dei mantiene un leve crecimiento numérico desde hace varios lustros, sobre todo en Europa. Desde 1990 ha habido aproximadamente un 4% de incremento en su número, mientras que en los años 1960 y 1970 habían aumentado sus miembros en más de un 45%. Probablemente esto puede atribuirse a la progresiva secularización de aquellos países donde tradicionalmente se había asentado en primer lugar, como España, Italia y Portugal y a un bajo índice de penetración en el resto de países europeos. Y en América Latina, debido en parte al fenómeno de la expansión de las iglesias protestantes, que en Brasil, por ejemplo, llegan a copar más del 20% de una población, antes casi enteramente católica. Su expansión es actualmente algo mayor en los países del antiguo bloque comunista, especialmente Polonia, patria del papa Juan Pablo II (en Polonia con 38.187.488 habitantes hay unos 450 miembros del Opus Dei), en los cuales, hasta la caída del Telón de Acero, el Opus Dei como organización no tenía presencia oficial, así como en otros de Asia, como Filipinas, en donde está el grupo más numeroso de este continente.

La distribución por continentes de los miembros, según datos del Anuario Pontificio 2017, es el siguiente siguiente:

Según Messori, en cuanto al nivel socioeconómico, lo predominante en el Opus Dei es la gente de los niveles medios y bajos y afirma que en Latinoamérica, por ejemplo, el Opus Dei es popular entre los campesinos. Gómez Pérez dice que la composición social del Opus Dei corresponde a la situación local y que hay más profesores entre los miembros, ya que el Opus Dei pone énfasis en el proselitismo entre intelectuales.

La Obra, con aportaciones económicas de distintas fuentes (donaciones de simpatizantes y el sueldo de los miembros numerarios), sostiene escuelas, institutos y varias universidades, y abre nuevos centros, ya que en el aspecto académico, por el prestigio y calidad técnica de sus centros de enseñanza, tiene actualmente una importante demanda social. Ejemplos de lo anterior sería la Universidad de Navarra y la Clínica Universidad de Navarra, con el IESE con sedes en Barcelona y Pamplona (Navarra). Otros ejemplos de esto son la Universidad de Piura, la Universidad de los Andes (Chile), la Universidad de La Sabana (Colombia), la Universidad Austral, el IAE Business School y el Hospital Austral, con sedes en Buenos Aires, Pilar y Rosario, Argentina, la Universidad Panamericana (Ciudad de México y Guadalajara), el IPADE, Universidad Monteávila en Caracas, Venezuela.

En su estudio de 2005, Allen dice que hay 608 proyectos en distinto grado de ejecución, promovidos por los laicos y sacerdotes de la Obra: de éstos, 41% son colegios, 26% son escuelas técnicas y agrícolas, 27% son residencias universitarias y el 6% son 18 universidades, 12 escuelas de negocios y 8 hospitales.




</doc>
<doc id="5383" url="https://es.wikipedia.org/wiki?curid=5383" title="Prospecto">
Prospecto

Un prospecto es la información escrita dirigida al consumidor o usuario, que acompaña al medicamento. Para la elaboración de este documento deben seguirse ciertas normas, elaboradas por la Agencia Europea de Medicamentos (EMA, siglas de la European Medicines Agency) de la Unión Europea.

En el prospecto figuran:



</doc>
<doc id="5384" url="https://es.wikipedia.org/wiki?curid=5384" title="Fármaco">
Fármaco

Un fármaco es una molécula bioactiva que en virtud de su estructura y configuración química puede interactuar con macromoléculas proteicas, generalmente denominadas receptores, localizadas en la membrana, citoplasma o núcleo de una célula, dando lugar a una acción y un efecto evidenciable.

Las enzimas también se consideran receptores catalíticos, pues están en condiciones de interactuar con ligandos. En este caso los fármacos (agonistas), en esa unión fármaco-receptor, intervienen casi siempre uniones supramoleculares, es decir, no de carácter covalente de alta energía. (alrededor de 60 kcal mol), sino más bien uniones más débiles y reversibles como hidrofóbicas, de Van der Waals o puentes de hidrógeno.

Modernamente en el diseño de nuevos fármacos se utilizan descriptores, que categorizan una molécula por aspectos electrónicos, geométricos, cuánticos, termodinámicos y de conectividad, eso viabiliza la utilización de herramientas informáticas en el diseño de estructuras referenciales o cabezas de serie.

Cuando el fármaco, que es el principio activo, se lo presenta como una forma farmacéutica determinada, se lo denomina medicamento; aquí ya se incluyen contingentes tecnológicos de fabricación, que determinarán una biodisponibilidad y estabilidad adecuada de esa presentación, es decir buena absorción en un lapso de tiempo, y no degradación química o físico-química que afecten su funcionamiento en un organismo vivo, es decir sin menoscabar una adecuada absorción, pasen de la fase biofarmacéutica a la fase farmacocinética que determina la llegada exitosa de una molécula bioactiva a la biofase o sitio de acción, en niveles de concentración que garanticen un efecto.

Hoy el tremendo avance en proteonómica y las consiguientes alteraciones que pueden sufrir las proteínas en sus estructuras terciarias principalmente, abren nuevos y sugestivos caminos en la investigación de moléculas bioactivas para combatir peligrosos agentes infecciosos, como virus o bacterias, y el cáncer.

Esta definición se acota a aquellas sustancias de interés clínico, es decir aquellas usadas para la prevención, diagnóstico, tratamiento, mitigación y cura de enfermedades, y se prefiere el nombre de tóxico para aquellas sustancias no destinadas al uso clínico pero que pueden ser absorbidas accidental o intencionalmente; y droga para aquellas sustancias de uso social que se utilizan para modificar estados del ánimo.

Los fármacos pueden ser sustancias creadas por el hombre o producidas por otros organismos y utilizadas por aquel. De esta forma, hormonas, anticuerpos, interleucinas y vacunas son considerados fármacos al ser administrados en forma farmacéutica. En resumen, para que una sustancia biológicamente activa se clasifique como fármaco, debe administrarse al cuerpo de manera exógena y con fines médicos.

Los fármacos se expenden y utilizan principalmente en la forma de medicamentos, los cuales contienen el o los fármacos prescritos por un facultativo

La palabra "fármaco" procede del griego "phármakon", que se utilizaba para nombrar tanto a las drogas como a los medicamentos. El término "phármakon" tenía variados significados, que incluían: "remedio", "cura", "veneno", "antídoto", "droga", "receta", "colorante artificial", "pintura", etc.

Los fármacos pueden ser sustancias idénticas a las producidas por el organismo (por ejemplo, las hormonas obtenidas por ingeniería genética) o sustancias químicas sintetizadas industrialmente que no existen en la naturaleza, pero que tienen zonas análogas en su estructura molecular y que provocan un cambio en la actividad de las células.

Históricamente, se ha entendido como sustancia medicinal, independiente de su origen o elaboración, cualquier producto consumible al que se le atribuyen efectos beneficiosos en el ser humano. Estas sustancias medicinales, tal como los medicamentos actuales, estaban constituidas por uno o varios fármacos, que se denominan principio activo o sustancia activa de dichas sustancias, para diferenciarlos de los elementos no medicinales que las componen.




Además de la denominación química de un fármaco, los fabricantes de productos farmacéuticos, en conjunto con instituciones científicas y académicas, le asignan un nombre oficial internacional, la "Denominación Común Internacional" del fármaco. Sin embargo, muchas veces el fabricante lo comercializa con un nombre patentado (o comercial), que puede variar entre distintas naciones, lo cual ha generado una gran confusión respecto de los nombres de los fármacos y medicamentos.

Para resolver esto, las distintas legislaciones han previsto diversos sistemas de control de los nombres de los fármacos y los medicamentos que se expenden.

Se entiende por "medicamento" el estado bajo el cual se presenta un fármaco para su uso práctico para la consideración del máximo beneficio terapéutico para el individuo y minimizando los efectos secundarios indeseables.

Un medicamento es la suma de una forma farmacéutica y su acondicionamiento (envasado, etiquetado, estuchado, prospecto).

El acondicionamiento primario es aquel envase o cualquier otra forma de acondicionamiento que se encuentre en contacto directo con el fármaco o forma farmacéutica (blíster, tubo, frasco, etc). El acondicionamiento secundario es el embalaje exterior en el que se encuentra el acondicionamiento primario (estuche, caja, prospecto, etc)

Las formas farmacéuticas son los principios activos más los excipientes. Son un producto semiterminado en presentación:





Los nombres comerciales de los medicamentos varían en muchos países aun cuando posean el mismo fármaco; por eso se recurre a utilizar el nombre del medicamento acompañado del nombre del fármaco.

Los fármacos pueden ser sintetizados o extraídos de un organismo vivo, en este último caso, el fármaco debe ser purificado y/o modificado químicamente, antes de ser considerado como tal.

La actividad de un fármaco varía debido a la naturaleza de estos, pero siempre está relacionada con la cantidad ingerida o absorbida. Por ejemplo, los medicamentos oncológicos, que curan el cáncer, son conocidos como ingredientes activos de gran potencia ("high potent active ingredients") y se usan en concentraciones muy pequeñas para curar algún tipo especial de cáncer. Cada uno de ellos causa múltiples efectos secundarios y la sobredosis puede afectar negativamente a las células sanas; tal es el caso del oxaliplatino, el letrozol, el cisplatino, el anastrozol, etc.


Son productos fitosanitarios a base de hierbas. El ingrediente activo puede ser el resultado de la interacción de una variedad de componentes que actúan tanto sobre un agente patógeno como sobre una variedad de sistemas del cuerpo que participan en la inmunidad.

El ingrediente farmacéutico activo puede ser desconocido o pueden existir cofactores, a fin de lograr los objetivos terapéuticos. Una manera como los fabricantes lo han tratado de indicar es la normalización de un marcador compuesto. Sin embargo la normalización no se ha estandarizado aún: las diferentes empresas utilizan diferentes marcadores, o diferentes niveles de los marcadores de la misma, o diferentes métodos de ensayo para los compuestos marcador.

Por ejemplo, la hierba de San Juan es a menudo normalizada a la hipericina que ahora se sabe que no es el "ingrediente activo" para el uso de antidepresivos. Otras empresas lo normalizan a hyperforin o a ambos, aunque puede haber unos 24 prinvipios activos conocidos posibles. Muchos herbolarios creen que el ingrediente activo en una planta es la planta en sí.

No se conoce del todo bien el riesgo de este tipo de contaminación pero se ha empezado a investigar en los últimos años los efectos y la manera de eliminarlos.

Analgésicos, antiinflamatorios, anticonceptivos, antibióticos y demás productos que llenan los botiquines y que son tomados con cierta asiduidad acaban en las aguas de los ríos, lagos, etc.

Las cantidades registradas por ahora no suponen un riesgo para la salud humana pero es preocupante el riesgo de interacción de las diferentes sustancias.

Cuando se ingiere un fármaco, gran parte del compuesto activo es excretado a través de la orina y de las heces, acabando en las aguas residuales que llegan a las depuradoras para ser tratadas. El problema es que los tratamientos en estas plantas de depuración no son suficientes para extraer los residuos farmacológicos en su totalidad, por lo que acaban viajando hasta los ríos, lagos, mares, acuíferos y al final aunque en cantidades pequeñas terminan en nuestros grifos.

Se calcula que en las aguas residuales se hallan más de 20 fármacos de composición variable según países.

En las aguas residuales de España se detectaron fármacos como los reguladores del colesterol ácido clofíbrico y gemfibrozil, los analgésicos naproxeno y diclofenaco, el antiinflamatorio ibuprofeno, el antiepiléptico carbamazepina y el beta-bloqueante atenolol.

El estudio releva que hasta la cafeína que se toma con el café puede acabar en estas aguas. Se han detectado cantidades de un desinfectante antibacteriano como el triclosán, que se incorpora a muchos detergentes y que es preocupante ya que podría generar resistencias en las bacterias. La concentración de estos residuos no supone un riesgo para la salud humana, lo que sí es preocupante es que en cada litro puede haber gran cantidad de fármacos que incluso pueden interaccionar entre ellos.

Otro efecto muy importante es el efecto acumulativo en los ecosistemas. Un temor fundado es que la exposición constante de los microorganismos del ecosistema a los antimicrobianos pueda generar patógenos resistentes a estos fármacos poniendo en peligro el tratamiento de futuras infecciones.

No menos importantes son los efectos de las píldoras anticonceptivas y tratamientos hormonales que siguen activos al llegar al medio ambiente y pueden alterar el sistema endocrino de los organismos.

Se estudian métodos para la eliminación los restos de fármacos en agua, entre ellos el tratamiento con ozono.

Una medida importante a tener en cuenta sería no derrochar el agua para así hacer llegar a las depuradoras un menor volumen de agua para que se pueda tratar con eficacia.



</doc>
<doc id="5386" url="https://es.wikipedia.org/wiki?curid=5386" title="Excipiente">
Excipiente

En farmacéutica, un excipiente es una sustancia inactiva usada para incorporar el principio activo. Además pueden usarse para ayudar al proceso de fabricación de un producto.

En general, las sustancias activas por sí mismas no pueden ser absorbidas fácilmente por el cuerpo humano; es necesario administrarlas en la forma apropiada; por lo tanto la sustancia debe ser disuelta o mezclada con una sustancia excipiente, si es sólido o blando; o un vehículo si es líquido. Además pueden usarse para ayudar al proceso de fabricación de un producto. 

Dependiendo de la vía de administración es posible usar distintos excipientes. 
Además, cuando un ingrediente activo ha sido purificado, muchas veces no puede permanecer así por mucho tiempo; otro uso de los excipientes es como estabilizadores que aseguran la activación del ingrediente activo lo suficiente como para hacer competitivo el producto. El uso más común y recomendado para la toma es el agua pura, disolvente universal.










</doc>
<doc id="5387" url="https://es.wikipedia.org/wiki?curid=5387" title="Medicamento">
Medicamento

Un medicamento es uno o más fármacos integrados en una forma farmacéutica, presentado para expendio y uso industrial o clínico, y destinado para su utilización en personas o en animales, dotado de propiedades que permiten el mejor efecto farmacológico de sus componentes con el fin de prevenir, aliviar o mejorar el estado de salud de las personas enfermas, o para modificar estados fisiológicos.

Desde las más antiguas civilizaciones el hombre ha utilizado como forma de alcanzar mejoría en distintas enfermedades productos de origen vegetal, mineral, animal o en los últimos tiempos sintéticos.El cuidado de la salud estaba en manos de personas que ejercen la doble función de médicos y farmacéuticos. Son en realidad médicos que preparan sus propios remedios curativos, llegando alguno de ellos a alcanzar un gran renombre en su época, como es el caso del griego Galeno (130-200 d.C.). De él proviene el nombre de la Galénica, como la forma adecuada de preparar, dosificar y administrar los fármacos. En la cultura romana existían numerosas formas de administrar las sustancias utilizadas para curar enfermedades. Así, se utilizaban los electuarios como una mezcla de varios polvos de hierbas y raíces medicinales a los que se les añadía una porción de miel fresca. 

La miel además de ser la sustancia que sirve como vehículo de los principios activos, daba mejor sabor al preparado. En ocasiones se usaba azúcar. También se utilizaba un jarabe, el cual ya contenía azúcar disuelta, en vez de agua y el conjunto se preparaba formando una masa pastosa. Precisamente Galeno hizo famosa la gran triaca a la que dedicó una obra completa, y que consistía en un electuario que llegaba a contener más de 60 principios activos diferentes. Por la importancia de Galeno en la Edad Media, se hizo muy popular durante esta época dejando de estar autorizada para su uso en España en pleno siglo XX.

Es precisamente en la Edad Media donde comienza su actividad el farmacéutico separado del médico. En su botica realiza sus preparaciones magistrales, entendidas como la preparación individualizada para cada paciente de los remedios prescritos, y se agrupan en gremios junto a los médicos. En el renacimiento se va produciendo una separación más clara de la actividad farmacéutica frente a médicos, cirujanos y especieros, mientras que se va produciendo una revolución en el conocimiento farmacéutico que se consolida como ciencia en la edad moderna. La formulación magistral es la base de la actividad farmacéutica conjuntamente con la formulación oficinal, debido al nacimiento y proliferación de farmacopeas y formularios, y esta situación continúa hasta la segunda mitad del siglo XIX.

A partir de este momento empiezan a aparecer los específicos, que consistían en medicamentos preparados industrialmente por laboratorios farmacéuticos. Es así, que las formas galénicas no adquirirán verdadero protagonismo hasta alrededor de 1940, cuando la industria farmacéutica se desarrolla y éstas comienzan a fabricarse en grandes cantidades.
Desde entonces hasta hoy en día las maneras en que se presentan los medicamentos han evolucionado y la diversidad que encontramos en el mercado es muy amplia.

Forma galénica o forma farmacéutica es la disposición individualizada a que se adaptan los fármacos (principios activos) y excipientes (materia farmacológicamente inactiva) para constituir un medicamento.
O dicho de otra forma, la disposición externa que se da a las sustancias medicamentosas para facilitar su administración.

El primer objetivo de las formas galénicas es normalizar la dosis de un medicamento, por ello, también se las conoce como unidades posológicas. Al principio, se elaboraron para poder establecer unidades que tuvieran una dosis fija de un fármaco con el que se pudiera tratar una determinada patología.

La importancia de la forma farmacéutica reside en que determina la eficacia del medicamento, ya sea liberando el principio activo de manera lenta, o en su lugar de mayor eficiencia en el tejido diana, evitar daños al paciente por interacción química, solubilizar sustancias insolubles, mejorar sabores, mejorar aspecto, etc.

Los medicamentos se dividen en cinco grupos:






Además, pueden recibir algunos calificativos específicos como:

En España y algunos países latinoamericanos, los medicamentos se dispensan, distribuyen o venden exclusivamente en las farmacias. Existen dos tipos de medicamentos según la prescripción médica:


Para prescribir una receta:
ARTÍCULO 28. La receta médica es el documento que contiene, entre otros elementos, la prescripción de uno o varios medicamentos y podrá ser emitida por:

I. Médicos;

II. Homeópatas;

III. Cirujanos dentistas;

IV. Médicos veterinarios, en el área de su competencia;

V. Pasantes en servicio social, de cualquiera de las carreras anteriores, y

VI. Enfermeras y parteras.

Los profesionales a que se refiere el presente artículo deberán contar con cédula profesional expedida por las autoridades educativas competentes. Los pasantes, enfermeras y parteras podrán prescribir ajustándose a las especificaciones que determine la Secretaría.

ARTÍCULO 29. La receta médica deberá contener impreso el nombre y el domicilio completos y el número de cédula profesional de quien prescribe, así como llevar la fecha y la firma autógrafa del emisor.

ARTÍCULO 30. El emisor de la receta al prescribir, indicará la dosis, presentación, vía de administración, frecuencia y tiempo de duración del tratamiento.

ARTÍCULO 31. El emisor de la receta prescribirá los medicamentos de conformidad con lo siguiente:

I. Cuando se trate de los incluidos en el Catálogo de Medicamentos Genéricos Intercambiables a que hace referencia el artículo 75 de este ordenamiento, deberá anotar la Denominación Genérica y, si lo desea, podrá indicar la Denominación Distintiva de su preferencia, y

II. En el caso de los que no estén incluidos en el Catálogo referido en la fracción anterior, podrá indistintamente expresar la Denominación Distintiva o conjuntamente las Denominaciones Genérica y Distintiva.

Cuando en la receta se exprese la Denominación Distintiva del medicamento, su venta o suministro deberá ajustarse precisamente a esta denominación y sólo podrá sustituirse cuando lo autorice expresamente quien lo prescribe.

La patente no se limita a la molécula, sino también a la formulación, mecanismo de producción, o asociación con otras moléculas. Mediante sucesión de patentes las casas farmacéuticas consiguen prolongar el periodo de exclusividad de sus presentaciones comerciales, aun cuando presentaciones anteriores de la misma molécula hayan quedado libres.
Pueden ser libremente producidas por otros laboratorios y suelen conllevar un menor precio. Las distintas Agencias del medicamento y organizaciones reguladores nacionales aseguran las similares bioequivalencia y biodisponibilidad de los medicamentos genéricos frente a aquellos que les son referencia.

Existen numerosas formas de clasificar las formas galénicas, según el factor que tengamos en cuenta: su estado físico, la vía de administración, el origen de sus componentes, etcétera. No obstante la más utilizada y la más útil desde el punto de vista de la medicina es la clasificación según la vía de administración que usen.

La mayor parte de los fármacos administrados vía oral buscan una acción sistémica, tras un proceso previo de absorción entérica. En la absorción oral intervienen factores dependientes del individuo y otros dependientes de los fármacos que van a influir en la mayor o menor eficacia del fármaco administrado. Así mismo, la vía oral es motivo frecuente de interacciones farmacológicas, artículo este que aconsejamos consultar para conocer la importancia de factores como el pH, toma o no de alimentos, tipo de éstos, velocidad del tránsito intestinal, u otros muchos que pueden influir en la absorción de un fármaco.
Una forma especial de administración oral es la vía sublingual. En esta vía normalmente, se utilizan comprimidos que se disuelven debajo de la lengua absorbiéndose directamente. Tiene el inconveniente de ser exclusivamente permeable al paso de sustancias no iónicas, muy liposolubles. Esto hace que sólo puedan administrarse por esta vía fármacos de gran potencia terapéutica como la nitroglicerina o el isosorbide.
Se utiliza para conseguir una acción terapéutica rápida o para fármacos que posean un alto grado de metabolización hepática, se degraden por el jugo gástrico o no sean absorbidos por vía oral. No obstante también se encuentran en el mercado presentaciones por comodidad del usuario. (Véase la formulación Flas en el epígrafe de innovaciones galénicas).

La biodisponibilidad de un fármaco administrado vía parenteral depende de sus características fisicoquímicas, de la forma farmacéutica y de las características anatomofisiológicas de la zona de inyección:

Proporciona un efecto rápido del fármaco y una dosificación precisa, sin problemas de biodisponibilidad. Puede presentar, no obstante, graves inconvenientes, como la aparición de tromboflebitis, así como problemas de incompatibilidades entre dos principios activos administrados conjuntamente en la misma vía. Tiene el inconveniente de que no permite la administración de preparados oleosos debido a la posibilidad de originar una embolia grasa. Tampoco podrán usarse productos que contengan componentes capaces de precipitar algún componente sanguíneo o hemolizar los hematíes.

Utilizada en el tratamiento quimioterápico de determinados cánceres; permite obtener una máxima concentración del fármaco en la zona tumoral, con unos mínimos efectos sistémicos.

Se utiliza para fármacos no absorbibles por vía oral o ante la imposibilidad de administración del fármaco al paciente por otra vía ya que admite el ser utilizada para sustancias irritantes. Numerosos factores van a influir en la biodisponibilidad del fármaco por vía IM (vascularización de la zona de inyección, grado de ionización y liposolubilidad del fármaco, volumen de inyección, etc.). Esta vía es muy utilizada para la administración de preparados de absorción lenta y prolongada (preparados “depot”).

De características similares a la anterior pero al ser la piel una zona menos vascularizada, la velocidad de absorción es mucho menor. Sin embargo, dicha velocidad puede ser incrementada o disminuida por distintos medios. No puede utilizarse para sustancias irritantes ya que podría producir necrosis del tejido.

De uso menos frecuente son la intradérmica, la intraaracnoidea o intratecal, epidural, intradural, intraósea, intraarticular, peritoneal o la intracardiaca.

Los preparados para administración parenteral son formulaciones estériles destinadas a
ser inyectadas o implantadas en el cuerpo humano. A continuación se enumeran cinco de las más
representativas:

Por vía parenteral es posible la liberación retardada o prolongada de los principios activos a partir del punto de inyección.
Esto se puede conseguir realizando diversas manipulaciones galénicas. Una de ellas consiste en
sustituir una solución acuosa por una oleosa, en el caso de que el principio activo sea
liposoluble. El método más clásico consiste en inyectar derivados poco hidrosolubles del
principio activo, en forma de suspensiones amorfas o cristalinas (p. e., preparaciones retard de insulina). A veces, el principio activo puede también adsorberse sobre un soporte inerte desde el que será liberado, o bien fijarse en forma de microcápsulas, o incorporarse en liposomas para vectorizar algunos fármacos e, incluso ser tratado químicamente (profármaco) a fin de modificar sus propiedades fisicoquímicas.

Consisten en la inmersión de todo o parte del cuerpo en una solución acuosa a la que se añaden determinados productos. Los más utilizados son los "baños coloidales", que tanto tibios como calientes actúan como sedantes y antipruriginosos. En su preparación se mezcla una taza de almidón en un litro de agua y posteriormente esta preparación se echa al agua del baño. En los baños oleosos se sustituye el almidón por aceites fácilmente dispersables, produciendo una suspensión homogénea. Actualmente se utilizan sobre todo los baños de sales, potenciados por el mundo de la cosmética.

A pesar de lo comentado respecto a la dificultad para la absorción de los principios activos, siempre existe la posibilidad de que se absorba una parte de los mismos a través de la piel. Este fenómeno se da con más intensidad en las zonas donde la piel está menos queratinizada (cara), donde se favorece la humedad (pliegues corporales) o en pieles más sensibles como la de los niños.
Por otra parte, en ocasiones, sí que nos interesa que el principio activo sea absorbido para hacer su efecto a nivel sistémico. Esta vía, se conoce como vía percutánea o vía transdérmica y presenta unas características especiales.

Los sistemas terapéuticos transdérmicos (STT) son formas de dosificación ideados para conseguir el aporte percutáneo de principios activos a una velocidad programada, o durante un periodo de tiempo establecido. Existen varios tipos de sistemas transdérmicos, entre los que se encuentran:

Forma galénica consistente en un reservorio con principio activo que se libera lentamente al aplicarlo sobre la piel. Se persigue que el fármaco pase a la circulación sistémica a través de la piel y no la actividad del fármaco en la propia piel. Estos parches proporcionan niveles plasmáticos terapéuticos constantes del fármaco, siempre que la piel permanezca intacta. Aunque relativamente recientes, tienen numerosas indicaciones y son objeto de continua investigación.

Técnica utilizada en fisioterapia que consiste en la introducción de un fármaco o principio activo (mediante la utilización de corriente galvánica u otras formas de corrientes derivadas de esta) a través de la piel.

Consiste en la colocación sobre la piel de dos electrodos que, por su polaridad, hacen que un fármaco cargado iónicamente atraviese la piel. Se produce por la interacción de la polaridad del electrodo con la carga iónica de la sustancia elegida (mediante el rechazo de los iones en el polo del mismo signo).

Las vías de introducción a través de la piel son sobre todo: folículos pilosos, glándulas sudoríparas y sebáceas, por interponer menor resistencia para el paso de sustancias.

De esta manera son administrados por vía percutánea fármacos cargados eléctricamente, sin producir efectos generales en el organismo, ya que sus efectos son locales.


Su uso por vía inhalada nos orienta hacia las patologías que se beneficiarían del tratamiento con los aerosoles:

Los medicamentos, tanto recetados como de venta libre, pueden contener gluten, harinas u otros productos derivados, hecho que es importante tener en cuenta en personas que padecen algún trastorno relacionado con el gluten.

En España, la legislación obliga a declarar en el prospecto de los medicamentos la presencia de gluten, cuando ha sido añadido como excipiente de forma intencionada. No obstante, no es posible conocer si un medicamento contiene trazas de gluten, las cuales suponen un riesgo para las personas celíacas y deben ser evitadas. Es probable que en este caso las trazas sean tan pequeñas que no supongan ningún riesgo, pero algunos autores consideran que para garantizar el uso seguro de los medicamentos deberían ser especificadas en el prospecto, el etiquetado y la ficha técnica.

La legislación actual en España, y en otros países como Estados Unidos, no exige a los fabricantes el análisis ni la declaración de las trazas de gluten derivadas de residuos durante el proceso de producción de los principios activos o excipientes. Asimismo, los excipientes y los procesos de fabricación pueden cambiar o no estar completamente descritos.

La legislación actual solo exige analizar si hay gluten cuando se utiliza almidón de trigo, avena, cebada o centeno como excipiente. En el caso de que se hayan empleado almidones de maíz, patata, arroz y sus derivados, solo obliga a analizar que no existan almidones de otro origen y especificar en el prospecto, el etiquetado y la ficha técnica del medicamento el almidón y la planta de la que procede.

Solo algunos fabricantes realizan controles del producto final y declaran voluntariamente que sus productos son libres de trazas de gluten. Las conclusiones del Grupo de Trabajo de Nutrición del Colegio Oficial de Farmacéuticos de Bizkaia son que ningún laboratorio verifica analíticamente el contenido de gluten de los medicamentos genéricos que elaboran, aunque aseguran que cumplen la legislación vigente.

Generalmente, los profesionales de la salud ignoran las fuentes potenciales de gluten y los farmacéuticos no siempre disponen del tiempo necesario ni los recursos de información adecuados. Por lo tanto, se convierte en labor de los pacientes asegurarse de si sus medicamentos son o no son libres de trazas de gluten. Para aclarar las dudas y recabar la información precisa, el procedimiento indicado es realizar una consulta directamente con el laboratorio.

Según el artículo 34 del Real Decreto 1345/2007 y la Circular 02/2008 de la Agencia Española de Medicamentos y Productos Sanitarios, cuando un medicamento contenga como excipiente almidón de trigo, avena, cebada, centeno o cualquiera de sus derivados, además de la declaración obligatoria de su presencia, se deberá incluir la siguiente información:
Los medicamentos son producidos generalmente por la industria farmacéutica. Los nuevos medicamentos pueden ser patentados, cuando la empresa farmacéutica ha sido la que ha investigado y lanzado al mercado el nuevo fármaco. Los derechos de producción o licencia de cada nuevo medicamento está limitado a un lapso que oscila entre 10 y 20 años. Los medicamentos que no están patentados se llaman medicamentos copia; en cambio, aquellos que no están patentados pero tienen un estudio de bioequivalencia, aprobado por las autoridades locales, se llaman medicamentos genéricos.

El objetivo de estos estudios es proveer evidencia documentada de como las características físicas, químicas, microbiológicas y biológica del medicamento varían con el tiempo bajo la influencia de factores como temperatura, humedad, luz y establecer las condiciones de almacenamiento adecuadas y el periodo de caducidad.

La estabilidad de un fármaco se puede definir como la propiedad de este envasado en un determinado material para mantener durante el tiempo de almacenamiento y uso de las características físicas, químicas, fisicoquímicas, microbiológicas y biológicas entre los límites especificados.

Las formas de inestabilidad de un fármaco son:


Solvólosis, oxidación, deshidratación, racemización, incompatibilidades de grupo.


Polimorfismo, vaporización, adsorción.


Fermentación y generación de toxinas

En el año 1979, en Estados Unidos se estableció una ley que pedía a las empresas farmacéuticas que pusieran la fecha hasta la cual se garantizaba la máxima potencia del medicamento. En otras palabras, no indica el momento en que ya no es seguro tomarlas ni cuando se vuelven dañinas. La mayoría de medicinas son muy durables y se degradan lentamente, por lo que la mayoría, mientras haya sido apropiadamente fabricada, mantienen su potencia en altos rangos mucho después de su fecha de caducidad. Un ejecutivo de Bayer, Chris Allen, dijo que las aspirinas que fabrican llevan una fecha de caducidad de tres años posterior a su fabricación, pero esta podría ser de incluso más y ésta aún estaría en su máxima potencia. El motivo detrás de esto yace en que constantemente están mejorando la fórmula de las aspirinas, y hacer pruebas de caducidad de más de 4 años por cada actualización sería irrazonable.
Sin embargo, se han registrado problemas con medicamentos que incluyen tetraciclina; también hay excepciones en la duración, como con nitroglicerina, insulina, y algunos antibióticos líquidos.

Los fármacos se administran con el fin de conseguir un objetivo terapéutico

La concentración adecuada y la dosis requerida para alcanzar este objetivo dependen, entre otros factores, del estado clínico del paciente, la gravedad de la patología a tratar, la presencia de otros fármacos y de enfermedades intercurrentes.

Para ello se requiere no solamente lograr una respuesta farmacológica y poder mantenerla; por lo tanto, es necesario alcanzar la concentración apropiada del fármaco en el lugar de acción. Para ello es necesario conocer su farmacocinética.

Debido a las diferencias individuales, el tratamiento eficaz requiere planificar la administración según las necesidades del paciente.

Tradicionalmente, esto se efectuaba por medio del ajuste empírico de la dosis hasta conseguir el objetivo terapéutico.

Sin embargo, a menudo este método es poco adecuado debido a la demora en conseguir el efecto o a la aparición de toxicidad. Una aproximación alternativa consiste en iniciar la administración de acuerdo con la absorción, distribución y eliminación, esperadas en el paciente y, posteriormente, ajustar la dosis según la respuesta clínica y por medio del monitoreo de las concentraciones plasmáticas.

Este enfoque requiere conocer la farmacocinética del fármaco en función de la edad, el peso y las consecuencias cinéticas de las posibles enfermedades intercurrentes (renales, hepáticas, cardiovasculares o una combinación de ellas).

El comportamiento farmacocinético de la mayoría de los fármacos puede resumirse por medio de algunos parámetros. Los parámetros son constantes, aunque sus valores pueden diferir de un paciente a otro y, en el mismo enfermo, en situaciones distintas.

La biodisponibilidad expresa el grado de absorción por la circulación sistémica.

La constante de absorción define la velocidad de absorción.

Las modificaciones de estos dos parámetros influyen sobre la concentración máxima, el tiempo que tarda en alcanzarse la concentración máxima y el área bajo la curva (AUC) concentración-tiempo tras una dosis oral única.

En los tratamientos crónicos, el grado de absorción es la medida más importante por la que se relaciona con la concentración media, mientras que el grado de fluctuación depende de la constante de absorción.

El volumen aparente de distribución es el líquido teórico corporal en que tendría que haberse disuelto el fármaco para alcanzar la misma concentración que en el plasma. Se usa para saber la dosis requerida para alcanzar una concentración determinada en la sangre. La fracción libre es útil porque relaciona la concentración total con la libre, que es quién, presumiblemente, está más asociada con los efectos farmacológicos. Es un parámetro útil sobre todo si se altera la fijación a las proteínas plasmáticas, por ejemplo en caso de hipoalbuminemia, de enfermedad renal o hepática y en interacciones por desplazamiento de la unión a dichas proteínas. El volumen aparente de distribución y la fracción libre son los parámetros más utilizados para el estudio de la distribución del fármaco.

La velocidad de eliminación de un fármaco del organismo es proporcional a su concentración plasmática. El parámetro que relaciona a ambas medidas es el aclaramiento total o clearance, que es la suma del aclaramiento renal más el aclaramiento extrarrenal o metabólico.

La fracción de fármaco eliminado sin cambios (en forma inalterada) es un parámetro útil para evaluar el efecto potencial de las enfermedades renales y hepáticas sobre la eliminación de los fármacos. Una fracción baja indica que el metabolismo hepático es el mecanismo de eliminación y que una enfermedad hepática podría afectar la eliminación del fármaco. Las patologías renales producen mayores efectos en la cinética de fármacos con elevada fracción de fármaco eliminado inalteradamente.

La velocidad con que se extrae un fármaco de la sangre por un órgano excretor como el hígado no puede exceder la velocidad a la que llega a dicho órgano. Es decir, el aclaramiento presenta un valor límite. Cuando la extracción es elevada, la eliminación está limitada por la llegada de fármaco al tejido y, por tanto, por la perfusión de este. Cuando el órgano de eliminación es el hígado o la pared intestinal y el fármaco se administra vía oral, una porción de la dosis administrada puede ser metabolizada durante su paso obligado a través de los tejidos hasta la circulación sistémica; es lo que se denomina efecto o metabolismo de primer paso hepático.

Por tanto, siempre que una sustancia tenga una extracción (aclaramiento) elevada en hígado o pared intestinal, la biodisponibilidad por vía oral, será baja, hasta el punto que a veces puede desaconsejar la administración vía oral. o requerir la administración de una dosis oral muy superior a la dosis parenteral equivalente.

Algunos ejemplos de fármacos presentan un efecto de primer paso importante;


La constante de eliminación está en función de cómo se elimina el fármaco de la sangre por parte de los órganos excretores y cómo se distribuye por el organismo.

La vida media de eliminación beta, (o semivida) es el tiempo requerido para que la concentración plasmática —o la cantidad de fármaco del organismo— se reduzca en un 50%.

Para la mayoría de los fármacos, la vida media se mantiene constante a pesar de la cantidad de fármaco que esté presente en el organismo, aunque existen excepciones (difenilhidantoína, teofilina y heparina).

Tiempo medio de permanencia (TMP) es otra medida de la eliminación del fármaco. Se define como el tiempo medio que la molécula de un fármaco permanece en el organismo tras una administración intravenosa rápida. Al igual que el aclaramiento, su valor es independiente de la dosis administrada. Para fármacos que describen un modelo de distribución monocompartimental, el TMP se iguala al recíproco de la constante de eliminación.

Se han identificado muchas de las variables que afectan los parámetros farmacocinéticos, las cuales deben tenerse en cuenta para ajustar la dosis administrada a las necesidades de cada paciente. Dado que incluso después del ajuste de las dosis suele persistir una notable variabilidad, es necesaria una monitorización cuidadosa de la respuesta farmacológica y, en muchos casos, de la concentración plasmática.

Para algunos fármacos se han establecido con precisión los cambios farmacocinéticos relacionados con la edad y el peso. En los niños y jóvenes (de 6 meses a 20 años), la función renal se correlaciona bien con la superficie corporal. Así, en el caso de los fármacos que se eliminan principalmente en forma inalterada por la orina, el aclaramiento varía con la edad de acuerdo con el cambio de la superficie corporal. En las personas mayores de 20 años, la función renal disminuye aproximadamente un 1% al año. Teniendo en cuenta estos cambios, es posible ajustar la dosis de dichos fármacos en cada edad. También se ha demostrado que la superficie corporal se correlaciona con el aclaramiento metabólico en los niños, aunque hay muchas excepciones.

En los recién nacidos y los lactantes, tanto la función renal como la hepática no están completamente desarrolladas y no es posible hacer generalizaciones, excepto que existen cambios rápidos.

El aclaramiento renal de la mayoría de los fármacos varía proporcionalmente al aclaramiento de creatinina, cualquiera que sea la patología renal. El cambio en el aclaramiento total depende de la contribución de los riñones en la eliminación total de dicho fármaco. Por tanto, se espera que el aclaramiento total sea proporcional a la función renal (aclaramiento de creatinina) para fármacos que se excretan exclusivamente de manera inalterada y no resulte afectado en el caso de fármacos que se eliminan por metabolismo.

A veces, en la insuficiencia renal se modifica el volumen aparente de distribución. En el caso de la digoxina, la reducción del volumen de distribución depende de la menor fijación a los tejidos. En el caso de la difenilhidantoína, el ácido salicílico y muchos otros fármacos, el volumen de distribución aumenta debido a que se reduce la fijación a las proteínas plasmáticas.

En las situaciones de estrés fisiológico (p.ej., infarto de miocardio, cirugía, colitis ulcerosa o enfermedad de Crohn) aumenta la concentración del reactante de fase aguda (1-glucoproteína ácida). En consecuencia, aumenta la fijación a las proteínas por parte de algunos fármacos de naturaleza básica como el propranolol, la quinidina y la disopiramida; por consiguiente, disminuye el volumen aparente de distribución de estos fármacos.

Las enfermedades hepáticas producen modificaciones en el aclaramiento metabólico, pero no se dispone de buenos predictores de estos cambios. Se ha descrito una reducción espectacular de la metabolización de fármacos en la cirrosis hepática. En esta enfermedad a menudo se observa una disminución de la fijación a las proteínas plasmáticas debido a la menor concentración de albúmina en sangre. Habitualmente, la hepatitis aguda cursa con elevación de las enzimas séricas, por eso no se asocia a una alteración del metabolismo.

Otras enfermedades como la insuficiencia cardíaca, la neumonía, el hipertiroidismo y muchas otras enfermedades también pueden alterar la farmacocinética.

Las interacciones farmacológicas provocan cambios en los valores de los parámetros farmacocinéticos y, por tanto, en la respuesta terapéutica. Se sabe que las interacciones afectan todos los parámetros. La mayoría de ellas son graduales y su magnitud depende de la concentración de los dos fármacos en interacción. Por todo ello, son difíciles de predecir y el ajuste de las dosis es complicado.

En determinados casos, los valores de los parámetros farmacocinéticos varían según la dosis administrada, la concentración plasmática o el tiempo; por ejemplo, la disminución de la biodisponibilidad de la griseofulvina al aumentar la dosis, debido a su menor solubilidad en las secreciones del tracto gastrointestinales alto. Otro ejemplo es el aumento desproporcionado de la concentración en equilibrio estacionario de la difenilhidantoína al aumentar el intervalo de dosificación, porque las enzimas metabolizadoras de difenilhidantoína tienen una capacidad limitada para eliminar el fármaco y la velocidad de administración habitual se aproxima a la velocidad máxima de metabolización. Por último, la reducción de la concentración plasmática de carbamazepina cuando se administra de manera crónica porque este fármaco induce su propio metabolismo.

Otras causas de variabilidad cinética dependiente de la dosis son: saturación de la fijación a las proteínas plasmáticas y tisulares (fenilbutazona), secreción saturable en el riñón (dosis altas de penicilina) y metabolismo de primer paso hepático saturable (propranolol).

Los fármacos son casi siempre compuestos extraños al organismo. Como tales, no se están formando y eliminando continuamente al igual que sucede con las sustancias endógenas. Por tanto, los procesos de absorción, biodisponibilidad, distribución y eliminación tienen una importancia capital para determinar el inicio, la duración y la intensidad del efecto farmacológico.

Proceso de transporte del fármaco desde el lugar de administración hasta la circulación sistémica atravesando por lo menos una membrana celular.

La absorción de los fármacos viene determinada por sus propiedades físico-químicas, formulaciones y vías de administración.

Las formas en las que se presentan los medicamentos (p. ej., píldoras, comprimidos, cápsulas o soluciones) consisten en el fármaco y otros ingredientes. Los medicamentos se formulan para poder administrarlos por diversas vías (oral, bucal, sublingual, rectal, parenteral, tópica e inhalatoria). Un requisito esencial para que cualquier fármaco pueda absorberse es que sea capaz de disolverse. Los medicamentos sólidos (p. ej., los comprimidos) pueden disgregarse y desintegrarse, pero la absorción solo ocurre cuando el principio activo se disuelve.

Cuando los fármacos penetran en el organismo a través de la mayoría de las vías de administración (excepto la vía intravenosa o intrarterial), deben atravesar varias membranas celulares semipermeables antes de llegar a la circulación general.

Estas membranas actúan como barreras biológicas que, de modo selectivo, inhiben el paso de las moléculas del fármaco.

Las membranas celulares se componen fundamentalmente de una matriz lipídica bimolecular que contiene colesterol y fosfolípidos. Los lípidos proporcionan estabilidad a la membrana y determinan sus características de permeabilidad. En la matriz lipídica se encuentran embutidas macromoléculas proteicas globulares de volumen y composición variables. Algunas de estas proteínas de la membrana participan en el proceso de transporte y también pueden tener la función de receptores para la regulación celular.

Los fármacos atraviesan las barreras biológicas por


En este proceso, el transporte a través de la membrana celular depende del gradiente de concentración del soluto. La mayoría de las moléculas pasan a través de la membrana por difusión simple desde una zona con elevada concentración (p. ej., líquidos gastrointestinales ) hasta una zona de baja concentración (p. ej., la sangre). Puesto que las moléculas del fármaco son rápidamente transportadas a través de la circulación sistémica y se distribuyen enseguida en un gran volumen de líquidos corporales y tejidos, su concentración en el plasma es baja al principio, en comparación con la concentración en el lugar de administración; este amplio gradiente es la fuerza impulsora del proceso. La velocidad neta de difusión es directamente proporcional a este gradiente, pero depende también de la liposolubilidad, grado de ionización y tamaño molecular del fármaco y de la superficie de absorción.

Sin embargo, puesto que la membrana celular es de naturaleza lipídica, los fármacos liposolubles difunden con mayor rapidez que aquellos relativamente no liposolubles. Además, las moléculas pequeñas tienden a penetrar en las membranas con mayor rapidez que las de mayor volumen.

La mayoría de los fármacos son ácidos o bases orgánicas débiles que en medio acuoso están de forma ionizada y no ionizada. La importancia de este concepto está en que la fracción no ionizada suele ser liposoluble y difunde fácilmente a través de las membranas celulares. La forma ionizada no puede penetrar en las membranas tan fácilmente, debido a su baja liposolubilidad y elevada resistencia eléctrica. La resistencia eléctrica es resultante de la carga de la molécula y de los grupos con carga eléctrica de la superficie de la membrana. Por tanto, la penetración de un fármaco puede atribuirse principalmente a la fracción no ionizada.

La distribución de un fármaco ionizable a través de una membrana en el equilibrio viene determinada por el pKa de la sustancia que es la inversa de la constante de disociación (cuando el pH y el pK son iguales, las concentraciones de la forma ionizada y no ionizada del fármaco son iguales) y por el gradiente de pH, si existe.

En el caso de un ácido débil, cuanto mayor sea el pH, menor será el cociente entre la fracción no ionizada y la fracción ionizada. En el plasma (pH = 7,4), la proporción entre las formas no ionizada e ionizada de un ácido débil (p. ej., con un pKa de 4,4) es 1:1.000; en el jugo gástrico (pH = 1,4), la proporción se invierte, es decir, 1.000:1.

Por ejemplo:

Cuando el ácido débil, como la aspirina, se administra vía oral, el gradiente de concentración para la fracción no ionizada entre el estómago y el plasma tiende a aumentar, situación que favorece la difusión a través de la mucosa gástrica. Al alcanzar el equilibrio, las concentraciones de fármaco no ionizado en el estómago y en el plasma son idénticas porque solo la forma no ionizada puede atravesar las membranas; la concentración de fármaco ionizado en plasma sería entonces aproximadamente 1.000 veces superior a la concentración de fármaco ionizado en la luz gástrica. En el caso de una base débil con un pKa de 4,4, la situación es la inversa.

Así, los fármacos que son ácidos débiles (p. ej., la aspirina), teóricamente deberían absorberse con mayor facilidad en un medio ácido (como la luz gástrica), que las bases débiles (p. ej., la quinidina). Sin embargo, independientemente del pH del fármaco, la mayor parte de la absorción tiene lugar en el intestino delgado por su extensión.

Para ciertas moléculas (p.ej., glucosa), la velocidad de penetración es mayor a la esperada por su baja liposolubilidad. Se postula que existe un transportador que se combina de manera reversible con la molécula sustrato en la parte externa de la membrana celular y que el complejo transportador-sustrato difunde rápidamente a través de la membrana, liberando el sustrato en la superficie interna de la membrana. Este proceso de difusión mediado por un transportador se caracteriza por la selectividad y la saturabilidad. El transportador solo acepta sustratos con una configuración molecular relativamente específica y el proceso está limitado por la disponibilidad de transportadores. Se trata de un mecanismo que no requiere energía, puesto que el sustrato no se transporta en contra de un gradiente de concentración.

Además de la selectividad y de la capacidad de saturación, el transporte activo se caracteriza porque requiere gasto de energía por parte de la célula. Los sustratos pueden acumularse en el interior de la célula contra gradiente de concentración. Los procesos de transporte activo están limitados a los fármacos con similitud estructural con las sustancias endógenas. Estos fármacos suelen absorberse en lugares específicos del intestino delgado. Se han identificado procesos de transporte activo para diversos iones, vitaminas, azúcares y aminoácidos.

Consiste en el englobamiento y la captación de partículas o líquido por parte de una célula. La membrana celular se invagina, encierra a la partícula o al líquido y luego vuelve a fusionarse formando una vesícula que más tarde se desprende y emigra hacia el interior de la célula. Este mecanismo también requiere gasto de energía. Probablemente, la pinocitosis desempeña un papel menor en el transporte de fármacos, con la excepción de los fármacos que son proteínas.

Puesto que la vía oral es el modo de administración más frecuente, la absorción suele referirse al transporte de los fármacos a través de las membranas de las células epiteliales del tracto gastrointestinales.

La absorción tras la administración oral depende de las diferencias del pH luminal a lo largo del tubo digestivo, de la superficie de absorción, de la perfusión tisular, de la presencia de flujo biliar y mucoso y de las membranas epiteliales.

Los ácidos se absorben más rápidamente en el intestino que en el estómago, en aparente contradicción con la hipótesis de que la forma no ionizada de un fármaco atraviesa con mayor facilidad las membranas. Sin embargo, esta discrepancia se debe a la enorme superficie del intestino delgado y a la mayor permeabilidad de sus membranas.

La mucosa oral posee un epitelio delgado y muy vascularizado que favorece la absorción, pero el contacto sucede por lo general por un tiempo demasiado corto, incluso para fármacos en solución, para que la absorción sea apreciable. En ocasiones, puede retenerse el fármaco durante más tiempo, para que la absorción sea más completa, situando el fármaco entre la encía y el carrillo (administración bucal) o colocándolo bajo la lengua ( sublingual ).

El estómago posee una superficie epitelial relativamente extensa, pero debido a la gruesa capa mucosa y a que el fármaco está en contacto relativamente poco tiempo, la absorción es limitada.

Dado que la absorción de prácticamente todos los fármacos es más rápida en el intestino delgado que en el estómago, la velocidad de vaciado gástrico es el paso limitante. Los alimentos, especialmente los grasos, enlentecen el vaciamiento gástrico (y la velocidad de absorción); este hecho explica por qué se recomienda tomar algunos fármacos con el estómago vacío cuando se desea que la acción comience rápidamente. La presencia de alimentos puede aumentar la absorción si el fármaco es poco soluble (p. ej., la griseofulvina); reducirla, si el fármaco se degrada en el estómago (p. ej., la penicilina G), o tener un efecto muy poco significativo o nulo. Además, los principios activos que alteran el vaciamiento gástrico (p. ej., los parasimpaticolíticos) también afectan la velocidad de absorción de otros fármacos.

El intestino delgado posee la mayor superficie para la absorción en el tracto gastrointestinales. En el duodeno el pH intraluminal oscila entre 4 y 5, pero se vuelve progresivamente más alcalino a lo largo del tubo digestivo (en la porción distal del íleon es cercano a 8). La flora gastrointestinales puede inactivar determinados fármacos, reduciendo así su absorción y su biodisponibilidad. La reducción del flujo sanguíneo (p. ej., en el shock) puede disminuir el gradiente de concentración a través de la mucosa intestinal y reducir la absorción por difusión pasiva. (La disminución del flujo sanguíneo periférico también altera la distribución y el metabolismo de los fármacos.)

La velocidad del tránsito intestinal puede influir en la absorción, especialmente en el caso de fármacos que se absorben por medio de transporte activo (p. ej., las vitaminas del complejo B), fármacos que se disuelven lentamente (p. ej., la griseofulvina) o los que son demasiado polares (poco liposolubles) para atravesar con facilidad las membranas (p. ej., muchos antibióticos). Para este tipo de fármacos, el tránsito debe ser muy lento para que la absorción sea completa.

Para las formas medicamentosas de liberación controlada, la absorción puede ocurrir inicialmente en el intestino grueso, especialmente cuando la liberación del principio activo de la forma medicamentosa dura más de 6 h, que es el tiempo de tránsito del intestino grueso.

La absorción de los fármacos que se administran vía oral en forma de solución depende de si éstos son capaces de sobrevivir a los «encuentros» peligrosos con las numerosas secreciones gastrointestinales, los bajos pH y las enzimas potencialmente degradadoras. Por lo general, incluso si un fármaco es estable en el ambiente intestinal, una escasa fracción de él pasa al intestino grueso. Fármacos escasamente lipofílicos (de baja permeabilidad), como los aminoglucósidos, se absorben lentamente de la solución en el estómago y en el intestino delgado; para estos fármacos, la absorción por el intestino grueso se supone que será incluso más lenta, ya que la superficie de absorción es menor. En consecuencia, estos fármacos no son candidatos para formas de liberación controlada.

La mayoría de los fármacos que se administran vía oral se presentan en forma de comprimidos o cápsulas, sobre todo por economía, estabilidad y aceptación por parte del paciente. Antes de absorberse se deben desintegrar y disolver. La desintegración aumenta considerablemente la superficie del fármaco que entra en contacto con los líquidos gastrointestinales y facilita su disolución y su absorción.

A menudo, durante el proceso de fabricación del medicamento se añaden los desintegrantes y otros excipientes (como disolventes, lubricantes, surfactantes, fijadores y dispersantes) para facilitar el proceso de desintegración.

Los surfactantes aumentan la velocidad de disolución del fármaco al incrementar su humectación, solubilidad y dispersabilidad. Entre los factores que modifican o retrasan la desintegración de las formas sólidas figuran la excesiva presión ejercida en la elaboración del comprimido y la aplicación de recubrimientos especiales para protegerlo de los procesos digestivos gastrointestinales. Los lubricantes hidrófobos (p. ej., el estearato de Mg) pueden fijar el principio activo y reducir su biodisponibilidad.

La velocidad de disolución determina la cantidad de fármaco disponible para la absorción. Cuando es más lenta que el proceso de absorción, la disolución constituye el paso limitante y puede manipularse por medio de cambios en la formulación del producto.

A menudo se emplea la reducción del tamaño de las partículas para aumentar la superficie del fármaco, lo cual resulta eficaz para aumentar la velocidad y la magnitud de la absorción gastrointestinales de un fármaco en el que estos parámetros están limitados por su lenta disolución. Entre los factores que afectan a la velocidad de disolución están si el fármaco está en forma de sal, en forma cristalina o en hidrato. Por ejemplo, las sales sódicas de los ácidos débiles (como barbitúricos y salicilatos) se disuelven más rápidamente que sus ácidos correspondientes, cualquiera que sea el pH del medio. Ciertos fármacos son polimorfos, pudiendo existir en forma amorfa o en varias formas cristalinas. El palmitato de cloranfenicol existe en dos formas, pero solo una posee un grado suficiente de disolución para que su absorción sea de utilidad clínica. Cuando una o más moléculas de agua se combinan con un fármaco en forma cristalina se constituye un hidrato. La solubilidad del hidrato puede ser muy distinta de la que posee la forma no hidratada del compuesto; así, la ampicilina anhidra tiene mayor velocidad de disolución y de absorción que su trihidrato correspondiente.

La administración directa de un fármaco en el torrente circulatorio (habitualmente por vía intravenosa asegura la llegada de toda la dosis a la circulación general. Sin embargo, la administración del fármaco por una vía que requiera su paso a través de una o más membranas biológicas (inyección intramuscular o s.c.) para alcanzar la sangre no garantiza que se absorba totalmente. Para fármacos proteicos con PM >20.000 g/mol, el paso a través de las membranas de los capilares es tan lento que tras la administración intramuscular o subcutanea la mayor parte de la absorción se realiza a través del sistema linfático «por defecto». En estos casos, la velocidad de liberación a la circulación sistémica es lenta e incompleta, ya que hay un fenómeno de metabolismo de primer paso por las enzimas proteolíticas de los vasos linfáticos.

Como los capilares tienden a ser muy porosos, la perfusión (flujo sanguíneo por gramo de tejido) es el factor determinante de la velocidad de absorción en el caso de moléculas pequeñas. Por tanto, el lugar de inyección influye en la absorción del fármaco; así, la velocidad de absorción del diazepam inyectado por vía intramuscular en una zona con escaso flujo sanguíneo puede ser mucho más lenta que tras la administración de una dosis vía oral

Cuando se inyectan sales de ácidos o bases poco solubles por vía intramuscular, es posible que la absorción se retrase o sea errática. Por ejemplo, la forma parenteral de difenilhidantoína (fenitoína) es una solución de la sal sódica en propilenglicol al 40%, con un pH cercano a 12. Cuando se inyecta por vía intramuscular, el propilenglicol se absorbe y los líquidos hísticos, actuando como tampón, reducen el pH y producen un desplazamiento del equilibrio entre la forma ionizada y el ácido libre. Es entonces cuando el ácido libre, que es poco soluble, precipita. En consecuencia, la disolución y la absorción son muy lentas (entre 1 y 2 semanas).

Las formas de liberación sostenida se han diseñado con el fin de reducir la frecuencia de administración y las fluctuaciones en las concentraciones plasmáticas y, de esta forma, conseguir un efecto farmacológico uniforme. Además, la menor frecuencia de administración es más cómoda para el paciente y puede mejorar el cumplimiento de la prescripción. En principio, los fármacos apropiados para este tipo de formas farmacéuticas son los que requieren una dosificación frecuente debido a su corta vida media de eliminación y a la breve duración de su efecto.

A menudo, las formas orales de liberación sostenida están diseñadas para mantener concentraciones terapéuticas del fármaco durante 12 h o más. La velocidad de absorción puede controlarse por distintas vías: recubriendo las partículas del fármaco con ceras u otras sustancias insolubles en agua, incluyendo al principio activo en una matriz de la que se libera lentamente a lo largo del tracto gastrointestinales o elaborando un complejo entre el fármaco y resinas de intercambio iónico.

Las formas tópicas de liberación sostenida se han diseñado para la presencia del fármaco durante períodos prolongados. Por ejemplo, la difusión de la clonidina a través de una membrana proporciona una liberación sostenida del fármaco durante 1 semana, y un polímero impregnado de nitroglicerina fijado a un vendaje adhesivo proporciona una liberación controlada durante 24 h. Los fármacos de liberación transdérmica deben poseer características adecuadas para la penetración cutánea y una potencia elevada, ya que la velocidad de penetración y el área de absorción son limitadas.

Se han formulado muchos preparados de administración parenteral no intravenosa, con el fin de proporcionar concentraciones plasmáticas sostenidas. En el caso de antibióticos, la inyección intramuscular de sales insolubles (p. ej., la penicilina G benzatina) consigue efectos farmacológicos clínicamente útiles durante largos períodos de tiempo. Otros fármacos están formulados como suspensiones o soluciones en vehículos no acuosos. Así, por ejemplo, la insulina puede inyectarse como suspensión cristalina para conseguir un efecto prolongado; la insulina amorfa, con una superficie de disolución mayor, posee un inicio del efecto rápido y una duración corta.

Biodisponibilidad es la proporción de principio activo (fármaco o metabolito) que entra en la circulación general y que, por consiguiente, llega al lugar de acción, así como la velocidad con que ello sucede.

Mientras que las propiedades fisicoquímicas de un fármaco condicionan su potencial de absorción, las propiedades de la forma farmacéutica (de su diseño y de su manufactura) son determinantes principales de su biodisponibilidad. Las diferencias en la biodisponibilidad entre diferentes formulaciones de un mismo fármaco pueden ser clínicamente relevantes.

El concepto de equivalencia entre las formulaciones de un fármaco es importante a la hora de decidir el tratamiento más adecuado en cada situación.

El término equivalente químico (o farmacéutico) se refiere a los medicamentos que contienen el mismo principio activo en la misma cantidad y que cumplen los estándares oficiales; sin embargo, los ingredientes inactivos de los medicamentos pueden ser distintos.

La palabra bioequivalencia se aplica a los equivalentes químicos que, administrados a la misma persona siguiendo la misma pauta, alcanzan concentraciones similares en el plasma y en los tejidos. Los equivalentes terapéuticos designan dos medicamentos que, administrados a la misma persona y con la misma pauta, proporcionan esencialmente el mismo efecto terapéutico o tóxico. Se supone que los medicamentos bioequivalentes son terapéuticamente equivalentes.

Con frecuencia, algunos de los problemas terapéuticos (p. ej., toxicidad, pérdida de eficacia) que ocurren en el curso de tratamientos prolongados, cuando la enfermedad estaba siendo controlada con una formulación de un fármaco, son debidos al cambio por sustitutos no equivalentes (como en el caso de la digoxina o la difenilhidantoína).

En ocasiones es posible la equivalencia terapéutica aunque haya variaciones en la biodisponibilidad. Por ejemplo, el índice terapéutico (relación entre la dosis máxima tolerada y la dosis mínima eficaz) de la penicilina es tan amplio que diferencias moderadas en la concentración plasmática debidas a diferencias en la biodisponibilidad de las formulaciones de penicilina pueden no afectar la eficacia terapéutica o la seguridad del fármaco. Por el contrario, si se tratara de un fármaco con un índice terapéutico relativamente estrecho, las diferencias en la biodisponibilidad sí serían determinantes.

La biodisponibilidad también depende de otros factores, como los relacionados con la fisiología y las patologías —principal y asociadas— del paciente.

Este concepto es fundamental para explicarse porque las drogas no tienen siempre la misma magnitud de efecto

La velocidad a la que se absorbe un fármaco es un factor importante incluso cuando el fármaco se absorbe totalmente. Puede ocurrir que sea demasiado lenta para alcanzar una concentración plasmática terapéutica o tan rápida que se alcancen concentraciones tóxicas tras cada dosis.

Cuando un fármaco se disuelve rápidamente de su formulación y atraviesa las membranas con facilidad, la absorción tiende a ser completa en la mayoría de las vías de administración. Este no siempre es el caso de los fármacos administrados vía oral Antes de alcanzar la vena cava, un fármaco debe descender por el tracto gastrointestinales y atravesar la pared intestinal y el hígado, que son lugares donde habitualmente se metabolizan los fármacos; por tanto, es posible que el fármaco se metabolice antes de llegar a la circulación general. Esta causa de baja biodisponibilidad se denomina metabolismo de primer paso. Muchos fármacos tienen una baja biodisponibilidad debido a que sufren un elevado metabolismo de primer paso. En muchos casos (p. ej., isoproterenol, noradrenalina, testosterona), la extracción en esos tejidos es tan completa que la biodisponibilidad es prácticamente cero. Sin embargo, para fármacos con metabolitos activos, las consecuencias terapéuticas de este efecto de primer paso dependen de la contribución del fármaco o del metabolito a los efectos farmacológicos deseables o tóxicos.

La biodisponibilidad escasa es frecuente en formulaciones para administración oral de fármacos poco solubles en agua, que se absorben muy lentamente. Cuando la absorción es lenta o incompleta, los factores que pueden afectar la biodisponibilidad son más numerosos que cuando es rápida o completa. En el primer caso, puede esperarse una respuesta terapéutica mucho más variable.

La permanencia insuficiente del fármaco en el tracto gastrointestinales es una de las causas más comunes de biodisponibilidad escasa. Al ingerir un fármaco, este no permanece en el tracto gastrointestinales más de 1-2 días y en el intestino delgado solo se halla 2-4 horas. Si el fármaco no se disuelve con facilidad o si es incapaz de atravesar el epitelio intestinal (fármacos polares, muy ionizados), el tiempo en el que permanece en el lugar de absorción puede ser insuficiente. En estos casos, la biodisponibilidad no solo es baja, sino que tiende a ser muy variable. Además, otros factores como edad, sexo, actividad, fenotipo genético, estrés, enfermedades (p. ej., aclorhidria, síndromes de mala absorción) o la cirugía gastrointestinales previa, pueden alterar e incluso aumentar todavía más las diferencias en la biodisponibilidad.

Las reacciones que compiten con la absorción pueden reducir la biodisponibilidad. Pueden ser la formación de complejos (p. ej., entre tetraciclina e iones metálicos polivalentes), la hidrólisis debida al ácido gástrico o a enzimas digestivas (p. ej., la hidrólisis de la penicilina y el palmitato de cloranfenicol), la conjugación en la pared intestinal (p. ej., la sulfo-conjugación del isoproterenol), la adsorción por otros fármacos (p. ej., digoxina y colestiramina) y el metabolismo por la microflora intestinal.

El análisis de la biodisponibilidad a partir de datos sobre la concentración plasmática respecto al tiempo suele requerir 3 medidas: la concentración plasmática máxima (pico) del fármaco, el tiempo en que aparece esta concentración máxima y el área bajo la curva concentración plasmática-tiempo

La concentración plasmática aumenta al hacerlo la velocidad y el grado de absorción; cuando la velocidad de eliminación del fármaco equivale a la velocidad de absorción, se alcanza el pico.

Las determinaciones de la biodisponibilidad basadas en la concentración plasmática máxima pueden ser erróneas, puesto que la eliminación empieza inmediatamente después de que el fármaco llega a la circulación. El tiempo en que se alcanza el pico plasmático depende de la velocidad de absorción; de hecho, es el índice más utilizado para medir este parámetro. Cuanto más lenta sea la absorción, más tarde se alcanzará el pico.

Sin embargo, a menudo el pico máximo no es un índice absolutamente definitivo, ya que se trata de un valor puntual, que depende de la frecuencia en la toma de muestras de sangre y, en caso de que las concentraciones cercanas al pico describan una curva relativamente plana, de la reproducibilidad de los resultados. Recordar que la concentración sanguínea de una droga muchas veces no es índice adecuado de la concentración en el sitio de acción.

Aunque para medir la velocidad de absorción la administración única proporciona más datos que las administraciones múltiples, la biodisponibilidad puede medirse tras administraciones únicas o tras administraciones repetidas.

El estudio de los parámetros farmacocinéticos tras dosis múltiple posee algunas ventajas, como permitir representar de manera más exacta la situación clínica habitual. Normalmente se consiguen concentraciones plasmáticas superiores a las obtenidas con dosis única, lo que facilita su determinación.

Tras administrar dosis repetidas con intervalos regulares durante el período correspondiente a 4-5 vidas media beta o de eliminación, las concentraciones plasmáticas deberían alcanzar el estado de equilibrio estacionario (la cantidad de fármaco absorbido se iguala con la cantidad de fármaco eliminado en cada intervalo de dosis).

En el caso de los fármacos que se excretan en forma inalterada (sin metabolizar) por la orina, puede estimarse la biodisponibilidad midiendo la cantidad total de fármaco excretado tras una administración única. Idealmente, debería recogerse la orina durante un período correspondiente a 7-10 vidas medias beta de eliminación con el fin de recuperar por completo el fármaco absorbido. En caso de múltiples dosis, la biodisponibilidad puede determinarse midiendo el fármaco inalterado en orina durante 24h en condiciones de equilibrio estacionario.

Tras llegar a la circulación general, el fármaco pasa a los tejidos del organismo. Por lo común la distribución es desigual por las diferencias en la perfusión sanguínea, el grado de unión a los tejidos, las variaciones regionales del pH y la distinta permeabilidad de las membranas celulares.

La velocidad de penetración del fármaco en el tejido depende del flujo sanguíneo, de la masa de tejido y de la proporción del fármaco en sangre y en tejido.

En las zonas con una vascularización rica se alcanza el equilibrio de distribución (la velocidad de entrada y la velocidad de salida son iguales) entre el plasma y el tejido más rápidamente que en las zonas poco perfundidas, a no ser que la difusión a través de las membranas sea un paso limitante. Tras alcanzar el equilibrio de distribución, las concentraciones del fármaco (libre y unida a proteínas, v. más adelante) en los tejidos y en el líquido extracelular quedan reflejadas por la concentración plasmática. El metabolismo y la excreción tienen lugar simultáneamente con la distribución, lo que determina un proceso dinámico y complejo.

El volumen de líquido en el que parece distribuirse o diluirse el fármaco se denomina volumen aparente de distribución (el volumen corporal en que tendría que haberse disuelto el fármaco para alcanzar la misma concentración que en el plasma). Este parámetro informa sobre la concentración plasmática esperada para una dosis concreta y también sobre la dosis requerida del fármaco para obtener una concentración concreta. Sin embargo, el volumen aparente de distribución no proporciona datos sobre el patrón específico de distribución. Cada fármaco se distribuye en el organismo de un modo particular. Algunos tienden a dirigirse a los tejidos grasos, otros permanecen en el líquido extracelular y, por último, otros se fijan con avidez a tejidos específicos, como el hígado o el riñón.

Muchos fármacos ácidos (p. ej., la warfarina y el ácido salicílico) se fijan mucho a proteínas y, por tanto, tienen un volumen aparente de distribución pequeño. Muchos fármacos básicos (como la anfetamina y la meperidina) son captados con avidez por los tejidos y su volumen de distribución es mayor que el volumen de todo el organismo.

El grado de distribución de los fármacos en los tejidos depende de su unión a las proteínas plasmáticas y a diversos componentes tisulares.

Los fármacos son transportados en la sangre en parte en solución (como fármaco libre, no unido) y en parte fijados a diversos componentes de la sangre (proteínas y células sanguíneas).

El principal determinante de la proporción entre el fármaco unido y el fármaco libre es la interacción reversible entre el fármaco y la proteína a la que se fija; esta interacción sigue la ley de acción de masas.

Muchas proteínas plasmáticas pueden interaccionar con los fármacos. Las más importantes son la albúmina, la glucoproteína ácida a1 y las lipoproteínas. Los fármacos de naturaleza ácida suelen fijarse a la albúmina, en tanto que los de tipo básico tienden a unirse a una de las dos últimas o a ambas

Puesto que sólo el fármaco libre puede sufrir una difusión pasiva hacia los tejidos y las zonas extravasculares en las que se ejerce el efecto farmacológico, la concentración de fármaco libre refleja mejor la concentración del fármaco en el lugar de acción y, por tanto, sus efectos.

La fracción libre (proporción de fármaco libre en relación a la concentración total) es un parámetro más útil que la fracción unida. La fijación a las proteínas plasmáticas influye en la distribución y en la relación aparente entre la actividad farmacológica y la concentración plasmática total del fármaco. A concentraciones elevadas de fármaco, la cantidad de fármaco unido se aproxima a un límite máximo, dependiendo del número de sitios de unión disponibles.

Por tanto, se dice que la fijación es saturable. La saturabilidad es la base de las interacciones por desplazamiento entre fármacos.

Los fármacos pueden unirse a muchas sustancias, además de a proteínas. Esta unión puede ser muy específica, como es el caso de la fijación de la cloroquina a los ácidos nucleicos. La unión tisular suele involucrar la asociación del fármaco con una macromolécula en un medio acuoso. Otro tipo de asociación que induce a pensar en una fijación tisular es la distribución del fármaco en la grasa corporal. Dado que el tejido adiposo está poco perfundido, el tiempo necesario para alcanzar el equilibrio en él es prolongado.

La acumulación en los tejidos o en los compartimientos corporales puede prolongar la permanencia de los fármacos en el plasma y sus acciones porque los tejidos sirven de depósito. A medida que la concentración plasmática disminuye, el fármaco almacenado se va liberando a la circulación. La localización del lugar de acción y las diferencias relativas en la distribución tisular también pueden ser importantes. El inductor anestésico tiopental, un tiobarbitúrico, es un ejemplo de fármaco cuyo almacenamiento en reservorios tisulares inicialmente acorta su efecto farmacológico, pero tras administraciones repetidas lo prolonga. El tiopental es un hipnótico muy liposoluble y se distribuye rápidamente en el cerebro tras la inyección intravenosa única. La concentración en el cerebro aumenta en 1 a 2 minutos, y posteriormente disminuye rápidamente en el cerebro y más lentamente la plasmática. La hipnosis finaliza a medida que el fármaco se redistribuye desde el cerebro al plasma y hacia los tejidos con perfusión más lenta, músculo y grasa. Sin embargo, si se determinan las concentraciones plasmáticas durante el tiempo suficiente, puede observarse una tercera fase de distribución que representa la liberación lenta del fármaco acumulado en el tejido adiposo. La administración continua de tiopental supone que grandes cantidades de fármaco se almacenan en el tejido graso, lo cual prolonga el efecto hipnótico.

Algunos fármacos se acumulan en las células en concentraciones superiores a las alcanzadas en el líquido extracelular. Esta acumulación suele implicar la fijación de fármacos a proteínas celulares, fosfolípidos o ácidos nucleicos. Los antipalúdicos como la cloroquina destacan por su notable fijación intracelular, de modo que pueden alcanzar concentraciones intracelulares en leucocitos y células hepáticas miles de veces superiores a las plasmáticas. El fármaco almacenado se encuentra en equilibrio con el plasmático y vuelve al plasma a medida que se va eliminando del organismo.

Los fármacos llegan al SNC por la circulación capilar y a través del LCR. Aunque el cerebro recibe una proporción importante del volumen minuto (aproximadamente 1/6), la distribución de los fármacos en el cerebro está restringida. Algunos fármacos liposolubles (como el tiopental) entran y ejercen sus efectos rápidamente, pero muchos otros —en particular los más hidrosolubles— penetran en el cerebro con mayor lentitud. Las células endoteliales de los capilares cerebrales están más estrechamente unidas entre sí que las de los demás lechos capilares del organismo; esto contribuye a la lenta penetración de las sustancias hidrosolubles. Otra barrera importante para los fármacos hidrosolubles son las células del tejido glial (los astrocitos) que forman una vaina pegada a la membrana basal del endotelio capilar.

El endotelio capilar y la vaina astrocítica constituyen la barrera hematoencefálica. Esta barrera es la que confiere las características diferenciales de permeabilidad entre estos tejidos y los del resto del organismo, en los que la barrera corresponde a la pared capilar y no a la célula parenquimatosa. Así, los compuestos polares son incapaces de penetrar en el cerebro, pero pueden acceder al líquido intersticial de la mayoría de los demás tejidos. El concepto de barrera hematoencefálica se definió tras la observación de que los colorantes polares podían penetrar en la mayoría de los tejidos, pero no en el SNC.

Los fármacos pueden pasar directamente al LCR ventricular a través del plexo coroideo, y tienen acceso al tejido cerebral por difusión pasiva desde el LCR. El plexo coroideo también es una zona de transporte activo de ácidos orgánicos (como la penicilina) desde el LCR a la sangre.

Los factores principales que determinan la velocidad de penetración en el LCR o en otras células son el grado de fijación a las proteínas, el grado de ionización y el cociente de partición lípido/agua del compuesto. La velocidad de penetración en el cerebro es lenta en los fármacos que se unen en gran proporción a proteínas. En el caso de ácidos y bases débiles ionizados, la penetración es tan lenta que se considera prácticamente inexistente.

En otros tejidos del organismo, la perfusión es el determinante principal de la velocidad de distribución, pero el SNC está tan bien perfundido que el factor más importante suele ser la permeabilidad. Sin embargo, en los tejidos poco perfundidos (p. ej., el músculo y el tejido adiposo), la distribución se prolonga notablemente, sobre todo si el tejido tiene mucha afinidad por el fármaco.

Suma de procesos que conducen a la desaparición (por metabolismo y excreción) del fármaco del organismo.

El hígado es el órgano principal donde se produce el metabolismo de los fármacos (modificaciones químicas), pero no es el único. Algunos metabolitos tienen actividad farmacológica (v. tabla 298-2). Cuando la sustancia administrada es inactiva pero da lugar a un metabolito activo, el compuesto administrado se denomina profármaco, especialmente si ha sido diseñado para liberar eficazmente el principio activo.

El metabolismo de los fármacos supone un amplio espectro de reacciones químicas:

Las enzimas implicadas en estas reacciones están presentes en numerosos tejidos, pero, por lo general, se encuentran más concentradas en el hígado. Para muchos fármacos, el metabolismo se produce en dos fases. Las reacciones de fase I suponen la formación de un nuevo grupo funcional o una partición de la molécula (oxidación, reducción, hidrólisis); se trata de reacciones no sintéticas.

Las reacciones de fase II conllevan la conjugación con un compuesto endógeno (p. ej., ácido glucurónico, sulfato, glicina); se trata, pues, de reacciones sintéticas. Los metabolitos formados en las reacciones sintéticas son más polares y más fácilmente excretados por el riñón (en la orina) y por el hígado (en la bilis) que los formados en las reacciones no sintéticas. Algunos fármacos sufren procesos de metabolismo de ambos tipos. Pese a que se denominan fasesI y II, se trata, como puede verse, de una clasificación funcional, no secuencial, de las reacciones de metabolismo de fármacos.

El sistema enzimático más importante del metabolismo de fase I es el citocromo P-450, una superfamilia de enzimas microsomales que catalizan reacciones de oxidación de numerosos fármacos por su capacidad de transferencia de electrones.

Los electrones son aportados por la NADPH-citocromo P-450-reductasa, una flavoproteína que transfiere electrones del NADPH (la forma reducida del fosfato dinu-cleótido de nicotinamida-adenina) al citocromo P-450. Las enzimas del citocromo P-450 están agrupadas en 14 familias de genes de mamífero que comparten secuencias idénticas y 17 subfamilias. Se denominan por un símbolo raíz (CYP), seguido de un numeral árabe para la familia, una letra para la subfamilia y otro número árabe para el gen específico.

Las enzimas de las subfamilias 1A, 2B, 2C, 2D y 3A son las más importantes del metabolismo en mamíferos. CYP1A2, CYP2C9, CYP2C19, CYP2D6 y CYP3A4 son los más importantes en el metabolismo humano. La especificidad de las enzimas permite explicar muchas interacciones entre fármacos. En la tabla 298-3 se presentan varios ejemplos de fármacos que interaccionan con enzimas específicas del complejo citocromo P-450 (v. también Interacciones farmacológicas, cap.301). Las diferencias genéticas entre pacientes pueden modificar la respuesta clínica.

La glucuronoconjugación es la reacción de fase II más común, y es la única que ocurre en el sistema enzimático microsomal hepático. Los glucurónidos se secretan por la bilis y se eliminan por la orina. El cloranfenicol, el meprobamato y la morfina son algunos ejemplos de fármacos metabolizados por esta vía.

La conjugación con aminoácidos, como la glutamina y la glicina, produce metabolitos (p. ej., ácido salicilúrico, de la conjugación de ácido salicílico y glicina) fácilmente excretables en la orina, pero que no suelen secretarse por la bilis. La acetilación es la vía metabólica principal de las sulfamidas. La hidralazina, la isoniazida y la procainamida también sufren acetilación. La sulfoconjugación es la reacción entre grupos fenol o alcohol y un sulfato inorgánico, que deriva en parte de aminoácidos que contienen azufre como la cisteína. Los ésteres de sulfato así obtenidos son polares y se excretan rápidamente en la orina. Algunos ejemplos de fármacos que forman sulfatos son: paracetamol, estradiol, metildopa, minoxidil y tiroxina. La metilación es la principal vía metabólica para inactivar algunas catecolaminas. La niacinamida y el tiouracilo también sufren procesos de metilación.

Los recién nacidos tienen un sistema enzimático microsomal hepático solo parcialmente desarrollado y, en consecuencia, presentan algunas dificultades para metabolizar muchos fármacos (p. ej., hexobarbital, fenazetina, anfetamina y clorpromazina). La experiencia con el cloranfenicol en recién nacidos muestra claramente las graves consecuencias que se pueden derivar del enlentecimiento de la glucuronoconjugación. Dosis equivalentes en mg/kg de cloranfenicol, bien toleradas por pacientes mayores, pueden provocar una toxicidad grave en los recién nacidos (síndrome del niño gris), asociada a la presencia de niveles plasmáticos elevados de cloranfenicol durante largo tiempo.

A menudo, la capacidad metabólica también se encuentra disminuida en los pacientes ancianos; esta reducción varía en función del fármaco y no es tan grave como en los recién nacidos.

Una vez terminado el tratamiento para el que fueron prescritos, los medicamentos deben depositarse en un punto limpio, puesto que desecharlos indiscriminadamente junto con el resto de los residuos puede deteriorar gravemente el medio ambiente.

Con el objetivo de cerrar correctamente el ciclo de vida de los medicamentos, la industria farmacéutica ha puesto en marcha SIGRE Medicamento y Medio Ambiente ("Sistema Integrado de Gestión y Recogida de Envases"), en colaboración con la farmacias y la distribución del sector, facilitando que los ciudadanos puedan desprenderse cómodamente, pero con todas la garantías sanitarias y medioambientales, de los restos de medicamentos y de sus envases a través del Punto SIGRE situado en las oficinas de farmacia.

La información que aparece en el etiquetado de los medicamentos, tanto del embalaje exterior como del acondicionamiento primario, así como los símbolos, siglas y leyendas que deben
contener, se define en los anexos III y IV del Real Decreto 1345/2007, de 11 de octubre, por el que se regula el procedimiento de autorización, registro y condiciones de dispensación de los medicamentos de uso humano fabricados industrialmente.

Todo medicamento debe contar, antes de ser distribuido en este país, con un registro sanitario. Para obtener el registro sanitario en República Dominicana es necesario obtener permiso o autorización ante la Secretaría de Estado de Salud Pública del ministerio de salud. En República Dominicana, para iniciar la comercialización de alimentos, productos farmacéuticos, de uso doméstico y productos de cuidado personal. De acuerdo a lo anterior, el fabricante debe reunir ciertos requisitos, para obtener el Registro Sanitario, antes de proceder con el registro de productos, incluyendo:

Registrar ante el Ministerio de Salud una compañía autorizada de la distribución (compañía dominicana o una subsidiaria de un fabricante incorporado bajo leyes dominicanas). En todo caso, para la obtención del Registro Sanitario, el fabricante puede decidir incorporar una compañía bajo leyes de República Dominicana para representarlo y distribuir los productos en el país y así evitar todas las obligaciones que establecen las leyes 173 sobre la comercialización y distribución de productos la cual otorga indemnizaciones altas a los distribuidores dominicanos en caso de rescisión (terminación) del contrato de distribución por parte del fabricante.




</doc>
<doc id="5389" url="https://es.wikipedia.org/wiki?curid=5389" title="Dosis">
Dosis

En farmacología se entiende por dosis la cantidad de principio activo de un medicamento, expresado en unidades de volumen o peso por unidad de toma en función de la presentación, que se administrará de una vez. También es la cantidad de fármaco efectiva. La sobredosis es la toma por encima de la dosis máxima tolerada. En su extremo, puede ser una dosis letal. Los medicamentos se pueden presentar en forma de multidosis o unidosis. En la unidosis o dosis unitaria, cada unidad de medicamento es una toma y viene identificada con su lote y caducidad. A nivel hospitalario se emplea cada vez más la unidosis por ser más cómoda y evitar errores en la toma. 

Se denomina dosis máxima tolerada, MTD, por sus siglas en inglés —"Maximum Tolerated Dose"— a la dosis más elevada de un medicamento o tratamiento que un paciente puede recibir sin causarle efectos secundarios inaceptables, tales como muerte o disfunción celular u orgánica o efectos que disminuyen la esperanza de vida o un retraso superior al 10 % del peso corporal respecto a los sujetos de control. Se suele determinar durante los ensayos clínicos, Mediante el procedimiento de aumentar las dosis gradualmente hasta que se encuentra la dosis más alta con efectos secundarios tolerables.

En Radiología y Protección Radiológica, se utiliza el término dosis para la cantidad de radiación recibida por material, y más típicamente, por un ser vivo. Dependiendo del objetivo de la medida, se definen diversas magnitudes:



</doc>
<doc id="5392" url="https://es.wikipedia.org/wiki?curid=5392" title="Ensayo clínico">
Ensayo clínico

Un ensayo clínico es una evaluación experimental de un producto, sustancia, medicamento, técnica diagnóstica o terapéutica que, en su aplicación a seres humanos, pretende valorar su eficacia y seguridad.

Los estudios de prometedores tratamientos nuevos o experimentales en pacientes se conocen como ensayos clínicos. Un ensayo clínico se realiza sólo cuando hay razones para creer que el tratamiento que se está estudiando puede ser beneficioso para el paciente. Los tratamientos usados en los ensayos clínicos con frecuencia demuestran tener beneficios reales. Los investigadores realizan estudios sobre nuevos tratamientos para conocer la utilidad del nuevo tratamiento, el mecanismo de acción del nuevo tratamiento, si la efectividad es mayor que otros tratamientos ya disponibles, los efectos secundarios del nuevo tratamiento y si son mayores o menores que el tratamiento convencional, si supera los beneficios a los efectos secundarios y en qué pacientes el nuevo tratamiento es más útil.

Puede encontrarse tantas definiciones de ensayo clínico como enfoques posibles tiene el tema, aunque predomina el enfoque epidemiológico y el finalista (su uso para investigar fármacos). Desde la más simple, que lo define como "una prueba científica de un fármaco, aceptada por el enfermo y amparada por la ley" a las más complejas; de las más amplias a las más restrictivas, hay una amplia variedad.
Podemos decir que el ensayo clínico consiste en un estudio experimental y prospectivo en el cual el investigador provoca y controla las variables y los sujetos (pacientes, la mayoría de los casos) son asignados de forma aleatoria a las distintas intervenciones que se comparan. Debido a que es el tipo de estudio epidemiológico que presenta menores errores sistemáticos o sesgos, constituye la mejor prueba científica para apoyar la eficacia de las intervenciones terapéuticas. El elemento esencial del ensayo es la existencia de un grupo de comparación o grupo de control, que permite probar si la nueva intervención (por ejemplo un nuevo fármaco) es mejor o no que las ya existentes o que no intervenir (placebo).

Un ensayo clínico se inicia cuando surge una hipótesis a partir de estudios no controlados observacionales, descriptivos o retrospectivos, o de estudios preclínicos. Frecuentemente se descubren en investigaciones preclínicas posibilidades terapéuticas que no tienen ningún beneficio en un ensayo clínico. Muchas veces se realizan actividades médicas cuya utilidad no ha sido demostrada mediante un ensayo clínico, sin embargo llevarlo a la práctica es difícil, sobre todo por el costo económico y de tiempo.

Después de ser diseñado debe ser aprobado por un comité de bioética, los pacientes que forman parte deben conocer los objetivos del estudio, sus riesgos y beneficios y firmar el consentimiento informado y podrán abandonar el estudio cuando quieran. El ensayo clínico finaliza cuando acaban los plazos de tiempo definidos en el protocolo, o cuando de forma prematura son manifiestamente perjudiciales o beneficiosos los efectos en el brazo experimental.

El ensayo clínico es el estudio clínico que posee el nivel de evidencia más alto para demostrar que el procedimiento médico que se realiza es el más adecuado con los conocimientos científicos que existen en ese momento, debido al diseño del estudio, donde las variables estadísticas están controladas para evitar los sesgos. Así pues, junto con los estudios de metaanálisis son la base de lo que se conoce como Medicina Basada en la Evidencia, que no es más que el respaldo de las prácticas clínicas con pruebas consistentes desde el punto de vista científico.

Existen diferentes tipos de clasificaciones en virtud del factor que tengamos en cuenta para realizarla. Las clases obtenidas no son, en la mayoría de los casos, incompatibles entre sí, sino que se solapan, perfeccionándose en la combinación de unas con otras. Así, en la adecuada combinación de las clases podremos llegar al ensayo clínico ideal (aunque este será variable en función de las circunstancias de la investigación). En la siguiente tabla se muestran algunas de ellas.

La adecuada combinación de las características de los distintos tipos de ensayos clínicos nos permitirán ir construyendo un ensayo clínico ideal, formado por las clases más potentes, fiables, rigurosas o reproducibles. Aunque el ensayo clínico ideal será aquel que, como se verá, mejor se adapte a las condiciones de cada intervención, en una hipotética situación ideal habrá de cumplir las siguientes características:

El ensayo clínico es un estudio experimental en el que en el diseño de investigación están definidas las variables y los mecanismos de control de dichas variables, cuya función es evitar los sesgos y las variables de confusión. Existe un grupo con el que se compara la intervención experimental. Este grupo sufre también una intervención con un procedimiento placebo o con un procedimiento estándar de referencia, ya validado para la situación objeto de estudio. Para que ambos grupos sean comparables todos los factores pronósticos, tanto los conocidos (mediante los criterios de selección) como los desconocidos (mediante la asignación aleatoria), deben estar repartidos por igual entre los grupos antes de iniciar el tratamiento. Pueden existir más de un grupo de intervención experimental cuando queremos probar más de una hipótesis, así como más de un grupo de control cuando existe más de una intervención validada que se sabe eficaz. Esto se suele hacer cuando se quiere estratificar la eficacia de las intervenciones. A cada grupo, tanto de intervención como de control, se les llama "brazos del estudio".

Un ejemplo: Queremos probar la eficacia de dos fármacos A y B en la enfermedad E, para la cual ya se han mostrado efectivos los fármacos C y D. Pues bien, podemos hacer los siguientes grupos: grupo del fármaco A, grupo del fármaco B, grupo del fármaco C, grupo del fármaco D y grupo con placebo. Tras el análisis estadístico de los resultados, la comparación de los brazos A y B con el brazo placebo nos dirá si los fármacos son o no eficaces en esa enfermedad. La comparación con los brazos C y D nos dirá el grado de eficacia respecto a lo ya conocido. Así, podemos conseguir resultados del tipo: El fármaco A no es eficaz, mientras que el B sí lo es, siendo más eficaz que el C, pero menos que el D.

Según la temporalidad, es decir, el momento en el que se define el estudio respecto al tiempo, los estudios pueden ser:
Las fechas de inicio y terminación se han definido previamente en el protocolo de investigación.

Lo podemos encontrar frecuentemente bajo el término "randomizado", neologismo inglés que en ocasiones hasta se declina como si fuera un verbo castellano: "randomizar".

La aleatorización significa que los casos son distribuidos al azar en cada brazo del estudio. El objetivo es conseguir que los diferentes grupos sean comparables u homogéneos, evitar el sesgo del investigador en la asignación de casos a los grupos y garantizar que los tests estadísticos tendrán valores de significación estadística válidos. Existen varias alternativas metodológicas de aleatorización, que deben ser consideradas en la fase de planificación del estudio, y cuya idoneidad depende de las características del ensayo a efectuar. En Medicina y otras ciencias biológicas, las técnicas de aleatorización más usadas son:

Consiste en la asignación a cada uno de los brazos con una probabilidad constante y conocida de antemano. Un ejemplo sería en un estudio con dos brazos tirar una moneda al aire. Si sale cara se asigna a un brazo y si sale cruz se asigna al otro brazo. La probabilidad sería constante y conocida: 1/2. En la actualidad se suele efectuar la asignación utilizando programas de ordenador y se realiza antes del inicio del estudio bien con una tabla o con un generador de números aleatorios oculto al investigador.

La aleatorización restrictiva o equilibrada se realiza igual que la asignación aleatoria simple, pero en ésta se asegura que el número de sujetos de cada brazo del ensayo sea el mismo, con lo cual se facilita el análisis de los resultados. El número de sujetos se puede determinar de forma global o creando bloques con aleatorización interna y entre ellos, pero que pueden o no ser iguales entre sí; teniendo cada elección sus ventajas e inconvenientes.

Cuando alguno de los factores pronósticos presenta gradientes en su definición (por ejemplo pacientes con enfermedad leve o grave), la aleatorización simple o la equilibrada no puede asegurar la adecuada distribución de estos gradientes en cada brazo del estudio. La aleatorización estratificada clasifica los pacientes en diferentes estratos o categorías según determinados criterios pronósticos conocidos antes de la asignación aleatoria. Los pacientes de cada categoría son asignados de forma independiente a cada brazo del estudio mediante un procedimiento de aleatorización propio. De esta manera se consigue que los grupos contengan aproximadamente el mismo número de sujetos en cada gradiente definido.

También conocida como aleatorización por agregados o "cluster design" (del inglés, diseño agrupado), esta técnica agrupa a sujetos heterogéneos entre sí en grupos que se consideran como sujetos individuales de cara al estudio. La homogeneización se realiza entre los grupos seleccionados al igual que la selección del grupo de control. Esta técnica es útil para valorar la utilidad de programas de educación para la salud u otros proyectos en los que exista un gran número de factores pronósticos con la presencia de varios gradientes en los mismos. Así, si por ejemplo queremos valorar la eficacia de un anuncio televisivo para evitar los accidentes de tráfico, los factores pronósticos serían muy variados (edad, sexo, raza, grupo cultural, conducir con o sin licencia, etc.) al igual que los gradientes presentes (antigüedad en la licencia, nivel cultural, duración de la conducción, etc.). Igualmente habría que tener en cuenta la interacción de los individuos en el trabajo, la compra o el colegio, al hablar del anuncio entre ellos. Las técnicas habituales serían muy inexactas o prácticamente imposibles de implementar. En este caso se podría coger como sujetos individuales los pueblos. Se realizaría el estudio en cada pueblo y se compararían los resultados entre ellos. Así, en unos se divulgaría el anuncio televisivo, mientras que en otros no (grupo de control), siendo la selección de los pueblos por sus características similares. Otros ejemplos de grupos podrían ser colegios, o empresas, etc.

El enmascaramiento de todos los involucrados en el estudio es una medida fundamental para lograr neutralizar la subjetividad, fuente permanente de sesgos y prejuicios. Existen tres formas de enmascaramiento:

En dependencia de esas tres formas, los ensayos clínicos pueden ser:

El comité de seguimiento es el encargado de romper el ciego si por alguna circunstancia se sospecha de que la intervención está resultando dañina para algún sujeto. En ocasiones también se puede apreciar que la intervención tiene unos resultados evidentes (positivos o negativos) por lo que la ética obliga a suspender el ensayo y asignar a todos los sujetos al brazo más beneficiado.

A la hora de realizar un estudio debemos definir claramente a qué población va dirigido (población de estudio). Así, sería inútil realizar un estudio sobre los anticonceptivos orales entre mujeres mayores de 70 años. Esta población pertenece a una población diana con unas características determinadas a la que pretendemos generalizar los resultados del estudio. Por ejemplo, no son comparables en muchos aspectos las poblaciones de Nigeria y Noruega. La población estudio representa fielmente las características de esa población general, pero habitualmente no podemos incluir a toda esa población diana en el estudio, por lo que hemos de seleccionar una muestra más pequeña de ella. Esta muestra ha de mantener las características generales de la población estudio por lo que hemos de obtenerla de una forma dirigida y siguiendo una serie de criterios científicos. Si no seleccionamos correctamente la muestra caemos en una desviación sobre lo habitual o sesgo de selección, uno de los errores a evitar a la hora de diseñar un ensayo clínico.

Los criterios que seguiremos para obtener una muestra recibien el nombre de criterios de selección. Los criterios de selección son pautas que definen determinados aspectos de una población y que en su conjunto delimitan a la población estudio. De todos los sujetos que cumplan los criterios de selección se elegirán una parte para realizar el estudio. Los criterios de selección pueden ser:

Además, habremos de tener en cuenta que de los sujetos que cumplen los requisitos y que han sido seleccionados no todos terminarán el estudio. Unos porque no lo empezarán (pérdidas prealeatorización) y otros porque no podrán terminarlo (pérdidas postaleatorización). Estas pérdidas habrán de ser sumadas al total de sujetos para realizar el cálculo del tamaño muestral.

El tamaño de la muestra debe ser el óptimo para detectar las diferencias estadística y clínicamente significativas entre dos intervenciones cuando realmente existen tales diferencias, y no son meramente debidas al azar. Una muestra demasiado grande encarece el estudio, tanto desde el punto de vista económico como de los recursos humanos y físicos. Por otra parte, una muestra demasiado pequeña puede hacer que el estudio sea incapaz de detectar las posibles diferencias entre grupos, llegando a conclusiones erróneas.

Para la correcta elección de la muestra habremos de utilizar técnicas de muestreo o de diseño muestral. En ocasiones, éstas habrán de basarse en técnicas de minería de datos, equivalente para algunos autores a la KDD (acrónimo en inglés de "Knowledge Discovery in Databases", extracción de conocimientos en bases de datos).

En conclusión podemos decir que el estudio clínico ideal es un estudio experimental, analítico, prospectivo, controlado con placebo (si es posible ciego, doble ciego o triple ciego) y aleatorizado y con muestras adecuadas y de tamaño suficiente como para permitir la extrapolación de los resultados a la población diana. Los ensayos clínicos pueden tener una duración desde días a años, sobre una muestra seleccionada de una población a la que se pueden extrapolar los resultados de la intervención y realizado bajo el prisma de la ética.

Los criterios éticos son indispensables dentro de todo ensayo clínico. Los participantes deben estar informados y dar su consentimiento informado cuando son incluidos dentro de un ensayo. Los pacientes deben estar advertidos de los eventuales riesgos de una forma exhaustiva. Los ensayos clínicos deben pasar por un comité de ética. Este comité verificará el interés científico y médico del estudio, la relación riesgo/beneficio, la conformidad con las buenas prácticas metodológicas sobre todo a las que conciernen al promotor y al investigador principal del estudio y la presencia de un seguro que permita indemnizar a los participantes en el estudio en caso de daño. Las relaciones financieras entre los investigadores y los promotores del estudio, cuando existan deben ser anunciadas. Los conflictos de intereses deben ser evitados.

Ezekiel Emanuel refiere siete requisitos que deben guiar la evaluación del marco ético de las propuestas de investigaciones clínicas. Estos requisitos especiales se hacen necesarios porque los sujetos de investigación pasan a ser el medio por el cual se obtiene el conocimiento y en esta condición, el hombre puede ser explotado al exponerlo al riesgo de ser perjudicado en pos del bien de otros. En este contexto, estos requisitos reducen al mínimo el riesgo de explotación y les asegura ser tratados con respeto.

Estos siete requisitos proporcionan un marco sistemático y racional para determinan si una investigación clínica es ética. A su vez, éstos han sido elaborados para guiar el desarrollo y la ejecución de los protocolos y su revisión.
Los requisitos han sido elaborados para ser universales, sin limitaciones a una situación en particular, un país o un grupo de investigación.

La investigación clínica debe tener valor (importancia social, científica o clínica), es decir, que sus resultados deben tener la probabilidad de promover mejoras en la salud, el bienestar o el conocimiento de la población. La razón por la cual una investigación clínica debe tener valor está en dos puntos: el uso responsable de recursos limitados (dinero, espacio y tiempo), esto de la mano del concepto de equidad, y evitar la explotación (la exposición de personas a riesgos y daños potenciales sin obtener resultados valiosos). El requisito de que la investigación clínica sea valiosa asegura a los sujetos de investigación que no serán expuestos a riesgos sin la probabilidad de algún beneficio personal o social.

En este caso, la mala ciencia no es ética. Un estudio con sujetos humanos que ha sido mal diseñado (y por lo tanto no puede producir observaciones reproducibles, o sea, hechos científicos) no es ético. Su metodología debe ser válida y prácticamente realizable, teniendo un objetivo científico claro, estar diseñada usando principios, métodos y prácticas de efecto seguro aceptados, tener poder suficiente para probar definitivamente el objetivo, un plan de análisis de datos verosímil y poder llevarse a cabo.

La búsqueda de validez científica se basa también en los dos principios: utilización de recursos limitados y evitar la explotación. Sin validez sería desperdiciar recursos y no se podría generar ningún conocimiento, producir algún beneficio o justificar la exposición de las personas a riesgos o daños.

Este requisito tiene cuatro facetas:
El requisito de la selección equitativa del sujeto se sustenta en el principio de la equidad distributiva (los beneficios y las cargas de la vida social deben ser distribuidos equitativamente). Así todos deben poder recibir los beneficios obtenidos en la investigación, especialmente, aquellos que corrieron el riesgo, y los riesgos a su vez no deben caer solo en grupos vulnerables.

Tomando en cuenta que el grado de riesgo-beneficio es incierto, siendo mayor la incertidumbre en las primeras etapas, a una investigación clínica se le pide que:

A su vez, todos los riesgos potenciales son sostenidos por los sujetos individuales, pudiendo recibir beneficios potenciales, mientras que los beneficios principales son recibidos por toda la sociedad. Por tal razón es válido tomar en cuenta los riesgos y beneficios potenciales para los sujetos, y los riesgos para los sujetos comparados con los beneficios de la sociedad.

A pesar de carecer de fórmulas que permitan determinar la proporcionalidad del riesgo-beneficio, las evaluaciones de los riesgos y beneficios de la investigación son juicios que pueden implicar normas explícitas basadas en un delineado sistemático, sobre la base de datos existentes, de los tipos potenciales de riesgo y beneficio, su probabilidad de ocurrir y sus consecuencias a largo plazo.

Se debe poner especial cuidado respecto a la cercanía a la explotación cuando los riesgos potenciales de los sujetos individuales superan el potencial de beneficio a la sociedad, sobre todo en la primera fase de la experimentación donde no se espera ningún beneficio para el individuo. Los individuos no sopesan esta situación por lo general, pero los responsables de las políticas habitualmente si lo hacen. Al respecto no existe un marco determinado sobre como se deben balancear esta situación.

Los principios insertos en este requisito son los de beneficencia y no-maleficencia. Esta última sostiene que no se debe causar daño a una persona, por lo que se deben minimizar los riesgos de la investigación. El principio de beneficencia se refiere a la obligación moral de actuar en beneficio de otros, en este caso maximizando los beneficios de la investigación para los sujetos involucrados y la sociedad. Asegurando que los beneficios excedan los riesgos se evita la explotación.

Dado que los investigadores tienen potencial de conflicto de múltiples intereses, pueden involuntariamente distorsionar sus juicios sobre el diseño y la realización de la investigación, al análisis de los datos y su adherencia a los requisitos éticos. Todo esto se puede minimizar por medio de una evaluación independiente realizada por peritos independientes al estudio, y con autoridad para aprobar, enmendar o cancelar la investigación.
Otra razón para hacer una evaluación independiente es la responsabilidad social. Así se vela por el cumplimiento de los requisitos éticos de un estudio o investigación, garantizando a la sociedad que las personas inscritas para los ensayos serán tratadas éticamente y no solo como medios.

Su objetivo es que los sujetos que participan en investigaciones clínicas lo hagan cuando esta sea compatible con sus valores, intereses y preferencias. El consentimiento informado tiene los siguientes requisitos: la provisión de información sobre la finalidad, los riesgos, los beneficios y las alternativas a la investigación y de su propia situación clínica, y la toma de una decisión libre no forzada sobre si participar o no. Con todo esto los sujetos pueden tomar decisiones racionales y libres.

El consentimiento informado atiende a la necesidad del respeto por las personas y a sus decisiones autónomas. Las personas tienen un valor intrínseco debido a su capacidad de elegir, modificar y proseguir su propio plan de vida. El consentimiento informado respeta entonces a la persona y a su autonomía.

Los individuos deben ser respetados durante todo el desarrollo de la investigación, no solo hasta firmar el consentimiento informando. Este respeto implica cinco actividades:


Aunque el uso de diversas sustancias químicas desde el punto de vista terapéutico existe desde tiempos remotos de la historia humana, muy pocas de ellas tenían realmente la acción deseada, como no fuera la derivada del efecto placebo originado por el consumo de tales preparados; debe reconocerse, sin embargo, que era bien poco lo que se conocía acerca de la producción de las enfermedades, así que difícilmente podían establecerse argumentos racionales a favor del uso o no de ciertos químicos, ni siquiera en el caso de aquellos en los que la eficacia terapéutica se demostrara como real (ejemplo: uso de preparados de las hojas de la dedalera o digital).

Pese a los múltiples cambios en el pensamiento científico que se dieron en el siglo XIX, a finales del mismo y principios del siglo XX la situación no era muy distinta, excepto por el hecho de que la manufactura de fármacos se convirtió en un negocio de gran importancia económica. Esta importancia se derivó básicamente de la producción en masa y del acceso de grupos poblacionales cada vez mayores, apareciendo un mercado regido por las leyes usuales de la oferta y la demanda, un mercado que se hizo cada vez más exigente con relación al producto buscado; en otras palabras, se comenzó a requerir una mayor eficacia para los fármacos, conjuntamente con una mayor seguridad en su uso.

Aun así, solo fue en la segunda mitad del siglo XX, cuando comenzaron a aparecer regulaciones que obligaban a comprobar, tan inequívocamente como fuera posible, que los fármacos comercializados fueran a la vez eficaces y seguros (al menos lo suficientemente seguros en el contexto de la patología por tratar). Este tipo de normas tuvo su mayor impulso en la tragedia relacionada con la talidomida, un calmante suave y muy eficaz, pero muy teratogénico, al punto de que se estima que como consecuencia de su uso deben haber ocurrido entre 10.000 y 20.000 nacimientos de bebés con graves deformaciones, las cuales afectaban sobre todo (pero no de manera exclusiva) el desarrollo de los miembros. Aunque esta tragedia dio impulso a mejores métodos para el estudio de nuevos fármacos, también trajo como consecuencia lamentable una considerable renuencia de las compañías farmacéuticas al desarrollo de fármacos para niños o embarazadas.

Así pues, desde la década de los sesenta, se fue instituyendo a nivel mundial la obligación de comprobar tanto la eficacia como la seguridad de las drogas antes de su comercialización, tal y como se describe posteriormente. Debe destacarse que esta obligación no ha impedido que el número de fármacos en el mercado aumente de manera exponencial, traducido en la aparición de numerosas alternativas terapéuticas con eficacia no siempre bien demostrada.

En la actualidad, un ensayo clínico farmacológico es toda evaluación experimental de una sustancia o fármaco, a través de su administración o aplicación a seres humanos, orientada a alguno de los siguientes fines:


Existen cinco fuentes principales de nuevos fármacos:

Se trata del diseño sobre la base de la comprensión del mecanismo patogénico de la enfermedad a nivel molecular. Uno de los casos más notables es el del diseño de las oximas para el tratamiento de la intoxicación por inhibidores de la colinesterasa, ya que estos agentes se derivaron del conocimiento del sitio de la acción tóxica, así como del tipo de interacción química que se presentaba en ese caso; en cierta forma, cuando se descubrió que los antipsicóticos más eficaces bloqueaban a los receptores D2 y 5HT2A, pudieron diseñarse agentes que bloquearan tales receptores. Otro ejemplo es el de la obtención de las estatinas a partir del conocimiento de la molécula del Acetil-coenzima-A. Aunque se trata de la fuente “ideal” de nuevos fármacos, es también una de las menos eficientes, ya que aún el conocimiento sobre las bases de la enfermedad humana son incompletos.

Aunque pudiera parecer lo contrario, este es uno de los métodos más provechosos para la obtención de fármacos nuevos, sobre todo desde el punto de vista económico. Con relación a los grandes costos de la obtención de un nuevo fármaco, el proceso de síntesis química “al azar” (sumado al de la realización de pruebas preliminares de la acción de los compuestos resultantes) resulta sumamente barato.

Aunque los primeros agentes farmacológicos realmente eficaces fueron obtenidos de fuentes naturales (digitálicos, penicilina, opioides, insulina, etc.), el tamizaje de productos naturales fue hasta cierto punto dejado de lado por un tiempo considerable. Sin embargo, esta metodología ha vuelto a ganar adeptos y se han obtenido moléculas que representan verdaderas novedades en campos diversos, como la terapia de las enfermedades neoplásicas (taxol) o el tratamiento de la malaria (artemisininas). Aunque cada vez aumenta el número de fármacos de otros orígenes, todavía en la actualidad alrededor de un 40% de los nuevos agentes deriva de productos naturales (más de la mitad de los cuales son plantas).

Este procedimiento en teoría debería limitarse al mejoramiento de la eficacia y o seguridad de fármacos ya existentes, pero en realidad se utiliza básicamente para la obtención de nuevas moléculas que puedan soslayar los derechos de patente de otras que ya hayan ganado cierto espacio en el mercado farmacéutico ("me too drugs").

Consiste fundamentalmente en el uso de métodos de clonación genética para la obtención de ciertas moléculas peptídicas.

Solo una de cada 5.000 a 10.000 nuevas moléculas evaluadas inicialmente llega a pasar por todo el proceso de evaluación de nuevos fármacos, aunque esto no garantiza que la misma sea finalmente comercializada, y, si es comercializada, no garantiza su persistencia ulterior en el mercado.

Una vez hallada una molécula que pudiera suponer un avance en la terapéutica, la siguiente fase consiste en la realización de pruebas físicas y químicas, básicamente orientadas a determinar la susceptibilidad a la degradación de moléculas potencialmente útiles. Usualmente las moléculas más inestables son rápidamente descartadas o, en el mejor de los casos, se modifican químicamente para aumentar su estabilidad.

Las moléculas más estables pasan entonces a ser probadas desde el punto de vista biológico, comprobando su efecto en diversos modelos experimentales, incluyendo el uso de cultivos celulares, órganos aislados o ensayos en animales de experimentación entre otros.

Estas pruebas biológicas son los primeros ensayos para comprobar tanto la eficacia como la seguridad de un nuevo fármaco y pueden llegar a determinar que no se continúe con el estudio del mismo.

Las pruebas de eficacia no solo implican la observación del efecto propiamente dicho, sino de un estudio farmacocinético y farmacodinámico tan completo como sea posible.

Las pruebas de seguridad en esta fase deben implicar la determinación global de la toxicidad (aguda, subaguda y crónica), los posibles efectos sobre el aparato reproductivo y la posibilidad de mutagénesis y/o carcinogénesis. En estas pruebas se usan dosis elevadas, lo que favorece limitar el número de animales utilizados así como la posible detección de respuestas tóxicas de baja frecuencia.

A los procedimientos descritos a veces se les conoce conjuntamente como Fase Preclínica del estudio de drogas, puesto que pueden conducir a la prueba del nuevo fármaco en humanos, en lo que se consideran las Fases Clínicas del estudio de drogas.

Dado que los animales de laboratorio se presentan como cepas con variabilidad biológica limitada, los estudios realizados con ellos no pueden ser suficientes para determinar sin dudas que un fármaco determinado tendrá las características deseadas de eficacia y seguridad en poblaciones humanas; esto no solo depende de las diferencias entre las especies, sino también de la posibilidad de reacciones que no pueden ser adecuadamente determinadas en animales (cefalea, depresión, tinitus, etc.).

Por esta razón, antes de su posible aprobación, un fármaco debe ser probado en seres humanos, a través de una metodología que distingue tres fases, considerando el estudio y seguimiento de un fármaco después de su comercialización como una cuarta fase.

Representa la primera administración en humanos, generalmente en pequeño número, que rara vez es mayor de 100. Para esta fase, la administración se realiza generalmente en adultos jóvenes sanos de sexo masculino, con el fin de detectar posibles signos incipientes de toxicidad, lo que permitiría determinar luego el rango seguro de dosificación. Los aspectos farmacocinéticos se suelen medir también, aunque su estudio no es el objetivo principal de esta fase.

Si la comprobación preliminar de seguridad en la fase I ha sido satisfactoria, se pasa a esta fase, la cual involucra la administración del fármaco a individuos que presentan la enfermedad para la que se ha concebido su empleo. Este grupo de pacientes debe ser relativamente homogéneo en sus características basales (presentar solo la enfermedad en cuestión) y no se suelen incluir más de 100 a 200 individuos. Se dividen en dos grupos, donde se comparan entre sí, el primer grupo (grupo control) usa los mejores medicamentos disponibles para el tratamiento de la enfermedad implicada y si tales fármacos no existen, la comparación sería con un grupo placebo, y el segundo con los fármacos en estudio.

La finalidad de la fase II es la de establecer mediciones preliminares de la relación eficacia terapéutica/toxicidad (rango terapéutico o margen de seguridad), así como establecer la dosis óptima o sus límites de variación en la condición a tratar.

Si se obtiene razonable evidencia de las fases I y II, comienzan los estudios de fase III, que pueden involucrar múltiples médicos tratando cientos o incluso miles de pacientes. Aparte de verificar la eficacia del medicamento, se busca determinar manifestaciones de toxicidad previamente no detectadas. En esta fase se obtiene una mejor perspectiva de la relación entre seguridad y eficacia, parámetros que han de cuantificarse en el contexto del desorden que se pretenda tratar.

También conocidos como estudios de farmacovigilancia consisten en el seguimiento del fármaco después de que ha sido comercializado. Se busca básicamente la detección de toxicidad previamente insospechada, así como de la evaluación de la eficacia a largo plazo. En la fase IV se pueden detectar reacciones adversas raras, mientras que en las fases previas es excepcional el descubrimiento de aquellas con frecuencia menor a 1/1000. En esta fase también se pueden valorar aspectos nuevos o desconocidos del fármaco que no se hayan probado en las fases anteriores, de tal forma que es posible encontrar aplicaciones potenciales no previstas inicialmente.

Clásicamente el periodo de desarrollo clínico de un producto farmacéutico
se divide en 4 fases consecutivas, pero que se pueden superponer. Estas fases se diferencian por unos objetivos distintos:




FECICLA Fundación Ética y Calidad en Investigación Clínica Latinoamericana


</doc>
<doc id="5394" url="https://es.wikipedia.org/wiki?curid=5394" title="Dispensación">
Dispensación

Dispensación puede referirse a:



</doc>
<doc id="5397" url="https://es.wikipedia.org/wiki?curid=5397" title="Ortografía">
Ortografía

La ortografía (del latín "orthographia" y del griego ὀρθογραφία "orthographía" 'escritura correcta') es el conjunto de reglas y convenciones que rigen el sistema de escritura habitual establecido para una lengua estándar.

La ortografía frecuentemente ha protagonizado debates; la reforma de la ortografía alemana de 1996 llevó a un amplio debate, y finalmente no fue aplicada ni en Austria ni en Suiza. Igualmente la propuesta de reforma ortográfica del francés de 1988 fue ampliamente contestada entre 1988 y 1991, llegando algunos periódicos a boicotear la reforma.

La actual ortografía española empieza a codificarse desde el siglo XVIII, con el establecimiento en 1727 de las primeras normas ortográficas por parte de la Real Academia Española al poco tiempo de su fundación. Hasta ese momento las vacilaciones en las grafías eran constantes: unos optan por soluciones fonémicas, tratando de adecuar su escritura a la pronunciación oral, y otros se decantaban por criterios etimologizantes, manteniendo grafías que carecían de correspondencia en la pronunciación del español de la época. El resultado era una falta de unidad que dificultaba la comprensión.

Actualmente las 22 academias del español mantienen acuerdos que garantizan la unidad ortográfica. De este modo, la edición de la "Ortografía de la lengua española" (1999) fue la primera en ser elaborada con la colaboración consensuada de todas las academias de América y de Filipinas.

Fuentes frecuentes de problemas en el uso de la ortografía son las grafías que presentan igual sonido, como la "b"/"v" (betacismo), "c"/"s"/"z" (seseo y ceceo), "g"/"j", "ll"/"y" (yeísmo). Otros aspectos problemáticos son la utilización correcta de los signos de puntuación y la acentuación gráfica (tildación). La ortografía del español utiliza una variante modificada del alfabeto latino, que consta de los 27 símbolos A, B, C, D, E, F, G, H, I, J, K, L, M, N, Ñ, O, P, Q, R, S, T, U, V, W, X, Y, Z. Asimismo, se emplean también cinco dígrafos para representar otros tantos fonemas: «ch», «ll», «rr», «gu» y «qu», considerados estos dos últimos como variantes posicionales para los fonemas /g/ y /k/. Los dígrafos "ch" y "ll" tienen valores fonéticos específicos, por lo que en la "Ortografía de la lengua española" de 1754 se les comenzó a considerar como letras del alfabeto español y a partir de la publicación de la cuarta edición del "Diccionario de la lengua española" en 1803 se ordenaron separadamente de "c" y "l", fue durante el X Congreso de la Asociación de Academias de la Lengua Española celebrado en Madrid en 1994, y por recomendación de varios organismos, que se acordó reordenar los dígrafos "ch" y "ll" en el lugar que el alfabeto latino universal les asigna, aunque todavía seguían formando parte del abecedario. Con la publicación de la "Ortografía de la lengua española" de 2010, ambas dejaron de considerarse letras del abecedario. Las vocales (A, E, I, O, U) aceptan, además, el acento agudo para indicar la sílaba acentuada y la diéresis o crema modifica a la «u» en las sílabas «gue», «gui» para indicar su sonoridad: «güe», «güi».

Desarrollada en varias etapas a partir del período alfonsino, la ortografía se estandarizó definitivamente bajo la guía de la Real Academia Española, y ha sufrido escasas modificaciones desde la publicación de la "Ortografía de la lengua castellana", de 1854. Las sucesivas decisiones han aplicado criterios a veces fonológicos y a veces etimológicos, dando lugar a un sistema híbrido y fuertemente convencional. Si bien, la correspondencia entre grafía y lenguaje hablado es predecible a partir de la escritura —es decir, un hablante competente es capaz de determinar inequívocamente la pronunciación estimada correcta para casi cualquier texto—, no sucede así a la inversa, existiendo numerosas letras que representan gráficamente fonemas idénticos. Los proyectos de reforma de la grafía en búsqueda de una correspondencia biunívoca, los primeros de los cuales datan del siglo XVII, han sido invariablemente rechazados sin explicaciones, pues ninguno de los fundamentos de la reforma ortográfica ha sido refutado. Es impertinente afirmar que la divergencia de la fonología de la lengua entre sus diversos dialectos impida la elaboración de una grafía puramente fonética, debido a que se cuenta con el castellano estándar, el cual es comprendido por todos los hispanohablantes, y es la base para una reforma ortográfica exitosa. De momento, la mayor parte de los cambios se han limitado a la simplificación de los símbolos homófonos que se conservan por razones etimológicas.

La ortografía del portugués está basada en gran medida en criterios fonológicos, al igual que sucede en español y a diferencia de lo que sucede en francés o inglés, donde factores históricos condicionan la correspondencia entre fonemas y grafías. Sin embargo, debido a la extensión de la lengua y la aparición de numerosas variantes regionales y dialectales, la ortografía usualmente usada no está en relación estrictamente fonológica con la pronunciación de todas las variantes.

A diferencia de lo que ocurre con la ortografía generalmente usada en español moderno la ortografía del inglés no está regulada por una institución, equiparable a la RAE, sino que es una ortografía de consenso. Por esa razón a veces existen diferencias menores entre el inglés británico y el inglés americano y de otros países ("color ~ colour" 'color', "center ~ centre" 'centro', etc). Un principio interesante de la ortografía del inglés es que no usa un criterio puramente fonológico para sus palabras razón por la cual a veces no existe una correspondencia predictible entre la forma escrita y hablada, esto se manifiesta por ejemplo en la variabilidad de pronunciaciones no enteramente predictibles que tiene, por ejemplo, el diptongo "ea":

Esta variabilidad de correspondencia entre la ortografía y la fonología de la lengua se debe a diversos accidentes históricos. En primer lugar, la ortografía del inglés se fijó aproximadamente hacia el siglo XV y desde entonces la lengua ha sufrido importantes cambios fonéticos, especialmente en las vocales, lo cual hace que la ortografía no sea una guía segura para la pronunciación moderna (y en parte la ortografía tiende a reflejar la pronunciación del inglés medio más que la del inglés moderno). Otro segundo factor es el conservadurismo usado en los neologismos con raíces en culturas grecorromanas, el inglés conserva/usa dígrafos como "th, ch, kh, ph" o vocales como "y" (mientras que en español o italiano se han adaptado fonéticamente a /t, k, p/ e /i/. Este conservadurismo también afecta a préstamos léxicos procedentes del francés, que son muy numerosos, para los cuales se mantiene la ortografía original aunque la pronunciación difiere notablemente de la pronunciación francesa.

En tanto que lengua artificial, la ortografía del esperanto propuesta por su creador trató de simplificar las dificultades de correspondencia entre sonido y grafía en las palabras de esta lengua. Así el esperanto tiene una ortografía guiada por criterios eminentemente fonológicos teniendo cada fonema una y solo una grafía posible.

Según Martínez de Sousa, la ortografía técnica comprende:





</doc>
<doc id="5401" url="https://es.wikipedia.org/wiki?curid=5401" title="Culombio">
Culombio

El culombio o coulomb (; símbolo: C) es la unidad derivada del sistema internacional para la medida de la magnitud física de cantidad de electricidad (carga eléctrica). 
Nombrada en honor del físico francés Charles-Augustin de Coulomb.

Se define como la cantidad de carga transportada en un segundo por una corriente de un amperio de intensidad de corriente eléctrica. 

En principio, el culombio sería definido en términos de cantidad de veces la carga elemental.

El culombio puede ser negativo o positivo.
El culombio negativo equivale a 6,241 509 629 152 650×10 veces la carga de un electrón.

El culombio positivo se obtiene de tener un déficit de electrones alrededor a 6,241 509 629 152 650×10, o una acumulación equivalente de cargas positivas.

También puede expresarse en términos de capacidad (F, faradio) y tensión (V, voltio), según la relación:

obtenida directamente de la definición de faradio.

Aunque el culombio es una unidad derivada del Sistema Internacional, en las baterías eléctricas es muy frecuente utilizar la unidad Ah (amperio-hora), que refleja la cantidad de carga total que puede acumular la batería.

La equivalencia es:

formula_3

formula_4

formula_5

A continuación una tabla de los múltiplos y submúltiplos del Sistema Internacional de Unidades.



</doc>
<doc id="5402" url="https://es.wikipedia.org/wiki?curid=5402" title="Arroba">
Arroba

El término arroba (del árabe clásico ربع "rubʿ", «cuarto, cuarta parte») suele verse representado por el símbolo tipográfico «@» en cualquiera de sus acepciones posibles:


</doc>
<doc id="5403" url="https://es.wikipedia.org/wiki?curid=5403" title="Peso">
Peso

En física moderna, el peso es una medida de la fuerza gravitatoria que actúa sobre un objeto. El peso equivale a la fuerza que ejerce un cuerpo sobre un punto de apoyo, originada por la acción del campo gravitatorio local sobre la masa del cuerpo. Por ser una fuerza, el peso se representa como un vector, definido por su módulo, dirección y sentido, aplicado en el centro de gravedad del cuerpo y dirigido aproximadamente hacia el centro de la Tierra. Por extensión de esta definición, también podemos referirnos al peso de un cuerpo en cualquier otro astro (Luna, Marte, entre otros) en cuyas proximidades se encuentre.

La magnitud del peso de un objeto, desde la definición operacional de peso, depende tan solo de la intensidad del campo gravitatorio local y de la masa del cuerpo, en un sentido estricto. Sin embargo, desde un punto de vista legal y práctico, se establece que el peso, cuando el sistema de referencia es la Tierra, comprende no solo la fuerza gravitatoria local, sino también la fuerza centrífuga local debido a la rotación de la Tierra; por el contrario, el empuje atmosférico no se incluye, ni ninguna otra fuerza externa.

Peso y masa son dos conceptos y magnitudes físicas muy diferentes, aunque aún en estos momentos, en el habla cotidiana, el término “peso” se utiliza a menudo erróneamente como sinónimo de masa, la cual es una magnitud gravitacional. La propia Academia reconoce esta confusión en la definición de «pesar»: “Determinar el peso, o más propiamente, la masa de algo por medio de la balanza o de otro instrumento equivalente”.

La masa de un cuerpo es una propiedad intrínseca del mismo, la cantidad de materia, independiente de la intensidad del campo gravitatorio y de cualquier otro efecto. Representa la inercia o resistencia del cuerpo a los cambios de estado de movimiento (aceleración, masa inercial), además de hacerla sensible a los efectos de los campos gravitatorios (masa gravitacional).

El peso de un cuerpo, en cambio, no es una propiedad intrínseca del mismo, ya que depende de la intensidad del campo gravitatorio en el lugar del espacio ocupado por el cuerpo. La distinción científica entre “masa” y “peso” no es importante para muchos efectos prácticos porque la fuerza gravitatoria no experimenta grandes cambios en las proximidades de la superficie terrestre. En un campo gravitatorio constante la fuerza que ejerce la gravedad sobre un cuerpo (su peso) es directamente proporcional a su masa. Pero en realidad el campo gravitatorio terrestre no es constante; puede llegar a variar hasta en un 0,5 % entre los distintos lugares de la Tierra, lo que significa que se altera la relación “masa-peso” con la variación de la fuerza de la gravedad.

Por el contrario, el peso de un mismo cuerpo experimenta cambios muy significativos al cambiar el objeto masivo que crea el campo gravitatorio. Así, por ejemplo, una persona de "60 kg" (6,118 UTM) de masa, pesa "588,60 N" (60 kgf) en la superficie de la Tierra. La misma persona, en la superficie de la Luna pesaría tan solo unos "98,05 N" (10 kgf); sin embargo, su masa seguirá siendo de "60 kg" (6,118 UTM). Nota: En "cursiva", Sistema Internacional; (entre paréntesis), Sistema Técnico de Unidades.

Bajo la denominación de peso aparente se incluyen otros efectos, además de la fuerza gravitatoria y el efecto centrífugo, como la flotación, el carácter no inercial del sistema de referencia (v.g., un ascensor acelerado), etc. El peso que mide el dinamómetro, es en realidad el peso aparente; el peso real sería el que mediría en el vacío en un referencial inercial.

Como el peso es una fuerza, se mide en unidades de fuerza. Sin embargo, las unidades de peso y masa tienen una larga historia compartida, en parte porque su diferencia no fue bien entendida cuando dichas unidades comenzaron a utilizarse.

Este sistema es el prioritario o único legal en la mayor parte de las naciones (excluidas Birmania y los Estados Unidos), por lo que en las publicaciones científicas, en los proyectos técnicos, en las especificaciones de máquinas, etc., las magnitudes físicas se expresan en unidades del Sistema Internacional de Unidades (SI). Así, el peso se expresa en unidades de fuerza del SI, esto es, en newtons (N):


En el Sistema Técnico de Unidades, el peso se mide en kilogramo-fuerza (kgf) o kilopondio (kp), definido como la fuerza ejercida sobre un kilogramo de masa por la aceleración en caída libre (g = 9,80665 m/s²)


También se suele indicar el peso en unidades de fuerza de otros sistemas, como la dina, la libra-fuerza, la onza-fuerza, etcétera.

La dina es la unidad CGS de fuerza y no forma parte del SI. Algunas unidades inglesas, como la libra, pueden ser de fuerza o de masa. Las unidades relacionadas, como el slug, forman parte de sub-sistemas de unidades.

El cálculo del peso de un cuerpo a partir de su masa se puede expresar mediante la segunda ley de la dinámica:

donde el valor de formula_1 es la aceleración de la gravedad en el lugar en el que se encuentra el cuerpo. En primera aproximación, si consideramos a la Tierra como una esfera homogénea, se puede expresar con la siguiente fórmula:


</doc>
<doc id="5411" url="https://es.wikipedia.org/wiki?curid=5411" title="Bolus">
Bolus

El bolus o bolo consiste en la administración enteral O parenteral de un medicamento a una velocidad rápida, pero controlada (por ejemplo, durante 2-3 minutos). Se opone a la administración mediante infusión continua por vía intravenosa (por ejemplo, durante 6 u 8 horas).


</doc>
<doc id="5413" url="https://es.wikipedia.org/wiki?curid=5413" title="Interacción">
Interacción

Interacción hace referencia a varios artículos:


La interacción es una acción recíproca entre dos o más objetos, sustancias, personas o agentes. 



Diferentes tipos de interacciones intermoleculares




</doc>
<doc id="5419" url="https://es.wikipedia.org/wiki?curid=5419" title="Preparación extemporánea">
Preparación extemporánea

La preparación extemporánea, generalmente de un medicamento, es aquella que se lleva a cabo en el momento de su uso, ya que de otra forma se pierden sus principios activos.

La preparación extemporánea de un medicamento generalmente resulta en una solución farmacéutica.

Se suele indicar los gramos que contiene el producto para preparar una cierta cantidad de mililitros (mL) de suspensión extemporánea, así como la cantidad de principio activo que se contienen en una cierta cantidad de mililitros de suspensión extemporánea reconstituida.

Es común que los medicamentos que se expenden como polvos para preparar suspensiones sean considerados como preparados extemporáneos. Un ejemplo de ellos son los antibióticos orales infantiles. Estos consisten en un polvo al que debe agregarse una cantidad determinada de agua para preparar la suspensión. Este tipo de preparados tiene una validez de 7 días si se mantiene fuera de un refrigerador y 14 días si se mantiene dentro de un refrigerador. 

Son ejemplos:


</doc>
<doc id="5421" url="https://es.wikipedia.org/wiki?curid=5421" title="1949">
1949

1949 () fue un año común comenzado en sábado según el calendario gregoriano.











































</doc>
<doc id="5422" url="https://es.wikipedia.org/wiki?curid=5422" title="1948">
1948

1948 () fue un año bisiesto comenzando en jueves según el calendario gregoriano.












16 de noviembre: En Colombia, se funda la Universidad de los Andes. Institución de educación superior privada de carácter laico e independiente de los partidos políticos de ese país.


























</doc>
<doc id="5423" url="https://es.wikipedia.org/wiki?curid=5423" title="1947">
1947

1947 () fue un año normal comenzado en miércoles según el calendario gregoriano.












































</doc>
<doc id="5424" url="https://es.wikipedia.org/wiki?curid=5424" title="1945">
1945

1945 () fue un año normal comenzado en lunes según el calendario gregoriano.

Este año marcó el fin de la Segunda Guerra Mundial, tras la derrota de las Potencias del Eje (la Italia Fascista, la Alemania Nazi y el Japón Imperial respectivamente); las fuerzas aliadas (particularmente Estados Unidos y la Unión Soviética) emergen como superpotencias globales y rápidamente establecen su hegemonía alrededor del mundo; pero eso traería como consecuencia el inicio de una rivalidad ideológica conocida como la Guerra Fría que iniciará dos años después.


































</doc>
<doc id="5425" url="https://es.wikipedia.org/wiki?curid=5425" title="Gran Bretaña">
Gran Bretaña

Gran Bretaña (en inglés Great Britain; en escocés Great Breetain; en galés, Prydain Fawr; en gaélico escocés Breatainn Mhòr; en córnico Breten Veur) es una isla situada en el océano Atlántico Norte frente a la costa noroeste de la Europa continental. Es la mayor isla del archipiélago de las islas británicas. Su superficie es de 229 850 km², siendo la mayor isla de Europa, la novena mayor del mundo y es —después de Java (Indonesia) y Honshu (Japón)— la tercera más poblada del planeta. El territorio de Gran Bretaña está formado por tres naciones: Inglaterra, Gales y Escocia, que junto con Irlanda del Norte forman el Reino Unido de Gran Bretaña e Irlanda del Norte.

El nombre de «Gran Bretaña» procede del latino "Britannia", usado por los romanos para denominar a una provincia que correspondía aproximadamente a Inglaterra, Gales y el sur de Escocia actuales. La etimología de este término ha sido controvertida, pero, en general, se piensa que es probable que sea un derivado de la palabra céltica "britani" ('pintado'), ya que los habitantes de estas islas se pintaban la piel.

Es similar el uso del término "británico" en referencia al Reino Unido. En castellano, el uso de "Britania" se limita al nombre de la antigua provincia romana.

Gran Bretaña es la mayor isla del Reino Unido. Políticamente tiene una organización sumamente compleja. Gran Bretaña se refiere al conjunto de las naciones constituyentes de Inglaterra, Escocia y Gales en combinación, pero no incluye a Irlanda del Norte, situada en la isla de Irlanda; básicamente, también incluye varias islas menores, como la isla de Wight, Anglesey, las islas de Scilly, las Hébridas, Gibraltar y los grupos de islas de Orkney y Shetland. No incluye la isla de Man y las islas del Canal, que son territorios autónomos dependientes. 

La unión política que integró los reinos de Inglaterra y Escocia sucedió en 1707, cuando el Acta de la Unión ratificó el Tratado de Unión de 1706 y fusionó los parlamentos de los dos países, para pasar a conformar el Reino de Gran Bretaña, que cubría toda la isla. Antes de esto, había existido una unión personal e informal entre estos dos países desde 1603, la Unión de las Coronas, en virtud del reinado unificador de Jacobo I de Inglaterra y VI de Escocia.

La geología de Gran Bretaña es bastante similar a la de las zonas adyacentes en el continente europeo. En efecto, estudios recientes, publicados en julio de 2007, señalan que la actual isla de Gran Bretaña hasta hace 200 000 años era una gran península de Europa continental, península unida al resto de la macro unidad geográfica (MUG) europea por un istmo llamado «Risco Weald-Artois». Hasta esa fecha, en plena glaciación de Riss o Illinois, al norte del istmo se encontraba, ocupando la parte meridional del mar del Norte, un gran lago de agua salada.


</doc>
