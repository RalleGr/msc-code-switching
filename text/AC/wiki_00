<doc id="23508" url="https://en.wikipedia.org/wiki?curid=23508" title="Plymouth">
Plymouth

Plymouth () is a port city situated on the south coast of Devon, England, approximately south-west of Exeter and west-south-west of London. Enclosing the city are the mouths of the river Plym and river Tamar, which are naturally incorporated into Plymouth Sound to form a boundary with Cornwall.

Plymouth's early history extends to the Bronze Age when a first settlement emerged at Mount Batten. This settlement continued as a trading post for the Roman Empire, until it was surpassed by the more prosperous village of Sutton founded in the ninth century, now called Plymouth. In 1620, the Pilgrim Fathers departed Plymouth for the New World and established Plymouth Colony, the second English settlement in what is now the United States of America. During the English Civil War, the town was held by the Parliamentarians and was besieged between 1642 and 1646.

Throughout the Industrial Revolution, Plymouth grew as a commercial shipping port, handling imports and passengers from the Americas, and exporting local minerals (tin, copper, lime, china clay and arsenic). The neighbouring town of Devonport became strategically important to the Royal Navy for its shipyards and dockyards. In 1914, three neighbouring independent towns, viz. the county borough of Plymouth, the county borough of Devonport, and the urban district of East Stonehouse were merged, becoming the County Borough of Plymouth. In 1928, it achieved city status. During World War II, due to the city's naval importance, the German military targeted and partially destroyed the city by bombing, an act known as the Plymouth Blitz. After the war, the city centre was completely rebuilt. Subsequent expansion led to the incorporation of Plympton, Plymstock, and other outlying suburbs, in 1967.

The city is home to () people, making it the 30th-most populous built-up area in the United Kingdom and the second-largest city in the South West, after Bristol. It is governed locally by Plymouth City Council and is represented nationally by three MPs. Plymouth's economy remains strongly influenced by shipbuilding and seafaring but has tended toward a service economy since the 1990s. It has ferry links to Brittany (Roscoff and St Malo) and to Spain (Santander). It has the largest operational naval base in Western Europe, HMNB Devonport, and is home to the University of Plymouth.

Upper Palaeolithic deposits, including bones of Homo sapiens, have been found in local caves, and artefacts dating from the Bronze Age to the Middle Iron Age have been found at Mount Batten, showing that it was one of few principal trading ports of pre-Roman Britannia dominating continental trade with Armorica. An unidentified settlement named "TAMARI OSTIA" (mouth/estuaries of the Tamar) is listed in Ptolemy's "Geographia" and is presumed to be located in the area of the modern city. An ancient promontory fort was located at Rame Head at the mouth of Plymouth Sound with ancient hillforts located at Lyneham Warren to the east , Boringdon Camp and Maristow Camp to the north .

The settlement of Plympton, further up the River Plym than the current Plymouth, was also an early trading port. As the river silted up in the early 11th century, mariners and merchants were forced to settle downriver, at the current day Barbican near the river mouth. At the time this village was called Sutton, meaning "south town" in Old English. The name "Plym Mouth", meaning "mouth of the River Plym" was first mentioned in a Pipe Roll of 1211.
The name "Plymouth" first officially replaced Sutton in a charter of King Henry VI in 1440. See Plympton for the derivation of the name "Plym".

During the Hundred Years' War a French attack (1340) burned a manor house and took some prisoners, but failed to get into the town. In 1403 the town was burned by Breton raiders. On 12 November 1439, the English Parliament made Plymouth the first town incorporated. In the late fifteenth century, Plymouth Castle, a "castle quadrate", was constructed close to the area now known as The Barbican; it included four round towers, one at each corner, as featured on the city coat of arms.

The castle served to protect Sutton Pool, which is where the fleet was based in Plymouth prior to the establishment of Plymouth Dockyard. In 1512, an Act of Parliament was passed to further fortify Plymouth. The work included defensive walls at the entrance to Sutton Pool (across which a chain was extended in times of danger). Defences on St Nicholas Island also date from this time, and a string of six artillery blockhouses were built, including one on Fishers Nose at the south-eastern corner of the Hoe. This location was further strengthened by the building of a fort (later known as Drake's Fort) in 1596; it was the site of the Citadel, established in the 1660s (see below).
During the 16th century, locally produced wool was the major export commodity. Plymouth was the home port for successful maritime traders, among them Sir John Hawkins, who led England's first foray into the Atlantic slave trade, as well as Sir Francis Drake, Mayor of Plymouth in 1581–2. Crews for the first English failed settlement attempt at Roanoke Colony in North America departed in 1587 under Sir Walter Raleigh's and Drake's leadership; returning bearing maize, tobacco and potatoes.
In 1588, according to legend, Drake insisted on completing his game of bowls on the Hoe before engaging the Spanish Armada. In 1620 the Pilgrim Fathers set sail for the New World from Plymouth, establishing Plymouth Colony – the second English colony in what is now the United States of America.

During the English Civil War Plymouth sided with the Parliamentarians and was besieged for almost four years by the Royalists. The last major attack by the Royalists was by Sir Richard Grenville leading thousands of soldiers towards Plymouth, but they were defeated by the Plymothians at Freedom Fields Park. The civil war ended as a Parliamentary win, but monarchy was restored by King Charles II in 1660, who imprisoned many of the Parliamentary heroes on Drake's Island. Construction of the Royal Citadel began in 1665, after the Restoration; it was armed with cannon facing both out to sea and into the town, rumoured to be a reminder to residents not to oppose the Crown. Mount Batten tower also dates from around this time.

Throughout the 17th century, Plymouth had gradually lost its pre-eminence as a trading port. By the mid-17th century, commodities manufactured elsewhere in England cost too much to transport to Plymouth, and the city had no means of processing sugar or tobacco imports, major products from the colonies. Local sailors turning to piracy such as Henry Every became infamous, celebrated in the London play The Successful Pyrate. It played a part in the Atlantic slave trade during the early 18th century, although it was relatively small.

In the nearby parish of Stoke Damerel the first dockyard, HMNB Devonport, opened in 1690 on the eastern bank of the River Tamar. Further docks were built here in 1727, 1762 and 1793. The settlement that developed here was called "Dock" or "Plymouth Dock" at the time, and a new town, separate from Plymouth, grew up. In 1712 there were 318 men employed and by 1733 the population had grown to 3,000 people.

Before the latter half of the 18th century, grain, timber and then coal were Plymouth's main imports. During this time the real source of wealth was from the neighbouring town of Plymouth Dock (renamed in 1824 to Devonport) and the major employer in the entire region was the dockyard. The "Three Towns" conurbation of Plymouth, Stonehouse and Devonport enjoyed some prosperity during the late 18th and early 19th century and were enriched by a series of neo-classical urban developments designed by London architect John Foulston. Foulston was important for both Devonport and Plymouth and was responsible for several grand public buildings, many now destroyed, including the Athenaeum, the Theatre Royal and Royal Hotel, and much of Union Street.

Local chemist William Cookworthy established his short-lived Plymouth Porcelain venture in 1768 to exploit the deposits of china clay that he had discovered in Cornwall. He was acquainted with engineer John Smeaton, the builder of the third Eddystone Lighthouse.

The Breakwater in Plymouth Sound was designed by John Rennie to protect the fleet moving in and out of Devonport; work started in 1812. Numerous technical difficulties and repeated storm damage meant that it was not completed until 1841, twenty years after Rennie's death. In the 1860s, a ring of Palmerston forts was constructed around the outskirts of Devonport, to protect the dockyard from attack from any direction.

Some of the most significant imports to Plymouth from the Americas and Europe during the latter half of the 19th century included maize, wheat, barley, sugar cane, guano, sodium nitrate and phosphate. Aside from the dockyard in the town of Devonport, industries in Plymouth such as the gasworks, the railways and tramways, and a number of small chemical works had begun to develop in the 19th century, continuing into the 20th century.

During the First World War, Plymouth was the port of entry for many troops from around the Empire. It was developed as a facility for the manufacture of munitions. Although major units of the Royal Navy moved to the safety of Scapa Flow, Devonport was an important base for escort vessels and repairs. Flying boats operated from Mount Batten.

During the Second World War, Devonport was the headquarters of Western Approaches Command until 1941, and Sunderland flying boats were operated by the Royal Australian Air Force. It was an important embarkation point for US troops for D-Day. The city was heavily bombed by the Luftwaffe, in a series of 59 raids known as the Plymouth Blitz. Although the dockyards were the principal targets, much of the city centre and over 3,700 houses were completely destroyed and more than 1,000 civilians lost their lives. This was largely due to Plymouth's status as a major port. Charles Church was hit by incendiary bombs and partially destroyed in 1941 during the Blitz, but has not been demolished. It has been designated as an official permanent monument to the bombing of Plymouth during World War II.

The redevelopment of the city was planned by Sir Patrick Abercrombie in his 1943 "Plan for Plymouth" whilst simultaneously working on the reconstruction plan for London. Between 1951 and 1957 over 1000 homes were completed every year, mostly using innovative prefabricated systems of just three main types/

The Plan for Plymouth was, on the one hand, a template for the rapid reassembly of a destroyed city centre, but Abercrombie also took the opportunity to lay out a whole hierarchy of settlements across the city of communities, neighbourhoods and districts. Central to this was a revision of transport infrastructure that prioritised the position of the railway as a gateway to the city centre and provided in the long-term for a dual carriageway road by-pass that only finally came into being in the 1980s (forty years after being planned).

By 1964 over 20,000 new homes had been built, transforming the dense overcrowded and unsanitary slums of the pre-war city into a low density, dispersed suburbia. Most of the city centre shops had been destroyed and those that remained were cleared to enable a zoned reconstruction according to his plan. In 1962 the modernist high rise of the Civic Centre was constructed, an architecturally significant example of mid-twentieth century civic slab-and-tower set piece. The Plymouth City Council allowed it to fall into disrepair but it was grade II listed in 2010 by English Heritage to prevent its demolition.

Post-war, Devonport Dockyard was kept busy refitting aircraft carriers such as the and, later, nuclear submarines. New light industrial factories were constructed in the newly zoned industrial sector, attracting rapid growth of the urban population. The army had substantially left the city by 1971, after barracks were pulled down in the 1960s. But the city remains home to the 42 Commando of the Royal Marines.

The first record of the existence of a settlement at Plymouth was in the Domesday Book in 1086 as "Sudtone", Saxon for south farm, located at the present-day Barbican. From Saxon times, it was in the hundred of Roborough. In 1254 it gained status as a town and in 1439, became the first town in England to be granted a Charter by Parliament. Between 1439 and 1934, Plymouth had a Mayor. In 1914 the county boroughs of Plymouth and Devonport, and the urban district of East Stonehouse merged to form a single county borough of Plymouth. Collectively they were referred to as "The Three Towns".

In 1919 Nancy Astor was elected the first-ever female member of parliament to take office in the British Houses of Parliament for the constituency of Plymouth Sutton. Taking over office from her husband Waldorf Astor, Lady Astor was a vibrantly active campaigner for her resident constituents. Plymouth was granted city status on 18 October 1928. The city's first Lord Mayor was appointed in 1935 and its boundaries further expanded in 1967 to include the town of Plympton and the parish of Plymstock.

In 1945, Plymouth-born Michael Foot was elected Labour MP for the war-torn constituency of Plymouth Devonport and after serving as Secretary of State for Education and responsible for the 1974 Health and Safety at Work Act, went on to become the leader of the Labour party (1980–1983).

The 1971 Local Government White Paper proposed abolishing county boroughs, which would have left Plymouth, a town of 250,000 people, being administered from a council based at the smaller Exeter, on the other side of the county. This led to Plymouth lobbying for the creation of a Tamarside county, to include Plymouth, Torpoint, Saltash, and the rural hinterland. The campaign was not successful, and Plymouth ceased to be a county borough on 1 April 1974 with responsibility for education, social services, highways and libraries transferred to Devon County Council. All powers returned when the city become a unitary authority on 1 April 1998 under recommendations of the Banham Commission.

In the Parliament of the United Kingdom, Plymouth is represented by the three constituencies of Plymouth Moor View, Plymouth Sutton and Devonport and South West Devon and within the European Parliament as South West England. In the 2017 general election the city two returned Conservative MPs, who were Gary Streeter (for South West Devon) and Johnny Mercer (for Moor View), and one Labour MP, Luke Pollard (for Sutton and Devonport), .

The City of Plymouth is divided into 20 wards, 17 of which elect three councillors and the other three electing two councillors, making up a total council of 57. Each year a third of the council is up for election for three consecutive years – there are no elections on the following "fourth" year, which is when County Council elections take place. The total for Plymouth was 188,924 in April 2015. The local election of 7 May 2015 resulted in a political composition of 28 Labour councillors, 26 Conservative and 3 UKIP resulting in a Conservative UKIP coalition administration. Plymouth City Council is formally twinned with: Brest, France (1963), Gdynia, Poland (1976), Novorossiysk, Russia (1990) San Sebastián, Spain (1990) and Plymouth, United States (2001).

Plymouth was granted the dignity of Lord Mayor by King George V in 1935. The position is elected each year by a group of six councillors. It is traditional that the position of the Lord Mayor alternates between the Conservative Party and the Labour Party annually and that the Lord Mayor chooses the Deputy Lord Mayor. Conservative councillor Dr John Mahony is the incumbent for 2015–16.
The Lord Mayor's official residence is 3 Elliot Terrace, located on the Hoe. Once a home of Waldorf and Nancy Astor, it was given by Lady Astor to the City of Plymouth as an official residence for future Lord Mayors and is also used today for civic hospitality, as lodgings for visiting dignitaries and High Court judges and it is also available to hire for private events. The Civic Centre municipal office building in Armada Way became a listed building in June 2007 because of its quality and period features, but has become the centre of a controversy as the council planned for its demolition estimating that it could cost £40m to refurbish it, resulting in possible job losses.

Plymouth lies between the River Plym to the east and the River Tamar to the west; both rivers flow into the natural harbour of Plymouth Sound. Since 1967, the unitary authority of Plymouth has included the, once independent, towns of Plympton and Plymstock which lie along the east of the River Plym. The River Tamar forms the county boundary between Devon and Cornwall and its estuary forms the Hamoaze on which is sited Devonport Dockyard.

The River Plym, which flows off Dartmoor to the north-east, forms a smaller estuary to the east of the city called Cattewater. Plymouth Sound is protected from the sea by the Plymouth Breakwater, in use since 1814. In the Sound is Drake's Island which is seen from Plymouth Hoe, a flat public area on top of limestone cliffs. The Unitary Authority of Plymouth is . The topography rises from sea level to a height, at Roborough, of about above Ordnance Datum (AOD).

Geologically, Plymouth has a mixture of limestone, Devonian slate, granite and Middle Devonian limestone. Plymouth Sound, Shores and Cliffs is a Site of Special Scientific Interest, because of its geology. The bulk of the city is built upon Upper Devonian slates and shales and the headlands at the entrance to Plymouth Sound are formed of Lower Devonian slates, which can withstand the power of the sea.

A band of Middle Devonian limestone runs west to east from Cremyll to Plymstock including the Hoe. Local limestone may be seen in numerous buildings, walls and pavements throughout Plymouth. To the north and northeast of the city is the granite mass of Dartmoor; the granite was mined and exported via Plymouth. Rocks brought down the Tamar from Dartmoor include ores containing tin, copper, tungsten, lead and other minerals. There is evidence that the middle Devonian limestone belt at the south edge of Plymouth and in Plymstock was quarried at West Hoe, Cattedown and Radford.

On 27 April 1944 Sir Patrick Abercrombie's "Plan for Plymouth" to rebuild the bomb-damaged city was published; it called for demolition of the few remaining pre-War buildings in the city centre to make way for their replacement with wide, parallel, modern boulevards aligned east–west linked by a north–south avenue (Armada Way) linking the railway station with the vista of Plymouth Hoe.

A peripheral road system connecting the historic Barbican on the east and Union Street to the west determines the principal form of the city centre, even following pedestrianisation of the shopping centre in the late 1980s, and continues to inform the present 'Vision for Plymouth' developed by a team led by Barcelona-based architect David MacKay in 2003 which calls for revivification of the city centre with mixed-use and residential.

In suburban areas, post-War prefabs had already begun to appear by 1946, and over 1,000 permanent council houses were built each year from 1951 to 1957 according to the Modernist zoned low-density garden city model advocated by Abercrombie. By 1964 over 20,000 new homes had been built, more than 13,500 of them permanent council homes and 853 built by the Admiralty.

Plymouth is home to 28 parks with an average size of . Its largest park is Central Park, with other sizeable green spaces including Victoria Park, Freedom Fields Park, Alexandra Park, Devonport Park and the Hoe. Central Park is the home of Plymouth Argyle Football Club and a number of other leisure facilities.

The Plymouth Plan 2019–2034 was published May 2019 and sets the direction for future development with a new spatial strategy which reinforces links with the wider region in west Devon and east Cornwall in its Joint Local Plan and identifies three development areas within the city: the City centre and waterfront; a 'northern corridor' including Derriford and the vacant airfield site at Roborough; and an 'eastern corridor' including major new settlements at Sherford and Langage.

Plymouth has a temperate oceanic climate bordering a type of Mediterranean climate (Köppen "Cfb") which is wetter and milder than the rest of England. This means a wide range of exotic plants and palm trees and yuccas can be cultivated The annual mean temperature is approximately . Due to the moderating effect of the sea and the southwesterly location, the climate is among the mildest of British cities, and one of the warmest UK cities in winter the uk met office predicts the Devon and Cornwall region will become the hottest in the uk in the future The with the warmest months, July and August, having mean daily maxima over . The coldest month of February is similarly moderate, having mild mean minimum temperatures between . Snow is extremely rare, usually no more than a few flakes when it occurs, but a noteworthy recent exception was the period of the European winter storms of 2009-10 which, in early January 2010, covered Plymouth in at least of snow; more on higher ground. Another notable event was the of snowfall between 17–19 December 2010 – though only would lie at any one time due to melting. Over the 1961–1990 period, annual snowfall accumulation averaged less than per year.

South West England has a favoured location when the Azores High pressure area extends north-eastwards towards the UK, particularly in summer. Coastal areas have average annual sunshine totals over 1,600 hours.

Owing to its geographic location, rainfall tends to be associated with Atlantic depressions or with convection and is more frequent than in London and Southeast England. The Atlantic depressions are more vigorous in autumn and winter and most of the rain which falls in those seasons in the south-west is from this source. Average annual rainfall is around . November to March have the highest mean wind speeds, with June to August having the lightest winds. The predominant wind direction is from the south-west.

Typically, the warmest day of the year (1971–2000) will achieve a temperature of , although in June 2020 the temperature reached 32.2 degrees(89.7)F, the site record. On average, 4.25 days of the year will report a maximum temperature of or above. During the winter half of the year, the coldest night will typically fall to although in January 1979 the temperature fell to . Typically, 18.6 nights of the year will register an air frost.

The University of Plymouth enrolls 23,155 total students as of 2018/2019 (largest in the UK out of ). It also employs 2,900 staff with an annual income of around £160 million. It was founded in 1992 from Polytechnic South West (formerly Plymouth Polytechnic) following the Further and Higher Education Act 1992. It has a wide range of courses including those in marine focused business, marine engineering, marine biology and Earth, ocean and environmental sciences, surf science, shipping and logistics. The university formed a joint venture with the fellow Devonian University of Exeter in 2000, establishing the Peninsula College of Medicine and Dentistry. The college is ranked 8th out of 30 universities in the UK in 2011 for medicine. Its dental school was established in 2006, which also provides free dental care in an attempt to improve access to dental care in the South West.

The University of St Mark & St John (known as "Marjon" or "Marjons") specialises in teacher training, and offers training across the country and abroad.

The city is also home to two large colleges. The City College Plymouth provides courses from the most basic to Foundation degrees for approximately 26,000 students. Plymouth College of Art offers a selection of courses including media. It was started 153 years ago and is now one of only four independent colleges of art and design in the UK.

Plymouth also has 71 state primary phase schools, 13 state secondary schools, eight special schools and three selective state grammar schools, Devonport High School for Girls, Devonport High School for Boys and Plymouth High School for Girls. There is also an independent school Plymouth College.

The city was also home to the Royal Naval Engineering College; opened in 1880 in Keyham, it trained engineering students for five years before they completed the remaining two years of the course at Greenwich. The college closed in 1910, but in 1940 a new college opened at Manadon. This was renamed "Dockyard Technical College" in 1959 before finally closing in 1994; training was transferred to the University of Southampton.

Plymouth is home to the Marine Biological Association of the United Kingdom (MBA; founded 1884) which conducts research in all areas of the marine sciences. The Plymouth Marine Laboratory (PML; founded 1988) was formed in part from components of the MBA. Together with the National Marine Aquarium, the Sir Alister Hardy Foundation for Ocean Sciences, Plymouth University's Marine Institute and the Diving Diseases Research Centre, these marine-related organisations form the Plymouth Marine Sciences Partnership. The Plymouth Marine Laboratory, which focuses on global issues of climate change and sustainability. It monitors the effects of ocean acidity on corals and shellfish and reports the results to the UK government. It also cultivates algae that could be used to make biofuels or in the treatment of wastewater by using technology such as photo-bioreactors. It works alongside the Boots Group to investigate the use of algae in skincare protects, taking advantage of the chemicals they contain that adapt to protect themselves from the sun.

A scheme is in operation over summer 2018 to provide meals during the summer holidays for children with parents on a low income, the parents cannot afford to provide their children with healthy meals.

UPSU also known as the University of Plymouth Student Union is based underground near the library. Every student at the University of Plymouth is a member of UPSU. The Union employs students across the University, from bar staff to events technicians. Every year the students at the University have an opportunity to vote which sabbatical officers represent them. In 2019 over 4000 students voted in the UPSU elections.

From the 2011 Census, the Office for National Statistics published that Plymouth's unitary authority area population was 256,384; 15,664 more people than that of the last census from 2001, which indicated that Plymouth had a population of 240,720. The Plymouth urban area had a population of 260,203 in 2011 (the urban sprawl which extends outside the authority's boundaries). The city's average household size was 2.3 persons. At the time of the 2011 UK census, the ethnic composition of Plymouth's population was 96.2% White (of 92.9% was White British), with the largest minority ethnic group being Chinese at 0.5%. The white Irish ethnic group saw the largest decline in its share of the population since the 2001 Census (−24%), while the "Other Asian" and Black African had the largest increases (360% and 351% respectively). This excludes the two new ethnic groups added to the 2011 census of Gypsy or Irish Traveller and Arab. The population rose rapidly during the second half of the 19th century, but declined by over 1.6% from 1931 to 1951.

Plymouth's gross value added (a measure of the size of its economy) was 5,169 million GBP in 2013 making up 25% of Devon's GVA. Its GVA per person was £19,943 and compared to the national average of £23,755, was £3,812 lower. Plymouth's unemployment rate was 7.0% in 2014 which was 2.0 points higher than the South West average and 0.8 points higher than the average for Great Britain (England, Wales and Scotland).

A 2014 profile by the National Health Service showed Plymouth had higher than average levels of poverty and deprivation (26.2% of the population among the poorest 20.4% nationally). Life expectancy, at 78.3 years for men and 82.1 for women, was the lowest of any region in the South West of England.

Because of its coastal location, the economy of Plymouth has traditionally been , in particular the defence sector with over 12,000 people employed and approximately 7,500 in the armed forces. The Plymouth Gin Distillery has been producing Plymouth Gin since 1793, which was exported around the world by the Royal Navy. During the 1930s, it was the most widely distributed gin and has a controlled term of origin until 2015. Since the 1980s, employment in the defence sector has decreased substantially and the public sector is now prominent particularly in administration, health, education, medicine and engineering.

Devonport Dockyard is the UK's only naval base that refits nuclear submarines and the Navy estimates that the Dockyard generates about 10% of Plymouth's income. Plymouth has the largest cluster of marine and maritime businesses in the south west with 270 firms operating within the sector. Other substantial employers include the university with almost 3,000 staff, the national retail chain The Range at their Estover headquarters, as well as the Plymouth Science Park employing 500 people in 50 companies.

Plymouth has a post-war shopping area in the city centre with substantial pedestrianisation. At the west end of the zone inside a grade II listed building is the Pannier Market that was completed in 1959 – "pannier" meaning "basket" from French, so it translates as "basket market". In terms of retail floorspace, Plymouth is ranked in the top five in the South West, and 29th nationally. Plymouth was one of the first ten British cities to trial the new Business improvement district initiative. The Tinside Pool is situated at the foot of the Hoe and became a grade II listed building in 1998 before being restored to its 1930s look for £3.4 million.

Since 2003, Plymouth Council has been undertaking a project of urban redevelopment called the "Vision for Plymouth" launched by the architect David Mackay and backed by both Plymouth City Council and the Plymouth Chamber of Commerce (PCC). Its projects range from shopping centres, a cruise terminal, a boulevard and to increase the population to 300,000 and build 33,000 dwellings.
In 2004 the old Drake Circus shopping centre and Charles Cross car park were demolished and replaced by the latest Drake Circus Shopping Centre, which opened in October 2006. It received negative feedback before opening when David Mackay said it was already "ten years out of date". In contrast, the Theatre Royal's production and education centre, TR2, which was built on wasteland at Cattedown, was a runner-up for the RIBA Stirling Prize for Architecture in 2003.

There is a project involving the future relocation of Plymouth City Council's headquarters, the civic centre, to the current location of the Bretonside bus station; it would involve both the bus station and civic centre being demolished and a rebuilt together at the location with the land from the civic centre being sold off. Other suggestions include the demolition of the Plymouth Pavilions entertainment arena to create a canal "boulevard" linking Millbay to the city centre. Millbay is being regenerated with mixed residential, retail and office space alongside the ferry port.

The A38 dual-carriageway runs from east to west across the north of the city. Within the city it is known as 'The Parkway' and represents the boundary between the older parts of the city and more recently developed suburban areas. Heading east, it connects Plymouth to the M5 motorway about away near Exeter; and heading west it connects Devon with Cornwall via the Tamar Bridge. Bus services are mainly provided by Plymouth Citybus and Stagecoach, but a few routes are served by smaller local operators. There are three Park and ride services at Milehouse, Coypool (Plympton) and George Junction (Plymouth City Airport), which are operated by Stagecoach South West.
A regular international ferry service provided by Brittany Ferries operates from Millbay taking cars and foot passengers directly to France (Roscoff) and Spain (Santander) on the three ferries, "MV Armorique", "MV Bretagne" and "MV Pont-Aven". The Cremyll Ferry is a passenger ferry between Stonehouse and the Cornish hamlet of Cremyll, which is believed to have operated continuously since 1204. There is also a pedestrian ferry from the Mayflower Steps to Mount Batten, and an alternative to using the Tamar Bridge via the Torpoint Ferry (vehicle and pedestrian) across the River Tamar.

The city's airport was Plymouth City Airport about north of the city centre.
The airport was home to the local airline Air Southwest,
which operated flights across the United Kingdom and Ireland. In June 2003, a report by the South West RDA was published looking at the future of aviation in the south-west and the possible closure of airports. It concluded that the best option for the south-west was to close Plymouth City Airport and expand Exeter International Airport and Newquay Cornwall Airport, although it did conclude that this was not the best option for Plymouth. In April 2011, it was announced that the airport would close, which it did on 23 December. A local company, FlyPlymouth, put forward plans in 2015 to reopen the airport by 2018, providing daily services to various destinations including London, but as of now, these projects have stalled.

Plymouth railway station, which opened on its present site in 1877, is managed by Great Western Railway and is also served by trains on the CrossCountry network. The station was previously described as 'Plymouth North Road', when there were other main line stations in the city at Millbay and Friary. These have now closed. Smaller stations in the suburban area west of the city centre are served by trains on the Tamar Valley Line to Gunnislake and local services on the Cornish Main Line, which crosses the Tamar on the Royal Albert Bridge. This was designed by Brunel and opened in 1859. The parallel road bridge was completed in 1961.

There have been proposals to reopen the Exeter to Plymouth railway of the LSWR which would connect Cornwall and Plymouth to Exeter using the former Southern Railway main line from Plymouth to Exeter via Okehampton, because the main line through South Devon is vulnerable to damage from rough seas at Dawlish, where some of the cliffs are also fragile. There are related proposals to reopen part of the old main line from Bere Alston on the Plymouth-Gunnislake line as far as Tavistock to serve a new housing development, but although the idea has been discussed since 2008 at least progress has been slow.

Plymouth is at the southern end of the long Devon Coast to Coast Cycle Route (National Cycle Route 27). The route runs mostly traffic-free on off-road sections between Ilfracombe and Plymouth. The route uses former railway lines, though there are some stretches on public roads.

Plymouth has about 150 churches and its Roman Catholic cathedral (1858) is in Stonehouse. The city's oldest church is Plymouth Minster, also known as St Andrew's Church, (Anglican) located at the top of Royal Parade—it is the largest parish church in Devon and has been a site of gathering since AD 800. The city also includes five Baptist churches, over twenty Methodist chapels, and thirteen Roman Catholic churches. In 1831 the first Brethren assembly in England, a movement of conservative non-denominational Evangelical Christians, was established in the city, so that Brethren are often called Plymouth Brethren, although the movement did not begin locally.

Plymouth has the first known reference to Jews in the South West from Sir Francis Drake's voyages in 1577 to 1580, as his log mentioned "Moses the Jew" – a man from Plymouth. The Plymouth Synagogue is a Listed Grade II* building, built in 1762 and is the oldest Ashkenazi Synagogue in the English speaking world. There are also places of worship for Islam, Bahá'í, Buddhism, Unitarianism, Chinese beliefs and Humanism.

58.1% of the population described themselves in the 2011 census return as being at least nominally Christian and 0.8% as Muslim with all other religions represented by less than 0.5% each. The portion of people without a religion is 32.9%; above the national average of 24.7%. 7.1% did not state their religious belief. Since the 2001 Census, the number of Christians and Jews has decreased (−16% and −7% respectively), while all other religions have increased and non-religious people have almost doubled in number.

Built in 1815, Union Street was at the heart of Plymouth's historical culture. It became known as "the servicemen's playground", as it was where sailors from the Royal Navy would seek entertainment of all kinds. During the 1930s, there were 30 pubs and it attracted such performers as Charlie Chaplin to the New Palace Theatre. It was described in 2008 as the late-night hub of Plymouth's entertainment strip.

Outdoor events and festivals are held including the annual British Firework Championships in August, which attracts tens of thousands of people across the waterfront. In August 2006 the world record for the most simultaneous fireworks was surpassed, by Roy Lowry of the University of Plymouth, over Plymouth Sound. From 2014 MTV Crashes Plymouth has taken place every July on Plymouth Hoe, hosting big-name acts such as The 1975, Little Mix, Tinie Tempah and Busted. Between 1992 and 2012 the Music of the Night celebration was performed in the Royal Citadel by the 29 Commando Regiment and local performers to raise money for local and military charities. A number of other smaller cultural events taken place annually, including Plymouth Art Weekender, Plymouth Fringe Festival and Illuminate Festival.

The city's main theatre is Theatre Royal Plymouth, presenting large-scale West End shows and smaller works as well as an extensive education and outreach programme. The main building is located in the city centre and contains three performance spaces – The Lyric (1,315 capacity), Drum Theatre (200 capacity), and The Lab (60 capacity) – and they also run their own specialised production and creative learning centre called TR2, based in Cattedown. Plymouth Pavilions has multiple uses for the city staging music concerts, basketball matches and stand-up comedy. There are also three cinemas: Reel Cinema at Derrys Cross, Plymouth Arts Centre at Looe Street and a Vue cinema at the Barbican Leisure Park. Barbican Theatre, Plymouth delivers a theatre and dance programme of performances and workshops focused on young people and emerging artists contains a main auditorium (110 – 140 capacity) and rehearsal studio; they also host the B-Bar (80 capacity), which offers a programme of music, comedy and spoken word performance. The Plymouth Athenaeum, which includes a local interest library, is a society dedicated to the promotion of learning in the fields of science, technology, literature and art. In 2017 its auditorium (340 capacity) returned to use as a theatre, having been out of service since 2009. The Plymouth City Museum and Art Gallery is operated by Plymouth City Council allowing free admission – it has six galleries.

Plymouth is the regional television centre of BBC South West. A team of journalists are headquartered at Plymouth for the ITV West Country regional station, after a merger with ITV West forced ITV Westcountry to close on 16 February 2009. The main local newspapers serving Plymouth are "The Herald" and "Western Morning News" with Radio Plymouth, BBC Radio Devon, Heart South West, and Pirate FM being the local radio stations with the most listeners.

Plymouth is home to Plymouth Argyle F.C., who play in the third tier of English football league known as Football League One. The team's home ground is called Home Park and is located in Central Park. It links itself with the group of English non-conformists that left Plymouth for the New World in 1620: its nickname is "The Pilgrims". The city also has three Non-League football clubs; Plymouth Parkway who play at Bolitho Park, Elburton Villa who play at Haye Road and Plymstock United who play at Dean Cross. Plymouth Parkway were recently promoted to the Western League from the South West Peninsula League, whilst Elburton Villa and Plymstock United continue to compete in the South West Peninsula League.

Other sports clubs include Plymouth Albion Plymouth Raiders and Plymouth Gladiators. Plymouth Albion Rugby Football Club is a rugby union club that was founded in 1875 and are currently competing in the third tier of Professional English Rugby the National League 1. They play at the Brickfields. Plymouth Raiders play in the British Basketball League – the top tier of British basketball. They play at the Plymouth Pavilions entertainment arena and were founded in 1983. Plymouth Gladiators are a speedway currently competing in the British National League, with home meetings taking place at the Plymouth Coliseum. Plymouth cricket club was formed in 1843, the current 1st XI play in the Devon Premier League. Plymouth is also home to Plymouth Marjons Hockey Club, with their 1st XI playing in the National League last season. Plymouth Mariners Baseball club play in the South West Baseball League, They play their home games at Wilson Field in Central Park. Plymouth was home to an American football club, the Plymouth Admirals until 2010.

Plymouth Leander is the most successful swimming club in Great Britain along with Plymouth Diving Club.

Plymouth is an important centre for watersports, especially scuba diving and sailing. The Port of Plymouth Regatta is one of the oldest regattas in the world, and has been held regularly since 1823. In September 2011, Plymouth hosted the America's Cup World Series for nine days.

Since 1973 Plymouth has been supplied water by South West Water. Prior to the 1973 take over it was supplied by Plymouth County Borough Corporation. Before the 19th century two leats were built to provide drinking water for the town. They carried water from Dartmoor to Plymouth. A watercourse, known as Plymouth or Drake's Leat, was opened on 24 April 1591 to tap the River Meavy. The Devonport Leat was constructed to carry fresh drinking water to the expanding town of Devonport and its ever-growing dockyard. It was fed by three Dartmoor rivers: The West Dart, Cowsic and Blackabrook. It seems to have been carrying water since 1797, but it was officially completed in 1801. It was originally designed to carry water to Devonport town but has since been shortened and now carries water to Burrator Reservoir, which feeds most of the water supply of Plymouth. Burrator Reservoir is located about north of the city and was constructed in 1898 and expanded in 1928.
Plymouth City Council is responsible for waste management throughout the city and South West Water is responsible for sewerage. Plymouth's electricity is supplied from the National Grid and distributed to Plymouth via Western Power Distribution. On the outskirts of Plympton a combined cycle gas-powered station, the Langage Power Station, which started to produce electricity for Plymouth at the end of 2009.

Her Majesty's Courts Service provide a magistrates' court and a Combined Crown and County Court centre in the city. The Plymouth Borough Police, formed in 1836, eventually became part of Devon and Cornwall Constabulary. There are police stations at Charles Cross and Crownhill (the Divisional HQ) and smaller stations at Plympton and Plymstock. The city has one of the Devon and Cornwall Area Crown Prosecution Service Divisional offices. Plymouth has five fire stations located in Camel's Head, Crownhill, Greenbank, Plympton and Plymstock which is part of Devon and Somerset Fire and Rescue Service. The Royal National Lifeboat Institution have an Atlantic 85 class lifeboat and Severn class lifeboat stationed at Millbay Docks.

Plymouth is served by Plymouth Hospitals NHS Trust and the city's NHS hospital is Derriford Hospital north of the city centre. The Royal Eye Infirmary is located at Derriford Hospital. South Western Ambulance Service NHS Foundation Trust operates in Plymouth and the rest of the south west; its headquarters are in Exeter.

The mid-19th-century burial ground at Ford Park Cemetery was reopened in 2007 by a successful trust and the City council operate two large early 20th century cemeteries at Weston Mill and Efford both with crematoria and chapels. There is also a privately owned cemetery on the outskirts of the city, Drake Memorial Park which does not allow headstones to mark graves, but a brass plaque set into the ground.

After the English Civil War the Royal Citadel was erected in 1666 towards the eastern section of Plymouth Hoe, to defend the port from naval attacks, suppress Plymothian Parliamentary leanings and to train the armed forces. Currently, guided tours are available in the summer months. Further west is Smeaton's Tower, which is a standard lighthouse that was constructed in 1759. Furthermore, Smeaton's Tower was dismantled in 1877 and the top two-thirds were reassembled on Plymouth Hoe. It is open to the public and has views over the Plymouth Sound and the city from the lantern room. Plymouth has 20 war memorials of which nine are on The Hoe including: Plymouth Naval Memorial, to remember those killed in World Wars I and II, and the Armada Memorial, to commemorate the defeat of the Spanish Armada.

The early port settlement of Plymouth, called "Sutton", approximates to the area now referred to as the Barbican and has 100 listed buildings and the largest concentration of cobbled streets in Britain. The Pilgrim Fathers left for the New World in 1620 near the commemorative Mayflower Steps in Sutton Pool. Also on Sutton Pool is the National Marine Aquarium which displays 400 marine species and includes Britain's deepest aquarium tank.

On the northern outskirts of the city, Crownhill Fort is a well-restored example of a "Palmerston's Folly". It is owned by the Landmark Trust and is open to the public.

To the west of the city is Devonport, one of Plymouth's historic quarters. As part of Devonport's millennium regeneration project, the "Devonport Heritage Trail" has been introduced, complete with over 70 waymarkers outlining the route.

Plymouth is often used as a base by visitors to Dartmoor, the Tamar Valley and the beaches of south-east Cornwall. Kingsand, Cawsand and Whitsand Bay are popular.

The Roland Levinsky building, the landmark building of the University of Plymouth, is located in the city's central quarter. Designed by leading architect Henning Larsen, the building was opened in 2008 and houses the University's Arts faculty. It has been consistently considered one of the UK's most beautiful university buildings.

Beckley Point, at 78m / 20 floors, is Plymouth's tallest building and was completed on 8 February 2018. It was designed by Boyes Rees Architects and built by contractors Kier.

People from Plymouth are known as Plymothians or less formally as Janners. Its meaning is described as a person from Devon, deriving from Cousin Jan (the Devon form of John), but more particularly in naval circles anyone from the Plymouth area.

The Elizabethan navigator, Sir Francis Drake was born in the nearby town of Tavistock and was the mayor of Plymouth. He was the first Englishman to circumnavigate the world and was known by the Spanish as "El Draco" meaning "The Dragon" after he raided many of their ships. He died of dysentery in 1596 off the coast of Portobelo, Panama. In 2002 a mission to recover his body and bring it to Plymouth was allowed by the Ministry of Defence. His cousin and contemporary John Hawkins was a Plymouth man. Painter Sir Joshua Reynolds, founder and first president of the Royal Academy was born and educated in nearby Plympton, now part of Plymouth. William Cookworthy born in Kingsbridge set up his successful porcelain business in the city and was a close friend of John Smeaton designer of the Eddystone Lighthouse. On 26 January 1786, Benjamin Robert Haydon, an English painter who specialised in grand historical pictures, was born here. The naturalist Dr William Elford Leach FRS, who did much to pave the way in Britain for Charles Darwin, was born at Hoe Gate in 1791.

Antarctic explorers Robert Falcon Scott who was born in Plymouth and Frank Bickerton both lived in the city. Artists include Beryl Cook whose paintings depict the culture of Plymouth and Robert Lenkiewicz, whose paintings investigated themes of vagrancy, sexual behaviour and suicide, lived in the city from the 1960s until his death in 2002. Illustrator and creator of children's series Mr Benn and King Rollo, David McKee, was born and brought up in South Devon and trained at Plymouth College of Art. Jazz musician John Surman, born in nearby Tavistock, has close connections to the area, evidenced by his 2012 album Saltash Bells. The avant garde prepared guitarist Keith Rowe was born in the city before establishing the jazz free improvisation band AMM in London in 1965 and MIMEO in 1997. The musician and film director Cosmo Jarvis has lived in several towns in South Devon and has filmed videos in and around Plymouth. In addition, actors Sir Donald Sinden and Judi Trott were born in Plymouth. George Passmore of Turner Prize winning duo Gilbert and George was also born in the city, as was Labour politician Michael Foot whose family reside at nearby Trematon Castle.

Notable athletes include swimmer Sharron Davies, diver Tom Daley, dancer Wayne Sleep, and footballer Trevor Francis. Other past residents include composer journalist and newspaper editor William Henry Wills, Ron Goodwin, and journalist Angela Rippon and comedian Dawn French. Canadian politician and legal scholar Chris Axworthy hails from Plymouth. America based actor Donald Moffat, whose roles include American Vice President Lyndon B. Johnson in the film "The Right Stuff", and fictional President Bennett in "Clear and Present Danger", was born in Plymouth.
Kevin Owen is an international TV news anchor who was born in Freedom Fields Hospital, while his father served as a Royal Navy Officer.
Cambridge spy Guy Burgess was born at 2 Albemarle Villas, Stoke whilst his father was a serving Royal Navy officer.






</doc>
<doc id="23511" url="https://en.wikipedia.org/wiki?curid=23511" title="Point-to-Point Protocol">
Point-to-Point Protocol

In computer networking, Point-to-Point Protocol (PPP) is a Data link layer (layer 2) communications protocol between two routers directly without any host or any other networking in between. It can provide connection authentication, transmission encryption, and compression.

PPP is used over many types of physical networks, including serial cable, phone line, trunk line, cellular telephone, specialized radio links, and fiber optic links, such as SONET. Internet service providers (ISPs) have used PPP for customer dial-up access to the Internet, since IP packets cannot be transmitted over a modem line on their own without some data link protocol that can identify where the transmitted frame starts and where it ends. 

Two derivatives of PPP, Point-to-Point Protocol over Ethernet (PPPoE) and Point-to-Point Protocol over ATM (PPPoA), are used most commonly by ISPs to establish a digital subscriber line (DSL) Internet service connection with customers.

PPP is commonly used as a data link layer protocol for connection over synchronous and asynchronous circuits, where it has largely superseded the older Serial Line Internet Protocol (SLIP) and telephone company mandated standards (such as Link Access Protocol, Balanced (LAPB) in the X.25 protocol suite). The only requirement for PPP is that the circuit provided be duplex. PPP was designed to work with numerous network layer protocols, including Internet Protocol (IP), TRILL, Novell's Internetwork Packet Exchange (IPX), NBF, DECnet and AppleTalk. Like SLIP, this is a full Internet connection over telephone lines via modem. It is more reliable than SLIP because it double checks to make sure that Internet packets arrive intact. It resends any damaged packets.

PPP was designed somewhat after the original HDLC specifications. The designers of PPP included many additional features that had been seen only in proprietary data-link protocols up to that time. PPP is specified in RFC 1661.

RFC 2516 describes Point-to-Point Protocol over Ethernet (PPPoE) as a method for transmitting PPP over Ethernet that is sometimes used with DSL. RFC 2364 describes Point-to-Point Protocol over ATM (PPPoA) as a method for transmitting PPP over ATM Adaptation Layer 5 (AAL5), which is also a common alternative to PPPoE used with DSL.

PPP is a layered protocol that has three components:

LCP initiates and terminates connections gracefully, allowing hosts to negotiate connection options. It is an integral part of PPP, and is defined in the same standard specification. LCP provides automatic configuration of the interfaces at each end (such as setting datagram size, escaped characters, and magic numbers) and for selecting optional authentication. The LCP protocol runs on top of PPP (with PPP protocol number 0xC021) and therefore a basic PPP connection has to be established before LCP is able to configure it.

RFC 1994 describes Challenge-Handshake Authentication Protocol (CHAP), which is preferred for establishing dial-up connections with ISPs.
Although deprecated, Password Authentication Protocol (PAP) is still sometimes used.

Another option for authentication over PPP is Extensible Authentication Protocol (EAP) described in RFC 2284.

After the link has been established, additional network (layer 3) configuration may take place. Most commonly, the Internet Protocol Control Protocol (IPCP) is used, although Internetwork Packet Exchange Control Protocol (IPXCP) and AppleTalk Control Protocol (ATCP) were once popular. Internet Protocol Version 6 Control Protocol (IPv6CP) will see extended use in the future, when IPv6 replaces IPv4 as the dominant layer-3 protocol.

PPP permits multiple network layer protocols to operate on the same communication link. For every network layer protocol used, a separate Network Control Protocol (NCP) is provided in order to encapsulate and negotiate options for the multiple network layer protocols. It negotiates network-layer information, e.g. network address or compression options, after the connection has been established.

For example, Internet Protocol (IP) uses the IP Control Protocol (IPCP), and Internetwork Packet Exchange (IPX) uses the Novell IPX Control Protocol (IPX/SPX). NCPs include fields containing standardized codes to indicate the network layer protocol type that the PPP connection encapsulates.

The following NCPs may be used with PPP:

PPP detects looped links using a feature involving magic numbers. When the node sends PPP LCP messages, these messages may include a magic number. If a line is looped, the node receives an LCP message with its own magic number, instead of getting a message with the peer's magic number.

The previous section introduced the use of LCP options to meet specific WAN connection requirements. PPP may include the following LCP options:

PPP frames are variants of HDLC frames:

If both peers agree to Address field and Control field compression during LCP, then those fields are omitted. Likewise if both peers agree to Protocol field compression, then the 0x00 byte can be omitted.

The Protocol field indicates the type of payload packet: 0xC021 for LCP, 0x80xy for various NCPs, 0x0021 for IP, 0x0029 AppleTalk, 0x002B for IPX, 0x003D for Multilink, 0x003F for NetBIOS, 0x00FD for MPPC and MPPE, etc. PPP is limited, and cannot contain general Layer 3 data, unlike EtherType.

The Information field contains the PPP payload; it has a variable length with a negotiated maximum called the Maximum Transmission Unit. By default, the maximum is 1500 octets. It might be padded on transmission; if the information for a particular protocol can be padded, that protocol must allow information to be distinguished from padding.

PPP frames are encapsulated in a lower-layer protocol that provides framing and may provide other functions such as a checksum to detect transmission errors. PPP on serial links is usually encapsulated in a framing similar to HDLC, described by IETF RFC 1662.
The Flag field is present when PPP with HDLC-like framing is used.

The Address and Control fields always have the value hex FF (for "all stations") and hex 03 (for "unnumbered information"), and can be omitted whenever PPP LCP Address-and-Control-Field-Compression (ACFC) is negotiated.

The frame check sequence (FCS) field is used for determining whether an individual frame has an error. It contains a checksum computed over the frame to provide basic protection against errors in transmission. This is a CRC code similar to the one used for other layer two protocol error protection schemes such as the one used in Ethernet. According to RFC 1662, it can be either 16 bits (2 bytes) or 32 bits (4 bytes) in size (default is 16 bits - Polynomial "x" + "x" + "x" + 1).

The FCS is calculated over the Address, Control, Protocol, Information and Padding fields after the message has been encapsulated.


Multilink PPP (also referred to as MLPPP, MP, MPPP, MLP, or Multilink) provides a method for spreading traffic across multiple distinct PPP connections. It is defined in RFC 1990. It can be used, for example, to connect a home computer to an Internet Service Provider using two traditional 56k modems, or to connect a company through two leased lines.

On a single PPP line frames cannot arrive out of order, but this is possible when the frames are divided among multiple PPP connections. Therefore, Multilink PPP must number the fragments so they can be put in the right order again when they arrive.

Multilink PPP is an example of a link aggregation technology. Cisco IOS Release 11.1 and later supports Multilink PPP. 

With PPP, one cannot establish several simultaneous distinct PPP connections over a single link.

That's not possible with Multilink PPP either. Multilink PPP uses contiguous numbers for all the fragments of a packet, and as a consequence it is not possible to suspend the sending of a sequence of fragments of one packet in order to send another packet. This prevents from running Multilink PPP multiple times on the same links.

Multiclass PPP is a kind of Multilink PPP where each "class" of traffic uses a separate sequence number space and reassembly buffer. Multiclass PPP is defined in RFC 2686

PPTP (Point-to-Point Tunneling Protocol) is a form of PPP between two hosts via GRE using encryption (MPPE) and compression (MPPC).

Many protocols can be used to tunnel data over IP networks. Some of them, like SSL, SSH, or L2TP create virtual network interfaces and give the impression of a direct physical connections between the tunnel endpoints. On a Linux host for example, these interfaces would be called tun0 or ppp0.

As there are only two endpoints on a tunnel, the tunnel is a point-to-point connection and PPP is a natural choice as a data link layer protocol between the virtual network interfaces. PPP can assign IP addresses to these virtual interfaces, and these IP addresses can be used, for example, to route between the networks on both sides of the tunnel.

IPsec in tunneling mode does not create virtual physical interfaces at the end of the tunnel, since the tunnel is handled directly by the TCP/IP stack. L2TP can be used to provide these interfaces, this technique is called L2TP/IPsec. In this case too, PPP provides IP addresses to the extremities of the tunnel.

PPP is defined in RFC 1661 (The Point-to-Point Protocol, July 1994). RFC 1547 (Requirements for an Internet Standard Point-to-Point Protocol, December 1993) provides historical information about the need for PPP and its development. A series of related RFCs have been written to define how a variety of network control protocols-including TCP/IP, DECnet, AppleTalk, IPX, and others-work with PPP.


Additional drafts:




</doc>
<doc id="23512" url="https://en.wikipedia.org/wiki?curid=23512" title="Patterson–Gimlin film">
Patterson–Gimlin film

Patterson–Gimlin film (also known as the Patterson film or the PGF) is an American short motion picture of an unidentified subject which the filmmakers have said was a Bigfoot. The footage was shot in 1967 in Northern California, and has since been subjected to many attempts to authenticate or debunk it.

The footage was filmed alongside Bluff Creek, a tributary of the Klamath River, about 25 logging-road miles northwest of Orleans, California, in Del Norte County. The film site is roughly 38 miles south of Oregon and 18 miles east of the Pacific Ocean. For decades, the exact location of the site was lost, primarily because of re-growth of foliage in the streambed after the flood of 1964. It was rediscovered in 2011. It is just south of a north-running segment of the creek informally known as "the bowling alley".

The filmmakers were Roger Patterson (February 14, 1933 – January 15, 1972) and Robert "'Bob" Gimlin (born October 18, 1931). Patterson died of cancer in 1972 and "maintained right to the end that the creature on the film was real". Patterson's friend, Gimlin, has always denied being involved in any part of a hoax with Patterson. Gimlin mostly avoided publicly discussing the subject from at least the early 1970s until about 2005 (except for three appearances), when he began giving interviews and appearing at Bigfoot conferences.

The film is 23.85 feet long (preceded by 76.15 feet of "horseback" footage), has 954 frames, and runs for 59.5 seconds at 16 frames per second. If the film was shot at 18 fps, as Grover Krantz believed, the event lasted 53 seconds. The date was October 20, 1967, according to the filmmakers, although some critics believe it was shot earlier.

Patterson said he became interested in Bigfoot after reading an article about the creature by Ivan T. Sanderson in "True" magazine in December 1959. In 1961 Sanderson published his encyclopedic "Abominable Snowmen: Legend Come to Life," a worldwide survey of accounts of Bigfoot-type creatures, including recent track finds, etc. in the Bluff Creek area, which heightened his interest. Thereafter, Marian Place wrote:

Patterson's book, "Do Abominable Snowmen of America Really Exist?", was self-published in 1966. The book has been characterized as "little more than a collection of newspaper clippings laced together with Patterson's circus-poster style prose". The book, however, contains 20 pages of previously unpublished interviews and letters, 17 drawings by Patterson of the encounters described in the text, 5 hand-drawn maps (rare in subsequent Bigfoot books), and almost 20 photos and illustrations from other sources. It was first reprinted in 1996 by Chris Murphy, and then again re-issued by Murphy in 2005 under the title "The Bigfoot Film Controversy," with 81 pages of additional material by Murphy.

In May/June 1967 Patterson began filming a docudrama or pseudo-documentary about cowboys being led by an old miner and a wise Indian tracker on a hunt for Bigfoot. The storyline called for Patterson, his Indian guide (Gimlin in a wig), and the cowboys to recall in flashbacks the stories of Fred Beck (of the 1924 Ape Canyon incident) and others as they tracked the beast on horseback. For actors and cameraman, Patterson used at least nine volunteer acquaintances, including Gimlin and Bob Heironimus, for three days of shooting, perhaps over the Memorial Day weekend. Patterson would have needed a costume to represent Bigfoot, if the time came to shoot such climactic scenes.

Prior to the October 1967 filming, Patterson apparently visited Los Angeles on these occasions:
Merritt soon moved back to Yakima and became Patterson's neighbor, and later his collaborator on his Bigfoot documentary. 

Both Patterson and Gimlin had been rodeo riders and amateur boxers—and local champions in their weight classes. Patterson had played high school football.

In October 1967, Patterson and his friend Gimlin set out for the Six Rivers National Forest in far Northern California. They drove in Gimlin's truck, carrying his provisions and three horses, positioned sideways. Patterson chose the area because of intermittent reports of the creatures in the past, and of their enormous footprints since 1958. (His familiarity with the area and its residents from prior visits may also have been a factor.)

The most recent of these reports was the nearby Blue Creek Mountain track find, which was investigated by journalist John Green, Bigfoot hunter René Dahinden, and archaeologist Don Abbott on and after August 28, 1967. This find was reported to Patterson (via his wife) soon thereafter by Al Hodgson, owner of the Willow Creek Variety Store.

Though Gimlin says he doubted the existence of Sasquatch-like creatures, he agreed to Patterson's insistence that they should not attempt to shoot one.

As their stories went, in the early afternoon of Friday, October 20, 1967, Patterson and Gimlin were riding generally northeast (upstream) on horseback along the east bank of Bluff Creek. At sometime between 1:15 and 1:40 PM, they "came to an overturned tree with a large root system at a turn in the creek, almost as high as a room".

When they rounded it, "there was a logjam—a 'crow's nest'—left over from the flood of '64," and then they spotted the figure behind it nearly simultaneously. It was either "crouching beside the creek to their left" or "standing" there, on the opposite bank. Gimlin later described himself as in a mild state of shock after first seeing the figure.

Patterson initially estimated its height at to , and later raised his estimate to about . Some later analysts, anthropologist Grover Krantz among them, have suggested Patterson's later estimate was about too tall. Gimlin's estimate was .

The film shows what Patterson and Gimlin claimed was a large, hairy, bipedal, apelike figure with short, "silvery brown" or "dark reddish-brown" or "black" hair covering most of its body, including its prominent breasts. The figure in the film generally matches the descriptions of Bigfoot offered by others who claim to have seen one.

Patterson estimated he was about away from the creature at his closest. Patterson said that his horse reared upon sensing the figure, and he spent about 20 seconds extricating himself from the saddle, controlling his horse, getting around to its other side, and getting his camera from a saddlebag before he could run toward the figure while operating his camera. He yelled "Cover me" to Gimlin, "meaning to get the gun out". Gimlin crossed the creek on horseback after Patterson had run well beyond it, riding on a path somewhat to the left of Patterson's and somewhat beyond his position. Perez estimates he came within of "Patty". Then, rifle in hand, he dismounted, but did not point his rifle at the creature.

The figure had walked away from them to a distance of about before Patterson began to run after it. The resulting film (about 59.5 seconds long at 16 fps) is initially quite shaky until Patterson got about from the figure. At that point, the figure glanced over its right shoulder at the men and Patterson fell to his knees; on Krantz's map this corresponds to frame 264. To researcher John Green, Patterson would later characterize the creature's expression as one of "contempt and disgust ... you know how it is when the umpire tells you 'one more word and you're out of the game.' That's the way it felt."

Shortly after this point the steady, middle portion of the film begins, containing the famous look-back frame 352. Patterson said, "it turned a total of I think three times," the other times therefore being before the filming began and/or while he was running with his finger off the trigger. Shortly after glancing over its shoulder on film, the creature disappeared behind a grove of trees for 14 seconds, then reappeared in the film's final 15 seconds after Patterson moved to a better vantage point, fading into the trees again and being lost to view at a distance of as the reel of film ran out.

Gimlin remounted and followed it on horseback, keeping his distance, until it disappeared around a bend in the road three hundred yards away. Patterson called him back at that point, feeling vulnerable on foot without a rifle, because he feared the creature's mate might approach. The entire encounter had lasted less than two minutes.

Next, Gimlin and Patterson rounded up Patterson's horses, which had run off in the opposite direction, downstream, before the filming began. Patterson got his second roll of film from his saddlebag and filmed the tracks. Then the men tracked "Patty" for either one mile or , but "lost it in the heavy undergrowth". They went to their campsite three miles south, picked up plaster, returned to the initial site, measured the creature's step-length, and made two plaster casts, one each of the best-quality right and left prints.

According to Patterson and Gimlin, they were the only witnesses to their brief encounter with what they claimed was a sasquatch. Their statements agree in general, but author Greg Long notes a number of inconsistencies. They offered somewhat different sequences in describing how they and the horses reacted upon seeing the creature. Patterson in particular increased his estimates of the creature's size in subsequent retellings of the encounter. In a different context, Long argues, these discrepancies would probably be considered minor, but given the extraordinary claims made by Patterson and Gimlin, any apparent disagreements in perception or memory are worth noting.

The film's defenders have responded by saying that commercially motivated hoaxers would have "got their stories straight" beforehand so they wouldn't have disagreed immediately upon being interviewed, and on so many points, and so they wouldn't have created a suit and a creature with foreseeably objectionable features and behaviors.

A more serious objection concerns the film's "timeline". This is important because Kodachrome II movie film, as far as is known, could only be developed by a lab containing a $60,000+ machine, and the few West Coast labs known to possess one did not do developing over weekends. Patterson's brother-in-law Al DeAtley claims not to remember where he took the film for development or where he picked it up.

Critics claim that too much happened between the filming (at 1:15 at the earliest) and the filmmakers' arrival in Willow Creek (at 6:30 at the latest). Daegling wrote, "All of the problems with the timeline disappear if the film is shot a few days or hours beforehand. If that is the case, one has to wonder what other details of this story are wrong." The film's defenders retort that although the time window was tight, it was do-able.

Chris Murphy wrote, "I have confirmed with Bob Gimlin that Patterson definitely rode a small quarter horse (which he owned), not his Welsh pony 'Peanuts'. Also, that Patterson had arranged to borrow a horse by the name of 'Chico' from Bob Heironimus for Gimlin to use ... Gimlin did not have a horse that was suitable (old enough) for the expedition." Heironimus stated that Chico (a middle-aged gelding) "wouldn't jump or buck ..."

At approximately 6:30 PM, Patterson and Gimlin met up with Al Hodgson at his variety store in Willow Creek, approximately 54.3 miles south by road, about 28.8 miles by Bluff Creek Road from their camp to the 1967 roadhead by Bluff Creek, and 25.5 miles down California State Route 96 to Willow Creek. Patterson intended to drive on to Eureka to ship his film. Either at that time, or when he arrived in the Eureka/Arcata area, he called Al DeAtley (his brother-in-law in Yakima) and told him to expect the film he was shipping. He requested Hodgson to call Donald Abbott, whom Grover Krantz described as "the only scientist of any stature to have demonstrated any serious interest in the [Bigfoot] subject," hoping he would help them search for the creature by bringing a tracking dog. Hodgson called, but Abbott declined. Krantz argued that this call the same day of the encounter is evidence against a hoax, at least on Patterson's part.

After shipping the film, they headed back toward their camp, where they had left their horses. On their way they "stopped at the Lower Trinity Ranger Station, as planned, arriving about 9:00 p.m. Here they met with Syl McCoy [another friend] and Al Hodgson." At this point Patterson called the daily "Times-Standard" newspaper in Eureka and related his story. They arrived back at their campsite at about midnight. At either 5 or 5:30 the next morning, after it started to rain heavily, Gimlin returned to the filmsite from the camp and covered the other prints with bark to protect them. The cardboard boxes he had been given by Al Hodgson for this purpose and had left outside were so soggy they were useless, so he left them.

When he returned to the camp he and Patterson aborted their plan to remain looking for more evidence and departed for home, fearing the rain would wash out their exit. After attempting to go out along "the low road"—Bluff Creek Road—and finding it blocked by a mudslide, they went instead up the steep Onion Mountain Road, off whose shoulder their truck slipped; extracting it required the (unauthorized) borrowing of a nearby front-end loader. The drive home from their campsite covered about 580 miles, the initial 28.8 miles on a low-speed logging road, and then about 110 miles on twisty Route 96. Driving a truck with three horses, and allowing for occasional stops, it would have taken 13 hours to get home Saturday evening, at an average speed of 45 mph; it would have taken 14.5 hours at a 40 mph average speed.

US Forest Service "Timber Management Assistant" Lyle Laverty said, "I [and his team of three, in a Jeep] passed the site on either Thursday the 19th or Friday the 20th" and noticed no tracks. After reading the news of Patterson's encounter on their weekend break, Laverty and his team returned to the site on Monday, the 23rd, and made six photos of the tracks. (Laverty later served as an Assistant Secretary of the Interior under George W. Bush.) Taxidermist and outdoorsman Robert Titmus went to the site with his sister and brother-in-law nine days later. Titmus made plaster casts of ten successive prints of the creature and, as best he could, plotted Patterson's and the creature's movements on a map.

Grover Krantz writes that "Patterson had the film developed as soon as possible. At first he thought he had brought in proof of Bigfoot's existence and really expected the scientists to accept it. But only a few scientists were willing to even look at the film," usually at showings at scientific organizations. These were usually arranged at the behest of zoologist, author, and media figure Ivan Sanderson, a supporter of Patterson's film. Seven showings occurred, in Vancouver, Manhattan, The Bronx, Washington, D.C., Atlanta, and Washington, D.C. again (all by the end of 1968); then, later, in Beaverton, Oregon. Of those who were quoted, most expressed various reservations, although some were willing to say they were intrigued by it.

Christopher Murphy wrote, "Dahinden traveled to Europe [with the film] in 1971. He visited England, Finland, Sweden, Switzerland and Russia. Although scientists in these countries were somewhat more open-minded than those in North America, their findings were basically the same . ... A real glimmer of hope, however, emerged [in Russia, where he met Bayanov, Bourtsev, and their associates]."

Though there was little scientific interest in the film, Patterson was still able to capitalize on it. He made a deal with the BBC, allowing the use of his footage in a docudrama made in return for letting him tour with their docudrama, into which he melded material from his own documentary and additional material he and Al DeAtley filmed. This film was shown in local movie houses around the Pacific Northwest and Midwest. A technique commonly used for nature films called "four-walling" was employed, involving heavy local advertising, mostly on TV, of a few days of showings. It was a modest financial success. Al DeAtley estimated that his 50% of the film's profits amounted to $75,000.

The film generated a fair amount of national publicity. Patterson appeared on a few popular TV talk shows to promote the film and belief in Bigfoot by showing excerpts from it: for instance, on the "Joe Pyne Show" in Los Angeles, in 1967, which covered most of the western US; on Merv Griffin's program, with Krantz offering his analysis of the film; on Joey Bishop's talk show, and also on Johnny Carson's "Tonight Show". Articles on the film appeared in "Argosy", "National Wildlife Magazine", and "Reader's Digest".

One radio interview, with Gimlin, by Vancouver-based Jack Webster in November 1967, was partly recorded by John Green and reprinted in Loren Coleman's "Bigfoot!" Patterson also appeared on broadcast interviews on local stations near where his film would be shown during his four-walling tour in 1968.

Patterson subsequently sold overlapping distribution rights for the film to several parties, which resulted in costly legal entanglements.

After Patterson's death, Michael McLeod wrote, "With the consent of Al DeAtley and Patricia Patterson, the film distributor Ron Olson took over the operation of Northwest Research ... and changed its name to the North American Wildlife Research Association. ... He worked full-time compiling reports, soliciting volunteers to join the hunt, and organizing several small expeditions. A Bigfoot trap Olson and his crew built still survives. ... Olson ... continued to lobby the company [American National Enterprises] to produce a Bigfoot film. ... In 1974 ... ANE finally agreed. ... [It was released in 1975,] titled "Bigfoot: Man or Beast". [H]e devised a storyline involving members of a Bigfoot research party. Olson spent several years exhibiting the film around the country. He planned to make millions with the film, but says it lost money." Olson is profiled in Barbara Wasson's "Sasquatch Apparitions".

On November 25, 1974, CBS broadcast "Monsters! Mystery or Myth", a documentary about the Loch Ness Monster and Bigfoot. (It was co-produced by the Smithsonian Institution, who cancelled their contract with the producer the next year). The show attracted fifty million viewers. In 1975, Sunn Classic Pictures released "Bigfoot: The Mysterious Monster" aka "The Mysterious Monsters", which remixed parts of "Monsters! Mystery or Myth" another documentary called "Land Of The Yeti", and also included footage from the Patterson–Gimlin film.

Patterson's expensive ($369) 16mm camera had been rented on May 13 from photographer Harold Mattson at Sheppard's Camera Shop in Yakima, but he had kept it longer than the contract had stipulated, and an arrest warrant had been issued for him on October 17; he was actually arrested within weeks of his return from Bluff Creek. After Patterson returned the camera in working order, this charge was ultimately dismissed, in 1969.

While Patterson sought publicity, Gimlin was conspicuous by his absence. He only briefly helped to promote the film and avoided discussing his Bigfoot encounter publicly for many subsequent years; he turned down requests for interviews. He later reported that he had avoided publicity after Patterson and promoter Al DeAtley had broken their agreement to pay him a one-third share of any profits generated by the film. Another factor was that his wife objected to publicity.

Daegling wrote, "Bigfoot advocates emphasize that Patterson remained an active Bigfoot hunter up until his death." For instance, in 1969, he hired a pair of brothers to travel around in a truck chasing down leads to Bigfoot witnesses and interviewing them. Later, in December of that year, he was one of those present in Bossburg, Washington, in the aftermath of the cripplefoot tracks found there. Krantz reports that "[a] few years after the film was made, Patterson received a letter from a man ["a US airman stationed in Thailand"] who assured him a Sasquatch was being held in a Buddhist monastery. Patterson spent most of his remaining money preparing an expedition to retrieve this creature" only to learn it was a hoax. He learned this only after having sent Dennis Jenson fruitlessly to Thailand (where he concluded that the airman was "mentally unbalanced") and then, after receiving a second untrue letter from the man, going himself to Thailand with Jenson.

To obtain money to travel to Thailand, "Patterson called Ron, who had returned to ANE, and sold the company the theatrical rights to the clip for what Olson described as a pretty good sum of money."

Patterson died of Hodgkin's lymphoma in 1972. According to Michael McLeod, Greg Long, and Bill Munns, "A few days before Roger died, he told [Bigfoot-book author Peter] Byrne that in retrospect, ... he [wished he] would have shot the thing and brought out a body instead of a reel of film." According to Grover Krantz and Robert Pyle, years later, Patterson and Gimlin both agreed they should have tried to shoot the creature, both for financial gain and to silence naysayers.

In 1995, almost three decades after the Patterson–Gimlin filming, Greg Long, a technical writer for a technology firm who had a hobby of investigating and writing about Northwest mysteries, started years of interviewing people who knew Patterson, some of whom described him as a liar and a conman. 

Greg Long reports that a 1978 legal "settlement gave Dahinden controlling rights—51 percent of the film footage, 51 percent of video cassette rights, and 100 percent of all 952 frames of the footage. Patty Patterson had 100 percent of all TV rights and 49 percent rights in the film footage. Dahinden had ... bought out Gimlin, who himself had received nothing from Patterson; and Mason and Radford, promised part of the profits by Patterson, had nothing to show for their investment or efforts."

Frame 352, the well-known look-back image, is in the public domain, having long been reprinted by others without protest by the copyright holder.

The whereabouts of the original is unknown, although there are several speculations as to what happened to it. 

At least seven copies were made of the original film.

Bill Munns listed four other missing reels of derivative works that would be helpful to film analysts.

The second reel, showing Patterson and Gimlin making and displaying plaster casts of some footprints, was not shown in conjunction with the first reel at Al DeAtley's house, according to those who were there. Chris Murphy wrote, "I believe the screening of this roll at the University of British Columbia on October 26, 1967, was the first and last major screening." It has subsequently been lost. 

A ten-foot strip from that reel, or from a copy of that reel, from which still images were taken by Chris Murphy, still exists, but it, too, has gone missing.

One factor that complicates discussion of the Patterson film is that Patterson said he normally filmed at 24 frames per second, but in his haste to capture the Bigfoot on film, he did not note the camera's setting. His Cine-Kodak K-100 camera had markings on its continuously variable dial at 16, 24, 32, 48, and 64 frames per second, but no click-stops, and was capable of filming at any frame speed within this range. Grover Krantz wrote, "Patterson clearly told John Green that he found, after the filming, that the camera was set on 18 frames per second (fps). ... " It has been suggested that Patterson simply misread "16" as "18".


The Patterson–Gimlin film has seen relatively little interest from mainstream scientists. Statements of scientists who viewed the film at a screening, or who conducted a study, are reprinted in Chris Murphy's "Bigfoot Film Journal". Typical objections include: Neither humans nor chimpanzees have hairy breasts as does the figure in the film, and Napier has noted that a sagittal crest is "only very occasionally seen, to an insignificant extent, in chimpanzees females". Critics have argued these features are evidence against authenticity. Krantz countered the latter point, saying "a sagittal crest ... is a consequence of absolute size alone."

As anthropologist David Daegling writes, "[t]he skeptics have not felt compelled to offer much of a detailed argument against the film; the burden of proof, rightly enough, should lie with the advocates." Yet, without a detailed argument against authenticity, Daegling notes that "the film has not gone away." Similarly, Krantz argues that of the many opinions offered about the Patterson film, "[o]nly a few of these opinions are based on technical expertise and careful study of the film itself."

Regarding the quality of the film, second-generation copies or copies from TV and DVD productions are inferior to first-generation copies. Many early frames are blurry due to camera shake, and the quality of subsequent frames varies for the same reason. Stabilization of the film (e.g., by M. K. Davis) to counter the effect of camera shake has improved viewers' ability to analyze it. Regarding "graininess," Bill Munns writes, "Based on transparencies taken off the camera original, ... the PGF original is as fine grain as any color 16mm film can achieve." He adds that graininess increases as images are magnified.

Bernard Heuvelmans—a zoologist and the so-called "father of cryptozoology"—thought the creature in the Patterson film was a suited human. He objected to the film subject's hair-flow pattern as being too uniform; to the hair on the breasts as not being like a primate; to its buttocks as being insufficiently separated; and to its too-calm retreat from the pursuing men.

Prominent primate expert John Napier (one-time director of the Smithsonian's Primate Biology Program) was one of the few mainstream scientists not only to critique the Patterson–Gimlin film but also to study then-available Bigfoot evidence in a generally sympathetic manner, in his 1973 book, "Bigfoot: The Sasquatch and Yeti in Myth and Reality".

Napier conceded the likelihood of Bigfoot as a real creature, stating, "I am convinced that Sasquatch exists." But he argued against the film being genuine: "There is little doubt that the scientific evidence taken collectively points to a hoax of some kind. The creature shown in the film does not stand up well to functional analysis." Napier gives several reasons for his and other's skepticism that are commonly raised, but apparently his main reasons are original with him. First, the length of "the footprints are totally at variance with its calculated height". Second, the footprints are of the "hourglass" type, which he is suspicious of. (In response, Barbara Wasson criticized Napier's logic at length.)

He adds, "I could not see the zipper; and I still can't. There I think we must leave the matter. Perhaps it was a man dressed up in a monkey-skin; if so it was a brilliantly executed hoax and the unknown perpetrator will take his place with the great hoaxers of the world. Perhaps it was the first film of a new type of hominid, quite unknown to science, in which case Roger Patterson deserves to rank with Dubois, the discoverer of "Pithecanthropus erectus", or Raymond Dart of Johannesburg, the man who introduced the world to its immediate human ancestor, "Australopithecus africanus"."

The skeptical views of Grieve and Napier are summarized favorably by Kenneth Wylie (and those of Bayanov and Donskoy negatively) in Appendix A of his 1980 book, "Bigfoot: A Personal Inquiry into a Phenomenon".

Esteban Sarmiento is a specialist in physical anthropology at the American Museum of Natural History. He has 25 years of experience with great apes in the wild. He writes, "I did find some inconsistencies in appearance and behavior that might suggest a fake ... but nothing that conclusively shows that this is the case." His most original criticism is this: "The plantar surface of the feet is decidedly pale, but the palm of the hand seems to be dark. There is no mammal I know of in which the plantar sole differs so drastically in color from the palm." (But see Meldrum, 170–71.) His most controversial statements are these: "The gluteals, although large, fail to show a humanlike cleft (or crack)." "Body proportions: ... In all of the above relative values, bigfoot is well within the human range and differs markedly from any living ape and from the 'australopithecine' fossils." (E.g., the IM index is in the normal human range.) And: "I estimate bigfoot's weight to be between 190 and 240 lbs."

When anthropologists David J. Daegling of the University of Florida and Daniel O. Schmitt examined the film, they concluded it was impossible to conclusively determine if the subject in the film is nonhuman, and additionally argued that flaws in the studies by Krantz and others invalidated their claims. Daegling and Schmitt noted problems of uncertainties in the subject and camera positions, camera movement, poor image quality, and artifacts of the subject. They concluded: "Based on our analysis of gait and problems inherent in estimating subject dimensions, it is our opinion that it is not possible to evaluate the identity of the film subject with any confidence."

Daegling has asserted that the creature's odd walk could be replicated: "Supposed peculiarities of subject speed, stride length, and posture are all reproducible by a human being employing this type of locomotion [a "compliant gait"]."

Daegling notes that in 1967, movie and television special effects were primitive compared to the more sophisticated effects in later decades, and allows that if the Patterson film depicts a man in a suit that "it is not unreasonable to suggest that it is better than some of the tackier monster outfits that got thrown together for television at that time."

Jessica Rose and James Gamble are authors of "the definitive text on human gait", "Human Walking". They operate the Motion and Gait Analysis Lab at Stanford University. They conducted a high-tech human-replication attempt of "Patty's" gait, in cooperation with Jeff Meldrum. Rose was certain their subject had matched Patty's gait, while Gamble was not quite as sure. Meldrum was impressed and acknowledged that "some aspects" of the creature's walk had been replicated, but not all. The narrator said, "even the experts can see the gait test could not replicate all parameters of the gait." It was shown in an episode of the Discovery Channel's "Best Evidence" series.

A computerized visual analysis of the video conducted by Cliff Crook, who once devoted rooms to sasquatch memorabilia in his home in Bothell, Washington, and Chris Murphy, a Canadian Bigfoot buff from Vancouver, British Columbia, was released in January 1999 and exposed an object which appeared to be the suit's zip-fastener. Zooming in on four magnified frames of the 16 mm footage video exposed what appeared to be tracings of a bell-shaped fastener on the creature's waist area, presumably used to hold a person's suit together. Since both Crook and Murphy were previously staunch supporters of the video's authenticity, Associated Press journalist John W. Humbell noted "Longtime enthusiasts smell a deserter."

Krantz also showed the film to Gordon Valient, a researcher for Nike shoes, who he says "made some rather useful observations about some rather unhuman movements he could see".

A first-season episode of "MonsterQuest" focuses on the Bigfoot phenomenon. One pair of scientists, Jurgen Konczak (Director, Human Sensorimotor Control Laboratory, University of Minnesota) and Esteban Sarmiento, attempts and fails to get a mime outfitted with LEDs on his joints to mimic the Patterson Bigfoot's gait. A second pair, Daris Swindler and Owen Caddy, employs digital enhancement and observes facial movements, such as moving eyelids, lips that compress like an upset chimp's, and a mouth that is lower than it appears, due to a false-lip anomaly like that of a chimp's. (Unfortunately, the show's narrator falsely claims, three times, that the original film shot by Patterson was used.) The episode concludes, "the new findings are intriguing but inconclusive, until a body is found."


Bill Munns, retired, was a special effects and make-up artist, cameraman, and film editor. He argues that Universal and Disney were not the most knowledgeable studios to consult with. He says that Fox, MGM, and special effects artist Stuart Freeborn in England, "who had just completed his groundbreaking ape suits for ""," would have been preferable.

Munns started posting his online analysis of the film in 2009 and summarizing it in the online Munns Report. In 2013 he and Jeff Meldrum co-authored three papers in Meldrum's online magazine, "Relict Hominoid Inquiry". In 2014, Munns self-published "When Roger Met Patty", a 488-page book incorporating material from those articles that analyses the film and film subject from various perspectives.

He argues the film depicts a non-human animal, not a man in a fur suit. He proposes a new diagnostic test of authenticity, at the armpit: natural concave skin fold vs. artificial vertical crease. Munns' analysis has been featured in an episode of the History Channel series "MonsterQuest".


The major hoax allegations are summarized and criticized in:


In 2002, Philip Morris, owner of Morris Costumes (a North Carolina-based company offering costumes, props and stage products) claimed that he made a gorilla costume that was used in the Patterson film. Morris says he discussed his role in the hoax "at costume conventions, lectures, [and] magician conventions" in the 1980s, but first addressed the public at large on August 16, 2002, on Charlotte, North Carolina, radio station WBT. His story was also printed in "The Charlotte Observer". Morris claims he was reluctant to expose the hoax earlier for fear of harming his business: giving away a performer's secrets, he said, would be widely regarded as disreputable.

Morris said that he sold an ape suit to Patterson via mail order in 1967, thinking it was going to be used in what Patterson described as a "prank". (Ordinarily the gorilla suits he sold were used for a popular sideshow routine that depicted an attractive woman, supposedly from some far-flung corner of the globe, being altered by a sorcerer or scientist into a gorilla or otherwise apelike monster.) After the initial sale, Morris said that Patterson telephoned him asking how to make the "shoulders more massive" and the "arms longer". Morris says he suggested that whoever wore the suit should wear football shoulder pads and hold sticks in his hands within the suit.

As for the creature's walk, Morris said:
The Bigfoot researchers say that no human can walk that way in the film. Oh, yes they can! When you're wearing long clown's feet, you can't place the ball of your foot down first. You have to put your foot down flat. Otherwise, you'll stumble. Another thing, when you put on the gorilla head, you can only turn your head maybe a quarter of the way. And to look behind you, you've got to turn your head and your shoulders and your hips. Plus, the shoulder pads in the suit are in the way of the jaw. That's why the Bigfoot turns and looks the way he does in the film. He has to twist his entire upper body.
Morris' wife and business partner Amy had vouched for her husband and claims to have helped frame the suit. Morris offered no evidence apart from his own testimony to support his account, the most conspicuous shortcoming being the absence of a gorilla suit or documentation that would match the detail evidenced in the film and could have been produced in 1967.

A re-creation of the PGF was undertaken on October 6, 2004, at "Cow Camp," near Rimrock Lake, a location 41 miles west of Yakima. This was six months after the publication of Long's book and 11 months after Long had first contacted Morris. Bigfooter Daniel Perez wrote, ""National Geographic's" [producer] Noel Dockster ... noted the suit used in the re-creation ... was in no way similar to what was depicted in the P–G film."

Morris wouldn't consent to release the video to National Geographic, the re-creation's sponsor, claiming he hadn't had adequate time to prepare and that the month was in the middle of his busy season. However, he has not attempted to create a suit more to his liking since that time.

Bob Heironimus claims to have been the figure depicted in the Patterson film. Heironimus says he had not previously publicly discussed his role in the hoax because he hoped to be paid eventually and was afraid of being convicted of fraud had he confessed. After speaking with his lawyer he was told that since he had not been paid for his involvement in the hoax, he could not be held accountable.

A month after watching the December 28, 1998, Fox-television special "World's Greatest Hoaxes: Secrets Finally Revealed?", he went public, via a January 30 press release by his lawyer, Barry Woodard, in a Yakima newspaper story. He stated, "I'm telling the truth. I'm tired after thirty-seven years." Five days later, a second newspaper story reported that his "lawyer's office has been inundated with calls from media outlets. ... 'We're just sort of waiting for the dust to settle,' he said, explaining he and his client are evaluating offers." He also said, "We anticipate that we will be telling the full story to somebody rather quickly."

Heironimus's name was first publicly revealed, and his allegations first publicly detailed, five years later, in Greg Long's book, "The Making of Bigfoot", which includes testimony that corroborates Heironimus' claims: 

Long argues that the suit Morris says he sold to Patterson was the same suit Heironimus claims to have worn in the Patterson film. However, Long quotes Heironimus and Morris describing different ape suits in many respects. Among the notable differences are:



Long speculates that Patterson modified the costume, but only by attaching Morris's loose hands and feet to the costume, and by replacing Morris's mask. However, there's nothing he wrote on "suit" modification. There's no evidence or testimony that Patterson changed the Morris suit to horsehide, or dyed it a darker color, or cut it in half at the waist to agree with Heironimus's description.

Some film proponents say that Heironimus' arms are too short to match that of a Bigfoot and that he was a few inches shorter than the creature on the film (up to 14 inches shorter).

It has also been said that Heironimus was not as bulky as the creature, but film critics claim that a suit could correct for that (and for height). However, Heironimus did not mention there being padding in the torso, either when questioned by Long about the suit or when specifically asked about padding by Rob McConnell in his 2nd "XZone" radio interview, on August 6, 2007.

Polygraph tests regarding their claims have been passed by both Heironimus and Patterson.

After the death of Ray Wallace in 2002, following a request by Loren Coleman to "The Seattle Times" reporter Bob Young to investigate, the family of Wallace went public with claims that he had started the Bigfoot phenomenon with fake footprints (made from a wooden foot-shaped cutout) left in Californian sites in 1958.











</doc>
<doc id="23513" url="https://en.wikipedia.org/wiki?curid=23513" title="Producer">
Producer

Producer or producers may refer to:





</doc>
<doc id="23517" url="https://en.wikipedia.org/wiki?curid=23517" title="List of Polish-language poets">
List of Polish-language poets

List of poets who have written much of their poetry in the Polish language. See also Discussion Page for additional poets not listed here.

There have been five Polish-language Nobel Prize laureates in literature: Henryk Sienkiewicz, Władysław Reymont, Czesław Miłosz, Wisława Szymborska and Olga Tokarczuk. Two of them have been poets (Miłosz and Szymborska). 














</doc>
<doc id="23519" url="https://en.wikipedia.org/wiki?curid=23519" title="Paul Valéry">
Paul Valéry

Ambroise Paul Toussaint Jules Valéry (; 30 October 1871 – 20 July 1945) was a French poet, essayist, and philosopher. In addition to his poetry and fiction (drama and dialogues), his interests included aphorisms on art, history, letters, music, and current events. Valéry was nominated for the Nobel Prize in Literature in 12 different years.

Valéry was born to a Corsican father and Genoese-Istrian mother in Sète, a town on the Mediterranean coast of the Hérault, but he was raised in Montpellier, a larger urban center close by. After a traditional Roman Catholic education, he studied law at university and then resided in Paris for most of the remainder of his life, where he was, for a while, part of the circle of Stéphane Mallarmé.

In 1900, he married Jeannine Gobillard, a friend of Stéphane Mallarmé's family, who was also a niece of the painter Berthe Morisot. The wedding was a double ceremony in which the bride's cousin, Morisot's daughter, Julie Manet, married the painter . Valéry and Gobillard had three children: Claude, Agathe and François.

Valéry served as a juror with Florence Meyer Blumenthal in awarding the Prix Blumenthal, a grant given between 1919 and 1954 to young French painters, sculptors, decorators, engravers, writers, and musicians.

Though his earliest publications date from his mid-twenties, Valéry did not become a full-time writer until 1920, when the man for whom he worked as private secretary, a former chief executive of the Havas news agency, Edouard Lebey, died of Parkinson's disease. Until then, Valéry had, briefly, earned his living at the Ministry of War before assuming the relatively flexible post as assistant to the increasingly impaired Lebey, a job he held for some twenty years.

After his election to the Académie française in 1925, Valéry became a tireless public speaker and intellectual figure in French society, touring Europe and giving lectures on cultural and social issues as well as assuming a number of official positions eagerly offered to him by an admiring French nation. He represented France on cultural matters at the League of Nations, and he served on several of its committees, including the sub-committee on Arts and Letters of the Committee on Intellectual Cooperation. The English-language collection "The Outlook for Intelligence" (1989) contains translations of a dozen essays related to these activities.

In 1931, he founded the Collège International de Cannes, a private institution teaching French language and civilization. The Collège is still operating today, offering professional courses for native speakers (for educational certification, law and business) as well as courses for foreign students.

He gave the keynote address at the 1932 German national celebration of the 100th anniversary of the death of Johann Wolfgang Goethe. This was a fitting choice, as Valéry shared Goethe's fascination with science (specifically, biology and optics).

In addition to his activities as a member of the Académie française, he was also a member of the Academy of Sciences of Lisbon, and of the "Front national des Ecrivains". In 1937, he was appointed chief executive of what later became the University of Nice. He was the inaugural holder of the Chair of Poetics at the Collège de France.

During World War II, the Vichy regime stripped him of some of these jobs and distinctions because of his quiet refusal to collaborate with Vichy and the German occupation, but Valéry continued, throughout these troubled years, to publish and to be active in French cultural life, especially as a member of the Académie française.

Valéry died in Paris in 1945. He is buried in the cemetery of his native town, Sète, the same cemetery celebrated in his famous poem "Le Cimetière marin".

Valéry is best known as a poet, and he is sometimes considered to be the last of the French symbolists. However, he published fewer than a hundred poems, and none of them drew much attention. On the night of 4 October 1892, during a heavy storm, Paul Valéry underwent an existential crisis, an event that made a huge impact on his writing career. Eventually, around 1898, he quit writing altogether, publishing not a word for nearly twenty years. This hiatus was in part due to the death of his mentor, Stéphane Mallarmé. When, in 1917, he finally broke his 'great silence' with the publication of "La Jeune Parque", he was forty-six years of age.

This obscure, but sublimely musical, masterpiece, of 512 alexandrine lines in rhyming couplets, had taken him four years to complete, and it immediately secured his fame. With "Le Cimetière marin" and "L'Ébauche d'un serpent," it is often considered one of the greatest French poems of the twentieth century.

The title was chosen late in the poem's gestation; it refers to the youngest of the three "Parcae" (the minor Roman deities also called "The Fates"), though for some readers the connection with that mythological figure is tenuous and problematic.

The poem is written in the first person, and is the soliloquy of a young woman contemplating life and death, engagement and withdrawal, love and estrangement, in a setting dominated by the sea, the sky, stars, rocky cliffs, and the rising sun. However, it is also possible to read the poem as an allegory on the way fate moves human affairs or as an attempt to comprehend the horrific violence in Europe at the time of the poem's composition. The poem is not about World War I, but it does try to address the relationships between destruction and beauty, and, in this sense, it resonates with ancient Greek meditations on these matters, especially in the plays of Sophocles and Aeschylus. There are, therefore, evident links with "le Cimetière marin", which is also a seaside meditation on comparably large themes.

Before "la Jeune Parque", Valéry's only publications of note were dialogues, articles, some poems, and a study of Leonardo da Vinci. In 1920 and 1922, he published two slim collections of verses. The first, "Album des vers anciens" (Album of old verses), was a revision of early but beautifully wrought smaller poems, some of which had been published individually before 1900. The second, "Charmes" (from the Latin "carmina", meaning "songs" and also "incantations"), further confirmed his reputation as a major French poet. The collection includes "le Cimetière marin", and many smaller poems with diverse structures. 'Le Cimetière marin' is mentioned or indirectly implied or referred to in at least four of Iris Murdoch's novels, The Unicorn, The Time of the Angels, The Nice and the Good and The Sea, The Sea.

Valéry's technique is quite orthodox in its essentials. His verse rhymes and scans in conventional ways, and it has much in common with the work of Mallarmé. His poem, "Palme", inspired James Merrill's celebrated 1974 poem "Lost in Translation", and his cerebral lyricism also influenced the American poet, Edgar Bowers.

His far more ample prose writings, peppered with many aphorisms and "bons mots", reveal a skeptical outlook on human nature, verging on the cynical. His view of state power was broadly liberal insofar as he believed that state power and infringements on the individual should be severely limited. Although he had flirted with nationalist ideas during the 1890s, he moved away from them by 1899, and believed that European culture owed its greatness to the ethnic diversity and universalism of the Roman Empire. He denounced the myth of "racial purity" and argued that such purity, if it existed, would only lead to stagnation—thus the mixing of races was necessary for progress and cultural development. In "America as a Projection of the European Mind", Valéry remarked that whenever he despaired about Europe's situation, he could "restore some degree of hope only by thinking of
the New World" and mused on the "happy variations" which could result from European "aesthetic ideas filtering into the powerful character of native Mexican art."

Raymond Poincaré, Louis de Broglie, André Gide, Henri Bergson, and Albert Einstein all respected Valéry's thinking and became friendly correspondents. Valéry was often asked to write articles on topics not of his choosing; the resulting intellectual journalism was collected in five volumes titled "Variétés".

Valéry's most striking achievement is perhaps his monumental intellectual diary, called the "Cahiers" (Notebooks). Early every morning of his adult life, he contributed something to the "Cahiers", prompting him to write: "Having dedicated those hours to the life of the mind, I thereby earn the right to be stupid for the rest of the day."

The subjects of his "Cahiers" entries often were, surprisingly, reflections on science and mathematics. In fact, arcane topics in these domains appear to have commanded far more of his considered attention than his celebrated poetry. The "Cahiers" also contain the first drafts of many aphorisms he later included in his books. To date, the "Cahiers" have been published in their entirety only as photostatic reproductions, and only since 1980 have they begun to receive scholarly scrutiny. The "Cahiers" have been translated into English in five volumes published by Peter Lang with the title "Cahiers/Notebooks".

In recent decades Valéry's thought has been considered a touchstone in the field of constructivist epistemology, as noted, for instance, by Jean-Louis Le Moigne in his description of constructivist history.

One of three epigraphs in Cormac McCarthy's novel Blood Meridian is from Valéry's Writing at the Yalu River (1895):

"Your ideas are terrifying and your hearts are faint. Your acts of pity and cruelty are absurd, committed with no calm, as if they were irresistible. Finally, you fear blood more and more. Blood and time".

In the book "El laberinto de la soledad" from Octavio Paz there are three verses of one of Valéry's poems: 
<poem>
Je pense, sur le bord doré de l’univers
A ce gout de périr qui prend la Pythonisse
En qui mugit l’espoir que le monde finisse.
</poem>

Oscar-winning Japanese director Hayao Miyazaki's 2013 film "The Wind Rises" and the Japanese novel of the same name (on which the film was partially based) take their title from Valéry's verse "Le vent se lève... il faut tenter de vivre !" ("The wind rises… We must try to live!") in the poem "Le Cimetière marin" ("The Graveyard by the Sea"). The same quote is used in the closing sentences of Anthony Burgess's 1962 novel The Wanting Seed.


In English translation:





</doc>
<doc id="23528" url="https://en.wikipedia.org/wiki?curid=23528" title="Pianist">
Pianist

A pianist () is an individual musician who plays the piano. Since most forms of Western music can make use of the piano, pianists have a wide repertoire and a wide variety of styles to choose from, among them traditional classical music, jazz, blues, and all sorts of popular music, including rock and roll. Most pianists can, to an extent, easily play other keyboard-related instruments such as the synthesizer, harpsichord, celesta, and the organ.

Modern classical pianists dedicate their careers to performing, recording, teaching, researching, and learning new works to expand their repertoire. They generally do not write or transcribe music as pianists did in the 19th century. Some classical pianists might specialize in accompaniment and chamber music, while others (though comparatively few) will perform as full-time soloists.

Mozart could be considered the first "concert pianist" as he performed widely on the piano. Composers Beethoven and Clementi from the classical era were also famed for their playing, as were, from the romantic era, Liszt, Brahms, Chopin, Mendelssohn, and Rachmaninoff. It was during the Classical period that the piano begins to establish its place in the hearts and homes of everyday people. From that era, leading performers less known as composers were Clara Schumann and Hans von Bülow. However, as we do not have modern audio recordings of most of these pianists, we rely mainly on written commentary to give us an account of their technique and style.

Jazz pianists almost always perform with other musicians. Their playing is more free than that of classical pianists and they create an air of spontaneity in their performances. They generally do not write down their compositions; improvisation is a significant part of their work. Well known jazz pianists include Art Tatum, Duke Ellington, Thelonious Monk, Oscar Peterson and Bud Powell.

Popular pianists might work as live performers (concert, theatre, etc.), session musicians, arrangers most likely feel at home with synthesizers and other electronic keyboard instruments. Notable popular pianists include Victor Borge who performed as a comedian; Richard Clayderman, who is known for his covers of popular tunes; and singer and entertainer Liberace, who at the height of his fame, was one of the highest-paid entertainers in the world.

A single listing of pianists in all genres would be impractical, given the multitude of musicians noted for their performances on the instrument. Below are links to lists of well-known or influential pianists divided by genres:







Many important composers were also virtuoso pianists. The following is an incomplete list of such musicians.




Some people, having received a solid piano training in their youth, decide not to continue their musical careers but choose nonmusical ones. As a result, there are prominent communities of "amateur pianists" all over the world that play at quite a high level and give concerts not to earn money but just for the love of music. The International Piano Competition for Outstanding Amateurs, held annually in Paris, attracts about one thousand listeners each year and is broadcast on French radio.

It is notable that Jon Nakamatsu, the Gold Medal winner of the Van Cliburn International Piano Competition for professional pianists in Fort Worth, Texas (1997) was at the moment of his victory technically an amateur: he never attended a music conservatory or majored in music, and worked as a high school German teacher at the time; it was only after the competition that he started pursuing a career as a classical pianist.

The German pianist Davide Martello is known for traveling around conflict zones to play his moving piano. Martello has previously been recognised by the European parliament for his “outstanding contribution to European cooperation and the promotion of common values”.



</doc>
<doc id="23529" url="https://en.wikipedia.org/wiki?curid=23529" title="Proverb">
Proverb

A proverb (from ) is a simple, concrete, traditional saying that expresses a perceived truth based on common sense or experience. Proverbs are often metaphorical and use formulaic language. Collectively, they form a genre of folklore.

Some proverbs exist in more than one language because people borrow them from languages and cultures similar to theirs. In the West, the Bible (including, but not limited to the Book of Proverbs) and medieval Latin (aided by the work of Erasmus) have played a considerable role in distributing proverbs. Not all Biblical proverbs, however, were distributed to the same extent: one scholar has gathered evidence to show that cultures in which the Bible is the "major spiritual book contain between three hundred and five hundred proverbs that stem from the Bible," whereas another shows that, of the 106 most common and widespread proverbs across Europe, eleven are from the Bible. However, almost every culture has its own unique proverbs.

Lord John Russell (c. 1850) observed poetically that a "proverb is the wit of one, and the wisdom of many." But giving the word "proverb" the sort of definition theorists need has proven to be a difficult task, and although scholars often quote Archer Taylor's argument that formulating a scientific "definition of a proverb is too difficult to repay the undertaking... An incommunicable quality tells us this sentence is proverbial and that one is not. Hence no definition will enable us to identify positively a sentence as proverbial," many students of proverbs have attempted to itemize its essential characteristics.

More constructively, Mieder has proposed the following definition, "A proverb is a short, generally known sentence of the folk which contains wisdom, truth, morals, and traditional views in a metaphorical, fixed, and memorizable form and which is handed down from generation to generation". Norrick created a table of distinctive features to distinguish proverbs from idioms, cliches, etc. Prahlad distinguishes proverbs from some other, closely related types of sayings, "True proverbs must further be distinguished from other types of proverbial speech, e.g. proverbial phrases, Wellerisms, maxims, quotations, and proverbial comparisons." Based on Persian proverbs, Zolfaghari and Ameri propose the following definition: "A proverb is a short sentence, which is well-known and at times rhythmic, including advice, sage themes and ethnic experiences, comprising simile, metaphor or irony which is well-known among people for its fluent wording, clarity of expression, simplicity, expansiveness and generality and is used either with or without change."

There are many sayings in English that are commonly referred to as "proverbs", such as weather sayings. Alan Dundes, however, rejects including such sayings among truly proverbs: "Are weather proverbs proverbs? I would say emphatically 'No!'" The definition of "proverb" has also changed over the years. For example, the following was labeled "A Yorkshire proverb" in 1883, but would not be categorized as a proverb by most today, "as throng as Throp's wife when she hanged herself with a dish-cloth". The changing of the definition of "proverb" is also noted in Turkish.

In other languages and cultures, the definition of "proverb" also differs from English. In the Chumburung language of Ghana, ""aŋase" are literal proverbs and "akpare" are metaphoric ones". Among the Bini of Nigeria, there are three words that are used to translate "proverb": "ere, ivbe", and "itan". The first relates to historical events, the second relates to current events, and the third was "linguistic ornamentation in formal discourse". Among the Balochi of Pakistan and Afghanistan, there is a word "batal" for ordinary proverbs and "bassīttuks" for "proverbs with background stories".

There are also language communities that combine proverbs and riddles in some sayings, leading some scholars to create the label "proverb riddles".


Proverbs come from a variety of sources. Some are, indeed, the result of people pondering and crafting language, such as some by Confucius, Plato, Baltasar Gracián, etc. Others are taken from such diverse sources as poetry, stories, songs, commercials, advertisements, movies, literature, etc. A number of the well known sayings of Jesus, Shakespeare, and others have become proverbs, though they were original at the time of their creation, and many of these sayings were not seen as proverbs when they were first coined. Many proverbs are also based on stories, often the end of a story. For example, the proverb "Who will bell the cat?" is from the end of a story about the mice planning how to be safe from the cat.

Some authors have created proverbs in their writings, such as J.R.R. Tolkien, and some of these proverbs have made their way into broader society. Similarly, C.S. Lewis' created proverb about a lobster in a pot, from the "Chronicles of Narnia", has also gained currency. In cases like this, deliberately created proverbs for fictional societies have become proverbs in real societies. In a fictional story set in a real society, the movie "Forrest Gump" introduced "Life is like a box of chocolates" into broad society. In at least one case, it appears that a proverb deliberately created by one writer has been naively picked up and used by another who assumed it to be an established Chinese proverb, Ford Madox Ford having picked up a proverb from Ernest Bramah, "It would be hypocrisy to seek for the person of the Sacred Emperor in a Low Tea House."

The proverb with "a longer history than any other recorded proverb in the world", going back to "around 1800 BC" is in a Sumerian clay tablet, "The bitch by her acting too hastily brought forth the blind". Though many proverbs are ancient, they were all newly created at some point by somebody. Sometimes it is easy to detect that a proverb is newly coined by a reference to something recent, such as the Haitian proverb "The fish that is being microwaved doesn't fear the lightning". Similarly, there is a recent Maltese proverb, "wil-muturi, ferh u duluri" "Women and motorcycles are joys and griefs"; the proverb is clearly new, but still formed as a traditional style couplet with rhyme. Also, there is a proverb in the Kafa language of Ethiopia that refers to the forced military conscription of the 1980s, "...the one who hid himself lived to have children." A Mongolian proverb also shows evidence of recent origin, "A beggar who sits on gold; Foam rubber piled on edge." A political candidate in Kenya popularised a new proverb in his 1995 campaign, "Chuth ber" "Immediacy is best". "The proverb has since been used in other contexts to prompt quick action." Over 1,400 new English proverbs are said to have been coined and gained currency in the 20th century. This process of creating proverbs is always ongoing, so that possible new proverbs are being created constantly. Those sayings that are adopted and used by an adequate number of people become proverbs in that society.
Interpreting proverbs is often complex, but is best done in a context. Interpreting proverbs from other cultures is much more difficult than interpreting proverbs in one's own culture. Even within English-speaking cultures, there is difference of opinion on how to interpret the proverb "A rolling stone gathers no moss." Some see it as condemning a person that keeps moving, seeing moss as a positive thing, such as profit; others see the proverb as praising people that keep moving and developing, seeing moss as a negative thing, such as negative habits.

Similarly, among Tajik speakers, the proverb "One hand cannot clap" has two significantly different interpretations. Most see the proverb as promoting teamwork. Others understand it to mean that an argument requires two people. In an extreme example, one researcher working in Ghana found that for a single Akan proverb, twelve different interpretations were given. Proveb interpretation is not automatic, even for people within a culture: Owomoyela tells of a Yoruba radio program that asked people to interpret an unfamiliar Yoruba proverb, "very few people could do so". Siran found that people who had moved out of the traditional Vute-speaking area of Cameroon were not able to interpret Vute proverbs correctly, even though they still spoke Vute. Their interpretations tended to be literal.

Children will sometimes interpret proverbs in a literal sense, not yet knowing how to understand the conventionalized metaphor. Interpretation of proverbs is also affected by injuries and diseases of the brain, "A hallmark of schizophrenia is impaired proverb interpretation."

Proverbs in various languages are found with a wide variety of grammatical structures. In English, for example, we find the following structures (in addition to others):

However, people will often quote only a fraction of a proverb to invoke an entire proverb, e.g. "All is fair" instead of "All is fair in love and war", and "A rolling stone" for "A rolling stone gathers no moss."

The grammar of proverbs is not always the typical grammar of the spoken language, often elements are moved around, to achieve rhyme or focus.

Another type of grammatical construction is the wellerism, a speaker and a quotation, often with an unusual circumstance, such as the following, a representative of a wellerism proverb found in many languages: "The bride couldn't dance; she said, 'The room floor isn't flat.'"

Another type of grammatical structure in proverbs is a short dialogue:

Because many proverbs are both poetic and traditional, they are often passed down in fixed forms. Though spoken language may change, many proverbs are often preserved in conservative, even archaic, form. In English, for example, "betwixt" is not used by many, but a form of it is still heard (or read) in the proverb "There is many a slip 'twixt the cup and the lip." The conservative form preserves the meter and the rhyme. This conservative nature of proverbs can result in archaic words and grammatical structures being preserved in individual proverbs, as has been documented in Amharic, Greek, Nsenga, Polish, Venda and Hebrew.

In addition, proverbs may still be used in languages which were once more widely known in a society, but are now no longer so widely known. For example, English speakers use some non-English proverbs that are drawn from languages that used to be widely understood by the educated class, e.g. "C'est la vie" from French and "Carpe diem" from Latin.

Proverbs are often handed down through generations. Therefore, "many proverbs refer to old measurements, obscure professions, outdated weapons, unknown plants, animals, names, and various other traditional matters."
Therefore, it is common that they preserve words that become less common and archaic in broader society. Proverbs in solid form—such as murals, carvings, and glass—can be viewed even after the language of their form is no longer widely understood, such as an Anglo-French proverb in a stained glass window in York.

Proverbs are often and easily translated and transferred from one language into another. "There is nothing so uncertain as the derivation of proverbs, the same proverb being often found in all nations, and it is impossible to assign its paternity."

Proverbs are often borrowed across lines of language, religion, and even time. For example, a proverb of the approximate form "No flies enter a mouth that is shut" is currently found in Spain, France, Ethiopia, and many countries in between. It is embraced as a true local proverb in many places and should not be excluded in any collection of proverbs because it is shared by the neighbors. However, though it has gone through multiple languages and millennia, the proverb can be traced back to an ancient Babylonian proverb (Pritchard 1958:146). Another example of a widely spread proverb is "A drowning person clutches at [frogs] foam", found in Peshai of Afghanistan and Orma of Kenya, and presumably places in between.

Proverbs about one hand clapping are common across Asia, from Dari in Afghanistan to Japan. Some studies have been done devoted to the spread of proverbs in certain regions, such as India and her neighbors and Europe. An extreme example of the borrowing and spread of proverbs was the work done to create a corpus of proverbs for Esperanto, where all the proverbs were translated from other languages.

It is often not possible to trace the direction of borrowing a proverb between languages. This is complicated by the fact that the borrowing may have been through plural languages. In some cases, it is possible to make a strong case for discerning the direction of the borrowing based on an artistic form of the proverb in one language, but a prosaic form in another language. For example, in Ethiopia there is a proverb "Of mothers and water, there is none evil." It is found in Amharic, Alaaba language, and Oromo, three languages of Ethiopia:
The Oromo version uses poetic features, such as the initial "ha" in both clauses with the final "-aa" in the same word, and both clauses ending with "-an". Also, both clauses are built with the vowel "a" in the first and last words, but the vowel "i" in the one syllable central word. In contrast, the Amharic and Alaaba versions of the proverb show little evidence of sound-based art.

However, not all languages have proverbs. Proverbs are (nearly) universal across Europe, Asia, and Africa. Some languages in the Pacific have them, such as Maori. Other Pacific languages do not, e.g. "there are no proverbs in Kilivila" of the Trobriand Islands. However, in the New World, there are almost no proverbs: "While proverbs abound in the thousands in most cultures of the world, it remains a riddle why the Native Americans have hardly any proverb tradition at all." Hakamies has examined the matter of whether proverbs are found universally, a universal genre, concluding that they are not.

Proverbs are used in conversation by adults more than children, partially because adults have learned more proverbs than children. Also, using proverbs well is a skill that is developed over years. Additionally, children have not mastered the patterns of metaphorical expression that are invoked in proverb use. Proverbs, because they are indirect, allow a speaker to disagree or give advice in a way that may be less offensive. Studying actual proverb use in conversation, however, is difficult since the researcher must wait for proverbs to happen. An Ethiopian researcher, Tadesse Jaleta Jirata, made headway in such research by attending and taking notes at events where he knew proverbs were expected to be part of the conversations.

Many authors have used proverbs in their writings, for a very wide variety of literary genres: epics, novels, poems, short stories.

Probably the most famous user of proverbs in novels is J. R. R. Tolkien in his "The Hobbit" and "The Lord of the Rings" series. Herman Melville is noted for creating proverbs in Moby Dick and in his poetry. Also, C. S. Lewis created a dozen proverbs in "The Horse and His Boy", and Mercedes Lackey created dozens for her invented Shin'a'in and Tale'edras cultures; Lackey's proverbs are notable in that they are reminiscent to those of Ancient Asia - e.g. "Just because you feel certain an enemy is lurking behind every bush, it doesn't follow that you are wrong" is like to "Before telling secrets on the road, look in the bushes." These authors are notable for not only using proverbs as integral to the development of the characters and the story line, but also for creating proverbs.

Among medieval literary texts, Geoffrey Chaucer's Troilus and Criseyde plays a special role because Chaucer's usage seems to challenge the truth value of proverbs by exposing their epistemological unreliability. Rabelais used proverbs to write an entire chapter of Gargantua.

The patterns of using proverbs in literature can change over time. A study of "classical Chinese novels" found proverb use as frequently as one proverb every 3,500 words in "Water Margin" ("Sui-hu chuan") and one proverb every 4,000 words in "Wen Jou-hsiang". But modern Chinese novels have fewer proverbs by far.
Proverbs (or portions of them) have been the inspiration for titles of books: "The Bigger they Come" by Erle Stanley Gardner, and "Birds of a Feather" (several books with this title), "Devil in the Details" (multiple books with this title). Sometimes a title alludes to a proverb, but does not actually quote much of it, such as "The Gift Horse's Mouth" by Robert Campbell. Some books or stories have titles that are twisted proverbs, anti-proverbs, such as "No use dying over spilled milk", "When life gives you lululemons," and two books titled "Blessed are the Cheesemakers". The twisted proverb of last title was also used in the Monty Python movie Life of Brian, where a person mishears one of Jesus Christ's beatitudes, "I think it was 'Blessed are the cheesemakers.'"

Some books and stories are built around a proverb. Some of Tolkien's books have been analyzed as having "governing proverbs" where "the acton of a book turns on or fulfills a proverbial saying." Some stories have been written with a proverb overtly as an opening, such as "A stitch in time saves nine" at the beginning of "Kitty's Class Day", one of Louisa May Alcott's "Proverb Stories". Other times, a proverb appears at the end of a story, summing up a moral to the story, frequently found in Aesop's Fables, such as "Heaven helps those who help themselves" from "Hercules and the Wagoner". In a novel by the Ivorian novelist Ahmadou Kourouma, "proverbs are used to conclude each chapter".

Proverbs have also been used strategically by poets. Sometimes proverbs (or portions of them or anti-proverbs) are used for titles, such as "A bird in the bush" by Lord Kennet and his stepson Peter Scott and "The blind leading the blind" by Lisa Mueller. Sometimes, multiple proverbs are important parts of poems, such as Paul Muldoon's "Symposium", which begins "You can lead a horse to water but you can't make it hold its nose to the grindstone and hunt with the hounds. Every dog has a stitch in time..." In Finnish there are proverb poems written hundreds of years ago. The Turkish poet Refiki wrote an entire poem by stringing proverbs together, which has been translated into English poetically yielding such verses as "Be watchful and be wary, / But seldom grant a boon; / The man who calls the piper / Will also call the tune." Eliza Griswold also created a poem by stringing proverbs together, Libyan proverbs translated into English.

Because proverbs are familiar and often pointed, they have been used by a number of hip-hop poets. This has been true not only in the USA, birthplace of hip-hop, but also in Nigeria. Since Nigeria is so multilingual, hip-hop poets there use proverbs from various languages, mixing them in as it fits their need, sometimes translating the original. For example,
"They forget say ogbon ju agbaralo
They forget that wisdom is greater than power"

Some authors have bent and twisted proverbs, creating anti-proverbs, for a variety of literary effects. For example, in the Harry Potter novels, J. K. Rowling reshapes a standard English proverb into "It's no good crying over spilt potion" and Dumbledore advises Harry not to "count your owls before they are delivered". In a slightly different use of reshaping proverbs, in the Aubrey–Maturin series of historical naval novels by Patrick O'Brian, Capt. Jack Aubrey humorously mangles and mis-splices proverbs, such as "Never count the bear's skin before it is hatched" and "There's a good deal to be said for making hay while the iron is hot." Earlier than O'Brian's Aubrey, Beatrice Grimshaw also used repeated splicings of proverbs in the mouth of an eccentric marquis to create a memorable character in "The Sorcerer's Stone", such as "The proof of the pudding sweeps clean" (p. 109) and "A stitch in time is as good as a mile" (p. 97).

Because proverbs are so much a part of the language and culture, authors have sometimes used proverbs in historical fiction effectively, but anachronistically, before the proverb was actually known. For example, the novel "Ramage and the Rebels", by Dudley Pope is set in approximately 1800. Captain Ramage reminds his adversary "You are supposed to know that it is dangerous to change horses in midstream" (p. 259), with another allusion to the same proverb three pages later. However, the proverb about changing horses in midstream is reliably dated to 1864, so the proverb could not have been known or used by a character from that period.

Some authors have used so many proverbs that there have been entire books written cataloging their proverb usage, such as Charles Dickens, Agatha Christie, George Bernard Shaw, Miguel de Cervantes, and Friedrich Nietzsche.

On the non-fiction side, proverbs have also been used by authors for articles that have no connection to the study of proverbs. Some have been used as the basis for book titles, e.g. "I Shop, Therefore I Am: Compulsive Buying and the Search for Self" by April Lane Benson. Some proverbs been used as the basis for article titles, though often in altered form: "All our eggs in a broken basket: How the Human Terrain System is undermining sustainable military cultural competence" and "Should Rolling Stones Worry About Gathering Moss?", "Between a Rock and a Soft Place", and the pair "Verbs of a feather flock together" and "Verbs of a feather flock together II". Proverbs have been noted as common in subtitles of articles such as "Discontinued intergenerational transmission of Czech in Texas: 'Hindsight is better than foresight'." Also, the reverse is found with a proverb (complete or partial) as the title, then an explanatory subtitle, "To Change or Not to Change Horses: The World War II Elections". Many authors have cited proverbs as epigrams at the beginning of their articles, e.g. "'If you want to dismantle a hedge, remove one thorn at a time' Somali proverb" in an article on peacemaking in Somalia. An article about research among the Māori used a Māori proverb as a title, then began the article with the Māori form of the proverb as an epigram "Set the overgrown bush alight and the new flax shoots will spring up", followed by three paragraphs about how the proverb served as a metaphor for the research and the present context. A British proverb has even been used as the title for a doctoral dissertation: "Where there is muck there is brass".

Similarly to other forms of literature, proverbs have also been used as important units of language in drama and films. This is true from the days of classical Greek works to old French to Shakespeare, to 19th Century Spanish, to today. The use of proverbs in drama and film today is still found in languages around the world, including Yorùbá and Igbo of Nigeria.

A film that makes rich use of proverbs is "Forrest Gump", known for both using and creating proverbs. Other studies of the use of proverbs in film include work by Kevin McKenna on the Russian film "Aleksandr Nevsky", Haase's study of an adaptation of Little Red Riding Hood, Elias Dominguez Barajas on the film "Viva Zapata!", and Aboneh Ashagrie on "The Athlete" (a movie in Amharic about Abebe Bikila).

Television programs have also been named with reference to proverbs, usually shortened, such Birds of a Feather and Diff'rent Strokes.

In the case of "Forrest Gump", the screenplay by Eric Roth had more proverbs than the novel by Winston Groom, but for "The Harder They Come", the reverse is true, where the novel derived from the movie by Michael Thelwell has many more proverbs than the movie.

Éric Rohmer, the French film director, directed a series of films, the "Comedies and Proverbs", where each film was based on a proverb: "The Aviator's Wife", "The Perfect Marriage", "Pauline at the Beach", "Full Moon in Paris" (the film's proverb was invented by Rohmer himself: "The one who has two wives loses his soul, the one who has two houses loses his mind."), "The Green Ray", "Boyfriends and Girlfriends".

Movie titles based on proverbs include "Murder Will Out (1939 film)", "Try, Try Again", and "The Harder They Fall". A twisted anti-proverb was the title for a Three Stooges film, "A Bird in the Head". The title of an award-winning Turkish film, Three Monkeys, also invokes a proverb, though the title does not fully quote it.

They have also been used as the titles of plays: "Baby with the Bathwater" by Christopher Durang, "Dog Eat Dog" by Mary Gallagher, and "The Dog in the Manger" by Charles Hale Hoyt. The use of proverbs as titles for plays is not, of course, limited to English plays: "Il faut qu'une porte soit ouverte ou fermée" (A door must be open or closed) by Paul de Musset. Proverbs have also been used in musical dramas, such as "The Full Monty", which has been shown to use proverbs in clever ways. In the lyrics for "Beauty and the Beast", Gaston plays with three proverbs in sequence, "All roads lead to.../The best things in life are.../All's well that ends with...me."

Proverbs are often poetic in and of themselves, making them ideally suited for adapting into songs. Proverbs have been used in music from opera to country to hip-hop. Proverbs have also been used in music in many languages, such as the Akan language the Igede language, and Spanish.
In English the proverb (or rather the beginning of the proverb), If the shoe fits has been used as a title for three albums and five songs. Other English examples of using proverbs in music include Elvis Presley's "Easy come, easy go", Harold Robe's "Never swap horses when you're crossing a stream", Arthur Gillespie's "Absence makes the heart grow fonder", Bob Dylan's "Like a rolling stone", Cher's "Apples don't fall far from the tree". Lynn Anderson made famous a song full of proverbs, "I never promised you a rose garden" (written by Joe South). In choral music, we find Michael Torke's "Proverbs" for female voice and ensemble. A number of Blues musicians have also used proverbs extensively. The frequent use of proverbs in Country music has led to published studies of proverbs in this genre. The Reggae artist Jahdan Blakkamoore has recorded a piece titled "Proverbs Remix". The opera "Maldobrìe" contains careful use of proverbs. An extreme example of many proverbs used in composing songs is a song consisting almost entirely of proverbs performed by Bruce Springsteen, "My best was never good enough". The Mighty Diamonds recorded a song called simply "Proverbs".

The band Fleet Foxes used the proverb painting Netherlandish Proverbs for the cover of their eponymous album Fleet Foxes.

In addition to proverbs being used in songs themselves, some rock bands have used parts of proverbs as their names, such as the Rolling Stones, Bad Company, The Mothers of Invention, Feast or Famine, Of Mice and Men. There have been at least two groups that called themselves "The Proverbs", and there is a hip-hop performer in South Africa known as "Proverb". In addition, many albums have been named with allusions to proverbs, such as "Spilt milk" (a title used by Jellyfish and also Kristina Train), "The more things change" by Machine Head, "Silk purse" by Linda Ronstadt, "Another day, another dollar" by DJ Scream Roccett, "The blind leading the naked" by Violent Femmes, "What's good for the goose is good for the gander" by Bobby Rush, "Resistance is Futile" by Steve Coleman, "Murder will out" by Fan the Fury. The proverb "Feast or famine" has been used as an album title by Chuck Ragan, Reef the Lost Cauze, Indiginus, and DaVinci. Whitehorse mixed two proverbs for the name of their album "Leave no bridge unburned". The band Splinter Group released an album titled "When in Rome, Eat Lions". The band Downcount used a proverb for the name of their tour, "Come and take it".

From ancient times, people around the world have recorded proverbs in visual form. This has been done in two ways. First, proverbs have been "written" to be displayed, often in a decorative manner, such as on pottery, cross-stitch, murals, kangas (East African women's wraps), quilts, a stained glass window, and graffiti.

Secondly, proverbs have often been visually depicted in a variety of media, including paintings, etchings, and sculpture. Jakob Jordaens painted a plaque with a proverb about drunkenness above a drunk man wearing a crown, titled "The King Drinks". Probably the most famous examples of depicting proverbs are the different versions of the paintings "Netherlandish Proverbs" by the father and son Pieter Bruegel the Elder and Pieter Brueghel the Younger, the proverbial meanings of these paintings being the subject of a 2004 conference, which led to a published volume of studies (Mieder 2004a). The same father and son also painted versions of The Blind Leading the Blind, a Biblical proverb. These and similar paintings inspired another famous painting depicting some proverbs and also idioms (leading to a series of additional paintings), such as "Proverbidioms" by T. E. Breitenbach. Another painting inspired by Bruegel's work is by the Chinese artist, Ah To, who created a painting illustrating 81 Cantonese sayings. Corey Barksdale has produced a book of paintings with specific proverbs and pithy quotations. The British artist Chris Gollon has painted a major work entitled "Big Fish Eat Little Fish", a title echoing Bruegel's painting Big Fishes Eat Little Fishes.

Sometimes well-known proverbs are pictured on objects, without a text actually quoting the proverb, such as the three wise monkeys who remind us "Hear no evil, see no evil, speak no evil". When the proverb is well known, viewers are able to recognize the proverb and understand the image appropriately, but if viewers do not recognize the proverb, much of the effect of the image is lost. For example, there is a Japanese painting in the Bonsai museum in Saitama city that depicted flowers on a dead tree, but only when the curator learned the ancient (and no longer current) proverb "Flowers on a dead tree" did the curator understand the deeper meaning of the painting.

A study of school students found that students remembered proverbs better when there were visual representations of proverbs along with the verbal form of the proverbs.

A bibliography on proverbs in visual form has been prepared by Mieder and Sobieski (1999). Interpreting visual images of proverbs is subjective, but familiarity with the depicted proverb helps.

Some artists have used proverbs and anti-proverbs for titles of their paintings, alluding to a proverb rather than picturing it. For example, Vivienne LeWitt painted a piece titled "If the shoe doesn't fit, must we change the foot?", which shows neither foot nor shoe, but a woman counting her money as she contemplates different options when buying vegetables.

In 2018, 13 sculptures depicting Maltese proverbs were installed in open spaces of downtown Valletta.

Cartoonists, both editorial and pure humorists, have often used proverbs, sometimes primarily building on the text, sometimes primarily on the situation visually, the best cartoons combining both. Not surprisingly, cartoonists often twist proverbs, such as visually depicting a proverb literally or twisting the text as an anti-proverb. An example with all of these traits is a cartoon showing a waitress delivering two plates with worms on them, telling the customers, "Two early bird specials... here ya go."

The traditional Three wise monkeys were depicted in Bizarro with different labels. Instead of the negative imperatives, the one with ears covered bore the sign "See and speak evil", the one with eyes covered bore the sign "See and hear evil", etc. The caption at the bottom read "The power of positive thinking." Another cartoon showed a customer in a pharmacy telling a pharmacist, "I'll have an ounce of prevention." The comic strip The Argyle Sweater showed an Egyptian archeologist loading a mummy on the roof of a vehicle, refusing the offer of a rope to tie it on, with the caption "A fool and his mummy are soon parted." The comic One Big Happy showed a conversation where one person repeatedly posed part of various proverb and the other tried to complete each one, resulting in such humorous results as "Don't change horses... unless you can lift those heavy diapers."

Editorial cartoons can use proverbs to make their points with extra force as they can invoke the wisdom of society, not just the opinion of the editors. In an example that invoked a proverb only visually, when a US government agency (GSA) was caught spending money extravagantly, a cartoon showed a black pot labeled "Congress" telling a black kettle labeled "GSA", "Stop wasting the taxpayers' money!" It may have taken some readers a moment of pondering to understand it, but the impact of the message was the stronger for it.

Cartoons with proverbs are so common that Wolfgang Mieder has published a collected volume of them, many of them editorial cartoons. For example, a German editorial cartoon linked a current politician to the Nazis, showing him with a bottle of swastika-labeled wine and the caption "In vino veritas".

One cartoonist very self-consciously drew and wrote cartoons based on proverbs for the University of Vermont student newspaper "The Water Tower", under the title "Proverb place".

Proverbs are frequently used in advertising, often in slightly modified form.
Ford once advertised its Thunderbird with, "One drive is worth a thousand words" (Mieder 2004b: 84). This is doubly interesting since the underlying proverb behind this, "One picture is worth a thousand words," was originally introduced into the English proverb repertoire in an ad for televisions (Mieder 2004b: 83).

A few of the many proverbs adapted and used in advertising include:

The GEICO company has created a series of television ads that are built around proverbs, such as "A bird in the hand is worth two in the bush", and "The pen is mightier than the sword", "Pigs may fly/When pigs fly", "If a tree falls in the forest...", and "Words can never hurt you". Doritos made a commercial based on the proverb, "When pigs fly." Many advertisements that use proverbs shorten or amend them, such as, "Think outside the shoebox." 
Use of proverbs in advertising is not limited to the English language. Seda Başer Çoban has studied the use of proverbs in Turkish advertising. Tatira has given a number of examples of proverbs used in advertising in Zimbabwe. However, unlike the examples given above in English, all of which are anti-proverbs, Tatira's examples are standard proverbs. Where the English proverbs above are meant to make a potential customer smile, in one of the Zimbabwean examples "both the content of the proverb and the fact that it is phrased as a proverb secure the idea of a secure time-honored relationship between the company and the individuals". When newer buses were imported, owners of older buses compensated by painting a traditional proverb on the sides of their buses, "Going fast does not assure safe arrival".

There are often proverbs that contradict each other, such as "Look before you leap" and "He who hesitates is lost", or "Many hands make light work" and "Too many cooks spoil the broth". These have been labeled "counter proverbs" or "antonymous proverbs". When there are such counter proverbs, each can be used in its own appropriate situation, and neither is intended to be a universal truth. Some pairs of proverbs are fully contradictory: “A messy desk is a sign of intelligence” and “A neat desk is a sign of a sick mind”.

The concept of "counter proverb" is more about pairs of contradictory proverbs than about the use of proverbs to counter each other in an argument. For example, from the Tafi language of Ghana, the following pair of proverbs are counter to each other but are each used in appropriate contexts, "A co-wife who is too powerful for you, you address her as your mother" and "Do not call your mother's co-wife your mother..." In Nepali, there is a set of totally contradictory proverbs: "Religion is victorious and sin erodes" and "Religion erodes and sin is victorious".
Also, the following pair are counter proverbs from the Kasena of Ghana: "It is the patient person who will milk a barren cow" and "The person who would milk a barren cow must prepare for a kick on the forehead". The two contradict each other, whether they are used in an argument or not (though indeed they were used in an argument). But the same work contains an appendix with many examples of proverbs used in arguing for contrary positions, but proverbs that are not inherently contradictory, such as "One is better off with hope of a cow's return than news of its death" countered by "If you don't know a goat [before its death] you mock at its skin". Though this pair was used in a contradictory way in a conversation, they are not a set of "counter proverbs".

Discussing counter proverbs in the Badaga language, Hockings explained that in his large collection "a few proverbs are mutually contradictory... we can be sure that the Badagas do not see the matter that way, and would explain such apparent contradictions by reasoning that proverb "x" is used in one context, while "y" is used in quite another." Comparing Korean proverbs, "when you compare two proverbs, often they will be contradictory." They are used for "a particular situation".

"Counter proverbs" are not the same as a "paradoxical proverb", a proverb that contains a seeming paradox.

In many cultures, proverbs are so important and so prominent that there are proverbs about proverbs, that is, "metaproverbs". The most famous one is from Yoruba of Nigeria, "Proverbs are the horses of speech, if communication is lost we use proverbs to find it," used by Wole Soyinka in "Death and the King's Horsemen". In Mieder's bibliography of proverb studies, there are twelve publications listed as describing metaproverbs. Other metaproverbs include:

There is a growing interest in deliberately using proverbs to achieve goals, usually to support and promote changes in society. Proverbs have also been used for public health promotion, such as promoting breast feeding with a shawl bearing a Swahili proverb "Mother's milk is sweet". Proverbs have also been applied for helping people manage diabetes, to combat prostitution, and for community development., to resolve conflicts, and to slow the transmission of HIV.

The most active field deliberately using proverbs is Christian ministry, where Joseph G. Healey and others have deliberately worked to catalyze the collection of proverbs from smaller languages and the application of them in a wide variety of church-related ministries, resulting in publications of collections and applications. This attention to proverbs by those in Christian ministries is not new, many pioneering proverb collections having been collected and published by Christian workers.

U.S. Navy Captain Edward Zellem pioneered the use of Afghan proverbs as a positive relationship-building tool during the war in Afghanistan, and in 2012 he published two bilingual collections of Afghan proverbs in Dari and English, part of an effort of nationbuilding, followed by a volume of Pashto proverbs in 2014.

There is a longstanding debate among proverb scholars as to whether the cultural values of specific language communities are reflected (to varying degree) in their proverbs. Many claim that the proverbs of a particular culture reflect the values of that specific culture, at least to some degree. Many writers have asserted that the proverbs of their cultures reflect their culture and values; this can be seen in such titles as the following: "An introduction to Kasena society and culture through their proverbs", Prejudice, power, and poverty in Haiti: a study of a nation's culture as seen through its proverbs, Proverbiality and worldview in Maltese and Arabic proverbs, Fatalistic traits in Finnish proverbs, "Vietnamese cultural patterns and values as expressed in proverbs", "The Wisdom and Philosophy of the Gikuyu proverbs: The Kihooto worldview", "Spanish Grammar and Culture through Proverbs," and "How Russian Proverbs Present the Russian National Character". Kohistani has written a thesis to show how understanding Afghan Dari proverbs will help Europeans understand Afghan culture.

However, a number of scholars argue that such claims are not valid. They have used a variety of arguments. Grauberg argues that since many proverbs are so widely circulated they are reflections of broad human experience, not any one culture's unique viewpoint. Related to this line of argument, from a collection of 199 American proverbs, Jente showed that only 10 were coined in the USA, so that most of these proverbs would not reflect uniquely American values. Giving another line of reasoning that proverbs should not be trusted as a simplistic guide to cultural values, Mieder once observed "proverbs come and go, that is, antiquated proverbs with messages and images we no longer relate to are dropped from our proverb repertoire, while new proverbs are created to reflect the mores and values of our time", so old proverbs still in circulation might reflect past values of a culture more than its current values. Also, within any language's proverb repertoire, there may be "counter proverbs", proverbs that contradict each other on the surface (see section above). When examining such counter proverbs, it is difficult to discern an underlying cultural value. With so many barriers to a simple calculation of values directly from proverbs, some feel "one cannot draw conclusions about values of speakers simply from the texts of proverbs".

Many outsiders have studied proverbs to discern and understand cultural values and world view of cultural communities. These outsider scholars are confident that they have gained insights into the local cultures by studying proverbs, but this is not universally accepted.
Seeking empirical evidence to evaluate the question of whether proverbs reflect a culture's values, some have counted the proverbs that support various values. For example, Moon lists what he sees as the top ten core cultural values of the Builsa society of Ghana, as exemplified by proverbs. He found that 18% of the proverbs he analyzed supported the value of being a member of the community, rather than being independent. This was corroboration to other evidence that collective community membership is an important value among the Builsa. In studying Tajik proverbs, Bell notes that the proverbs in his corpus "Consistently illustrate Tajik values" and "The most often observed proverbs reflect the focal and specific values" discerned in the thesis.

A study of English proverbs created since 1900 showed in the 1960s a sudden and significant increase in proverbs that reflected more casual attitudes toward sex. Since the 1960s was also the decade of the Sexual revolution, this shows a strong statistical link between the changed values of the decades and a change in the proverbs coined and used. Another study mining the same volume counted Anglo-American proverbs about religion to show that proverbs indicate attitudes toward religion are going downhill.

There are many examples where cultural values have been explained and illustrated by proverbs. For example, from India, the concept that birth determines one's nature "is illustrated in the oft-repeated proverb: there can be no friendship between grass-eaters and meat-eaters, between a food and its eater". Proverbs have been used to explain and illustrate the Fulani cultural value of "pulaaku". But using proverbs to "illustrate" a cultural value is not the same as using a collection of proverbs to "discern" cultural values. In a comparative study between Spanish and Jordanian proverbs it is defined the social imagination for the mother as an archetype in the context of role transformation and in contrast with the roles of husband, son and brother, in two societies which might be occasionally associated with sexist and /or rural ideologies.

Some scholars have adopted a cautious approach, acknowledging at least a genuine, though limited, link between cultural values and proverbs: "The cultural portrait painted by proverbs may be fragmented, contradictory, or otherwise at variance with reality... but must be regarded not as accurate renderings but rather as tantalizing shadows of the culture which spawned them." There is not yet agreement on the issue of whether, and how much, cultural values are reflected in a culture's proverbs.

It is clear that the Soviet Union believed that proverbs had a direct link to the values of a culture, as they used them to try to create changes in the values of cultures within their sphere of domination. Sometimes they took old Russian proverbs and altered them into socialist forms. These new proverbs promoted Socialism and its attendant values, such as atheism and collectivism, e.g. "Bread is given to us not by Christ, but by machines and collective farms" and "A good harvest is had only by a collective farm." They did not limit their efforts to Russian, but also produced "newly coined proverbs that conformed to socialist thought" in Tajik and other languages of the USSR.
Many proverbs from around the world address matters of ethics and expected of behavior. Therefore, it is not surprising that proverbs are often important texts in religions. The most obvious example is the Book of Proverbs in the Bible. Additional proverbs have also been coined to support religious values, such as the following from Dari of Afghanistan: "In childhood you're playful, In youth you're lustful, In old age you're feeble, So when will you before God be worshipful?"

Clearly proverbs in religion are not limited to monotheists; among the Badagas of India (Sahivite Hindus), there is a traditional proverb "Catch hold of and join with the man who has placed sacred ash [on himself]." Proverbs are widely associated with large religions that draw from sacred books, but they are also used for religious purposes among groups with their own traditional religions, such as the Guji Oromo. The broadest comparative study of proverbs across religions is "The eleven religions and their proverbial lore, a comparative study. A reference book to the eleven surviving major religions of the world" by Selwyn Gurney Champion, from 1945. Some sayings from sacred books also become proverbs, even if they were not obviously proverbs in the original passage of the sacred book. For example, many quote "Be sure your sin will find you out" as a proverb from the Bible, but there is no evidence it was proverbial in its original usage (Numbers 32:23).

Not all religious references in proverbs are positive, some are cynical, such as the Tajik, "Do as the mullah says, not as he does." Also, note the Italian proverb, "One barrel of wine can work more miracles than a church full of saints". An Indian proverb is cynical about devotees of Hinduism, "[Only] When in distress, a man calls on Rama". In the context of Tibetan Buddhism, some Ladakhi proverbs mock the lamas, e.g. "If the lama's own head does not come out cleanly, how will he do the drawing upwards of the dead?... used for deriding the immoral life of the lamas." Proverbs do not have to explicitly mention religion or religious figures to be used to mock a religion, seen in the fact that in a collection of 555 proverbs from the Lur, a Muslim group in Iran, the explanations for 15 of them use illustrations that mock Muslim clerics.

Dammann wrote, "In the [African] traditional religions, specific religious ideas recede into the background... The influence of Islam manifests itself in African proverbs... Christian influences, on the contrary, are rare." If widely true in Africa, this is likely due to the longer presence of Islam in many parts of Africa. Reflection of Christian values is common in Amharic proverbs of Ethiopia, an area that has had a presence of Christianity for well over 1,000 years. The Islamic proverbial reproduction may also be shown in the image of some animals such as the dog. Although dog is portrayed in many European proverbs as the most faithful friend of man, it is represented in some Islamic countries as impure, dirty, vile, cowardly, ungrateful and treacherous, in addition to links to negative human superstitions such as loneliness, indifference and bad luck.

Though much proverb scholarship is done by literary scholars, those studying the human mind have used proverbs in a variety of studies. One of the earliest studies in this field is the "Proverbs Test" by Gorham, developed in 1956. A similar test is being prepared in German. Proverbs have been used to evaluate dementia, study the cognitive development of children, measure the results of brain injuries, and study how the mind processes figurative language.

The study of proverbs is called paremiology which has a variety of uses in the study of such topics as philosophy, linguistics, and folklore. There are several types and styles of proverbs which are analyzed within Paremiology as is the use and misuse of familiar expressions which are not strictly 'proverbial' in the dictionary definition of being fixed sentences

Grigorii Permjakov developed the concept of the core set of proverbs that full members of society know, what he called the "paremiological minimum" (1979). For example, an adult American is expected to be familiar with "Birds of a feather flock together", part of the American paremiological minimum. However, an average adult American is not expected to know "Fair in the cradle, foul in the saddle", an old English proverb that is not part of the current American paremiological minimum. Thinking more widely than merely proverbs, Permjakov observed "every adult Russian language speaker (over 20 years of age) knows no fewer than 800 proverbs, proverbial expressions, popular literary quotations and other forms of cliches". Studies of the paremiological minimum have been done for a limited number of languages, including Russian, Hungarian, Czech, Somali, Nepali, Gujarati, Spanish, Esperanto, Polish, Ukrainian. Two noted examples of attempts to establish a paremiological minimum in America are by Haas (2008) and Hirsch, Kett, and Trefil (1988), the latter more prescriptive than descriptive. There is not yet a recognized standard method for calculating the paremiological minimum, as seen by comparing the various efforts to establish the paremiological minimum in a number of languages.

A seminal work in the study of proverbs is Archer Taylor's "The Proverb" (1931), later republished by Wolfgang Mieder with Taylor's Index included (1985/1934). A good introduction to the study of proverbs is Mieder's 2004 volume, "Proverbs: A Handbook". Mieder has also published a series of bibliography volumes on proverb research, as well as a large number of articles and other books in the field. Stan Nussbaum has edited a large collection on proverbs of Africa, published on a CD, including reprints of out-of-print collections, original collections, and works on analysis, bibliography, and application of proverbs to Christian ministry (1998). Paczolay has compared proverbs across Europe and published a collection of similar proverbs in 55 languages (1997). Mieder edits an academic journal of proverb study, "Proverbium" (), many back issues of which are available online. A volume containing articles on a wide variety of topics touching on proverbs was edited by Mieder and Alan Dundes (1994/1981). "Paremia" is a Spanish-language journal on proverbs, with articles available online. There are also papers on proverbs published in conference proceedings volumes from the annual Interdisciplinary Colloquium on Proverbs in Tavira, Portugal. Mieder has published a two-volume "International Bibliography of Paremiology and Phraseology", with a topical, language, and author index. Mieder has published a bibliography of collections of proverbs from around the world. A broad introduction to proverb study, "Introduction to Paremiology", edited by Hrisztalina Hrisztova-Gotthardt and Melita Aleksa Varga has been published in both hardcover and free open access, with articles by a dozen different authors.

The study of proverbs has been built by a number of notable scholars and contributors. Earlier scholars were more concerned with collecting than analyzing. Desiderius Erasmus was a Latin scholar (1466 – 1536), whose collection of Latin proverbs, known as "Adagia", spread Latin proverbs across Europe. Juan de Mal Lara was a 16th century Spanish scholar, one of his books being 1568 "Philosophia vulgar", the first part of which contains one thousand and one sayings. Hernán Núñez published a collection of Spanish proverbs (1555).

In the 19th century, a growing number of scholars published collections of proverbs, such as Samuel Adalberg who published collections of Yiddish proverbs (1888 & 1890) and Polish proverbs (1889–1894). Samuel Ajayi Crowther, the Anglican bishop in Nigeria, published a collection of Yoruba proverbs (1852). Elias Lönnrot published a collection of Finnish proverbs (1842).

From the 20th century onwards, proverb scholars were involved in not only collecting proverbs, but also analyzing and comparing proverbs. Alan Dundes was a 20th century American folklorist whose scholarly output on proverbs led Wolfgang Mieder to refer to him as a "pioneering paremiologist". Matti Kuusi was a 20th century Finnish paremiologist, the creator of the Matti Kuusi international type system of proverbs. With encouragement from Archer Taylor, he founded the journal "Proverbium: Bulletin d'Information sur les Recherches Parémiologiques", published from 1965 to 1975 by the Society for Finnish Literature, which was later restarted as "Proverbium: International Yearbook of Proverb Scholarship". Archer Taylor was a 20th century American scholar, best known for his "magisterial" book "The Proverb". Dimitrios Loukatos was a 20th century Greek proverb scholar, author of such works as "Aetiological Tales of Modern Greek Proverbs". Arvo Krikmann (1939 – 2017) was an Estonian proverb scholar, whom Wolfgang Mieder called "one of the leading paremiologists in the world" and "master folklorist and paremiologist". Elisabeth Piirainen was a German scholar with 50 proverb-related publications.

Current proverb scholars have continued the trend to be involved in analysis as well as collection of proverbs. Claude Buridant is a 20th century French scholar whose work has concentrated on Romance languages. Galit Hasan-Rokem is an Israeli scholar, associate editor of "Proverbium: The yearbook of international proverb scholarship", since 1984. She has written on proverbs in Jewish traditions. Joseph G. Healey is an American Catholic missionary in Kenya who has led a movement to sponsor African proverb scholars to collect proverbs from their own language communities. This led Wolfgang Mieder to dedicate the "International Bibliography of New and Reprinted Proverb Collections" section of "Proverbium" 32 to Healey. Barbara Kirshenblatt-Gimblett is a scholar of Jewish history and folklore, including proverbs. Wolfgang Mieder is a German-born proverb scholar who has worked his entire academic career in the USA. He is the editor of ‘’Proverbium’’ and the author of the two volume "International Bibliography of Paremiology and Phraseology". He has been honored by four festschrift publications. He has also been recognized by biographical publications that focused on his scholarship. Dora Sakayan is a scholar who has written about German and Armenian studies, including "Armenian Proverbs: A Paremiological Study with an Anthology of 2,500 Armenian Folk Sayings Selected and Translated into English". An extensive introduction addresses the language and structure, as well as the origin of Armenian proverbs (international, borrowed and specifically Armenian proverbs). Mineke Schipper is a Dutch scholar, best known for her book "Never Marry a Woman with Big Feet - Women in Proverbs from Around the World". Edward Zellem is an American proverb scholar who has edited books of Afghan proverbs, developed a method of collecting proverbs via the Web.



Websites related to the study of proverbs, and some that list regional proverbs:


</doc>
<doc id="23531" url="https://en.wikipedia.org/wiki?curid=23531" title="Portability (social security)">
Portability (social security)

The portability of social security benefits is the ability of workers to preserve, maintain, and transfer acquired social security rights and social security rights in the process of being acquired from one private, occupational, or public social security scheme to another. Social security rights refer to rights stemming from pension schemes (old age, survivor, disability), unemployment insurance, health insurance, workers' compensation, and sickness benefits. 

Hence, if social security benefits are portable, contributors to, for example, old-age pension schemes do not experience any disadvantage such as the loss of contributions and benefits associated with these contributions when moving from one job to another, from one occupation to another, or from the public to the private sector or vice versa.

International portability of social security rights allows international migrants, who have contributed to a social security scheme for some time in a particular country, to maintain acquired benefits or benefits in the process of being acquired when moving to another country. International portability of social security benefits is therefore understood as the migrant's ability to preserve, maintain, and transfer acquired social security rights independent of nationality and country of residence.

International portability of social security benefits is achieved through bilateral or multilateral social security agreements between countries. These agreements guarantee the totalization of periods of contribution to the social security systems of both countries and the extraterritorial payment of benefits. Currently it is estimated that approximately 23 per cent of migrants worldwide are covered by bilateral social security agreements.


</doc>
<doc id="23534" url="https://en.wikipedia.org/wiki?curid=23534" title="Percopsiformes">
Percopsiformes

The Percopsiformes are a small order of ray-finned fishes, comprising the trout-perch and its allies. It contains just ten extant species, grouped into seven genera and three families. Five of these genera are monotypic

They are generally small fish, ranging from in adult body length. They inhabit freshwater habitats in North America. They are grouped together because of technical characteristics of their internal anatomy, and the different species may appear quite different externally.



</doc>
<doc id="23535" url="https://en.wikipedia.org/wiki?curid=23535" title="Photon">
Photon

The photon is a type of elementary particle. It is the quantum of the electromagnetic field including electromagnetic radiation such as light and radio waves, and the force carrier for the electromagnetic force. Photons are massless, and they always move at the speed of light in vacuum, .

Like all elementary particles, photons are currently best explained by quantum mechanics and exhibit wave–particle duality, their behavior featuring properties of both waves and particles. The modern photon concept originated during the first two decades of the 20th century with the work of Albert Einstein, who built upon the research of Max Planck. While trying to explain how matter and electromagnetic radiation could be in thermal equilibrium with one another, Planck proposed that the energy stored within a material object should be regarded as composed of an integer number of discrete, equal-sized parts. Einstein introduced the idea that light itself is made of discrete units of energy. Experiments validated Einstein's approach, and in 1926, Gilbert N. Lewis popularized the term "photon" for these energy units.

In the Standard Model of particle physics, photons and other elementary particles are described as a necessary consequence of physical laws having a certain symmetry at every point in spacetime. The intrinsic properties of particles, such as charge, mass, and spin, are determined by this gauge symmetry. The photon concept has led to momentous advances in experimental and theoretical physics, including lasers, Bose–Einstein condensation, quantum field theory, and the probabilistic interpretation of quantum mechanics. It has been applied to photochemistry, high-resolution microscopy, and measurements of molecular distances. Recently, photons have been studied as elements of quantum computers, and for applications in optical imaging and optical communication such as quantum cryptography.

The word "quanta" (singular "quantum," Latin for "how much") was used before 1900 to mean particles or amounts of different quantities, including electricity. In 1900, the German physicist Max Planck was studying black-body radiation, and he suggested that the experimental observations, specifically at shorter wavelengths, would be explained if the energy stored within a molecule was a "discrete quantity composed of an integral number of finite equal parts", which he called "energy elements". In 1905, Albert Einstein published a paper in which he proposed that many light-related phenomena—including black-body radiation and the photoelectric effect—would be better explained by modelling electromagnetic waves as consisting of spatially localized, discrete wave-packets. He called such a wave-packet "the light quantum" (German: "das Lichtquant").

The name "photon" derives from the Greek word for light, "" (transliterated "phôs"). Arthur Compton used "photon" in 1928, referring to Gilbert N. Lewis, who coined the term in a letter to "Nature" on December 18, 1926. The same name was used earlier but was never widely adopted before Lewis: in 1916 by the American physicist and psychologist Leonard T. Troland, in 1921 by the Irish physicist John Joly, in 1924 by the French physiologist René Wurmser (1890–1993), and in 1926 by the French physicist Frithiof Wolfers (1891–1971). The name was suggested initially as a unit related to the illumination of the eye and the resulting sensation of light and was used later in a physiological context. Although Wolfers's and Lewis's theories were contradicted by many experiments and never accepted, the new name was adopted very soon by most physicists after Compton used it.

In physics, a photon is usually denoted by the symbol "γ" (the Greek letter gamma). This symbol for the photon probably derives from gamma rays, which were discovered in 1900 by Paul Villard, named by Ernest Rutherford in 1903, and shown to be a form of electromagnetic radiation in 1914 by Rutherford and Edward Andrade. In chemistry and optical engineering, photons are usually symbolized by "hν", which is the photon energy, where "h" is Planck constant and the Greek letter "ν" (nu) is the photon's frequency. Much less commonly, the photon can be symbolized by "hf", where its frequency is denoted by "f".

A photon is massless, has no electric charge, and is a stable particle. In vacuum, a photon has two possible polarization states. The photon is the gauge boson for electromagnetism, and therefore all other quantum numbers of the photon (such as lepton number, baryon number, and flavour quantum numbers) are zero. Also, the photon does not obey the Pauli exclusion principle, but instead obeys Bose–Einstein statistics.

Photons are emitted in many natural processes. For example, when a charge is accelerated it emits synchrotron radiation. During a molecular, atomic or nuclear transition to a lower energy level, photons of various energy will be emitted, ranging from radio waves to gamma rays. Photons can also be emitted when a particle and its corresponding antiparticle are annihilated (for example, electron–positron annihilation).

In empty space, the photon moves at "c" (the speed of light) and its energy and momentum are related by , where "p" is the magnitude of the momentum vector p. This derives from the following relativistic relation, with :

The energy and momentum of a photon depend only on its frequency ("formula_2") or inversely, its wavelength ("λ"):

where k is the wave vector (where the wave number ), is the angular frequency, and is the reduced Planck constant.

Since p points in the direction of the photon's propagation, the magnitude of the momentum is

The photon also carries a quantity called spin angular momentum that does not depend on its frequency. Because photons always move at the speed of light, the spin is best expressed in terms of the component measured along its direction of motion, its helicity, which must be ±"ħ". These two possible helicities, called right-handed and left-handed, correspond to the two possible circular polarization states of the photon.

To illustrate the significance of these formulae, the annihilation of a particle with its antiparticle in free space must result in the creation of at least "two" photons for the following reason. In the center of momentum frame, the colliding antiparticles have no net momentum, whereas a single photon always has momentum (since, as we have seen, it is determined by the photon's frequency or wavelength, which cannot be zero). Hence, conservation of momentum (or equivalently, translational invariance) requires that at least two photons are created, with zero net momentum. (However, it is possible if the system interacts with another particle or field for the annihilation to produce one photon, as when a positron annihilates with a bound atomic electron, it is possible for only one photon to be emitted, as the nuclear Coulomb field breaks translational symmetry.) The energy of the two photons, or, equivalently, their frequency, may be determined from conservation of four-momentum.
Seen another way, the photon can be considered as its own antiparticle (thus an "antiphoton" is simply a normal photon). The reverse process, pair production, is the dominant mechanism by which high-energy photons such as gamma rays lose energy while passing through matter. That process is the reverse of "annihilation to one photon" allowed in the electric field of an atomic nucleus.

The classical formulae for the energy and momentum of electromagnetic radiation can be re-expressed in terms of photon events. For example, the pressure of electromagnetic radiation on an object derives from the transfer of photon momentum per unit time and unit area to that object, since pressure is force per unit area and force is the change in momentum per unit time.

Each photon carries two distinct and independent forms of angular momentum of light. The spin angular momentum of light of a particular photon is always either +"ħ" or −"ħ".
The light orbital angular momentum of a particular photon can be any integer "N", including zero.

Current commonly accepted physical theories imply or assume the photon to be strictly massless. If the photon is not a strictly massless particle, it would not move at the exact speed of light, "c", in vacuum. Its speed would be lower and depend on its frequency. Relativity would be unaffected by this; the so-called speed of light, "c", would then not be the actual speed at which light moves, but a constant of nature which is the upper bound on speed that any object could theoretically attain in spacetime. Thus, it would still be the speed of spacetime ripples (gravitational waves and gravitons), but it would not be the speed of photons.

If a photon did have non-zero mass, there would be other effects as well. Coulomb's law would be modified and the electromagnetic field would have an extra physical degree of freedom. These effects yield more sensitive experimental probes of the photon mass than the frequency dependence of the speed of light. If Coulomb's law is not exactly valid, then that would allow the presence of an electric field to exist within a hollow conductor when it is subjected to an external electric field. This provides a means for very-high-precision tests of Coulomb's law. A null result of such an experiment has set a limit of .

Sharper upper limits on the speed of light have been obtained in experiments designed to detect effects caused by the galactic vector potential. Although the galactic vector potential is very large because the galactic magnetic field exists on very great length scales, only the magnetic field would be observable if the photon is massless. In the case that the photon has mass, the mass term "m'A'A" would affect the galactic plasma. The fact that no such effects are seen implies an upper bound on the photon mass of . The galactic vector potential can also be probed directly by measuring the torque exerted on a magnetized ring. Such methods were used to obtain the sharper upper limit of (the equivalent of ) given by the Particle Data Group.

These sharp limits from the non-observation of the effects caused by the galactic vector potential have been shown to be model-dependent. If the photon mass is generated via the Higgs mechanism then the upper limit of from the test of Coulomb's law is valid.

In most theories up to the eighteenth century, light was pictured as being made up of particles. Since particle models cannot easily account for the refraction, diffraction and birefringence of light, wave theories of light were proposed by René Descartes (1637), Robert Hooke (1665), and Christiaan Huygens (1678); however, particle models remained dominant, chiefly due to the influence of Isaac Newton. In the early nineteenth century, Thomas Young and August Fresnel clearly demonstrated the interference and diffraction of light and by 1850 wave models were generally accepted. In 1865, James Clerk Maxwell's prediction that light was an electromagnetic wave—which was confirmed experimentally in 1888 by Heinrich Hertz's detection of radio waves—seemed to be the final blow to particle models of light.
The Maxwell wave theory, however, does not account for "all" properties of light. The Maxwell theory predicts that the energy of a light wave depends only on its intensity, not on its frequency; nevertheless, several independent types of experiments show that the energy imparted by light to atoms depends only on the light's frequency, not on its intensity. For example, some chemical reactions are provoked only by light of frequency higher than a certain threshold; light of frequency lower than the threshold, no matter how intense, does not initiate the reaction. Similarly, electrons can be ejected from a metal plate by shining light of sufficiently high frequency on it (the photoelectric effect); the energy of the ejected electron is related only to the light's frequency, not to its intensity.

At the same time, investigations of black-body radiation carried out over four decades (1860–1900) by various researchers culminated in Max Planck's hypothesis that the energy of "any" system that absorbs or emits electromagnetic radiation of frequency "ν" is an integer multiple of an energy quantum . As shown by Albert Einstein, some form of energy quantization "must" be assumed to account for the thermal equilibrium observed between matter and electromagnetic radiation; for this explanation of the photoelectric effect, Einstein received the 1921 Nobel Prize in physics.

Since the Maxwell theory of light allows for all possible energies of electromagnetic radiation, most physicists assumed initially that the energy quantization resulted from some unknown constraint on the matter that absorbs or emits the radiation. In 1905, Einstein was the first to propose that energy quantization was a property of electromagnetic radiation itself. Although he accepted the validity of Maxwell's theory, Einstein pointed out that many anomalous experiments could be explained if the "energy" of a Maxwellian light wave were localized into point-like quanta that move independently of one another, even if the wave itself is spread continuously over space. In 1909 and 1916, Einstein showed that, if Planck's law of black-body radiation is accepted, the energy quanta must also carry momentum , making them full-fledged particles. This photon momentum was observed experimentally by Arthur Compton, for which he received the Nobel Prize in 1927. The pivotal question was then: how to unify Maxwell's wave theory of light with its experimentally observed particle nature? The answer to this question occupied Albert Einstein for the rest of his life, and was solved in quantum electrodynamics and its successor, the Standard Model (see ' and ', below).

Einstein's 1905 predictions were verified experimentally in several ways in the first two decades of the 20th century, as recounted in Robert Millikan's Nobel lecture. However, before Compton's experiment showed that photons carried momentum proportional to their wave number (1922), most physicists were reluctant to believe that electromagnetic radiation itself might be particulate. (See, for example, the Nobel lectures of Wien, Planck and Millikan.) Instead, there was a widespread belief that energy quantization resulted from some unknown constraint on the matter that absorbed or emitted radiation. Attitudes changed over time. In part, the change can be traced to experiments such as Compton scattering, where it was much more difficult not to ascribe quantization to light itself to explain the observed results.

Even after Compton's experiment, Niels Bohr, Hendrik Kramers and John Slater made one last attempt to preserve the Maxwellian continuous electromagnetic field model of light, the so-called BKS theory. An important feature of the BKS theory is how it treated the conservation of energy and the conservation of momentum. In the BKS theory, energy and momentum are only conserved on the average across many interactions between matter and radiation. However, refined Compton experiments showed that the conservation laws hold for individual interactions. Accordingly, Bohr and his co-workers gave their model "as honorable a funeral as possible". Nevertheless, the failures of the BKS model inspired Werner Heisenberg in his development of matrix mechanics.

A few physicists persisted in developing semiclassical models in which electromagnetic radiation is not quantized, but matter appears to obey the laws of quantum mechanics. Although the evidence from chemical and physical experiments for the existence of photons was overwhelming by the 1970s, this evidence could not be considered as "absolutely" definitive; since it relied on the interaction of light with matter, and a sufficiently complete theory of matter could in principle account for the evidence. Nevertheless, "all" semiclassical theories were refuted definitively in the 1970s and 1980s by photon-correlation experiments. Hence, Einstein's hypothesis that quantization is a property of light itself is considered to be proven.

Photons obey the laws of quantum mechanics, and so their behavior has both wave-like and particle-like aspects. When a photon is detected by a measuring instrument, it is registered as a single, particulate unit. However, the "probability" of detecting a photon is calculated by equations that describe waves. This combination of aspects is known as wave–particle duality. For example, the probability distribution for the location at which a photon might be detected displays clearly wave-like phenomena such as diffraction and interference. A single photon passing through a double-slit experiment lands on the screen with a probability distribution given by its interference pattern determined by Maxwell's equations. However, experiments confirm that the photon is "not" a short pulse of electromagnetic radiation; it does not spread out as it propagates, nor does it divide when it encounters a beam splitter. Rather, the photon seems to be a point-like particle since it is absorbed or emitted "as a whole" by arbitrarily small systems, including systems much smaller than its wavelength, such as an atomic nucleus (≈10 m across) or even the point-like electron.

While many introductory texts treat photons using the mathematical techniques of non-relativistic quantum mechanics, this is in some ways an awkward oversimplification, as photons are by nature intrinsically relativistic. Because photons have zero rest mass, no wave function defined for a photon can have all the properties familiar from wave functions in non-relativistic quantum mechanics. In order to avoid these difficulties, physicists employ the second-quantized theory of photons described below, quantum electrodynamics, in which photons are quantized excitations of electromagnetic modes.

Another difficulty is finding the proper analogue for the uncertainty principle, an idea frequently attributed to Heisenberg, who introduced the concept in analyzing a thought experiment involving an electron and a high-energy photon. However, Heisenberg did not give precise mathematical definitions of what the "uncertainty" in these measurements meant. The precise mathematical statement of the position–momentum uncertainty principle is due to Kennard, Pauli, and Weyl. The uncertainty principle applies to situations where an experimenter has a choice of measuring either one of two "canonically conjugate" quantities, like the position and the momentum of a particle. According to the uncertainty principle, no matter how the particle is prepared, it is not possible to make a precise prediction for both of the two alternative measurements: if the outcome of the position measurement is made more certain, the outcome of the momentum measurement becomes less so, and vice versa. A coherent state minimizes the overall uncertainty as far as quantum mechanics allows. Quantum optics makes use of coherent states for modes of the electromagnetic field. There is a tradeoff, reminiscent of the position–momentum uncertainty relation, between measurements of an electromagnetic wave's amplitude and its phase. This is sometimes informally expressed in terms of the uncertainty in the number of photons present in the electromagnetic wave, formula_6, and the uncertainty in the phase of the wave, formula_7. However, this cannot be an uncertainty relation of the Kennard–Pauli–Weyl type, since unlike position and momentum, the phase formula_8 cannot be represented by a Hermitian operator.

In 1924, Satyendra Nath Bose derived Planck's law of black-body radiation without using any electromagnetism, but rather by using a modification of coarse-grained counting of phase space. Einstein showed that this modification is equivalent to assuming that photons are rigorously identical and that it implied a "mysterious non-local interaction", now understood as the requirement for a symmetric quantum mechanical state. This work led to the concept of coherent states and the development of the laser. In the same papers, Einstein extended Bose's formalism to material particles (bosons) and predicted that they would condense into their lowest quantum state at low enough temperatures; this Bose–Einstein condensation was observed experimentally in 1995. It was later used by Lene Hau to slow, and then completely stop, light in 1999 and 2001.

The modern view on this is that photons are, by virtue of their integer spin, bosons (as opposed to fermions with half-integer spin). By the spin-statistics theorem, all bosons obey Bose–Einstein statistics (whereas all fermions obey Fermi–Dirac statistics).

In 1916, Albert Einstein showed that Planck's radiation law could be derived from a semi-classical, statistical treatment of photons and atoms, which implies a link between the rates at which atoms emit and absorb photons. The condition follows from the assumption that functions of the emission and absorption of radiation by the atoms are independent of each other, and that thermal equilibrium is made by way of the radiation's interaction with the atoms. Consider a cavity in thermal equilibrium with all parts of itself and filled with electromagnetic radiation and that the atoms can emit and absorb that radiation. Thermal equilibrium requires that the energy density formula_9 of photons with frequency formula_2 (which is proportional to their number density) is, on average, constant in time; hence, the rate at which photons of any particular frequency are "emitted" must equal the rate at which they are "absorbed".

Einstein began by postulating simple proportionality relations for the different reaction rates involved. In his model, the rate formula_11 for a system to "absorb" a photon of frequency formula_2 and transition from a lower energy formula_13 to a higher energy formula_14 is proportional to the number formula_15 of atoms with energy formula_13 and to the energy density formula_9 of ambient photons of that frequency,

where formula_19 is the rate constant for absorption. For the reverse process, there are two possibilities: spontaneous emission of a photon, or the emission of a photon initiated by the interaction of the atom with a passing photon and the return of the atom to the lower-energy state. Following Einstein's approach, the corresponding rate formula_20 for the emission of photons of frequency formula_2 and transition from a higher energy formula_14 to a lower energy formula_13 is

where formula_25 is the rate constant for emitting a photon spontaneously, and formula_26 is the rate constant for emissions in response to ambient photons (induced or stimulated emission). In thermodynamic equilibrium, the number of atoms in state formula_27 and those in state formula_28 must, on average, be constant; hence, the rates formula_11 and formula_20 must be equal. Also, by arguments analogous to the derivation of Boltzmann statistics, the ratio of formula_31 and formula_15 is formula_33 where formula_34 and formula_35 are the degeneracy of the state formula_27 and that of formula_28, respectively, formula_38 and formula_39 their energies, formula_40 the Boltzmann constant and formula_41 the system's temperature. From this, it is readily derived that
formula_42 and
The formula_25 and formula_26 are collectively known as the "Einstein coefficients".

Einstein could not fully justify his rate equations, but claimed that it should be possible to calculate the coefficients formula_25, formula_19 and formula_26 once physicists had obtained "mechanics and electrodynamics modified to accommodate the quantum hypothesis". Not long thereafter, in 1926, Paul Dirac derived the formula_26 rate constants by using a semiclassical approach, and, in 1927, succeeded in deriving "all" the rate constants from first principles within the framework of quantum theory. Dirac's work was the foundation of quantum electrodynamics, i.e., the quantization of the electromagnetic field itself. Dirac's approach is also called "second quantization" or quantum field theory; earlier quantum mechanical treatments only treat material particles as quantum mechanical, not the electromagnetic field.

Einstein was troubled by the fact that his theory seemed incomplete, since it did not determine the "direction" of a spontaneously emitted photon. A probabilistic nature of light-particle motion was first considered by Newton in his treatment of birefringence and, more generally, of the splitting of light beams at interfaces into a transmitted beam and a reflected beam. Newton hypothesized that hidden variables in the light particle determined which of the two paths a single photon would take. Similarly, Einstein hoped for a more complete theory that would leave nothing to chance, beginning his separation from quantum mechanics. Ironically, Max Born's probabilistic interpretation of the wave function was inspired by Einstein's later work searching for a more complete theory.

In 1910, Peter Debye derived Planck's law of black-body radiation from a relatively simple assumption. He decomposed the electromagnetic field in a cavity into its Fourier modes, and assumed that the energy in any mode was an integer multiple of formula_50, where formula_2 is the frequency of the electromagnetic mode. Planck's law of black-body radiation follows immediately as a geometric sum. However, Debye's approach failed to give the correct formula for the energy fluctuations of black-body radiation, which were derived by Einstein in 1909.

In 1925, Born, Heisenberg and Jordan reinterpreted Debye's concept in a key way. As may be shown classically, the Fourier modes of the electromagnetic field—a complete set of electromagnetic plane waves indexed by their wave vector k and polarization state—are equivalent to a set of uncoupled simple harmonic oscillators. Treated quantum mechanically, the energy levels of such oscillators are known to be formula_52, where formula_2 is the oscillator frequency. The key new step was to identify an electromagnetic mode with energy formula_52 as a state with formula_55 photons, each of energy formula_50. This approach gives the correct energy fluctuation formula.
Dirac took this one step further. He treated the interaction between a charge and an electromagnetic field as a small perturbation that induces transitions in the photon states, changing the numbers of photons in the modes, while conserving energy and momentum overall. Dirac was able to derive Einstein's formula_25 and formula_26 coefficients from first principles, and showed that the Bose–Einstein statistics of photons is a natural consequence of quantizing the electromagnetic field correctly (Bose's reasoning went in the opposite direction; he derived Planck's law of black-body radiation by "assuming" B–E statistics). In Dirac's time, it was not yet known that all bosons, including photons, must obey Bose–Einstein statistics.

Dirac's second-order perturbation theory can involve virtual photons, transient intermediate states of the electromagnetic field; the static electric and magnetic interactions are mediated by such virtual photons. In such quantum field theories, the probability amplitude of observable events is calculated by summing over "all" possible intermediate steps, even ones that are unphysical; hence, virtual photons are not constrained to satisfy formula_59, and may have extra polarization states; depending on the gauge used, virtual photons may have three or four polarization states, instead of the two states of real photons. Although these transient virtual photons can never be observed, they contribute measurably to the probabilities of observable events. Indeed, such second-order and higher-order perturbation calculations can give apparently infinite contributions to the sum. Such unphysical results are corrected for using the technique of renormalization.

Other virtual particles may contribute to the summation as well; for example, two photons may interact indirectly through virtual electron–positron pairs. Such photon–photon scattering (see two-photon physics), as well as electron–photon scattering, is meant to be one of the modes of operations of the planned particle accelerator, the International Linear Collider.

In modern physics notation, the quantum state of the electromagnetic field is written as a Fock state, a tensor product of the states for each electromagnetic mode

where formula_61 represents the state in which formula_62 photons are in the mode formula_63. In this notation, the creation of a new photon in mode formula_63 (e.g., emitted from an atomic transition) is written as formula_65. This notation merely expresses the concept of Born, Heisenberg and Jordan described above, and does not add any physics.

The electromagnetic field can be understood as a gauge field, i.e., as a field that results from requiring that a gauge symmetry holds independently at every position in spacetime. For the electromagnetic field, this gauge symmetry is the Abelian U(1) symmetry of complex numbers of absolute value 1, which reflects the ability to vary the phase of a complex field without affecting observables or real valued functions made from it, such as the energy or the Lagrangian.

The quanta of an Abelian gauge field must be massless, uncharged bosons, as long as the symmetry is not broken; hence, the photon is predicted to be massless, and to have zero electric charge and integer spin. The particular form of the electromagnetic interaction specifies that the photon must have spin ±1; thus, its helicity must be formula_66. These two spin components correspond to the classical concepts of right-handed and left-handed circularly polarized light. However, the transient virtual photons of quantum electrodynamics may also adopt unphysical polarization states.

In the prevailing Standard Model of physics, the photon is one of four gauge bosons in the electroweak interaction; the other three are denoted W, W and Z and are responsible for the weak interaction. Unlike the photon, these gauge bosons have mass, owing to a mechanism that breaks their SU(2) gauge symmetry. The unification of the photon with W and Z gauge bosons in the electroweak interaction was accomplished by Sheldon Glashow, Abdus Salam and Steven Weinberg, for which they were awarded the 1979 Nobel Prize in physics. Physicists continue to hypothesize grand unified theories that connect these four gauge bosons with the eight gluon gauge bosons of quantum chromodynamics; however, key predictions of these theories, such as proton decay, have not been observed experimentally.

Measurements of the interaction between energetic photons and hadrons show that the interaction is much more intense than expected by the interaction of merely photons with the hadron's electric charge. Furthermore, the interaction of energetic photons with protons is similar to the interaction of photons with neutrons in spite of the fact that the electric charge structures of protons and neutrons are substantially different. A theory called Vector Meson Dominance (VMD) was developed to explain this effect. According to VMD, the photon is a superposition of the pure electromagnetic photon which interacts only with electric charges and vector mesons. However, if experimentally probed at very short distances, the intrinsic structure of the photon is recognized as a flux of quark and gluon components, quasi-free according to asymptotic freedom in QCD and described by the photon structure function. A comprehensive comparison of data with theoretical predictions was presented in a review in 2000.

The energy of a system that emits a photon is "decreased" by the energy formula_67 of the photon as measured in the rest frame of the emitting system, which may result in a reduction in mass in the amount formula_68. Similarly, the mass of a system that absorbs a photon is "increased" by a corresponding amount. As an application, the energy balance of nuclear reactions involving photons is commonly written in terms of the masses of the nuclei involved, and terms of the form formula_68 for the gamma photons (and for other relevant energies, such as the recoil energy of nuclei).

This concept is applied in key predictions of quantum electrodynamics (QED, see above). In that theory, the mass of electrons (or, more generally, leptons) is modified by including the mass contributions of virtual photons, in a technique known as renormalization. Such "radiative corrections" contribute to a number of predictions of QED, such as the magnetic dipole moment of leptons, the Lamb shift, and the hyperfine structure of bound lepton pairs, such as muonium and positronium.

Since photons contribute to the stress–energy tensor, they exert a gravitational attraction on other objects, according to the theory of general relativity. Conversely, photons are themselves affected by gravity; their normally straight trajectories may be bent by warped spacetime, as in gravitational lensing, and their frequencies may be lowered by moving to a higher gravitational potential, as in the Pound–Rebka experiment. However, these effects are not specific to photons; exactly the same effects would be predicted for classical electromagnetic waves.

Light that travels through transparent matter does so at a lower speed than "c", the speed of light in a vacuum. The factor by which the speed is decreased is called the refractive index of the material. In a classical wave picture, the slowing can be explained by the light inducing electric polarization in the matter, the polarized matter radiating new light, and that new light interfering with the original light wave to form a delayed wave. In a particle picture, the slowing can instead be described as a blending of the photon with quantum excitations of the matter to produce quasi-particles known as polariton (see this list for some other quasi-particles); this polariton has a nonzero effective mass, which means that it cannot travel at "c". Light of different frequencies may travel through matter at different speeds; this is called dispersion (not to be confused with scattering). In some cases, it can result in extremely slow speeds of light in matter. The effects of photon interactions with other quasi-particles may be observed directly in Raman scattering and Brillouin scattering.

Photons can be scattered by matter. For example, photons engage in so many collisions on the way from the core of the Sun that radiant energy can take about a million years to reach the surface; however, once in open space, a photon takes only 8.3 minutes to reach Earth.

Photons can also be absorbed by nuclei, atoms or molecules, provoking transitions between their energy levels. A classic example is the molecular transition of retinal (CHO), which is responsible for vision, as discovered in 1958 by Nobel laureate biochemist George Wald and co-workers. The absorption provokes a cis–trans isomerization that, in combination with other such transitions, is transduced into nerve impulses. The absorption of photons can even break chemical bonds, as in the photodissociation of chlorine; this is the subject of photochemistry.

Photons have many applications in technology. These examples are chosen to illustrate applications of photons "per se", rather than general optical devices such as lenses, etc. that could operate under a classical theory of light. The laser is an extremely important application and is discussed above under stimulated emission.

Individual photons can be detected by several methods. The classic photomultiplier tube exploits the photoelectric effect: a photon of sufficient energy strikes a metal plate and knocks free an electron, initiating an ever-amplifying avalanche of electrons. Semiconductor charge-coupled device chips use a similar effect: an incident photon generates a charge on a microscopic capacitor that can be detected. Other detectors such as Geiger counters use the ability of photons to ionize gas molecules contained in the device, causing a detectable change of conductivity of the gas.

Planck's energy formula formula_70 is often used by engineers and chemists in design, both to compute the change in energy resulting from a photon absorption and to determine the frequency of the light emitted from a given photon emission. For example, the emission spectrum of a gas-discharge lamp can be altered by filling it with (mixtures of) gases with different electronic energy level configurations.

Under some conditions, an energy transition can be excited by "two" photons that individually would be insufficient. This allows for higher resolution microscopy, because the sample absorbs energy only in the spectrum where two beams of different colors overlap significantly, which can be made much smaller than the excitation volume of a single beam (see two-photon excitation microscopy). Moreover, these photons cause less damage to the sample, since they are of lower energy.

In some cases, two energy transitions can be coupled so that, as one system absorbs a photon, another nearby system "steals" its energy and re-emits a photon of a different frequency. This is the basis of fluorescence resonance energy transfer, a technique that is used in molecular biology to study the interaction of suitable proteins.

Several different kinds of hardware random number generators involve the detection of single photons. In one example, for each bit in the random sequence that is to be produced, a photon is sent to a beam-splitter. In such a situation, there are two possible outcomes of equal probability. The actual outcome is used to determine whether the next bit in the sequence is "0" or "1".

Much research has been devoted to applications of photons in the field of quantum optics. Photons seem well-suited to be elements of an extremely fast quantum computer, and the quantum entanglement of photons is a focus of research. Nonlinear optical processes are another active research area, with topics such as two-photon absorption, self-phase modulation, modulational instability and optical parametric oscillators. However, such processes generally do not require the assumption of photons "per se"; they may often be modeled by treating atoms as nonlinear oscillators. The nonlinear process of spontaneous parametric down conversion is often used to produce single-photon states. Finally, photons are essential in some aspects of optical communication, especially for quantum cryptography.

Two-photon physics studies interactions between photons, which are rare. In 2018, MIT researchers announced the discovery of bound photon triplets, which may involve polaritons.

By date of publication:
Education with single photons:



</doc>
<doc id="23537" url="https://en.wikipedia.org/wiki?curid=23537" title="Philipp Franz von Siebold">
Philipp Franz von Siebold

Philipp Franz Balthasar von Siebold (17 February 1796 – 18 October 1866) was a German physician, botanist, and traveler. He achieved prominence by his studies of Japanese flora and fauna and the introduction of Western medicine in Japan. He was the father of the first female Japanese doctor, Kusumoto Ine.

Born into a family of doctors and professors of medicine in Würzburg (then in the Bishopric of Würzburg, later part of Bavaria), Siebold initially studied medicine at University of Würzburg from November 1815, where he became a member of the Corps Moenania Würzburg. One of his professors was Franz Xaver Heller (1775–1840), author of the " ("Flora of the Grand Duchy of Würzburg", 1810–1811). Ignaz Döllinger (1770–1841), his professor of anatomy and physiology, however, most influenced him. Döllinger was one of the first professors to understand and treat medicine as a natural science. Siebold stayed with Döllinger, where he came in regular contact with other scientists. He read the books of Humboldt, a famous naturalist and explorer, which probably raised his desire to travel to distant lands. Philipp Franz von Siebold became a physician by earning his M.D. degree in 1820. He initially practiced medicine in Heidingsfeld, in the Kingdom of Bavaria, now part of Würzburg.

Invited to Holland by an acquaintance of his family, Siebold applied for a position as a military physician, which would enable him to travel to the Dutch colonies. He entered the Dutch military service on June 19, 1822, and was appointed as ship's surgeon on the frigate "Adriana", sailing from Rotterdam to Batavia (present-day Jakarta) in the Dutch East Indies (now called Indonesia). On his trip to Batavia on the frigate "Adriana", Siebold practiced his knowledge of the Dutch language and also rapidly learned Malay, and during the long voyage he began a collection of marine fauna. He arrived in Batavia on February 18, 1823.

As an army medical officer, Siebold was posted to an artillery unit. However, he was given a room for a few weeks at the residence of the Governor-General of the Dutch East Indies, Baron Godert van der Capellen, to recover from an illness. With his erudition, he impressed the Governor-General, and also the director of the botanical garden at Buitenzorg (now Bogor), Caspar Georg Carl Reinwardt. These men sensed in Siebold a worthy successor to Engelbert Kaempfer and Carl Peter Thunberg, two former resident physicians at Dejima, a Dutch trading post in Japan, the latter of whom was the author of "". The Batavian Academy of Arts and Sciences soon elected Siebold as a member.

On 28 June 1823, after only a few months in the Dutch East Indies, Siebold was posted as resident physician and scientist to Dejima, a small artificial island and trading post at Nagasaki, and arrived there on 11 August 1823. During an eventful voyage to Japan he only just escaped drowning during a typhoon in the East China Sea. As only a very small number of Dutch personnel were allowed to live on this island, the posts of physician and scientist had to be combined. Dejima had been in the possession of the Dutch East India Company (known as the VOC) since the 17th century, but the Company had gone bankrupt in 1798, after which a trading post was operated there by the Dutch state for political considerations, with notable benefits to the Japanese.

The European tradition of sending doctors with botanical training to Japan was a long one. Sent on a mission by the Dutch East India Company, Engelbert Kaempfer (1651–1716), a German physician and botanist who lived in Japan from 1690 until 1692, ushered in this tradition of a combination of physician and botanist. The Dutch East India Company did not, however, actually employ the Swedish botanist and physician Carl Peter Thunberg (1743–1828), who had arrived in Japan in 1775.

Japanese scientists invited Siebold to show them the marvels of western science, and he learned in return through them much about the Japanese and their customs. After curing an influential local officer, Siebold gained the permission to leave the trade post. He used this opportunity to treat Japanese patients in the greater area around the trade post. Siebold is credited with the introduction of vaccination and pathological anatomy for the first time in Japan.

In 1824, Siebold started a medical school in Nagasaki, the "Narutaki-juku", that grew into a meeting place for around fifty "students". They helped him in his botanical and naturalistic studies. The Dutch language became the "lingua franca" (common spoken language) for these academic and scholarly contacts for a generation, until the Meiji Restoration.

His patients paid him in kind with a variety of objects and artifacts that would later gain historical significance. These everyday objects later became the basis of his large ethnographic collection, which consisted of everyday household goods, woodblock prints, tools and hand-crafted objects used by the Japanese people.

During his stay in Japan, Siebold "lived together" with Kusumoto Taki (楠本滝), who gave birth to their daughter Kusumoto (O-)Ine in 1827. Siebold used to call his wife "Otakusa" (probably derived from O-Taki-san) and named a "Hydrangea" after her. Kusumoto Ine eventually became the first Japanese woman known to have received a physician's training and became a highly regarded practicing physician and court physician to the Empress in 1882. She died at court in 1903.

His main interest, however, focused on the study of Japanese fauna and flora. He collected as much material as he could. Starting a small botanical garden behind his home (there was not much room on the small island) Siebold amassed over 1,000 native plants. In a specially built glasshouse he cultivated the Japanese plants to endure the Dutch climate. Local Japanese artists like Kawahara Keiga drew and painted images of these plants, creating botanical illustrations but also images of the daily life in Japan, which complemented his ethnographic collection. He hired Japanese hunters to track rare animals and collect specimens. Many specimens were collected with the help of his Japanese collaborators Keisuke Ito (1803–1901), Mizutani Sugeroku (1779–1833), Ōkochi Zonshin (1796–1882) and Katsuragawa Hoken (1797–1844), a physician to the "shōgun". As well, Siebold's assistant and later successor, Heinrich Bürger (1806–1858), proved to be indispensable in carrying on Siebold's work in Japan.

Siebold first introduced to Europe such familiar garden-plants as the "Hosta" and the "Hydrangea otaksa". Unknown to the Japanese, he was also able to smuggle out germinative seeds of tea plants to the botanical garden "" in Batavia. Through this single act, he started the tea culture in Java, a Dutch colony at the time. Until then Japan had strictly guarded the trade in tea plants. Remarkably, in 1833, Java already could boast a half million tea plants.

He also introduced Japanese knotweed ("Reynoutria japonica", syn. "Fallopia japonica"), which has become a highly invasive weed in Europe and North America. All derive from a single female plant collected by Siebold.

During his stay at Dejima, Siebold sent three shipments with an unknown number of herbarium specimens to Leiden, Ghent, Brussels and Antwerp. The shipment to Leiden contained the first specimens of the Japanese giant salamander ("Andrias japonicus") to be sent to Europe.

In 1825 the government of the Dutch-Indies provided him with two assistants: apothecary and mineralogist Heinrich Bürger (his later successor) and the painter Carl Hubert de Villeneuve. Each would prove to be useful to Siebold's efforts that ranged from ethnographical to botanical to horticultural, when attempting to document the exotic Eastern Japanese experience. De Villeneuve taught Kawahara the techniques of Western painting.

Reportedly, Siebold was not the easiest man to deal with. He was in continuous conflict with his Dutch superiors who felt he was arrogant. This threat of conflict resulted in his recall in July 1827 back to Batavia. But the ship, the "Cornelis Houtman", sent to carry him back to Batavia, was thrown ashore by a typhoon in Nagasaki bay. The same storm badly damaged Dejima and destroyed Siebold's botanical garden. Repaired, the "Cornelis Houtman" was refloated. It left for Batavia with 89 crates of Siebold's salvaged botanical collection, but Siebold himself remained behind in Dejima.

In 1826 Siebold made the court journey to Edo. During this long trip he collected many plants and animals. But he also obtained from the court astronomer Takahashi Kageyasu several detailed maps of Japan and Korea (written by Inō Tadataka), an act strictly forbidden by the Japanese government. When the Japanese discovered, by accident, that Siebold had a map of the northern parts of Japan, the government accused him of high treason and of being a spy for Russia.

The Japanese placed Siebold under house arrest and expelled him from Japan on 22 October 1829. Satisfied that his Japanese collaborators would continue his work, he journeyed back on the frigate "Java" to his former residence, Batavia, in possession of his enormous collection of thousands of animals and plants, his books and his maps. The botanical garden of "" would soon house Siebold's surviving, living flora collection of 2,000 plants. He arrived in the Netherlands on 7 July 1830. His stay in Japan and Batavia had lasted for a period of eight years.

Philipp Franz von Siebold arrived in the Netherlands in 1830, just at a time when political troubles erupted in Brussels, leading soon to Belgian independence. Hastily he salvaged his ethnographic collections in Antwerp and his herbarium specimens in Brussels and took them to Leiden, helped by Johann Baptist Fischer. He left behind his botanical collections of living plants that were sent to the University of Ghent. The consequent expansion of this collection of rare and exotic plants led to the horticultural fame of Ghent. In gratitude the University of Ghent presented him in 1841 with specimens of every plant from his original collection.

Siebold settled in Leiden, taking with him the major part of his collection. The "Philipp Franz von Siebold collection", containing many type specimens, was the earliest botanical collection from Japan. Even today, it still remains a subject of ongoing research, a testimony to the depth of work undertaken by Siebold. It contained about 12,000 specimens, from which he could describe only about 2,300 species. The whole collection was purchased for a handsome amount by the Dutch government. Siebold was also granted a substantial annual allowance by the Dutch King William II and was appointed "Advisor to the King for Japanese Affairs". In 1842, the King even raised Siebold to the nobility as an esquire.

The "Siebold collection" opened to the public in 1831. He founded a museum in his home in 1837. This small, private museum would eventually evolve into the National Museum of Ethnology in Leiden. Siebold's successor in Japan, Heinrich Bürger, sent Siebold three more shipments of herbarium specimens collected in Japan. This flora collection formed the basis of the Japanese collections of the National Herbarium of the Netherlands in Leiden, while the zoological specimens Siebold collected were kept by the Rijksmuseum van Natuurlijke Historie ("National Museum of Natural History") in Leiden, which later became Naturalis. Both institutions merged into Naturalis Biodiversity Center in 2010, which now maintains the entire natural history collection that Siebold brought back to Leiden.

In 1845 Siebold married Helene von Gagern (1820–1877), they had three sons and two daughters.

During his stay in Leiden, Siebold wrote "Nippon" in 1832, the first part of a volume of a richly illustrated ethnographical and geographical work on Japan. The 'Archiv zur Beschreibung Nippons' also contained a report of his journey to the Shogunate Court at Edo. He wrote six further parts, the last ones published posthumously in 1882; his sons published an edited and lower-priced reprint in 1887.

The " appeared between 1833 and 1841. This work was co-authored by Joseph Hoffmann and Kuo Cheng-Chang, a Javanese of Chinese extraction, who had journeyed along with Siebold from Batavia. It contained a survey of Japanese literature and a Chinese, Japanese and Korean dictionary. Siebold's writing on Japanese religion and customs notably shaped early modern European conceptions of Buddhism and Shinto; he notably suggested that Japanese Buddhism was a form of Monotheism.

The zoologists Coenraad Temminck (1777–1858), Hermann Schlegel (1804–1884), and Wilhem de Haan (1801–1855) scientifically described and documented Siebold's collection of Japanese animals. The ', a series of monographs published between 1833 and 1850, was mainly based on Siebold's collection, making the Japanese fauna the best-described non-European fauna – "a remarkable feat". A significant part of the ' was also based on the collections of Siebold's successor on Dejima, Heinrich Bürger.

Siebold wrote his "" in collaboration with the German botanist Joseph Gerhard Zuccarini (1797–1848). It first appeared in 1835, but the work was not completed until after his death, finished in 1870 by F.A.W. Miquel (1811–1871), director of the Rijksherbarium in Leiden. This work expanded Siebold's scientific fame from Japan to Europe.

From the Hortus Botanicus Leiden – the botanical garden of Leiden – many of Siebold's plants spread to Europe and from there to other countries. "Hosta" and "Hortensia", "Azalea", and the Japanese butterbur and the coltsfoot as well as the Japanese larch began to inhabit gardens across the world.

After his return to Europe, Siebold tried to exploit his knowledge of Japan. Whilst living in Boppard, from 1852 he corresponded with Russian diplomats such as Baron von Budberg-Bönninghausen, the Russian ambassador to Prussia, which resulted in an invitation to go to St Petersburg to advise the Russian government how to open trade relations with Japan. Though still employed by the Dutch government he did not inform the Dutch of this voyage until after his return.

American Naval Commodore Matthew C. Perry consulted Siebold in advance of his voyage to Japan in 1854. He notably advised Townsend Harris on how Christianity might be spread to Japan, alleging based on his time there that the Japanese "hated" Christianity.

In 1858, the Japanese government lifted the banishment of Siebold. He returned to Japan in 1859 as an adviser to the Agent of the Dutch Trading Society (Nederlandsche Handel-Maatschappij) in Nagasaki, Albert Bauduin. After two years the connection with the Trading Society was severed as the advice of Siebold was considered to be of no value. In Nagasaki he fathered another child with one of his female servants.

In 1861 Siebold organised his appointment as an adviser to the Japanese government and went in that function to Edo. There he tried to obtain a position between the foreign representatives and the Japanese government. As he had been specially admonished by the Dutch authorities before going to Japan that he was to abstain from all interference in politics, the Dutch Consul General in Japan, J.K. de Wit, was ordered to ask Siebold's removal. Siebold was ordered to return to Batavia and from there he returned to Europe.

After his return he asked the Dutch government to employ him as Consul General in Japan but the Dutch government severed all relations with Siebold who had a huge debt because of loans given to him, except for the payment of his pension.

Siebold kept trying to organise another voyage to Japan. After he did not succeed in gaining employment with the Russian government, he went to Paris in 1865 to try to interest the French government in funding another expedition to Japan, but failed. He died in Munich on 18 October 1866.

The botanical and horticultural spheres of influence have honored Philipp Franz von Siebold by naming some of the very garden-worthy plants that he studied after him. Examples include:



Though he is well known in Japan, where he is called "Shiboruto-san", and although mentioned in the relevant schoolbooks, Siebold is almost unknown elsewhere, except among gardeners who admire the many plants whose names incorporate "sieboldii" and "sieboldiana". The Hortus Botanicus in Leiden has recently laid out the "Von Siebold Memorial Garden", a Japanese garden with plants sent by Siebold. The garden was laid out under a 150-year-old "Zelkova serrata" tree dating from Siebold's lifetime. Japanese visitors come and visit this garden, to pay their respect for him.

Although he was disillusioned by what he perceived as a lack of appreciation for Japan and his contributions to its understanding, a testimony of the remarkable character of Siebold is found in museums that honor him.

His collections laid the foundation for the ethnographic museums of Munich and Leiden. Alexander von Siebold, his son by his European wife, donated much of the material left behind after Siebold's death in Würzburg to the British Museum in London. The Royal Scientific Academy of St. Petersburg purchased 600 colored plates of the "".

Another son, Heinrich (or Henry) von Siebold (1852–1908), continued part of his father's research. He is recognized, together with Edward S. Morse, as one of the founders of modern archaeological efforts in Japan.


The standard author abbreviation Siebold is used to indicate Philipp Franz von Siebold as the author when citing a botanical name.




</doc>
<doc id="23538" url="https://en.wikipedia.org/wiki?curid=23538" title="Probability interpretations">
Probability interpretations

The word probability has been used in a variety of ways since it was first applied to the mathematical study of games of chance. Does probability measure the real, physical tendency of something to occur or is it a measure of how strongly one believes it will occur, or does it draw on both these elements? In answering such questions, mathematicians interpret the probability values of probability theory.

There are two broad categories of probability interpretations which can be called "physical" and "evidential" probabilities. Physical probabilities, which are also called objective or frequency probabilities, are associated with random physical systems such as roulette wheels, rolling dice and radioactive atoms. In such systems, a given type of event (such as a die yielding a six) tends to occur at a persistent rate, or "relative frequency", in a long run of trials. Physical probabilities either explain, or are invoked to explain, these stable frequencies. The two main kinds of theory of physical probability are frequentist accounts (such as those of Venn, Reichenbach and von Mises) and propensity accounts (such as those of Popper, Miller, Giere and Fetzer).

Evidential probability, also called Bayesian probability, can be assigned to any statement whatsoever, even when no random process is involved, as a way to represent its subjective plausibility, or the degree to which the statement is supported by the available evidence. On most accounts, evidential probabilities are considered to be degrees of belief, defined in terms of dispositions to gamble at certain odds. The four main evidential interpretations are the classical (e.g. Laplace's) interpretation, the subjective interpretation (de Finetti and Savage), the epistemic or inductive interpretation (Ramsey, Cox) and the logical interpretation (Keynes and Carnap). There are also evidential interpretations of probability covering groups, which are often labelled as 'intersubjective' (proposed by Gillies and Rowbottom).

Some interpretations of probability are associated with approaches to statistical inference, including theories of estimation and hypothesis testing. The physical interpretation, for example, is taken by followers of "frequentist" statistical methods, such as Ronald Fisher, Jerzy Neyman and Egon Pearson. Statisticians of the opposing Bayesian school typically accept the existence and importance of physical probabilities, but also consider the calculation of evidential probabilities to be both valid and necessary in statistics. This article, however, focuses on the interpretations of probability rather than theories of statistical inference.

The terminology of this topic is rather confusing, in part because probabilities are studied within a variety of academic fields. The word "frequentist" is especially tricky. To philosophers it refers to a particular theory of physical probability, one that has more or less been abandoned. To scientists, on the other hand, "frequentist probability" is just another name for physical (or objective) probability. Those who promote Bayesian inference view "frequentist statistics" as an approach to statistical inference that recognises only physical probabilities. Also the word "objective", as applied to probability, sometimes means exactly what "physical" means here, but is also used of evidential probabilities that are fixed by rational constraints, such as logical and epistemic probabilities.

The philosophy of probability presents problems chiefly in matters of epistemology and the uneasy interface between mathematical concepts and ordinary language as it is used by non-mathematicians.
Probability theory is an established field of study in mathematics. It has its origins in correspondence discussing the mathematics of games of chance between Blaise Pascal and Pierre de Fermat in the seventeenth century, and was formalized and rendered axiomatic as a distinct branch of mathematics by Andrey Kolmogorov in the twentieth century. In axiomatic form, mathematical statements about probability theory carry the same sort of epistemological confidence within the philosophy of mathematics as are shared by other mathematical statements.

The mathematical analysis originated in observations of the behaviour of game equipment such as playing cards and dice, which are designed specifically to introduce random and equalized elements; in mathematical terms, they are subjects of indifference. This is not the only way probabilistic statements are used in ordinary human language: when people say that ""it will probably rain"", they typically do not mean that the outcome of rain versus not-rain is a random factor that the odds currently favor; instead, such statements are perhaps better understood as qualifying their expectation of rain with a degree of confidence. Likewise, when it is written that "the most probable explanation" of the name of Ludlow, Massachusetts "is that it was named after Roger Ludlow", what is meant here is not that Roger Ludlow is favored by a random factor, but rather that this is the most plausible explanation of the evidence, which admits other, less likely explanations.

Thomas Bayes attempted to provide a logic that could handle varying degrees of confidence; as such, Bayesian probability is an attempt to recast the representation of probabilistic statements as an expression of the degree of confidence by which the beliefs they express are held.

Though probability initially had somewhat mundane motivations, its modern influence and use is widespread ranging from evidence-based medicine, through six sigma, all the way to the probabilistically checkable proof and the string theory landscape.

The first attempt at mathematical rigour in the field of probability, championed by Pierre-Simon Laplace, is now known as the classical definition. Developed from studies of games of chance (such as rolling dice) it states that probability is shared equally between all the possible outcomes, provided these outcomes can be deemed equally likely. (3.1)

This can be represented mathematically as follows:
If a random experiment can result in "N" mutually exclusive and equally likely outcomes and if N of these outcomes result in the occurrence of the event "A", the probability of "A" is defined by

There are two clear limitations to the classical definition. Firstly, it is applicable only to situations in which there is only a 'finite' number of possible outcomes. But some important random experiments, such as tossing a coin until it rises heads, give rise to an infinite set of outcomes. And secondly, you need to determine in advance that all the possible outcomes are equally likely without relying on the notion of probability to avoid circularity—for instance, by symmetry considerations.

Frequentists posit that the probability of an event is its relative frequency over time, (3.4) i.e., its relative frequency of occurrence after repeating a process a large number of times under similar conditions. This is also known as aleatory probability. The events are assumed to be governed by some random physical phenomena, which are either phenomena that are predictable, in principle, with sufficient information (see determinism); or phenomena which are essentially unpredictable. Examples of the first kind include tossing dice or spinning a roulette wheel; an example of the second kind is radioactive decay. In the case of tossing a fair coin, frequentists say that the probability of getting a heads is 1/2, not because there are two equally likely outcomes but because repeated series of large numbers of trials demonstrate that the empirical frequency converges to the limit 1/2 as the number of trials goes to infinity.

If we denote by formula_2 the number of occurrences of an event formula_3 in formula_4 trials, then if formula_5 we say that "formula_6".

The frequentist view has its own problems. It is of course impossible to actually perform an infinity of repetitions of a random experiment to determine the probability of an event. But if only a finite number of repetitions of the process are performed, different relative frequencies will appear in different series of trials. If these relative frequencies are to define the probability, the probability will be slightly different every time it is measured. But the real probability should be the same every time. If we acknowledge the fact that we only can measure a probability with some error of measurement attached, we still get into problems as the error of measurement can only be expressed as a probability, the very concept we are trying to define. This renders even the frequency definition circular; see for example “What is the Chance of an Earthquake?” 

Subjectivists, also known as Bayesians or followers of epistemic probability, give the notion of probability a subjective status by regarding it as a measure of the 'degree of belief' of the individual assessing the uncertainty of a particular situation. Epistemic or subjective probability is sometimes called credence, as opposed to the term chance for a propensity probability.

Some examples of epistemic probability are to assign a probability to the proposition that a proposed law of physics is true or to determine how probable it is that a suspect committed a crime, based on the evidence presented.

Gambling odds don't reflect the bookies' belief in a likely winner, so much as the other bettors' belief, because the bettors are actually betting against one another. The odds are set based on how many people have bet on a possible winner, so that even if the high odds players always win, the bookies will always make their percentages anyway.

The use of Bayesian probability raises the philosophical debate as to whether it can contribute valid justifications of belief.

Bayesians point to the work of Ramsey (p 182) and de Finetti (p 103) as proving that subjective beliefs must follow the laws of probability if they are to be coherent. Evidence casts doubt that humans will have coherent beliefs.

The use of Bayesian probability involves specifying a prior probability. This may be obtained from consideration of whether the required prior probability is greater or lesser than a reference probability associated with an urn model or a thought experiment. The issue is that for a given problem, multiple thought experiments could apply, and choosing one is a matter of judgement: different people may assign different prior probabilities, known as the reference class problem.
The "sunrise problem" provides an example.

Propensity theorists think of probability as a physical propensity, or disposition, or tendency of a given type of physical situation to yield an outcome of a certain kind or to yield a long run relative frequency of such an outcome. This kind of objective probability is sometimes called 'chance'.

Propensities, or chances, are not relative frequencies, but purported causes of the observed stable relative frequencies. Propensities are invoked to explain why repeating a certain kind of experiment will generate given outcome types at persistent rates, which are known as propensities or chances. Frequentists are unable to take this approach, since relative frequencies do not exist for single tosses of a coin, but only for large ensembles or collectives (see "single case possible" in the table above). In contrast, a propensitist is able to use the law of large numbers to explain the behaviour of long-run frequencies. This law, which is a consequence of the axioms of probability, says that if (for example) a coin is tossed repeatedly many times, in such a way that its probability of landing heads is the same on each toss, and the outcomes are probabilistically independent, then the relative frequency of heads will be close to the probability of heads on each single toss. This law allows that stable long-run frequencies are a manifestation of invariant "single-case" probabilities. In addition to explaining the emergence of stable relative frequencies, the idea of propensity is motivated by the desire to make sense of single-case probability attributions in quantum mechanics, such as the probability of decay of a particular atom at a particular time.

The main challenge facing propensity theories is to say exactly what propensity means. (And then, of course, to show that propensity thus defined has the required properties.) At present, unfortunately, none of the well-recognised accounts of propensity comes close to meeting this challenge.

A propensity theory of probability was given by Charles Sanders Peirce. A later propensity theory was proposed by philosopher Karl Popper, who had only slight acquaintance with the writings of C. S. Peirce, however. Popper noted that the outcome of a physical experiment is produced by a certain set of "generating conditions". When we repeat an experiment, as the saying goes, we really perform another experiment with a (more or less) similar set of generating conditions. To say that a set of generating conditions has propensity "p" of producing the outcome "E" means that those exact conditions, if repeated indefinitely, would produce an outcome sequence in which "E" occurred with limiting relative frequency "p". For Popper then, a deterministic experiment would have propensity 0 or 1 for each outcome, since those generating conditions would have same outcome on each trial. In other words, non-trivial propensities (those that differ from 0 and 1) only exist for genuinely nondeterministic experiments.

A number of other philosophers, including David Miller and Donald A. Gillies, have proposed propensity theories somewhat similar to Popper's.

Other propensity theorists (e.g. Ronald Giere) do not explicitly define propensities at all, but rather see propensity as defined by the theoretical role it plays in science. They argued, for example, that physical magnitudes such as electrical charge cannot be explicitly defined either, in terms of more basic things, but only in terms of what they do (such as attracting and repelling other electrical charges). In a similar way, propensity is whatever fills the various roles that physical probability plays in science.

What roles does physical probability play in science? What are its properties? One central property of chance is that, when known, it constrains rational belief to take the same numerical value. David Lewis called this the "Principal Principle", (3.3 & 3.5) a term that philosophers have mostly adopted. For example, suppose you are certain that a particular biased coin has propensity 0.32 to land heads every time it is tossed. What is then the correct price for a gamble that pays $1 if the coin lands heads, and nothing otherwise? According to the Principal Principle, the fair price is 32 cents.

It is widely recognized that the term "probability" is sometimes used in contexts where it has nothing to do with physical randomness. Consider, for example, the claim that the extinction of the dinosaurs was probably caused by a large meteorite hitting the earth. Statements such as "Hypothesis H is probably true" have been interpreted to mean that the (presently available) empirical evidence (E, say) supports H to a high degree. This degree of support of H by E has been called the logical probability of H given E, or the epistemic probability of H given E, or the inductive probability of H given E.

The differences between these interpretations are rather small, and may seem inconsequential. One of the main points of disagreement lies in the relation between probability and belief. Logical probabilities are conceived (for example in Keynes' Treatise on Probability) to be objective, logical relations between propositions (or sentences), and hence not to depend in any way upon belief. They are degrees of (partial) entailment, or degrees of logical consequence, not degrees of belief. (They do, nevertheless, dictate proper degrees of belief, as is discussed below.) Frank P. Ramsey, on the other hand, was skeptical about the existence of such objective logical relations and argued that (evidential) probability is "the logic of partial belief". (p 157) In other words, Ramsey held that epistemic probabilities simply "are" degrees of rational belief, rather than being logical relations that merely "constrain" degrees of rational belief.

Another point of disagreement concerns the "uniqueness" of evidential probability, relative to a given state of knowledge. Rudolf Carnap held, for example, that logical principles always determine a unique logical probability for any statement, relative to any body of evidence. Ramsey, by contrast, thought that while degrees of belief are subject to some rational constraints (such as, but not limited to, the axioms of probability) these constraints usually do not determine a unique value. Rational people, in other words, may differ somewhat in their degrees of belief, even if they all have the same information.

An alternative account of probability emphasizes the role of "prediction" – predicting future observations on the basis of past observations, not on unobservable parameters. In its modern form, it is mainly in the Bayesian vein. This was the main function of probability before the 20th century, but fell out of favor compared to the parametric approach, which modeled phenomena as a physical system that was observed with error, such as in celestial mechanics.

The modern predictive approach was pioneered by Bruno de Finetti, with the central idea of exchangeability – that future observations should behave like past observations. This view came to the attention of the Anglophone world with the 1974 translation of de Finetti's book, and has
since been propounded by such statisticians as Seymour Geisser.

The mathematics of probability can be developed on an entirely axiomatic basis that is independent of any interpretation: see the articles on probability theory and probability axioms for a detailed treatment.





</doc>
<doc id="23539" url="https://en.wikipedia.org/wiki?curid=23539" title="Probability axioms">
Probability axioms

The Kolmogorov axioms are the foundations of probability theory introduced by Andrey Kolmogorov in 1933. These axioms remain central and have direct contributions to mathematics, the physical sciences, and real-world probability cases. An alternative approach to formalising probability, favoured by some Bayesians, is given by Cox's theorem.

The assumptions as to setting up the axioms can be summarised as follows: Let (Ω, "F", "P") be a measure space with "P" being the probability of some event "E", denoted formula_1"," and formula_2 = 1. Then (Ω, "F", "P") is a probability space, with sample space Ω, event space "F" and probability measure "P".

The probability of an event is a non-negative real number:

where formula_4 is the event space. It follows that formula_1 is always finite, in contrast with more general measure theory. Theories which assign negative probability relax the first axiom.

This is the assumption of unit measure: that the probability that at least one of the elementary events in the entire sample space will occur is 1

This is the assumption of σ-additivity:
Some authors consider merely finitely additive probability spaces, in which case one just needs an algebra of sets, rather than a σ-algebra. Quasiprobability distributions in general relax the third axiom.

From the Kolmogorov axioms, one can deduce other useful rules for studying probabilities. The proofs of these rules are a very insightful procedure that illustrates the power the third axiom, and its interaction with the remaining two axioms. Four of the immediate corollaries and their proofs are shown below:

If A is a subset of, or equal to B, then the probability of A is less than, or equal to the probability of B.

In order to verify the monotonicity property, we set formula_10 and formula_11, where formula_12 and formula_13 for formula_14. It is easy to see that the sets formula_15 are pairwise disjoint and formula_16. Hence, we obtain from the third axiom that

Since, by the first axiom, the left-hand side of this equation is a series of non-negative numbers, and since it converges to formula_18 which is finite, we obtain both formula_19 and formula_20.

In some cases, formula_22 is not the only event with probability 0.

As shown in the previous proof, formula_20. However, this statement is seen by contradiction: if formula_24 then the left hand side formula_25 is not less than infinity; formula_26

If formula_27 then we obtain a contradiction, because the sum does not exceed formula_18 which is finite. Thus, formula_29. We have shown as a byproduct of the proof of monotonicity that formula_20.

formula_31

Given formula_32 and formula_33are mutually exclusive and that formula_34:

formula_35 "... (by axiom 3)"

and, formula_36 ... "(by axiom 2)"

formula_37

formula_38

It immediately follows from the monotonicity property that

Given the complement rule formula_40 and "axiom 1" formula_41:

formula_42

formula_43

formula_44

Another important property is:

This is called the addition law of probability, or the sum rule.
That is, the probability that "A" "or" "B" will happen is the sum of the probabilities that "A" will happen and that "B" will happen, minus the probability that both "A" "and" "B" will happen. The proof of this is as follows:

Firstly,

So,

Also,

and eliminating formula_50 from both equations gives us the desired result.

An extension of the addition law to any number of sets is the inclusion–exclusion principle.

Setting "B" to the complement "A" of "A" in the addition law gives

That is, the probability that any event will "not" happen (or the event's complement) is 1 minus the probability that it will.

Consider a single coin-toss, and assume that the coin will either land heads (H) or tails (T) (but not both). No assumption is made as to whether the coin is fair.

We may define:

Kolmogorov's axioms imply that:

The probability of "neither" heads "nor" tails, is 0.

The probability of "either" heads "or" tails, is 1.

The sum of the probability of heads and the probability of tails, is 1.




</doc>
<doc id="23542" url="https://en.wikipedia.org/wiki?curid=23542" title="Probability theory">
Probability theory

Probability theory is the branch of mathematics concerned with probability. Although there are several different probability interpretations, probability theory treats the concept in a rigorous mathematical manner by expressing it through a set of axioms. Typically these axioms formalise probability in terms of a probability space, which assigns a measure taking values between 0 and 1, termed the probability measure, to a set of outcomes called the sample space. Any specified subset of these outcomes is called an event.

Central subjects in probability theory include discrete and continuous random variables, probability distributions, and stochastic processes, which provide mathematical abstractions of non-deterministic or uncertain processes or measured quantities that may either be single occurrences or evolve over time in a random fashion.

Although it is not possible to perfectly predict random events, much can be said about their behavior. Two major results in probability theory describing such behaviour are the law of large numbers and the central limit theorem.

As a mathematical foundation for statistics, probability theory is essential to many human activities that involve quantitative analysis of data. Methods of probability theory also apply to descriptions of complex systems given only partial knowledge of their state, as in statistical mechanics. A great discovery of twentieth-century physics was the probabilistic nature of physical phenomena at atomic scales, described in quantum mechanics.

The earliest known forms of probability and statistics were developed by Arab mathematicians studying cryptography between the 8th and 13th centuries. Al-Khalil (717–786) wrote the "Book of Cryptographic Messages" which contains the first use of permutations and combinations to list all possible Arabic words with and without vowels. Al-Kindi (801–873) made the earliest known use of statistical inference in his work on cryptanalysis and frequency analysis. An important contribution of Ibn Adlan (1187–1268) was on sample size for use of frequency analysis.

The modern mathematical theory of probability has its roots in attempts to analyze games of chance by Gerolamo Cardano in the sixteenth century, and by Pierre de Fermat and Blaise Pascal in the seventeenth century (for example the "problem of points"). Christiaan Huygens published a book on the subject in 1657 and in the 19th century, Pierre Laplace completed what is today considered the classic interpretation.

Initially, probability theory mainly considered events, and its methods were mainly combinatorial. Eventually, analytical considerations compelled the incorporation of variables into the theory.

This culminated in modern probability theory, on foundations laid by Andrey Nikolaevich Kolmogorov. Kolmogorov combined the notion of sample space, introduced by Richard von Mises, and measure theory and presented his axiom system for probability theory in 1933. This became the mostly undisputed axiomatic basis for modern probability theory; but, alternatives exist, such as the adoption of finite rather than countable additivity by Bruno de Finetti.

Most introductions to probability theory treat discrete probability distributions and continuous probability distributions separately. The measure theory-based treatment of probability covers the discrete, continuous, a mix of the two, and more.

Consider an experiment that can produce a number of outcomes. The set of all outcomes is called the "sample space" of the experiment. The "power set" of the sample space (or equivalently, the event space) is formed by considering all different collections of possible results. For example, rolling an honest die produces one of six possible results. One collection of possible results corresponds to getting an odd number. Thus, the subset {1,3,5} is an element of the power set of the sample space of die rolls. These collections are called "events". In this case, {1,3,5} is the event that the die falls on some odd number. If the results that actually occur fall in a given event, that event is said to have occurred.

Probability is a way of assigning every "event" a value between zero and one, with the requirement that the event made up of all possible results (in our example, the event {1,2,3,4,5,6}) be assigned a value of one. To qualify as a probability distribution, the assignment of values must satisfy the requirement that if you look at a collection of mutually exclusive events (events that contain no common results, e.g., the events {1,6}, {3}, and {2,4} are all mutually exclusive), the probability that any of these events occurs is given by the sum of the probabilities of the events.

The probability that any one of the events {1,6}, {3}, or {2,4} will occur is 5/6. This is the same as saying that the probability of event {1,2,3,4,6} is 5/6. This event encompasses the possibility of any number except five being rolled. The mutually exclusive event {5} has a probability of 1/6, and the event {1,2,3,4,5,6} has a probability of 1, that is, absolute certainty.

When doing calculations using the outcomes of an experiment, it is necessary that all those elementary events have a number assigned to them. This is done using a random variable. A random variable is a function that assigns to each elementary event in the sample space a real number. This function is usually denoted by a capital letter. In the case of a die, the assignment of a number to a certain elementary events can be done using the identity function. This does not always work. For example, when flipping a coin the two possible outcomes are "heads" and "tails". In this example, the random variable "X" could assign to the outcome "heads" the number "0" (formula_1) and to the outcome "tails" the number "1" (formula_2).

 deals with events that occur in countable sample spaces.

Examples: Throwing dice, experiments with decks of cards, random walk, and tossing coins
Initially the probability of an event to occur was defined as the number of cases favorable for the event, over the number of total outcomes possible in an equiprobable sample space: see Classical definition of probability.

For example, if the event is "occurrence of an even number when a die is rolled", the probability is given by formula_3, since 3 faces out of the 6 have even numbers and each face has the same probability of appearing.
The modern definition starts with a finite or countable set called the sample space, which relates to the set of all "possible outcomes" in classical sense, denoted by formula_4. It is then assumed that for each element formula_5, an intrinsic "probability" value formula_6 is attached, which satisfies the following properties:

That is, the probability function "f"("x") lies between zero and one for every value of "x" in the sample space "Ω", and the sum of "f"("x") over all values "x" in the sample space "Ω" is equal to 1. An is defined as any subset formula_9 of the sample space formula_10. The of the event formula_9 is defined as

So, the probability of the entire sample space is 1, and the probability of the null event is 0.

The function formula_6 mapping a point in the sample space to the "probability" value is called a abbreviated as . The modern definition does not try to answer how probability mass functions are obtained; instead, it builds a theory that assumes their existence.

 deals with events that occur in a continuous sample space.
The classical definition breaks down when confronted with the continuous case. See Bertrand's paradox.
If the outcome space of a random variable "X" is the set of real numbers (formula_14) or a subset thereof, then a function called the (or ) formula_15 exists, defined by formula_16. That is, "F"("x") returns the probability that "X" will be less than or equal to "x".

The cdf necessarily satisfies the following properties.

If formula_15 is absolutely continuous, i.e., its derivative exists and integrating the derivative gives us the cdf back again, then the random variable "X" is said to have a or or simply formula_21

For a set formula_22, the probability of the random variable "X" being in formula_9 is

In case the probability density function exists, this can be written as

Whereas the "pdf" exists only for continuous random variables, the "cdf" exists for all random variables (including discrete random variables) that take values in formula_26

These concepts can be generalized for multidimensional cases on formula_27 and other continuous sample spaces.

The "raison d'être" of the measure-theoretic treatment of probability is that it unifies the discrete and the continuous cases, and makes the difference a question of which measure is used. Furthermore, it covers distributions that are neither discrete nor continuous nor mixtures of the two.

An example of such distributions could be a mix of discrete and continuous distributions—for example, a random variable that is 0 with probability 1/2, and takes a random value from a normal distribution with probability 1/2. It can still be studied to some extent by considering it to have a pdf of formula_28, where formula_29 is the Dirac delta function.

Other distributions may not even be a mix, for example, the Cantor distribution has no positive probability for any single point, neither does it have a density. The modern approach to probability theory solves these problems using measure theory to define the probability space:

Given any set formula_10 (also called ) and a σ-algebra formula_31 on it, a measure formula_32 defined on formula_31 is called a if formula_34

If formula_31 is the Borel σ-algebra on the set of real numbers, then there is a unique probability measure on formula_31 for any cdf, and vice versa. The measure corresponding to a cdf is said to be by the cdf. This measure coincides with the pmf for discrete variables and pdf for continuous variables, making the measure-theoretic approach free of fallacies.

The "probability" of a set formula_9 in the σ-algebra formula_31 is defined as

where the integration is with respect to the measure formula_40 induced by formula_41

Along with providing better understanding and unification of discrete and continuous probabilities, measure-theoretic treatment also allows us to work on probabilities outside formula_27, as in the theory of stochastic processes. For example, to study Brownian motion, probability is defined on a space of functions.

When it's convenient to work with a dominating measure, the Radon-Nikodym theorem is used to define a density as the Radon-Nikodym derivative of the probability distribution of interest with respect to this dominating measure. Discrete densities are usually defined as this derivative with respect to a counting measure over the set of all possible outcomes. Densities for absolutely continuous distributions are usually defined as this derivative with respect to the Lebesgue measure. If a theorem can be proved in this general setting, it holds for both discrete and continuous distributions as well as others; separate proofs are not required for discrete and continuous distributions.

Certain random variables occur very often in probability theory because they well describe many natural or physical processes. Their distributions, therefore, have gained "special importance" in probability theory. Some fundamental "discrete distributions" are the discrete uniform, Bernoulli, binomial, negative binomial, Poisson and geometric distributions. Important "continuous distributions" include the continuous uniform, normal, exponential, gamma and beta distributions.

In probability theory, there are several notions of convergence for random variables. They are listed below in the order of strength, i.e., any subsequent notion of convergence in the list implies convergence according to all of the preceding notions.




As the names indicate, weak convergence is weaker than strong convergence. In fact, strong convergence implies convergence in probability, and convergence in probability implies weak convergence. The reverse statements are not always true.

Common intuition suggests that if a fair coin is tossed many times, then "roughly" half of the time it will turn up "heads", and the other half it will turn up "tails". Furthermore, the more often the coin is tossed, the more likely it should be that the ratio of the number of "heads" to the number of "tails" will approach unity. Modern probability theory provides a formal version of this intuitive idea, known as the . This law is remarkable because it is not assumed in the foundations of probability theory, but instead emerges from these foundations as a theorem. Since it links theoretically derived probabilities to their actual frequency of occurrence in the real world, the law of large numbers is considered as a pillar in the history of statistical theory and has had widespread influence.
The (LLN) states that the sample average
of a sequence of independent and
identically distributed random variables formula_59 converges towards their common expectation formula_60, provided that the expectation of formula_61 is finite.

It is in the different forms of convergence of random variables that separates the "weak" and the "strong" law of large numbers

It follows from the LLN that if an event of probability "p" is observed repeatedly during independent experiments, the ratio of the observed frequency of that event to the total number of repetitions converges towards "p".

For example, if formula_66 are independent Bernoulli random variables taking values 1 with probability "p" and 0 with probability 1-"p", then formula_67 for all "i", so that formula_68 converges to "p" almost surely.

"The central limit theorem (CLT) is one of the great results of mathematics." (Chapter 18 in)
It explains the ubiquitous occurrence of the normal distribution in nature.

The theorem states that the average of many independent and identically distributed random variables with finite variance tends towards a normal distribution "irrespective" of the distribution followed by the original random variables. Formally, let formula_50 be independent random variables with mean formula_60 and variance formula_71 Then the sequence of random variables
converges in distribution to a standard normal random variable.

For some classes of random variables the classic central limit theorem works rather fast (see Berry–Esseen theorem), for example the distributions with finite first, second, and third moment from the exponential family; on the other hand, for some random variables of the heavy tail and fat tail variety, it works very slowly or may not work at all: in such cases one may use the Generalized Central Limit Theorem (GCLT).



</doc>
<doc id="23543" url="https://en.wikipedia.org/wiki?curid=23543" title="Probability distribution">
Probability distribution

In probability theory and statistics, a probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment. It is a mathematical description of a random phenomenon in terms of its sample space and the probabilities of events (subsets of the sample space).

For instance, if the random variable is used to denote the outcome of a coin toss ("the experiment"), then the probability distribution of would take the value 0.5 for , and 0.5 for (assuming the coin is fair). Examples of random phenomena include the weather condition in a future date, the height of a person, the fraction of male students in a school, the results of a survey, etc.

A probability distribution is a mathematical function that has a sample space as its input, and gives a probability as its output. The sample space is the set of all possible outcomes of a random phenomenon being observed; it may be the set of real numbers or a set of vectors, or it may be a list of non-numerical values. For example, the sample space of a coin flip would be .

Probability distributions are generally divided into two classes. A discrete probability distribution is applicable to the scenarios where the set of possible outcomes is discrete (e.g. a coin toss or the roll of a dice), and the probabilities are here encoded by a discrete list of the probabilities of the outcomes, known as the probability mass function. On the other hand, continuous probability distributions are applicable to scenarios where the set of possible outcomes can take on values in a continuous range (e.g. real numbers), such as the temperature on a given day. In this case, probabilities are typically described by a probability density function. The normal distribution is a commonly encountered continuous probability distribution. More complex experiments, such as those involving stochastic processes defined in continuous time, may demand the use of more general probability measures.

A probability distribution whose sample space is one-dimensional (for example real numbers, list of labels, ordered labels or binary) is called univariate, while a distribution whose sample space is a vector space of dimension 2 or more is called multivariate. A univariate distribution gives the probabilities of a single random variable taking on various alternative values; a multivariate distribution (a joint probability distribution) gives the probabilities of a random vector – a list of two or more random variables – taking on various combinations of values. Important and commonly encountered univariate probability distributions include the binomial distribution, the hypergeometric distribution, and the normal distribution. The multivariate normal distribution is a commonly encountered multivariate distribution.

To define probability distributions for the simplest cases, it is necessary to distinguish between discrete and continuous random variables. In the discrete case, it is sufficient to specify a probability mass function formula_1 assigning a probability to each possible outcome: for example, when throwing a fair die, each of the six values 1 to 6 has the probability 1/6. The probability of an event is then defined to be the sum of the probabilities of the outcomes that satisfy the event; for example, the probability of the event "the dice rolls an even value" is 

In contrast, when a random variable takes values from a continuum then typically, any individual outcome has probability zero and only events that include infinitely many outcomes, such as intervals, can have positive probability. For example, the probability that a given object weighs "exactly" 500 g is zero, because the probability of measuring exactly 500 g tends to zero as the accuracy of our measuring instruments increases. Nevertheless, in quality control one might demand that the probability of a "500 g" package containing between 490 g and 510 g should be no less than 98%, and this demand is less sensitive to the accuracy of measurement instruments.

Continuous probability distributions can be described in several ways. The probability density function describes the infinitesimal probability of any given value, and the probability that the outcome lies in a given interval can be computed by integrating the probability density function over that interval. The probability that the possible values lie in some fixed interval can be related to the way sums converge to an integral; therefore, continuous probability is based on the definition of an integral. 

The cumulative distribution function describes the probability that the random variable is no larger than a given value; the probability that the outcome lies in a given interval can be computed by taking the difference between the values of the cumulative distribution function at the endpoints of the interval. The cumulative distribution function is the antiderivative of the probability density function provided that the latter function exists. The cumulative distribution function is the area under the probability density function from minus infinity formula_3 to formula_4 as described by the picture to the right.

Some key concepts and terms, widely used in the literature on the topic of probability distributions, are listed below.




A discrete probability distribution is a probability distribution that can take on a countable number of values. For the probabilities to add up to 1, they have to decline to zero fast enough. For example, if formula_12 for "n" = 1, 2, ..., the sum of probabilities would be 1/2 + 1/4 + 1/8 + ... = 1.

Well-known discrete probability distributions used in statistical modeling include the Poisson distribution, the Bernoulli distribution, the binomial distribution, the geometric distribution, and the negative binomial distribution. Additionally, the discrete uniform distribution is commonly used in computer programs that make equal-probability random selections between a number of choices.

When a sample (a set of observations) is drawn from a larger population, the sample points have an empirical distribution that is discrete and that provides information about the population distribution.

A measurable function formula_13 between a probability space formula_14 and a measurable space formula_15 is called a discrete random variable provided that its image is a countable set. In this case measurability of formula_5 means that the pre-images of singleton sets are measurable, i.e., formula_17 for all formula_18.
The latter requirement induces a probability mass function formula_19 via formula_20. Since the pre-images of disjoint sets
are disjoint,
This recovers the definition given above.

Equivalently to the above, a discrete random variable can be defined as a random variable whose cumulative distribution function (cdf) increases only by jump discontinuities—that is, its cdf increases only where it "jumps" to a higher value, and is constant between those jumps. Note however that the points where the cdf jumps may form a dense set of the real numbers. The points where jumps occur are precisely the values which the random variable may take.

Consequently, a discrete probability distribution is often represented as a generalized probability density function involving Dirac delta functions, which substantially unifies the treatment of continuous and discrete distributions. This is especially useful when dealing with probability distributions involving both a continuous and a discrete part.

For a discrete random variable "X", let "u", "u", ... be the values it can take with non-zero probability. Denote

These are disjoint sets, and for such sets

It follows that the probability that "X" takes any value except for "u", "u", ... is zero, and thus one can write "X" as

except on a set of probability zero, where formula_25 is the indicator function of "A". This may serve as an alternative definition of discrete random variables.

A continuous probability distribution is a probability distribution whose support is an uncountable set, such as an interval in the real line. They are uniquely characterized by a cumulative density function that can be used to calculate the probability for each subset of the support. There are many examples of continuous probability distributions: normal, uniform, chi-squared, and others.

A random variable formula_5 has a continuous probability distribution if there is a function formula_27 such that for each interval formula_28 the probability of formula_5 belonging to formula_30 is given by the integral of formula_31 over formula_30. For example, if formula_33 then we would have:
In particular, the probability for formula_5 to take any single value formula_36 (that is formula_37) is zero, because an integral with coinciding upper and lower limits is always equal to zero. A variable that satisfies the above is called continuous random variable. Its cumulative density function is defined as
which, by this definition, has the properties:

It is also possible to think in the opposite direction, which allows more flexibility. Say formula_39 is a function that satisfies all but the last of the properties above, then formula_40 represents the cumulative density function for some random variable: a discrete random variable if formula_40 is a step function, and a continuous random variable otherwise. This allows for continuous distributions that has a cumulative density function, but not a probability density function, such as the Cantor distribution.

It is often necessary to generalize the above definition for more arbitrary subsets of the real line. In these contexts, a continuous probability distribution is defined as a probability distribution with a cumulative distribution function that is absolutely continuous. Equivalently, it is a probability distribution on the real numbers that is absolutely continuous with respect to the Lebesgue measure. Such distributions can be represented by their probability density functions. If formula_5 is such an absolutely continuous random variable, then it has a probability density function formula_43, and its probability of falling into a Lebesgue-measurable set formula_44 is:
where formula_46 is the Lebesgue measure.

Note on terminology: some authors use the term "continuous distribution" to denote distributions whose cumulative distribution functions are continuous, rather than absolutely continuous. These distributions are the ones formula_46 such that formula_48 for all formula_49. This definition includes the (absolutely) continuous distributions defined above, but it also includes singular distributions, which are neither absolutely continuous nor discrete nor a mixture of those, and do not have a density. An example is given by the Cantor distribution.

In the measure-theoretic formalization of probability theory, a random variable is defined as a measurable function formula_5 from a probability space formula_51 to a measurable space formula_52. Given that probabilities of events of the form formula_53 satisfy Kolmogorov's probability axioms, the probability distribution of "X" is the pushforward measure formula_54 of formula_5 , which is a probability measure on formula_52 satisfying formula_57.

Most algorithms are based on a pseudorandom number generator that produces numbers "X" that are uniformly distributed in the half-open interval [0,1). These random variates "X" are then transformed via some algorithm to create a new random variate having the required probability distribution. With this source of uniform pseudo-randomness, realizations of any random variable can be generated.

For example, suppose formula_58 has a uniform distribution between 0 and 1. To construct a random Bernoulli variable for some formula_59, we define

<math>{\displaystyle X ={\begin{cases}1,&{\mbox{if }}U

so that

<math>\textrm{P}(X=1) = \textrm{P}(U

This random variable X has a Bernoulli distribution with parameter formula_1. Note that this is a transformation of discrete random variable.

For a distribution function formula_40 of a continuous random variable, a continuous random variable must be constructed. formula_62, an inverse function of formula_40, relates to the uniform variable formula_58:

formula_65

For example, suppose a random variable that has an exponential distribution formula_66 must be constructed.

formula_67

so formula_68 and if formula_58 has a formula_70 distribution, then the random variable formula_5 is defined by formula_72. This has an exponential distribution of formula_73.

A frequent problem in statistical simulations (the Monte Carlo method) is the generation of pseudo-random numbers that are distributed in a given way.

The concept of the probability distribution and the random variables which they describe underlies the mathematical discipline of probability theory, and the science of statistics. There is spread or variability in almost any value that can be measured in a population (e.g. height of people, durability of a metal, sales growth, traffic flow, etc.); almost all measurements are made with some intrinsic error; in physics many processes are described probabilistically, from the kinetic properties of gases to the quantum mechanical description of fundamental particles. For these and many other reasons, simple numbers are often inadequate for describing a quantity, while probability distributions are often more appropriate.

The following is a list of some of the most common probability distributions, grouped by the type of process that they are related to. For a more complete list, see list of probability distributions, which groups by the nature of the outcome being considered (discrete, continuous, multivariate, etc.)

All of the univariate distributions below are singly peaked; that is, it is assumed that the values cluster around a single point. In practice, actually observed quantities may cluster around multiple values. Such quantities can be modeled using a mixture distribution.














</doc>
<doc id="23545" url="https://en.wikipedia.org/wiki?curid=23545" title="Psychological statistics">
Psychological statistics

Psychological statistics is application of formulas, theorems, numbers and laws to psychology. 
Statistical Methods for psychology include development and application statistical theory and methods for modeling psychological data. 
These methods include psychometrics, Factor analysis, Experimental Designs, Multivariate Behavioral Research. The article also discusses journals in the same field Wilcox, R. (2012).

Psychometrics deals with measurement of psychological attributes. It involved developing and applying statistical models for mental measurements (Lord and Novik, ; etc.) The measurement theories are divided into two major areas: (1) Classical test theory; (2) Item Response Theory (Nunnally, J. & Bernstein, I. (1994)).

The classical test theory or true score theory or reliability theory in statistics is a set of statistical procedures useful for development of psychological tests and scales. It is based on fundamental equation 
X = T + E
where, X is total score, T is a true score and E is error of measurement. For each participant, it assumes that there exist a true score and it need to be obtained score (X) has to be as close to it as possible (Lord, F. M. , and Novick, M. R. ( 1 968), Raykov, T. & Marcoulides, G.A. (2010) ). The closeness of X has with T is expressed in terms of ratability of the obtained score. The reliability in terms of classical test procedure is correlation between true score and obtained score. The typical test construction procedures has following steps:

(1) Determine the construct 
(2) Outline the behavioral domain of the construct
(3) Write 3 to 5 times more items than desired test length
(4) Get item content analyzed by experts and cull items
(5) Obtain data on initial version of the test 
(8) After the second cull, make final version
(9) Use it for research

The reliability is computed in specific ways. 
(A) Inter-Rater reliability: Inter-Rater reliability is estimate of agreement between independent raters. This is most useful for subjective responses. Cohen's Kappa, Krippendorff's Alpha, Intra-Class correlation coefficients, Correlation coefficients, Kendal's concordance coefficient, etc. are useful statistical tools. 
(B) Test-Retest Reliability: Test-Retest Procedure is estimation of temporal consistency of the test. A test is administered twice to the same sample with a time interval. Correlation between two sets of scores is used as an estimate of reliability. Testing conditions are assumed to be identical. 
(C) Internal Consistency Reliability: Internal consistency reliability estimates consistency of items with each other. Split-half reliability (Spearman- Brown Prophecy) and Cronbach Alpha are popular estimates of this reliability . (Cronbach LJ (1951)). 
(D) Parallel Form Reliability: It is an estimate of consistency between two different instruments of measurement. The inter-correlation between two parallel forms of a test or scale is used as an estimate of parallel form reliability.

Validity of a scale or test is ability of the instrument to measure what it purports to measure (Nunnally, J. & Bernstein, I. (1994)). Construct validity, Content Validity, Criterion Validity are types of validity. 
Construct validity is estimated by convergent and discriminant validity and factor analysis. Convergent and discriminant validity are ascertained by correlation between similar of different constructs. 
Content Validity: Subject matter experts evaluate content validity. 
Criterion Validity is correlation between the test and a criterion variable (or variables) of the construct. Regression analysis, Multiple regression analysis, Logistic regression is used as an estimate of criterion validity. 
Software applications: The R software has ‘psych’ package that is useful for classical test theory analysis.

The modern test theory is based on latent trait model. Every item estimates the ability of the test taker. The ability parameter is called as theta (θ). The difficulty parameter is called b. the two important assumptions are local independence and unidimensionality. 
The Item Response Theory has three models. They are one parameter logistic model, two parameter logistic model and three parameter logistic model. In addition, Polychromous IRT Model are also useful (Hambleton & Swaminathan, 1985).

The R Software has ‘ltm’, packages useful for IRT analysis.

Factor analysis is at the core of psychological statistics. It has two schools: (1) Exploratory Factor analysis (2) Confirmatory Factor analysis

The exploratory factor analysis begins without a theory or with a very tentative theory. It is a dimension reduction technique. It is useful in psychometrics, multivariate analysis of data and data analytics. 
Typically a k-dimensional correlation matrix or covariance matrix of variables is reduced to k X r factor pattern matrix where r < k. Principal Component analysis and common factor analysis are two ways of extracting data. Principal axis factoring, ML factor analysis, alpha factor analysis and image factor analysis is most useful ways of EFA. 
It employees various factor rotation methods which can be classified into orthogonal (resulting in uncorrelated factors) and oblique (resulting correlated factors).

The ‘psych’ package in R is useful for EFA.

Confirmatory Factor Analysis (CFA) is factor analytic technique that begins with theory and test the theory by carrying out factor analysis. 
The CFA is also called as latent structure analysis, which considers factor as latent variables causing actual observable variables. The basic equation of the CFA is

X = Λξ + δ

where, X is observed variables, Λ are structural coefficients, ξ are latent variables (factors) and δ are errors. 
The parameters are estimated using ML methods however; other methods of estimation are also available. The chi-square test is very sensitive and hence various fit measures are used (Bollen,1989, Loehlin, 1992).
R package ‘sem’, ‘lavaan’ are useful for the same.

Experimental Methods are very popular in psychology. It has more than 100 years tradition. Experimental psychology has a status of sub-discipline in psychology .
The statistical methods are applied for designing and analyzing experimental data. They involve, t-test, ANOVA, ANCOVA, MANOVA, MANCOVA, binomial test, chi-square etc. are used for the analysis of the experimental data.

Multivariate behavioral research is becoming very popular in psychology. These methods include Multiple Regression and Prediction; Moderated and Mediated Regression Analysis; Logistics Regression; Canonical Correlations; Cluster analysis; Multi-level modeling; Survival-Failure analysis; Structural Equations Modeling; hierarchical linear modelling etc. are very useful for psychological statistics (Hayes, 2013; Agresti, 1990; Loehlin, 1992; Menard, 2001; Tabachnick, & Fidell, 2007).

There are many specialized journals that publish advances in statistical analysis for psychology. Psychometrika is at the forefront. Educational and Psychological Measurement, Assessment, American Journal of Evaluation, Applied Psychological Measurement, Behavior Research Methods, British Journal of Mathematical and Statistical Psychology, Journal of Educational and Behavioral Statistics, Journal of Mathematical Psychology, Multivariate Behavioral Research, Psychological Assessment, Structural Equation Modeling are other useful journals.

Various software packages are available for statistical methods for psychological research. They can be classified as commercial software (e.g., JMP and SPSS) and Open-Source (e.g., R). Among the free-wares, the R software is most popular one. There are many online references for R and specialised books on R for Psychologist are also being written (e.g., Belhekar, 2016 ). The "psych" package of R is very useful for psychologists. Among others, "lavaan", "sem", "ltm", "ggplot2" are some of the popular packages. PSPP and KNIME are other free packages. Among the commercial packages include JMP, SPSS and SAS. JMP and SPSS are commonly reported in books.





</doc>
<doc id="23547" url="https://en.wikipedia.org/wiki?curid=23547" title="Peter Cook">
Peter Cook

Peter Edward Cook (17 November 1937 – 9 January 1995) was an English satirist and comedic actor. He was a leading figure of the British satire boom of the 1960s, and he was associated with the anti-establishment comedic movement that emerged in the United Kingdom in the late 1950s.

Born in Torquay, he was educated at the University of Cambridge. There he became involved with the Footlights Club, of which he later became president. After graduating he created the comedy stage revue "Beyond the Fringe", beginning a long-running partnership with Dudley Moore. In 1961, Cook opened the comedy club The Establishment in Soho, Central London. In 1965, Cook and Moore began a television career, beginning with "Not Only... But Also". Following the success of the show, the duo appeared together in films "The Wrong Box" (1966) and "Bedazzled" (1967). Cook and Moore returned to television projects continuing to the late 1970s, including co-presenting "Saturday Night Live" in the United States. From 1978 until his death in 1995, Cook no longer collaborated with Moore but continued to be a regular performer in British television and film.

Referred to as "the father of modern satire" by "The Guardian" in 2005, Cook was ranked number one in the "Comedians' Comedian", a poll of more than 300 comics, comedy writers, producers, and directors throughout the English-speaking world.

Cook was born at his parents' house, "Shearbridge", in Middle Warberry Road, Torquay, Devon. He was the only son, and eldest of the three children, of Alexander Edward "Alec" Cook (1906–1984), a colonial civil servant and his wife Ethel Catherine Margaret (1908–1994), daughter of solicitor Charles Mayo. His father served as political officer and later district officer in Nigeria, then as financial secretary to the colony of Gibraltar, followed by a return to Nigeria as Permanent Secretary of the Eastern Region based at Enugu. Cook's grandfather, Edward Arthur Cook (1869–1914), had also been a colonial civil servant, traffic manager for the Federated Malay States Railway in Kuala Lumpur, Malaya; stress that he suffered, in the lead-up to an interview regarding promotion, led him to commit suicide. His wife, Minnie Jane (1869–1957; daughter of Thomas Wreford, of Thelbridge and Witheridge, Devon, and of Stratford-upon-Avon, of a prominent Devonshire family traced back to 1440), kept this fact secret; Peter Cook only discovered the truth when later researching his family.

Cook was educated at Radley College and then went up to Pembroke College, Cambridge, where he read French and German. As a student, Cook initially intended to become a career diplomat like his father, but Britain "had run out of colonies", as he put it. Although largely apathetic politically, particularly in later life when he displayed a deep distrust of politicians of all hues, he joined the Cambridge University Liberal Club. At Pembroke, Cook performed and wrote comedy sketches as a member of the Cambridge Footlights Club, of which he became president in 1960. His hero was fellow Footlights writer and Cambridge magazine writer David Nobbs.

While still at university, Cook wrote for Kenneth Williams, providing several sketches for Williams' hit West End comedy revue "Pieces of Eight" and much of the follow-up, "One Over the Eight", before finding prominence in his own right in a four-man group satirical stage show, "Beyond the Fringe", alongside Jonathan Miller, Alan Bennett, and Dudley Moore.

"Beyond the Fringe" became a great success in London after being first performed at the Edinburgh Festival and included Cook impersonating the prime minister, Harold Macmillan. This was one of the first occasions satirical political mimicry had been attempted in live theatre, and it shocked audiences. During one performance, Macmillan was in the theatre and Cook departed from his script and attacked him verbally.

In 1961, Cook opened The Establishment, a club at 18 Greek Street in Soho in central London, presenting fellow comedians in a nightclub setting, including American Lenny Bruce. Cook said it was a satirical venue modelled on "those wonderful Berlin cabarets ... which did so much to stop the rise of Hitler and prevent the outbreak of the Second World War"; as a members-only venue, it was outside the censorship restrictions. The Establishment's regular cabaret performers were Eleanor Bron, John Bird, and John Fortune. 
Cook befriended and supported Australian comedian and actor Barry Humphries, who began his British solo career at the club. Humphries said in his autobiography, "My Life As Me", that he found Cook's lack of interest in art and literature off-putting. Dudley Moore's jazz trio played in the basement of the club during the early 1960s.

Cook also opened an Establishment club in New York in 1963 and Lenny Bruce performed there, as well.

In 1962, the BBC commissioned a pilot for a television series of satirical sketches based on the Establishment Club, but it was not immediately picked up and Cook went to New York City for a year to perform "Beyond the Fringe" on Broadway. When he returned, the pilot had been refashioned as "That Was the Week That Was" and had made a star of David Frost, something Cook resented.

The 1960s satire boom was coming to an end and Cook said: "England was about to sink giggling into the sea." He complained that Frost's success was based on copying Cook's own stage persona and Cook dubbed him "the bubonic plagiarist", and said that his only regret in life, according to Alan Bennett, had been saving Frost from drowning. This incident occurred in the summer of 1963, when the rivalry between the two men was at its height. Cook had realised that Frost's potential drowning would have looked deliberate if he had not been rescued.

Around this time, Cook provided financial backing for the satirical magazine "Private Eye", supporting it through difficult periods, particularly in libel trials. Cook invested his own money and solicited investment from his friends. For a time, the magazine was produced from the premises of the Establishment Club. In 1963, Cook married Wendy Snowden; the couple had two daughters, Lucy and Daisy, but the marriage ended in 1970.

Cook's first regular television spot was on Granada Television's "Braden Beat" with Bernard Braden, where he featured his most enduring character: the static, dour and monotonal E. L. Wisty, whom Cook had conceived for Radley College's Marionette Society.

Cook's comedy partnership with Dudley Moore led to "Not Only... But Also". This was originally intended by the BBC as a vehicle for Moore's music, but Moore invited Cook to write sketches and appear with him. Using few props, they created dry, absurd television that proved hugely popular and lasted for three series between 1965 and 1970. Cook played characters such as Sir Arthur Streeb-Greebling and the two men created their Pete and Dud alter egos. Other sketches included "Superthunderstingcar", a parody of the Gerry Anderson marionette TV shows, and Cook's pastiche of 1960s trendy arts documentaries – satirised in a parodic segment on Greta Garbo.

When Cook learned a few years later that the videotapes of the series were to be wiped, a common practice at the time, he offered to buy the recordings from the BBC but was refused because of copyright issues. He suggested he could purchase new tapes so that the BBC would have no need to erase the originals, but this was also turned down. Of the original 22 programmes, only eight still survive complete. A compilation of six half-hour programmes, "The Best of... What's Left of... Not Only...But Also" was shown on television and has been released on both VHS and DVD.

With "The Wrong Box" (1966) and "Bedazzled" (1967), Cook and Moore began to act in films together. Directed by Stanley Donen, the underlying story of "Bedazzled" is credited to Cook and Moore and its screenplay to Cook. A comic parody of Faust, it stars Cook as George Spigott (the Devil) who tempts Stanley Moon (Moore), a frustrated, short-order chef, with the promise of gaining his heart's desire – the unattainable beauty and waitress at his cafe, Margaret Spencer (Eleanor Bron) – in exchange for his soul, but repeatedly tricks him. The film features cameo appearances by Barry Humphries as Envy and Raquel Welch as Lust. Moore composed the soundtrack music and co-wrote (with Cook) the songs performed in the film. His jazz trio backed Cook on the theme, a parodic anti-love song, which Cook delivered in a deadpan monotone and included his familiar put-down, "you fill me with inertia".

In 1968, Cook and Moore briefly switched to ATV for four one-hour programmes titled "Goodbye Again", based on the Pete and Dud characters. Cook's increasing alcoholism led him to become reliant on cue cards; the show was not a popular success, owing in part to a strike causing the suspension of the publication of the ITV listings magazine "TV Times". John Cleese was a cast member.

In 1970, Cook took over a project initiated by David Frost for a satirical film about an opinion pollster who rises to become President of Great Britain. Under Cook's guidance, the character became modelled on Frost. The film, "The Rise and Rise of Michael Rimmer", was not a success, although the cast contained notable names (including Monty Python's John Cleese and Graham Chapman, who co-wrote the film).

Cook became a favourite of the chat show circuit, but his own effort at hosting such a show for the BBC in 1971, "Where Do I Sit?", was said by the critics to have been a disappointment. It was axed after only three episodes, and was replaced by Michael Parkinson, the start of Parkinson's career as a chat show host. Parkinson later asked Cook what his ambitions were, Cook replied jocularly "[...] in fact, my ambition is to shut you up altogether you see!"

Cook and Moore fashioned sketches from "Not Only...But Also" and "Goodbye Again" with new material into the stage revue called "Behind the Fridge". This show toured Australia in 1972 before transferring to New York City in 1973, re-titled as "Good Evening". Cook frequently appeared on and off stage the worse for drink. Nonetheless, the show proved very popular and it won Tony and Grammy Awards. When it finished, Moore stayed in the United States to pursue his film acting ambitions in Hollywood. Cook returned to Britain and in 1973, married the actress and model Judy Huxtable.

Later, the more risqué humour of Pete and Dud went farther on long-playing records as "Derek and Clive". The first recording was initiated by Cook to alleviate boredom during the Broadway run of "Good Evening", and used material conceived years before for the two characters but considered too outrageous. One of these audio recordings was also filmed, and therein tensions between the duo are seen to rise. Chris Blackwell circulated bootleg copies to friends in the music business. The popularity of the recording convinced Cook to release it commercially, although Moore was initially reluctant, fearing that his rising fame as a Hollywood star would be undermined. Two further "Derek and Clive" albums were released, the last accompanied by a film.

Cook and Moore hosted "Saturday Night Live" on 24 January 1976 during the show's first season. They did a number of their classic stage routines, including "One Leg Too Few" and "Frog and Peach" among others, in addition to participating in some skits with the show's ensemble cast.

In 1978, Cook appeared on the British music series "Revolver" as the manager of a ballroom where emerging punk and new wave acts played. For some groups, these were their first appearances on television. Cook's acerbic commentary was a distinctive aspect of the programme.

In 1979, Cook recorded comedy-segments as B-sides to the Sparks 12-inch singles "Number One Song in Heaven" and "Tryouts for the Human Race". The main songwriter Ron Mael often began with a banal situation in his lyrics, and then went at surreal tangents in the style of Cook and S. J. Perelman.

Cook played multiple roles on the 1977 concept album "Consequences", written and produced by former 10cc members Kevin Godley and Lol Creme. A mixture of spoken comedy and progressive rock with an environmental subtext, "Consequences" started as a single that Godley and Creme planned to make to demonstrate their invention, an electric guitar effect called the Gizmo, which they developed in 10cc. The project grew into a three-LP box set. The comedy sections were originally intended to be performed by a cast including Spike Milligan and Peter Ustinov, but Godley and Creme eventually settled on Cook once they realised he could perform most parts himself.

The storyline centres on the impending divorce of ineffectual Englishman Walter Stapleton (Cook) and his French wife Lulu (Judy Huxtable). While meeting their lawyers – the bibulous Mr. Haig and overbearing Mr. Pepperman (both played by Cook) – the encroaching global catastrophe interrupts proceedings with bizarre and mysterious happenings, which seem to centre on Mr. Blint (Cook), a musician and composer living in the flat below Haig's office, to which it is connected by a large hole in the floor.

Although it has since developed a cult following due to Cook's presence, "Consequences" was released as punk was sweeping the UK and proved a resounding commercial failure, savaged by critics who found the music self-indulgent. The script and story have evident connections to Cook's own life – his then-wife Judy Huxtable plays Walter's wife. Cook's struggles with alcohol are mirrored in Haig's drinking, and there is a parallel between the fictional divorce of Walter and Lulu and Cook's own divorce from his first wife. The voice and accent Cook used for the character of Stapleton are similar to those of Cook's "Beyond the Fringe" colleague, Alan Bennett, and a book on Cook's comedy, "How Very Interesting", speculates that the characters Cook plays in "Consequences" are caricatures of the four "Beyond the Fringe" cast members – the alcoholic Haig represents Cook, the tremulous Stapleton is Bennett, the parodically Jewish Pepperman is Miller, and the pianist Blint represents Moore.

Cook appeared at the first three fundraising galas staged by humorists John Cleese and Martin Lewis on behalf of Amnesty International. The benefits were dubbed "The Secret Policeman's Balls", though it wasn't until the third show in 1979 that the title was used. He performed on all three nights of the first show in April 1976, "A Poke in the Eye (With a Sharp Stick)", as an individual performer and as a member of the cast of "Beyond the Fringe", which reunited for the first time since the 1960s. He also appeared in a Monty Python sketch, taking the place of Eric Idle. Cook was on the cast album of the show and in the film, "Pleasure at Her Majesty's". He was in the second Amnesty gala in May 1977, "An Evening Without Sir Bernard Miles". It was retitled "The Mermaid Frolics" for the cast album and TV special. Cook performed monologues and skits with Terry Jones.

In June 1979, Cook performed all four nights of "The Secret Policeman's Ball", teaming with John Cleese. Cook performed a couple of solo pieces and a sketch with Eleanor Bron. He also led the ensemble in the finale – the "End of the World" sketch from "Beyond the Fringe".

In response to a barb in "The Daily Telegraph" that the show was recycled material, Cook wrote a satire of the summing-up by Justice Cantley in the trial of former Liberal Party leader Jeremy Thorpe, a summary now widely thought to show bias in favour of Thorpe. Cook performed it that same night (Friday 29 June – the third of the four nights) and the following night. The nine-minute opus, "Entirely a Matter for You", is considered by many fans and critics to be one of the finest works of Cook's career. Along with Cook, producer of the show Martin Lewis brought out an album on Virgin Records entitled "Here Comes the Judge: Live", containing the live performance together with three studio tracks that further lampooned the Thorpe trial.

Although unable to take part in the 1981 gala, Cook supplied the narration over the animated opening title sequence of the 1982 film of the show. With Lewis, he wrote and voiced radio commercials to advertise the film in the UK. He also hosted a spoof film awards ceremony that was part of the world première of the film in London in March 1982.

Following Cook's 1987 stage reunion with Moore for the annual American benefit for the homeless, Comic Relief (not related to the UK Comic Relief benefits), Cook repeated the reunion for a British audience by performing with Moore at the 1989 Amnesty benefit "The Secret Policeman's Biggest Ball".

Cook starred in the LWT special "Peter Cook & Co." in 1980. The show included comedy sketches, including a "Tales of the Unexpected" parody "Tales of the Much As We Expected". This involved Cook as Roald Dahl, explaining his name had been Ronald before he dropped the "n". The cast included John Cleese, Rowan Atkinson, Beryl Reid, Paula Wilcox, and Terry Jones. Partly spurred by Moore's growing film star status, Cook moved to Hollywood in that year, and appeared as an uptight English butler to a wealthy American woman in a short-lived United States television sitcom, "The Two of Us", also making cameo appearances in a couple of undistinguished films.

In 1983, Cook played the role of Richard III in the first episode of "Blackadder", "The Foretelling", which parodies Laurence Olivier's portrayal. In 1984, he played the role of Nigel, the mathematics teacher, in Jeannot Szwarc's film "Supergirl", working alongside the evil Selena played by Faye Dunaway. He then narrated the short film "Diplomatix" by Norwegian comedy trio Kirkvaag, Lystad, and Mjøen, which won the "Special Prize of the City of Montreux" at the Montreux Comedy Festival in 1985. In 1986, he partnered Joan Rivers on her UK talk show. He appeared as Mr Jolly in 1987 in "The Comic Strip Presents..."' episode "Mr. Jolly Lives Next Door", playing an assassin who covers the sound of his murders by playing Tom Jones records. That same year, Cook appeared in "The Princess Bride" as the "Impressive Clergyman" who officiates at the wedding ceremony between Buttercup and Prince Humperdinck, uttering the now-famous line "Mawage!" Also that year, he spent time working with humourist Martin Lewis on a political satire about the 1988 US presidential elections for HBO, but the script went unproduced. Lewis suggested that Cook team with Moore for the US Comic Relief telethon for the homeless. The duo reunited and performed their "One Leg Too Few" sketch.

A 1984 commercial for John Harvey & Sons showed Cook at a poolside party drinking Harvey's Bristol Cream sherry. He then says to "throw away those silly little glasses" whereupon the other party guests toss their sunglasses in the swimming pool.

In 1988, Cook appeared as a contestant on the improvisation comedy show "Whose Line Is It Anyway?". He was declared the winner, his prize being to read the credits in the style of a New York cab driver – a character he had portrayed in "Peter Cook & Co."

Cook occasionally called in to Clive Bull's night-time phone-in radio show on LBC in London. Using the name "Sven from Swiss Cottage", he mused on love, loneliness, and herrings in a mock Norwegian accent. Jokes included Sven's attempts to find his estranged wife, in which he often claimed to be telephoning the show from all over the world, and his hatred of the Norwegian obsession with fish. While Bull was clearly aware that Sven was fictional, he did not learn of the caller's real identity until later.

In late 1989, Cook married for the third time, to Malaysian-born property developer Chiew Lin Chong (1945–2016), in Torbay, Devon. She provided him with some stability in his personal life, and he reduced his drinking to the extent that for a time he was teetotal. He lived alone in a small 18th-century house in Perrins Walk, Hampstead, while she kept her own property just away.

Cook returned to the BBC as Sir Arthur Streeb-Greebling for an appearance with Ludovic Kennedy in "A Life in Pieces". The 12 interviews saw Sir Arthur recount his life, based on the song "Twelve Days of Christmas". Unscripted interviews with Cook as Streeb-Greebling and satirist Chris Morris were recorded in late 1993 and broadcast as "Why Bother?" on BBC Radio 3. Morris described them:

On 17 December 1993, Cook appeared on "Clive Anderson Talks Back" as four characters – biscuit tester and alien abductee Norman House, football manager and motivational speaker Alan Latchley, judge Sir James Beauchamp, and rock legend Eric Daley. The following day, he appeared on BBC2 performing links for "Arena"'s "Radio Night". He also appeared in the 1993 Christmas special of "One Foot in the Grave" ("One Foot in the Algarve"), playing a muckraking tabloid photographer. Before the end of the next year, his mother died, and a grief-stricken Cook returned to heavy drinking. He made his last television appearance on the show "Pebble Mill at One" in November 1994.

Cook was married three times. He was first married to Wendy Snowden, whom he met at university, in 1963; they had two daughters, Lucy and Daisy, but divorced in 1971. Cook then married his second wife, model and actress Judy Huxtable, in 1973, the marriage formally ending in 1989 after they had been separated for some years. He married his third and final wife, Chiew Lin Chong, in 1989, to whom he remained married until his death. Cook became stepfather to Chong's daughter, Nina. Chong died at the age of 71 in November 2016.

Cook died on 9 January 1995, aged 57 from a gastrointestinal hemorrhage, a complication likely the result of years of heavy drinking. His body was cremated at Golders Green Crematorium, and his ashes were buried in an unmarked plot behind St John-at-Hampstead, not far from his home in Perrins Walk.

Dudley Moore attended Cook's memorial service at St John-at-Hampstead on 1 May 1995. He and Martin Lewis presented a two-night memorial for Cook at The Improv in Los Angeles, on 15 and 16 November 1995, to mark what would have been Cook's 58th birthday.

Cook was an avid spectator of most sports and was a supporter of Tottenham Hotspur football club.

Cook is widely acknowledged as a strong influence on the many British comedians who followed him from the amateur dramatic clubs of British universities to the Edinburgh Festival Fringe, and then to radio and television. On his death, some critics choose to see Cook's life as tragic, insofar as the brilliance of his youth had not been sustained in his later years. However, Cook himself always maintained he had no ambitions at all for sustained success. He assessed happiness by his friendships and his enjoyment of life. Eric Idle said Cook had not wasted his talent, but rather that the newspapers had tried to waste him.

Several friends honoured him with a dedication in the closing credits of "Fierce Creatures" (1997), a comedy film written by John Cleese about a zoo in peril of being closed. It starred Cleese alongside Jamie Lee Curtis, Kevin Kline, and Michael Palin. The dedication displays photos and the lifespan dates of Cook and of British naturalist and humourist Gerald Durrell.

In 1999, the minor planet 20468 Petercook, in the main asteroid belt, was named after Cook.

Channel 4 broadcast "Not Only But Always", a television film dramatising the relationship between Cook and Moore, with Rhys Ifans portraying Cook. At the 2005 Edinburgh Festival Fringe, a play, "" written by Chris Bartlett and Nick Awde, examined the relationship from Moore's view. The play was transferred to London's West End at The Venue in 2006 and toured the UK the following year. During the West End run, Tom Goodman-Hill starred as Cook, with Kevin Bishop as Moore.

A green plaque to honour Cook was unveiled by the Westminster City Council and the Heritage Foundation at the site of the Establishment Club, at 18 Greek Street, on 15 February 2009 after an online campaign by satirist and event organiser Mark Biddiss, who also organised 'The World's 1st Peter Cook is Dead Birthday Party / Long overdue Public Wake' to promote the plaque, which featured a live reworking of Derek and Clive material, titled "Derek & Clive are Alive Again".

A blue plaque was unveiled by the Torbay Civic Society on 17 November 2014 at Cook's place of birth, "Shearbridge", Middle Warberry Road, Torquay, with his widow Lin and other members of the family in attendance. A further blue plaque was commissioned and erected at the home of Torquay United, Plainmoor, Torquay, in 2015.




UK chart singles:

Albums:




</doc>
<doc id="23549" url="https://en.wikipedia.org/wiki?curid=23549" title="Psychedelic rock">
Psychedelic rock

Psychedelic rock, also referred to as psychedelia, is a diverse style of rock music inspired, influenced, or representative of psychedelic culture, which is centred on perception-altering hallucinogenic drugs. The music is intended to replicate and enhance the mind-altering experiences of psychedelic drugs, most notably LSD. Many psychedelic groups differ in style, and the label is often applied spuriously.

Originating in the mid-1960s among British and American musicians, the sound of psychedelic rock invokes three core effects of LSD: depersonalization, dechronicization, and dynamization, all of which detach the user from reality. Musically, the effects may be represented via novelty studio tricks, electronic or non-Western instrumentation, disjunctive song structures, and extended instrumental segments. Some of the earlier 1960s psychedelic rock musicians were based in folk, jazz, and the blues, while others showcased an explicit Indian classical influence called "raga rock". In the 1960s, there existed two main variants of the genre: the more whimsical, surrealist British psychedelia and the harder American West Coast acid rock. While "acid rock" is sometimes deployed interchangeably with the term "psychedelic rock", it also refers more specifically to the heavier and more extreme ends of the genre.

The peak years of psychedelic rock were between 1967 and 1969, with milestone events including the 1967 Summer of Love and the 1969 Woodstock Rock Festival, becoming an international musical movement associated with a widespread counterculture before beginning a decline as changing attitudes, the loss of some key individuals, and a back-to-basics movement led surviving performers to move into new musical areas. The genre bridged the transition from early blues and folk-based rock to progressive rock and hard rock, and as a result contributed to the development of sub-genres such as heavy metal. Since the late 1970s it has been revived in various forms of neo-psychedelia. 

As a musical style, psychedelic rock attempts to replicate the effects of and enhance the mind-altering experiences of hallucinogenic drugs, incorporating new electronic sound effects and recording effects, extended solos, and improvisation. Common features include:

The term "psychedelic" was coined in 1956 by psychiatrist Humphry Osmond in a letter to LSD exponent Aldous Huxley and used as an alternative descriptor for hallucinogenic drugs in the context of psychedelic psychotherapy. As the countercultural scene developed in San Francisco, the terms acid rock and psychedelic rock were used in 1966 to describe the new drug-influenced music and were being widely used by 1967. The two terms are often used interchangeably, but acid rock may be distinguished as a more extreme variation that was heavier, louder, relied on long jams, focused more directly on LSD, and made greater use of distortion.

Music critic Richie Unterberger says that attempts to "pin down" the first psychedelic record are "nearly as elusive as trying to name the first rock & roll record". Some of the "far-fetched claims" include the instrumental "Telstar" (produced by Joe Meek for the Tornados in 1962) and the Dave Clark Five's "massively reverb-laden" "Any Way You Want It" (1964). The first mention of LSD on a rock record was the Gamblers' 1960 surf instrumental "LSD 25". A 1962 single by the Ventures, "The 2000 Pound Bee", issued forth the buzz of a distorted, "fuzztone" guitar, and the quest into "the possibilities of heavy, transistorised distortion" and other effects, like improved reverb and echo began in earnest on London's fertile rock 'n' roll scene. By 1964 fuzztone could be heard on singles by P.J. Proby, and the Beatles had employed feedback in "I Feel Fine", their sixth consecutive number 1 hit in the UK.

According to AllMusic, the emergence of psychedelic rock in the mid-1960s resulted from British groups who made up the British Invasion of the US market and folk rock bands seeking to broaden "the sonic possibilities of their music". Writing in his 1969 book "The Rock Revolution", Arnold Shaw said the genre in its American form represented generational escapism, which he identified as a development of youth culture's "protest against the sexual taboos, racism, violence, hypocrisy and materialism of adult life".

American folk singer Bob Dylan's influence was central to the creation of the folk rock movement in 1965, and his lyrics remained a touchstone for the psychedelic songwriters of the late 1960s. Virtuoso sitarist Ravi Shankar had begun in 1956 a mission to bring Indian classical music to the West, inspiring jazz, classical and folk musicians. By the mid-1960s, his influence extended to a generation of young rock musicians who soon made raga rock part of the psychedelic rock aesthetic and one of the many intersecting cultural motifs of the era. In the British folk scene, blues, drugs, jazz and Eastern influences blended in the early 1960s work of Davy Graham, who adopted modal guitar tunings to transpose Indian ragas and Celtic reels. Graham was highly influential on Scottish folk virtuoso Bert Jansch and other pioneering guitarists across a spectrum of styles and genres in the mid-1960s. Jazz saxophonist and composer John Coltrane had a similar impact, as the exotic sounds on his albums "My Favorite Things" (1960) and "A Love Supreme" (1964), the latter influenced by the ragas of Shankar, were source material for guitar players and others looking to improvise or "jam".

Barry Miles, a leading figure in the 1960s UK underground, says that "Hippies didn't just pop up overnight" and that "1965 was the first year in which a discernible youth movement began to emerge [in the US]. Many of the key 'psychedelic' rock bands formed this year." On the US West Coast, underground chemist Augustus Owsley Stanley III and Ken Kesey (along with his followers known as the Merry Pranksters) helped thousands of people take uncontrolled trips at Kesey's Acid Tests and in the new psychedelic dance halls. In Britain, Michael Hollingshead opened the World Psychedelic Centre and Beat Generation poets Allen Ginsberg, Lawrence Ferlinghetti and Gregory Corso read at the Royal Albert Hall. Miles adds: "The readings acted as a catalyst for underground activity in London, as people suddenly realized just how many like-minded people there were around. This was also the year that London began to blossom into colour with the opening of the Granny Takes a Trip and Hung On You clothes shops." Thanks to media coverage, use of LSD became widespread.

According to music critic Jim DeRogatis, writing in his book on psychedelic rock, "Turn on Your Mind", the Beatles are seen as the "Acid Apostles of the New Age". Producer George Martin, who was initially known as a specialist in comedy and novelty records, responded to the Beatles' requests by providing a range of studio tricks that ensured the group played a leading role in the development of psychedelic effects. Anticipating their overtly psychedelic work, "Ticket to Ride" (April 1965) introduced a subtle, drug-inspired drone suggestive of India, played on rhythm guitar. Musicologist William Echard writes that the Beatles employed several techniques in the years up to 1965 that soon became elements of psychedelic music, an approach he describes as "cognate" and reflective of how they, like the Yardbirds, were early pioneers in psychedelia. As important aspects the group brought to the genre, Echard cites the Beatles' rhythmic originality and unpredictability; "true" tonal ambiguity; leadership in incorporating elements from Indian music and studio techniques such as vari-speed, tape loops and reverse tape sounds; and their embrace of the avant-garde.

In Unterberger's opinion, the Byrds, emerging from the Los Angeles folk rock scene, and the Yardbirds, from England's blues scene, were more responsible than the Beatles for "sounding the psychedelic siren". Drug use and attempts at psychedelic music moved out of acoustic folk-based music towards rock soon after the Byrds, inspired by the Beatles' 1964 film "A Hard Day's Night", adopted electric instruments to produce a chart-topping version of Dylan's "Mr. Tambourine Man" in the summer of 1965. On the Yardbirds, Unterberger identifies lead guitarist Jeff Beck as having "laid the blueprint for psychedelic guitar", and says that their "ominous minor key melodies, hyperactive instrumental breaks (called rave-ups), unpredictable tempo changes, and use of Gregorian chants" helped to define the "manic eclecticism" typical of early psychedelic rock. The band's "Heart Full of Soul" (June 1965), which includes a distorted guitar riff that replicates the sound of a sitar, peaked at number 2 in the UK and number 9 in the US. In Echard's description, the song "carried the energy of a new scene" as the guitar-hero phenomenon emerged in rock, and it heralded the arrival of new Eastern sounds. The Kinks provided the first example of sustained Indian-style drone in rock when they used open-tuned guitars to mimic the tambura on "See My Friends" (July 1965), which became a top 10 hit in the UK.
The Beatles' "Norwegian Wood" from the December 1965 album "Rubber Soul" marked the first released recording on which a member of a Western rock group played the sitar. The song sparked a craze for the sitar and other Indian instrumentation – a trend that fueled the growth of raga rock as the India exotic became part of the essence of psychedelic rock. Music historian George Case recognises "Rubber Soul" as the first of two Beatles albums that "marked the authentic beginning of the psychedelic era", while music critic Robert Christgau similarly wrote that "Psychedelia starts here". San Francisco historian Charles Perry recalled the album being "the soundtrack of the Haight-Ashbury, Berkeley and the whole circuit", as pre-hippie youths suspected that the songs were inspired by drugs.
Although psychedelia was introduced in Los Angeles through the Byrds, according to Shaw, San Francisco emerged as the movement's capital on the West Coast. Several California-based folk acts followed the Byrds into folk rock, bringing their psychedelic influences with them, to produce the "San Francisco Sound". Music historian Simon Philo writes that although some commentators would state that the centre of influence had moved from London to California by 1967, it was British acts like the Beatles and the Rolling Stones that helped inspire and "nourish" the new American music in the mid-1960s, especially in the formative San Francisco scene. The music scene there developed in the city's Haight-Ashbury neighborhood in 1965 at basement shows organised by Chet Helms of the Family Dog; and as Jefferson Airplane founder Marty Balin and investors opened The Matrix nightclub that summer and began booking his and other local bands such as the Grateful Dead, the Steve Miller Band and Country Joe & the Fish. Helms and San Francisco Mime Troupe manager Bill Graham in the fall of 1965 organised larger scale multi-media community events/benefits featuring the Airplane, the Diggers and Allen Ginsberg. By early 1966 Graham had secured booking at The Fillmore, and Helms at the Avalon Ballroom, where in-house psychedelic-themed light shows replicated the visual effects of the psychedelic experience. Graham became a major figure in the growth of psychedelic rock, attracting most of the major psychedelic rock bands of the day to The Fillmore.

According to author Kevin McEneaney, the Grateful Dead "invented" acid rock in front of a crowd of concertgoers in San Jose, California on December 4, 1965, the date of the second Acid Test held by novelist Ken Kesey and the Merry Pranksters. Their stage performance involved the use of strobe lights to reproduce LSD's "surrealistic fragmenting" or "vivid isolating of caught moments". The Acid Test experiments subsequently launched the entire psychedelic subculture.

Echard writes that in 1966, "the psychedelic implications" advanced by recent rock experiments "became fully explicit and much more widely distributed", and by the end of the year, "most of the key elements of psychedelic topicality had been at least broached." DeRogatis says the start of psychedelic (or acid) rock is "best listed at 1966". Music journalists Pete Prown and Harvey P. Newquist locate the "peak years" of psychedelic rock between 1966 and 1969. In 1966, media coverage of rock music changed considerably as the music became reevaluated as a new form of art in tandem with the growing psychedelic community.

In February and March, two singles were released that later achieved recognition as the first psychedelic hits: the Yardbirds' "Shapes of Things" and the Byrds' "Eight Miles High". The former reached number 3 in the UK and number 11 in the US, and continued the Yardbirds' exploration of guitar effects, Eastern-sounding scales, and shifting rhythms. By overdubbing guitar parts, Beck layered multiple takes for his solo, which included extensive use of fuzz tone and harmonic feedback. The song's lyrics, which Unterberger describes as "stream-of-consciousness", have been interpreted as pro-environmental or anti-war. The Yardbirds became the first British band to have the term "psychedelic" applied to one of its songs. On "Eight Miles High", Roger McGuinn's 12-string Rickenbacker guitar provided a psychedelic interpretation of free jazz and Indian raga, channelling Coltrane and Shankar, respectively. The song's lyrics were widely taken to refer to drug use, although the Byrds denied it at the time. "Eight Miles High" peaked at number 14 in the US and reached the top 30 in the UK.

Contributing to psychedelia's emergence into the pop mainstream was the release of the Beach Boys' "Pet Sounds" (May 1966) and the Beatles' "Revolver" (August 1966). Often considered one of the earliest albums in the canon of psychedelic rock, "Pet Sounds" contained many elements that would be incorporated into psychedelia, with its artful experiments, psychedelic lyrics based on emotional longings and self-doubts, elaborate sound effects and new sounds on both conventional and unconventional instruments. The album track "I Just Wasn't Made for These Times" contained the first use of theremin sounds on a rock record. Scholar Philip Auslander says that even though psychedelic music is not normally associated with the Beach Boys, the "odd directions" and experiments in "Pet Sounds" "put it all on the map. ... basically that sort of opened the door — not for groups to be formed or to start to make music, but certainly to become as visible as say Jefferson Airplane or somebody like that."

DeRogatis views "Revolver" as another of "the first psychedelic rock masterpieces", along with "Pet Sounds". The Beatles' May 1966 B-side "Rain", recorded during the "Revolver" sessions, was the first pop recording to contain reversed sounds. Together with further studio tricks such as varispeed, the song includes a droning melody that reflected the band's growing interest in non-Western musical form and lyrics conveying the division between an enlightened psychedelic outlook and conformism. Philo cites "Rain" as "the birth of British psychedelic rock" and describes "Revolver" as "[the] most sustained deployment of Indian instruments, musical form and even religious philosophy" heard in popular music up to that time. Author Steve Turner recognises the Beatles' success in conveying an LSD-inspired worldview on "Revolver", particularly with "Tomorrow Never Knows", as having "opened the doors to psychedelic rock (or acid rock)". In author Shawn Levy's description, it was "the first true drug album, not [just] a pop record with some druggy insinuations", while musicologists Russell Reising and Jim LeBlanc credit the Beatles with "set[ting] the stage for an important subgenre of psychedelic music, that of the messianic pronouncement".

Echard highlights early records by the 13th Floor Elevators and Love among the key psychedelic releases of 1966, along with "Shapes of Things", "Eight Miles High", "Rain" and "Revolver". Originating from Austin, Texas, the first of these new bands came to the genre via the garage scene before releasing their debut album, "The Psychedelic Sounds of the 13th Floor Elevators" in December that year. It was the first rock album to include the adjective in its title, although the LP was released on an independent label and was little noticed at the time. Having formed in late 1965 with the aim of spreading LSD consciousness, the Elevators commissioned business cards containing an image of the third eye and the caption "Psychedelic rock". "Rolling Stone" highlights the 13th Floor Elevators as arguably "the most important early progenitors of psychedelic garage rock".

The Beach Boys' October 1966 single "Good Vibrations" was another early pop song to incorporate psychedelic lyrics and sounds. The single's success prompted an unexpected revival in theremins and increased the awareness of analog synthesizers. As psychedelia gained prominence, Beach Boys-style harmonies would be ingrained into the newer psychedelic pop.

In 1967, psychedelic rock received widespread media attention and a larger audience beyond local psychedelic communities. From 1967 to 1968, it was the prevailing sound of rock music, either in the more whimsical British variant, or the harder American West Coast acid rock. Music historian David Simonelli says the genre's commercial peak lasted "a brief year", with San Francisco and London recognised as the two key cultural centres. Compared with the American form, British psychedelic music was often more arty in its experimentation, and it tended to stick within pop song structures. Music journalist Mark Prendergast writes that it was only in US garage-band psychedelia that the often whimsical traits of UK psychedelic music were found. He says that aside from the work of the Byrds, Love and the Doors, there were three categories of US psychedelia: the "acid jams" of the San Francisco bands, who favoured albums over singles; pop psychedelia typified by groups such as the Beach Boys and Buffalo Springfield; and the "wigged-out" music of bands following in the example of the Beatles and the Yardbirds, such as the Electric Prunes, the Nazz, the Chocolate Watchband and the Seeds. 

In February 1967, the Beatles released the double A-side single "Strawberry Fields Forever" / "Penny Lane", which Ian MacDonald says launched both the "English pop-pastoral mood" typified by bands such as Pink Floyd, Family, Traffic and Fairport Convention, and English psychedelia's LSD-inspired preoccupation with "nostalgia for the innocent vision of a child". The Mellotron parts on "Strawberry Fields Forever" remain the most celebrated example of the instrument on a pop or rock recording. According to Simonelli, the two songs heralded the Beatles' brand of Romanticism as a central tenet of psychedelic rock. 

Jefferson Airplane's "Surrealistic Pillow" (February 1967) was one of the first albums to come out of San Francisco that sold well enough to bring national attention to the city's music scene. The LP tracks "White Rabbit" and "Somebody to Love" subsequently became top 10 hits in the US.

Pink Floyd's "Arnold Layne" (March 1967) and "See Emily Play" (June 1967), both written by Syd Barrett, helped set the pattern for pop-psychedelia in the UK. There, "underground" venues like the UFO Club, Middle Earth Club, The Roundhouse, the Country Club and the Art Lab drew capacity audiences with psychedelic rock and ground-breaking liquid light shows. A major figure in the development of British psychedelia was the American promoter and record producer Joe Boyd, who moved to London in 1966. He co-founded venues including the UFO Club, produced Pink Floyd's "Arnold Layne", and went on to manage folk and folk rock acts including Nick Drake, the Incredible String Band and Fairport Convention.

Psychedelic rock's popularity accelerated following the release of the Beatles' album "Sgt. Pepper's Lonely Hearts Club Band" (May 1967) and the staging of the Monterey Pop Festival in June. "Sgt. Pepper" was the first commercially successful work that critics recognised as a landmark aspect of psychedelia, and the Beatles' mass appeal meant that the record was played virtually everywhere. The album was highly influential on bands in the US psychedelic rock scene and its elevation of the LP format benefited the San Francisco bands. Among many changes brought about by its success, artists sought to imitate its psychedelic effects and devoted more time to creating their albums; the counterculture was scrutinised by musicians; and acts adopted its non-conformist sentiments.

The 1967 Summer of Love saw a huge number of young people from across America and the world travel to Haight-Ashbury, boosting the area's population from 15,000 to around 100,000. It was prefaced by the Human Be-In event in March and reached its peak at the Monterey Pop Festival in June, the latter helping to make major American stars of Janis Joplin, lead singer of Big Brother and the Holding Company, Jimi Hendrix, and the Who. Several established British acts joined the psychedelic revolution, including Eric Burdon (previously of the Animals) and the Who, whose "The Who Sell Out" (December 1967) included the psychedelic-influenced "I Can See for Miles" and "Armenia City in the Sky". The Incredible String Band's "The 5000 Spirits or the Layers of the Onion" (July 1967) developed their folk music into a pastoral form of psychedelia.

According to author Edward Macan, there ultimately existed three distinct branches of British psychedelic music. The first, dominated by Cream, the Yardbirds and Hendrix, was founded on a heavy, electric adaptation of the blues played by the Rolling Stones, adding elements such as the Who's power chord style and feedback. The second, considerably more complex form drew strongly from jazz sources and was typified by Traffic, Colosseum, If, and Canterbury scene bands such as Soft Machine and Caravan. The third branch, represented by the Moody Blues, Pink Floyd, Procol Harum and the Nice, was influenced by the later music of the Beatles. Several of the post-"Sgt. Pepper" English psychedelic groups developed the Beatles' classical influences further than either the Beatles or contemporaneous West Coast psychedelic bands. Among such groups, the Pretty Things abandoned their R&B roots to create "S.F. Sorrow" (December 1968), the first example of a psychedelic rock opera.

The US and UK were the major centres of psychedelic music, but in the late 1960s scenes began to develop across the world, including continental Europe, Australasia, Asia and south and Central America. In the later 1960s psychedelic scenes developed in a large number of countries in continental Europe, including the Netherlands with bands like The Outsiders, Denmark where it was pioneered by Steppeulvene, and Germany, where musicians began to fuse music of psychedelia and the electronic avant-garde. 1968 saw the first major German rock festival, the in Essen, and the foundation of the Zodiak Free Arts Lab in Berlin by Hans-Joachim Roedelius, and Conrad Schnitzler, which helped bands like Tangerine Dream and Amon Düül achieve cult status.

A thriving psychedelic music scene in Cambodia, influenced by psychedelic rock and soul broadcast by US forces radio in Vietnam, was pioneered by artists such as Sinn Sisamouth and Ros Serey Sothea. In South Korea, Shin Jung-Hyeon, often considered the godfather of Korean rock, played psychedelic-influenced music for the American soldiers stationed in the country. Following Shin Jung-Hyeon, the band San Ul Lim (Mountain Echo) often combined psychedelic rock with a more folk sound. In Turkey, Anatolian rock artist Erkin Koray blended classic Turkish music and Middle Eastern themes into his psychedelic-driven rock, helping to found the Turkish rock scene with artists such as Cem Karaca, Mogollar, Baris Manco and Erkin Koray.In Brasil the Tropicalia movement merged Brazilian and African rhythms with psychedelic rock. Musicians who were part of the movement include Caetano Veloso, Gilberto Gil, Os Mutantes, Gal Costa, Tom Zé, and the poet/lyricist Torquato Neto, all of whom participated in the 1968 album "", which served as a musical manifesto.

By the end of the 1960s, psychedelic rock was in retreat. Psychedelic trends climaxed in the 1969 Woodstock festival, which saw performances by most of the major psychedelic acts, including Jimi Hendrix, Jefferson Airplane, and the Grateful Dead. LSD had been made illegal in the UK in September 1966 and in California in October; by 1967, it was outlawed throughout the United States. In 1969, the murders of Sharon Tate and Leno and Rosemary LaBianca by Charles Manson and his cult of followers, claiming to have been inspired by Beatles' songs such as "Helter Skelter", has been seen as contributing to an anti-hippie backlash. At the end of the same year, the Altamont Free Concert in California, headlined by the Rolling Stones, became notorious for the fatal stabbing of black teenager Meredith Hunter by Hells Angel security guards.

Brian Wilson of the Beach Boys, Brian Jones of the Rolling Stones, Peter Green and Danny Kirwan of Fleetwood Mac and Syd Barrett of Pink Floyd were early "acid casualties", helping to shift the focus of the respective bands of which they had been leading figures. Some groups, such as the Jimi Hendrix Experience and Cream, broke up. Hendrix died in London in September 1970, shortly after recording "Band of Gypsys" (1970), Janis Joplin died of a heroin overdose in October 1970 and they were closely followed by Jim Morrison of the Doors, who died in Paris in July 1971. By this point, many surviving acts had moved away from psychedelia into either more back-to-basics "roots rock", traditional-based, pastoral or whimsical folk, the wider experimentation of progressive rock, or riff-based heavy rock. 

Following the lead of Hendrix in rock, psychedelia began to influence African American musicians, particularly the stars of the Motown label. This psychedelic soul was influenced by the civil rights movement, giving it a darker and more political edge than much psychedelic rock. Building on the funk sound of James Brown, it was pioneered from about 1968 by Sly and the Family Stone and The Temptations. Acts that followed them into this territory included Edwin Starr and the Undisputed Truth. George Clinton's interdependent Funkadelic and Parliament ensembles and their various spin-offs took the genre to its most extreme lengths making funk almost a religion in the 1970s, producing over forty singles, including three in the US top ten, and three platinum albums.

While psychedelic rock began to waver at the end of the 1960s, psychedelic soul continued into the 1970s, peaking in popularity in the early years of the decade, and only disappearing in the late 1970s as tastes began to change. Songwriter Norman Whitfield wrote psychedelic soul songs for The Temptations and Marvin Gaye.

Many of the British musicians and bands that had embraced psychedelia went on to create progressive rock in the 1970s, including Pink Floyd, Soft Machine and members of Yes. King Crimson's album "In the Court of the Crimson King" (1969) has been seen as an important link between psychedelia and progressive rock. While bands such as Hawkwind maintained an explicitly psychedelic course into the 1970s, most dropped the psychedelic elements in favour of wider experimentation. The incorporation of jazz into the music of bands like Soft Machine and Can also contributed to the development of the jazz rock of bands like Colosseum. As they moved away from their psychedelic roots and placed increasing emphasis on electronic experimentation, German bands like Kraftwerk, Tangerine Dream, Can and Faust developed a distinctive brand of electronic rock, known as kosmische musik, or in the British press as "Kraut rock". The adoption of electronic synthesisers, pioneered by Popol Vuh from 1970, together with the work of figures like Brian Eno (for a time the keyboard player with Roxy Music), would be a major influence on subsequent electronic rock.

The jam band movement, which began in the late 1980s, was influenced by the Grateful Dead's improvisational and psychedelic musical style. The Vermont band Phish developed a sizable and devoted fan following during the 1990s, and were described as "heirs" to the Grateful Dead after the death of Jerry Garcia in 1995.

Psychedelic rock, with its distorted guitar sound, extended solos and adventurous compositions, has been seen as an important bridge between blues-oriented rock and later heavy metal. American bands whose loud, repetitive psychedelic rock emerged as early heavy metal included the Amboy Dukes and Steppenwolf. From England, two former guitarists with the Yardbirds, Jeff Beck and Jimmy Page, moved on to form key acts in the genre, The Jeff Beck Group and Led Zeppelin respectively. Other major pioneers of the genre had begun as blues-based psychedelic bands, including Black Sabbath, Deep Purple, Judas Priest and UFO. Psychedelic music also contributed to the origins of glam rock, with Marc Bolan changing his psychedelic folk duo into rock band T. Rex and becoming the first glam rock star from 1970. From 1971 David Bowie moved on from his early psychedelic work to develop his Ziggy Stardust persona, incorporating elements of professional make up, mime and performance into his act.

Emerging in the 1990s, stoner rock combined elements of psychedelic rock and doom metal. Typically using a slow-to-mid tempo and featuring low-tuned guitars in a bass-heavy sound, with melodic vocals, and 'retro' production, it was pioneered by the Californian bands Kyuss and Sleep. Modern festivals focusing on psychedelic music include Austin Psych Fest in Texas, founded in 2008, Liverpool Psych Fest, and Desert Daze in Southern California.

There were occasional mainstream acts that dabbled in neo-psychedelia, a style of music which emerged in late 1970s post-punk circles. Although it has mainly been an influence on alternative and indie rock bands, neo-psychedelia sometimes updated the approach of 1960s psychedelic rock. In the US in the early 1980s it was joined by the Paisley Underground movement, based in Los Angeles and fronted by acts such as Dream Syndicate, the Bangles and Rain Parade.
Madchester was a music and cultural scene that developed in the Manchester area of North West England in the late 1980s, in which artists merged alternative rock with acid house and dance culture as well as other sources, including psychedelia and 1960s pop. The label was popularised by the British music press in the early 1990s, and its most famous groups include the Stone Roses, Happy Mondays, Inspiral Carpets, the Charlatans and 808 State. The rave-influenced scene is widely seen as heavily influenced by drugs, especially ecstasy (MDMA). At that time, the Haçienda nightclub, co-owned by members of New Order, was a major catalyst for the distinctive musical ethos in the city that was called the Second Summer of Love. "Screamadelica" is the third studio album by Scottish rock band Primal Scream released on 1991. The album marked a significant departure from the band's early indie rock sound, drawing inspiration from the blossoming house music scene and associated drugs such as LSD and MDMA. It won the first Mercury Music Prize in 1992, and has sold over three million copies worldwide.




</doc>
<doc id="23550" url="https://en.wikipedia.org/wiki?curid=23550" title="Philips">
Philips

Koninklijke Philips N.V. (literally "Royal Philips", stylized as PHILIPS) is a Dutch multinational conglomerate corporation headquartered in Amsterdam, formerly one of the largest electronics companies in the world, currently focused in the area of health technology, with other divisions being successfully divested. It was founded in Eindhoven in 1891 by Gerard Philips and his father Frederik, with their first products being light bulbs. It currently employs around 74,000 people across 100 countries. The company gained its royal honorary title in 1998 and dropped the "Electronics" in its name in 2013, due to its refocusing from consumer electronics to healthcare technology.

Philips is organized into three main divisions: Personal Health (formerly Philips Consumer Electronics and Philips Domestic Appliances and Personal Care), Connected Care, and Diagnosis & Treatment (formerly Philips Medical Systems). The lighting division was spun off as a separate company, Signify N.V. (formerly Philips Lighting prior to 2018). The company started making electric shavers in 1939 under the Philishave brand, and post-war they developed the Compact Cassette format and co-developed the Compact Disc format with Sony, as well as numerous other technologies. As of 2012, Philips was the largest manufacturer of lighting in the world as measured by applicable revenues.

Philips has a primary listing on the Euronext Amsterdam stock exchange and is a component of the Euro Stoxx 50 stock market index. It has a secondary listing on the New York Stock Exchange. Acquisitions include that of Signetics and Magnavox. They also have had a sports club since 1913 called PSV Eindhoven.

The Philips Company was founded in 1891, by Gerard Philips and his father Frederik Philips. Frederik, a banker based in Zaltbommel, financed the purchase and setup of an empty factory building in Eindhoven, where the company started the production of carbon-filament lamps and other electro-technical products in 1892. This first factory has since been adapted and is used as a museum.

In 1895, after a difficult first few years and near bankruptcy, the Philipses brought in Anton, Gerard's younger brother by sixteen years. Though he had earned a degree in engineering, Anton started work as a sales representative; soon, however, he began to contribute many important business ideas. With Anton's arrival, the family business began to expand rapidly, resulting in the founding of Philips Metaalgloeilampfabriek N.V. (Philips Metal Filament Lamp Factory Ltd.) in Eindhoven in 1908, followed in 1912, by the foundation of Philips Gloeilampenfabrieken N.V. (Philips Lightbulb Factories Ltd.). After Gerard and Anton Philips changed their family business by founding the Philips corporation, they laid the foundations for the later electronics multinational.

In the 1920s, the company started to manufacture other products, such as vacuum tubes. In 1939, they introduced their electric razor, the "Philishave" (marketed in the US using the Norelco brand name). The "Chapel" is a radio with built-in loudspeaker, which was designed during the early 1930s.

On 11 March 1927, Philips went on the air with shortwave radio station PCJJ (later PCJ) which was joined in 1929 by sister station PHOHI (Philips Omroep Holland-Indië). PHOHI broadcast in Dutch to the Dutch East Indies (now Indonesia) while PCJJ broadcast in English, Spanish and German to the rest of the world.

The international program on Sundays commenced in 1928, with host Eddie Startz hosting the "Happy Station" show, which became the world's longest-running shortwave program. Broadcasts from the Netherlands were interrupted by the German invasion in May 1940. The Germans commandeered the transmitters in Huizen to use for pro-Nazi broadcasts, some originating from Germany, others concerts from Dutch broadcasters under German control.

Philips Radio was absorbed shortly after liberation when its two shortwave stations were nationalised in 1947 and renamed Radio Netherlands Worldwide, the Dutch International Service. Some PCJ programs, such as "Happy Station", continued on the new station.

Philips was instrumental in the revival of the Stirling engine when, in the early 1930s, the management decided that offering a low-power portable generator would assist in expanding sales of its radios into parts of the world where mains electricity was unavailable and the supply of batteries uncertain. Engineers at the company's research lab carried out a systematic comparison of various power sources and determined that the almost forgotten Stirling engine would be most suitable, citing its quiet operation (both audibly and in terms of radio interference) and ability to run on a variety of heat sources (common lamp oil – "cheap and available everywhere" – was favored). They were also aware that, unlike steam and internal combustion engines, virtually no serious development work had been carried out on the Stirling engine for many years and asserted that modern materials and know-how should enable great improvements.

Encouraged by their first experimental engine, which produced 16 W of shaft power from a bore and stroke of , various development models were produced in a program which continued throughout World War II. By the late 1940s, the 'Type 10' was ready to be handed over to Philips's subsidiary Johan de Witt in Dordrecht to be produced and incorporated into a generator set as originally planned. The result, rated at 180/200 W electrical output from a bore and stroke of , was designated MP1002CA (known as the "Bungalow set"). Production of an initial batch of 250 began in 1951, but it became clear that they could not be made at a competitive price, besides with the advent of transistor radios with their much lower power requirements meant that the original rationale for the set was disappearing. Approximately 150 of these sets were eventually produced.

In parallel with the generator set, Philips developed experimental Stirling engines for a wide variety of applications and continued to work in the field until the late 1970s, though the only commercial success was the 'reversed Stirling engine' cryocooler. However, they filed a large number of patents and amassed a wealth of information, which they later licensed to other companies.

The first Philips shaver was introduced in the 1930s, and was simply called Philishave. In the US, it was called Norelco. The Philishave has remained part of the Philips product line-up until the present.

On 9 May 1940, the Philips directors learned that the German invasion of the Netherlands was to take place the following day. Having prepared for this, Anton Philips and his son in law Frans Otten, as well as other Philips family members, fled to the United States, taking a large amount of the company capital with them. Operating from the US as the North American Philips Company, they managed to run the company throughout the war. At the same time, the company was moved (on paper) to the Netherlands Antilles to keep it out of German hands.

On 6 December 1942, the British No. 2 Group RAF undertook Operation Oyster, which heavily damaged the Philips Radio factory in Eindhoven with few casualties among the Dutch workers and civilians. The Philips works in Eindhoven was bombed again by the RAF on 30 March 1943.

Frits Philips, the son of Anton, was the only Philips family member to stay in the Netherlands. He saved the lives of 382 Jews by convincing the Nazis that they were indispensable for the production process at Philips. In 1943, he was held at the internment camp for political prisoners at Vught for several months because a strike at his factory reduced production. For his actions in saving the hundreds of Jews, he was recognized by Yad Vashem in 1995 as a "Righteous Among the Nations".

After the war, the company was moved back to the Netherlands, with their headquarters in Eindhoven.
In 1949, the company began selling television sets. In 1950, it formed Philips Records, which eventually formed part of PolyGram in 1962.

Philips introduced the audio Compact Audio Cassette tape in 1963, and it was wildly successful. Compact cassettes were initially used for dictation machines for office typing stenographers and professional journalists. As their sound quality improved, cassettes would also be used to record sound and became the second mass media alongside vinyl records used to sell recorded music.
Philips introduced the first combination portable radio and cassette recorder, which was marketed as the "radiorecorder", and is now better known as the boom box. Later, the cassette was used in telephone answering machines, including a special form of cassette where the tape was wound on an endless loop. The C-cassette was used as the first mass storage device for early personal computers in the 1970s and 1980s. Philips reduced the cassette size for professional needs with the Mini-Cassette, although it would not be as successful as the Olympus Microcassette. This became the predominant dictation medium up to the advent of fully digital dictation machines. Philips continued with computers through the early 1990s (see separate article: Philips Computers).

In 1972, Philips launched the world's first home video cassette recorder, in the UK, the N1500. Its relatively bulky video cassettes could record 30 minutes or 45 minutes. Later one-hour tapes were also offered. As competition came from Sony's Betamax and the VHS group of manufacturers, Philips introduced the N1700 system which allowed double-length recording. For the first time, a 2-hour movie could fit onto one video cassette. In 1977, the company unveiled a special promotional film for this system in the UK, featuring comedy writer and presenter Denis Norden. The concept was quickly copied by the Japanese makers, whose tapes were significantly cheaper. Philips made one last attempt at a new standard for video recorders with the Video 2000 system, with tapes that could be used on both sides and had 8 hours of total recording time. As Philips only sold its systems on the PAL standard and in Europe, and the Japanese makers sold globally, the scale advantages of the Japanese proved insurmountable and Philips withdrew the V2000 system and joined the VHS Coalition.
Philips had developed a LaserDisc early on for selling movies, but delayed its commercial launch for fear of cannibalizing its video recorder sales. Later Philips joined with MCA to launch the first commercial LaserDisc standard and players. In 1982, Philips teamed with Sony to launch the Compact Disc; this format evolved into the CD-R, CD-RW, DVD and later Blu-ray, which Philips launched with Sony in 1997 and 2006 respectively.

In 1984, the Dutch Philips Group bought out nearly a one-third share and took over the management of German company Grundig.

In 1984, Philips split off its activities on the field of photolithographic integrated circuit production equipment, the so-called wafer steppers, into a joint venture with ASM International, located in Veldhoven under the name ASML. Over the years, this new company has evolved into the world's leading manufacturer of chip production machines at the expense of competitors like Nikon and Canon.

Philips partnered with Sony again later to develop a new "interactive" disc format called CD-i, described by them as a "new way of interacting with a television set". Philips created the majority of CD-i compatible players. After low sales, Philips repositioned the format as a video game console, but it was soon discontinued after being heavily criticized amongst the gaming community.

In the 1980s, Philips's profit margin dropped below 1 percent, and in 1990 the company lost more than US$2 billion (biggest corporate loss in Dutch history). Troubles for the company continued into the 1990s as its status as a leading electronics company was swiftly lost.

In 1991, the company's name was changed from N.V. Philips Gloeilampenfabrieken to Philips Electronics N.V. At the same time, North American Philips was formally dissolved, and a new corporate division was formed in the US with the name Philips Electronics North America Corp.

In 1997, the company officers decided to move the headquarters from Eindhoven to Amsterdam along with the corporate name change to Koninklijke Philips Electronics N.V., the latter of which was finalized on 16 March 1998.

In 1998, looking to spur innovation, Philips created an Emerging Businesses group for its Semiconductors unit, based in Silicon Valley. The group was designed to be an incubator where promising technologies and products could be developed.

The move of the headquarters to Amsterdam was completed in 2001. Initially, the company was housed in the Rembrandt Tower. In 2002, it moved again, this time to the Breitner Tower. Philips Lighting, Philips Research, Philips Semiconductors (spun off as NXP in September 2006) and Philips Design, are still based in Eindhoven. Philips Healthcare is headquartered in both Best, Netherlands (near Eindhoven) and Andover, Massachusetts, United States (near Boston).

In 2000, Philips bought Optiva Corporation, the maker of Sonicare electric toothbrushes. The company was renamed Philips Oral Healthcare and made a subsidiary of Philips DAP. In 2001, Philips acquired Agilent Technologies' Healthcare Solutions Group (HSG) for EUR 2 billion. Philips created a computer monitors joint venture with LG called LG.Philips Displays in 2001.

In 2001, after growing the unit's Emerging Businesses group to nearly $1 billion in revenue, Scott A. McGregor was named the new President and CEO of Philips Semiconductors. McGregor's appointment completed the company's shift to having dedicated CEOs for all five of the company's product divisions, which would in turn leave the Board of Management to concentrate on issues confronting the Philips Group as a whole.
In 2004, Philips abandoned the slogan "Let's make things better" in favour of a new one: "Sense and Simplicity".

In December 2005, Philips announced its intention to sell or demerge its semiconductor division. On 1 September 2006, it was announced in Berlin that the name of the new company formed by the division would be NXP Semiconductors. On 2 August 2006, Philips completed an agreement to sell a controlling 80.1% stake in NXP Semiconductors to a consortium of private equity investors consisting of Kohlberg Kravis Roberts & Co. (KKR), Silver Lake Partners and AlpInvest Partners. On 21 August 2006, Bain Capital and Apax Partners announced that they had signed definitive commitments to join the acquiring consortium, a process which was completed on 1 October 2006.

In 2006, Philips bought out the company Lifeline Systems headquartered in Framingham, Massachusetts, in a deal valued at $750 million, its biggest move yet to expand its consumer-health business (M). In August 2007, Philips acquired the company Ximis, Inc. headquartered in El Paso, Texas, for their Medical Informatics Division. In October 2007, it purchased a Moore Microprocessor Patent (MPP) Portfolio license from The TPL Group.

On 21 December 2007, Philips and Respironics, Inc. announced a definitive agreement pursuant to which Philips acquired all of the outstanding shares of Respironics for US$66 per share, or a total purchase price of approximately €3.6 billion (US$5.1 billion) in cash.

On 21 February 2008, Philips completed the acquisition of VISICU Baltimore, Maryland through the merger of its indirect wholly owned subsidiary into VISICU. As a result of that merger, VISICU has become an indirect wholly owned subsidiary of Philips. VISICU was the creator of the eICU concept of the use of Telemedicine from a centralized facility to monitor and care for ICU patients.

The Philips physics laboratory was scaled down in the early 21st century, as the company ceased trying to be innovative in consumer electronics through fundamental research.

In January 2011, Philips agreed to acquire the assets of Preethi, a leading India-based kitchen appliances company. On 27 June 2011, Philips acquired Sectra Mamea AB, the mammography division of Sectra AB.

Because net profit slumped 85 percent in Q3 2011, Philips announced a cut of 4,500 jobs to match part of an €800 million ($1.1 billion) cost-cutting scheme to boosts profits and meet its financial target. In 2011, the company posted a loss of €1.3 billion, but earned a net profit in Q1 and Q2 2012, however the management wanted €1.1 billion cost-cutting which was an increase from €800 million and may cut another 2,200 jobs until end of 2014.
In March 2012, Philips announced its intention to sell, or demerge its television manufacturing operations to TPV Technology.

Following two decades in decline, Philips went through major restructuring, shifting its focus from electronics to healthcare. Particularly from 2011 when a new CEO was appointed, Frans van Houten. The new health and medical strategy has helped Philips to thrive again in the 2010s.

On 5 December 2012, the antitrust regulators of the European Union fined Philips and several other major companies for fixing prices of TV cathode-ray tubes in two cartels lasting nearly a decade.

On 29 January 2013, it was announced that Philips had agreed to sell its audio and video operations to the Japan-based Funai Electric for €150 million, with the audio business planned to transfer to Funai in the latter half of 2013, and the video business in 2017. As part of the transaction, Funai was to pay a regular licensing fee to Philips for the use of the Philips brand. The purchase agreement was terminated by Philips in October because of breach of contract and the consumer electronics operations remain under Philips. Philips said it would seek damages for breach of contract in the US$200-million sale. In April 2016, the International Court of Arbitration ruled in favour of Philips, awarding compensation of €135 million in the process.

In April 2013, Philips announced a collaboration with Paradox Engineering for the realization and implementation of a "pilot project" on network-connected street-lighting management solutions. This project was endorsed by the San Francisco Public Utilities Commission (SFPUC).

In 2013, Philips removed the word "Electronics" from its name – becoming Royal Philips N.V. On 13 November 2013, Philips unveiled its new brand line "Innovation and You" and a new design of its shield mark. The new brand positioning is cited by Philips to signify company's evolution and emphasize that innovation is only meaningful if it is based on an understanding of people's needs and desires.

On 28 April 2014, Philips agreed to sell their Woox Innovations subsidiary (consumer electronics) to Gibson Brands for $US135 million. On 23 September 2014, Philips announced a plan to split the company into two, separating the lighting business from the healthcare and consumer lifestyle divisions. It moved to complete this in March 2015 to an investment group for $3.3 billion.

In February 2015, Philips acquired Volcano Corporation to strengthen its position in non-invasive surgery and imaging. In June 2016, Philips spun off its lighting division to focus on the healthcare division. In June 2017, Philips announced it would acquire US-based Spectranetics Corp, a manufacturer of devices to treat heart disease, for €1.9 billion (£1.68 billion) expanding its current image-guided therapy business.

In May 2016, Philips' lighting division Philips Lighting went through a spin-off process, and became an independent public company named Philips Lighting N.V.

In 2017, Philips launched Philips Ventures, with a health technology venture fund as its main focus. Philips Ventures invested in companies including Mytonomy (2017) and DEARhealth (2019).

In 2018, the independent Philips Lighting N.V. was renamed Signify N.V. However, it continues to produce and market Philips-branded products such as Philips Hue color-changing LED light bulbs.

Past and present CEOs:

CEOs lighting:

Past and present CFOs (chief financial officer)

In January 2013, Hugo Barbosa Vazquez was hired to own market analysis and forecasting activities across all Philips businesses (Consumer Lifestyle, Healthcare and Lighting) and geographies.

Companies acquired by Philips through the years include ADAC Laboratories, Agilent Healthcare Solutions Group, Amperex, ATL Ultrasound, EKCO, Lifeline Systems, Magnavox, Marconi Medical Systems, Philips Medical purchased Intermagnetics based out of Latham, New York for 1.3 billion in 2006, Optiva, Preethi, Pye, Respironics, Inc., Sectra Mamea AB, Signetics, VISICU, Volcano, VLSI, Ximis, portions of Westinghouse and the consumer electronics operations of Philco and Sylvania. Philips abandoned the Sylvania trademark which is now owned by Havells Sylvania except in Australia, Canada, Mexico, New Zealand, Puerto Rico and the US where it is owned by Osram. Formed in November 1999 as an equal joint venture between Philips and Agilent Technologies, the light-emitting diode manufacturer Lumileds became a subsidiary of Phillips Lighting in August 2005 and a fully owned subsidiary in December 2006. An 80.1 percent stake in Lumileds was sold to Apollo Global Management in 2017.

Philips is registered in the Netherlands as a naamloze vennootschap and has its global headquarters in Amsterdam. At the end of 2013, Philips had 111 manufacturing facilities, 59 R&D Facilities across 26 countries and sales and service operations in around 100 countries.

Philips is organized into three main divisions: Philips Consumer Lifestyle (formerly Philips Consumer Electronics and Philips Domestic Appliances and Personal Care), Philips Healthcare (formerly Philips Medical Systems) and Philips Lighting (Former). Philips achieved total revenues of €22.579 billion in 2011, of which €8.852 billion were generated by Philips Healthcare, €7.638 billion by Philips Lighting, €5.823 billion by Philips Consumer Lifestyle and €266 million from group activities. At the end of 2011, Philips had a total of 121,888 employees, of whom around 44% were employed in Philips Lighting, 31% in Philips Healthcare and 15% in Philips Consumer Lifestyle. The lighting division was spun out as a new company called Signify, which uses the Philips brand under license.

Philips invested a total of €1.61 billion in research and development in 2011, equivalent to 7.10% of sales. Philips Intellectual Property and Standards is the group-wide division responsible for licensing, trademark protection and patenting. Philips currently holds around 54,000 patent rights, 39,000 trademarks, 70,000 design rights and 4,400 domain name registrations.

Philips Thailand was established in 1952. It is a subsidiary which produces healthcare, lifestyle and lighting products. Philips started manufacturing in Thailand in 1960 with an incandescent lamp factory. Philips has diversified its production facilities to include a fluorescent lamp factory and a luminaries factory, serving Thai and worldwide markets.

Philips Hong Kong began operation in 1948. Philips Hong Kong houses the global headquarters of Philips' Audio Business Unit. It also house Philips' Asia Pacific regional office and headquarters for its Design Division, Domestic Appliances & Personal Care Products Division, Lighting Products Division and Medical System Products Division.

In 1974, Philips opened a lamp factory in Hong Kong. This has a capacity of 200 million pieces a year and is certified with ISO 9001:2000 and ISO 14001. Its product portfolio includes prefocus, lensend and E10 miniature light bulbs.

Philips established in Zhuhai, Guangdong, in 1990. The site mainly manufactures Philishaves and healthcare products. In early 2008, Philips Lighting, a division of Royal Philips Electronics, opened a small engineering center in Shanghai to adapt the company's products to vehicles in Asia.

Philips began operations in India in 1930, with the establishment of Philips Electrical Co. (India) Pvt Ltd in Kolkata as a sales outlet for imported Philips lamps. In 1938, Philips established its first Indian lamp-manufacturing factory in Kolkata. In 1948, Philips started manufacturing radios in Kolkata. In 1959, a second radio factory was established near Pune. This was closed and sold around 2006. In 1957, the company converted into a public limited company, renamed "Philips India Ltd". In 1970, a new consumer electronics factory began operations in Pimpri near Pune. This is now called the 'Philips Healthcare Innovation Centre'. Also, a manufacturing facility 'Philips Centre for Manufacturing Excellence' was set up in Chakan, Pune in 2012. In 1996, the Philips Software Centre was established in Bangalore, later renamed the Philips Innovation Campus. In 2008, Philips India entered the water purifier market. In 2014, Philips was ranked 12th among India's most trusted brands according to the Brand Trust Report, a study conducted by Trust Research Advisory.
Now Philips working in India as one of the most diversified health care company & broadly focusing on Imaging, Utlrasound, MA & TC products & Sleep & respiratory care products. Philips is aspiring to touch life of 40 Million patients in India by next 2 years. In 2020, Philips introduced mobile ICUs in order to support clinicians to meet the rising demand of ICU beds due to the COVID-19 pandemic.

Philips has been active in Israel since 1948 and in 1998, set up a wholly owned subsidiary, Philips Electronics (Israel) Ltd. The company has over 700 employees in Israel and generated sales of over $300 million in 2007.

Philips Medical Systems Technologies Ltd. (Haifa) is a developer and manufacturer of Computerized Tomography (CT), diagnostic and Medical Imaging systems. The company was founded in 1969 as Elscint by Elron Electronic Industries and was acquired by Marconi Medical Systems in 1998, which was itself acquired by Philips in 2001.

Philips Semiconductors formerly had major operations in Israel; these now form part of NXP Semiconductors.

On 1 August 2019, Philips acquired Carestream HCIS division from Onex Corporation. As part of the acquisition, Algotec Systems LTD (Carestream HCIS R&D) located in Raanana Israel changed ownership in a share deal. In addition to that, Algotec changed its name to Philips Algotec and is part of Philips HCIS. Philips HCIS is a provider of medical imaging systems.

Philips has been active in Pakistan since 1948 and has a wholly owned subsidiary, Philips Pakistan Limited (Formerly Philips Electrical Industries of Pakistan Limited).

The head office is in Karachi with regional sales offices in Lahore and Rawalpindi.

Philips France has its headquarters in Suresnes. The company employs over 3600 people nationwide.

Philips Lighting has manufacturing facilities in Chalon-sur-Saône (fluorescent lamps), Chartres (automotive lighting), Lamotte-Beuvron (architectural lighting by LEDs and professional indoor lighting), Longvic (lamps), Miribel (outdoor lighting), Nevers (professional indoor lighting).

Philips Germany was founded in 1926 in Berlin. Now its headquarters is located in Hamburg. Over 4900 people are employed in Germany.


Philips' Greece is headquartered in Halandri, Attica. As of 2012, Philips has no manufacturing plants in Greece, although previously there have been audio, lighting and telecommunications factories.

Philips founded its Italian headquarter in 1918, basing it in Monza (Milan) where it still operates, for commercial activities only.

Philips founded PACH (Philips Assembly Centre Hungary) in 1992, producing televisions and consumer electronics in Székesfehérvár. After TPV entering the Philips TV business, the factory was moved under TP Vision, the new joint-venture company in 2011. Products have been transferred to Poland and China and factory was closed in 2013.

By Philips acquiring PLI in 2007 another Hungarian Philips factory emerged in Tamási, producing lamps under the name of Philips IPSC Tamási, later Philips Lighting. The factory was renamed to Signify in 2017, still producing Philips lighting products.

Philips' operations in Poland include: a European financial and accounting centre in Łódź; Philips Lighting facilities in Bielsko-Biała, Piła, and Kętrzyn; and a Philips Domestic Appliances facility in Białystok.

Philips started business in Portugal in 1927, as "Philips Portuguesa S.A.R.L.". Currently, Philips Portuguesa S.A. is headquartered in Oeiras near Lisbon. There were three Philips factories in Portugal: the FAPAE lamp factory in Lisbon; the Carnaxide magnetic-core memory factory near Lisbon, where the Philips Service organization was also based; and the Ovar factory in northern Portugal making camera components and remote control devices. The company still operates in Portugal with divisions for commercial lighting, medical systems and domestic appliances.

Philips Sweden has two main sites, Kista, Stockholm County, with regional sales, marketing and a customer support organization and Solna, Stockholm County, with the main office of the mammography division.

Philips UK has its headquarters in Guildford. The company employs over 2500 people nationwide.


In the past, Philips UK also included:

Philips Canada was founded in 1941. It is well known in medical systems for diagnosis and therapy, lighting technologies, shavers, and consumer electronics.

The Canadian headquarters are located in Markham, Ontario.

For several years, Philips manufactured lighting products in two Canadian factories. The London, Ontario, plant opened in 1971. It produced A19 lamps (including the "Royale" long life bulbs), PAR38 lamps and T19 lamps (originally a Westinghouse lamp shape). Philips closed the factory in May 2003. The Trois-Rivières, Quebec plant was a Westinghouse facility which Philips continued to run it after buying Westinghouse's lamp division in 1983. Philips closed this factory a few years later, in the late 1980s.

Philips Mexicana SA de CV is headquartered in Mexico City. Philips Lighting has manufacturing facilities in: Monterrey, Nuevo León; Ciudad Juárez, Chihuahua; and Tijuana, Baja California. Philips Consumer Electronics has a manufacturing facility in Ciudad Juárez, Chihuahua. Philips Domestic Appliances formerly operated a large factory in the Industrial Vallejo sector of Mexico City but this was closed in 2004.

Philips' Electronics North American headquarters is in Andover, Massachusetts. In early 2018, it was announced that the US headquarters would move to Cambridge, Massachusetts by 2020. Philips Lighting has its corporate office in Somerset, New Jersey, with manufacturing plants in Danville, Kentucky, Dallas, Salina, Kansas, and Paris, Texas and distribution centers in Mountain Top, Pennsylvania, El Paso, Texas, Ontario, California, and Memphis, Tennessee. Philips Healthcare is headquartered in Cambridge, Massachusetts, and operates a health-tech hub in Nashville, Tennessee, with over 1000 jobs. The North American sales organization is based in Bothell, Washington. There are also manufacturing facilities in Andover, Massachusetts; Bothell, Washington; Baltimore, Maryland; Cleveland, Ohio; Foster City, California; Gainesville, Florida; Milpitas, California; and Reedsville, Pennsylvania. Philips Healthcare also formerly had a factory in Knoxville, Tennessee. Philips Consumer Lifestyle has its corporate office in Stamford, Connecticut. Philips Lighting has a Color Kinetics office in Burlington, Massachusetts. Philips Research North American headquarters is in Cambridge, Massachusetts.

In 2007, Philips entered into a definitive merger agreement with North American luminaires company Genlyte Group Incorporated, which provides the company with a leading position in the North American luminaires (also known as "lighting fixtures"), controls and related products for a wide variety of applications, including solid state lighting. The company also acquired Respironics, which was a significant gain for its healthcare sector. On 21 February 2008, Philips completed the acquisition of VISICU Baltimore, Maryland. VISICU was the creator of the eICU concept of the use of Telemedicine from a centralized facility to monitor and care for ICU patients.

Philips Australia was founded in 1927 and is headquartered in North Ryde, New South Wales and also manages the New Zealand operation from there. The company currently employs around 800 people. Regional sales and support offices are located in Melbourne, Brisbane, Adelaide, Perth and Auckland.

Current activities include: Philips Healthcare (also responsible for New Zealand operations); Philips Lighting (also responsible for New Zealand operations); Phillips Oral Healthcare, Phillips Professional Dictation Solutions, Phillips Professional Display Solutions, Phillips AVENT Professional, Philips Consumer Lifestyle (also responsible for New Zealand operations); Philips Sleep & Respiratory Care (formerly Respironics), with its ever-increasing national network of Sleepeasy Centres ; Philips Dynalite (Lighting Control systems, acquired in 2009, global design and manufacturing centre) and Philips Selecon NZ (Lighting Entertainment product design and manufacture).

Philips do Brasil () was founded in 1924 in Rio de Janeiro. In 1929, Philips started to sell radio receivers. In the 1930s, Philips was making its light bulbs and radio receivers in Brazil. From 1939 to 1945, World War II forced Brazilian branch of Philips to sell bicycles, refrigerators and insecticides. After the war, Philips had a great industrial expansion in Brazil, and was among the first groups to establish in Manaus Free Zone. In the 1970s, Philips Records was a major player in Brazil recording industry. Nowadays, Philips do Brasil is one of the largest foreign-owned companies in Brazil. Philips uses the brand Walita for domestic appliances in Brazil.

Colour television was introduced in South America by then CEO, Cor Dillen.

Philips subsidiary Philips-Duphar(nl) manufactured pharmaceuticals for human and veterinary use and products for crop protection. Duphar was sold to Solvay in 1990. In subsequent years, Solvay sold off all divisions to other companies (crop protection to UniRoyal, now Chemtura, the veterinary division to Fort Dodge, a division of Wyeth, and the pharmaceutical division to Abbott Laboratories).

PolyGram, Philips' music television and movies division, was sold to Seagram in 1998; merged into Universal Music Group. Philips Records continues to operate as record label of UMG, its name licensed from its former parent.

In 1980 Philips acquired Marantz, a company renowned for high-end audio and video products, based at Kanagawa, Japan. In 2002 Marantz Japan merged with Denon to form D&M Holdings and Philips sold its remaining stake in D&M Holdings in 2008. 

Origin, now part of Atos Origin, is a former division of Philips.

ASM Lithography is a spin-off from a division of Philips.

Hollandse Signaalapparaten was a manufacturer of military electronics. The business was sold to Thomson-CSF in 1990 and is now Thales Nederland.

NXP Semiconductors, formerly known as Philips Semiconductors, was sold a consortium of private equity investors in 2006. On 6 August 2010, NXP completed its IPO, with shares trading on NASDAQ.

Ignis, of Comerio, in the province of Varese, Italy, produced washing machines, dishwashers and microwave ovens, was one of the leading companies in the domestic appliance market, holding a 38% share in 1960. In 1970, 50% of the company's capital was taken over by Philips, which acquired full control in 1972. Ignis was in those years, after Zanussi, the second largest domestic appliance manufacturer, and in 1973 its factories numbered over 10,000 employees only in Italy. With the transfer of ownership to the Dutch multinational, the corporate name of the company was changed, which became "IRE SpA" ("Industrie Riunite Eurodomestici"). Thereafter Philips used to sell major household appliances (whitegoods) under the name Philips. After selling the Major Domestic Appliances division to Whirlpool Corporation it changed from Philips Whirlpool to Whirlpool Philips and finally to just Whirlpool. Whirlpool bought a 53% stake in Philips' major appliance operations to form Whirlpool International. Whirlpool bought Philips' remaining interest in Whirlpool International in 1991.

Philips Cryogenics was split off in 1990 to form the Stirling Cryogenics BV, Netherlands. This company is still active in the development and manufacturing of Stirling cryocoolers and cryogenic cooling systems.

North American Philips distributed AKG Acoustics products under the AKG of America, Philips Audio/Video, Norelco and AKG Acoustics Inc. branding until AKG set up its North American division in San Leandro, California, in 1985. (AKG's North American division has since moved to Northridge, California.)

Polymer Vision was a Philips spin-off that manufactured a flexible e-ink display screen. The company was acquired by Taiwanese contract electronics manufacturer Wistron in 2009 and it was shut down in 2012, after repeated failed attempts to find a potential buyer.

Philips' core products are consumer electronics and electrical products (including small domestic appliances, shavers, beauty appliances, mother and childcare appliances, electric toothbrushes and coffee makers (products like Smart Phones, audio equipment, Blu-ray players, computer accessories and televisions are sold under license)); and healthcare products (including CT scanners, ECG equipment, mammography equipment, monitoring equipment, MRI scanners, radiography equipment, resuscitation equipment, ultrasound equipment and X-ray equipment);

In January 2020 Phillips announced that it is looking to sell its domestic appliances division, which includes products like coffee machines, air purifiers and airfryers.



Philips healthcare products include:







In 1913, in celebration of the 100th anniversary of the liberation of the Netherlands, Philips founded "Philips Sports Vereniging" (Philips Sports Club, now commonly known as PSV). The club is active in numerous sports but is now best known for its football team, PSV Eindhoven, and swimming team. Philips owns the naming rights to Philips Stadion in Eindhoven, which is the home ground of PSV Eindhoven.

Outside of the Netherlands, Philips sponsors and has sponsored numerous sports clubs, sports facilities and events. In November 2008, Philips renewed and extended its F1 partnership with AT&T Williams. Philips owns the naming rights to the "Philips Championship", the premier basketball league in Australia, traditionally known as the National Basketball League. From 1988 to 1993, Philips was the principal sponsor of the Australian rugby league team The Balmain Tigers and Indonesian football club side Persiba Balikpapan. From 1998 to 2000, Philips sponsored the Winston Cup No. 7 entry for Geoff Bodine Racing, later Ultra Motorsports, for drivers Geoff Bodine and Michael Waltrip. From 1999 to 2018, Philips held the naming rights to Philips Arena in Atlanta, home of the Atlanta Hawks of the National Basketball Association and former home of the defunct Atlanta Thrashers of the National Hockey League.

Outside of sports, Philips sponsors the international "Philips Monsters of Rock festival".

Philips and its CEO, Frans van Houten, hold several global leadership positions in advancing the circular economy, including as a founding member and co-chair of the board of directors for the Platform for Accelerating the Circular Economy (PACE), applying circular approaches in its capital equipment business, and as a global partner of the Ellen MacArthur Foundation.

Philips also runs the EcoVision initiative, which commits to a number of environmentally positive improvements, focusing on energy efficiency.

Also, Philips marks its "green" products with the Philips Green Logo, identifying them as products that have a significantly better environmental performance than their competitors or predecessors.

In 2011, Philips won a $10 million cash prize from the US Department of Energy for winning its L-Prize competition, to produce a high-efficiency, long operating life replacement for a standard 60-W incandescent lightbulb. The winning LED lightbulb, which was made available to consumers in April 2012, produces slightly more than 900 lumens at an input power of only 10 W.

In Greenpeace's 2012 Guide to Greener Electronics that ranks electronics manufacturers on sustainability, climate and energy and how green their products are, Philips ranks 10th place with a score of 3.8/10. The company was the top scorer in the Energy section due to its energy advocacy work calling upon the EU to adopt a 30% reduction for greenhouse gas emissions by 2020. It is also praised for its new products which are free from PVC plastic and BFRs. However, the guide criticizes Phillips' sourcing of fibres for paper, arguing it must develop a paper procurement policy which excludes suppliers involved in deforestation and illegal logging.

Philips have made some considerable progress since 2007 (when it was first ranked in this guide), in particular by supporting the Individual Producer Responsibility principle, which means that the company is accepting the responsibility for the toxic impacts of its products on e-waste dumps around the world.



</doc>
<doc id="23551" url="https://en.wikipedia.org/wiki?curid=23551" title="Perciformes">
Perciformes

Perciformes, also called the Percomorpha or Acanthopteri, is an order or superorder of ray-finned fish. If considered a single order, they are the most numerous order of vertebrates, containing about 41% of all bony fish. Perciformes means "perch-like". This group comprises over 10,000 species found in almost all aquatic ecosystems.

The order contains about 160 families, which is the most of any order within the vertebrates. It is also the most variably sized order of vertebrates, ranging from the 7-mm (1/4-in) "Schindleria brevipinguis" to the 5-m (16.4 ft) marlin in the genus "Makaira". They first appeared and diversified in the Late Cretaceous.

Among the well-known members of this group are perch and darters (Percidae), sea bass and groupers (Serranidae).

The dorsal and anal fins are divided into anterior spiny and posterior soft-rayed portions, which may be partially or completely separated. The pelvic fins usually have one spine and up to five soft rays, positioned unusually far forward under the chin or under the belly. Scales are usually ctenoid (rough to the touch), although sometimes they are cycloid (smooth to the touch) or otherwise modified.

Classification of this group is controversial. As traditionally defined before the introduction of cladistics, the Perciformes are almost certainly paraphyletic. Other orders that should possibly be included as suborders are the Scorpaeniformes, Tetraodontiformes, and Pleuronectiformes.
Of the presently recognized suborders, several may be paraphyletic, as well. These are grouped by suborder/superfamily, generally following the text "Fishes of the World".


</doc>
<doc id="23553" url="https://en.wikipedia.org/wiki?curid=23553" title="Asimina">
Asimina

Asimina is a genus of small trees or shrubs described as a genus in 1763.

"Asimina" has large simple leaves and large fruit. It is native to eastern North America and collectively referred to as pawpaw. The genus includes the widespread common pawpaw "Asimina triloba," which bears the largest edible fruit indigenous to the continent. Pawpaws are native to 26 states of the U.S. and to Ontario in Canada. The common pawpaw is a patch-forming (clonal) understory tree found in well-drained, deep, fertile bottomland and hilly upland habitat. Pawpaws are in the same plant family (Annonaceae) as the custard-apple, cherimoya, sweetsop, soursop, and ylang-ylang; the genus is the only member of that family not confined to the tropics.

The genus name "Asimina" was first described and named by Michel Adanson, a French naturalist of Scottish descent. The name is adapted from the Native American name "assimin" through the French colonial "asiminier."

The common name pawpaw, also spelled paw paw, paw-paw, and papaw, probably derives from the Spanish "papaya", perhaps because of the superficial similarity of their fruits.

Pawpaws are shrubs or small trees to tall. The northern, cold-tolerant common pawpaw ("Asimina triloba") is deciduous, while the southern species are often evergreen.

The leaves are alternate, obovate, entire, long and broad.

The flowers of pawpaws are produced singly or in clusters of up to eight together; they are large, 4–6 cm across, perfect, with six sepals and petals (three large outer petals, three smaller inner petals). The petal color varies from white to purple or red-brown.

The fruit of the common pawpaw is a large edible berry, long and broad, weighing from , with numerous seeds; it is green when unripe, maturing to yellow or brown. It has a flavor somewhat similar to both banana and mango, varying significantly by cultivar, and has more protein than most fruits.


The common pawpaw is native to shady, rich bottom lands, where it often forms a dense undergrowth in the forest, often appearing as a patch or thicket of individual small slender trees.

Pawpaw flowers are insect-pollinated, but fruit production is limited since few if any pollinators are attracted to the flower's faint, or sometimes non-existent scent. The flowers produce an odor similar to that of rotting meat to attract blowflies or carrion beetles for cross pollination. Other insects that are attracted to pawpaw plants include scavenging fruit flies, carrion flies and beetles. Because of difficult pollination, some believe the flowers are self-incompatible.

Pawpaw fruit may be eaten by foxes, opossums, squirrels and raccoons. However, pawpaw leaves and twigs are seldom consumed by rabbits or deer.

The leaves, twigs, and bark of the common pawpaw tree contain natural insecticides known as acetogenins.

Larvae of the zebra swallowtail butterfly feed exclusively on young leaves of the various pawpaw species, but never occur in great numbers on the plants.

The paw paw in considered an evolutionary anachronism, where a now-extinct evolutionary partner, such as a Pleistocene megafauna species, formerly consumed the fruit and assisted in seed dispersal.

Wild-collected fruits of the common pawpaw ("Asimina triloba") have long been a favorite treat throughout the tree's extensive native range in eastern North America. Fresh pawpaw fruits are commonly eaten raw; however, they do not store or ship well unless frozen. The fruit pulp is also often used locally in baked dessert recipes, with pawpaw often substituted in many banana-based recipes.

Pawpaws have never been cultivated for fruit on the scale of apples and peaches, but interest in pawpaw cultivation has increased in recent decades. However, only frozen fruit will store or ship well. Other methods of preservation include dehydration, production of jams or jellies, and pressure canning.

The pawpaw is also gaining in popularity among backyard gardeners because of the tree's distinctive growth habit, the appeal of its fresh fruit, and its relatively low maintenance needs once established. The common pawpaw is also of interest in ecological restoration plantings since this tree grows well in wet soil and has a strong tendency to form well-rooted clonal thickets.

The several other species of "Asimina" have few economic uses.

The earliest documentation of pawpaws is in the 1541 report of the Spanish de Soto expedition, who found Native Americans cultivating it east of the Mississippi River. Chilled pawpaw fruit was a favorite dessert of George Washington, and Thomas Jefferson planted it at his home in Virginia, Monticello. The Lewis and Clark Expedition sometimes subsisted on pawpaws during their travels. Daniel Boone was also a consumer and fan of the pawpaw. The common pawpaw was designated as the Ohio state native fruit in 2009.


 


</doc>
<doc id="23555" url="https://en.wikipedia.org/wiki?curid=23555" title="Pentecostalism">
Pentecostalism

Pentecostalism or Classical Pentecostalism is a Protestant Christian movement that emphasises direct personal experience of God through baptism with the Holy Spirit. The term "Pentecostal" is derived from Pentecost, an event that commemorates the descent of the Holy Ghost upon the followers of Jesus Christ, and the speaking in "foreign" tongues as described in the second chapter of the Acts of the Apostles. In the Greek it is name for the Jewish Feast of Weeks. 

Like other forms of evangelical Protestantism, Pentecostalism adheres to the inerrancy of the Bible and the necessity of accepting Jesus Christ as personal Lord and Savior. It is distinguished by belief in the baptism in the Holy Spirit that enables a Christian to live a Spirit-filled and empowered life. This empowerment includes the use of spiritual gifts such as speaking in tongues and divine healing—two other defining characteristics of Pentecostalism. Because of their commitment to biblical authority, spiritual gifts, and the miraculous, Pentecostals tend to see their movement as reflecting the same kind of spiritual power and teachings that were found in the Apostolic Age of the early church. For this reason, some Pentecostals also use the term "Apostolic" or "Full Gospel" to describe their movement.

Pentecostalism emerged in the early 20th century among radical adherents of the Holiness movement who were energized by revivalism and expectation for the imminent Second Coming of Christ. Believing that they were living in the end times, they expected God to spiritually renew the Christian Church thereby bringing to pass the restoration of spiritual gifts and the evangelization of the world. In 1900, Charles Parham, an American evangelist and faith healer, began teaching that speaking in tongues was the Bible evidence of Spirit baptism and along with William J. Seymour, a Wesleyan-Holiness preacher, he taught that this was the third work of grace. The three-year-long Azusa Street Revival, founded and led by Seymour in Los Angeles, California, resulted in the spread of Pentecostalism throughout the United States and the rest of the world as visitors carried the Pentecostal experience back to their home churches or felt called to the mission field. While virtually all Pentecostal denominations trace their origins to Azusa Street, the movement has experienced a variety of divisions and controversies. An early dispute centered on challenges to the doctrine of the Trinity. As a result, the Pentecostal movement is divided between trinitarian and non-trinitarian branches, resulting in the emergence of Oneness Pentecostals.

Comprising over 700 denominations and many independent churches, there is no central authority governing Pentecostalism; however, many denominations are affiliated with the Pentecostal World Fellowship. There are over 279 million Pentecostals worldwide, and the movement is growing in many parts of the world, especially the global South. Since the 1960s, Pentecostalism has increasingly gained acceptance from other Christian traditions, and Pentecostal beliefs concerning Spirit baptism and spiritual gifts have been embraced by non-Pentecostal Christians in Protestant and Catholic churches through the Charismatic Movement. Together, Pentecostal and Charismatic Christianity numbers over 500 million adherents. While the movement originally attracted mostly lower classes in the global South, there is an increasing appeal to middle classes. Middle class congregations tend to be more adapted to society and withdraw strong spiritual practices such as divine healing.

Pentecostalism is an evangelical faith, emphasizing the reliability of the Bible and the need for the transformation of an individual's life through faith in Jesus. Like other evangelicals, Pentecostals generally adhere to the Bible's divine inspiration and inerrancy—the belief that the Bible, in the original manuscripts in which it was written, is without error. Pentecostals emphasize the teaching of the "full gospel" or "foursquare gospel". The term "foursquare" refers to the four fundamental beliefs of Pentecostalism: Jesus saves according to ; baptizes with the Holy Spirit according to Acts 2:4; heals bodily according to James 5:15; and is coming again to receive those who are saved according to 1 Thessalonians 4:16–17.

The central belief of classical Pentecostalism is that through the death, burial, and resurrection of Jesus Christ, sins can be forgiven and humanity reconciled with God. This is the Gospel or "good news". The fundamental requirement of Pentecostalism is that one be born again. The new birth is received by the grace of God through faith in Christ as Lord and Savior. In being born again, the believer is regenerated, justified, adopted into the family of God, and the Holy Spirit's work of sanctification is initiated.

Classical Pentecostal soteriology is generally Arminian rather than Calvinist. The security of the believer is a doctrine held within Pentecostalism; nevertheless, this security is conditional upon continual faith and repentance. Pentecostals believe in both a literal heaven and hell, the former for those who have accepted God's gift of salvation and the latter for those who have rejected it.

For most Pentecostals there is no other requirement to receive salvation. Baptism with the Holy Spirit and speaking in tongues are not generally required, though Pentecostal converts are usually encouraged to seek these experiences. A notable exception is Jesus' Name Pentecostalism, most adherents of which believe both water baptism and Spirit baptism are integral components of salvation.

Pentecostals identify three distinct uses of the word "baptism" in the New Testament:


While the figure of Jesus Christ and his redemptive work are at the center of Pentecostal theology, that redemptive work is believed to provide for a fullness of the Holy Spirit of which believers in Christ may take advantage. The majority of Pentecostals believe that at the moment a person is born again, the new believer has the presence (indwelling) of the Holy Spirit. While the Spirit "dwells" in every Christian, Pentecostals believe that all Christians should seek to be "filled" with him. The Spirit's "filling", "falling upon", "coming upon", or being "poured out upon" believers is called the baptism with the Holy Spirit. Pentecostals define it as a definite experience occurring after salvation whereby the Holy Spirit comes upon the believer to anoint and empower him or her for special service. It has also been described as "a baptism into the love of God".

The main purpose of the experience is to grant power for Christian service. Other purposes include power for spiritual warfare (the Christian struggles against spiritual enemies and thus requires spiritual power), power for overflow (the believer's experience of the presence and power of God in his or her life flows out into the lives of others), and power for ability (to follow divine direction, to face persecution, to exercise spiritual gifts for the edification of the church, etc.).

Pentecostals believe that the baptism with the Holy Spirit is available to all Christians. Repentance from sin and being born again are fundamental requirements to receive it. There must also be in the believer a deep conviction of needing more of God in his or her life, and a measure of consecration by which the believer yields himself or herself to the will of God. Citing instances in the Book of Acts where believers were Spirit baptized before they were baptized with water, most Pentecostals believe a Christian need not have been baptized in water to receive Spirit baptism. However, Pentecostals do believe that the biblical pattern is "repentance, regeneration, water baptism, and then the baptism with the Holy Ghost". There are Pentecostal believers who have claimed to receive their baptism with the Holy Spirit while being water baptized.

It is received by having faith in God's promise to fill the believer and in yielding the entire being to Christ. Certain conditions, if present in a believer's life, could cause delay in receiving Spirit baptism, such as "weak faith, unholy living, imperfect consecration, and egocentric motives". In the absence of these, Pentecostals teach that seekers should maintain a persistent faith in the knowledge that God will fulfill his promise. For Pentecostals, there is no prescribed manner in which a believer will be filled with the Spirit. It could be expected or unexpected, during public or private prayer.

Pentecostals expect certain results following baptism with the Holy Spirit. Some of these are immediate while others are enduring or permanent. Most Pentecostal denominations teach that speaking in tongues is an immediate or initial physical evidence that one has received the experience. Some teach that any of the gifts of the Spirit can be evidence of having received Spirit baptism. Other immediate evidences include giving God praise, having joy, and desiring to testify about Jesus. Enduring or permanent results in the believer's life include Christ glorified and revealed in a greater way, a "deeper passion for souls", greater power to witness to nonbelievers, a more effective prayer life, greater love for and insight into the Bible, and the manifestation of the gifts of the Spirit.

Pentecostals, with their background in the Holiness movement, historically teach that baptism with the Holy Spirit, as evidenced by glossolalia, is the third work of grace, which follows the new birth (first work of grace) and entire sanctification (second work of grace).

While the baptism with the Holy Spirit is a definite experience in a believer's life, Pentecostals view it as just the beginning of living a Spirit-filled life. Pentecostal teaching stresses the importance of continually being filled with the Spirit. There is only one baptism with the Spirit, but there should be many infillings with the Spirit throughout the believer's life.

Pentecostalism is a holistic faith, and the belief that Jesus is Healer is one quarter of the full gospel. Pentecostals cite four major reasons for believing in divine healing: 1) it is reported in the Bible, 2) Jesus' healing ministry is included in his atonement (thus divine healing is part of salvation), 3) "the whole gospel is for the whole person"—spirit, soul, and body, 4) sickness is a consequence of the Fall of Man and salvation is ultimately the restoration of the fallen world. In the words of Pentecostal scholar Vernon L. Purdy, "Because sin leads to human suffering, it was only natural for the Early Church to understand the ministry of Christ as the alleviation of human suffering, since he was God's answer to sin ... The restoration of fellowship with God is the most important thing, but this restoration not only results in spiritual healing but many times in physical healing as well." In the book "In Pursuit of Wholeness: Experiencing God's Salvation for the Total Person", Pentecostal writer and Church historian Wilfred Graves, Jr. describes the healing of the body as a physical expression of salvation.

For Pentecostals, spiritual and physical healing serves as a reminder and testimony to Christ's future return when his people will be completely delivered from all the consequences of the fall. However, not everyone receives healing when they pray. It is God in his sovereign wisdom who either grants or withholds healing. Common reasons that are given in answer to the question as to why all are not healed include: God teaches through suffering, healing is not always immediate, lack of faith on the part of the person needing healing, and personal sin in one's life (however, this does not mean that all illness is caused by personal sin). Regarding healing and prayer Purdy states:

Pentecostals believe that prayer and faith are central in receiving healing. Pentecostals look to scriptures such as James 5:13–16 for direction regarding healing prayer. One can pray for one's own healing (verse 13) and for the healing of others (verse 16); no special gift or clerical status is necessary. Verses 14–16 supply the framework for congregational healing prayer. The sick person expresses his or her faith by calling for the elders of the church who pray over and anoint the sick with olive oil. The oil is a symbol of the Holy Spirit.

Besides prayer, there are other ways in which Pentecostals believe healing can be received. One way is based on Mark 16:17–18 and involves believers laying hands on the sick. This is done in imitation of Jesus who often healed in this manner. Another method that is found in some Pentecostal churches is based on the account in Acts 19:11–12 where people were healed when given handkerchiefs or aprons worn by the Apostle Paul. This practice is described by Duffield and Van Cleave in "Foundations of Pentecostal Theology":

During the initial decades of the movement, Pentecostals thought it was sinful to take medicine or receive care from doctors. Over time, Pentecostals moderated their views concerning medicine and doctor visits; however, a minority of Pentecostal churches continues to rely exclusively on prayer and divine healing. For example, doctors in the United Kingdom reported that a minority of Pentecostal HIV patients were encouraged to stop taking their medicines and parents were told to stop giving medicine to their children, trends that placed lives at risk.

The last element of the gospel is that Jesus is the "Soon Coming King". For Pentecostals, "every moment is eschatological" since at any time Christ may return. This "personal and imminent" Second Coming is for Pentecostals the motivation for practical Christian living including: personal holiness, meeting together for worship, faithful Christian service, and evangelism (both personal and worldwide). Globally, Pentecostal attitudes to the End Times range from enthusiastic participation in the prophecy subculture to a complete lack of interest through to the more recent, optimistic belief in the coming restoration of God's kingdom.

Historically, however, they have been premillennial dispensationalists believing in a pretribulation rapture. Pre-tribulation rapture theology was popularized extensively in the 1830s by John Nelson Darby, and further popularized in the United States in the early 20th century by the wide circulation of the Scofield Reference Bible.

Pentecostals are continuationists, meaning they believe that all of the spiritual gifts, including the miraculous or "sign gifts", found in 1 Corinthians 12:4–11, 12:27–31, Romans 12:3–8, and Ephesians 4:7–16 continue to operate within the Church in the present time. Pentecostals place the gifts of the Spirit in context with the fruit of the Spirit. The fruit of the Spirit is the result of the new birth and continuing to abide in Christ. It is by the fruit exhibited that spiritual character is assessed. Spiritual gifts are received as a result of the baptism with the Holy Spirit. As gifts freely given by the Holy Spirit, they cannot be earned or merited, and they are not appropriate criteria with which to evaluate one's spiritual life or maturity. Pentecostals see in the biblical writings of Paul an emphasis on having both character and power, exercising the gifts in love.

Just as fruit should be evident in the life of every Christian, Pentecostals believe that every Spirit-filled believer is given some capacity for the manifestation of the Spirit. It is important to note that the exercise of a gift is a manifestation of the Spirit, not of the gifted person, and though the gifts operate through people, they are primarily gifts given to the Church. They are valuable only when they minister spiritual profit and edification to the body of Christ. Pentecostal writers point out that the lists of spiritual gifts in the New Testament do not seem to be exhaustive. It is generally believed that there are as many gifts as there are useful ministries and functions in the Church. A spiritual gift is often exercised in partnership with another gift. For example, in a Pentecostal church service, the gift of tongues might be exercised followed by the operation of the gift of interpretation.

According to Pentecostals, all manifestations of the Spirit are to be judged by the church. This is made possible, in part, by the gift of discerning of spirits, which is the capacity for discerning the source of a spiritual manifestation—whether from the Holy Spirit, an evil spirit, or from the human spirit. While Pentecostals believe in the current operation of all the spiritual gifts within the church, their teaching on some of these gifts has generated more controversy and interest than others. There are different ways in which the gifts have been grouped. W. R. Jones suggests three categories, illumination (Word of Wisdom, word of knowledge, discerning of spirits), action (Faith, working of miracles and gifts of healings) and communication (Prophecy, tongues and interpretation of tongues). Duffield and Van Cleave use two categories: the vocal and the power gifts.

The gifts of prophecy, tongues, interpretation of tongues, and words of wisdom and knowledge are called the vocal gifts. Pentecostals look to 1 Corinthians 14 for instructions on the proper use of the spiritual gifts, especially the vocal ones. Pentecostals believe that prophecy is the vocal gift of preference, a view derived from 1 Corinthians 14. Some teach that the gift of tongues is equal to the gift of prophecy when tongues are interpreted. Prophetic and glossolalic utterances are not to replace the preaching of the Word of God nor to be considered as equal to or superseding the written Word of God, which is the final authority for determining teaching and doctrine.

Pentecostals understand the word of wisdom and the word of knowledge to be supernatural revelations of wisdom and knowledge by the Holy Spirit. The word of wisdom is defined as a revelation of the Holy Spirit that applies scriptural wisdom to a specific situation that a Christian community faces. The word of knowledge is often defined as the ability of one person to know what God is currently doing or intends to do in the life of another person.

Pentecostals agree with the Protestant principle of "sola Scriptura". The Bible is the "all sufficient rule for faith and practice"; it is "fixed, finished, and objective revelation". Alongside this high regard for the authority of scripture is a belief that the gift of prophecy continues to operate within the Church. Pentecostal theologians Duffield and van Cleave described the gift of prophecy in the following manner: "Normally, in the operation of the gift of prophecy, the Spirit heavily anoints the believer to speak forth to the body not premeditated words, but words the Spirit supplies spontaneously in order to uplift and encourage, incite to faithful obedience and service, and to bring comfort and consolation."

Any Spirit-filled Christian, according to Pentecostal theology, has the potential, as with all the gifts, to prophesy. Sometimes, prophecy can overlap with preaching "where great unpremeditated truth or application is provided by the Spirit, or where special revelation is given beforehand in prayer and is empowered in the delivery".

While a prophetic utterance at times might foretell future events, this is not the primary purpose of Pentecostal prophecy and is never to be used for personal guidance. For Pentecostals, prophetic utterances are fallible, i.e. subject to error. Pentecostals teach that believers must discern whether the utterance has edifying value for themselves and the local church. Because prophecies are subject to the judgement and discernment of other Christians, most Pentecostals teach that prophetic utterances should never be spoken in the first person (e.g. "I, the Lord") but always in the third person (e.g. "Thus saith the Lord" or "The Lord would have...").

A Pentecostal believer in a spiritual experience may vocalize fluent, unintelligible utterances (glossolalia) or articulate a natural language previously unknown to them (xenoglossy). Commonly termed "speaking in tongues", this vocal phenomenon is believed by Pentecostals to include an endless variety of languages. According to Pentecostal theology, the language spoken (1) may be an unlearned human language, such as the Bible claims happened on the Day of Pentecost, or (2) it might be of heavenly (angelic) origin. In the first case, tongues could work as a sign by which witness is given to the unsaved. In the second case, tongues are used for praise and prayer when the mind is superseded and "the speaker in tongues speaks to God, speaks mysteries, and ... no one understands him".

Within Pentecostalism, there is a belief that speaking in tongues serves two functions. Tongues as the "initial evidence" of the third work of grace, baptism with the Holy Spirit, and in individual prayer serves a different purpose than tongues as a spiritual gift. All Spirit-filled believers, according to initial evidence proponents, will speak in tongues when baptized in the Spirit and, thereafter, will be able to express prayer and praise to God in an unknown tongue. This type of tongue speaking forms an important part of many Pentecostals' personal daily devotions. When used in this way, it is referred to as a "prayer language" as the believer is speaking unknown languages not for the purpose of communicating with others but for "communication between the soul and God". Its purpose is for the spiritual edification of the individual. Pentecostals believe the private use of tongues in prayer (i.e. "prayer in the Spirit") "promotes a deepening of the prayer life and the spiritual development of the personality". From Romans 8:26–27, Pentecostals believe that the Spirit intercedes for believers through tongues; in other words, when a believer prays in an unknown tongue, the Holy Spirit is supernaturally directing the believer's prayer.

Besides acting as a prayer language, tongues also function as the "gift of tongues". Not all Spirit-filled believers possess the gift of tongues. Its purpose is for gifted persons to publicly "speak with God in praise, to pray or sing in the Spirit, or to speak forth in the congregation". There is a division among Pentecostals on the relationship between the gifts of tongues and prophecy. One school of thought believes that the gift of tongues is always directed from man to God, in which case it is always prayer or praise spoken to God but in the hearing of the entire congregation for encouragement and consolation. Another school of thought believes that the gift of tongues can be prophetic, in which case the believer delivers a "message in tongues"—a prophetic utterance given under the influence of the Holy Spirit—to a congregation.

Whether prophetic or not, however, Pentecostals are agreed that all public utterances in an unknown tongue must be interpreted in the language of the gathered Christians. This is accomplished by the gift of interpretation, and this gift can be exercised by the same individual who first delivered the message (if he or she possesses the gift of interpretation) or by another individual who possesses the required gift. If a person with the gift of tongues is not sure that a person with the gift of interpretation is present and is unable to interpret the utterance him or herself, then the person should not speak. Pentecostals teach that those with the gift of tongues should pray for the gift of interpretation. Pentecostals do not require that an interpretation be a literal word-for-word translation of a glossolalic utterance. Rather, as the word "interpretation" implies, Pentecostals expect only an accurate explanation of the utterance's meaning.

Besides the gift of tongues, Pentecostals may also use glossolalia as a form of praise and worship in corporate settings. Pentecostals in a church service may pray aloud in tongues while others pray simultaneously in the common language of the gathered Christians. This use of glossolalia is seen as an acceptable form of prayer and therefore requires no interpretation. Congregations may also corporately sing in tongues, a phenomenon known as singing in the Spirit.

Speaking in tongues is not universal among Pentecostal Christians. In 2006, a ten-country survey by the Pew Forum on Religion and Public Life found that 49 percent of Pentecostals in the US, 50 percent in Brazil, 41 percent in South Africa, and 54 percent in India said they "never" speak or pray in tongues.

The gifts of power are distinct from the vocal gifts in that they do not involve utterance. Included in this category are the gift of faith, gifts of healing, and the gift of miracles. The gift of faith (sometimes called "special" faith) is different from "saving faith" and normal Christian faith in its degree and application. This type of faith is a manifestation of the Spirit granted only to certain individuals "in times of special crisis or opportunity" and endues them with "a divine certainty ... that triumphs over everything". It is sometimes called the "faith of miracles" and is fundamental to the operation of the other two power gifts.

During the 1910s, the Pentecostal movement split over the nature of the Godhead into two camps – Trinitarian and Apostolic (as they called themselves) or Oneness. The Oneness doctrine viewed the doctrine of the Trinity as polytheistic.

The majority of Pentecostal denominations believe in the doctrine of the Trinity, which is considered by them to be Christian orthodoxy. Oneness Pentecostals are nontrinitarian Christians, believing in the Oneness theology about God.

In Oneness theology, the Godhead is not three persons united by one substance, but one person who reveals himself as three different modes. Thus, God manifests himself as Father within creation, he becomes Son by virtue of his incarnation as Jesus Christ, and he becomes the Holy Spirit by way of his activity in the life of the believer. The Oneness doctrine may be considered a form of Modalism, an ancient teaching considered heresy by most Christians. In contrast, Trinitarian Pentecostals hold to the traditional doctrine of the Trinity, that is, the Godhead is not seen as simply three modes or titles of God manifest at different points in history, but is composed of three completely distinct persons who are co-eternal with each other and united as one substance. The Son is from all eternity who became incarnate as Jesus, and likewise the Holy Spirit is from all eternity, and both are with the eternal Father from all eternity.

Traditional Pentecostal worship has been described as a "gestalt made up of prayer, singing, sermon, the operation of the gifts of the Spirit, altar intercession, offering, announcements, testimonies, musical specials, Scripture reading, and occasionally the Lord's supper". Russell P. Spittler identified five values that govern Pentecostal spirituality. The first was individual experience, which emphasizes the Holy Spirit's personal work in the life of the believer. Second was orality, a feature that might explain Pentecostalism's success in evangelizing nonliterate cultures. The third was spontaneity; members of Pentecostal congregations are expected to follow the leading of the Holy Spirit, sometimes resulting in unpredictable services. The fourth value governing Pentecostal spirituality was "otherworldliness" or asceticism, which was partly informed by Pentecostal eschatology. The final and fifth value was a commitment to biblical authority, and many of the distinctive practices of Pentecostals are derived from a literal reading of scripture.

Spontaneity is a characteristic element of Pentecostal worship. This was especially true in the movement's earlier history, when anyone could initiate a song, chorus, or spiritual gift. Even as Pentecostalism has become more organized and formal, with more control exerted over services, the concept of spontaneity has retained an important place within the movement and continues to inform stereotypical imagery, such as the derogatory "holy roller". The phrase "Quench not the Spirit", derived from 1 Thessalonians 5:19, is used commonly and captures the thought behind Pentecostal spontaneity.

Prayer plays an important role in Pentecostal worship. Collective oral prayer, whether glossolalic or in the vernacular or a mix of both, is common. While praying, individuals may lay hands on a person in need of prayer, or they may raise their hands in response to biblical commands (1 Timothy 2:8). The raising of hands (which itself is a revival of the ancient orans posture) is an example of some Pentecostal worship practices that have been widely adopted by the larger Christian world. Pentecostal musical and liturgical practice have also played an influential role in shaping contemporary worship trends, with Pentecostal churches such as Hillsong Church being the leading producers of congregational music.

Several spontaneous practices have become characteristic of Pentecostal worship. Being "slain in the Spirit" or "falling under the power" is a form of prostration in which a person falls backwards, as if fainting, while being prayed over. It is at times accompanied by glossolalic prayer; at other times, the person is silent. It is believed by Pentecostals to be caused by "an overwhelming experience of the presence of God", and Pentecostals sometimes receive the baptism in the Holy Spirit in this posture. Another spontaneous practice is "dancing in the Spirit". This is when a person leaves their seat "spontaneously 'dancing' with eyes closed without bumping into nearby persons or objects". It is explained as the worshipper becoming "so enraptured with God's presence that the Spirit takes control of physical motions as well as the spiritual and emotional being". Pentecostals derive biblical precedent for dancing in worship from 2 Samuel 6, where David danced before the Lord. A similar occurrence is often called "running the aisles". The "Jericho march" (inspired by Book of Joshua 6:1–27) is a celebratory practice occurring at times of high enthusiasm. Members of a congregation began to spontaneously leave their seats and walk in the aisles inviting other members as they go. Eventually, a full column is formed around the perimeter of the meeting space as worshipers march with singing and loud shouts of praise and jubilation. Another spontaneous manifestation found in some Pentecostal churches is holy laughter, in which worshippers uncontrollably laugh. In some Pentecostal churches, these spontaneous expressions are primarily found in revival meetings or special prayer meetings, being rare or non-existent in the main services.

Like other Christian churches, Pentecostals believe that certain rituals or ceremonies were instituted as a pattern and command by Jesus in the New Testament. Pentecostals commonly call these ceremonies ordinances. Many Christians call these sacraments, but this term is not generally used by Pentecostals and certain other Protestants as they do not see ordinances as imparting grace. Instead the term sacerdotal ordinance is used to denote the distinctive belief that grace is received directly from God by the congregant with the officiant serving only to facilitate rather than acting as a conduit or vicar.

The ordinance of water baptism is an outward symbol of an inner conversion that has already taken place. Therefore, most Pentecostal groups practice believer's baptism by immersion. The majority of Pentecostals do not view baptism as essential for salvation, and likewise, most Pentecostals are Trinitarian and use the traditional Trinitarian baptismal formula. However, Oneness Pentecostals view baptism as an essential and necessary part of the salvation experience and, as non-Trinitarians, reject the use of the traditional baptismal formula. For more information on Oneness Pentecostal baptismal beliefs, see the following section on Statistics and denominations.

The ordinance of Holy Communion, or the Lord's Supper, is seen as a direct command given by Jesus at the Last Supper, to be done in remembrance of him. Pentecostal denominations reject the use of wine as part of communion, using grape juice instead.

Foot washing is also held as an ordinance by some Pentecostals. It is considered an "ordinance of humility" because Jesus showed humility when washing his disciples' feet in John 13:14–17. Other Pentecostals do not consider it an ordinance; however, they may still recognize spiritual value in the practice.

In 1995, David Barrett estimated there were 217 million "Denominational Pentecostals" throughout the world. In 2011, a Pew Forum study of global Christianity found that there were an estimated 279 million classical Pentecostals, making 4 percent of the total world population and 12.8 percent of the world's Christian population Pentecostal. The study found "Historically Pentecostal denominations" (a category that did not include independent Pentecostal churches) to be the largest Protestant denominational family.

The largest percentage of Pentecostals are found in Sub-Saharan Africa (44 percent), followed by the Americas (37 percent) and Asia and the Pacific (16 percent). The movement is enjoying its greatest surge today in the global South, which includes Africa, Latin America, and most of Asia. There are 740 recognized Pentecostal denominations, but the movement also has a significant number of independent churches that are not organized into denominations.

Among the over 700 Pentecostal denominations, 240 are classified as part of Wesleyan, Holiness, or "Methodistic" Pentecostalism. Until 1910, Pentecostalism was universally Wesleyan in doctrine, and Holiness Pentecostalism continues to predominate in the Southern United States. Wesleyan Pentecostals teach that there are three crisis experiences within a Christian's life: conversion, sanctification, and Spirit baptism. They inherited the holiness movement's belief in entire sanctification. According to Wesleyan Pentecostals, entire sanctification is a definite event that occurs after salvation but before Spirit baptism. This inward experience cleanses and enables the believer to live a life of outward holiness. This personal cleansing prepares the believer to receive the baptism in the Holy Spirit. Holiness Pentecostal denominations include the Church of God in Christ, Church of God (Cleveland, Tennessee), and the Pentecostal Holiness Church.

After William H. Durham began preaching his Finished Work doctrine in 1910, many Pentecostals rejected the Wesleyan doctrine of entire sanctification and began to teach that there were only two definite crisis experiences in the life of a Christian: conversion and Spirit baptism. These Finished Work Pentecostals (also known as "Baptistic" or "Reformed" Pentecostals because many converts were originally drawn from Baptist and Presbyterian backgrounds) teach that a person is initially sanctified at the moment of conversion. After conversion, the believer grows in grace through a lifelong process of progressive sanctification. There are 390 denominations that adhere to the finished work position. They include the Assemblies of God, the Foursquare Gospel Church, and the Open Bible Churches.

The 1904–1905 Welsh Revival laid the foundation for British Pentecostalism and especially for a distinct family of denominations known as Apostolic Pentecostalism (not to be confused with Oneness Pentecostalism). These Pentecostals are led by a hierarchy of living apostles, prophets, and other charismatic offices. Apostolic Pentecostals are found worldwide in 30 denominations, including the Apostolic Church based in the United Kingdom.

There are 80 Pentecostal denominations that are classified as Jesus' Name or Oneness Pentecostalism (often self identifying as "Apostolic Pentecostals"). These differ from the rest of Pentecostalism in several significant ways. Oneness Pentecostals reject the doctrine of the Trinity. They do not describe God as three persons but rather as three manifestations of the one living God. Oneness Pentecostals practice Jesus' Name Baptism—water baptisms performed in the name of Jesus Christ, rather than that of the Trinity. Oneness Pentecostal adherents believe repentance, baptism in Jesus' name, and Spirit baptism are all essential elements of the conversion experience. Oneness Pentecostals hold that repentance is necessary before baptism to make the ordinance valid, and receipt of the Holy Spirit manifested by speaking in other tongues is necessary afterwards, to complete the work of baptism. This differs from other Pentecostals, along with evangelical Christians in general, who see only repentance and faith in Christ as essential to salvation. This has resulted in Oneness believers being accused by some (including other Pentecostals) of a "works-salvation" soteriology, a charge they vehemently deny. Oneness Pentecostals insist that salvation comes by grace through faith in Christ, coupled with obedience to his command to be "born of water and of the Spirit"; hence, no good works or obedience to laws or rules can save anyone. For them, baptism is not seen as a "work" but rather the indispensable means that Jesus himself provided to come into his kingdom. The major Oneness churches include the United Pentecostal Church International and the Pentecostal Assemblies of the World.

In addition to the denominational Pentecostal churches, there are many Pentecostal churches that choose to exist independently of denominational oversight. Some of these churches may be doctrinally identical to the various Pentecostal denominations, while others may adopt beliefs and practices that differ considerably from classical Pentecostalism, such as Word of Faith teachings or Kingdom Now theology. Some of these groups have been successful in utilizing the mass media, especially television and radio, to spread their message.


The charismatic experiences found in Pentecostalism is believed to have precedents in earlier movements in Christianity. Early Pentecostals have considered the movement a latter-day restoration of the church's apostolic power, and historians such as Cecil M. Robeck, Jr. and Edith Blumhofer write that the movement emerged from late 19th-century radical evangelical revival movements in America and in Great Britain.

Within this radical evangelicalism, expressed most strongly in the Wesleyan—holiness and Higher Life movements, themes of restorationism, premillennialism, faith healing, and greater attention on the person and work of the Holy Spirit were central to emerging Pentecostalism. Believing that the second coming of Christ was imminent, these Christians expected an endtime revival of apostolic power, spiritual gifts, and miracle—working. Figures such as Dwight L. Moody and R. A. Torrey began to speak of an experience available to all Christians which would empower believers to evangelize the world, often termed "baptism with the Holy Spirit".

Certain Christian leaders and movements had important influences on early Pentecostals. The essentially universal belief in the continuation of all the spiritual gifts in the Keswick and Higher Life movements constituted a crucial historical background for the rise of Pentecostalism. Albert Benjamin Simpson (1843–1919) and his Christian and Missionary Alliance (founded in 1887) was very influential in the early years of Pentecostalism, especially on the development of the Assemblies of God. Another early influence on Pentecostals was John Alexander Dowie (1847–1907) and his Christian Catholic Apostolic Church (founded in 1896). Pentecostals embraced the teachings of Simpson, Dowie, Adoniram Judson Gordon (1836–1895) and Maria Woodworth-Etter (1844–1924; she later joined the Pentecostal movement) on healing. Edward Irving's Catholic Apostolic Church (founded c. 1831) also displayed many characteristics later found in the Pentecostal revival.

No one person or group founded Pentecostalism. Instead, isolated Christian groups were experiencing charismatic phenomena such as divine healing and speaking in tongues. The holiness movement provided a theological explanation for what was happening to these Christians, and they adapted Wesleyan soteriology to accommodate their new understanding.

Charles Fox Parham, an independent holiness evangelist who believed strongly in divine healing, was an important figure to the emergence of Pentecostalism as a distinct Christian movement. In 1900, he started a school near Topeka, Kansas, which he named Bethel Bible School. There he taught that speaking in tongues was the scriptural evidence for the reception of the baptism with the Holy Spirit. On January 1, 1901, after a watch night service, the students prayed for and received the baptism with the Holy Spirit with the evidence of speaking in tongues. Parham received this same experience sometime later and began preaching it in all his services. Parham believed this was xenoglossia and that missionaries would no longer need to study foreign languages. After 1901, Parham closed his Topeka school and began a four-year revival tour throughout Kansas and Missouri. He taught that the baptism with the Holy Spirit was a third experience, subsequent to conversion and sanctification. Sanctification cleansed the believer, but Spirit baptism empowered for service.

At about the same time that Parham was spreading his doctrine of initial evidence in the Midwestern United States, news of the Welsh Revival of 1904–05 ignited intense speculation among radical evangelicals around the world and particularly in the US of a coming move of the Spirit which would renew the entire Christian Church. This revival saw thousands of conversions and also exhibited speaking in tongues.

In 1905, Parham moved to Houston, Texas, where he started a Bible training school. One of his students was William J. Seymour, a one-eyed black preacher. Seymour traveled to Los Angeles where his preaching sparked the three-year-long Azusa Street Revival in 1906. The revival first broke out on Monday April 9, 1906 at 214 Bonnie Brae Street and then moved to 312 Azusa Street on Friday, April 14, 1906. Worship at the racially integrated Azusa Mission featured an absence of any order of service. People preached and testified as moved by the Spirit, spoke and sung in tongues, and fell in the Spirit. The revival attracted both religious and secular media attention, and thousands of visitors flocked to the mission, carrying the "fire" back to their home churches. Despite the work of various Wesleyan groups such as Parham's and D. L. Moody's revivals, the beginning of the widespread Pentecostal movement in the US is generally considered to have begun with Seymour's Azusa Street Revival.
The crowds of African-Americans and whites worshiping together at William Seymour's Azusa Street Mission set the tone for much of the early Pentecostal movement. During the period of 1906–24, Pentecostals defied social, cultural and political norms of the time that called for racial segregation and the enactment of Jim Crow laws. The Church of God in Christ, the Church of God (Cleveland), the Pentecostal Holiness Church, and the Pentecostal Assemblies of the World were all interracial denominations before the 1920s. These groups, especially in the Jim Crow South were under great pressure to conform to segregation. Ultimately, North American Pentecostalism would divide into white and African-American branches. Though it never entirely disappeared, interracial worship within Pentecostalism would not reemerge as a widespread practice until after the civil rights movement.

Women were vital to the early Pentecostal movement. Believing that whoever received the Pentecostal experience had the responsibility to use it towards the preparation for Christ's second coming, Pentecostal women held that the baptism in the Holy Spirit gave them empowerment and justification to engage in activities traditionally denied to them. The first person at Parham's Bible college to receive Spirit baptism with the evidence of speaking in tongues was a woman, Agnes Ozman. Women such as Florence Crawford, Ida Robinson, and Aimee Semple McPherson founded new denominations, and many women served as pastors, co-pastors, and missionaries. Women wrote religious songs, edited Pentecostal papers, and taught and ran Bible schools. The unconventionally intense and emotional environment generated in Pentecostal meetings dually promoted, and was itself created by, other forms of participation such as personal testimony and spontaneous prayer and singing. Women did not shy away from engaging in this forum, and in the early movement the majority of converts and church-goers were female. Nevertheless, there was considerable ambiguity surrounding the role of women in the church. The subsiding of the early Pentecostal movement allowed a socially more conservative approach to women to settle in, and, as a result, female participation was channeled into more supportive and traditionally accepted roles. Auxiliary women's organizations were created to focus women's talents on more traditional activities. Women also became much more likely to be evangelists and missionaries than pastors. When they were pastors, they often co-pastored with their husbands.

The majority of early Pentecostal denominations taught pacifism and adopted military service articles that advocated conscientious objection.

Azusa participants returned to their homes carrying their new experience with them. In many cases, whole churches were converted to the Pentecostal faith, but many times Pentecostals were forced to establish new religious communities when their experience was rejected by the established churches. One of the first areas of involvement was the African continent, where, by 1907, American missionaries were established in Liberia, as well as in South Africa by 1908. Because speaking in tongues was initially believed to always be actual foreign languages, it was believed that missionaries would no longer have to learn the languages of the peoples they evangelized because the Holy Spirit would provide whatever foreign language was required. (When the majority of missionaries, to their disappointment, learned that tongues speech was unintelligible on the mission field, Pentecostal leaders were forced to modify their understanding of tongues.) Thus, as the experience of speaking in tongues spread, a sense of the immediacy of Christ's return took hold and that energy would be directed into missionary and evangelistic activity. Early Pentecostals saw themselves as outsiders from mainstream society, dedicated solely to preparing the way for Christ's return.

An associate of Seymour's, Florence Crawford, brought the message to the Northwest, forming what would become the Apostolic Faith Church by 1908. After 1907, Azusa participant William Howard Durham, pastor of the North Avenue Mission in Chicago, returned to the Midwest to lay the groundwork for the movement in that region. It was from Durham's church that future leaders of the Pentecostal Assemblies of Canada would hear the Pentecostal message. One of the most well known Pentecostal pioneers was Gaston B. Cashwell (the "Apostle of Pentecost" to the South), whose evangelistic work led three Southeastern holiness denominations into the new movement.

The Pentecostal movement, especially in its early stages, was typically associated with the impoverished and marginalized of America, especially African Americans and Southern Whites. With the help of many healing evangelists such as Oral Roberts, Pentecostalism spread across America by the 1950s.

International visitors and Pentecostal missionaries would eventually export the revival to other nations. The first foreign Pentecostal missionaries were A. G. Garr and his wife, who were Spirit baptized at Azusa and traveled to India and later Hong Kong. The Norwegian Methodist pastor T. B. Barratt was influenced by Seymour during a tour of the United States. By December 1906, he had returned to Europe and is credited with beginning the Pentecostal movement in Sweden, Norway, Denmark, Germany, France and England. A notable convert of Barratt was Alexander Boddy, the Anglican vicar of All Saints' in Sunderland, England, who became a founder of British Pentecostalism. Other important converts of Barratt were German minister Jonathan Paul who founded the first German Pentecostal denomination (the Mülheim Association) and Lewi Pethrus, the Swedish Baptist minister who founded the Swedish Pentecostal movement.

Through Durham's ministry, Italian immigrant Luigi Francescon received the Pentecostal experience in 1907 and established Italian Pentecostal congregations in the US, Argentina (Christian Assembly in Argentina), and Brazil (Christian Congregation of Brazil). In 1908, Giacomo Lombardi led the first Pentecostal services in Italy. In November 1910, two Swedish Pentecostal missionaries arrived in Belem, Brazil and established what would become the Assembleias de Deus (Assemblies of God of Brazil). In 1908, John G. Lake, a follower of Alexander Dowie who had experienced Pentecostal Spirit baptism, traveled to South Africa and founded what would become the Apostolic Faith Mission of South Africa and the Zion Christian Church. As a result of this missionary zeal, practically all Pentecostal denominations today trace their historical roots to the Azusa Street Revival.

The first generation of Pentecostal believers faced immense criticism and ostracism from other Christians, most vehemently from the Holiness movement from which they originated. Alma White, leader of the Pillar of Fire Church, wrote a book against the movement titled "Demons and Tongues" in 1910. She called Pentecostal tongues "satanic gibberish" and Pentecostal services "the climax of demon worship". Famous holiness preacher W. B. Godbey characterized those at Azusa Street as "Satan's preachers, jugglers, necromancers, enchanters, magicians, and all sorts of mendicants". To Dr. G. Campbell Morgan, Pentecostalism was "the last vomit of Satan", while Dr. R. A. Torrey thought it was "emphatically not of God, and founded by a Sodomite". The Pentecostal Church of the Nazarene, one of the largest holiness groups, was strongly opposed to the new Pentecostal movement. To avoid confusion, the church changed its name in 1919 to the Church of the Nazarene. A. B. Simpson's Christian and Missionary Alliance negotiated a compromise position unique for the time. Simpson believed that Pentecostal tongues speaking was a legitimate manifestation of the Holy Spirit, but he did not believe it was a necessary evidence of Spirit baptism. This view on speaking in tongues ultimately led to what became known as the "Alliance position" articulated by A. W. Tozer as "seek not—forbid not".

The first Pentecostal converts were mainly derived from the Holiness movement and adhered to a Wesleyan understanding of sanctification as a definite, instantaneous experience and second work of grace. Problems with this view arose when large numbers of converts entered the movement from non-Wesleyan backgrounds, especially from Baptist churches. In 1910, William Durham of Chicago first articulated the Finished Work, a doctrine which located sanctification at the moment of salvation and held that after conversion the Christian would progressively grow in grace in a lifelong process. This teaching polarized the Pentecostal movement into two factions. The Wesleyan doctrine was strongest in the Southern denominations, such as the Church of God (Cleveland), Church of God in Christ, and the Pentecostal Holiness Church. The Finished Work, however, would ultimately gain ascendancy among Pentecostals. After 1911, most new Pentecostal denominations would adhere to Finished Work sanctification.

In 1914, a group of predominately 300 white Pentecostal ministers and laymen from all regions of the United States gathered in Hot Springs, Arkansas, to create a new, national Pentecostal fellowship—the General Council of the Assemblies of God. By 1911, many of these white ministers were distancing themselves from an existing arrangement under an African-American leader. Many of these white ministers were licensed by the African-American, C. H. Mason under the auspices of the Church of God in Christ, one of the few legally chartered Pentecostal organizations at the time credentialing and licensing ordained Pentecostal clergy. To further such distance, Bishop Mason and other African-American Pentecostal leaders were not invited to the initial 1914 fellowship of Pentecostal ministers. These predominately white ministers adopted a congregational polity (whereas the COGIC and other Southern groups remained largely episcopal) and rejected a Finished Work understanding of Sanctification. Thus, the creation of the Assemblies of God marked an official end of Pentecostal doctrinal unity and racial integration.

The new Assemblies of God would soon face a "new issue" which first emerged at a 1913 camp meeting. During a baptism service, the speaker, R. E. McAlister, mentioned that the Apostles baptized converts once in the name of Jesus Christ, and the words "Father, Son, and Holy Ghost" were never used in baptism. This inspired Frank Ewart who claimed to have received as a divine prophecy revealing a nontrinitarian conception of God. Ewart believed that there was only one personality in the Godhead—Jesus Christ. The terms "Father" and "Holy Ghost" were titles designating different aspects of Christ. Those who had been baptized in the Trinitarian fashion needed to submit to rebaptism in Jesus' name. Furthermore, Ewart believed that Jesus' name baptism and the gift of tongues were essential for salvation. Ewart and those who adopted his belief called themselves "oneness" or "Jesus' Name" Pentecostals, but their opponents called them "Jesus Only".

Amid great controversy, the Assemblies of God rejected the Oneness teaching, and many of its churches and pastors were forced to withdraw from the denomination in 1916. They organized their own Oneness groups. Most of these joined Garfield T. Haywood, an African-American preacher from Indianapolis, to form the Pentecostal Assemblies of the World. This church maintained an interracial identity until 1924 when the white ministers withdrew to form the Pentecostal Church, Incorporated. This church later merged with another group forming the United Pentecostal Church International.

While Pentecostals shared many basic assumptions with conservative Protestants, the earliest Pentecostals were rejected by Fundamentalist Christians who adhered to cessationism. In 1928, the World Christian Fundamentals Association labeled Pentecostalism "fanatical" and "unscriptural". By the early 1940s, this rejection of Pentecostals was giving way to a new cooperation between them and leaders of the "new evangelicalism", and American Pentecostals were involved in the founding of the 1942 National Association of Evangelicals. Pentecostal denominations also began to interact with each other both on national levels and international levels through the Pentecostal World Fellowship, which was founded in 1947.

Some Pentecostal churches in Europe, especially in Italy and Germany, during the war were also victims of the Shoah. Because of their tongues speaking their members were considered mentally ill, and many pastors were sent either to confinement or to concentration camps.

Though Pentecostals began to find acceptance among evangelicals in the 1940s, the previous decade was widely viewed as a time of spiritual dryness, when healings and other miraculous phenomena were perceived as being less prevalent than in earlier decades of the movement. It was in this environment that the Latter Rain Movement, the most important controversy to affect Pentecostalism since World War II, began in North America and spread around the world in the late 1940s. Latter Rain leaders taught the restoration of the fivefold ministry led by apostles. These apostles were believed capable of imparting spiritual gifts through the laying on of hands. There were prominent participants of the early Pentecostal revivals, such as Stanley Frodsham and Lewi Pethrus, who endorsed the movement citing similarities to early Pentecostalism. However, Pentecostal denominations were critical of the movement and condemned many of its practices as unscriptural. One reason for the conflict with the denominations was the sectarianism of Latter Rain adherents. Many autonomous churches were birthed out of the revival.

A simultaneous development within Pentecostalism was the postwar Healing Revival. Led by healing evangelists William Branham, Oral Roberts, Gordon Lindsay, and T. L. Osborn, the Healing Revival developed a following among non-Pentecostals as well as Pentecostals. Many of these non-Pentecostals were baptized in the Holy Spirit through these ministries. The Latter Rain and the Healing Revival influenced many leaders of the charismatic movement of the 1960s and 1970s.

Before the 1960s, most non-Pentecostal Christians who experienced the Pentecostal baptism in the Holy Spirit typically kept their experience a private matter or joined a Pentecostal church afterward. The 1960s saw a new pattern develop where large numbers of Spirit baptized Christians from mainline churches in the US, Europe, and other parts of the world chose to remain and work for spiritual renewal within their traditional churches. This initially became known as New or Neo-Pentecostalism (in contrast to the older classical Pentecostalism) but eventually became known as the Charismatic Movement. While cautiously supportive of the Charismatic Movement, the failure of Charismatics to embrace traditional Pentecostal taboos on dancing, drinking alcohol, smoking, and restrictions on dress and appearance initiated an identity crisis for classical Pentecostals, who were forced to reexamine long held assumptions about what it meant to be Spirit filled. The liberalizing influence of the Charismatic Movement on classical Pentecostalism can be seen in the disappearance of many of these taboos since the 1960s. Because of this, the cultural differences between classical Pentecostals and charismatics have lessened over time. The global renewal movements manifest many of these tensions as inherent characteristics of Pentecostalism and as representative of the character of global Christianity.

Zora Neale Hurston performed anthropological, sociological studies examining the spread of Pentecostalism. According to scholar of religion Ashon Crawley, Hurston's analysis is important because she understood the class struggle that this seemingly new religiocultural movement articulated: "The Sanctified Church is a protest against the high-brow tendency in Negro Protestant congregations as the Negroes gain more education and wealth." She stated that this sect was "a revitalizing element in Negro music and religion" and that this collection of groups was "putting back into Negro religion those elements which were brought over from Africa and grafted onto Christianity." Crawley would go on to argue that the shouting that Hurston documented was evidence of what Martinique psychoanalyst Frantz Fanon called the refusal of positionality wherein "no strategic position is given preference" as the creation of, the grounds for, social form.

Pentecostalism is a religious phenomenon more visible in the cities. However, it has attracted significant rural populations in Latin America, Africa, and Eastern Europe. Sociologist David Martin has called attention on an overview on the rural Protestantism in Latin America, focusing on the indigenous and peasant conversion to Pentecostalism. The cultural change resulting from the countryside modernization has reflected on the peasant way of life. Consequently, many peasants – especially in Latin America – have experienced collective conversion to different forms of Pentecostalism and interpreted as a response to modernization in the countryside

Rather than a mere religious shift from folk Catholicism to Pentecostalism, Peasant Pentecostals have dealt with agency to employ many of their cultural resources to respond development projects in a modernization framework

Researching Guatemalan peasants and indigenous communities, Sheldon Annis argued that conversion to Pentecostalism was a way to quit the burdensome obligations of the cargo-system. Mayan folk Catholicism has many fiestas with a rotation leadership who must pay the costs and organize the yearly patron-saint festivities. One of the socially-accepted many to opt out those obligations was to convert to Pentecostalism. By doing so, the Pentecostal Peasant engage in a “penny capitalism”. In the same lines of moral obligations but with different mechanism economic self-help, Paul Chandler has compared the differences between Catholic and Pentecostal peasants, and has found a web of reciprocity among Catholics compadres, which the Pentecostals lacked. However, Alves has found that the different Pentecostal congregations replaces the compadrazgo system and still provide channels to exercise the reciprocal obligations that the peasant moral economy demands.

Conversion to Pentecostalism provides a rupture with a socially disrupted past while allowing to maintain elements of the peasant ethos. Brazil has provided many cases to evaluate this thesis. Hoekstra has found out that rural Pentecostalism more as a continuity of the traditional past though with some ruptures. Anthropologist Brandão sees the small town and rural Pentecostalism as another face for folk religiosity instead of a path to modernization. With similar finding, Abumanssur regards Pentecostalism as an attempt to conciliate traditional worldviews of folk religion with modernity.

Identity shift has been noticed among rural converts to Pentecostalism. Indigenous and peasant communities have found in the Pentecostal religion a new identity that helps them navigate the challenges posed by modernity. This identity shift corroborates the thesis that the peasant Pentecostals pave their own ways when facing modernization.







</doc>
<doc id="23558" url="https://en.wikipedia.org/wiki?curid=23558" title="Pangenesis">
Pangenesis

Pangenesis was Charles Darwin's hypothetical mechanism for heredity, in which he proposed that each part of the body continually emitted its own type of small organic particles called gemmules that aggregated in the gonads, contributing heritable information to the gametes. He presented this 'provisional hypothesis' in his 1868 work "The Variation of Animals and Plants under Domestication", intending it to fill what he perceived as a major gap in evolutionary theory at the time. The etymology of the word comes from the Greek words "pan" (a prefix meaning "whole", "encompassing") and "genesis" ("birth") or "genos" ("origin"). Pangenesis mirrored ideas originally formulated by Hippocrates and other pre-Darwinian scientists, but built off of new concepts such as cell theory, explaining cell development as beginning with gemmules which were specified to be necessary for the occurrence of new growths in an organism, both in initial development and regeneration. It also accounted for regeneration and the Lamarckian concept of the inheritance of acquired characteristics, as a body part altered by the environment would produce altered gemmules. This made Pangenesis popular among the neo-Lamarckian school of evolutionary thought. This hypothesis was made effectively obsolete after the 1900 rediscovery among biologists of Gregor Mendel's theory of the particulate nature of inheritance.

Pangenesis was similar to ideas put forth by Hippocrates, Democritus and other pre-Darwinian scientists in proposing that the whole of parental organisms participate in heredity (thus the prefix "pan"). Darwin wrote that Hippocrates' pangenesis was "almost identical with mine—merely a change of terms—and an application of them to classes of facts necessarily unknown to the old philosopher."

Science historian Conway Zirkle wrote that:

Zirkle demonstrated that the idea of inheritance of acquired characteristics had become fully accepted by the 16th century and remained immensely popular through to the time of Lamarck's work, at which point it began to draw more criticism due to lack of hard evidence. He also stated that pangenesis was the only scientific explanation ever offered for this concept, developing from Hippocrates' belief that "the semen was derived from the whole body." In the 13th century, pangenesis was commonly accepted on the principle that semen was a refined version of food unused by the body, which eventually translated to 15th and 16th century widespread use of pangenetic principles in medical literature, especially in gynecology. Later pre-Darwinian important applications of the idea included hypotheses about the origin of the differentiation of races.

A theory put forth by Pierre Louis Maupertuis in 1745 called for particles from both parents governing the attributes of the child, although some historians have called his remarks on the subject cursory and vague.

In 1749, French naturalist Georges-Louis Leclerc, Comte de Buffon developed a hypothetical system of heredity much like Darwin's pangenesis, wherein 'organic molecules' were transferred to offspring during reproduction and stored in the body during development. Commenting on Buffon's views, Darwin stated, "If Buffon had assumed that his organic molecules had been formed by each separate unit throughout the body, his view and mine would have been very closely similar."

In 1801, Erasmus Darwin advocated a hypothesis of pangenesis in the third edition of his book "Zoonomia". In 1809, Jean-Baptiste Lamarck in his "Philosophie Zoologique" put forth evidence for the idea that characteristics acquired during the lifetime of an organism, either from effects of the environment or may be passed on to the offspring. Charles Darwin first had significant contact with Lamarckism during his time at the University of Edinburgh Medical School in the late 1820s, both through Robert Edmond Grant, whom he assisted in research, and in Erasmus's journals. Darwin's first known writings on the topic of Lamarckian ideas as they related to inheritance are found in a notebook he opened in 1837, also entitled "Zoonomia". Historian Johnathan Hodge states that the theory of pangenesis itself first appeared in Darwin's notebooks in 1841.

In 1861, Irish physician Henry Freke developed a variant of pangenesis in his book "Origin of Species by Means of Organic Affinity". Freke proposed that all life was developed from microscopic organic agents which he named "granules", which existed as 'distinct species of organizing matter' and would develop into different biological structures.

In 1864, four years before the publication of "Variation", Herbert Spencer in his book "Principles of Biology" proposed a theory of "physiological units" similar to Darwin's gemmules, which likewise were said to be related to specific body parts and responsible for the transmission of characteristics of those body parts to offspring. He also supported the Lamarckian idea of transmission of acquired characteristics.

Darwin had debated whether to publish a theory of heredity for an extended period of time due to its highly speculative nature. He decided to include pangenesis in "Variation" after sending a 30 page manuscript to his close friend and supporter Thomas Huxley in May 1865, which was met by significant criticism from Huxley that made Darwin even more hesitant. However, Huxley eventually advised Darwin to publish, writing: "Somebody rummaging among your papers half a century hence will find Pangenesis & say 'See this wonderful anticipation of our modern Theories—and that stupid ass, Huxley, prevented his publishing them'" Darwin's initial version of pangenesis appeared in the first edition of "Variation" in 1868, and was later reworked for the publication of a second edition in 1875.

Darwin's pangenesis theory attempted to explain the process of sexual reproduction, inheritance of traits, and complex developmental phenomena such as cellular regeneration in a unified mechanistic structure. Yongshen Liu wrote that in modern terms, pangenesis deals with issues of "dominance inheritance, graft hybridization, reversion, xenia, telegony, the inheritance of acquired characters, regeneration and many groups of facts pertaining to variation, inheritance and development." Mechanistically, Darwin proposed pangenesis to occur through the transfer of organic particles which he named 'gemmules.' Gemmules, which he also sometimes referred to as "}," pangenes, granules, or germs, were supposed to be shed by the organs of the body and carried in the bloodstream to the reproductive organs where they accumulated in the germ cells or gametes. Their accumulation was thought to occur by some sort of a 'mutual affinity.' Each gemmule was said to be specifically related to a certain body part- as described, they did not contain information about the entire organism. The different types were assumed to be dispersed through the whole body, and capable of self-replication given 'proper nutriment'. When passed on to offspring via the reproductive process, gemmules were thought to be responsible for developing into each part of an organism and expressing characteristics inherited from both parents. Darwin thought this to occur in a literal sense: he explained cell proliferation to progress as gemmules to bind to more developed cells of their same character and mature. In this sense, the uniqueness of each individual would be due to their unique mixture of their parents' gemmules, and therefore characters. Similarity to one parent over the other could be explained by a quantitative superiority of one parent's gemmules. Yongshen Lu points out that Darwin knew of cells' ability to multiply by self-division, so it is unclear how Darwin supposed the two proliferation mechanisms to relate to each other. He did clarify in a later statement that he had always supposed gemmules to only bind to and proliferate from developing cells, not mature ones. Darwin hypothesized that gemmules might be able to survive and multiply outside of the body in a letter to J. D. Hooker in 1870.
Some gemmules were thought to remain dormant for generations, whereas others were routinely expressed by all offspring. Every child was built up from selective expression of the mixture of the parents and grandparents' gemmules coming from either side. Darwin likened this to gardening: a flowerbed could be sprinkled with seeds "most of which soon germinate, some lie for a period dormant, whilst others perish." He did not claim gemmules were in the blood, although his theory was often interpreted in this way. Responding to Fleming Jenkin's review of "On the Origin of Species", he argued that pangenesis would permit the preservation of some favourable variations in a population so that they wouldn't die out through blending.

Darwin thought that environmental effects that caused altered characteristics would lead to altered gemmules for the affected body part. The altered gemmules would then have a chance of being transferred to offspring, since they were assumed to be produced throughout an organisms life. Thus, pangenesis theory allowed for the Lamarckian idea of transmission of characteristics acquired through use and disuse. Accidental gemmule development in incorrect parts of the body could explain deformations and the 'monstrosities' Darwin cited in "Variation".

Hugo de Vries characterized his own version of pangenesis theory in his 1889 book "Intracellular Pangenesis" with two propositions, of which he only accepted the first:

De Vries also coined the term 'pangene' which 20 years later was shortened by Wilhelm Johannsen to gene.

Science historian Janet Browne points out that while Herbert Spencer and Carl von Nägeli also put forth ideas for systems of inheritance involving gemmules, their version of gemmules differed in that it contained "a complete microscopic blueprint for an entire creature." Spencer published his theory of "physiological units" three years prior to Darwin's publication of "Variation".

She goes on to say that Darwin believed specifically in gemmules for each body part because they might explain how environmental effects could be passed on as characteristics to offspring.

Interpretations and applications of pangenesis continued to appear frequently in medical literature up until Weismann's experiments and subsequent publication on germ-plasm theory in 1892. For instance, an address by Huxley spurred on substantial work by Dr. James Ross in linking ideas found in Darwin's pangenesis to the germ theory of disease. Ross cites the work of both Darwin and Spencer as key to his application of pangenetic theory.

Darwin's half-cousin Francis Galton conducted wide-ranging inquiries into heredity which led him to refute Charles Darwin's hypothetical theory of pangenesis. In consultation with Darwin, he set out to see if gemmules were transported in the blood. In a long series of experiments from 1869 to 1871, he transfused the blood between dissimilar breeds of rabbits, and examined the features of their offspring. He found no evidence of characters transmitted in the transfused blood.

Galton was troubled because he began the work in good faith, intending to prove Darwin right, and having praised pangenesis in "Hereditary Genius" in 1869. Cautiously, he criticized his cousin's theory, although qualifying his remarks by saying that Darwin's gemmules, which he called "pangenes", might be temporary inhabitants of the blood that his experiments had failed to pick up.

Darwin challenged the validity of Galton's experiment, giving his reasons in an article published in "Nature" where he wrote:

After the circulation of Galton's results, the perception of pangenesis quickly changed to severe skepticism if not outright disbelief.

August Weismann's idea, set out in his 1892 book "Das Keimplasma: eine Theorie der Vererbung" (The Germ Plasm: a Theory of Inheritance), was that the hereditary material, which he called the germ plasm, and the rest of the body (the soma) had a one-way relationship: the germ-plasm formed the body, but the body did not influence the germ-plasm, except indirectly in its participation in a population subject to natural selection. This distinction is commonly referred to as the Weismann Barrier. If correct, this made Darwin's pangenesis wrong and Lamarckian inheritance impossible. His experiment on mice, cutting off their tails and showing that their offspring had normal tails across multiple generations, was proposed as a proof of the non-existence of Lamarckian inheritance, although Peter Gauthier has argued that Weismann's experiment showed only that injury did not affect the germ plasm and neglected to test the effect of Lamarckian use and disuse. Weismann argued strongly and dogmatically for Darwinism and against neo-Lamarckism, polarising opinions among other scientists. This increased anti-Darwinian feeling, contributing to its eclipse.

Darwin's pangenesis theory was widely criticised, in part for its Lamarckian premise that parents could pass on traits acquired in their lifetime. Conversely, the neo-Lamarckians of the time seized upon pangenesis as evidence to support their case. Italian Botanist Federico Delpino's objection that gemmules' ability to self-divide is contrary to their supposedly innate nature gained considerable traction; however, Darwin was dismissive of this criticism, remarking that the particulate agents of small pox and scarlet fever seem to have such characteristics. Lamarckism fell from favour after August Weismann's research in the 1880s indicated that changes from use (such as lifting weights to increase muscle mass) and disuse (such as being lazy and becoming weak) were not heritable. However, some scientists continued to voice their support in spite of Galton's and Weismann's results: notably, in 1900 Karl Pearson wrote that pangenesis "is no more disproved by the statement that 'gemmules have not been found in the blood,' than the atomic theory is disproved by the fact that no atoms have been found in the air." Finally, the rediscovery of Mendel's Laws of Inheritance in 1900 led to pangenesis being fully set aside. Julian Huxley has observed that the later discovery of chromosomes and the research of T. H. Morgan also made pangenesis untenable.

Some of Darwin's pangenesis principles do relate to heritable aspects of phenotypic plasticity, although the status of gemmules as a distinct class of organic particles has been firmly rejected. However, starting in the 1950s, many research groups in revisiting Galton's experiments found that heritable characteristics could indeed arise in rabbits and chickens following DNA injection or blood transfusion. This type of research originated in the Soviet Union in the late 1940s in the work of Sopikov and others, and was later corroborated by researchers in Switzerland as it was being further developed by the Soviet scientists. Notably, this work was supported in the USSR in part due to its conformation with the ideas of Trofim Lysenko, who espoused a version of neo-Lamarckism as part of Lysenkoism. Further research of this heritability of acquired characteristics developed into, in part, the modern field of epigenetics. Darwin himself had noted that "the existence of free gemmules is a gratuitous assumption"; by some accounts in modern interpretation, gemmules may be considered a prescient mix of DNA, RNA, proteins, prions, and other mobile elements that are heritable in a non-Mendelian manner at the molecular level. Liu points out that Darwin's ideas about gemmules replicating outside of the body are predictive of "in vitro" gene replication used, for instance, in PCR. It is worth noting, however, that this reinterpretation of pangenesis's viability in modern terms is the work of a niche group of scholars and does not necessarily reflect a renewal of interest by related fields as a whole.





</doc>
<doc id="23560" url="https://en.wikipedia.org/wiki?curid=23560" title="Proboscidea">
Proboscidea

The Proboscidea (from the Greek and the Latin "proboscis") are a taxonomic order of afrotherian mammals containing one living family (Elephantidae) and several extinct families. This order, first described by J. Illiger in 1811, encompasses the trunked mammals. In addition to their enormous size, later proboscideans are distinguished by tusks and long, muscular trunks; these features were less developed or absent in the smaller early proboscideans. Beginning in the mid-Miocene, most members of this order were very large animals. The largest land mammal today is the African elephant weighing up to 10.4 tonnes with a shoulder height of up to . The largest land mammal of all time may have also been a proboscidean: "Palaeoloxodon namadicus", which may have weighed up to with a shoulder height up to , surpassing several sauropod dinosaurs (in height).

The earliest known proboscidean is "Eritherium", followed by "Phosphatherium", a small animal about the size of a fox. These both date from late Paleocene deposits of Morocco.

Proboscideans evolved in Africa, where they increased in size and diversity during the Eocene and early Oligocene. Several primitive families from these epochs have been described, including the Numidotheriidae, Moeritheriidae, and Barytheriidae, all found exclusively in Africa. The Anthracobunidae from the Indian subcontinent were also believed to be a family of proboscideans, but were excluded from the Proboscidea by Shoshani and Tassy (2005) and have more recently been assigned to the Perissodactyla. When Africa became connected to Europe and Asia after the shrinking of the Tethys Sea, proboscideans began to migrate into Eurasia, and some families eventually reached the Americas. Proboscideans found in Eurasia in addition to Africa include the Deinotheriidae, which thrived during the Miocene and into the early Quaternary, "Stegolophodon", an early genus of the disputed family Stegodontidae; the diverse family of Gomphotheriidae, such as "Platybelodon" and "Amebelodon"; and the Mammutidae, or mastodons.

Most families of the Proboscidea are now extinct, including all proboscideans that lived in the Americas, Europe, and northern Asia. Many of these extinctions occurred during or shortly after the last glacial period. Recently extinct species include the last examples of gomphotheres in the Americas, the American mastodon of family Mammutidae in North America, numerous stegodonts once found in Asia, the last of the mammoths throughout the Northern Hemisphere, and several species of dwarf elephants found on various islands scattered around the world.

Below is the current taxonomy of the proboscidean genera as of 2019.



</doc>
<doc id="23561" url="https://en.wikipedia.org/wiki?curid=23561" title="Paranthropus">
Paranthropus

Paranthropus is a genus of extinct hominin which contains two widely accepted species: "P. robustus" and "P. boisei". However, the validity of "Paranthropus" is contested, and it is sometimes considered to be synonymous with "Australopithecus". They are also referred to as the robust australopithecines. They lived between approximately 2.6 and 0.6 million years ago (mya) from the end of the Pliocene to the Middle Pleistocene.

"Paranthropus" is characterised by robust skulls, with a prominent gorilla-like sagittal crest along the midline–which suggest strong chewing muscles–and broad, herbivorous teeth used for grinding. However, they likely preferred soft food over tough and hard food. "Paranthropus" species were generalist feeders, but "P. robustus" was likely an omnivore, whereas "P. boisei" was likely herbivorous and mainly ate bulbotubers. They were bipeds. Despite their robust heads, they had comparatively small bodies. Average weight and height are estimated to be at for "P. robustus" males, at for "P. boisei" males, at for "P. robustus" females, and at for "P. boisei" females.

They were possibly polygamous and patrilocal, but there are no modern analogues for australopithecine societies. They are associated with bone tools and contestedly the earliest evidence of fire usage. They typically inhabited woodlands, and coexisted with some early human species, namely "A. africanus", "H. habilis", and "H. erectus". They were preyed upon by the large carnivores of the time, specifically crocodiles, leopards, sabertoothed cats, and hyaenas.

The genus "Paranthropus" was first erected by Scottish South African palaeontologist Robert Broom in 1938, with the type species "P. robustus". ""Paranthropus"" derives from Ancient Greek παρα "para" beside or alongside; and άνθρωπος "ánthropos" man. The type specimen, a male braincase, TM 1517, was discovered by schoolboy Gert Terblanche at the Kromdraai fossil site, about southwest of Pretoria, South Africa. By 1988, at least 6 individuals were unearthed in around the same area, now known as the Cradle of Humankind.

In 1948, at Swartkrans Cave, in about the same vicinity as Kromdraai, Broom and South African palaeontologist John Talbot Robinson described "P. crassidens" based on a subadult jaw, SK 6. He believed later "Paranthropus" were morphologically distinct from earlier "Paranthropus" in the cave—that is, the Swartkrans "Paranthropus" were reproductively isolated from Kromdraai "Paranthropus" and the former eventually speciated. By 1988, several specimens from Swartkrans had been placed into "P. crassidens". However, this has since been synonymised with "P. robustus" as the two populations do not seem to be very distinct.

In 1959, "P. boisei" was discovered by Mary Leakey at Olduvai Gorge, Tanzania (specimen OH 5). Her husband Louis named it "Zinjanthropus boisei" because he believed it differed greatly from "Paranthropus" and "Australopithecus". The name derives from "Zinj", an ancient Arabic word for the coast of East Africa, and "boisei", referring to their financial benefactor Charles Watson Boise. However, this genus was rejected at Mr. Leakey's presentation before the 4th Pan-African Congress on Prehistory, as it was based on a single specimen. The discovery of the Peninj Mandible made the Leakey's reclassify their species as "Australopithecus (Zinjanthropus) boisei" in 1964, but in 1967, South African palaeoanthropologist Phillip V. Tobias subsumed it into "Australopithecus" as "A. boisei". However, as more specimens were found, the combination "Paranthropus boisei" became more popular.

It is debated whether the wide range of variation in jaw size indicates simply sexual dimorphism or a grounds for identifying a new species. It could be explained as groundmass filling in cracks naturally formed after death, inflating the perceived size of the bone. "P. boisei" also has a notably wide range of variation in skull anatomy, but these features likely have no taxonomic bearing.


In 1968, French palaeontologists Camille Arambourg and Yves Coppens described ""Paraustralopithecus aethiopicus"" based on a toothless mandible from the Shungura Formation, Ethiopia (Omo 18). In 1976, American anthropologist Francis Clark Howell and Bretton anthropologist Yves Coppens reclassified it as "A. africanus". In 1986, after the discovery of the skull KNM WT 17000 by English anthropologist Alan Walker and Richard Leakey classified it into "Paranthropus" as "P. aethiopicus". There is debate whether this is synonymous with "P. boisei", the main argument for separation being the skull seems less adapted for chewing tough vegetation.

In 1989, palaeoartist and zoologist Walter Ferguson reclassified KNM WT 17000 into a new species, "walkeri", because he considered the skull's species designation questionable as it comprised the skull whereas the holotype of "P. aethiopicus" comprised only the mandible. Ferguson's classification is almost universally ignored, and is considered to be synonymous with "P. aethiopicus".

In 1963, while in the Congo, French ethnographer Charles Cordier assigned the name "P. congensis" to a super-strong, monstrous ape-man cryptid called "Kikomba", "Apamándi", "Abanaánji", "Zuluzúgu", or "Tshingómbe" by various native tribes which he heard stories about.

In 2015, Ethiopian palaeoanthropologist Yohannes Haile-Selassie and colleagues described the 3.5–3.2 Ma "A. deyiremeda" based on 3 jawbones from the Afar Region, Ethiopia. They noted that, though it shares many similarities with "Paranthropus", it may not have been closely related because it lacked enlarged molars which characterize the genus. Nonetheless, in 2018, independent researcher Johan Nygren recommended moving it to "Paranthropus" based on dental and presumed dietary similarity.

In 1951, American anthropologists Sherwood Washburn and Bruce D. Patterson were the first to suggest that "Paranthropus" should be considered a junior synonym of "Australopithecus" as the former was only known from fragmentary remains at the time, and dental differences were too minute to serve as justification. In face of calls for subsumation, Leakey and Robinson continued defending its validity. Various other authors were still unsure until more complete remains were found. "Paranthropus" is sometimes classified as a subgenus of "Australopithecus".
There is currently no clear consensus on the validity of "Paranthropus". The argument rests upon whether the genus is monophyletic—is composed of a common ancestor and all of its descendants—and the argument against monophyly (that the genus is paraphyletic) says that "P. robustus" and "P. boisei" evolved similar gorilla-like heads independently of each other by coincidence (convergent evolution), as chewing adaptations in hominins evolve very rapidly and multiple times at various points in the family tree (homoplasy). In 1999, a chimp-like ulna forearm bone was assigned to "P. boisei", the first discovered ulna of the species, which was markedly different from "P. robustus" ulnae, which could suggest paraphyly.

"P. aethiopicus" is the earliest member of the genus, with the oldest remains, from the Ethiopian Omo Kibish Formation, dated to 2.6 mya at the end of the Pliocene. It is sometimes regarded as the direct ancestor of "P. boisei" and "P. robustus". It is possible that "P. aethiopicus" evolved even earlier, up to 3.3 mya, on the expansive Kenyan floodplains of the time. The oldest "P. boisei" remains date to about 2.3 mya from Malema, Malawi. "P. boisei" changed remarkably little over its nearly 1 million year existence. "Paranthropus" had spread into South Africa by 2 mya with the earliest "P. robustus" remains.

It is sometimes suggested that "Paranthropus" and "Homo" are sister taxa, both evolving from "Australopithecus". This may have occurred during a drying trend 2.8–2.5 mya in the Great Rift Valley, which caused the retreat of woodland environments in favor of open savanna, with forests growing only along rivers and lakes. "Homo" evolved in the former, and "Paranthropus" in the latter riparian environment. However, the classifications of "Australopithecus" species is problematic.
Evolutionary tree according to a 2019 study:

"Paranthropus" had a massively built, tall, and flat skull, with a prominent gorilla-like sagittal crest along the midline which anchored massive temporalis muscles used in chewing. Like other australopithecines, "Paranthropus" exhibited sexual dimorphism, with males notably larger than females. They had large molars with a relatively thick tooth enamel coating (post-canine megadontia), and comparatively small incisors (similar in size to modern humans), possibly adaptations to processing abrasive foods. The teeth of "P. aethiopicus" developed faster than those of "P. boisei".

"Paranthropus" had adaptations to the skull to resist large bite loads while feeding, namely the expansive squamosal sutures. The notably thick palate was once thought to have been an adaptation to resist a high bite force, but is better explained as a byproduct of facial lengthening and nasal anatomy.

In "P. boisei", the jaw hinge was adapted to grinding food side-to-side (rather than up-and-down in modern humans), which is better at processing the starchy abrasive foods that likely made up the bulk of its diet. "P. robustus" may have chewed in a front-to-back direction instead, and had less exaggerated (less derived) anatomical features than "P. boisei" as it perhaps did not require them with this kind of chewing strategy. This may have also allowed "P. robustus" to better process tougher foods.

The braincase volume averaged about , comparable to gracile australopithecines, but smaller than "Homo". Modern human brain volume averages for men and for women.

Unlike "P. robustus", the forearms of "P. boisei" were heavily built, which might suggest habitual suspensory behaviour as in orangutans and gibbons. A "P. boisei" shoulder blade indicates long infraspinatus muscles, which is also associated with suspensory behavior. A "P. aethiopicus" ulna, on the other hand, shows more similarities to "Homo" than "P. boisei".

"Paranthropus" were bipeds, and their hips, legs, and feet resemble "A. afarensis" and modern humans. The pelvis is similar to "A. afarensis", but the hip joints are smaller in "P. robustus". The physical similarity implies a similar walking gait. Their modern-humanlike big toe indicates a modern-humanlike foot posture and range of motion, but the more distal ankle joint would have inhibited the modern human toe-off gait cycle. By 1.8 mya, "Paranthropus" and "H. habilis" may have achieved about the same grade of bipedality.

In comparison to the large, robust head, the body was rather small. Average weight for "P. robustus" may have been for males and for females; and for "P. boisei" for males and for females. At Swartkrans Cave Members 1 and 2, about 35% of the "P. robustus" individuals are estimated to have weighed , 22% about , and the remaining 43% bigger than the former but less than . At Member 3, all individuals were about . Female weight was about the same in contemporaneous "H. erectus", but male "H. erectus" were on average 13 kg (28.7 lbs) heavier than "P. robustus" males. "P. robustus" sites are oddly dominated by small adults, which could be explained as heightened predation or mortality of the larger males of a group. The largest known "Paranthropus" individual was estimated at .

According to a 1991 study, based on femur length and using the dimensions of modern humans, male and female "P. robustus" are estimated to have stood on average respectively; and "P. boisei" . However, the latter estimates are problematic as there were no positively identified male "P. boisei" femurs at the time. In 2013, a 1.34 Ma male "P. boisei" partial skeleton was estimated to be at least and .

"Paranthropus" seems to have had notably high rates of pitting enamel hypoplasia (PEH), where tooth enamel formation is spotty instead of mostly uniform. In "P. robustus", about 47% of baby teeth and 14% of adult teeth were affected, in comparison to about 6.7% and 4.3% respectively in any other tested hominin species. The condition of these holes covering the entire tooth is consistent with the modern human ailment amelogenesis imperfecta. However, since circular holes in enamel coverage are uniform in size, only present on the molar teeth, and have the same severity across individuals, the PEH may have been a genetic condition. It is possible that the coding-DNA concerned with thickening enamel also left them more vulnerable to PEH.

There have been 10 identified cases of cavities in "P. robustus", indicating a rate similar to modern humans. A molar from Drimolen, South Africa, showed a cavity on the tooth root, a rare occurrence in fossil great apes. In order for cavity-creating bacteria to reach this area, the individual would have had to have also presented either alveolar resportion, which is commonly associated with gum disease; or super-eruption of teeth which occurs when teeth become worn down and have to erupt a bit more in order to maintain a proper bite, and this exposed the root. The latter is most likely, and the exposed root seems to have caused hypercementosis to anchor the tooth in place. The cavity seems to have been healing, which may have been caused by a change in diet or mouth microbiome, or the loss of the adjacent molar.

It was once thought "P. boisei" cracked open nuts with its powerful teeth, giving OH 5 the nickname "Nutcracker Man". However, like gorillas, "Paranthropus" likely preferred soft foods, but would consume tough or hard food during leaner times, and the powerful jaws were used only in the latter situation. In "P. boisei", thick enamel was more likely used to resist abrasive gritty particles rather than to minimize chipping while eating hard foods. In fact, there is a distinct lack of tooth fractures which would have resulted from such activity.

"Paranthropus" were generalist feeders, but diet seems to have ranged dramatically with location. The South African "P. robustus" appears to have been an omnivore, with a diet similar to contemporaneous "Homo" and nearly identical to the later "H. ergaster", and subsisted on mainly C4 savanna plants and C3 forest plants, which could indicate either seasonal shifts in diet or seasonal migration from forest to savanna. In leaner times it may have fallen back on brittle food. It likely also consumed seeds and possibly tubers or termites. A high cavity rate could indicate honey consumption.

The East African "P. boisei", on the other hand, seems to have been largely herbivorous and fed on C4 plants. Its powerful jaws allowed it to consume a wide variety of different plants, though it may have largely preferred nutrient-rich bulbotubers as these are known to thrive in the well-watered woodlands it is thought to have inhabited. Feeding on these, "P. boisei" may have been able to meet its daily caloric requirements of approximately 9700 kJ after about 6 hours of foraging.

Juvenile "P. robustus" may have relied more on tubers than adults, given the elevated levels of Strontium compared to adults in teeth from Swartkrans Cave, which, in the area, was most likely sourced from tubers. Dentin exposure on juvenile teeth could indicate early weaning, or a more abrasive diet than adults which wore away the cementum and enamel coatings, or both. It is also possible juveniles were less capable of removing grit from dug-up food rather than purposefully seeking out more abrasive foods.

Bone tools dating between 2.3 and 0.6 mya have been found in abundance in Swartkrans, Kromdraai, and Drimolen Caves, and are often associated with "P. robustus". Though "Homo" is also known from these caves, their remains are comparatively scarce to "Paranthropus", making "Homo"-attribution unlikely. The tools also cooccur with "Homo"-associated Oldawan and possibly Acheulian stone tool industries. The bone tools were typically sourced from the shaft of long bones from medium- to large-sized mammals, but tools made sourced from mandibles, ribs, and horn cores have also been found. Bone tools have also been found at Oldawan Gorge and directly associated with "P. boisei", the youngest dating to 1.34 mya, though a great proportion of other bone tools from here have ambiguous attribution. Stone tools from Kromdraai could possibly be attributed to "P. robustus", as no "Homo" have been found there yet.

The bone tools were not manufactured or purposefully shaped for a task. However, since the bones display no weathering (and were not scavenged randomly), and there is a preference displayed for certain bones, raw materials were likely specifically hand picked. This could indicate a similar cognitive ability to contemporary Stone Age "Homo".

Bone tools may have been used to cut or process vegetation, or dig up tubers or termites, The form of "P. robustus" incisors appear to be intermediate between "H. erectus" and modern humans, which could indicate less food processing done by the teeth due to preparation with simple tools.

Burnt bones were also associated with the inhabitants of Swartkrans, which could indicate some of the earliest fire usage. However, these bones were found in Member 3, where "Paranthropus" remains are rarer than "H. erectus", and it is also possible the bones were burned in a wildfire and washed into the cave as it is known the bones were not burned onsite.

Given the marked anatomical and physical differences with modern great apes, there may be no modern analogue for australopithecine societies, so comparisons drawn with modern primates will not be entirely accurate.

"Paranthropus" had pronounced sexual dimorphism, with males notably larger than females, which is commonly correlated with a male-dominated polygamous society. "P. robustus" may have had a harem society similar to modern forest-dwelling silverback gorillas, where one male has exclusive breeding rights to a group of females, as male-female size disparity is comparable to gorillas (based on facial dimensions), and younger males were less robust than older males (delayed maturity is also exhibited in gorillas).

However, if "P. robustus" preferred a savanna habitat, a multi-male society would have been more productive to better defend the troop from predators in the more exposed environment, much like savanna baboons. Further, among primates, delayed maturity is also exhibited in the rhesus monkey which has a multi-male society, and may not be an accurate indicator of social structure.

A 2011 Strontium isotope study of "P. robustus" teeth from the dolomite Sterkfontein Valley found that, like other hominins, but unlike other great apes, "P. robustus" females were more likely to leave their place of birth (patrilocal). This also discounts the plausibility of a harem society, which would have resulted in a matrilocal society due to heightened male–male competition. Males did not seem to have ventured very far from the valley, which could either indicate small home ranges, or that they preferred dolomitic landscapes due to perhaps cave abundance or factors related to vegetation growth.

Dental development seems to have followed about the same timeframe as it does in modern humans and most other hominins, but, since "Paranthropus" molars are markedly larger, rate of tooth eruption would have been accelerated. Their life history may have mirrored that of gorillas as they have the same brain volume, which (depending on the subspecies) reach physical maturity from 12–18 years and have birthing intervals of 40–70 months.

It is generally thought that "Paranthropus" preferred to inhabit wooded, riverine landscapes. The teeth of "Paranthropus", "H. habilis", and "H. erectus" are all known from various overlapping beds in East Africa, such as at Olduvai Gorge and the Turkana Basin. "P. robustus" and "H. erectus" also appear to have coexisted.

"P. boisei", known from the Great Rift Valley, may have typically inhabited wetlands along lakes and rivers, wooded or arid shrublands, and semiarid woodlands, though their presence in the savanna-dominated Malawian Chiwondo Beds implies they could tolerate a range of habitats. During the Pleistocene, there seems to have been coastal and montane forests in Eastern Africa. More expansive river valleys–namely the Omo River Valley–may have served as important refuges for forest-dwelling creatures. Being cut off from the forests of Central Africa by a savanna corridor, these East African forests would have promoted high rates of endemism, especially during times of climatic volatility.

The Cradle of Humankind, the only area "P. robustus" is known from, was mainly dominated by the springbok "Antidorcas recki", but other antelope, giraffes, and elephants were also seemingly abundant megafauna. Other known primates are early "Homo", the Hamadryas baboon, and the extinct colobine monkey "Cercopithecoides williamsi".

The left foot of a "P. boisei" specimen (though perhaps actually belonging to "H. habilis") from Olduvai Gorge seems to have been bitten off by a crocodile, possibly "Crocodylus anthropophagus", and another's leg shows evidence of leopard predation. Other likely Olduvan predators of great apes include the hunting hyaena "Chasmaporthetes nitidula", and the sabertoothed cats "Dinofelis" and "Megantereon". The carnivore assemblage at the Cradle of Humankind comprises the two sabertooths, and the hyaena "Lycyaenops silberbergi".

Male "P. robustus" appear to have had a higher mortality rate than females. It is possible that males were more likely to be kicked out of a group, and these lone males had a higher risk of predation.

It was once thought that "Paranthropus" had become a specialist feeder, and were inferior to the more adaptable tool-producing "Homo", leading to their extinction, but this has been called into question. However, smaller brain size may have been a factor in their extinction along with gracile australopithecines. "P. boisei" may have died out due to an arid trend starting 1.45 mya, causing the retreat of woodlands, and more competition with savanna baboons and "Homo" for alternative food resources.

South African "Paranthropus" appear to have outlasted their East African counterparts. The youngest record of "P. boisei" comes from Konso, Ethiopia about 1.4 mya, however there are no East African sites dated between 1.4 and 1 mya, so it may have persisted until 1 mya. "P. robustus", on the other hand, was recorded in Swartkrans until Member 3 dated to 1–0.6 mya (the Middle Pleistocene), though more likely the younger side of the estimate.




</doc>
<doc id="23562" url="https://en.wikipedia.org/wiki?curid=23562" title="Odd-toed ungulate">
Odd-toed ungulate

The order includes about 17 species divided into three families: Equidae (horses, asses, and zebras), Rhinocerotidae (rhinoceroses), and Tapiridae (tapirs).

Despite their very different appearances, they were recognized as related families in the 19th century by the zoologist Richard Owen, who also coined the order name.

The largest odd-toed ungulates are rhinoceroses, and the extinct "Paraceratherium", a hornless rhino from the Oligocene, is considered one of the largest land mammals of all time. At the other extreme, an early member of the order, the prehistoric horse "Eohippus", had a withers height of only . Apart from dwarf varieties of the domestic horse and donkey, perissodactyls reach a body length of and a weight of . While rhinos have only sparse hair and exhibit a thick epidermis, tapirs and horses have dense, short coats. Most species are grey or brown, although zebras and young tapirs are striped.

The main axes of both the front and rear feet pass through the third toe, which is always the largest. The remaining toes have been reduced in size to varying degrees. Tapirs, which are adapted to walking on soft ground, have four toes on their fore feet and three on their hind feet. Living rhinos have three toes on both the front and hind feet. Modern equines possess only a single toe; however, their feet are equipped with hooves, which almost completely cover the toe. Rhinos and tapirs, by contrast, have hooves covering only the leading edge of the toes, with the bottom being soft.

The ulnae and fibulae are reduced in horses. A common feature that clearly distinguishes this group from other mammals is the saddle-shaped ankle between the astragalus and the scaphoid, which greatly restricts the mobility of the foot. The thigh is relatively short, and the clavicle is absent.

Odd-toed ungulates have a long upper jaw with an extended diastema between the front and cheek teeth, giving them an elongated head. The various forms of snout between families are due to differences in the form of the premaxilla. The lacrimal bone has projecting cusps in the eye sockets and a wide contact with the nasal bone. The temporomandibular joint is high and the mandible is enlarged.

Rhinos have one or two horns made of agglutinated keratin, unlike the horns of even-toed ungulates, which have a bony core.

The number and form of the teeth vary according to diet. The incisors and canines can be very small or completely absent, as in the two African species of rhinoceros. In the horses, usually only the males possess canines. The surface shape and height of the molars is heavily dependent on whether soft leaves or hard grass makes up the main component of their diets. Three or four cheek teeth are present on each jaw half, so the dental formula of odd-toed ungulates is: 
All perissodactyls are hindgut fermenters. In contrast to ruminants, hindgut fermenters store digested food that has left the stomach in an enlarged cecum, where the food is digested by bacteria. No gallbladder is present. The stomach of perissodactyls is simply built, while the cecum accommodates up to in horses. The intestine is very long, reaching up to in horses. Extraction of nutrients from food is relatively inefficient, which probably explains why no odd-toed ungulates are small; for large animals, nutritional requirements per unit of body weight are lower and the surface-area-to-volume ratio is smaller.

The present distribution of most perissodactyl species is only a small fraction of their original range. Members of this group are now found only in Central and South America, eastern and southern Africa, and central, southern, and southeastern Asia. During the peak of odd-toed ungulate existence, from the Eocene to the Oligocene, perissodactyls were distributed over much of the globe, the only exceptions being Australia and Antarctica. Horses and tapirs arrived in South America after the formation of the Isthmus of Panama in the Pliocene, around 3 million years ago. In North America, they died out around 10,000 years ago, while in Europe, the tarpans disappeared in the 19th century. Hunting and habitat restriction have reduced the present-day species to fragmented relict populations. In contrast, domesticated horses and donkeys have gained a worldwide distribution, and feral animals of both species are now also found in regions outside of their original range, such as in Australia.

Perissodactyls inhabit a number of different habitats, leading to different lifestyles. Tapirs are solitary and inhabit mainly tropical rainforests. Rhinos tend to live alone in rather dry savannas, and in Asia, wet marsh or forest areas. Horses inhabit open areas such as grasslands, steppes, or semi-deserts, and live together in groups. Odd-toed ungulates are exclusively herbivores that feed, to varying degrees, on grasses, leaves, and other plant parts. A distinction is often made between primarily grass feeders (white rhinos, equines) and leaf feeders (tapirs, other rhinos).

Odd-toed ungulates are characterized by a long gestation period and a small litter size, usually delivering a single young. The gestation period is 330–500 days, being longest in the rhinos. Newborn perissodactyls are precocial; young horses can follow the mother after a few hours. The young are nursed for a relatively long time, often into their second year, reaching sexual maturity around eight or ten years old. Perissodactyls are long-lived, with several species reaching an age of almost 50 years in captivity.

Traditionally, the odd-toed ungulates were classified with other mammals such as artiodactyls, hyraxes, mammals with a proboscis, and other "ungulates". A close family relationship with hyraxes was suspected based on similarities in the construction of the ear and the course of the carotid artery.

Recent molecular genetic studies, however, have shown the ungulates to be polyphyletic, meaning that in some cases the similarities are the result of convergent evolution rather than common ancestry. Elephants and hyraxes are now considered to belong to Afrotheria, so are not closely related to the perissodactyls. These, in turn, are in the Laurasiatheria, a superorder that had its origin in the former supercontinent Laurasia. Molecular genetic findings suggest that the cloven Artiodactyla (containing the cetaceans as a deeply nested subclade) are the sister taxon of the Perissodactyla; together, the two groups form the Euungulata. More distant are the bats (Chiroptera) and Ferae (a common taxon of carnivorans, Carnivora, and pangolins, Pholidota). In a discredited alternative scenario, a close relationship exists between perissodactyls, carnivores, and bats, this assembly comprising the Pegasoferae.

According to studies published in March 2015, odd-toed ungulates are in a close family relationship with at least some of the so-called Meridiungulata, a very diverse group of mammals living from the Paleocene to the Pleistocene in South America, whose systematic unity is largely unexplained. Some of these were classified on the basis of their paleogeographic distribution. However, a close relationship can be worked out to perissodactyls by means of protein sequencing and comparison with fossil collagen from remnants of phylogenetically young members of the Meridiungulata (specifically "Macrauchenia" from the Litopterna and "Toxodon" from the Notoungulata). Both kinship groups, the odd-toed ungulates and the Litopterna-Notoungulata, are now in the higher-level taxon of Panperissodactyla. This kinship group is included among the Euungulata which also contains the even-toed ungulates and whales (Artiodactyla). The separation of the Litopterna-Notoungulata group from the perissodactyls probably took place before the Cretaceous–Paleogene extinction event. "Condylarths" can probably be considered the starting point for the development of the two groups, as they represent a heterogeneous group of primitive ungulates that mainly inhabited the northern hemisphere in the Paleogene.

Odd-toed ungulates (Perissodactyla) comprise three living families with around 17 species—in the horse the exact count is still controversial. Rhinos and tapirs are more closely related to each other than to the horses. The separation of horses from other perissodactyls took place according to molecular genetic analysis in the Paleocene some 56 million years ago, while the rhinos and tapirs split off in the lower-middle Eocene, about 47 million years ago. 

There are many perissodactyl fossils of multivariant form. The major lines of development include the following groups:


Relationships within the large group of odd-toed ungulates are not fully understood. Initially, after the establishment of "Perissodactyla" by Richard Owen in 1848, the present-day representatives were considered equal in rank. In the first half of the 20th century, a more systematic differentiation of odd-toed ungulates began, based on a consideration of fossil forms, and they were placed in two major suborders: Hippomorpha and Ceratomorpha. The Hippomorpha comprises today's horses and their extinct members (Equoidea); the Ceratomorpha consist of tapirs and rhinos plus their extinct members (Tapiroidea and Rhinocerotoidea). The names Hippomorpha and Ceratomorpha were introduced in 1937 by Horace Elmer Wood, in response to criticism of the name "Solidungula" that he proposed three years previously. It had been based on the grouping of horses and Tridactyla and on the rhinoceros/tapir complex. The extinct brontotheriidae were also classified under Hippomorpha and therefore possess a close relationship to horses. Some researchers accept this assignment because of similar dental features, but there is also the view that a very basal position within the odd-toed ungulates places them rather in the group of "Titanotheriomorpha".

Originally, the Chalicotheriidae were seen as members of Hippomorpha, and presented as such in 1941. William Berryman Scott thought that, as claw-bearing perissodactyls, they belong in the new suborder Ancylopoda (where Ceratomorpha and Hippomorpha as odd-toed ungulates were combined in the group of Chelopoda). The term Ancylopoda, coined by Edward Drinker Cope in 1889, had been established for chalicotheres. However, further morphological studies from the 1960s showed a middle position of Ancylopoda between Hippomorpha and Ceratomorpha. Leonard Burton Radinsky saw all three major groups of odd-toed ungulates as peers, based on the extremely long and independent phylogenetic development of the three lines. In the 1980s, Jeremy J. Hooker saw a general similarity of Ancylopoda and "Ceratomorpha" based on dentition, especially in the earliest members, leading to the unification in 1984 of the two submissions in the interim order, "Tapiromorpha". At the same time he expanded the Ancylopoda to include the "Lophiodontidae". The name "Tapiromorpha" goes back to Ernst Haeckel, who coined it in 1873, but it was long considered synonymous to Ceratomorpha because Wood had not considered it in 1937 when Ceratomorpha were named, since the term had been used quite differently in the past. Also in 1984, Robert M. Schoch used the conceptually similar term Moropomorpha, which today applies synonymously to Tapiromorpha. Included within the Tapiromorpha are the now extinct Isectolophidae, a sister group of the Ancylopoda-Ceratomorpha group and thus the most primitive members of this relationship complex.

The evolutionary development of Perissodactyla is well documented in the fossil record. Numerous finds are evidence of the adaptive radiation of this group, which was once much more varied and widely dispersed. "Radinskya" from the late Paleocene of East Asia is often considered to be one of the oldest close relatives of the ungulates. Its 8 cm skull must have belonged to a very small and primitive animal with a π-shaped crown pattern on the enamel of its rear molars similar to that of perissodactyls and their relatives, especially the rhinos. Finds of "Cambaytherium" and "Kalitherium" in the Cambay shale of western India indicate an origin in Asia dating to the Lower Eocene roughly 54.5 million years ago. Their teeth also show similarities to "Radinskya" as well as to the Tethytheria clade. The saddle-shaped configuration of the navicular joints and the mesaxonic construction of the front and hind feet also indicates a close relationship to Tethytheria. However, this construction deviates from that of "Cambaytherium", indicating that it is actually a member of a sister group. Ancestors of Perissodactyla may have arrived via an island bridge from the Afro-Arab landmass onto the Indian subcontinent as it drifted north towards Asia.

The alignment of hyopsodontids and phenacodontids to Perissodactyla in general suggests an older Laurasian origin and distribution for the clade, dispersed across the northern continents already in the early Paleocene. These forms already show a fairly well-developed molar morphology, with no intermediary forms as evidence of the course of its development. The close relationship between meridiungulate mammals and perissoodactyls in particular is of interest since the latter appear in South America soon after the K–T event, implying rapid ecological radiation and dispersal after the mass extinction.

The Perissodactyla appear relatively abruptly at the beginning of the Lower Paleocene before about 63 million years ago, both in North America and Asia, in the form of phenacodontids and hyopsodontids. The oldest finds from an extant group originate among other sources from "Sifrhippus", an ancestor of the horses from the Willswood lineup in northwestern Wyoming. The distant ancestors of tapirs appeared not too long after that in the Ghazij lineup in Balochistan, such as "Ganderalophus", as well as "Litolophus" from the Chalicotheriidae line, or "Eotitanops" from the group of brontotheriidae. Initially, the members of the different lineages looked quite similar with an arched back and generally four toes on the front and three on the hind feet. "Eohippus", which is considered a member of the horse family, outwardly resembled "Hyrachyus", the first representative of the rhino and tapir line. All were small compared to later forms and lived as fruit and foliage eaters in forests. The first of the megafauna to emerge were the brontotheres, in the Middle and Upper Eocene. "Megacerops", known from North America, reached a withers height of and could have weighed just over . The decline of brontotheres at the end of the Eocene is associated with competition arising from the advent of more successful herbivores.

More successful lines of odd-toed ungulates emerged at the end of the Eocene when dense jungles gave way to steppe, such as the chalicotheriid rhinos, and their immediate relatives; their development also began with very small forms. "Paraceratherium", one of the largest mammals ever to walk the earth, evolved during this era. They weighed up to and lived throughout the Oligocene in Eurasia. About 20 million years ago at the onset of the Miocene the perissodactyls first reached Africa when it became connected to Eurasia because of the closing of the Tethys Ocean. For the same reason, however, new animals such as the mammoths also entered the ancient settlement areas of odd-toed ungulates, creating competition that led to the extinction of some of their lines. The rise of ruminants, which occupied similar ecological niches and had a much more efficient digestive system, is also associated with the decline in diversity of odd-toed ungulates. A significant cause for the decline of perissodactyls was climate change during the Miocene, leading to a cooler and drier climate accompanied by the spread of open landscapes. However, some lines flourished, such as the horses and rhinos; anatomical adaptations made it possible for them to consume tougher grass food. This led to open land forms that dominated the newly created landscapes. With the emergence of the Isthmus of Panama in the Pliocene, perissodactyls and other megafauna were given access to one of their last habitable continents: South America. However, many perissodactyls became extinct at the end of the ice ages, including American horses and the "Elasmotherium". Whether over-hunting by humans (overkill hypothesis), climatic change, or a combination of both factors was responsible for the extinction of ice age mega-fauna, remains controversial.

In 1758, in his seminal work "Systema Naturae", Linnaeus (1707–1778) classified horses ("Equus") together with hippos ("Hippopotamus"). At that time, this category also included the tapirs ("Tapirus"), more precisely the lowland or South American tapir ("Tapirus terrestus"), the only tapir then known in Europe. Linnaeus classified this tapir as "Hippopotamus terrestris" and put both genera in the group of the "Belluae" ("beasts"). He combined the rhinos with the Glires, a group now consisting of the lagomorphs and rodents. Mathurin Jacques Brisson (1723–1806) first separated the tapirs and hippos in 1762 with the introduction of the concept "le tapir". He also separated the rhinos from the rodents, but did not combine the three families now known as the odd-toed ungulates. In the transition to the 19th century, the individual perissodactyl genera were associated with various other groups, such as the proboscidean and even-toed ungulates. In 1795, Étienne Geoffroy Saint-Hilaire (1772–1844) and Georges Cuvier (1769–1832) introduced the term "pachyderm" (Pachydermata), including in it not only the rhinos and elephants, but also the hippos, pigs, peccaries, tapirs and hyrax . The horses were still generally regarded as a group separate from other mammals and were often classified under the name "Solidungula" or "Solipèdes", meaning "one-hoof animal".

In 1861, Henri Marie Ducrotay de Blainville (1777–1850) classified ungulates by the structure of their feet, differentiating those with an even number of toes from those with an odd number. He moved the horses as "solidungulate" over to the tapirs and rhinos as "multungulate" animals and referred to all of them together as "onguligrades à doigts impairs", coming close to the concept of the odd-toed ungulate as a systematic unit. Richard Owen (1804–1892) quoted Blainville in his study on fossil mammals of the Isle of Wight and introduced the name "Perissodactyla".

In 1884, Othniel Charles Marsh (1831–1899) came up with the concept "Mesaxonia", which he used for what are today called the odd-toed ungulates, including their extinct relatives, but explicitly excluding the hyrax. "Mesaxonia" is now considered a synonym of "Perissodactyla", but it was sometimes also used for the true odd-toed ungulates as a subcategory (rhinos, horses, tapirs), while "Perissodactyla" stood for the entire order, including the hyrax. The assumption that hyraxes were "Perissodactyla" was held well into the 20th century. Only with the advent of molecular genetic research methods had it been recognized that the hyrax is not closely related to perissodactyls but rather to elephants and manatees.

The domestic horse and the donkey play an important role in human history particularly as transport, work and pack animals. The domestication of both species began several millennia BCE. Due to the motorisation of agriculture and the spread of automobile traffic, such use has declined sharply in Western industrial countries; riding is usually undertaken more as a hobby or sport. In less developed regions of the world, the traditional uses for these animals are, however, still widespread. To a lesser extent, horses and donkeys are also kept for their meat and their milk.

In contrast, the existence in the wild of almost all other odd-toed ungulates species has declined dramatically because of hunting and habitat destruction. The quagga is extinct and Przewalski's horse was once eradicated in the wild.

Present threat levels, according to the International Union for Conservation of Nature (2012):




</doc>
<doc id="23565" url="https://en.wikipedia.org/wiki?curid=23565" title="Pai gow">
Pai gow

Pai gow () is a Chinese gambling game, played with a set of 32 Chinese dominoes. It is played in major casinos in China (including Macau); the United States (including Boston, Massachusetts; Las Vegas, Nevada; Reno, Nevada; Connecticut; Atlantic City, New Jersey; Pennsylvania; Mississippi; and cardrooms in California); Canada (including Edmonton, Alberta and Calgary, Alberta); Australia; and, New Zealand.

The name "pai gow" is sometimes used to refer to a card game called pai gow poker (or “double-hand poker”), which is loosely based on pai gow.

Tiles are shuffled on the table and are arranged into eight face-down stacks of four tiles each in an assembly known as the "woodpile". Individual stacks or tiles may then be moved in specific ways to rearrange the woodpile, after which the players place their bets.

Next, each player (including the dealer) is given one stack of tiles and must use them to form two hands of two tiles each. The hand with the lower value is called the "front hand", and the hand with the higher value is called the "rear hand". If a player's front hand beats the dealer's front hand, and the player's rear hand beats the dealer's rear hand, then that player wins the bet. If a player's front and rear hands both lose to the dealer's respective hands, the player loses the bet. If one hand wins and the other loses, the player is said to "push", and gets back only the money he or she bet. Generally seven players will play, and each player's hands are compared only against the dealer's hands; comparisons are always front-front and rear-rear, never one of each.

There are 35,960 possible ways to select 4 of the 32 tiles when the 32 tiles are considered distinguishable. However, there are 3620 distinct sets of 4 tiles when the tiles of a pair are considered indistinguishable. There are 496 ways to select 2 of the 32 tiles when the 32 tiles are considered distinguishable. There are 136 distinct hands (pairs of tiles) when the tiles of a pair are considered indistinguishable.

The name "pai gow" is loosely translated as "make nine" or "card nine". This reflects the fact that, with a few high-scoring exceptions, the maximum score for a hand is nine. If a hand consists of two tiles that do not form a pair, its value is determined by adding up the total number of pips on the tiles and dropping the tens digit (if any). Examples:


There are special ways in which a hand can score more than nine points. The double-one tiles and double-six tiles are known as the "Day" and "Teen" tiles, respectively. The combination of a Day or Teen with an eight results in a "Gong", worth 10 points, while putting either of them with a nine creates a "Wong", worth 11. However, when a Day or Teen is paired with any other tile, the standard scoring rules apply.

The 1-2 and the 2-4 tiles are called "Gee Joon" tiles and act as limited wild cards. When used as part of a hand, these tiles may be scored as either 3 or 6, whichever results in a higher hand value. For example, a hand of 1-2 and 5-6 scores as seven rather than four.

The 32 tiles in a Chinese dominoes set can be arranged into 16 pairs, as shown in the picture at the top of this article. Eleven of these pairs have identical tiles, and five of these pairs are made up of two tiles that score the same, but look different. (The latter group includes the Gee Joon tiles, which can score the same, whether as three or six.) Any hand consisting of a pair outscores a non-pair, regardless of the pip counts. (Pairs are often thought of as being worth 12 points each.)

When the player and dealer both have a pair, the higher-ranked pair wins. Ranking is determined not by the sum of the tiles' pips, but rather by aesthetics; the order must be memorized. The highest pairs are the Gee Joon tiles, the Teens, the Days, and the red eights. The lowest pairs are the mismatched nines, eights, sevens, and fives.

When the player and dealer display hands with the same score, the one with the highest-valued tile (based on the pair rankings described above) is the winner. For example, a player's hand of 3-4 and 2-2 and a dealer's hand of 5-6 and 5-5 would each score one point. However, since the dealer's 5-5 outranks the other three tiles, he would win the hand.

If the scores are tied, and if the player and dealer each have an identical highest-ranking tile, the hand is ruled a "copy" and the dealer wins. For example, if the player held 2-2 and 1–6, and the dealer held 2-2 and 3–4, the dealer would win since the scores (1 each) and the higher tiles (2-2) are the same. The lower-ranked tile in each hand is never used to break a tie.

There are two exceptions to the method described above. First, although the Gee Joon tiles form the highest-ranking pair, they are considered to have no value when evaluating ties. Second, any zero-zero tie is won by the dealer, regardless of the tiles in the two hands.

The key element of pai gow strategy is to present the optimal front and rear hands based on the tiles dealt to the player. There are three ways to arrange four tiles into two hands when no two of them form a pair. However, if there is at least one pair among the tiles, there are only two distinct ways to form two hands.

Using the tiles shown at right, the following hands and scores are possible:


The player must decide which combination is most likely to give a set of front/rear hands that can beat the dealer, or at least break a tie in the player's favor. In some cases, a player with weaker tiles may deliberately attempt to attain a push so as to avoid losing the bet outright. Many players rely on superstition or tradition to choose tile pairings.




</doc>
<doc id="23572" url="https://en.wikipedia.org/wiki?curid=23572" title="Partially ordered set">
Partially ordered set

In mathematics, especially order theory, a partially ordered set (also poset) formalizes and generalizes the intuitive concept of an ordering, sequencing, or arrangement of the elements of a set. A poset consists of a set together with a binary relation indicating that, for certain pairs of elements in the set, one of the elements precedes the other in the ordering. The relation itself is called a "partial order." The word "partial" in the names "partial order" and "partially ordered set" is used as an indication that not every pair of elements needs to be comparable. That is, there may be pairs of elements for which neither element precedes the other in the poset. Partial orders thus generalize total orders, in which every pair is comparable. 

Formally, a partial order is any binary relation that is reflexive (each element is comparable to itself), antisymmetric (no two different elements precede each other), and transitive (the start of a chain of precedence relations must precede the end of the chain).

One familiar example of a partially ordered set is a collection of people ordered by genealogical descendancy. Some pairs of people bear the descendant-ancestor relationship, but other pairs of people are incomparable, with neither being a descendant of the other. 

A poset can be visualized through its Hasse diagram, which depicts the ordering relation.

A (non-strict) partial order is a binary relation ≤ over a set "P" satisfying particular axioms which are discussed below. When "a" ≤ "b", we say that "a" is related to "b". (This does not imply that "b" is also related to "a", because the relation need not be symmetric.) 

The axioms for a non-strict partial order state that the relation ≤ is reflexive, antisymmetric, and transitive. That is, for all "a", "b", and "c" in "P", it must satisfy:


In other words, a partial order is an antisymmetric preorder.

A set with a partial order is called a partially ordered set (also called a poset). The term "ordered set" is sometimes also used, as long as it is clear from the context that no other kind of order is meant. In particular, totally ordered sets can also be referred to as "ordered sets", especially in areas where these structures are more common than posets.

For "a, b", elements of a partially ordered set "P", if "a" ≤ "b" or "b" ≤ "a", then "a" and "b" are comparable. Otherwise they are incomparable. In the figure on top-right, e.g. {x} and {x,y,z} are comparable, while {x} and {y} are not. A partial order under which every pair of elements is comparable is called a total order or linear order; a totally ordered set is also called a chain (e.g., the natural numbers with their standard order). A subset of a poset in which no two distinct elements are comparable is called an antichain (e.g. the set of singletons in the top-right figure). An element "a" is said to be strictly less than an element b, if "a" ≤ "b" and "a"≠"b". An element "a" is said to be covered by another element "b", written "a"⋖"b" (or "a"<:"b"), if "a" is strictly less than "b" and no third element "c" fits between them; formally: if both "a"≤"b" and "a"≠"b" are true, and "a"≤"c"≤"b" is false for each "c" with "a"≠"c"≠"b". A more concise definition will be given below using the strict order corresponding to "≤". For example, {x} is covered by {x,z} in the top-right figure, but not by {x,y,z}.

Standard examples of posets arising in mathematics include:


There are several notions of "greatest" and "least" element in a poset "P", notably:

For example, consider the positive integers, ordered by divisibility: 1 is a least element, as it divides all other elements; on the other hand this poset does not have a greatest element (although if one would include 0 in the poset, which is a multiple of any integer, that would be a greatest element; see figure). This partially ordered set does not even have any maximal elements, since any "g" divides for instance 2"g", which is distinct from it, so "g" is not maximal. If the number 1 is excluded, while keeping divisibility as ordering on the elements greater than 1, then the resulting poset does not have a least element, but any prime number is a minimal element for it. In this poset, 60 is an upper bound (though not a least upper bound) of the subset {2,3,5,10}, which does not have any lower bound (since 1 is not in the poset); on the other hand 2 is a lower bound of the subset of powers of 2, which does not have any upper bound.

In order of increasing strength, i.e., decreasing sets of pairs, three of the possible partial orders on the Cartesian product of two partially ordered sets are (see figures):

All three can similarly be defined for the Cartesian product of more than two sets.

Applied to ordered vector spaces over the same field, the result is in each case also an ordered vector space.

See also orders on the Cartesian product of totally ordered sets.

Another way to combine two posets is the ordinal sum (or linear sum), "Z" = "X" ⊕ "Y", defined on the union of the underlying sets "X" and "Y" by the order "a" ≤ "b" if and only if:

If two posets are well-ordered, then so is their ordinal sum.
The ordinal sum operation is one of two operations used to form series-parallel partial orders, and in this context is called series composition. The other operation used to form these orders, the disjoint union of two partially ordered sets (with no order relation between elements of one set and elements of the other set) is called in this context parallel composition.

In some contexts, the partial order defined above is called a non-strict (or reflexive) partial order. In these contexts, a strict (or irreflexive) partial order "<" is a binary relation that is irreflexive, transitive and asymmetric, i.e. which satisfies for all "a", "b", and "c" in "P":


Strict and non-strict partial orders are closely related. A non-strict partial order may be converted to a strict partial order by removing all relationships of the form "a" ≤ "a". Conversely, a strict partial order may be converted to a non-strict partial order by adjoining all relationships of that form. Thus, if "≤" is a non-strict partial order, then the corresponding strict partial order "<" is the irreflexive kernel given by:

Conversely, if "<" is a strict partial order, then the corresponding non-strict partial order "≤" is the reflexive closure given by:

This is the reason for using the notation "≤".

Using the strict order "<", the relation ""a" is covered by "b"" can be equivalently rephrased as ""a"<"b", but not "a"<"c"<"b" for any "c"".
Strict partial orders are useful because they correspond more directly to directed acyclic graphs (dags): every strict partial order is a dag, and the transitive closure of a dag is both a strict partial order and also a dag itself.

The inverse (or converse) of a partial order relation ≤ is the converse of ≤. Typically denoted ≥, it is the relation that satisfies "x" ≥ "y" if and only if "y" ≤ "x". The inverse of a partial order relation is reflexive, transitive, and antisymmetric, and hence itself a partial order relation. The order dual of a partially ordered set is the same set with the partial order relation replaced by its inverse. The irreflexive relation > is to ≥ as < is to ≤.

Any one of the four relations ≤, <, ≥, and > on a given set uniquely determines the other three.

In general two elements "x" and "y" of a partial order may stand in any of four mutually exclusive relationships to each other: either "x" < "y", or "x" = "y", or "x" > "y", or "x" and "y" are "incomparable" (none of the other three). A totally ordered set is one that rules out this fourth possibility: all pairs of elements are comparable and we then say that trichotomy holds. The natural numbers, the integers, the rationals, and the reals are all totally ordered by their algebraic (signed) magnitude whereas the complex numbers are not. This is not to say that the complex numbers cannot be totally ordered; we could for example order them lexicographically via "x"+iy" < "u"+iv" if and only if "x" < "u" or ("x" = "u" and "y" < "v"), but this is not ordering by magnitude in any reasonable sense as it makes 1 greater than 100i. Ordering them by absolute magnitude yields a preorder in which all pairs are comparable, but this is not a partial order since 1 and i have the same absolute magnitude but are not equal, violating antisymmetry.

Given two partially ordered sets ("S",≤) and ("T",≤), a function "f": "S" → "T" is called order-preserving, or monotone, or isotone, if for all "x" and "y" in "S", "x"≤"y" implies "f"("x") ≤ "f"("y").
If ("U",≤) is also a partially ordered set, and both "f": "S" → "T" and "g": "T" → "U" are order-preserving, their composition ("g"∘"f"): "S" → "U" is order-preserving, too.
A function "f": "S" → "T" is called order-reflecting if for all "x" and "y" in "S", "f"("x") ≤ "f"("y") implies "x"≤"y".
If "f" is both order-preserving and order-reflecting, then it is called an order-embedding of ("S",≤) into ("T",≤).
In the latter case, "f" is necessarily injective, since "f"("x") = "f"("y") implies "x" ≤ "y" and "y" ≤ "x". If an order-embedding between two posets "S" and "T" exists, one says that "S" can be embedded into "T". If an order-embedding "f": "S" → "T" is bijective, it is called an order isomorphism, and the partial orders ("S",≤) and ("T",≤) are said to be isomorphic. Isomorphic orders have structurally similar Hasse diagrams (cf. right picture). It can be shown that if order-preserving maps "f": "S" → "T" and "g": "T" → "S" exist such that "g"∘"f" and "f"∘"g" yields the identity function on "S" and "T", respectively, then "S" and "T" are order-isomorphic.
For example, a mapping "f": ℕ → ℙ(ℕ) from the set of natural numbers (ordered by divisibility) to the power set of natural numbers (ordered by set inclusion) can be defined by taking each number to the set of its prime divisors. It is order-preserving: if "x" divides "y", then each prime divisor of "x" is also a prime divisor of "y". However, it is neither injective (since it maps both 12 and 6 to {2,3}) nor order-reflecting (since 12 doesn't divide 6). Taking instead each number to the set of its prime power divisors defines a map "g": ℕ → ℙ(ℕ) that is order-preserving, order-reflecting, and hence an order-embedding. It is not an order-isomorphism (since it e.g. doesn't map any number to the set {4}), but it can be made one by restricting its codomain to "g"(ℕ). The right picture shows a subset of ℕ and its isomorphic image under "g". The construction of such an order-isomorphism into a power set can be generalized to a wide class of partial orders, called distributive lattices, see "Birkhoff's representation theorem".

Sequence [ A001035] in OEIS gives the number of partial orders on a set of "n" labeled elements:

The number of strict partial orders is the same as that of partial orders.

If the count is made only up to isomorphism, the sequence 1, 1, 2, 5, 16, 63, 318, … is obtained.

A partial order ≤ on a set "X" is an extension of another partial order ≤ on "X" provided that for all elements "x" and "y" of "X", whenever "x" ≤ "y", it is also the case that "x" ≤ "y". A linear extension is an extension that is also a linear (i.e., total) order. Every partial order can be extended to a total order (order-extension principle).

In computer science, algorithms for finding linear extensions of partial orders (represented as the reachability orders of directed acyclic graphs) are called topological sorting.

Every poset (and every preordered set) may be considered as a category where, for objects "x" and "y", there is at most one morphism from "x" to "y". More explicitly, let hom("x", "y") = {("x", "y")} if "x" ≤ "y" (and otherwise the empty set) and ("y", "z")∘("x", "y") = ("x", "z"). Such categories are sometimes called "posetal".

Posets are equivalent to one another if and only if they are isomorphic. In a poset, the smallest element, if it exists, is an initial object, and the largest element, if it exists, is a terminal object. Also, every preordered set is equivalent to a poset. Finally, every subcategory of a poset is isomorphism-closed.

If "P" is a partially ordered set that has also been given the structure of a topological space, then it is customary to assume that formula_1 is a closed subset of the topological product space formula_2. Under this assumption partial order relations are well behaved at limits in the sense that if formula_3, and formula_4, and for all formula_5   formula_6, then formula_7.

An "interval" in a poset "P" is a subset of "P" with the property that, for any "x" and "y" in and any "z" in "P", if "x" ≤ "z" ≤ "y", then "z" is also in . (This definition generalizes the "interval" definition for real numbers.)

For "a" ≤ "b", the closed interval is the set of elements "x" satisfying "a" ≤ "x" ≤ "b" (i.e. "a" ≤ "x" and "x" ≤ "b"). It contains at least the elements "a" and "b".

Using the corresponding strict relation "<", the open interval is the set of elements "x" satisfying "a" < "x" < "b" (i.e. "a" < "x" and "x" < "b"). An open interval may be empty even if "a" < "b". For example, the open interval on the integers is empty since there are no integers such that .

The "half-open intervals" and are defined similarly.

Sometimes the definitions are extended to allow "a" > "b", in which case the interval is empty.

An interval is bounded if there exist elements "a" and "b" of "P" such that . Every interval that can be represented in interval notation is obviously bounded, but the converse is not true. For example, let as a subposet of the real numbers. The subset is a bounded interval, but it has no infimum or supremum in "P", so it cannot be written in interval notation using elements of "P".

A poset is called locally finite if every bounded interval is finite. For example, the integers are locally finite under their natural ordering. The lexicographical order on the cartesian product ℕ×ℕ is not locally finite, since .
Using the interval notation, the property ""a" is covered by "b"" can be rephrased equivalently as ["a", "b"] = {"a", "b"}.

This concept of an interval in a partial order should not be confused with the particular class of partial orders known as the interval orders.




</doc>
<doc id="23574" url="https://en.wikipedia.org/wiki?curid=23574" title="Psyche">
Psyche

Psyche (Psyché in French) is the Greek term for "soul" or "spirit" (ψυχή).

It may also refer to:









</doc>
<doc id="23575" url="https://en.wikipedia.org/wiki?curid=23575" title="Parmenides">
Parmenides

Parmenides of Elea (; ; ) was a pre-Socratic Greek philosopher from Elea in Magna Graecia (meaning "Great Greece," the term which Romans gave to Greek-populated coastal areas in Southern Italy). He is thought to have been in his prime (or "floruit") around 475 BC. 

Parmenides has been considered the founder of metaphysics or ontology and has influenced the whole history of Western philosophy. He was the founder of the Eleatic school of philosophy, which also included Zeno of Elea and Melissus of Samos. Zeno's paradoxes of motion were to defend Parmenides' view.

The single known work by Parmenides is a poem, "On Nature", only fragments of which survive, containing the first sustained argument in the history of Western philosophy. In it, Parmenides prescribes two views of reality. In "the way of truth" (a part of the poem), he explains how all reality is one, change is impossible, and existence is timeless, uniform, and necessary. In "the way of opinion", Parmenides explains the world of appearances, in which one's sensory faculties lead to conceptions which are false and deceitful, yet he does offer a cosmology.

Parmenides' philosophy has been explained with the slogan "whatever is is, and what is not cannot be". He is also credited with the phrase out of nothing nothing comes. He argues that "A is not" can never be thought or said truthfully, and thus despite appearances everything exists as one, giant, unchanging thing. This is generally considered one of the first digressions into the philosophical concept of being, and has been contrasted with Heraclitus's statement that "No man ever steps into the same river twice" as one of the first digressions into the philosophical concept of becoming. Scholars have generally believed that either Parmenides was responding to Heraclitus, or Heraclitus to Parmenides.

Parmenides' views have remained relevant in philosophy, even thousands of years after his death. Alexius Meinong, much like Parmenides, defended the view that even the "golden mountain" is real since it can be talked about. The rivalry between Heraclitus and Parmenides has also been re-introduced in debates in the philosophy of time between A theory and B theory.

Parmenides was born in the Greek colony of Elea (now Ascea), which, according to Herodotus, had been founded shortly before 535 BC. He was descended from a wealthy and illustrious family. It was said that he had written the laws of the city. 

His dates are uncertain; according to doxographer Diogenes Laërtius, he flourished just before 500 BC, which would put his year of birth near 540 BC, but in the dialogue "Parmenides" Plato has him visiting Athens at the age of 65, when Socrates was a young man, c. 450 BC, which, if true, suggests a year of birth of c. 515 BC. 

Parmenides was the founder of the School of Elea, which also included Zeno of Elea and Melissus of Samos. His most important pupil was Zeno, who according to Plato was 25 years his junior, and was regarded as his "eromenos".

He was said to have been a pupil of Xenophanes, and regardless of whether they actually knew each other, Xenophanes' philosophy is the most obvious influence on Parmenides. 
Eusebius quoting Aristocles of Messene says that Parmenides was part of a line of philosophy that culminated in Pyrrhonism. This line begins with Xenophenes and goes through Parmenides, Melissus of Samos, Zeno of Elea, Leucippus, Democritus, Protagoras, Nessas of Chios, Metrodorus of Chios, Diogenes of Smyrna, Anaxarchus, and finally Pyrrho.

Though there are no obvious Pythagorean elements in his thought, Diogenes Laërtius describes Parmenides as a disciple of "Ameinias, son of Diochaites, the Pythagorean". According to Sir William Smith, in "Dictionary of Greek and Roman Biography and Mythology" (1870):
"Others content themselves with reckoning Parmenides as well as Zeno as belonging to the Pythagorean school, or with speaking of a "Parmenidean life", in the same way as a "Pythagorean life" is spoken of."

The first purported hero cult of a philosopher we know of was Parmenides' dedication of a heroon to his Ameinias in Elea.

Parmenides is one of the most significant of the pre-Socratic philosophers. His single known work, a poem conventionally titled "On Nature", has survived only in fragments. Approximately 160 verses remain today from an original total that was probably near 800. The poem was originally divided into three parts:

The proem is a narrative sequence in which the narrator travels "beyond the beaten paths of mortal men" to receive a revelation from an unnamed goddess (generally thought to be Persephone or Dikē) on the nature of reality. "Aletheia", an estimated 90% of which has survived, and "doxa", most of which no longer exists, are then presented as the spoken revelation of the goddess without any accompanying narrative.

Parmenides attempted to distinguish between the unity of nature and its variety, insisting in the "Way of Truth" upon the reality of its unity, which is therefore the object of knowledge, and upon the unreality of its variety, which is therefore the object, not of knowledge, but of opinion. In the "Way of Opinion" he propounded a theory of the world of seeming and its development, pointing out, however, that, in accordance with the principles already laid down, these cosmological speculations do not pretend to anything more than mere appearance.

In the proem, Parmenides describes the journey of the poet, escorted by maidens ("the daughters of the Sun made haste to escort me, having left the halls of Night for the light"), from the ordinary daytime world to a strange destination, outside our human paths. Carried in a whirling chariot, and attended by the daughters of Helios the Sun, the man reaches a temple sacred to an unnamed goddess (variously identified by the commentators as Nature, Wisdom, Necessity or Themis), by whom the rest of the poem is spoken. The goddess resides in a well-known mythological space: where Night and Day have their meeting place. Its essential character is that here all opposites are undivided, or one. He must learn all things, she tells him – both truth, which is certain, and human opinions, which are uncertain – for though one cannot rely on human opinions, they represent an aspect of the whole truth.Welcome, youth, who come attended by immortal charioteers and mares which bear you on your journey to our dwelling. For it is no evil fate that has set you to travel on this road, far from the beaten paths of men, but right and justice. It is meet that you learn all things — both the unshakable heart of well-rounded truth and the opinions of mortals in which there is not true belief. (B 1.24–30)

The section known as "the way of truth" discusses that which is real and contrasts with the argument in the section called "the way of opinion," which discusses that which is illusory. Under the "way of truth," Parmenides stated that there are two ways of inquiry: that it "is", on the one side, and that it "is not" on the other side. He said that the latter argument is never feasible because there is no thing that can "not be": "For never shall this prevail, that things that are not are." Thinking and the thought that it is are the same; for you will not find thinking apart from what is, in relation to which it is uttered. (B 8.34–36)For to be aware and to be are the same. (B 3)It is necessary to speak and to think what is; for being is, but nothing is not. (B 6.1–2)Helplessness guides the wandering thought in their breasts; they are carried along deaf and blind alike, dazed, beasts without judgment, convinced that to be and not to be are the same and not the same, and that the road of all things is a backward-turning one. (B 6.5–9)Only one thing exists, which is timeless, uniform, and unchanging:How could what is perish? How could it have come to be? For if it came into being, it is not; nor is it if ever it is going to be. Thus coming into being is extinguished, and destruction unknown. (B 8.20–22)Nor was [it] once, nor will [it] be, since [it] is, now, all together, / One, continuous; for what coming-to-be of it will you seek? / In what way, whence, did [it] grow? Neither from what-is-not shall I allow / You to say or think; for it is not to be said or thought / That [it] is not. And what need could have impelled it to grow / Later or sooner, if it began from nothing? Thus [it] must either be completely or not at all. (B 8.5–11)[What exists] is now, all at once, one and continuous... Nor is it divisible, since it is all alike; nor is there any more or less of it in one place which might prevent it from holding together, but all is full of what is. (B 8.5–6, 8.22–24)And it is all one to me / Where I am to begin; for I shall return there again. (B 5)

Parmenides claimed that there is no truth in the opinions of the mortals. Genesis-and-destruction, as Parmenides emphasizes, is a false opinion, because to be means to be completely, once and for all. What exists can in no way not exist. For this view, that That Which Is Not exists, can never predominate. You must debar your thought from this way of search, nor let ordinary experience in its variety force you along this way, (namely, that of allowing) the eye, sightless as it is, and the ear, full of sound, and the tongue, to rule; but (you must) judge by means of the Reason (Logos) the much-contested proof which is expounded by me. (B 7.1–8.2)

After the exposition of the "arche" (ἀρχή), i.e. the origin, the necessary part of reality that is understood through reason or logos ("that [it] Is"), in the next section, "the Way of Appearance/Opinion/Seeming", Parmenides gives a cosmology. He proceeds to explain the structure of the becoming cosmos (which is an illusion, of course) that comes from this origin.
The structure of the cosmos is a fundamental binary principle that governs the manifestations of all the particulars: "the aether fire of flame" (B 8.56), which is gentle, mild, soft, thin and clear, and self-identical, and the other is "ignorant night", body thick and heavy.The mortals lay down and decided well to name two forms (i.e. the flaming light and obscure darkness of night), out of which it is necessary not to make one, and in this they are led astray. (B 8.53–4)The structure of the cosmos then generated is recollected by Aetius (II, 7, 1):
For Parmenides says that there are circular bands wound round one upon the other, one made of the rare, the other of the dense; and others between these mixed of light and darkness. What surrounds them all is solid like a wall. Beneath it is a fiery band, and what is in the very middle of them all is solid, around which again is a fiery band. The most central of the mixed bands is for them all the origin and cause of motion and becoming, which he also calls steering goddess and keyholder and Justice and Necessity. The air has been separated off from the earth, vapourized by its more violent condensation, and the sun and the circle of the Milky Way are exhalations of fire. The moon is a mixture of both earth and fire. The "aether" lies around above all else, and beneath it is ranged that fiery part which we call heaven, beneath which are the regions around the earth.Cosmology originally comprised the greater part of his poem, him explaining the world's origins and operations. Some idea of the sphericity of the Earth seems to have been known to Parmenides.

Parmenides also outlined the phases of the moon, highlighted in a rhymed translation by Karl Popper:
Smith stated:Of the cosmogony of Parmenides, which was carried out very much in detail, we possess only a few fragments and notices, which are difficult to understand, according to which, with an approach to the doctrines of the "Pythagoreans", he conceived the spherical mundane system, surrounded by a circle of the pure light (Olympus, Uranus); in the centre of this mundane system the solid earth, and between the two the circle of the milkyway, of the morning or evening star, of the sun, the planets, and the moon; which circle he regarded as a mixture of the two primordial elements.

The fragments read:

The traditional interpretation of Parmenides' work is that he argued that the every-day perception of reality of the physical world (as described in "doxa") is mistaken, and that the reality of the world is 'One Being' (as described in "aletheia"): an unchanging, ungenerated, indestructible whole. Under the "Way of Opinion", Parmenides set out a contrasting but more conventional view of the world, thereby becoming an early exponent of the duality of appearance and reality. For him and his pupils, the phenomena of movement and change are simply appearances of a changeless, eternal reality. 

Parmenides was not struggling to formulate the laws of conservation of mass and conservation of energy; he was struggling with the metaphysics of change, which is still a relevant philosophical topic today. Moreover, he argued that movement was impossible because it requires moving into "the void", and Parmenides identified "the void" with nothing, and therefore (by definition) it does not exist. That which does exist is "The Parmenidean One."

Since existence is an immediately intuited fact, non-existence is the wrong path because a thing cannot disappear, just as something cannot originate from nothing. In such mystical experience ("unio mystica"), however, the distinction between subject and object disappears along with the distinctions between objects, in addition to the fact that if nothing cannot be, it cannot be the object of thought either:

William Smith also wrote in "Dictionary of Greek and Roman Biography and Mythology":On the former reason is our guide; on the latter the eye that does not catch the object and re-echoing hearing. On the former path we convince ourselves that the existent neither has come into being, nor is perishable, and is entirely of one sort, without change and limit, neither past nor future, entirely included in the present. For it is as impossible that it can become and grow out of the existent, as that it could do so out of the non-existent; since the latter, non-existence, is absolutely inconceivable, and the former cannot precede itself; and every coming into existence presupposes a non-existence. By similar arguments divisibility, motion or change, as also infinity, are shut out from the absolutely existent, and the latter is represented as shut up in itself, so that it may be compared to a well-rounded ball; while thought is appropriated to it as its only positive definition. Thought and that which is thought of (Object) coinciding; the corresponding passages of Plato, Aristotle, Theophrastus, and others, which authenticate this view of his theory.

The religious/mystical context of the poem has caused recent generations of scholars such as Alexander P. Mourelatos, Charles H. Kahn, and Peter Kingsley to call parts of the traditional, rational logical/philosophical interpretation of Parmenides into question (Kingsley in particular stating that Parmenides practiced iatromancy). The philosophy was, he says, given to him by a goddess. It has been claimed that previous scholars placed too little emphasis on the apocalyptic context in which Parmenides frames his revelation. As a result, traditional interpretations have put Parmenidean philosophy into a more modern, metaphysical context to which it is not necessarily well suited, which has led to misunderstanding of the true meaning and intention of Parmenides' message. The obscurity and fragmentary state of the text, however, renders almost every claim that can be made about Parmenides extremely contentious, and the traditional interpretation has by no means been abandoned. The "mythological" details in Parmenides' poem do not bear any close correspondence to anything known from traditional Greek mythology:

One issue is the grammar. In the original Greek the two ways are simply named "that Is" (ὅπως ἐστίν) and "that Not-Is" (ὡς οὐκ ἐστίν) (B 2.3 and 2.5) without the "it" inserted in our English translation. In ancient Greek, which, like many languages in the world, does not always require the presence of a subject for a verb, "is" functions as a grammatically complete sentence. Much debate has been focused on where and what the subject is. The simplest explanation as to why there is no subject here is that Parmenides wishes to express the simple, bare fact of existence in his mystical experience without the ordinary distinctions, just as the Latin "pluit" and the Greek "huei" (ὕει "rains") mean "it rains"; there is no subject for these impersonal verbs because they express the simple fact of raining without specifying what is doing the raining. This is, for instance, Hermann Fränkel's thesis. Many scholars still reject this explanation and have produced more complex metaphysical explanations.

There is the possibility for various wrong translations of the fragments. For example, it is not at all clear that Parmenides refuted that which we call perception. The verb "noein", used frequently by Parmenides, could better be translated as 'to be aware of' than as 'to think'. Furthermore, it is hard to believe that 'being' is only within our heads, according to Parmenides.

John Anderson Palmer notes "Parmenides’ distinction among the principal modes of being and his derivation of the attributes that must belong to what must be, simply as such, qualify him to be seen as the founder of metaphysics or ontology as a domain of inquiry distinct from theology." 

Parmenides' considerable influence on the thinking of Plato is undeniable, and in this respect, Parmenides has influenced the whole history of Western philosophy, and is often seen as its grandfather. In Plato's dialogue, the "Sophist", the main speaker (an unnamed character from Parmenides' hometown, Elea) refers to the work of "our Father Parmenides" as something to be taken very seriously and treated with respect. In the "Parmenides", Parmenides and Socrates argue about dialectic. In the "Theaetetus", Socrates says that Parmenides alone among the wise (Protagoras, Heraclitus, Empedocles, Epicharmus, and Homer) denied that everything is change and motion. "Even the censorious Timon allows Parmenides to have been a high-minded man; while Plato speaks of him with veneration, and Aristotle and others give him an unqualified preference over the rest of the Eleatics."

He is credited with a great deal of influence as the author of this "Eleatic challenge" or "Parmenides problem" that determined the course of subsequent philosophers' inquiries. For example, the ideas of Empedocles, Anaxagoras, Leucippus, and Democritus have been seen as in response to Parmenides' arguments and conclusions. According to Aristotle, Democritus and Leucippus, and many other physicists, proposed the atomic theory, which supposes that everything in the universe is either atoms or voids, specifically to contradict Parmenides' argument. Karl Popper wrote:
So what was really new in Parmenides was his axiomatic-deductive method, which Leucippus and Democritus turned into a hypothetical-deductive method, and thus made part of scientific methodology.

Alexius Meinong, much like Parmenides, believed that while anything which can be spoken of meaningfully may not "exist", it must still "subsist" and therefore have being. Bertrand Russell famously responded to this view when he proposed a solution to the problem of negative existentials in "On Denoting", as did W.V.O. Quine in his "On What There Is".

A view analogous to Parmenides with respect to time can be seen in the B theory of time and the concept of Block time, which considers existence to consist of past, present, and future, and the flow of time to be illusory. In his critique of this idea, Popper called Einstein "Parmenides".

His proto-monism of the One also influenced Plotinus and Neoplatonism against the third century AD background of Hellenistic philosophy, thus influencing many later Jewish, Christian, and Muslim thinkers of the Middle Ages as well.

Parmenides' influence on philosophy reaches up until present times. The Italian philosopher Emanuele Severino has founded his extended philosophical investigations on the words of Parmenides. His philosophy is sometimes called Neo Parmenideism, and can be understood as an attempt to build a bridge between the poem on truth and the poem on opinion. He also studies non-being, so-called meontology.

Erwin Schrödinger identified Parmenides' monad of the "Way of Truth" as being the conscious self in "Nature and the Greeks". The scientific implications of this view have been discussed by scientist Anthony Hyman.





</doc>
<doc id="23576" url="https://en.wikipedia.org/wiki?curid=23576" title="Tetraodontidae">
Tetraodontidae

The Tetraodontidae are a family of primarily marine and estuarine fish of the order Tetraodontiformes. The family includes many familiar species which are variously called pufferfish, puffers, balloonfish, blowfish, blowies, bubblefish, globefish, swellfish, toadfish, toadies, honey toads, sugar toads, and sea squab. They are morphologically similar to the closely related porcupinefish, which have large external spines (unlike the thinner, hidden spines of the Tetraodontidae, which are only visible when the fish has puffed up). The scientific name refers to the four large teeth, fused into an upper and lower plate, which are used for crushing the hard shells of crustaceans and mollusks, their natural prey.

The majority of pufferfish species are toxic and some are among the most poisonous vertebrates in the world. In certain species, the internal organs, such as the liver, and sometimes their skin, contain tetrodotoxin and are highly toxic to most animals when eaten; nevertheless, the meat of some species is considered a delicacy in Japan (as 河豚, pronounced "fugu"), Korea (as 복 ,"bok", or 복어, "bogeo"), and China (as 河豚, "hétún") when prepared by specially trained chefs who know which part is safe to eat and in what quantity. Other pufferfish species with nontoxic flesh, such as the northern puffer, "Sphoeroides maculatus", of Chesapeake Bay, are considered a delicacy elsewhere.

The species "Torquigener albomaculosus" was called by David Attenborough "the greatest artist of the animal kingdom" due to the males<nowiki>'</nowiki> unique habit of wooing females by creating nests in sand composed of complex geometric designs.

The Tetraodontidae contain at least 200 species of puffers in 29 genera:




They are typically small to medium in size all round, although a few species can reach lengths greater than .

They are most diverse in the tropics, relatively uncommon in the temperate zone, and completely absent from cold waters.

Most pufferfish species live in marine or brackish waters, but some can enter fresh water. About 35 species spend their entire lifecycles in fresh water. These fresh water species are found in disjunct tropical regions of South America ("Colomesus asellus"), Africa (six "Tetraodon" species) and Southeast Asia ("Auriglobus", "Carinotetraodon", "Dichotomyctere", "Leiodon" and "Pao").

The puffer's unique and distinctive natural defenses help compensate for its slow locomotion. It moves by combining pectoral, dorsal, anal, and caudal fins. This makes it highly maneuverable, but very slow, and therefore a comparatively easy predation target. Its tail fin is mainly used as a rudder, but it can be used for a sudden evasive burst of speed that shows none of the care and precision of its usual movements. The puffer's excellent eyesight, combined with this speed burst, is the first and most important defense against predators.

The pufferfish's secondary defense mechanism, used if successfully pursued, is to fill its extremely elastic stomach with water (or air when outside the water) until it is much larger and almost spherical in shape. Even if they are not visible when the puffer is not inflated, all puffers have pointed spines, so a hungry predator may suddenly find itself facing an unpalatable, pointy ball rather than a slow, tasty fish. Predators that do not heed this warning (or are "lucky" enough to catch the puffer suddenly, before or during inflation) may die from choking, and predators that do manage to swallow the puffer may find their stomachs full of tetrodotoxin (TTX), making puffers an unpleasant, possibly lethal, choice of prey. This neurotoxin is found primarily in the ovaries and liver, although smaller amounts exist in the intestines and skin, as well as trace amounts in muscle. It does not always have a lethal effect on large predators, such as sharks, but it can kill humans.

Larval pufferfish are chemically defended by the presence of TTX on the surface of skin, which causes predators to spit them out.

Not all puffers are necessarily poisonous; the flesh of the northern puffer is not toxic (a level of poison can be found in its viscera) and it is considered a delicacy in North America. "Takifugu oblongus", for example, is a "fugu" puffer that is not poisonous, and toxin level varies widely even in fish that are. A puffer's neurotoxin is not necessarily as toxic to other animals as it is to humans, and puffers are eaten routinely by some species of fish, such as lizardfish and sharks.

Puffers are able to move their eyes independently, and many species can change the color or intensity of their patterns in response to environmental changes. In these respects, they are somewhat similar to the terrestrial chameleon. Although most puffers are drab, many have bright colors and distinctive markings, and make no attempt to hide from predators. This is likely an example of honestly signaled aposematism.

Dolphins have been filmed expertly handling pufferfish amongst themselves in an apparent attempt to get high or enter a trance-like state.

Many marine puffers have a pelagic, or open-ocean, life stage. Spawning occurs after males slowly push females to the water surface or join females already present. The eggs are spherical and buoyant. Hatching occurs after roughly four days. The fry are tiny, but under magnification have a shape usually reminiscent of a pufferfish. They have a functional mouth and eyes, and must eat within a few days. Brackish-water puffers may breed in bays in a similar manner to marine species, or may breed more similarly to the freshwater species, in cases where they have moved far enough upriver.

Reproduction in freshwater species varies quite a bit. The dwarf puffers court with males following females, possibly displaying the crests and keels unique to this subgroup of species. After the female accepts his advances, she will lead the male into plants or another form of cover, where she can release eggs for fertilization. The male may help her by rubbing against her side. This has been observed in captivity, and they are the only commonly captive-spawned puffer species.

Target-group puffers have also been spawned in aquariums, and follow a similar courting behavior, minus the crest/keel display. However, eggs are laid on a flat piece of slate or other smooth, hard material, to which they adhere. The male will guard them until they hatch, carefully blowing water over them regularly to keep the eggs healthy. His parenting is finished when the young hatch, and the fry are on their own.

Information on breeding of specific species is very limited. "T. nigroviridis", the green-spotted puffer, has recently been artificially spawned under captive conditions. It is believed to spawn in bays in a similar manner to saltwater species, as their sperm was found to be motile only at full marine salinities, but actual wild breeding has never been observed. "Xenopterus naritus" has been reported to be first breed artificially in Sarawak, Northwestern Borneo in June 2016, which the main purpose is for development in aquaculture of the species.

In 2012, males of the species "Torquigener albomaculosus" were documented carving large geometric, circular structures in the seabed sand in Amami Ōshima, Japan. The structures serve to attract females and provide a safe place for them to lay their eggs.

A pufferfish's diet can vary depending on its environment. Traditionally, their diet consists mostly of algae and small invertebrates. They can survive on a completely vegetarian diet if their environment is lacking resources, but prefer an omnivorous food selection. Larger species of pufferfish are able to use their beak-like front teeth to break open clams, mussels, as well as other shellfish. Some species of pufferfish have also been known to enact various hunting techniques ranging from ambush to open water hunting.

The tetraodontids have been estimated to diverge from diodontids between 89 and 138 million years ago. The four major clades diverged during the Cretaceous between 80 and 101 million years ago. The oldest known pufferfish genus is "Eotetraodon", from the Lutetian epoch of Middle Eocene Europe, with fossils found in Monte Bolca and the Caucasus Mountains. The Monte Bolca species, "E. pygmaeus", coexisted with several other tetraodontiforms, including an extinct species of diodontid, primitive boxfish ("Proaracana" and "Eolactoria"), and other, totally extinct forms, such as "Zignoichthys" and the spinacanthids. The extinct genus, "Archaeotetraodon" is known from Miocene-aged fossils from Europe.

Pufferfish can be lethal if not served properly. Puffer poisoning usually results from consumption of incorrectly prepared puffer soup, "fugu chiri", or occasionally from raw puffer meat, "sashimi fugu". While "chiri" is much more likely to cause death, "sashimi fugu" often causes intoxication, light-headedness, and numbness of the lips. Pufferfish tetrodotoxin deadens the tongue and lips, and induces dizziness and vomiting, followed by numbness and prickling over the body, rapid heart rate, decreased blood pressure, and muscle paralysis. The toxin paralyzes the diaphragm muscle and stops the person who has ingested it from breathing. People who live longer than 24 hours typically survive, although possibly after a coma lasting several days.

The source of tetrodotoxin in puffers has been a matter of debate, but it is increasingly accepted that bacteria in the fish's intestinal tract are the source.

Saxitoxin, the cause of paralytic shellfish poisoning and red tide, can also be found in certain puffers.

In September 2012, the Bureau of Fisheries and Aquatic Resources in the Philippines issued a warning not to eat puffer fish, after local fishermen died upon consuming puffer fish for dinner. The warning indicated that puffer fish toxin is 100 times more potent than cyanide.

Pufferfish, called "pakapao" in Thailand, are usually consumed by mistake. They are often cheaper than other fish, and because they contain inconsistent levels of toxins between fish and season, there is little awareness or monitoring of the danger. Consumers are regularly hospitalized and some even die from the poisoning.

Cases of neurological symptoms, including numbness and tingling of the lips and mouth, have been reported to rise after the consumption of puffers caught in the area of Titusville, Florida, U.S. The symptoms generally resolve within hours to days, although one affected individual required intubation for 72 hours. As a result, Florida banned the harvesting of puffers from certain bodies of water.

Treatment is mainly supportive and consists of intestinal decontamination with gastric lavage and activated charcoal, and life-support until the toxin is metabolized. Case reports suggest anticholinesterases such as edrophonium may be effective.





</doc>
<doc id="23577" url="https://en.wikipedia.org/wiki?curid=23577" title="Partial function">
Partial function

In mathematics, a partial function from a set to a set is a function from a subset of (possibly itself) to . The subset , that is, the domain of viewed as a function, is called the domain of definition of . If equals , the partial function is said to be total.

More technically, a partial function is a binary relation over two sets that associates to every element of the first set "at most" one element of the second set; it is thus a functional binary relation. It generalizes the concept of a function by not requiring every element of the first set to be associated to "exactly" one element of the second set.

A partial functions is often used when its exact domain of definition is not known or difficult to specify. This is the case in calculus, where, for example, the quotient of two functions is a partial function whose domain of definition cannot contain the zeros of the denominator. For this reason, in calculus, and more generally in mathematical analysis, a partial function is generally called simply a "function". In computability theory, a general recursive function is a partial function from the integers to the integers; for many of them no algorithm can exist for deciding whether they are in fact total.

When arrow notation is used for functions, a partial function from to is sometimes written as , , or . However, there is no general convention, and the latter notation is more commonly used for injective functions..

Specifically, for a partial function , and any , one has either:

For example, if is the square root function restricted to the integers
then is only defined if is a perfect square (that is, ). So, , but is undefined.

A partial function is said to be injective, surjective, or bijective when the function given by the restriction of the partial function to its domain of definition is injective, surjective, bijective respectively.

Because a function is trivially surjective when restricted to its image, the term partial bijection denotes a partial function which is injective.

An injective partial function may be inverted to an injective partial function, and a partial function which is both injective and surjective has an injective function as inverse. Furthermore, a function which is injective may be inverted to an injective partial function.

The notion of transformation can be generalized to partial functions as well. A partial transformation is a function , where both and are subsets of some set .

A function is a binary relation that is functional (also called right-unique) and serial (also called left-total). This is a stronger definition than that of a partial function which only requires the functional property.

The set of all partial functions from a set "X" to a set "Y", denoted by , is the union of all functions defined on subsets of "X" with same codomain "Y":
the latter also written as formula_2. In finite case, its cardinality is
because any partial function can be extended to a function by any fixed value "c" not contained in "Y", so that the codomain is }, an operation which is injective (unique and invertible by restriction).

The first diagram at the top of the article represents a partial function that is "not" a function since the element 1 in the left-hand set is not associated with anything in the right-hand set. Whereas, the second diagram represents a function since every element on the left-hand set is associated with exactly one element in the right hand set.

Consider the natural logarithm function mapping the real numbers to themselves. The logarithm of a non-positive real is not a real number, so the natural logarithm function doesn't associate any real number in the codomain with any non-positive real number in the domain. Therefore, the natural logarithm function is not a function when viewed as a function from the reals to themselves, but it is a partial function. If the domain is restricted to only include the positive reals (that is, if the natural logarithm function is viewed as a function from the positive reals to the reals), then the natural logarithm is a function.

Subtraction of natural numbers (non-negative integers) can be viewed as a partial function:

It is defined only when formula_6.

In denotational semantics a partial function is considered as returning the bottom element when it is undefined.

In computer science a partial function corresponds to a subroutine that raises an exception or loops forever. The IEEE floating point standard defines a not-a-number value which is returned when a floating point operation is undefined and exceptions are suppressed, e.g. when the square root of a negative number is requested.

In a programming language where function parameters are statically typed, a function may be defined as a partial function because the language's type system cannot express the exact domain of the function, so the programmer instead gives it the smallest domain which is expressible as a type and contains the domain of definition of the function.

In category theory, when considering the operation of morphism composition in concrete categories, the composition operation formula_7 is a function if and only if formula_8 has one element. The reason for this is that two morphisms formula_9 and formula_10 can only be composed as formula_11 if formula_12, that is, the codomain of formula_13 must equal the domain of formula_14.

The category of sets and partial functions is equivalent to but not isomorphic with the category of pointed sets and point-preserving maps. One textbook notes that "This formal completion of sets and partial maps by adding “improper,” “infinite” elements was reinvented many times, in particular, in topology (one-point compactification) and in theoretical computer science."

The category of sets and partial bijections is equivalent to its dual. It is the prototypical inverse category.

Partial algebra generalizes the notion of universal algebra to partial operations. An example would be a field, in which the multiplicative inversion is the only proper partial operation (because division by zero is not defined).

The set of all partial functions (partial transformations) on a given base set, "X", forms a regular semigroup called the semigroup of all partial transformations (or the partial transformation semigroup on "X"), typically denoted by formula_15. The set of all partial bijections on "X" forms the symmetric inverse semigroup.

Charts in the atlases which specify the structure of manifolds and fiber bundles are partial functions. In the case of manifolds, the domain is the point set of the manifold. In the case of fiber bundles, the domain is the space of the fiber bundle. In these applications, the most important construction is the transition map, which is the composite of one chart with the inverse of another. The initial classification of manifolds and fiber bundles is largely expressed in terms of constraints on these transition maps.

The reason for the use of partial functions instead of functions is to permit general global topologies to be represented by stitching together local patches to describe the global structure. The "patches" are the domains where the charts are defined.




</doc>
<doc id="23579" url="https://en.wikipedia.org/wiki?curid=23579" title="Photoelectric effect">
Photoelectric effect

The photoelectric effect is the emission of electrons when electromagnetic radiation, such as light, hits a material. Electrons emitted in this manner are called photoelectrons. The phenomenon is studied in condensed matter physics, and solid state and quantum chemistry to draw inferences about the properties of atoms, molecules and solids. The effect has found use in electronic devices specialized for light detection and precisely timed electron emission.

In classical electromagnetic theory, the photoelectric effect would be attributed to the transfer of energy from the continuous light waves to an electron. An alteration in the intensity of light would change the kinetic energy of the emitted electrons, and sufficiently dim light would result in the emission delayed by the time it would take the electrons to accumulate enough energy to leave the material. The experimental results, however, disagree with both predictions. Instead, they show that electrons are dislodged only when the light exceeds a threshold frequency. Below that threshold, no electrons are emitted from the material, regardless of the light intensity or the length of time of exposure to the light. Because a low-frequency beam at a high intensity could not build up the energy required to produce photoelectrons like it would have if light's energy was coming from a continuous wave, Einstein proposed that a beam of light is not a wave propagating through space, but a collection of discrete wave packets—photons.

Emission of conduction electrons from typical metals requires a few electron-volt (eV) light quanta, corresponding to short-wavelength visible or ultraviolet light. In extreme cases, emissions are induced with photons approaching zero energy, like in systems with negative electron affinity and the emission from excited states, or a few hundred keV photons for core electrons in elements with a high atomic number. Study of the photoelectric effect led to important steps in understanding the quantum nature of light and electrons and influenced the formation of the concept of wave–particle duality. Other phenomena where light affects the movement of electric charges include the photoconductive effect, the photovoltaic effect, and the photoelectrochemical effect.

The photons of a light beam have a characteristic energy, called photon energy, which is proportional to the frequency of the light. In the photoemission process, when an electron within some material absorbs the energy of a photon and acquires more energy than its binding energy, it is likely to be ejected. If the photon energy is too low, the electron is unable to escape the material. Since an increase in the intensity of low-frequency light will only increase the number of low-energy photons, this change in intensity will not create any single photon with enough energy to dislodge an electron. Moreover, the energy of the emitted electrons will not depend on the intensity of the incoming light of a given frequency, but only on the energy of the individual photons.

While free electrons can absorb any energy when irradiated as long as this is followed by an immediate re-emission, like in the Compton effect, in quantum systems all of the energy from one photon is absorbed—if the process is allowed by quantum mechanics—or none at all. Part of the acquired energy is used to liberate the electron from its atomic binding, and the rest contributes to the electron's kinetic energy as a free particle. Because electrons in a material occupy many different quantum states with different binding energies, and because they can sustain energy losses on their way out of the material, the emitted electrons will have a range of kinetic energies. The electrons from the highest occupied states will have the highest kinetic energy. In metals, those electrons will be emitted from the Fermi level.

When the photoelectron is emitted into a solid rather than into a vacuum, the term "internal photoemission" is often used, and emission into a vacuum is distinguished as "external photoemission".

Even though photoemission can occur from any material, it is most readily observed from metals and other conductors. This is because the process produces a charge imbalance which, if not neutralized by current flow, results in the increasing potential barrier until the emission completely ceases. The energy barrier to photoemission is usually increased by nonconductive oxide layers on metal surfaces, so most practical experiments and devices based on the photoelectric effect use clean metal surfaces in evacuated tubes. Vacuum also helps observing the electrons since it prevents gases from impeding their flow between the electrodes.

As sunlight, due to atmosphere's absorption, does not provide much ultraviolet light, the light rich in ultraviolet rays used to be obtained by burning magnesium or from an arc lamp. At the present time, mercury-vapor lamps, noble-gas discharge UV lamps and radio-frequency plasma sources, ultraviolet lasers, and synchrotron insertion device light sources prevail.The classical setup to observe the photoelectric effect includes a light source, a set of filters to monochromatize the light, a vacuum tube transparent to ultraviolet light, an emitting electrode (E) exposed to the light, and a collector (C) whose voltage "V" can be externally controlled.

A positive external voltage is used to direct the photoemitted electrons onto the collector. If the frequency and the intensity of the incident radiation are fixed, the photoelectric current "I" increases with an increase in the positive voltage, as more and more electrons are directed onto the electrode. When no additional photoelectrons can be collected, the photoelectric current attains a saturation value. This current can only increase with the increase of the intensity of light.

An increasing negative voltage prevents all but the highest-energy electrons from reaching the collector. When no current is observed through the tube, the negative voltage has reached the value that is high enough to slow down and stop the most energetic photoelectrons of kinetic energy "K". This value of the retarding voltage is called the "stopping potential" or "cut off" potential "V". Since the work done by the retarding potential in stopping the electron of charge "e" is "eV", the following must hold "eV""=K"

The current–voltage curve is sigmoidal, but its exact shape depends on the experimental geometry and the electrode material properties.

For a given metal surface, there exists a certain minimum frequency of incident radiation below which no photoelectrons are emitted. This frequency is called the "threshold frequency". Increasing the frequency of the incident beam increases the maximum kinetic energy of the emitted photoelectrons, and the stopping voltage has to increase. The number of emitted electrons may also change because the probability that each photon results in an emitted electron is a function of photon energy.

An increase in the intensity of the same monochromatic light (so long as the intensity is not too high), which is proportional to the number of photons impinging on the surface in a given time, increases the rate at which electrons are ejected—the photoelectric current "I—"but the kinetic energy of the photoelectrons and the stopping voltage remain the same. For a given metal and frequency of incident radiation, the rate at which photoelectrons are ejected is directly proportional to the intensity of the incident light.

The time lag between the incidence of radiation and the emission of a photoelectron is very small, less than 10 second. Angular distribution of the photoelectrons is highly dependent on polarization (the direction of the electric field) of the incident light, as well as the emitting material's quantum properties such as atomic and molecular orbital symmetries and the electronic band structure of crystalline solids. In materials without macroscopic order, the distribution of electrons tends peak in the direction of polarization of linearly polarized light. The experimental technique that can measure these distributions to infer the material's properties is angle-resolved photoemission spectroscopy.

In 1905, Einstein proposed a theory of the photoelectric effect using a concept first put forward by Max Planck that light consists of tiny packets of energy known as photons or light quanta. Each packet carries energy formula_1 that is proportional to the frequency formula_2 of the corresponding electromagnetic wave. The proportionality constant formula_3 has become known as the Planck constant. The maximum kinetic energy formula_4 of the electrons that were delivered this much energy before being removed from their atomic binding is

where formula_6 is the minimum energy required to remove an electron from the surface of the material. It is called the work function of the surface and is sometimes denoted formula_7 or formula_8. If the work function is written as

the formula for the maximum kinetic energy of the ejected electrons becomes

Kinetic energy is positive, and formula_11 is required for the photoelectric effect to occur. The frequency formula_12 is the threshold frequency for the given material. Above that frequency, the maximum kinetic energy of the photoelectrons as well as the stopping voltage in the experiment formula_13 rise linearly with the frequency, and have no dependence on the number of photons and the intensity of the impinging monochromatic light. Einstein's formula, however simple, explained all the phenomenology of the photoelectric effect, and had far-reaching consequences in the development of quantum mechanics.

Electrons that are bound in atoms, molecules and solids each occupy distinct states of well-defined binding energies. When light quanta deliver more than this amount of energy to an individual electron, the electron may be emitted into free space with excess (kinetic) energy that is formula_1 higher than the electron's binding energy. The distribution of kinetic energies thus reflects the distribution of the binding energies of the electrons in the atomic, molecular or crystalline system: an electron emitted from the state at binding energy formula_15 is found at kinetic energy formula_16. This distribution is one of the main characteristics of the quantum system, and can be used for further studies in quantum chemistry and quantum physics.

The electronic properties of ordered, crystalline solids are determined by the distribution of the electronic states with respect to energy and momentum—the electronic band structure of the solid. Theoretical models of photoemission from solids show that this distribution is, for the most part, preserved in the photoelectric effect. The phenomenological "three-step model" for ultraviolet and soft X-ray excitation decomposes the effect into these steps:


There are cases where the three-step model fails to explain peculiarities of the photoelectron intensity distributions. The more elaborate "one-step model" treats the effect as a coherent process of photoexcitation into the final state of a finite crystal for which the wave function is free-electron-like outside of the crystal, but has a decaying envelope inside.

In 1839, Alexandre Edmond Becquerel discovered the photovoltaic effect while studying the effect of light on electrolytic cells. Though not equivalent to the photoelectric effect, his work on photovoltaics was instrumental in showing a strong relationship between light and electronic properties of materials. In 1873, Willoughby Smith discovered photoconductivity in selenium while testing the metal for its high resistance properties in conjunction with his work involving submarine telegraph cables.

Johann Elster (1854–1920) and Hans Geitel (1855–1923), students in Heidelberg, investigated the effects produced by light on electrified bodies and developed the first practical photoelectric cells that could be used to measure the intensity of light. They arranged metals with respect to their power of discharging negative electricity: rubidium, potassium, alloy of potassium and sodium, sodium, lithium, magnesium, thallium and zinc; for copper, platinum, lead, iron, cadmium, carbon, and mercury the effects with ordinary light were too small to be measurable. The order of the metals for this effect was the same as in Volta's series for contact-electricity, the most electropositive metals giving the largest photo-electric effect.

In 1887, Heinrich Hertz observed the photoelectric effect and reported on the production and reception of electromagnetic waves. The receiver in his apparatus consisted of a coil with a spark gap, where a spark would be seen upon detection of electromagnetic waves. He placed the apparatus in a darkened box to see the spark better. However, he noticed that the maximum spark length was reduced when inside the box. A glass panel placed between the source of electromagnetic waves and the receiver absorbed ultraviolet radiation that assisted the electrons in jumping across the gap. When removed, the spark length would increase. He observed no decrease in spark length when he replaced the glass with quartz, as quartz does not absorb UV radiation.

The discoveries by Hertz led to a series of investigations by Hallwachs, Hoor, Righi and Stoletov on the effect of light, and especially of ultraviolet light, on charged bodies. Hallwachs connected a zinc plate to an electroscope. He allowed ultraviolet light to fall on a freshly cleaned zinc plate and observed that the zinc plate became uncharged if initially negatively charged, positively charged if initially uncharged, and more positively charged if initially positively charged. From these observations he concluded that some negatively charged particles were emitted by the zinc plate when exposed to ultraviolet light. 

With regard to the "Hertz effect", the researchers from the start showed the complexity of the phenomenon of photoelectric fatigue—the progressive diminution of the effect observed upon fresh metallic surfaces. According to Hallwachs, ozone played an important part in the phenomenon, and the emission was influenced by oxidation, humidity, and the degree of polishing of the surface. It was at the time unclear whether fatigue is absent in a vacuum.

In the period from 1888 until 1891, a detailed analysis of the photoeffect was performed by Aleksandr Stoletov with results reported in six publications. Stoletov invented a new experimental setup which was more suitable for a quantitative analysis of the photoeffect. He discovered a direct proportionality between the intensity of light and the induced photoelectric current (the first law of photoeffect or Stoletov's law). He measured the dependence of the intensity of the photo electric current on the gas pressure, where he found the existence of an optimal gas pressure corresponding to a maximum photocurrent; this property was used for the creation of solar cells.

Many substances besides metals discharge negative electricity under the action of ultraviolet light. G. C. Schmidt and O. Knoblauch compiled a list of these substances. Under certain circumstances light can ionize gases, first reported by Philipp Lenard in 1900.

In 1899, J. J. Thomson investigated ultraviolet light in Crookes tubes. Thomson deduced that the ejected particles, which he called corpuscles, were of the same nature as cathode rays. These particles later became known as the electrons. Thomson enclosed a metal plate (a cathode) in a vacuum tube, and exposed it to high-frequency radiation. It was thought that the oscillating electromagnetic fields caused the atoms' field to resonate and, after reaching a certain amplitude, caused a subatomic corpuscles to be emitted, and current to be detected. The amount of this current varied with the intensity and color of the radiation. Larger radiation intensity or frequency would produce more current.

During the years 1886–1902, Wilhelm Hallwachs and Philipp Lenard investigated the phenomenon of photoelectric emission in detail. Lenard observed that a current flows through an evacuated glass tube enclosing two electrodes when ultraviolet radiation falls on one of them. As soon as ultraviolet radiation is stopped, the current also stops. This initiated the concept of photoelectric emission. The discovery of the ionization of gases by ultraviolet light was made by Philipp Lenard in 1900. As the effect was produced across several centimeters of air and yielded a greater number of positive ions than negative, it was natural to interpret the phenomenon, as J. J. Thomson did, as a "Hertz effect" upon the particles present in the gas.

In 1902, Lenard observed that the energy of individual emitted electrons increased with the frequency (which is related to the color) of the light. This appeared to be at odds with Maxwell's wave theory of light, which predicted that the electron energy would be proportional to the intensity of the radiation.

Lenard observed the variation in electron energy with light frequency using a powerful electric arc lamp which enabled him to investigate large changes in intensity, and that had sufficient power to enable him to investigate the variation of the electrode's potential with light frequency. He found the electron energy by relating it to the maximum stopping potential (voltage) in a phototube. He found that the maximum electron kinetic energy is determined by the frequency of the light. For example, an increase in frequency results in an increase in the maximum kinetic energy calculated for an electron upon liberation – ultraviolet radiation would require a higher applied stopping potential to stop current in a phototube than blue light. However, Lenard's results were qualitative rather than quantitative because of the difficulty in performing the experiments: the experiments needed to be done on freshly cut metal so that the pure metal was observed, but it oxidized in a matter of minutes even in the partial vacuums he used. The current emitted by the surface was determined by the light's intensity, or brightness: doubling the intensity of the light doubled the number of electrons emitted from the surface.

The researches of Langevin and those of Eugene Bloch have shown that the greater part of the Lenard effect is certainly due to the "Hertz effect". The Lenard effect upon the gas itself nevertheless does exist. Refound by J. J. Thomson and then more decisively by Frederic Palmer, Jr., the gas photoemission was studied and showed very different characteristics than those at first attributed to it by Lenard.

In 1900, while studying black-body radiation, the German physicist Max Planck suggested in his ""On the Law of Distribution of Energy in the Normal Spectrum"" paper that the energy carried by electromagnetic waves could only be released in "packets" of energy. In 1905, Albert Einstein published a paper advancing the hypothesis that light energy is carried in discrete quantized packets to explain experimental data from the photoelectric effect. Einstein theorized that the energy in each quantum of light was equal to the frequency of light multiplied by a constant, later called Planck's constant. A photon above a threshold frequency has the required energy to eject a single electron, creating the observed effect. This was a key step in the development of quantum mechanics. In 1914, Millikan's experiment supported Einstein's model of the photoelectric effect. In 1922, Einstein was awarded the Nobel Prize in Physics 1921 for "his discovery of the law of the photoelectric effect", and Robert Millikan was awarded the Nobel Prize in 1923 for "his work on the elementary charge of electricity and on the photoelectric effect". In quantum perturbation theory of atoms and solids acted upon by electromagnetic radiation, the photoelectric effect is still commonly analyzed in terms of waves; the two approaches are equivalent because photon or wave absorption can only happen between quantized energy levels whose energy difference is that of the energy of photon.

Albert Einstein's mathematical description of how the photoelectric effect was caused by absorption of quanta of light was in one of his Annus Mirabilis papers, named ""On a Heuristic Viewpoint Concerning the Production and Transformation of Light"". The paper proposed a simple description of "light quanta", or photons, and showed how they explained such phenomena as the photoelectric effect. His simple explanation in terms of absorption of discrete quanta of light agreed with experimental results. It explained why the energy of photoelectrons was dependent only on the "frequency" of the incident light and not on its "intensity": at low-intensity, the high-frequency source could supply a few high energy photons, whereas at high-intensity, the low-frequency source would supply no photons of sufficient individual energy to dislodge any electrons. This was an enormous theoretical leap, but the concept was strongly resisted at first because it contradicted the wave theory of light that followed naturally from James Clerk Maxwell's equations of electromagnetism, and more generally, the assumption of infinite divisibility of energy in physical systems. Even after experiments showed that Einstein's equations for the photoelectric effect were accurate, resistance to the idea of photons continued.

Einstein's work predicted that the energy of individual ejected electrons increases linearly with the frequency of the light. Perhaps surprisingly, the precise relationship had not at that time been tested. By 1905 it was known that the energy of photoelectrons increases with increasing "frequency" of incident light and is independent of the "intensity" of the light. However, the manner of the increase was not experimentally determined until 1914 when Robert Andrews Millikan showed that Einstein's prediction was correct.

The photoelectric effect helped to propel the then-emerging concept of wave–particle duality in the nature of light. Light simultaneously possesses the characteristics of both waves and particles, each being manifested according to the circumstances. The effect was impossible to understand in terms of the classical wave description of light, as the energy of the emitted electrons did not depend on the intensity of the incident radiation. Classical theory predicted that the electrons would 'gather up' energy over a period of time, and then be emitted.

These are extremely light-sensitive vacuum tubes with a coated photocathode inside the envelope. The photo cathode contains combinations of materials such as cesium, rubidium, and antimony specially selected to provide a low work function, so when illuminated even by very low levels of light, the photocathode readily releases electrons. By means of a series of electrodes (dynodes) at ever-higher potentials, these electrons are accelerated and substantially increased in number through secondary emission to provide a readily detectable output current. Photomultipliers are still commonly used wherever low levels of light must be detected.

Video camera tubes in the early days of television used the photoelectric effect, for example, Philo Farnsworth's "Image dissector" used a screen charged by the photoelectric effect to transform an optical image into a scanned electronic signal.

Since the kinetic energy of the emitted electrons is exactly the energy of the incident photon minus the energy of the electron's binding within an atom, molecule or solid, the binding energy can be determined by shining a monochromatic X-ray or UV light of a known energy and measuring the kinetic energy distribution of the photoelectrons. The distribution of electron energies is a valuable information when studying quantum properties of such systems. It can also be used to determine the elemental composition of the samples. For solids, the distribution with respect to energy and emission angle can be measured for the complete determination of the electronic band structure in terms of the allowed energies and momenta of the electrons. Modern instruments for angle-resolved photoemission spectroscopy are capable of measuring these quantities with a precision better than 1 meV and 0.1°.

Photoelectron spectroscopy measurements are usually performed in a high-vacuum environment, since the electrons would be scattered by gas molecules if they were present. However, some companies are now selling products that allow photoemission in air. The light source can be a laser, a discharge tube, or a synchrotron radiation source.

The concentric hemispherical analyzer is a typical electron energy analyzer. It uses an electric field between two hemispheres to change the trajectory of incident electrons depending on their kinetic energies. These can be used to determine the elemental composition of the sample.

Photons hitting a thin film of alkali metal or semiconductor material such as gallium arsenide in an image intensifier tube cause the ejection of photoelectrons due to the photoelectric effect. These are accelerated by an electrostatic field where they strike a phosphor coated screen, converting the electrons back into photons. Intensification of the signal is achieved either through acceleration of the electrons or by increasing the number of electrons through secondary emissions, such as with a micro-channel plate. Sometimes a combination of both methods is used. Additional kinetic energy is required to move an electron out of the conduction band and into the vacuum level. This is known as the electron affinity of the photocathode and is another barrier to photoemission other than the forbidden band, explained by the band gap model. Some materials such as Gallium Arsenide have an effective electron affinity that is below the level of the conduction band. In these materials, electrons that move to the conduction band are all of the sufficient energy to be emitted from the material and as such, the film that absorbs photons can be quite thick. These materials are known as negative electron affinity materials.

The photoelectric effect will cause spacecraft exposed to sunlight to develop a positive charge. This can be a major problem, as other parts of the spacecraft are in shadow which will result in the spacecraft developing a negative charge from nearby plasmas. The imbalance can discharge through delicate electrical components. The static charge created by the photoelectric effect is self-limiting, because a higher charged object doesn't give up its electrons as easily as a lower charged object does.

Light from the sun hitting lunar dust causes it to become positively charged from the photoelectric effect. The charged dust then repels itself and lifts off the surface of the Moon by electrostatic levitation. This manifests itself almost like an "atmosphere of dust", visible as a thin haze and blurring of distant features, and visible as a dim glow after the sun has set. This was first photographed by the Surveyor program probes in the 1960s. It is thought that the smallest particles are repelled kilometers from the surface and that the particles move in "fountains" as they charge and discharge.

When photon energies are as high as the electron rest energy of , yet another process, the Compton scattering, may take place. Above twice this energy, at pair production is also more likely. Compton scattering and pair production are examples of two other competing mechanisms.

Even if the photoelectric effect is the favoured reaction for a particular interaction of a single photon with a bound electron, the result is also subject to quantum statistics and is not guaranteed. The probability of the photoelectric effect occurring is measured by the cross section of the interaction, σ. This has been found to be a function of the atomic number of the target atom and photon energy. In a crude approximation, for photon energies above the highest atomic binding energy, the cross section is given by:

Here "Z" is the atomic number and "n" is a number which varies between 4 and 5. The photoelectric effect rapidly decreases in significance in the gamma-ray region of the spectrum, with increasing photon energy. It is also more likely from elements with high atomic number. Consequently, high-"Z" materials make good gamma-ray shields, which is the principal reason why lead ("Z" = 82) is preferred and most widely used.



"Applets"


</doc>
<doc id="23580" url="https://en.wikipedia.org/wiki?curid=23580" title="Paleogene">
Paleogene

The Paleogene ( ; also spelled Palaeogene or Palæogene; informally Lower Tertiary or Early Tertiary) is a geologic period and system that spans 43 million years from the end of the Cretaceous Period million years ago (Mya) to the beginning of the Neogene Period Mya. It is the beginning of the Cenozoic Era of the present Phanerozoic Eon. The earlier term Tertiary Period was used to define the span of time now covered by the Paleogene and subsequent Neogene periods; despite no longer being recognised as a formal stratigraphic term, 'Tertiary' is still widely found in earth science literature and remains in informal use. The Paleogene is most notable for being the time during which mammals diversified from relatively small, simple forms into a large group of diverse animals in the wake of the Cretaceous–Paleogene extinction event that ended the preceding Cretaceous Period. The United States Geological Survey uses the abbreviation P for the Paleogene, but the more commonly used abbreviation is P with the P being used for Paleocene.

This period consists of the Paleocene, Eocene, and Oligocene epochs. The end of the Paleocene (55.5/54.8 Mya) was marked by the Paleocene–Eocene Thermal Maximum, one of the most significant periods of global change during the Cenozoic, which upset oceanic and atmospheric circulation and led to the extinction of numerous deep-sea benthic foraminifera and on land, a major turnover in mammals. The term 'Paleogene System' is applied to the rocks deposited during the 'Paleogene Period'.

The global climate during the Paleogene departed from the hot and humid conditions of the late Mesozoic era and began a cooling and drying trend which, despite having been periodically disrupted by warm periods such as the Paleocene–Eocene Thermal Maximum, persisted until the temperature began to rise again due to the end of the most recent glacial period of the current ice age. The trend was partly caused by the formation of the Antarctic Circumpolar Current, which significantly lowered oceanic water temperatures. A 2018 study estimated that during the early Palaeogene about 56-48 million years ago, annual air temperatures, over land and at mid-latitude, averaged about 23–29 °C (± 4.7 °C), which is 5–10 °C higher than most previous estimates. Or for comparison, it was 10 to 15 °C higher than current annual mean temperatures in these areas; the authors suggest that the current atmospheric carbon dioxide trajectory, if it continues, could establish these temperatures again.

During the Paleogene, the continents continued to drift closer to their current positions. India was in the process of colliding with Asia, forming the Himalayas. The Atlantic Ocean continued to widen by a few centimeters each year. Africa was moving north to meet with Europe and form the Mediterranean Sea, while South America was moving closer to North America (they would later connect via the Isthmus of Panama). Inland seas retreated from North America early in the period. Australia had also separated from Antarctica and was drifting toward Southeast Asia.

Mammals began a rapid diversification during this period. After the Cretaceous–Paleogene extinction event, which saw the demise of the non-avian dinosaurs, mammals transformed from a few small and generalized forms that began to evolve into most of the modern varieties we see today. Some of these mammals would evolve into large forms that would dominate the land, while others would become capable of living in marine, specialized terrestrial, and airborne environments. Those that took to the oceans became modern cetaceans, while those that took to the trees became primates, the group to which humans belong. Birds, which were already well established by the end of the Cretaceous, also experienced adaptive radiation as they took over the skies left empty by the now extinct pterosaurs.

Pronounced cooling in the Oligocene led to a massive floral shift and many extant modern plants arose during this time. Grasses and herbs such as "Artemisia" began to appear at the expense of tropical plants, which began to decline. Conifer forests developed in mountainous areas. This cooling trend continued, with major fluctuation, until the end of the Pleistocene. This evidence for this floral shift is found in the palynological record. 

The Paleogene is notable in the context of offshore oil drilling, and especially in Gulf of Mexico oil exploration, where it is commonly referred to as the "Lower Tertiary". These rock formations represent the current cutting edge of deep-water oil discovery.

Lower Tertiary rock formations encountered in the Gulf of Mexico oil industry usually tend to be comparatively high temperature and high pressure reservoirs, often with high sand content (70%+) or under very thick evaporite sediment layers.

Lower Tertiary explorations include (partial list):



</doc>
<doc id="23582" url="https://en.wikipedia.org/wiki?curid=23582" title="Preorder">
Preorder

In mathematics, especially in order theory, a preorder or quasiorder is a binary relation that is reflexive and transitive. Preorders are more general than equivalence relations and (non-strict) partial orders, both of which are special cases of a preorder. An antisymmetric preorder is a partial order, and a symmetric preorder is an equivalence relation.

The name "preorder" comes from the idea that preorders (that are not partial orders) are 'almost' (partial) orders, but not quite; they are neither necessarily antisymmetric nor asymmetric. Because a preorder is a binary relation, the symbol ≤ can be used as the notational device for the relation. However, because they are not necessarily antisymmetric, some of the ordinary intuition associated to the symbol ≤ may not apply. On the other hand, a preorder can be used, in a straightforward fashion, to define a partial order and an equivalence relation. Doing so, however, is not always useful or worthwhile, depending on the problem domain being studied.

In words, when , one may say that "b" "covers" "a" or that "a" "precedes" "b", or that "b" "reduces" to "a". Occasionally, the notation ← or ≲ is used instead of ≤.

To every preorder, there corresponds a directed graph, with elements of the set corresponding to vertices, and the order relation between pairs of elements corresponding to the directed edges between vertices. The converse is not true: most directed graphs are neither reflexive nor transitive. In general, the corresponding graphs may contain cycles. A preorder that is antisymmetric no longer has cycles; it is a partial order, and corresponds to a directed acyclic graph. A preorder that is symmetric is an equivalence relation; it can be thought of as having lost the direction markers on the edges of the graph. In general, a preorder's corresponding directed graph may have many disconnected components.

Consider some set "P" and a binary relation ≤ on "P". Then ≤ is a preorder, or quasiorder, if it is reflexive and transitive; i.e., for all "a", "b" and "c" in "P", we have that:

A set that is equipped with a preorder is called a preordered set (or proset).

If a preorder is also antisymmetric, that is, and implies , then it is a partial order.

On the other hand, if it is symmetric, that is, if implies , then it is an equivalence relation.

A preorder is total if or for all "a", "b".

Equivalently, the notion of a preordered set "P" can be formulated in a categorical framework as a thin category; i.e., as a category with at most one morphism from an object to another. Here the objects correspond to the elements of "P", and there is one morphism for objects which are related, zero otherwise. Alternately, a preordered set can be understood as an enriched category, enriched over the category .

A preordered class is a class equipped with a preorder. Every set is a class and so every preordered set is a preordered class.


In computer science, one can find examples of the following preorders.

Example of a total preorder:

Preorders play a pivotal role in several situations:

Every binary relation R on a set S can be extended to a preorder on S by taking the transitive closure and reflexive closure, R. The transitive closure indicates path connection in if and only if there is an R-path from "x" to y.

Given a preorder ≲ on S one may define an equivalence relation ~ on S such that if and only if and . (The resulting relation is reflexive since a preorder is reflexive, transitive by applying transitivity of the preorder twice, and symmetric by definition.)

Using this relation, it is possible to construct a partial order on the quotient set of the equivalence, S / ~, the set of all equivalence classes of ~. Note that if the preorder is R, is the set of R-cycle equivalence classes: if and only if or "x" is in an R-cycle with "y". In any case, on we can define if and only if . By the construction of ~, this definition is independent of the chosen representatives and the corresponding relation is indeed well-defined. It is readily verified that this yields a partially ordered set.

Conversely, from a partial order on a partition of a set S one can construct a preorder on S. There is a 1-to-1 correspondence between preorders and pairs (partition, partial order).

For a preorder "≲", a relation "<" can be defined as if and only if ( and not ), or equivalently, using the equivalence relation introduced above, ("a" ≲ "b" and not "a" ~ "b"). It is a strict partial order; every strict partial order can be the result of such a construction. If the preorder is antisymmetric, hence a partial order "≤", the equivalence is equality, so the relation "<" can also be defined as if and only if ( and ).

Conversely we have if and only if or . This is the reason for using the notation "≲"; "≤" can be confusing for a preorder that is not antisymmetric, it may suggest that implies that or .

Note that with this construction multiple preorders "≲" can give the same relation "<", so without more information, such as the equivalence relation, "≲" cannot be reconstructed from "<". Possible preorders include the following:

Given a binary relation formula_1, the complemented composition formula_2 forms a preorder called the left residual, where formula_3 denotes the converse relation of formula_1, and formula_5 denotes the complement relation of formula_1, while formula_7 denotes relation composition.

As explained above, there is a 1-to-1 correspondence between preorders and pairs (partition, partial order). Thus the number of preorders is the sum of the number of partial orders on every partition. For example:
For , the interval is the set of points "x" satisfying and , also written . It contains at least the points "a" and "b". One may choose to extend the definition to all pairs . The extra intervals are all empty.

Using the corresponding strict relation "<", one can also define the interval as the set of points "x" satisfying and , also written . An open interval may be empty even if .

Also and can be defined similarly.




</doc>
<doc id="23585" url="https://en.wikipedia.org/wiki?curid=23585" title="Psychoanalysis">
Psychoanalysis

Psychoanalysis (from Greek: + ) is a set of theories and therapeutic techniques related to the study of the unconscious mind, which together form a method of treatment for mental disorders. The discipline was established in the early 1890s by Austrian neurologist Sigmund Freud, who retained the term "psychoanalysis" for his own school of thought, and stemmed partly from the clinical work of Josef Breuer and others. Psychoanalysis was later developed in different directions, mostly by students of Freud, such as Alfred Adler and his collaborator, Carl Gustav Jung, as well as by neo-Freudian thinkers, such as Erich Fromm, Karen Horney, and Harry Stack Sullivan.

Psychoanalysis has been known to be a controversial discipline, and its validity as a science is often contested. Nonetheless, it remains a strong influence within psychiatry, more so in some quarters than others. Psychoanalytic concepts are also widely used outside the therapeutic arena, in areas such as psychoanalytic literary criticism, as well as in the analysis of film, fairy tales and other cultural phenomena.

The basic tenets of psychoanalysis include:


During psychoanalytic sessions, typically lasting 50 minutes, ideally 4–5 times a week, the patient (or ) may lie on a couch, with the analyst often sitting just behind and out of sight. The patient expresses his or her thoughts, including free associations, fantasies, and dreams, from which the analyst infers the unconscious conflicts causing the patient's symptoms and character problems. Through the analysis of these conflicts, which includes interpreting the transference and countertransference (the analyst's feelings for the patient), the analyst confronts the patient's pathological defenses to help the patient gain insight.

Sigmund Freud first used the term 'psychoanalysis' () in 1896, ultimately retaining the term for his own school of thought. In November 1899, he wrote the "Interpretation of Dreams" (), which Freud thought of as his "most significant work."

Psychoanalysis was later developed in different directions, mostly by students of Freud such as Alfred Adler and Carl Gustav Jung, and by neo-Freudians such as Erich Fromm, Karen Horney and Harry Stack Sullivan.

The idea of psychoanalysis () first began to receive serious attention under Sigmund Freud, who formulated his own theory of psychoanalysis in Vienna in the 1890s. Freud was a neurologist trying to find an effective treatment for patients with neurotic or hysterical symptoms. Freud realised that there were mental processes that were not conscious, whilst he was employed as a neurological consultant at the Children's Hospital, where he noticed that many aphasic children had no apparent organic cause for their symptoms. He then wrote a monograph about this subject. In 1885, Freud obtained a grant to study with Jean-Martin Charcot, a famed neurologist, at the Salpêtrière in Paris, where Freud followed the clinical presentations of Charcot, particularly in the areas of hysteria, paralyses and the anaesthesias. Charcot had introduced hypnotism as an experimental research tool and developed the photographic representation of clinical symptoms.

Freud's first theory to explain hysterical symptoms was presented in "Studies on Hysteria" (1895; ), co-authored with his mentor the distinguished physician Josef Breuer, which was generally seen as the birth of psychoanalysis. The work was based on Breuer's treatment of Bertha Pappenheim, referred to in case studies by the pseudonym "Anna O.", treatment which Pappenheim herself had dubbed the "talking cure". Breuer wrote that many factors could result in such symptoms, including various types of emotional trauma, and he also credited work by others such as Pierre Janet; while Freud contended that at the root of hysterical symptoms were repressed memories of distressing occurrences, almost always having direct or indirect sexual associations.

Around the same time, Freud attempted to develop a neuro-physiological theory of unconscious mental mechanisms, which he soon gave up. It remained unpublished in his lifetime. The term 'psychoanalysis' () was first introduced by Freud in his essay titled "Heredity and etiology of neuroses" (""), written and published in French in 1896.

In 1896, Freud also published his "seduction theory", claiming to have uncovered repressed memories of incidents of sexual abuse for all his current patients, from which he proposed that the preconditions for hysterical symptoms are sexual excitations in infancy. However, by 1898 he had privately acknowledged to his friend and colleague Wilhelm Fliess that he no longer believed in his theory, though he did not state this publicly until 1906. Though in 1896 he had reported that his patients "had no feeling of remembering the [infantile sexual] scenes", and assured him "emphatically of their unbelief," in later accounts he claimed that they had told him that they had been sexually abused in infancy. This became the received historical account until challenged by several Freud scholars in the latter part of the 20th century who argued that he had imposed his preconceived notions on his patients. However, building on his claims that the patients reported infantile sexual abuse experiences, Freud subsequently contended that his clinical findings in the mid-1890s provided evidence of the occurrence of unconscious fantasies, supposedly to cover up memories of infantile masturbation. Only much later did he claim the same findings as evidence for Oedipal desires.

By 1899, Freud had theorised that dreams had symbolic significance, and generally were specific to the dreamer. Freud formulated his second psychological theory— which hypotheses that the unconscious has or is a "primary process" consisting of symbolic and condensed thoughts, and a "secondary process" of logical, conscious thoughts. This theory was published in his 1899 book, "The Interpretation of Dreams". Chapter VII is a re-working of the earlier "Project" and Freud outlined his "topographic theory". In this theory, which was mostly later supplanted by the Structural Theory, unacceptable sexual wishes were repressed into the "System Unconscious," unconscious due to society's condemnation of premarital sexual activity, and this repression created anxiety. This "topographic theory" is still popular in much of Europe, although it has fallen out of favour in much of North America.

In 1905, Freud published "Three Essays on the Theory of Sexuality" in which he laid out his discovery of the "psychosexual phases":


His early formulation included the idea that because of societal restrictions, sexual wishes were repressed into an unconscious state, and that the energy of these unconscious wishes could be turned into anxiety or physical symptoms. Therefore, the early treatment techniques, including hypnotism and abreaction, were designed to make the unconscious conscious in order to relieve the pressure and the apparently resulting symptoms. This method would later on be left aside by Freud, giving free association a bigger role.

In "On Narcissism" (1915), Freud turned his attention to the titular subject of narcissism. Still using an energic system, Freud characterized the difference between energy directed at the self versus energy directed at others, called "cathexis". By 1917, in "Mourning and Melancholia," he suggested that certain depressions were caused by turning guilt-ridden anger on the self. In 1919, through "A Child is Being Beaten," he began to address the problems of self-destructive behavior (moral masochism) and frank sexual masochism. Based on his experience with depressed and self-destructive patients, and pondering the carnage of World War I, Freud became dissatisfied with considering only oral and sexual motivations for behavior. By 1920, Freud addressed the power of identification (with the leader and with other members) in groups as a motivation for behavior (in "Group Psychology and the Analysis of the Ego"). In that same year, Freud suggested his '"dual drive' theory" of sexuality and aggression in "Beyond the Pleasure Principle," to try to begin to explain human destructiveness. Also, it was the first appearance of his "structural theory" consisting of three new concepts id, ego, and superego.

Three years later, in 1923, he summarised the ideas of id, ego, and superego in "The Ego and the Id." In the book, he revised the whole theory of mental functioning, now considering that repression was only one of many defense mechanisms, and that it occurred to reduce anxiety. Hence, Freud characterised repression as both a cause and a result of anxiety. In 1926, in "Inhibitions, Symptoms and Anxiety," Freud characterised how intrapsychic conflict among drive and superego (wishes and guilt) caused anxiety, and how that anxiety could lead to an inhibition of mental functions, such as intellect and speech. "Inhibitions, Symptoms and Anxiety" was written in response to Otto Rank, who, in 1924, published "Das Trauma der Geburt" ("The Trauma of Birth"), analysing how art, myth, religion, philosophy and therapy were illuminated by separation anxiety in the "phase before the development of the Oedipus complex." Freud's theories, however, characterized no such phase. According to Freud, the Oedipus complex, was at the centre of neurosis, and was the foundational source of all art, myth, religion, philosophy, therapy—indeed of all human culture and civilization. It was the first time that anyone in the inner circle had characterised something other than the Oedipus complex as contributing to intrapsychic development, a notion that was rejected by Freud and his followers at the time.

By 1936 the "Principle of Multiple Function" was clarified by Robert Waelder. He widened the formulation that psychological symptoms were caused by and relieved conflict simultaneously. Moreover, symptoms (such as phobias and compulsions) each represented elements of some drive wish (sexual and/or aggressive), superego, anxiety, reality, and defenses. Also in 1936, Anna Freud, Sigmund's daughter, published her seminal book, "The Ego and the Mechanisms of Defense", outlining numerous ways the mind could shut upsetting things out of consciousness.

When Hitler's power grew, the Freud family and many of their colleagues fled to London. Within a year, Sigmund Freud died. In the United States, also following the death of Freud, a new group of psychoanalysts began to explore the function of the ego. Led by Heinz Hartmann, the group built upon understandings of the "synthetic" function of the ego as a mediator in psychic functioning, distinguishing such from "autonomous" ego functions (e.g. memory and intellect, which could be secondarily affected by conflict). These "Ego Psychologists" of the 1950s paved a way to focus analytic work by attending to the defenses (mediated by the ego) before exploring the deeper roots to the unconscious conflicts.

In addition, there was burgeoning interest in child psychoanalysis. Although criticized since its inception, psychoanalysis has been used as a research tool into childhood development, and is still used to treat certain mental disturbances. In the 1960s, Freud's early thoughts on the childhood development of female sexuality were challenged; this challenge led to the development of a variety of understandings of female sexual development, many of which modified the timing and normality of several of Freud's theories (which had been gleaned from the treatment of women with mental disturbances). Several researchers followed Karen Horney's studies of societal pressures that influence the development of women.

In the first decade of the 21st century, there were approximately 35 training institutes for psychoanalysis in the United States accredited by the American Psychoanalytic Association (APsaA), which is a component organization of the International Psychoanalytical Association (IPA), and there are over 3000 graduated psychoanalysts practicing in the United States. The IPA accredits psychoanalytic training centers through such "component organisations" throughout the rest of the world, including countries such as Serbia, France, Germany, Austria, Italy, Switzerland, and many others, as well as about six institutes directly in the United States.

The predominant psychoanalytic theories can be organised into several theoretical schools. Although these perspectives differ, most of them emphasize the influence of unconscious elements on the conscious. There has also been considerable work done on consolidating elements of conflicting theories.

As in the field of medicine, there are some persistent conflicts regarding specific causes of certain syndromes, and disputes regarding the ideal treatment techniques. In the 21st century, psychoanalytic ideas are embedded in Western culture, especially in fields such as childcare, education, literary criticism, cultural studies, mental health, and particularly psychotherapy. Though there is a mainstream of evolved analytic ideas, there are groups who follow the precepts of one or more of the later theoreticians. Psychoanalytic ideas also play roles in some types of literary analysis such as Archetypal literary criticism.

"Topographic theory" was named and first described by Sigmund Freud in "The Interpretation of Dreams" (1899). The theory hypothesizes that the mental apparatus can be divided into the systems Conscious, Preconscious, and Unconscious. These systems are not anatomical structures of the brain but, rather, mental processes. Although Freud retained this theory throughout his life he largely replaced it with the "structural theory". The Topographic theory remains as one of the meta-psychological points of view for describing how the mind functions in classical psychoanalytic theory.

Structural theory divides the psyche into the id, the ego, and the super-ego. The id is present at birth as the repository of basic instincts, which Freud called ""Triebe"" ("drives"): unorganized and unconscious, it operates merely on the 'pleasure principle', without realism or foresight. The ego develops slowly and gradually, being concerned with mediating between the urging of the id and the realities of the external world; it thus operates on the 'reality principle'. The super-ego is held to be the part of the ego in which self-observation, self-criticism and other reflective and judgmental faculties develop. The ego and the super-ego are both partly conscious and partly unconscious.

During the twentieth century, many different clinical and theoretical models of psychoanalysis emerged.

Ego psychology was initially suggested by Freud in "Inhibitions, Symptoms and Anxiety" (1926), while major steps forward would be made through Anna Freud's work on defense mechanisms, first published in her book "The Ego and the Mechanisms of Defence" (1936).

The theory was refined by Hartmann, Loewenstein, and Kris in a series of papers and books from 1939 through the late 1960s. Leo Bellak was a later contributor. This series of constructs, paralleling some of the later developments of cognitive theory, includes the notions of autonomous ego functions: mental functions not dependent, at least in origin, on intrapsychic conflict. Such functions include: sensory perception, motor control, symbolic thought, logical thought, speech, abstraction, integration (synthesis), orientation, concentration, judgment about danger, reality testing, adaptive ability, executive decision-making, hygiene, and self-preservation. Freud noted that inhibition is one method that the mind may utilize to interfere with any of these functions in order to avoid painful emotions. Hartmann (1950s) pointed out that there may be delays or deficits in such functions.

Frosch (1964) described differences in those people who demonstrated damage to their relationship to reality, but who seemed able to test it.

According to ego psychology, ego strengths, later described by Otto F. Kernberg (1975), include the capacities to control oral, sexual, and destructive impulses; to tolerate painful affects without falling apart; and to prevent the eruption into consciousness of bizarre symbolic fantasy. Synthetic functions, in contrast to autonomous functions, arise from the development of the ego and serve the purpose of managing conflict processes. Defenses are synthetic functions that protect the conscious mind from awareness of forbidden impulses and thoughts. One purpose of ego psychology has been to emphasize that some mental functions can be considered to be basic, rather than derivatives of wishes, affects, or defenses. However, autonomous ego functions can be secondarily affected because of unconscious conflict. For example, a patient may have an hysterical amnesia (memory being an autonomous function) because of intrapsychic conflict (wishing not to remember because it is too painful).

Taken together, the above theories present a group of metapsychological assumptions. Therefore, the inclusive group of the different classical theories provides a cross-sectional view of human mentation. There are six "points of view", five described by Freud and a sixth added by Hartmann. Unconscious processes can therefore be evaluated from each of these six points of view:


"Modern conflict theory", a variation of ego psychology, is a revised version of structural theory, most notably different by altering concepts related to where repressed thoughts were stored. Modern conflict theory addresses emotional symptoms and character traits as complex solutions to mental conflict. It dispenses with the concepts of a fixed id, ego and superego, and instead posits conscious and unconscious conflict among wishes (dependent, controlling, sexual, and aggressive), guilt and shame, emotions (especially anxiety and depressive affect), and defensive operations that shut off from consciousness some aspect of the others. Moreover, healthy functioning (adaptive) is also determined, to a great extent, by resolutions of conflict.

A major objective of modern conflict-theory psychoanalysis is to change the balance of conflict in a patient by making aspects of the less adaptive solutions (also called "compromise formations") conscious so that they can be rethought, and more adaptive solutions found. Current theoreticians who follow the work of Charles Brenner, especially "The Mind in Conflict" (1982), include Sandor Abend, Jacob Arlow, and Jerome Blackman.

"Object relations theory" attempts to explain the ups and downs of human relationships through a study of how internal representations of the self and others are organized. The clinical symptoms that suggest object relations problems (typically developmental delays throughout life) include disturbances in an individual's capacity to feel: warmth, empathy, trust, sense of security, identity stability, consistent emotional closeness, and stability in relationships with significant others

Concepts regarding internal representation (aka 'introspect,' 'self and object representation,' or 'internalization of self and other'), although often attributed to Melanie Klein, were actually first mentioned by Sigmund Freud in his early concepts of drive theory ("Three Essays on the Theory of Sexuality", 1905). Freud's 1917 paper "Mourning and Melancholia," for example, hypothesized that unresolved grief was caused by the survivor's internalized image of the deceased becoming fused with that of the survivor, and then the survivor shifting unacceptable anger toward the deceased onto the now complex self-image.

Vamik Volkan, in "Linking Objects and Linking Phenomena," expanded on Freud's thoughts on this, describing the syndromes of "established pathological mourning" vs. "reactive depression" based on similar dynamics. Melanie Klein's hypotheses regarding internalization during the first year of life, leading to paranoid and depressive positions, were later challenged by René Spitz (e.g., "The First Year of Life", 1965), who divided the first year of life into a coenesthetic phase of the first six months, and then a diacritic phase for the second six months. Mahler, Fine, and Bergman (1975) describe distinct phases and subphases of child development leading to "separation-individuation" during the first three years of life, stressing the importance of constancy of parental figures in the face of the child's destructive aggression, internalizations, stability of affect management, and ability to develop healthy autonomy.

John Frosch, Otto Kernberg, Salman Akhtar, and Sheldon Bach have developed the theory of self and object constancy as it affects adult psychiatric problems such as psychosis and borderline states. Blos (1960) described how similar separation-individuation struggles occur during adolescence, of course with a different outcome from the first three years of life: the teen usually, eventually, leaves the parents' house (varying with culture).

During adolescence, Erik Erikson (1950–1960s) described the 'identity crisis,' that involves identity-diffusion anxiety. In order for an adult to be able to experience "Warm-ETHICS: (warmth, Empathy, Trust, Holding environment, Identity, Closeness, and Stability) in relationships, the teenager must resolve the problems with identity and redevelop self and object constancy.

"Self psychology" emphasizes the development of a stable and integrated sense of self through empathic contacts with other humans, primary significant others conceived of as 'selfobjects.' "Selfobjects" meet the developing self's needs for mirroring, idealization, and twinship, and thereby strengthen the developing self. The process of treatment proceeds through "transmuting internalizations" in which the patient gradually internalizes the selfobject functions provided by the therapist.
Self psychology was proposed originally by Heinz Kohut, and has been further developed by Arnold Goldberg, Frank Lachmann, Paul and Anna Ornstein, Marian Tolpin, and others.

Lacanian psychoanalysis, which integrates psychoanalysis with structural linguistics and Hegelian philosophy, is especially popular in France and parts of Latin America. Lacanian psychoanalysis is a departure from the traditional British and American psychoanalysis. Jacques Lacan frequently used the phrase "retourner à Freud" ("return to Freud") in his seminars and writings, as he claimed that his theories were an extension of Freud's own, contrary to those of Anna Freud, the Ego Psychology, object relations and "self" theories and also claims the necessity of reading Freud's complete works, not only a part of them. Lacan's concepts concern the "mirror stage", the "Real", the "Imaginary", and the "Symbolic", and the claim that "the unconscious is structured as a language."

Though a major influence on psychoanalysis in France and parts of Latin America, Lacan and his ideas have taken longer to be translated into English and he has thus had a lesser impact on psychoanalysis and psychotherapy in the English-speaking world. In the United Kingdom and the United States, his ideas are most widely used to analyze texts in literary theory. Due to his increasingly critical stance towards the deviation from Freud's thought, often singling out particular texts and readings from his colleagues, Lacan was excluded from acting as a training analyst in the IPA, thus leading him to create his own school in order to maintain an institutional structure for the many candidates who desired to continue their analysis with him.

The "adaptive paradigm" of psychotherapy develops out of the work of Robert Langs. The "adaptive paradigm" interprets psychic conflict primarily in terms of conscious and unconscious adaptation to reality. Langs’ recent work in some measure returns to the earlier Freud, in that Langs prefers a modified version of the topographic model of the mind (conscious, preconscious, and unconscious) over the structural model (id, ego, and super-ego), including the former's emphasis on trauma (though Langs looks to death-related traumas rather than sexual traumas). At the same time, Langs’ model of the mind differs from Freud's in that it understands the mind in terms of evolutionary biological principles.

"Relational psychoanalysis" combines interpersonal psychoanalysis with object-relations theory and with inter-subjective theory as critical for mental health. It was introduced by Stephen Mitchell. Relational psychoanalysis stresses how the individual's personality is shaped by both real and imagined relationships with others, and how these relationship patterns are re-enacted in the interactions between analyst and patient. In New York, key proponents of relational psychoanalysis include Lew Aron, Jessica Benjamin, and Adrienne Harris. Fonagy and Target, in London, have propounded their view of the necessity of helping certain detached, isolated patients, develop the capacity for "mentalization" associated with thinking about relationships and themselves. Arietta Slade, Susan Coates, and Daniel Schechter in New York have additionally contributed to the application of relational psychoanalysis to treatment of the adult patient-as-parent, the clinical study of mentalization in parent-infant relationships, and the intergenerational transmission of attachment and trauma.

The term "interpersonal-relational psychoanalysis" is often used as a professional identification. Psychoanalysts under this broader umbrella debate about what precisely are the differences between the two schools, without any current clear consensus.

The various psychoses involve deficits in the autonomous ego functions (see above) of integration (organization) of thought, in abstraction ability, in relationship to reality and in reality testing. In depressions with psychotic features, the self-preservation function may also be damaged (sometimes by overwhelming depressive affect). Because of the integrative deficits (often causing what general psychiatrists call "loose associations," "blocking," "flight of ideas," "verbigeration," and "thought withdrawal"), the development of self and object representations is also impaired. Clinically, therefore, psychotic individuals manifest limitations in warmth, empathy, trust, identity, closeness and/or stability in relationships (due to problems with self-object fusion anxiety) as well.

In patients whose autonomous ego functions are more intact, but who still show problems with object relations, the diagnosis often falls into the category known as "borderline". Borderline patients also show deficits, often in controlling impulses, affects, or fantasies – but their ability to test reality remains more or less intact. Adults who do not experience guilt and shame, and who indulge in criminal behavior, are usually diagnosed as psychopaths, or, using DSM-IV-TR, antisocial personality disorder.

"Neurotic symptoms"—including panic, phobias, conversions, obsessions, compulsions and depressions—are not usually caused by deficits in functions. Instead, they are caused by intrapsychic conflicts. The conflicts are generally among sexual and hostile-aggressive wishes, guilt and shame, and reality factors. The conflicts may be conscious or unconscious, but create anxiety, depressive affect, and anger. Finally, the various elements are managed by defensive operations—essentially shut-off brain mechanisms that make people unaware of that element of conflict.

"Repression" is the term given to the mechanism that shuts thoughts out of consciousness. "Isolation of affect" is the term used for the mechanism that shuts sensations out of consciousness. Neurotic symptoms may occur with or without deficits in ego functions, object relations, and ego strengths. Therefore, it is not uncommon to encounter obsessive-compulsive schizophrenics, panic patients who also suffer with borderline personality disorder, etc.

This section above is partial to ego psychoanalytic theory "autonomous ego functions". As the "autonomous ego functions theory" is only a theory, it may yet be proven incorrect.

Freudian theories hold that adult problems can be traced to unresolved conflicts from certain phases of childhood and adolescence, caused by fantasy, stemming from their own drives. Freud, based on the data gathered from his patients early in his career, suspected that neurotic disturbances occurred when children were sexually abused in childhood (i.e. "seduction theory"). Later, Freud came to believe that, although child abuse occurs, neurotic symptoms were not associated with this. He believed that neurotic people often had unconscious conflicts that involved incestuous fantasies deriving from different stages of development. He found the stage from about three to six years of age (preschool years, today called the "first genital stage") to be filled with fantasies of having romantic relationships with both parents. Arguments were quickly generated in early 20th-century Vienna about whether adult seduction of children, i.e. child sexual abuse, was the basis of neurotic illness. There still is no complete agreement, although nowadays professionals recognize the negative effects of child sexual abuse on mental health.

Many psychoanalysts who work with children have studied the actual effects of child abuse, which include ego and object relations deficits and severe neurotic conflicts. Much research has been done on these types of trauma in childhood, and the adult sequelae of those. In studying the childhood factors that start neurotic symptom development, Freud found a constellation of factors that, for literary reasons, he termed the Oedipus complex, based on the play by Sophocles, "Oedipus Rex", in which the protagonist unwittingly kills his father and marries his mother. The validity of the "Oedipus complex" is now widely disputed and rejected.

The shorthand term, "oedipal"—later explicated by Joseph J. Sandler in "On the Concept Superego" (1960) and modified by Charles Brenner in "The Mind in Conflict" (1982)—refers to the powerful attachments that children make to their parents in the preschool years. These attachments involve fantasies of sexual relationships with either (or both) parent, and, therefore, competitive fantasies toward either (or both) parents. Humberto Nagera (1975) has been particularly helpful in clarifying many of the complexities of the child through these years.

"Positive" and "negative" oedipal conflicts have been attached to the heterosexual and homosexual aspects, respectively. Both seem to occur in development of most children. Eventually, the developing child's concessions to reality (that they will neither marry one parent nor eliminate the other) lead to identifications with parental values. These identifications generally create a new set of mental operations regarding values and guilt, subsumed under the term "superego". Besides superego development, children "resolve" their preschool oedipal conflicts through channeling wishes into something their parents approve of ("sublimation") and the development, during the school-age years ("latency") of age-appropriate obsessive-compulsive defensive maneuvers (rules, repetitive games).

Using the various analytic and psychological techniques to assess mental problems, some believe that there are particular constellations of problems that are especially suited for analytic treatment (see below) whereas other problems might respond better to medicines and other interpersonal interventions. To be treated with psychoanalysis, whatever the presenting problem, the person requesting help must demonstrate a desire to start an analysis. The person wishing to start an analysis must have some capacity for speech and communication. As well, they need to be able to have or develop trust and insight within the psychoanalytic session. Potential patients must undergo a preliminary stage of treatment to assess their amenability to psychoanalysis at that time, and also to enable the analyst to form a working psychological model, which the analyst will use to direct the treatment. Psychoanalysts mainly work with neurosis and hysteria in particular; however, adapted forms of psychoanalysis are used in working with schizophrenia and other forms of psychosis or mental disorder. Finally, if a prospective patient is severely suicidal a longer preliminary stage may be employed, sometimes with sessions which have a twenty-minute break in the middle. There are numerous modifications in technique under the heading of psychoanalysis due to the individualistic nature of personality in both analyst and patient.

The most common problems treatable with psychoanalysis include: phobias, conversions, compulsions, obsessions, anxiety attacks, depressions, sexual dysfunctions, a wide variety of relationship problems (such as dating and marital strife), and a wide variety of character problems (for example, painful shyness, meanness, obnoxiousness, workaholism, hyperseductiveness, hyperemotionality, hyperfastidiousness). The fact that many of such patients also demonstrate deficits above makes diagnosis and treatment selection difficult.

Analytical organizations such as the IPA, APsaA and the European Federation for Psychoanalytic Psychotherapy have established procedures and models for the indication and practice of psychoanalytical therapy for trainees in analysis. The match between the analyst and the patient can be viewed as another contributing factor for the indication and contraindication for psychoanalytic treatment. The analyst decides whether the patient is suitable for psychoanalysis. This decision made by the analyst, besides made on the usual indications and pathology, is also based to a certain degree by the "fit" between analyst and patient. A person's suitability for analysis at any particular time is based on their desire to know something about where their illness has come from. Someone who is not suitable for analysis expresses no desire to know more about the root causes of their illness.

An evaluation may include one or more other analysts' independent opinions and will include discussion of the patient's financial situation and insurances.

The basic method of psychoanalysis is interpretation of the patient's unconscious conflicts that are interfering with current-day functioning – conflicts that are causing painful symptoms such as phobias, anxiety, depression, and compulsions. Strachey (1936) stressed that figuring out ways the patient distorted perceptions about the analyst led to understanding what may have been forgotten. In particular, unconscious hostile feelings toward the analyst could be found in symbolic, negative reactions to what Robert Langs later called the "frame" of the therapy—the setup that included times of the sessions, payment of fees, and necessity of talking. In patients who made mistakes, forgot, or showed other peculiarities regarding time, fees, and talking, the analyst can usually find various unconscious "resistances" to the flow of thoughts (aka free association).

When the patient reclines on a couch with the analyst out of view, the patient tends to remember more experiences, more resistance and transference, and is able to reorganize thoughts after the development of insight – through the interpretive work of the analyst. Although fantasy life can be understood through the examination of dreams, masturbation fantasies are also important. The analyst is interested in how the patient reacts to and avoids such fantasies. Various memories of early life are generally distorted—what Freud called "screen memories"—and in any case, very early experiences (before age two)—cannot be remembered.

There is what is known among psychoanalysts as "classical technique", although Freud throughout his writings deviated from this considerably, depending on the problems of any given patient.

"Classical technique" was summarized by Allan Compton as comprising:


As well, the analyst can also use confrontation to bringing an aspect of functioning, usually a defense, to the patient's attention. The analyst then uses a variety of interpretation methods, such as:


Analysts can also use reconstruction to estimate what may have happened in the past that created some current issue. These techniques are primarily based on "conflict theory" (see above). As "object relations theory" evolved, supplemented by the work of John Bowlby and Mary Ainsworth, techniques with patients who had more severe problems with basic trust (Erikson, 1950) and a history of maternal deprivation (see the works of Augusta Alpert) led to new techniques with adults. These have sometimes been called interpersonal, intersubjective (cf. Stolorow), relational, or corrective object relations techniques. These techniques include expressing an empathic attunement to the patient or warmth; exposing a bit of the analyst's personal life or attitudes to the patient; allowing the patient autonomy in the form of disagreement with the analyst (cf. I. H. Paul, "Letters to Simon"); and explaining the motivations of others which the patient misperceives.

Ego psychological concepts of deficit in functioning led to refinements in supportive therapy. These techniques are particularly applicable to psychotic and near-psychotic (cf., Eric Marcus, "Psychosis and Near-psychosis") patients. These supportive therapy techniques include discussions of reality; encouragement to stay alive (including hospitalization); psychotropic medicines to relieve overwhelming depressive affect or overwhelming fantasies (hallucinations and delusions); and advice about the meanings of things (to counter abstraction failures).

The notion of the "silent analyst" has been criticized. Actually, the analyst listens using Arlow's approach as set out in "The Genesis of Interpretation", using active intervention to interpret resistances, defenses creating pathology, and fantasies. Silence is not a technique of psychoanalysis (see also the studies and opinion papers of Owen Renik). "Analytic neutrality" is a concept that does not mean the analyst is silent. It refers to the analyst's position of not taking sides in the internal struggles of the patient. For example, if a patient feels guilty, the analyst might explore what the patient has been doing or thinking that causes the guilt, but not reassure the patient not to feel guilty. The analyst might also explore the identifications with parents and others that led to the guilt.

Interpersonal–relational psychoanalysts emphasize the notion that it is impossible to be neutral. Sullivan introduced the term "participant-observer" to indicate the analyst inevitably interacts with the analysand, and suggested the detailed inquiry as an alternative to interpretation. The detailed inquiry involves noting where the analysand is leaving out important elements of an account and noting when the story is obfuscated, and asking careful questions to open up the dialogue.

Although single-client sessions remain the norm, psychoanalytic theory has been used to develop other types of psychological treatment. Psychoanalytic group therapy was pioneered by Trigant Burrow, Joseph Pratt, Paul F. Schilder, Samuel R. Slavson, Harry Stack Sullivan, and Wolfe. Child-centered counseling for parents was instituted early in analytic history by Freud, and was later further developed by Irwin Marcus, Edith Schulhofer, and Gilbert Kliman. Psychoanalytically based couples therapy has been promulgated and explicated by Fred Sander. Techniques and tools developed in the first decade of the 21st century have made psychoanalysis available to patients who were not treatable by earlier techniques. This meant that the analytic situation was modified so that it would be more suitable and more likely to be helpful for these patients. Eagle (2007) believes that psychoanalysis cannot be a self-contained discipline but instead must be open to influence from and integration with findings and theory from other disciplines.

Psychoanalytic constructs have been adapted for use with children with treatments such as play therapy, art therapy, and storytelling. Throughout her career, from the 1920s through the 1970s, Anna Freud adapted psychoanalysis for children through play. This is still used today for children, especially those who are preadolescent. Using toys and games, children are able to symbolically demonstrate their fears, fantasies, and defenses; although not identical, this technique, in children, is analogous to the aim of free association in adults. Psychoanalytic play therapy allows the child and analyst to understand children's conflicts, particularly defenses such as disobedience and withdrawal, that have been guarding against various unpleasant feelings and hostile wishes. In art therapy, the counselor may have a child draw a portrait and then tell a story about the portrait. The counselor watches for recurring themes—regardless of whether it is with art or toys.

Psychoanalysis can be adapted to different cultures, as long as the therapist or counselor understands the client's culture. For example, Tori and Blimes found that defense mechanisms were valid in a normative sample of 2,624 Thais. The use of certain defense mechanisms was related to cultural values. For example, Thais value calmness and collectiveness (because of Buddhist beliefs), so they were low on regressive emotionality. Psychoanalysis also applies because Freud used techniques that allowed him to get the subjective perceptions of his patients. He takes an objective approach by not facing his clients during his talk therapy sessions. He met with his patients wherever they were, such as when he used free association—where clients would say whatever came to mind without self-censorship. His treatments had little to no structure for most cultures, especially Asian cultures. Therefore, it is more likely that Freudian constructs will be used in structured therapy. In addition, Corey postulates that it will be necessary for a therapist to help clients develop a cultural identity as well as an ego identity.

The cost to the patient of psychoanalytic treatment ranges widely from place to place and between practitioners. Low-fee analysis is often available in a psychoanalytic training clinic and graduate schools. Otherwise, the fee set by each analyst varies with the analyst's training and experience. Since, in most locations in the United States, unlike in Ontario and Germany, classical analysis (which usually requires sessions three to five times per week) is not covered by health insurance, many analysts may negotiate their fees with patients whom they feel they can help, but who have financial difficulties. The modifications of analysis, which include psychodynamic therapy, brief therapies, and certain types of group therapy, are carried out on a less frequent basis – usually once, twice, or three times a week – and usually the patient sits facing the therapist. As a result of the defense mechanisms and the lack of access to the unfathomable elements of the unconscious, psychoanalysis can be an expansive process that involves 2 to 5 sessions per week for several years. This type of therapy relies on the belief that reducing the symptoms will not actually help with the root causes or irrational drives. The analyst typically is a 'blank screen', disclosing very little about themselves in order that the client can use the space in the relationship to work on their unconscious without interference from outside.

The psychoanalyst uses various methods to help the patient to become more self-aware and to develop insights into their behavior and into the meanings of symptoms. First and foremost, the psychoanalyst attempts to develop a confidential atmosphere in which the patient can feel safe reporting his feelings, thoughts and fantasies. Analysands (as people in analysis are called) are asked to report whatever comes to mind without fear of reprisal. Freud called this the "fundamental rule". Analysands are asked to talk about their lives, including their early life, current life and hopes and aspirations for the future. They are encouraged to report their fantasies, "flash thoughts" and dreams. In fact, Freud believed that dreams were, "the royal road to the unconscious"; he devoted an entire volume to the interpretation of dreams. Also, psychoanalysts encourage their patients to recline on a couch. Typically, the psychoanalyst sits, out of sight, behind the patient.

The psychoanalyst's task, in collaboration with the analysand, is to help deepen the analysand's understanding of those factors, outside of his awareness, that drive his behaviors. In the safe environment of the psychoanalytic setting, the analysand becomes attached to the analyst and pretty soon he begins to experience the same conflicts with his analyst that he experiences with key figures in his life such as his parents, his boss, his significant other, etc. It is the psychoanalyst's role to point out these conflicts and to interpret them. The transferring of these internal conflicts onto the analyst is called "transference".

Many studies have also been done on briefer "dynamic" treatments; these are more expedient to measure, and shed light on the therapeutic process to some extent. Brief Relational Therapy (BRT), Brief Psychodynamic Therapy (BPT), and Time-Limited Dynamic Therapy (TLDP) limit treatment to 20–30 sessions. On average, classical analysis may last 5.7 years, but for phobias and depressions uncomplicated by ego deficits or object relations deficits, analysis may run for a shorter period of time. Longer analyses are indicated for those with more serious disturbances in object relations, more symptoms, and more ingrained character pathology.

Psychoanalysis continues to be practiced by psychiatrists, social workers, and other mental health professionals; however, its practice has declined.

In 2015, psychoanalyst Bradley Peterson, who is also a child psychiatrist and director of the Institute for the Developing Mind at Children's Hospital Los Angeles, said: "I think most people would agree that psychoanalysis as a form of treatment is on its last legs." However psychoanalytic approaches continue to be listed by the UK NHS as possibly helpful for depression.

Psychoanalytic training in the United States involves a personal psychoanalysis for the trainee, approximately 600 hours of class instruction, with a standard curriculum, over a four or five-year period.

Typically, this psychoanalysis must be conducted by a Supervising and Training Analyst. Most institutes (but not all) within the American Psychoanalytic Association, require that Supervising and Training Analysts become certified by the American Board of Psychoanalysts. Certification entails a blind review in which the psychoanalyst's work is vetted by psychoanalysts outside of their local community. After earning certification, these psychoanalysts undergo another hurdle in which they are specially vetted by senior members of their own institute. Supervising and Training analysts are held to the highest clinical and ethical standards. Moreover, they are required to have extensive experience conducting psychoanalyses.

Similarly, class instruction for psychoanalytic candidates is rigorous. Typically classes meet several hours a week, or for a full day or two every other weekend during the academic year; this varies with the institute.

Candidates generally have an hour of supervision each week, with a Supervising and Training Analyst, on each psychoanalytic case. The minimum number of cases varies between institutes, often two to four cases. Male and female cases are required. Supervision must go on for at least a few years on one or more cases. Supervision is done in the supervisor's office, where the trainee presents material from the psychoanalytic work that week. In supervision, the patient's unconscious conflicts are explored, also, transference-countertransference constellations are examined. Also, clinical technique is taught.

Many psychoanalytic training centers in the United States have been accredited by special committees of the APsaA or the IPA. Because of theoretical differences, there are independent institutes, usually founded by psychologists, who until 1987 were not permitted access to psychoanalytic training institutes of the APsaA. Currently there are between 75 and 100 independent institutes in the United States. As well, other institutes are affiliated to other organizations such as the American Academy of Psychoanalysis and Dynamic Psychiatry, and the National Association for the Advancement of Psychoanalysis. At most psychoanalytic institutes in the United States, qualifications for entry include a terminal degree in a mental health field, such as Ph.D., Psy.D., M.S.W., or M.D. A few institutes restrict applicants to those already holding an M.D. or Ph.D., and most institutes in Southern California confer a Ph.D. or Psy.D. in psychoanalysis upon graduation, which involves completion of the necessary requirements for the state boards that confer that doctoral degree. The first training institute in America to educate non-medical psychoanalysts was The National Psychological Association for Psychoanalysis (1978) in New York City. It was founded by the analyst Theodor Reik. The Contemporary Freudian (originally the New York Freudian Society) an offshoot of the National Psychological Association has a branch in Washington, DC. It is a component society/institute or the IPA.

Some psychoanalytic training has been set up as a post-doctoral fellowship in university settings, such as at Duke University, Yale University, New York University, Adelphi University and Columbia University. Other psychoanalytic institutes may not be directly associated with universities, but the faculty at those institutes usually hold contemporaneous faculty positions with psychology Ph.D. programs and/or with medical school psychiatry residency programs.

The IPA is the world's primary accrediting and regulatory body for psychoanalysis. Their mission is to assure the continued vigor and development of psychoanalysis for the benefit of psychoanalytic patients. It works in partnership with its 70 constituent organizations in 33 countries to support 11,500 members. In the US, there are 77 psychoanalytical organizations, institutes associations in the United States, which are spread across the states of America. APSaA has 38 affiliated societies which have 10 or more active members who practice in a given geographical area. The aims of APSaA and other psychoanalytical organizations are: provide ongoing educational opportunities for its members, stimulate the development and research of psychoanalysis, provide training and organize conferences. There are eight affiliated study groups in the United States. A study group is the first level of integration of a psychoanalytical body within the IPA, followed by a provisional society and finally a member society.

The Division of Psychoanalysis (39) of the American Psychological Association (APA) was established in the early 1980s by several psychologists. Until the establishment of the Division of Psychoanalysis, psychologists who had trained in independent institutes had no national organization. The Division of Psychoanalysis now has approximately 4,000 members and approximately 30 local chapters in the United States. The Division of Psychoanalysis holds two annual meetings or conferences and offers continuing education in theory, research and clinical technique, as do their affiliated local chapters. The European Psychoanalytical Federation (EPF) is the organization which consolidates all European psychoanalytic societies. This organization is affiliated with the IPA. In 2002 there were approximately 3,900 individual members in 22 countries, speaking 18 different languages. There are also 25 psychoanalytic societies.

The American Association of Psychoanalysis in Clinical Social Work (AAPCSW) was established by Crayton Rowe in 1980 as a division of the Federation of Clinical Societies of Social Work and became an independent entity in 1990. Until 2007 it was known as the National Membership Committee on Psychoanalysis. The organization was founded because although social workers represented the larger number of people who were training to be psychoanalysts, they were underrepresented as supervisors and teachers at the institutes they attended. AAPCSW now has over 1000 members and has over 20 chapters. It holds a bi-annual national conference and numerous annual local conferences.

Experiences of psychoanalysts and psychoanalytic psychotherapists and research into infant and child development have led to new insights. Theories have been further developed and the results of empirical research are now more integrated in the psychoanalytic theory.

The London Psychoanalytical Society was founded by Ernest Jones on 30 October 1913. After World War I with the expansion of psychoanalysis in the United Kingdom, the Society was reconstituted and named the British Psychoanalytical Society in 1919. Soon after, the Institute of Psychoanalysis was established to administer the Society's activities. These include: the training of psychoanalysts, the development of the theory and practice of psychoanalysis, the provision of treatment through The London Clinic of Psychoanalysis, the publication of books in The New Library of Psychoanalysis and Psychoanalytic Ideas. The Institute of Psychoanalysis also publishes "The International Journal of Psychoanalysis", maintains a library, furthers research, and holds public lectures. The society has a Code of Ethics and an Ethical Committee. The society, the institute and the clinic are all located at "Byron House" in West London.

The Society is a constituent society of the International Psychoanalytical Association, IPA, a body with members on all five continents which safeguards professional and ethical practice. The Society is a member of the British Psychoanalytic Council (BPC); the BPC publishes a register of British psychoanalysts and psychoanalytical psychotherapists. All members of the British Psychoanalytic Council are required to undertake continuing professional development, CPD. Members of the Society teach and hold posts on other approved psychoanalytic courses, e.g.: British Psychotherapy Foundation and in academic departments, e.g.University College London.

Members of the Society have included: Michael Balint, Wilfred Bion, John Bowlby, Ronald Fairbairn, Anna Freud, Harry Guntrip, Melanie Klein, Donald Meltzer, Joseph J. Sandler, Hanna Segal, J. D. Sutherland and Donald Winnicott.

The Institute of Psychoanalysis is the foremost publisher of psychoanalytic literature. The 24-volume "Standard Edition of the Complete Psychological Works of Sigmund Freud" was conceived, translated, and produced under the direction of the British Psychoanalytical Society. The Society, in conjunction with Random House, will soon publish a new, revised and expanded Standard Edition. With the New Library of Psychoanalysis the Institute continues to publish the books of leading theorists and practitioners. "The International Journal of Psychoanalysis" is published by the Institute of Psychoanalysis. Now in its 84th year, it has one of the largest circulations of any psychoanalytic journal.

There are different forms of psychoanalysis and psychotherapies in which psychoanalytic thinking is practiced. Besides classical psychoanalysis there is for example psychoanalytic psychotherapy, a therapeutic approach which widens "the accessibility of psychoanalytic theory and clinical practices that had evolved over 100 plus years to a larger number of individuals." Other examples of well known therapies which also use insights of psychoanalysis are mentalization-based treatment (MBT), and transference focused psychotherapy (TFP). There is also a continuing influence of psychoanalytic thinking in mental health care.

Over a hundred years of case reports and studies in the journal "Modern Psychoanalysis", the "Psychoanalytic Quarterly", the "International Journal of Psychoanalysis" and the "Journal of the American Psychoanalytic Association" have analyzed the efficacy of analysis in cases of neurosis and character or problems. Psychoanalysis modified by object relations techniques has been shown to be effective in many cases of ingrained problems of intimacy and relationship (cf. the many books of Otto Kernberg). Psychoanalytic treatment, in other situations, may run from about a year to many years, depending on the severity and complexity of the pathology.

Psychoanalytic theory has, from its inception, been the subject of criticism and controversy. Freud remarked on this early in his career, when other physicians in Vienna ostracized him for his findings that hysterical conversion symptoms were not limited to women. Challenges to analytic theory began with Otto Rank and Alfred Adler (turn of the 20th century), continued with behaviorists (e.g. Wolpe) into the 1940s and '50s, and have persisted (e.g. Miller). Criticisms come from those who object to the notion that there are mechanisms, thoughts or feelings in the mind that could be unconscious. Criticisms also have been leveled against the idea of "infantile sexuality" (the recognition that children between ages two and six imagine things about procreation). Criticisms of theory have led to variations in analytic theories, such as the work of Ronald Fairbairn, Michael Balint, and John Bowlby. In the past 30 years or so, the criticisms have centered on the issue of empirical verification.

Psychoanalysis has been used as a research tool into childhood development (cf. the journal "The Psychoanalytic Study of the Child"), and has developed into a flexible, effective treatment for certain mental disturbances. In the 1960s, Freud's early (1905) thoughts on the childhood development of female sexuality were challenged; this challenge led to major research in the 1970s and 80s, and then to a reformulation of female sexual development that corrected some of Freud's concepts. Also see the various works of Eleanor Galenson, Nancy Chodorow, Karen Horney, Françoise Dolto, Melanie Klein, Selma Fraiberg, and others. Most recently, psychoanalytic researchers who have integrated attachment theory into their work, including Alicia Lieberman, Susan Coates, and Daniel Schechter have explored the role of parental traumatization in the development of young children's mental representations of self and others.

The psychoanalytic profession has been resistant to researching efficacy. Evaluations of effectiveness based on the interpretation of the therapist alone cannot be proven.

Meta-analyses in 2012 and 2013 found support or evidence for the efficacy of psychoanalytic therapy, thus further research is needed. Other meta-analyses published in the recent years showed psychoanalysis and psychodynamic therapy to be effective, with outcomes comparable or greater than other kinds of psychotherapy or antidepressant drugs, but these arguments have also been subjected to various criticisms. In particular, the inclusion of pre/post studies rather than randomized controlled trials, and the absence of adequate comparisons with control treatments is a serious limitation in interpreting the results.

In 2011, the American Psychological Association made 103 comparisons between psychodynamic treatment and a non-dynamic competitor and found that 6 were superior, 5 were inferior, 28 had no difference and 63 were adequate. The study found that this could be used as a basis "to make psychodynamic psychotherapy an 'empirically validated' treatment."

Meta-analyses of Short Term Psychodynamic Psychotherapy (STPP) have found effect sizes (Cohen's d) ranging from .34 to .71 compared to no treatment and was found to be slightly better than other therapies in follow up. Other reviews have found an effect size of .78 to .91 for somatic disorders compared to no treatment and .69 for treating depression. A 2012 "Harvard Review of Psychiatry" meta-analysis of Intensive Short-Term Dynamic Psychotherapy (ISTDP) found effect sizes ranging from .84 for interpersonal problems to 1.51 for depression. Overall ISTDP had an effect size of 1.18 compared to no treatment.

A meta-analysis of Long Term Psychodynamic Psychotherapy in 2012 found an overall effect size of .33, which is modest. This study concluded the recovery rate following LTPP was equal to control treatments, including treatment as usual, and found the evidence for the effectiveness of LTPP to be limited and at best conflicting. Others have found effect sizes of .44–.68.

According to a 2004 French review conducted by INSERM, psychoanalysis was presumed or proven effective at treating panic disorder, post-traumatic stress, and personality disorders, but did not find evidence of its effectiveness in treating schizophrenia, panic disorder, obsessive compulsive disorder, specific phobia, bulimia and anorexia.

A 2001 systematic review of the medical literature by the Cochrane Collaboration concluded that no data exist demonstrating that psychodynamic psychotherapy is effective in treating schizophrenia and severe mental illness, and cautioned that medication should always be used alongside any type of talk therapy in schizophrenia cases. A French review from 2004 found the same. The Schizophrenia Patient Outcomes Research Team advises against the use of psychodynamic therapy in cases of schizophrenia, arguing that more trials are necessary to verify its effectiveness.

Both Freud and psychoanalysis have been criticized in extreme terms. Exchanges between critics and defenders of psychoanalysis have often been so heated that they have come to be characterized as the "Freud Wars".

Early critics of psychoanalysis believed that its theories were based too little on quantitative and experimental research, and too much on the clinical case study method. Some have accused Freud of fabrication, most famously in the case of Anna O. Philosopher Frank Cioffi cites false claims of a sound scientific verification of the theory and its elements as the strongest basis for classifying the work of Freud and his school as pseudoscience.

Others have speculated that patients suffered from now easily identifiable conditions unrelated to psychoanalysis; for instance, Anna O. is thought to have suffered from an organic impairment such as tuberculous meningitis or temporal lobe epilepsy and not hysteria (see modern interpretations).

Karl Popper argued that psychoanalysis is a pseudoscience because its claims are not testable and cannot be refuted; that is, they are not falsifiable. In addition, Imre Lakatos wrote that "Freudians have been nonplussed by Popper's basic challenge concerning scientific honesty. Indeed, they have refused to specify experimental conditions under which they would give up their basic assumptions." In "Sexual Desire" (1986), philosopher Roger Scruton rejects Popper's arguments pointing to the theory of repression as an example of a Freudian theory that does have testable consequences. Scruton nevertheless concluded that psychoanalysis is not genuinely scientific, on the grounds that it involves an unacceptable dependence on metaphor. The philosopher and physicist Mario Bunge argued that psychoanalysis is a pseudoscience because it violates the ontology and methodology inherent to science. According to Bunge, most psychoanalytic theories are either untestable or unsupported by evidence.

Cognitive scientists, in particular, have also weighed in. Martin Seligman, a prominent academic in positive psychology wrote that: Thirty years ago, the cognitive revolution in psychology overthrew both Freud and the behaviorists, at least in academia.… [T]hinking…is not just a [result] of emotion or behavior.… [E]motion is always generated by cognition, not the other way around.Linguist Noam Chomsky has criticized psychoanalysis for lacking a scientific basis. Steven Pinker considers Freudian theory unscientific for understanding the mind. Evolutionary biologist Stephen Jay Gould considered psychoanalysis influenced by pseudoscientific theories such as recapitulation theory. Psychologists Hans Eysenck (1985) and John F. Kihlstrom (2012/2000) have also criticized the field as pseudoscience.

Adolf Grünbaum argues in "Validation in the Clinical Theory of Psychoanalysis" (1993) that psychoanalytic based theories are falsifiable, but that the causal claims of psychoanalysis are unsupported by the available clinical evidence.

Richard Feynman wrote off psychoanalysts as mere "witch doctors:"

Likewise, psychiatrist E. Fuller Torrey, in "Witchdoctors and Psychiatrists" (1986), agreed that psychoanalytic theories have no more scientific basis than the theories of traditional native healers, "witchdoctors" or modern "cult" alternatives such as EST. Psychologist Alice Miller charged psychoanalysis with being similar to the poisonous pedagogies, which she described in her book "For Your Own Good". She scrutinized and rejected the validity of Freud's drive theory, including the Oedipus complex, which, according to her and Jeffrey Masson, blames the child for the abusive sexual behavior of adults. Psychologist Joel Kupfersmid investigated the validity of the Oedipus complex, examining its nature and origins. He concluded that there is little evidence to support the existence of the Oedipus complex.

Michel Foucault and Gilles Deleuze claimed that the institution of psychoanalysis has become a center of power and that its confessional techniques resemble the Christian tradition. Jacques Lacan criticized the emphasis of some American and British psychoanalytical traditions on what he has viewed as the suggestion of imaginary "causes" for symptoms, and recommended the return to Freud. Together with Deleuze, Félix Guattari criticised the Oedipal structure. Luce Irigaray criticised psychoanalysis, employing Jacques Derrida's concept of phallogocentrism to describe the exclusion of the woman from Freudian and Lacanian psychoanalytical theories. Deleuze and Guattari (1972), in "Anti-Œdipus", take the cases of Gérard Mendel, Bela Grunberger and Janine Chasseguet-Smirgel, prominent members of the most respected associations (IPA), to suggest that, traditionally, psychoanalysis enthusiastically embraces a police state.

The theoretical foundations of psychoanalysis lie in the same philosophical currents that lead to interpretive phenomenology rather than in those that lead to scientific positivism, making the theory largely incompatible with positivist approaches to the study of the mind.

Although numerous studies have shown that the efficacy of therapy is primarily related to the quality of the therapist, rather than the school or technique or training, a French 2004 report from INSERM concluded that psychoanalytic therapy is less effective than other psychotherapies (including cognitive behavioral therapy) for certain diseases. This report used a meta-analysis of numerous other studies to find whether the treatment was "proven" or "presumed" to be effective on different diseases.

A survey of scientific research suggested that while personality traits corresponding to Freud's oral, anal, Oedipal, and genital phases can be observed, they do not necessarily manifest as stages in the development of children. These studies also have not confirmed that such traits in adults result from childhood experiences. However, these stages should not be viewed as crucial to modern psychoanalysis. What is crucial to modern psychoanalytic theory and practice is the power of the unconscious and the transference phenomenon.

The idea of "unconscious" is contested because human behavior can be observed while human mental activity has to be inferred. However, the unconscious is now a popular topic of study in the fields of experimental and social psychology (e.g., implicit attitude measures, fMRI, and PET scans, and other indirect tests). The idea of unconscious, and the transference phenomenon, have been widely researched and, it is claimed, validated in the fields of cognitive psychology and social psychology, though a Freudian interpretation of unconscious mental activity is not held by the majority of cognitive psychologists. Recent developments in neuroscience have resulted in one side arguing that it has provided a biological basis for unconscious emotional processing in line with psychoanalytic theory i.e., neuropsychoanalysis, while the other side argues that such findings make psychoanalytic theory obsolete and irrelevant.

Shlomo Kalo explains that the scientific materialism that flourished in the 19th century severely harmed religion and rejected whatever called spiritual. The institution of the confession priest in particular was badly damaged. The empty void that this institution left behind was swiftly occupied by the newborn psychoanalysis. In his writings, Kalo claims that psychoanalysis basic approach is erroneous. It represents the mainline wrong assumptions that happiness is unreachable and that the natural desire of a human being is to exploit his fellow men for his own pleasure and benefit.

Jacques Derrida incorporated aspects of psychoanalytic theory into his theory of deconstruction in order to question what he called the 'metaphysics of presence'. Derrida also turns some of these ideas against Freud, to reveal tensions and contradictions in his work. For example, although Freud defines religion and metaphysics as displacements of the identification with the father in the resolution of the Oedipal complex, Derrida (1987) insists that the prominence of the father in Freud's own analysis is itself indebted to the prominence given to the father in Western metaphysics and theology since Plato.






</doc>
<doc id="23587" url="https://en.wikipedia.org/wiki?curid=23587" title="Peking (disambiguation)">
Peking (disambiguation)

Peking or Beijing is the capital city of the People's Republic of China. 

Peking may also refer to:




</doc>
<doc id="23588" url="https://en.wikipedia.org/wiki?curid=23588" title="Pinyin">
Pinyin

Hanyu Pinyin (), often abbreviated to pinyin, is the official romanization system for Standard Chinese in mainland China, and to some extent in Taiwan. It is often used to teach Standard Mandarin Chinese, which is normally written using Chinese characters. The system includes four diacritics denoting tones. Pinyin without tone marks is used to spell Chinese names and words in languages written with the Latin alphabet and also in certain computer input methods to enter Chinese characters.

The pinyin system was developed in the 1950s by a group of Chinese linguists including Zhou Youguang, and was based on earlier forms of romanizations of Chinese. It was published by the Chinese government in 1958 and revised several times. The International Organization for Standardization (ISO) adopted pinyin as an international standard in 1982, and was followed by the United Nations in 1986. Attempts to make pinyin standard in Taiwan occurred in 2002 and 2009, but "Today Taiwan has no standardized spelling system" so that in 2019 "alphabetic spellings in Taiwan are marked more by a lack of system than the presence of one." Moreover, "some cities, businesses, and organizations, notably in the south of Taiwan, did not accept [efforts to introduce pinyin], as it suggested that Taiwan is more closely tied to the PRC", so it remains one of several rival romanization systems in use.

The word ' () means 'the spoken language of the Han people', while ' () literally means 'spelled sounds'.
When a foreign writing system with one set of coding/decoding system is taken to write a language, certain compromises may have to be made. The result is that the decoding systems used in some foreign languages will enable non-native speakers to produce sounds more closely resembling the target language than will the coding/decoding system used by other foreign languages. Native speakers of English will decode pinyin spellings to fairly close approximations of Mandarin except in the case of certain speech sounds that are not ordinarily produced by most native speakers of English: "j" , "q" ", x" , "z" , "c" , "zh" , "ch" , "sh" , "h" , and "r" exhibiting the greatest discrepancies.

In this system, the correspondence between the Roman letter and the sound is sometimes idiosyncratic, though not necessarily more so than the way the Latin script is employed in other languages. For example, the aspiration distinction between "b", "d", "g" and "p", "t", "k" is similar to that of these syllable-initial consonants English (in which the two sets are however also differentiated by voicing), but not to that of French. Letters "z" and "c" also have that distinction, pronounced as and (whilst reminiscent of both of them being used for the phoneme in the German language and Latin script-using Slavic languages respectively). From "s, z, c" come the digraphs "sh, zh, ch" by analogy with English "sh, ch". Although this introduces the novel combination "zh", it is internally consistent in how the two series are related, and reminds the trained reader that many Chinese people pronounce "sh, zh, ch" as "s, z, c" (and English-speakers use "zh" to represent in foreign languages such as Russian anyway). In the "x, j, q" series, the pinyin use of "x" is similar to its use in Portuguese, Galician, Catalan, Basque, and Maltese; and the pinyin "q" is akin to its value in Albanian; both pinyin and Albanian pronunciations may sound similar to the "ch" to the untrained ear. Pinyin vowels are pronounced in a similar way to vowels in Romance languages.

The pronunciation and spelling of Chinese words are generally given in terms of initials and finals, which represent the "segmental phonemic" portion of the language, rather than letter by letter. Initials are initial consonants, while finals are all possible combinations of medials (semivowels coming before the vowel), a nucleus vowel, and coda (final vowel or consonant).

In 1605, the Jesuit missionary Matteo Ricci published "Xizi Qiji" () in Beijing. This was the first book to use the Roman alphabet to write the Chinese language. Twenty years later, another Jesuit in China, Nicolas Trigault, issued his "" () at Hangzhou. Neither book had much immediate impact on the way in which Chinese thought about their writing system, and the romanizations they described were intended more for Westerners than for the Chinese.

One of the earliest Chinese thinkers to relate Western alphabets to Chinese was late Ming to early Qing dynasty scholar-official, Fang Yizhi (; 1611–1671).

The first late Qing reformer to propose that China adopt a system of spelling was Song Shu (1862–1910). A student of the great scholars Yu Yue and Zhang Taiyan, Song had been to Japan and observed the stunning effect of the "kana" syllabaries and Western learning there. This galvanized him into activity on a number of fronts, one of the most important being reform of the script. While Song did not himself actually create a system for spelling Sinitic languages, his discussion proved fertile and led to a proliferation of schemes for phonetic scripts.

The Wade–Giles system was produced by Thomas Wade in 1859, and further improved by Herbert Giles in the Chinese–English Dictionary of 1892. It was popular and used in English-language publications outside China until 1979.

In the early 1930s, Communist Party of China leaders trained in Moscow introduced a phonetic alphabet using Roman letters which had been developed in the Soviet Oriental Institute of Leningrad and was originally intended to improve literacy in the Russian Far East. This Sin Wenz or "New Writing" was much more linguistically sophisticated than earlier alphabets, but with the major exception that it did not indicate tones of Chinese.

In 1940, several thousand members attended a Border Region Sin Wenz Society convention. Mao Zedong and Zhu De, head of the army, both contributed their calligraphy (in characters) for the masthead of the Sin Wenz Society's new journal. Outside the CCP, other prominent supporters included Dr. Sun Yat-sen's son, Sun Fo; Cai Yuanpei, the country's most prestigious educator; Tao Xingzhi, a leading educational reformer; and Lu Xun. Over thirty journals soon appeared written in Sin Wenz, plus large numbers of translations, biographies (including Lincoln, Franklin, Edison, Ford, and Charlie Chaplin), some contemporary Chinese literature, and a spectrum of textbooks. In 1940, the movement reached an apex when Mao's Border Region Government declared that the Sin Wenz had the same legal status as traditional characters in government and public documents. Many educators and political leaders looked forward to the day when they would be universally accepted and completely replace Chinese characters. Opposition arose, however, because the system was less well adapted to writing regional languages, and therefore would require learning Mandarin. Sin Wenz fell into relative disuse during the following years.

In 1943, the U.S. military engaged Yale University to develop a romanization of Mandarin Chinese for its pilots flying over China. The resulting system is very close to "pinyin", but does not use English letters in unfamiliar ways; for example, "pinyin" x for is written as sy in the Yale system. Medial semivowels are written with y and w (instead of "pinyin" i and u), and apical vowels (syllabic consonants) with r or z. Accent marks are used to indicate tone.

Pinyin was created by Chinese linguists, including Zhou Youguang, as part of a Chinese government project in the 1950s. Zhou, often called "the father of pinyin," worked as a banker in New York when he decided to return to China to help rebuild the country after the establishment of the People's Republic of China in 1949. He became an economics professor in Shanghai, and in 1955, when China's Ministry of Education created a Committee for the Reform of the Chinese Written Language, Premier Zhou Enlai assigned Zhou Youguang the task of developing a new romanization system, despite the fact that he was not a professional linguist.

"Hanyu Pinyin" was based on several existing systems: "Gwoyeu Romatzyh" of 1928, "Latinxua Sin Wenz" of 1931, and the diacritic markings from "zhuyin" (bopomofo). "I'm not the father of pinyin," Zhou said years later; "I'm the son of pinyin. It's [the result of] a long tradition from the later years of the Qing dynasty down to today. But we restudied the problem and revisited it and made it more perfect."

A draft was published on February 12, 1956. The first edition of "Hanyu Pinyin" was approved and adopted at the Fifth Session of the 1st National People's Congress on February 11, 1958. It was then introduced to primary schools as a way to teach Standard Chinese pronunciation and used to improve the literacy rate among adults.

During the height of the Cold War, the use of pinyin system over the Yale romanization outside of China was regarded as a political statement or identification with the communist Chinese regime. Beginning in the early 1980s, Western publications addressing Mainland China began using the Hanyu Pinyin romanization system instead of earlier romanization systems; this change followed the normalization of diplomatic relations between the United States and the PRC in 1979. In 2001, the PRC Government issued the "National Common Language Law", providing a legal basis for applying pinyin. The current specification of the orthographic rules is laid down in the National Standard GB/T 16159–2012.

Unlike European languages, clusters of letters — initials () and finals () — and not consonant and vowel letters, form the fundamental elements in pinyin (and most other phonetic systems used to describe the Han language). Every Mandarin syllable can be spelled with exactly one initial followed by one final, except for the special syllable "er" or when a trailing "-r" is considered part of a syllable (see below, and see erhua). The latter case, though a common practice in some sub-dialects, is rarely used in official publications.

Even though most initials contain a consonant, finals are not always simple vowels, especially in compound finals (), i.e. when a "medial" is placed in front of the final. For example, the medials and are pronounced with such tight openings at the beginning of a final that some native Chinese speakers (especially when singing) pronounce "yī" (, clothes, officially pronounced ) as and "wéi" (, to enclose, officially pronounced ) as or . Often these medials are treated as separate from the finals rather than as part of them; this convention is followed in the chart of finals below.

In each cell below, the bold letters indicate pinyin and the brackets enclose the symbol in the International Phonetic Alphabet.

The conventional lexicographical order (excluding "w" and "y"), derived from the zhuyin system ("bopomofo"), is:

According to "Scheme for the Chinese Phonetic Alphabet", "zh", "ch", and "sh" can be abbreviated as "ẑ", "ĉ", and "ŝ" ("z", "c", "s" with a circumflex). However, the shorthands are rarely used due to difficulty of entering them on computers and are confined mainly to Esperanto keyboard layouts.

In each cell below, the first line indicates IPA, the second indicates pinyin for a standalone (no-initial) form, and the third indicates pinyin for a combination with an initial. Other than finals modified by an "-r", which are omitted, the following is an exhaustive table of all possible finals.

The only syllable-final consonants in Standard Chinese are "-n" and "-ng", and "-r", the last of which is attached as a grammatical suffix. A Chinese syllable ending with any other consonant either is from a non-Mandarin language (a southern Chinese language such as Cantonese, or a minority language of China; possibly reflecting final consonants in Old Chinese), or indicates the use of a non-pinyin romanization system (where final consonants may be used to indicate tones). 

Technically, "i, u, ü" without a following vowel are finals, not medials, and therefore take the tone marks, but they are more concisely displayed as above. In addition, "ê" () and syllabic nasals "m" (, ), "n" (, ), "ng" (, ) are used as interjections.

According to "Scheme for the Chinese Phonetic Alphabet", "ng" can be abbreviated with a shorthand of "ŋ". However, this shorthand is rarely used due to difficulty of entering them on computers.

An umlaut is placed over the letter "u" when it occurs after the initials "l" and "n" when necessary in order to represent the sound [y]. This is necessary in order to distinguish the front high rounded vowel in "lü" (e.g. ) from the back high rounded vowel in "lu" (e.g. ). Tonal markers are added on top of the umlaut, as in "lǘ".

However, the "ü" is "not" used in the other contexts where it could represent a front high rounded vowel, namely after the letters "j", "q", "x", and "y". For example, the sound of the word / (fish) is transcribed in pinyin simply as "yú", not as "yǘ". This practice is opposed to Wade–Giles, which always uses "ü", and "Tongyong Pinyin", which always uses "yu". Whereas Wade–Giles needs of using the umlaut to distinguish between "chü" (pinyin "ju") and "chu" (pinyin "zhu"), this ambiguity does not arise with pinyin, so the more convenient form "ju" is used instead of "jü". Genuine ambiguities only happen with "nu"/"nü" and "lu"/"lü", which are then distinguished by an umlaut.

Many fonts or output methods do not support an umlaut for "ü" or cannot place tone marks on top of "ü". Likewise, using "ü" in input methods is difficult because it is not present as a simple key on many keyboard layouts. For these reasons "v" is sometimes used instead by convention. For example, it is common for cellphones to use "v" instead of "ü". Additionally, some stores in China use "v" instead of "ü" in the transliteration of their names. The drawback is that there are no tone marks for the letter "v".

This also presents a problem in transcribing names for use on passports, affecting people with names that consist of the sound "lü" or "nü", particularly people with the surname ("Lǚ"), a fairly common surname, particularly compared to the surnames (Lù), (Lǔ), (Lú) and (Lù). Previously, the practice varied among different passport issuing offices, with some transcribing as "LV" and "NV" while others used "LU" and "NU". On 10 July 2012, the Ministry of Public Security standardized the practice to use "LYU" and "NYU" in passports.

Although "nüe" written as "nue", and "lüe" written as "lue" are not ambiguous, "nue" or "lue" are not correct according to the rules; "nüe" and "lüe" should be used instead. However, some Chinese input methods (e.g. Microsoft Pinyin IME) support both "nve"/"lve" (typing "v" for "ü") and "nue"/"lue".

Most rules given here in terms of English pronunciation are approximations, as several of these sounds do not correspond directly to sounds in English.

"Y" and "w" are equivalent to the semivowel medials "i, u", and "ü" (see below). They are spelled differently when there is no initial consonant in order to mark a new syllable: "fanguan" is "fan-guan", while "fangwan" is "fang-wan" (and equivalent to "*fang-uan)". With this convention, an apostrophe only needs to be used to mark an initial "a, e", or "o: Xi'an" (two syllables: ) vs. "xian" (one syllable: ). In addition, "y" and "w" are added to fully vocalic "i, u", and "ü" when these occur without an initial consonant, so that they are written "yi, wu", and "yu". Some Mandarin speakers do pronounce a or sound at the beginning of such words—that is, "yi" or , "wu" or , "yu" or ,—so this is an intuitive convention. See below for a few finals which are abbreviated after a consonant plus "w/u" or "y/i" medial: "wen" → C+"un", "wei" → C+"ui", "weng" → C+"ong", and "you" → C+"iu".

The apostrophe (') () is used before a syllable starting with a vowel (, , or ) in a multiple-syllable word when the syllable does not start the word, unless the syllable immediately follows a hyphen or other dash. For example, is written as Xi'an or Xī'ān, and is written as Tian'e or Tiān'é, but is written "dì-èr", without an apostrophe. This apostrophe is not used in the Taipei Metro names.

Apostrophes (as well as hyphens and tone marks) are omitted on Chinese passports.

The following is a list of finals in Standard Chinese, excepting most of those ending with "r".

To find a given final:

The pinyin system also uses diacritics to mark the four tones of Mandarin. The diacritic is placed over the letter that represents the syllable nucleus, unless that letter is missing (see below).

Many books printed in China use a mix of fonts, with vowels and tone marks rendered in a different font from the surrounding text, tending to give such pinyin texts a typographically ungainly appearance. This style, most likely rooted in early technical limitations, has led many to believe that pinyin's rules call for this practice, e.g. the use of a Latin alpha (ɑ) rather than the standard style (a) found in most fonts, or g often written with a single-storey ɡ. The rules of "Hanyu Pinyin", however, specify no such practice.


These tone marks normally are only used in Mandarin textbooks or in foreign learning texts, but they are essential for correct pronunciation of Mandarin syllables, as exemplified by the following classic example of five characters whose pronunciations differ only in their tones:

The words are "mother", "hemp", "horse", "scold", and a question particle, respectively.

Before the advent of computers, many typewriter fonts did not contain vowels with macron or caron diacritics. Tones were thus represented by placing a tone number at the end of individual syllables. For example, "tóng" is written "tong²".
The number used for each tone is as the order listed above, except the neutral tone, which is either not numbered, or given the number 0 or 5, e.g. "ma⁵" for ／, an interrogative marker.
Briefly, the tone mark should always be placed by the order—"a, o, e, i, u, ü", with the only exception being "iu", where the tone mark is placed on the "u" instead. Pinyin tone marks appear primarily above the nucleus of the syllable, for example as in "kuài", where "k" is the initial, "u" the medial, "a" the nucleus, and "i" the coda. The exception is syllabic nasals like /m/, where the nucleus of the syllable is a consonant, the diacritic will be carried by a written dummy vowel.

When the nucleus is /ə/ (written "e" or "o"), and there is both a medial and a coda, the nucleus may be dropped from writing. In this case, when the coda is a consonant "n" or "ng", the only vowel left is the medial "i, u", or "ü", and so this takes the diacritic. However, when the coda is a vowel, it is the coda rather than the medial which takes the diacritic in the absence of a written nucleus. This occurs with syllables ending in "-ui" (from "wei": ("wèi" → "-uì") and in "-iu" (from "you": "yòu" → "-iù".) That is, in the absence of a written nucleus the finals have priority for receiving the tone marker, as long as they are vowels: if not, the medial takes the diacritic.

An algorithm to find the correct vowel letter (when there is more than one) is as follows:


Worded differently,

If the tone is written over an "i", the tittle above the "i" is omitted, as in "yī".

The placement of the tone marker, when more than one of the written letters "a, e, i, o", and "u" appears, can also be inferred from the nature of the vowel sound in the medial and final. The rule is that the tone marker goes on the spelled vowel that is not a (near-)semi-vowel. The exception is that, for triphthongs that are spelled with only two vowel letters, both of which are the semi-vowels, the tone marker goes on the second spelled vowel.

Specifically, if the spelling of a diphthong begins with "i" (as in "ia") or "u" (as in "ua"), which serves as a near-semi-vowel, this letter does not take the tone marker. Likewise, if the spelling of a diphthong ends with "o" or "u" representing a near-semi-vowel (as in "ao" or "ou"), this letter does not receive a tone marker. In a triphthong spelled with three of "a, e, i, o", and "u" (with "i" or "u" replaced by "y" or "w" at the start of a syllable), the first and third letters coincide with near-semi-vowels and hence do not receive the tone marker (as in "iao" or "uai" or "iou"). But if no letter is written to represent a triphthong's middle (non-semi-vowel) sound (as in "ui" or "iu"), then the tone marker goes on the final (second) vowel letter.

In addition to tone number and mark, tone color has been suggested as a visual aid for learning. Although there are no formal standards, there are a number of different color schemes in use.

In spoken Chinese, the third tone is often pronounced as a "half third tone", in which the pitch does not rise. Additionally, when two third tones appear consecutively, such as in ("nǐhǎo", hello), the first syllable is pronounced with the second tone — this is called tone sandhi. In pinyin, words like "hello" are still written with two third tones ("nǐhǎo").

Pinyin differs from other romanizations in several aspects, such as the following:


Most of the above are used to avoid ambiguity when words of more than one syllable are written in pinyin. For example, "uenian" is written as "wenyan" because it is not clear which syllables make up "uenian"; "uen-ian", "uen-i-an", "u-en-i-an", "u-e-nian", and "u-e-ni-an" are all possible combinations, but "wenyan" is unambiguous since "we", "nya", etc. do not exist in pinyin. See the pinyin table article for a summary of possible pinyin syllables (not including tones).

Although Chinese characters represent single syllables, Mandarin Chinese is a polysyllabic language. Spacing in pinyin is usually based on words, and not on single syllables. However, there are often ambiguities in partitioning a word. "The Basic Rules of the Chinese Phonetic Alphabet Orthography" () were put into effect in 1988 by the National Educational Commission () and the National Language Commission (). These rules became a Guóbiāo recommendation in 1996 and were updated in 2012.


Pinyin is now used by foreign students learning Chinese as a second language, as well as Bopomofo.

Pinyin assigns some Latin letters sound values which are quite different from that of most languages. This has drawn some criticism as it may lead to confusion when uninformed speakers apply either native or English assumed pronunciations to words. However, this problem is not limited only to pinyin, since many languages that use the Latin alphabet natively also assign different values to the same letters. A recent study on Chinese writing and literacy concluded, "By and large, pinyin represents the Chinese sounds better than the Wade–Giles system, and does so with fewer extra marks."

Because Pinyin is purely a representation of the sounds of Mandarin, it completely lacks the semantic cues and contexts inherent in Chinese characters. Pinyin is also unsuitable for transcribing some Chinese spoken languages other than Mandarin, languages which by contrast have traditionally been written with Han characters allowing for written communication which, by its unified semanto-phonetic orthography, could theoretically be readable in any of the various vernaculars of Chinese where a phonetic script would have only localized utility.

Based on ISO 7098:2015, "Information and Documentation: Chinese Romanization" (), tonal marks for pinyin should use the symbols from Combining Diacritical Marks, as opposed by the use of Spacing Modifier Letters in Bopomofo. Lowercase letters with tone marks are included in GB/T 2312 and their uppercase counterparts are included in JIS X 0212; thus Unicode includes all the common accented characters from pinyin.

Due to "The Basic Rules of the Chinese Phonetic Alphabet Orthography", all accented letters are required to have both uppercase and lowercase characters as per their normal counterparts.
GBK has mapped two characters ‘ḿ’ and ‘ǹ’ to Private Use Areas in Unicode as U+E7C7 () and U+E7C8 () respectively, thus some Simplified Chinese fonts (e.g. SimSun) that adheres to GBK include both characters in the Private Use Areas, and some input methods (e.g. Sogou Pinyin) also outputs the Private Use Areas code point instead of the original character. As the superset GB 18030 changed the mappings of ‘ḿ’ and ‘ǹ’, this has caused issue where the input methods and font files use different encoding standard, and thus the input and output of both characters are mixed up.
Other symbols that are used in pinyin is as follow:

Other punctuation mark and symbols in Chinese are to use the equivalent symbol in English noted in to GB/T 15834.

In educational usage, to match the handwritten style, some fonts used a different style for the letter "a" and "g" to have an appearance of single-storey "a" and single-storey "g". Fonts that follow GB/T 2312 usually make single-storey "a" in the accented pinyin characters but leaving unaccented double-storey "a", causing a discrepancy in the font itself. Unicode did not provide an official way to encode single-storey "a" and single-storey "g", but as IPA require the differentiation of single-storey and double-storey "a" and "g", thus the single-storey character "ɑ"/"ɡ" in IPA should be used if the need to separate single-storey "a" and "g" arises. For daily usage there is no need to differentiate single-storey and double-storey "a"/"g".

Pinyin superseded older romanization systems such as Wade–Giles (1859; modified 1892) and postal romanization, and replaced zhuyin as the method of Chinese phonetic instruction in mainland China. The ISO adopted pinyin as the standard romanization for modern Chinese in 1982 (ISO 7098:1982, superseded by ISO 7098:2015). The United Nations followed suit in 1986. It has also been accepted by the government of Singapore, the United States's Library of Congress, the American Library Association, and many other international institutions.

The spelling of Chinese geographical or personal names in pinyin has become the most common way to transcribe them in English. Pinyin has also become the dominant method for entering Chinese text into computers in Mainland China, in contrast to Taiwan; where Bopomofo is most commonly used.

Families outside of Taiwan who speak Mandarin as a mother tongue use pinyin to help children associate characters with spoken words which they already know. Chinese families outside of Taiwan who speak some other language as their mother tongue use the system to teach children Mandarin pronunciation when they learn vocabulary in elementary school.

Since 1958, pinyin has been actively used in adult education as well, making it easier for formerly illiterate people to continue with self-study after a short period of pinyin literacy instruction.

Pinyin has become a tool for many foreigners to learn Mandarin pronunciation, and is used to explain both the grammar and spoken Mandarin coupled with Chinese characters (). Books containing both Chinese characters and pinyin are often used by foreign learners of Chinese. Pinyin's role in teaching pronunciation to foreigners and children is similar in some respects to furigana-based books (with hiragana letters written above or next to kanji, directly analogous to zhuyin) in Japanese or fully vocalised texts in Arabic ("vocalised Arabic").

The tone-marking diacritics are commonly omitted in popular news stories and even in scholarly works. This results in some degree of ambiguity as to which words are being represented.

Simple computer systems, able to display only 7-bit ASCII text (essentially the 26 Latin letters, 10 digits, and punctuation marks), long provided a convincing argument for using unaccented pinyin instead of Chinese characters. Today, however, most computer systems are able to display characters from Chinese and many other writing systems as well, and have them entered with a Latin keyboard using an input method editor. Alternatively, some PDAs, tablet computers, and digitizing tablets allow users to input characters graphically by writing with a stylus, with concurrent online handwriting recognition.

Pinyin with accents can be entered with the use of special keyboard layouts or various character map utilities. X keyboard extension includes a "Hanyu Pinyin (altgr)" layout for AltGr-triggered dead key input of accented characters.

Taiwan (Republic of China) adopted "Tongyong Pinyin", a modification of "Hanyu Pinyin", as the official romanization system on the national level between October 2002 and January 2009, when it decided to promote "Hanyu Pinyin". "Tongyong Pinyin" ("common phonetic"), a romanization system developed in Taiwan, was designed to romanize languages and dialects spoken on the island in addition to Mandarin Chinese. The Kuomintang (KMT) party resisted its adoption, preferring the "Hanyu Pinyin" system used in mainland China and in general use internationally. Romanization preferences quickly became associated with issues of national identity. Preferences split along party lines: the KMT and its affiliated parties in the pan-blue coalition supported the use of Hanyu Pinyin while the Democratic Progressive Party and its affiliated parties in the pan-green coalition favored the use of Tongyong Pinyin.

"Tongyong Pinyin" was made the official system in an administrative order that allowed its adoption by local governments to be voluntary. Locales in Kaohsiung, Tainan and other areas use romanizations derived from Tongyong Pinyin for some district and street names. A few localities with governments controlled by the KMT, most notably Taipei, Hsinchu, and Kinmen County, overrode the order and converted to "Hanyu Pinyin" before the January 1, 2009 national-level decision, though with a slightly different capitalization convention than mainland China. Most areas of Taiwan adopted Tongyong Pinyin, consistent with the national policy. Today, many street signs in Taiwan are using "Tongyong Pinyin"-derived romanizations, but some, especially in northern Taiwan, display "Hanyu Pinyin"-derived romanizations. It is not unusual to see spellings on street signs and buildings derived from the older Wade–Giles, MPS2 and other systems.

Attempts to make pinyin standard in Taiwan have had uneven success, with most place and proper names remaining unaffected, including all major cities. Personal names on Taiwanese passports honor the choices of Taiwanese citizens, who can choose Wade-Giles, Hakka, Hoklo, Tongyong, aboriginal, or pinyin. Official pinyin use is controversial, as when pinyin use for a metro line in 2017 provoked protests, despite government responses that “The romanization used on road signs and at transportation stations is intended for foreigners... Every foreigner learning Mandarin learns Hanyu pinyin, because it is the international standard...The decision has nothing to do with the nation’s self-determination or any ideologies, because the key point is to ensure that foreigners can read signs.”

Pinyin-like systems have been devised for other variants of Chinese. Guangdong Romanization is a set of romanizations devised by the government of Guangdong province for Cantonese, Teochew, Hakka (Moiyen dialect), and Hainanese. All of these are designed to use Latin letters in a similar way to pinyin.

In addition, in accordance to the "Regulation of Phonetic Transcription in Hanyu Pinyin Letters of Place Names in Minority Nationality Languages" () promulgated in 1976, place names in non-Han languages like Mongolian, Uyghur, and Tibetan are also officially transcribed using pinyin in a system adopted by the State Administration of Surveying and Mapping and Geographical Names Committee known as SASM/GNC romanization. The pinyin letters (26 Roman letters, plus "ü" and "ê") are used to approximate the non-Han language in question as closely as possible. This results in spellings that are different from both the customary spelling of the place name, and the pinyin spelling of the name in Chinese:

"Tongyong Pinyin" was developed in Taiwan for use in rendering not only Mandarin Chinese, but other languages and dialects spoken on the island such as Taiwanese, Hakka, and aboriginal languages.





</doc>
<doc id="23589" url="https://en.wikipedia.org/wiki?curid=23589" title="Parable of the Pearl">
Parable of the Pearl

The Parable of the Pearl (also called the Pearl of Great Price) is one of the parables of Jesus. It appears in and illustrates the great value of the Kingdom of Heaven.

This is the penultimate parable in Matthew 13, coming just before the Parable of the Dragnet. It immediately follows the Parable of the Hidden Treasure, which has a similar theme. It does not appear in the other synoptic gospels, but a version of this parable does appear in the non-canonical Gospel of Thomas, Saying 76. The parable has been depicted by artists such as Domenico Fetti.

The parable reads as follows:

This parable is generally interpreted as illustrating the great value of the Kingdom of Heaven. Theologian E. H. Plumptre, in Anglican bishop Charles Ellicott's "Commentary", notes that:
"the caprices of luxury in the Roman empire had given a prominence to pearls, as an article of commerce, which they had never had before, and have probably never had since. They, rather than emeralds and sapphires, were the typical instance of all costliest adornments (; ). The story of Cleopatra and the fact that the opening of a new pearl market was one of the alleged motives which led the Emperor Claudius to invade Britain, are indications of the value that was then set on the “goodly pearls” of the parable." 
Theologian John Nolland likewise notes that pearls at that time had a greater value than they do today, and it thus has a similar theme to its partner, the parable of the hidden treasure. Nolland comments that it shares with that parable the notions of "good fortune and demanding action in attaining the kingdom of heaven", but adds in this case the notion of "diligent seeking".

The valuable pearl is the "deal of a lifetime" for the merchant in the story. However, those who do not believe in the kingdom of heaven enough to stake their whole future on it are unworthy of the kingdom.

This interpretation of the parable is the inspiration for a number of hymns, including the anonymous Swedish hymn "Den Kostliga Pärlan" ("O That Pearl of Great Price!"), which begins:

<poem>
O that Pearl of great price! have you found it?
Is the Savior supreme in your love?
O consider it well, ere you answer,
As you hope for a welcome above.
Have you given up all for this Treasure?
Have you counted past gains as but loss?
Has your trust in yourself and your merits
Come to naught before Christ and His cross?
</poem>

A less common interpretation of the parable is that the merchant represents Jesus, and the pearl represents the Christian Church. This interpretation would give the parable a similar theme to that of the Parable of the Lost Sheep, the Lost Coin, and the Prodigal Son.

Pope Pius XII used the phrase to describe virginity.

"Pearl of Great Price" is the title of a selection of Mormon writings, one of the standard works of The Church of Jesus Christ of Latter-day Saints and some other Latter Day Saint denominations.
A version of the parable also appears in the Gnostic Gospel of Thomas (Saying 76):
This work's version of the parable of the Hidden Treasure appears later (Saying 109), rather than immediately preceding, as in Matthew. However, the mention of a treasure in Saying 76 may reflect a source for the Gospel of Thomas in which the parables were adjacent, so that the original pair of parables has been "broken apart, placed in separate contexts, and expanded in a manner characteristic of folklore." In Gnostic thought the pearl may represent Christ or the true self. In the Gnostic Acts of Peter and the Twelve, found with the Gospel of Thomas in the Nag Hammadi library, the travelling pearl merchant Lithargoel is eventually revealed to be Jesus.

There have been several depictions of the New Testament parable in art, including works by Domenico Fetti, John Everett Millais and Jan Luyken.

The parable is referenced in Nathaniel Hawthorne's novel "The Scarlet Letter" on page 82:
The parable is referenced in "Star Trek" by Scotty at the end of an episode in the original series entitled "The Empath".



</doc>
<doc id="23590" url="https://en.wikipedia.org/wiki?curid=23590" title="Pantheism">
Pantheism

Pantheism is the belief that reality is identical with divinity, or that all-things compose an all-encompassing, immanent god. Pantheist belief does not recognize a distinct personal god, anthropomorphic or otherwise, but instead characterizes a broad range of doctrines differing in forms of relationships between reality and divinity. Pantheistic concepts date back thousands of years, and pantheistic elements have been identified in various religious traditions. The term "pantheism" was coined by mathematician Joseph Raphson in 1697 and has since been used to describe the beliefs of a variety of people and organizations.

Pantheism was popularized in Western culture as a theology and philosophy based on the work of the 17th-century philosopher Baruch Spinoza, in particular, his book "Ethics". A pantheistic stance was also taken in the 16th century by philosopher and cosmologist Giordano Bruno.

"Pantheism" derives from the Greek πᾶν "pan" (meaning "all, of everything") and θεός "theos" (meaning "god, divine"). The first known combination of these roots appears in Latin, in Joseph Raphson's 1697 book "De Spatio Reali seu Ente Infinito", where he refers to the "pantheismus" of Spinoza and others.
It was subsequently translated into English as "pantheism" in 1702.

There are numerous definitions of pantheism. Some consider it a theological and philosophical position concerning God.

Pantheism is the view that everything is part of an all-encompassing, immanent God. All forms of reality may then be considered either modes of that Being, or identical with it. Some hold that pantheism is a non-religious philosophical position. To them, pantheism is the view that the Universe (in the sense of the totality of all existence) and God are identical (implying a denial of the personality and transcendence of God).

Early traces of pantheist thought can be found within the theology of the ancient Greek religion of Orphism, where "pan" (the all) is made cognate with the creator God Phanes (symbolizing the universe), and with Zeus, after the swallowing of Phanes.

Pantheistic tendencies existed in a number of early Gnostic groups, with pantheistic thought appearing throughout the Middle Ages. These included a section of Johannes Scotus Eriugena's 9th-century work De divisione naturae and the beliefs of mystics such as Amalric of Bena (11th12th centuries) and Eckhart (12th13th).

The Catholic Church has long regarded pantheistic ideas as heresy. Giordano Bruno, an Italian monk who evangelized about a transcendent and infinite God, was burned at the stake in 1600 by the Roman Inquisition. He has since become known as a celebrated pantheist and martyr of science, and an influence on many later thinkers.

In the West, pantheism was formalized as a separate theology and philosophy based on the work of the 17th-century philosopher Baruch Spinoza. Spinoza was a Dutch philosopher of Portuguese descent raised in the Sephardi Jewish community in Amsterdam. He developed highly controversial ideas regarding the authenticity of the Hebrew Bible and the nature of the Divine, and was effectively excluded from Jewish society at age 23, when the local synagogue issued a "herem" against him. A number of his books were published posthumously, and shortly thereafter included in the Catholic Church's "Index of Forbidden Books". The breadth and importance of Spinoza's work would not be realized for many years – as the groundwork for the 18th-century Enlightenment and modern biblical criticism, including modern conceptions of the self and the universe.

In the posthumous "Ethics", "Spinoza wrote the last indisputable Latin masterpiece, and one in which the refined conceptions of medieval philosophy are finally turned against themselves and destroyed entirely.". In particular, he opposed René Descartes' famous mind–body dualism, the theory that the body and spirit are separate. Spinoza held the monist view that the two are the same, and monism is a fundamental part of his philosophy. He was described as a "God-intoxicated man," and used the word God to describe the unity of all substance. This view influenced philosophers such as Georg Wilhelm Friedrich Hegel, who said, "You are either a Spinozist or not a philosopher at all." 
Spinoza earned praise as one of the great rationalists of 17th-century philosophy and one of Western philosophy's most important thinkers. Although the term "pantheism" was not coined until after his death, he is regarded as the most celebrated advocate of the concept. "Ethics" was the major source from which Western pantheism spread.

Heinrich Heine, in his "Concerning the History of Religion and Philosophy in Germany" (1833–36), remarked that "I don't remember now where I read that Herder once exploded peevishly at the constant preoccupation with Spinoza, "If Goethe would only for once pick up some other Latin book than Spinoza!" But this applies not only to Goethe; quite a number of his friends, who later became more or less well-known as poets, paid homage to pantheism in their youth, and this doctrine flourished actively in German art before it attained supremacy among us as a philosophic theory."

In their "The Holy Family" (1844) Karl Marx and Friedrich Engels notes, "Spinozism dominated the eighteenth century both in its later French variety, which made matter into substance, and in deism, which conferred on matter a more spiritual name... Spinoza's French school and the supporters of deism were but two sects disputing over the true meaning of his system..."

In George Henry Lewes's words (1846), "Pantheism is as old as philosophy. It was taught in the old Greek schools — by Plato, by St. Augustine, and by the Jews. Indeed, one may say that Pantheism, under one of its various shapes, is the necessary consequence of all metaphysical inquiry, when pushed to its logical limits; and from this reason do we find it in every age and nation. The dreamy contemplative Indian, the quick versatile Greek, the practical Roman, the quibbling Scholastic, the ardent Italian, the lively Frenchman, and the bold Englishman, have all pronounced it as the final truth of philosophy. Wherein consists Spinoza's originality? — what is his merit? — are natural questions, when we see him only lead to the same result as others had before proclaimed. His merit and originality consist in the systematic exposition and development of that doctrine — in his hands, for the first time, it assumes the aspect of a science. The Greek and Indian Pantheism is a vague fanciful doctrine, carrying with it no scientific conviction; it may be true — it looks true — but the proof is wanting. But with Spinoza there is no choice: if you understand his terms, admit the possibility of his science, and seize his meaning; you can no more doubt his conclusions than you can doubt Euclid; no mere opinion is possible, conviction only is possible."

S. M. Melamed (1933) noted, "It may be observed, however, that Spinoza was not the first prominent monist and pantheist in modern Europe. A generation before him Bruno conveyed a similar message to humanity. Yet Bruno is merely a beautiful episode in the history of the human mind, while Spinoza is one of its most potent forces. Bruno was a rhapsodist and a poet, who was overwhelmed with artistic emotions; Spinoza, however, was spiritus purus and in his method the prototype of the philosopher."

The first known use of the term "pantheism" was in Latin ("pantheismus" ) by the English mathematician Joseph Raphson in his work "De Spatio Reali seu Ente Infinito", published in 1697. Raphson begins with a distinction between atheistic "panhylists" (from the Greek roots "pan", "all", and "hyle", "matter"), who believe everything is matter, and Spinozan "pantheists" who believe in "a certain universal substance, material as well as intelligence, that fashions all things that exist out of its own essence." Raphson thought that the universe was immeasurable in respect to a human's capacity of understanding, and believed that humans would never be able to comprehend it. He referred to the pantheism of the Ancient Egyptians, Persians, Syrians, Assyrians, Greek, Indians, and Jewish Kabbalists, specifically referring to Spinoza.

The term was first used in English by a translation of Raphson's work in 1702. It was later used and popularized by Irish writer John Toland in his work of 1705 "Socinianism Truly Stated, by a pantheist". Toland was influenced by both Spinoza and Bruno, and had read Joseph Raphson's "De Spatio Reali", referring to it as "the ingenious Mr. Ralphson's (sic) Book of Real Space". Like Raphson, he used the terms "pantheist" and "Spinozist" interchangeably. In 1720 he wrote the "Pantheisticon: or The Form of Celebrating the Socratic-Society" in Latin, envisioning a pantheist society that believed, "All things in the world are one, and one is all in all things ... what is all in all things is God, eternal and immense, neither born nor ever to perish." He clarified his idea of pantheism in a letter to Gottfried Leibniz in 1710 when he referred to "the pantheistic opinion of those who believe in no other eternal being but the universe".

In the mid-eighteenth century, the English theologian Daniel Waterland defined pantheism this way: "It supposes God and nature, or God and the whole universe, to be one and the same substance—one universal being; insomuch that men's souls are only modifications of the divine substance." In the early nineteenth century, the German theologian Julius Wegscheider defined pantheism as the belief that God and the world established by God are one and the same.

Between 1785–89, a major controversy about Spinoza's philosophy arose between the German philosophers Friedrich Heinrich Jacobi (a critic) and Moses Mendelssohn (a defender). Known in German as the "Pantheismusstreit" (pantheism controversy), it helped spread pantheism to many German thinkers.
A 1780 conversation with the German dramatist Gotthold Ephraim Lessing led Jacobi to a protracted study of Spinoza's works. Lessing stated that he knew no other philosophy than Spinozism.
Jacobi's "Über die Lehre des Spinozas" (1st ed. 1785, 2nd ed. 1789) expressed his strenuous objection to a dogmatic system in philosophy, and drew upon him the enmity of the Berlin group, led by Mendelssohn. Jacobi claimed that Spinoza's doctrine was pure materialism, because all Nature and God are said to be nothing but extended substance. This, for Jacobi, was the result of Enlightenment rationalism and it would finally end in absolute atheism. Mendelssohn disagreed with Jacobi, saying that pantheism shares more characteristics of theism than of atheism. The entire issue became a major intellectual and religious concern for European civilization at the time.

Willi Goetschel argues that Jacobi's publication significantly shaped Spinoza's wide reception for centuries following its publication, obscuring the nuance of Spinoza's philosophic work.

During the beginning of the 19th century, pantheism was the viewpoint of many leading writers and philosophers, attracting figures such as William Wordsworth and Samuel Coleridge in Britain; Johann Gottlieb Fichte, Schelling and Hegel in Germany; Knut Hamsun in Norway; and Walt Whitman, Ralph Waldo Emerson and Henry David Thoreau in the United States. Seen as a growing threat by the Vatican, in 1864 it was formally condemned by Pope Pius IX in the "Syllabus of Errors".

A letter written by William Herndon, Abraham Lincoln's law partner in 1886, was sold at auction for US$30,000 in 2011. In it, Herndon writes of the U.S. President's evolving religious views, which included pantheism.
The subject is understandably controversial, but the content of the letter is consistent with Lincoln's fairly lukewarm approach to organized religion.

Some 19th-century theologians thought that various pre-Christian religions and philosophies were pantheistic. They thought Pantheism was similar to the ancient Hindu philosophy of Advaita (non-dualism) to the extent that the 19th-century German Sanskritist Theodore Goldstücker remarked that Spinoza's thought was "... a western system of philosophy which occupies a foremost rank amongst the philosophies of all nations and ages, and which is so exact a representation of the ideas of the Vedanta, that we might have suspected its founder to have borrowed the fundamental principles of his system from the Hindus."

19th-century European theologians also considered Ancient Egyptian religion to contain pantheistic elements and pointed to Egyptian philosophy as a source of Greek Pantheism. The latter included some of the Presocratics, such as Heraclitus and Anaximander. The Stoics were pantheists, beginning with Zeno of Citium and culminating in the emperor-philosopher Marcus Aurelius. During the pre-Christian Roman Empire, Stoicism was one of the three dominant schools of philosophy, along with Epicureanism and Neoplatonism. The early Taoism of Laozi and Zhuangzi is also sometimes considered pantheistic, although it could be more similar to Panentheism.

Cheondoism, which arose in the Joseon Dynasty of Korea, and Won Buddhism are also considered pantheistic. The Realist Society of Canada believes that the consciousness of the self-aware universe is reality, which is an alternative view of Pantheism. 

In a letter written to Eduard Büsching (25 October 1929), after Büsching sent Albert Einstein a copy of his book "Es gibt keinen Gott" ("There is no God"), Einstein wrote, "We followers of Spinoza see our God in the wonderful order and lawfulness of all that exists and in its soul ["Beseeltheit"] as it reveals itself in man and animal." According to Einstein, the book only dealt with the concept of a personal god and not the impersonal God of pantheism. In a letter written in 1954 to philosopher Eric Gutkind, Einstein wrote "the word God is for me nothing more than the expression and product of human weaknesses." In another letter written in 1954 he wrote "I do not believe in a personal God and I have never denied this but have expressed it clearly." In "Ideas And Opinions", published a year before his death, Einstein stated his precise conception of the word God:

Scientific research can reduce superstition by encouraging people to think and view things in terms of cause and effect. Certain it is that a conviction, akin to religious feeling, of the rationality and intelligibility of the world lies behind all scientific work of a higher order. [...] This firm belief, a belief bound up with a deep feeling, in a superior mind that reveals itself in the world of experience, represents my conception of God. In common parlance this may be described as "pantheistic" (Spinoza).

In the late 20th century, some declared that pantheism was an underlying theology of Neopaganism, and pantheists began forming organizations devoted specifically to pantheism and treating it as a separate religion.

Dorion Sagan, son of famous scientist and science communicator Carl Sagan, published the 2007 book "Dazzle Gradually: Reflections on the Nature of Nature", co-written with his mother Lynn Margulis. In the chapter "Truth of My Father", Sagan writes that his "father believed in the God of Spinoza and Einstein, God not behind nature, but as nature, equivalent to it."

In 2009, pantheism was mentioned in a Papal encyclical and in a statement on New Year's Day, 2010, criticizing pantheism for denying the superiority of humans over nature and seeing the source of man salvation in nature.

In a 2009 review of the film "Avatar", Ross Douthat described pantheism as "Hollywood's religion of choice for a generation now".

In 2015 The Paradise Project, an organization "dedicated to celebrating and spreading awareness about pantheism," commissioned Los Angeles muralist Levi Ponce to paint the 75-foot mural "Luminaries of Pantheism" in Venice, California near the organization's offices. The mural depicts Albert Einstein, Alan Watts, Baruch Spinoza, Terence McKenna, Carl Jung, Carl Sagan, Emily Dickinson, Nikola Tesla, Friedrich Nietzsche, Ralph Waldo Emerson, W.E.B. Du Bois, Henry David Thoreau, Elizabeth Cady Stanton, Rumi, Adi Shankara, and Laozi.

There are multiple varieties of pantheism and various systems of classifying them relying upon one or more spectra or in discrete categories.

The philosopher Charles Hartshorne used the term Classical Pantheism to describe the deterministic philosophies of Baruch Spinoza, the Stoics, and other like-minded figures. Pantheism (All-is-God) is often associated with monism (All-is-One) and some have suggested that it logically implies determinism (All-is-Now). Albert Einstein explained theological determinism by stating, "the past, present, and future are an 'illusion'". This form of pantheism has been referred to as "extreme monism", in which in the words of one commentator "God decides or determines everything, including our supposed decisions." Other examples of determinism-inclined pantheisms include those of Ralph Waldo Emerson, and Hegel.

However, some have argued against treating every meaning of "unity" as an aspect of pantheism, and there exist versions of pantheism that regard determinism as an inaccurate or incomplete view of nature. Examples include the beliefs of John Scotus Eriugena, Friedrich Wilhelm Joseph Schelling and William James.

It may also be possible to distinguish two types of pantheism, one being more religious and the other being more philosophical. The Columbia Encyclopedia writes of the distinction:

Philosophers and theologians have often suggested that pantheism implies monism. Different types of monism include:


Views contrasting with monism are:

Monism in modern philosophy of mind can be divided into three broad categories:

Certain positions do not fit easily into the above categories, such as functionalism, anomalous monism, and reflexive monism. Moreover, they do not define the meaning of "real".

In 1896, J. H. Worman, a theologian, identified seven categories of pantheism: Mechanical or materialistic (God the mechanical unity of existence); Ontological (fundamental unity, Spinoza); Dynamic; Psychical (God is the soul of the world); Ethical (God is the universal moral order, Fichte); Logical (Hegel); and Pure (absorption of God into nature, which Worman equates with atheism).

More recently, Paul D. Feinberg, professor of biblical and systematic theology at Trinity Evangelical Divinity School, also identified seven: Hylozoistic; Immanentistic; Absolutistic monistic; Relativistic monistic; Acosmic; Identity of opposites; and Neoplatonic or emanationistic.

Nature worship or nature mysticism is often conflated and confused with pantheism. It is pointed out by at least one expert, Harold Wood, founder of the Universal Pantheist Society, that in pantheist philosophy Spinoza's identification of God with nature is very different from a recent idea of a self identifying pantheist with environmental ethical concerns. His use of the word nature to describe his worldview may be vastly different from the "nature" of modern sciences. He and other nature mystics who also identify as pantheists use "nature" to refer to the limited natural environment (as opposed to man-made built environment). This use of "nature" is different from the broader use from Spinoza and other pantheists describing natural laws and the overall phenomena of the physical world. Nature mysticism may be compatible with pantheism but it may also be compatible with theism and other views.

Nontheism is an umbrella term which has been used to refer to a variety of religions not fitting traditional theism, and under which pantheism has been included.

Panentheism (from Greek πᾶν (pân) "all"; ἐν (en) "in"; and θεός (theós) "God"; "all-in-God") was formally coined in Germany in the 19th century in an attempt to offer a philosophical synthesis between traditional theism and pantheism, stating that God is substantially omnipresent in the physical universe but also exists "apart from" or "beyond" it as its Creator and Sustainer. Thus panentheism separates itself from pantheism, positing the extra claim that God exists above and beyond the world as we know it. The line between pantheism and panentheism can be blurred depending on varying definitions of God, so there have been disagreements when assigning particular notable figures to pantheism or panentheism.

Pandeism is another word derived from pantheism, and is characterized as a combination of reconcilable elements of pantheism and deism. It assumes a Creator-deity that is at some point distinct from the universe and then transforms into it, resulting in a universe similar to the pantheistic one in present essence, but differing in origin.

Panpsychism is the philosophical view held by many pantheists that consciousness, mind, or soul is a universal feature of all things. Some pantheists also subscribe to the distinct philosophical views hylozoism (or panvitalism), the view that everything is alive, and its close neighbor animism, the view that everything has a soul or spirit.

Many traditional and folk religions including African traditional religions and Native American religions can be seen as pantheistic, or a mixture of pantheism and other doctrines such as polytheism and animism. According to pantheists, there are elements of pantheism in some forms of Christianity.

Ideas resembling pantheism existed in East/South Asian religions before the 18th century (notably Sikhism, Hinduism, Confucianism, and Taoism). Although there is no evidence that these influenced Spinoza's work, there is such evidence regarding other contemporary philosophers, such as Leibniz, and later Voltaire. In the case of Hinduism, pantheistic views exist alongside panentheistic, polytheistic, monotheistic, and atheistic ones. In the case of Sikhism, stories attributed to Guru Nanak suggest that he believed God was everywhere in the physical world, and the Sikh tradition typically describes God as the preservative force within the physical world, present in all material forms, each created as a manifestation of God. However, Sikhs view God as the transcendent creator, "immanent in the phenomenal reality of the world in the same way in which an artist can be said to be present in his art". This implies a more panentheistic position.

Pantheism is popular in modern spirituality and new religious movements, such as Neopaganism and Theosophy. Two organizations that specify the word pantheism in their title formed in the last quarter of the 20th century. The Universal Pantheist Society, open to all varieties of pantheists and supportive of environmental causes, was founded in 1975. The World Pantheist Movement is headed by Paul Harrison, an environmentalist, writer and a former vice president of the Universal Pantheist Society, from which he resigned in 1996. The World Pantheist Movement was incorporated in 1999 to focus exclusively on promoting naturalistic pantheism – a strict metaphysical naturalistic version of pantheism, considered by some a form of religious naturalism. It has been described as an example of "dark green religion" with a focus on environmental ethics.






</doc>
<doc id="23591" url="https://en.wikipedia.org/wiki?curid=23591" title="Panentheism">
Panentheism

Panentheism (meaning "all-in-God", from the Greek "pân", "all", "en", "in" and "Theós", "God") is the belief that the divine pervades and interpenetrates every part of the universe and also extends beyond space and time. The term was coined by the German philosopher Karl Krause in 1828 to distinguish the ideas of Georg Wilhelm Friedrich Hegel (1770–1831) and Friedrich Wilhelm Joseph Schelling (1775–1854) about the relation of God and the universe from the supposed pantheism of Baruch Spinoza. Unlike pantheism, which holds that the divine and the universe are identical, panentheism maintains an ontological distinction between the divine and the non-divine and the significance of both.


The religious beliefs of Neoplatonism can be regarded as panentheistic. Plotinus taught that there was an ineffable transcendent God ("the One", "to En", τὸ Ἕν) of which subsequent realities were emanations. From "the One" emanates the Divine Mind ("Nous", Νοῦς) and the Cosmic Soul ("Psyche", Ψυχή). In Neoplatonism the world itself is God (according to Plato's Timaeus 37). This concept of divinity is associated with that of the "Logos" (Λόγος), which had originated centuries earlier with Heraclitus (c. 535–475 BC). The "Logos" pervades the cosmos, whereby all thoughts and all things originate, or as Heraclitus said: "He who hears not me but the Logos will say: All is one." Neoplatonists such as Iamblichus attempted to reconcile this perspective by adding another hypostasis above the original monad of force or "Dunamis" (Δύναμις). This new all-pervasive monad encompassed all creation and its original uncreated emanations.

Baruch Spinoza later claimed that "Whatsoever is, is in God, and without God nothing can be, or be conceived." "Individual things are nothing but modifications of the attributes of God, or modes by which the attributes of God are expressed in a fixed and definite manner." Though Spinoza has been called the "prophet" and "prince" of pantheism, in a letter to Henry Oldenburg Spinoza states that: "as to the view of certain people that I identify god with nature (taken as a kind of mass or corporeal matter), they are quite mistaken". For Spinoza, our universe (cosmos) is a mode under two attributes of Thought and Extension. God has infinitely many other attributes which are not present in our world.

According to German philosopher Karl Jaspers, when Spinoza wrote "Deus sive Natura" (God or Nature) Spinoza did not mean to say that God and Nature are interchangeable terms, but rather that God's transcendence was attested by his infinitely many attributes, and that two attributes known by humans, namely Thought and Extension, signified God's immanence. Furthermore, Martial Guéroult suggested the term "panentheism", rather than "pantheism" to describe Spinoza's view of the relation between God and the world. The world is not God, but it is, in a strong sense, "in" God. Yet, American philosopher and self-described panentheist Charles Hartshorne referred to Spinoza's philosophy as "classical pantheism" and distinguished Spinoza's philosophy from panentheism.

In 1828, the German philosopher Karl Christian Friedrich Krause (1781–1832) seeking to reconcile monotheism and pantheism, coined the term "panentheism" (from the Ancient Greek expression πᾶν ἐν θεῷ, "pān en theṓ", literally "all in god"). This conception of God influenced New England transcendentalists such as Ralph Waldo Emerson. The term was popularized by Charles Hartshorne in his development of process theology and has also been closely identified with the New Thought. The formalization of this term in the West in the 19th century was not new; philosophical treatises had been written on it in the context of Hinduism for millennia.

Philosophers who embraced panentheism have included Thomas Hill Green (1839–1882), James Ward (1843–1925), Andrew Seth Pringle-Pattison (1856–1931) and Samuel Alexander (1859–1938). Beginning in the 1940s, Hartshorne examined numerous conceptions of God. He reviewed and discarded pantheism, deism, and pandeism in favor of panentheism, finding that such a "doctrine contains all of deism and pandeism except their arbitrary negations". Hartshorne formulated God as a being who could become "more perfect": He has absolute perfection in categories for which absolute perfection is possible, and relative perfection (i. e., is superior to all others) in categories for which perfection cannot be precisely determined.

The earliest reference to panentheistic thought in Hindu philosophy is in a creation myth contained in the later section of Rig Veda called the Purusha Sukta, which was compiled before 1100 BCE. The Purusha Sukta gives a description of the spiritual unity of the cosmos. It presents the nature of Purusha or the cosmic being as both immanent in the manifested world and yet transcendent to it. From this being the sukta holds, the original creative will proceeds, by which this vast universe is projected in space and time.

The most influential and dominant school of Indian philosophy, Advaita Vedanta, rejects theism and dualism by insisting that "Brahman [ultimate reality] is without parts or attributes...one without a second." Since Brahman has no properties, contains no internal diversity and is identical with the whole reality it cannot be understood as an anthropomorphic personal God. The relationship between Brahman and the creation is often thought to be panentheistic.

Panentheism is also expressed in the Bhagavad Gita. In verse IX.4, Krishna states: 

Many schools of Hindu thought espouse monistic theism, which is thought to be similar to a panentheistic viewpoint. Nimbarka's school of differential monism (Dvaitadvaita), Ramanuja's school of qualified monism (Vishistadvaita) and Saiva Siddhanta and Kashmir Shaivism are all considered to be panentheistic. Chaitanya Mahaprabhu's Gaudiya Vaishnavism, which elucidates the doctrine of Achintya Bheda Abheda (inconceivable oneness and difference), is also thought to be panentheistic. In Kashmir Shaivism, all things are believed to be a manifestation of Universal Consciousness (Cit or Brahman). So from the point of view of this school, the phenomenal world ("Śakti") is real, and it exists and has its being in Consciousness ("Cit"). Thus, Kashmir Shaivism is also propounding of theistic monism or panentheism.

Shaktism, or Tantra, is regarded as an Indian prototype of Panentheism. Shakti is considered to be the cosmos itself – she is the embodiment of energy and dynamism, and the motivating force behind all action and existence in the material universe. Shiva is her transcendent masculine aspect, providing the divine ground of all being. "There is no Shiva without Shakti, or Shakti without Shiva. The two ... in themselves are One." Thus, it is She who becomes the time and space, the cosmos, it is She who becomes the five elements, and thus all animate life and inanimate forms. She is the primordial energy that holds all creation and destruction, all cycles of birth and death, all laws of cause and effect within Herself, and yet is greater than the sum total of all these. She is transcendent, but becomes immanent as the cosmos (Mula Prakriti). She, the Primordial Energy, directly becomes Matter.

Taoism says that all is part of the eternal tao, and that all interact through qi. Chapter 6 of the Tao Te Ching describes the Tao thus: ""The heart of Tao is immortal, the mysterious fertile mother of us all, of heaven and earth, of every thing and not-thing.""

Panentheism is also a feature of some Christian philosophical theologies and resonates strongly within the theological tradition of the Orthodox Church. It also appears in process theology. Process theological thinkers are generally regarded in the Christian West as unorthodox. Furthermore, process philosophical thought is widely believed to have paved the way for open theism, a movement that tends to associate itself primarily with the Evangelical branch of Protestantism, but is also generally considered unorthodox by most Evangelicals.

In Christianity, creation is not considered a literal "part of" God, and divinity is essentially distinct from creation (i.e., transcendent). There is, in other words, an irradicable difference between the uncreated (i.e., God) and the created (i.e., everything else). This does not mean, however, that the creation is wholly separated from God, because the creation exists in and from the divine energies. In Eastern Orthodoxy, these energies or operations are the natural activity of God and are in some sense identifiable with God, but at the same time the creation is wholly distinct from the divine essence. God creates the universe by His will and from His energies. It is, however, not an imprint or emanation of God's own essence ("ousia"), the essence He shares pre-eternally with His Word and Holy Spirit. Neither is it a directly literal outworking or effulgence of the divine, nor any other process which implies that creation is essentially God or a necessary part of God. The use of the term "panentheism" to describe the divine concept in Orthodox Christian theology is problematic for those who would insist that panentheism requires creation to be "part of" God.

God is not merely Creator of the universe, as His dynamic presence is necessary to sustain the existence of every created thing, small and great, visible and invisible. That is, God's energies maintain the existence of the created order and all created beings, even if those agencies have explicitly rejected him. His love for creation is such that He will not withdraw His presence, which would be the ultimate form of annihilation, not merely imposing death, but ending existence altogether. By this token, the entirety of creation is fundamentally "good" in its very being, and is not innately evil either in whole or in part. This does not deny the existence of spiritual or moral evil in a fallen universe, only the claim that it is an intrinsic property of creation. Sin results from the essential freedom of creatures to operate outside the divine order, not as a "necessary" consequence of having inherited human nature.

Many Christians who believe in universalism – mainly expressed in the Universalist Church of America, originating, as a fusion of Pietist and Anabaptist influences, from the American colonies of the 18th century – hold panentheistic views of God in conjunction with their belief in "apocatastasis", also called universal reconciliation. Panentheistic Christian Universalists often believe that all creation's subsistence in God renders untenable the notion of final and permanent alienation from Him, citing Scriptural passages such as Ephesians 4:6 ("[God] is over all and through all and in all") and Romans 11:36 ("from [God] and through him and to him are all things") to justify both panentheism and universalism. Panentheism was also a major force in the Unitarian church for a long time, based in part on Ralph Waldo Emerson's concept of the Over-soul (from the synonymous essay of 1841).

Panentheistic conceptions of God occur amongst some modern theologians. Process theology and Creation Spirituality, two recent developments in Christian theology, contain panentheistic ideas. Charles Hartshorne (1897–2000), who conjoined process theology with panentheism, maintained a lifelong membership in the Methodist church but was also a Unitarian. In later years he joined the Austin, Texas, Unitarian Universalist congregation and was an active participant in that church. Referring to the ideas such as Thomas Oord's ‘theocosmocentrism’ (2010), the soft panentheism of open theism, Keith Ward's comparative theology and John Polkinghorne's critical realism (2009), Raymond Potgieter observes distinctions such as dipolar and bipolar:

The former suggests two poles separated such as God influencing creation and it in turn its creator (Bangert 2006:168), whereas bipolarity completes God’s being implying interdependence between temporal and eternal poles. (Marbaniang 2011:133), in dealing with Whitehead’s approach, does not make this distinction. I use the term bipolar as a generic term to include suggestions of the structural definition of God’s transcendence and immanence; to for instance accommodate a present and future reality into which deity must reasonably fit and function, and yet maintain separation from this world and evil whilst remaining within it.
Some argue that panentheism should also include the notion that God has always been related to some world or another, which denies the idea of creation out of nothing ("creatio ex nihilo"). Nazarene Methodist theologian Thomas Jay Oord (* 1965) advocates panentheism, but he uses the word "theocosmocentrism" to highlight the notion that God and some world or another are the primary conceptual starting blocks for eminently fruitful theology. This form of panentheism helps in overcoming the problem of evil and in proposing that God's love for the world is essential to who God is.

The Christian Church International also holds to a panentheist doctrine. The Latter Day Saint movement teaches that the Light of Christ "proceeds from God through Christ and gives life and light to all things."

"Gnosticism" is a modern name for a variety of ancient religious ideas and systems prevalent in the first and second century AD. The teachings of the various gnostic groups were very diverse. In his "Dictionary of Gnosticism", Andrew Phillip Smith has written that some branches of Gnosticism taught a panentheistic view of reality, and held to the belief that God exists in the visible world only as sparks of spiritual "light". The goal of human existence is to know the sparks within oneself in order to return to God, who is in the Fullness (or Pleroma).

Gnosticism was panentheistic, believing that the true God is simultaneously both separate from the physical universe and present within it. As Jesus states in the Gospel of Thomas, "I am the light that is over all things. I am all ... . Split a piece of wood; I am there. Lift up the stone, and you will find me there." This seemingly contradictory interpretation of gnostic theology is not without controversy, since one interpretation of dualistic theology holds that a perfect God of pure spirit would not manifest himself through the fallen world of matter.

Manichaeism, being another gnostic sect, preached a very different doctrine in positioning the true Manichaean God against matter as well as other deities, that it described as enmeshed with the world, namely the gods of Jews, Christians and pagans. Nevertheless, this dualistic teaching included an elaborate cosmological myth that narrates the defeat of primal man by the powers of darkness that devoured and imprisoned the particles of light.

Valentinian Gnosticism taught that matter came about through emanations of the supreme being, even if to some this event is held to be more accidental than intentional. To other gnostics, these emanations were akin to the Sephirot of the Kabbalists and deliberate manifestations of a transcendent God through a complex system of intermediaries.

While mainstream Rabbinic Judaism is classically monotheistic, and follows in the footsteps of Maimonides (c. 1135–1204), the panentheistic conception of God can be found among certain mystical Jewish traditions. A leading scholar of Kabbalah, Moshe Idel ascribes this doctrine to the kabbalistic system of Moses ben Jacob Cordovero (1522–1570) and in the eighteenth century to the Baal Shem Tov (c. 1700–1760), founder of the Hasidic movement, as well as his contemporaries, Rabbi Dov Ber, the Maggid of Mezeritch (died 1772), and Menahem Mendel, the Maggid of Bar. This may be said of many, if not most, subsequent Hasidic masters. There is some debate as to whether Isaac Luria (1534–1572) and Lurianic Kabbalah, with its doctrine of tzimtzum, can be regarded as panentheistic.

According to Hasidism, the infinite Ein Sof is incorporeal and exists in a state that is both transcendent and immanent. This appears to be the view of non-Hasidic Rabbi Chaim of Volozhin, as well. Hasidic Judaism merges the elite ideal of nullification to a transcendent God, via the intellectual articulation of inner dimensions through Kabbalah and with emphasis on the panentheistic divine immanence in everything.

Many scholars would argue that "panentheism" is the best single-word description of the philosophical theology of Baruch Spinoza. It is therefore no surprise, that aspects of panentheism are also evident in the theology of Reconstructionist Judaism as presented in the writings of Mordecai Kaplan (1881–1983), who was strongly influenced by Spinoza.

Several Sufi saints and thinkers, primarily Ibn Arabi, held beliefs that have been considered panentheistic. These notions later took shape in the theory of wahdat ul-wujud (the Unity of All Things). Some Sufi Orders, notably the Bektashis and the Universal Sufi movement, continue to espouse panentheistic beliefs. Nizari Ismaili follow panentheism according to Ismaili doctrine. Nevertheless, some Shia Muslims also do believe in different degrees of Panentheism.

Al-Qayyuum is a Name of God in the Qur'an which translates to "The Self-Existing by Whom all subsist". In Islam the universe can not exist if Allah doesn't exist, and it is only by His power which encompasses everything and which is everywhere that the universe can exist. In Ayaẗ al-Kursii God's throne is described as "extending over the heavens and the earth" and "He feels no fatigue in guarding and preserving them". This does not mean though that the universe is God, or that a creature (like a tree or an animal) is God, because those would be respectively pantheism, which is a heresy in traditional Islam, and the worst heresy in Islam, shirk (polytheism). God is separated by His creation but His creation can not survive without Him.

The Mesoamerican empires of the Mayas, Aztecs as well as the South American Incas (Tahuatinsuyu) have typically been characterized as polytheistic, with strong male and female deities. According to Charles C. Mann's history book "", only the lower classes of Aztec society were polytheistic. Philosopher James Maffie has argued that Aztec metaphysics was pantheistic rather than panentheistic, since Teotl was considered by Aztec philosophers to be the ultimate all-encompassing yet all-transcending force defined by its inherit duality.

Native American beliefs in North America have been characterized as panentheistic in that there is an emphasis on a single, unified divine spirit that is manifest in each individual entity. (North American Native writers have also translated the word for God as the Great Mystery or as the Sacred Other) This concept is referred to by many as the Great Spirit. Philosopher J. Baird Callicott has described Lakota theology as panentheistic, in that the divine both transcends and is immanent in everything.

One exception can be modern Cherokee who are predominantly monotheistic but apparently not panentheistic; yet in older Cherokee traditions many observe both aspects of pantheism and panentheism, and are often not beholden to exclusivity, encompassing other spiritual traditions without contradiction, a common trait among some tribes in the Americas. In the stories of Keetoowah storytellers Sequoyah Guess and Dennis Sixkiller, God is known as ᎤᏁᎳᏅᎯ, commonly pronounced "unehlanv," and visited earth in prehistoric times, but then left earth and her people to rely on themselves. This shows a parallel to Vaishnava cosmology.

The Sikh gurus have described God in numerous ways in their hymns included in the Guru Granth Sahib, the holy scripture of Sikhism, but the oneness of the deity is consistently emphasized throughout. God is described in the Mool Mantar, the first passage in the Guru Granth Sahib, and the basic formula of the faith is:

(Sri Guru Granth Sahib Ji, Ang 1) in Punjabi
— ੴ ਸਤਿ ਨਾਮੁ ਕਰਤਾ ਪੁਰਖੁ ਨਿਰਭਉ ਨਿਰਵੈਰੁ ਅਕਾਲ ਮੂਰਤਿ ਅਜੂਨੀ ਸੈਭੰ ਗੁਰਪ੍ਰਸਾਦਿ ॥
"Ik Oankar Satnaam KartaaPurakh Nirbhau Nirvair AkaalMoorat Ajooni Saibhan GurPrasad"
One primal being who made the sound (oan) that expanded and created the world. Truth is the name. Creative being personified. Without fear, without hate. Image of the undying. Beyond birth, self existent. By Guru's grace~

Guru Arjan, the fifth guru of Sikhs, says, "God is beyond colour and form, yet His/Her presence is clearly visible" (Sri Guru Granth Sahib, Ang 74), and "Nanak's Lord transcends the world as well as the scriptures of the east and the west, and yet He/She is clearly manifest" (Sri Guru Granth Sahib, Ang 397).

Knowledge of the ultimate Reality is not a matter for reason; it comes by revelation of the ultimate reality through nadar (grace) and by anubhava (mystical experience). Says Guru Nanak; ""budhi pathi na paiai bahu chaturaiai bhai milai mani bhane."" This translates to "He/She is not accessible through intellect, or through mere scholarship or cleverness at argument; He/She is met, when He/She pleases, through devotion" (GG, 436).

Guru Nanak prefixed the numeral one (ik) to it, making it Ik Oankar or Ek Oankar to stress God's oneness. God is named and known only through his Own immanent nature. The only name which can be said to truly fit God's transcendent state is SatNam ( Sat Sanskrit, Truth), the changeless and timeless Reality. God is transcendent and all-pervasive at the same time. Transcendence and immanence are two aspects of the same single Supreme Reality. The Reality is immanent in the entire creation, but the creation as a whole fails to contain God fully. As says Guru Tegh Bahadur, Nanak IX, "He has himself spread out His/Her Own “maya” (worldly illusion) which He oversees; many different forms He assumes in many colours, yet He stays independent of all" (GG, 537).

In the Bahá'í Faith, God is described as a single, imperishable God, the creator of all things, including all the creatures and forces in the universe. The connection between God and the world is that of the creator to his creation. God is understood to be independent of his creation, and that creation is dependent and contingent on God. Accordingly, the Bahá'í Faith is much more closely aligned with traditions of monotheism than panentheism. God is not seen to be part of creation as he cannot be divided and does not descend to the condition of his creatures. Instead, in the Bahá'í understanding, the world of creation emanates from God, in that all things have been realized by him and have attained to existence. Creation is seen as the expression of God's will in the contingent world, and every created thing is seen as a sign of God's sovereignty, and leading to knowledge of him; the signs of God are most particularly revealed in human beings.

In Konkōkyō, God is named “Tenchi Kane no Kami-Sama” which can mean “Golden spirit of the universe.” Kami (God) is also seen as infinitely loving and powerful.

People associated with panentheism:






</doc>
<doc id="23592" url="https://en.wikipedia.org/wiki?curid=23592" title="Paraphilia">
Paraphilia

Paraphilia (previously known as sexual perversion and sexual deviation) is the experience of intense sexual arousal to atypical objects, situations, fantasies, behaviors, or individuals. 

No consensus has been found for any precise border between unusual sexual interests and paraphilic ones. There is debate over which, if any, of the paraphilias should be listed in diagnostic manuals, such as the "Diagnostic and Statistical Manual of Mental Disorders" (DSM) or the International Classification of Diseases (ICD).

The number and taxonomy of paraphilia is under debate; one source lists as many as 549 types of paraphilia. The DSM-5 has specific listings for eight paraphilic disorders. Several sub-classifications of the paraphilias have been proposed, and some argue that a fully dimensional, spectrum or complaint-oriented approach would better reflect the evidence.

Many terms have been used to describe atypical sexual interests, and there remains debate regarding technical accuracy and perceptions of stigma. Sexologist John Money popularized the term "paraphilia" as a non-pejorative designation for unusual sexual interests. Money described paraphilia as "a sexuoerotic embellishment of, or alternative to the official, ideological norm." Psychiatrist Glen Gabbard writes that despite efforts by Stekel and Money, "the term "paraphilia" remains pejorative in most circumstances."

Coinage of the term "paraphilia" ("paraphilie") has been credited to Friedrich Salomon Krauss in 1903, and it entered the English language in 1913, in reference to Krauss by urologist William J. Robinson. It was used with some regularity by Wilhelm Stekel in the 1920s. The term comes from the Greek παρά ("para") "beside" and φιλία ("-philia") "friendship, love".

In the late 19th century, psychologists and psychiatrists started to categorize various paraphilias as they wanted a more descriptive system than the legal and religious constructs of sodomy and perversion. Before the introduction of the term "paraphilia" in the DSM-III (1980), the term "sexual deviation" was used to refer to paraphilias in the first two editions of the manual. In 1981, an article published in "American Journal of Psychiatry" described paraphilia as "recurrent, intense sexually arousing fantasies, sexual urges, or behaviors generally involving:

Homosexuality, now widely accepted to be a normal variant of human sexuality, was at one time discussed as a sexual deviation. Sigmund Freud and subsequent psychoanalytic thinkers considered homosexuality and paraphilias to result from psychosexual non-normative relations to the Oedipal complex. As such, the term "sexual perversion" or the epithet "pervert" have historically referred to gay men, as well as other non-heterosexuals (people who fall out of the perceived norms of sexual orientation).

By the mid-20th century, mental health practitioners began formalizing "deviant sexuality" classifications into categories. Originally coded as 000-x63, homosexuality was the top of the classification list (Code 302.0) until the American Psychiatric Association removed homosexuality from the DSM in 1973. Martin Kafka writes, "Sexual disorders once considered paraphilias (e.g., homosexuality) are now regarded as variants of normal sexuality."

A 2012 literature study by clinical psychologist James Cantor, when comparing homosexuality with paraphilias, found that both share "the features of onset and course (both homosexuality and paraphilia being life-long), but they appear to differ on sex ratio, fraternal birth order, handedness, IQ and cognitive profile, and neuroanatomy". The research then concluded that the data seemed to suggest paraphilias and homosexuality as two distinct categories, but regarded the conclusion as "quite tentative" given the current limited understanding of paraphilias.

The causes of paraphilic sexual preferences in people are unclear, although a growing body of research points to a possible prenatal neurodevelopmental correlation. A 2008 study analyzing the sexual fantasies of 200 heterosexual men by using the Wilson Sex Fantasy Questionnaire exam determined that males with a pronounced degree of fetish interest had a greater number of older brothers, a high 2D:4D digit ratio (which would indicate excessive prenatal estrogen exposure), and an elevated probability of being left-handed, suggesting that disturbed hemispheric brain lateralization may play a role in deviant attractions.

Behavioral explanations propose that paraphilias are conditioned early in life, during an experience that pairs the paraphilic stimulus with intense sexual arousal. Susan Nolen-Hoeksema suggests that, once established, masturbatory fantasies about the stimulus reinforce and broaden the paraphilic arousal.

There is scientific and political controversy regarding the continued inclusion of sex-related diagnoses such as the paraphilias in the DSM, due to the stigma of being classified as a mental illness.

Some groups, seeking greater understanding and acceptance of sexual diversity, have lobbied for changes to the legal and medical status of unusual sexual interests and practices. Charles Allen Moser, a physician and advocate for sexual minorities, has argued that the diagnoses should be eliminated from diagnostic manuals.

Albert Eulenburg (1914) noted a commonality across the paraphilias, using the terminology of his time, "All the forms of sexual perversion...have one thing in common: their roots reach down into the matrix of natural and normal sex life; there they are somehow closely connected with the feelings and expressions of our physiological erotism. They are...hyperbolic intensifications, distortions, monstrous fruits of certain partial and secondary expressions of this erotism which is considered 'normal' or at least within the limits of healthy sex feeling."

The clinical literature contains reports of many paraphilias, only some of which receive their own entries in the diagnostic taxonomies of the American Psychiatric Association or the World Health Organization. There is disagreement regarding which sexual interests should be deemed paraphilic disorders versus normal variants of sexual interest. For example, as of May 2000, per DSM-IV-TR, "Because some cases of Sexual Sadism may not involve harm to a victim (e.g., inflicting humiliation on a consenting partner), the wording for sexual sadism involves a hybrid of the DSM-III-R and DSM-IV wording (i.e., "the person has acted on these urges with a non-consenting person, or the urges, sexual fantasies, or behaviors cause marked distress or interpersonal difficulty").

The DSM-IV-TR also acknowledges that the diagnosis and classification of paraphilias across cultures or religions "is complicated by the fact that what is considered deviant in one cultural setting may be more acceptable in another setting”. Some argue that cultural relativism is important to consider when discussing paraphilias, because there is wide variance concerning what is sexually acceptable across cultures.

Consensual adult activities and adult entertainment involving sexual roleplay, novel, superficial, or trivial aspects of sexual fetishism, or incorporating the use of sex toys are not necessarily paraphilic. Paraphilial psychopathology is not the same as psychologically normative adult human sexual behaviors, sexual fantasy, and sex play.

Clinicians distinguish between optional, preferred and exclusive paraphilias, though the terminology is not completely standardized. An "optional" paraphilia is an alternative route to sexual arousal. In preferred paraphilias, a person prefers the paraphilia to conventional sexual activities, but also engages in conventional sexual activities.

The literature includes single-case studies of exceedingly rare and idiosyncratic paraphilias. These include an adolescent male who had a strong fetishistic interest in the exhaust pipes of cars, a young man with a similar interest in a specific type of car, and a man who had a paraphilic interest in sneezing (both his own and the sneezing of others).

In American psychiatry, prior to the publication of the DSM-I, paraphilias were classified as cases of "psychopathic personality with pathologic sexuality". The DSM-I (1952) included sexual deviation as a personality disorder of sociopathic subtype. The only diagnostic guidance was that sexual deviation should have been "reserved for deviant sexuality which [was] not symptomatic of more extensive syndromes, such as schizophrenic or obsessional reactions". The specifics of the disorder were to be provided by the clinician as a "supplementary term" to the sexual deviation diagnosis; there were no restrictions in the DSM-I on what this supplementary term could be. Researcher Anil Aggrawal writes that the now-obsolete DSM-I listed examples of supplementary terms for pathological behavior to include "homosexuality, transvestism, pedophilia, fetishism, and sexual sadism, including rape, sexual assault, mutilation."

The DSM-II (1968) continued to use the term "sexual deviations", but no longer ascribed them under personality disorders, but rather alongside them in a broad category titled "personality disorders and certain other nonpsychotic mental disorders". The types of sexual deviations listed in the DSM-II were: sexual orientation disturbance (homosexuality), fetishism, pedophilia, transvestitism (sic), exhibitionism, voyeurism, sadism, masochism, and "other sexual deviation". No definition or examples were provided for "other sexual deviation", but the general category of sexual deviation was meant to describe the sexual preference of individuals that was "directed primarily toward objects other than people of opposite sex, toward sexual acts not usually associated with coitus, or toward coitus performed under bizarre circumstances, as in necrophilia, pedophilia, sexual sadism, and fetishism." Except for the removal of homosexuality from the DSM-III onwards, this definition provided a general standard that has guided specific definitions of paraphilias in subsequent DSM editions, up to DSM-IV-TR.

The term "paraphilia" was introduced in the DSM-III (1980) as a subset of the new category of "psychosexual disorders."

The DSM-III-R (1987) renamed the broad category to sexual disorders, renamed atypical paraphilia to paraphilia NOS (not otherwise specified), renamed transvestism as transvestic fetishism, added frotteurism, and moved zoophilia to the NOS category. It also provided seven nonexhaustive examples of NOS paraphilias, which besides zoophilia included telephone scatologia, necrophilia, partialism, coprophilia, klismaphilia, and urophilia.

The DSM-IV (1994) retained the sexual disorders classification for paraphilias, but added an even broader category, "sexual and gender identity disorders," which includes them. The DSM-IV retained the same types of paraphilias listed in DSM-III-R, including the NOS examples, but introduced some changes to the definitions of some specific types.

The DSM-IV-TR describes paraphilias as "recurrent, intense sexually arousing fantasies, sexual urges or behaviors generally involving nonhuman objects, the suffering or humiliation of oneself or one's partner, or children or other nonconsenting persons that occur over a period of six months" (criterion A), which "cause clinically significant distress or impairment in social, occupational, or other important areas of functioning" (criterion B). DSM-IV-TR names eight specific paraphilic disorders (exhibitionism, fetishism, frotteurism, pedophilia, sexual masochism, sexual sadism, voyeurism, and transvestic fetishism, plus a residual category, paraphilia—not otherwise specified). Criterion B differs for exhibitionism, frotteurism, and pedophilia to include acting on these urges, and for sadism, acting on these urges with a nonconsenting person. Sexual arousal in association with objects that were designed for sexual purposes is not diagnosable.

Some paraphilias may interfere with the capacity for sexual activity with consenting adult partners.

In the current version of the Diagnostic and Statistical Manual of Mental Disorders (DSM-IV-TR), a paraphilia is not diagnosable as a psychiatric disorder unless it causes distress to the individual or harm to others.

The DSM-5 adds a distinction between "paraphilias" and "paraphilic disorders", stating that paraphilias do not require or justify psychiatric treatment in themselves, and defining "paraphilic disorder" as "a paraphilia that is currently causing distress or impairment to the individual or a paraphilia whose satisfaction has entailed personal harm, or risk of harm, to others".

The DSM-5 Paraphilias Subworkgroup reached a "consensus that paraphilias are not "ipso facto" psychiatric disorders", and proposed "that the DSM-V make a distinction between "paraphilias" and paraphilic "disorders". [...] One would "ascertain" a paraphilia (according to the nature of the urges, fantasies, or behaviors) but "diagnose" a paraphilic disorder (on the basis of distress and impairment). In this conception, having a paraphilia would be a necessary but not a sufficient condition for having a paraphilic disorder." The 'Rationale' page of any paraphilia in the electronic DSM-5 draft continues: "This approach leaves intact the distinction between normative and non-normative sexual behavior, which could be important to researchers, but without automatically labeling non-normative sexual behavior as psychopathological. It also eliminates certain logical absurdities in the DSM-IV-TR. In that version, for example, a man cannot be classified as a transvestite—however much he cross-dresses and however sexually exciting that is to him—unless he is unhappy about this activity or impaired by it. This change in viewpoint would be reflected in the diagnostic criteria sets by the addition of the word 'Disorder' to all the paraphilias. Thus, Sexual Sadism would become Sexual Sadism Disorder; Sexual Masochism would become Sexual Masochism Disorder, and so on."

Bioethics professor Alice Dreger interpreted these changes as "a subtle way of saying sexual kinks are basically okay – so okay, the sub-work group doesn't actually bother to define paraphilia. But a paraphilic disorder is defined: that's when an atypical sexual interest causes distress or impairment to the individual or harm to others." Interviewed by Dreger, Ray Blanchard, the Chair of the Paraphilias Sub-Work Group, stated, "We tried to go as far as we could in depathologizing mild and harmless paraphilias, while recognizing that severe paraphilias that distress or impair people or cause them to do harm to others are validly regarded as disorders."

Charles Allen Moser stated that this change is not really substantive, as the DSM-IV already acknowledged a difference between paraphilias and non-pathological but unusual sexual interests, a distinction that is virtually identical to what was being proposed for DSM-5, and it is a distinction that, in practice, has often been ignored. Linguist Andrew Clinton Hinderliter argued that "including some sexual interests—but not others—in the DSM creates a fundamental asymmetry and communicates a negative value judgment against the sexual interests included," and leaves the paraphilias in a situation similar to ego-dystonic homosexuality, which was removed from the DSM because it was realized not to be a mental disorder.

The DSM-5 acknowledges that many dozens of paraphilias exist, but only has specific listings for eight that are forensically important and relatively common. These are voyeuristic disorder, exhibitionistic disorder, frotteuristic disorder, sexual masochism disorder, sexual sadism disorder, pedophilic disorder, fetishistic disorder, and transvestic disorder. Other paraphilias can be diagnosed under the Other Specified Paraphilic Disorder or Unspecified Paraphilic Disorder listings, if accompanied by distress or impairment.

Most clinicians and researchers believe that paraphilic sexual interests cannot be altered, although evidence is needed to support this. Instead, the goal of therapy is normally to reduce the person's discomfort with their paraphilia and limit any criminal behavior. Both psychotherapeutic and pharmacological methods are available to these ends.

Cognitive behavioral therapy, at times, can help people with paraphilias develop strategies to avoid acting on their interests. Patients are taught to identify and cope with factors that make acting on their interests more likely, such as stress. It is currently the only form of psychotherapy for paraphilias supported by randomized double-blind trials, as opposed to case studies and consensus of expert opinion.

Pharmacological treatments can help people control their sexual behaviors, but do not change the content of the paraphilia. They are typically combined with cognitive behavioral therapy for best effect.

Selective serotonin reuptake inhibitors (SSRIs) are used, especially with exhibitionists, non-offending pedophiles, and compulsive masturbators. They are proposed to work by reducing sexual arousal, compulsivity, and depressive symptoms. They have been well received and are considered an important pharmacological treatment of paraphilia.

Antiandrogens are used in more severe cases. Similar to physical castration, they work by reducing androgen levels, and have thus been described as chemical castration. The antiandrogen cyproterone acetate has been shown to substantially reduce sexual fantasies and offending behaviors. Medroxyprogesterone acetate and gonadotropin-releasing hormone agonists (such as leuprorelin) have also been used to lower sex drive. Due to the side effects, the World Federation of Societies of Biological Psychiatry recommends that hormonal treatments only be used when there is a serious risk of sexual violence, or when other methods have failed. Surgical castration has largely been abandoned because these pharmacological alternatives are similarly effective and less invasive.

Research has shown that paraphilias are rarely observed in women. However, there have been some studies on females with paraphilias. Sexual masochism has been found to be the most commonly observed paraphilia in women, with approximately 1 in 20 cases of sexual masochism being female.

Many acknowledge the scarcity of research on female paraphilias. The majority of paraphilia studies are conducted on people who have been convicted of sex crimes. Since the number of male convicted sex offenders far exceeds the number of female convicted sex offenders, research on paraphilic behavior in women is consequently lacking. Some researchers argue that an underrepresentation exists concerning pedophilia in females. Due to the low number of women in studies on pedophilia, most studies are based from "exclusively male samples". This likely underrepresentation may also be attributable to a "societal tendency to dismiss the negative impact of sexual relationships between young boys and adult women". Michele Elliott has done extensive research on child sexual abuse committed by females, publishing the book "Female Sexual Abuse of Children: The Last Taboo" in an attempt to challenge the gender-biased discourse surrounding sex crimes. John Hunsley states that physiological limitations in the study of female sexuality must also be acknowledged when considering research on paraphilias. He states that while a man's sexual arousal can be directly measured from his erection (see penile plethysmograph), a woman's sexual arousal cannot be measured as clearly (see vaginal photoplethysmograph), and therefore research concerning female sexuality is rarely as conclusive as research on men.

In the United States, since 1990 a significant number of states have passed sexually violent predator laws. Following a series of landmark cases in the Supreme Court of the United States, persons diagnosed with paraphilias, particularly pedophilia ("Kansas v. Hendricks", 1997) and exhibitionism ("Kansas v. Crane", 2002), with a history of anti-social behavior and related criminal history, can be held indefinitely in civil confinement under various state legislation generically known as sexually violent predator laws and the federal Adam Walsh Act ("United States v. Comstock", 2010).






</doc>
<doc id="23593" url="https://en.wikipedia.org/wiki?curid=23593" title="Pediatrics">
Pediatrics

Pediatrics (also spelled paediatrics or pædiatrics) is the branch of medicine that involves the medical care of infants, children, and adolescents. The American Academy of Pediatrics recommends people be under pediatric care through the age of 21 (though usually only minors are required to be under pediatric care). In the United Kingdom, pediatrics covers patients until age 18. A medical doctor who specializes in this area is known as a pediatrician, or paediatrician. The word "pediatrics" and its cognates mean "healer of children"; they derive from two Greek words: ("pais" "child") and ("iatros" "doctor, healer"). Pediatricians work in hospitals, particularly those working in its subspecialties (e.g. neonatology), and as outpatient primary care physicians.

Already Hippocrates, Aristotle, Celsus, Soranus, and Galen understood the differences in growing and maturing organisms that necessitated different treatment: "" ("In general, boys should not be treated in the same way as men").

Some of the oldest traces of pediatrics can be discovered in Ancient India where children's doctors were called "kumara bhrtya". "Sushruta Samhita" an ayurvedic text, composed during the sixth century BC contains the text about pediatrics. Another ayurvedic text from this period is "Kashyapa Samhita".

A second century AD manuscript by the Greek physician and gynecologist Soranus of Ephesus dealt with neonatal pediatrics. Byzantine physicians Oribasius, Aëtius of Amida, Alexander Trallianus, and Paulus Aegineta contributed to the field. The Byzantines also built "brephotrophia" (crêches). Islamic writers served as a bridge for Greco-Roman and Byzantine medicine and added ideas of their own, especially Haly Abbas, Serapion, Avicenna, and Averroes. The Persian philosopher and physician al-Razi (865–925) published a monograph on pediatrics titled "Diseases in Children" as well as the first definite description of smallpox as a clinical entity. Also among the first books about pediatrics was "Libellus [Opusculum] de aegritudinibus et remediis infantium" 1472 ("Little Book on Children Diseases and Treatment"), by the Italian pediatrician Paolo Bagellardo. In sequence came Bartholomäus Metlinger's "Ein Regiment der Jungerkinder" 1473, Cornelius Roelans (1450–1525) no title Buchlein, or Latin compendium, 1483, and Heinrich von Louffenburg (1391–1460) "Versehung des Leibs" written in 1429 (published 1491), together form the "Pediatric Incunabula", four great medical treatises on children's physiology and pathology.

The Swedish physician Nils Rosén von Rosenstein (1706–1773) is considered to be the founder of modern pediatrics as a medical specialty, while his work "The diseases of children, and their remedies" (1764) is considered to be "the first modern textbook on the subject". Pediatrics as a specialized field of medicine continued to develop in the mid-19th century; German physician Abraham Jacobi (1830–1919) is known as the "father of American pediatrics" because of his many contributions to the field. He received his medical training in Germany and later practiced in New York City.

The first generally accepted pediatric hospital is the "Hôpital des Enfants Malades" (), which opened in Paris in June 1802 on the site of a previous orphanage. From its beginning, this famous hospital accepted patients up to the age of fifteen years, and it continues to this day as the pediatric division of the Necker-Enfants Malades Hospital, created in 1920 by merging with the physically contiguous "Necker Hospital", founded in 1778.

In other European countries, the Charité (a hospital founded in 1710) in Berlin established a separate Pediatric Pavilion in 1830, followed by similar institutions at Saint Petersburg in 1834, and at Vienna and Breslau (now Wrocław), both in 1837. In 1852 Britain's first pediatric hospital, the Hospital for Sick Children, Great Ormond Street was founded by Charles West. The first Children's hospital in Scotland opened in 1860 in Edinburgh. In the US, the first similar institutions were the Children's Hospital of Philadelphia, which opened in 1855, and then Boston Children's Hospital (1869). Subspecialties in pediatrics were created at the Harriet Lane Home at Johns Hopkins by Edwards A. Park.

The body size differences are paralleled by maturation changes. The smaller body of an infant or neonate is substantially different physiologically from that of an adult. Congenital defects, genetic variance, and developmental issues are of greater concern to pediatricians than they often are to adult physicians. A common adage is that children are not simply "little adults". The clinician must take into account the immature physiology of the infant or child when considering symptoms, prescribing medications, and diagnosing illnesses.

Pediatric physiology directly impacts the pharmacokinetic properties of drugs that enter the body. The absorption, distribution, metabolism, and elimination of medications differ between developing children and grown adults. Despite completed studies and reviews, continual research is needed to better understand how these factors should affect the decisions of healthcare providers when prescribing and administering medications to the pediatric population.

Many drug absorption differences between pediatric and adult populations revolve around the stomach. Neonates and young infants have increased stomach pH due to decreased acid secretion, thereby creating a more basic environment for drugs that are taken by mouth. Acid is essential to degrading certain oral drugs before systemic absorption. Therefore, the absorption of these drugs in children is greater than in adults due to decreased breakdown and increased preservation in a less acidic gastric space.

Children also have an extended rate of gastric emptying, which slows the rate of drug absorption.

Drug absorption also depends on specific enzymes that come in contact with the oral drug as it travels through the body. Supply of these enzymes increase as children continue to develop their gastrointestinal tract. Pediatric patients have underdeveloped proteins, which leads to decreased metabolism and increased serum concentrations of specific drugs. However, prodrugs experience the opposite effect because enzymes are necessary in allowing their active form to enter systemic circulation.

Percentage of total body water and extracellular fluid volume both decrease as children grow and develop with time. Pediatric patients thus have a larger volume of distribution than adults, which directly affects the dosing of hydrophilic drugs such as beta-lactam antibiotics like ampicillin. Thus, these drugs are administered at greater weight-based doses or with adjusted dosing intervals in children to account for this key difference in body composition.

Infants and neonates also have less plasma proteins. Thus, highly protein-bound drugs have fewer opportunities for protein binding, leading to increased distribution.

Drug metabolism primarily occurs via enzymes in the liver and can vary according to which specific enzymes are affected in a specific stage of development. Phase I and Phase II enzymes have different rates of maturation and development, depending on their specific mechanism of action (i.e. oxidation, hydrolysis, acetylation, methylation, etc.). Enzyme capacity, clearance, and half-life are all factors that contribute to metabolism differences between children and adults. Drug metabolism can even differ within the pediatric population, separating neonates and infants from young children.

Drug elimination is primarily facilitated via the liver and kidneys. In infants and young children, the larger relative size of their kidneys leads to increased renal clearance of medications that are eliminated through urine. In preterm neonates and infants, their kidneys are slower to mature and thus are unable to clear as much drug as fully developed kidneys. This can cause unwanted drug build-up, which is why it is important to consider lower doses and greater dosing intervals for this population. Diseases that negatively affect kidney function can also have the same effect and thus warrant similar considerations.

A major difference between the practice of pediatric and adult medicine is that children, in most jurisdictions and with certain exceptions, cannot make decisions for themselves. The issues of guardianship, privacy, legal responsibility and informed consent must always be considered in every pediatric procedure. Pediatricians often have to treat the parents and sometimes, the family, rather than just the child. Adolescents are in their own legal class, having rights to their own health care decisions in certain circumstances. The concept of legal consent combined with the non-legal consent (assent) of the child when considering treatment options, especially in the face of conditions with poor prognosis or complicated and painful procedures/surgeries, means the pediatrician must take into account the desires of many people, in addition to those of the patient.

Aspiring medical students will need 4 years of undergraduate courses at a college or university, which will get them a BS, BA or other bachelor's degree. After completing college future pediatricians will need to attend 4 years of medical school (MD/MBBS) and later do 3 more years of residency training, the first year of which is called "internship." After completing the 3 years of residency, physicians are eligible to become certified in pediatrics by passing a rigorous test that deals with medical conditions related to young children.

In high school, future pediatricians are required to take basic science classes such as biology, chemistry, physics, algebra, geometry, and calculus. It is also advisable to learn a foreign language (preferably Spanish in the United States) and be involved in high school organizations and extracurricular activities. After high school, college students simply need to fulfill the basic science course requirements that most medical schools recommend and will need to prepare to take the MCAT (Medical College Admission Test) in their junior or early senior year in college. Once attending medical school, student courses will focus on basic medical sciences like human anatomy, physiology, chemistry, etc., for the first three years, the second year of which is when medical students start to get hands-on experience with actual patients.

The training of pediatricians varies considerably across the world. Depending on jurisdiction and university, a medical degree course may be either undergraduate-entry or graduate-entry. The former commonly takes five or six years, and has been usual in the Commonwealth. Entrants to graduate-entry courses (as in the US), usually lasting four or five years, have previously completed a three- or four-year university degree, commonly but by no means always in sciences. Medical graduates hold a degree specific to the country and university in and from which they graduated. This degree qualifies that medical practitioner to become licensed or registered under the laws of that particular country, and sometimes of several countries, subject to requirements for "internship" or "conditional registration".

Pediatricians must undertake further training in their chosen field. This may take from four to eleven or more years depending on jurisdiction and the degree of specialization.

In the United States, a medical school graduate wishing to specialize in pediatrics must undergo a three-year residency composed of outpatient, inpatient, and critical care rotations. Subspecialties within pediatrics require further training in the form of 3-year fellowships. Subspecialties include critical care, gastroenterology, neurology, infectious disease, hematology/oncology, rheumatology, pulmonology, child abuse, emergency medicine, endocrinology, neonatology, and others.

In most jurisdictions, entry-level degrees are common to all branches of the medical profession, but in some jurisdictions, specialization in pediatrics may begin before completion of this degree. In some jurisdictions, pediatric training is begun immediately following completion of entry-level training. In other jurisdictions, junior medical doctors must undertake generalist (unstreamed) training for a number of years before commencing pediatric (or any other) specialization. Specialist training is often largely under the control of pediatric organizations (see below) rather than universities, and depend on jurisdiction.

Subspecialties of pediatrics include:







</doc>
<doc id="23597" url="https://en.wikipedia.org/wiki?curid=23597" title="Physiology">
Physiology

Physiology (; ) is the scientific study of functions and mechanisms in a living system. As a sub-discipline of biology, physiology focuses on how organisms, organ systems, individual organs, cells, and biomolecules carry out the chemical and physical functions in a living system. According to the classes of organisms, the field can be divided into medical physiology, animal physiology, plant physiology, cell physiology, and comparative physiology.

Central to physiological functioning are biophysical and biochemical processes, homeostatic control mechanisms, and communication between cells. "Physiological state" is the condition of normal function, while "pathological state" refers to abnormal conditions, including human diseases.

The Nobel Prize in Physiology or Medicine is awarded by the Royal Swedish Academy of Sciences for exceptional scientific achievements in physiology related to the field of medicine. 

Human physiology seeks to understand the mechanisms that work to keep the human body alive and functioning, through scientific enquiry into the nature of mechanical, physical, and biochemical functions of humans, their organs, and the cells of which they are composed. The principal level of focus of physiology is at the level of organs and systems within systems. The endocrine and nervous systems play major roles in the reception and transmission of signals that integrate function in animals. Homeostasis is a major aspect with regard to such interactions within plants as well as animals. The biological basis of the study of physiology, integration refers to the overlap of many functions of the systems of the human body, as well as its accompanied form. It is achieved through communication that occurs in a variety of ways, both electrical and chemical.

Changes in physiology can impact the mental functions of individuals. Examples of this would be the effects of certain medications or toxic levels of substances. Change in behavior as a result of these substances is often used to assess the health of individuals.

Much of the foundation of knowledge in human physiology was provided by animal experimentation. Due to the frequent connection between form and function, physiology and anatomy are intrinsically linked and are studied in tandem as part of a medical curriculum.

Plant physiology is a subdiscipline of botany concerned with the functioning of plants. Closely related fields include plant morphology, plant ecology, phytochemistry, cell biology, genetics, biophysics, and molecular biology. Fundamental processes of plant physiology include photosynthesis, respiration, plant nutrition, tropisms, nastic movements, photoperiodism, photomorphogenesis, circadian rhythms, seed germination, dormancy, and stomata function and transpiration. Absorption of water by roots, production of food in the leaves, and growth of shoots towards light are examples of plant physiology.

Although there are differences between animal, plant, and microbial cells, the basic physiological functions of cells can be divided into the processes of cell division, cell signaling, cell growth, and cell metabolism.

Involving evolutionary physiology and environmental physiology, comparative physiology considers the diversity of functional characteristics across organisms.

The study of human physiology as a medical field originates in classical Greece, at the time of Hippocrates (late 5th century BC). 
Outside of Western tradition, early forms of physiology or anatomy can be reconstructed as having been present at around the same time in China, India and elsewhere.
Hippocrates incorporated his belief system called the theory of humours, which consisted of four basic substance: earth, water, air and fire. Each substance is known for having a corresponding humour: black bile, phlegm, blood and yellow bile, respectively. Hippocrates also noted some emotional connections to the four humours, which Claudius Galenus would later expand on. The critical thinking of Aristotle and his emphasis on the relationship between structure and function marked the beginning of physiology in Ancient Greece. Like Hippocrates, Aristotle took to the humoral theory of disease, which also consisted of four primary qualities in life: hot, cold, wet and dry. Claudius Galenus (c. 130–200 AD), known as Galen of Pergamum, was the first to use experiments to probe the functions of the body. Unlike Hippocrates, Galen argued that humoral imbalances can be located in specific organs, including the entire body. His modification of this theory better equipped doctors to make more precise diagnoses. Galen also played off of Hippocrates idea that emotions were also tied to the humours, and added the notion of temperaments: sanguine corresponds with blood; phlegmatic is tied to phlegm; yellow bile is connected to choleric; and black bile corresponds with melancholy. Galen also saw the human body consisting of three connected systems: the brain and nerves, which are responsible for thoughts and sensations; the heart and arteries, which give life; and the liver and veins, which can be attributed to nutrition and growth. Galen was also the founder of experimental physiology. And for the next 1,400 years, Galenic physiology was a powerful and influential tool in medicine.

Jean Fernel (1497–1558), a French physician, introduced the term "physiology". Galen, Ibn al-Nafis, Michael Servetus, Realdo Colombo, Amato Lusitano and William Harvey, are credited as making important discoveries in the circulation of the blood. Santorio Santorio in 1610s was the first to use a device to measure the pulse rate (the "pulsilogium"), and a thermoscope to measure temperature.

In 1791 Luigi Galvani described the role of electricity in nerves of dissected frogs. In 1811, César Julien Jean Legallois studied respiration in animal dissection and lesions and found the center of respiration in the medulla oblongata. In the same year, Charles Bell finished work on what would later become known as the Bell-Magendie law, which compared functional differences between dorsal and ventral roots of the spinal cord. In 1824, François Magendie described the sensory roots and produced the first evidence of the cerebellum's role in equilibration to complete the Bell-Magendie law.

In the 1820s, the French physiologist Henri Milne-Edwards introduced the notion of physiological division of labor, which allowed to "compare and study living things as if they were machines created by the industry of man." Inspired in the work of Adam Smith, Milne-Edwards wrote that the "body of all living beings, whether animal or plant, resembles a factory ... where the organs, comparable to workers, work incessantly to produce the phenomena that constitute the life of the individual." In more differentiated organisms, the functional labor could be apportioned between different instruments or systems (called by him as "appareils").

In 1858, Joseph Lister studied the cause of blood coagulation and inflammation that resulted after previous injuries and surgical wounds. He later discovered and implemented antiseptics in the operating room, and as a result, decreased death rate from surgery by a substantial amount.

The Physiological Society was founded in London in 1876 as a dining club. The American Physiological Society (APS) is a nonprofit organization that was founded in 1887. The Society is, "devoted to fostering education, scientific research, and dissemination of information in the physiological sciences."

In 1891, Ivan Pavlov performed research on "conditional responses" that involved dogs' saliva production in response to a bell and visual stimuli.

In the 19th century, physiological knowledge began to accumulate at a rapid rate, in particular with the 1838 appearance of the Cell theory of Matthias Schleiden and Theodor Schwann. It radically stated that organisms are made up of units called cells. Claude Bernard's (1813–1878) further discoveries ultimately led to his concept of "milieu interieur" (internal environment), which would later be taken up and championed as "homeostasis" by American physiologist Walter B. Cannon in 1929. By homeostasis, Cannon meant "the maintenance of steady states in the body and the physiological processes through which they are regulated." In other words, the body's ability to regulate its internal environment. William Beaumont was the first American to utilize the practical application of physiology.

Nineteenth-century physiologists such as Michael Foster, Max Verworn, and Alfred Binet, based on Haeckel's ideas, elaborated what came to be called "general physiology", a unified science of life based on the cell actions, later renamed in the 20th century as cell biology.

In the 20th century, biologists became interested in how organisms other than human beings function, eventually spawning the fields of comparative physiology and ecophysiology. Major figures in these fields include Knut Schmidt-Nielsen and George Bartholomew. Most recently, evolutionary physiology has become a distinct subdiscipline.

In 1920, August Krogh won the Nobel Prize for discovering how, in capillaries, blood flow is regulated.

In 1954, Andrew Huxley and Hugh Huxley, alongside their research team, discovered the sliding filaments in skeletal muscle, known today as the sliding filament theory.

Recently, there have been intense debates about the vitality of physiology as a discipline (Is it dead or alive?). If physiology is perhaps less visible nowadays than during the golden age of the 19th century, it is in large part because the field has given birth to some of the most active domains of today's biological sciences, such as neuroscience, endocrinology, and immunology. Furthermore, physiology is still often seen as an integrative discipline, which can put together into a coherent framework data coming from various different domains.

Initially, women were largely excluded from official involvement in any physiological society. The American Physiological Society, for example, was founded in 1887 and included only men in its ranks. In 1902, the American Physiological Society elected Ida Hyde as the first female member of the society. Hyde, a representative of the American Association of University Women and a global advocate for gender equality in education, attempted to promote gender equality in every aspect of science and medicine.

Soon thereafter, in 1913, J.S. Haldane proposed that women be allowed to formally join The Physiological Society, which had been founded in 1876. On 3 July 1915, six women were officially admitted: Florence Buchanan, Winifred Cullis, Ruth C. Skelton, Sarah C. M. Sowton, Constance Leetham Terry, and Enid M. Tribe. The centenary of the election of women was celebrated in 2015 with the publication of the book "Women Physiologists: Centenary Celebrations And Beyond For The Physiological Society." ()

Prominent women physiologists include:

There are many ways to categorize the subdisciplines of physiology:

Transnational physiological societies include:

National physiological societies include:

Human physiology

Animal physiology

Plant physiology

Fungal physiology

Protistan physiology

Algal physiology

Bacterial physiology



</doc>
<doc id="23601" url="https://en.wikipedia.org/wiki?curid=23601" title="Pi">
Pi

The number () is a mathematical constant. It is defined as the ratio of a circle's circumference to its diameter, and it also has various equivalent definitions. It appears in many formulas in all areas of mathematics and physics. It is approximately equal to 3.14159. It has been represented by the Greek letter "" since the mid-18th century, and is spelled out as "pi". It is also referred to as Archimedes' constant.

Being an irrational number, cannot be expressed as a common fraction, although fractions such as 22/7 are commonly used to approximate it. Equivalently, its decimal representation never ends and never settles into a permanently repeating pattern. Its decimal (or other base) digits appear to be randomly distributed, and are conjectured to satisfy a specific kind of statistical randomness. 

It is known that is a transcendental number: it is not the root of any polynomial with rational coefficients. The transcendence of implies that it is impossible to solve the ancient challenge of squaring the circle with a compass and straightedge.

Ancient civilizations, including the Egyptians and Babylonians, required fairly accurate approximations of for practical computations. Around 250 BC, the Greek mathematician Archimedes created an algorithm to approximate with arbitrary accuracy. In the 5th century AD, Chinese mathematics approximated to seven digits, while Indian mathematics made a five-digit approximation, both using geometrical techniques. The first exact formula for , based on infinite series, was discovered a millennium later, when in the 14th century the Madhava–Leibniz series was discovered in Indian mathematics. 

The invention of calculus soon led to the calculation of hundreds of digits of , enough for all practical scientific computations. Nevertheless, in the 20th and 21st centuries, mathematicians and computer scientists have pursued new approaches that, when combined with increasing computational power, extended the decimal representation of to many trillions of digits. The primary motivation for these computations is as a test case to develop efficient algorithms to calculate numeric series, as well as the quest to break records. The extensive calculations involved have also been used to test supercomputers and high-precision multiplication algorithms.

Because its most elementary definition relates to the circle, is found in many formulae in trigonometry and geometry, especially those concerning circles, ellipses, and spheres. In more modern mathematical analysis, the number is instead defined using the spectral properties of the real number system, as an eigenvalue or a period, without any reference to geometry. It appears therefore in areas of mathematics and sciences having little to do with geometry of circles, such as number theory and statistics, as well as in almost all areas of physics. The ubiquity of makes it one of the most widely known mathematical constants—both inside and outside the scientific community. Several books devoted to have been published, and record-setting calculations of the digits of often result in news headlines. Adepts have succeeded in memorizing the value of to over 70,000 digits.

The symbol used by mathematicians to represent the ratio of a circle's circumference to its diameter is the lowercase Greek letter, sometimes spelled out as "pi," and derived from the first letter of the Greek word "perimetros," meaning circumference. In English, is pronounced as "pie" ( ). In mathematical use, the lowercase letter is distinguished from its capitalized and enlarged counterpart , which denotes a product of a sequence, analogous to how denotes summation.

The choice of the symbol is discussed in the section "Adoption of the symbol ".

 is commonly defined as the ratio of a circle's circumference to its diameter :
The ratio is constant, regardless of the circle's size. For example, if a circle has twice the diameter of another circle, it will also have twice the circumference, preserving the ratio . This definition of implicitly makes use of flat (Euclidean) geometry; although the notion of a circle can be extended to any curve (non-Euclidean) geometry, these new circles will no longer satisfy the formula .

Here, the circumference of a circle is the arc length around the perimeter of the circle, a quantity which can be formally defined independently of geometry using limits—a concept in calculus. For example, one may directly compute the arc length of the top half of the unit circle, given in Cartesian coordinates by the equation , as the integral:
An integral such as this was adopted as the definition of by Karl Weierstrass, who defined it directly as an integral in 1841.

Definitions of such as these that rely on concepts of the integral calculus are no longer common in the literature. explains that this is because in many modern treatments of calculus, differential calculus typically precedes integral calculus in the university curriculum, so it is desirable to have a definition of that does not rely on the latter. One such definition, due to Richard Baltzer and popularized by Edmund Landau, is the following: is twice the smallest positive number at which the cosine function equals 0. The cosine can be defined independently of geometry as a power series, or as the solution of a differential equation.

In a similar spirit, can be defined using properties of the complex exponential, , of a complex variable . Like the cosine, the complex exponential can be defined in one of several ways. The set of complex numbers at which is equal to one is then an (imaginary) arithmetic progression of the form:
and there is a unique positive real number with this property.

A more abstract variation on the same idea, making use of sophisticated mathematical concepts of topology and algebra, is the following theorem: there is a unique (up to automorphism) continuous isomorphism from the group R/Z of real numbers under addition modulo integers (the circle group), onto the multiplicative group of complex numbers of absolute value one. The number is then defined as half the magnitude of the derivative of this homomorphism.

A circle encloses the largest area that can be attained within a given perimeter. Thus the number is also characterized as the best constant in the isoperimetric inequality (times one-fourth). There are many other closely related ways in which appears as an eigenvalue of some geometrical or physical process; see below.

 is an irrational number, meaning that it cannot be written as the ratio of two integers. Fractions such as and are commonly used to approximate , but no common fraction (ratio of whole numbers) can be its exact value. Because is irrational, it has an infinite number of digits in its decimal representation, and does not settle into an infinitely repeating pattern of digits. There are several proofs that is irrational; they generally require calculus and rely on the "reductio ad absurdum" technique. The degree to which can be approximated by rational numbers (called the irrationality measure) is not precisely known; estimates have established that the irrationality measure is larger than the measure of or but smaller than the measure of Liouville numbers.

The digits of have no apparent pattern and have passed tests for statistical randomness, including tests for normality; a number of infinite length is called normal when all possible sequences of digits (of any given length) appear equally often. The conjecture that is normal has not been proven or disproven.

Since the advent of computers, a large number of digits of have been available on which to perform statistical analysis. Yasumasa Kanada has performed detailed statistical analyses on the decimal digits of , and found them consistent with normality; for example, the frequencies of the ten digits 0 to 9 were subjected to statistical significance tests, and no evidence of a pattern was found. Any random sequence of digits contains arbitrarily long subsequences that appear non-random, by the infinite monkey theorem. Thus, because the sequence of 's digits passes statistical tests for randomness, it contains some sequences of digits that may appear non-random, such as a sequence of six consecutive 9s that begins at the 762nd decimal place of the decimal representation of . This is also called the "Feynman point" in mathematical folklore, after Richard Feynman, although no connection to Feynman is known.

In addition to being irrational, is also a transcendental number, which means that it is not the solution of any non-constant polynomial equation with rational coefficients, such as .

The transcendence of has two important consequences: First, cannot be expressed using any finite combination of rational numbers and square roots or "n"-th roots (such as or ). Second, since no transcendental number can be constructed with compass and straightedge, it is not possible to "square the circle". In other words, it is impossible to construct, using compass and straightedge alone, a square whose area is exactly equal to the area of a given circle. Squaring a circle was one of the important geometry problems of the classical antiquity. Amateur mathematicians in modern times have sometimes attempted to square the circle and claim success—despite the fact that it is mathematically impossible.

Like all irrational numbers, cannot be represented as a common fraction (also known as a simple or vulgar fraction), by the very definition of irrational number (i.e., not a rational number). But every irrational number, including , can be represented by an infinite series of nested fractions, called a continued fraction:

Truncating the continued fraction at any point yields a rational approximation for ; the first four of these are 3, 22/7, 333/106, and 355/113. These numbers are among the best-known and most widely used historical approximations of the constant. Each approximation generated in this way is a best rational approximation; that is, each is closer to than any other fraction with the same or a smaller denominator. Because is known to be transcendental, it is by definition not algebraic and so cannot be a quadratic irrational. Therefore, cannot have a periodic continued fraction. Although the simple continued fraction for (shown above) also does not exhibit any other obvious pattern, mathematicians have discovered several generalized continued fractions that do, such as:

Some approximations of "pi" include:

Digits in other number systems

Any complex number, say , can be expressed using a pair of real numbers. In the polar coordinate system, one number (radius or "r") is used to represent 's distance from the origin of the complex plane, and the other (angle or ) the counter-clockwise rotation from the positive real line:
where is the imaginary unit satisfying = −1. The frequent appearance of in complex analysis can be related to the behaviour of the exponential function of a complex variable, described by Euler's formula:

where the constant is the base of the natural logarithm. This formula establishes a correspondence between imaginary powers of and points on the unit circle centered at the origin of the complex plane. Setting = in Euler's formula results in Euler's identity, celebrated in mathematics due to it containing the five most important mathematical constants:

There are different complex numbers satisfying , and these are called the "-th roots of unity" and are given by the formula:

The best-known approximations to dating before the Common Era were accurate to two decimal places; this was improved upon in Chinese mathematics in particular by the mid-first millennium, to an accuracy of seven decimal places.
After this, no further progress was made until the late medieval period.

Based on the measurements of the Great Pyramid of Giza , some Egyptologists have claimed that the ancient Egyptians used an approximation of as from as early as the Old Kingdom. This claim has been met with skepticism.
The earliest written approximations of are found in Babylon and Egypt, both within one per cent of the true value. In Babylon, a clay tablet dated 1900–1600 BC has a geometrical statement that, by implication, treats as  = 3.125. In Egypt, the Rhind Papyrus, dated around 1650 BC but copied from a document dated to 1850 BC, has a formula for the area of a circle that treats as 3.16.

Astronomical calculations in the "Shatapatha Brahmana" (ca. 4th century BC) use a fractional approximation of  ≈ 3.139 (an accuracy of 9×10). Other Indian sources by about 150 BC treat as  ≈ 3.1622.

The first recorded algorithm for rigorously calculating the value of was a geometrical approach using polygons, devised around 250 BC by the Greek mathematician Archimedes. This polygonal algorithm dominated for over 1,000 years, and as a result is sometimes referred to as "Archimedes' constant". Archimedes computed upper and lower bounds of by drawing a regular hexagon inside and outside a circle, and successively doubling the number of sides until he reached a 96-sided regular polygon. By calculating the perimeters of these polygons, he proved that (that is ). Archimedes' upper bound of may have led to a widespread popular belief that is equal to . Around 150 AD, Greek-Roman scientist Ptolemy, in his "Almagest", gave a value for of 3.1416, which he may have obtained from Archimedes or from Apollonius of Perga. Mathematicians using polygonal algorithms reached 39 digits of in 1630, a record only broken in 1699 when infinite series were used to reach 71 digits.
In ancient China, values for included 3.1547 (around 1 AD), (100 AD, approximately 3.1623), and (3rd century, approximately 3.1556). Around 265 AD, the Wei Kingdom mathematician Liu Hui created a polygon-based iterative algorithm and used it with a 3,072-sided polygon to obtain a value of of 3.1416. Liu later invented a faster method of calculating and obtained a value of 3.14 with a 96-sided polygon, by taking advantage of the fact that the differences in area of successive polygons form a geometric series with a factor of 4. The Chinese mathematician Zu Chongzhi, around 480 AD, calculated that and suggested the approximations = 3.14159292035... and = 3.142857142857..., which he termed the "Milü" (<nowiki>"</nowiki>close ratio") and "Yuelü" ("approximate ratio"), respectively, using Liu Hui's algorithm applied to a 12,288-sided polygon. With a correct value for its seven first decimal digits, this value of remained the most accurate approximation of available for the next 800 years.

The Indian astronomer Aryabhata used a value of 3.1416 in his "Āryabhaṭīya" (499 AD). Fibonacci in c. 1220 computed 3.1418 using a polygonal method, independent of Archimedes. Italian author Dante apparently employed the value .

The Persian astronomer Jamshīd al-Kāshī produced 9 sexagesimal digits, roughly the equivalent of 16 decimal digits, in 1424 using a polygon with 3×2 sides, which stood as the world record for about 180 years. French mathematician François Viète in 1579 achieved 9 digits with a polygon of 3×2 sides. Flemish mathematician Adriaan van Roomen arrived at 15 decimal places in 1593. In 1596, Dutch mathematician Ludolph van Ceulen reached 20 digits, a record he later increased to 35 digits (as a result, was called the "Ludolphian number" in Germany until the early 20th century). Dutch scientist Willebrord Snellius reached 34 digits in 1621, and Austrian astronomer Christoph Grienberger arrived at 38 digits in 1630 using 10 sides, which remains the most accurate approximation manually achieved using polygonal algorithms.

The calculation of was revolutionized by the development of infinite series techniques in the 16th and 17th centuries. An infinite series is the sum of the terms of an infinite sequence. Infinite series allowed mathematicians to compute with much greater precision than Archimedes and others who used geometrical techniques. Although infinite series were exploited for most notably by European mathematicians such as James Gregory and Gottfried Wilhelm Leibniz, the approach was first discovered in India sometime between 1400 and 1500 AD. The first written description of an infinite series that could be used to compute was laid out in Sanskrit verse by Indian astronomer Nilakantha Somayaji in his "Tantrasamgraha", around 1500 AD. The series are presented without proof, but proofs are presented in a later Indian work, "Yuktibhāṣā", from around 1530 AD. Nilakantha attributes the series to an earlier Indian mathematician, Madhava of Sangamagrama, who lived c. 1350 – c. 1425. Several infinite series are described, including series for sine, tangent, and cosine, which are now referred to as the Madhava series or Gregory–Leibniz series. Madhava used infinite series to estimate to 11 digits around 1400, but that value was improved on around 1430 by the Persian mathematician Jamshīd al-Kāshī, using a polygonal algorithm.
The first infinite sequence discovered in Europe was an infinite product (rather than an infinite sum, which are more typically used in calculations) found by French mathematician François Viète in 1593:

The second infinite sequence found in Europe, by John Wallis in 1655, was also an infinite product:
The discovery of calculus, by English scientist Isaac Newton and German mathematician Gottfried Wilhelm Leibniz in the 1660s, led to the development of many infinite series for approximating . Newton himself used an arcsin series to compute a 15 digit approximation of in 1665 or 1666, later writing "I am ashamed to tell you to how many figures I carried these computations, having no other business at the time."

In Europe, Madhava's formula was rediscovered by Scottish mathematician James Gregory in 1671, and by Leibniz in 1674:

This formula, the Gregory–Leibniz series, equals when evaluated with  = 1. In 1699, English mathematician Abraham Sharp used the Gregory–Leibniz series for formula_13 to compute to 71 digits, breaking the previous record of 39 digits, which was set with a polygonal algorithm. The Gregory–Leibniz for formula_14 series is simple, but converges very slowly (that is, approaches the answer gradually), so it is not used in modern calculations.

In 1706 John Machin used the Gregory–Leibniz series to produce an algorithm that converged much faster:
Machin reached 100 digits of with this formula. Other mathematicians created variants, now known as Machin-like formulae, that were used to set several successive records for calculating digits of . Machin-like formulae remained the best-known method for calculating well into the age of computers, and were used to set records for 250 years, culminating in a 620-digit approximation in 1946 by Daniel Ferguson – the best approximation achieved without the aid of a calculating device.

A record was set by the calculating prodigy Zacharias Dase, who in 1844 employed a Machin-like formula to calculate 200 decimals of in his head at the behest of German mathematician Carl Friedrich Gauss. British mathematician William Shanks famously took 15 years to calculate to 707 digits, but made a mistake in the 528th digit, rendering all subsequent digits incorrect.

Some infinite series for converge faster than others. Given the choice of two infinite series for , mathematicians will generally use the one that converges more rapidly because faster convergence reduces the amount of computation needed to calculate to any given accuracy. A simple infinite series for is the Gregory–Leibniz series:
As individual terms of this infinite series are added to the sum, the total gradually gets closer to , and – with a sufficient number of terms – can get as close to as desired. It converges quite slowly, though – after 500,000 terms, it produces only five correct decimal digits of .

An infinite series for (published by Nilakantha in the 15th century) that converges more rapidly than the Gregory–Leibniz series is: Note that ("n" − 1)"n"("n" + 1) = "n" − "n".

The following table compares the convergence rates of these two series:

After five terms, the sum of the Gregory–Leibniz series is within 0.2 of the correct value of , whereas the sum of Nilakantha's series is within 0.002 of the correct value of . Nilakantha's series converges faster and is more useful for computing digits of . Series that converge even faster include Machin's series and Chudnovsky's series, the latter producing 14 correct decimal digits per term.

Not all mathematical advances relating to were aimed at increasing the accuracy of approximations. When Euler solved the Basel problem in 1735, finding the exact value of the sum of the reciprocal squares, he established a connection between and the prime numbers that later contributed to the development and study of the Riemann zeta function:

Swiss scientist Johann Heinrich Lambert in 1761 proved that is irrational, meaning it is not equal to the quotient of any two whole numbers. Lambert's proof exploited a continued-fraction representation of the tangent function. French mathematician Adrien-Marie Legendre proved in 1794 that is also irrational. In 1882, German mathematician Ferdinand von Lindemann proved that is transcendental, confirming a conjecture made by both Legendre and Euler. Hardy and Wright states that "the proofs were afterwards modified and simplified by Hilbert, Hurwitz, and other writers".

In the earliest usages, the Greek letter was an abbreviation of the Greek word for periphery (), and was combined in ratios with δ (for diameter) or ρ (for radius) to form circle constants. (Before then, mathematicians sometimes used letters such as "c" or "p" instead.) The first recorded use is Oughtred's "formula_19", to express the ratio of periphery and diameter in the 1647 and later editions of . Barrow likewise used "formula_20" to represent the constant 3.14..., while Gregory instead used "formula_21" to represent 6.28... .

The earliest known use of the Greek letter alone to represent the ratio of a circle's circumference to its diameter was by Welsh mathematician William Jones in his 1706 work "; or, a New Introduction to the Mathematics". The Greek letter first appears there in the phrase "1/2 Periphery ()" in the discussion of a circle with radius one. However, he writes that his equations for are from the "ready pen of the truly ingenious Mr. John Machin", leading to speculation that Machin may have employed the Greek letter before Jones. Jones' notation was not immediately adopted by other mathematicians, with the fraction notation still being used as late as 1767.

Euler started using the single-letter form beginning with his 1727 "Essay Explaining the Properties of Air", though he used , the ratio of radius to periphery, in this and some later writing. Euler first used in his 1736 work "Mechanica", and continued in his widely-read 1748 work (he wrote: "for the sake of brevity we will write this number as ; thus is equal to half the circumference of a circle of radius 1"). Because Euler corresponded heavily with other mathematicians in Europe, the use of the Greek letter spread rapidly, and the practice was universally adopted thereafter in the Western world, though the definition still varied between 3.14... and 6.28... as late as 1761.

The development of computers in the mid-20th century again revolutionized the hunt for digits of . Mathematicians John Wrench and Levi Smith reached 1,120 digits in 1949 using a desk calculator. Using an inverse tangent (arctan) infinite series, a team led by George Reitwiesner and John von Neumann that same year achieved 2,037 digits with a calculation that took 70 hours of computer time on the ENIAC computer. The record, always relying on an arctan series, was broken repeatedly (7,480 digits in 1957; 10,000 digits in 1958; 100,000 digits in 1961) until 1 million digits were reached in 1973.

Two additional developments around 1980 once again accelerated the ability to compute . First, the discovery of new iterative algorithms for computing , which were much faster than the infinite series; and second, the invention of fast multiplication algorithms that could multiply large numbers very rapidly. Such algorithms are particularly important in modern computations because most of the computer's time is devoted to multiplication. They include the Karatsuba algorithm, Toom–Cook multiplication, and Fourier transform-based methods.

The iterative algorithms were independently published in 1975–1976 by physicist Eugene Salamin and scientist Richard Brent. These avoid reliance on infinite series. An iterative algorithm repeats a specific calculation, each iteration using the outputs from prior steps as its inputs, and produces a result in each step that converges to the desired value. The approach was actually invented over 160 years earlier by Carl Friedrich Gauss, in what is now termed the arithmetic–geometric mean method (AGM method) or Gauss–Legendre algorithm. As modified by Salamin and Brent, it is also referred to as the Brent–Salamin algorithm.

The iterative algorithms were widely used after 1980 because they are faster than infinite series algorithms: whereas infinite series typically increase the number of correct digits additively in successive terms, iterative algorithms generally "multiply" the number of correct digits at each step. For example, the Brent-Salamin algorithm doubles the number of digits in each iteration. In 1984, brothers John and Peter Borwein produced an iterative algorithm that quadruples the number of digits in each step; and in 1987, one that increases the number of digits five times in each step. Iterative methods were used by Japanese mathematician Yasumasa Kanada to set several records for computing between 1995 and 2002. This rapid convergence comes at a price: the iterative algorithms require significantly more memory than infinite series.

For most numerical calculations involving , a handful of digits provide sufficient precision. According to Jörg Arndt and Christoph Haenel, thirty-nine digits are sufficient to perform most cosmological calculations, because that is the accuracy necessary to calculate the circumference of the observable universe with a precision of one atom. Accounting for additional digits needed to compensate for computational round-off errors, Arndt concludes that a few hundred digits would suffice for any scientific application. Despite this, people have worked strenuously to compute to thousands and millions of digits. This effort may be partly ascribed to the human compulsion to break records, and such achievements with often make headlines around the world. They also have practical benefits, such as testing supercomputers, testing numerical analysis algorithms (including high-precision multiplication algorithms); and within pure mathematics itself, providing data for evaluating the randomness of the digits of .

Modern calculators do not use iterative algorithms exclusively. New infinite series were discovered in the 1980s and 1990s that are as fast as iterative algorithms, yet are simpler and less memory intensive. The fast iterative algorithms were anticipated in 1914, when the Indian mathematician Srinivasa Ramanujan published dozens of innovative new formulae for , remarkable for their elegance, mathematical depth, and rapid convergence. One of his formulae, based on modular equations, is
This series converges much more rapidly than most arctan series, including Machin's formula. Bill Gosper was the first to use it for advances in the calculation of , setting a record of 17 million digits in 1985. Ramanujan's formulae anticipated the modern algorithms developed by the Borwein brothers and the Chudnovsky brothers. The Chudnovsky formula developed in 1987 is
It produces about 14 digits of per term, and has been used for several record-setting calculations, including the first to surpass 1 billion (10) digits in 1989 by the Chudnovsky brothers, 2.7 trillion (2.7×10) digits by Fabrice Bellard in 2009, 10 trillion (10) digits in 2011 by Alexander Yee and Shigeru Kondo, and over 22 trillion digits in 2016 by Peter Trueb. For similar formulas, see also the Ramanujan–Sato series.

In 2006, mathematician Simon Plouffe used the PSLQ integer relation algorithm to generate several new formulas for , conforming to the following template:
where is (Gelfond's constant), is an odd number, and are certain rational numbers that Plouffe computed.

Monte Carlo methods, which evaluate the results of multiple random trials, can be used to create approximations of . Buffon's needle is one such technique: If a needle of length is dropped times on a surface on which parallel lines are drawn units apart, and if of those times it comes to rest crossing a line ( > 0), then one may approximate based on the counts:

Another Monte Carlo method for computing is to draw a circle inscribed in a square, and randomly place dots in the square. The ratio of dots inside the circle to the total number of dots will approximately equal .
Another way to calculate using probability is to start with a random walk, generated by a sequence of (fair) coin tosses: independent random variables such that with equal probabilities. The associated random walk is
so that, for each , is drawn from a shifted and scaled binomial distribution. As varies, defines a (discrete) stochastic process. Then can be calculated by
This Monte Carlo method is independent of any relation to circles, and is a consequence of the central limit theorem, discussed below.

These Monte Carlo methods for approximating are very slow compared to other methods, and do not provide any information on the exact number of digits that are obtained. Thus they are never used to approximate when speed or accuracy is desired.

Two algorithms were discovered in 1995 that opened up new avenues of research into . They are called spigot algorithms because, like water dripping from a spigot, they produce single digits of that are not reused after they are calculated. This is in contrast to infinite series or iterative algorithms, which retain and use all intermediate digits until the final result is produced.

Mathematicians Stan Wagon and Stanley Rabinowitz produced a simple spigot algorithm in 1995. Its speed is comparable to arctan algorithms, but not as fast as iterative algorithms.

Another spigot algorithm, the BBP digit extraction algorithm, was discovered in 1995 by Simon Plouffe:
This formula, unlike others before it, can produce any individual hexadecimal digit of without calculating all the preceding digits. Individual binary digits may be extracted from individual hexadecimal digits, and octal digits can be extracted from one or two hexadecimal digits. Variations of the algorithm have been discovered, but no digit extraction algorithm has yet been found that rapidly produces decimal digits. An important application of digit extraction algorithms is to validate new claims of record computations: After a new record is claimed, the decimal result is converted to hexadecimal, and then a digit extraction algorithm is used to calculate several random hexadecimal digits near the end; if they match, this provides a measure of confidence that the entire computation is correct.

Between 1998 and 2000, the distributed computing project PiHex used Bellard's formula (a modification of the BBP algorithm) to compute the quadrillionth (10th) bit of , which turned out to be 0. In September 2010, a Yahoo! employee used the company's Hadoop application on one thousand computers over a 23-day period to compute 256 bits of at the two-quadrillionth (2×10th) bit, which also happens to be zero.

Because is closely related to the circle, it is found in many formulae from the fields of geometry and trigonometry, particularly those concerning circles, spheres, or ellipses. Other branches of science, such as statistics, physics, Fourier analysis, and number theory, also include in some of their important formulae.

 appears in formulae for areas and volumes of geometrical shapes based on circles, such as ellipses, spheres, cones, and tori. Below are some of the more common formulae that involve .

The formulae above are special cases of the volume of the "n"-dimensional ball and the surface area of its boundary, the ("n"−1)-dimensional sphere, given below.

Definite integrals that describe circumference, area, or volume of shapes generated by circles typically have values that involve . For example, an integral that specifies half the area of a circle of radius one is given by:

In that integral the function represents the top half of a circle (the square root is a consequence of the Pythagorean theorem), and the integral computes the area between that half of a circle and the axis.

The trigonometric functions rely on angles, and mathematicians generally use radians as units of measurement. plays an important role in angles measured in radians, which are defined so that a complete circle spans an angle of 2 radians. The angle measure of 180° is equal to radians, and 1° = /180 radians.

Common trigonometric functions have periods that are multiples of ; for example, sine and cosine have period 2, so for any angle and any integer ,

Many of the appearances of in the formulas of mathematics and the sciences have to do with its close relationship with geometry. However, also appears in many natural situations having apparently nothing to do with geometry.

In many applications, it plays a distinguished role as an eigenvalue. For example, an idealized vibrating string can be modelled as the graph of a function on the unit interval , with fixed ends . The modes of vibration of the string are solutions of the differential equation formula_31, or formula_32. Thus is an eigenvalue of the second derivative operator formula_33, and is constrained by Sturm–Liouville theory to take on only certain specific values. It must be positive, since the operator is negative definite, so it is convenient to write , where is called the wavenumber. Then satisfies the boundary conditions and the differential equation with .

The value is, in fact, the "least" such value of the wavenumber, and is associated with the fundamental mode of vibration of the string. One way to show this is by estimating the energy, which satisfies Wirtinger's inequality: for a function with and , both square integrable, we have:

with equality precisely when is a multiple of . Here appears as an optimal constant in Wirtinger's inequality, and it follows that it is the smallest wavenumber, using the variational characterization of the eigenvalue. As a consequence, is the smallest singular value of the derivative operator on the space of functions on vanishing at both endpoints (the Sobolev space formula_35).

The number serves appears in similar eigenvalue problems in higher-dimensional analysis. As mentioned above, it can be characterized via its role as the best constant in the isoperimetric inequality: the area enclosed by a plane Jordan curve of perimeter satisfies the inequality
and equality is clearly achieved for the circle, since in that case and .

Ultimately as a consequence of the isoperimetric inequality, appears in the optimal constant for the critical Sobolev inequality in "n" dimensions, which thus characterizes the role of in many physical phenomena as well, for example those of classical potential theory. In two dimensions, the critical Sobolev inequality is
for "f" a smooth function with compact support in , formula_38 is the gradient of "f", and formula_39 and formula_40 refer respectively to the and -norm. The Sobolev inequality is equivalent to the isoperimetric inequality (in any dimension), with the same best constants.

Wirtinger's inequality also generalizes to higher-dimensional Poincaré inequalities that provide best constants for the Dirichlet energy of an "n"-dimensional membrane. Specifically, is the greatest constant such that
for all convex subsets of of diameter 1, and square-integrable functions "u" on of mean zero. Just as Wirtinger's inequality is the variational form of the Dirichlet eigenvalue problem in one dimension, the Poincaré inequality is the variational form of the Neumann eigenvalue problem, in any dimension.

The constant also appears as a critical spectral parameter in the Fourier transform. This is the integral transform, that takes a complex-valued integrable function on the real line to the function defined as:

Although there are several different conventions for the Fourier transform and its inverse, any such convention must involve "somewhere". The above is the most canonical definition, however, giving the unique unitary operator on that is also an algebra homomorphism of to .

The Heisenberg uncertainty principle also contains the number . The uncertainty principle gives a sharp lower bound on the extent to which it is possible to localize a function both in space and in frequency: with our conventions for the Fourier transform,
The physical consequence, about the uncertainty in simultaneous position and momentum observations of a quantum mechanical system, is discussed below. The appearance of in the formulae of Fourier analysis is ultimately a consequence of the Stone–von Neumann theorem, asserting the uniqueness of the Schrödinger representation of the Heisenberg group.

The fields of probability and statistics frequently use the normal distribution as a simple model for complex phenomena; for example, scientists generally assume that the observational error in most experiments follows a normal distribution. The Gaussian function, which is the probability density function of the normal distribution with mean and standard deviation , naturally contains :

The factor of formula_45 makes the area under the graph of equal to one, as is required for a probability distribution. This follows from a change of variables in the Gaussian integral:

which says that the area under the basic bell curve in the figure is equal to the square root of .
The central limit theorem explains the central role of normal distributions, and thus of , in probability and statistics. This theorem is ultimately connected with the spectral characterization of as the eigenvalue associated with the Heisenberg uncertainty principle, and the fact that equality holds in the uncertainty principle only for the Gaussian function. Equivalently, is the unique constant making the Gaussian normal distribution equal to its own Fourier transform. Indeed, according to , the "whole business" of establishing the fundamental theorems of Fourier analysis reduces to the Gaussian integral.

Let be the set of all twice differentiable real functions formula_47 that satisfy the ordinary differential equation formula_48. Then is a two-dimensional real vector space, with two parameters corresponding to a pair of initial conditions for the differential equation. For any formula_49, let formula_50 be the evaluation functional, which associates to each formula_51 the value formula_52 of the function at the real point . Then, for each "t", the kernel of formula_53 is a one-dimensional linear subspace of . Hence formula_54 defines a function from formula_55 from the real line to the real projective line. This function is periodic, and the quantity can be characterized as the period of this map.

The constant appears in the Gauss–Bonnet formula which relates the differential geometry of surfaces to their topology. Specifically, if a compact surface has Gauss curvature "K", then
where is the Euler characteristic, which is an integer. An example is the surface area of a sphere "S" of curvature 1 (so that its radius of curvature, which coincides with its radius, is also 1.) The Euler characteristic of a sphere can be computed from its homology groups and is found to be equal to two. Thus we have
reproducing the formula for the surface area of a sphere of radius 1.

The constant appears in many other integral formulae in topology, in particular, those involving characteristic classes via the Chern–Weil homomorphism.

Vector calculus is a branch of calculus that is concerned with the properties of vector fields, and has many physical applications such as to electricity and magnetism. The Newtonian potential for a point source situated at the origin of a three-dimensional Cartesian coordinate system is
which represents the potential energy of a unit mass (or charge) placed a distance from the source, and is a dimensional constant. The field, denoted here by , which may be the (Newtonian) gravitational field or the (Coulomb) electric field, is the negative gradient of the potential:
Special cases include Coulomb's law and Newton's law of universal gravitation. Gauss' law states that the outward flux of the field through any smooth, simple, closed, orientable surface containing the origin is equal to :

It is standard to absorb this factor of into the constant , but this argument shows why it must appear "somewhere". Furthermore, is the surface area of the unit sphere, but we have not assumed that is the sphere. However, as a consequence of the divergence theorem, because the region away from the origin is vacuum (source-free) it is only the homology class of the surface in that matters in computing the integral, so it can be replaced by any convenient surface in the same homology class, in particular, a sphere, where spherical coordinates can be used to calculate the integral.

A consequence of the Gauss law is that the negative Laplacian of the potential is equal to times the Dirac delta function:
More general distributions of matter (or charge) are obtained from this by convolution, giving the Poisson equation
where is the distribution function.
The constant also plays an analogous role in four-dimensional potentials associated with Einstein's equations, a fundamental formula which forms the basis of the general theory of relativity and describes the fundamental interaction of gravitation as a result of spacetime being curved by matter and energy:
where is the Ricci curvature tensor, is the scalar curvature, is the metric tensor, is the cosmological constant, is Newton's gravitational constant, is the speed of light in vacuum, and is the stress–energy tensor. The left-hand side of Einstein's equation is a non-linear analogue of the Laplacian of the metric tensor, and reduces to that in the weak field limit, with the formula_63 term playing the role of a Lagrange multiplier, and the right-hand side is the analogue of the distribution function, times .

One of the key tools in complex analysis is contour integration of a function over a positively oriented (rectifiable) Jordan curve . A form of Cauchy's integral formula states that if a point is interior to , then

Although the curve is not a circle, and hence does not have any obvious connection to the constant , a standard proof of this result uses Morera's theorem, which implies that the integral is invariant under homotopy of the curve, so that it can be deformed to a circle and then integrated explicitly in polar coordinates. More generally, it is true that if a rectifiable closed curve does not contain , then the above integral is times the winding number of the curve.

The general form of Cauchy's integral formula establishes the relationship between the values of a complex analytic function on the Jordan curve and the value of at any interior point of :
provided is analytic in the region enclosed by and extends continuously to . Cauchy's integral formula is a special case of the residue theorem, that if is a meromorphic function the region enclosed by and is continuous in a neighbourhood of , then
where the sum is of the residues at the poles of .

The factorial function is the product of all of the positive integers through . The gamma function extends the concept of factorial (normally defined only for non-negative integers) to all complex numbers, except the negative real integers. When the gamma function is evaluated at half-integers, the result contains ; for example formula_67 and formula_68.

The gamma function is defined by its Weierstrass product development:
where is the Euler–Mascheroni constant. Evaluated at and squared, the equation reduces to the Wallis product formula. The gamma function is also connected to the Riemann zeta function and identities for the functional determinant, in which the constant plays an important role.

The gamma function is used to calculate the volume of the "n"-dimensional ball of radius "r" in Euclidean "n"-dimensional space, and the surface area of its boundary, the ("n"−1)-dimensional sphere:

Further, it follows from the functional equation that

The gamma function can be used to create a simple approximation to the factorial function for large : formula_73 which is known as Stirling's approximation. Equivalently,

As a geometrical application of Stirling's approximation, let denote the standard simplex in "n"-dimensional Euclidean space, and denote the simplex having all of its sides scaled up by a factor of . Then

Ehrhart's volume conjecture is that this is the (optimal) upper bound on the volume of a convex body containing only one lattice point.

The Riemann zeta function is used in many areas of mathematics. When evaluated at it can be written as
Finding a simple solution for this infinite series was a famous problem in mathematics called the Basel problem. Leonhard Euler solved it in 1735 when he showed it was equal to . Euler's result leads to the number theory result that the probability of two random numbers being relatively prime (that is, having no shared factors) is equal to . This probability is based on the observation that the probability that any number is divisible by a prime is (for example, every 7th integer is divisible by 7.) Hence the probability that two numbers are both divisible by this prime is , and the probability that at least one of them is not is . For distinct primes, these divisibility events are mutually independent; so the probability that two numbers are relatively prime is given by a product over all primes:

This probability can be used in conjunction with a random number generator to approximate using a Monte Carlo approach.

The solution to the Basel problem implies that the geometrically derived quantity is connected in a deep way to the distribution of prime numbers. This is a special case of Weil's conjecture on Tamagawa numbers, which asserts the equality of similar such infinite products of "arithmetic" quantities, localized at each prime "p", and a "geometrical" quantity: the reciprocal of the volume of a certain locally symmetric space. In the case of the Basel problem, it is the hyperbolic 3-manifold .

The zeta function also satisfies Riemann's functional equation, which involves as well as the gamma function:

Furthermore, the derivative of the zeta function satisfies
A consequence is that can be obtained from the functional determinant of the harmonic oscillator. This functional determinant can be computed via a product expansion, and is equivalent to the Wallis product formula. The calculation can be recast in quantum mechanics, specifically the variational approach to the spectrum of the hydrogen atom.

The constant also appears naturally in Fourier series of periodic functions. Periodic functions are functions on the group of fractional parts of real numbers. The Fourier decomposition shows that a complex-valued function on can be written as an infinite linear superposition of unitary characters of . That is, continuous group homomorphisms from to the circle group of unit modulus complex numbers. It is a theorem that every character of is one of the complex exponentials formula_80.

There is a unique character on , up to complex conjugation, that is a group isomorphism. Using the Haar measure on the circle group, the constant is half the magnitude of the Radon–Nikodym derivative of this character. The other characters have derivatives whose magnitudes are positive integral multiples of 2. As a result, the constant is the unique number such that the group T, equipped with its Haar measure, is Pontrjagin dual to the lattice of integral multiples of 2. This is a version of the one-dimensional Poisson summation formula.

The constant is connected in a deep way with the theory of modular forms and theta functions. For example, the Chudnovsky algorithm involves in an essential way the j-invariant of an elliptic curve.

Modular forms are holomorphic functions in the upper half plane characterized by their transformation properties under the modular group formula_81 (or its various subgroups), a lattice in the group formula_82. An example is the Jacobi theta function
which is a kind of modular form called a Jacobi form. This is sometimes written in terms of the nome formula_84.

The constant is the unique constant making the Jacobi theta function an automorphic form, which means that it transforms in a specific way. Certain identities hold for all automorphic forms. An example is
which implies that transforms as a representation under the discrete Heisenberg group. General modular forms and other theta functions also involve , once again because of the Stone–von Neumann theorem.

The Cauchy distribution

is a probability density function. The total probability is equal to one, owing to the integral:

The Shannon entropy of the Cauchy distribution is equal to , which also involves .
The Cauchy distribution plays an important role in potential theory because it is the simplest Furstenberg measure, the classical Poisson kernel associated with a Brownian motion in a half-plane. Conjugate harmonic functions and so also the Hilbert transform are associated with the asymptotics of the Poisson kernel. The Hilbert transform "H" is the integral transform given by the Cauchy principal value of the singular integral

The constant is the unique (positive) normalizing factor such that "H" defines a linear complex structure on the Hilbert space of square-integrable real-valued functions on the real line. The Hilbert transform, like the Fourier transform, can be characterized purely in terms of its transformation properties on the Hilbert space : up to a normalization factor, it is the unique bounded linear operator that commutes with positive dilations and anti-commutes with all reflections of the real line. The constant is the unique normalizing factor that makes this transformation unitary.

An occurrence of in the Mandelbrot set fractal was discovered by David Boll in 1991. He examined the behaviour of the Mandelbrot set near the "neck" at . If points with coordinates are considered, as tends to zero, the number of iterations until divergence for the point multiplied by converges to . The point at the cusp of the large "valley" on the right side of the Mandelbrot set behaves similarly: the number of iterations until divergence multiplied by the square root of tends to .

Although not a physical constant, appears routinely in equations describing fundamental principles of the universe, often because of 's relationship to the circle and to spherical coordinate systems. A simple formula from the field of classical mechanics gives the approximate period of a simple pendulum of length , swinging with a small amplitude ( is the earth's gravitational acceleration):
One of the key formulae of quantum mechanics is Heisenberg's uncertainty principle, which shows that the uncertainty in the measurement of a particle's position (Δ) and momentum (Δ) cannot both be arbitrarily small at the same time (where is Planck's constant):

The fact that is approximately equal to 3 plays a role in the relatively long lifetime of orthopositronium. The inverse lifetime to lowest order in the fine-structure constant is
where is the mass of the electron.

The field of fluid dynamics contains in Stokes' law, which approximates the frictional force exerted on small, spherical objects of radius , moving with velocity in a fluid with dynamic viscosity :

In electromagnetics, the vacuum permeability constant "μ" appears in Maxwell's equations, which describe the properties of electric and magnetic fields and electromagnetic radiation. Before 20 May 2019, it was defined as exactly
A relation for the speed of light in vacuum, can be derived from Maxwell's equations in the medium of classical vacuum using a relationship between "μ" and the electric constant (vacuum permittivity), in SI units:

Under ideal conditions (uniform gentle slope on a homogeneously erodible substrate), the sinuosity of a meandering river approaches . The sinuosity is the ratio between the actual length and the straight-line distance from source to mouth. Faster currents along the outside edges of a river's bends cause more erosion than along the inside edges, thus pushing the bends even farther out, and increasing the overall loopiness of the river. However, that loopiness eventually causes the river to double back on itself in places and "short-circuit", creating an ox-bow lake in the process. The balance between these two opposing factors leads to an average ratio of between the actual length and the direct distance between source and mouth.

Piphilology is the practice of memorizing large numbers of digits of , and world-records are kept by the "Guinness World Records". The record for memorizing digits of , certified by Guinness World Records, is 70,000 digits, recited in India by Rajveer Meena in 9 hours and 27 minutes on 21 March 2015. In 2006, Akira Haraguchi, a retired Japanese engineer, claimed to have recited 100,000 decimal places, but the claim was not verified by Guinness World Records.

One common technique is to memorize a story or poem in which the word lengths represent the digits of : The first word has three letters, the second word has one, the third has four, the fourth has one, the fifth has five, and so on. Such memorization aids are called mnemonics. An early example of a mnemonic for pi, originally devised by English scientist James Jeans, is "How I want a drink, alcoholic of course, after the heavy lectures involving quantum mechanics." When a poem is used, it is sometimes referred to as a "piem". Poems for memorizing have been composed in several languages in addition to English. Record-setting memorizers typically do not rely on poems, but instead use methods such as remembering number patterns and the method of loci.

A few authors have used the digits of to establish a new form of constrained writing, where the word lengths are required to represent the digits of . The "Cadaeic Cadenza" contains the first 3835 digits of in this manner, and the full-length book "Not a Wake" contains 10,000 words, each representing one digit of .

Perhaps because of the simplicity of its definition and its ubiquitous presence in formulae, has been represented in popular culture more than other mathematical constructs.

In the 2008 Open University and BBC documentary co-production, "The Story of Maths", aired in October 2008 on BBC Four, British mathematician Marcus du Sautoy shows a visualization of the – historically first exact – formula for calculating when visiting India and exploring its contributions to trigonometry.

In the Palais de la Découverte (a science museum in Paris) there is a circular room known as the "pi room". On its wall are inscribed 707 digits of . The digits are large wooden characters attached to the dome-like ceiling. The digits were based on an 1853 calculation by English mathematician William Shanks, which included an error beginning at the 528th digit. The error was detected in 1946 and corrected in 1949.

In Carl Sagan's novel "Contact" it is suggested that the creator of the universe buried a message deep within the digits of . The digits of have also been incorporated into the lyrics of the song "Pi" from the album "Aerial" by Kate Bush.

In the United States, Pi Day falls on 14 March (written 3/14 in the US style), and is popular among students. and its digital representation are often used by self-described "math geeks" for inside jokes among mathematically and technologically minded groups. Several college cheers at the Massachusetts Institute of Technology include "3.14159". Pi Day in 2015 was particularly significant because the date and time 3/14/15 9:26:53 reflected many more digits of pi. In parts of the world where dates are commonly noted in day/month/year format, 22 July represents "Pi Approximation Day," as 22/7 = 3.142857.

During the 2011 auction for Nortel's portfolio of valuable technology patents, Google made a series of unusually specific bids based on mathematical and scientific constants, including .
In 1958 Albert Eagle proposed replacing by (tau), where , to simplify formulas. However, no other authors are known to use in this way. Some people use a different value, , arguing that , as the number of radians in one turn, or as the ratio of a circle's circumference to its radius rather than its diameter, is more natural than and simplifies many formulas. Celebrations of this number, because it approximately equals 6.28, by making 28 June "Tau Day" and eating "twice the pie", have been reported in the media. However, this use of has not made its way into mainstream mathematics.

In 1897, an amateur mathematician attempted to persuade the Indiana legislature to pass the Indiana Pi Bill, which described a method to square the circle and contained text that implied various incorrect values for , including 3.2. The bill is notorious as an attempt to establish a value of scientific constant by legislative fiat. The bill was passed by the Indiana House of Representatives, but rejected by the Senate, meaning it did not become a law.

In contemporary internet culture, individuals and organizations frequently pay homage to the number . For instance, the computer scientist Donald Knuth let the version numbers of his program TeX approach . The versions are 3, 3.1, 3.14, and so forth.






</doc>
<doc id="23603" url="https://en.wikipedia.org/wiki?curid=23603" title="Postmodernism">
Postmodernism

Postmodernism is a broad movement that developed in the mid- to late 20th century across philosophy, the arts, architecture, and criticism, marking a departure from modernism. The term has been more generally applied to describe a historical era said to follow after modernity and the tendencies of this era. 
Postmodernism is generally defined by an attitude of skepticism, irony, or rejection toward what it describes as the grand narratives and ideologies associated with modernism, often criticizing Enlightenment rationality and focusing on the role of ideology in maintaining political or economic power. Postmodern thinkers frequently describe knowledge claims and value systems as contingent or socially-conditioned, framing them as products of political, historical, or cultural discourses and hierarchies. Common targets of postmodern criticism include universalist ideas of objective reality, morality, truth, human nature, reason, science, language, and social progress. Accordingly, postmodern thought is broadly characterized by tendencies to self-consciousness, self-referentiality, epistemological and moral relativism, pluralism, and irreverence.

Postmodern critical approaches gained purchase in the 1980s and 1990s, and have been adopted in a variety of academic and theoretical disciplines, including cultural studies, philosophy of science, economics, linguistics, architecture, feminist theory, and literary criticism, as well as art movements in fields such as literature, contemporary art, and music. Postmodernism is often associated with schools of thought such as deconstruction, post-structuralism, and institutional critique, as well as philosophers such as Jean-François Lyotard, Jacques Derrida, and Fredric Jameson.

Criticisms of postmodernism are intellectually diverse and include arguments that postmodernism promotes obscurantism, is meaningless, and that it adds nothing to analytical or empirical knowledge.

Postmodernism is an intellectual stance or mode of discourse defined by an attitude of skepticism toward what it describes as the grand narratives and ideologies of modernism, as well as opposition to epistemic certainty and the stability of meaning. It questions or criticizes viewpoints associated with Enlightenment rationality dating back to the 17th century, and is characterized by irony, eclecticism, and its rejection of the "universal validity" of binary oppositions, stable identity, hierarchy, and categorization. Postmodernism is associated with relativism and a focus on ideology in the maintenance of economic and political power. Postmodernists are generally "skeptical of explanations which claim to be valid for all groups, cultures, traditions, or races," and describe truth as relative. It can be described as a reaction against attempts to explain reality in an objective manner by claiming that reality is a mental construct. Access to an unmediated reality or to objectively rational knowledge is rejected on the grounds that all interpretations are contingent on when they are made; as such, claims to objective fact are dismissed as "naive realism."

Postmodern thinkers frequently describe knowledge claims and value systems as contingent or socially-conditioned, describing them as products of political, historical, or cultural discourses and hierarchies. Accordingly, postmodern thought is broadly characterized by tendencies to self-referentiality, epistemological and moral relativism, pluralism, and irreverence. Postmodernism is often associated with schools of thought such as deconstruction and post-structuralism. Postmodernism relies on critical theory, which considers the effects of ideology, society, and history on culture. Postmodernism and critical theory commonly criticize universalist ideas of objective reality, morality, truth, human nature, reason, language, and social progress.

Initially, postmodernism was a mode of discourse on literature and literary criticism, commenting on the nature of literary text, meaning, author and reader, writing, and reading. Postmodernism developed in the mid- to late-twentieth century across philosophy, the arts, architecture, and criticism as a departure or rejection of modernism. Postmodernist approaches have been adopted in a variety of academic and theoretical disciplines, including political science, organization theory, cultural studies, philosophy of science, economics, linguistics, architecture, feminist theory, and literary criticism, as well as art movements in fields such as literature and music. As a critical practice, postmodernism employs concepts such as hyperreality, simulacrum, trace, and difference, and rejects abstract principles in favor of direct experience.

Criticisms of postmodernism are intellectually diverse, and include arguments that postmodernism promotes obscurantism, is meaningless, and adds nothing to analytical or empirical knowledge. Some philosophers, beginning with the pragmatist philosopher Jürgen Habermas, say that postmodernism contradicts itself through self-reference, as their critique would be impossible without the concepts and methods that modern reason provides. Various authors have criticized postmodernism, or trends under the general postmodern umbrella, as abandoning Enlightenment rationalism or scientific rigor.

The term "postmodern" was first used in 1870. John Watkins Chapman suggested "a Postmodern style of painting" as a way to depart from French Impressionism. J. M. Thompson, in his 1914 article in "The Hibbert Journal" (a quarterly philosophical review), used it to describe changes in attitudes and beliefs in the critique of religion, writing: "The raison d'être of Post-Modernism is to escape from the double-mindedness of Modernism by being thorough in its criticism by extending it to religion as well as theology, to Catholic feeling as well as to Catholic tradition."

In 1942 H. R. Hays described postmodernism as a new literary form.

In 1926, Bernard Iddings Bell, president of St. Stephen's College (now Bard College), published "Postmodernism and Other Essays", marking the first use of the term to describe the historical period following Modernity. The essay criticizes the lingering socio-cultural norms, attitudes, and practices of the Age of Enlightenment. It also forecasts the major cultural shifts toward Postmodernity and (being an Anglo-Catholic priest) suggests orthodox religion as a solution. However, the term postmodernity was first used as a general theory for a historical movement in 1939 by Arnold J. Toynbee: "Our own Post-Modern Age has been inaugurated by the general war of 1914–1918".

In 1949 the term was used to describe a dissatisfaction with modern architecture and led to the postmodern architecture movement in response to the modernist architectural movement known as the International Style. Postmodernism in architecture was initially marked by a re-emergence of surface ornament, reference to surrounding buildings in urban settings, historical reference in decorative forms (eclecticism), and non-orthogonal angles.

Author Peter Drucker suggested the transformation into a post-modern world happened between 1937 and 1957 and described it as a "nameless era" characterized as a shift to a conceptual world based on pattern, purpose, and process rather than a mechanical cause. This shift was outlined by four new realities: the emergence of an Educated Society, the importance of international development, the decline of the nation-state, and the collapse of the viability of non-Western cultures.

In 1971, in a lecture delivered at the Institute of Contemporary Art, London, Mel Bochner described "post-modernism" in art as having started with Jasper Johns, "who first rejected sense-data and the singular point-of-view as the basis for his art, and treated art as a critical investigation."

In 1996, Walter Truett Anderson described postmodernism as belonging to one of four typological world views which he identifies as:

(a) Postmodern-ironist, which sees truth as socially constructed.

(b) Scientific-rational, in which truth is defined through methodical, disciplined inquiry.

(c) Social-traditional, in which truth is found in the heritage of American and Western civilization.

(d) Neo-Romantic, in which truth is found through attaining harmony with nature or spiritual exploration of the inner self.

The basic features of what is now called postmodernism can be found as early as the 1940s, most notably in the work of artists such as Jorge Luis Borges. However, most scholars today agree postmodernism began to compete with modernism in the late 1950s and gained ascendancy over it in the 1960s. Since then, postmodernism has been a powerful, though not undisputed, force in art, literature, film, music, drama, architecture, history, and continental philosophy.

The primary features of postmodernism typically include the ironic play with styles, citations and narrative levels, a metaphysical skepticism or nihilism towards a "grand narrative" of Western culture, and a preference for the virtual at the expense of the Real (or more accurately, a fundamental questioning of what 'the real' constitutes).

Since the late 1990s, there has been a growing sentiment in popular culture and in academia that postmodernism "has gone out of fashion". Others argue that postmodernism is dead in the context of current cultural production.

Structuralism was a philosophical movement developed by French academics in the 1950s, partly in response to French Existentialism, and often interpreted in relation to Modernism and High modernism. Thinkers who have been called structuralists include the anthropologist Claude Lévi-Strauss, the linguist Ferdinand de Saussure, the Marxist philosopher Louis Althusser, and the semiotician Algirdas Greimas. The early writings of the psychoanalyst Jacques Lacan and the literary theorist Roland Barthes have also been called structuralist. Those who began as structuralists but became post-structuralists include Michel Foucault, Roland Barthes, Jean Baudrillard, and Gilles Deleuze. Other post-structuralists include Jacques Derrida, Pierre Bourdieu, Jean-François Lyotard, Julia Kristeva, Hélène Cixous, and Luce Irigaray. The American cultural theorists, critics and intellectuals whom they influenced include Judith Butler, John Fiske, Rosalind Krauss, Avital Ronell, and Hayden White.

Like structuralists, post-structuralists start from the assumption that people's identities, values and economic conditions determine each other rather than having "intrinsic" properties that can be understood in isolation. Thus the French structuralists considered themselves to be espousing Relativism and Constructionism. But they nevertheless tended to explore how the subjects of their study might be described, reductively, as a set of "essential" relationships, schematics, or mathematical symbols. (An example is Claude Lévi-Strauss's algebraic formulation of mythological transformation in "The Structural Study of Myth").

Postmodernist ideas in philosophy and in the analysis of culture and society have expanded the importance of critical theory. They have been the point of departure for works of literature, architecture and design, as well as being visible in marketing/business and the interpretation of history, law and culture, starting in the late 20th century. These developments—re-evaluation of the entire Western value system (love, marriage, popular culture, shift from industrial to service economy) that took place since the 1950s and 1960s, with a peak in the Social Revolution of 1968—are described with the term "postmodernity", as opposed to "Postmodernism", a term referring to an opinion or movement. Post-structuralism is characterized by new ways of thinking through structuralism, contrary to the original form.

One of the most well-known postmodernist concerns is "deconstruction," a theory for philosophy, literary criticism, and textual analysis developed by Jacques Derrida. Critics have insisted that Derrida's work is rooted in a statement found in "Of Grammatology": "Il n'y a pas de hors-texte" ("there is no Outside-text"). Such critics misinterpret the statement as denying any reality outside of books. The statement is actually part of a critique of "inside" and "outside" metaphors when referring to text, and is corollary to the observation that there is no "inside" of a text as well. This attention to a text's unacknowledged reliance on metaphors and figures embedded within its discourse is characteristic of Derrida's approach. Derrida's method sometimes involves demonstrating that a given philosophical discourse depends on binary oppositions or excluding terms that the discourse itself has declared to be irrelevant or inapplicable. Derrida's philosophy inspired a postmodern movement called deconstructivism among architects, characterized by design that rejects structural "centers" and encourages decentralized play among its elements. Derrida discontinued his involvement with the movement after the publication of his collaborative project with architect Peter Eisenman in "Chora L Works: Jacques Derrida and Peter Eisenman".

The connection between postmodernism, posthumanism, and cyborgism has led to a challenge to postmodernism, for which the terms "postpostmodernism" and "postpoststructuralism" were first coined in 2003:

More recently metamodernism, post-postmodernism and the "death of postmodernism" have been widely debated: in 2007 Andrew Hoberek noted in his introduction to a special issue of the journal "Twentieth Century Literature" titled "After Postmodernism" that "declarations of postmodernism's demise have become a critical commonplace". A small group of critics has put forth a range of theories that aim to describe culture or society in the alleged aftermath of postmodernism, most notably Raoul Eshelman (performatism), Gilles Lipovetsky (hypermodernity), Nicolas Bourriaud (altermodern), and Alan Kirby (digimodernism, formerly called pseudo-modernism). None of these new theories or labels have so far gained very widespread acceptance. Sociocultural anthropologist Nina Müller-Schwarze offers neostructuralism as a possible direction. The exhibition "Postmodernism – Style and Subversion 1970–1990" at the Victoria and Albert Museum (London, 24 September 2011 – 15 January 2012) was billed as the first show to document postmodernism as a historical movement.

In the 1970s a group of poststructuralists in France developed a radical critique of modern philosophy with roots discernible in Nietzsche, Kierkegaard, and Heidegger, and became known as postmodern theorists, notably including Jacques Derrida, Michel Foucault, Jean-François Lyotard, Jean Baudrillard, and others. New and challenging modes of thought and writing pushed the development of new areas and topics in philosophy. By the 1980s, this spread to America (Richard Rorty) and the world.

Jacques Derrida was a French philosopher best known for developing a form of semiotic analysis known as deconstruction, which he discussed in numerous texts, and developed in the context of phenomenology. He is one of the major figures associated with post-structuralism and postmodern philosophy.

Derrida re-examined the fundamentals of writing and its consequences on philosophy in general; sought to undermine the language of "presence" or metaphysics in an analytical technique which, beginning as a point of departure from Heidegger's notion of "Destruktion", came to be known as Deconstruction.

Michel Foucault was a French philosopher, historian of ideas, social theorist, and literary critic. First associated with structuralism, Foucault created an oeuvre that today is seen as belonging to post-structuralism and to postmodern philosophy. Considered a leading figure of , his work remains fruitful in the English-speaking academic world in a large number of sub-disciplines. The Times Higher Education Guide described him in 2009 as the most cited author in the humanities.

Michel Foucault introduced concepts such as 'discursive regime', or re-invoked those of older philosophers like 'episteme' and 'genealogy' in order to explain the relationship between meaning, power, and social behavior within social orders (see "The Order of Things", "The Archaeology of Knowledge", "Discipline and Punish", and "The History of Sexuality").

Jean-François Lyotard is credited with being the first to use the term in a philosophical context, in his 1979 work "". In it, he follows Wittgenstein's language games model and speech act theory, contrasting two different language games, that of the expert, and that of the philosopher. He talks about transformation of knowledge into information in the computer age, and likens the transmission or reception of coded messages (information) to a position within a language game.

Lyotard defined philosophical postmodernism in "The Postmodern Condition", writing: "Simplifying to the extreme, I define postmodern as incredulity towards meta narratives..." where what he means by metanarrative is something like a unified, complete, universal, and epistemically certain story about everything that is. Postmodernists reject metanarratives because they reject the concept of truth that metanarratives presuppose. Postmodernist philosophers in general argue that truth is always contingent on historical and social context rather than being absolute and universal and that truth is always partial and "at issue" rather than being complete and certain.

Richard Rorty argues in "Philosophy and the Mirror of Nature" that contemporary analytic philosophy mistakenly imitates scientific methods. In addition, he denounces the traditional epistemological perspectives of representationalism and correspondence theory that rely upon the independence of knowers and observers from phenomena and the passivity of natural phenomena in relation to consciousness.

Jean Baudrillard, in "Simulacra and Simulation", introduced the concept that reality or the principle of "The Real" is short-circuited by the interchangeability of signs in an era whose communicative and semantic acts are dominated by electronic media and digital technologies. Baudrillard proposes the notion that, in such a state, where subjects are detached from the outcomes of events (political, literary, artistic, personal, or otherwise), events no longer hold any particular sway on the subject nor have any identifiable context; they therefore have the effect of producing widespread indifference, detachment, and passivity in industrialized populations. He claimed that a constant stream of appearances and references without any direct consequences to viewers or readers could eventually render the division between appearance and object indiscernible, resulting, ironically, in the "disappearance" of mankind in what is, in effect, a virtual or holographic state, composed only of appearances. For Baudrillard, "simulation is no longer that of a territory, a referential being or a substance. It is the generation by models of a real without origin or a reality: a hyperreal."

Fredric Jameson set forth one of the first expansive theoretical treatments of postmodernism as a historical period, intellectual trend, and social phenomenon in a series of lectures at the Whitney Museum, later expanded as "Postmodernism, or, the Cultural Logic of Late Capitalism" (1991).

In "Analysis of the Journey", a journal birthed from postmodernism, Douglas Kellner insists that the "assumptions and procedures of modern theory" must be forgotten. Extensively, Kellner analyzes the terms of this theory in real-life experiences and examples. Kellner used science and technology studies as a major part of his analysis; he urged that the theory is incomplete without it. The scale was larger than just postmodernism alone; it must be interpreted through cultural studies where science and technology studies play a huge role. The reality of the September 11 attacks on the United States of America is the catalyst for his explanation. This catalyst is used as a great representation due to the mere fact of the planned ambush and destruction of "symbols of globalization", insinuating the World Trade Center.

The conclusion he depicts is simple: postmodernism, as most use it today, will decide what experiences and signs in one's reality will be one's reality as they know it.

The idea of Postmodernism in architecture began as a response to the perceived blandness and failed Utopianism of the Modern movement. Modern Architecture, as established and developed by Walter Gropius and Le Corbusier, was focused on:
They argued for an architecture that represented the spirit of the age as depicted in cutting-edge technology, be it airplanes, cars, ocean liners or even supposedly artless grain silos. 
Modernist Ludwig Mies van der Rohe is associated with the phrase "less is more".

Critics of Modernism have:

The intellectual scholarship regarding postmodernism and architecture is closely linked with the writings of critic-turned-architect Charles Jencks, beginning with lectures in the early 1970s and his essay "The Rise of Post Modern Architecture" from 1975. His "magnum opus", however, is the book "The Language of Post-Modern Architecture", first published in 1977, and since running to seven editions. Jencks makes the point that Post-Modernism (like Modernism) varies for each field of art, and that for architecture it is not just a reaction to Modernism but what he terms "double coding": "Double Coding: the combination of Modern techniques with something else (usually traditional building) in order for architecture to communicate with the public and a concerned minority, usually other architects." In their book, "Revisiting Postmodernism", Terry Farrell and Adam Furman argue that postmodernism brought a more joyous and sensual experience to the culture, particularly in architecture.

Postmodern art is a body of art movements that sought to contradict some aspects of modernism or some aspects that emerged or developed in its aftermath. Cultural production manifesting as intermedia, installation art, conceptual art, deconstructionist display, and multimedia, particularly involving video, are described as postmodern.

Early mention of postmodernism as an element of graphic design appeared in the British magazine, "Design." A characteristic of postmodern graphic design is that "retro, techno, punk, grunge, beach, parody, and pastiche were all conspicuous trends. Each had its own sites and venues, detractors and advocates."

Jorge Luis Borges' (1939) short story "Pierre Menard, Author of the Quixote", is often considered as predicting postmodernism and is a paragon of the ultimate parody. Samuel Beckett is also considered an important precursor and influence. Novelists who are commonly connected with postmodern literature include Vladimir Nabokov, William Gaddis, Umberto Eco, Pier Vittorio Tondelli, John Hawkes, William S. Burroughs, Giannina Braschi, Kurt Vonnegut, John Barth, Jean Rhys, Donald Barthelme, E. L. Doctorow, Richard Kalich, Jerzy Kosiński, Don DeLillo, Thomas Pynchon (Pynchon's work has also been described as "high modern"), Ishmael Reed, Kathy Acker, Ana Lydia Vega, Jáchym Topol and Paul Auster.

In 1971, the Arab-American scholar Ihab Hassan published "The Dismemberment of Orpheus: Toward a Postmodern Literature," an early work of literary criticism from a postmodern perspective that traces the development of what he calls "literature of silence" through Marquis de Sade, Franz Kafka, Ernest Hemingway, Samuel Beckett, and many others, including developments such as the Theatre of the Absurd and the nouveau roman.

In "Postmodernist Fiction" (1987), Brian McHale details the shift from modernism to postmodernism, arguing that the former is characterized by an epistemological dominant and that postmodern works have developed out of modernism and are primarily concerned with questions of ontology. McHale's second book, "Constructing Postmodernism" (1992), provides readings of postmodern fiction and some contemporary writers who go under the label of cyberpunk. McHale's "What Was Postmodernism?" (2007) follows Raymond Federman's lead in now using the past tense when discussing postmodernism.

Jonathan Kramer has written that avant-garde musical compositions (which some would consider modernist rather than postmodernist) "defy more than seduce the listener, and they extend by potentially unsettling means the very idea of what music is." The postmodern impulse in classical music arose in the 1960s with the advent of musical minimalism. Composers such as Terry Riley, Henryk Górecki, Bradley Joseph, John Adams, Steve Reich, Philip Glass, Michael Nyman, and Lou Harrison reacted to the perceived elitism and dissonant sound of atonal academic modernism by producing music with simple textures and relatively consonant harmonies, whilst others, most notably John Cage challenged the prevailing narratives of beauty and objectivity common to Modernism.

Author on postmodernism, Dominic Strinati, has noted, it is also important "to include in this category the so-called 'art rock' musical innovations and mixing of styles associated with groups like Talking Heads, and performers like Laurie Anderson, together with the self-conscious 'reinvention of disco' by the Pet Shop Boys".

Modernism sought to design and plan cities which followed the logic of the new model of industrial mass production; reverting to large-scale solutions, aesthetic standardisation and prefabricated design solutions. Modernism eroded urban living by its failure to recognise differences and aim towards homogeneous landscapes (Simonsen 1990, 57). Jane Jacobs' 1961 book "The Death and Life of Great American Cities" was a sustained critique of urban planning as it had developed within Modernism and marked a transition from modernity to postmodernity in thinking about urban planning (Irving 1993, 479).

The transition from Modernism to Postmodernism is often said to have happened at 3:32pm on 15 July in 1972, when Pruitt–Igoe, a housing development for low-income people in St. Louis designed by architect Minoru Yamasaki, which had been a prize-winning version of Le Corbusier's 'machine for modern living,' was deemed uninhabitable and was torn down (Irving 1993, 480). Since then, Postmodernism has involved theories that embrace and aim to create diversity. It exalts uncertainty, flexibility and change (Hatuka & D'Hooghe 2007) and rejects utopianism while embracing a utopian way of thinking and acting. Postmodernity of 'resistance' seeks to deconstruct Modernism and is a critique of the origins without necessarily returning to them (Irving 1993, 60). As a result of Postmodernism, planners are much less inclined to lay a firm or steady claim to there being one single 'right way' of engaging in urban planning and are more open to different styles and ideas of 'how to plan' (Irving 474).

The study of postmodern urbanism itself, i.e. the postmodern way of creating and perpetuating the urban form, and the postmodern approach to understanding the city was pioneered in the 1980s by what could be called the "Los Angeles School of Geography" centered on the UCLA's Urban Planning Department in the 1980s, where contemporary Los Angeles was taken to be the postmodern city par excellence, contraposed to what had been the dominant ideas of the Chicago School formed in the 1920s at the University of Chicago, with its framework of "urban ecology" and its emphasis on functional areas of use within a city and the "concentric circles" to understand the sorting of different population groups. Edward Soja of the Los Angeles School combined Marxist and postmodern perspectives and focused on the economic and social changes (globalization, specialization, industrialization/deindustrialization, Neo-Liberalism, mass migration) that lead to the creation of large city-regions with their patchwork of population groups and economic uses

Criticisms of postmodernism are intellectually diverse, including the argument that postmodernism is meaningless and promotes obscurantism.

In part in reference to post-modernism, conservative English philosopher Roger Scruton wrote, "A writer who says that there are no truths, or that all truth is 'merely relative,' is asking you not to believe him. So don't." Similarly, Dick Hebdige criticized the vagueness of the term, enumerating a long list of otherwise unrelated concepts that people have designated as "postmodernism", from "the décor of a room" or "a 'scratch' video", to fear of nuclear armageddon and the "implosion of meaning", and stated that anything that could signify all of those things was "a buzzword".

The linguist and philosopher Noam Chomsky has said that postmodernism is meaningless because it adds nothing to analytical or empirical knowledge. He asks why postmodernist intellectuals do not respond like people in other fields when asked, "what are the principles of their theories, on what evidence are they based, what do they explain that wasn't already obvious, etc.?...If [these requests] can't be met, then I'd suggest recourse to Hume's advice in similar circumstances: 'to the flames'."

Christian philosopher William Lane Craig has said "The idea that we live in a postmodern culture is a myth. In fact, a postmodern culture is an impossibility; it would be utterly unliveable. People are not relativistic when it comes to matters of science, engineering, and technology; rather, they are relativistic and pluralistic in matters of religion and ethics. But, of course, that's not postmodernism; that's modernism!"

American academic and aesthete Camille Paglia has said "The end result of four decades of postmodernism permeating the art world is that there is very little interesting or important work being done right now in the fine arts. Irony was a bold and creative posture when Duchamp did it, but it is now an utterly banal, exhausted, and tedious strategy. Young artists have been taught to be "cool" and "hip" and thus painfully self-conscious. They are not encouraged to be enthusiastic, emotional, and visionary. They have been cut off from artistic tradition by the crippled skepticism about history that they have been taught by ignorant and solipsistic postmodernists. In short, the art world will never revive until postmodernism fades away. Postmodernism is a plague upon the mind and the heart."

German philosopher Albrecht Wellmer has said that "postmodernism at its best might be seen as a self-critical - a sceptical, ironic, but nevertheless unrelenting - form of modernism; a modernism beyond utopianism, scientism and foundationalism; in short a postmetaphysical modernism."

A formal, academic critique of postmodernism can be found in "Beyond the Hoax" by physics professor Alan Sokal and in "Fashionable Nonsense" by Sokal and Belgian physicist Jean Bricmont, both books discussing the so-called Sokal affair. In 1996, Sokal wrote a deliberately nonsensical article in a style similar to postmodernist articles, which was accepted for publication by the postmodern cultural studies journal, "Social Text". On the same day of the release he published another article in a different journal explaining the "Social Text" article hoax. The philosopher Thomas Nagel has supported Sokal and Bricmont, describing their book "Fashionable Nonsense" as consisting largely of "extensive quotations of scientific gibberish from name-brand French intellectuals, together with eerily patient explanations of why it is gibberish," and agreeing that "there does seem to be something about the Parisian scene that is particularly hospitable to reckless verbosity."

A more recent example of the difficulty of distinguishing nonsensical artifacts from genuine postmodernist scholarship is the Grievance Studies affair.

The French psychotherapist and philosopher, Félix Guattari, often considered a "postmodernist", rejected its theoretical assumptions by arguing that the structuralist and postmodernist visions of the world were not flexible enough to seek explanations in psychological, social and environmental domains at the same time.

Zimbabwean-born British Marxist Alex Callinicos says that postmodernism "reflects the disappointed revolutionary generation of '68, and the incorporation of many of its members into the professional and managerial 'new middle class'. It is best read as a symptom of political frustration and social mobility rather than as a significant intellectual or cultural phenomenon in its own right."

Christopher Hitchens in his book, "Why Orwell Matters", writes, in advocating for simple, clear and direct expression of ideas, "The Postmodernists' tyranny wears people down by boredom and semi-literate prose."

Analytic philosopher Daniel Dennett said, "Postmodernism, the school of 'thought' that proclaimed 'There are no truths, only interpretations' has largely played itself out in absurdity, but it has left behind a generation of academics in the humanities disabled by their distrust of the very idea of truth and their disrespect for evidence, settling for 'conversations' in which nobody is wrong and nothing can be confirmed, only asserted with whatever style you can muster."

American historian Richard Wolin traces the origins of postmodernism to intellectual roots in fascism, writing "postmodernism has been nourished by the doctrines of Friedrich Nietzsche, Martin Heidegger, Maurice Blanchot, and Paul de Man—all of whom either prefigured or succumbed to the proverbial intellectual fascination with fascism."

Daniel A. Farber and Suzanna Sherry criticised postmodernism for reducing the complexity of the modern world to an expression of power and for undermining truth and reason:
Richard Caputo, William Epstein, David Stoesz & Bruce Thyer consider postmodernism to be a "dead end in social work epistemology." They write:
H. Sidky pointed out what he sees as several "inherent flaws" of a postmodern antiscience perspective, including the confusion of the authority of science (evidence) with the scientist conveying the knowledge; its self-contradictory claim that all truths are relative; and its strategic ambiguity. He sees 21st-century anti-scientific and pseudo-scientific approaches to knowledge, particularly in the United States, as rooted in a postmodernist "decades-long academic assault on science:"








</doc>
<doc id="23604" url="https://en.wikipedia.org/wiki?curid=23604" title="Photography">
Photography

Photography is the art, application and practice of creating durable images by recording light, either electronically by means of an image sensor, or chemically by means of a light-sensitive material such as photographic film. It is employed in many fields of science, manufacturing (e.g., photolithography), and business, as well as its more direct uses for art, film and video production, recreational purposes, hobby, and mass communication.

Typically, a lens is used to focus the light reflected or emitted from objects into a real image on the light-sensitive surface inside a camera during a timed exposure. With an electronic image sensor, this produces an electrical charge at each pixel, which is electronically processed and stored in a digital image file for subsequent display or processing. The result with photographic emulsion is an invisible latent image, which is later chemically "developed" into a visible image, either negative or positive depending on the purpose of the photographic material and the method of processing. A negative image on film is traditionally used to photographically create a positive image on a paper base, known as a print, either by using an enlarger or by contact printing.

The word "photography" was created from the Greek roots φωτός ("phōtos"), genitive of φῶς ("phōs"), "light" and γραφή ("graphé") "representation by means of lines" or "drawing", together meaning "drawing with light".

Several people may have coined the same new term from these roots independently. Hercules Florence, a French painter and inventor living in Campinas, Brazil, used the French form of the word, "photographie", in private notes which a Brazilian historian believes were written in 1834. This claim is widely reported but is not yet largely recognized internationally. The first use of the word by the Franco-Brazilian inventor became widely known after the research of Boris Kossoy in 1980.

The German newspaper "Vossische Zeitung" of 25 February 1839 contained an article entitled "Photographie", discussing several priority claims – especially Henry Fox Talbot's – regarding Daguerre's claim of invention. The article is the earliest known occurrence of the word in public print. It was signed "J.M.", believed to have been Berlin astronomer Johann von Maedler. The astronomer Sir John Herschel is also credited with coining the word, independent of Talbot, in 1839.

The inventors Nicéphore Niépce, Henry Fox Talbot and Louis Daguerre seem not to have known or used the word "photography", but referred to their processes as "Heliography" (Niépce), "Photogenic Drawing"/"Talbotype"/"Calotype" (Talbot) and "Daguerreotype" (Daguerre).

Photography is the result of combining several technical discoveries, relating to seeing an image and capturing the image. The discovery of the camera obscura ("dark chamber" in Latin) that provides an image of a scene dates back to ancient China. Greek mathematicians Aristotle and Euclid independently described a camera obscura in the 5th and 4th centuries BCE. In the 6th century CE, Byzantine mathematician Anthemius of Tralles used a type of camera obscura in his experiments.

The Arab physicist Ibn al-Haytham (Alhazen) (965–1040) also invented a camera obscura as well as the first true pinhole camera. The invention of the camera has been traced back to the work of Ibn al-Haytham. While the effects of a single light passing through a pinhole had been described earlier, Ibn al-Haytham gave the first correct analysis of the camera obscura, including the first geometrical and quantitative descriptions of the phenomenon, and was the first to use a screen in a dark room so that an image from one side of a hole in the surface could be projected onto a screen on the other side. He also first understood the relationship between the focal point and the pinhole, and performed early experiments with afterimages, laying the foundations for the invention of photography in the 19th century.

Leonardo da Vinci mentions natural camera obscura that are formed by dark caves on the edge of a sunlit valley. A hole in the cave wall will act as a pinhole camera and project a laterally reversed, upside down image on a piece of paper. Renaissance painters used the camera obscura which, in fact, gives the optical rendering in color that dominates Western Art. It is a box with a hole in it which allows light to go through and create an image onto the piece of paper.

The birth of photography was then concerned with inventing means to capture and keep the image produced by the camera obscura. Albertus Magnus (1193–1280) discovered silver nitrate, and Georg Fabricius (1516–1571) discovered silver chloride, and the techniques described in Ibn al-Haytham's Book of Optics are capable of producing primitive photographs using medieval materials.

Daniele Barbaro described a diaphragm in 1566. Wilhelm Homberg described how light darkened some chemicals (photochemical effect) in 1694. The fiction book "Giphantie", published in 1760, by French author Tiphaigne de la Roche, described what can be interpreted as photography.

Around the year 1800, British inventor Thomas Wedgwood made the first known attempt to capture the image in a camera obscura by means of a light-sensitive substance. He used paper or white leather treated with silver nitrate. Although he succeeded in capturing the shadows of objects placed on the surface in direct sunlight, and even made shadow copies of paintings on glass, it was reported in 1802 that "the images formed by means of a camera obscura have been found too faint to produce, in any moderate time, an effect upon the nitrate of silver." The shadow images eventually darkened all over.

The first permanent photoetching was an image produced in 1822 by the French inventor Nicéphore Niépce, but it was destroyed in a later attempt to make prints from it. Niépce was successful again in 1825. In 1826 or 1827, he made the "View from the Window at Le Gras", the earliest surviving photograph from nature (i.e., of the image of a real-world scene, as formed in a camera obscura by a lens).

Because Niépce's camera photographs required an extremely long exposure (at least eight hours and probably several days), he sought to greatly improve his bitumen process or replace it with one that was more practical. In partnership with Louis Daguerre, he worked out post-exposure processing methods that produced visually superior results and replaced the bitumen with a more light-sensitive resin, but hours of exposure in the camera were still required. With an eye to eventual commercial exploitation, the partners opted for total secrecy.

Niépce died in 1833 and Daguerre then redirected the experiments toward the light-sensitive silver halides, which Niépce had abandoned many years earlier because of his inability to make the images he captured with them light-fast and permanent. Daguerre's efforts culminated in what would later be named the daguerreotype process. The essential elements—a silver-plated surface sensitized by iodine vapor, developed by mercury vapor, and "fixed" with hot saturated salt water—were in place in 1837. The required exposure time was measured in minutes instead of hours. Daguerre took the earliest confirmed photograph of a person in 1838 while capturing a view of a Paris street: unlike the other pedestrian and horse-drawn traffic on the busy boulevard, which appears deserted, one man having his boots polished stood sufficiently still throughout the several-minutes-long exposure to be visible. The existence of Daguerre's process was publicly announced, without details, on 7 January 1839. The news created an international sensation. France soon agreed to pay Daguerre a pension in exchange for the right to present his invention to the world as the gift of France, which occurred when complete working instructions were unveiled on 19 August 1839. In that same year, American photographer Robert Cornelius is credited with taking the earliest surviving photographic self-portrait.

In Brazil, Hercules Florence had apparently started working out a silver-salt-based paper process in 1832, later naming it "Photographie".

Meanwhile, a British inventor, William Fox Talbot, had succeeded in making crude but reasonably light-fast silver images on paper as early as 1834 but had kept his work secret. After reading about Daguerre's invention in January 1839, Talbot published his hitherto secret method and set about improving on it. At first, like other pre-daguerreotype processes, Talbot's paper-based photography typically required hours-long exposures in the camera, but in 1840 he created the calotype process, which used the chemical development of a latent image to greatly reduce the exposure needed and compete with the daguerreotype. In both its original and calotype forms, Talbot's process, unlike Daguerre's, created a translucent negative which could be used to print multiple positive copies; this is the basis of most modern chemical photography up to the present day, as daguerreotypes could only be replicated by rephotographing them with a camera. Talbot's famous tiny paper negative of the Oriel window in Lacock Abbey, one of a number of camera photographs he made in the summer of 1835, may be the oldest camera negative in existence.

In France, Hippolyte Bayard invented his own process for producing direct positive paper prints and claimed to have invented photography earlier than Daguerre or Talbot.

British chemist John Herschel made many contributions to the new field. He invented the cyanotype process, later familiar as the "blueprint". He was the first to use the terms "photography", "negative" and "positive". He had discovered in 1819 that sodium thiosulphate was a solvent of silver halides, and in 1839 he informed Talbot (and, indirectly, Daguerre) that it could be used to "fix" silver-halide-based photographs and make them completely light-fast. He made the first glass negative in late 1839.
In the March 1851 issue of "The Chemist", Frederick Scott Archer published his wet plate collodion process. It became the most widely used photographic medium until the gelatin dry plate, introduced in the 1870s, eventually replaced it. There are three subsets to the collodion process; the Ambrotype (a positive image on glass), the Ferrotype or Tintype (a positive image on metal) and the glass negative, which was used to make positive prints on albumen or salted paper.

Many advances in photographic glass plates and printing were made during the rest of the 19th century. In 1891, Gabriel Lippmann introduced a process for making natural-color photographs based on the optical phenomenon of the interference of light waves. His scientifically elegant and important but ultimately impractical invention earned him the Nobel Prize in Physics in 1908.

Glass plates were the medium for most original camera photography from the late 1850s until the general introduction of flexible plastic films during the 1890s. Although the convenience of the film greatly popularized amateur photography, early films were somewhat more expensive and of markedly lower optical quality than their glass plate equivalents, and until the late 1910s they were not available in the large formats preferred by most professional photographers, so the new medium did not immediately or completely replace the old. Because of the superior dimensional stability of glass, the use of plates for some scientific applications, such as astrophotography, continued into the 1990s, and in the niche field of laser holography, it has persisted into the 2010s.

Hurter and Driffield began pioneering work on the light sensitivity of photographic emulsions in 1876. Their work enabled the first quantitative measure of film speed to be devised.

The first flexible photographic roll film was marketed by George Eastman, founder of Kodak in 1885, but this original "film" was actually a coating on a paper base. As part of the processing, the image-bearing layer was stripped from the paper and transferred to a hardened gelatin support. The first transparent plastic roll film followed in 1889. It was made from highly flammable nitrocellulose known as nitrate film.

Although cellulose acetate or "safety film" had been introduced by Kodak in 1908, at first it found only a few special applications as an alternative to the hazardous nitrate film, which had the advantages of being considerably tougher, slightly more transparent, and cheaper. The changeover was not completed for X-ray films until 1933, and although safety film was always used for 16 mm and 8 mm home movies, nitrate film remained standard for theatrical 35 mm motion pictures until it was finally discontinued in 1951.

Films remained the dominant form of photography until the early 21st century when advances in digital photography drew consumers to digital formats. Although modern photography is dominated by digital users, film continues to be used by enthusiasts and professional photographers. The distinctive "look" of film based photographs compared to digital images is likely due to a combination of factors, including: (1) differences in spectral and tonal sensitivity (S-shaped density-to-exposure (H&D curve) with film vs. linear response curve for digital CCD sensors) (2) resolution and (3) continuity of tone.

Originally, all photography was monochrome, or "black-and-white". Even after color film was readily available, black-and-white photography continued to dominate for decades, due to its lower cost, chemical stability, and its "classic" photographic look. The tones and contrast between light and dark areas define black-and-white photography. It is important to note that monochromatic pictures are not necessarily composed of pure blacks, whites, and intermediate shades of gray but can involve shades of one particular hue depending on the process. The cyanotype process, for example, produces an image composed of blue tones. The albumen print process first used more than years ago, produces brownish tones.

Many photographers continue to produce some monochrome images, sometimes because of the established archival permanence of well-processed silver-halide-based materials. Some full-color digital images are processed using a variety of techniques to create black-and-white results, and some manufacturers produce digital cameras that exclusively shoot monochrome. Monochrome printing or electronic display can be used to salvage certain photographs taken in color which are unsatisfactory in their original form; sometimes when presented as black-and-white or single-color-toned images they are found to be more effective. Although color photography has long predominated, monochrome images are still produced, mostly for artistic reasons. Almost all digital cameras have an option to shoot in monochrome, and almost all image editing software can combine or selectively discard RGB color channels to produce a monochrome image from one shot in color.

Color photography was explored beginning in the 1840s. Early experiments in color required extremely long exposures (hours or days for camera images) and could not "fix" the photograph to prevent the color from quickly fading when exposed to white light.

The first permanent color photograph was taken in 1861 using the three-color-separation principle first published by Scottish physicist James Clerk Maxwell in 1855. The foundation of virtually all practical color processes, Maxwell's idea was to take three separate black-and-white photographs through red, green and blue filters. This provides the photographer with the three basic channels required to recreate a color image. Transparent prints of the images could be projected through similar color filters and superimposed on the projection screen, an additive method of color reproduction. A color print on paper could be produced by superimposing carbon prints of the three images made in their complementary colors, a subtractive method of color reproduction pioneered by Louis Ducos du Hauron in the late 1860s.

Russian photographer Sergei Mikhailovich Prokudin-Gorskii made extensive use of this color separation technique, employing a special camera which successively exposed the three color-filtered images on different parts of an oblong plate. Because his exposures were not simultaneous, unsteady subjects exhibited color "fringes" or, if rapidly moving through the scene, appeared as brightly colored ghosts in the resulting projected or printed images.

Implementation of color photography was hindered by the limited sensitivity of early photographic materials, which were mostly sensitive to blue, only slightly sensitive to green, and virtually insensitive to red. The discovery of dye sensitization by photochemist Hermann Vogel in 1873 suddenly made it possible to add sensitivity to green, yellow and even red. Improved color sensitizers and ongoing improvements in the overall sensitivity of emulsions steadily reduced the once-prohibitive long exposure times required for color, bringing it ever closer to commercial viability.

Autochrome, the first commercially successful color process, was introduced by the Lumière brothers in 1907. Autochrome plates incorporated a mosaic color filter layer made of dyed grains of potato starch, which allowed the three color components to be recorded as adjacent microscopic image fragments. After an Autochrome plate was reversal processed to produce a positive transparency, the starch grains served to illuminate each fragment with the correct color and the tiny colored points blended together in the eye, synthesizing the color of the subject by the additive method. Autochrome plates were one of several varieties of additive color screen plates and films marketed between the 1890s and the 1950s.

Kodachrome, the first modern "integral tripack" (or "monopack") color film, was introduced by Kodak in 1935. It captured the three color components in a multi-layer emulsion. One layer was sensitized to record the red-dominated part of the spectrum, another layer recorded only the green part and a third recorded only the blue. Without special film processing, the result would simply be three superimposed black-and-white images, but complementary cyan, magenta, and yellow dye images were created in those layers by adding color couplers during a complex processing procedure.

Agfa's similarly structured Agfacolor Neu was introduced in 1936. Unlike Kodachrome, the color couplers in Agfacolor Neu were incorporated into the emulsion layers during manufacture, which greatly simplified the processing. Currently, available color films still employ a multi-layer emulsion and the same principles, most closely resembling Agfa's product.

Instant color film, used in a special camera which yielded a unique finished color print only a minute or two after the exposure, was introduced by Polaroid in 1963.

Color photography may form images as positive transparencies, which can be used in a slide projector, or as color negatives intended for use in creating positive color enlargements on specially coated paper. The latter is now the most common form of film (non-digital) color photography owing to the introduction of automated photo printing equipment. After a transition period centered around 1995–2005, color film was relegated to a niche market by inexpensive multi-megapixel digital cameras. Film continues to be the preference of some photographers because of its distinctive "look".

In 1981, Sony unveiled the first consumer camera to use a charge-coupled device for imaging, eliminating the need for film: the Sony Mavica. While the Mavica saved images to disk, the images were displayed on television, and the camera was not fully digital.

The first digital camera to both record and save images in a digital format was the Fujix DS-1P created by Fujfilm in 1988. https://www.fujifilm.com/innovation/achievements/ds-1p/

In 1991, Kodak unveiled the DCS 100, the first commercially available digital single lens reflex camera. Although its high cost precluded uses other than photojournalism and professional photography, commercial digital photography was born.

Digital imaging uses an electronic image sensor to record the image as a set of electronic data rather than as chemical changes on film. An important difference between digital and chemical photography is that chemical photography resists photo manipulation because it involves film and photographic paper, while digital imaging is a highly manipulative medium. This difference allows for a degree of image post-processing that is comparatively difficult in film-based photography and permits different communicative potentials and applications.
Digital photography dominates the 21st century. More than 99% of photographs taken around the world are through digital cameras, increasingly through smartphones.

Synthesis photography is part of computer-generated imagery (CGI) where the shooting process is modeled on real photography. The CGI, creating digital copies of real universe, requires a visual representation process of these universes. Synthesis photography is the application of analog and digital photography in digital space. With the characteristics of the real photography but not being constrained by the physical limits of real world, synthesis photography allows artists to move into areas beyond the grasp of real photography.

A large variety of photographic techniques and media are used in the process of capturing images for photography. These include the camera; stereoscopy; dualphotography; full-spectrum, ultraviolet and infrared media; light field photography; and other imaging techniques.

The camera is the image-forming device, and a photographic plate, photographic film or a silicon electronic image sensor is the capture medium. The respective recording medium can be the plate or film itself, or a digital magnetic or electronic memory.

Photographers control the camera and lens to "expose" the light recording material to the required amount of light to form a "latent image" (on plate or film) or RAW file (in digital cameras) which, after appropriate processing, is converted to a usable image. Digital cameras use an electronic image sensor based on light-sensitive electronics such as charge-coupled device (CCD) or complementary metal-oxide-semiconductor (CMOS) technology. The resulting digital image is stored electronically, but can be reproduced on a paper.

The camera (or 'camera obscura') is a dark room or chamber from which, as far as possible, all light is excluded except the light that forms the image. It was discovered and used in the 16th century by painters. The subject being photographed, however, must be illuminated. Cameras can range from small to very large, a whole room that is kept dark while the object to be photographed is in another room where it is properly illuminated. This was common for reproduction photography of flat copy when large film negatives were used (see Process camera).

As soon as photographic materials became "fast" (sensitive) enough for taking candid or surreptitious pictures, small "detective" cameras were made, some actually disguised as a book or handbag or pocket watch (the "Ticka" camera) or even worn hidden behind an Ascot necktie with a tie pin that was really the lens.

The movie camera is a type of photographic camera which takes a rapid sequence of photographs on recording medium. In contrast to a still camera, which captures a single snapshot at a time, the movie camera takes a series of images, each called a "frame". This is accomplished through an intermittent mechanism. The frames are later played back in a movie projector at a specific speed, called the "frame rate" (number of frames per second). While viewing, a person's eyes and brain merge the separate pictures to create the illusion of motion.

Photographs, both monochrome and color, can be captured and displayed through two side-by-side images that emulate human stereoscopic vision. Stereoscopic photography was the first that captured figures in motion. While known colloquially as "3-D" photography, the more accurate term is stereoscopy. Such cameras have long been realized by using film and more recently in digital electronic methods (including cell phone cameras).

Dualphotography consists of photographing a scene from both sides of a photographic device at once (e.g. camera for back-to-back dualphotography, or two networked cameras for portal-plane dualphotography). The dualphoto apparatus can be used to simultaneously capture both the subject and the photographer, or both sides of a geographical place at once, thus adding a supplementary narrative layer to that of a single image.

Ultraviolet and infrared films have been available for many decades and employed in a variety of photographic avenues since the 1960s. New technological trends in digital photography have opened a new direction in full spectrum photography, where careful filtering choices across the ultraviolet, visible and infrared lead to new artistic visions.

Modified digital cameras can detect some ultraviolet, all of the visible and much of the near infrared spectrum, as most digital imaging sensors are sensitive from about 350 nm to 1000 nm. An off-the-shelf digital camera contains an infrared hot mirror filter that blocks most of the infrared and a bit of the ultraviolet that would otherwise be detected by the sensor, narrowing the accepted range from about 400 nm to 700 nm.

Replacing a hot mirror or infrared blocking filter with an infrared pass or a wide spectrally transmitting filter allows the camera to detect the wider spectrum light at greater sensitivity. Without the hot-mirror, the red, green and blue (or cyan, yellow and magenta) colored micro-filters placed over the sensor elements pass varying amounts of ultraviolet (blue window) and infrared (primarily red and somewhat lesser the green and blue micro-filters).

Uses of full spectrum photography are for fine art photography, geology, forensics and law enforcement.

Digital methods of image capture and display processing have enabled the new technology of "light field photography" (also known as synthetic aperture photography). This process allows focusing at various depths of field to be selected "after" the photograph has been captured. As explained by Michael Faraday in 1846, the "light field" is understood as 5-dimensional, with each point in 3-D space having attributes of two more angles that define the direction of each ray passing through that point.

These additional vector attributes can be captured optically through the use of microlenses at each pixel point within the 2-dimensional image sensor. Every pixel of the final image is actually a selection from each sub-array located under each microlens, as identified by a post-image capture focus algorithm.

Besides the camera, other methods of forming images with light are available. For instance, a photocopy or xerography machine forms permanent images but uses the transfer of static electrical charges rather than photographic medium, hence the term electrophotography. Photograms are images produced by the shadows of objects cast on the photographic paper, without the use of a camera. Objects can also be placed directly on the glass of an image scanner to produce digital pictures.

An amateur photographer is one who practices photography as a hobby/passion and not necessarily for profit. The quality of some amateur work is comparable to that of many professionals and may be highly specialized or eclectic in choice of subjects. Amateur photography is often pre-eminent in photographic subjects which have little prospect of commercial use or reward. Amateur photography grew during the late 19th century due to the popularization of the hand-held camera. Nowadays it has spread widely through social media and is carried out throughout different platforms and equipment, switching to the use of cell phone. Good pictures can now be taken with a cell phone which is a key tool for making photography more accessible to everyone.

Commercial photography is probably best defined as any photography for which the photographer is paid for images rather than works of art. In this light, money could be paid for the subject of the photograph or the photograph itself. Wholesale, retail, and professional uses of photography would fall under this definition. The commercial photographic world could include:



During the 20th century, both fine art photography and documentary photography became accepted by the English-speaking art world and the gallery system. In the United States, a handful of photographers, including Alfred Stieglitz, Edward Steichen, John Szarkowski, F. Holland Day, and Edward Weston, spent their lives advocating for photography as a fine art.
At first, fine art photographers tried to imitate painting styles. This movement is called Pictorialism, often using soft focus for a dreamy, 'romantic' look. In reaction to that, Weston, Ansel Adams, and others formed the Group f/64 to advocate 'straight photography', the photograph as a (sharply focused) thing in itself and not an imitation of something else.

The aesthetics of photography is a matter that continues to be discussed regularly, especially in artistic circles. Many artists argued that photography was the mechanical reproduction of an image. If photography is authentically art, then photography in the context of art would need redefinition, such as determining what component of a photograph makes it beautiful to the viewer. The controversy began with the earliest images "written with light"; Nicéphore Niépce, Louis Daguerre, and others among the very earliest photographers were met with acclaim, but some questioned if their work met the definitions and purposes of art.

Clive Bell in his classic essay "Art" states that only "significant form" can distinguish art from what is not art.

On 7 February 2007, Sotheby's London sold the 2001 photograph "99 Cent II Diptychon" for an unprecedented $3,346,456 to an anonymous bidder, making it the most expensive at the time.

Conceptual photography turns a concept or idea into a photograph. Even though what is depicted in the photographs are real objects, the subject is strictly abstract.

Photojournalism is a particular form of photography (the collecting, editing, and presenting of news material for publication or broadcast) that employs images in order to tell a news story. It is now usually understood to refer only to still images, but in some cases the term also refers to video used in broadcast journalism. Photojournalism is distinguished from other close branches of photography (e.g., documentary photography, social documentary photography, street photography or celebrity photography) by complying with a rigid ethical framework which demands that the work be both honest and impartial whilst telling the story in strictly journalistic terms. Photojournalists create pictures that contribute to the news media, and help communities connect with one other. Photojournalists must be well informed and knowledgeable about events happening right outside their door. They deliver news in a creative format that is not only informative, but also entertaining.

The camera has a long and distinguished history as a means of recording scientific phenomena from the first use by Daguerre and Fox-Talbot, such as astronomical events (eclipses for example), small creatures and plants when the camera was attached to the eyepiece of microscopes (in photomicroscopy) and for macro photography of larger specimens. The camera also proved useful in recording crime scenes and the scenes of accidents, such as the Wootton bridge collapse in 1861. The methods used in analysing photographs for use in legal cases are collectively known as forensic photography. Crime scene photos are taken from three vantage point. The vantage points are overview, mid-range, and close-up.

In 1845 Francis Ronalds, the Honorary Director of the Kew Observatory, invented the first successful camera to make continuous recordings of meteorological and geomagnetic parameters. Different machines produced 12- or 24- hour photographic traces of the minute-by-minute variations of atmospheric pressure, temperature, humidity, atmospheric electricity, and the three components of geomagnetic forces. The cameras were supplied to numerous observatories around the world and some remained in use until well into the 20th century. Charles Brooke a little later developed similar instruments for the Greenwich Observatory.

Science uses image technology that has derived from the design of the Pin Hole camera. X-Ray machines are similar in design to Pin Hole cameras with high-grade filters and laser radiation.
Photography has become universal in recording events and data in science and engineering, and at crime scenes or accident scenes. The method has been much extended by using other wavelengths, such as infrared photography and ultraviolet photography, as well as spectroscopy. Those methods were first used in the Victorian era and improved much further since that time.

The first photographed atom was discovered in 2012 by physicists at Griffith University, Australia. They used an electric field to trap an "Ion" of the element, Ytterbium. The image was recorded on a CCD, an electronic photographic film.

Wildlife photography involves capturing images of various forms of wildlife . Unlike other forms of photography such as product or food photography, successful wildlife photography requires a photographer to choose the right place and right time when specific wildlife are present and active. It often requires great patience and considerable skill and command of the right photographic equipment.

There are many ongoing questions about different aspects of photography. In her "On Photography" (1977), Susan Sontag dismisses the objectivity of photography. This is a highly debated subject within the photographic community. Sontag argues, "To photograph is to appropriate the thing photographed. It means putting one's self into a certain relation to the world that feels like knowledge, and therefore like power." Photographers decide what to take a photo of, what elements to exclude and what angle to frame the photo, and these factors may reflect a particular socio-historical context. Along these lines, it can be argued that photography is a subjective form of representation.

Modern photography has raised a number of concerns on its effect on society. In Alfred Hitchcock's "Rear Window" (1954), the camera is presented as promoting voyeurism. 'Although the camera is an observation station, the act of photographing is more than passive observing'.
The camera doesn't rape or even possess, though it may presume, intrude, trespass, distort, exploit, and, at the farthest reach of metaphor, assassinate – all activities that, unlike the sexual push and shove, can be conducted from a distance, and with some detachment.
Digital imaging has raised ethical concerns because of the ease of manipulating digital photographs in post-processing. Many photojournalists have declared they will not crop their pictures or are forbidden from combining elements of multiple photos to make "photomontages", passing them as "real" photographs. Today's technology has made image editing relatively simple for even the novice photographer. However, recent changes of in-camera processing allow digital fingerprinting of photos to detect tampering for purposes of forensic photography.

Photography is one of the new media forms that changes perception and changes the structure of society. Further unease has been caused around cameras in regards to desensitization. Fears that disturbing or explicit images are widely accessible to children and society at large have been raised. Particularly, photos of war and pornography are causing a stir. Sontag is concerned that "to photograph is to turn people into objects that can be symbolically possessed." Desensitization discussion goes hand in hand with debates about censored images. Sontag writes of her concern that the ability to censor pictures means the photographer has the ability to construct reality.

One of the practices through which photography constitutes society is tourism. Tourism and photography combine to create a "tourist gaze"
in which local inhabitants are positioned and defined by the camera lens. However, it has also been argued that there exists a "reverse gaze" through which indigenous photographees can position the tourist photographer as a shallow consumer of images.

Additionally, photography has been the topic of many songs in popular culture.

Photography is both restricted as well as protected by the law in many jurisdictions. Protection of photographs is typically achieved through the granting of copyright or moral rights to the photographer. In the United States, photography is protected as a First Amendment right and anyone is free to photograph anything seen in public spaces as long as it is in plain view. In the UK a recent law (Counter-Terrorism Act 2008) increases the power of the police to prevent people, even press photographers, from taking pictures in public places. In South Africa, any person may photograph any other person, without their permission, in public spaces and the only specific restriction placed on what may not be photographed by government is related to anything classed as national security. Each country has different laws.








</doc>
<doc id="23607" url="https://en.wikipedia.org/wiki?curid=23607" title="Pentateuch (disambiguation)">
Pentateuch (disambiguation)

The Pentateuch is the first part of the Bible, consisting of Genesis, Exodus, Leviticus, Numbers, and Deuteronomy.

It can also refer to:



</doc>
<doc id="23612" url="https://en.wikipedia.org/wiki?curid=23612" title="Postmodern philosophy">
Postmodern philosophy

Postmodern philosophy is a philosophical movement that arose in the second half of the 20th century as a critical response to assumptions allegedly present in modernist philosophical ideas regarding culture, identity, history, or language that were developed during the 18th-century Enlightenment. Postmodernist thinkers developed concepts like difference, repetition, trace, and hyperreality to subvert "grand narratives", univocity of being, and epistemic certainty. Postmodern philosophy questions the importance of power relationships, personalization, and discourse in the "construction" of truth and world views. Many postmodernists appear to deny that an objective reality exists, and appear to deny that there are objective moral values.

Jean-François Lyotard defined philosophical postmodernism in "The Postmodern Condition", writing "Simplifying to the extreme, I define postmodern as incredulity towards meta narratives..." where what he means by metanarrative is something like a unified, complete, universal, and epistemically certain story about everything that is. Postmodernists reject metanarratives because they reject the concept of truth that metanarratives presuppose. Postmodernist philosophers in general argue that truth is always contingent on historical and social context rather than being absolute and universal and that truth is always partial and "at issue" rather than being complete and certain.

Postmodern philosophy is often particularly skeptical about simple binary oppositions characteristic of structuralism, emphasizing the problem of the philosopher cleanly distinguishing knowledge from ignorance, social progress from reversion, dominance from submission, good from bad, and presence from absence. But, for the same reasons, postmodern philosophy should often be particularly skeptical about the complex spectral characteristics of things, emphasizing the problem of the philosopher again cleanly distinguishing concepts, for a concept must be understood in the context of its opposite, such as existence and nothingness, normality and abnormality, speech and writing, and the like.

Postmodern philosophy also has strong relations with the substantial literature of critical theory.

Many postmodern claims are a deliberate repudiation of certain 18th-century Enlightenment values. Such a postmodernist believes that there is no objective natural reality, and that logic and reason are mere conceptual constructs that are not universally valid. Two other characteristic postmodern practices are a denial that human nature exists, and a (sometimes moderate) skepticism toward claims that science and technology will change society for the better. Postmodernists also believe there are no objective moral values. A postmodernist then tolerates multiple conceptions of morality, even if he or she disagrees with them subjectively. Postmodern writings often focus on deconstructing the role that power and ideology play in shaping discourse and belief. Postmodern philosophy shares ontological similarities with classical skeptical and relativistic belief systems.

The "Routledge Encyclopedia of Philosophy" states that "The assumption that there is no common denominator in 'nature' or 'truth' ... that guarantees the possibility of neutral or objective thought" is a key assumption of postmodernism. The National Research Council has characterized the belief that "social science research can never generate objective or trustworthy knowledge" as an example of a postmodernist belief. Jean-François Lyotard's seminal 1979 "The Postmodern Condition" stated that its hypotheses "should not be accorded predictive value in relation to reality, but strategic value in relation to the questions raised". Lyotard's statement in 1984 that "I define postmodern as incredulity toward meta-narratives" extends to incredulity toward science. Jacques Derrida, who is generally identified as a postmodernist, stated that "every referent, all reality has the structure of a differential trace". Paul Feyerabend, one of the most famous twentieth-century philosophers of science, is often classified as a postmodernist; Feyerabend held that modern science is no more justified than witchcraft, and has denounced the "tyranny" of "abstract concepts such as 'truth', 'reality', or 'objectivity', which narrow people's vision and ways of being in the world". Feyerabend also defended astrology, adopted alternative medicine, and sympathized with creationism. Defenders of postmodernism state that many descriptions of postmodernism exaggerate its antipathy to science; for example, Feyerabend denied that he was "anti-science", accepted that some scientific "theories" are superior to other theories (even if science itself is not superior to other modes of inquiry), and attempted conventional medical treatments during his fight against cancer.

Philosopher John Deely has argued for the contentious claim that the label "postmodern" for thinkers such as Derrida "et al." is "premature". Insofar as the "so-called" postmoderns follow the thoroughly "modern" trend of idealism, it is more an "ultra"modernism than anything else. A postmodernism that lives up to its name, therefore, must no longer confine itself to the premodern preoccupation with "things" nor with the modern confinement to "ideas", but must come to terms with the way of signs embodied in the semiotic doctrines of such thinkers as the Portuguese philosopher John Poinsot and the American philosopher Charles Sanders Peirce. Writes Deely,

The epoch of Greek and Latin philosophy was based on "being" in a quite precise sense: the existence exercised by things independently of human apprehension and attitude. The much briefer epoch of modern philosophy based itself rather on the instruments of human knowing, but in a way that unnecessarily compromised being. As the 20th century ends, there is reason to believe that a new philosophical epoch is dawning along with the new century, promising to be the richest epoch yet for human understanding. The postmodern era is positioned to synthesize at a higher level—the level of experience, where the being of things and the activity of the finite knower compenetrate one another and provide the materials whence can be derived knowledge of nature and knowledge of culture in their full symbiosis—the achievements of the ancients and the moderns in a way that gives full credit to the preoccupations of the two. The postmodern era has for its distinctive task in philosophy the exploration of a new path, no longer the ancient way of things nor the modern way of ideas, but the way of signs, whereby the peaks and valleys of ancient and modern thought alike can be surveyed and cultivated by a generation which has yet further peaks to climb and valleys to find.
Postmodern philosophy originated primarily in France during the mid-20th century. However, several philosophical antecedents inform many of postmodern philosophy's concerns.

It was greatly influenced by the writings of Søren Kierkegaard and Friedrich Nietzsche in the 19th century and other early-to-mid 20th-century philosophers, including phenomenologists Edmund Husserl and Martin Heidegger, psychoanalyst Jacques Lacan, structuralist Roland Barthes, Georges Bataille, and the later work of Ludwig Wittgenstein. Postmodern philosophy also drew from the world of the arts and architecture, particularly Marcel Duchamp, John Cage and artists who practiced collage, and the architecture of Las Vegas and the Pompidou Centre.

The most influential early postmodern philosophers were Jean Baudrillard, Jean-François Lyotard, and Jacques Derrida. Michel Foucault is also often cited as an early postmodernist although he personally rejected that label. Following Nietzsche, Foucault argued that knowledge is produced through the operations of "power", and changes fundamentally in different historical periods.

The writings of Lyotard were largely concerned with the role of narrative in human culture, and particularly how that role has changed as we have left modernity and entered a "postindustrial" or postmodern condition. He argued that modern philosophies legitimized their truth-claims not (as they themselves claimed) on logical or empirical grounds, but rather on the grounds of accepted stories (or "metanarratives") about knowledge and the world—comparing these with Wittgenstein's concept of language-games. He further argued that in our postmodern condition, these metanarratives no longer work to legitimize truth-claims. He suggested that in the wake of the collapse of modern metanarratives, people are developing a new "language-game"—one that does not make claims to absolute truth but rather celebrates a world of ever-changing relationships (among people and between people and the world).

Derrida, the father of deconstruction, practiced philosophy as a form of textual criticism. He criticized Western philosophy as privileging the concept of presence and "logos", as opposed to absence and markings or writings.

In the United States, the most famous pragmatist and self-proclaimed postmodernist was Richard Rorty. An analytic philosopher, Rorty believed that combining Willard Van Orman Quine's criticism of the analytic-synthetic distinction with Wilfrid Sellars's critique of the "Myth of the Given" allowed for an abandonment of the view of the thought or language as a mirror of a reality or external world. Further, drawing upon Donald Davidson's criticism of the dualism between conceptual scheme and empirical content, he challenges the sense of questioning whether our particular concepts are related to the world in an appropriate way, whether we can justify our ways of describing the world as compared with other ways. He argued that truth was not about getting it right or representing reality, but was part of a social practice and language was what served our purposes in a particular time; ancient languages are sometimes untranslatable into modern ones because they possess a different vocabulary and are unuseful today. Donald Davidson is not usually considered a postmodernist, although he and Rorty have both acknowledged that there are few differences between their philosophies.

Criticisms of postmodernism, while intellectually diverse, share the opinion that it lacks coherence and is hostile to notions such as truth, logic, and objectivity. Specifically, it is held that postmodernism can be meaningless, promotes obscurantism and uses relativism (in culture, morality, knowledge) to the extent that it cripples most judgement calls.





</doc>
<doc id="23613" url="https://en.wikipedia.org/wiki?curid=23613" title="Postmodern music">
Postmodern music

Postmodern music is music in the art music tradition produced in the postmodern era. It also describes any music that follows aesthetical and philosophical trends of postmodernism. As an aesthetic movement it was formed partly in reaction to modernism but is not primarily defined as oppositional to modernist music. Postmodernists question the tight definitions and categories of academic disciplines, which they regard simply as the remnants of modernity .

Postmodernism in music is not a distinct musical style, but rather refers to music of the postmodern era. Postmodernist music, on the other hand, shares characteristics with postmodernist art—that is, art that comes after and reacts against modernism (see Modernism in Music).

Fredric Jameson, a major figure in the thinking on postmodernism and culture, calls postmodernism "the cultural dominant of the logic of late capitalism" , meaning that, through globalization, postmodern culture is tied inextricably with capitalism (Mark Fisher, writing 20 years later, goes further, essentially calling it the sole cultural possibility ). Drawing from Jameson and other theorists, David Beard and Kenneth Gloag argue that, in music, postmodernism is not just an attitude but also an inevitability in the current cultural climate of fragmentation . As early as 1938, Theodor Adorno had already identified a trend toward the dissolution of "a culturally dominant set of values" , citing the commodification of all genres as beginning of the end of genre or value distinctions in music .

In some respects, Postmodern music could be categorized as simply the music of the postmodern era, or music that follows aesthetic and philosophical trends of postmodernism, but with Jameson in mind, it is clear these definitions are inadequate. As the name suggests, the postmodernist movement formed partly in reaction to the ideals of modernism, but in fact postmodern music is more to do with functionality and the effect of globalization than it is with a specific reaction, movement, or attitude . In the face of capitalism, Jameson says, "It is safest to grasp the concept of the postmodern as an attempt to think the present historically in an age that has forgotten how to think historically in the first place" .

Jonathan Kramer posits the idea (following Umberto Eco and Jean-François Lyotard) that postmodernism (including "musical" postmodernism) is less a surface style or historical period (i.e., condition) than an "attitude". Kramer enumerates 16 (arguably subjective) "characteristics of postmodern music, by which I mean music that is understood in a postmodern manner, or that calls forth postmodern listening strategies, or that provides postmodern listening experiences, or that exhibits postmodern compositional practices." According to , postmodern music:


Daniel Albright summarizes the main tendencies of musical postmodernism as :

One author has suggested that the emergence of postmodern music in popular music occurred in the late 1960s, influenced in part by psychedelic rock and one or more of the later Beatles albums . Beard and Gloag support this position, citing Jameson's theory that "the radical changes of musical styles and languages throughout the 1960s [are] now seen as a reflection of postmodernism" (; see also ). Others have placed the beginnings of postmodernism in the arts, with particular reference to music, at around 1930 (; ).







</doc>
<doc id="23615" url="https://en.wikipedia.org/wiki?curid=23615" title="Protocol">
Protocol

Protocol may refer to:





</doc>
<doc id="23617" url="https://en.wikipedia.org/wiki?curid=23617" title="Pump">
Pump

A pump is a device that moves fluids (liquids or gases), or sometimes slurries, by mechanical action, typically converted from electrical energy into Hydraulic energy. Pumps can be classified into three major groups according to the method they use to move the fluid: "direct lift", "displacement", and "gravity" pumps.

Pumps operate by some mechanism (typically reciprocating or rotary), and consume energy to perform mechanical work moving the fluid. Pumps operate via many energy sources, including manual operation, electricity, engines, or wind power, and come in many sizes, from microscopic for use in medical applications, to large industrial pumps.

Mechanical pumps serve in a wide range of applications such as pumping water from wells, aquarium filtering, pond filtering and aeration, in the car industry for water-cooling and fuel injection, in the energy industry for pumping oil and natural gas or for operating cooling towers and other components of heating, ventilation and air conditioning systems. In the medical industry, pumps are used for biochemical processes in developing and manufacturing medicine, and as artificial replacements for body parts, in particular the artificial heart and penile prosthesis.

When a casing contains only one revolving impeller, it is called a single-stage pump. When a casing contains two or more revolving impellers, it is called a double- or multi-stage pump.

In biology, many different types of chemical and biomechanical pumps have evolved; biomimicry is sometimes used in developing new types of mechanical pumps.
Mechanical pumps may be submerged in the fluid they are pumping or be placed external to the fluid.

Pumps can be classified by their method of displacement into positive-displacement pumps, impulse pumps, velocity pumps, gravity pumps, steam pumps and valveless pumps. There are three basic types of pumps: positive-displacement, centrifugal and axial-flow pumps. In centrifugal pumps the direction of flow of the fluid changes by ninety degrees as it flows over impeller, while in axial flow pumps the direction of flow is unchanged.

A positive-displacement pump makes a fluid move by trapping a fixed amount and forcing (displacing) that trapped volume into the discharge pipe.

Some positive-displacement pumps use an expanding cavity on the suction side and a decreasing cavity on the discharge side. Liquid flows into the pump as the cavity on the suction side expands and the liquid flows out of the discharge as the cavity collapses. The volume is constant through each cycle of operation.

Positive-displacement pumps, unlike centrifugal or rotodynamic pumps, theoretically can produce the same flow at a given speed (RPM) no matter what the discharge pressure. Thus, positive-displacement pumps are "constant flow machines". However, a slight increase in internal leakage as the pressure increases prevents a truly constant flow rate.

A positive-displacement pump must not operate against a closed valve on the discharge side of the pump, because it has no shutoff head like centrifugal pumps. A positive-displacement pump operating against a closed discharge valve continues to produce flow and the pressure in the discharge line increases until the line bursts, the pump is severely damaged, or both.

A relief or safety valve on the discharge side of the positive-displacement pump is therefore necessary. The relief valve can be internal or external. The pump manufacturer normally has the option to supply internal relief or safety valves. The internal valve is usually used only as a safety precaution. An external relief valve in the discharge line, with a return line back to the suction line or supply tank provides increased safety.

A positive-displacement pump can be further classified according to the mechanism used to move the fluid:


These pumps move fluid using a rotating mechanism that creates a vacuum that captures and draws in the liquid.

"Advantages:" Rotary pumps are very efficient because they can handle highly viscous fluids with higher flow rates as viscosity increases.

"Drawbacks:" The nature of the pump requires very close clearances between the rotating pump and the outer edge, making it rotate at a slow, steady speed. If rotary pumps are operated at high speeds, the fluids cause erosion, which eventually causes enlarged clearances that liquid can pass through, which reduces efficiency.

Rotary positive-displacement pumps fall into 5 main types: 

Reciprocating pumps move the fluid using one or more oscillating pistons, plungers, or membranes (diaphragms), while valves restrict fluid motion to the desired direction. In order for suction to take place, the pump must first pull the plunger in an outward motion to decrease pressure in the chamber. Once the plunger pushes back, it will increase the pressure chamber and the inward pressure of the plunger will then open the discharge valve and release the fluid into the delivery pipe at a high velocity.

Pumps in this category range from "simplex", with one cylinder, to in some cases "quad" (four) cylinders, or more. Many reciprocating-type pumps are "duplex" (two) or "triplex" (three) cylinder. They can be either "single-acting" with suction during one direction of piston motion and discharge on the other, or "double-acting" with suction and discharge in both directions. The pumps can be powered manually, by air or steam, or by a belt driven by an engine. This type of pump was used extensively in the 19th century—in the early days of steam propulsion—as boiler feed water pumps. Now reciprocating pumps typically pump highly viscous fluids like concrete and heavy oils, and serve in special applications that demand low flow rates against high resistance. Reciprocating hand pumps were widely used to pump water from wells. Common bicycle pumps and foot pumps for inflation use reciprocating action.

These positive-displacement pumps have an expanding cavity on the suction side and a decreasing cavity on the discharge side. Liquid flows into the pumps as the cavity on the suction side expands and the liquid flows out of the discharge as the cavity collapses. The volume is constant given each cycle of operation and the pump's volumetric efficiency can be achieved through routine maintenance and inspection of its valves.

Typical reciprocating pumps are:

The positive-displacement principle applies in these pumps:

This is the simplest of rotary positive-displacement pumps. It consists of two meshed gears that rotate in a closely fitted casing. The tooth spaces trap fluid and force it around the outer periphery. The fluid does not travel back on the meshed part, because the teeth mesh closely in the center. Gear pumps see wide use in car engine oil pumps and in various hydraulic power packs.

A screw pump is a more complicated type of rotary pump that uses two or three screws with opposing thread — e.g., one screw turns clockwise and the other counterclockwise. The screws are mounted on parallel shafts that have gears that mesh so the shafts turn together and everything stays in place. The screws turn on the shafts and drive fluid through the pump. As with other forms of rotary pumps, the clearance between moving parts and the pump's casing is minimal.

Widely used for pumping difficult materials, such as sewage sludge contaminated with large particles, this pump consists of a helical rotor, about ten times as long as its width. This can be visualized as a central core of diameter "x" with, typically, a curved spiral wound around of thickness half "x", though in reality it is manufactured in a single casting. This shaft fits inside a heavy-duty rubber sleeve, of wall thickness also typically "x". As the shaft rotates, the rotor gradually forces fluid up the rubber sleeve. Such pumps can develop very high pressure at low volumes.

Named after the Roots brothers who invented it, this lobe pump displaces the liquid trapped between two long helical rotors, each fitted into the other when perpendicular at 90°, rotating inside a triangular shaped sealing line configuration, both at the point of suction and at the point of discharge. This design produces a continuous flow with equal volume and no vortex. It can work at low pulsation rates, and offers gentle performance that some applications require.

Applications include:

A "peristaltic pump" is a type of positive-displacement pump. It contains fluid within a flexible tube fitted inside a circular pump casing (though linear peristaltic pumps have been made). A number of "rollers", "shoes", or "wipers" attached to a rotor compresses the flexible tube. As the rotor turns, the part of the tube under compression closes (or "occludes"), forcing the fluid through the tube. Additionally, when the tube opens to its natural state after the passing of the cam it draws ("restitution") fluid into the pump. This process is called peristalsis and is used in many biological systems such as the gastrointestinal tract.

"Plunger pumps" are reciprocating positive-displacement pumps.

These consist of a cylinder with a reciprocating plunger. The suction and discharge valves are mounted in the head of the cylinder. In the suction stroke, the plunger retracts and the suction valves open causing suction of fluid into the cylinder. In the forward stroke, the plunger pushes the liquid out of the discharge valve.
Efficiency and common problems: With only one cylinder in plunger pumps, the fluid flow varies between maximum flow when the plunger moves through the middle positions, and zero flow when the plunger is at the end positions. A lot of energy is wasted when the fluid is accelerated in the piping system. Vibration and "water hammer" may be a serious problem. In general, the problems are compensated for by using two or more cylinders not working in phase with each other.

Triplex plunger pumps use three plungers, which reduces the pulsation of single reciprocating plunger pumps. Adding a pulsation dampener on the pump outlet can further smooth the "pump ripple", or ripple graph of a pump transducer. The dynamic relationship of the high-pressure fluid and plunger generally requires high-quality plunger seals. Plunger pumps with a larger number of plungers have the benefit of increased flow, or smoother flow without a pulsation damper. The increase in moving parts and crankshaft load is one drawback.

Car washes often use these triplex-style plunger pumps (perhaps without pulsation dampers). In 1968, William Bruggeman reduced the size of the triplex pump and increased the lifespan so that car washes could use equipment with smaller footprints. Durable high-pressure seals, low-pressure seals and oil seals, hardened crankshafts, hardened connecting rods, thick ceramic plungers and heavier duty ball and roller bearings improve reliability in triplex pumps. Triplex pumps now are in a myriad of markets across the world.

Triplex pumps with shorter lifetimes are commonplace to the home user. A person who uses a home pressure washer for 10 hours a year may be satisfied with a pump that lasts 100 hours between rebuilds. Industrial-grade or continuous duty triplex pumps on the other end of the quality spectrum may run for as much as 2,080 hours a year.

The oil and gas drilling industry uses massive semi trailer-transported triplex pumps called mud pumps to pump drilling mud, which cools the drill bit and carries the cuttings back to the surface.
Drillers use triplex or even quintuplex pumps to inject water and solvents deep into shale in the extraction process called "fracking".

One modern application of positive-displacement pumps is compressed-air-powered double-diaphragm pumps. Run on compressed air, these pumps are intrinsically safe by design, although all manufacturers offer ATEX certified models to comply with industry regulation. These pumps are relatively inexpensive and can perform a wide variety of duties, from pumping water out of bunds to pumping hydrochloric acid from secure storage (dependent on how the pump is manufactured – elastomers / body construction). These double-diaphragm pumps can handle viscous fluids and abrasive materials with a gentle pumping process ideal for transporting shear-sensitive media.

Devised in China as chain pumps over 1000 years ago, these pumps can be made from very simple materials: A rope, a wheel and a PVC pipe are sufficient to make a simple rope pump. Rope pump efficiency has been studied by grassroots organizations and the techniques for making and running them have been continuously improved.

Impulse pumps use pressure created by gas (usually air). In some impulse pumps the gas trapped in the liquid (usually water), is released and accumulated somewhere in the pump, creating a pressure that can push part of the liquid upwards.

Conventional impulse pumps include:

Instead of a gas accumulation and releasing cycle, the pressure can be created by burning of hydrocarbons. Such combustion driven pumps directly transmit the impulse from a combustion event through the actuation membrane to the pump fluid. In order to allow this direct transmission, the pump needs to be almost entirely made of an elastomer (e.g. silicone rubber). Hence, the combustion causes the membrane to expand and thereby pumps the fluid out of the adjacent pumping chamber. The first combustion-driven soft pump was developed by ETH Zurich.

A hydraulic ram is a water pump powered by hydropower.

It takes in water at relatively low pressure and high flow-rate and outputs water at a higher hydraulic-head and lower flow-rate. The device uses the water hammer effect to develop pressure that lifts a portion of the input water that powers the pump to a point higher than where the water started.

The hydraulic ram is sometimes used in remote areas, where there is both a source of low-head hydropower, and a need for pumping water to a destination higher in elevation than the source. In this situation, the ram is often useful, since it requires no outside source of power other than the kinetic energy of flowing water.

Rotodynamic pumps (or dynamic pumps) are a type of velocity pump in which kinetic energy is added to the fluid by increasing the flow velocity. This increase in energy is converted to a gain in potential energy (pressure) when the velocity is reduced prior to or as the flow exits the pump into the discharge pipe. This conversion of kinetic energy to pressure is explained by the "First law of thermodynamics", or more specifically by "Bernoulli's principle".

Dynamic pumps can be further subdivided according to the means in which the velocity gain is achieved.

These types of pumps have a number of characteristics:

A practical difference between dynamic and positive-displacement pumps is how they operate under closed valve conditions. Positive-displacement pumps physically displace fluid, so closing a valve downstream of a positive-displacement pump produces a continual pressure build up that can cause mechanical failure of pipeline or pump. Dynamic pumps differ in that they can be safely operated under closed valve conditions (for short periods of time).

Such a pump is also referred to as a centrifugal pump. The fluid enters along the axis or center, is accelerated by the impeller and exits at right angles to the shaft (radially); an example is the centrifugal fan, which is commonly used to implement a vacuum cleaner. Another type of radial-flow pump is a vortex pump. The liquid in them moves in tangential direction around the working wheel. The conversion from the mechanical energy of motor into the potential energy of flow comes by means of multiple whirls, which are excited by the impeller in the working channel of the pump. Generally, a radial-flow pump operates at higher pressures and lower flow rates than an axial- or a mixed-flow pump.

These are also referred to as All fluid pumps. The fluid is pushed outward or inward to move fluid axially. They operate at much lower pressures and higher flow rates than radial-flow (centrifugal) pumps. Axial-flow pumps cannot be run up to speed without special precaution. If at a low flow rate, the total head rise and high torque associated with this pipe would mean that the starting torque would have to become a function of acceleration for the whole mass of liquid in the pipe system. If there is a large amount of fluid in the system, accelerate the pump slowly.
Mixed-flow pumps function as a compromise between radial and axial-flow pumps. The fluid experiences both radial acceleration and lift and exits the impeller somewhere between 0 and 90 degrees from the axial direction. As a consequence mixed-flow pumps operate at higher pressures than axial-flow pumps while delivering higher discharges than radial-flow pumps. The exit angle of the flow dictates the pressure head-discharge characteristic in relation to radial and mixed-flow.

This uses a jet, often of steam, to create a low pressure. This low pressure sucks in fluid and propels it into a higher pressure region.

Gravity pumps include the "syphon" and "Heron's fountain". The "hydraulic ram" is also sometimes called a gravity pump; in a gravity pump the water is lifted by gravitational force and so called gravity pump

Steam pumps have been for a long time mainly of historical interest. They include any type of pump powered by a steam engine and also pistonless pumps such as Thomas Savery's or the Pulsometer steam pump.

Recently there has been a resurgence of interest in low power solar steam pumps for use in smallholder irrigation in developing countries. Previously small steam engines have not been viable because of escalating inefficiencies as vapour engines decrease in size. However the use of modern engineering materials coupled with alternative engine configurations has meant that these types of system are now a cost-effective opportunity.

Valveless pumping assists in fluid transport in various biomedical and engineering systems. In a valveless pumping system, no valves (or physical occlusions) are present to regulate the flow direction. The fluid pumping efficiency of a valveless system, however, is not necessarily lower than that having valves. In fact, many fluid-dynamical systems in nature and engineering more or less rely upon valveless pumping to transport the working fluids therein. For instance, blood circulation in the cardiovascular system is maintained to some extent even when the heart's valves fail. Meanwhile, the embryonic vertebrate heart begins pumping blood long before the development of discernible chambers and valves. In microfluidics, valveless impedance pumps have been fabricated, and are expected to be particularly suitable for handling sensitive biofluids. Ink jet printers operating on the piezoelectric transducer principle also use valveless pumping. The pump chamber is emptied through the printing jet due to reduced flow impedance in that direction and refilled by capillary action.

Examining pump repair records and mean time between failures (MTBF) is of great importance to responsible and conscientious pump users. In view of that fact, the preface to the 2006 Pump User's Handbook alludes to "pump failure" statistics. For the sake of convenience, these failure statistics often are translated into MTBF (in this case, installed life before failure).

In early 2005, Gordon Buck, John Crane Inc.’s chief engineer for field operations in Baton Rouge, Louisiana, examined the repair records for a number of refinery and chemical plants to obtain meaningful reliability data for centrifugal pumps. A total of 15 operating plants having nearly 15,000 pumps were included in the survey. The smallest of these plants had about 100 pumps; several plants had over 2000. All facilities were located in the United States. In addition, considered as "new", others as "renewed" and still others as "established". Many of these plants—but not all—had an alliance arrangement with John Crane. In some cases, the alliance contract included having a John Crane Inc. technician or engineer on-site to coordinate various aspects of the program.

Not all plants are refineries, however, and different results occur elsewhere. In chemical plants, pumps have historically been "throw-away" items as chemical attack limits life. Things have improved in recent years, but the somewhat restricted space available in "old" DIN and ASME-standardized stuffing boxes places limits on the type of seal that fits. Unless the pump user upgrades the seal chamber, the pump only accommodates more compact and simple versions. Without this upgrading, lifetimes in chemical installations are generally around 50 to 60 percent of the refinery values.

Unscheduled maintenance is often one of the most significant costs of ownership, and failures of mechanical seals and bearings are among the major causes. Keep in mind the potential value of selecting pumps that cost more initially, but last much longer between repairs. The MTBF of a better pump may be one to four years longer than that of its non-upgraded counterpart. Consider that published average values of avoided pump failures range from US$2600 to US$12,000. This does not include lost opportunity costs. One pump fire occurs per 1000 failures. Having fewer pump failures means having fewer destructive pump fires.

As has been noted, a typical pump failure, based on actual year 2002 reports, costs US$5,000 on average. This includes costs for material, parts, labor and overhead. Extending a pump's MTBF from 12 to 18 months would save US$1,667 per year — which might be greater than the cost to upgrade the centrifugal pump's reliability.

Pumps are used throughout society for a variety of purposes. Early applications includes the use of the windmill or watermill to pump water. Today, the pump is used for irrigation, water supply, gasoline supply, air conditioning systems, refrigeration (usually called a compressor), chemical movement, sewage movement, flood control, marine services, etc.

Because of the wide variety of applications, pumps have a plethora of shapes and sizes: from very large to very small, from handling gas to handling liquid, from high pressure to low pressure, and from high volume to low volume.

Typically, a liquid pump can't simply draw air. The feed line of the pump and the internal body surrounding the pumping mechanism must first be filled with the liquid that requires pumping: An operator must introduce liquid into the system to initiate the pumping. This is called "priming" the pump. Loss of prime is usually due to ingestion of air into the pump. The clearances and displacement ratios in pumps for liquids, whether thin or more viscous, usually cannot displace air due to its compressibility. This is the case with most velocity (rotodynamic) pumps — for example, centrifugal pumps. For such pumps the position of the pump should always be lower than the suction point, if not the pump should be manually filled with liquid or a secondary pump should be used until all air is removed from the suction line and the pump casing.

Positive–displacement pumps, however, tend to have sufficiently tight sealing between the moving parts and the casing or housing of the pump that they can be described as "self-priming". Such pumps can also serve as "priming pumps", so called when they are used to fulfill that need for other pumps in lieu of action taken by a human operator.

One sort of pump once common worldwide was a hand-powered water pump, or 'pitcher pump'. It was commonly installed over community water wells in the days before piped water supplies.

In parts of the British Isles, it was often called "the parish pump". Though such community pumps are no longer common, people still used the expression "parish pump" to describe a place or forum where matters of local interest are discussed.

Because water from pitcher pumps is drawn directly from the soil, it is more prone to contamination. If such water is not filtered and purified, consumption of it might lead to gastrointestinal or other water-borne diseases. A notorious case is the 1854 Broad Street cholera outbreak. At the time it was not known how cholera was transmitted, but physician John Snow suspected contaminated water and had the handle of the public pump he suspected removed; the outbreak then subsided.

Modern hand-operated community pumps are considered the most sustainable low-cost option for safe water supply in resource-poor settings, often in rural areas in developing countries. A hand pump opens access to deeper groundwater that is often not polluted and also improves the safety of a well by protecting the water source from contaminated buckets. Pumps such as the Afridev pump are designed to be cheap to build and install, and easy to maintain with simple parts. However, scarcity of spare parts for these type of pumps in some regions of Africa has diminished their utility for these areas.

Multiphase pumping applications, also referred to as tri-phase, have grown due to increased oil drilling activity. In addition, the economics of multiphase production is attractive to upstream operations as it leads to simpler, smaller in-field installations, reduced equipment costs and improved production rates. In essence, the multiphase pump can accommodate all fluid stream properties with one piece of equipment, which has a smaller footprint. Often, two smaller multiphase pumps are installed in series rather than having just one massive pump.

For midstream and upstream operations, multiphase pumps can be located onshore or offshore and can be connected to single or multiple wellheads. Basically, multiphase pumps are used to transport the untreated flow stream produced from oil wells to downstream processes or gathering facilities. This means that the pump may handle a flow stream (well stream) from 100 percent gas to 100 percent liquid and every imaginable combination in between. The flow stream can also contain abrasives such as sand and dirt. Multiphase pumps are designed to operate under changing or fluctuating process conditions. Multiphase pumping also helps eliminate emissions of greenhouse gases as operators strive to minimize the flaring of gas and the venting of tanks where possible.

A rotodynamic pump with one single shaft that requires two mechanical seals, this pump uses an open-type axial impeller. It's often called a "Poseidon pump", and can be described as a cross between an axial compressor and a centrifugal pump.

The twin-screw pump is constructed of two inter-meshing screws that move the pumped fluid. Twin screw pumps are often used when pumping conditions contain high gas volume fractions and fluctuating inlet conditions. Four mechanical seals are required to seal the two shafts.

When the pumping application is not suited to a centrifugal pump, a progressive cavity pump is used instead. Progressive cavity pumps are single-screw types typically used in shallow wells or at the surface. This pump is mainly used on surface applications where the pumped fluid may contain a considerable amount of solids such as sand and dirt. The volumetric efficiency and mechanical efficiency of a progressive cavity pump increases as the viscosity of the liquid does.

These pumps are basically multistage centrifugal pumps and are widely used in oil well applications as a method for artificial lift. These pumps are usually specified when the pumped fluid is mainly liquid.

"Buffer tank"
A buffer tank is often installed upstream of the pump suction nozzle in case of a slug flow. The buffer tank breaks the energy of the liquid slug, smooths any fluctuations in the incoming flow and acts as a sand trap.

As the name indicates, multiphase pumps and their mechanical seals can encounter a large variation in service conditions such as changing process fluid composition, temperature variations, high and low operating pressures and exposure to abrasive/erosive media. The challenge is selecting the appropriate mechanical seal arrangement and support system to ensure maximized seal life and its overall effectiveness.

Pumps are commonly rated by horsepower, volumetric flow rate, outlet pressure in metres (or feet) of head, inlet suction in suction feet (or metres) of head.
The head can be simplified as the number of feet or metres the pump can raise or lower a column of water at atmospheric pressure.

From an initial design point of view, engineers often use a quantity termed the specific speed to identify the most suitable pump type for a particular combination of flow rate and head.

The power imparted into a fluid increases the energy of the fluid per unit volume. Thus the power relationship is between the conversion of the mechanical energy of the pump mechanism and the fluid elements within the pump. In general, this is governed by a series of simultaneous differential equations, known as the Navier–Stokes equations. However a more simple equation relating only the different energies in the fluid, known as Bernoulli's equation can be used. Hence the power, P, required by the pump:

where Δp is the change in total pressure between the inlet and outlet (in Pa), and Q, the volume flow-rate of the fluid is given in m/s. The total pressure may have gravitational, static pressure and kinetic energy components; i.e. energy is distributed between change in the fluid's gravitational potential energy (going up or down hill), change in velocity, or change in static pressure. η is the pump efficiency, and may be given by the manufacturer's information, such as in the form of a pump curve, and is typically derived from either fluid dynamics simulation (i.e. solutions to the Navier–Stokes for the particular pump geometry), or by testing. The efficiency of the pump depends upon the pump's configuration and operating conditions (such as rotational speed, fluid density and viscosity etc.)

For a typical "pumping" configuration, the work is imparted on the fluid, and is thus positive. For the fluid imparting the work on the pump (i.e. a turbine), the work is negative. Power required to drive the pump is determined by dividing the output power by the pump efficiency. Furthermore, this definition encompasses pumps with no moving parts, such as a siphon.

Pump efficiency is defined as the ratio of the power imparted on the fluid by the pump in relation to the power supplied to drive the pump. Its value is not fixed for a given pump, efficiency is a function of the discharge and therefore also operating head. For centrifugal pumps, the efficiency tends to increase with flow rate up to a point midway through the operating range (peak efficiency or Best Efficiency Point (BEP) ) and then declines as flow rates rise further. Pump performance data such as this is usually supplied by the manufacturer before pump selection. Pump efficiencies tend to decline over time due to wear (e.g. increasing clearances as impellers reduce in size).

When a system includes a centrifugal pump, an important design issue is matching the "head loss-flow characteristic" with the pump so that it operates at or close to the point of its maximum efficiency.

Pump efficiency is an important aspect and pumps should be regularly tested. Thermodynamic pump testing is one method.



</doc>
<doc id="23618" url="https://en.wikipedia.org/wiki?curid=23618" title="Progressive">
Progressive

Progressive may refer to:










</doc>
<doc id="23619" url="https://en.wikipedia.org/wiki?curid=23619" title="Pressure">
Pressure

Pressure (symbol: "p" or "P") is the force applied perpendicular to the surface of an object per unit area over which that force is distributed. Gauge pressure (also spelled "gage" pressure) is the pressure relative to the ambient pressure.

Various units are used to express pressure. Some of these derive from a unit of force divided by a unit of area; the SI unit of pressure, the pascal (Pa), for example, is one newton per square metre (N/m); similarly, the pound-force per square inch (psi) is the traditional unit of pressure in the imperial and U.S. customary systems. Pressure may also be expressed in terms of standard atmospheric pressure; the atmosphere (atm) is equal to this pressure, and the torr is defined as of this. Manometric units such as the centimetre of water, millimetre of mercury, and inch of mercury are used to express pressures in terms of the height of column of a particular fluid in a manometer.

Pressure is the amount of force applied at right angles to the surface of an object per unit area. The symbol for it is "p" or "P".
The IUPAC recommendation for pressure is a lower-case "p".
However, upper-case "P" is widely used. The usage of "P" vs "p" depends upon the field in which one is working, on the nearby presence of other symbols for quantities such as power and momentum, and on writing style.

Mathematically:
where:

Pressure is a scalar quantity. It relates the vector area element (a vector normal to the surface) with the normal force acting on it. The pressure is the scalar proportionality constant that relates the two normal vectors:

The minus sign comes from the fact that the force is considered towards the surface element, while the normal vector points outward. The equation has meaning in that, for any surface "S" in contact with the fluid, the total force exerted by the fluid on that surface is the surface integral over "S" of the right-hand side of the above equation.

It is incorrect (although rather usual) to say "the pressure is directed in such or such direction". The pressure, as a scalar, has no direction. The force given by the previous relationship to the quantity has a direction, but the pressure does not. If we change the orientation of the surface element, the direction of the normal force changes accordingly, but the pressure remains the same.

Pressure is distributed to solid boundaries or across arbitrary sections of fluid "normal to" these boundaries or sections at every point. It is a fundamental parameter in thermodynamics, and it is conjugate to volume.

The SI unit for pressure is the pascal (Pa), equal to one newton per square metre (N/m, or kg·m·s). This name for the unit was added in 1971; before that, pressure in SI was expressed simply in newtons per square metre.

Other units of pressure, such as pounds per square inch (Ibf/in) and bar, are also in common use. The CGS unit of pressure is the barye (Ba), equal to 1 dyn·cm, or 0.1 Pa. Pressure is sometimes expressed in grams-force or kilograms-force per square centimetre (g/cm or kg/cm) and the like without properly identifying the force units. But using the names kilogram, gram, kilogram-force, or gram-force (or their symbols) as units of force is expressly forbidden in SI. The technical atmosphere (symbol: at) is 1 kgf/cm (98.0665 kPa, or 14.223 psi).

Since a system under pressure has the potential to perform work on its surroundings, pressure is a measure of potential energy stored per unit volume. It is therefore related to energy density and may be expressed in units such as joules per cubic metre (J/m, which is equal to Pa).
Mathematically:

Some meteorologists prefer the hectopascal (hPa) for atmospheric air pressure, which is equivalent to the older unit millibar (mbar). Similar pressures are given in kilopascals (kPa) in most other fields, where the hecto- prefix is rarely used. The inch of mercury is still used in the United States. Oceanographers usually measure underwater pressure in decibars (dbar) because pressure in the ocean increases by approximately one decibar per metre depth.
The standard atmosphere (atm) is an established constant. It is approximately equal to typical air pressure at Earth mean sea level and is defined as .

Because pressure is commonly measured by its ability to displace a column of liquid in a manometer, pressures are often expressed as a depth of a particular fluid (e.g., centimetres of water, millimetres of mercury or inches of mercury). The most common choices are mercury (Hg) and water; water is nontoxic and readily available, while mercury's high density allows a shorter column (and so a smaller manometer) to be used to measure a given pressure. The pressure exerted by a column of liquid of height "h" and density "ρ" is given by the hydrostatic pressure equation , where "g" is the gravitational acceleration. Fluid density and local gravity can vary from one reading to another depending on local factors, so the height of a fluid column does not define pressure precisely. When millimetres of mercury or inches of mercury are quoted today, these units are not based on a physical column of mercury; rather, they have been given precise definitions that can be expressed in terms of SI units. One millimetre of mercury is approximately equal to one torr. The water-based units still depend on the density of water, a measured, rather than defined, quantity. These "manometric units" are still encountered in many fields. Blood pressure is measured in millimetres of mercury in most of the world, and lung pressures in centimetres of water are still common.

Underwater divers use the metre sea water (msw or MSW) and foot sea water (fsw or FSW) units of pressure, and these are the standard units for pressure gauges used to measure pressure exposure in diving chambers and personal decompression computers. A msw is defined as 0.1 bar (= 100000 Pa = 10000 Pa), is not the same as a linear metre of depth. 33.066 fsw = 1 atm (1 atm = 101325 Pa / 33.066 = 3064.326 Pa). Note that the pressure conversion from msw to fsw is different from the length conversion: 10 msw = 32.6336 fsw, while 10 m = 32.8083 ft.

Gauge pressure is often given in units with "g" appended, e.g. "kPag", "barg" or "psig", and units for measurements of absolute pressure are sometimes given a suffix of "a", to avoid confusion, for example "kPaa", "psia". However, the US National Institute of Standards and Technology recommends that, to avoid confusion, any modifiers be instead applied to the quantity being measured rather than the unit of measure. For example, rather than .

Differential pressure is expressed in units with "d" appended; this type of measurement is useful when considering sealing performance or whether a valve will open or close.

Presently or formerly popular pressure units include the following:

As an example of varying pressures, a finger can be pressed against a wall without making any lasting impression; however, the same finger pushing a thumbtack can easily damage the wall. Although the force applied to the surface is the same, the thumbtack applies more pressure because the point concentrates that force into a smaller area. Pressure is transmitted to solid boundaries or across arbitrary sections of fluid "normal to" these boundaries or sections at every point. Unlike stress, pressure is defined as a scalar quantity. The negative gradient of pressure is called the force density.

Another example is a knife. If we try to cut with the flat edge, force is distributed over a larger surface area resulting in less pressure, and it will not cut. Whereas using the sharp edge, which has less surface area, results in greater pressure, and so the knife cuts smoothly. This is one example of a practical application of pressure.

For gases, pressure is sometimes measured not as an "absolute pressure", but relative to atmospheric pressure; such measurements are called "gauge pressure". An example of this is the air pressure in an automobile tire, which might be said to be "", but is actually 220 kPa (32 psi) above atmospheric pressure. Since atmospheric pressure at sea level is about 100 kPa (14.7 psi), the absolute pressure in the tire is therefore about . In technical work, this is written "a gauge pressure of ". Where space is limited, such as on pressure gauges, name plates, graph labels, and table headings, the use of a modifier in parentheses, such as "kPa (gauge)" or "kPa (absolute)", is permitted. In non-SI technical work, a gauge pressure of is sometimes written as "32 psig", and an absolute pressure as "32 psia", though the other methods explained above that avoid attaching characters to the unit of pressure are preferred.

Gauge pressure is the relevant measure of pressure wherever one is interested in the stress on storage vessels and the plumbing components of fluidics systems. However, whenever equation-of-state properties, such as densities or changes in densities, must be calculated, pressures must be expressed in terms of their absolute values. For instance, if the atmospheric pressure is , a gas (such as helium) at (gauge) ( [absolute]) is 50% denser than the same gas at (gauge) ( [absolute]). Focusing on gauge values, one might erroneously conclude the first sample had twice the density of the second one.

In a static gas, the gas as a whole does not appear to move. The individual molecules of the gas, however, are in constant random motion. Because we are dealing with an extremely large number of molecules and because the motion of the individual molecules is random in every direction, we do not detect any motion. If we enclose the gas within a container, we detect a pressure in the gas from the molecules colliding with the walls of our container. We can put the walls of our container anywhere inside the gas, and the force per unit area (the pressure) is the same. We can shrink the size of our "container" down to a very small point (becoming less true as we approach the atomic scale), and the pressure will still have a single value at that point. Therefore, pressure is a scalar quantity, not a vector quantity. It has magnitude but no direction sense associated with it. Pressure force acts in all directions at a point inside a gas. At the surface of a gas, the pressure force acts perpendicular (at right angle) to the surface.

A closely related quantity is the stress tensor "σ", which relates the vector force formula_7 to the 
vector area formula_8 via the linear relation formula_9.

This tensor may be expressed as the sum of the viscous stress tensor minus the hydrostatic pressure. The negative of the stress tensor is sometimes called the pressure tensor, but in the following, the term "pressure" will refer only to the scalar pressure.

According to the theory of general relativity, pressure increases the strength of a gravitational field (see stress–energy tensor) and so adds to the mass-energy cause of gravity. This effect is unnoticeable at everyday pressures but is significant in neutron stars, although it has not been experimentally tested.

Fluid pressure is most often the compressive stress at some point within a fluid. (The term "fluid" refers to both liquids and gases – for more information specifically about liquid pressure, see section below.)

Fluid pressure occurs in one of two situations:

Pressure in open conditions usually can be approximated as the pressure in "static" or non-moving conditions (even in the ocean where there are waves and currents), because the motions create only negligible changes in the pressure. Such conditions conform with principles of fluid statics. The pressure at any given point of a non-moving (static) fluid is called the hydrostatic pressure. 

Closed bodies of fluid are either "static", when the fluid is not moving, or "dynamic", when the fluid can move as in either a pipe or by compressing an air gap in a closed container. The pressure in closed conditions conforms with the principles of fluid dynamics.

The concepts of fluid pressure are predominantly attributed to the discoveries of Blaise Pascal and Daniel Bernoulli. Bernoulli's equation can be used in almost any situation to determine the pressure at any point in a fluid. The equation makes some assumptions about the fluid, such as the fluid being ideal and incompressible. An ideal fluid is a fluid in which there is no friction, it is inviscid (zero viscosity). The equation for all points of a system filled with a constant-density fluid is

where:


Explosion or deflagration pressures are the result of the ignition of explosive gases, mists, dust/air suspensions, in unconfined and confined spaces.

While pressures are, in general, positive, there are several situations in which negative pressures may be encountered:

Stagnation pressure is the pressure a fluid exerts when it is forced to stop moving. Consequently, although a fluid moving at higher speed will have a lower static pressure, it may have a higher stagnation pressure when forced to a standstill. Static pressure and stagnation pressure are related by:

where 

The pressure of a moving fluid can be measured using a Pitot tube, or one of its variations such as a Kiel probe or Cobra probe, connected to a manometer. Depending on where the inlet holes are located on the probe, it can measure static pressures or stagnation pressures.

There is a two-dimensional analog of pressure – the lateral force per unit length applied on a line perpendicular to the force.

Surface pressure is denoted by π:
and shares many similar properties with three-dimensional pressure. Properties of surface chemicals can be investigated by measuring pressure/area isotherms, as the two-dimensional analog of Boyle's law, , at constant temperature.

Surface tension is another example of surface pressure, but with a reversed sign, because "tension" is the opposite to "pressure".

In an ideal gas, molecules have no volume and do not interact. According to the ideal gas law, pressure varies linearly with temperature and quantity, and inversely with volume:

where:

Real gases exhibit a more complex dependence on the variables of state.

Vapour pressure is the pressure of a vapour in thermodynamic equilibrium with its condensed phases in a closed system. All liquids and solids have a tendency to evaporate into a gaseous form, and all gases have a tendency to condense back to their liquid or solid form.

The atmospheric pressure boiling point of a liquid (also known as the normal boiling point) is the temperature at which the vapor pressure equals the ambient atmospheric pressure. With any incremental increase in that temperature, the vapor pressure becomes sufficient to overcome atmospheric pressure and lift the liquid to form vapour bubbles inside the bulk of the substance. Bubble formation deeper in the liquid requires a higher pressure, and therefore higher temperature, because the fluid pressure increases above the atmospheric pressure as the depth increases.

The vapor pressure that a single component in a mixture contributes to the total pressure in the system is called partial vapor pressure.

When a person swims under the water, water pressure is felt acting on the person's eardrums. The deeper that person swims, the greater the pressure. The pressure felt is due to the weight of the water above the person. As someone swims deeper, there is more water above the person and therefore greater pressure. The pressure a liquid exerts depends on its depth.

Liquid pressure also depends on the density of the liquid. If someone was submerged in a liquid more dense than water, the pressure would be correspondingly greater. Thus, we can say that the depth, density and liquid pressure are directly proportionate. The pressure due to a liquid in liquid columns of constant density or at a depth within a substance is represented by the following formula:

where:

Another way of saying the same formula is the following:

The pressure a liquid exerts against the sides and bottom of a container depends on the density and the depth of the liquid. If atmospheric pressure is neglected, liquid pressure against the bottom is twice as great at twice the depth; at three times the depth, the liquid pressure is threefold; etc. Or, if the liquid is two or three times as dense, the liquid pressure is correspondingly two or three times as great for any given depth. Liquids are practically incompressible – that is, their volume can hardly be changed by pressure (water volume decreases by only 50 millionths of its original volume for each atmospheric increase in pressure). Thus, except for small changes produced by temperature, the density of a particular liquid is practically the same at all depths.

Atmospheric pressure pressing on the surface of a liquid must be taken into account when trying to discover the "total" pressure acting on a liquid. The total pressure of a liquid, then, is "ρgh" plus the pressure of the atmosphere. When this distinction is important, the term "total pressure" is used. Otherwise, discussions of liquid pressure refer to pressure without regard to the normally ever-present atmospheric pressure.

The pressure does not depend on the "amount" of liquid present. Volume is not the important factor – depth is. The average water pressure acting against a dam depends on the average depth of the water and not on the volume of water held back. For example, a wide but shallow lake with a depth of exerts only half the average pressure that a small deep pond does. (The "total force" applied to the longer dam will be greater, due to the greater total surface area for the pressure to act upon. But for a given -wide section of each dam, the deep water will apply one quarter the force of deep water). A person will feel the same pressure whether his/her head is dunked a metre beneath the surface of the water in a small pool or to the same depth in the middle of a large lake. If four vases contain different amounts of water but are all filled to equal depths, then a fish with its head dunked a few centimetres under the surface will be acted on by water pressure that is the same in any of the vases. If the fish swims a few centimetres deeper, the pressure on the fish will increase with depth and be the same no matter which vase the fish is in. If the fish swims to the bottom, the pressure will be greater, but it makes no difference what vase it is in. All vases are filled to equal depths, so the water pressure is the same at the bottom of each vase, regardless of its shape or volume. If water pressure at the bottom of a vase were greater than water pressure at the bottom of a neighboring vase, the greater pressure would force water sideways and then up the narrower vase to a higher level until the pressures at the bottom were equalized. Pressure is depth dependent, not volume dependent, so there is a reason that water seeks its own level.

Restating this as energy equation, the energy per unit volume in an ideal, incompressible liquid is constant throughout its vessel. At the surface, gravitational potential energy is large but liquid pressure energy is low. At the bottom of the vessel, all the gravitational potential energy is converted to pressure energy. The sum of pressure energy and gravitational potential energy per unit volume is constant throughout the volume of the fluid and the two energy components change linearly with the depth. Mathematically, it is described by Bernoulli's equation, where velocity head is zero and comparisons per unit volume in the vessel are

Terms have the same meaning as in section Fluid pressure.

An experimentally determined fact about liquid pressure is that it is exerted equally in all directions. If someone is submerged in water, no matter which way that person tilts his/her head, the person will feel the same amount of water pressure on his/her ears. Because a liquid can flow, this pressure isn't only downward. Pressure is seen acting sideways when water spurts sideways from a leak in the side of an upright can. Pressure also acts upward, as demonstrated when someone tries to push a beach ball beneath the surface of the water. The bottom of a boat is pushed upward by water pressure (buoyancy).

When a liquid presses against a surface, there is a net force that is perpendicular to the surface. Although pressure doesn't have a specific direction, force does. A submerged triangular block has water forced against each point from many directions, but components of the force that are not perpendicular to the surface cancel each other out, leaving only a net perpendicular point. This is why water spurting from a hole in a bucket initially exits the bucket in a direction at right angles to the surface of the bucket in which the hole is located. Then it curves downward due to gravity. If there are three holes in a bucket (top, bottom, and middle), then the force vectors perpendicular to the inner container surface will increase with increasing depth – that is, a greater pressure at the bottom makes it so that the bottom hole will shoot water out the farthest. The force exerted by a fluid on a smooth surface is always at right angles to the surface. The speed of liquid out of the hole is formula_23, where "h" is the depth below the free surface. This is the same speed the water (or anything else) would have if freely falling the same vertical distance "h".

is the kinematic pressure, where formula_2 is the pressure and formula_26 constant mass density. The SI unit of "P" is m/s. Kinematic pressure is used in the same manner as kinematic viscosity formula_27 in order to compute the Navier–Stokes equation without explicitly showing the density formula_26.




</doc>
<doc id="23621" url="https://en.wikipedia.org/wiki?curid=23621" title="Polygon">
Polygon

In geometry, a polygon () is a plane figure that is described by a finite number of straight line segments connected to form a closed polygonal chain or "polygonal circuit". The solid plane region, the bounding circuit, or the two together, may be called a polygon.

The segments of a polygonal circuit are called its "edges" or "sides", and the points where two edges meet are the polygon's "vertices" (singular: vertex) or "corners". The interior of a solid polygon is sometimes called its "body". An "n"-gon is a polygon with "n" sides; for example, a triangle is a 3-gon.

A simple polygon is one which does not intersect itself. Mathematicians are often concerned only with the bounding polygonal chains of simple polygons and they often define a polygon accordingly. A polygonal boundary may be allowed to cross over itself, creating star polygons and other self-intersecting polygons.

A polygon is a 2-dimensional example of the more general polytope in any number of dimensions. There are many more generalizations of polygons defined for different purposes.

The word "polygon" derives from the Greek adjective πολύς ("polús") "much", "many" and γωνία ("gōnía") "corner" or "angle". It has been suggested that γόνυ ("gónu") "knee" may be the origin of "gon".

Polygons are primarily classified by the number of sides. See the table below.

Polygons may be characterized by their convexity or type of non-convexity:



Euclidean geometry is assumed throughout.

Any polygon has as many corners as it has sides. Each corner has several angles. The two most important ones are:

In this section, the vertices of the polygon under consideration are taken to be formula_6 in order. For convenience in some formulas, the notation will also be used.

If the polygon is non-self-intersecting (that is, simple), the signed area is
or, using determinants
where formula_9 is the squared distance between formula_10 and formula_11 

The signed area depends on the ordering of the vertices and of the orientation of the plane. Commonly, the positive orientation is defined by the (counterclockwise) rotation that maps the positive -axis to the positive -axis. If the vertices are ordered counterclockwise (that is, according to positive orientation), the signed area is positive; otherwise, it is negative. In either case, the area formula is correct in absolute value. This is commonly called the shoelace formula or Surveyor's formula.

The area "A" of a simple polygon can also be computed if the lengths of the sides, "a", "a", ..., "a" and the exterior angles, "θ", "θ", ..., "θ" are known, from:
The formula was described by Lopshits in 1963.

If the polygon can be drawn on an equally spaced grid such that all its vertices are grid points, Pick's theorem gives a simple formula for the polygon's area based on the numbers of interior and boundary grid points: the former number plus one-half the latter number, minus 1.

In every polygon with perimeter "p" and area "A ", the isoperimetric inequality formula_13 holds.

For any two simple polygons of equal area, the Bolyai–Gerwien theorem asserts that the first can be cut into polygonal pieces which can be reassembled to form the second polygon.

The lengths of the sides of a polygon do not in general determine its area. However, if the polygon is cyclic then the sides "do" determine the area. Of all "n"-gons with given side lengths, the one with the largest area is cyclic. Of all "n"-gons with a given perimeter, the one with the largest area is regular (and therefore cyclic).

Many specialized formulas apply to the areas of regular polygons.

The area of a regular polygon is given in terms of the radius "r" of its inscribed circle and its perimeter "p" by 
This radius is also termed its apothem and is often represented as "a".

The area of a regular "n"-gon with side "s" inscribed in a unit circle is

The area of a regular "n"-gon in terms of the radius "R" of its circumscribed circle and its perimeter "p" is given by

The area of a regular "n"-gon inscribed in a unit-radius circle, with side "s" and interior angle formula_17 can also be expressed trigonometrically as

The area of a self-intersecting polygon can be defined in two different ways, giving different answers:

Using the same convention for vertex coordinates as in the previous section, the coordinates of the centroid of a solid simple polygon are 
In these formulas, the signed value of area formula_21 must be used.

For triangles (), the centroids of the vertices and of the solid shape are the same, but, in general, this is not true for . The centroid of the vertex set of a polygon with vertices has the coordinates

The idea of a polygon has been generalized in various ways. Some of the more important include:

The word "polygon" comes from Late Latin "polygōnum" (a noun), from Greek πολύγωνον ("polygōnon/polugōnon"), noun use of neuter of πολύγωνος ("polygōnos/polugōnos", the masculine adjective), meaning "many-angled". Individual polygons are named (and sometimes classified) according to the number of sides, combining a Greek-derived numerical prefix with the suffix "-gon", e.g. "pentagon", "dodecagon". The triangle, quadrilateral and nonagon are exceptions.

Beyond decagons (10-sided) and dodecagons (12-sided), mathematicians generally use numerical notation, for example 17-gon and 257-gon.

Exceptions exist for side counts that are more easily expressed in verbal form (e.g. 20 and 30), or are used by non-mathematicians. Some special polygons also have their own names; for example the regular star pentagon is also known as the pentagram.
To construct the name of a polygon with more than 20 and less than 100 edges, combine the prefixes as follows. The "kai" term applies to 13-gons and higher and was used by Kepler, and advocated by John H. Conway for clarity to concatenated prefix numbers in the naming of quasiregular polyhedra.

Polygons have been known since ancient times. The regular polygons were known to the ancient Greeks, with the pentagram, a non-convex regular polygon (star polygon), appearing as early as the 7th century B.C. on a krater by Aristophanes, found at Caere and now in the Capitoline Museum.

The first known systematic study of non-convex polygons in general was made by Thomas Bradwardine in the 14th century.

In 1952, Geoffrey Colin Shephard generalized the idea of polygons to the complex plane, where each real dimension is accompanied by an imaginary one, to create complex polygons.

Polygons appear in rock formations, most commonly as the flat facets of crystals, where the angles between the sides depend on the type of mineral from which the crystal is made.

Regular hexagons can occur when the cooling of lava forms areas of tightly packed columns of basalt, which may be seen at the Giant's Causeway in Northern Ireland, or at the Devil's Postpile in California.

In biology, the surface of the wax honeycomb made by bees is an array of hexagons, and the sides and base of each cell are also polygons.

In computer graphics, a polygon is a primitive used in modelling and rendering. They are defined in a database, containing arrays of vertices (the coordinates of the geometrical vertices, as well as other attributes of the polygon, such as color, shading and texture), connectivity information, and materials.

Any surface is modelled as a tessellation called polygon mesh. If a square mesh has points (vertices) per side, there are "n" squared squares in the mesh, or 2"n" squared triangles since there are two triangles in a square. There are vertices per triangle. Where "n" is large, this approaches one half. Or, each vertex inside the square mesh connects four edges (lines).

The imaging system calls up the structure of polygons needed for the scene to be created from the database. This is transferred to active memory and finally, to the display system (screen, TV monitors etc.) so that the scene can be viewed. During this process, the imaging system renders polygons in correct perspective ready for transmission of the processed data to the display system. Although polygons are two-dimensional, through the system computer they are placed in a visual scene in the correct three-dimensional orientation.

In computer graphics and computational geometry, it is often necessary to determine whether a given point "P" = ("x","y") lies inside a simple polygon given by a sequence of line segments. This is called the point in polygon test.





</doc>
<doc id="23622" url="https://en.wikipedia.org/wiki?curid=23622" title="Player character">
Player character

A player character (also known as PC and playable character) is a fictional character in a video game or tabletop role-playing game whose actions are directly controlled by a player of the game rather than the rules of the game. The characters that are not controlled by a player are called non-player characters (NPCs). The actions of non-player characters are typically handled by the game itself in video games, or according to rules followed by a gamemaster refereeing tabletop role-playing games. The player character functions as a fictional, alternate body for the player controlling the character.

Video games typically have one player character for each person playing the game. Some games, such as multiplayer online battle arena, hero shooter and fighting games, offer a group of player characters for the player to choose from, allowing the player to control one of them at a time. Where more than one player character is available, the characters may have different abilities, strengths, and weaknesses to make the game play style different.

A player character may sometimes be based on a real person, especially in sports games that use the names and likenesses of real sports people. Historical people and leaders may sometimes appear as characters too, particularly in strategy or empire building games such as in Sid Meier's "Civilization" series. Such a player character is more properly an avatar as the player character's name and image typically have little bearing on the game itself. Avatars are also commonly seen in casino game simulations.

In many video games, and especially first-person shooters, the player character is a "blank slate" without any notable characteristics or even backstory. Pac-Man, Crono, Link and Chell are examples of such characters. These characters are generally silent protagonists.

Some games will go even further, never showing or naming the player-character at all. This is somewhat common in first-person videogames, such as in "Myst", but is more often done in strategy video games such as "Dune 2000," "," and "Command & Conquer" series. In such games, the only real indication that the player has a character (instead of an omnipresent status), is from the cutscenes during which the character is being given a mission briefing or debriefing; the player is usually addressed as "general", "commander", or another military rank.

In gaming culture, such a character was called Ageless, Faceless, Gender-Neutral, Culturally Ambiguous Adventure Person, abbreviated as AFGNCAAP; a term that originated in "" where it is used satirically to refer to the player.

Fighting games typically have a larger number of player characters to choose from, with some basic moves available to all or most characters and some unique moves only available to one or a few characters. Having many different characters to play as and against, all possessing different moves and abilities, is necessary to create a larger gameplay variety in such games.

Similar to fighting games, MOBAs offer a large group of viable player characters for the player to choose from, each of which having different abilities, strengths, and weaknesses to make the game play style different. Choosing a character who complements player's teammates and counters his opponents, opens up a strategy before the beginning of the match itself. Playable characters blend a variety of fantasy tropes, such as high fantasy, dark fantasy, science fiction, sword and sorcery, Lovecraftian horror, cyberpunk and steampunk, featuring numerous references to popular culture and mythology.

In role playing games such as "Dungeons & Dragons" or "Final Fantasy," a player typically creates or takes on the identity of a character that may have nothing in common with the player. The character is often of a certain (usually fictional) race and class (such as zombie, berserker, rifleman, elf, or cleric), each with strengths and weaknesses. The attributes of the characters (such as magic and fighting ability) are given as numerical values which can be increased as the gamer progresses and gains rank and experience points through accomplishing goals or fighting enemies.

A secret or unlockable character is a playable character in a video game available only after either completing the game or meeting another requirement. In some video games, characters that are not secret but appear only as non-player characters like bosses or enemies become playable characters after completing certain requirements, or sometimes cheating.



</doc>
<doc id="23623" url="https://en.wikipedia.org/wiki?curid=23623" title="Parish">
Parish

A parish is a territorial entity in many Christian denominations, constituting a division within a diocese. A parish is under the pastoral care and clerical jurisdiction of a priest, often termed a parish priest, who might be assisted by one or more curates, and who operates from a parish church. Historically, a parish often covered the same geographical area as a manor. Its association with the parish church remains paramount.

By extension the term "parish" refers not only to the territorial entity but to the people of its community or congregation as well as to church property within it. In England this church property was technically in ownership of the parish priest "ex-officio", vested in him on his institution to that parish.

First attested in English in the late, 13th century, the word "parish" comes from the Old French "paroisse", in turn from , the latinisation of the , "sojourning in a foreign land", itself from ("paroikos"), "dwelling beside, stranger, sojourner", which is a compound of ("pará"), "beside, by, near" and οἶκος ("oîkos"), "house".

As an ancient concept, the term "parish" occurs in the long-established Christian denominations: Catholic, Anglican Communion, the Eastern Orthodox Church, and Lutheran churches, and in some Methodist, Congregationalist and Presbyterian administrations.

The eighth Archbishop of Canterbury Theodore of Tarsus (c. 602–690) appended the parish structure to the Anglo-Saxon township unit, where it existed, and where minsters catered to the surrounding district.

Broadly speaking, the parish is the standard unit in episcopal polity of church administration, although parts of a parish may be subdivided as a "chapelry", with a chapel of ease or filial church serving as the local place of worship in cases of difficulty to access the main parish church.

In the wider picture of ecclesiastical polity, a "parish" comprises a division of a diocese or see. Parishes within a diocese may be grouped into a deanery or "vicariate forane" (or simply "vicariate"), overseen by a dean or "vicar forane", or in some cases by an archpriest. Some churches of the Anglican Communion have deaneries as units of an archdeaconry.
An outstation is a newly-created congregation, a term usually used where the church is evangelical, or a mission and particularly in African countries, but also historically in Australia. They exist mostly within the Catholic and Anglican parishes.

The Anglican Diocese of Cameroon describes their outstations as the result of outreach work "initiated, sponsored and supervised by the mother parishes". Once there is a big enough group of worshippers in the same place, the outstation in named by the bishop of the diocese. They are run by "catechists/evangelists" or lay readers, and supervised by the creator parish or archdeaconry.

Outstations are not self-supporting, and in poor areas often consist of a very simple structure. The parish priest visits as often as possible. If and when the community has grown enough, the outstation may become a parish and have a parish priest assigned to it.

The Assemblies of God denomination has churches and outstations throughout the world.

The Church of England geographical structure uses the local parish church as its basic unit. The parish system survived the Reformation with the Anglican Church's secession from Rome remaining largely untouched, thus it shares its roots with the Catholic Church's system described above. Parishes may extend into different counties or hundreds and historically many parishes comprised extra outlying portions in addition to its principal district, usually being described as 'detached' and intermixed with the lands of other parishes. Church of England parishes nowadays all lie within one of 44 dioceses divided between the provinces of Canterbury, 30 and York, 14.

Each parish normally has its own parish priest (either a vicar or rector, owing to the vagaries of the feudal tithe system: rectories usually having had greater income) and perhaps supported by one or more curates or deacons - although as a result of ecclesiastical pluralism some parish priests might have held more than one parish living, placing a curate in charge of those where they do not reside. Now, however, it is common for a number of neighbouring parishes to be placed under one benefice in the charge of a priest who conducts services by rotation, with additional services being provided by lay readers or other non-ordained members of the church community.

A chapelry was a subdivision of an ecclesiastical parish in England, and parts of Lowland Scotland up to the mid 19th century. It had a similar status to a township but was so named as it had a chapel which acted as a subsidiary place of worship to the main parish church.

In England civil parishes and their governing parish councils evolved in the 19th century as ecclesiastical parishes began to be relieved of what became considered to be civic responsibilities. Thus their boundaries began to diverge. The word "parish" acquired a secular usage. Since 1895, a parish council elected by public vote or a (civil) parish meeting administers a civil parish and is formally recognised as the level of local government below a district council.

The traditional structure of the Church of England with the parish as the basic unit has been exported to other countries and churches throughout the Anglican Communion and Commonwealth but does not necessarily continue to be administered in the same way.

The parish is also the basic level of church administration in the Church of Scotland. Spiritual oversight of each parish church in Scotland is responsibility of the congregation's Kirk Session. Patronage was regulated in 1711 (Patronage Act) and abolished in 1874, with the result that ministers must be elected by members of the congregation. Many parish churches in Scotland today are "linked" with neighbouring parish churches served by a single minister. Since the abolition of parishes as a unit of civil government in Scotland in 1929, Scottish parishes have purely ecclesiastical significance and the boundaries may be adjusted by the local Presbytery.

The church in Wales was disestablished in 1920 and is made up of six dioceses. Parishes were also civil administration areas until communities were established in 1974.

Although they are more often simply called congregations and have no geographic boundaries, in the United Methodist Church congregations are called parishes. A prominent example of this usage comes in "The Book of Discipline of The United Methodist Church", in which the committee of every local congregation that handles staff support is referred to as the committee on Pastor-Parish Relations. This committee gives recommendations to the bishop on behalf of the parish/congregation since it is the United Methodist Bishop of the episcopal area who appoints a pastor to each congregation. The same is true in the African Methodist Episcopal Church and the Christian Methodist Episcopal Church.

In New Zealand, a local grouping of Methodist churches that share one or more ministers (which in the United Kingdom would be called a circuit) is referred to as a parish.

In the Catholic Church, each parish normally has its own parish priest (in some countries called pastor), who has responsibility and canonical authority over the parish.

What in most English-speaking countries is termed the "parish priest" is referred to as the "pastor" in the United States, where the term "parish priest" is used of any priest assigned to a parish even in a subordinate capacity. These are called "assistant priests", "parochial vicars", "curates", or, in the United States, "associate pastors" and "assistant pastors".

Each diocese (administrative region) is divided into parishes, each with their own central church called the parish church, where religious services take place. Some larger parishes or parishes that have been combined under one parish priest may have two or more such churches, or the parish may be responsible for chapels (or chapels of ease) located at some distance from the mother church for the convenience of distant parishioners.

Normally, a parish comprises all Catholics living within its geographically defined area, but non-territorial parishes can also be established within a defined area on a personal basis for Catholics belonging to a particular rite, language, nationality, or community. An example is that of personal parishes established in accordance with the 7 July 2007 "motu proprio" "Summorum Pontificum" for those attached to the pre-Vatican II liturgy.

Most Catholic parishes are part of Latin Rite dioceses, which together cover the whole territory of a country. There can also be overlapping parishes of eparchies of Eastern Catholic Churches, personal ordinariates or military ordinariates. Parishes are generally territorial, but may be personal.





</doc>
