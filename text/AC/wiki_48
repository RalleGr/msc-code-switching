<doc id="28898" url="https://en.wikipedia.org/wiki?curid=28898" title="Special Operations Executive">
Special Operations Executive

The Special Operations Executive (SOE) was a secret British World War II organisation. It was officially formed on 22 July 1940 under Minister of Economic Warfare Hugh Dalton, from the amalgamation of three existing secret organisations. Its purpose was to conduct espionage, sabotage and reconnaissance in occupied Europe (and later, also in occupied Southeast Asia) against the Axis powers, and to aid local resistance movements.

Few people were aware of SOE's existence. Those who were part of it or liaised with it were sometimes referred to as the "Baker Street Irregulars", after the location of its London headquarters. It was also known as "Churchill's Secret Army" or the "Ministry of Ungentlemanly Warfare". Its various branches, and sometimes the organisation as a whole, were concealed for security purposes behind names such as the "Joint Technical Board" or the "Inter-Service Research Bureau", or fictitious branches of the Air Ministry, Admiralty or War Office.

SOE operated in all territories occupied or attacked by the Axis forces, except where demarcation lines were agreed with Britain's principal Allies (the United States and the Soviet Union). It also made use of neutral territory on occasion or made plans and preparations in case neutral countries were attacked by the Axis. The organisation directly employed or controlled more than 13,000 people, about 3,200 of whom were women.

After the war, the organisation was officially dissolved on 15 January 1946. On the wall of the west cloister of Westminster Abbey is the official memorial to all those who served in the SOE during the Second World War. It was unveiled on 13 February 1996 by Queen Elizabeth The Queen Mother. A further memorial to SOE's agents was unveiled in October 2009 on the Albert Embankment by Lambeth Palace in London.
The Valen√ßay SOE Memorial honours 104 SOE agents who lost their lives while working in France.

The organisation was formed from the merger of three existing secret departments, which had been formed shortly before the outbreak of the Second World War. Immediately after Germany annexed Austria (the "Anschluss") in March 1938, the Foreign Office created a propaganda organisation known as Department EH (after Electra House, its headquarters), run by Canadian newspaper magnate Sir Campbell Stuart. Later that month, the Secret Intelligence Service (SIS, also known as MI6) formed a section known as Section D, under Major Lawrence Grand RE, to investigate the use of sabotage, propaganda, and other irregular means to weaken an enemy. In the autumn of the same year, the War Office expanded an existing research department known as GS (R) and appointed Major J. C. Holland RE as its head to conduct research into guerrilla warfare. GS (R) was renamed MI(R) in early 1939.

These three departments worked with few resources until the outbreak of war. There was much overlap between their activities. Section D and EH duplicated much of each other's work. On the other hand, the heads of Section D and MI(R) knew each other and shared information. They agreed to a rough division of their activities; MI(R) researched irregular operations that could be undertaken by regular uniformed troops, while Section D dealt with truly undercover work.

During the early months of the war, Section D was based first at St Ermin's Hotel in Westminster and then the Metropole Hotel near Trafalgar Square. The Section attempted unsuccessfully to sabotage deliveries of vital strategic materials to Germany from neutral countries by mining the Iron Gate on the River Danube. MI(R) meanwhile produced pamphlets and technical handbooks for guerrilla leaders. MI(R) was also involved in the formation of the Independent Companies, autonomous units intended to carry out sabotage and guerrilla operations behind enemy lines in the Norwegian Campaign, and the Auxiliary Units, stay-behind commando units based on the Home Guard which would act in the event of an Axis invasion of Britain, as seemed possible in the early years of the war.

On 13 June 1940, at the instigation of newly appointed Prime Minister Winston Churchill, Lord Hankey (who held the Cabinet post of Chancellor of the Duchy of Lancaster) persuaded Section D and MI(R) that their operations should be coordinated. On 1 July, a Cabinet level meeting arranged the formation of a single sabotage organisation. On 16 July, Hugh Dalton, the Minister of Economic Warfare, was appointed to take political responsibility for the new organisation, which was formally created on 22 July 1940. Dalton recorded in his diary that on that day the War Cabinet agreed to his new duties and that Churchill had told him, "And now go and set Europe ablaze." Dalton used the Irish Republican Army (IRA) during the Irish War of Independence as a model for the organisation.

Sir Frank Nelson was nominated by SIS to be director of the new organisation, and a senior civil servant, Gladwyn Jebb, transferred from the Foreign Office to it, with the title of Chief Executive Officer. Campbell Stuart left the organisation, and the flamboyant Major Grand was returned to the regular army. At his own request, Major Holland also left to take up a regular appointment in the Royal Engineers. (Both Grand and Holland eventually attained the rank of major-general.) However, Holland's former deputy at MI(R), Brigadier Colin Gubbins, returned from command of the Auxiliary Units to be Director of Operations of SOE.

One department of MI(R), MI R(C), which was involved in the development of weapons for irregular warfare, was not formally integrated into SOE but became an independent body codenamed MD1. Directed by Major (later Lieutenant Colonel) Millis Jefferis, it was located at The Firs in Whitchurch and nicknamed "Churchill's Toyshop" from the Prime Minister's close interest in it and his enthusiastic support.

The director of SOE was usually referred to by the initials "CD". Nelson, the first director to be appointed, was a former head of a trading firm in India, a back bench Conservative Member of Parliament and Consul in Basel, Switzerland. There he had also been engaged in undercover intelligence work.

In February 1942 Dalton became President of the Board of Trade and was replaced as Minister of Economic Warfare by Lord Selborne. Selborne in turn retired Nelson, who had suffered ill health as a result of his hard work, and appointed Sir Charles Hambro, head of Hambros Bank, to replace him. He also transferred Jebb back to the Foreign Office.

Hambro had been a close friend of Churchill before the war and had won the Military Cross in the First World War. He retained several other interests, for example remaining chairman of Hambros and a director of the Great Western Railway. Some of his subordinates and associates expressed reservations that these interests distracted him from his duties as director. Selborne and Hambro nevertheless cooperated closely until August 1943, when they fell out over the question of whether SOE should remain a separate body or co-ordinate its operations with those of the British Army in several theatres of war. Hambro felt that any loss of autonomy would cause a number of problems for SOE in the future. At the same time, Hambro was found to have failed to pass on vital information to Selborne. He was dismissed as director, and became head of a raw materials purchasing commission in Washington, D.C., which was involved in the exchange of nuclear information.
As part of the subsequent closer ties between the Imperial General Staff and SOE (although SOE had no representation on the Chiefs of Staff Committee), Hambro's replacement as director from September 1943 was Gubbins, now a Major-general. Gubbins had wide experience of commando and clandestine operations and had played a major part in MI(R)'s and SOE's early operations. He also put into practice many of the lessons he learned from the IRA during the Irish War of Independence.

The organisation of SOE continually evolved and changed during the war. Initially, it consisted of three broad departments: SO1, which dealt with propaganda; SO2 (operations); and SO3 (research). SO3 was quickly overloaded with paperwork and was merged into SO2. In August 1941, following quarrels between the Ministry of Economic Warfare and the Ministry of Information over their relative responsibilities, SO1 was removed from SOE and became an independent organisation, the Political Warfare Executive.

Thereafter, a single, broad "Operations" department controlled the Sections operating into enemy and sometimes neutral territory, and the selection and training of agents. Sections, usually referred to by code letters or groups of letters, were assigned to a single country. Some enemy-occupied countries had two or more sections assigned to deal with politically disparate resistance movements. (France had no less than six). For security purposes, each section had its own headquarters and training establishments. This strict compartmentalisation was so effective that in mid-1942 five governments in exile jointly suggested that a single sabotage organisation be created, and were startled to learn that SOE had been in existence for two years.

Four departments and some smaller groups were controlled by the director of scientific research, Professor Dudley Maurice Newitt, and were concerned with the development or acquisition and production of special equipment. A few other sections were involved with finance, security, economic research and administration, although SOE had no central registry or filing system. When Gubbins was appointed director, he formalised some of the administrative practices which had grown in an "ad hoc" fashion and appointed an establishment officer to oversee the manpower and other requirements of the various departments.

The main controlling body of SOE was its council, consisting of around fifteen heads of departments or sections. About half of the council were from the armed forces (although some were specialists who were only commissioned after the outbreak of war), the rest were various civil servants, lawyers, or business or industrial experts. Most of the members of the council, and the senior officers and functionaries of SOE generally, were recruited by word of mouth among public school alumni and Oxbridge graduates, although this did not notably affect SOE's political complexion.

Several subsidiary SOE headquarters and stations were set up to manage operations which were too distant for London to control directly. SOE's operations in the Middle East and Balkans were controlled from a headquarters in Cairo, which was notorious for poor security, infighting and conflicts with other agencies. It finally became known in April 1944 as Special Operations (Mediterranean), or SO(M). Shortly after the Allied landings in North Africa, a station codenamed ""Massingham"" was established near Algiers in late 1942, which operated into Southern France. Following the Allied invasion of Italy, personnel from "Massingham" established forward stations in Brindisi and near Naples. A subsidiary headquarters initially known as "Force 133" was later set up in Bari in Southern Italy, under the Cairo headquarters, to control operations in the Balkans and Northern Italy.

An SOE station, which was first called the "India Mission", and was subsequently known as "GS I(k)" was set up in India late in 1940. It subsequently moved to Ceylon so as to be closer to the headquarters of the Allied South East Asia Command and became known as Force 136. A "Singapore Mission" was set up at the same time as the India Mission but was unable to overcome official opposition to its attempts to form resistance movements in Malaya before the Japanese overran Singapore. Force 136 took over its surviving staff and operations.

New York City also had a branch office, formally titled British Security Coordination, and headed by the Canadian businessman Sir William Stephenson. This office, located at Room 3603, 630 Fifth Avenue, Rockefeller Center, coordinated the work of SOE, SIS and MI5 with the American FBI and Office of Strategic Services.

As with its leadership and organisation, the aims and objectives of SOE changed throughout the war, although they revolved around sabotaging and subverting the Axis war machines through indirect methods. SOE occasionally carried out operations with direct military objectives, such as Operation Harling, originally designed to cut one of the Axis supply lines to their troops fighting in North Africa. They also carried out some high-profile operations aimed mainly at the morale both of the Axis and occupied nations, such as Operation Anthropoid, the assassination in Prague of Reinhard Heydrich. In general also, SOE's objectives were to foment mutual hatred between the population of Axis-occupied countries and the occupiers, and to force the Axis to expend manpower and resources on maintaining their control of subjugated populations.

Dalton's early enthusiasm for fomenting widespread strikes, civil disobedience and nuisance sabotage in Axis-occupied areas had to be curbed. Thereafter, there were two main aims, often mutually incompatible; sabotage of the Axis war effort, and the creation of secret armies which would rise up to assist the liberation of their countries when Allied troops arrived or were about to do so. It was recognised that acts of sabotage would bring about reprisals and increased Axis security measures which would hamper the creation of underground armies. As the tide of war turned in the Allies' favour, these underground armies became more important.

At the government level, SOE's relationships with the Foreign Office were often difficult. On several occasions, various governments in exile protested at operations taking place without their knowledge or approval, provoking Axis reprisals against civilian populations, or complained about SOE's support for movements opposed to the exiled governments. SOE's activities also threatened relationships with neutral countries. SOE nevertheless generally adhered to the rule, ""No bangs without Foreign Office approval.""

Early attempts at bureaucratic control of Jefferis's MIR(c) by the Ministry of Supply were eventually foiled by Churchill's intervention. Thereafter, they co-operated, though at arm's length, with Dudley Newitt's various supply and development departments. The Treasury were accommodating from the start and were often prepared to turn a blind eye to some of SOE's questionable activities.

With other military headquarters and commands, SOE cooperated fairly well with Combined Operations Headquarters during the middle years of the war, usually on technical matters as SOE's equipment was readily adopted by commandos and other raiders. This support was lost when Vice Admiral Louis Mountbatten left Combined Operations, though by this time SOE had its own transport and had no need to rely on Combined Operations for resources. On the other hand, the Admiralty objected to SOE developing its own underwater vessels, and the duplication of effort this involved. The Royal Air Force, and in particular RAF Bomber Command under "Bomber" Harris were usually reluctant to allocate aircraft to SOE.

Towards the end of the war, as Allied forces began to liberate territories occupied by the Axis and in which SOE had established resistance forces, SOE also liaised with and to some extent came under the control of the Allied theatre commands. Relationships with Supreme Headquarters Allied Expeditionary Force in north-west Europe (whose commander was General Dwight D. Eisenhower) and South East Asia Command (whose commander was Admiral Louis Mountbatten, already well known to SOE) were generally excellent. However, there were difficulties with the Commanders in Chief in the Mediterranean, partly because of the complaints over impropriety at SOE's Cairo headquarters during 1941 and partly because both the supreme command in the Mediterranean and SOE's establishments were split in 1942 and 1943, leading to divisions of responsibility and authority.

There was tension between SOE and SIS, which the Foreign Office controlled. Where SIS preferred placid conditions in which it could gather intelligence and work through influential persons or authorities, SOE was intended to create unrest and turbulence, and often backed anti-establishment organisations, such as the Communists, in several countries. At one stage, SIS actively hindered SOE's attempts to infiltrate agents into enemy-occupied France.

Even before the United States joined the war, the head of the newly formed Office of the Coordinator of Information (COI), William J. Donovan, had received technical information from SOE and had arranged for some members of his organisation to undergo training at a camp run by SOE in Oshawa in Canada. In early 1942, Donovan's organisation became the Office of Strategic Services. SOE and OSS worked out respective areas of operation: OSS's exclusive sphere included China (including Manchuria), Korea and Australia, the Atlantic islands and Finland. SOE retained India, the Middle East and East Africa, and the Balkans. While the two services both worked in Western Europe, it was expected that SOE would be the leading partner.

In the middle of the war, the relations between SOE and OSS were not often smooth. They established a joint headquarters in Algiers but the officers of the two organisations working there refused to share information with each other. In the Balkans, and Yugoslavia especially, SOE and OSS several times worked at cross-purposes, reflecting their governments' differing (and changing) attitudes to the Partisans and Chetniks. However, in 1944 SOE and OSS successfully pooled their personnel and resources to mount Operation Jedburgh, providing large scale support to the French Resistance following the Normandy landings.

SOE had some nominal contact with the Soviet NKVD, but this was limited to a single liaison officer at each other's headquarters.

After working from temporary offices in Central London, the headquarters of SOE was moved on 31 October 1940 into 64 Baker Street (hence the nickname ""the Baker Street Irregulars""). Ultimately, SOE occupied much of the western side of Baker Street. "Baker Street" became the euphemistic way of referring to SOE. The precise nature of the buildings remained concealed; it had no entry in the telephone directories, and correspondence to external bodies bore service addresses; MO1 (SP) (a War Office branch), NID(Q) (Admiralty), AI10 (Air Ministry), or other fictitious bodies or civilian companies.

SOE maintained a large number of training, research and development or administrative centres. It was a joke that ""SOE"" stood for ""Stately 'omes of England"", after the large number of country houses and estates it requisitioned and used.

The establishments connected with experimentation and production of equipment were mainly concentrated in and around Hertfordshire and were designated by roman numbers. The main weapons and devices research establishments were The Firs, the home of MD1 near Aylesbury in Buckinghamshire (although this was not formally part of SOE), and Station IX at The Frythe, a country house (and former private hotel) outside Welwyn Garden City where, under the cover name of ISRB (Inter Services Research Bureau), SOE developed radios, weapons, explosive devices and booby traps.

Section D originally had a research station at Bletchley Park, which also held the Government Code and Cipher School, until in November 1940 it was decided that it was unwise to conduct codebreaking and explosives experiments on the same site. The establishment moved to Aston House near Stevenage in Hertfordshire and was renamed Station XII. It originally conducted research and development but from 1941 it became a production, storage and distribution centre for devices already developed.

Station XV, at the Thatched Barn near Borehamwood, was devoted to camouflage, which usually meant equipping agents with authentic local clothing and personal effects. Various sub-stations in London were also involved in this task. Station XV and other camouflage sections also devised methods of hiding weapons, explosives or radios in innocuous-seeming items.

Agents also needed identity papers, ration cards, currency and so on. Station XIV, at Briggens House near Roydon in Essex, was originally the home of STS38, a training facility for Polish saboteurs, ) who set up their own forgery section. As the work expanded, it became the central forgery department for SOE and the Poles eventually moved out on 1 April 1942. The technicians at Station XIV included a number of ex-convicts.

The training establishments, and properties used by country sections, were designated by Arabic numbers and were widely distributed. The initial training centres of the SOE were at country houses such as Wanborough Manor, Guildford. Agents destined to serve in the field underwent commando training at Arisaig in Scotland, where they were taught armed and unarmed combat skills by William E. Fairbairn and Eric A. Sykes, former Inspectors in the Shanghai Municipal Police. Those who passed this course received parachute training by STS 51 and 51a situated near Altrincham, Cheshire with the assistance of No.1 Parachute Training School RAF, at RAF Ringway (which later became Manchester Airport). They then attended courses in security and Tradecraft at Group B schools around Beaulieu in Hampshire. Finally, depending on their intended role, they received specialist training in skills such as demolition techniques or Morse code telegraphy at various country houses in England.

SOE's Cairo branch established a commando and parachute training school numbered STS 102 at Ramat David near Haifa. This school trained agents who joined SOE from among the armed forces stationed in the Middle East, and also members of the Special Air Service and Greek Sacred Squadron.

A commando training centre similar to Arisaig and run by Fairbairn was later set up at Oshawa, for Canadian members of SOE and members of the newly created American organisation, the Office of Strategic Services.

A variety of people from all classes and pre-war occupations served SOE in the field. The backgrounds of agents in F Section, for example, ranged from aristocrats such as Polish-born Countess Krystyna Skarbek, and Noor Inayat Khan, the daughter of an Indian Sufi leader, to working-class people such as Violette Szabo, with some even reputedly from the criminal underworld.

In most cases, the primary quality required of an agent was a deep knowledge of the country in which he or she was to operate, and especially its language, if the agent was to pass as a native of the country. Dual nationality was often a prized attribute. This was particularly so of France. In other cases, especially in the Balkans, a lesser degree of fluency was required as the resistance groups concerned were already in open rebellion and a clandestine existence was unnecessary. A flair for diplomacy combined with a taste for rough soldiering was more necessary. Some regular army officers proved adept as envoys, although others (such as the former diplomat Fitzroy Maclean or the classicist Christopher Woodhouse) were commissioned only during wartime.

Several of SOE's agents were from the Jewish Parachutists of Mandate Palestine, many of whom were already √âmigr√©s from Nazi or other oppressive or anti-semitic regimes in Europe. Thirty-two of them served as agents in the field, seven of whom were captured and executed.

Exiled or escaped members of the armed forces of some occupied countries were obvious sources of agents. This was particularly true of Norway and the Netherlands. In other cases (such as Frenchmen owing loyalty to Charles de Gaulle and especially the Poles), the agents' first loyalty was to their leaders or governments in exile, and they treated SOE only as a means to an end. This could occasionally lead to mistrust and strained relations in Britain.

The organisation was prepared to ignore almost any contemporary social convention in its fight against the Axis. It employed known homosexuals, people with criminal records (some of whom taught skills such as picking locks) or bad conduct records in the armed forces, Communists and anti-British nationalists. Some of these might have been considered a security risk, but no known case exists of an SOE agent wholeheartedly going over to the enemy. The case of Henri D√©ricourt is an example in which the conduct of agents was questionable, but it was impossible to establish whether they were acting under secret orders from SOE or MI6.

SOE was also far ahead of contemporary attitudes in its use of women in armed combat. Although women were first considered only as couriers in the field or as wireless operators or administrative staff in Britain, those sent into the field were trained to use weapons and in unarmed combat. Most were commissioned into either the First Aid Nursing Yeomanry (FANY) or the Women's Auxiliary Air Force. Women often assumed leadership roles in the field. Pearl Witherington became the organiser (leader) of a highly successful resistance network in France. Early in the war, American Virginia Hall functioned as the unofficial nerve center of several SOE networks in Vichy France. Many women agents such as Odette Hallowes or Violette Szabo were decorated for bravery, posthumously in Szabo's case. Of SOE's 41 (or 39 in some estimates) female agents serving in Section F (France) sixteen did not survive with twelve killed or executed in Nazi concentration camps.

Most of the resistance networks which SOE formed or liaised with were controlled by radio directly from Britain or one of SOE's subsidiary headquarters. All resistance circuits contained at least one wireless operator, and all drops or landings were arranged by radio, except for some early exploratory missions sent "blind" into enemy-occupied territory.

At first, SOE's radio traffic went through the SIS-controlled radio station at Bletchley Park. From 1 June 1942 SOE used its own transmitting and receiving stations at Grendon Underwood in Buckinghamshire and Poundon nearby, as the location and topography were suitable. Teleprinters linked the radio stations with SOE's HQ in Baker Street. Operators in the Balkans worked to radio stations in Cairo.

SOE was highly dependent upon the security of radio transmissions, involving three factors: the physical qualities and capabilities of the radio sets, the security of the transmission procedures and the provision of proper ciphers.

SOE's first radios were supplied by SIS. They were large, clumsy and required large amounts of power. SOE acquired a few, much more suitable, sets from the Poles in exile, but eventually designed and manufactured their own, such as the Paraset, under the direction of Lieutenant Colonel F. W. Nicholls R. Sigs who had served with Gubbins between the wars. The A Mk III, with its batteries and accessories, weighed only , and could fit into a small attache case, although the B Mk II, otherwise known as the B2, which weighed , was required to work over ranges greater than about .

Operating procedures were insecure at first. Operators were forced to transmit verbose messages on fixed frequencies and at fixed times and intervals. This allowed German direction finding teams time to triangulate their positions. After several operators were captured or killed, procedures were made more flexible and secure. The SOE wireless operators were also known as "The Pianists".

As with their first radio sets, SOE's first ciphers were inherited from SIS. Leo Marks, SOE's chief cryptographer, was responsible for the development of better codes to replace the insecure poem codes. Eventually, SOE settled on single use ciphers, printed on silk. Unlike paper, which would be given away by rustling, silk would not be detected by a casual search if it was concealed in the lining of clothing.

The BBC also played its part in communications with agents or groups in the field. During the war, it broadcast to almost all Axis-occupied countries, and was avidly listened to, even at risk of arrest. The BBC included various "personal messages" in its broadcasts, which could include lines of poetry or apparently nonsensical items. They could be used to announce the safe arrival of an agent or message in London for example, or could be instructions to carry out operations on a given date. These were used for example to mobilise the resistance groups in the hours before Operation Overlord.

In the field, agents could sometimes make use of the postal services, though these were slow, not always reliable and letters were almost certain to be opened and read by the Axis security services. In training, agents were taught to use a variety of easily available substances to make invisible ink, though most of these could be detected by a cursory examination, or to hide coded messages in apparently innocent letters. The telephone services were even more certain to be intercepted and listened to by the enemy, and could be used only with great care.

The most secure method of communication in the field was by courier. In the earlier part of the war, most women sent as agents in the field were employed as couriers, on the assumption that they would be less likely to be suspected of illicit activities.

Although SOE used some suppressed assassination weapons such as the De Lisle carbine and the Welrod (specifically developed for SOE at Station IX), it took the view that weapons issued to resisters should not require extensive training in their use, or need careful maintenance. The crude and cheap Sten was a favourite. For issue to large forces such as the Yugoslav Partisans, SOE used captured German or Italian weapons. These were available in large quantities after the Tunisian and Sicilian campaigns and the surrender of Italy, and the partisans could acquire ammunition for these weapons (and the Sten) from enemy sources.

SOE also adhered to the principle that resistance fighters would be handicapped rather than helped by heavy equipment such as mortars or anti-tank guns. These were awkward to transport, almost impossible to conceal and required skilled and highly trained operators. Later in the war however, when resistance groups staged open rebellions against enemy occupation, some heavy weapons were dispatched, for example to the Maquis du Vercors. Weapons such as the British Army's standard Bren light machine gun were also supplied in such cases.

Most SOE agents received training on captured enemy weapons before being sent into enemy-occupied territory. Ordinary SOE agents were also armed with handguns acquired abroad, such as, from 1941, a variety of US pistols, and a large quantity of the Spanish Llama .38 ACP in 1944. Such was SOE's demand for weapons, a consignment of 8,000 Ballester‚ÄìMolina .45 calibre weapons was purchased from Argentina, apparently with the mediation of the US.

SOE agents were issued with the Fairbairn‚ÄìSykes fighting knife also issued to Commandos. For specialised operations or use in extreme circumstances, SOE issued small fighting knives which could be concealed in the heel of a hard leather shoe or behind a coat lapel. Given the likely fate of agents captured by the Gestapo, SOE also disguised suicide pills as coat buttons.

SOE developed a wide range of explosive devices for sabotage, such as limpet mines, shaped charges and time fuses, which were also widely used by commando units. Most of these devices were designed and produced at The Firs. The Time Pencil, invented by Commander A.J.G. Langley, the first commandant of Station XII at Aston was used to give a saboteur time to escape after setting a charge and was far simpler to carry and use than lighted fuses or electrical detonators. It relied on crushing an internal vial of acid which then corroded a retaining wire, which sometimes made it inaccurate in cold or hot conditions. Later the L-Delay, which instead allowed a lead retaining wire to "creep" until it broke and was less affected by the temperature, was introduced.

SOE pioneered the use of plastic explosive. (The term "plastique" comes from plastic explosive packaged by SOE and originally destined for France but taken to the United States instead.) Plastic explosive could be shaped and cut to perform almost any demolition task. It was also inert and required a powerful detonator to cause it to explode, and was therefore safe to transport and store. It was used in everything from car bombs, to exploding rats designed to destroy coal-fired boilers.

Other, more subtle sabotage methods included lubricants laced with grinding materials, intended for introduction into vehicle oil systems, railway wagon axle boxes, etc., incendiaries disguised as innocuous objects, explosive material concealed in coal piles to destroy locomotives, and land mines disguised as cow or elephant dung. On the other hand, some sabotage methods were extremely simple but effective, such as using sledgehammers to crack cast-iron mountings for machinery.

Station IX developed several miniature submersible craft. The Welman submarine and "Sleeping Beauty" were offensive weapons, intended to place explosive charges on or adjacent to enemy vessels at anchor. The Welman was used once or twice in action, but without success. The Welfreighter was intended to deliver stores to beaches or inlets, but it too was unsuccessful.

A sea trials unit was set up in West Wales at Goodwick, by Fishguard (station IXa) where these craft were tested. In late 1944 craft were dispatched to Australia to the Allied Intelligence Bureau (SRD), for tropical testing.

SOE also revived some medieval devices, such as the caltrop, which could be used to burst the tyres of vehicles or injure foot soldiers and crossbows powered by multiple rubber bands to shoot incendiary bolts. There were two types, known as ""Big Joe"" and ""Lil Joe"" respectively. They had tubular alloy skeleton stocks and were designed to be collapsible for ease of concealment.

An important section of SOE was the Operation Research and Trials Section, which was formally established in August 1943. The section had the responsibility both for issuing formal requirements and specifications to the relevant development and production sections, and for testing prototypes of the devices produced under field conditions. Over the period from 1 November 1943 to 1 November 1944, the section tested 78 devices. Some of these were weapons such as the Sleeve gun or fuses or adhesion devices to be used in sabotage, others were utility objects such as waterproof containers for stores to be dropped by parachute, or night glasses (lightweight binoculars with plastic lenses). Of the devices tested, 47% were accepted for use with little or no modification, 31% were accepted only after considerable modification and the remaining 22% were rejected.

Before SOE's research and development procedures were formalised in 1943, a variety of more or less useful devices were developed. Some of the more imaginative devices invented by SOE included exploding pens with enough explosive power to blast a hole in the bearer's body, or guns concealed in tobacco pipes, though there is no record of any of these being used in action. Station IX developed a miniature folding motorbike (the "Welbike") for use by parachutists, though this was noisy and conspicuous, used scarce petrol and was of little use on rough ground.

The continent of Europe was largely closed to normal travel. Although it was possible in some cases to cross frontiers from neutral countries such as Spain or Sweden, it was slow and there were problems over violating these countries' neutrality. SOE had to rely largely on its own air or sea transport for movement of people, arms and equipment.

SOE never had its own air force, but had to rely on the RAF for its planes. It was engaged in disputes with the RAF from its early days. In January 1941, an intended ambush (Operation Savanna) against the aircrew of a German "pathfinder" air group near Vannes in Brittany was thwarted when Air Vice Marshal Charles Portal, the Chief of the Air Staff, objected on moral grounds to parachuting what he regarded as assassins, although Portal's objections were later overcome and "Savanna" was mounted, unsuccessfully. From 1942, when Air Marshal Arthur Harris (""Bomber Harris"") became the Commander-in-Chief of RAF Bomber Command, he consistently resisted the diversion of the most capable types of bombers to SOE purposes.

SOE's first aircraft were two Armstrong Whitworth Whitleys belonging to 419 Flight RAF, which was formed in September 1940. In 1941, the flight was expanded to become No. 138 Squadron RAF. In February 1942, they were joined by No. 161 Squadron RAF. 161 Squadron flew agent insertions and pick-ups, while 138 Squadron delivered arms and stores by parachute. "C" flight from No. 138 Squadron later became No. 1368 Flight of the Polish Air Force, which joined No. 624 Squadron flying Halifaxes in the Mediterranean. By the later stages of the war several United States Army Air Forces squadrons were operating Douglas C-47 Skytrains in the Mediterranean, although by this time their operations had passed from SOE proper to the "Balkan Air Terminal Service". Three Special Duties squadrons operated in the Far East using a variety of aircraft, including the very long-range Consolidated B-24 Liberator.

Nos. 161 and 138 Squadrons were based at RAF Tempsford in Bedfordshire though No. 161 Squadron often moved forward to RAF Tangmere, close to the coast in West Sussex, to shorten their flights. The airfield at Tempsford became the RAF's most secret base. (Tempsford had been rejected for Bomber Command's purposes by Harris in March 1942, as it frequently became waterlogged.) RAF Tempsford was designed to look like an ordinary working farm. The SOE used Tangmere Cottage, opposite the main entrance to the base. SOE agents were lodged in a local hotel before being ferried to farm buildings, the "Gibraltar Farm" within the airfield's perimeter track. After final briefings and checks at the farm, the agents were issued firearms in the barn, and then boarded a waiting aircraft.

The squadrons' first task was to take agents to France who could select suitable fields for their aircraft. Most of these agents were French expatriates, some of whom had been pilots in the French Arm√©e de l'Air. Once the agent was in place and had selected a number of potential fields, 161 Squadron delivered SOE agents, wireless equipment and operators and weapons, and flew French political leaders, resistance leaders or their family members, and downed allied airmen to Britain. Between them, the two squadrons transported 101 agents to, and recovered 128 agents, diplomats and airmen from occupied France.

161 Squadron's principal aircraft was the Westland Lysander. It handled very well at low speed and could use landing grounds only long. It had a range of and could carry one to three passengers in the rear cockpit and stores in a pannier underneath the fuselage. It was flown by a single pilot, who also had to navigate, so missions had to be flown on clear nights with a full or near full moon. Bad weather often thwarted missions, German night fighters were also a hazard, and pilots could never know when landing whether they would be greeted by the resistance or the Gestapo.

The procedure once a Lysander reached its destination in France was described by Squadron Leader Hugh Verity. Once the aircraft reached the airfield the agent on the ground would signal the aircraft by flashing a prearranged code letter in Morse. The aircraft would respond by blinking back the appropriate code response letter. The agent and his men would then mark the field by lighting the three landing lights, which were flashlights attached to poles. The "A" lamp was at the base of the landing ground. 150 metres beyond it and into the wind was the "B" light, and 50 metres to the right of "B" was the "C" light. The three lights formed an inverted "L", with the "B" and "C" lights upwind from "A". With the code passed the pilot would land the aircraft. He then would taxi back to the "A" lamp, where the passengers would clamber down the fixed ladder to the ground, often while the pilot was making a slow U-turn. Before leaving the last passenger would hand off the luggage and then take aboard the outgoing luggage before climbing down the ladder as well. Then the outgoing passengers would climb aboard and the aircraft would take off. The whole exchange might take as little as 3 minutes.

The Lockheed Hudson had a range greater and could carry more passengers (ten or more), but required landing strips three times as long as those needed for the Lysander (450 yards vs. 150 yards). It carried a navigator, to ease the load on the pilot, and could also be fitted with navigational equipment such as the "Rebecca" homing receiver. The Hudson's use with 161 Squadron was developed by Charles Pickard and Hugh Verity. Pickard determined that the Hudson's stall speed was actually some 20¬†mph slower than its manual stated. Before it was first used on 13 January 1943, 161 Squadron had to send two Lysander aircraft in what they termed "a double" if larger parties needed to be picked up.

No. 138 Squadron's primary mission was the delivery of equipment, and occasionally agents, by parachute. It flew a variety of bomber-type aircraft, often modified with extra fuel tanks and flame-suppressing exhaust shrouds: the Armstrong Whitworth Whitley until November 1942, the Handley Page Halifax and later the Short Stirling. The Stirling could carry a very large load, but the aircraft with the longest range was the Halifax, which when based in Italy could reach drop zones as far away as eastern Poland.

Stores were usually parachuted in cylindrical containers. The "C" type was long, and when fully loaded could weigh up to . The "H" type was the same size overall but could be broken down into five smaller sections. This made it easier to carry and conceal but it could not be loaded with longer loads such as rifles. Some inert stores such as boots and blankets were "free-dropped" i.e. simply thrown out of the aircraft bundled together without a parachute, often to the hazard of any receiving committee on the ground.

Some devices used by SOE were designed specifically to guide aircraft to landing strips and dropping zones. Such sites could be marked by an agent on the ground with bonfires or bicycle lamps, but this required good visibility, as the pilot or navigator of a plane had not only to spot the ground signals, but also to navigate by visible landmarks to correct dead reckoning. Many landings or drops were thwarted by bad weather. To overcome these problems, SOE and Allied airborne forces used the Rebecca/Eureka transponding radar, which enabled a Hudson or larger aircraft to home in on a point on the ground even in thick weather. It was however difficult for agents or resistance fighters to carry or conceal the ground-based "Eureka" equipment.

SOE also developed the S-Phone, which allowed a pilot or radio operator aboard an aircraft to communicate by voice with the "reception committee". Sound quality was good enough for voices to be recognisable, so that a mission could be aborted in case of any doubts of an agent's identity.

SOE also experienced difficulties with the Royal Navy, who were usually unwilling to allow SOE to use its submarines or motor torpedo boats to deliver agents or equipment. Submarines were regarded as too valuable to risk within range of enemy coastal defences. They could also carry only small numbers of agents, in great discomfort, and could disembark stores only in small dinghies or canoes, which made it difficult to land large quantities of equipment. SOE nevertheless used them in the Indian Ocean where the distances made it impracticable to use any smaller craft. 

The vessels used by SOE during the early part of the war were clandestine craft such as fishing boats or caiques. They could pass muster as innocent local craft and carry large quantities of stores. They also had the advantage of being largely outside Admiralty control. However, SOE's first small craft organisation, which was set up in the Helford estuary, suffered from obstruction from SIS, which had a similar private navy nearby. Eventually, in spring 1943, the Admiralty created a Deputy Director of Operations (Irregular), to superintend all such private navies. This officer turned out to be the former commander of SIS's craft in the Helford estuary, but his successor in charge of SIS's Helford base cooperated much better with SOE's flotilla. 

While SIS and SOE (and MI9) landed and embarked several dozen agents, refugees and Allied aircrew, it was impossible to transport large quantities of arms and equipment inland from beaches in heavily patrolled coastal areas, until France was almost liberated. After the German occupation of Norway, many Norwegian merchant seamen and fishermen made their way to Britain. SOE recruited several to maintain communications to Norway, using fishing boats from a base in the Shetland Islands. The service became so reliable that it became known as the Shetland Bus. One of its boats and crews launched a daring but unsuccessful attack ("Operation Title") against the German battleship Tirpitz. 

A similar organisation ran missions to occupied Denmark (and neutral Sweden) from the east coast of Britain. The "Shetland Bus" was unable to operate only during the very long hours of daylight in the Arctic summer, because of the risk that the slow fishing boats would be attacked by patrolling German aircraft. Late in the war, the unit acquired three fast Submarine chasers for such missions. About the same time, SOE also acquired MTBs and Motor Gun Boats for the Helford flotilla. SOE also ran feluccas from Algiers into southern France and Corsica, and some ca√Øques in the Aegean.

In France, most agents were directed by two London-based country sections. F Section was under SOE control, while RF Section was linked to Charles de Gaulle's Free French Government in exile. Most native French agents served in RF. Two smaller sections also existed: EU/P Section, which dealt with the Polish community in France, and the DF Section which was responsible for establishing escape routes. During the latter part of 1942 another section known as AMF was established in Algiers, to operate into Southern France.

On 5 May 1941 Georges B√©gu√© (1911‚Äì1993) became the first SOE agent dropped into German occupied France. Between B√©gu√©'s first drop in May 1941 and August 1944, more than four hundred F Section agents were sent into occupied France, with Andr√©e Borrel (1919‚Äì1944) being the first woman parachuted into France on 24 September 1942. They served in a variety of functions including arms and sabotage instructors, couriers, circuit organisers, liaison officers and radio operators. RF sent about the same number; AMF sent 600 (although not all of these belonged to SOE). EU/P and DF sent a few dozen agents each.

Some networks were compromised, with the loss of many agents. In particular agents continued to be sent to the "Prosper" network headed by Francis Suttill for months after it had been controlled by Germans. The head of F Section, Maurice Buckmaster was blamed by many as he failed to see signs that the network was compromised.

To support the Allied invasion of France on D Day in June 1944, SOE and OSS supplemented their agents by air-dropping three-man parties of uniformed military personnel into France as part of Operation Jedburgh. They were to work with the French Resistance to co-ordinate widespread overt (as opposed to clandestine) acts of resistance. 100 men were eventually dropped, with 6,000 tons of military stores (4,000 tons had been dropped during the years before D-Day). At the same time, all the various sections operating in France (except EU/P) were nominally placed under a London-based HQ titled √âtat-major des Forces Fran√ßaises de l'Int√©rieur (EMFFI).

It was to take many weeks for a full assessment of the contributions of SOE and the Jedburgh teams to the Allied landings in Normandy, but when it came it vindicated Gubbins' belief that carefully planned sabotage could cripple a modern army. General Eisenhower's staff at the Supreme Headquarters of the Allied Expeditionary Force said that the Jedburghs had "succeeded in imposing more or less serious delays on all the division moved to Normandy". This had prevented Hitler from striking back in the crucial opening hours of Operation Overlord. Eisenhower's staff singled out the work of Tommy Macpherson and his two comrades-in-arms for particular praise. The most "outstanding example was the delay to the 2nd SS Panzer Division‚Äù, they said, and added a very personal endorsement, agreeing that the work carried out under Gubbins' leadership played a "very considerable part in our complete and final victory".

Many agents were captured, killed in action, executed, or died in German concentration camps. About one-third of 42 female agents of Section F did not survive the war; the death toll for more than 400 male agents was one-fourth and the toll of thousands of French people helping SOE agents and networks was about one-fifth.Of 119 SOE agents captured by the Germans and deported to concentration camps in Germany, only 23 men and three women survived.

SOE did not need to instigate Polish resistance, because unlike the Vichy French the Poles overwhelmingly refused to collaborate with the Nazis. Early in the war the Poles established the Polish Home Army, led by a clandestine resistance government known as the Polish Secret State. Nevertheless, many members of SOE were Polish and the SOE and the Polish resistance cooperated extensively.

SOE assisted the Polish government in exile with training facilities and logistical support for its 605 special forces operatives known as the Cichociemni, or ""The Dark and Silent"". Members of the unit, which was based in Audley End House, Essex, were rigorously trained before being parachuted into occupied Poland. Because of the distance involved in air travel to Poland, customised aircraft with extra fuel capacity were used in Polish operations such as Operation Wildhorn III. Sue Ryder chose the title Baroness Ryder of Warsaw in honour of these operations.

Secret Intelligence Service member Krystyna Skarbek ("nom de guerre" Christine Granville) was a founder member of SOE and helped establish a cell of Polish spies in Central Europe. She ran several operations in Poland, Egypt, Hungary (with Andrzej Kowerski) and France, often using the staunchly anti-Nazi Polish expatriate community as a secure international network. Non-official cover agents Elzbieta Zawacka and Jan Nowak-Jezioranski perfected the Gibraltar courier route out of occupied Europe. Maciej Kalenkiewicz was parachuted into occupied Poland, only to be killed by the Soviets. A Polish agent was integral to SOE's Operation Foxley, the plan to assassinate Hitler.

Thanks to co-operation between SOE and the Polish Home Army, the Poles were able to deliver the first Allied intelligence on the Holocaust to London in June 1942. Witold Pilecki of the Polish Home Army designed a joint operation with SOE to liberate Auschwitz, but the British rejected it as infeasible. Joint Anglo-Polish operations provided London with vital intelligence on the V-2 rocket, German troops movements on the Eastern Front, and the Soviet repressions of Polish citizens.

RAF 'Special Duties Flights' were sent to Poland to assist the Warsaw uprising against the Nazis. The rebellion was defeated with a loss of 200,000 casualties (mostly German executions of Polish civilians) after the nearby Red Army refused military assistance to the Polish Home Army. RAF Special Duties Flights were refused landing rights at Soviet-held airfields near Warsaw, even when requiring emergency landings after battle damage. These flights were also attacked by Soviet fighters, despite the USSR's officially Allied status.

Due to the dangers and lack of friendly population few operations were conducted in Germany itself. The German and Austrian section of SOE was run by Lieutenant Colonel Ronald Thornley for most of the war, and was mainly involved with black propaganda and administrative sabotage in collaboration with the German section of the Political Warfare Executive. After D-Day, the section was re-organised and enlarged with Major General Gerald Templer heading the Directorate, with Thornley as his deputy.

Several major operations were planned, including Operation Foxley, a plan to assassinate Hitler, and Operation Periwig, an ingenious plan to simulate the existence of a large-scale anti-Nazi resistance movement within Germany. "Foxley" was never carried out but "Periwig" went ahead despite restrictions placed on it by SIS and SHAEF. Several German prisoners of war were trained as agents, briefed to make contact with the anti-Nazi resistance and to conduct sabotage. They were then parachuted into Germany in the hope that they would either hand themselves in to the "Gestapo" or be captured by them, and reveal their supposed mission. Fake coded wireless transmissions were broadcast to Germany and various pieces of agent paraphernalia such as code books and wireless receivers were allowed to fall into the hands of the German authorities.

In Austria a resistance group formed around Kaplan Heinrich Maier. The Maier group was informed very early about the mass murder of Jews through its contacts with the Semperit factory near Auschwitz. SOE was in contact with this resistance group through its colleague G. E. R. Gedye in 1943, but was not convinced of the reliability of the contact and did not cooperate due to security concerns.

Section N of SOE ran operations in the Netherlands. They committed some of SOE's worst blunders in security, which allowed the Germans to capture many agents and much sabotage material, in what the Germans called the 'Englandspiel'. SOE ignored the absence of security checks in radio transmissions, and other warnings from their chief cryptographer, Leo Marks, that the Germans were running the supposed resistance networks. A total of 50 agents were caught and brought to Camp Haaren in the South of the Netherlands.

Five captured men managed to escape from the camp. Two of them, Pieter Dourlein and Ben Ubbink, escaped on 29 August 1943 and found their way to Switzerland. There, the Netherlands Embassy sent messages over their controlled sets to England that SOE Netherlands was compromised. SOE set up new elaborate networks, which continued to operate until the Netherlands were liberated at the end of the war.

From September 1944 to April 1945, eight Jedburgh teams were also active in the Netherlands. The first team, code named "Dudley" was parachuted into the east of the Netherlands one week before Operation Market Garden. The next four teams were attached to the Airborne forces that carried out Market Garden. After the failure of Market Garden, one Jedburgh team trained (former) resistance men in the liberated South of the Netherlands. In April 1945 the last two Dutch Jedburgh teams became operational. One team code named "Gambling", was a combined Jedburgh/Special Air Service (SAS) group that was dropped into the centre of the Netherlands to assist the Allied advance. The last team was parachuted into the Northern Netherlands as part of SAS operation "Amherst". Despite the fact that operating in the flat and densely populated Netherlands was very difficult for the Jedburghs, the teams were quite successful.

Section T established some effective networks in Belgium, in part orchestrated by fashion designer Hardy Amies, who rose to the rank of lieutenant colonel. Amies adapted names of fashion accessories for use as code words, while managing some of the most murderous and ruthless agents in the field. The rapid liberation of the country by Allied forces in September 1944 provided the resistance with little time to stage an uprising. They did assist the Allies to bypass German rearguards, and enabled the Allies to capture the vital Port of Antwerp intact.

After Brussels was liberated, Amies outraged his superiors by setting up a "Vogue" photo-shoot in Belgium. In 1946, he was knighted in Belgium for his service with SOE, being named an officer of the Order of the Crown.

As both an enemy country, and supposedly a monolithic fascist state with no organised opposition which SOE could use, SOE made little effort in Italy before mid-1943, when Mussolini's government collapsed and Allied forces already occupied Sicily. In April 1941, in a mission codenamed "Yak", Peter Fleming attempted to recruit agents from among the many thousands of Italian prisoners of war captured in the Western Desert Campaign. He met with no response. Attempts to search among Italian immigrants in the United States, Britain and Canada for agents to be sent to Italy had similarly poor results.

During the first three years of war, the most important "episode" of the collaboration between SOE and Italian anti-fascism was a project of an anti-fascist uprising in Sardinia, which the SOE supported at some stage but did not receive approval from the Foreign Office.

In the aftermath of the Italian collapse, SOE (in Italy renamed No. 1 Special Force) helped build a large resistance organisation in the cities of Northern Italy, and in the Alps. Italian partisans harassed German forces in Italy throughout the autumn and winter of 1944, and in the Spring 1945 offensive in Italy they captured Genoa and other cities unaided by Allied forces. SOE helped the Italian Resistance send British missions to the partisan formations and supply war material to the bands of patriots, a supply made without political prejudices, and which also helped the Communist formations (Brigate Garibaldi).

Late in 1943, SOE established a base at Bari in Southern Italy, from which they operated their networks and agents in the Balkans. This organisation had the codename ""Force 133"". This later became ""Force 266"", reserving 133 for operations run from Cairo rather than the heel of Italy. Flights from Brindisi were run to the Balkans and Poland, particularly once control had been wrested from SOE's Cairo headquarters and was exercised directly by Gubbins. SOE established a new packing station for the parachute containers close to Brindisi Air base, along the lines of those created at Saffron Walden. This was ME 54, a factory employing hundreds, the American (OSS) side of which was known as "Paradise Camp".

In the aftermath of the German invasion in 1941, the Kingdom of Yugoslavia fragmented. Croatia had a substantial pro-Axis movement, the Usta≈°e. In Croatia as well as the remainder of Yugoslavia, two resistance movements formed: the royalist Chetniks under Dra≈æa Mihailoviƒá, and the Communist Partisans under Josip Broz Tito.

Mihailoviƒá was the first to attempt to contact the Allies, and SOE despatched a party on 20 September 1941 under Major "Marko" Hudson. Hudson also encountered Tito's forces. Notable members of this party included Sir Christopher Lee. Through the royalist government in exile, SOE at first supported the Chetniks. Eventually, however, due to reports that the Chetniks were less effective and even collaborating with German and Italian forces on occasion, British support was redirected to the Partisans, even before the Tehran Conference in 1943.

Although relations were often touchy throughout the war, it can be argued that SOE's unstinting support was a factor in Yugoslavia's maintaining a neutral stance during the Cold War. However, accounts vary dramatically between all historical works on the ""Chetnik controversy"".

SOE was unable to establish links or contacts in Hungary before the regime of Mikl√≥s Horthy aligned itself with the Axis Powers. Distance and lack of such contacts prevented any effort being made by SOE until the Hungarians themselves dispatched a diplomat (L√°szl√≥ Veress) in a clandestine attempt to contact the Western Allies. SOE facilitated his return, with some radio sets. Before the Allied governments could agree terms, Hungary was placed under German military occupation and Veress was forced to flee the country.

Two missions subsequently dropped "blind" i.e. without prior arrangement for a reception party, failed. So too did an attempt by Basil Davidson to incite a partisan movement in Hungary, after he made his way there from northeastern Yugoslavia.

Greece was overrun by the Axis after a desperate defence lasting several months. In the aftermath, SIS and another intelligence organisation, SIME, discouraged attempts at sabotage or resistance as this might imperil relations with Turkey, although SOE maintained contacts with resistance groups in Crete. When an agent, "Odysseus", a former tobacco-smuggler, attempted to contact potential resistance groups in Greece, he reported that no group was prepared to co-operate with the monarchist government in exile in Cairo.

In late 1942, at the army's instigation, SOE mounted its first operation, codenamed Operation Harling, into Greece in an attempt to disrupt the railway which was being used to move materials to the German Panzer Army Africa. A party under Colonel (later Brigadier) Eddie Myers, assisted by Christopher Woodhouse, was parachuted into Greece and discovered two guerrilla groups operating in the mountains: the pro-Communist ELAS and the republican EDES. On 25 November 1942, Myers's party blew up one of the spans of the railway viaduct at Gorgopotamos, supported by 150 Greek partisans from these two organisations who engaged Italians guarding the viaduct. This cut the railway linking Thessaloniki with Athens and Piraeus.

Relations between the resistance groups and the British soured. When the British needed once again to disrupt the railway across Greece as part of the deception operations preceding Operation Husky, the Allied invasion of Sicily, the resistance groups refused to take part, rightly fearing German reprisals against civilians. Instead, a six-man commando party from the British and New Zealand armies, led by New Zealander Lieutenant Colonel Cecil Edward Barnes a civil engineer, carried out the destruction of the Asopos viaduct on 21 June 1943. Two attempts by Mike Cumberlege to make the Corinth Canal unnavigable ended in failure.

EDES received most aid from SOE, but ELAS secured many weapons when Italy collapsed and Italian military forces in Greece dissolved. ELAS and EDES fought a vicious civil war in 1943 until SOE brokered an uneasy armistice (the Plaka agreement).

A lesser known, but important function of the SOE in Greece was to inform the Cairo headquarters of the movement of the German military aircraft that were serviced and repaired at the two former Greek military aircraft facilities in and around Athens.

Eventually, the British Army occupied Athens and Piraeus in the aftermath of the German withdrawal, and fought a street-by-street battle to drive ELAS from these cities and impose an interim government under Archbishop Damaskinos. SOE's last act was to evacuate several hundred disarmed EDES fighters to Corfu, preventing their massacre by ELAS.

Several resistance groups and Allied stay-behind parties operated in Crete after the Germans occupied the island in the Battle of Crete. SOE's operations involved figures such as Patrick Leigh Fermor, John Lewis, Harry Rudolph Fox Burr, Tom Dunbabin, Sandy Rendel, John Houseman, Xan Fielding and Bill Stanley Moss. Some of the most famous moments included the abduction of General Heinrich Kreipe led by Leigh Fermor and Moss ‚Äì subsequently portrayed in the film "Ill Met by Moonlight", and the sabotage of Damasta led by Moss.

Albania had been under Italian influence since 1923, and was occupied by the Italian Army in 1939. In 1943, a small liaison party entered Albania from northwestern Greece. SOE agents who entered Albania then or later included Julian Amery, Anthony Quayle, David Smiley and Neil "Billy" McLean. They discovered another internecine war between the Communist partisans under Enver Hoxha, and the republican Balli Komb√´tar. As the latter had collaborated with the Italian occupiers, Hoxha gained Allied support.

SOE's envoy to Albania, Brigadier Edmund "Trotsky" Davies, was captured by the Germans early in 1944. Some SOE officers warned that Hoxha's aim was primacy after the war, rather than fighting Germans. They were ignored, but Albania was never a major factor in the effort against the Germans.

SOE sent many missions into the Czech areas of the so-called Protectorate of Bohemia and Moravia, and later into Slovakia. The most famous mission was Operation Anthropoid, the assassination of SS-Obergruppenf√ºhrer Reinhard Heydrich in Prague. From 1942 to 1943 the Czechoslovaks had their own Special Training School (STS) at Chicheley Hall in Buckinghamshire. In 1944, SOE sent men to support the Slovak National uprising.

In March 1941 a group performing commando raids in Norway, Norwegian Independent Company 1 (NOR.I.C.1) was organised under leadership of Captain Martin Linge. Their initial raid in 1941 was Operation Archery, the best known raid was probably the Norwegian heavy water sabotage. Communication lines with London were gradually improved so that by 1945, 64 radio operators were spread throughout Norway.

The Danish resistance assisted SOE in its activities in neutral Sweden. For example, SOE was able to obtain several shiploads of vital ball-bearings which had been interned in Swedish ports. The Danes also pioneered several secure communications methods; for example, a burst transmitter/receiver which transcribed Morse code onto a paper tape faster than a human operator could handle.

In 1943 an SOE delegation was parachuted into Romania to instigate resistance against the Nazi occupation at "any cost" (Operation Autonomous). The delegation, including Colonel Gardyne de Chastelain, Captain Silviu Me≈£ianu and Ivor Porter, was captured by the Romanian Gendarmerie and held until the night of King Michael's Coup on 23 August 1944.

Abyssinia was the scene of some of SOE's earliest and most successful efforts. SOE organised a force of Ethiopian irregulars under Orde Charles Wingate in support of the exiled Emperor Haile Selassie. This force (named Gideon Force by Wingate) caused heavy casualties to the Italian occupation forces, and contributed to the successful British campaign there. Wingate was to use his experience to create the Chindits in Burma.

The neutral Spanish island of Fernando Po was the scene of Operation Postmaster, one of SOE's most successful exploits. The large Italian merchant vessel "Duchessa d'Aosta" and the German tug "Likomba" had taken refuge in the harbour of Santa Isabel. On 14 January 1942, while the ships' officers were attending a party ashore thrown by an SOE agent, commandos and SOE personnel led by Gus March-Phillipps boarded the two vessels, cut the anchor cables and towed them out to sea, where they later rendezvoused with Royal Navy ships. Several neutral authorities and observers were impressed by the British display of ruthlessness.

As early as 1940, SOE was preparing plans for operations in Southeast Asia. As in Europe, after initial Allied military disasters, SOE built up indigenous resistance organisations and guerrilla armies in enemy (Japanese) occupied territory. SOE also launched ""Operation Remorse"" (1944‚Äì45), which was ultimately aimed at protecting the economic and political status of Hong Kong. Force 136 engaged in covert trading of goods and currencies in China. Its agents proved remarkably successful, raising ¬£77m through their activities, which were used to provide assistance for Allied prisoners of war and, more controversially, to buy influence locally to facilitate a smooth return to pre-war conditions.

In late 1944, as it became clear that the war would soon be over, Lord Selborne advocated keeping SOE or a similar body in being, and that it would report to the Ministry of Defence. Anthony Eden, the Foreign Secretary, insisted that his ministry, already responsible for the SIS, should control SOE or its successors. The Joint Intelligence Committee, which had a broad co-ordinating role over Britain's intelligence services and operations, took the view that SOE was a more effective organisation than the SIS but that it was unwise to split the responsibility for espionage and more direct action between separate ministries, or to perform special operations outside the ultimate control of the Chiefs of Staff. The debate continued for several months until on 22 May 1945, Selborne wrote:

Churchill took no immediate decision, and after he lost the general election on 5 July 1945, the matter was dealt with by the Labour Prime Minister, Clement Attlee. Selborne told Attlee that SOE still possessed a worldwide network of clandestine radio networks and sympathisers. Attlee replied that he had no wish to own a British Comintern, and closed Selborne's network down at 48 hours' notice.

SOE was dissolved officially on 15 January 1946. Some of its senior staff moved easily into financial services in the City of London, although some of them had not lost their undercover mentality and did little for the City's name. Most of SOE's other personnel reverted to their peacetime occupations or regular service in the armed forces, but 280 of them were taken into the ""Special Operations Branch"" of MI6. Some of these had served as agents in the field, but MI6 was most interested in SOE's training and research staff. Sir Stewart Menzies, the head of MI6 (who was generally known simply as "C") soon decided that a separate Special Operations branch was unsound, and merged it into the general body of MI6.

Gubbins, the last director, was not given further employment by the Army, but he later founded the Special Forces Club for former members of SOE and similar organisations.

Although the wartime British government considered the activities of the SOE to be lawful, the German invaders, as in World War I and the War of 1870, argued that those engaging in resistance (local resistance fighters and the agents of foreign governments who supported them) were "bandits" and "terrorists", maintaining that all "Francs-tireurs" (and said agents) were engaging in an illegal form of warfare, and, as such, had no legal rights. A view expressed by Fritz Sauckel, the General Plenipotentiary for Labour Deployment, making him the man in charge of bringing workers to the factories in Germany for forced labour, who demanded the flight of young French men to the countryside be stopped and called the "maquis" "terrorists", "bandits" and "criminals" for their opposition to lawful authority.

The mode of warfare encouraged and promoted by SOE is considered by several modern commentators to have established the modern model that many alleged terrorist organisations emulate. Two opposed views were quoted by Tony Geraghty in "The Irish War: The Hidden Conflict Between the IRA and British Intelligence". M. R. D. Foot, who wrote several official histories of SOE wrote,

The British military historian John Keegan wrote,

Another, later view, on the moral contribution of SOE, was expressed by the writer Max Hastings,

Since the end of the war, the SOE has appeared in many films, comics, books, and television.


Official publications/academic histories


 Biographies/popular books by outsiders

Commentaries




</doc>
<doc id="28899" url="https://en.wikipedia.org/wiki?curid=28899" title="System request">
System request

System request (SysRq or Sys Req) is a key on personal computer keyboards that has no standard use. Introduced by IBM with the PC/AT, it was intended to be available as a special key to directly invoke low-level operating system functions with no possibility of conflicting with any existing software. A special BIOS routine ‚Äì software interrupt 0x15, subfunction 0x85 ‚Äì was added to signal the OS when SysRq was pushed or released. Unlike most keys, when it is pressed nothing is stored in the keyboard buffer.

The specific low level function intended for the SysRq key was to switch between operating systems. When the original IBM-PC was created in 1980, there were three leading competing operating systems: PC DOS, CP/M-86, and UCSD p-System, while Xenix was added in 1983‚Äì1984. The SysRq key was added so that multiple operating systems could be run on the same computer, making use of the capabilities of the 286 chip in the PC/AT.

A special key was needed because most software of the day operated at a low level, often bypassing the OS entirely, and typically made use of many hotkey combinations. The use of Terminate and Stay Resident (TSR) programs further complicated matters. To implement a task switching or multitasking environment, it was thought that a special, separate key was needed. This is similar to the way "Control-Alt-Delete" is used under Windows NT.

On 84-key keyboards (except the 84-key IBM Model M space saver keyboard), SysRq was a key of its own. On the later 101-key keyboard, it shares a physical key with the Print screen key function. The Alt key must be held down while pressing this dual-function key to invoke SysRq.

The default BIOS keyboard routines simply ignore SysRq and return without taking action. So did the MS-DOS input routines. The keyboard routines in libraries supplied with many high-level languages followed suit. Although it is still included on most PC keyboards manufactured, and though it is used by some debugging software, the key is of no use for the vast majority of users.

On the Hyundai/Hynix Super-16 computer, pressing will hard boot the system (it will reboot when is unresponsive, and it will invoke startup memory tests that are bypassed on soft-boot).

In Linux, the kernel can be configured to provide functions for system debugging and crash recovery. This use is known as the "magic SysRq key".

Microsoft has also used SysRq for various OS- and application-level debuggers. In the CodeView debugger, it was sometimes used to break into the debugging during program execution. For the Windows NT remote kernel debugger, it can be used to force the system into the debugger.

IBM 3270-type console keyboards of the IBM System/370 mainframe computer, created in 1970, had an operator interrupt key that was used to cause the operating system such as VM/370 or MVS to allow the console to give input to the operating system.



</doc>
<doc id="28900" url="https://en.wikipedia.org/wiki?curid=28900" title="Split infinitive">
Split infinitive

In the English language, a split infinitive or cleft infinitive is a grammatical construction in which a word or phrase comes between the "to" and the bare infinitive of the "to" form of the infinitive verb. Usually, an adverb or an adverbial phrase comes between them. The opening sequence of the "Star Trek" television series contains a well-known example, where William Shatner says "to "boldly" go where no man has gone before"; the adverb "boldly" is said to split the infinitive "to go". There are occasions where more than one word splits the infinitive, such as: "The population is expected to more than double in the next ten years".

In the 19th century, some linguistic prescriptivists sought to introduce a prescriptive rule against the split infinitive. The construction is to some extent still the subject of disagreement, but modern English usage guides have dropped the objection to it.

In Old English, infinitives were single words ending in "-n" or "-an" (comparative to modern Dutch and German "-n", "-en"). Gerunds were formed using "to" followed by a verbal noun in the dative case, which ended in "-anne" or "-enne" (e.g. "t≈ç cumenne" = "coming, to come"). In Middle English, the bare infinitive and the gerund coalesced into the same form ending in "-(e)n" (e.g. "comen" "come"; "to comen" "to come"). The "to" infinitive was not split in Old or Early Middle English.

The first known example of a split infinitive in English, in which a pronoun rather than an adverb splits the infinitive, is in Layamon's "Brut" (early 13th century):

This may be a poetic inversion for the sake of meter, and therefore says little about whether Layamon would have felt the construction to be syntactically natural. However, no such reservation applies to the following prose example from John Wycliffe (14th century), who often split infinitives:

After its rise in Middle English, the construction became rare in the 15th and 16th centuries. William Shakespeare used it once, or perhaps twice. The uncontroversial example appears to be a syntactical inversion for the sake of meter:

Edmund Spenser, John Dryden, Alexander Pope, and the King James Version of the Bible used none, and they are very rare in the writing of Samuel Johnson. John Donne used them several times, though, and Samuel Pepys also used at least one. No reason for the near disappearance of the split infinitive is known; in particular, no prohibition is recorded.

Split infinitives reappeared in the 18th century and became more common in the 19th.
Daniel Defoe, Benjamin Franklin, William Wordsworth, Abraham Lincoln, George Eliot, Henry James, and Willa Cather are among the writers who used them. Examples in the poems of Robert Burns attest its presence also in 18th-century Scots:

In colloquial speech the construction came to enjoy widespread use. Today, according to the "American Heritage Book of English Usage", "people split infinitives all the time without giving it a thought". In corpora of contemporary spoken English, some adverbs such as "always" and "completely" appear more often in the split position than the unsplit.

Although it is difficult to say why the construction developed in Middle English, or why it revived so powerfully in Modern English, a number of theories have been postulated.

Traditional grammarians have suggested that the construction appeared because people frequently place adverbs before finite verbs. George Curme writes: "If the adverb should immediately precede the finite verb, we feel that it should immediately precede also the infinitive‚Ä¶" Thus, if one says:
one may, by analogy, wish to say:
This is supported by the fact that split infinitives are often used as echoes, as in the following exchange, in which the riposte parodies the slightly odd collocation in the original sentence:
Here is an example of an adverb being transferred into split infinitive position from a parallel position in a different construction.

Transformational grammarians have attributed the construction to a re-analysis of the role of "to".

In the modern language, splitting usually involves a single adverb coming between the verb and its marker. Very frequently, this is an emphatic adverb, for example:

Sometimes it is a negation, as in the self-referential joke:

However, in modern colloquial English, almost any adverb may be found in this syntactic position, especially when the adverb and the verb form a close syntactic unit (really-pull, not-split).

Compound split infinitives, i.e., infinitives split by more than one word, usually involve a pair of adverbs or a multi-word adverbial:

Examples of non-adverbial elements participating in the split-infinitive construction seem rarer in Modern English than in Middle English. The pronoun "all" commonly appears in this position:
and may even be combined with an adverb:
However an object pronoun, as in the Layamon example above, would be unusual in modern English, perhaps because this might cause a listener to misunderstand the "to" as a preposition:

While, structurally, acceptable as poetic formulation, this would result in a garden path sentence¬† particularly evident if the indirect object is omitted:

Other parts of speech would be very unusual in this position. However, in verse, poetic inversion for the sake of meter or of bringing a rhyme word to the end of a line often results in abnormal syntax, as with Shakespeare's split infinitive ("to pitied be", cited above), in fact an inverted passive construction in which the infinitive is split by a past participle. Presumably, this would not have occurred in a prose text by the same author.

Finally, there is a construction with a word or words between "to" and an infinitive that nevertheless is not considered a split infinitive, namely, infinitives joined by a conjunction. This is not objected to even when an adverb precedes the second infinitive. Examples include "We pray you "to" proceed/ And "justly and religiously unfold"..." (Shakespeare, "Henry V", Act II, scene 9) and "...she is determined "to" be independent, and "not live" with aunt Pullet" (George Eliot, "The Mill on the Floss", volume VI, chapter I).

It was not until the very end of the 19th century that terminology emerged to describe the construction. The earliest use of the term "split infinitive" on record dates from 1890. The now rare "cleft infinitive" is almost as old, attested from 1893. "Splitting the infinitive" is slightly older, back to 1887. According to the main etymological dictionaries, "infinitive-splitting" and "infinitive-splitter" followed in 1926 and 1927, respectively. The term "compound split infinitive" is not found in these dictionaries and appears to be very recent.

This terminology implies analysing the full infinitive as a two-word infinitive, which not all grammarians accept. As one who used "infinitive" to mean the single-word verb, Otto Jespersen challenged the epithet: "'To' is no more an essential part of an infinitive than the definite article is an essential part of a nominative, and no one would think of calling 'the good man' a split nominative." However, no alternative terminology has been proposed.

Although it is sometimes reported that a prohibition on split infinitives goes back to Renaissance times, and frequently the 18th century scholar Robert Lowth is cited as the originator of the prescriptive rule, such a rule is not to be found in Lowth's writing, and is not known to appear in any text before the 19th century.

Possibly the earliest comment against split infinitives was by the American John Comly in 1803.

An adverb should not be placed between the verb of the infinitive mood and the preposition "to", which governs it; as "Patiently" to wait‚Äînot To "patiently" wait.

Another early prohibition came from an anonymous American in 1834:

The practice of separating the prefix of the infinitive mode from the verb, by the intervention of an adverb, is not unfrequent among uneducated persons¬†‚Ä¶ I am not conscious, that any rule has been heretofore given in relation to this point¬†‚Ä¶ The practice, however, of not separating the particle from its verb, is so general and uniform among good authors, and the exceptions are so rare, that the rule which I am about to propose will, I believe, prove to be as accurate as most rules, and may be found beneficial to inexperienced writers. It is this :‚Äî"The particle, "TO", which comes before the verb in the infinitive mode, must not be separated from it by the intervention of an adverb or any other word or phrase; but the adverb should immediately precede the particle, or immediately follow the verb."

In 1840, Richard Taylor also condemned split infinitives as a "disagreeable affectation", and in 1859, Solomon Barrett, Jr., called them "a common fault". However, the issue seems not to have attracted wider public attention until Henry Alford addressed it in his "Plea for the Queen's English" in 1864:
A correspondent states as his own usage, and defends, the insertion of an adverb between the sign of the infinitive mood and the verb. He gives as an instance, ""to scientifically illustrate"". But surely this is a practice entirely unknown to English speakers and writers. It seems to me, that we ever regard the "to" of the infinitive as inseparable from its verb. And, when we have already a choice between two forms of expression, "scientifically to illustrate" and "to illustrate scientifically", there seems no good reason for flying in the face of common usage.

Others followed, among them Bache, 1869 ("The "to" of the infinitive mood is inseparable from the verb"); William B. Hodgson, 1889; and Raub, 1897 ("The sign "to" must not be separated from the remaining part of the infinitive by an intervening word").

Even as these authorities were condemning the split infinitive, others were endorsing it: Brown, 1851 (saying some grammarians had criticized it and it was less elegant than other adverb placements but sometimes clearer); Hall, 1882; Onions, 1904; Jespersen, 1905; and Fowler and Fowler, 1906. Despite the defence by some grammarians, by the beginning of the 20th century the prohibition was firmly established in the press. In the 1907 edition of "The King's English", the Fowler brothers wrote:

The 'split' infinitive has taken such hold upon the consciences of journalists that, instead of warning the novice against splitting his infinitives, we must warn him against the curious superstition that the splitting or not splitting makes the difference between a good and a bad writer.

In large parts of the school system, the construction was opposed with ruthless vigour. A correspondent to the BBC on a programme about English grammar in 1983 remarked:

One reason why the older generation feel so strongly about English grammar is that we were severely punished if we didn't obey the rules! One split infinitive, one whack; two split infinitives, two whacks; and so on.

As a result, the debate took on a degree of passion which the bare facts of the matter never warranted. There was frequent skirmishing between the splitters and anti-splitters until the 1960s. George Bernard Shaw wrote letters to newspapers supporting writers who used the split infinitive and Raymond Chandler complained to the editor of "The Atlantic" about a proofreader who interfered with Chandler's split infinitives:

By the way, would you convey my compliments to the purist who reads your proofs and tell him or her that I write in a sort of broken-down patois which is something like the way a Swiss-waiter talks, and that when I split an infinitive, God damn it, I split it so it will remain split, and when I interrupt the velvety smoothness of my more or less literate syntax with a few sudden words of barroom vernacular, this is done with the eyes wide open and the mind relaxed and attentive. The method may not be perfect, but it is all I have.

Post-1960 authorities show a strong tendency to accept the split infinitive. Follett, in "Modern American Usage" (1966) writes: "The split infinitive has its place in good composition. It should be used when it is expressive and well led up to." Fowler (Gowers' revised second edition, 1965) offers the following example of the consequences of refusal to split infinitives: "The greatest difficulty about assessing the economic achievements of the Soviet Union is that its spokesmen try "absurdly to exaggerate" them; in consequence the visitor may tend "badly to underrate" them" (italics added). This question results: "Has dread of the split infinitive led the writer to attach the adverbs ['absurdly' and 'badly'] to the wrong verbs, and would he not have done better "to boldly split" both infinitives, since he cannot put the adverbs after them without spoiling his rhythm" (italics added)? Bernstein (1985) argues that, although infinitives should not always be split, they should be split where doing so improves the sentence: "The natural position for a modifier is before the word it modifies. Thus the natural position for an adverb modifying an infinitive should be just¬†‚Ä¶ "after" the to" (italics added). Bernstein continues: "Curme's contention that the split infinitive is often an improvement¬†‚Ä¶ cannot be disputed." Heffernan and Lincoln, in their modern English composition textbook, agree with the above authors. Some sentences, they write, "are weakened by¬†‚Ä¶ cumbersome splitting", but in other sentences "an infinitive may be split by a one-word modifier that would be awkward in any other position".

Objections to the split infinitive fall into three categories, of which only the first is accorded any credence by linguists.

An early proposed rule proscribing the split infinitive, which was expressed by an anonymous author in the "New-England Magazine" in 1834, was based on the purported observation that it was a feature of a form of English commonly used by uneducated persons but not by "good authors."
Henry Alford, in his "Plea for the Queen's English" in 1864 went further, stating that use of the "split infinitive" was "a practice entirely unknown to English speakers and writers."

A second argument is summed up by Alford's statement "It seems to me that we ever regard the "to" of the infinitive as inseparable from its verb."

The "to" in the infinitive construction, which is found throughout the Germanic languages, is originally a preposition before the dative of a verbal noun, but in the modern languages it is widely regarded as a particle which serves as a marker of the infinitive. In German and Dutch, this marker ("zu" and "te" respectively) sometimes precedes the infinitive, but is not regarded as part of it. In English, on the other hand, it is traditional to speak of the "bare infinitive" without "to" and the "full infinitive" with it, and to conceive of "to" as part of the full infinitive. (In the sentence "I had my daughter clean her room", "clean" is a bare infinitive; in "I told my daughter to clean her room", "to clean" is a full infinitive.) Possibly this is because the absence of an "inflected" infinitive form made it useful to include the particle in the citation form of the verb, and in some nominal constructions in which other Germanic languages would omit it (e.g. "to know her is to love her"). The concept of a two-word infinitive can reinforce an intuitive sense that the two words belong together. For instance, the rhetorician John Duncan Quackenbos said, ""To have" is as much one thing, and as inseparable by modifiers, as the original form "habban", or the Latin "habere"." The usage writer John Opdycke based a similar argument on the closest French, German, and Latin translations.

That there are two parts to the infinitive is disputed, and some linguists say that the infinitive in English is a single-word verb form, which may or may not be preceded by the particle "to". Some modern generative analysts classify "to" as a "peculiar" auxiliary verb; other analysts, as the infinitival subordinator. Moreover, even when the concept of the full infinitive is accepted, it does not necessarily follow that any two words that belong together grammatically need be adjacent to each other. They usually are, but counter-examples are easily found, such as an adverb splitting a two-word finite verb ("will not do", "has not done").

A frequently discussed argument states that the split-infinitive prohibition is based on Latin. An infinitive in Latin is never used with a marker equivalent to English "to", and thus there is no parallel there for the construction. The claim that those who dislike split infinitives are applying rules of Latin grammar to English is asserted in many references that accept the split infinitive. One example is in the "American Heritage Book of English Usage": "The only rationale for condemning the construction is based on a false analogy with Latin." In more detail, the usage author Marilyn Moriarty states:
The rule forbidding a split infinitive comes from the time when Latin was the universal language of the world. All scholarly, respectable writing was done in Latin. Scientists and scholars even took Latin names to show that they were learned. In Latin, infinitives appear as a single word. The rule which prohibits splitting an infinite shows deference to Latin and to the time when the rules which governed Latin grammar were applied to other languages.

The assertion is also made in the "Oxford Guide to Plain English", "Compact Oxford English Dictionary", and Steven Pinker's "The Language Instinct", among other sources.
The argument implies an adherence to the humanist idea of the greater purity of the classics, which, particularly in Renaissance times, led people to regard as inferior aspects of English that differed from Latin. However, by the 19th century, such views were no longer widespread; Moriarty is in error about the age of the prohibition.
It has also been stated that an argument from Latin would be fallacious because "there is no precedent in these languages for condemning the split infinitive because in Greek and Latin (and all the other romance languages) the infinitive is a single word that is impossible to sever".

Although many sources suggest that the argument from classical languages motivated the early opponents of the split infinitive, there is little primary source evidence; indeed, Richard Bailey has noted that despite the lack of evidence, this theory has simply become ‚Äúpart of the folklore of linguistics.‚Äù

Present style and usage manuals deem simple split infinitives unobjectionable. For example, Curme's "Grammar of the English Language" (1931) says that not only is the split infinitive correct, but it "should be furthered rather than censured, for it makes for clearer expression". "The Columbia Guide to Standard American English" notes that the split infinitive "eliminates all possibility of ambiguity", in contrast to the "potential for confusion" in an unsplit construction. "Merriam‚ÄìWebster's Dictionary of English Usage" says: "the objection to the split infinitive has never had a rational basis". According to Mignon Fogarty, "today almost everyone agrees that it is OK to split infinitives".

Nevertheless, many teachers of English still admonish students against using split infinitives in writing. Because the prohibition has become so widely known, the "Columbia Guide" recommends that writers "follow the conservative path [of avoiding split infinitives when they are not necessary], especially when you're uncertain of your readers' expectations and sensitivities in this matter". Likewise, the Oxford Dictionaries do not regard the split infinitive as ungrammatical, but on balance consider it likely to produce a weak style and advise against its use for formal correspondence. R. W. Burchfield's revision of Fowler's "Modern English Usage" goes farther (quoting Burchfield's own 1981 book "The Spoken Word"): "Avoid splitting infinitives whenever possible, but do not suffer undue remorse if a split infinitive is unavoidable for the completion of a sentence already begun." Still more strongly, older editions of "The Economist" Style Guide said, "Happy the man who has never been told that it is wrong to split an infinitive: the ban is pointless. Unfortunately, to see it broken is so annoying to so many people that you should observe it" (but added "To never split an infinitive is quite easy."). This recommendation, however, is weakened in the 12th edition. After stating that the ban is pointless, "The Economist Style Guide" now says "To see a split infinitive nevertheless annoys some readers, so try to avoid placing a modifier between "to" and the verb in an infinitive. But if moving the modifier would ruin the rhythm, change the meaning or even just put the emphasis in the wrong place, splitting the infinitive is the best option."

As well as varying according to register, tolerance of split infinitives varies according to type. While most authorities accept split infinitives in general, it is not hard to construct an example which any native speaker would reject. Wycliff's Middle English compound split would, if transferred to modern English, be regarded by most people as un-English:
Attempts to define the boundaries of normality are controversial. In 1996, the usage panel of "The American Heritage Book" was evenly divided for and against such sentences as,
but more than three-quarters of the panel rejected
Here the problem appears to be the breaking up of the verbal phrase "to be seeking a plan to relieve": a segment of the head verbal phrase is so far removed from the remainder that the listener or reader must expend greater effort to understand the sentence. By contrast, 87 percent of the panel deemed acceptable the multi-word adverbial in
not surprisingly perhaps, because here there is no other place to put the words "more than" without substantially recasting the sentence.

A special case is the splitting of an infinitive by the negation in sentences like
Here traditional idiom, placing the negation before the marker ("I soon learned not to provoke her") or with verbs of desire, negating the finite verb ("I don't want to see you anymore") remains easy and natural, and is still overwhelmingly the more common construction. Some argue that the two forms have different meanings, while others see a grammatical difference, but most speakers do not make such a distinction.
In an example drawn from the British National Corpus the use of "to not be" against "not to be" is only 0.35% (from a total of 3121 sampled usages).

Writers who avoid splitting infinitives either place the splitting element elsewhere in the sentence or reformulate the sentence, perhaps rephrasing it without an infinitive and thus avoiding the issue. However, a sentence such as "to more than double" must be completely rewritten to avoid the split infinitive; it is ungrammatical to put the words "more than" anywhere else in the sentence. While split infinitives can be avoided, a writer must be careful not to produce an awkward or ambiguous sentence. Fowler (1926) stressed that, if a sentence is to be rewritten to remove a split infinitive, this must be done without compromising the language:

It is of no avail merely to fling oneself desperately out of temptation; one must so do it that no traces of the struggle remain; that is, sentences must be thoroughly remodeled instead of having a word lifted from its original place & dumped elsewhere¬†‚Ä¶
In some cases, moving the adverbial creates an ungrammatical sentence or changes the meaning. R.¬†L. Trask uses this example:

The sentence can be rewritten to maintain its meaning, however, by using a noun or a different grammatical aspect of the verb, or by avoiding the informal "get rid":

Fowler notes that the option of rewriting is always available but questions whether it is always worth the trouble.





</doc>
<doc id="28901" url="https://en.wikipedia.org/wiki?curid=28901" title="Symmetric group">
Symmetric group

In abstract algebra, the symmetric group defined over any set is the group whose elements are all the bijections from the set to itself, and whose group operation is the composition of functions. In particular, the finite symmetric group S defined over a finite set of "n" symbols consists of the permutation operations that can be performed on the "n" symbols. Since there are "n"! ("n" factorial) such permutation operations, the order (number of elements) of the symmetric group S is "n"!.

Although symmetric groups can be defined on infinite sets, this article focuses on the finite symmetric groups: their applications, their elements, their conjugacy classes, a finite presentation, their subgroups, their automorphism groups, and their representation theory. For the remainder of this article, "symmetric group" will mean a symmetric group on a finite set.

The symmetric group is important to diverse areas of mathematics such as Galois theory, invariant theory, the representation theory of Lie groups, and combinatorics. Cayley's theorem states that every group "G" is isomorphic to a subgroup of the symmetric group on "G".

The symmetric group on a finite set "X" is the group whose elements are all bijective functions from "X" to "X" and whose group operation is that of function composition. For finite sets, "permutations" and "bijective functions" refer to the same operation, namely rearrangement. The symmetric group of degree "n" is the symmetric group on the set 

The symmetric group on a set "X" is denoted in various ways, including S, ùîñ, Œ£, Œ£(X), "X"! and Sym("X"). If "X" is the set } then the name may be abbreviated to S, ùîñ, Œ£, or Sym("n").

Symmetric groups on infinite sets behave quite differently from symmetric groups on finite sets, and are discussed in , , and .

The symmetric group on a set of "n" elements has order "n"! (the factorial of "n"). It is abelian if and only if "n" is less than or equal to 2. For and (the empty set and the singleton set), the symmetric group is trivial (it has order ). The group S is solvable if and only if . This is an essential part of the proof of the Abel‚ÄìRuffini theorem that shows that for every there are polynomials of degree "n" which are not solvable by radicals, that is, the solutions cannot be expressed by performing a finite number of operations of addition, subtraction, multiplication, division and root extraction on the polynomial's coefficients.

The symmetric group on a set of size "n" is the Galois group of the general polynomial of degree "n" and plays an important role in Galois theory. In invariant theory, the symmetric group acts on the variables of a multi-variate function, and the functions left invariant are the so-called symmetric functions. In the representation theory of Lie groups, the representation theory of the symmetric group plays a fundamental role through the ideas of Schur functors. In the theory of Coxeter groups, the symmetric group is the Coxeter group of type A and occurs as the Weyl group of the general linear group. In combinatorics, the symmetric groups, their elements (permutations), and their representations provide a rich source of problems involving Young tableaux, plactic monoids, and the Bruhat order. Subgroups of symmetric groups are called permutation groups and are widely studied because of their importance in understanding group actions, homogeneous spaces, and automorphism groups of graphs, such as the Higman‚ÄìSims group and the Higman‚ÄìSims graph.

The elements of the symmetric group on a set "X" are the permutations of "X".

The group operation in a symmetric group is function composition, denoted by the symbol ‚àò or simply by juxtaposition of the permutations. The composition of permutations "f" and "g", pronounced ""f" of "g"", maps any element "x" of "X" to "f"("g"("x")). Concretely, let (see permutation for an explanation of notation):

Applying "f" after "g" maps 1 first to 2 and then 2 to itself; 2 to 5 and then to 4; 3 to 4 and then to 5, and so on. So composing "f" and "g" gives

A cycle of length , taken to the "k"-th power, will decompose into "k" cycles of length "m": For example, (, ),

To check that the symmetric group on a set "X" is indeed a group, it is necessary to verify the group axioms of closure, associativity, identity, and inverses.

A transposition is a permutation which exchanges two elements and keeps all others fixed; for example (1 3) is a transposition. Every permutation can be written as a product of transpositions; for instance, the permutation "g" from above can be written as "g" = (1 2)(2 5)(3 4). Since "g" can be written as a product of an odd number of transpositions, it is then called an odd permutation, whereas "f" is an even permutation.

The representation of a permutation as a product of transpositions is not unique; however, the number of transpositions needed to represent a given permutation is either always even or always odd. There are several short proofs of the invariance of this parity of a permutation.

The product of two even permutations is even, the product of two odd permutations is even, and all other products are odd. Thus we can define the sign of a permutation:

With this definition,
is a group homomorphism ({+1, ‚Äì1} is a group under multiplication, where +1 is e, the neutral element). The kernel of this homomorphism, that is, the set of all even permutations, is called the alternating group A. It is a normal subgroup of S, and for it has elements. The group S is the semidirect product of A and any subgroup generated by a single transposition.

Furthermore, every permutation can be written as a product of "adjacent transpositions", that is, transpositions of the form . For instance, the permutation "g" from above can also be written as . The sorting algorithm bubble sort is an application of this fact. The representation of a permutation as a product of adjacent transpositions is also not unique.

A cycle of "length" "k" is a permutation "f" for which there exists an element "x" in {1...,"n"} such that "x", "f"("x"), "f"("x"), ..., "f"("x") = "x" are the only elements moved by "f"; it is required that since with the element "x" itself would not be moved either. The permutation "h" defined by

is a cycle of length three, since , and , leaving 2 and 5 untouched. We denote such a cycle by , but it could equally well be written or by starting at a different point. The order of a cycle is equal to its length. Cycles of length two are transpositions. Two cycles are "disjoint" if they move disjoint subsets of elements. Disjoint cycles commute: for example, in S there is the equality . Every element of S can be written as a product of disjoint cycles; this representation is unique up to the order of the factors, and the freedom present in representing each individual cycle by choosing its starting point.

Cycles admit the following conjugation property with any permutation formula_8, this property is often used to obtain its generators and relations.

Certain elements of the symmetric group of {1, 2, ..., "n"} are of particular interest (these can be generalized to the symmetric group of any finite totally ordered set, but not to that of an unordered set).

The is the one given by:
This is the unique maximal element with respect to the Bruhat order and the
longest element in the symmetric group with respect to generating set consisting of the adjacent transpositions , .

This is an involution, and consists of formula_11 (non-adjacent) transpositions

so it thus has sign:

which is 4-periodic in "n".

In S, the "perfect shuffle" is the permutation that splits the set into 2 piles and interleaves them. Its sign is also formula_15

Note that the reverse on "n" elements and perfect shuffle on 2"n" elements have the same sign; these are important to the classification of Clifford algebras, which are 8-periodic.

The conjugacy classes of S correspond to the cycle structures of permutations; that is, two elements of S are conjugate in S if and only if they consist of the same number of disjoint cycles of the same lengths. For instance, in S, (1 2 3)(4 5) and (1 4 3)(2 5) are conjugate; (1 2 3)(4 5) and (1 2)(4 5) are not. A conjugating element of S can be constructed in "two line notation" by placing the "cycle notations" of the two conjugate permutations on top of one another. Continuing the previous example:

which can be written as the product of cycles, namely: (2 4).

This permutation then relates (1 2 3)(4 5) and (1 4 3)(2 5) via conjugation, that is,

It is clear that such a permutation is not unique.

The low-degree symmetric groups have simpler and exceptional structure, and often must be treated separately.







Other than the trivial map and the sign map , the most notable homomorphisms between symmetric groups, in order of relative dimension, are:
There are also a host of other homomorphisms where .

For , the alternating group A is simple, and the induced quotient is the sign map: which is split by taking a transposition of two elements. Thus S is the semidirect product , and has no other proper normal subgroups, as they would intersect A in either the identity (and thus themselves be the identity or a 2-element group, which is not normal), or in A (and thus themselves be A or S).

S acts on its subgroup A by conjugation, and for , S is the full automorphism group of A: Aut(A) ‚âÖ S. Conjugation by even elements are inner automorphisms of A while the outer automorphism of A of order 2 corresponds to conjugation by an odd element. For , there is an exceptional outer automorphism of A so S is not the full automorphism group of A.

Conversely, for , S has no outer automorphisms, and for it has no center, so for it is a complete group, as discussed in automorphism group, below.

For , S is an almost simple group, as it lies between the simple group A and its group of automorphisms.

S can be embedded into A by appending the transposition to all odd permutations, while embedding into A is impossible for .

The symmetric group on letters is generated by the adjacent transpositions formula_18 that swap and . The collection formula_19 generates subject to the following relations:
where 1 represents the identity permutation. This representation endows the symmetric group with the structure of a Coxeter group (and so also a reflection group).

Other possible generating sets include the set of transpositions that swap and for , and a set containing any -cycle and a -cycle of adjacent elements in the -cycle.

A subgroup of a symmetric group is called a permutation group.

The normal subgroups of the finite symmetric groups are well understood. If , S has at most 2 elements, and so has no nontrivial proper subgroups. The alternating group of degree "n" is always a normal subgroup, a proper one for and nontrivial for ; for it is in fact the only non-identity proper normal subgroup of S, except when where there is one additional such normal subgroup, which is isomorphic to the Klein four group.

The symmetric group on an infinite set does not have a subgroup of index 2, as Vitali (1915) proved that each permutation can be written as a product of three squares. However it contains the normal subgroup "S" of permutations that fix all but finitely many elements, which is generated by transpositions. Those elements of "S" that are products of an even number of transpositions form a subgroup of index 2 in "S", called the alternating subgroup "A". Since "A" is even a characteristic subgroup of "S", it is also a normal subgroup of the full symmetric group of the infinite set. The groups "A" and "S" are the only non-identity proper normal subgroups of the symmetric group on a countably infinite set. This was first proved by Onofri (1929) and independently Schreier-Ulam (1934). For more details see or .

The maximal subgroups of the finite symmetric groups fall into three classes: the intransitive, the imprimitive, and the primitive. The intransitive maximal subgroups are exactly those of the form for . The imprimitive maximal subgroups are exactly those of the form Sym("k") wr Sym("n"/"k") where is a proper divisor of "n" and "wr" denotes the wreath product acting imprimitively. The primitive maximal subgroups are more difficult to identify, but with the assistance of the O'Nan‚ÄìScott theorem and the classification of finite simple groups, gave a fairly satisfactory description of the maximal subgroups of this type according to .

The Sylow subgroups of the symmetric groups are important examples of "p"-groups. They are more easily described in special cases first:

The Sylow "p"-subgroups of the symmetric group of degree "p" are just the cyclic subgroups generated by "p"-cycles. There are such subgroups simply by counting generators. The normalizer therefore has order and is known as a Frobenius group (especially for ), and is the affine general linear group, .

The Sylow "p"-subgroups of the symmetric group of degree "p" are the wreath product of two cyclic groups of order "p". For instance, when , a Sylow 3-subgroup of Sym(9) is generated by and the elements "x" = (1 2 3), "y" = (4 5 6), "z" = (7 8 9), and every element of the Sylow 3-subgroup has the form "a""x""y""z" for 0 ‚â§ "i","j","k","l" ‚â§ 2.

The Sylow "p"-subgroups of the symmetric group of degree "p" are sometimes denoted W("n"), and using this notation one has that is the wreath product of W("n") and W(1).

In general, the Sylow "p"-subgroups of the symmetric group of degree "n" are a direct product of "a" copies of W("i"), where 0 ‚â§ "a" ‚â§ "p" ‚àí 1 and "n" = "a"¬†+¬†"p"¬∑"a"¬†+¬†...¬†+¬†"p"¬∑"a" (the base "p" expansion of "n").

For instance, W(1) =¬†C and W(2) =¬†D, the dihedral group of order 8, and so a Sylow 2-subgroup of the symmetric group of degree 7 is generated by and is isomorphic to .

These calculations are attributed to and described in more detail in . Note however that attributes the result to an 1844 work of Cauchy, and mentions that it is even covered in textbook form in .

A transitive subgroup of S is a subgroup whose action on {1,¬†2,¬†...,¬†"n"} is transitive. For example, the Galois group of a (finite) Galois extension is a transitive subgroup of S, for some "n".

Cayley's theorem states that every group "G" is isomorphic to a subgroup of some symmetric group. In particular, one may take a subgroup of the symmetric group on the elements of "G", since every group acts on itself faithfully by (left or right) multiplication.

For , S is a complete group: its center and outer automorphism group are both trivial.

For , the automorphism group is trivial, but S is not trivial: it is isomorphic to C, which is abelian, and hence the center is the whole group.

For , it has an outer automorphism of order 2: , and the automorphism group is a semidirect product .

In fact, for any set "X" of cardinality other than 6, every automorphism of the symmetric group on "X" is inner, a result first due to according to .

The group homology of S is quite regular and stabilizes: the first homology (concretely, the abelianization) is:

The first homology group is the abelianization, and corresponds to the sign map S ‚Üí S which is the abelianization for "n" ‚â• 2; for "n" < 2 the symmetric group is trivial. This homology is easily computed as follows: S is generated by involutions (2-cycles, which have order 2), so the only non-trivial maps are to S and all involutions are conjugate, hence map to the same element in the abelianization (since conjugation is trivial in abelian groups). Thus the only possible maps send an involution to 1 (the trivial map) or to ‚àí1 (the sign map). One must also show that the sign map is well-defined, but assuming that, this gives the first homology of S.

The second homology (concretely, the Schur multiplier) is:
This was computed in , and corresponds to the double cover of the symmetric group, 2 ¬∑ S.

Note that the exceptional low-dimensional homology of the alternating group (formula_26 corresponding to non-trivial abelianization, and formula_27 due to the exceptional 3-fold cover) does not change the homology of the symmetric group; the alternating group phenomena do yield symmetric group phenomena ‚Äì the map formula_28 extends to formula_29 and the triple covers of A and A extend to triple covers of S and S ‚Äì but these are not "homological" ‚Äì the map formula_30 does not change the abelianization of S, and the triple covers do not correspond to homology either.

The homology "stabilizes" in the sense of stable homotopy theory: there is an inclusion map , and for fixed "k", the induced map on homology is an isomorphism for sufficiently high "n". This is analogous to the homology of families Lie groups stabilizing.

The homology of the infinite symmetric group is computed in , with the cohomology algebra forming a Hopf algebra.

The representation theory of the symmetric group is a particular case of the representation theory of finite groups, for which a concrete and detailed theory can be obtained. This has a large area of potential applications, from symmetric function theory to problems of quantum mechanics for a number of identical particles.

The symmetric group S has order "n"<nowiki>!</nowiki>. Its conjugacy classes are labeled by partitions of¬†"n". Therefore, according to the representation theory of a finite group, the number of inequivalent irreducible representations, over the complex numbers, is equal to the number of partitions of¬†"n". Unlike the general situation for finite groups, there is in fact a natural way to parametrize irreducible representation by the same set that parametrizes conjugacy classes, namely by partitions of "n" or equivalently Young diagrams of size¬†"n".

Each such irreducible representation can be realized over the integers (every permutation acting by a matrix with integer coefficients); it can be explicitly constructed by computing the Young symmetrizers acting on a space generated by the Young tableaux of shape given by the Young diagram.

Over other fields the situation can become much more complicated. If the field "K" has characteristic equal to zero or greater than "n" then by Maschke's theorem the group algebra "K"S is semisimple. In these cases the irreducible representations defined over the integers give the complete set of irreducible representations (after reduction modulo the characteristic if necessary).

However, the irreducible representations of the symmetric group are not known in arbitrary characteristic. In this context it is more usual to use the language of modules rather than representations. The representation obtained from an irreducible representation defined over the integers by reducing modulo the characteristic will not in general be irreducible. The modules so constructed are called "Specht modules", and every irreducible does arise inside some such module. There are now fewer irreducibles, and although they can be classified they are very poorly understood. For example, even their dimensions are not known in general.

The determination of the irreducible modules for the symmetric group over an arbitrary field is widely regarded as one of the most important open problems in representation theory.





</doc>
<doc id="28902" url="https://en.wikipedia.org/wiki?curid=28902" title="SMS (disambiguation)">
SMS (disambiguation)

SMS is short message service, a form of text-messaging communication based on phones.

SMS may also refer to:










</doc>
<doc id="28903" url="https://en.wikipedia.org/wiki?curid=28903" title="SMPP">
SMPP

SMPP may refer to:



</doc>
<doc id="28904" url="https://en.wikipedia.org/wiki?curid=28904" title="Short Message Peer-to-Peer">
Short Message Peer-to-Peer

Short Message Peer-to-Peer (SMPP) in the telecommunications industry is an open, industry standard protocol designed to provide a flexible data communication interface for the transfer of short message data between External Short Messaging Entities (ESMEs), Routing Entities (REs) and SMSC.

SMPP is often used to allow third parties (e.g. value-added service providers like news organizations) to submit messages, often in bulk, but it may be used for SMS peering as well. SMPP is able to carry short messages including EMS, voicemail notifications, Cell Broadcasts, WAP messages including WAP Push messages (used to deliver MMS notifications), USSD messages and others. Because of its versatility and support for non-GSM SMS protocols, like UMTS, IS-95 (CDMA), CDMA2000, ANSI-136 (TDMA) and iDEN, SMPP is the most commonly used protocol for short message exchange outside SS7 networks.

SMPP (Short Message Peer-to-Peer) was originally designed by Aldiscon, a small Irish company that was later acquired by Logica (since 2016, after a number of changes Mavenir). The protocol was originally created by a developer, Ian J Chambers, to test the functionality of the SMSC without using SS7 test equipment to submit messages. In 1999, Logica formally handed over SMPP to the SMPP Developers Forum, later renamed as The SMS Forum and now disbanded. The SMPP protocol specifications are still available through the website which also carries a notice stating that it will be taken down at the end of 2007. As part of the original handover terms, SMPP ownership has now returned to Mavenir due to the disbanding of the SMS Forum.

To date, SMPP development is suspended and SMS Forum is disbanded. From the SMS Forum website:

July 31, 2007 - The SMS Forum, a non-profit organization with a mission to develop, foster and promote SMS (short message service) to the benefit of the global wireless industry will disband by July 27, 2007

A press release, attached to the news, also warns that site will be suspended soon. In spite of this, the site is still mostly functioning and specifications can still be downloaded (as of 31 January 2012).

The site has ceased operation according to Cormac Long, former technical moderator and webmaster for the SMS Forum. Please contact Mavenir for the SMPP specification. The specifications are also available from the former site of the SMPP Developers Forum (predecessor to SMS Forum) at SMPP Protocol - SMS API.

Contrary to its name, the SMPP uses the client-server model of operation. The Short Message Service Center (SMSC) usually acts as a server, awaiting connections from ESMEs. When SMPP is used for SMS peering, the sending MC usually acts as a client.

The protocol is based on pairs of request/response PDUs (protocol data units, or packets) exchanged over OSI layer 4 (TCP session or X.25 SVC3) connections. The well-known port assigned by the IANA for SMPP when operating over TCP is 2775, but multiple arbitrary port numbers are often used in messaging environments.

Before exchanging any messages, a bind command must be sent and acknowledged. The bind command determines in which direction will be possible to send messages; bind_transmitter only allows client to submit messages to the server, bind_receiver means that the client will only receive the messages, and bind_transceiver (introduced in SMPP 3.4) allows message transfer in both directions. In the bind command the ESME identifies itself using system_id, system_type and password; the address_range field designed to contain ESME address is usually left empty. The bind command contains interface_version parameter to specify which version of SMPP protocol will be used.

Message exchange may be synchronous, where each peer waits for a response for each PDU being sent, or asynchronous, where multiple requests can be issued without waiting and acknowledged in a skew order by the other peer; the number of unacknowledged requests is called a "window"; for the best performance both communicating sides must be configured with the same window size.

The SMPP standard has evolved during the time. The most commonly used versions of SMPP are:


The applicable version is passed in the interface_version parameter of a bind command.

The SMPP PDUs are binary encoded for efficiency. They start with a header which may be followed by a body:

Each PDU starts with a header. The header consists of 4 fields, each of length of 4 octets:


All numeric fields in SMPP use the big endian order, which means that the first octet is the Most Significant Byte (MSB).

This is an example of the binary encoding of a 60-octet "submit_sm" PDU. The data is shown in Hex octet values as a single dump and followed by a header and body break-down of that PDU.

This is best compared with the definition of the submit_sm PDU from the SMPP specification in order to understand how the encoding matches the field by field definition.

The value break-downs are shown with decimal in parentheses and Hex values after that. Where you see one or several hex octets appended, this is because the given field size uses 1 or more octets encoding.

Again, reading the definition of the submit_sm PDU from the spec will make all this clearer.

 'command_length', (60) ... 00 00 00 3C

 'service_type', () ... 00

Note that the text in the short_message field must match the data_coding. When the data_coding is 8 (UCS2), the text must be in UCS-2BE (or its extension, UTF-16BE). When the data_coding indicates a 7-bit encoding, each septet is stored in a separate octet in the short_message field (with the most significant bit set to 0). SMPP 3.3 data_coding exactly copied TP-DCS values of GSM 03.38, which make it suitable only for GSM 7-bit default alphabet, UCS2 or binary messages; SMPP 3.4 introduced a new list of data_coding values:

The meaning of the data_coding=4 or 8 is the same as in SMPP 3.3. Other values in the range 1-15 are reserved in SMPP 3.3. Unfortunately, unlike SMPP 3.3, where data_coding=0 was unambiguously GSM 7-bit default alphabet, for SMPP 3.4 and higher the GSM 7-bit default alphabet is missing in this list, and data_coding=0 may differ for various Short message service centers‚Äîit may be ISO-8859-1, ASCII, GSM 7-bit default alphabet, UTF-8 or even configurable per ESME. When using data_coding=0, both sides (ESME and SMSC) must be sure they consider it the same encoding. Otherwise it is better not to use data_coding=0. It may be tricky to use the GSM 7-bit default alphabet, some Short message service centers requires data_coding=0, others e.g. data_coding=241.

Despite its wide acceptance, the SMPP has a number of problematic features:


Although data_coding value in SMPP 3.3 are based on the GSM 03.38, since SMPP 3.4 there is no data_coding value for GSM 7-bit alphabet (GSM 03.38). However, it is common for DCS=0 to indicate the GSM 7-bit alphabet, particularly for SMPP connections to SMSCs on GSM mobile networks.

According to SMPP 3.4 and 5.0 the data_coding=0 means ‚Ä≥SMSC Default Alphabet‚Ä≥. Which encoding it really is, depends on the type of the SMSC and its configuration.

One of the encodings in CDMA standard C.R1001 is Shift-JIS used for Japanese. SMPP 3.4 and 5.0 specifies three encodings for Japanese (JIS, ISO-2022-JP and Extended Kanji JIS), but none of them is identical with CDMA MSG_ENCODING 00101. It seems that the Pictogram encoding (data_coding=9) is used to carry the messages in Shift-JIS in SMPP.

When a submit_sm fails, the SMSC returns a submit_sm_resp with non-zero value of command_status and ‚Ä≥empty‚Ä≥ message_id.


For the best compatibility, any SMPP implementation should accept both variants of negative submit_sm_resp regardless of the version of SMPP standard used for the communication.

The only way to pass delivery receipts in SMPP 3.3 is to put information in a text form to the short_message field; however, the format of the text is described in Appendix B of SMPP 3.4, although SMPP 3.4 may (and should) use receipted_message_id and message_state for the purpose. While SMPP 3.3 states that Message ID is a C-Octet String (Hex) of up to 8 characters (plus terminating '\0'), the SMPP 3.4 states that the id field in the Delivery Receipt Format is a C-Octet String (Decimal) of up to 10 characters. This splits SMPP implementations to 2 groups:


Since introduction of Tag-Length-Value (TLV) parameters in version 3.4, the SMPP may be regarded an extensible protocol. In order to achieve the highest possible degree of compatibility and interoperability any implementation should apply the Internet robustness principle: ‚Ä≥Be conservative in what you send, be liberal in what you accept‚Ä≥. It should use a minimal set of features which are necessary to accomplish a task. And if the goal is communication and not quibbling, each implementation should overcome minor nonconformities with standard:


Information applicable to one version of SMPP can often be found in another version of SMPP, for example with the case of SMPP 3.4 describing the only mechanism of delivery receipts in SMPP 3.3 described above.

The SMPP protocol is designed on a clear-text binary protocol which needs to be considered if using for potentially sensitive information such as one-time passwords via SMS. There are, however, implementations of SMPP over secure SSL/TLS if required.




</doc>
<doc id="28908" url="https://en.wikipedia.org/wiki?curid=28908" title="Suburb">
Suburb

A suburb or suburban area is a mixed-use or residential area, existing either as part of a city or urban area or as a separate residential community within commuting distance of a city. Suburbs might have their own political jurisdiction, especially in the United States, but this is not always the case, especially in the United Kingdom where suburbs are located within the administrative boundaries of cities. In most English-speaking countries, suburban areas are defined in contrast to central or inner-city areas, but in Australian English and South African English, "suburb" has become largely synonymous with what is called a "neighborhood" in other countries and the term extends to inner-city areas. In some areas, such as Australia, India, China, New Zealand, the United Kingdom, and parts of the United States and Canada, new suburbs are routinely annexed by adjacent cities. In others, such as Morocco, France, and much of the United States and Canada, many suburbs remain separate municipalities or are governed as part of a larger local government area such as a county. In the United States, beyond the suburbs are exurbs, or "exurban areas", with less density but linked to the metropolitan area economically and by commuters.

Suburbs first emerged on a large scale in the 19th and 20th centuries as a result of improved rail and road transport, which led to an increase in commuting. In general, they have lower population densities than inner city neighborhoods within a metropolitan area, and most residents commute to central cities or other business districts; however, there are many exceptions, including industrial suburbs, planned communities, and satellite cities. Suburbs tend to proliferate around cities that have an abundance of adjacent flat land.

The English word is derived from the Old French "subburbe", which is in turn derived from the Latin "suburbium", formed from "sub" (meaning "under" or "below") and "urbs" ("city"). The first recorded usage of the term in English, was made by John Wycliffe in 1380, where the form "subarbis" was used, according to the "Oxford English Dictionary".

In Australia and New Zealand, suburbs (in the wider sense noted in the lead paragraph) have become formalised as geographic subdivisions of a city and are used by postal services in addressing. In rural areas in both countries, their equivalents are called localities (see suburbs and localities). The terms "inner suburb" and "outer suburb" are used to differentiate between the higher-density areas in proximity to the city centre (which would not be referred to as 'suburbs' in most other countries), and the lower-density suburbs on the outskirts of the urban area. The term 'middle suburbs' is also used. Inner suburbs, such as Te Aro in Wellington, Eden Terrace in Auckland, Prahran in Melbourne and Ultimo in Sydney, are usually characterised by higher density apartment housing and greater integration between commercial and residential areas.

In New Zealand, most suburbs are not legally defined which can lead to confusion as to where they may begin and end. Although there is a geospatial file defining suburbs for use by emergency services developed and maintained by Fire and Emergency New Zealand (formerly the New Zealand Fire Service), in collaboration with other government agencies, to date this file has not been released publicly. New Zealand company Koordinates Limited requested access to the geospatial file under the Official Information Act 1982 but this request was rejected by the New Zealand Fire Service on the basis that it would prejudice the health & safety of, or cause material loss, to the public. In September 2014 a decision was made by the Ombudsman of New Zealand ruling that the New Zealand Fire Service refusal to release the geospatial file without agreeing to terms which included, among other restrictions, a prohibition on redistribution of the geospatial file, was reasonable.

In the United Kingdom and in Ireland, "suburb" merely refers to a residential area outside the city centre, regardless of administrative boundaries. Suburbs, in this sense, can range from areas that seem more like residential areas of a city proper to areas separated by open countryside from the city centre. In large cities such as London and Leeds, suburbs include formerly separate towns and villages that have been gradually absorbed during a city's growth and expansion, such as Ealing, Bromley, and Guiseley.

In the United States and Canada, "suburb" can refer either to an outlying residential area of a city or town or to a separate municipality or unincorporated area outside a town or city.

The earliest appearance of suburbs coincided with the spread of the first urban settlements. Large walled towns tended to be the focus around which smaller villages grew up in a symbiotic relationship with the market town. The word 'suburbani' was first employed by the Roman statesman Cicero in reference to the large villas and estates built by the wealthy patricians of Rome on the city's outskirts.

Towards the end of the Eastern Han Dynasty (up until 190 AD, when Dong Zhuo razed the city), the capital, Luoyang, was mainly occupied by the emperor and important officials; the city's people mostly lived in small cities right outside Luoyang, which were suburbs in all but name.

As populations grew during the Early Modern Period in Europe, urban towns swelled with a steady influx of people from the countryside. In some places, nearby settlements were swallowed up as the main city expanded. The peripheral areas on the outskirts of the city were generally inhabited by the very poorest.

Due to the rapid migration of the rural poor to the industrialising cities of England in the late 18th century, a trend in the opposite direction began to develop; that is, newly rich members of the middle classes began to purchase estates and villas on the outskirts of London. This trend accelerated through the 19th century, especially in cities like London and Manchester that were growing rapidly, and the first suburban districts sprung up around the city centres to accommodate those who wanted to escape the squalid conditions of the industrial towns. Toward the end of the century, with the development of public transit systems such as the underground railways, trams and buses, it became possible for the majority of the city's population to reside outside the city and to commute into the center for work.

By the mid-19th century, the first major suburban areas were springing up around London as the city (then the largest in the world) became more overcrowded and unsanitary. A major catalyst for suburban growth was the opening of the Metropolitan Railway in the 1860s. The line joined the capital's financial heart in the City to what were to become the suburbs of Middlesex. Harrow was reached in 1880.

Unlike other railway companies, which were required to dispose of surplus land, the Met was allowed to retain such land that it believed was necessary for future railway use. Initially, the surplus land was managed by the Land Committee, and, from the 1880s, the land was developed and sold to domestic buyers in places like Willesden Park Estate, Cecil Park, near Pinner and at Wembley Park.

In 1912, it was suggested that a specially formed company should take over from the Surplus Lands Committee and develop suburban estates near the railway. However, World War I delayed these plans and it was only in 1919, with the expectation of a postwar housing boom, that Metropolitan Railway Country Estates Limited (MRCE) was formed. MRCE went on to develop estates at Kingsbury Garden Village near Neasden, Wembley Park, Cecil Park and Grange Estate at Pinner and the Cedars Estate at Rickmansworth and create places such as Harrow Garden Village.

The term "Metro-land" was coined by the Met's marketing department in 1915 when the "Guide to the Extension Line" became the "Metro-land" guide, priced at 1d. This promoted the land served by the Met for the walker, visitor and later the house-hunter. Published annually until 1932, the last full year of independence for the Met, the guide extolled the benefits of "The good air of the Chilterns", using language such as "Each lover of Metroland may well have his own favourite wood beech and coppice¬†‚Äî all tremulous green loveliness in Spring and russet and gold in October". The dream promoted was of a modern home in beautiful countryside with a fast railway service to central London. By 1915, people from across London had flocked to live the new suburban dream in large newly built areas across North West London.

Suburbanisation in the interwar period was heavily influenced by the garden city movement of Ebenezer Howard and the creation of the first garden suburbs at the turn of the 20th century. The first garden suburb was developed through the efforts of social reformer Henrietta Barnett and her husband; inspired by Ebenezer Howard and the model housing development movement (then exemplified by Letchworth garden city), as well as the desire to protect part of Hampstead Heath from development, they established trusts in 1904 which bought 243 acres of land along the newly opened Northern line extension to Golders Green and created the Hampstead Garden Suburb. The suburb attracted the talents of architects including Raymond Unwin and Sir Edwin Lutyens, and it ultimately grew to encompass over 800 acres.

During the First World War the Tudor Walters Committee was commissioned to make recommendations for the post war reconstruction and housebuilding. In part, this was a response to the shocking lack of fitness amongst many recruits during World War One, attributed to poor living conditions; a belief summed up in a housing poster of the period "you cannot expect to get an A1 population out of C3 homes" - referring to military fitness classifications of the period.

The Committee's report of 1917 was taken up by the government, which passed the Housing, Town Planning, &c. Act 1919, also known as the Addison Act after Dr. Christopher Addison, the then Minister for Housing. The Act allowed for the building of large new housing estates in the suburbs after the First World War, and marked the start of a long 20th century tradition of state-owned housing, which would later evolve into council estates.

The Report also legislated on the required, minimum standards necessary for further suburban construction; this included regulation on the maximum housing density and their arrangement and it even made recommendations on the ideal number of bedrooms and other rooms per house. Although the semi-detached house was first designed by the Shaws (a father and son architectural partnership) in the 19th century, it was during the suburban housing boom of the interwar period that the design first proliferated as a suburban icon, being preferred by middle class home owners to the smaller terraced houses. The design of many of these houses, highly characteristic of the era, was heavily influenced by the Art Deco movement, taking influence from Tudor Revival, chalet style, and even ship design.

Within just a decade suburbs dramatically increased in size. Harrow Weald went from just 1,500 to over 10,000 while Pinner jumped from 3,000 to over 20,000. During the 1930s, over 4 million new suburban houses were built, the 'suburban revolution' had made England the most heavily suburbanized country in the world, by a considerable margin.

Boston and New York spawned the first major suburbs. The streetcar lines in Boston and the rail lines in Manhattan made daily commutes possible. No metropolitan area in the world was as well served by railroad commuter lines at the turn of the twentieth century as New York, and it was the rail lines to Westchester from the Grand Central Terminal commuter hub that enabled its development. Westchester's true importance in the history of American suburbanization derives from the upper-middle class development of villages including Scarsdale, New Rochelle and Rye serving thousands of businessmen and executives from Manhattan.

The suburban population in North America exploded during the post-World War II economic expansion. Returning veterans wishing to start a settled life moved in masses to the suburbs. Levittown developed as a major prototype of mass-produced housing. Due to the influx of people in these suburban areas, the amount of shopping centers began to increase as suburban America took shape. These malls helped supply goods and services to the growing urban population. Shopping for different goods and services in one central location without having to travel to multiple locations, helped to keep shopping centers a component of these newly designed suburbs which were booming in population. The television helped contribute to the rise of shopping centers due to the increased advertisement on television in addition to a desire to have products shown in suburban life in various television programs. Another factor that led to the rise of these shopping centers was the building of many highways. The Highway Act of 1956 helped to fund the building of 64,000 kilometers across the nation by having $26 thousand-million to use, which helped to link many more to these shopping centers with ease. These newly built shopping centers, which were often large buildings full of multiple stores, and services, were being used for more than shopping, but as a place of leisure and a meeting point for those who lived within suburban America at this time. These centers thrived offering goods and services to the growing populations in suburban America. In 1957, 940 Shopping centers were built and this number more than doubled by 1960 to keep up with the demand of these densely populated areas. 

Very little housing had been built during the Great Depression and World War II, except for emergency quarters near war industries. Overcrowded and inadequate apartments was the common condition. Some suburbs had developed around large cities where there was rail transportation to the jobs downtown. However, the real growth in suburbia depended on the availability of automobiles, highways, and inexpensive housing. The population had grown, and the stock of family savings had accumulated the money for down payments, automobiles and appliances. The product was a great housing boom. Whereas, an average of 316,000 new housing non-farm units should have been constructed 1930s through 1945, there were 1,450,000 annually from 1946 through 1955. The G.I. Bill guaranteed low cost loans for veterans, with very low down payments, and low interest rates. With 16 million eligible veterans, the opportunity to buy a house was suddenly at hand. In 1947 alone, 540,000 veterans bought one; their average price was $7300. The construction industry kept prices low by standardization ‚Äì for example standardizing sizes for kitchen cabinets, refrigerators and stoves, allowed for mass production of kitchen furnishings. Developers purchased empty land just outside the city, installed tract houses based on a handful of designs, and provided streets and utilities, or local public officials race to build schools. The most famous development was Levittown, in Long Island just east of New York City. It offered a new house for $1000 down, and $70 a month; it featured three bedrooms, fireplace, gas range and gas furnace, and a landscaped lot of 75 by 100 feet, all for a total price of $10,000. Veterans could get one with a much lower down payment.

At the same time, African Americans were rapidly moving north and west for better jobs and educational opportunities than were available to them in the segregated South. Their arrival in Northern and Western cities en masse, in addition to being followed by race riots in several large cities such as Philadelphia, Los Angeles, Detroit, Chicago, and Washington, D.C., further stimulated white suburban migration. The growth of the suburbs was facilitated by the development of zoning laws, redlining and numerous innovations in transport. After World War II, availability of FHA loans stimulated a housing boom in American suburbs. In the older cities of the northeast U.S., streetcar suburbs originally developed along train or trolley lines that could shuttle workers into and out of city centers where the jobs were located. This practice gave rise to the term "bedroom community", meaning that most daytime business activity took place in the city, with the working population leaving the city at night for the purpose of going home to sleep.

Economic growth in the United States encouraged the suburbanization of American cities that required massive investments for the new infrastructure and homes. Consumer patterns were also shifting at this time, as purchasing power was becoming stronger and more accessible to a wider range of families. Suburban houses also brought about needs for products that were not needed in urban neighborhoods, such as lawnmowers and automobiles. During this time commercial shopping malls were being developed near suburbs to satisfy consumers' needs and their car‚Äìdependent lifestyle.

Zoning laws also contributed to the location of residential areas outside of the city center by creating wide areas or "zones" where only residential buildings were permitted. These suburban residences are built on larger lots of land than in the central city. For example, the lot size for a residence in Chicago is usually deep, while the width can vary from wide for a row house to wide for a large stand‚Äìalone house. In the suburbs, where stand‚Äìalone houses are the rule, lots may be wide by deep, as in the Chicago suburb of Naperville. Manufacturing and commercial buildings were segregated in other areas of the city.

Alongside suburbanization, many companies began locating their offices and other facilities in the outer areas of the cities, which resulted in the increased density of older suburbs and the growth of lower density suburbs even further from city centers. An alternative strategy is the deliberate design of "new towns" and the protection of green belts around cities. Some social reformers attempted to combine the best of both concepts in the garden city movement.

In the U.S., 1950 was the first year that more people lived in suburbs than elsewhere. In the U.S, the development of the skyscraper and the sharp inflation of downtown real estate prices also led to downtowns being more fully dedicated to businesses, thus pushing residents outside the city center.

In the 20th century, many suburban areas, especially those not within the political boundaries of the city containing the central business area, began to see independence from the central city as an asset. In some cases, suburbanites saw self-government as a means to keep out people who could not afford the added suburban property maintenance costs not needed in city living. Federal subsidies for suburban development accelerated this process as did the practice of redlining by banks and other lending institutions. In some cities such as Miami and San Francisco, the main city is much smaller than the surrounding suburban areas, leaving the city proper with a small portion of the metro area's population and land area.

Mesa, Arizona and Virginia Beach, the two most populous suburbs in the United States, are actually more populous than many of America's largest cities, including Miami, Minneapolis, New Orleans, Cleveland, Tampa, St. Louis, Pittsburgh, Cincinnati, and others. Virginia Beach is now the largest city in its metropolitan area of Hampton Roads, having long since exceeded the population of its neighboring primary city, Norfolk. While Virginia Beach has slowly been taking on the characteristics of an urban city, it will not likely achieve the population density and urban characteristics of Norfolk. It is generally assumed that the population of Chesapeake, another Hampton Roads city, will also exceed that of Norfolk in 2018 if its current growth rate continues at its same pace.

Cleveland, Ohio is typical of many American central cities; its municipal borders have changed little since 1922, even though the Cleveland urbanized area has grown many times over. Several layers of suburban municipalities now surround cities like Boston, Cleveland, Chicago, Detroit, Los Angeles, Dallas, Denver, Houston, New York City, San Francisco, Sacramento, Atlanta, Miami, Baltimore, Milwaukee, Pittsburgh, Philadelphia, Phoenix, Roanoke, St. Louis, Salt Lake City, Las Vegas, Minneapolis, and Washington, D.C..

Suburbs in the United States have a prevalence of usually detached single-family homes.

They are characterized by:


By 2010, suburbs increasingly gained people in racial minority groups, as many members of minority groups gained better access to education and sought more favorable living conditions compared to inner city areas.

Conversely, many white Americans also moved back to city centers. Nearly all major city downtowns (such as Downtown Miami, Downtown Detroit, Downtown Philadelphia, Downtown Roanoke, or Downtown Los Angeles) are experiencing a renewal, with large population growth, residential apartment construction, and increased social, cultural, and infrastructural investments, as have suburban neighborhoods close to city centers. Better public transit, proximity to work and cultural attractions, and frustration with suburban life and gridlock have attracted young Americans to the city centers.

Canada is an urbanized nation where over 80% of the population live in urban areas (loosely defined), and roughly two-thirds live in one of Canada's 33 census metropolitan areas (CMAs) with a population of over 100,000. However, of this metropolitan population, in 2001 nearly half lived in low-density neighborhoods, with only one in five living in a typical "urban" neighborhood. The percentage living in low-density neighborhoods varied from a high of nearly two-thirds of Calgary CMA residents (67%), to a low of about one-third of Montr√©al CMA residents (34%).

Often, Canadian suburbs are less automobile-centred and public transit use is encouraged but can be notably unused. Throughout Canada, there are comprehensive plans in place to curb sprawl.

Population and income growth in Canadian suburbs had tended to outpace growth in core urban or rural areas, but in many areas this trend has now reversed. The suburban population increased 87% between 1981 and 2001, well ahead of urban growth. The majority of recent population growth in Canada's three largest metropolitan areas (Greater Toronto, Greater Montr√©al, and Greater Vancouver) has occurred in non-core municipalities. This trend is also beginning to take effect in Vancouver, and to a lesser extent, Montr√©al. In certain cities, particularly Edmonton and Calgary, suburban growth takes place within the city boundaries as opposed to in bedroom communities. This is due to annexation and large geographic footprint within the city borders.
Calgary is unusual among Canadian cities because it has developed as a unicity - it has annexed most of its surrounding towns and large amounts of undeveloped land around the city. As a result, most of the communities that Calgarians refer to as "suburbs" are actually inside the city limits. In the 2016 census, the City of Calgary had a population of 1,239,220, whereas the Calgary Metropolitan Area had a population of 1,392,609, indicating the vast majority of people in the Calgary CMA lived within the city limits. The perceived low population density of Calgary largely results from its many internal suburbs and the large amount of undeveloped land within the city. The city actually has a policy of densifying its new developments.

In many parts of the developed world, suburbs can be economically distressed areas, inhabited by higher proportions of recent immigrants, with higher delinquency rates and social problems. Sometimes the notion of suburb may even refer to people in real misery, who are kept at the limit of the city borders for economic, social, and sometimes ethnic reasons. An example in the developed world would be the "banlieues" of France, or the concrete suburbs of Sweden, even if the suburbs of these countries also include middle-class and upper-class neighborhoods that often consist of single-family houses. Thus some of the suburbs of most of the developed world are comparable to several inner cities of the U.S.

The growth in the use of trains, and later automobiles and highways, increased the ease with which workers could have a job in the city while commuting in from the suburbs. In the United Kingdom, as mentioned above, railways stimulated the first mass exodus to the suburbs. The Metropolitan Railway, for example, was active in building and promoting its own housing estates in the north-west of London, consisting mostly of detached houses on large plots, which it then marketed as "Metro-land". The Australian and New Zealand usage came about as outer areas were quickly surrounded in fast-growing cities, but retained the appellation "suburb"; the term was eventually applied to the original core as well. In Australia, Sydney's urban sprawl has occurred predominantly in the Western Suburbs. The locality of Olympic Park was designated an official suburb in 2009.

In the UK, the government is seeking to impose minimum densities on newly approved housing schemes in parts of South East England. The goal is to "build sustainable communities" rather than housing estates. However, commercial concerns tend to delay the opening of services until a large number of residents have occupied the new neighbourhood.

In Mexico, suburbs are generally similar to their United States counterparts. Houses are made in many different architectural styles which may be of European, American and International architecture and which vary in size. Suburbs can be found in Guadalajara, Mexico City, Monterrey, and most major cities. Lomas de Chapultepec is an example of an affluent suburb, although it is located inside the city and by no means is today a suburb in the strict sense of the word. In other countries, the situation is similar to that of Mexico, with many suburbs being built, most notably in Peru and Chile, which have experienced a boom in the construction of suburbs since the late 1970s and early 80s. As the growth of middle-class and upper-class suburbs increased, low-class squatter areas have increased, most notably "lost cities" in Mexico, campamentos in Chile, barriadas in Peru, villa miserias in Argentina, asentamientos in Guatemala and favelas of Brazil.

Brazilian affluent suburbs are generally denser, more vertical and mixed in use inner suburbs. They concentrate infrastructure, investment and attention from the municipal seat and the best offer of mass transit. True sprawling towards neighboring municipalities is typically empoverished ‚Äì ("the periphery", in the sense of it dealing with spatial marginalization) ‚Äì, with a very noticeable example being the rail suburbs of Rio de Janeiro ‚Äì the North Zone, the Baixada Fluminense, the part of the West Zone associated with SuperVia's Ramal de Santa Cruz. These, in comparison with the inner suburbs, often prove to be remote, violent food deserts with inadequate sewer structure coverage, saturated mass transit, more precarious running water, electricity and communication services, and lack of urban planning and landscaping, while also not necessarily qualifying as actual or slums. They often are former agricultural land or wild areas settled through squatting, and grew in amount particularly due to mass rural exodus during the years of the military dictatorship. This is particularly true to S√£o Paulo, Rio de Janeiro and Bras√≠lia, which grew with migration from more distant and empoverished parts of the country and dealt with overpopulation as a result.

In Africa, since the beginning of the 1990s, the development of middle-class suburbs boomed. Due to the industrialization of many African countries, particularly in cities such as Cairo, Johannesburg and Lagos, the middle class has grown. In an illustrative case of South Africa, RDP housing has been built. In much of Soweto, many houses are American in appearance, but are smaller, and often consist of a kitchen and living room, two or three bedrooms, and a bathroom. However, there are more affluent neighborhoods, more comparable to American suburbs, particularly east of the FNB Stadium. In Cape Town there is a distinct European style which is due to the European influence during the mid-1600s when the Dutch conquered the area. Houses like these are called Cape Dutch Houses and can be found in the affluent suburbs of Constantia and Bishopscourt.

In the illustrative case of Rome, Italy, in the 1920s and 1930s, suburbs were intentionally created "ex novo" in order to give lower classes a destination, in consideration of the actual and foreseen massive arrival of poor people from other areas of the country. Many critics have seen in this development pattern (which was circularly distributed in every direction) also a quick solution to a problem of public order (keeping the unwelcome poorest classes together with the criminals, in this way better controlled, comfortably remote from the elegant "official" town). On the other hand, the expected huge expansion of the town soon effectively covered the distance from the central town, and now those suburbs are completely engulfed by the main territory of the town. Other newer suburbs (called exurbs) were created at a further distance from them.

In Russia, the term suburb refers to high-rise residential apartments which usually consist of two bedrooms, one bathroom, a kitchen and a living room. These suburbs, however are usually not in poor neighborhoods, unlike the banlieuees.
In China, the term suburb is new, although suburbs are already being constructed rapidly. Chinese suburbs mostly consist of rows upon rows of apartment blocks and condos that end abruptly into the countryside. Also new town developments are extremely common. Single family suburban homes tend to be similar to their Western equivalents; although primarily outside Beijing and Shanghai, also mimic Spanish and Italian architecture. 

In Hong Kong, however, suburbs are mostly government-planned new towns containing numerous public housing estates. New Towns such as Tin Shui Wai may gain notoriety as a slum. However, other new towns also contain private housing estates and low density developments for the upper classes.

In Japan, the construction of suburbs has boomed since the end of World War II and many cities are experiencing the urban sprawl effect.
In Malaysia, suburbs are common, especially in areas surrounding the Klang Valley, which is the largest conurbation in the country. These suburbs also serve as major housing areas and commuter towns. Terraced houses, semi-detached houses and shophouses are common concepts in suburbs. In certain areas such as Klang, Subang Jaya and Petaling Jaya, suburbs form the core of these places. The latter one has been turned into a satellite city of Kuala Lumpur. Suburbs are also evident in other major conurbations in the country including Penang (e.g. Pulau Tikus), Ipoh (e.g. Bercham), Johor Bahru (e.g. Tebrau), Kota Kinabalu (e.g. Likas), Kuching (e.g. Stampin), Melaka City (e.g. Batu Berendam) and Alor Setar (e.g. Anak Bukit).

Suburbs typically have longer travel times to work than traditional neighborhoods. Only the traffic "within" the short streets themselves is less. This is due to three factors: almost-mandatory automobile ownership due to poor suburban bus systems, longer travel distances and the hierarchy system, which is less efficient at distributing traffic than the traditional grid of streets.

In the suburban system, most trips from one component to another component requires that cars enter a collector road, no matter how short or long the distance is. This is compounded by the hierarchy of streets, where entire neighborhoods and subdivisions are dependent on one or two collector roads. Because all traffic is forced onto these roads, they are often heavy with traffic all day. If a traffic crash occurs on a collector road, or if road construction inhibits the flow, then the entire road system may be rendered useless until the blockage is cleared. The traditional "grown" grid, in turn, allows for a larger number of choices and alternate routes.

Suburban systems of the sprawl type are also quite inefficient for cyclists or pedestrians, as the direct route is usually not available for them either. This encourages car trips even for distances as low as several hundreds of yards or meters (which may have become up to several miles or kilometers due to the road network). Improved sprawl systems, though retaining the car detours, possess cycle paths and footpaths connecting across the arms of the sprawl system, allowing a more direct route while still keeping the cars out of the residential and side streets.

More commonly, central cities seek ways to tax nonresidents working downtown ‚Äì known as commuter taxes ‚Äì as property tax bases dwindle. Taken together, these two groups of taxpayers represent a largely untapped source of potential revenue that cities may begin to target more aggressively, particularly if they're struggling. According to struggling cities, this will help bring in a substantial revenue for the city which is a great way to tax the people who make the most use of the highways and repairs.

Today more companies settle down in suburbs because of low property costs.

The history of suburbia is part of the study of urban history, which focuses on the origins, growth, diverse typologies, culture, and politics of suburbs, as well as on the gendered and family-oriented nature of suburban space. Many people have assumed that early-20th-century suburbs were enclaves for middle-class whites, a concept that carries tremendous cultural influence yet is actually stereotypical. Some suburbs are based on a society of working-class and minority residents, many of whom want to own their own house. Meanwhile, other suburbs have instituted "explicitly racist" policies to deter people deemed as "other", a practice most common in the United States in contrast to other countries around the world. Mary Corbin Sies argues that it is necessary to examine how "suburb" is defined as well as the distinction made between cities and suburbs, geography, economic circumstances, and the interaction of numerous factors that move research beyond acceptance of stereotyping and its influence on scholarly assumptions.

Suburbs and suburban living have been the subject for a wide variety of films, books, television shows and songs.

French songs like "La Zone" by Fr√©hel (1933), "Aux quatre coins de la banlieue" by Damia (1936), "Ma banlieue" by Reda Caire (1937), or "Banlieue" by Robert Lamoureux (1953), evoke the suburbs of Paris explicitly since the 1930s. Those singers give a sunny festive, almost bucolic, image of the suburbs, yet still few urbanized. During the fifties and the sixties, French singer-songwriter L√©o Ferr√© evokes in his songs popular and proletarian suburbs of Paris, to oppose them to the city, considered by comparison as a bourgeois and conservative place.

French cinema was although soon interested in urban changes in the suburbs, with such movies as "Mon oncle" by Jacques Tati (1958), "L'Amour existe" by Maurice Pialat (1961) or "Two or Three Things I Know About Her" by Jean-Luc Godard (1967).

In his one-act opera "Trouble in Tahiti" (1952), Leonard Bernstein skewers American suburbia, which produces misery instead of happiness.

The American photojournalist Bill Owens documented the culture of suburbia in the 1970s, most notably in his book "Suburbia". The 1962 song "Little Boxes" by Malvina Reynolds lampoons the development of suburbia and its perceived bourgeois and conformist values, while the 1982 song "Subdivisions" by the Canadian band Rush also discusses suburbia, as does Rockin' the Suburbs by Ben Folds. The 2010 album "The Suburbs" by the Canadian-based alternative band Arcade Fire dealt with aspects of growing up in suburbia, suggesting aimlessness, apathy and endless rushing are ingrained into the suburban culture and mentality. "Suburb The Musical," was written by Robert S. Cohen and David Javerbaum. Over the Hedge is a syndicated comic strip written and drawn by Michael Fry and T. Lewis. It tells the story of a raccoon, turtle, a squirrel, and their friends who come to terms with their woodlands being taken over by suburbia, trying to survive the increasing flow of humanity and technology while becoming enticed by it at the same time. A film adaptation of Over the Hedge was produced in 2006.

British television series such as "The Good Life", "Butterflies" and "The Fall and Rise of Reginald Perrin" have depicted suburbia as well-manicured but relentlessly boring, and its residents as either overly conforming or prone to going stir crazy. In contrast, U.S. shows ‚Äì such as "Knots Landing", "Desperate Housewives" and "Weeds" ‚Äì portray the suburbs as concealing darker secrets behind a fa√ßade of perfectly manicured lawns, friendly people, and beautifully kept houses. Films such as "The 'Burbs", "Disturbia" and "Hot Fuzz", have brought this theme to the cinema. This trope was also used in the episode of "The X-Files" "Arcadia" and on one level of the video game "Psychonauts".





</doc>
<doc id="28910" url="https://en.wikipedia.org/wiki?curid=28910" title="Sh≈çnen manga">
Sh≈çnen manga

The kanji characters (Â∞ëÂπ¥) literally mean "boy" or "youth", and the characters (Êº´Áîª) means "comic". Thus, the complete phrase means "young person's comic", or simply "boys' comic"; its female equivalent is "sh≈çjo" manga. "Sh≈çnen" manga is the most popular and best-selling form of manga.

"Sh≈çnen" manga is typically characterized by high-action, often humorous plots featuring male protagonists. Commonly-found themes in "sh≈çnen" manga include martial arts, robots, science fiction, sports, horror or mythological creatures. The camaraderie between boys or men on sports teams, fighting squads, and the like are often emphasized. Protagonists of such manga often feature an ongoing desire to better themselves, and often face challenges to their abilities, skills and maturity, where self-perfection, austere self-discipline, sacrifice in the cause of duty and honorable service to society, community, family and friends are stressed.

None of these listed characteristics are a requirement, as seen in "sh≈çnen" manga like "Yotsuba&!", which features a female lead and almost no fan service or action; what defines whether or not a series is "sh≈çnen" is the official classification of the magazine it is serialized in.

The art style of "sh≈çnen" is generally less "flowery" than that of "sh≈çjo" manga, although this varies greatly from artist to artist, and some artists draw both "sh≈çnen" and "sh≈çjo" manga.

Manga has been said to have existed since the eighteenth century, but originally did not target a specific gender or age group. By 1905, however, a boom in publishing manga magazines occurred, and began targeting genders as evidenced by their names, such as "Sh≈çnen Sekai", "Sh≈çjo Sekai", and "Sh≈çnen Pakku" (a children's manga magazine). "Sh≈çnen Sekai" was one of the first "sh≈çnen" manga magazines, and was published from 1895 to 1914.

The post-World War II occupation of Japan had a profound impact on its culture during the 1950s and beyond (see culture of Post-occupation Japan), including on manga. Modern manga developed during this period, including the modern format of "sh≈çnen" manga we experience today, of which boys and young men were among the earliest readers. During this time, "sh≈çnen" manga focused on topics thought to interest the typical boy: sci-tech subjects like robots and space travel, and heroic action-adventure. Osamu Tezuka, creator of "Astro Boy" is said to have played an influential role in manga during this period. Between 1950 and 1969, an increasingly large readership for manga emerged in Japan with the solidification of its two main marketing genres, "sh≈çnen" manga aimed at boys and "sh≈çjo" manga aimed at girls.

The magazine "Weekly Sh≈çnen Jump" began production in 1968, and continues to be produced today as the best-selling manga magazine in Japan. Many of the most popular "sh≈çnen" manga titles have been serialized in "Jump", including "Dragon Ball", "Captain Tsubasa", "Slam Dunk", "One Piece", "Naruto", "Bleach", and others.

With the relaxation of censorship in Japan in the 1990s, a wide variety of explicit sexual themes appeared in manga intended for male readers, and correspondingly occur in English translations. However, in 2010 the Tokyo Metropolitan Government passed the controversial Bill 156 to restrict harmful content despite opposition by many authors and publishers in the manga industry.

Buronson's "Fist of the North Star" (1983‚Äì1988) and especially Akira Toriyama's "Dragon Ball" (1984‚Äì1995) are credited with setting the trend of popular "sh≈çnen" manga from the 1980s onwards. In turn, both manga works were influenced by the martial arts films of Hong Kong action cinema, particularly 1970s kung fu films such as Bruce Lee's "Enter the Dragon" (1973) and Jackie Chan's "Drunken Master" (1978). Manga critic Jason Thompson calls "Dragon Ball" "by far the most influential "sh≈çnen" manga of the last 30 years." Many currently successful "sh≈çnen" authors such as Eiichiro Oda, Masashi Kishimoto, Tite Kubo, Hiro Mashima and Kentaro Yabuki cite Toriyama and "Dragon Ball" as an influence on their own now popular works.

After the arrest and trial of serial killer Tsutomu Miyazaki, depictions of violence and sexual matters became more highly regulated in manga in general, but especially in "sh≈çnen" manga.

In early "sh≈çnen" manga, men and boys played all the major roles. Of the nine cyborgs in Shotaro Ishinomori's 1964 "Cyborg 009", only one is female, and she soon vanishes from the action. Even some more modern instances of "sh≈çnen" manga virtually omit women, e.g. the martial arts story "Baki the Grappler" by Itagaki Keisuke, and the supernatural fantasy "Sand Land" by Akira Toriyama. By the 1980s, however, girls and women began to play increasingly important roles in "sh≈çnen" manga. For example, in Toriyama's 1980 "Dr. Slump", the main character is the mischievous and powerful girl robot Arale Norimaki. Discussing his character Lisa Lisa from "Battle Tendency", the second story arc of the manga series "Jojo‚Äôs Bizarre Adventure", author Hirohiko Araki stated that at the time female characters in "sh≈çnen" manga were typically cute and designed to be "a man's ideal woman." He said readers were not interested in realistic portrayals of women, but rather the type of girl "that giggles during a conversation" with heart marks next to her. He believes this made the warrior-type Lisa Lisa feel fresh and "unheard of" in both manga and society in general and said it was exciting to challenge people's expectations with her. Araki also said that the supernatural basis of the fights in his series evened the battlefield for women and children to match up against strong men.

The role of girls and women in manga for male readers has evolved considerably since Arale. One class is the "bish≈çjo" or "beautiful young girl." Sometimes the woman is unattainable, and she is always an object of the hero's emotional and/or sexual interest, like Shao-lin from "Guardian Angel Getten" by Minene Sakurano or Belldandy from the "seinen" manga "Oh My Goddess!" by K≈çsuke Fujishima. In other stories, the hero is surrounded by such girls and women, as in "Negima! Magister Negi Magi" by Ken Akamatsu and "Hanaukyo Maid Team" by Morishige. The male protagonist does not always succeed in forming a relationship with the woman, for example when Bright Honda and Aimi Komori fail to bond in "Shadow Lady" by Masakazu Katsura. In other cases, a successful couple's sexual activities are depicted or implied, like in "Outlanders" by Johji Manabe. In still other cases, the initially naive and immature hero grows up to become a man by learning how to deal and live with women emotionally and sexually; examples of heroes who follow this path include Yota in "Video Girl Ai" by Masakazu Katsura and Train Man in the "seinen" manga "" by Hidenori Hara.

However, since the 1980s, there has been an increase in female protagonists in "sh≈çnen" manga, although they remain lesser in number than male protagonists. They are often portrayed as central characters or characters with important roles in manga. Some examples include "Fullmetal Alchemist", "Urusei Yatsura", "Inuyasha", "Attack on Titan", "Ranma ¬Ω", "Fairy Tail", "Gunslinger Girl", "WataMote", "Nisekoi", "Strawberry Marshmallow", "School Rumble" and "Soul Eater".




</doc>
<doc id="28912" url="https://en.wikipedia.org/wiki?curid=28912" title="Srebrenica">
Srebrenica

Srebrenica (, ) is a town and municipality located in the easternmost part of Republika Srpska, an entity of Bosnia and Herzegovina. It is a small mountain town, with its main industry being salt mining and a nearby spa. As of 2013, the town has a population of 2,607 inhabitants, while the municipality has 13,409 inhabitants.

During the Bosnian War, Srebrenica was the site of a massacre of more than 8,000 Bosniak men and boys, which was subsequently designated as an act of genocide by the ICTY and the International Court of Justice.

During the Roman times, there was a settlement of Domavia, known to have been near a mine. Silver ore from there was moved to the mints in Salona in the southwest and Sirmium in the northeast using the Via Argentaria.

A Roman tombstone was excavated close by Sase Monastery.

In the 13th and 14th century the region was part of the Banate of Bosnia, and, subsequently, the Bosnian Kingdom. The earliest reference to the name Srebrenica was in 1376, by which time it was already an important centre for trade in the western Balkans, based especially on the silver mines of the region. (Compare modern srebro "silver".) By that time, a large number of merchants of the Republic of Ragusa were established there, and they controlled the domestic silver trade and the export by sea, almost entirely via the port of Ragusa (Dubrovnik). During the 14th century, many German miners moved into the area. There were often armed conflicts about Srebrenica because of its mines. According to Czech historian Konstantin Josef Jireƒçek, from 1410 to 1460, Srebrenica switched hands several times, being Serbian five times, Bosnian four times, and Ottoman three times. The mines of Bosnian Podrinje and Usora were part of the Serbian Despotate prior to the Ottoman conquest.

With the town coming under Ottoman rule, becoming less influenced by the Republic of Ragusa, the economic importance of Srebrenica went into decline, as did the proportion of Christians in the population. The Franciscan church of St. Nicholas was converted into the White Mosque, but the large number of Catholics, Ragusan and Saxon, caused the transformation of the town to Islam to be slower than in most of the other towns in the area.

The area of Osat was liberated for a short time during the First Serbian Uprising (1804‚Äì13), under the leadership of Kara-Marko Vasiƒá from Crvica. Upon the breakout of the uprising, Metropolitan Had≈æi Melentije Stevanoviƒá contacted Vasiƒá, who met with the rebel leadership. After participated in battles on the Drina (1804), Vasiƒá asked Karaƒëorƒëe for an army to liberate Osat; Lazar Mutap was dispatched and the region came under rebel rule. In 1808, the Ottomans cleared out Osat, and by 1813, the rebels left the region.

The town came under Austro-Hungarian rule in 1878, when the Congress of Berlin approved the occupation of the Bosnia Vilayet, which later in 1908 became a condominium under the joint control of Austria and Hungary. The natural mineral water springs Crni Guber ("Black Guber") developed into an important part of the local economy. The Bohemian company Mattoni established a distribution infrastructure to tap and export the water named "Guber-Quelle" ("Guber Spring") throughout the monarchy and abroad. The construction of a spa was recommended. Modern infrastructure such as administration, electricity, roads, schools, telephone, healthcare, a postal service and other things were introduced. 

Although the Austrian rulers tried to stop the spread of nationalism and favoured a multi-religious and multi-cultural makeup with religious tolerance under their hegemony, Serbian nationalism was viewed with suspicion and hostility, since it demanded a unification of Bosnia with Serbia. As modern education raised the levels of general literacy, ideas spread through the advent of newspapers and publications. The region became increasingly restless as nationalism spread to all groups. 

During the First World War, one of the region's main battle areas was in Eastern Bosnia and the Drina, from where the units of Austria-Hungary advanced towards the Kingdom of Serbia. In late summer 1914 Srebrenica was taken over by Serbian volunteers under Kosta Todoroviƒá but later retaken by Austro-Hungarian units. Following World War I, Bosnia was incorporated into the South Slav kingdom of Serbs, Croats and Slovenes, which later was renamed Yugoslavia.

During the Second World War there were many atrocities committed by the Chetniks and Ustashas. Partisans fought Chetniks and Ustashe during the war and the people of Srebrenica built a partisan memorial cemetery monument for the fallen victims.

Tourism gained importance during the communist Yugoslav period and wellness spa and taking to the waters became an important part of the local economy. The "Banja Guber" was constructed for that purpose. Up to the 1990s over 90,000 overnight stays were recorded and an annual income of about three million dollars generated.

The town of Srebrenica came to international prominence as a result of events during the Bosnian War (1992‚Äì1995). The strategic objectives proclaimed by the secessionist Bosnian Serb presidency included the creation of a border separating the Serb people from Bosnia's other ethnic communities and the abolition of the border along the River Drina separating Serbia and the Bosnian Serbs' Republika Srpska. The Bosnian Muslim/Bosniak majority population of the Drina Valley posed a major obstacle to the achievement of these objectives. In the early days of the campaign of forcible transfer (ethnic cleansing) that followed the outbreak of war in April 1992 the town of Srebrenica was occupied by Serb/Serbian forces. It was subsequently retaken by Bosniak resistance groups. Refugees expelled from towns and villages across the central Drina valley sought shelter in Srebrenica, swelling the town's population.

The town and its surrounding area was surrounded and besieged by Serb forces. On 16 April 1993, the United Nations declared the Bosnian Muslim/Bosniak enclave a UN safe area, to be "free from any armed attack or any other hostile act", and guarded by a small Dutch unit operating under the mandate of United Nations Protection Force (UNPROFOR), which did not get permission to use force from the UN, which they needed to defend the local population.

Srebrenica and the other UN safe areas of ≈Ωepa and Gora≈æde were isolated pockets of Bosnian government-held territory in eastern Bosnia. In July 1995, despite the town's UN-protected status, it was attacked and captured by the Army of Republika Srpska. Following the town's capture, all men of fighting age who fell into Bosnian Serb hands were massacred in a systematically organised series of summary executions. The women of the town and men below 12 years of age and above 65 were transferred by bus to Tuzla. The Srebrenica massacre is considered the worst genocide in post-Second World War European history to this day.

In 2001, the Srebrenica massacre was determined by judgement of the International Criminal Tribunal for the former Yugoslavia (ICTY) to have been a crime of genocide (confirmed on appeal in 2004). This finding was upheld in 2007 by the International Court of Justice. The decision of the ICTY was followed by an admission to and an apology for the massacre by the Republika Srpska government.

Under the 1995 Dayton Agreement which ended the Bosnian War, Srebrenica was included in the territory assigned to Bosnian Serb control as the Republika Srpska entity of Bosnia and Herzegovina. Although guaranteed under the provisions of the Dayton Agreement, the return of survivors was repeatedly obstructed. In 2007, verbal and physical attacks on returning refugees continued to be reported in the region around Srebrenica.

In 1992, Bosniak villages around Srebrenica were under constant attacks by Serb forces. The Bosnian Institute in the United Kingdom has published a list of 296 villages destroyed by Serb forces around Srebrenica three years before the genocide and in the first three months of war (April‚ÄìJune 1992):

According to the Naser Oriƒá trial judgement:

The British National Archives in Kew released the documents dating back to July 1995 which deal with communication between British military and political actors during the Bosnian war. Several of the reports appear to blame the Bosniak Army (BiH) for provoking the Srebrenica attack. British intelligence doubted that Pale (Bosnian Serb headquarters) had any plans to overrun Srebrenica. Instead, the manoeuvre came as a response due to repeated Bosniak Army (BiH) attacks on BSA (Bosnian Serb Army) supply lines.

When Serbs entered the town, General Mladiƒá threatened to shell the Dutch camp if UN troops do not disarm Bosniak troops. However, the report confirms no Bosniak army soldiers remained at the camp, all 2,000 armed Muslims "had simply left during the night" in the direction of Tuzla.

The town has a religious makeup of roughly half Muslim and half Orthodox. Most of the town's 23 mosques that were destroyed were reconstructed with donations and aid, also from abroad. 

Unemployment rates are high since the economy was destroyed and reconstruction slow, as in many parts of the country. There are plans to revive the mineral water and spa business again, the reconstruction of the "Banja Guber" was scheduled for 2019 but subject to delays.

In 2007, Srebrenica's municipal assembly adopted a resolution demanding independence from the Republika Srpska entity (although not from Bosnia's sovereignty); the Serb members of the assembly did not vote on the resolution. In the 2016 elections Mladen Grujiƒçiƒá, a Bosnian Serb and native of the town of Srebrenica, was elected as mayor.

The municipality emblem was developed during the Yugoslav period and depicts a red and white stylised "S" with a depiction of the mineral water spring in the lower middle and a tree in the upper middle. The spring underscores the historical importance to the town's economy and the tree the nature and forests of the region.

The municipality (–æ–ø—à—Ç–∏–Ω–∞ or "op≈°tina") is further subdivided into the following local communities (–º—ò–µ—Å–Ω–µ –∑–∞—ò–µ–¥–Ω–∏—Ü–µ or "mjesne zajednice"):

The borders of the municipality in the 1953 and 1961 census were different. In 1953, a distinctive Muslim nationality had been yet to emerge as an ethnicity, leading Slavic Muslims to identify as Yugoslavs. As "Yugoslav" was itself not adopted in 1948, they were classified as "other" and while many self-identified as ‚ÄúSerbs‚Äù or ‚ÄúCroats‚Äù. Until 1961 census, the municipality of Srebrenica included today's territory of Bratunac municipality. The ethnic composition of the municipality:


Before 1992, there was a metal factory in the town, and lead, zinc, and gold mines nearby. The town's name (Srebrenica) means "silver mine", the same meaning of its old Latin name "Argentaria".

Before the war, Srebrenica also had a big spa and the town prospered from wellness tourism from the Crni Guber ("Black Guber") ferruginous spring water and other springs. Nowadays, Srebrenica has some tourism but a lot less developed than before the war. Currently, a pension, motel and a hostel are operating in the town.

The following table gives a preview of total number of registered people employed in legal entities per their core activity (as of 2018):



</doc>
<doc id="28913" url="https://en.wikipedia.org/wiki?curid=28913" title="Steve Bracks">
Steve Bracks

Stephen Phillip Bracks AC (born 15 October 1954) is a former Australian politician and the 44th Premier of Victoria. He first won the electoral district of Williamstown in 1994 for the Labor Party and was party leader and premier from 1999 to 2007

Bracks led Labor in Victoria to minority government at the 1999 election, defeating the incumbent Jeff Kennett Liberal and National coalition government. Labor was returned with a majority government after a landslide win at the 2002 election. Labor was elected for a third term at the 2006 election with a substantial but reduced majority. Bracks is the second-longest-serving Labor premier in Victorian history, only John Cain Jr. served for a longer period. The treasurer, John Brumby, became Labor leader and premier in 2007 when Bracks retired from politics.

Bracks will serve as the 6th Chancellor of Victoria University from 2021.

Steve Bracks was born in Ballarat, where his family owns a fashion business. He is a Lebanese Australian; his paternal grandfather came to Australia as a child from Zahl√© in the Beqaa Valley of Lebanon in the 1890s. His family were Melkite Catholic before migrating and became Roman Catholic.

Bracks was educated in Ballarat at St Patrick's College and the Ballarat College of Advanced Education (now the Federation University), where he graduated in business studies and education. He became a keen follower of Australian rules football, supporting the Geelong Football Club.

From 1976 to 1981 Bracks was a school commerce teacher at Sacred Heart College, Ballarat. During the 1980s he worked in local government in Ballarat and then as Executive Director of the Ballarat Education Centre. While in these positions he twice (1985 and 1988) contested the seat of Ballarat North in the Victorian Legislative Assembly for the Labor Party.

In 1989 Bracks was appointed statewide manager of Victorian state government employment programs, under the Labor government of John Cain Jr. He then became an adviser to both Cain and Cain's successor as Premier, Joan Kirner. Here he was able to witness from the inside the collapse of the Labor government following the economic and budgetary crisis which began in 1988. This experience gave Bracks a very conservative and cautious view of economic management in government.

Following the defeat of the Kirner government by the Liberal leader Jeff Kennett in late 1992, Bracks became Executive Director of the Victorian Printing Industry Training Board. He quit this post in 1994 when Kirner resigned from Parliament and Bracks was elected for Kirner's seat of Williamstown in the western suburbs of Melbourne, where he lived with his wife Terry and their three children. One of his children is Nick Bracks, Australian model.

Bracks was immediately elected to Labor's front bench, as Shadow Minister for Employment, Industrial Relations and Tourism. In 1996, after Labor under John Brumby was again defeated, he became Shadow Treasurer. In March 1999, when it became apparent that Labor was headed for another defeat under Brumby's leadership, Brumby resigned and Bracks was elected Opposition Leader.

Political observers were almost unanimous that Bracks had no chance of defeating Liberal premier Jeff Kennett at the September 1999 election: polls gave Kennett a 60% popularity rating. Bracks and his senior colleagues (particularly Brumby, who comes from Bendigo) campaigned heavily in regional areas, accusing Kennett of ignoring regional communities. In response, voters in regional areas deserted the Kennett government. On election night, much to its own surprise, Labor increased its seat count from 29 to 41, with the Liberals and their National Party allies retaining 43, and three falling to rural independents. With the Coalition one seat short of government, the election was to be decided in Frankston East, when the death of incumbent Peter McLellan forced a supplementary election. That supplementary election was won by Labor on a large swing, resulting in a hung parliament. The independents then threw their support to Labor, allowing Bracks to form government by one seat.

The Coalition briefly considered forcing Bracks to demonstrate that he had support on the floor of the Assembly. However, two of the independents, Russell Savage and Susan Davies, felt Kennett had given them short shrift in the previous legislature, and would not have even considered supporting him. In any event, this gambit was brought undone when Kennett announced his retirement from politics on 20 October. Bracks then advised the Governor, Sir James Gobbo, that he could form a government, which was duly sworn in later that day. Bracks became the first Catholic Labor Premier of Victoria since 1932.

Former leader Brumby, appointed Treasurer, was regarded as a major part of the government's success. He and the Deputy Premier and Minister for Health, John Thwaites, and the Attorney-General, Rob Hulls, were regarded as the key ministers in the Bracks government.

Following a pre-1999 election commitment to consider the feasibility of introducing fast rail services to regional centres, in 2000 the government approved funding for the Regional Fast Rail project, upgrading rail lines between Melbourne and Ballarat, Bendigo, Geelong and Traralgon. However, in 2006 the Victorian Auditor General noted that in spite of $750¬†million spent, "We found that the delivery of more frequent fast rail services in the Geelong, Ballarat, and Bendigo corridors by the agreed dates was not achieved. In total, the journey time outcomes will be more modest than we would have expected with only a minority of travellers likely to benefit from significant journey time improvements. These outcomes occur because giving some passengers full express services means bypassing often large numbers of passengers at intermediate stations along the corridors."

On 14 December 2000, Steve Bracks released a document outlining his government's intent to introduce the Racial and Religious Tolerance Act 2001.

The major criticism of Bracks's first government was that their insistence on consultation stood in the way of effective, proactive government. Bracks, according to critics, achieved little, and lost the excitement of constant change that was characteristic of the Kennett years. The talents of some of the more junior ministers in the government were also questioned. Nevertheless, Bracks got through his first term without major mishaps, and his popularity undiminished.

Labor won the 2002 election in a landslide, taking 62 seats out of 88 in the Legislative Assembly‚Äîonly the third time in Victoria's history that a Labor government had been reelected. In another first, Labor won a slim but clear majority in the Legislative Council as well. While this was the greatest victory Labor had ever had in a Victorian state election, it brought with it considerable risks. With majorities in both houses Bracks could no longer cite his weak parliamentary position as an excuse for inaction.

On 28 August 2002, Bracks, in conjunction with his then New South Wales counterpart, Bob Carr, opened the Mowamba aqueduct between Jindabyne and Dalgety, to divert 38 gigalitres of water a year from Lake Eucumbene to the Snowy and Murray rivers. The ten-year plan cost A$300¬†million with Victoria and NSW splitting the costs. Melbourne Water has stated that within 50 years there will be 20 per cent less water going into Victorian reservoirs.

In May 2003 Bracks broke an election promise and announced that the proposed Scoresby Freeway in Melbourne's eastern suburbs would be a tollway rather than a freeway, as promised at the 2002 elections. As well as risking a loss of support in marginal seats in eastern Melbourne, this decision brought about a strong response from the Howard Federal government, which cut off federal funding for the project on the grounds that the Bracks government had reneged on the terms of the federal-state funding agreement. The decision seems to have been on the recommendation of Brumby, who was concerned with the state's budgetary position. Also opposing the decision was the Federal Labor Opposition, which feared anti-Labor reaction at the 2004 Federal election. The then Opposition Leader Mark Latham described a meeting with Bracks and federal shadow ministers, writing:
This backflip, while seen by many as an opportunity for the Liberals to make ground, saw the then leader of the Liberals, Robert Doyle, adopt a much-criticised policy of half tolls, which was later overturned by his successor, Ted Baillieu.

In 2005, following extensive independent studies it was found that cattle had created extensive damage to the high country National Park and their continued presence in the Park was incompatible with the values of National Parks. Bracks backed the environment and his environment minister, John Thwaites and announced that Victoria would follow the NSW example and cattlemen would be banned from using the "High Plains" in Victoria's National Parks to graze cattle. Some said this ended a 170-year tradition, the reality was the ban was only in the National Parks. Stockmen had been fearing this decision since 1984, when a Labor government excised land to create the Alpine National Park. Some estimated three hundred cattlemen rode horses down Bourke street in protest while police said it was closer to 100. Colourful Victorian National Party leader Peter Ryan was quoted as saying that Bracks had "killed the man from Snowy River", a reference to the Banjo Paterson poem "The Man from Snowy River"... which was a bit strange because the Poem was about mustering horses not cattle ‚Äì a practice which was stopped in the high country just after World War 2.

Bracks' second government achieved one of Victorian Labor's longest-held goals with a complete reform of the state's system for electing its upper house. It saw the introduction of proportional representation, with eight five-member regions replacing the current single-member constituencies. This system increases the opportunity for minor parties such as the Greens and DLP to win seats in the Legislative Council, giving them a greater chance of holding the balance of power. Illustrating the historic importance Labor assigns to the changes, in a speech to a conference celebrating the 150th anniversary of the Eureka Stockade, Bracks said it was "another victory for the aspirations of Eureka", and has described the changes as "his proudest achievement".

The staging of the 2006 Commonwealth Games, generally viewed as a success (albeit an expensive one), was viewed as a plus for Bracks and the government. With times reasonably good, a perception arguably reinforced by an extensive government advertising campaign selling the virtues of Victoria to Victorians, polls indicated little interest in change, although towards the end of the election campaign polling indicated that the Liberals under Baillieu were closing the gap.

The election campaign was a relatively low-key affair, with the Government and Bracks largely running on their record, as well as their plans to tackle infrastructure issues in their third term. Bracks' image loomed large in Labor's election advertising. Liberal attacks concentrated on the slow process of infrastructure development under Bracks (notably on water supply issues relating to the severe drought affecting Victoria in the election leadup), and new Liberal leader Ted Baillieu promised to start construction on a range of new infrastructure initiatives, including a new dam on the Maribyrnong River and a desalination plant. Labor's broken election promise on Eastlink was also expected to be a factor in some seats in the eastern suburbs of Melbourne.

On 25 November 2006, Steve Bracks won his third election, comfortably defeating Baillieu to secure a third term, with a slightly reduced majority in the Lower House. This marked only the second time that the Victorian Labor Party had won a third term in office. His third term Cabinet was sworn in on 1 December 2006 with Bracks also holding the portfolio of Veterans' Affairs and Multicultural Affairs.

Bracks announced his resignation as Premier on 27 July 2007, saying this was to spend more time with his family. He stepped down on 30 July 2007. According to the ABC, Bracks had been under political and personal pressure in the weeks before his resignation. Alone among State Premiers, he had refused to agree to the Federal Government's $10¬†billion Murray-Darling Basin water conservation plan, and his son had been involved in an accident involving a charge of drink driving. Bracks told a media conference he could no longer give a 100 per cent commitment to politics:

Bracks' deputy John Thwaites announced his resignation on the same day. News of the resignations caused surprise to the general community as well as to politicians. It was revealed that then Federal Labor Leader Kevin Rudd was informed only minutes before the announcement, and tried to talk Bracks out of his decision. Bracks' Treasurer John Brumby was elected unopposed by the Victorian Labor Caucus as Premier, while Attorney-General Rob Hulls was elected Deputy Premier.

One consequence of Bracks leaving politics may have been the introduction of abortion law reform in Victoria. It has been suggested that the resignation of Premier Bracks sowed the seeds for abortion law reform by legislation that parliamentarians previously had refused to support, fearing a backlash from anti-abortion groups led by veteran campaigner Margaret Tighe. Bracks, as a Catholic of Lebanese descent, almost certainly would not have allowed abortion legislation into the parliament, but his successor John Brumby did not share this view, and the Abortion Law Reform Bill introduced by upper house member Candy Broad was passed by the Parliament in 2008.

In August 2007, following his resignation as Premier, Bracks announced he would provide a short-term pro bono advising role in East Timor working alongside the newly elected Prime Minister Xanana Gusm√£o. Bracks was to spend a year travelling between Melbourne and Dili helping with the establishment of Gusm√£o's administration, the key departments that would need to be involved, and advising on how they would be accountable and reportable to the legislature.

During 2008 Bracks indicated his support for Victorian abortion law reform in Victoria.

In addition to his role advising Gusm√£o, Bracks also joined several company advisory boards: KPMG, insurance firm Jardine Lloyd Thompson Group, the AIMS Financial Group and the NAB. The KPMG appointment was controversial, as the Victorian government had awarded the firm over 100 contracts during Bracks' time as Premier. On 14 February 2008, the Federal Labor Government appointed Bracks to head an inquiry into the ongoing viability of the Australian car industry.

In 2010, Bracks was appointed a Companion of the Order of Australia for services to the community and the Parliament of Victoria. In recognition of his distinguished services to the Victorian community, he was awarded the degree of Doctor of Laws (honoris causa) ‚Äì LL.D "(h.c.)" by Deakin University on 27 April 2010. He was also appointed to the Honorary Chair of the Deakin University Foundation.

In February 2013 after the announcement that Nicola Roxon would retire from federal politics, Bracks was cited as a possible candidate for her safe Labor seat of Gellibrand, but he ruled out running for the seat.

Bracks was appointed to the role of Australian Consul-General in New York in May 2013, by the Federal ALP Government of Julia Gillard. At the time, the shadow Foreign Minister, the Coalition's Julie Bishop, described the appointment as "inappropriate" because of the proximity to the upcoming election and "arrogant" because of a lack of consultation with the then-opposition. Following the defeat of the ALP at the 7 September election, incoming foreign minister Julie Bishop reversed the appointment in a decision described as 'petty and vindictive' by acting ALP foreign affairs spokeswoman Tanya Plibersek.

In March 2019, it was announced that Bracks will serve as the 6th Chancellor of Victoria University from 2021.

In June 2020, Bracks and former federal Labor deputy leader Jenny Macklin were appointed as administrators of the Victorian branch of the Australian Labor Party by the party's National Executive until early 2021, after allegations of branch-stacking by Victorian minister Adem Somyurek were revealed. The pair will review the state party‚Äôs operations and provide detailed recommendations to tackle the issue of branch-stacking within the party.

 


</doc>
<doc id="28915" url="https://en.wikipedia.org/wiki?curid=28915" title="Small Isles">
Small Isles

The Small Isles ("") are a small archipelago of islands in the Inner Hebrides, off the west coast of Scotland. They lie south of Skye and north of Mull and Ardnamurchan¬†‚Äì the most westerly point of mainland Scotland. 

The islands form part of the Lochaber area of the Highland council area. Until 1891 Canna, R√πm and Muck were historically part of the shire of Argyll; Eigg was historically part of Inverness-shire. All of the Small Isles were in Inverness-shire between 1891 and 1975, and remain part of the registration county of Inverness for land registration and statistical purposes. A single community council covers the islands.

"Small Isles" is the name of the conterminous civil parish and former Church of Scotland parish, originally created in 1726 from part of Sleat parish, the balance of which lies on the much larger island of Skye. The original name of the new parish was Eigg or Short Isles. "In process of time the name was by an easy transition changed from 'Short' to 'Small' Isles." The islands are not especially small, with R√πm being the 15th largest in Scotland. The Gaelic name of "" translates as "cross isles" referring to the islands position between Morar and the Uists.

The four main islands are Canna, R√πm, Eigg and Muck. The largest is R√πm with an area of .

Smaller islands surrounding the main four include:

There are also a number of skerries:

According to the 2011 census, the total population of the Small Isles was 153. Five of the islands are inhabited: Eigg (83), Muck (27), R√πm (22), Canna (12) and Sanday (9).

The inhabited islands are in contrasting forms of ownership: Canna (along with the tidally linked Sanday) is owned by a national conservation charity, the National Trust for Scotland; Eigg has been owned by a local community trust since 1997; Muck remains in private ownership; and R√πm is largely in the hands of the state (via Scottish Natural Heritage), although some land in and around the only village (Kinloch) is owned by a community trust.

A Caledonian MacBrayne ferry, , links the Small Isles to each other and to the mainland port of Mallaig. The ferry runs a daily service, calling at different islands depending on the day of the week; there are two calls at certain islands on each day to allow for day visits to and from each island. The Lochnevis has a landing craft-style stern ramp allowing vehicles to be driven onto and off the vessel at a new slipway constructed in 2001, however visitors are not normally permitted to bring vehicles to the Small Isles. During the summer months the islands are also served by Arisaig Marine's passenger ferry "MV Sheerwater" from Arisaig, south of Mallaig. Timetables are also arranged to allow time onshore on different islands depending on the day of the week.

The Small Isles are all important for their wildlife, with R√πm being designated as both a national nature reserve and a Special Area of Conservation (SAC). R√πm is home to one of the world‚Äôs largest colony of Manx shearwater, and was the location for the first stage of the reintroduction of white-tailed sea-eagles into Scotland, with 82 birds being released between 1975 and 1985. R√πm, and Canna and Sanday (jointly), are designated as Special Protection Areas (SPA) due their birdlife, with all three islands hosting important breeding populations of guillemots and kittiwakes. The Canna and Sanday SPA is also designated due to its importance to breeding Atlantic puffins and shags, whilst the R√πm SPA designation notes the presence of golden eagles, Manx shearwaters, and red-throated divers.

Around of the waters around R√πm, Canna and the low-lying rocky islet of Oigh-sgeir have been designated as the Small Isles Nature Conservation Marine Protected Area (NCMPA). Of particular note is that this area holds the UK's only known colony of fan mussels. The seas surrounding all of the Small Isles have also been designated as a SAC due to their importance for harbour porpoises.

The islands and surrounding sea area together form the Small Isles national scenic area, one of the forty such areas in Scotland, which are defined so as to identify areas of exceptional scenery and to ensure its protection from inappropriate development. The designated area covers in total, of which is on land and the remaining is marine (i.e. below low tide level).



</doc>
<doc id="28916" url="https://en.wikipedia.org/wiki?curid=28916" title="Shetland">
Shetland

Shetland (; ), also called the Shetland Islands and formerly Zetland, is a subarctic archipelago in the Northern Isles of Scotland, situated in the Northern Atlantic, between Great Britain, the Faroe Islands and Norway.

The islands lie some to the northeast of Orkney, from the Scottish mainland and west of Norway. They form part of the division between the Atlantic Ocean to the west and the North Sea to the east. The total area is , and the population totalled 22,920 in 2019. The islands comprise the Shetland constituency of the Scottish Parliament. The local authority, Shetland Islands Council, is one of the 32 council areas of Scotland. The islands' administrative centre and only burgh is Lerwick, which has been the capital of Shetland since taking over from Scalloway in 1708.

The largest island, known as "Mainland", has an area of , making it the third-largest Scottish island and the fifth-largest of the British Isles. There are an additional 15 inhabited islands. The archipelago has an oceanic climate, a complex geology, a rugged coastline and many low, rolling hills.

Humans have lived in Shetland since the Mesolithic period. The early historic period was dominated by Scandinavian influences, especially from Norway. The islands became part of Scotland in the 15th century. When Scotland became part of the Kingdom of Great Britain in 1707, trade with northern Europe decreased. Fishing continues to be an important aspect of the economy up to the present day. The discovery of North Sea oil in the 1970s significantly boosted Shetland's economy, employment and public sector revenues.

The local way of life reflects the Scottish and Norse heritage of the isles, including the Up Helly Aa fire festival and a strong musical tradition, especially the traditional fiddle style. The islands have produced a variety of writers of prose and poetry, often in the distinct Shetland dialect of the Scots language. There are numerous areas set aside to protect the local fauna and flora, including a number of important sea bird nesting sites. The Shetland pony and Shetland Sheepdog are two well-known Shetland animal breeds. Other local breeds include the Shetland sheep, cow, goose, and duck. The Shetland pig, or grice, has been extinct since about 1930.

The islands' motto, which appears on the Council's coat of arms, is "". The Old Norse origin of this phrase is likely from the Norwegian provincial laws, such as the Frostathing Law. It is also mentioned in "Nj√°ls saga", and means "By law shall land be built".

The name of Shetland is derived from the Old Norse words, ('hilt'), and ('land').

In AD¬†43 and 77 the Roman authors Pomponius Mela and Pliny the Elder referred to seven islands they respectively called and , both of which are assumed to be Shetland. Another possible early written reference to the islands is Tacitus' report in "Agricola" in AD¬†98. After describing the discovery and conquest of Orkney, he wrote that the Roman fleet had seen "Thule, too". In early Irish literature, Shetland is referred to as ‚Äî"the Isles of Cats", which may have been the pre-Norse inhabitants' name for the islands. The Cat clan also occupied parts of the northern Scottish mainland (see Kingdom of Cat); and their name can be found in Caithness and in the Scottish Gaelic name for Sutherland (, meaning "among the Cats").

The oldest version of the modern name Shetland is , the Latinised adjectival form of the Old Norse name, recorded in a letter from Harald, Count of Shetland, in 1190, becoming "Hetland" in 1431 after various intermediate transformations. It is possible that the Pictish "cat" sound forms part of this Norse name. It then became in the 16th century.

As Norn was gradually replaced by Scots in the form of the Shetland dialect, became . The initial letter is the Middle Scots letter, "yogh", the pronunciation of which is almost identical to the original Norn sound, . When the use of the letter yogh was discontinued, it was often replaced by the similar-looking letter z (which at the time was usually rendered with a curled tail: ‚ü® í‚ü©) hence , the form used in the name of the pre-1975 county council. This is also the source of the ZE postcode used for Shetland.

Most of the individual islands have Norse names, although the derivations of some are obscure and may represent pre-Norse, possibly Pictish or even pre-Celtic names or elements.

Shetland is around north of mainland Scotland, covers an area of and has a coastline long.

Lerwick, the capital and largest settlement, has a population of 6,958 and about half of the archipelago's total population of 22,920 people live within of the town.

Scalloway on the west coast, which was the capital until 1708, has a population of less than 1,000.

Only 16 of about 100 islands are inhabited. The main island of the group is known as Mainland. The next largest are Yell, Unst, and Fetlar, which lie to the north, and Bressay and Whalsay, which lie to the east. East and West Burra, Muckle Roe, Papa Stour, Trondra and Vaila are smaller islands to the west of Mainland. The other inhabited islands are Foula west of Walls, Fair Isle south-west of Sumburgh Head, and the Out Skerries to the east.

The uninhabited islands include Mousa, known for the Broch of Mousa, the finest preserved example in Scotland of an Iron Age broch; Noss to the east of Bressay, which has been a national nature reserve since 1955; St Ninian's Isle, connected to Mainland by the largest active tombolo in the UK; and Out Stack, the northernmost point of the British Isles. Shetland's location means that it provides a number of such records: Muness is the most northerly castle in the United Kingdom and Skaw the most northerly settlement.

The geology of Shetland is complex, with numerous faults and fold axes. These islands are the northern outpost of the Caledonian orogeny, and there are outcrops of Lewisian, Dalradian and Moine metamorphic rocks with histories similar to their equivalents on the Scottish mainland. There are also Old Red Sandstone deposits and granite intrusions. The most distinctive feature is the ophiolite in Unst and Fetlar which is a remnant of the Iapetus Ocean floor made up of ultrabasic peridotite and gabbro.

Much of Shetland's economy depends on the oil-bearing sediments in the surrounding seas. Geological evidence shows that in around 6100 BC a tsunami caused by the Storegga Slides hit Shetland, as well as the rest of the east coast of Scotland, and may have created a wave of up to high in the voes where modern populations are highest.

The highest point of Shetland is Ronas Hill at . The Pleistocene glaciations entirely covered the islands. During that period, the Stanes of Stofast, a 2000-tonne glacial erratic, came to rest on a prominent hilltop in Lunnasting.

Shetland has a national scenic area which, unusually, includes a number of discrete locations: Fair Isle, Foula, South West Mainland (including the Scalloway Islands), Muckle Roe, Esha Ness, Fethaland and Herma Ness. The total area covered by the designation is 41,833¬†ha, of which 26,347¬†ha is marine (i.e. below low tide).

In October 2018, legislation came into force in Scotland to prevent public bodies, without good reason, showing Shetland in a separate box in maps, as had often been the practice. The legislation requires the islands to be "displayed in a manner that accurately and proportionately represents their geographical location in relation to the rest of Scotland", so as make clear the islands' real distance from other areas.

Shetland has an oceanic temperate maritime climate (K√∂ppen: "Cfb"), bordering on, but very slightly above average in summer temperatures, the subpolar variety, with long but cool winters and short mild summers. The climate all year round is moderate owing to the influence of the surrounding seas, with average night-time low temperatures a little above in January and February and average daytime high temperatures of near in July and August. The highest temperature on record was on the 6th of August 1910 and the lowest in the Januaries of 1952 and 1959. The frost-free period may be as little as three months. In contrast, inland areas of nearby Scandinavia on similar latitudes experience significantly larger temperature differences between summer and winter, with the average highs of regular July days comparable to Lerwick's all-time record heat that is around , further demonstrating the moderating effect of the Atlantic Ocean. In contrast, winters are considerably milder than those expected in nearby continental areas, even comparable to winter temperatures of many parts of England and Wales much further south.

The general character of the climate is windy and cloudy with at least of rain falling on more than 250 days a year. Average yearly precipitation is , with November and December the wettest months. Snowfall is usually confined to the period November to February, and snow seldom lies on the ground for more than a day. Less rain falls from April to August although no month receives less than . Fog is common during summer due to the cooling effect of the sea on mild southerly airflows.

Because of the islands' latitude, on clear winter nights the "northern lights" can sometimes be seen in the sky, while in summer there is almost perpetual daylight, a state of affairs known locally as the "simmer dim". Annual bright sunshine averages 1110 hours, and overcast days are common.

Due to the practice, dating to at least the early Neolithic, of building in stone on virtually treeless islands, Shetland is extremely rich in physical remains of the prehistoric eras and there are over 5,000 archaeological sites all told. A midden site at West Voe on the south coast of Mainland, dated to 4320‚Äì4030 BC, has provided the first evidence of Mesolithic human activity in Shetland. The same site provides dates for early Neolithic activity and finds at Scord of Brouster in Walls have been dated to 3400 BC. "Shetland knives" are stone tools that date from this period made from felsite from Northmavine.

Pottery shards found at the important site of Jarlshof also indicate that there was Neolithic activity there although the main settlement dates from the Bronze Age. This includes a smithy, a cluster of wheelhouses and a later broch. The site has provided evidence of habitation during various phases right up until Viking times. Heel-shaped cairns, are a style of chambered cairn unique to Shetland, with a particularly large example in Vementry.

Numerous brochs were erected during the Iron Age. In addition to Mousa there are significant ruins at Clickimin, Culswick, Old Scatness and West Burrafirth, although their origin and purpose is a matter of some controversy. The later Iron Age inhabitants of the Northern Isles were probably Pictish, although the historical record is sparse. Hunter (2000) states in relation to King Bridei I of the Picts in the sixth century AD: "As for Shetland, Orkney, Skye and the Western Isles, their inhabitants, most of whom appear to have been Pictish in culture and speech at this time, are likely to have regarded Bridei as a fairly distant presence.‚Äù In 2011, the collective site, "The Crucible of Iron Age Shetland", including Broch of Mousa, Old Scatness and Jarlshof, joined the UKs "Tentative List" of World Heritage Sites.

The expanding population of Scandinavia led to a shortage of available resources and arable land there and led to a period of Viking expansion, the Norse gradually shifting their attention from plundering to invasion. Shetland was colonised during the late 8th and 9th centuries, the fate of the existing indigenous population being uncertain. Modern Shetlanders have almost identical proportions of Scandinavian matrilineal and patrilineal genetic ancestry, suggesting that the islands were settled by both men and women in equal measure.

Vikings then used the islands as a base for pirate expeditions to Norway and the coasts of mainland Scotland. In response, Norwegian king Harald H√•rfagre ("Harald Fair Hair") annexed the Northern Isles (comprising Orkney and Shetland) in 875. Rognvald Eysteinsson received Orkney and Shetland from Harald as an earldom as reparation for the death of his son in battle in Scotland, and then passed the earldom on to his brother Sigurd the Mighty.

The islands converted to Christianity in the late 10th century. King Olav Tryggvasson summoned the "jarl" Sigurd the Stout during a visit to Orkney and said, "I order you and all your subjects to be baptised. If you refuse, I'll have you killed on the spot and I swear I will ravage every island with fire and steel." Unsurprisingly, Sigurd agreed and the islands became Christian at a stroke. Unusually, from c. 1100 onwards the Norse "jarls" owed allegiance both to Norway and to the Scottish crown through their holdings as Earls of Caithness.

In 1194, when Harald Maddadsson was Earl of Orkney and Shetland, a rebellion broke out against King Sverre Sigurdsson of Norway. The ("Island Beardies") sailed for Norway but were beaten in the Battle of Florv√•g near Bergen. After his victory King Sverre placed Shetland under direct Norwegian rule, a state of affairs that continued for nearly two centuries.

From the mid-13th century onwards Scottish monarchs increasingly sought to take control of the islands surrounding the mainland. The process was begun in earnest by Alexander II and was continued by his successor Alexander III. This strategy eventually led to an invasion of Scotland by Haakon Haakonsson, King of Norway. His fleet assembled in Bressay Sound before sailing for Scotland. After the stalemate of the Battle of Largs, Haakon retreated to Orkney, where he died in December 1263, entertained on his deathbed by recitations of the sagas. His death halted any further Norwegian expansion in Scotland and following this ill-fated expedition, the Hebrides and Mann were yielded to the Kingdom of Scotland as a result of the 1266 Treaty of Perth, although the Scots recognised continuing Norwegian sovereignty over Orkney and Shetland.

In the 14th century, Orkney and Shetland remained a Norwegian possession, but Scottish influence was growing. Jon Haraldsson, who was murdered in Thurso in 1231, was the last of an unbroken line of Norse jarls, and thereafter the earls were Scots noblemen of the houses of Angus and St Clair. On the death of Haakon VI in 1380, Norway formed a political union with Denmark, after which the interest of the royal house in the islands declined. In 1469, Shetland was pledged by Christian I, in his capacity as King of Norway, as security against the payment of the dowry of his daughter Margaret, betrothed to James III of Scotland. As the money was never paid, the connection with the Crown of Scotland became permanent. In 1470, William Sinclair, 1st Earl of Caithness ceded his title to James III, and the following year the Northern Isles were directly absorbed to the Crown of Scotland, an action confirmed by the Parliament of Scotland in 1472. Nonetheless, Shetland's connection with Norway has proved to be enduring.

From the early 15th century onward Shetlanders sold their goods through the Hanseatic League of German merchantmen. The Hansa would buy shiploads of salted fish, wool and butter, and import salt, cloth, beer and other goods. The late 16th century and early 17th century were dominated by the influence of the despotic Robert Stewart, Earl of Orkney, who was granted the islands by his half-sister Mary Queen of Scots, and his son Patrick. The latter commenced the building of Scalloway Castle, but after his imprisonment in 1609 the Crown annexed Orkney and Shetland again until 1643 when Charles I granted them to William Douglas, 7th Earl of Morton. These rights were held on and off by the Mortons until 1766, when they were sold by James Douglas, 14th Earl of Morton to Laurence Dundas.

The trade with the North German towns lasted until the 1707 Act of Union, when high salt duties prevented the German merchants from trading with Shetland. Shetland then went into an economic depression, as the local traders were not as skilled in trading salted fish. However, some local merchant-lairds took up where the German merchants had left off, and fitted out their own ships to export fish from Shetland to the Continent. For the independent farmers of Shetland this had negative consequences, as they now had to fish for these merchant-lairds.

Smallpox afflicted the islands in the 17th and 18th centuries (as it did all of Europe), but as vaccines became available after 1800, health improved. The islands were very badly hit by the potato famine of 1846 and the government introduced a Relief Plan for the islands under the command of Captain Robert Craigie of the Royal Navy who stayed in Lerwick to oversee the project 1847-1852. During this period Craigie also did much to improve and increase roads in the islands.

Population increased to a maximum of 31,670 in 1861. However, British rule came at price for many ordinary people as well as traders. The Shetlanders' nautical skills were sought by the Royal Navy. Some 3,000 served during the Napoleonic wars from 1800 to 1815 and press gangs were rife. During this period 120 men were taken from Fetlar alone, and only 20 of them returned home. By the late 19th century 90% of all Shetland was owned by just 32 people, and between 1861 and 1881 more than 8,000 Shetlanders emigrated. With the passing of the Crofters' Act in 1886 the Liberal prime minister William Gladstone emancipated crofters from the rule of the landlords. The Act enabled those who had effectively been landowners' serfs to become owner-occupiers of their own small farms. By this time fishermen from Holland, who had traditionally gathered each year off the coast of Shetland to fish for herring, triggered an industry in the islands that boomed from around 1880 until the 1920s when stocks of the fish began to dwindle. The production peaked in 1905 at more than a million barrels, of which 708,000 were exported.

During World War I many Shetlanders served in the Gordon Highlanders, a further 3,000 served in the Merchant Navy, and more than 1,500 in a special local naval reserve. The 10th Cruiser Squadron was stationed at Swarbacks Minn (the stretch of water to the south of Muckle Roe), and during a single year from March 1917 more than 4,500 ships sailed from Lerwick as part of an escorted convoy system. In total, Shetland lost more than 500 men, a higher proportion than any other part of Britain, and there were further waves of emigration in the 1920s and 1930s.

During World War II a Norwegian naval unit nicknamed the "Shetland Bus" was established by the Special Operations Executive in the autumn of 1940 with a base first at Lunna and later in Scalloway to conduct operations around the coast of Norway. About 30 fishing vessels used by Norwegian refugees were gathered and the Shetland Bus conducted covert operations, carrying intelligence agents, refugees, instructors for the resistance, and military supplies. It made over 200 trips across the sea, and Leif Larsen, the most highly decorated allied naval officer of the war, made 52 of them. Several RAF airfields and sites were also established at Sullom Voe and several lighthouses suffered enemy air attacks.

Oil reserves discovered in the later 20th century in the seas both east and west of Shetland have provided a much-needed alternative source of income for the islands. The East Shetland Basin is one of Europe's largest oil fields and as a result of the oil revenue and the cultural links with Norway, a small Home Rule movement developed briefly to recast the constitutional position of Shetland. It saw as its models the Isle of Man, as well as Shetland's closest neighbour, the Faroe Islands, an autonomous dependency of Denmark.

The population stood at 17,814 in 1961.

Today, the main revenue producers in Shetland are agriculture, aquaculture, fishing, renewable energy, the petroleum industry (crude oil and natural gas production), the creative industries and tourism.

Fishing remains central to the islands' economy today, with the total catch being in 2009, valued at over ¬£73.2 million. Mackerel makes up more than half of the catch in Shetland by weight and value, and there are significant landings of haddock, cod, herring, whiting, monkfish and shellfish.

Oil and gas were first landed in 1978 at Sullom Voe, which has subsequently become one of the largest terminals in Europe. Taxes from the oil have increased public sector spending on social welfare, art, sport, environmental measures and financial development. Three quarters of the islands' workforce is employed in the service sector, and the Shetland Islands Council alone accounted for 27.9% of output in 2003. Shetland's access to oil revenues has funded the Shetland Charitable Trust, which in turn funds a wide variety of local programmes. The balance of the fund in 2011 was ¬£217 million, i.e., about ¬£9,500 per head.

In January 2007, the Shetland Islands Council signed a partnership agreement with Scottish and Southern Energy for the Viking Wind Farm, a 200-turbine wind farm and subsea cable. This renewable energy project would produce about 600 megawatts and contribute about ¬£20 million to the Shetland economy per year. The plan met with significant opposition within the islands, primarily resulting from the anticipated visual impact of the development. The PURE project in Unst is a research centre which uses a combination of wind power and fuel cells to create a wind hydrogen system. The project is run by the Unst Partnership, the local community's development trust.

Farming is mostly concerned with the raising of Shetland sheep, known for their unusually fine wool.

Knitwear is important both to the economy and culture of Shetland, and the Fair Isle design is well known. However, the industry faces challenges due to plagiarism of the word "Shetland" by manufacturers operating elsewhere, and a certification trademark, "The Shetland Lady", has been registered.

Crofting, the farming of small plots of land on a legally restricted tenancy basis, is still practised and is viewed as a key Shetland tradition as well as an important source of income. Crops raised include oats and barley; however, the cold, windswept islands make for a harsh environment for most plants.

Shetland is served by a weekly local newspaper, "The Shetland Times" and the online "Shetland News" with radio service being provided by BBC Radio Shetland and the commercial radio station SIBC.

Shetland is a popular destination for cruise ships, and in 2010 the Lonely Planet guide named Shetland as the sixth best region in the world for tourists seeking unspoilt destinations. The islands were described as "beautiful and rewarding" and the Shetlanders as "a fiercely independent and self-reliant bunch". Overall visitor expenditure was worth ¬£16.4 million in 2006, in which year just under 26,000 cruise liner passengers arrived at Lerwick Harbour. This business has grown substantially with 109 cruise ships already booked in for 2019, representing over 107,000 passenger visits. In 2009, the most popular visitor attractions were the Shetland Museum, the RSPB reserve at Sumburgh Head, Bonhoga Gallery at Weisdale Mill and Jarlshof. Geopark Shetland (now Shetland UNESCO Global Geopark) was established by the Amenity Trust in 2009 to boost sustainable tourism to the islands.


Transport between islands is primarily by ferry, and Shetland Islands Council operates various inter-island services. Shetland is also served by a domestic connection from Lerwick to Aberdeen on mainland Scotland. This service, which takes about 12 hours, is operated by NorthLink Ferries. Some services also call at Kirkwall, Orkney, which increases the journey time between Aberdeen and Lerwick by 2 hours. There are plans for road tunnels to some of the islands, especially Bressay and Whalsay; however, it is hard to convince the mainland government to finance them.

Sumburgh Airport, the main airport in Shetland, is located close to Sumburgh Head, south of Lerwick. Loganair operates flights to other parts of Scotland up to ten times a day, the destinations being Kirkwall, Aberdeen, Inverness, Glasgow and Edinburgh. Lerwick/Tingwall Airport is located west of Lerwick. Operated by Directflight Limited in partnership with Shetland Islands Council, it is devoted to inter-island flights from the Shetland Mainland to most of the inhabited islands.

Scatsta Airport near Sullom Voe allows frequent charter flights from Aberdeen to transport oilfield workers and this small terminal has the fifth largest number of international passengers in Scotland.

Public bus services are operated in Mainland, Whalsay, Burra, Unst and Yell.

The archipelago is exposed to wind and tide, and there are numerous sites of wrecked ships. Lighthouses are sited as an aid to navigation at various locations.

The Shetland Islands Council is the Local Government authority for all the islands and is based in Lerwick Town Hall.

Shetland is sub-divided into 18 community council areas and into 12 civil parishes that are used for statistical purposes.

In Shetland there are two high schools‚ÄîAnderson and Brae‚Äîfive junior high schools, and 24 primary schools.

In 2014 there were plans to close other junior high schools and require boarding at Anderson.

Shetland is also home to the North Atlantic Fisheries College, the Centre for Nordic Studies and Shetland College, which are all associated with the University of the Highlands and Islands.
The Shetland Football Association oversees two divisions ‚Äî a Premier League and a Reserve League ‚Äî which are affiliated with the Scottish Amateur Football Association. Seasons take place during summer.

The islands are represented by the Shetland football team, which regularly competes in the Island Games.

The Reformation reached the archipelago in 1560. This was an apparently peaceful transition and there is little evidence of religious intolerance in Shetland's recorded history.

In the 2011 census, Shetland registered a higher proportion of people with no religion than the Scottish average. Nevertheless, a variety of religious denominations are represented in the islands.

The Methodist Church has a relatively high membership in Shetland, which is a District of the Methodist Church (with the rest of Scotland comprising a separate District).

The Church of Scotland has a Presbytery of Shetland that includes St. Columba's Church in Lerwick.

The Catholic population is served by the church of St. Margaret and the Sacred Heart in Lerwick. The Parish is part of the Diocese of Aberdeen.

The Scottish Episcopal Church (part of the Anglican Communion) has regular worship at St Magnus' Church, Lerwick, St Colman's Church, Burravoe, and the Chapel of Christ the Encompasser, Fetlar, the last of which is maintained by the Society of Our Lady of the Isles, the most northerly and remote Anglican religious order of nuns.

The Church of Jesus Christ of Latter-day Saints has a congregation in Lerwick. The former print works and offices of the local newspaper, The Shetland Times, has been converted into a chapel.

Shetland is represented in the House of Commons as part of the Orkney and Shetland constituency, which elects one Member of Parliament. Since 2001, the MP has been Alistair Carmichael. This seat has been held by the Liberal Democrats or their predecessors the Liberal Party since 1950, longer than any other seat in the UK.

In the Scottish Parliament the Shetland constituency elects one Member of the Scottish Parliament (MSP) by the first past the post system. Tavish Scott of the Scottish Liberal Democrats had held the seat since the creation of the Scottish Parliament in 1999. Beatrice Wishart MSP, also of the Scottish Liberal Democrats, was elected to replace Tavish Scott in August 2019. Shetland is within the Highlands and Islands electoral region.

The political composition of the Shetland Islands Council is 21 Independents and 1 Scottish National Party.

In the 2014 referendum on Scottish independence from the United Kingdom, Shetland voted to remain in the UK by the third largest margin of the 32 local authority areas, by 63.71% to 36.29% in favour of the status quo.

The Wir Shetland movement was set up in 2015 to campaign for greater autonomy. As of early 2018, however, the movement appears to be inactive.

Roy Gr√∂nneberg, who founded the local chapter of the Scottish National Party in 1966, designed the flag of Shetland in cooperation with Bill Adams to mark the 500th anniversary of the transfer of the islands from Norway to Scotland. The colours are identical to those of the flag of Scotland, but are shaped in the Nordic cross. After several unsuccessful attempts, including a plebiscite in 1985, the Lord Lyon King of Arms approved it as the official flag of Shetland in 2005.

After the islands were officially transferred from Norway to Scotland in 1472, several Scots families from the Scottish Lowlands emigrated to Shetland in the 16th and 17th centuries. Studies of the genetic makeup of the islands' population, however, indicate that Shetlanders are just under half Scandinavian in origin, and sizeable amounts of Scandinavian ancestry, both patrilineal and matrilineal, have been reported in Orkney (55%) and Shetland (68%). This combination is reflected in many aspects of local life. For example, almost every place name in use can be traced back to the Vikings. The Lerwick Up Helly Aa is one of several fire festivals held in Shetland annually in the middle of winter, starting on the last Tuesday of January. The festival is just over 100 years old in its present, highly organised form. Originally held to break up the long nights of winter and mark the end of Yule, the festival has become one celebrating the isles' heritage and includes a procession of men dressed as Vikings and the burning of a replica longship. 
Shetland also competes in the biennial International Island Games, which it hosted in 2005.

The cuisine of Shetland is based on locally produced lamb, beef and seafood, much of it organic. Inevitably, the real ale-producing Valhalla Brewery is the most northerly in Britain. The Shetland Black is a variety of blue potato with a dark skin and indigo-coloured flesh markings.

The Norn language was a form of Old Norse spoken in the Northern Isles, and continued to be spoken until the 19th century. It was gradually replaced in Shetland by an insular dialect of Scots, known as Shetlandic, which is in turn being replaced in some areas by Scottish English. Although Norn was spoken for hundreds of years, it is now extinct and few written sources remain, although influences remain in the Insular Scots dialects. Shetland dialect is used in local radio and dialect writing, and is kept alive by organisations such as Shetland Forwirds, Isle Folk, and the Shetland Folk Society.

Shetland's culture and landscapes have inspired a variety of musicians, writers and film-makers. The Forty Fiddlers was formed in the 1950s to promote the traditional fiddle style, which is a vibrant part of local culture today. Notable exponents of Shetland folk music include Aly Bain, Jenna Reid, Fiddlers' Bid, and the late Tom Anderson and Peerie Willie Johnson. Thomas Fraser was a country musician who never released a commercial recording during his life, but whose work has become popular more than 20 years after his death in 1978.

The annual Shetland Folk Festival began in 1981 and is hosted on the first weekend of May.

Walter Scott's 1822 novel "The Pirate" is set in "a remote part of Shetland", and was inspired by his 1814 visit to the islands. The name "Jarlshof" meaning "Earl's Mansion" is a coinage of his. Robert Cowie, a doctor born in Lerwick published the 1874 work 

Hugh MacDiarmid, the Scots poet and writer, lived in Whalsay from the mid-1930s through 1942, and wrote many poems there, including a number that directly address or reflect the Shetland environment, such as "On A Raised Beach", which was inspired by a visit to West Linga. The 1975 novel "North Star" by Hammond Innes is largely set in Shetland and Raman Mundair's 2007 book of poetry "A Choreographer's Cartography" offers a British Asian perspective on the landscape. The "Shetland Quartet" by Ann Cleeves, who previously lived in Fair Isle, is a series of crime novels set around the islands. In 2013 her novel "Red Bones" became the basis of BBC crime drama television series "Shetland".

Vagaland, who grew up in Walls, was arguably Shetland's finest poet of the 20th century. Haldane Burgess was a Shetland historian, poet, novelist, violinist, linguist and socialist, and Rhoda Bulter (1929‚Äì94) is one of the best-known Shetland poets of recent times. Other 20th- and 21st-century poets and novelists include Christine De Luca, Robert Alan Jamieson who grew up in Sandness, the late Lollie Graham of Veensgarth, Stella Sutherland of Bressay, the late William J Tait from Yell and Laureen Johnson.

There are two monthly magazines in production: "Shetland Life" and "i'i' Shetland". The quarterly "The New Shetlander", founded in 1947, is said to be Scotland's longest-running literary magazine. For much of the later 20th century it was the major vehicle for the work of local writers‚Äîand of others, including early work by George Mackay Brown.

Michael Powell made "The Edge of the World" in 1937, a dramatisation based on the true story of the evacuation of the last 36 inhabitants of the remote island of St Kilda on 29 August 1930. St Kilda lies in the Atlantic Ocean, west of the Outer Hebrides but Powell was unable to get permission to film there. Undaunted, he made the film over four months during the summer of 1936 in Foula and the film transposes these events to Shetland. Forty years later, the documentary "Return to the Edge of the World" was filmed, capturing a reunion of cast and crew of the film as they revisited the island in 1978.

A number of other films have been made on or about Shetland including "A Crofter's Life in Shetland" (1932) "A Shetland Lyric" (1934), "Devil's Gate" (2003) and "It's Nice Up North" (2006), a comedy documentary by Graham Fellows. The Screenplay film festival takes place annually in Mareel, a cinema, music and education venue.

The BBC One television series "Shetland", a crime drama, is set in the islands and is based on the book series by Ann Cleeves. The programme is filmed partly in Shetland and partly on the Scottish mainland.

Shetland has three national nature reserves, at the seabird colonies of Hermaness and Noss, and at Keen of Hamar to preserve the serpentine flora. There are a further 81 SSSIs, which cover 66% or more of the land surfaces of Fair Isle, Papa Stour, Fetlar, Noss and Foula. Mainland has 45 separate sites.

The landscape in Shetland is marked by the grazing of sheep and the harsh conditions have limited the total number of plant species to about 400. Native trees such as rowan and crab apple are only found in a few isolated places such as cliffs and loch islands. The flora is dominated by Arctic-alpine plants, wild flowers, moss and lichen. Spring squill, buck's-horn plantain, Scots lovage, roseroot and sea campion are abundant, especially in sheltered places. Shetland mouse-ear ("Cerastium nigrescens") is an endemic flowering plant found only in Shetland. It was first recorded in 1837 by botanist Thomas Edmondston. Although reported from two other sites in the nineteenth century, it currently grows only on two serpentine hills in the island of Unst. The nationally scarce oysterplant is found in several islands and the British Red Listed bryophyte "Thamnobryum alopecurum" has also been recorded. Listed marine algae include: "Polysiphonia fibrillosa" (Dillwyn) Sprengel, "Polysiphonia atlantica" Kapraun and J.Norris, "Polysiphonia brodiaei" (Dillwyn) Sprengel, "Polysiphonia elongata" (Hudson) Sprengel, "Polysiphonia elongella". Harvey The Shetland Monkeyflower is unique to Shetland and is a mutation of the Monkeyflower "(mimulus guttatus") introduced to Shetland in the 19th century.

Shetland has numerous seabird colonies. Birds found in the islands include Atlantic puffin, storm-petrel, red-throated diver, northern gannet and great skua (locally called the "bonxie"). Numerous rarities have also been recorded including black-browed albatross and snow goose, and a single pair of snowy owls bred in Fetlar from 1967 to 1975. The Shetland wren, Fair Isle wren and Shetland starling are subspecies endemic to Shetland. There are also populations of various moorland birds such as curlew, snipe and golden plover.

One of the early ornithologists that wrote about the wealth of birdlife in Shetland was Edmund Selous (1857-1934) in his book "The Bird Watcher in the Shetlands" (1905). He writes extensively about the gulls and terns, about the arctic skuas, the black guillemots and many other birds (and the seals) of the islands.

The geographical isolation and recent glacial history of Shetland have resulted in a depleted mammalian fauna and the brown rat and house mouse are two of only three species of rodent present in the islands. The Shetland field mouse is the third and the archipelago's fourth endemic subspecies, of which there are three varieties in Yell, Foula and Fair Isle. They are variants of "Apodemus sylvaticus" and archaeological evidence suggests that this species was present during the Middle Iron Age (around 200 BC to AD 400). It is possible that "Apodemus" was introduced from Orkney where a population has existed since at the least the Bronze Age.

There is a variety of indigenous breeds, of which the diminutive Shetland pony is probably the best known, as well as being an important part of the Shetland farming tradition. The first written record of the pony was in 1603 in the Court Books of Shetland and, for its size, it is the strongest of all the horse breeds. Others are the Shetland Sheepdog or "Sheltie", the endangered Shetland cattle and Shetland goose and the Shetland sheep which is believed to have originated prior to 1000 AD. The Grice was a breed of semi-domesticated pig that had a habit of attacking lambs. It became extinct sometime between the middle of the nineteenth century and the 1930s.







</doc>
<doc id="28917" url="https://en.wikipedia.org/wiki?curid=28917" title="Soay, Inner Hebrides">
Soay, Inner Hebrides

Soay (, ) is an island just off the coast of Skye, in the Inner Hebrides of Scotland.

Soay lies to the west of Loch Scavaig on the south-west coast of Skye, from which it is separated by Soay Sound. Unlike its neighbours Skye and R√πm, Soay is low-lying, reaching at Beinn Bhreac. The dumb-bell shaped island is virtually cut in half by inlets that form Soay Harbour (north) and the main bay, Camas nan Gall (to the south). The main settlement, Mol-chlach, is on the shore of Camas nan Gall. It is normally reached by boat from Elgol. The island is part of the Cuillin Hills National Scenic Area, one of 40 in Scotland.

The name derives from Old Norse "Sau√∞a-ey" meaning "Sheep Island". Camas nan Gall (G: "Bay of Foreigners") is probably named after the Norse invaders, after whom the Hebrides ("Na h-Innse Gall") are also named.

The population peaked at 158 in 1851, following eviction of crofters from Skye in the Highland Clearances.

In 1946, author Gavin Maxwell bought the island and established a factory to process shark oil from basking sharks. The enterprise was unsuccessful, lasting just three years. Maxwell wrote about it in his book "Harpoon at a Venture". After the failure of the business the island was sold on to Maxwell's business partner‚Äôs wife (Jeanne Geddes), Tex Geddes. The island had the first solar-powered telephone exchange in the world.

Previously mainly Scottish Gaelic-speaking, most of the population was evacuated, at their request, to Mull on 20 June 1953 due to the poor ferry schedule in the winter when it was frequently cancelled due to bad weather , since when the island has been sparsely populated. In 2001 the population was 7. By 2003 this had dwindled to 2 and the usually resident population in 2011 were three people.

Local stamps were issued for Soay between 1965 and 1967, all on the Europa theme, some being overprinted to commemorate Sir Winston Churchill. As the stamps were produced without the owner's permission, they are regarded as bogus.




</doc>
<doc id="28918" url="https://en.wikipedia.org/wiki?curid=28918" title="Storytelling game">
Storytelling game

A storytelling game is a game where multiple players collaborate on telling a spontaneous story. Usually, each player takes care of one or more characters in the developing story. Some games in the tradition of role-playing games require one participant to take the roles of the various supporting characters, as well as introducing non-character forces (for example, a flood), but other systems dispense with this figure and distribute this function among all players.

Since this person usually sets the ground and setting for the story, he or she is often referred to as the "storyteller" (often contracted to "ST") or "narrator". Any number of other alternate forms may be used, many of which are variations on the term "gamemaster"; these variants are especially common in storytelling games derived from or similar to role-playing games.

In contrast to improv theater, storytelling gamers describe the actions of their characters rather than acting them out, except during dialogue or, in some games, monologue. However, "live action" versions exist, which are very much akin to theater except in the crucial absence of a non-participating audience.

The most popular modern storytelling games originated as a subgenre of role-playing games, where the game rules and statistics are heavily de-emphasised in favor of creating a believable story and immersive experience for all involved. So while in a conventional game the announcement that one's character is going to leap over a seven-meters-wide canyon will be greeted with the request to roll a number of dice, a player in a storytelling game who wishes to have a character perform a similar feat will have to convince the others (especially the storyteller) why it is both probable and keeping within the established traits of their character to successfully do so. As such, these games are a subclass of diceless role-playing games.

Not all players find the storytelling style of role-playing satisfying. Many role-playing gamers are more comfortable in a system that gives them less freedom, but where they do not need to police themselves; others find it easier to enjoy a system where a more concrete framework of rules is already present. These three types of player are discussed by the GNS theory.

Some role-playing game systems which describe themselves as "storytelling games" nevertheless use randomisers rather than story in the arbitration of the rules, often in the form of a contest of Rock, Paper, Scissors or a card drawn from a deck of cards. Such "storytelling" games are instead simplified or streamlined forms of traditional role-playing games. Conversely, most modern role-playing games encourage gamemasters to ignore their gaming systems if it makes for a more enjoyable story, even though they may not describe themselves as "storytelling" games.

A growing number of websites utilize a bulletin board system, in which the gaming is akin to Collaborative Fiction but known as a "Literary Role-Playing Game". The players contribute to an ongoing story with defined parameters but no narrator or directing force. A 'moderator' may oversee the gamers to ensure that the rules, guidelines and parameters of the gaming "world" are being upheld, but otherwise the writers are free to interact as players in an improvisational play. Many of these "Literary RPGs" are fan-fiction based, such as (most prevalently) Tolkien's Middle-earth, Star Wars, Harry Potter, Twilight, any number of anime and manga sources, or they are simply based in thematic worlds such as the mythologies of Ancient Greece, fairy tales, the Renaissance or science fiction. Most often referred to as "Literary RPGs" and place a greater emphasis on writing skill and storytelling ability than on any sense of competition driven outcome.

White Wolf Game Studio's Storyteller System, which is used in World of Darkness role-playing games such as "" and live-action games under the Mind's Eye Theatre imprint, is the best-known and most popular role-playing game described as a "storytelling game".

An early design of a collaborative storytelling game not based in simulation was created by Chris Engle c. 1988 with his "Matrix Game". In this system, a referee decides the likeliness of the facts proposed by the players, and those facts happen or are rejected according with a dice roll. Players can propose counter-arguments that are resolved in a dice rolling contest. A conflict round can follow to resolve any inconsistencies or further detail new plot points. Matrix Games are now presented in a board game format.

In 1999, game designer Ian Millington developed an early work called "Ergo" which established the basis for collaborative role-playing. It was designed with the rules of the Fudge universal role-playing system in mind but added modifications necessary to get rid of the need for a gamemaster, distributing the responsibility for the game and story equally among all players and undoing the equivalence between player and character.

Modern rule systems (such as the coin system in Universalis) rely less on randomness and more in collaboration between players. This includes rules based on economic systems that force players to negotiate the details of the story, and solve conflicts based on the importance that they give to a given plot element and the resources they're willing to spend to make it into the story.

Collaborative fiction is a form of storytelling which uses collaborative writing as the primary medium, where a group of authors share creative control of a story. Collaborative fiction can occur for commercial gain, as part of education, or recreationally ‚Äì many collaboratively written works have been the subject of a large degree of academic research.



</doc>
<doc id="28922" url="https://en.wikipedia.org/wiki?curid=28922" title="Scorpion">
Scorpion

Scorpions are predatory arachnids of the order Scorpiones. They have eight legs and are easily recognized by the pair of grasping pedipalps and the narrow, segmented tail, often carried in a characteristic forward curve over the back, ending with a venomous stinger. Scorpions range in size from in "Microtityus minimus" to in "Heterometrus swammerdami".

The evolutionary history of scorpions goes back to the Silurian period 435 million years ago. They have adapted to a wide range of environmental conditions, and they can now be found on all continents except Antarctica. There are about 1,750 described species, with 13 extant (living) families recognised to date. Their taxonomy is being revised in the light of genomic studies.

The vast majority do not represent a serious threat to humans, and healthy adults usually do not need medical treatment after being stung. Only about 25 species have venom capable of killing a human. In some parts of the world with highly venomous species, human fatalities regularly occur, primarily in areas with limited access to medical treatment. 

Scorpions with their powerful sting appear in art, folklore, mythology and numerous brands. Scorpion motifs are woven into kilim carpets for protection. Scorpio is the name of a constellation and the corresponding astrological sign; a classical myth tells how the giant scorpion and its enemy, Orion, became constellations on opposite sides of the sky. The name has been used for a Roman siege engine, several warships, a type of tank, and a yoga pose with the legs pointing forwards over the head, like the animal's tail.

The word "scorpion" is thought to have originated in Middle English between 1175 and 1225 AD from Old French ', or from Italian ', both derived from the Latin ', which is the romanization of the Greek ¬†‚Äì '. The Proto-Indo-European root word "*(s)ker-" has been translated as "to cut".

Scorpions are found on all major land masses except Antarctica. The diversity of scorpions is greatest in subtropical areas; it decreases towards both the poles and the equator. Scorpions did not occur naturally in Great Britain, New Zealand and some of the islands in Oceania, but have now been accidentally introduced into these places by humans. Five colonies of "Euscorpius flavicaudis" have established themselves in Sheerness on the Isle of Sheppey in the United Kingdom. At just over 51¬∞N, this marks the northernmost limit where scorpions live in the wild. 

Scorpions primarily live in deserts, but can be found in virtually every terrestrial habitat including high-elevation mountains, caves, and intertidal zones. However, they are largely absent from boreal ecosystems such as the tundra, high-altitude taiga, and mountain tops. As regards microhabitats, scorpions may be ground-dwelling, tree-living, rock-loving or sand-loving. Some species, such as "Vaejovis janssi", are versatile and are found in every type of habitat in Baja California, while others such as "Euscorpius carpathicus", endemic to the littoral zone of rivers in Romania, occupy specialized niches.

Scorpion fossils have been found in many strata, including marine Silurian and estuarine Devonian deposits, coal deposits from the Carboniferous Period and in amber. Whether the early scorpions were marine or terrestrial has been debated, though they had book lungs like modern terrestrial species. Over 100 fossil species of scorpion have been described. The oldest found to date is "Parioscorpio venator", which lived 437 million years ago, during the Silurian. Unlike present day scorpions, but like its marine ancestors, it had compound eyes. "Gondwanascorpio" from the Devonian is the earliest known terrestrial animal on the Gondwana supercontinent.

The Scorpiones are a clade of pulmonate Arachnida within the Chelicerata, a subphylum of Arthropoda that contains sea spiders and horseshoe crabs, and terrestrial animals without book-lungs such as ticks and harvestmen. Scorpiones is sister to the Tetrapulmonata, a terrestrial group with book-lungs that contains the spiders and whip scorpions.

The internal phylogeny of the scorpions has been debated, but genomic analysis consistently places the Bothriuridae as sister to a clade consisting of Scorpionoidea and "Chactoidea". The scorpions diversified between the Devonian and the early Carboniferous. The main division is into the clades Buthida and Iurida. The Bothriuridae diverged starting before temperate Gondwana broke up into separate land masses. The Iuroidea and Chactoidea are both broken up and are shown as "paraphyletic" (with quotation marks).

Thirteen families and about 1,750 species and subspecies of scorpions have been described. In addition, 111 described taxa of scorpions are extinct. The smallest known species is the "Microtityus minimus" in the Buthidae, while the largest species is the "Heterometrus swammerdami" in the Scorpionidae. This classification is based on Soleglad and Fet (2003), which replaced Stockwell's older, unpublished classification. Additional taxonomic changes are from papers by Soleglad et al. (2005).

The extant taxa to the rank of family (numbers of species in parentheses) are:


The body of a scorpion is divided into two parts or tagmata: the cephalothorax or prosoma, and the abdomen or opisthosoma. The opisthosoma is subdivided into a broad anterior portion, the mesosoma or pre-abdomen, and a narrow tail-like posterior, the metasoma or post-abdomen.

The cephalothorax comprises the carapace, eyes, chelicerae (mouth parts), pedipalps (which have chelae, commonly called claws or pincers) and four pairs of walking legs. Scorpions have two eyes on the top of the cephalothorax, and usually two to five pairs of eyes along the front corners of the cephalothorax. While unable to form sharp images, their central eyes are amongst the most light sensitive in the animal kingdom, especially in dim light, and makes it possible for nocturnal species to use starlight to navigate at night. Some species also have light receptors in their tail. The chelicerae are at the front and underneath the carapace. They are pincer-like and have three segments and sharp "teeth". The brain of a scorpion is in the back of the cephalothorax, just above the esophagus. As in other arachnids, the nervous system is highly concentrated in the cephalothorax, but it also has a long ventral nerve cord with segmented ganglia. This may be a "primitive" trait. 

The pedipalp is a segmented, clawed appendage used for prey immobilization, defense and sensory purposes. The segments of the pedipalp (from closest to the body outwards) are coxa, trochanter, femur (humerus), patella, tibia (including the fixed claw and the manus) and tarsus (moveable claw). A scorpion has darkened or granular raised linear ridges, called "keels" or "carinae" on the pedipalp segments and on other parts of the body; these are useful taxonomically. Unlike those of some other arachnids, the legs have not been modified for other purposes, though they may occasionally be used for digging, and females may use them to catch emerging young. The legs are covered in proprioceptors, bristles and sensory setae. Depending on the species, the legs may also have spines and spurs.

The mesosoma or preabdomen is the broad part of the opisthosoma. It consists of the anterior seven somites (segments) of the opisthosoma, each covered dorsally by a sclerotised plate, its tergite. Ventrally, somites 3 to 7 are armoured with matching plates called sternites. The ventral side of somites 1 has a pair of genital opercula covering the gonopore. Sternite 2 forms the basal plate bearing the pectines. Morphologically the pectines are a pair of limbs that function as sensory organs.

The next four somites, 3 to 6, all bear pairs of spiracles. They serve as openings for the scorpion's respiratory organs, known as book lungs. The spiracle openings may be slits, circular, elliptical or oval according to the species. There are thus four pairs of book lungs; each consists of some 140 to 150 thin lamellae filled with air inside a pulmonary chamber, connected on the ventral side to an atrial chamber which opens into a spiracle. Bristles hold the lamellae apart. A muscle opens the spiracle and widens the atrial chamber; dorsoventral muscles contract to compress the pulmonary chamber, forcing air out, and relax to allow the chamber to refill. The 7th and last somite do not bear appendages or any other significant external structures.

The mesosoma contains the heart or "dorsal vessel" which is the center of the scorpion's open circulatory system. The heart is continuous with a deep arterial system which spreads throughout the body. Sinuses return deoxygenated hemolymph to the heart; the hemolymph is re-oxygenated by cardiac pores. The mesosoma also contains the reproductive system. The female gonads are made of three or four tubes that run parallel to each other and are connected by two to four transverse anastomoses. These tubes are the sites for both oocyte formation and embryonic development. They connect to two oviducts which connect to a single atrium leading to the genital orifice. Males have two gonads made of two cylindrical tubes with a ladder-like configuration; they contain cysts which produce spermatozoa. Both tubes end in a spermiduct, one on each side of the mesosoma. They connect to glandular symmetrical structures called paraxial organs, which end at the genital orifice. These secrete chitin-based structures which come together to form the spermatophore.

The "tail" or metasoma consists of five segments and the telson, not strictly a segment. The five segments are merely body rings; they lack apparent sterna or terga, and become larger distally. These segments have keels, setae and bristles which may be used for taxonomic classification. The anus is at the distal and ventral end of the last segment, and is encircled by four anal papillae and the anal arch. 

The telson includes the vesicle, which contains a symmetrical pair of venom glands. Externally it bears the curved sting, the hypodermic aculeus or stinger, equipped with sensory hairs. Each of the venom glands has its own duct to convey its secretion along the aculeus from the bulb of the gland to immediately subterminal of the point of the aculeus, where each of the paired ducts has its own venom pore. An extrinsic muscle system in the tail moves it forward and propels and penetrates with the aculeus, while an intrinsic muscle system attached to the glands pumps venom.

Most scorpions species are nocturnal or crepuscular, finding shelter during the day in burrows and other shelters such as cracks in rocks and tree bark. They prefer areas where the temperature remains between , but may survive temperatures from well below freezing to desert heat. Scorpions can withstand intense heat: "Leiurus quinquestriatus", "Scorpio maurus" and "Hadrurus arizonensis" can live in temperatures of if they are sufficiently hydrated. Desert species must deal with the extreme changes in temperature from day to night or between seasons; "Pectinibuthus birulai" lives in a temperature range of . Scorpions that live outside deserts prefer lower temperatures. The ability to resist cold may be related to the increase in the sugar trehalose when the temperature drops. Scorpions may also hibernate.

Desert scorpions are xerocoles, having several adaptations for water conservation. They excrete insoluble compounds such as xanthine, guanine, and uric acid, not requiring water for their removal from the body. Guanine is the main component and maximizes the amount of nitrogen excreted. A scorpion's cuticle holds in moisture via lipids and waxes from epidermal glands, and protects against ultraviolet radiation. Even when dehydrated, a scorpion can tolerate high osmotic ion concentrations in its hemolymph.

Scorpions make be attacked by other arthropods like ants, spiders, solifugids and centipedes. Major predators include frogs, lizards, snakes, birds and mammals. When threatened, a scorpion raises its claws and tail in a defensive posture. Some species stridulate to warn off predators by rubbing certain hairs, the stinger or the claws. Scorpions host parasites including mites, scuttle flies, nematodes and bacteria. 

Scorpions generally prey on insects, particularly grasshoppers, crickets, termites, beetles and wasps. They also take spiders. sun spiders, woodlice and even small vertebrates including lizards, snakes and mammals. Species with large claws may prey on earthworms and mollusks. The majority of species are opportunistic and consume a variety of prey though some may be highly specialized. Prey size depends on the size of the species. Several scorpion species are sit-and-wait predators, which involves them waiting for prey at or near the entrance to their burrow. Others will actively seek out them out. Scorpions detect their prey with mechanoreceptive and chemoreceptive hairs on their bodies. Scorpions capture prey with their claws. Small animals are merely killed with the claws, particularly by large-clawed species. Larger and more aggressive prey is given a sting, which can happen very quickly at 0.75 seconds. 

Scorpions, like other arachnids, digest their food externally. The chelicerae, which are very sharp, are used to pull small amounts of food off the prey item into a "pre-oral cavity" below the chelicerae and carapace. The digestive juices from the gut are egested onto the food, and the digested food is then sucked into the gut in liquid form. Any solid indigestible matter (such as exoskeleton fragments) is trapped by setae in the pre-oral cavity and ejected. The sucked-in food is pumped into the midgut by the pharynx, where it is further digested. The waste passes through the hindgut and out of the anus. Scorpions can consume large amounts of food at one sitting. They have an efficient food storage organ and a very low metabolic rate, and a relatively inactive lifestyle. This enables them to survive long periods without food. Some are able to survive 6 to 12 months of starvation.

Most scorpions reproduce sexually, with male and female individuals; however, species in some genera, such as "Hottentotta" and "Tityus", and the species "Centruroides gracilis", "Liocheles australasiae", and "Ananteris coineaui" have been reported, not necessarily reliably, to reproduce through parthenogenesis, in which unfertilized eggs develop into living embryos. Receptive females produce pheromones which are picked up by wandering males using their pectines to comb the substrate. Males begin courtship by moving their bodies back and forth, without moving the legs, a behavior known as "juddering". This appears to produce ground vibrations that are picked up by the female. 

The pair then make contact using their pedipalps, and perform a "dance" called the "promenade √† deux" ("walk for two"). In this dance, the male and female move backwards and forwards while facing each other, searching for a suitable place for the male to deposit his spermatophore. The courtship ritual can involve several other behaviors such as a cheliceral kiss, in which the male and female grasp each other's chelicerae, and sexual stinging, in which the male stings the female in the chelae or mesosoma to subdue her. When the male has located a suitably stable substrate, such as hard ground, agglomerated sand, rock, or tree bark, he deposits the spermatophore and guides the female over it. This allows the spermatophore to enter her genital opercula, which triggers release of the sperm, thus fertilizing the female. A mating plug then forms in the female to prevent her from mating again before the young are born. The male and female then abruptly separate. Sexual cannibalism after mating has only been reported anecdotally in scorpions.

Unlike the majority of arachnids, which are oviparous, scorpions seem to be universally viviparous. They are also unusual among terrestrial arthropods in the amount of care a female gives to her offspring. Gestation can last for over a year in some species. The size of a brood varies by species, from three to over 100. Before giving birth, the female elevates the front of her body and positions her pedipalps and front legs under her to catch the young. The young emerge one by one from the genital opercula, expel the embryonic membrane, if any, and are placed on the mother's back where they remain until they have gone though at least one molt. 

The period before the first molt is called the pro-juvenile stage; the young are unable to feed or sting, but have suckers on their tarsi, used to hold on to their mother. This period lasts 5 to 25 days, depending on the species. The brood molt for the first time simultaneously and last 6 to 8 hours, which signals the beginning of the juvenile stage. Juveniles generally resemble smaller versions of adults, with fully-developed pincers, trichobothria and stingers. They are still soft and lack pigments, and thus continue to ride on their mother's back for protection. They became harder and more pigmented over the next couple of days. They may leave their mother temporarily, returning when they sense potential danger. Once the tegument is fully hardened, the young can hunt prey on their own and may soon leave their mother. A scorpion may molt six times on average before reaching maturity, which may not occur until it is 6 to 83 months old, depending on the species. They may live up to 25 years.

Scorpions glow a vibrant blue-green when exposed to certain wavelengths of ultraviolet light such as that produced by a black light, due to the presence of fluorescent chemicals in the cuticle. One fluorescent component is beta-carboline. Accordingly, a hand-held ultraviolet lamp has long been a standard tool for nocturnal field surveys of these animals. Fluorescence occurs as a result of sclerotisation and increases in intensity with each successive instar. This fluorescence may have an active role in the scorpion's ability to detect light.

All known scorpion species possess venom and use it primarily to kill or paralyze their prey so that it can be eaten. Scorpion venom is fast-acting, for effective prey capture and defense. The venom is a mixture of compounds that cause different effects. Only 25 species have venom that is deadly to humans; most of those belong to the family Buthidae (including "Leiurus quinquestriatus", "Hottentotta" spp., "Centruroides" spp., and "Androctonus" spp.). People with allergies are especially at risk; otherwise, first aid is symptomatic, with analgesia. Cases of very high blood pressure are treated with medications that relieve anxiety and relax the blood vessels. Scorpion envenomation with high morbidity and mortality is usually due to either excessive autonomic activity and cardiovascular toxic effects or neuromuscular toxic effects. Antivenom is the specific treatment for scorpion envenomation combined with supportive measures including vasodilators in patients with cardiovascular toxic effects, and benzodiazepines when there is neuromuscular involvement. Although rare, severe hypersensitivity reactions including anaphylaxis to scorpion antivenin are possible.<ref name="10.4103/0972-5229.164807"></ref>

Scorpion venom contains a mixture of neurotoxins, varying by species. Most are peptides (chains of amino acids). Many of them interfere with membrane channels that transport sodium, potassium, calcium, or chloride ions. These channels are essential for nerve conduction, muscle contraction and many other biological processes. Some of these molecules may be useful in medical research and might lead to the development of new disease treatments. Among their potential therapeutic uses are as analgesic, anti-cancer, antibacterial, antifungal, antiviral, antiparasitic, bradykinin-potentiating, and immunosuppressive drugs. As of 2020, no scorpion toxin-based drug is on sale, though chlorotoxin is being trialled for use against glioma, a brain cancer.

Fried scorpion is traditionally eaten in Shandong, China. There, scorpions can be cooked and eaten in a variety of ways, such as roasting, frying, grilling, raw, or alive. The stingers are typically not removed, since direct and sustained heat negates the harmful effects of the venom.

The scorpion is a significant animal culturally, appearing as a motif in art, especially in Islamic art in the Middle East. 
A scorpion motif is often woven into Turkish kilim flat-weave carpets, for protection from their sting. The scorpion is perceived both as an embodiment of evil and a protective force such as a dervish's powers to combat evil. In another context, the scorpion portrays human sexuality. Scorpions are used in folk medicine in South Asia, especially in antidotes for scorpion stings.

One of the earliest occurrences of the scorpion in culture is its inclusion, as "Scorpio", in the 12 signs of the Zodiac by Babylonian astronomers during the Chaldean period. This was then taken up by western astrology.
In ancient Egypt, the goddess Serket was often depicted as a scorpion, one of several goddesses who protected the Pharaoh.
In ancient Greece, a warrior's shield sometimes carried a scorpion device, as seen in red-figure pottery from the 5th century BC. A myth reported by Hesiod and Ovid tells that Gaia sent a giant scorpion to kill the hunter Orion, who had said he would kill all the world's animals; in different versions, either he kills the scorpion or it kills him. Orion and the scorpion both became constellations; as enemies they were placed on opposite sides of the world, so when one rises in the sky, the other sets.

Alongside serpents, scorpions are used to symbolize evil in the New Testament. In Luke 10:19 it is written, "Behold, I give unto you power to tread on serpents and scorpions, and over all the power of the enemy: and nothing shall by any means hurt you." Here, scorpions and serpents symbolize evil. Revelation 9:3 speaks of "the power of the scorpions of the earth."

The scorpion with its powerful sting has been used as the name or symbol of various products and brands, including Italy's Abarth racing cars. 
In the Roman army, the scorpio was a torsion siege engine used to shoot a projectile. 
The British Army's FV101 Scorpion was an armoured reconnaissance vehicle or light tank in service from 1972 to 1994. It holds the Guinness world record for the fastest production tank. A version of the Matilda II tank, fitted with a flail to clear mines, was named the Matilda Scorpion.
Several ships of the Royal Navy have been named HMS "Scorpion", including an 18-gun sloop in 1803, a turret ship in 1863, and a destroyer in 1910. A Montesa scrambler motorcycle was named Scorpion. 

A hand- or forearm-balancing asana in modern yoga as exercise with the back arched and one or both legs pointing forwards over the head is called Scorpion pose. A variety of martial arts films and video games have been entitled "Scorpion King". Scorpions have equally appeared in western artforms including film and poetry: the surrealist filmmaker Luis Bu√±uel made symbolic use of scorpions in his 1930 classic "L'Age d'or" ("The Golden Age"), while Stevie Smith's last collection of poems was entitled "Scorpion and other Poems".



</doc>
<doc id="28923" url="https://en.wikipedia.org/wiki?curid=28923" title="Shriners">
Shriners

Shriners International, also commonly known as The Shriners or formerly known as the Ancient Arabic Order of the Nobles of the Mystic Shrine, (AAONMS) is a Masonic society established in 1870 and is headquartered in Tampa, Florida.

Shriners International describes itself as a fraternity based on fun, fellowship, and the Masonic principles of brotherly love, relief, and truth. There are approximately 350,000 members from 196 temples (chapters) in the U.S.A., Canada, Brazil, Bolivia, Mexico, the Republic of Panama, the Philippines, Europe, and Australia. The organization is best known for the Shriners Hospitals for Children that it administers, and the red fezzes that members wear.

The organization was previously known as "Shriners North America". The name was changed in 2010 across North America, Central America, South America, Europe, and Southeast Asia.

In 1870 there were several thousand Freemasons in Manhattan, many of whom lunched at the Knickerbocker Cottage at a special table on the second floor. There, the idea of a new fraternity for Masons, stressing fun and fellowship, was discussed. Walter M. Fleming, and William J. Florence took the idea seriously enough to act upon it.

Florence, a world-renowned actor, while on tour in Marseille, was invited to a party given by an Arab diplomat. The entertainment was something in the nature of an elaborately staged musical comedy. At its conclusion, the guests became members of a secret society. Florence took copious notes and drawings at his initial viewing and on two other occasions, once in Algiers and once in Cairo. When he returned to New York in 1870, he showed his material to Fleming.

Fleming created the ritual, emblem and costumes. Florence and Fleming were initiated August 13, 1870, and they initiated 11 other men on June 16, 1871.

The group adopted a Middle Eastern theme and soon established Temples (though the term Temple has now generally been replaced by Shrine Auditorium or Shrine Center). The first Temple established was Mecca Temple (now known as Mecca Shriners), established at the New York City Masonic Hall on September 26, 1872. Fleming was the first Potentate.

In 1875, there were only 43 Shriners in the organization. In an effort to encourage membership, at the June 6, 1876 meeting of Mecca Temple, the Imperial Grand Council of the Ancient Order of the Nobles of the Mystic Shrine for North America was created. Fleming was elected the first Imperial Potentate. After some other reworking, by 1878 there were 425 members in 13 temples in eight states, and by 1888, there were 7,210 members in 48 temples in the United States and Canada. By the Imperial Session held in Washington, D.C. in 1900, there were 55,000 members and 82 Temples.

By 1938 there were about 340,000 members in the United States. That year "Life" published photographs of its rites for the first time. It described the Shriners as "among secret lodges the No. 1 in prestige, wealth and show", and stated that "[i]n the typical city, especially in the Middle West, the Shriners will include most of the prominent citizens."

Shriners often participate in local parades, sometimes as rather elaborate units: miniature vehicles in themes (all sports cars; all miniature 18-wheeler trucks; all fire engines, and so on), an "Oriental Band" dressed in cartoonish versions of Middle Eastern dress; pipe bands, drummers, motorcycle units, Drum and Bugle Corps, and even traditional brass bands.

Until 2000, before being eligible for membership in the Shrine, a Mason had to complete either the Scottish Rite or York Rite systems, but now any Master Mason can join.

In the past, Shriners have practiced hazing rituals as a part of initiating new members: in 1991, a would-be Shriner sued the Oleika Shrine Temple of Lexington, Kentucky over injuries suffered during the hazing, which included being blindfolded and having a jolt of electricity applied to his bare buttocks.

While there are plenty of activities for Shriners, there are two organizations tied to the Shrine that are for women only: The Ladies' Oriental Shrine and the Daughters of the Nile. They both support the Shriners Hospitals and promote sociability, and membership in either organization is open to any woman 18 years of age and older who is related to a Shriner or Master Mason by birth or marriage.

The Ladies Oriental Shrine of North America was founded in 1903 in Wheeling, West Virginia, and the Daughters of the Nile was founded in 1913 in Seattle, Washington. The latter organization has locals called "Temples". There were ten of these in 1922. Among the famous members of the Daughters of the Nile was First Lady Florence Harding, wife of Warren G. Harding.

Some of the earliest Shrine Centers often chose a Moorish Revival style for their Temples. Architecturally notable Shriners Temples include the Shrine Auditorium in Los Angeles, the former Mecca Temple, now called New York City Center and used primarily as a concert hall, Newark Symphony Hall, the Landmark Theater (formerly The Mosque) in Richmond, Virginia, the Tripoli Shrine Temple in Milwaukee, Wisconsin, the Polly Rosenbaum Building (formerly the El Zaribah Shrine Auditorium) in Phoenix, the Helena Civic Center (Montana) (formerly the Algeria Shrine Temple), Abou Ben Adhem Shrine Mosque in Springfield, Missouri and the Fox Theatre (Atlanta, Georgia) which was jointly built between the Atlanta Shriners and movie mogul William Fox.

The Shrine's charitable arm is the Shriners Hospitals for Children, a network of 22 healthcare facilities in the United States, Mexico, and Canada.

In June 1920, the Imperial Council Session voted to establish a "Shriners Hospital for Crippled Children." The purpose of this hospital was to treat orthopedic injuries and conditions, diseases, burns, spinal cord injuries, and birth defects, such as cleft lip and palate, in children. After much research and debate, the committee chosen to determine the site of the hospital decided there should be a network of hospitals across North America. The first hospital opened in 1922 in Shreveport, Louisiana. By the end of the decade 13 more hospitals were operational. Shriners Hospitals now provide orthopedic care, burn treatment, cleft lip and palate care and spinal cord injury rehabilitation.

The rules for all of the Shriners Hospitals are simple and to the point: Any child under the age of 18 can be admitted to the hospital if, in the opinion of the doctors, the child can be treated. There is no requirement for religion, race or relationship to a Shriner.

Until June 2012, all care at Shriners Hospitals was provided without charge to patients and their families. At that time, because the size of their endowment had decreased due to losses in the stock market, Shriners Hospitals started billing patients' insurance companies, but still offered free care to children without insurance and waives all out of pocket costs insurance does not cover. Shriners Hospitals for Children is a 501(c)(3) nonprofit organization, meaning that they rely on the generosity of donors to cover the cost of treatment for their patients.

In 2008, Shriners Hospitals had a total budget of $826 million. In 2007 they approved 39,454 new patient applications, and attended to the needs of 125,125 patients. Shriners Hospitals for Children can be found in these cities:


<nowiki>*</nowiki>This location is an outpatient, ambulatory care center.

Most Shrine Temples support several parade units. These units are responsible for promoting a positive Shriner image to the public by participating in local parades. The parade units often include miniature cars powered by lawn mower engines.
An example of a Shrine parade unit is the Heart Shrine Clubs' Original Fire Patrol of Effingham, Illinois. This unit operates miniature fire engines, memorializing a hospital fire that took place in the 1940s in Effingham. They participate in most parades in a 100-mile radius of Effingham. Shriners in Dallas, Texas participate annually in the Twilight Parade at the Texas State Fair.

Shriners in St. Louis have several parade motor units, including miniature cars styled after 1932 Ford coupes and 1970s-era Jeep CJ models, and a unit of miniature Indianapolis-styled race cars. Some of these are outfitted with high-performance, alcohol-fueled engines. The drivers' skills are demonstrated during parades with high-speed spinouts.

The Shriners are committed to community service and have been instrumental in countless public projects throughout their domain.

Shriners host the annual "East-West Shrine Game", a college football all-star game.

The Shriners originally hosted a golf tournament in association with singer/actor Justin Timberlake, titled the "Justin Timberlake Shriners Hospitals for Children Open", a PGA Tour golf tournament held in Las Vegas, Nevada. The relationship between Timberlake and the Shriners ended in 2012, due to the lack of previously agreed participation on Timberlake's part. In July 2012, the PGA Tour and Shriners Hospitals for Children announced a five-year title sponsorship extension, carrying the commitment to the Shriners Hospitals for Children Open through 2017. now titled "The Shriners Hospitals for Children Open", It is still held in Las Vegas, Nevada.

Once a year, the fraternity meets for the Imperial Council Session in a major North American city. It is not uncommon for these conventions to have 20,000 participants or more, which generates significant revenue for the local economy.

Many Shrine Centers also hold a yearly "Shrine Circus" as a fundraiser.

Singer Ray Stevens had a hit record with the country-and-western novelty song "Shriner's Convention" in 1980. Real Shriners have taken the song in stride, and have even welcomed Stevens's participation in fundraising activities, as his fame attracts attendees to charity events.




</doc>
<doc id="28925" url="https://en.wikipedia.org/wiki?curid=28925" title="Science fiction fandom">
Science fiction fandom

Science fiction fandom or SF fandom is a community or fandom of people interested in science fiction in contact with one another based upon that interest. SF fandom has a life of its own, but not much in the way of formal organization (although clubs such as the Futurians (1937‚Äì1945) are a recognized example of organized fandom).

Most often called simply "fandom" within the community, it can be viewed as a distinct subculture, with its own literature and jargon; marriages and other relationships among fans are common, as are multi-generational fan families.

Science fiction fandom started through the letter column of Hugo Gernsback's fiction magazines. Not only did fans write comments about the stories‚Äîthey sent their addresses, and Gernsback published them. Soon, fans were writing letters directly to each other, and meeting in person when they lived close together, or when one of them could manage a trip. In New York City, David Lasser, Gernsback's managing editor, nurtured the birth of a small local club called the Scienceers, which held its first meeting in a Harlem apartment on December 11, 1929. Almost all the members were adolescent boys. Around this time a few other small local groups began to spring up in metropolitan areas around the United States, many of them connecting with fellow enthusiasts via the Science Correspondence Club. In May 1930 the first science-fiction fan magazine, "The Comet", was produced by the Chicago branch of the Science Correspondence Club under the editorship of Raymond A. Palmer (later a noted, and notorious, sf magazine editor) and Walter Dennis. In January 1932, the New York City circle, which by then included future comic-book editors Julius Schwartz and Mort Weisinger, brought out the first issue of their own publication, "The Time Traveller", with Forrest J Ackerman of the embryonic Los Angeles group as a contributing editor.

In 1934, Gernsback established a correspondence club for fans called the Science Fiction League, the first fannish organization. Local groups across the nation could join by filling out an application. A number of clubs came into being around this time. LASFS (the Los Angeles Science Fantasy Society) was founded at this time as a local branch of the SFL, while several competing local branches sprang up in New York City and immediately began feuding among themselves.

In 1935, PSFS (the Philadelphia Science Fiction Society, 1935‚Äìpresent) was formed. The next year, half a dozen fans from NYC came to Philadelphia to meet with the PSFS members, as the first Philadelphia Science Fiction Conference, which some claim as the world's first science fiction convention.

Soon after the fans started to communicate directly with each other came the creation of science fiction fanzines. These amateur publications might or might not discuss science fiction and were generally traded rather than sold. They ranged from the utilitarian or inept to professional-quality printing and editing. In recent years, Usenet newsgroups such as rec.arts.sf.fandom, websites and blogs have somewhat supplanted printed fanzines as an outlet for expression in fandom, though many popular fanzines continue to be published. Science-fiction fans have been among the first users of computers, email, personal computers and the Internet.

Many professional science fiction authors started their interest in science fiction as fans, and some still publish their own fanzines or contribute to those published by others.

A widely regarded (though by no means error-free) history of fandom in the 1930s can be found in Sam Moskowitz's "The Immortal Storm: A History of Science Fiction Fandom" Hyperion Press 1988 (original edition The Atlanta Science Fiction Organization Press, Atlanta, Georgia 1954). Moskowitz was himself involved in some of the incidents chronicled and has his own point of view, which has often been criticized.

Organized fandom in Sweden ("Sverifandom") emerged during the early-1950s. The first Swedish science fiction fanzine was started in the early 1950s. The oldest still existing club, Club Cosmos in Gothenburg, was formed in 1954, and the first Swedish science-fiction convention, LunCon, was held in Lund in 1956.

Today, there are a number of science fiction clubs in the country, including Skandinavisk F√∂rening f√∂r Science Fiction (whose club fanzine, "Science Fiction Forum", was once edited by Stieg Larsson, a board member and one-time chairman thereof), Link√∂pings Science Fiction-F√∂rening and Sigma Terra Corps. Between one and four science-fiction conventions are held each year in Sweden, among them Swecon, the annual national Swedish con. An annual prize is awarded to someone that has contributed to the national fandom by the Alvar Appeltofft Memorial Fund.

SF fandom in the UK has close ties with that in the USA. In the UK there are multiple conventions. The largest regular convention for Literary SF (Book focused) fandom is the British National convention or Eastercon. Strangely enough this is held over the Easter weekend. Committee membership and location changes year-to-year. The license to use the Eastercon name for a year is awarded by votes of the business meeting of the Eastercon two years previously. There are substantially larger events run by UK Media Fandom and commercial organisations also run Gate Shows (for-profit operations with paid staff.) The UK has also hosted the Worldcon several times, most recently in 2014. News of UK events appears in the fanzine Ansible produced by David Langford each month.

The beginning of an Italian science fiction fandom can be located between the late 1950s and early 1960s, when magazines such as "Oltre il Cielo" and "Futuro" started to publish readers‚Äô letters and promote correspondences and the setting-up of clubs in various cities. Among the first fanzines, "Futuria Fantasia" was cyclostyled in Milan in 1963 by Luigi Cozzi (later to become a filmmaker), its title paid homage to Ray Bradbury's fanzine by the same name; "L‚ÄôAspidistra", edited by Riccardo Leveghi in Trento starting in 1965 featured contributions by Gianfranco de Turris, Gian Luigi Staffilano, and Sebastiano Fusco, future editors of professional magazines and book series; also Luigi Naviglio, editor in 1965 of the fanzine "Nuovi Orizzonti", was soon to become a writer for "I Romanzi del Cosmo". During subsequent years fanzines continued to function as training grounds for future editors and writers, and the general trend was towards improved quality and life expectancy (e.g. "The Time Machine" run for 50 issues starting in 1975, "Intercom" for 149 issues between 1979 and 1999, before its migration to the web as an e-zine until 2003, then as a website).

In 1963 the first Trieste Festival of Science Fiction Cinema took place, anticipating the first conventions as an opportunity for a nationwide social gathering. Informal meetings were organized in Milan, Turin and Carrara between 1965 and 1967. In 1972, the first European convention, Eurocon, was organized in Trieste, during which an Italia Award was also created. Eurocon was back in Italy in 1980 and 2009 (in 1989 a Eurocon was held in San Marino).

Since its foundation in 2013, the association "World SF Italia" coordinates the organization the annual national convention (Italcon) and awards (Premio Italia ‚Äì with thirty- two categories across media ‚Äì and Premio Vegetti ‚Äì best Italian novel and essay).

Since the late 1930s, SF fans have organized conventions, non-profit gatherings where the fans (some of whom are also professionals in the field) meet to discuss SF and generally enjoy themselves. (A few fannish couples have held their weddings at conventions.) The 1st World Science Fiction Convention or Worldcon was held in conjunction with the 1939 New York World's Fair, and has been held annually since the end of World War II. Worldcon has been the premier convention in fandom for over half a century; it is at this convention that the Hugo Awards are bestowed, and attendance can approach 8,000 or more.

SF writer Cory Doctorow calls science fiction "perhaps the most social of all literary genres", and states, "Science fiction is driven by organized fandom, volunteers who put on hundreds of literary conventions in every corner of the globe, every weekend of the year."

SF conventions can vary from minimalist "relaxacons" with a hundred or so attendees to heavily programmed events with four to six or more simultaneous tracks of programming, such as WisCon and Worldcons.

Commercial shows dealing with SF-related fields are sometimes billed as 'science fiction conventions,' but are operated as for-profit ventures, with an orientation towards passive spectators, rather than involved fans, and a tendency to neglect or ignore written SF in favor of television, film, comics, video games, etc. One of the largest of these is the annual Dragon*Con in Atlanta, Georgia with an attendance of more than 20,000 since 2000.

In the United States, many science-fiction societies were launched as chapters of the Science Fiction League and, when it faded into history, several of the original League chapters remained viable and were subsequently incorporated as independent organizations. Most notable among the former League chapters which were spun off was the Philadelphia Science Fiction Society, which served as a model for subsequent SF societies formed independent of the League history.

Science-fiction societies, more commonly referred to as "clubs" except on the most formal of occasions, form a year-round base of activities for science-fiction fans. They are often associated with an SF convention or group of conventions, but maintain a separate existence as cultural institutions within specific geographic regions. Several have purchased property and maintain ongoing collections of SF literature available for research, as in the case of the Los Angeles Science Fantasy Society, the New England Science Fiction Association, and the Baltimore Science Fiction Society. Other SF Societies maintain a more informal existence, meeting at general public facilities or the homes of individual members, such as the Bay Area Science Fiction Association.

As a community devoted to discussion and exploration of new ideas, fandom has become an incubator for many groups that started out as special interests within fandom, some of which have partially separated into independent intentional communities not directly associated with science fiction. Among these groups are comic-book fandom, media fandom, the Society for Creative Anachronism, gaming, and furry fandom, sometimes referred to collectively as "fringe fandoms" with the implication that the original fandom centered on science-fiction texts (magazines and later books and fanzines) is the "true" or "core" fandom. Fandom also welcomes and shares interest with other groups including LGBT communities, libertarians, neo-pagans, and space activist groups like the L5 Society, among many others. Some groups exist almost entirely within fandom but are distinct and cohesive subcultures in their own rights, such as filkers, costumers, and convention runners (sometimes called "SMOFs").

Fandom encompasses subsets of fans that are principally interested in a single writer or subgenre, such as Tolkien fandom, and ("Trekkies"). Even short-lived television series may have dedicated followings, such as the fans of Joss Whedon's "Firefly" television series and movie "Serenity", known as Browncoats.

Participation in science fiction fandom often overlaps with other similar interests, such as fantasy role-playing games, comic books and anime, and in the broadest sense fans of these activities are felt to be part of the greater community of SF fandom.

There are active SF fandoms around the world. Fandom in non-Anglophone countries is based partially on local literature and media, with cons and other elements resembling those of English-speaking fandom, but with distinguishing local features. For example, Finland's national gathering Finncon is funded by the government, while all conventions and fan activities in Japan are heavily influenced by anime and manga.

Science fiction and fantasy fandom has its own slang or jargon, sometimes called "fanspeak" (the term has been in use since at least 1962).

Fanspeak is made up of acronyms, blended words, obscure in-jokes, and standard terms used in specific ways. Some terms used in fanspeak have spread to members of the Society for Creative Anachronism ("Scadians"), Renaissance Fair participants ("Rennies"), hacktivists, and internet gaming and chat fans, due to the social and contextual intersection between the communities. Examples of fanspeak used in these broader fannish communities include gafiate, a term meaning to drop out of SF related community activities, with the implication to Get A Life. The word is derived via the acronym for "get away from it all". A related term is fafiate, for "forced away from it all". The implication is that one would really rather still be involved in fandom, but circumstances make it impossible.

Two other acronyms commonly used in the community are FIAWOL (Fandom Is A Way Of Life) and its opposite FIJAGH (Fandom Is Just A Goddamned Hobby) to describe two ways of looking at the place of fandom in one's life.

Science-fiction fans often refer to themselves using the irregular plural "fen": man/men, fan/fen.

As science fiction fans became professional writers, they started slipping the names of their friends into stories. Wilson "Bob" Tucker slipped so many of his fellow fans and authors into his works that doing so is called tuckerization.

The subgenre of "recursive science fiction" has a fan-maintained bibliography at the New England Science Fiction Association's website; some of it is about science fiction fandom, some not.

In Robert Bloch's 1956 short story, "A Way Of Life", science-fiction fandom is the only institution to survive a nuclear holocaust and eventually becomes the basis for the reconstitution of civilization. The science-fiction novel "Gather in the Hall of the Planets", by K.M. O'Donnell (aka Barry Malzberg), 1971, takes place at a New York City science-fiction convention and features broad parodies of many SF fans and authors. A pair of SF novels by Gene DeWeese and Robert "Buck" Coulson, "Now You See It/Him/Them" and "Charles Fort Never Mentioned Wombats" are set at Worldcons; the latter includes an in-character "introduction" by Wilson Tucker (himself a character in the novel) which is a sly self-parody verging on a self-tuckerization.

The 1991 SF novel "Fallen Angels" by Larry Niven, Jerry Pournelle and Michael Flynn constitutes a tribute to SF fandom. The story includes a semi-illegal fictional Minneapolis Worldcon in a post-disaster world where science, and thus fandom, is disparaged. Many of the characters are barely tuckerized fans, mostly from the Greater Los Angeles area.

Mystery writer Sharyn McCrumb's "Bimbos of the Death Sun" and "Zombies of the Gene Pool" are murder mysteries set at a science-fiction convention and within the broader culture of fandom respectively. While containing mostly nasty caricatures of fans and fandom, some fans take them with good humor; others consider them vicious and cruel.

In 1994 and 1996, two anthologies of alternate history science fiction involving World Science Fiction Conventions, titled "Alternate Worldcons" and "Again, Alternate Worldcons", edited by Mike Resnick were published.

A.E. van Vogt's 1940 novel "Slan" was about a mutant variety of humans who are superior to regular humanity and are therefore hunted down and killed by the normal human population. While the story has nothing to do with fandom, many science-fiction fans felt very close to the protagonists, feeling their experience as bright people in a mundane world mirrored that of the mutants; hence, the rallying cry, "Fans Are Slans!"; and the tradition that a building inhabited primarily by fans can be called a slan shack.




</doc>
<doc id="28926" url="https://en.wikipedia.org/wiki?curid=28926" title="Spin">
Spin

Spin or spinning may refer to:















</doc>
<doc id="28927" url="https://en.wikipedia.org/wiki?curid=28927" title="Stellar classification">
Stellar classification

In astronomy, stellar classification is the classification of stars based on their spectral characteristics. Electromagnetic radiation from the star is analyzed by splitting it with a prism or diffraction grating into a spectrum exhibiting the rainbow of colors interspersed with spectral lines. Each line indicates a particular chemical element or molecule, with the line strength indicating the abundance of that element. The strengths of the different spectral lines vary mainly due to the temperature of the photosphere, although in some cases there are true abundance differences. The "spectral class" of a star is a short code primarily summarizing the ionization state, giving an objective measure of the photosphere's temperature.

Most stars are currently classified under the Morgan‚ÄìKeenan (MK) system using the letters "O", "B", "A", "F", "G", "K", and "M", a sequence from the hottest ("O" type) to the coolest ("M" type). Each letter class is then subdivided using a numeric digit with "0" being hottest and "9" being coolest (e.g., A8, A9, F0, and F1 form a sequence from hotter to cooler). The sequence has been expanded with classes for other stars and star-like objects that do not fit in the classical system, such as class¬†"D" for white dwarfs and classes¬†"S" and "C" for carbon stars.

In the MK system, a luminosity class is added to the spectral class using Roman numerals. This is based on the width of certain absorption lines in the star's spectrum, which vary with the density of the atmosphere and so distinguish giant stars from dwarfs. Luminosity class¬†"0" or "Ia+" is used for "hypergiants", class¬†"I" for "supergiants", class¬†"II" for bright "giants", class¬†"III" for regular "giants", class¬†"IV" for "sub-giants", class¬†"V" for "main-sequence stars", class¬†"sd" (or "VI") for "sub-dwarfs", and class¬†"D" (or "VII") for "white dwarfs". The full spectral class for the Sun is then G2V, indicating a main-sequence star with a surface temperature around 5,800¬†K.

The conventional color description takes into account only the peak of the stellar spectrum. In actuality, however, stars radiate in all parts of the spectrum. Because all spectral colors combined appear white, the actual apparent colors the human eye would observe are far lighter than the conventional color descriptions would suggest. This characteristic of 'lightness' indicates that the simplified assignment of colors within the spectrum can be misleading. Excluding color-contrast illusions in dim light, there are no green, indigo, or violet stars. Red dwarfs are a deep shade of orange, and brown dwarfs do not literally appear brown, but hypothetically would appear dim grey to a nearby observer.

The modern classification system is known as the "Morgan‚ÄìKeenan" (MK) classification. Each star is assigned a spectral class from the older Harvard spectral classification and a luminosity class using Roman numerals as explained below, forming the star's spectral type.

Other modern stellar classification systems, such as the UBV system, are based on color indexes‚Äîthe measured differences in three or more color magnitudes. Those numbers are given labels such as "U‚àíV" or "B‚àíV", which represent the colors passed by two standard filters (e.g. "U"ltraviolet, "B"lue and "V"isual).

The "Harvard system" is a one-dimensional classification scheme by astronomer Annie Jump Cannon, who re-ordered and simplified the prior alphabetical system by Draper (see next paragraph). Stars are grouped according to their spectral characteristics by single letters of the alphabet, optionally with numeric subdivisions. Main-sequence stars vary in surface temperature from approximately 2,000 to 50,000¬†K, whereas more-evolved stars can have temperatures above 100,000¬†K. Physically, the classes indicate the temperature of the star's atmosphere and are normally listed from hottest to coldest.

The spectral classes O through M, as well as other more specialized classes discussed later, are subdivided by Arabic numerals (0‚Äì9), where 0 denotes the hottest stars of a given class. For example, A0 denotes the hottest stars in class¬†A and A9 denotes the coolest ones. Fractional numbers are allowed; for example, the star Mu Normae is classified as O9.7. The Sun is classified as G2.

Conventional color descriptions are traditional in astronomy, and represent colors relative to the mean color of an A¬†class star, which is considered to be white. The apparent color descriptions are what the observer would see if trying to describe the stars under a dark sky without aid to the eye, or with binoculars. However, most stars in the sky, except the brightest ones, appear white or bluish white to the unaided eye because they are too dim for color vision to work. Red supergiants are cooler and redder than dwarfs of the same spectral type, and stars with particular spectral features such as carbon stars may be far redder than any black body.

The fact that the Harvard classification of a star indicated its surface or photospheric temperature (or more precisely, its effective temperature) was not fully understood until after its development, though by the time the first Hertzsprung‚ÄìRussell diagram was formulated (by 1914), this was generally suspected to be true. In the 1920s, the Indian physicist Meghnad Saha derived a theory of ionization by extending well-known ideas in physical chemistry pertaining to the dissociation of molecules to the ionization of atoms. First he applied it to the solar chromosphere, then to stellar spectra.

Harvard astronomer Cecilia Payne then demonstrated that the "O-B-A-F-G-K-M" spectral sequence is actually a sequence in temperature. Because the classification sequence predates our understanding that it is a temperature sequence, the placement of a spectrum into a given subtype, such as B3 or A7, depends upon (largely subjective) estimates of the strengths of absorption features in stellar spectra. As a result, these subtypes are not evenly divided into any sort of mathematically representable intervals.

The "Yerkes spectral classification", also called the "MKK" system from the authors' initials, is a system of stellar spectral classification introduced in 1943 by William Wilson Morgan, Philip C. Keenan, and Edith Kellman from Yerkes Observatory. This two-dimensional (temperature and luminosity) classification scheme is based on spectral lines sensitive to stellar temperature and surface gravity, which is related to luminosity (whilst the "Harvard classification" is based on just surface temperature). Later, in 1953, after some revisions of list of standard stars and classification criteria, the scheme was named the "Morgan‚ÄìKeenan classification", or "MK", and this system remains in use.

Denser stars with higher surface gravity exhibit greater pressure broadening of spectral lines. The gravity, and hence the pressure, on the surface of a giant star is much lower than for a dwarf star because the radius of the giant is much greater than a dwarf of similar mass. Therefore, differences in the spectrum can be interpreted as "luminosity effects" and a luminosity class can be assigned purely from examination of the spectrum.

A number of different "luminosity classes" are distinguished, as listed in the table below.

Marginal cases are allowed; for example, a star may be either a supergiant or a bright giant, or may be in between the subgiant and main-sequence classifications. 
In these cases, two special symbols are used:

For example, a star classified as A3-4III/IV would be in between spectral types A3 and A4, while being either a giant star or a subgiant.

Sub-dwarf classes have also been used: VI for sub-dwarfs (stars slightly less luminous than the main sequence).

Nominal luminosity class¬†VII (and sometimes higher numerals) is now rarely used for white dwarf or "hot sub-dwarf" classes, since the temperature-letters of the main sequence and giant stars no longer apply to white dwarfs.

Occasionally, letters "a" and "b" are applied to luminosity classes other than supergiants; for example, a giant star slightly more luminous than typical may be given a luminosity class of IIIb.

A sample of extreme V stars with strong absorption in He II Œª4686 spectral lines have been given the "Vz" designation. An example star is HD 93129 B.

Additional nomenclature, in the form of lower-case letters, can follow the spectral type to indicate peculiar features of the spectrum.

For example, 59 Cygni is listed as spectral type¬†B1.5Vnne, indicating a spectrum with the general classification B1.5V, as well as very broad absorption lines and certain emission lines.

The reason for the odd arrangement of letters in the Harvard classification is historical, having evolved from the earlier Secchi classes and been progressively modified as understanding improved.

During the 1860s and 1870s, pioneering stellar spectroscopist Angelo Secchi created the "Secchi classes" in order to classify observed spectra. By 1866, he had developed three classes of stellar spectra, shown in the table below.

In the late 1890s, this classification began to be superseded by the Harvard classification, which is discussed in the remainder of this article.

The Roman numerals used for Secchi classes should not be confused with the completely unrelated Roman numerals used for Yerkes luminosity classes and the proposed neutron star classes.

In the 1880s, the astronomer Edward C. Pickering began to make a survey of stellar spectra at the Harvard College Observatory, using the objective-prism method. A first result of this work was the "Draper Catalogue of Stellar Spectra", published in 1890. Williamina Fleming classified most of the spectra in this catalogue and was credited with classifying over 10,000 featured stars and discovering 10 novae and more than 200 variable stars. With the help of the Harvard computers, especially Williamina Fleming, the first iteration of the Henry Draper catalogue was devised to replace the Roman-numeral scheme established by Angelo Secchi.

The catalogue used a scheme in which the previously used Secchi classes (I to V) were subdivided into more specific classes, given letters from A to P. Also, the letter Q was used for stars not fitting into any other class. Fleming worked with Pickering to differentiate 17 different classes based on the intensity of hydrogen spectral lines, which causes variation in the wavelengths emanated from stars and results in variation in color appearance. The spectra in class A tended to produce the strongest hydrogen absorption lines while spectra in class O produced virtually no visible lines. The lettering system displayed the gradual decrease in hydrogen absorption in the spectral classes when moving down the alphabet. This classification system was later modified by Annie Jump Cannon and Antonia Maury to produce the Harvard spectral classification scheme.

In 1897, another computer at Harvard, Antonia Maury, placed the Orion subtype of Secchi class¬†I ahead of the remainder of Secchi class¬†I, thus placing the modern type¬†B ahead of the modern type¬†A. She was the first to do so, although she did not use lettered spectral types, but rather a series of twenty-two types numbered from I to XXII. Because the 22 Roman numeral groupings didn‚Äôt account for additional variations in spectra, three additional divisions were made to further specify differences. Groups I through V included Orion type stars that displayed an increasing strength in hydrogen absorption lines from group I to group V. Groups VII to XI were Secchi type I stars with decreasing strength in hydrogen absorption lines from groups VII to XI.¬† Group VI acted as an intermediate between the Orion type and Secchi type I group, while groups XIII to XVI included Secchi type 2 stars with decreasing hydrogen absorption lines and increasing solar-type metallic lines. Groups XVII to XX included Secchi type 3 stars with increasing spectral lines. Group XXI included Secchi type 4 stars, and group XXII included Wolf-Reyet stars. An additional categorization using lowercase letters was added to differentiate relative line appearance in spectra. The lines were defined as a) average width, b) hazy, or c) sharp.

Antonia Maury published her own stellar classification catalogue in 1897 called "Spectra of Bright Stars Photographed with the 11-inch Draper Telescope as Part of the Henry Draper Memorial", which included 4,800 photographs and Maury‚Äôs analyses of 681 bright northern stars. This was the first instance in which a woman was credited for an observatory publication.

In 1901, Annie Jump Cannon returned to the lettered types, but dropped all letters except O, B, A, F, G, K, M, and N used in that order, as well as P for planetary nebulae and Q for some peculiar spectra. She also used types such as B5A for stars halfway between types B and A, F2G for stars one-fifth of the way from F to G, and so on. Finally, by 1912, Cannon had changed the types B, A, B5A, F2G, etc. to B0, A0, B5, F2, etc. This is essentially the modern form of the Harvard classification system. This system was developed through the analysis of spectra on photographic plates, which could convert light emanated from stars into a readable spectra.

A common mnemonic for remembering the order of the spectral type letters, from hottest to coolest, is "Oh, Be A Fine Guy/Girl: Kiss Me!".

A luminosity classification known as the Mount Wilson system was used to distinguish between stars of different luminosities. This notation system is still sometimes seen on modern spectra.

The stellar classification system is taxonomic, based on type specimens, similar to classification of species in biology: The categories are defined by one or more standard stars for each category and sub-category, with an associated description of the distinguishing features.

Stars are often referred to as "early" or "late" types. "Early" is a synonym for "hotter", while "late" is a synonym for "cooler".

Depending on the context, "early" and "late" may be absolute or relative terms. "Early" as an absolute term would therefore refer to O or B, and possibly A stars. As a relative reference it relates to stars hotter than others, such as "early K" being perhaps K0, K1, and K3.

"Late" is used in the same way, with an unqualified use of the term indicating stars with spectral types such as K and M, but it can also be used for stars that are cool relative to other stars, as in using "late G" to refer to G7, G8, and G9.

In the relative sense, "early" means a lower Arabic numeral following the class letter, and "late" means a higher number.

This obscure terminology is a hold-over from an early 20th¬†century model of stellar evolution, which supposed that stars were powered by gravitational contraction via the Kelvin‚ÄìHelmholtz mechanism, which is now known to not apply to main sequence stars. If that were true, then stars would start their lives as very hot "early-type" stars and then gradually cool down into "late-type" stars. This mechanism provided ages of the Sun that were much smaller than what is observed in the geologic record, and was rendered obsolete by the discovery that stars are powered by nuclear fusion. The terms "early" and "late" were carried over, beyond the demise of the model they were based on.

O-type stars are very hot and extremely luminous, with most of their radiated output in the ultraviolet range. These are the rarest of all main-sequence stars. About 1 in 3,000,000 (0.00003%) of the main-sequence stars in the solar neighborhood are O-type stars. Some of the most massive stars lie within this spectral class. O-type stars frequently have complicated surroundings that make measurement of their spectra difficult.

O-type spectra formerly were defined by the ratio of the strength of the He¬†II Œª4541 relative to that of He¬†I Œª4471, where Œª is the radiation wavelength. Spectral type O7 was defined to be the point at which the two intensities are equal, with the He¬†I line weakening towards earlier types. Type¬†O3 was, by definition, the point at which said line disappears altogether, although it can be seen very faintly with modern technology. Due to this, the modern definition uses the ratio of the nitrogen line N¬†IV Œª4058 to N¬†III ŒªŒª4634-40-42.

O-type stars have dominant lines of absorption and sometimes emission for He¬†II lines, prominent ionized (Si¬†IV, O¬†III, N¬†III, and C¬†III) and neutral helium lines, strengthening from O5 to O9, and prominent hydrogen Balmer lines, although not as strong as in later types. Because they are so massive, O-type stars have very hot cores and burn through their hydrogen fuel very quickly, so they are the first stars to leave the main sequence.

When the MKK classification scheme was first described in 1943, the only subtypes of class¬†O used were O5 to O9.5. The MKK scheme was extended to O9.7 in 1971 and O4 in 1978, and new classification schemes that add types O2, O3, and O3.5 have subsequently been introduced.

Spectral standards:

B-type stars are very luminous and blue. Their spectra have neutral helium lines, which are most prominent at the B2 subclass, and moderate hydrogen lines. As O- and B-type stars are so energetic, they only live for a relatively short time. Thus, due to the low probability of kinematic interaction during their lifetime, they are unable to stray far from the area in which they formed, apart from runaway stars.

The transition from class¬†O to class¬†B was originally defined to be the point at which the He¬†II Œª4541 disappears. However, with modern equipment, the line is still apparent in the early B-type stars. Today for main-sequence stars, the B-class is instead defined by the intensity of the He¬†I violet spectrum, with the maximum intensity corresponding to class¬†B2. For supergiants, lines of silicon are used instead; the Si¬†IV Œª4089 and Si¬†III Œª4552 lines are indicative of early B. At mid B, the intensity of the latter relative to that of Si¬†II ŒªŒª4128-30 is the defining characteristic, while for late B, it is the intensity of Mg¬†II Œª4481 relative to that of He¬†I Œª4471.

These stars tend to be found in their originating OB associations, which are associated with giant molecular clouds. The Orion OB1 association occupies a large portion of a spiral arm of the Milky Way and contains many of the brighter stars of the constellation Orion. About 1 in 800 (0.125%) of the main-sequence stars in the solar neighborhood are B-type main-sequence stars.

Massive yet non-supergiant entities known as "Be¬†stars" are main-sequence stars that notably have, or had at some time, one or more Balmer lines in emission, with the hydrogen-related electromagnetic radiation series projected out by the stars being of particular interest. Be stars are generally thought to feature unusually strong stellar winds, high surface temperatures, and significant attrition of stellar mass as the objects rotate at a curiously rapid rate. Objects known as "B(e)" or "B[e]" stars possess distinctive neutral or low ionisation emission lines that are considered to have 'forbidden mechanisms', undergoing processes not normally allowed under current understandings of quantum mechanics.

Spectral standards:

A-type stars are among the more common naked eye stars, and are white or bluish-white. They have strong hydrogen lines, at a maximum by A0, and also lines of ionized metals (Fe¬†II, Mg¬†II, Si¬†II) at a maximum at A5. The presence of Ca¬†II lines is notably strengthening by this point. About 1 in 160 (0.625%) of the main-sequence stars in the solar neighborhood are A-type stars.

Spectral standards:

F-type stars have strengthening spectral lines "H" and "K" of Ca¬†II. Neutral metals (Fe¬†I, Cr¬†I) beginning to gain on ionized metal lines by late F. Their spectra are characterized by the weaker hydrogen lines and ionized metals. Their color is white. About 1 in 33 (3.03%) of the main-sequence stars in the solar neighborhood are F-type stars.

Spectral standards:

G-type stars, including the Sun, have prominent spectral lines "H" and "K" of Ca¬†II, which are most pronounced at G2. They have even weaker hydrogen lines than F, but along with the ionized metals, they have neutral metals. There is a prominent spike in the G band of CH molecules. Class¬†G main-sequence stars make up about 7.5%, nearly one in thirteen, of the main-sequence stars in the solar neighborhood.

Class G contains the "Yellow Evolutionary Void". Supergiant stars often swing between O or B (blue) and K or M (red). While they do this, they do not stay for long in the yellow supergiant G class, as this is an extremely unstable place for a supergiant to be.

Spectral standards:

K-type stars are orangish stars that are slightly cooler than the Sun. They make up about 12% of the main-sequence stars in the solar neighborhood. There are also giant K-type stars, which range from hypergiants like RW Cephei, to giants and supergiants, such as Arcturus, whereas orange dwarfs, like Alpha Centauri¬†B, are main-sequence stars.

They have extremely weak hydrogen lines, if those are present at all, and mostly neutral metals (Mn¬†I, Fe¬†I, Si¬†I). By late K, molecular bands of titanium oxide become present. Mainstream theories (those rooted in lower harmful radioactivity and star longevity) would thus suggest such stars have the optimal chances of heavily-evolved life developing on orbiting planets (if such life is directly analogous to earth's) due to a broad habitable zone yet much lower harmful periods of emission compared to those with the broadest such zones.

Spectral standards:

Class¬†M stars are by far the most common. About 76% of the main-sequence stars in the solar neighborhood are class¬†M stars. However, class¬†M main-sequence stars (red dwarfs) have such low luminosities that none are bright enough to be seen with the unaided eye, unless under exceptional conditions. The brightest known M-class main-sequence star is M0V Lacaille 8760, with magnitude 6.6 (the limiting magnitude for typical naked-eye visibility under good conditions is typically quoted as 6.5), and it is extremely unlikely that any brighter examples will be found.

Although most class¬†M stars are red dwarfs, most of the largest ever supergiant stars in the Milky Way are M stars, such as VV Cephei, Antares, and Betelgeuse, which are also class¬†M. Furthermore, the larger, hotter brown dwarfs are late class¬†M, usually in the range of M6.5 to M9.5.

The spectrum of a class¬†M star contains lines from oxide molecules (in the visible spectrum, especially TiO) and all neutral metals, but absorption lines of hydrogen are usually absent. TiO bands can be strong in class¬†M stars, usually dominating their visible spectrum by about M5. Vanadium(II) oxide bands become present by late M.

Spectral standards:

A number of new spectral types have been taken into use from newly discovered types of stars.

Spectra of some very hot and bluish stars exhibit marked emission lines from carbon or nitrogen, or sometimes oxygen.

Once included as type O stars, the Wolf-Rayet stars of class¬†W or WR are notable for spectra lacking hydrogen lines. Instead their spectra are dominated by broad emission lines of highly ionized helium, nitrogen, carbon, and sometimes oxygen. They are thought to mostly be dying supergiants with their hydrogen layers blown away by stellar winds, thereby directly exposing their hot helium shells. Class¬†W is further divided into subclasses according to the relative strength of nitrogen and carbon emission lines in their spectra (and outer layers).

WR spectra range is listed below:

Although the central stars of most planetary nebulae (CSPNe) show O¬†type spectra, around 10% are hydrogen-deficient and show WR spectra. These are low-mass stars and to distinguish them from the massive Wolf-Rayet stars, their spectra are enclosed in square brackets: e.g. [WC]. Most of these show [WC] spectra, some [WO], and very rarely [WN].

The "slash" stars are O-type stars with WN-like lines in their spectra. The name "slash" comes from their printed spectral type having a slash in it (e.g. "Of/WNL").

There is a secondary group found with this spectra, a cooler, "intermediate" group designated "Ofpe/WN9". These stars have also been referred to as WN10 or WN11, but that has become less popular with the realisation of the evolutionary difference from other Wolf‚ÄìRayet stars. Recent discoveries of even rarer stars have extended the range of slash stars as far as O2-3.5If/WN5-7, which are even hotter than the original "slash" stars.

They are O stars with strong magnetic fields. Designation is Of?p.

The new spectral types L, T, and Y were created to classify infrared spectra of cool stars. This includes both red dwarfs and brown dwarfs that are very faint in the visible spectrum.

Brown dwarfs, stars that do not undergo hydrogen fusion, cool as they age and so progress to later spectral types. Brown dwarfs start their lives with M-type spectra and will cool through the L, T, and Y spectral classes, faster the less massive they are; the highest-mass brown dwarfs cannot have cooled to Y or even T dwarfs within the age of the universe. Because this leads to an unresolvable overlap between spectral types effective temperature and luminosity for some masses and ages of different L-T-Y types, no distinct temperature or luminosity values can be given.

Class L dwarfs get their designation because they are cooler than M stars and L is the remaining letter alphabetically closest to M. Some of these objects have masses large enough to support hydrogen fusion and are therefore stars, but most are of substellar mass and are therefore brown dwarfs. They are a very dark red in color and brightest in infrared. Their atmosphere is cool enough to allow metal hydrides and alkali metals to be prominent in their spectra.

Due to low surface gravity in giant stars, TiO- and VO-bearing condensates never form. Thus, L-type stars larger than dwarfs can never form in an isolated environment. However, it may be possible for these L-type supergiants to form through stellar collisions, an example of which is V838 Monocerotis while in the height of its luminous red nova eruption.

Class¬†T dwarfs are cool brown dwarfs with surface temperatures between approximately . Their emission peaks in the infrared. Methane is prominent in their spectra.

Classes¬†T and L could be more common than all the other classes combined if recent research is accurate. Because brown dwarfs persist for so long‚Äîa few times the age of the universe‚Äîin the absence of catastrophic collisions these smaller bodies can only increase in number.

Study of the number of proplyds (protoplanetary disks, clumps of gas in nebulae from which stars and planetary systems are formed) indicates that the number of stars in the galaxy should be several orders of magnitude higher than what was previously conjectured. It is theorized that these proplyds are in a race with each other. The first one to form will become a protostar, which are very violent objects and will disrupt other proplyds in the vicinity, stripping them of their gas. The victim proplyds will then probably go on to become main-sequence stars or brown dwarfs of the L and T classes, which are quite invisible to us.

Brown dwarfs of spectral class¬†Y are cooler than those of spectral class¬†T and have qualitatively different spectra from them. A total of 17¬†objects have been placed in class¬†Y as of August 2013. Although such dwarfs have been modelled and detected within forty light-years by the Wide-field Infrared Survey Explorer (WISE) there is no well-defined spectral sequence yet and no prototypes. Nevertheless, several objects have been proposed as spectral classes Y0, Y1, and Y2.

The spectra of these prospective Y objects display absorption around 1.55¬†micrometers. Delorme et al. have suggested that this feature is due to absorption from ammonia, and that this should be taken as the indicative feature for the T-Y transition. In fact, this ammonia-absorption feature is the main criterion that has been adopted to define this class. However, this feature is difficult to distinguish from absorption by water and methane, and other authors have stated that the assignment of class Y0 is premature.

The latest brown dwarf proposed for the Y spectral type, WISE 1828+2650, is a >¬†Y2¬†dwarf with an effective temperature originally estimated around 300¬†K, the temperature of the human body. Parallax measurements have, however, since shown that its luminosity is inconsistent with it being colder than ~400¬†K. The coolest Y dwarf currently known is WISE 0855‚àí0714 with an approximate temperature of 250¬†K.

The mass range for Y¬†dwarfs is 9‚Äì25¬†Jupiter masses, but young objects might reach below one Jupiter mass, which means that Y class objects straddle the 13¬†Jupiter mass deuterium-fusion limit that marks the current IAU division between brown dwarfs and planets.

Young brown dwarfs have low surface gravities because they have larger radii and lower masses compared to the field stars of similar spectral type. These sources are marked by a letter beta (Œ≤) for intermediate surface gravity and gamma (Œ≥) for low surface gravity. Indication for low surface gravity are weak CaH, K I and Na I lines, as well as strong VO line. Alpha (Œ±) stands for normal surface gravity and is usually dropped. Sometimes an extremely low surface gravity is denoted by a delta (Œ¥). The suffix "pec" stands for peculiar. The peculiar suffix is still used for other features that are unusual and summarizes different properties, indicative of low surface gravity, subdwarfs and unresolved binaries. The prefix sd stands for subdwarf and only includes cool subdwarfs. This prefix indicates a low metallicity and kinematic properties that are more similar to halo stars than to disk stars. Subdwarfs appear bluer than disk objects. The red suffix describes objects with red color, but an older age. This is not interpreted as low surface gravity, but as a high dust content. The blue suffix describes objects with blue near-infrared colors that cannot be explained with low metallicity. Some are explained as L+T binaries, others are not binaries, such as 2MASS J11263991‚àí5003550 and are explained with thin and/or large-grained clouds.

Carbon-stars are stars whose spectra indicate production of carbon‚Äîa byproduct of triple-alpha helium fusion. With increased carbon abundance, and some parallel s-process heavy element production, the spectra of these stars become increasingly deviant from the usual late spectral classes¬†G, K, and M. Equivalent classes for carbon-rich stars are S and C.

The giants among those stars are presumed to produce this carbon themselves, but some stars in this class are double stars, whose odd atmosphere is suspected of having been transferred from a companion that is now a white dwarf, when the companion was a carbon-star.

Originally classified as R and N stars, these are also known as "carbon stars". These are red giants, near the end of their lives, in which there is an excess of carbon in the atmosphere. The old R and N classes ran parallel to the normal classification system from roughly mid G to late M. These have more recently been remapped into a unified carbon classifier C with N0 starting at roughly C6. Another subset of cool carbon stars are the C-J¬†type stars, which are characterized by the strong presence of molecules of CN in addition to those of CN. A few main-sequence carbon stars are known, but the overwhelming majority of known carbon stars are giants or supergiants. There are several subclasses:

Class¬†S stars form a continuum between class¬†M stars and carbon stars. Those most similar to class¬†M stars have strong ZrO absorption bands analogous to the TiO bands of class¬†M stars, whereas those most similar to carbon stars have strong sodium D lines and weak C bands. Class¬†S stars have excess amounts of zirconium and other elements produced by the s-process, and have more similar carbon and oxygen abundances than class¬†M or carbon stars. Like carbon stars, nearly all known class¬†S stars are asymptotic-giant-branch stars.

The spectral type is formed by the letter¬†S and a number between zero and ten. This number corresponds to the temperature of the star and approximately follows the temperature scale used for class¬†M giants. The most common types are S3 to S5. The non-standard designation S10 has only been used for the star Chi Cygni when at an extreme minimum.

The basic classification is usually followed by an abundance indication, following one of several schemes: S2,5; S2/5; S2 Zr4 Ti2; or S2*5. A number following a comma is a scale between 1 and 9 based on the ratio of ZrO and TiO. A number following a slash is a more recent but less common scheme designed to represent the ratio of carbon to oxygen on a scale of 1 to 10, where a 0 would be an MS star. Intensities of zirconium and titanium may be indicated explicitly. Also occasionally seen is a number following an asterisk, which represents the strength of the ZrO bands on a scale from 1 to 5.

In between the M and S classes, border cases are named MS stars. In a similar way, border cases between the S and C-N classes are named SC or CS. The sequence M ‚Üí MS ‚Üí S ‚Üí SC ‚Üí C-N is hypothesized to be a sequence of increased carbon abundance with age for carbon stars in the asymptotic giant branch.

The class¬†D (for Degenerate) is the modern classification used for white dwarfs ‚Äì low-mass stars that are no longer undergoing nuclear fusion and have shrunk to planetary size, slowly cooling down. Class¬†D is further divided into spectral types DA, DB, DC, DO, DQ, DX, and DZ. The letters are not related to the letters used in the classification of other stars, but instead indicate the composition of the white dwarf's visible outer layer or atmosphere.

The white dwarf types are as follows:

The type is followed by a number giving the white dwarf's surface temperature. This number is a rounded form of 50400/"T", where "T" is the effective surface temperature, measured in kelvins. Originally, this number was rounded to one of the digits 1 through 9, but more recently fractional values have started to be used, as well as values below 1 and above 9.

Two or more of the type letters may be used to indicate a white dwarf that displays more than one of the spectral features above.


A different set of spectral peculiarity symbols are used for white dwarfs than for other types of stars:

Finally, the classes P and Q, left over from the Draper system by Cannon, are occasionally used for certain non-stellar objects. Type P objects are stars within planetary nebulae and type Q objects are novae.

Stellar remnants are objects associated with the death of stars. Included in the category are white dwarfs, and as can be seen from the radically different classification scheme for class¬†D, non-stellar objects are difficult to fit into the MK system.

The Hertzsprung-Russell diagram, which the MK system is based on, is observational in nature so these remnants cannot easily be plotted on the diagram, or cannot be placed at all. Old neutron stars are relatively small and cold, and would fall on the far right side of the diagram. Planetary nebulae are dynamic and tend to quickly fade in brightness as the progenitor star transitions to the white dwarf branch. If shown, a planetary nebula would be plotted to the right of the diagram's upper right quadrant. A black hole emits no visible light of its own, and therefore would not appear on the diagram.

A classification system for neutron stars using Roman numerals has been proposed: type I for less massive neutron stars with low cooling rates, type II for more massive neutron stars with higher cooling rates, and a proposed type III for more massive neutron stars (possible exotic star candidates) with higher cooling rates. The more massive a neutron star is, the higher neutrino flux it carries. These neutrinos carry away so much heat energy that after only a few years the temperature of an isolated neutron star falls from the order of billions to only around a million Kelvin. This proposed neutron star classification system is not to be confused with the earlier Secchi spectral classes and the Yerkes luminosity classes.

Several spectral types, all previously used for non-standard stars in the mid-20th century, have been replaced during revisions of the stellar classification system. They may still be found in old editions of star catalogs: R and N have been subsumed into the new C class as C-R and C-N.

Humans may eventually be able to colonize any kind of stellar habitat, this section will address the probability of life arising around other stars.

Stability, luminosity, and lifespan are all factors in stellar habitability. We only know of one star that hosts life, and that is our own‚Äîa G class star with an abundance of heavy elements and low variability in brightness. It is also unlike many stellar systems in that it only has one star in it (see Planetary habitability, under the binary systems section).

Working from these constraints and the problems of having an empirical sample set of only one, the range of stars that are predicted to be able to support life as we know it is limited by a few factors. Of the main-sequence star types, stars more massive than 1.5 times that of the Sun (spectral types O, B, and A) age too quickly for advanced life to develop (using Earth as a guideline). On the other extreme, dwarfs of less than half the mass of our Sun (spectral type M) are likely to tidally lock planets within their habitable zone, along with other problems (see Habitability of red dwarf systems). While there are many problems facing life on red dwarfs, due to their sheer numbers and longevity, many astronomers continue to model these systems.

For these reasons NASA's Kepler Mission is searching for habitable planets at nearby main sequence stars that are less massive than spectral type A but more massive than type M -- making the most probable stars to host life dwarf stars of types F, G, and K.




</doc>
<doc id="28928" url="https://en.wikipedia.org/wiki?curid=28928" title="Sinope">
Sinope

Sinope may refer to:




</doc>
<doc id="28929" url="https://en.wikipedia.org/wiki?curid=28929" title="Seven Sisters">
Seven Sisters

Seven Sisters may refer to:















</doc>
<doc id="28930" url="https://en.wikipedia.org/wiki?curid=28930" title="SN 1987A">
SN 1987A

SN 1987A was a type II supernova in the Large Magellanic Cloud, a dwarf satellite galaxy of the Milky Way. It occurred approximately from Earth and was the closest observed supernova since Kepler's Supernova. 1987A's light reached Earth on February 23, 1987, and as the earliest supernova discovered that year, was labeled "1987A". Its brightness peaked in May, with an apparent magnitude of about 3.

It was the first supernova that modern astronomers were able to study in great detail, and its observations have provided much insight into core-collapse supernovae.

SN 1987A provided the first opportunity to confirm by direct observation the radioactive source of the energy for visible light emissions, by detecting predicted gamma-ray line radiation from two of its abundant radioactive nuclei. This proved the radioactive nature of the long-duration post-explosion glow of supernovae.

For over thirty years, the expected collapsed neutron star could not be found, but in 2019 it was announced found using the ALMA telescope.

SN 1987A was discovered independently by Ian Shelton and Oscar Duhalde at the Las Campanas Observatory in Chile on February 24, 1987, and within the same 24 hours by Albert Jones in New Zealand.

Later investigations found photographs showing the supernova brightening rapidly early on February 23rd. On March 4‚Äì12, 1987, it was observed from space by "Astron", the largest ultraviolet space telescope of that time.

[[File:New Hubble Observations of Supernova 1987A Trace Shock Wave (4954621859).jpg|thumb|left|The remnant of SN 1987A]]
Four days after the event was recorded, the progenitor star was tentatively identified as Sanduleak ‚àí69 202 (Sk -69 202), a [[blue supergiant]].
After the supernova faded, that identification was definitively confirmed by Sk ‚àí69 202 having disappeared. This was an unexpected identification, because models of [[Stellar evolution#Massive stars|high mass stellar evolution]] at the time did not predict that blue supergiants are susceptible to a supernova event.

Some models of the progenitor attributed the color to its chemical composition rather than its evolutionary state, particularly the low levels of heavy elements, among other factors. There was some speculation that the star might have merged with a [[Binary star|companion star]] before the supernova. However, it is now widely understood that blue supergiants are natural progenitors of some supernovae, although there is still speculation that the evolution of such stars could require mass loss involving a binary companion.

[[Image:Composite image of Supernova 1987A.jpg|thumb|left|Remnant of SN 1987A seen in light overlays of different spectra. [[Atacama Large Millimeter Array|ALMA]] data ([[Radio astronomy|radio]], in red) shows newly formed dust in the center of the remnant. [[Hubble Space Telescope|Hubble]] ([[Visible-light astronomy|visible]], in green) and [[Chandra X-ray Observatory|Chandra]] ([[X-ray astronomy|X-ray]], in blue) data show the expanding [[shock wave]].]]
Approximately two to three hours before the visible light from SN 1987A reached Earth, a burst of [[neutrino]]s was observed at three [[neutrino detector|neutrino observatories]]. This was likely due to neutrino emission, which occurs simultaneously with core collapse, but before visible light is emitted. Visible light is transmitted only after the shock wave reaches the stellar surface. At 07:35 [[Universal Time|UT]], [[Kamiokande II]] detected 12 [[antineutrino]]s; [[Irvine-Michigan-Brookhaven (detector)|IMB]], 8 antineutrinos; and [[Baksan Neutrino Observatory|Baksan]], 5 antineutrinos; in a burst lasting less than 13 seconds. Approximately three hours earlier, the [[Mont Blanc]] [[Neutrino detector#Scintillators|liquid scintillator]] detected a five-neutrino burst, but this is generally not believed to be associated with SN 1987A.

The Kamiokande II detection, which at 12 neutrinos had the largest sample population, showed the neutrinos arriving in two distinct pulses. The first pulse started at 07:35:35 and comprised 9 neutrinos, all of which arrived over a period of 1.915 seconds. A second pulse of three neutrinos arrived between 9.219 and 12.439 seconds after the first neutrino was detected, for a pulse duration of 3.220 seconds.

Although only 25 neutrinos were detected during the event, it was a significant increase from the previously observed background level. This was the first time neutrinos known to be emitted from a supernova had been observed directly, which marked the beginning of [[neutrino astronomy]]. The observations were consistent with theoretical supernova models in which 99% of the energy of the collapse is radiated away in the form of neutrinos. The observations are also consistent with the models' estimates of a total neutrino count of 10 with a total energy of 10 joules, i.e. a mean value of some dozens of MeV per neutrino.

The neutrino measurements allowed upper bounds on neutrino mass and charge, as well as the number of flavors of neutrinos and other properties. For example, the data show that within 5% confidence, the rest mass of the electron neutrino is at most 16 eV/c, 1/30,000 the mass of an electron. The data suggest that the total number of neutrino flavors is at most 8 but other observations and experiments give tighter estimates. Many of these results have since been confirmed or tightened by other neutrino experiments such as more careful analysis of solar neutrinos and atmospheric neutrinos as well as experiments with artificial neutrino sources.

[[File:New image of SN 1987A.jpg|thumb|The bright ring around the central region of the exploded star is composed of ejected material.]]

SN 1987A appears to be a core-collapse supernova, which should result in a [[neutron star]] given the size of the original star. The neutrino data indicate that a compact object did form at the star's core. Since the supernova first became visible, astronomers have been searching for the collapsed core. The [[Hubble Space Telescope]] has taken images of the supernova regularly since August 1990 without a clear detection of a neutron star.

A number of possibilities for the "missing" neutron star are being considered. The first is that the neutron star is enshrouded in dense dust clouds so that it cannot be seen. Another is that a [[pulsar]] was formed, but with either an unusually large or small magnetic field. It is also possible that large amounts of material fell back on the neutron star, so that it further collapsed into a [[black hole]]. Neutron stars and black holes often give off light as material falls onto them. If there is a compact object in the supernova remnant, but no material to fall onto it, it would be very dim and could therefore avoid detection. Other scenarios have also been considered, such as whether the collapsed core became a [[quark star]]. In 2019, evidence was presented that a neutron star was inside one of the brightest dust clumps close to the expected position of the supernova remnant.

Much of the [[light curve]], or graph of luminosity as a function of time, after the explosion of a [[type II supernova]] such as SN 1987A is produced by the energy from [[radioactive decay]]. Although the luminous emission consists of optical photons, it is the radioactive power absorbed that keeps the remnant hot enough to radiate light. Without the radioactive heat, it would quickly dim. The radioactive decay of [[isotopes of nickel|Ni]] through its daughters [[isotopes of cobalt|Co]] to [[isotopes of iron|Fe]] produces gamma-ray [[photon]]s that are absorbed and dominate the heating and thus the luminosity of the ejecta at intermediate times (several weeks) to late times (several months). Energy for the peak of the light curve of SN1987A was provided by the decay of Ni to Co (half life of 6 days) while energy for the later light curve in particular fit very closely with the 77.3-day half-life of Co decaying to Fe. Later measurements by space gamma-ray telescopes of the small fraction of the Co and Co gamma rays that escaped the SN1987A remnant without absorption confirmed earlier predictions that those two radioactive nuclei were the power source.

Because the Co in SN1987A has now completely decayed, it no longer supports the luminosity of the SN 1987A ejecta. That is currently powered by the radioactive decay of [[Isotopes of titanium|Ti]] with a half life of about 60 years. With this change, X-rays produced by the ring interactions of the ejecta began to contribute significantly to the total light curve. This was noticed by the Hubble Space Telescope as a steady increase in luminosity 10,000 days after the event in the blue and red spectral bands. X-ray lines Ti observed by the [[INTEGRAL]] space X-ray telescope showed that the total mass of radioactive Ti synthesized during the explosion was .

Observations of the radioactive power from their decays in the 1987A light curve have measured accurate total masses of the Ni, Ni, and Ti created in the explosion, which agree with the masses measured by gamma-ray line space telescopes and provides nucleosynthesis constraints on the computed supernova model.

[[Image:Sn87a.jpg|thumb|left|upright=1.3|The expanding ring-shaped [[Supernova remnant|remnant]] of SN 1987A and its interaction with its surroundings, seen in X-ray and visible light.]]
[[File:SN1987a debris evolution animation time scaled.gif|thumb|Sequence of [[Hubble Space Telescope|HST]] images from 1994 to 2009, showing the collision of the expanding [[supernova remnant|remnant]] with a ring of material ejected by the progenitor 20,000 years before the supernova]]

The three bright rings around SN 1987A that were visible after a few months in images by the Hubble Space Telescope are material from the [[stellar wind]] of the progenitor. These rings were ionized by the ultraviolet flash from the supernova explosion, and consequently began emitting in various emission lines. These rings did not "turn on" until several months after the supernova; the turn-on process can be very accurately studied through spectroscopy. The rings are large enough that their angular size can be measured accurately: the inner ring is 0.808 arcseconds in radius. The time light traveled to light up the inner ring gives its radius of 0.66 (ly) [[light years]]. Using this as the base of a right angle triangle and the angular size as seen from the Earth for the local angle, one can use basic trigonometry to calculate the distance to SN 1987A, which is about 168,000 light-years. The material from the explosion is catching up with the material expelled during both its red and blue supergiant phases and heating it, so we observe ring structures about the star.

Around 2001, the expanding (>7000¬†km/s) supernova ejecta collided with the inner ring. This caused its heating and the generation of x-rays‚Äîthe x-ray flux from the ring increased by a factor of three between 2001 and 2009. A part of the x-ray radiation, which is absorbed by the dense ejecta close to the center, is responsible for a comparable increase in the optical flux from the supernova remnant in 2001‚Äì2009. This increase of the brightness of the remnant reversed the trend observed before 2001, when the optical flux was decreasing due to the decaying of [[Isotopes of titanium|Ti]] isotope.

A study reported in June 2015, using images from the Hubble Space Telescope and the [[Very Large Telescope]] taken between 1994 and 2014, shows that the emissions from the clumps of matter making up the rings are fading as the clumps are destroyed by the shock wave. It is predicted the ring will fade away between 2020 and 2030. These findings are also supported by the results of a three-dimensional hydrodynamic model which describes the interaction of the blast wave with the circumstellar nebula. 
The model also shows that X-ray emission from ejecta heated up by the shock will be dominant very soon, after the ring will fade away. As the shock wave passes the circumstellar ring it will trace the history of mass loss of the supernova's progenitor and provide useful information for discriminating among various models for the progenitor of SN 1987A.

In 2018, radio observations from the interaction between the circumstellar ring of dust and the shockwave has confirmed the shockwave has now left the circumstellar material. It also shows that the speed of the shockwave, which slowed down to 2,300¬†km/s while interacting with the dust in the ring, has now re-accelerated to 3,600¬†km/s.

[[File:Images of the Warm Dust in the SN 1987A debris.png|thumb|left|Images of the SN 1987A debris obtained with the instruments T-ReCS at the 8-m Gemini telescope and VISIR at one of the four VLT. Dates are indicated. An HST image is inserted at the bottom right (credits Patrice Bouchet, CEA-Saclay)]]

Soon after the SN 1987A outburst, three major groups embarked in a photometric monitoring of the supernova: [[SAAO]], [[CTIO]], and [[ESO]]. In particular, the ESO team reported an [[infrared excess]] which became apparent beginning less than one month after the explosion (March 11, 1987). Three possible interpretations for it were discussed in this work: the infrared echo hypothesis was discarded, and [[thermal emission]] from dust that could have condensed in the ejecta was favoured (in which case the estimated temperature at that epoch was ~ 1250 K, and the dust mass was approximately ). The possibility that the IR excess could be produced by optically thick [[free-free emission]] seemed unlikely because the luminosity in UV photons needed to keep the envelope ionized was much larger than what was available, but it was not ruled out in view of the eventuality of electron scattering, which had not been considered.

However, none of these three groups had sufficiently convincing proofs to claim for a dusty ejecta on the basis of an IR excess alone. 
[[File:Model of the dust distribution.png|thumb|right|Distribution of the dust inside the SN 1987A ejecta, as from the Lucy et al.'s model built at ESO
An independent Australian team advanced several argument in favour of an echo interpretation. This seemingly straightforward interpretation of the nature of the IR emission was challenged by the ESO group and definitively ruled out after presenting optical evidence for the presence of dust in the SN ejecta.
To discriminate between the two interpretations, they considered the implication of the presence of an echoing dust cloud on the optical light curve, and on the existence of diffuse optical emission around the SN. They concluded that the expected optical echo from the cloud should be resolvable, and could be very bright with an integrated visual brightness of [[apparent magnitude|magnitude]] 10.3 around day 650. However, further optical observations, as expressed in SN light curve, showed no [[inflection point|inflection]] in the light curve at the predicted level. Finally, the ESO team presented a convincing clumpy model for dust condensation in the ejecta.

Although it had been thought more than 50 years ago that dust could form in the ejecta of a core-collapse supernova, which in particular could explain the origin of the dust seen in young galaxies, that was the first time that such a condensation was observed. If SN 1987A is a typical representative of its class then the derived mass of the warm dust formed in the debris of core collapse supernovae is not sufficient to account for all the dust observed in the early universe. However, a much larger reservoir of ~0.25 solar mass of colder dust (at ~26 K) in the ejecta of SN 1987A was found with the Hershel infrared space telescope in 2011 and confirmed by ALMA later on (in 2014).

Following the confirmation of a large amount of cold dust in the ejecta, ALMA has continued observing SN 1987A. Synchrotron radiation due to shock interaction in the equatorial ring has been measured. Cold (20‚Äì100K) carbon monoxide (CO) and silicate molecules (SiO) were observed. The data show that CO and SiO distributions are clumpy, and that different nucleosynthesis products (C, O and Si) are located in different places of the ejecta, indicating the footprints of the stellar interior at the time of the explosion.



[[Category:Supernova remnants]]
[[Category:Supernovae]]
[[Category:Large Magellanic Cloud]]
[[Category:Astronomical objects discovered in 1987]]

</doc>
<doc id="28931" url="https://en.wikipedia.org/wiki?curid=28931" title="Standard Oil">
Standard Oil

Standard Oil Co. was an American oil-producing, transporting, refining, marketing company. Established in 1870, by John D. Rockefeller and Henry Flagler as a corporation in Ohio, it was the largest oil refiner in the world of its time. Its history as one of the world's first and largest multinational corporations ended in 1911, when the U.S. Supreme Court ruled, in a landmark case, that Standard Oil was an illegal monopoly.

Standard Oil dominated the oil products market initially through horizontal integration in the refining sector, then, in later years vertical integration; the company was an innovator in the development of the business trust. The Standard Oil trust streamlined production and logistics, lowered costs, and undercut competitors. "Trust-busting" critics accused Standard Oil of using aggressive pricing to destroy competitors and form a monopoly that threatened other businesses.

Rockefeller ran the company as its chairman, until his retirement in 1897. He remained the major shareholder, and in 1911, with the dissolution of the Standard Oil trust into 34 smaller companies, Rockefeller became the richest person in modern history, as the initial income of these individual enterprises proved to be much bigger than that of a single larger company. Its successors such as ExxonMobil, Marathon Petroleum, Amoco, and Chevron are still among the companies with the largest revenues in the world. By 1882, his top aide was John Dustin Archbold. After 1896, Rockefeller disengaged from business to concentrate on his philanthropy, leaving Archbold in control. Other notable Standard Oil principals include Henry Flagler, developer of the Florida East Coast Railway and resort cities, and Henry H. Rogers, who built the Virginian Railway.

Standard Oil's pre-history began in 1863, as an Ohio partnership formed by industrialist John D. Rockefeller, his brother William Rockefeller, Henry Flagler, chemist Samuel Andrews, silent partner Stephen V. Harkness, and Oliver Burr Jennings, who had married the sister of William Rockefeller's wife. In 1870, Rockefeller abolished the partnership and incorporated Standard Oil in Ohio. Of the initial 10,000 shares, John D. Rockefeller received 2,667; Harkness received 1,334; William Rockefeller, Flagler, and Andrews received 1,333 each; Jennings received 1,000, and the firm of Rockefeller, Andrews & Flagler received 1,000. Rockefeller chose the "Standard Oil" name as a symbol of the reliable "standards" of quality and service that he envisioned for the nascent oil industry.

In the early years, John D. Rockefeller dominated the combine; he was the single most important figure in shaping the new oil industry. He quickly distributed power and the tasks of policy formation to a system of committees, but always remained the largest shareholder. Authority was centralized in the company's main office in Cleveland, but decisions in the office were made in a cooperative way.

The company grew by increasing sales and through acquisitions. After purchasing competing firms, Rockefeller shut down those he believed to be inefficient and kept the others. In a seminal deal, in 1868, the Lake Shore Railroad, a part of the New York Central, gave Rockefeller's firm a going rate of one cent a gallon or forty-two cents a barrel, an effective 71% discount from its listed rates in return for a promise to ship at least 60 carloads of oil daily and to handle load and unload on its own. Smaller companies decried such deals as unfair because they were not producing enough oil to qualify for discounts.

Standard's actions and secret transport deals helped its kerosene price to drop from 58 to 26 cents from 1865 to 1870. Rockefeller used the Erie Canal as a cheap alternative form of transportation‚Äîin the summer months when it was not frozen‚Äîto ship his refined oil from Cleveland to New York City. In the winter months his only options were the three trunk lines‚Äîthe Erie Railroad and the New York Central Railroad to New York City, and the Pennsylvania Railroad to Philadelphia. Competitors disliked the company's business practices, but consumers liked the lower prices. Standard Oil, being formed well before the discovery of the Spindletop oil field (in Texas, far from Standard Oil's base in the Midwest) and a demand for oil other than for heat and light, was well placed to control the growth of the oil business. The company was perceived to own and control all aspects of the trade.

In 1872, Rockefeller joined the South Improvement Co. which would have allowed him to receive rebates for shipping and drawbacks on oil his competitors shipped. But when this deal became known, competitors convinced the Pennsylvania Legislature to revoke South Improvement's charter. No oil was ever shipped under this arrangement. Using highly effective tactics, later widely criticized, it absorbed or destroyed most of its competition in Cleveland in less than two months and later throughout the northeastern United States.

A. Barton Hepburn was directed by the New York State Legislature in 1879, to investigate the railroads' practice of giving rebates within the state. Merchants without ties to the oil industry had pressed for the hearings. Prior to the committee's investigation, few knew of the size of Standard Oil's control and influence on seemingly unaffiliated oil refineries and pipelines‚ÄîHawke (1980) cites that only a dozen or so within Standard Oil knew the extent of company operations. The committee counsel, Simon Sterne, questioned representatives from the Erie Railroad and the New York Central Railroad and discovered that at least half of their long-haul traffic granted rebates, and that much of this traffic came from Standard Oil. The committee then shifted focus to Standard Oil's operations. John Dustin Archbold, as president of Acme Oil Company, denied that Acme was associated with Standard Oil. He then admitted to being a director of Standard Oil. The committee's final report scolded the railroads for their rebate policies and cited Standard Oil as an example. This scolding was largely moot to Standard Oil's interests since long-distance oil pipelines were now their preferred method of transportation.

In response to state laws trying to limit the scale of companies, Rockefeller and his associates developed innovative ways of organizing, to effectively manage their fast growing enterprise. On January 2, 1882, they combined their disparate companies, spread across dozens of states, under a single group of trustees. By a secret agreement, the existing 37 stockholders conveyed their shares "in trust" to nine trustees: John and William Rockefeller, Oliver H. Payne, Charles Pratt, Henry Flagler, John D. Archbold, William G. Warden, Jabez Bostwick, and Benjamin Brewster. This organization proved so successful that other giant enterprises adopted this "trust" form.

In 1885, Standard Oil of Ohio moved its headquarters from Cleveland to its permanent headquarters at 26 Broadway in New York City. Concurrently, the trustees of Standard Oil of Ohio chartered the Standard Oil Co. of New Jersey (SOCNJ) to take advantages of New Jersey's more lenient corporate stock ownership laws.

In 1890, Congress overwhelmingly passed the Sherman Antitrust Act (Senate 51-1; House 242-0), a source of American anti-monopoly laws. The law forbade every contract, scheme, deal, or conspiracy to restrain trade, though the phrase "restraint of trade" remained subjective. The Standard Oil group quickly attracted attention from antitrust authorities leading to a lawsuit filed by Ohio Attorney General David K. Watson.

From 1882 to 1906, Standard paid out $548,436,000 in dividends at 65.4% payout ratio. The total net earnings from 1882 to 1906 amounted to $838,783,800, exceeding the dividends by $290,347,800, which was used for plant expansions.

In 1896, John Rockefeller retired from the Standard Oil Co. of New Jersey, the holding company of the group, but remained president and a major shareholder. Vice-president John Dustin Archbold took a large part in the running of the firm. In the year 1904, Standard Oil controlled 91% of oil refinement and 85% of final sales in the United States. At this point in time, state and federal laws sought to counter this development with antitrust laws. In 1911, the U.S. Justice Department sued the group under the federal antitrust law and ordered its breakup into 34 companies.

Standard Oil's market position was initially established through an emphasis on efficiency and responsibility. While most companies dumped gasoline in rivers (this was before the automobile was popular), Standard used it to fuel its machines. While other companies' refineries piled mountains of heavy waste, Rockefeller found ways to sell it. For example, Standard created the first synthetic competitor for beeswax and bought the company that invented and produced Vaseline, the Chesebrough Manufacturing Co., which was a Standard company only from 1908 until 1911.

One of the original "Muckrakers" was Ida M. Tarbell, an American author and journalist. Her father was an oil producer whose business had failed because of Rockefeller's business dealings. After extensive interviews with a sympathetic senior executive of Standard Oil, Henry H. Rogers, Tarbell's investigations of Standard Oil fueled growing public attacks on Standard Oil and on monopolies in general. Her work was published in 19 parts in "McClure's" magazine from November 1902 to October 1904, then in 1904 as the book "The History of the Standard Oil Co".

The Standard Oil Trust was controlled by a small group of families. Rockefeller stated in 1910: "I think it is true that the Pratt family, the Payne‚ÄìWhitney family (which were one, as all the stock came from Colonel Payne), the Harkness-Flagler family (which came into the company together) and the Rockefeller family controlled a majority of the stock during all the history of the company up to the present time."

These families reinvested most of the dividends in other industries, especially railroads. They also invested heavily in the gas and the electric lighting business (including the giant Consolidated Gas Co. of New York City). They made large purchases of stock in U.S. Steel, Amalgamated Copper, and even Corn Products Refining Co.

Weetman Pearson, a British petroleum entrepreneur in Mexico, began negotiating with Standard Oil in 1912‚Äì13 to sell his "El Aguila" oil company, since Pearson was no longer bound to promises to the Porfirio D√≠az regime (1876‚Äì1911) to not to sell to U.S. interests. However, the deal fell through and the firm was sold to Royal Dutch Shell.

Standard Oil's production increased so rapidly it soon exceeded U.S. demand and the company began viewing export markets. In the 1890s, Standard Oil began marketing kerosene to China's large population of close to 400 million as lamp fuel. For its Chinese trademark and brand Standard Oil adopted the name "Mei Foo" (), (which translates to Mobil). Mei Foo also became the name of the tin lamp that Standard Oil produced and gave away or sold cheaply to Chinese farmers, encouraging them to switch from vegetable oil to kerosene. Response was positive, sales boomed and China became Standard Oil's largest market in Asia. Prior to Pearl Harbor, Stanvac was the largest single U.S. investment in Southeast Asia.

The North China Department of Socony (Standard Oil Company of New York) operated a subsidiary called Socony River and Coastal Fleet, North Coast Division, which became the North China Division of Stanvac (Standard Vacuum Oil Company) after that company was formed in 1933. To distribute its products, Standard Oil constructed storage tanks, canneries (bulk oil from large ocean tankers was re-packaged into tins), warehouses and offices in key Chinese cities. For inland distribution the company had motor tank trucks and railway tank cars, and for river navigation it had a fleet of low-draft steamers and other vessels.

Stanvac's North China Division, based in Shanghai, owned hundreds of river going vessels, including motor barges, steamers, launches, tugboats and tankers. Up to 13 tankers operated on the Yangtze River, the largest of which were "Mei Ping" (), "Mei Hsia" (), and "Mei An" (). All three were destroyed in the 1937 USS "Panay" incident. "Mei An" was launched in 1901 and was the first vessel in the fleet. Other vessels included "Mei Chuen", "Mei Foo", "Mei Hung", "Mei Kiang", "Mei Lu", "Mei Tan", "Mei Su", "Mei Xia", "Mei Ying", and "Mei Yun". "Mei Hsia", a tanker, was specially designed for river duty and was built by New Engineering and Shipbuilding Works of Shanghai, who also built the 500-ton launch "Mei Foo" in 1912. "Mei Hsia" ("Beautiful Gorges") was launched in 1926 and carried 350 tons of bulk oil in three holds, plus a forward cargo hold, and space between decks for carrying general cargo or packed oil. She had a length of , a beam of , depth of , and had a bullet-proof wheelhouse. "Mei Ping" ("Beautiful Tranquility"), launched in 1927, was designed offshore, but assembled and finished in Shanghai. Its oil-fuel burners came from the U.S. and water-tube boilers came from England.

Standard Oil Company and Socony-Vacuum Oil Company became partners in providing markets for the oil reserves in the Middle East. In 1906, SOCONY (later Mobil) opened its first fuel terminals in Alexandria. It explored in Palestine before the World War broke out, but ran into conflict with the local authorities.

By 1890, Standard Oil controlled 88 percent of the refined oil flows in the United States. The state of Ohio successfully sued Standard, compelling the dissolution of the trust in 1892. But Standard simply separated Standard Oil of Ohio and kept control of it. Eventually, the state of New Jersey changed its incorporation laws to allow a company to hold shares in other companies in any state. So, in 1899, the Standard Oil Trust, based at 26 Broadway in New York, was legally reborn as a holding company, the "Standard Oil Co. of New Jersey" (SOCNJ), which held stock in 41 other companies, which controlled other companies, which in turn controlled yet other companies. According to Daniel Yergin in his Pulitzer Prize-winning "" (1990), this conglomerate was seen by the public as all-pervasive, controlled by a select group of directors, and completely unaccountable.

In 1904, Standard controlled 91 percent of production and 85 percent of final sales. Most of its output was kerosene, of which 55 percent was exported around the world. After 1900 it did not try to force competitors out of business by underpricing them. The federal Commissioner of Corporations studied Standard's operations from the period of 1904 to 1906 and concluded that "beyond question ... the dominant position of the Standard Oil Co. in the refining industry was due to unfair practices‚Äîto abuse of the control of pipe-lines, to railroad discriminations, and to unfair methods of competition in the sale of the refined petroleum products". Because of competition from other firms, their market share had gradually eroded to 70 percent by 1906 which was the year when the antitrust case was filed against Standard, and down to 64 percent by 1911 when Standard was ordered broken up and at least 147 refining companies were competing with Standard including Gulf, Texaco, and Shell. It did not try to monopolize the exploration and pumping of oil (its share in 1911 was 11 percent).

In 1909, the U.S. Justice Department sued Standard under federal antitrust law, the Sherman Antitrust Act of 1890, for sustaining a monopoly and restraining interstate commerce by:

Rebates, preferences, and other discriminatory practices in favor of the combination by railroad companies; restraint and monopolization by control of pipe lines, and unfair practices against competing pipe lines; contracts with competitors in restraint of trade; unfair methods of competition, such as local price cutting at the points where necessary to suppress competition; [and] espionage of the business of competitors, the operation of bogus independent companies, and payment of rebates on oil, with the like intent.

The lawsuit argued that Standard's monopolistic practices had taken place over the preceding four years:

The general result of the investigation has been to disclose the existence of numerous and flagrant discriminations by the railroads in behalf of the Standard Oil Co. and its affiliated corporations. With comparatively few exceptions, mainly of other large concerns in California, the Standard has been the sole beneficiary of such discriminations. In almost every section of the country that company has been found to enjoy some unfair advantages over its competitors, and some of these discriminations affect enormous areas.

The government identified four illegal patterns: (1) secret and semi-secret railroad rates; (2) discriminations in the open arrangement of rates; (3) discriminations in classification and rules of shipment; (4) discriminations in the treatment of private tank cars. The government alleged:

Almost everywhere the rates from the shipping points used exclusively, or almost exclusively, by the Standard are relatively lower than the rates from the shipping points of its competitors. Rates have been made low to let the Standard into markets, or they have been made high to keep its competitors out of markets. Trifling differences in distances are made an excuse for large differences in rates favorable to the Standard Oil Co., while large differences in distances are ignored where they are against the Standard. Sometimes connecting roads prorate on oil‚Äîthat is, make through rates which are lower than the combination of local rates; sometimes they refuse to prorate; but in either case the result of their policy is to favor the Standard Oil Co. Different methods are used in different places and under different conditions, but the net result is that from Maine to California the general arrangement of open rates on petroleum oil is such as to give the Standard an unreasonable advantage over its competitors.

The government said that Standard raised prices to its monopolistic customers but lowered them to hurt competitors, often disguising its illegal actions by using bogus supposedly independent companies it controlled.

The evidence is, in fact, absolutely conclusive that the Standard Oil Co. charges altogether excessive prices where it meets no competition, and particularly where there is little likelihood of competitors entering the field, and that, on the other hand, where competition is active, it frequently cuts prices to a point which leaves even the Standard little or no profit, and which more often leaves no profit to the competitor, whose costs are ordinarily somewhat higher.

On May 15, 1911, the US Supreme Court upheld the lower court judgment and declared the Standard Oil group to be an "unreasonable" monopoly under the Sherman Antitrust Act, Section II. It ordered Standard to break up into 34 independent companies with different boards of directors, the biggest two of the companies were Standard Oil of New Jersey (which became Exxon) and Standard Oil of New York (which became Mobil).

Standard's president, John D. Rockefeller, had long since retired from any management role. But, as he owned a quarter of the shares of the resultant companies, and those share values mostly doubled, he emerged from the dissolution as the richest man in the world. The dissolution had actually propelled Rockefeller's personal wealth.

By 1911, with public outcry at a climax, the Supreme Court of the United States ruled, in "Standard Oil Co. of New Jersey v. United States", that Standard Oil of New Jersey must be dissolved under the Sherman Antitrust Act and split into 34 companies. Two of these companies were Standard Oil of New Jersey (Jersey Standard or Esso), which eventually became Exxon, and Standard Oil of New York (Socony), which eventually became Mobil; those two companies later merged into ExxonMobil.

Over the next few decades, both companies grew significantly. Jersey Standard, led by Walter C. Teagle, became the largest oil producer in the world. It acquired a 50 percent share in Humble Oil & Refining Co., a Texas oil producer. Socony purchased a 45 percent interest in Magnolia Petroleum Co., a major refiner, marketer and pipeline transporter. In 1931, Socony merged with Vacuum Oil Co., an industry pioneer dating back to 1866, and a growing Standard Oil spin-off in its own right.

In the Asia-Pacific region, Jersey Standard had oil production and refineries in Indonesia but no marketing network. Socony-Vacuum had Asian marketing outlets supplied remotely from California. In 1933, Jersey Standard and Socony-Vacuum merged their interests in the region into a 50‚Äì50 joint venture. Standard-Vacuum Oil Co., or "Stanvac", operated in 50 countries, from East Africa to New Zealand, before it was dissolved in 1962.

The original Standard Oil Company corporate entity continues in existence and was the operating entity for Sohio; it is now a subsidiary of BP. BP continued to sell gasoline under the Sohio brand until 1991. Other Standard oil entities include "Standard Oil of Indiana" which became Amoco after other mergers and a name change in the 1980s, and "Standard Oil of California" which became the Chevron Corp.

The U.S. Supreme Court ruled in 1911 that antitrust law required Standard Oil to be broken into smaller, independent companies. Among the "baby Standards" that still exist are ExxonMobil and Chevron. Some have speculated that if not for that court ruling, Standard Oil could have possibly been worth more than $1¬†trillion in the 2000s.
Whether the breakup of Standard Oil was beneficial is a matter of some controversy.
Some economists believe that Standard Oil was not a monopoly, and also argue that the intense free market competition resulted in cheaper oil prices and more diverse petroleum products. Critics claimed that success in meeting consumer needs was driving other companies out of the market who were not as successful. An example of this thinking was given in 1890, when Rep. William Mason, arguing in favor of the Sherman Antitrust Act, said: "trusts have made products cheaper, have reduced prices; but if the price of oil, for instance, were reduced to one cent a barrel, it would not right the wrong done to people of this country by the "trusts" which have destroyed legitimate competition and driven honest men from legitimate business enterprise".

The Sherman Antitrust Act prohibits the restraint of trade. Defenders of Standard Oil insist that the company did not restrain trade; they were simply superior competitors. The federal courts ruled otherwise.

Some economic historians have observed that Standard Oil was in the process of losing its monopoly at the time of its breakup in 1911. Although Standard had 90 percent of American refining capacity in 1880, by 1911, that had shrunk to between 60 and 65 percent because of the expansion in capacity by competitors. Numerous regional competitors (such as Pure Oil in the East, Texaco and Gulf Oil in the Gulf Coast, Cities Service and Sun in the Midcontinent, Union in California, and Shell overseas) had organized themselves into competitive vertically integrated oil companies, the industry structure pioneered years earlier by Standard itself. In addition, demand for petroleum products was increasing more rapidly than the ability of Standard to expand. The result was that although in 1911 Standard still controlled most production in the older regions of the Appalachian Basin (78 percent share, down from 92 percent in 1880), Lima-Indiana (90 percent, down from 95 percent in 1906), and the Illinois Basin (83 percent, down from 100 percent in 1906), its share was much lower in the rapidly expanding new regions that would dominate U.S. oil production in the 20th century. In 1911, Standard controlled only 44 percent of production in the Midcontinent, 29 percent in California, and 10 percent on the Gulf Coast.

Some analysts argue that the breakup was beneficial to consumers in the long run, and no one has ever proposed that Standard Oil be reassembled in pre-1911 form. ExxonMobil, however, does represent a substantial part of the original company.

Since the breakup of Standard Oil, several companies, such as General Motors and Microsoft, have come under antitrust investigation for being inherently too large for market competition; however, most of them remained together. The only company since the breakup of Standard Oil that was divided into parts like Standard Oil was AT&T, which after decades as a regulated natural monopoly, was forced to divest itself of the Bell System in 1984.

The successor companies from Standard Oil's breakup form the core of today's US oil industry. (Several of these companies were considered among the Seven Sisters who dominated the industry worldwide for much of the 20th century.) They include:

Other Standard Oil spin-offs:

Other companies divested in the 1911 breakup:


Note: Standard Oil of Colorado was not a successor company; the name was used to capitalize on the Standard Oil brand in the 1930s. Standard Oil of Connecticut is a fuel oil marketer not related to the Rockefeller companies.

Of the 34 "Baby Standards", 11 were given rights to the Standard Oil name, based on the state they were in. Conoco and Atlantic elected to use their respective names instead of the Standard name, and their rights would be claimed by other companies.

By the 1980s, most companies were using their individual brand names instead of the Standard name, with Amoco being the last one to have widespread use of the "Standard" name, as it gave Midwestern owners the option of using the Amoco name or Standard.

Three supermajor companies now own the rights to the Standard name in the United States: ExxonMobil, Chevron Corp., and BP. BP acquired its rights through acquiring Standard Oil of Ohio and Amoco, and has a small handful of stations in the Midwestern United States using the Standard name. Likewise, BP continues to sell marine fuel under the Sohio brand at various marinas throughout Ohio. ExxonMobil keeps the Esso trademark alive at stations that sell diesel fuel by selling "Esso Diesel" displayed on the pumps. ExxonMobil has full international rights to the Standard name, and continues to use the Esso name overseas and in Canada. To protect its trademark Chevron has one station in each state it owns the rights to branded as Standard. Some of its Standard-branded stations have a mix of some signs that say Standard and some signs that say Chevron. Over time, Chevron has changed which station in a given state is the Standard station.





</doc>
<doc id="28935" url="https://en.wikipedia.org/wiki?curid=28935" title="Seismology">
Seismology

Seismology (; from Ancient Greek œÉŒµŒπœÉŒºœåœÇ ("seism√≥s") meaning "earthquake" and -ŒªŒøŒ≥ŒØŒ± ("-log√≠a") meaning "study of") is the scientific study of earthquakes and the propagation of elastic waves through the Earth or through other planet-like bodies. The field also includes studies of earthquake environmental effects such as tsunamis as well as diverse seismic sources such as volcanic, tectonic, glacial, fluvial, oceanic, atmospheric, and artificial processes such as explosions. A related field that uses geology to infer information regarding past earthquakes is paleoseismology. A recording of Earth motion as a function of time is called a seismogram. A seismologist is a scientist who does research in seismology.

Scholarly interest in earthquakes can be traced back to antiquity. Early speculations on the natural causes of earthquakes were included in the writings of Thales of Miletus (c. 585 BCE), Anaximenes of Miletus (c. 550 BCE), Aristotle (c. 340 BCE), and Zhang Heng (132 CE).

In 132 CE, Zhang Heng of China's Han dynasty designed the first known seismoscope.

In the 17th century, Athanasius Kircher argued that earthquakes were caused by the movement of fire within a system of channels inside the Earth. Martin Lister (1638 to 1712) and Nicolas Lemery (1645 to 1715) proposed that earthquakes were caused by chemical explosions within the earth.

The Lisbon earthquake of 1755, coinciding with the general flowering of science in Europe, set in motion intensified scientific attempts to understand the behaviour and causation of earthquakes. The earliest responses include work by John Bevis (1757) and John Michell (1761). Michell determined that earthquakes originate within the Earth and were waves of movement caused by "shifting masses of rock miles below the surface".

From 1857, Robert Mallet laid the foundation of instrumental seismology and carried out seismological experiments using explosives. He is also responsible for coining the word "seismology".

In 1897, Emil Wiechert's theoretical calculations led him to conclude that the Earth's interior consists of a mantle of silicates, surrounding a core of iron.

In 1906 Richard Dixon Oldham identified the separate arrival of P-waves, S-waves and surface waves on seismograms and found the first clear evidence that the Earth has a central core.

In 1910, after studying the April 1906 San Francisco earthquake, Harry Fielding Reid put forward the "elastic rebound theory" which remains the foundation for modern tectonic studies. The development of this theory depended on the considerable progress of earlier independent streams of work on the behaviour of elastic materials and in mathematics.

In 1926, Harold Jeffreys was the first to claim, based on his study of earthquake waves, that below the mantle, the core of the Earth is liquid.

In 1937, Inge Lehmann determined that within Earth's liquid outer core there is a solid inner core.

By the 1960s, Earth science had developed to the point where a comprehensive theory of the causation of seismic events and geodetic motions had come together in the now well-established theory of plate tectonics.

Seismic waves are elastic waves that propagate in solid or fluid materials. They can be divided into "body waves" that travel through the interior of the materials; "surface waves" that travel along surfaces or interfaces between materials; and "normal modes", a form of standing wave.

There are two types of body waves, pressure waves or primary waves (P-waves) and shear or secondary waves (S-waves). P-waves are longitudinal waves that involve compression and expansion in the direction that the wave is moving and are always the first waves to appear on a seismogram as they are the fastest moving waves through solids. S-waves are transverse waves that move perpendicular to the direction of propagation. S-waves are slower than P-waves. Therefore, they appear later than P-waves on a seismogram. Fluids cannot support transverse elastic waves because of their low shear strength, so S-waves only travel in solids.

Surface waves are the result of P- and S-waves interacting with the surface of the Earth. These waves are dispersive, meaning that different frequencies have different velocities. The two main surface wave types are Rayleigh waves, which have both compressional and shear motions, and Love waves, which are purely shear. Rayleigh waves result from the interaction of P-waves and vertically polarized S-waves with the surface and can exist in any solid medium. Love waves are formed by horizontally polarized S-waves interacting with the surface, and can only exist if there is a change in the elastic properties with depth in a solid medium, which is always the case in seismological applications. Surface waves travel more slowly than P-waves and S-waves because they are the result of these waves traveling along indirect paths to interact with Earth's surface. Because they travel along the surface of the Earth, their energy decays less rapidly than body waves (1/distance vs. 1/distance), and thus the shaking caused by surface waves is generally stronger than that of body waves, and the primary surface waves are often thus the largest signals on earthquake seismograms. Surface waves are strongly excited when their source is close to the surface, as in a shallow earthquake or a near-surface explosion, and are much weaker for deep earthquake sources.

Both body and surface waves are traveling waves; however, large earthquakes can also make the entire Earth "ring" like a resonant bell. This ringing is a mixture of normal modes with discrete frequencies and periods of approximately an hour or shorter. Normal mode motion caused by a very large earthquake can be observed for up to a month after the event. The first observations of normal modes were made in the 1960s as the advent of higher fidelity instruments coincided with two of the largest earthquakes of the 20th century ‚Äì the 1960 Valdivia earthquake and the 1964 Alaska earthquake. Since then, the normal modes of the Earth have given us some of the strongest constraints on the deep structure of the Earth.

One of the first attempts at the scientific study of earthquakes followed the 1755 Lisbon earthquake. Other notable earthquakes that spurred major advancements in the science of seismology include the 1857 Basilicata earthquake, the 1906 San Francisco earthquake, the 1964 Alaska earthquake, the 2004 Sumatra-Andaman earthquake, and the 2011 Great East Japan earthquake.

Seismic waves produced by explosions or vibrating controlled sources are one of the primary methods of underground exploration in geophysics (in addition to many different electromagnetic methods such as induced polarization and magnetotellurics). Controlled-source seismology has been used to map salt domes, anticlines and other geologic traps in petroleum-bearing rocks, faults, rock types, and long-buried giant meteor craters. For example, the Chicxulub Crater, which was caused by an impact that has been implicated in the extinction of the dinosaurs, was localized to Central America by analyzing ejecta in the Cretaceous‚ÄìPaleogene boundary, and then physically proven to exist using seismic maps from oil exploration.

Seismometers are sensors that detect and record the motion of the Earth arising from elastic waves. Seismometers may be deployed at the Earth's surface, in shallow vaults, in boreholes, or underwater. A complete instrument package that records seismic signals is called a seismograph. Networks of seismographs continuously record ground motions around the world to facilitate the monitoring and analysis of global earthquakes and other sources of seismic activity. Rapid location of earthquakes makes tsunami warnings possible because seismic waves travel considerably faster than tsunami waves. Seismometers also record signals from non-earthquake sources ranging from explosions (nuclear and chemical), to local noise from wind or anthropogenic activities, to incessant signals generated at the ocean floor and coasts induced by ocean waves (the global microseism), to cryospheric events associated with large icebergs and glaciers. Above-ocean meteor strikes with energies as high as 4.2 √ó 10 J (equivalent to that released by an explosion of ten kilotons of TNT) have been recorded by seismographs, as have a number of industrial accidents and terrorist bombs and events (a field of study referred to as forensic seismology). A major long-term motivation for the global seismographic monitoring has been for the detection and study of nuclear testing.

Because seismic waves commonly propagate efficiently as they interact with the internal structure of the Earth, they provide high-resolution noninvasive methods for studying the planet's interior. One of the earliest important discoveries (suggested by Richard Dixon Oldham in 1906 and definitively shown by Harold Jeffreys in 1926) was that the outer core of the earth is liquid. Since S-waves do not pass through liquids, the liquid core causes a "shadow" on the side of the planet opposite the earthquake where no direct S-waves are observed. In addition, P-waves travel much slower through the outer core than the mantle.

Processing readings from many seismometers using seismic tomography, seismologists have mapped the mantle of the earth to a resolution of several hundred kilometers. This has enabled scientists to identify convection cells and other large-scale features such as the large low-shear-velocity provinces near the core‚Äìmantle boundary.

Forecasting a probable timing, location, magnitude and other important features of a forthcoming seismic event is called earthquake prediction. Various attempts have been made by seismologists and others to create effective systems for precise earthquake predictions, including the VAN method. Most seismologists do not believe that a system to provide timely warnings for individual earthquakes has yet been developed, and many believe that such a system would be unlikely to give useful warning of impending seismic events. However, more general forecasts routinely predict seismic hazard. Such forecasts estimate the probability of an earthquake of a particular size affecting a particular location within a particular time-span, and they are routinely used in earthquake engineering.

Public controversy over earthquake prediction erupted after Italian authorities indicted six seismologists and one government official for manslaughter in connection with a magnitude 6.3 earthquake in L'Aquila, Italy on April 5, 2009. The indictment has been widely perceived as an indictment for failing to predict the earthquake and has drawn condemnation from the American Association for the Advancement of Science and the American Geophysical Union. The indictment claims that, at a special meeting in L'Aquila the week before the earthquake occurred, scientists and officials were more interested in pacifying the population than providing adequate information about earthquake risk and preparedness.

Engineering seismology is the study and application of seismology for engineering purposes. It generally applied to the branch of seismology that deals with the assessment of the seismic hazard of a site or region for the purposes of earthquake engineering. It is, therefore, a link between earth science and civil engineering. There are two principal components of engineering seismology. Firstly, studying earthquake history (e.g. historical and instrumental catalogs of seismicity) and tectonics to assess the earthquakes that could occur in a region and their characteristics and frequency of occurrence. Secondly, studying strong ground motions generated by earthquakes to assess the expected shaking from future earthquakes with similar characteristics. These strong ground motions could either be observations from accelerometers or seismometers or those simulated by computers using various techniques, which are then often used to develop ground motion prediction equations (or ground-motion models).

Seismological instruments can generate large amounts of data. Systems for processing such data include:






</doc>
<doc id="28936" url="https://en.wikipedia.org/wiki?curid=28936" title="Cyanoacrylate">
Cyanoacrylate

Cyanoacrylates are a family of strong fast-acting adhesives with industrial, medical, and household uses. They are derived from ethyl cyanoacrylate and related esters. The cyanoacrylate group in the monomer rapidly polymerize in the presence of water to form long, strong chains. They have some minor toxicity.

Specific cyanoacrylates include methyl 2-cyanoacrylate (MCA), ethyl 2-cyanoacrylate (ECA, commonly sold under trade names such as "Super Glue" and "Krazy Glue", or Toagosei), "n"-butyl cyanoacrylate (n-BCA), octyl cyanoacrylate, and 2-octyl cyanoacrylate (used in medical, veterinary and first aid applications). Octyl cyanoacrylate was developed to address toxicity concerns and to reduce skin irritation and allergic response. Cyanoacrylate adhesives are sometimes known generically as instant glues, power glues or superglues. The abbreviation "CA" is commonly used for industrial grade cyanoacrylate.

The original patent for cyanoacrylate was filed in 1942 by the B.F. Goodrich Company as an outgrowth of a search for materials suitable for clear plastic gun sights for the war effort. In 1942, a team of scientists headed by Harry Coover Jr. stumbled upon a formulation that stuck to everything with which it came in contact. The team quickly rejected the substance for the wartime application, but in 1951, while working as researchers for Eastman Kodak, Coover and a colleague, Fred Joyner, rediscovered cyanoacrylates. The two realized the true commercial potential, and a form of the adhesive was first sold in 1958 under the title "Eastman #910" (later "Eastman 910").

During the 1960s, Eastman Kodak sold cyanoacrylate to Loctite, which in turn repackaged and distributed it under a different brand name "Loctite Quick Set 404". In 1971, Loctite developed its own manufacturing technology and introduced its own line of cyanoacrylate, called "Super Bonder". Loctite quickly gained market share, and by the late 1970s it was believed to have exceeded Eastman Kodak's share in the North American industrial cyanoacrylate market. National Starch and Chemical Company purchased Eastman Kodak's cyanoacrylate business and combined it with several acquisitions made throughout the 1970s forming Permabond. Other manufacturers of cyanoacrylate include LePage (a Canadian company acquired by Henkel in 1996), the Permabond Division of National Starch and Chemical, which was a subsidiary of Unilever. Together, Loctite, Eastman and Permabond accounted for approximately 75% of the industrial cyanoacrylate market. Permabond continued to manufacture the original 910 formula.

In its liquid form, cyanoacrylate consists of monomers of cyanoacrylate ester molecules. Methyl 2-cyanoacrylate (CH=C(C‚â°N)COOCH) has a molecular weight of 111.1¬†g/mol, a flashpoint of , and a density of 1.1¬†g/mL. Ethyl 2-cyanoacrylate ((CH=C(C‚â°N)COOCHCH)) has a molecular weight of 125¬†g/mol and a flashpoint of more than . To facilitate easy handling, a cyanoacrylate adhesive is frequently formulated with an ingredient such as fumed silica to make it more viscous or gel-like. More recently, formulations are available with additives to increase shear strength, creating a more impact resistant bond. Such additives may include rubber, as in Loctite's "Ultra Gel", or others which are not specified.

In general, the acryl groups rapidly undergo chain-growth polymerisation in the presence of water (specifically hydroxide ions), forming long, strong chains, joining the bonded surfaces together. Because the presence of moisture causes the glue to set, exposure to normal levels of humidity in the air causes a thin skin to start to form within seconds, which very greatly slows the reaction; hence, cyanoacrylates are applied as thin coats to ensure that the reaction proceeds rapidly for bonding.

Cyanoacrylate adhesives have a short shelf life‚Äîabout one year from manufacture if unopened, and one month once opened. 
Cyanoacrylates are mainly used as adhesives. Thin layers bond effectively. Thick layers one do not as effectively. They bond many substances, including human skin and tissues, natural fibres, cotton, wool, leather.

Cyanoacrylate glue has a low shearing strength, which has led to its use as a temporary adhesive in cases where the piece needs to be sheared off later. Common examples include mounting a workpiece to a sacrificial glue block on a lathe, and tightening pins and bolts. It is also used in conjunction with another, slower, but more resilient adhesive as a way of rapidly forming a joint, which then holds the pieces in the appropriate configuration until the second adhesive has set.

Cyanoacrylate-based glue has a weak bond with smooth surfaces and as such easily gives to friction; a good example of this is the fact that cyanoacrylates may be removed from human skin by means of abrasives (e.g. sugar or sandpaper).

Cyanoacrylates are used to assemble prototype electronics (used in wire wrap), flying model aircraft, and as retention dressings for nuts and bolts. Their effectiveness in bonding metal and general versatility have made them popular among modeling and miniatures hobbyists.

Cyanoacrylate glue's ability to resist water has made it popular with marine aquarium hobbyists for fragmenting corals. The cut branches of hard corals, such as Acropora, can be glued to a piece of live rock (harvested reef coral) or Milliput (epoxy putty) to allow the new fragment to grow out. It is safe to use directly in the tank, unlike silicone which must be cured to be safe. However, as a class of adhesives, traditional cyanoacrylates are classified as having "weak" resistance to both moisture and heat although the inclusion of phthalic anhydride reportedly counteracts both of these characteristics. Cyanoacrylate glue is also used frequently in aquascaping both freshwater and marine aquariums for the purpose of securing the rhizomes of live plants to pieces of wood or stone.

Most standard cyanoacrylate adhesives do not bond well with smooth glass, although they can be used as a quick, temporary bond prior to application of an epoxy or cyanoacrylate specifically formulated for use on glass. A mechanical adhesive bond may be formed around glass fibre mat or tissue to reinforce joints or to fabricate small parts.

When added to baking soda (sodium bicarbonate), cyanoacrylate glue forms a hard, lightweight adhesive filler. This works well with porous materials that do not work well with the adhesive alone. This method is sometimes used by aircraft modelers to assemble or repair polystyrene foam parts. It is also used to repair small nicks in the leading edge of wood propeller blades on light aircraft, although this technique is limited to use on aircraft registered in the "experimental" category (composite propellers can be repaired in a similar way using two-part epoxies). This technique can also be used to fill in the slots in the nut of a guitar so that new ones can be cut. The reaction between cyanoacrylate and baking soda is very exothermic (heat-producing) and also produces noxious vapors.

One brand of cyanoacrylate, "SupaFix", uses calcium oxide as a filler giving an even harder (mortar-like in texture) result that can be used to join hard materials and even repair cracked castings.

Cyanoacrylate is used as a forensic tool to capture latent fingerprints on non-porous surfaces like glass, plastic, etc. Cyanoacrylate is warmed to produce fumes that react with the invisible fingerprint residues and atmospheric moisture to form a white polymer (polycyanoacrylate) on the fingerprint ridges. The ridges can then be recorded. The developed fingerprints are, on most surfaces (except on white plastic or similar), visible to the naked eye. Invisible or poorly visible prints can be further enhanced by applying a luminescent or non-luminescent stain.

Thin cyanoacrylate glue has application in woodworking. It can be used as a fast-drying, glossy finish. The use of oil, such as boiled linseed oil, may be used to control the rate at which the cyanoacrylate cures. Cyanoacrylate glue is also used in combination with sawdust (from a saw or sanding) to fill voids and cracks. These repair methods are used on piano soundboards, wood instruments, and wood furniture. Cyanoacrylate glue is also used in the finishing of pen blanks (wooden blanks for turning pens) that have been turned on a lathe by applying multiple thin layers to build up a hard, clear finish that can then be sanded and polished to a glossy finish.

Cyanoacrylate glue was in veterinary use for mending bone, hide, and tortoise shell by the early 1970s or before. Harry Coover said in 1966 that a cyanoacrylate spray was used in the Vietnam War to reduce bleeding in wounded soldiers until they could be taken to a hospital. "n"-Butyl cyanoacrylate has been used medically since the 1970s. In the US, due to its potential to irritate the skin, the U.S. Food and Drug Administration (FDA) did not approve its use as a medical adhesive until 1998 with Dermabond (2-octyl cyanoacrylate). A 1986 independent study suggests that cyanoacrylate can be safer and more functional for wound closure than traditional suturing (stitches). The adhesive is superior in time required to close wounds, incidence of infection (suture canals through the skin's epidermal, dermal, and subcutaneous fat layers introduce additional routes of contamination), and finally cosmetic appearance.

Some rock climbers use cyanoacrylate to repair damage to the skin on their fingertips. Similarly, stringed-instrument players can form protective finger caps (typically, when they lose their calluses due to inactivity or accidents) with cyanoacrylates. While the glue is not very toxic and wears off quickly with shed skin, applying large quantities of glue and its fumes directly to the skin can cause chemical burns.

While standard "superglue" is 100% ethyl 2-cyanoacrylate, many custom formulations (e.g."," 91% ECA, 9% poly(methyl methacrylate), <0.5% hydroquinone, and a small amount of organic sulfonic acid, and variations on the compound "n"-butyl cyanoacrylate for medical applications) have come to be used for specific applications. There are three cyanoacrylate compounds currently available as topical skin adhesives. 2-Octyl cyanoacrylate is marketed as Dermabond, SurgiSeal and more recently LiquiBand Exceed. "n"-Butyl cyanoacrylate is marketed as Histoacryl, Indermil, GluStitch, GluSeal, PeriAcryl, and LiquiBand. The compound ethyl 2-cyanoacrylate is available as Epiglu.

Cyanoacrylate is used in archery to glue fletching to arrow shafts. Some special fletching glues are primarily cyanoacrylate repackaged in special fletching glue kits. Such tubes often have a long, thin metal nozzle for improved precision in applying the glue to the base of the fletching and to ensure secure bonding to the arrow shaft.

Cyanoacrylate is used in the cosmetology and beauty industry as an eyelash extension glue, or a "nail glue" for some artificial nail enhancements such as nail tips and nail wraps, and is sometimes mistaken for eye drops causing accidental injury (chemical eye injury).

Cyanoacrylate adhesives may adhere to body parts, and injuries may occur when parts of the skin are torn off. Without force, however, the glue will spontaneously separate from the skin in time (up to four days). Separation can be accelerated by applying vegetable oil near, on, and around the glue. In the case of glued eyelids, a doctor should be consulted.

The fumes from cyanoacrylate are a vaporized form of the cyanoacrylate monomer that irritate the sensitive mucous membranes of the respiratory tract (i.e., eyes, nose, throat, and lungs). They are immediately polymerized by the moisture in the membranes and become inert. These risks can be minimized by using cyanoacrylate in well-ventilated areas. About 5% of the population can become sensitized to cyanoacrylate fumes after repeated exposure, resulting in flu-like symptoms. Cyanoacrylate may also be a skin irritant, causing an allergic skin reaction. The American Conference of Governmental Industrial Hygienists (ACGIH) assign a threshold limit value exposure limit of 200 parts per billion. On rare occasions, inhalation may trigger asthma. There is no singular measurement of toxicity for all cyanoacrylate adhesives because of the large number of adhesives that contain various cyanoacrylate formulations.

The United Kingdom's Health and Safety Executive and the United States National Toxicology Program have concluded that the use of ethyl cyanoacrylate is safe and that additional study is unnecessary. The compound 2-octyl cyanoacrylate degrades much more slowly due to its longer organic backbone (series of covalently bonded carbon molecules) and the adhesive does not reach the threshold of tissue toxicity. Due to the toxicity issues of ethyl cyanoacrylate, the use of 2-octyl cyanoacrylate for sutures is preferred.

Applying cyanoacrylate to some natural materials such as cotton (jeans, cotton swabs, cotton balls, and certain yarns or fabrics), or leather or wool results in a powerful, rapid, exothermic reaction. This reaction also occurs with fiberglass and carbon fiber. The heat released may cause serious burns or release irritating white smoke. Material Safety Data Sheets for cyanoacrylate instruct users not to wear cotton (jeans) or wool clothing, especially cotton gloves, when applying or handling cyanoacrylates.

Acetone, commonly found as a fraction of nail polish remover (or at hardware stores in pure form), is a widely available solvent capable of softening cured cyanoacrylate. Other solvents include nitromethane, dimethyl sulfoxide, and methylene chloride. gamma-Butyrolactone may also be used to remove cured cyanoacrylate. Commercial debonders are also available.

Cyanoacrylate adhesives have a short shelf life. Date-stamped containers help to ensure that the adhesive is still viable. One manufacturer supplies the following information and advice: When kept unopened in a cool, dry location such as a refrigerator at a temperature of about 55¬†¬∞F (13¬†¬∞C), the shelf life of cyanoacrylate will be extended from about one year from manufacture to at least 15 months. If the adhesive is to be used within six months, it is not necessary to refrigerate it. Cyanoacrylates are moisture-sensitive, and moving from a cool to a hot location will create condensation; after removing from the refrigerator, it is best to let the adhesive reach room temperature before opening. After opening, it should be used within 30 days. Open containers should not be refrigerated. Another manufacturer says that the maximum shelf life of 12 months is obtained for some of their cyanoacrylates if the original containers are stored at . User forums and some manufacturers say that an almost unlimited shelf life is attainable by storing unopened at , the typical temperature of a domestic freezer, and allowing the contents to reach room temperature before use. Rechilling an opened container may cause moisture from the air to condense in the container; however, reports from hobbyists suggest that storing cyanoacrylate in a freezer can preserve opened cyanoacrylate indefinitely.

As cyanoacrylates age, they polymerize, become thicker, and cure more slowly. They can be thinned with a cyanoacrylate of the same chemical composition with lower viscosity. Storing cyanoacrylates below will nearly stop the polymerization process and prevent aging.




</doc>
<doc id="28938" url="https://en.wikipedia.org/wiki?curid=28938" title="Shell script">
Shell script

A shell script is a computer program designed to be run by the Unix shell, a command-line interpreter. The various dialects of shell scripts are considered to be scripting languages. Typical operations performed by shell scripts include file manipulation, program execution, and printing text. A script which sets up the environment, runs the program, and does any necessary cleanup, logging, etc. is called a wrapper.

The term is also used more generally to mean the automated mode of running an operating system shell; in specific operating systems they are called other things such as batch files (MSDos-Win95 stream, OS/2), command procedures (VMS), and shell scripts (Windows NT stream and third-party derivatives like 4NT‚Äîarticle is at cmd.exe), and mainframe operating systems are associated with a number of terms.

The typical Unix/Linux/POSIX-compliant installation includes the KornShell (codice_1) in several possible versions such as ksh88, Korn Shell '93 and others. The oldest shell still in common use is the Bourne shell (codice_2); Unix systems invariably also include the C shell (codice_3), Bash (codice_4), a Remote Shell (codice_5), a Secure Shell (codice_6) for SSL telnet connections, and a shell which is a main component of the Tcl/Tk installation usually called codice_7; wish is a GUI-based Tcl/Tk shell. The C and Tcl shells have syntax quite similar to that of said programming languages, and the Korn shells and Bash are developments of the Bourne shell, which is based on the ALGOL language with elements of a number of others added as well. On the other hand, the various shells plus tools like awk, sed, grep, and BASIC, Lisp, C and so forth contributed to the Perl programming language.

Other shells available on a machine or available for download and/or purchase include Almquist shell (codice_8), PowerShell (codice_9), Z shell (codice_10, a particularly common enhanced KornShell), the Tenex C Shell (codice_11), a Perl-like shell (codice_12). Related programs such as shells based on Python, Ruby, C, Java, Perl, Pascal, Rexx &c in various forms are also widely available. Another somewhat common shell is osh, whose manual page states it "is an enhanced, backward-compatible port of the standard command interpreter from Sixth Edition UNIX."

Windows-Unix interoperability software such as the MKS Toolkit, Cygwin, UWIN, Interix and others make the above shells and Unix programming available on Windows systems, providing functionality all the way down to signals and other inter-process communication, system calls and APIs. The Hamilton C shell is a Windows shell that is very similar to the Unix C Shell. Microsoft distributed Windows Services for UNIX for use with its NT-based operating systems in particular, which have a POSIX environmental subsystem.

Comments are ignored by the shell. They typically begin with the hash symbol (#), and continue until the end of the line.

The shebang, or hash-bang, is a special kind of comment which the system uses to determine what interpreter to use to execute the file. The shebang must be the first line of the file, and start with "#!". In Unix-like operating systems, the characters following the "#!" prefix are interpreted as a path to an executable program that will interpret the script.

A shell script can provide a convenient variation of a system command where special environment settings, command options, or post-processing apply automatically, but in a way that allows the new script to still act as a fully normal Unix command.

One example would be to create a version of ls, the command to list files, giving it a shorter command name of l, which would be normally saved in a user's bin directory as /home/"username"/bin/l, and a default set of command options pre-supplied.
LC_COLLATE=C ls -FCas "$@"
Here, the first line uses a shebang to indicate which interpreter should execute the rest of the script, and the second line makes a listing with options for file format indicators, columns, all files (none omitted), and a size in blocks. The LC_COLLATE=C sets the default collation order to not fold upper and lower case together, not intermix dotfiles with normal filenames as a side effect of ignoring punctuation in the names (dotfiles are usually only shown if an option like -a is used), and the "$@" causes any parameters given to l to pass through as parameters to ls, so that all of the normal options and other syntax known to ls can still be used.

The user could then simply use l for the most commonly used short listing.

Another example of a shell script that could be used as a shortcut would be to print a list of all the files and directories within a given directory.

clear
ls -al
In this case, the shell script would start with its normal starting line of #!/bin/sh. Following this, the script executes the command clear which clears the terminal of all text before going to the next line. The following line provides the main function of the script. The ls -al command lists the files and directories that are in the directory from which the script is being run. The ls command attributes could be changed to reflect the needs of the user.

Note: If an implementation does not have the clear command, try using the clr command instead.

Shell scripts allow several commands that would be entered manually at a command-line interface to be executed automatically, and without having to wait for a user to trigger each stage of the sequence. For example, in a directory with three C source code files, rather than manually running the four commands required to build the final program from them, one could instead create a script for POSIX-compliant shells, here named build and kept in the directory with them, which would compile them automatically:
printf 'compiling...\n'
cc -c foo.c
cc -c bar.c
cc -c qux.c
cc -o myprog foo.o bar.o qux.o
printf 'done.\n'
The script would allow a user to save the file being edited, pause the editor, and then just run ./build to create the updated program, test it, and then return to the editor. Since the 1980s or so, however, scripts of this type have been replaced with utilities like make which are specialized for building programs.

Simple batch jobs are not unusual for isolated tasks, but using shell loops, tests, and variables provides much more flexibility to users. A POSIX sh script to convert JPEG images to PNG images, where the image names are provided on the command-line‚Äîpossibly via wildcards‚Äîinstead of each being listed within the script, can be created with this file, typically saved in a file like /home/"username"/bin/jpg2png
for jpg; do # use $jpg in place of each filename given, in turn
done # the end of the "for" loop
printf 'all conversions successful\n' # tell the user the good news
The jpg2png command can then be run on an entire directory full of JPEG images with just /home/"username"/bin/jpg2png *.jpg

A key feature of shell scripts is that the invocation of their interpreters is handled as a core operating system feature. So rather than a user's shell only being able to execute scripts in that shell's language, or a script only having its interpreter directive handled correctly if it was run from a shell (both of which were limitations in the early Bourne shell's handling of scripts), shell scripts are set up and executed by the OS itself. A modern shell script is not just on the same footing as system commands, but rather many system commands are actually shell scripts (or more generally, scripts, since some of them are not interpreted by a shell, but instead by Perl, Python, or some other language). This extends to returning exit codes like other system utilities to indicate success or failure, and allows them to be called as components of larger programs regardless of how those larger tools are implemented.

Like standard system commands, shell scripts classically omit any kind of filename extension unless intended to be read into a running shell through a special mechanism for this purpose (such as sh‚Äôs "codice_13", or csh‚Äôs source).

Many modern shells also supply various features usually found only in more sophisticated general-purpose programming languages, such as control-flow constructs, variables, comments, arrays, subroutines and so on. With these sorts of features available, it is possible to write reasonably sophisticated applications as shell scripts. However, they are still limited by the fact that most shell languages have little or no support for data typing systems, classes, threading, complex math, and other common full language features, and are also generally much slower than compiled code or interpreted languages written with speed as a performance goal.

The standard Unix tools sed and awk provide extra capabilities for shell programming; Perl can also be embedded in shell scripts as can other scripting languages like Tcl. Perl and Tcl come with graphics toolkits as well.

Many powerful scripting languages have been introduced for tasks that are too large or complex to be comfortably handled with ordinary shell scripts, but for which the advantages of a script are desirable and the development overhead of a full-blown, compiled programming language would be disadvantageous. The specifics of what separates scripting languages from high-level programming languages is a frequent source of debate, but, generally speaking, a scripting language is one which requires an interpreter.
Shell scripts often serve as an initial stage in software development, and are often subject to conversion later to a different underlying implementation, most commonly being converted to Perl, Python, or C. The interpreter directive allows the implementation detail to be fully hidden inside the script, rather than being exposed as a filename extension, and provides for seamless reimplementation in different languages with no impact on end users.

While files with the ".sh" file extension are usually a shell script of some kind, most shell scripts do not have any filename extension.

Perhaps the biggest advantage of writing a shell script is that the commands and syntax are exactly the same as those directly entered at the command-line. The programmer does not have to switch to a totally different syntax, as they would if the script were written in a different language, or if a compiled language were used.

Often, writing a shell script is much quicker than writing the equivalent code in other programming languages. The many advantages include easy program or file selection, quick start, and interactive debugging. A shell script can be used to provide a sequencing and decision-making linkage around existing programs, and for moderately sized scripts the absence of a compilation step is an advantage. Interpretive running makes it easy to write debugging code into a script and re-run it to detect and fix bugs. Non-expert users can use scripting to tailor the behavior of programs, and shell scripting provides some limited scope for multiprocessing.

On the other hand, shell scripting is prone to costly errors. Inadvertent typing errors such as rm -rf * / (instead of the intended rm -rf */) are folklore in the Unix community; a single extra space converts the command from one that deletes everything in the sub-directories to one which deletes everything‚Äîand also tries to delete everything in the root directory. Similar problems can transform cp and mv into dangerous weapons, and misuse of the > redirect can delete the contents of a file. This is made more problematic by the fact that many UNIX commands differ in name by only one letter: cp, cd, dd, df, etc.

Another significant disadvantage is the slow execution speed and the need to launch a new process for almost every shell command executed. When a script's job can be accomplished by setting up a pipeline in which efficient filter commands perform most of the work, the slowdown is mitigated, but a complex script is typically several orders of magnitude slower than a conventional compiled program that performs an equivalent task.

There are also compatibility problems between different platforms. Larry Wall, creator of Perl, famously wrote that "It is easier to port a shell than a shell script."

Similarly, more complex scripts can run into the limitations of the shell scripting language itself; the limits make it difficult to write quality code, and extensions by various shells to ameliorate problems with the original shell language can make problems worse.

Many disadvantages of using some script languages are caused by design flaws within the language syntax or implementation, and are not necessarily imposed by the use of a text-based command-line; there are a number of shells which use other shell programming languages or even full-fledged languages like Scsh (which uses Scheme).

Interoperability software such as Cygwin, the MKS Toolkit, Interix (which is available in the Microsoft Windows Services for UNIX), Hamilton C shell, UWIN (AT&T Unix for Windows) and others allow Unix shell programs to be run on machines running Windows NT and its successors, with some loss of functionality on the MS-DOS-Windows 95 branch, as well as earlier MKS Toolkit versions for OS/2. At least three DCL implementations for Windows type operating systems‚Äîin addition to XLNT, a multiple-use scripting language package which is used with the command shell, Windows Script Host and CGI programming‚Äîare available for these systems as well. Mac OS X and subsequent are Unix-like as well.

In addition to the aforementioned tools, some POSIX and OS/2 functionality can be used with the corresponding environmental subsystems of the Windows NT operating system series up to Windows 2000 as well. A third, 16-bit subsystem often called the MS-DOS subsystem uses the Command.com provided with these operating systems to run the aforementioned MS-DOS batch files.

The console alternatives 4DOS, 4OS2, FreeDOS, Peter Norton's NDOS and 4NT / Take Command which add functionality to the Windows NT-style cmd.exe, MS-DOS/Windows 95 batch files (run by Command.com), OS/2's cmd.exe, and 4NT respectively are similar to the shells that they enhance and are more integrated with the Windows Script Host, which comes with three pre-installed engines, VBScript, JScript, and VBA and to which numerous third-party engines can be added, with Rexx, Perl, Python, Ruby, and Tcl having pre-defined functions in 4NT and related programs. PC DOS is quite similar to MS-DOS, whilst DR DOS is more different. Earlier versions of Windows NT are able to run contemporary versions of 4OS2 by the OS/2 subsystem.

Scripting languages are, by definition, able to be extended; for example, a MS-DOS/Windows 95/98 and Windows NT type systems allows for shell/batch programs to call tools like KixTart, QBasic, various BASIC, Rexx, Perl, and Python implementations, the Windows Script Host and its installed engines. On Unix and other POSIX-compliant systems, awk and sed are used to extend the string and numeric processing ability of shell scripts. Tcl, Perl, Rexx, and Python have graphics toolkits and can be used to code functions and procedures for shell scripts which pose a speed bottleneck (C, Fortran, assembly language &c are much faster still) and to add functionality not available in the shell language such as sockets and other connectivity functions, heavy-duty text processing, working with numbers if the calling script does not have those abilities, self-writing and self-modifying code, techniques like recursion, direct memory access, various types of sorting and more, which are difficult or impossible in the main script, and so on. Visual Basic for Applications and VBScript can be used to control and communicate with such things as spreadsheets, databases, scriptable programs of all types, telecommunications software, development tools, graphics tools and other software which can be accessed through the Component Object Model.




</doc>
<doc id="28940" url="https://en.wikipedia.org/wiki?curid=28940" title="Subtitle (disambiguation)">
Subtitle (disambiguation)

Subtitles are text derived from film or television show dialogue that is usually displayed at the bottom of the screen.

Subtitle or Subtitles may also refer to:




</doc>
<doc id="28942" url="https://en.wikipedia.org/wiki?curid=28942" title="Solder">
Solder

Solder (, or in North America ) is a fusible metal alloy used to create a permanent bond between metal workpieces. Solder is melted in order to adhere to and connect the pieces after cooling, which requires that an alloy suitable for use as solder have a lower melting point than the pieces being joined. The solder should also be resistant to oxidative and corrosive effects that would degrade the joint over time. Solder used in making electrical connections also needs to have favorable electrical characteristics.

Soft solder typically has a melting point range of , and is commonly used in electronics, plumbing, and sheet metal work. Alloys that melt between are the most commonly used. Soldering performed using alloys with a melting point above is called "hard soldering", "silver soldering", or brazing.

In specific proportions, some alloys are eutectic ‚Äî that is, the alloy's melting point is the lowest possible for a mixture of those components, and coincides with the freezing point. Non-eutectic alloys can have markedly different "solidus" and "liquidus" temperatures, as they have distinct liquid and solid transitions. Non-eutectic mixtures often exist as a paste of solid particles in a melted matrix of the lower-melting phase as they approach high enough temperatures. In electrical work, if the joint is disturbed while in this "pasty" state before it fully solidifies, a poor electrical connection may result; use of eutectic solder reduces this problem. The pasty state of a non-eutectic solder can be exploited in plumbing, as it allows molding of the solder during cooling, e.g. for ensuring watertight joint of pipes, resulting in a so-called "wiped joint".

For electrical and electronics work, solder wire is available in a range of thicknesses for hand-soldering (manual soldering is performed using a soldering iron or soldering gun), and with cores containing flux. It is also available as a room temperature paste, as a preformed foil shaped to match the workpiece which may be more suited for mechanized mass-production, or in small "tabs" that can be wrapped around the joint and melted with a flame where an iron isn't usable or available, as for instance in field repairs. Alloys of lead and tin were commonly used in the past and are still available; they are particularly convenient for hand-soldering. Lead-free solders have been increasing in use due to regulatory requirements plus the health and environmental benefits of avoiding lead-based electronic components. They are almost exclusively used today in consumer electronics.

Plumbers often use bars of solder, much thicker than the wire used for electrical applications, and apply flux separately; many plumbing-suitable soldering fluxes are too corrosive (or conductive) to be used in electrical or electronic work. Jewelers often use solder in thin sheets, which they cut into snippets.

The word solder comes from the Middle English word , via Old French and , from the Latin , meaning "to make solid".

Tin-lead (Sn-Pb) solders, also called soft solders, are commercially available with tin concentrations between 5% and 70% by weight. The greater the tin concentration, the greater the solder‚Äôs tensile and shear strengths. Historically, lead has been widely believed to mitigate the formation of tin whiskers, though the precise mechanism for this is unknown. Today, many techniques are used to mitigate the problem, including changes to the annealing process (heating and cooling), addition of elements like copper and nickel, and the application of conformal coatings. Alloys commonly used for electrical soldering are 60/40¬†Sn-Pb, which melts at , and 63/37¬†Sn-Pb used principally in electrical/electronic work. This mixture is a eutectic alloy of these metals, which:

In the United States, since 1974, lead is prohibited in solder and flux in plumbing applications for drinking water use, per the Safe Drinking Water Act (SDWA). Historically, a higher proportion of lead was used, commonly 50/50. This had the advantage of making the alloy solidify more slowly. With the pipes being physically fitted together before soldering, the solder could be wiped over the joint to ensure water tightness. Although lead water pipes were displaced by copper when the significance of lead poisoning began to be fully appreciated, lead solder was still used until the 1980s because it was thought that the amount of lead that could leach into water from the solder was negligible from a properly soldered joint. The electrochemical couple of copper and lead promotes corrosion of the lead and tin. Tin, however, is protected by insoluble oxide. Since even small amounts of lead have been found detrimental to health as a potent neurotoxin, lead in plumbing solder was replaced by silver (food-grade applications) or antimony, with copper often added, and the proportion of tin was increased (see Lead-free solder.)

The addition of tin‚Äîmore expensive than lead‚Äîimproves wetting properties of the alloy; lead itself has poor wetting characteristics. High-tin tin-lead alloys have limited use as the workability range can be provided by a cheaper high-lead alloy.

Lead-tin solders readily dissolve gold plating and form brittle intermetallics.
60/40¬†Sn-Pb solder oxidizes on the surface, forming a complex 4-layer structure: tin(IV) oxide on the surface, below it a layer of tin(II) oxide with finely dispersed lead, followed by a layer of tin(II) oxide with finely dispersed tin and lead, and the solder alloy itself underneath.

Lead, and to some degree tin, as used in solder contains small but significant amounts of radioisotope impurities. Radioisotopes undergoing alpha decay are a concern due to their tendency to cause soft errors. Polonium-210 is especially problematic; lead-210 beta decays to bismuth-210 which then beta decays to polonium-210, an intense emitter of alpha particles. Uranium-238 and thorium-232 are other significant contaminants of alloys of lead.

The European Union Waste Electrical and Electronic Equipment Directive (WEEE) and Restriction of Hazardous Substances Directive (RoHS) were adopted in early 2003 and came into effect on July 1, 2006, restricting the inclusion of lead in most consumer electronics sold in the EU, and having a broad effect on consumer electronics sold worldwide. In the US, manufacturers may receive tax benefits by reducing the use of lead-based solder. Lead-free solders in commercial use may contain tin, copper, silver, bismuth, indium, zinc, antimony, and traces of other metals. Most lead-free replacements for conventional 60/40 and 63/37¬†Sn-Pb solder have melting points from 50 to 200¬†¬∞C higher, though there are also solders with much lower melting points. Lead-free solder typically requires around 2% flux by mass for adequate wetting ability.

When lead-free solder is used in wave soldering, a slightly modified solder pot may be desirable (e.g. titanium liners or impellers) to reduce maintenance cost due to increased tin-scavenging of high-tin solder.

Lead-free solder may be less desirable for critical applications, such as aerospace and medical projects, because its properties are less thoroughly known.

Tin-silver-copper (Sn-Ag-Cu, or "SAC") solders are used by two-thirds of Japanese manufacturers for reflow and wave soldering, and by about 75% of companies for hand soldering. The widespread use of this popular lead-free solder alloy family is based on the reduced melting point of the Sn-Ag-Cu ternary eutectic behavior (), which is below the 22/78¬†Sn-Ag (wt.%) eutectic of and the 59/41¬†Sn-Cu eutectic of . The ternary eutectic behavior of Sn-Ag-Cu and its application for electronics assembly was discovered (and patented) by a team of researchers from Ames Laboratory, Iowa State University, and from Sandia National Laboratories-Albuquerque.

Much recent research has focused on the addition of a fourth element to Sn-Ag-Cu solder, in order to provide compatibility for the reduced cooling rate of solder sphere reflow for assembly of ball grid arrays. Examples of these four-element compositions are 18/64/14/4¬†tin-silver-copper-zinc (Sn-Ag-Cu-Zn) (melting range 217‚Äì220¬†¬∞C) and 18/64/16/2¬†tin-silver-copper-manganese (Sn-Ag-Cu-Mn) (melting range of 211‚Äì215¬†¬∞C).

Tin-based solders readily dissolve gold, forming brittle intermetallic joins; for Sn-Pb alloys the critical concentration of gold to embrittle the joint is about 4%. Indium-rich solders (usually indium-lead) are more suitable for soldering thicker gold layer as the dissolution rate of gold in indium is much slower. Tin-rich solders also readily dissolve silver; for soldering silver metallization or surfaces, alloys with addition of silvers are suitable; tin-free alloys are also a choice, though their wettability is poorer. If the soldering time is long enough to form the intermetallics, the tin surface of a joint soldered to gold is very dull.

Hard solders are used for brazing, and melt at higher temperatures. Alloys of copper with either zinc or silver are the most common.

In silversmithing or jewelry making, special hard solders are used that will pass assay. They contain a high proportion of the metal being soldered and lead is not used in these alloys. These solders vary in hardness, designated as "enameling", "hard", "medium" and "easy". Enameling solder has a high melting point, close to that of the material itself, to prevent the joint desoldering during firing in the enameling process. The remaining solder types are used in decreasing order of hardness during the process of making an item, to prevent a previously soldered seam or joint desoldering while additional sites are soldered. Easy solder is also often used for repair work for the same reason. Flux is also used to prevent joints from desoldering.

Silver solder is also used in manufacturing to join metal parts that cannot be welded. The alloys used for these purposes contain a high proportion of silver (up to 40%), and may also contain cadmium.

Different elements serve different roles in the solder alloy:


Impurities usually enter the solder reservoir by dissolving the metals present in the assemblies being soldered. Dissolving of process equipment is not common as the materials are usually chosen to be insoluble in solder.

Board finishes vs wave soldering bath impurities buildup:


Flux is a reducing agent designed to help reduce (return oxidized metals to their metallic state) metal oxides at the points of contact to improve the electrical connection and mechanical strength. The two principal types of flux are acid flux (sometimes called "active flux"), containing strong acids, used for metal mending and plumbing, and rosin flux (sometimes called "passive flux"), used in electronics. Rosin flux comes in a variety of "activities", corresponding roughly to the speed and effectiveness of the organic acid components of the rosin in dissolving metallic surface oxides, and consequently the corrosiveness of the flux residue.

Due to concerns over atmospheric pollution and hazardous waste disposal, the electronics industry has been gradually shifting from rosin flux to water-soluble flux, which can be removed with deionized water and detergent, instead of hydrocarbon solvents. Water soluble fluxes are generally more conductive than traditionally used electrical / electronic fluxes and so have more potential for electrically interacting with a circuit; in general it is important to remove their traces after soldering. Some rosin type flux traces likewise should be removed, and for the same reason.

In contrast to using traditional bars or coiled wires of all-metal solder and manually applying flux to the parts being joined, much hand soldering since the mid-20th century has used flux-core solder. This is manufactured as a coiled wire of solder, with one or more continuous bodies of inorganic acid or rosin flux embedded lengthwise inside it. As the solder melts onto the joint, it frees the flux and releases that on it as well.

The solidifying behavior depends on the alloy composition. Pure metals solidify at a certain temperature, forming crystals of one phase. Eutectic alloys also solidify at a single temperature, all components precipitating simultaneously in so-called coupled growth. Non-eutectic compositions on cooling start to first precipitate the non-eutectic phase; dendrites when it is a metal, large crystals when it is an intermetallic compound. Such a mixture of solid particles in a molten eutectic is referred to as a mushy state. Even a relatively small proportion of solids in the liquid can dramatically lower its fluidity.

The temperature of total solidification is the solidus of the alloy, the temperature at which all components are molten is the liquidus.

The mushy state is desired where a degree of plasticity is beneficial for creating the joint, allowing filling larger gaps or being wiped over the joint (e.g. when soldering pipes). In hand soldering of electronics it may be detrimental as the joint may appear solidified while it is not yet. Premature handling of such joint then disrupts its internal structure and leads to compromised mechanical integrity.

Many different intermetallic compounds are formed during solidifying of solders and during their reactions with the soldered surfaces. The intermetallics form distinct phases, usually as inclusions in a ductile solid solution matrix, but also can form the matrix itself with metal inclusions or form crystalline matter with different intermetallics. Intermetallics are often hard and brittle. Finely distributed intermetallics in a ductile matrix yield a hard alloy while coarse structure gives a softer alloy. A range of intermetallics often forms between the metal and the solder, with increasing proportion of the metal; e.g. forming a structure of Cu-CuSn-CuSn-Sn. Layers of intermetallics can form between the solder and the soldered material. These layers may cause mechanical reliability weakening and brittleness, increased electrical resistance, or electromigration and formation of voids. The gold-tin intermetallics layer is responsible for poor mechanical reliability of tin-soldered gold-plated surfaces where the gold plating did not completely dissolve in the solder.

Two processes play a role in a solder joint formation: interaction between the substrate and molten solder, and solid-state growth of intermetallic compounds. The base metal dissolves in the molten solder in an amount depending on its solubility in the solder. The active constituent of the solder reacts with the base metal with a rate dependent on the solubility of the active constituents in the base metal. The solid-state reactions are more complex ‚Äì the formation of intermetallics can be inhibited by changing the composition of the base metal or the solder alloy, or by using a suitable barrier layer to inhibit diffusion of the metals.

Some example interactions include:


A preform is a pre-made shape of solder specially designed for the application where it is to be used. Many methods are used to manufacture the solder preform, stamping being the most common. The solder preform may include the solder flux needed for the soldering process. This can be an internal flux, inside the solder preform, or external, with the solder preform coated.

Glass solder is used to join glasses to other glasses, ceramics, metals, semiconductors, mica, and other materials, in a process called glass frit bonding. The glass solder has to flow and wet the soldered surfaces well below the temperature where deformation or degradation of either of the joined materials or nearby structures (e.g., metallization layers on chips or ceramic substrates) occurs. The usual temperature of achieving flowing and wetting is between .




</doc>
<doc id="28943" url="https://en.wikipedia.org/wiki?curid=28943" title="Shogun">
Shogun

The shoguns officials were collectively referred to as the "bakufu," or "tent government"; they were the ones who carried out the actual duties of administration, while the Imperial court retained only nominal authority. The tent symbolized the shoguns role as the military's field commander, but also denoted that such an office was meant to be temporary. Nevertheless, the institution, known in English as the shogunate ( ), persisted for nearly 700 years, ending when Tokugawa Yoshinobu relinquished the office to Emperor Meiji in 1867 as part of the Meiji Restoration.

The term is the abbreviation of the historical title "Seii Taish≈çgun." ÂæÅ ("sei",„Åõ„ÅÑ) means "conquer" or "subjugate," and Â§∑ ("i", „ÅÑ) means "barbarian" or "savage." Â§ß ("dai", „Å†„ÅÑ) means "great," Â∞Ü ("sh≈ç", „Åó„Çá„ÅÜ) means "commander," and Ëªç ("gun", „Åê„Çì) means "army." Thus, a literal translation of Seii Taish≈çgun would be "Commander-in-Chief of the Expeditionary Force Against the Barbarians."

The term was originally used to refer to the general who commanded the army sent to fight the tribes of northern Japan, but after the twelfth century, the term was used to designate the leader of the samurai.

The administration of a shogun is called in Japanese and literally means "government from the maku ()." During the battles, the head of the samurai army used to be sitting in a scissor chair inside a semi-open tent called maku that exhibited its respective mon or blazon. The application of the term bakufu to the shogun government shows an extremely strong and representative symbolism.

Historically, similar terms to "Seii Taish≈çgun" were used with varying degrees of responsibility, although none of them had equal or more importance than "Seii Taish≈çgun". Some of them were:

There is no consensus among the various authors since some sources consider Tajihi no Agatamori the first, others say ≈åtomo no Otomaro, other sources assure that the first was Sakanoue no Tamuramaro, while others avoid the problem by just mentioning from the first Kamakura shogun Minamoto no Yoritomo.

Originally, the title of "Sei-i Taish≈çgun" ("Commander-in-Chief of the Expeditionary Force Against the Barbarians") was given to military commanders during the early Heian period for the duration of military campaigns against the Emishi, who resisted the governance of the Kyoto-based imperial court. ≈åtomo no Otomaro was the first "Sei-i Taish≈çgun". The most famous of these shoguns was Sakanoue no Tamuramaro.

In the later Heian period, one more shogun was appointed. Minamoto no Yoshinaka was named "sei-i taish≈çgun" during the Genpei War, only to be killed shortly thereafter by Minamoto no Yoshitsune.

Sakanoue no Tamuramaro (758-811) was a Japanese general who fought against the tribes of northern Japan (settled in the territory that today integrates the provinces of Mutsu and Dewa). Tamarumaro was the first general to bend these tribes, integrating its territory to that of the Japanese State. For his military feats he was named Seii Taish≈çgun and probably because he was the first to win the victory against the northern tribes he is generally recognized as the first shogun in history. (Note: according to historical sources ≈åtomo no Otomaro also had the title of Seii Taish≈çgun).

In the early 11th century, "daimy≈ç" protected by samurai came to dominate internal Japanese politics. Two of the most powerful families¬†‚Äì the Taira and Minamoto¬†‚Äì fought for control over the declining imperial court. The Taira family seized control from 1160 to 1185, but was defeated by the Minamoto in the Battle of Dan-no-ura. Minamoto no Yoritomo seized power from the central government and aristocracy and established a feudal system based in Kamakura in which the private military, the samurai, gained some political powers while the Emperor and the aristocracy remained the "de jure" rulers. In 1192, Yoritomo was awarded the title of "Sei-i Taish≈çgun" by Emperor Go-Toba and the political system he developed with a succession of shoguns as the head became known as a shogunate. Yoritomo's wife's family, the H≈çj≈ç, seized power from the Kamakura shoguns. When Yoritomo's sons and heirs were assassinated, the shogun himself became a hereditary figurehead. Real power rested with the H≈çj≈ç regents. The Kamakura shogunate lasted for almost 150 years, from 1192 to 1333.

The end of the Kamakura shogunate came when Kamakura fell in 1333, and the H≈çj≈ç Regency was destroyed. Two imperial families ‚Äì the senior Northern Court and the junior Southern Court ‚Äì had a claim to the throne. The problem was solved with the intercession of the Kamakura shogunate, who had the two lines alternate. This lasted until 1331, when Emperor Go-Daigo (of the Southern Court) tried to overthrow the shogunate to stop the alternation. As a result, Daigo was exiled. Around 1334‚Äì1336, Ashikaga Takauji helped Daigo regain his throne.

The fight against the shogunate left the Emperor with too many people claiming a limited supply of land. Takauji turned against the Emperor when the discontent about the distribution of land grew great enough. In 1336 Daigo was banished again, in favor of a new Emperor.

During the Kenmu Restoration, after the fall of the Kamakura shogunate in 1333, another short-lived shogun arose. Prince Moriyoshi (Morinaga), son of Go-Daigo, was awarded the title of "Sei-i Taish≈çgun". However, Prince Moriyoshi was later put under house arrest and, in 1335, killed by Ashikaga Tadayoshi.

In 1338, Ashikaga Takauji, like Minamoto no Yoritomo, a descendant of the Minamoto princes, was awarded the title of "sei-i taish≈çgun" and established the Ashikaga shogunate, which nominally lasted until 1573. The Ashikaga had their headquarters in the Muromachi district of Kyoto, and the time during which they ruled is also known as the Muromachi period.

While the title of shogun went into abeyance due to technical reasons, Oda Nobunaga and his successor, Toyotomi Hideyoshi, who later obtained the position of Imperial Regent, gained far greater power than any of their predecessors had. Hideyoshi is considered by many historians to be among Japan's greatest rulers.

Tokugawa Ieyasu seized power and established a government at Edo (now known as Tokyo) in 1600. He received the title "sei-i taish≈çgun" in 1603, after he forged a family tree to show he was of Minamoto descent. The Tokugawa shogunate lasted until 1867, when Tokugawa Yoshinobu resigned as shogun and abdicated his authority to Emperor Meiji. Ieyasu set a precedent in 1605 when he retired as shogun in favour of his son Tokugawa Hidetada, though he maintained power from behind the scenes as (, cloistered shogun).

During the Edo period, effective power rested with the Tokugawa shogun, not the Emperor in Kyoto, even though the former ostensibly owed his position to the latter. The shogun controlled foreign policy, the military, and feudal patronage. The role of the Emperor was ceremonial, similar to the position of the Japanese monarchy after the Second World War.

The term originally meant the dwelling and household of a shogun, but in time, became a metonym for the system of government of a feudal military dictatorship, exercised in the name of the shogun or by the shogun himself. Therefore, various "bakufu" held absolute power over the country (territory ruled at that time) without pause from 1192 to 1867, glossing over actual power, clan and title transfers.

The shogunate system was originally established under the Kamakura shogunate by Minamoto no Yoritomo. Although theoretically, the state (and therefore the Emperor) held ownership of all land in Japan. The system had some feudal elements, with lesser territorial lords pledging their allegiance to greater ones. Samurai were rewarded for their loyalty with agricultural surplus, usually rice, or labor services from peasants. In contrast to European feudal knights, samurai were not landowners. The hierarchy that held this system of government together was reinforced by close ties of loyalty between the "daimy≈çs", samurai and their subordinates.

Each shogunate was dynamic, not static. Power was constantly shifting and authority was often ambiguous. The study of the ebbs and flows in this complex history continues to occupy the attention of scholars. Each shogunate encountered competition. Sources of competition included the Emperor and the court aristocracy, the remnants of the imperial governmental systems, the "daimy≈çs", the "sh≈çen" system, the great temples and shrines, the "s≈çhei", the "shugo" and "jit≈ç", the "jizamurai" and early modern "daimy≈ç". Each shogunate reflected the necessity of new ways of balancing the changing requirements of central and regional authorities.

Since Minamoto no Yoritomo turned the figure of the shogun into a permanent and hereditary position and until the Meiji Restoration there were two ruling classes in Japan: 1. the emperor or , who acted as "chief priest" of the official religion of the country, Shinto, and 2. the shogun, head of the army who also enjoyed civil, military, diplomatic and judicial authority. Although in theory the shogun was an emperor's servant, it became the true power behind the throne.

No shogun tried to usurp the throne, even when they had at their disposal the military power of the territory. There were two reasons primarily:


Unable to usurp the throne, the shoguns sought throughout history to keep the emperor away from the country's political activity, relegating them from the sphere of influence. One of the few powers that the imperial house could retain was that of being able to "control time" through the designation of the Japanese Neng≈ç or Eras and the issuance of calendars.

This is a highlight of two historical attempts of the emperor to recover the power they enjoyed before the establishment of the shogunate. In 1219 the Emperor Go-Toba accused the H≈çj≈ç as outlaws. Imperial troops mobilized, leading to the J≈çky≈´ War (1219-1221), which would culminate in the third Battle of Uji (1221). During this, the imperial troops were defeated and the emperor Go-Toba was exiled. With the defeat of Go-Toba, the samurai government over the country was confirmed. At the beginning of the fourteenth century the Emperor Go-Daigo decided to rebel, but the H≈çj≈ç, who were then regents, sent an army from Kamakura. The emperor fled before the troops arrived and took the imperial insignia. The shogun named his own emperor, giving rise to the era .

During the 1850s and 1860s, the shogunate was severely pressured both abroad and by foreign powers. It was then that various groups angry with the shogunate for the concessions made to the various European countries found in the figure of the emperor an ally through which they could expel the Tokugawa shogunate from power. The motto of this movement was and they finally succeeded in 1868, when imperial power was restored after centuries of being in the shadow of the country's political life.

Upon Japan's surrender after World War II, American Army General Douglas MacArthur became Japan's "de facto" ruler during the years of occupation. So great was his influence in Japan that he has been dubbed the .

Today, the head of the Japanese government is the Prime Minister; the usage of the term "shogun" has nevertheless continued in colloquialisms. A retired Prime Minister who still wields considerable power and influence behind the scenes is called a , a sort of modern incarnation of the cloistered rule. Examples of "shadow shoguns" are former Prime Minister Kakuei Tanaka and the politician Ichir≈ç Ozawa.





</doc>
<doc id="28944" url="https://en.wikipedia.org/wiki?curid=28944" title="Short-term memory">
Short-term memory

Short-term memory (or "primary" or "active memory") is the capacity for holding, but not manipulating, a small amount of information in mind in an active, readily available state for a short period of time. For example, short-term memory can be used to remember a phone number that has just been recited. The duration of short-term memory (when rehearsal or active maintenance is prevented) is believed to be in the order of seconds. A commonly cited capacity of items to remember is "The Magical Number Seven, Plus or Minus Two" (also called Miller's Law, despite Miller calling the figure "little more than a joke" (Miller, 1989, page 401)). Cowan (2001) suggests that a more realistic figure is 4¬±1 items). In contrast, long-term memory holds information indefinitely.
Short-term memory should be distinguished from working memory, which refers to structures and processes used for temporarily storing and manipulating information (see details below).

The idea of the division of memory into short-term and long-term dates back to the 19th century. A classical model of memory developed in the 1960s assumed that all memories pass from a short-term to a long-term store after a small period of time. This model is referred to as the "modal model" and has been most famously detailed by Shiffrin. The model states that memory is first stored in the sensory memory which has a very large capacity but can only maintain information for milliseconds. A partial content of the rapidly decaying memory is moved to Short term memory. Short term memory does not have a large capacity like sensory memory but holds information longer for several seconds or minutes. The final storage is the long term memory which has a very large capacity and is capable of holding information as long as one‚Äôs lifetime.

The exact mechanisms by which this transfer takes place, whether all or only some memories are retained permanently, and indeed the existence of a genuine distinction between the two stores, remain controversial topics among experts.

One form of evidence, cited in favor of the separate existence of a short-term store comes from anterograde amnesia, the inability to learn new facts and episodes. Patients with this form of amnesia, have intact ability to retain small amounts of information over short time scales (up to 30 seconds) but are dramatically impaired in their ability to form longer-term memories (a famous example is patient HM). This is interpreted as showing that the short-term store is spared from amnesia and other brain diseases.

Other evidence comes from experimental studies showing that some manipulations (e.g., a distractor task, such as repeatedly subtracting a single-digit number from a larger number following learning; cf Brown-Peterson procedure) impair memory for the 3 to 5 most recently learned words of a list (it is presumed, still held in short-term memory), while leaving recall for words from earlier in the list (it is presumed, stored in long-term memory) unaffected; other manipulations (e.g., semantic similarity of the words) affect only memory for earlier list words, but do not affect memory for the last few words in a list. These results show that different factors affect short-term recall (disruption of rehearsal) and long-term recall (semantic similarity). Together, these findings show that long-term memory and short-term memory can vary independently of each other.

Not all researchers agree that short-term and long-term memory are separate systems. The Unitary Model proposes that Short term memory consists of temporary activations of long term representation. Some theorists propose that memory is unitary over all time scales, from milliseconds to years. Support for the unitary memory hypothesis comes from the fact that it has been difficult to demarcate a clear boundary between short-term and long-term memory. For instance, Tarnow shows that the recall probability vs. latency curve is a straight line from 6 to 600 seconds (ten minutes), with the probability of failure to recall only saturating after 600 seconds. If there were really two different memory stores operating in this time frame, one could expect a discontinuity in this curve. Other research has shown that the detailed pattern of recall errors looks remarkably similar for recall of a list immediately after learning (it is presumed, from short-term memory) and recall after 24 hours (necessarily from long-term memory).

Further evidence against the existence of a short-term memory store comes from experiments involving continual distractor tasks. In 1974, Robert Bjork and William B. Whitten presented subjects with word pairs to be remembered; however, before and after each word pair, subjects had to do a simple multiplication task for 12 seconds. After the final word-pair, subjects had to do the multiplication distractor task for 20 seconds. In their results, Bjork and Whitten found that the recency effect (the increased probability of recall of the last items studied) and the primacy effect (the increased probability of recall of the first few items) still remained. These results would seem inconsistent with the idea of short-term memory as the distractor items would have taken the place of some of the word-pairs in the buffer, thereby weakening the associated strength of the items in long-term memory. Bjork and Whitten hypothesized that these results could be attributed to the memory processes at work for long-term memory retrieval versus short-term memory retrieval.
Ovid J. L. Tzeng (1973) also found an instance where the recency effect in free recall did not seem to result from the function of a short-term memory store. Subjects were presented with four study-test periods of 10 word lists, with a continual distractor task (20-second period of counting-backward). At the end of each list, participants had to free recall as many words from the list as possible. After free-recall of the fourth list, participants were asked to free recall items from all four lists. Both the initial free recall and the final free recall showed a recency effect. These results went against the predictions of a short-term memory model, where no recency effect would be expected in either initial or final free recall.
Koppenaal and Glanzer (1990) attempted to explain these phenomena as a result of the subjects' adaptation to the distractor task, which therefore allowed them to preserve at least some of the functions of the short-term memory store. As evidence, they provided the results of their experiment, in which the long-term recency effect disappeared when the distractor after the last item differed from the distractors that preceded and followed all the other items (e.g., arithmetic distractor task and word reading distractor task).
Thapar and Greene challenged this theory. In one of their experiments, participants were given a different distractor task after every item to be studied. According to Koppenaal's and Glanzer's theory, there should be no recency effect as subjects would not have had time to adapt to the distractor; yet such a recency effect remained in place in the experiment.

One proposed explanation of the existence of the recency effect in a continual distractor condition, and the disappearance of it in an end-only distractor task is the influence of contextual and distinctive processes. According to this model, recency is a result of the final items' processing context being similar to the processing context of the other items and the distinctive position of the final items versus items in the middle of the list. In the end distractor task, the processing context of the final items is no longer similar to the processing context of the other list items. At the same time, retrieval cues for these items are no longer as effective as without the distractor. Therefore, the recency effect recedes or vanishes. However, when distractor tasks are placed before and after each item, the recency effect returns, because all the list items once again have similar processing context.

Various researchers have proposed that stimuli are coded in short-term memory using transmitter depletion. According to this hypothesis, a stimulus activates a spatial pattern of activity across neurons in a brain region. As these neurons fire, the available neurotransmitters in their store are depleted and this pattern of depletion is iconic, representing stimulus information and functions as a memory trace. The memory trace decays over time as a consequence of neurotransmitter reuptake mechanisms that restore neurotransmitters to the levels that existed prior to stimulus presentation.

The relationship between short-term memory and working memory is described differently by various theories, but it is generally acknowledged that the two concepts are distinct. They both do not hold information for very long but short term memory simply stores information for a short while, while working memory retains the information in order to manipulate it. Short term memory is part of working memory but that doesn‚Äôt make it the same thing.

Working memory is a theoretical framework that refers to structures and processes used for temporarily storing and manipulating information. As such, working memory might also be referred to as "working attention". Working memory and attention together play a major role in the processes of thinking. Short-term memory in general refers, in a theory-neutral manner, to the short-term storage of information, and it does not entail the manipulation or organization of material held in memory. Thus, while there are short-term memory components to working memory models, the concept of short-term memory is distinct from these more hypothetical concepts.

Within Baddeley's influential 1986 model of working memory there are two short-term storage mechanisms: the phonological loop and the visuospatial sketchpad. Most of the research referred to here involves the phonological loop, because most of the work done on short-term memory has used verbal material. Since the 1990s, however, there has been a surge in research on visual short-term memory, and also increasing work on spatial short-term memory.

The limited duration of short-term memory (~18 seconds without a form of memory rehearsal) quickly suggests that its contents spontaneously decay over time. The decay assumption is part of many theories of short-term memory, the most notable one being Baddeley's model of working memory. The decay assumption is usually paired with the idea of rapid covert rehearsal: In order to overcome the limitation of short-term memory, and retain information for longer, information must be periodically repeated or rehearsed‚Äîeither by articulating it out loud or by mentally simulating such articulation. There is also another type of rehearsal that can also be used to improve short term memory is attention based rehearsal. The information is mentally searched in a particular sequence or list. The information is likely to re-enter the short-term store and be retained for a further period using either of these ways

Several researchers; however, dispute that spontaneous decay plays any significant role in forgetting over the short-term, and the evidence is far from conclusive.

Authors doubting that decay causes forgetting from short-term memory often offer as an alternative some form of interference: When several elements (such as digits, words, or pictures, or logos in general) are held in short-term memory simultaneously, their representations compete with each other for recall, or degrade each other. Thereby, new content gradually pushes out older content, unless the older content is actively protected against interference by rehearsal or by directing attention to it.

Whatever the cause or causes of forgetting over the short-term may be, there is consensus that it severely limits the amount of new information that we can retain over brief periods of time. This limit is referred to as the finite capacity of short-term memory. The capacity of short-term memory is often called memory span, in reference to a common procedure of measuring it. In a memory span test, the experimenter presents lists of items (e.g. digits or words) of increasing length. An individual's span is determined as the longest list length that he or she can recall correctly in the given order on at least half of all trials.

In an early and highly influential article, The Magical Number Seven, Plus or Minus Two, psychologist George Miller suggested that human short-term memory has a forward memory span of approximately seven items plus or minus two and that that was well known at the time (it seems to go back to the 19th-century researcher Wundt). More recent research has shown that this "magical number seven" is roughly accurate for college students recalling lists of digits, but memory span varies widely with populations tested and with material used. For example, the ability to recall words in order depends on a number of characteristics of these words: fewer words can be recalled when the words have longer spoken duration; this is known as the "word-length effect", or when their speech sounds are similar to each other; this is called the "phonological similarity effect". More words can be recalled when the words are highly familiar or occur frequently in the language. Recall performance is also better when all of the words in a list are taken from a single semantic category (such as games) than when the words are taken from different categories. A more up-to-date estimate of short-term memory capacity is about four pieces or "chunks" of information. However other prominent theories of short-term memory capacity argue against measuring capacity in terms of a fixed number of elements.

Rehearsal is the process where information is kept in short-term memory by mentally repeating it. When the information is repeated each time, that information is reentered into the short-term memory, thus keeping that information for another 10 to 20 seconds (the average storage time for short-term memory).

Chunking is a process by which one can expand his/her ability to remember things in the short term. Chunking is also a process by which a person organizes material into meaningful groups. Although the average person may retain only about four different units in short-term memory, chunking can greatly increase a person's recall capacity. For example, in recalling a phone number, the person could chunk the digits into three groups: first, the area code (such as 123), then a three-digit chunk (456), and, last, a four-digit chunk (7890). This method of remembering phone numbers is far more effective than attempting to remember a string of 10 digits.

Practice and the usage of existing information in long-term memory can lead to additional improvements in one's ability to use chunking. In one testing session, an American cross-country runner was able to recall a string of 79 digits after hearing them only once by chunking them into different running times (e.g., the first four numbers were 1518, a three-mile time).

It is very difficult to demonstrate the exact capacity of short-term memory (STM) because it will vary depending on the nature of the material to be recalled. There is currently no way of defining the basic unit of information to be stored in the STM store. It is also possible that STM is not the store described by Atkinson and Shiffrin. In that case, the task of defining the task of STM becomes even more difficult.

However, capacity of STM can be affected by the following:Sleep deprivation has a major effect on performance of short term memory research has shown that people who are chronically sleep deprived perform worse on a task that requires working memory than the non-deprived participants.

Research has shown that short term memory capability tends to decrease with age. The short term memory tends to increase during adolescence and the decline appears to be constant and continuous at the beginning of twenties gradually till old age.

Emotions may have a minor effect on short term memory as research has shown. Moreover, emotion in itself does impair cognition and hence influence working memory performance.

Diseases that cause neurodegeneration, such as Alzheimer's disease, can also be a factor in a person's short-term and eventually long-term memory. Short term memory performance is majorly influenced by the nature of diet that one takes. More intake of blue berries has shown to improve short term memory after continuous use whereas alcohol decreases short term memory performance. Damage to certain sections of the brain due to this disease causes a shrinkage in the cerebral cortex which disables the ability to think and recall memories.

Memory loss is a natural process in aging. Research has shown that short term memory capability tends to decrease with age. The short term memory tends to increase during adolescence and the decline appears to be constant and continuous at the beginning of twenties gradually till old age.

One study investigated whether or not there were deficits in short-term memory in older adults. This was a previous study which compiled normative French data for three short-term memory tasks (Verbal, visual and spatial). They found impairments present in participants between the ages of 55 and 85 years of age.

Memory distortion in Alzheimer's disease is a very common disorder found in older adults. Performance of patients with mild to moderate Alzheimer's disease was compared with the performance of age matched healthy adults. Researchers concluded the study with findings that showed reduced short-term memory recall for Alzheimer's patients. Episodic memory and semantic abilities deteriorate early in Alzheimer's disease. Since the cognitive system includes interconnected and reciprocally influenced neuronal networks, one study hypothesized that stimulation of lexical-semantic abilities may benefit semantically structured episodic memory. They found that with Lexical-Semantic stimulation treatment may improve episodic memory in Alzheimer's Disease patients. It could also be regarded as a clinical option to counteract the cognitive decline typical of the disease.

Aphasias are also seen in many elder adults.Semantic aphasia common among stroke patients lack the comprehension of words and objects in a flexible way.

Many language-impaired patients make several complaints about short-term memory deficits, with several family members confirming that patients have trouble recalling previously known names and events. The opinion is supported by many studies showing that many aphasics also have trouble with visual-memory required tasks.

Core symptoms of schizophrenia patients have been linked to cognitive deficits. One neglected factor that contributes to those deficits is the comprehension of time. In this study, results confirm that cognitive dysfunctions are a major deficit in patients with schizophrenia. The study provided evidence that patients with schizophrenia process temporal information inefficiently.

Advanced age is associated with decrements in episodic memory. The associative deficit is in which age differences in recognition memory reflect difficulty in binding components of a memory episode and bound units. A previous study used mixed and blocked test designs to examine deficits in short-term memory of older adults and found there was an associative deficit for older adults. This study along with many other previous studies, continue to build evidence of deficits found in older adults short-term memory.

Even when neurological diseases and disorders are not present, there is a progressive and gradual loss of some intellectual functions that become evident in later years. There are several tests used to examine the psychophysical characteristics of the elderly and of them, a well suitable test would be the functional reach (FR) test, and the mini‚Äìmental state examination (MMSE). The FR test is an index of the aptitude to maintain balance in an upright position and the MMSE test is a global index of cognitive abilities. These tests were both used by Costarella et al. to evaluate the psychophysical characteristics of older adults. They found a loss of physical performance (FR, related to height) as well as a loss of cognitive abilities (MMSE).

Posttraumatic stress disorder (PTSD) is associated with altered processing of emotional material with a strong attentional bias toward trauma-related information and interferes with cognitive processing. Aside from trauma processing specificities, a wide range of cognitive impairments have been related to PTSD state with predominant attention and verbal memory deficits.

There have been few studies done on the relationship between short-term memory and intelligence in PTSD. However, examined whether people with PTSD had equivalent levels of short-term, non-verbal memory on the Benton Visual Retention Test (BVRT), and whether they had equivalent levels of intelligence on the Raven Standard Progressive Matrices (RSPM). They found that people with PTSD had worse short-term, non-verbal memory on the BVRT, despite having comparable levels of intelligence on the RSPM, concluding impairments in memory influence intelligence assessments in the subjects.

There are many tests to measure digit span and short term visual memory, some paper- and some computer-based, including the following:





</doc>
<doc id="28945" url="https://en.wikipedia.org/wiki?curid=28945" title="State supreme court">
State supreme court

In the United States, a state supreme court (known by other names in some states) is the highest court in the state judiciary of a U.S. state. On matters of state law, the judgment of a state supreme court is considered final and binding in both state and federal courts. 

Generally, a state supreme court, like most appellate tribunals, is exclusively for hearing appeals of legal issues. Although state supreme court rulings on matters of state law are final, rulings on matters of federal law can be appealed to the Supreme Court of the United States. Each state supreme courts consists of a panel of judges selected by methods outlined in the state constitution. Among the most common methods for selection are gubernatorial appointment, non-partisan election, and partisan election, but the different states follow a variety of procedures.

Under the system of federalism established by the United States Constitution, federal courts have limited jurisdiction, and state courts handle many more cases than do federal courts. Each of the fifty states has at least one supreme court that serves as the highest court in the state; two states, Texas and Oklahoma, have separate supreme courts for civil and criminal matters. The five permanently inhabited U.S. territories, as well Washington, D.C., each have comparable supreme courts. On matters of state law, the judgment of a state supreme court is considered final and binding in both state and federal courts. State supreme courts are completely distinct from any United States federal courts located within the geographical boundaries of a state's territory, or the federal-level Supreme Court.

The exact duties and powers of the state supreme courts are established by state constitutions and state law. Generally, state supreme courts, like most appellate tribunals, are exclusively for hearing appeals on decisions issued by lower courts, and do not make any finding of facts or hold trials. They can, however, overrule the decisions of lower courts, remand cases to lower courts for further proceedings, and establish binding precedent for future cases. Some state supreme courts do have original jurisdiction over specific issues; for example, the Supreme Court of Virginia has original jurisdiction over cases of habeas corpus, mandamus, prohibition, and writs of actual innocence based on DNA or other biological evidence.

As the highest court in the state, a state supreme court has appellate jurisdiction over all matters of state law. Many states have two or more levels of courts below the state supreme court; for example, in Pennsylvania, a case might first be heard in one of the Pennsylvania courts of common pleas, be appealed to the Superior Court of Pennsylvania, and then finally be appealed to the Supreme Court of Pennsylvania. In other states, including Delaware, the state supreme court is the only appellate court in the state and thus has direct appellate jurisdiction over all lower courts.

Like the U.S. Supreme Court, most state supreme courts have implemented "discretionary review." Under such a system, intermediate appellate courts are entrusted with deciding the vast majority of appeals. Intermediate appellate courts generally focus on the mundane task of what appellate specialists call "error correction," which means their primary task is to decide whether the record reflects that the trial court correctly applied existing law. In a few states without intermediate appellate courts, the state supreme court may operate under "mandatory review", in which it "must" hear all appeals from the trial courts. This was the case, for example, in Nevada prior to 2014. For certain categories of cases, many state supreme courts that otherwise have discretionary review operate under mandatory review, usually with regard to cases involving the interpretation of the state constitution or capital punishment.

One of the informal traditions of the American legal system is that all litigants are entitled to at least one appeal after a final judgment on the merits. However, appeal is merely a "privilege" provided by statute, court rules, or custom in 49 states and in federal judicial proceedings; the U.S. Supreme Court has repeatedly ruled that there is no federal constitutional "right" to an appeal.

Iowa, Oklahoma, and Nevada have a unique procedure for appeals. In those states, "all" appeals are filed with the appropriate Supreme Court (Iowa has a single Supreme Court, while Oklahoma has separate civil and criminal Supreme Courts) which then keeps all cases of first impression for itself to decide. It forwards the remaining caseswhich deal with points of law it has already addressedto the intermediate Court of Appeals. Under this so-called "push-down" or "deflection" model of appellate procedure, the state supreme court can immediately establish final statewide precedents on important issues of first impression as soon as they arise, rather than waiting several months or years for the intermediate appellate court to make a first attempt at resolving the issue (and leaving the law uncertain in the interim). 

Notably, the Supreme Court of Virginia operates under discretionary review for nearly all cases, but the intermediate Court of Appeals of Virginia hears appeals as a matter of right only in family and administrative cases. The result is that there is "no" first appeal of right for the vast majority of civil and criminal cases in that state. Appellants are still free to petition for review, of course, but such petitions are subject to severe length constraints (6,125 words or 35 pages in Virginia) and necessarily are more narrowly targeted than a long opening appellate brief to an intermediate appellate court (in contrast, an opening brief to a California intermediate appellate court can run up to 14,000 words). In turn, the vast majority of decisions of Virginia circuit courts in civil and criminal cases are thereby insulated from appellate review on the merits.

Under American federalism, a state supreme court's ruling on a matter of purely state law is final and binding and must be accepted in both state and federal courts. However, when a case involves federal statutory or constitutional law, review of state supreme court decisions may be sought by way of a petition for writ of "certiorari" to the Supreme Court of the United States. The U.S. Supreme Court is the only federal court that has jurisdiction over direct appeals from state court decisions, although other federal courts are sometimes allowed "collateral review" of state cases in specific situations, for example regarding individuals on death row.

As the U.S. Supreme Court recognized in "Erie Railroad Co. v. Tompkins" (1938), no part of the federal Constitution actually grants federal courts or the federal Congress the power to directly dictate the content of state law (as distinguished from creating altogether separate federal law that in a particular situation may override state law). Clause 1 of Section 2 of Article Three of the United States Constitution describes the scope of federal judicial power, but only extended it to "the Laws of the United States" and not the laws of the several or individual states. It is this silence on that latter issue that gave rise to the American distinction between state and federal common law not found in other English-speaking common law federations like Australia and Canada.

In theory, state supreme courts are bound by the precedent established by the U.S. Supreme Court as to all issues of federal law, but in practice, the Supreme Court reviews very few decisions from state courts. For example, in 2007 the Court reviewed 244 cases appealed from federal courts and only 22 from state courts. Despite the relatively small number of decisions reviewed, Professors Sara Benesh and Wendy Martinek found that state supreme courts follow precedent more closely than federal courts in the area of search and seizure and appear to follow precedent in confessions as well.

State supreme court judges are selected in a variety of ways, with the method of selection often depending on the circumstances in which the seat is filled. Under one common method, the Missouri Plan, the governor fills judicial vacancies by choosing from a list compiled by a non-partisan commission. These judges serve an interim term until they stand in a retention election, in which they win a full term if a majority of voters vote for retention. Many other states elect judges through non-partisan elections in which multiple candidates appear on the ballot without their partisan affiliation listed. Most of the remaining states base their judicial selection system on gubernatorial appointments or partisan elections, although several states use a mix of different methods. South Carolina and Virginia use a system of legislative appointment, while in Vermont, the governor makes the initial appointment of judges, but the legislature has the power to re-appoint judges to new terms.

Various other factors can influence the appointment and re-appointment of state supreme court judges. Most judicial selection systems involving gubernatorial appointment make use of a nominating commission to recommend a list of candidates from which the governor must choose, but a minority of states allow the governor to nominate candidates even if they were not recommended by the commission. Many of the states that use gubernatorial appointment require the appointment to be confirmed by the state legislature or some other body, such as the Massachusetts Governor's Council. Although most states limit judicial terms to a set number of years, judges in Massachusetts and New Hampshire serve until they reach a mandatory retirement limit, while in Rhode Island, judges serve lifetime appointments. Most judges represent the entire state, but in Illinois, Kentucky, Louisiana, and Mississippi, judges represent districts of the state. Many states, including some states in which the governor is not otherwise involved in the appointment process, allow the governor to make interim appointments to fill judicial vacancies.

In many states with judicial elections, political contributions from groups such as trade associations and political action committees are allowed.

The various states provide different methods for the removal of state supreme court judges during their terms, with many states providing multiple methods. Two common methods of removal are impeachment by the state legislature, and removal by state judicial boards or commissions. Other states provide for the removal of judges through recall elections, court action, gubernatorial action (with legislative consent), or through a resolution passed by a super-majority in both houses of the state legislature.

Traditionally, state supreme courts are headquartered in the capital cities of their respective states, though they may occasionally hold oral arguments elsewhere. The seven main exceptions are:

As for the court's actual facilities, a state supreme court may be housed in the state capitol, in a nearby state office building shared with other courts or state executive branch agencies, or in a small courthouse reserved for its exclusive use. State supreme courts normally require a courtroom for oral argument, private chambers for all justices, a conference room, offices for law clerks and other support staff, a law library, and a lobby with a window where the court clerk can accept filings and release new decisions in the form of "slip opinions" (that is, in looseleaf format held together only by a staple).

Because state supreme courts generally hear only appeals, some courts have names which directly indicate their functionin the states of New York and Maryland, and in the District of Columbia, the highest court is called the "Court of Appeals". In New York, the "Supreme Court" is the trial court of general unlimited jurisdiction and the intermediate appellate court is called the "Supreme Court‚ÄîAppellate Division". Maryland's jury trial courts are called "Circuit Courts" (non-jury trials are usually conducted by the "District Courts," whose decisions may be appealed to the Circuit Courts), and the intermediate appellate court is called the "Court of Special Appeals". West Virginia mixes the two; its highest court is called the "Supreme Court of Appeals".

Other states' supreme courts have used the term "Appeals": New Jersey's supreme courts under the 1844 constitution and Delaware's supreme court were both the "Court of Errors and Appeals"; The term "Errors" refers to the now-obsolete writ of error, which was used by state supreme courts to correct certain types of egregious errors committed by lower courts.

Massachusetts and New Hampshire originally named their highest courts the "Superior Court of Judicature." Currently, Massachusetts uses the names "Supreme Judicial Court" (to distinguish itself from the state legislature, which is called the Massachusetts General Court), while New Hampshire uses the name "Supreme Court". Additionally the highest court in Maine is named the "Supreme Judicial Court". This similar terminology is probably a holdover from the time when Maine was part of Massachusetts. In Connecticut, Delaware, New Jersey, and New York, the highest courts formerly used variations of the term "Court of Errors," which indicated that the court's primary purpose was to correct the errors of lower courts.

States
Territories and federal district



</doc>
<doc id="28946" url="https://en.wikipedia.org/wiki?curid=28946" title="Stability">
Stability

Stability may refer to:










</doc>
<doc id="28951" url="https://en.wikipedia.org/wiki?curid=28951" title="Spyware">
Spyware

Spyware describes software with malicious behavior that aims to gather information about a person or organization and send such information to another entity in a way that harms the user; for example by violating their privacy or endangering their device's security. This behavior may be present in malware as well as in legitimate software. Websites may also engage in spyware behaviors like web tracking. Hardware devices may also be affected. Spyware is frequently associated with advertising and involves many of the same issues. Because these behaviors are so common, and can have non-harmful uses, providing a precise definition of spyware is a difficult task.

Spyware is mostly classified into four types: adware, system monitors, tracking including web tracking, and trojans; examples of other notorious types include digital rights management capabilities that "phone home", keyloggers, rootkits, and web beacons.

Spyware is mostly used for the stealing information and storing Internet users' movements on the Web and serving up pop-up ads to Internet users. Whenever spyware is used for malicious purposes, its presence is typically hidden from the user and can be difficult to detect. Some spyware, such as keyloggers, may be installed by the owner of a shared, corporate, or public computer intentionally in order to monitor users.

While the term "spyware" suggests software that monitors a user's computing, the functions of spyware can extend beyond simple monitoring. Spyware can collect almost any type of data, including personal information like internet surfing habits, user logins, and bank or credit account information. Spyware can also interfere with a user's control of a computer by installing additional software or redirecting web browsers. Some spyware can change computer settings, which can result in slow Internet connection speeds, un-authorized changes in browser settings, or changes to software settings.

Sometimes, spyware is included along with genuine software, and may come from a malicious website or may have been added to the intentional functionality of genuine software (see the paragraph about Facebook, below). In response to the emergence of spyware, a small industry has sprung up dealing in anti-spyware software. Running anti-spyware software has become a widely recognized element of computer security practices, especially for computers running Microsoft Windows. A number of jurisdictions have passed anti-spyware laws, which usually target any software that is surreptitiously installed to control a user's computer.

In German-speaking countries, spyware used or made by the government is called "govware" by computer experts (in common parlance: "Regierungstrojaner", literally "Government Trojan"). Govware is typically a trojan horse software used to intercept communications from the target computer. Some countries, like Switzerland and Germany, have a legal framework governing the use of such software. In the US, the term "policeware" has been used for similar purposes.

Use of the term "spyware" has eventually declined as the practice of tracking users has been pushed ever further into the mainstream by major websites and data mining companies; these generally break no known laws and compel users to be tracked, not by fraudulent practices "per se", but by the default settings created for users and the language of terms-of-service agreements. In one documented example, on CBS/CNet News reported, on March 7, 2011, on a "Wall Street Journal" analysis revealing the practice of Facebook and other websites of tracking users' browsing activity, linked to their identity, far beyond users' visits and activity within the Facebook site itself. The report stated: "Here's how it works. You go to Facebook, you log in, you spend some time there, and then ... you move on without logging out. Let's say the next site you go to is "New York Times". Those buttons, without you clicking on them, have just reported back to Facebook and Twitter that you went there and also your identity within those accounts. Let's say you moved on to something like a site about depression. This one also has a tweet button, a Google widget, and those, too, can report back who you are and that you went there." The" WSJ" analysis was researched by Brian Kennish, founder of Disconnect, Inc.

Spyware does not necessarily spread in the same way as a virus or worm because infected systems generally do not attempt to transmit or copy the software to other computers. Instead, spyware installs itself on a system by deceiving the user or by exploiting software vulnerabilities.

Most spyware is installed without knowledge, or by using deceptive tactics. Spyware may try to deceive users by bundling itself with desirable software. Other common tactics are using a Trojan horse, spy gadgets that look like normal devices but turn out to be something else, such as a USB Keylogger. These devices actually are connected to the device as memory units but are capable of recording each stroke made on the keyboard. Some spyware authors infect a system through security holes in the Web browser or in other software. When the user navigates to a Web page controlled by the spyware author, the page contains code which attacks the browser and forces the download and installation of spyware.

The installation of spyware frequently involves Internet Explorer. Its popularity and history of security issues have made it a frequent target. Its deep integration with the Windows environment make it susceptible to attack into the Windows operating system. Internet Explorer also serves as a point of attachment for spyware in the form of Browser Helper Objects, which modify the browser's behaviour.

A spyware rarely operates alone on a computer; an affected machine usually has multiple infections. Users frequently notice unwanted behavior and degradation of system performance. A spyware infestation can create significant unwanted CPU activity, disk usage, and network traffic. Stability issues, such as applications freezing, failure to boot, and system-wide crashes are also common. Spyware, which interferes with networking software commonly causes difficulty connecting to the Internet.

In some infections, the spyware is not even evident. Users assume in those situations that the performance issues relate to faulty hardware, Windows installation problems, or another malware infection. Some owners of badly infected systems resort to contacting technical support experts, or even buying a new computer because the existing system "has become too slow". Badly infected systems may require a clean reinstallation of all their software in order to return to full functionality.

Moreover, some types of spyware disable software firewalls and antivirus software, and/or reduce browser security settings, which opens the system to further opportunistic infections. Some spyware disables or even removes competing spyware programs, on the grounds that more spyware-related annoyances increase the likelihood that users will take action to remove the programs.

Keyloggers are sometimes part of malware packages downloaded onto computers without the owners' knowledge. Some keylogger software is freely available on the internet, while others are commercial or private applications. Most keyloggers allow not only keyboard keystrokes to be captured, they also are often capable of collecting screen captures from the computer.

A typical Windows user has administrative privileges, mostly for convenience. Because of this, any program the user runs has unrestricted access to the system. As with other operating systems, Windows users are able to follow the principle of least privilege and use non-administrator accounts. Alternatively, they can reduce the privileges of specific vulnerable Internet-facing processes, such as Internet Explorer.

Since Windows Vista is, by default, a computer administrator that runs everything under limited user privileges, when a program requires administrative privileges, a User Account Control pop-up will prompt the user to allow or deny the action. This improves on the design used by previous versions of Windows.

As the spyware threat has evolved, a number of techniques have emerged to counteract it. These include programs designed to remove or block spyware, as well as various user practices which reduce the chance of getting spyware on a system.

Nonetheless, spyware remains a costly problem. When a large number of pieces of spyware have infected a Windows computer, the only remedy may involve backing up user data, and fully reinstalling the operating system. For instance, some spyware cannot be completely removed by Symantec, Microsoft, PC Tools.

Many programmers and some commercial firms have released products dedicated to remove or block spyware. Programs such as PC Tools' Spyware Doctor, Lavasoft's "Ad-Aware SE" and Patrick Kolla's "Spybot - Search & Destroy" rapidly gained popularity as tools to remove, and in some cases intercept, spyware programs. On December 16, 2004, Microsoft acquired the "GIANT AntiSpyware" software, rebranding it as "Windows AntiSpyware beta" and releasing it as a free download for Genuine Windows XP and Windows 2003 users. (In 2006 it was renamed Windows Defender).

Major anti-virus firms such as Symantec, PC Tools, McAfee and Sophos have also added anti-spyware features to their existing anti-virus products. Early on, anti-virus firms expressed reluctance to add anti-spyware functions, citing lawsuits brought by spyware authors against the authors of web sites and programs which described their products as "spyware". However, recent versions of these major firms home and business anti-virus products do include anti-spyware functions, albeit treated differently from viruses. Symantec Anti-Virus, for instance, categorizes spyware programs as "extended threats" and now offers real-time protection against these threats.

Anti-spyware programs can combat spyware in two ways:
Such programs inspect the contents of the Windows registry, operating system files, and installed programs, and remove files and entries which match a list of known spyware. Real-time protection from spyware works identically to real-time anti-virus protection: the software scans disk files at download time, and blocks the activity of components known to represent spyware.
In some cases, it may also intercept attempts to install start-up items or to modify browser settings. Earlier versions of anti-spyware programs focused chiefly on detection and removal. Javacool Software's SpywareBlaster, one of the first to offer real-time protection, blocked the installation of ActiveX-based spyware.

Like most anti-virus software, many anti-spyware/adware tools require a frequently updated database of threats. As new spyware programs are released, anti-spyware developers discover and evaluate them, adding to the list of known spyware, which allows the software to detect and remove new spyware. As a result, anti-spyware software is of limited usefulness without regular updates. Updates may be installed automatically or manually.

A popular generic spyware removal tool used by those that requires a certain degree of expertise is HijackThis, which scans certain areas of the Windows OS where spyware often resides and presents a list with items to delete manually. As most of the items are legitimate windows files/registry entries it is advised for those who are less knowledgeable on this subject to post a HijackThis log on the numerous antispyware sites and let the experts decide what to delete.

If a spyware program is not blocked and manages to get itself installed, it may resist attempts to terminate or uninstall it. Some programs work in pairs: when an anti-spyware scanner (or the user) terminates one running process, the other one respawns the killed program. Likewise, some spyware will detect attempts to remove registry keys and immediately add them again. Usually, booting the infected computer in safe mode allows an anti-spyware program a better chance of removing persistent spyware. Killing the process tree may also work.

To detect spyware, computer users have found several practices useful in addition to installing anti-spyware programs. Many users have installed a web browser other than Internet Explorer, such as Mozilla Firefox or Google Chrome. Though no browser is completely safe, Internet Explorer was once at a greater risk for spyware infection due to its large user base as well as vulnerabilities such as ActiveX but these three major browsers are now close to equivalent when it comes to security.

Some ISPs‚Äîparticularly colleges and universities‚Äîhave taken a different approach to blocking spyware: they use their network firewalls and web proxies to block access to Web sites known to install spyware. On March 31, 2005, Cornell University's Information Technology department released a report detailing the behavior of one particular piece of proxy-based spyware, "Marketscore", and the steps the university took to intercept it. Many other educational institutions have taken similar steps.

Individual users can also install firewalls from a variety of companies. These monitor the flow of information going to and from a networked computer and provide protection against spyware and malware. Some users install a large hosts file which prevents the user's computer from connecting to known spyware-related web addresses. Spyware may get installed via certain shareware programs offered for download. Downloading programs only from reputable sources can provide some protection from this source of attack.

Individual users can use cellphone / computer with physical (electric) switch, or isolated electronic switch that disconnects microphone, camera without bypass and keep it in disconnected position where not in use, that limits information that spyware can collect. (Policy recommended by NIST Guidelines for Managing the Security of Mobile Devices, 2013).

A few spyware vendors, notably 180 Solutions, have written what the "New York Times" has dubbed "stealware", and what spyware researcher Ben Edelman terms "affiliate fraud", a form of click fraud. Stealware diverts the payment of affiliate marketing revenues from the legitimate affiliate to the spyware vendor.

Spyware which attacks affiliate networks places the spyware operator's affiliate tag on the user's activity ‚Äì replacing any other tag, if there is one. The spyware operator is the only party that gains from this. The user has their choices thwarted, a legitimate affiliate loses revenue, networks' reputations are injured, and vendors are harmed by having to pay out affiliate revenues to an "affiliate" who is not party to a contract. Affiliate fraud is a violation of the terms of service of most affiliate marketing networks. As a result, spyware operators such as 180 Solutions have been terminated from affiliate networks including LinkShare and ShareSale. Mobile devices can also be vulnerable to chargeware, which manipulates users into illegitimate mobile charges.

In one case, spyware has been closely associated with identity theft. In August 2005, researchers from security software firm Sunbelt Software suspected the creators of the common CoolWebSearch spyware had used it to transmit "chat sessions, user names, passwords, bank information, etc."; however it turned out that "it actually (was) its own sophisticated criminal little trojan that's independent of CWS." This case is currently under investigation by the FBI.

The Federal Trade Commission estimates that 27.3¬†million Americans have been victims of identity theft, and that financial losses from identity theft totaled nearly $48¬†billion for businesses and financial institutions and at least $5¬†billion in out-of-pocket expenses for individuals.

Some copy-protection technologies have borrowed from spyware. In 2005, Sony BMG Music Entertainment was found to be using rootkits in its XCP digital rights management technology Like spyware, not only was it difficult to detect and uninstall, it was so poorly written that most efforts to remove it could have rendered computers unable to function.
Texas Attorney General Greg Abbott filed suit, and three separate class-action suits were filed. Sony BMG later provided a workaround on its website to help users remove it.

Beginning on April 25, 2006, Microsoft's Windows Genuine Advantage Notifications application was installed on most Windows PCs as a "critical security update". While the main purpose of this deliberately uninstallable application is to ensure the copy of Windows on the machine was lawfully purchased and installed, it also installs software that has been accused of "phoning home" on a daily basis, like spyware. It can be removed with the RemoveWGA tool.

Stalkerware is spyware that has been used to monitor electronic activities of partners in intimate relationships. At least one software package, Loverspy, was specifically marketed for this purpose. Depending on local laws regarding communal/marital property, observing a partner's online activity without their consent may be illegal; the author of Loverspy and several users of the product were indicted in California in 2005 on charges of wiretapping and various computer crimes.

Anti-spyware programs often report Web advertisers' HTTP cookies, the small text files that track browsing activity, as spyware. While they are not always inherently malicious, many users object to third parties using space on their personal computers for their business purposes, and many anti-spyware programs offer to remove them.

These common spyware programs illustrate the diversity of behaviors found in these attacks. Note that as with computer viruses, researchers give names to spyware programs which may not be used by their creators. Programs may be grouped into "families" based not on shared program code, but on common behaviors, or by "following the money" of apparent financial or business connections. For instance, a number of the spyware programs distributed by Claria are collectively known as "Gator". Likewise, programs that are frequently installed together may be described as parts of the same spyware package, even if they function separately.

The first recorded use of the term spyware occurred on October 16, 1995 in a Usenet post that poked fun at Microsoft's business model. "Spyware" at first denoted "software" meant for espionage purposes. However, in early 2000 the founder of Zone Labs, Gregor Freund, used the term in a press release for the ZoneAlarm Personal Firewall. Later in 2000, a parent using ZoneAlarm was alerted to the fact that "Reader Rabbit," educational software marketed to children by the Mattel toy company, was surreptitiously sending data back to Mattel. Since then, "spyware" has taken on its present sense.

According to a 2005 study by AOL and the National Cyber-Security Alliance, 61 percent of surveyed users' computers were infected with form of spyware. 92 percent of surveyed users with spyware reported that they did not know of its presence, and 91 percent reported that they had not given permission for the installation of the spyware.
, spyware has become one of the preeminent security threats to computer systems running Microsoft Windows operating systems. Computers on which Internet Explorer (IE) is the primary browser are particularly vulnerable to such attacks, not only because IE is the most widely used, but because its tight integration with Windows allows spyware access to crucial parts of the operating system.

Before Internet Explorer 6 SP2 was released as part of Windows XP Service Pack 2, the browser would automatically display an installation window for any ActiveX component that a website wanted to install. The combination of user ignorance about these changes, and the assumption by Internet Explorer that all ActiveX components are benign, helped to spread spyware significantly. Many spyware components would also make use of exploits in JavaScript, Internet Explorer and Windows to install without user knowledge or permission.

The Windows Registry contains multiple sections where modification of key values allows software to be executed automatically when the operating system boots. Spyware can exploit this design to circumvent attempts at removal. The spyware typically will link itself from each location in the registry that allows execution. Once running, the spyware will periodically check if any of these links are removed. If so, they will be automatically restored. This ensures that the spyware will execute when the operating system is booted, even if some (or most) of the registry links are removed.



Malicious programmers have released a large number of rogue (fake) anti-spyware programs, and widely distributed Web banner ads can warn users that their computers have been infected with spyware, directing them to purchase programs which do not actually remove spyware‚Äîor else, may add more spyware of their own.

The proliferation of fake or spoofed antivirus products that bill themselves as antispyware can be troublesome. Users may receive popups prompting them to install them to protect their computer, when it will in fact add spyware. This software is called rogue software. It is recommended that users do not install any freeware claiming to be anti-spyware unless it is verified to be legitimate. Some known offenders include:

Fake antivirus products constitute 15 percent of all malware.

On January 26, 2006, Microsoft and the Washington state attorney general filed suit against Secure Computer for its Spyware Cleaner product.

Unauthorized access to a computer is illegal under computer crime laws, such as the U.S. Computer Fraud and Abuse Act, the U.K.'s Computer Misuse Act, and similar laws in other countries. Since owners of computers infected with spyware generally claim that they never authorized the installation, a "prima facie" reading would suggest that the promulgation of spyware would count as a criminal act. Law enforcement has often pursued the authors of other malware, particularly viruses. However, few spyware developers have been prosecuted, and many operate openly as strictly legitimate businesses, though some have faced lawsuits.

Spyware producers argue that, contrary to the users' claims, users do in fact give consent to installations. Spyware that comes bundled with shareware applications may be described in the legalese text of an end-user license agreement (EULA). Many users habitually ignore these purported contracts, but spyware companies such as Claria say these demonstrate that users have consented.

Despite the ubiquity of EULAs agreements, under which a single click can be taken as consent to the entire text, relatively little caselaw has resulted from their use. It has been established in most common law jurisdictions that this type of agreement can be a binding contract "in certain circumstances." This does not, however, mean that every such agreement is a contract, or that every term in one is enforceable.

Some jurisdictions, including the U.S. states of Iowa and Washington, have passed laws criminalizing some forms of spyware. Such laws make it illegal for anyone other than the owner or operator of a computer to install software that alters Web-browser settings, monitors keystrokes, or disables computer-security software.

In the United States, lawmakers introduced a bill in 2005 entitled the Internet Spyware Prevention Act, which would imprison creators of spyware.

The US Federal Trade Commission has sued Internet marketing organizations under the "unfairness doctrine" to make them stop infecting consumers' PCs with spyware. In one case, that against Seismic Entertainment Productions, the FTC accused the defendants of developing a program that seized control of PCs nationwide, infected them with spyware and other malicious software, bombarded them with a barrage of pop-up advertising for Seismic's clients, exposed the PCs to security risks, and caused them to malfunction. Seismic then offered to sell the victims an "antispyware" program to fix the computers, and stop the popups and other problems that Seismic had caused. On November 21, 2006, a settlement was entered in federal court under which a $1.75¬†million judgment was imposed in one case and $1.86¬†million in another, but the defendants were insolvent

In a second case, brought against CyberSpy Software LLC, the FTC charged that CyberSpy marketed and sold "RemoteSpy" keylogger spyware to clients who would then secretly monitor unsuspecting consumers' computers. According to the FTC, Cyberspy touted RemoteSpy as a "100% undetectable" way to "Spy on Anyone. From Anywhere." The FTC has obtained a temporary order prohibiting the defendants from selling the software and disconnecting from the Internet any of their servers that collect, store, or provide access to information that this software has gathered. The case is still in its preliminary stages. A complaint filed by the Electronic Privacy Information Center (EPIC) brought the RemoteSpy software to the FTC's attention.

An administrative fine, the first of its kind in Europe, has been issued by the Independent Authority of Posts and Telecommunications (OPTA) from the Netherlands. It applied fines in total value of Euro 1,000,000 for infecting 22 million computers. The spyware concerned is called DollarRevenue. The law articles that have been violated are art. 4.1 of the Decision on universal service providers and on the interests of end users; the fines have been issued based on art. 15.4 taken together with art. 15.10 of the Dutch telecommunications law.

Former New York State Attorney General and former Governor of New York Eliot Spitzer has pursued spyware companies for fraudulent installation of software. In a suit brought in 2005 by Spitzer, the California firm Intermix Media, Inc. ended up settling, by agreeing to pay US$7.5¬†million and to stop distributing spyware.

The hijacking of Web advertisements has also led to litigation. In June 2002, a number of large Web publishers sued Claria for replacing advertisements, but settled out of court.

Courts have not yet had to decide whether advertisers can be held liable for spyware that displays their ads. In many cases, the companies whose advertisements appear in spyware pop-ups do not directly do business with the spyware firm. Rather, they have contracted with an advertising agency, which in turn contracts with an online subcontractor who gets paid by the number of "impressions" or appearances of the advertisement. Some major firms such as Dell Computer and Mercedes-Benz have sacked advertising agencies that have run their ads in spyware.

Litigation has gone both ways. Since "spyware" has become a common pejorative, some makers have filed libel and defamation actions when their products have been so described. In 2003, Gator (now known as Claria) filed suit against the website PC Pitstop for describing its program as "spyware". PC Pitstop settled, agreeing not to use the word "spyware", but continues to describe harm caused by the Gator/Claria software. As a result, other anti-spyware and anti-virus companies have also used other terms such as "potentially unwanted programs" or greyware to denote these products.

In the 2010 WebcamGate case, plaintiffs charged two suburban Philadelphia high schools secretly spied on students by surreptitiously and remotely activating webcams embedded in school-issued laptops the students were using at home, and therefore infringed on their privacy rights. The school loaded each student's computer with LANrev's remote activation tracking software. This included the now-discontinued "TheftTrack". While TheftTrack was not enabled by default on the software, the program allowed the school district to elect to activate it, and to choose which of the TheftTrack surveillance options the school wanted to enable.

TheftTrack allowed school district employees to secretly remotely activate the webcam embedded in the student's laptop, above the laptop's screen. That allowed school officials to secretly take photos through the webcam, of whatever was in front of it and in its line of sight, and send the photos to the school's server. The LANrev software disabled the webcams for all other uses ("e.g.", students were unable to use Photo Booth or video chat), so most students mistakenly believed their webcams did not work at all. In addition to webcam surveillance, TheftTrack allowed school officials to take screenshots, and send them to the school's server. In addition, LANrev allowed school officials to take snapshots of instant messages, web browsing, music playlists, and written compositions. The schools admitted to secretly snapping over 66,000 webshots and screenshots, including webcam shots of students in their bedrooms.





</doc>
<doc id="28952" url="https://en.wikipedia.org/wiki?curid=28952" title="William Jones (philologist)">
William Jones (philologist)

Sir William Jones FRS FRSE (28 September 1746 ‚Äì 27 April 1794) was an Anglo-Welsh philologist, a puisne judge on the Supreme Court of Judicature at Fort William in Bengal, and a scholar of ancient India, particularly known for his proposition of the existence of a relationship among European and Indo-Aryan languages, which he coined as Indo-European.

Jones is also credited for establishing the Asiatic Society of Bengal in the year 1784.

William Jones was born in London at Beaufort Buildings, Westminster; his father William Jones (1675‚Äì1749) was a mathematician from Anglesey in Wales, noted for introducing the use of the symbol œÄ. The young William Jones was a linguistic prodigy, who in addition to his native languages English and Welsh, learned Greek, Latin, Persian, Arabic, Hebrew and the basics of Chinese writing at an early age. By the end of his life he knew eight languages with critical thoroughness, was fluent in a further eight, with a dictionary at hand, and had a fair competence in another twelve.

Jones' father died when he was aged three, and his mother Mary Nix Jones raised him. He was sent to Harrow School in September 1753 and then went on to University College, Oxford. He graduated there in 1768 and became M.A. in 1773. Financially constrained, he took a position tutoring the seven-year-old Lord Althorp, son of Earl Spencer. For the next six years he worked as a tutor and translator. During this time he published "Histoire de Nader Chah" (1770), a French translation of a work originally written in Persian by Mirza Mehdi Khan Astarabadi. This was done at the request of King Christian VII of Denmark: he had visited Jones, who by the age of 24 had already acquired a reputation as an orientalist. This would be the first of numerous works on Persia, Turkey, and the Middle East in general.

In 1770, Jones joined the Middle Temple and studied law for three years, a preliminary to his life-work in India. He was elected a Fellow of the Royal Society on 30 April 1772. After a spell as a circuit judge in Wales, and a fruitless attempt to resolve the conflict that eventually led to the American Revolution in concert with Benjamin Franklin in Paris, he was appointed puisne judge to the Supreme Court of Judicature at Fort William in Calcutta, Bengal on 4 March 1783, and on 20 March he was knighted. In April 1783 he married Anna Maria Shipley, the eldest daughter of Dr. Jonathan Shipley, Bishop of Llandaff and Bishop of St Asaph. Anna Maria used her artistic skills to help Jones document life in India. On 25 September 1783 he arrived in Calcutta.

Jones was a radical political thinker, a friend of American independence. His work, "The principles of government; in a dialogue between a scholar and a peasant" (1783), was the subject of a trial for seditious libel after it was reprinted by his brother-in-law William Shipley.

In the Subcontinent he was entranced by Indian culture, an as-yet untouched field in European scholarship, and on 15 January 1784 he founded the Asiatic Society in Calcutta and started a journal called "Asiatick Researches". He studied the Vedas with RƒÅmalocana, a pandit teaching at the Nadiya Hindu university, becoming a proficient Sanskritist. Jones kept up a ten-year correspondence on the topic of "jyotisa" or Hindu astronomy with fellow orientalist Samuel Davis. He learnt the ancient concept of Hindu Laws from Pandit Jagannath Tarka Panchanan.

Over the next ten years he would produce a flood of works on India, launching the modern study of the subcontinent in virtually every social science. He also wrote on the local laws, music, literature, botany, and geography, and made the first English translations of several important works of Indian literature.

Sir William Jones sometimes also went by the nom de plume Youns Uksfardi (€åŸàŸÜÿ≥ ÿßŸà⁄©ÿ≥ŸÅÿ±ÿØ€å, "Jones of Oxford"). This pen name can be seen on the inner front cover of his "Persian Grammar" published in 1771 (and in subsequent editions).

He died in Calcutta on 27 April 1794 at the age of 47 and is buried in South Park Street Cemetery.

Jones is known today for making and propagating the observation about relationships between the Indo-European languages. In his "Third Anniversary Discourse " to the Asiatic Society (1786) he suggested that Sanskrit, Greek and Latin languages had a common root, and that indeed they may all be further related, in turn, to Gothic and the Celtic languages, as well as to Persian.
Although his name is closely associated with this observation, he was not the first to make it. In the 16th century, European visitors to India became aware of similarities between Indian and European languages and as early as 1653 Van Boxhorn had published a proposal for a proto-language ("Scythian") for Germanic, Romance, Greek, Baltic, Slavic, Celtic and Iranian. Finally, in a memoir sent to the French Academy of Sciences in 1767 Gaston-Laurent Coeurdoux, a French Jesuit who spent all his life in India, had specifically demonstrated the existing analogy between Sanskrit and European languages. In 1786 Jones postulated a proto-language uniting Sanskrit, Iranian, Greek, Latin, Germanic and Celtic, but in many ways his work was less accurate than his predecessors', as he erroneously included Egyptian, Japanese and Chinese in the Indo-European languages, while omitting Hindustani and Slavic. Jones also erroneously suggested that Sanskrit ‚Äòwas introduced [to north India] by conquerors from other kingdoms in some very remote age‚Äô displacing ‚Äòthe pure Hindi‚Äô of north India .
Nevertheless, Jones' third annual discourse before the Asiatic Society on the history and culture of the Hindus (delivered on 2 February 1786 and published in 1788) with the famed "philologer" passage is often cited as the beginning of comparative linguistics and Indo-European studies.

This common source came to be known as Proto-Indo-European.

Jones was the first to propose a racial division of India involving an Aryan invasion but at that time there was insufficient evidence to support it. It was an idea later taken up by British administrators such as Herbert Hope Risley but remains disputed today.

Jones also propounded theories that might appear peculiar today but were less so in his time. For example, he believed that Egyptian priests had migrated and settled down in India in prehistoric times. He also posited that the Chinese were originally Hindus belonging to the Kshatriya caste.

Jones, in his 1772 'Essay on the Arts called Imitative', was one of the first to propound an expressive theory of poetry, valorising expression over description or imitation: "If the arguments, used in this essay, have any weight, it will appear, that the finest parts of poetry, musick, and painting, are expressive of the passions...the inferior parts of them are descriptive of natural objects". He thereby anticipated Wordsworth in grounding poetry on the basis of a Romantic subjectivity.

Jones was a contributor to Hyde's Notebooks during his term on the bench of the Supreme Court of Judicature. The notebooks are a valuable primary source of information for life in late 18th century Bengal and are the only remaining source for the proceedings of the Supreme Court.

In Europe a discussion as to the authenticity of the first translation of the Avesta scriptures arose. It was the first evidence of an Indo-European language as old as Sanskrit to be translated into a modern European language. It was suggested that the so-called Zend-Avesta was not the genuine work of the prophet Zoroaster, but was a recent forgery. Foremost among the detractors, it is to be regretted, was the distinguished (though young) orientalist William Jones. He claimed, in a letter published in French (1771), that the translator Anquetil-Duperron had been duped, that the Parsis of Surat had palmed off upon him a conglomeration of worthless fabrications and absurdities. In England, Jones was supported by Richardson and Sir John Chardin; in Germany, by Meiners. Anquetil-Duperron was labelled an impostor who had invented his own script to support his claim. This debate was not settled for almost a century.

It is not curious that Jones didn't include Iranian in his naming of the cluster of Indo-European languages, since he hadn't any idea about the relationship between Avestan and Sanskrit as two main branches of this language group.

In 1763, at the age of 17, Jones wrote the poem "Caissa", based on a 658-line poem called "Scacchia, Ludus" published in 1527 by Marco Girolamo Vida, giving a mythical origin of chess that has become well known in the chess world. This poem he wrote in English.

In the poem the nymph Caissa initially repels the advances of Mars, the god of war. Spurned, Mars seeks the aid of the god of sport, who creates the game of chess as a gift for Mars to win Caissa's favour. Mars wins her over with the game.

Caissa has since been characterised as the "goddess" of chess, her name being used in several contexts in modern chess playing.

Arthur Schopenhauer referred to one of Sir William Jones's publications in ¬ß1 of "The World as Will and Representation" (1819). Schopenhauer was trying to support the doctrine that "everything that exists for knowledge, and hence the whole of this world, is only object in relation to the subject, perception of the perceiver, in a word, representation." He quoted Jones's original English:
... how early this basic truth was recognized by the sages of India, since it appears as the fundamental tenet of the Ved√¢nta philosophy ascribed to Vyasa, is proved by Sir William Jones in the last of his essays: "On the Philosophy of the Asiatics" ("Asiatic Researches", vol. IV, p. 164): "The fundamental tenet of the Ved√¢nta school consisted not in denying the existence of matter, that is solidity, impenetrability, and extended figure (to deny which would be lunacy), but in correcting the popular notion of it, and in contending that it has no essence independent of mental perception; that existence and perceptibility are convertible terms."

Schopenhauer used Jones's authority to relate the basic principle of his philosophy to what was, according to Jones, the most important underlying proposition of Ved√¢nta. He made more passing reference to Sir William Jones's writings elsewhere in his works.

On 28 September 1822 the Dutch orientalist Hendrik Arent Hamaker, who accepted a professorship at the University of Leiden, gave his inaugural lecture in Latin "De vita et meritis Guilielmi Jonesii (The Life and Works of William Jones)(Leiden, 1823).

Edgar Allan Poe's short story "Berenice" starts with a motto, the first half of a poem, by Ibn Zaiat: "Dicebant mihi sodales si sepulchrum amicae visitarem, curas meas aliquantulum fore levatas." It was taken from the works of William Jones, and here is the missing part (from Complete Works, Vol. 2, London, 1799): 
"Dixi autem, an ideo aliud praeter hoc pectus habet sepulchrum?"

My companions said to me, if I would visit the grave of my friend, I might somewhat alleviate my worries. I answered "could she be buried elsewhere than in my heart?"

Listing in most cases only editions and reprints that came out during Jones's own lifetime, books by, or prominently including work by, William Jones, are:






</doc>
<doc id="28953" url="https://en.wikipedia.org/wiki?curid=28953" title="Stephen, King of England">
Stephen, King of England

Stephen (1092 or 1096 ‚Äì 25 October 1154), often referred to as Stephen of Blois, was King of England from 22 December 1135 to his death. He was Count of Boulogne from 1125 until 1147 and Duke of Normandy from 1135 until 1144. His reign was marked by the Anarchy, a civil war with his cousin and rival, the Empress Matilda, whose son, Henry II, succeeded Stephen as the first of the Angevin kings of England.

Stephen was born in the County of Blois in central France; his father, Count Stephen-Henry, died while Stephen was still young, and he was brought up by his mother, Adela, daughter of William the Conqueror. Placed into the court of his uncle, Henry I of England, Stephen rose in prominence and was granted extensive lands. He married Matilda of Boulogne, inheriting additional estates in Kent and Boulogne that made the couple one of the wealthiest in England. Stephen narrowly escaped drowning with Henry I's son, William Adelin, in the sinking of the "White Ship" in 1120; William's death left the succession of the English throne open to challenge. When Henry died in 1135, Stephen quickly crossed the English Channel and with the help of his brother Henry, Bishop of Winchester and Abbot of Glastonbury, took the throne, arguing that the preservation of order across the kingdom took priority over his earlier oaths to support the claim of Henry I's daughter, the Empress Matilda.

The early years of Stephen's reign were largely successful, despite a series of attacks on his possessions in England and Normandy by David I of Scotland, Welsh rebels, and the Empress Matilda's husband Geoffrey Plantagenet, Count of Anjou. In 1138, the Empress's half-brother Robert of Gloucester rebelled against Stephen, threatening civil war. Together with his close advisor, Waleran de Beaumont, Stephen took firm steps to defend his rule, including arresting a powerful family of bishops. When the Empress and Robert invaded in 1139, Stephen was unable to crush the revolt rapidly, and it took hold in the south-west of England. Captured at the battle of Lincoln in 1141, he was abandoned by many of his followers and lost control of Normandy. He was freed only after his wife and William of Ypres, one of his military commanders, captured Robert at the Rout of Winchester, but the war dragged on for many years with neither side able to win an advantage.

Stephen became increasingly concerned with ensuring that his son Eustace would inherit his throne. The King tried to convince the Church to agree to crown Eustace to reinforce his claim; Pope Eugene III refused, and Stephen found himself in a sequence of increasingly bitter arguments with his senior clergy. In 1153, the Empress's son Henry invaded England and built an alliance of powerful regional barons to support his claim for the throne. The two armies met at Wallingford, but neither side's barons were keen to fight another pitched battle. Stephen began to examine a negotiated peace, a process hastened by the sudden death of Eustace. Later in the year Stephen and Henry agreed to the Treaty of Winchester, in which Stephen recognised Henry as his heir in exchange for peace, passing over William, Stephen's second son. Stephen died the following year. Modern historians have extensively debated the extent to which his personality, external events, or the weaknesses in the Norman state contributed to this prolonged period of civil war.

Stephen was born in Blois, France, in either 1092 or 1096. His father was Stephen-Henry, Count of Blois and Chartres, an important French nobleman, and an active crusader, who played only a brief part in Stephen's early life. During the First Crusade Stephen-Henry had acquired a reputation for cowardice, and he returned to the Levant again in 1101 to rebuild his reputation; there he was killed at the battle of Ramlah. Stephen's mother, Adela, was the daughter of William the Conqueror and Matilda of Flanders, famous amongst her contemporaries for her piety, wealth and political talent. She had a strong matriarchal influence on Stephen during his early years.

France in the 12th century was a loose collection of counties and smaller polities, under the minimal control of the King of France. The King's power was linked to his control of the rich province of √éle-de-France, just to the east of Stephen's home county of Blois. In the west lay the three counties of Maine, Anjou and Touraine, and to the north of Blois was the Duchy of Normandy, from which William the Conqueror had conquered England in 1066. William's children were still fighting over the collective Anglo-Norman inheritance. The rulers across this region spoke a similar language, albeit with regional dialects, followed the same religion, and were closely interrelated; they were also highly competitive and frequently in conflict with one another for valuable territory and the castles that controlled them.

Stephen had at least four brothers and one sister, along with two probable half-sisters. His eldest brother was William, who under normal circumstances would have ruled Blois and Chartres. William was probably intellectually disabled, and Adela instead had the counties pass to her second son, later also Count Theobald II of Champagne. Stephen's remaining older brother, Odo, died young, probably in his early teens. His younger brother, Henry of Blois, was probably born four years after him. The brothers formed a close-knit family group, and Adela encouraged Stephen to take up the role of a feudal knight, whilst steering Henry towards a career in the church, possibly so that their personal career interests would not overlap. Unusually, Stephen was raised in his mother's household rather than being sent to a close relative; he was taught Latin and riding, and was educated in recent history and Biblical stories by his tutor, William the Norman.

Stephen's early life was heavily influenced by his relationship with his uncle Henry I. Henry seized power in England following the death of his elder brother William Rufus. In 1106 he invaded and captured the Duchy of Normandy, controlled by his eldest brother, Robert Curthose, defeating Robert's army at the battle of Tinchebray. Henry then found himself in conflict with Louis VI of France, who took the opportunity to declare Robert's son William Clito the Duke of Normandy. Henry responded by forming a network of alliances with the western counties of France against Louis, resulting in a regional conflict that would last throughout Stephen's early life. Adela and Theobald allied themselves with Henry, and Stephen's mother decided to place him in Henry's court. Henry fought his next military campaign in Normandy, from 1111 onwards, where rebels led by Robert of Bell√™me were opposing his rule. Stephen was probably with Henry during the military campaign of 1112, when he was knighted by the King. He was present at court during the King's visit to the Abbey of Saint-Evroul in 1113. Stephen probably first visited England in either 1113 or 1115, almost certainly as part of Henry's court.

Henry became a powerful patron of Stephen, and probably chose to support him because Stephen was part of his extended family and a regional ally, yet not sufficiently wealthy or powerful in his own right to represent a threat to either the King or his heir, William Adelin. As a third surviving son, even of an influential regional family, Stephen still needed the support of a powerful patron to progress in life. With Henry's support, he rapidly began to accumulate lands and possessions. Following the battle of Tinchebray in 1106, Henry confiscated the County of Mortain from his cousin William, and the Honour of Eye, a large lordship previously owned by Robert Malet. In 1113, Stephen was granted both the title and the honour, although without the lands previously held by William in England. The gift of the Honour of Lancaster also followed after it was confiscated by Henry from Roger the Poitevin. Stephen was also given lands in Alen√ßon in southern Normandy by Henry, but the local Normans rebelled, seeking assistance from Fulk IV, Count of Anjou. Stephen and his older brother Theobald were comprehensively beaten in the subsequent campaign, which culminated in the battle of Alen√ßon, and the territories were not recovered.

Finally, the King arranged for Stephen to marry Matilda in 1125, the daughter and only heiress of Eustace III, Count of Boulogne, who owned both the important continental port of Boulogne and vast estates in the north-west and south-east of England. In 1127, William Clito, a potential claimant to the English throne, seemed likely to become the Count of Flanders; Stephen was sent by the King on a mission to prevent this, and in the aftermath of his successful election, William Clito attacked Stephen's lands in neighbouring Boulogne in retaliation. Eventually a truce was declared, and William Clito died the following year.

In 1120, the English political landscape changed dramatically. Three hundred passengers embarked on the "White Ship" to travel from Barfleur in Normandy to England, including the heir to the throne, William Adelin, and many other senior nobles. Stephen had intended to sail on the same ship but changed his mind at the last moment and got off to await another vessel, either out of concern for overcrowding on board the ship, or because he was suffering from diarrhea. The ship foundered en route, and all but two of the passengers died, including William Adelin.

With Adelin dead, the inheritance to the English throne was thrown into doubt. Rules of succession in western Europe at the time were uncertain; in some parts of France, male primogeniture, in which the eldest son would inherit a title, was becoming more popular. It was also traditional for the King of France to crown his successor whilst he himself was still alive, making the intended line of succession relatively clear, but this was not the case in England. In other parts of Europe, including Normandy and England, the tradition was for lands to be divided up, with the eldest son taking patrimonial lands‚Äîusually considered to be the most valuable‚Äîand younger sons being given smaller, or more recently acquired, partitions or estates. The problem was further complicated by the sequence of unstable Anglo-Norman successions over the previous sixty years‚ÄîWilliam the Conqueror had gained England by force, William Rufus and Robert Curthose had fought a war between them to establish their inheritance, and Henry had only acquired control of Normandy by force. There had been no peaceful, uncontested successions.

With William Adelin dead, Henry had only one other legitimate child, Empress Matilda, but as a woman she was at a substantial political disadvantage. Despite the King taking a second wife, Adeliza of Louvain, it became increasingly unlikely that he would have another legitimate son, and he instead looked to Matilda as his intended heir. Matilda claimed the title of Holy Roman Empress through her marriage to Emperor Henry V, but her husband died in 1125, and she was remarried in 1128 to Geoffrey Plantagenet, Count of Anjou, whose lands bordered the Duchy of Normandy. Geoffrey was unpopular with the Anglo-Norman elite: as an Angevin ruler, he was a traditional enemy of the Normans. At the same time, tensions continued to grow as a result of Henry's domestic policies, in particular the high level of revenue he was raising to pay for his various wars. Conflict was curtailed, however, by the power of the King's personality and reputation.

Henry attempted to build up a base of political support for Matilda in both England and Normandy, demanding that his court take oaths first in 1127, and then again in 1128 and 1131, to recognise Matilda as his immediate successor and recognise her descendants as the rightful rulers after her. Stephen was amongst those who took this oath in 1127. Nonetheless, relations between Henry, Matilda, and Geoffrey became increasingly strained towards the end of the King's life. Matilda and Geoffrey suspected that they lacked genuine support in England, and proposed to Henry in 1135 that the King should hand over the royal castles in Normandy to Matilda whilst he was still alive and insist on the Norman nobility swearing immediate allegiance to her, thereby giving the couple a much more powerful position after Henry's death. Henry angrily declined to do so, probably out of a concern that Geoffrey would try to seize power in Normandy somewhat earlier than intended. A fresh rebellion broke out in southern Normandy, and Geoffrey and Matilda intervened militarily on behalf of the rebels. In the middle of this confrontation, Henry unexpectedly fell ill and died near Lyons-la-For√™t.

Stephen was a well established figure in Anglo-Norman society by 1135. He was extremely wealthy, well-mannered and liked by his peers; he was also considered a man capable of firm action. Chroniclers recorded that despite his wealth and power he was a modest and easy-going leader, happy to sit with his men and servants, casually laughing and eating with them. He was very pious, both in terms of his observance of religious rituals and his personal generosity to the church. Stephen also had a personal Augustinian confessor appointed to him by the Archbishop of Canterbury, who implemented a penitential regime for him, and Stephen encouraged the new order of Cistercians to form abbeys on his estates, winning him additional allies within the church.

Rumours about his father's cowardice during the First Crusade, however, continued to circulate, and a desire to avoid the same reputation may have influenced some of Stephen's rasher military actions. His wife, Matilda, played a major role in running their vast English estates, which contributed to the couple being the second-richest lay household in the country after the King and Queen. The landless Flemish nobleman William of Ypres had joined Stephen's household in 1133.

Stephen's younger brother, Henry of Blois, had also risen to power under Henry I. Henry of Blois had become a Cluniac monk and followed Stephen to England, where the King made him Abbot of Glastonbury, the richest abbey in England. The King then appointed him Bishop of Winchester, one of the richest bishoprics, allowing him to retain Glastonbury as well. The combined revenues of the two positions made Henry of Winchester the second-richest man in England after the King. Henry of Winchester was keen to reverse what he perceived as encroachment by the Norman kings on the rights of the church. The Norman kings had traditionally exercised a great deal of power and autonomy over the church within their territories. From the 1040s onwards, however, successive popes had put forward a reforming message that emphasised the importance of the church being "governed more coherently and more hierarchically from the centre" and established "its own sphere of authority and jurisdiction, separate from and independent of that of the lay ruler", in the words of historian Richard Huscroft.
When news began to spread of Henry I's death, many of the potential claimants to the throne were not well placed to respond. Geoffrey and Matilda were in Anjou, rather awkwardly supporting the rebels in their campaign against the royal army, which included a number of Matilda's supporters such as Robert of Gloucester. Many of these barons had taken an oath to stay in Normandy until the late King was properly buried, which prevented them from returning to England. Stephen's elder brother Theobald was further south still, in Blois. Stephen, however, was in Boulogne, and when news reached him of Henry's death he left for England, accompanied by his military household. Robert of Gloucester had garrisoned the ports of Dover and Canterbury and some accounts suggest that they refused Stephen access when he first arrived. Nonetheless Stephen probably reached his own estate on the edge of London by 8 December and over the next week he began to seize power in England.

The crowds in London traditionally claimed a right to elect the King, and they proclaimed Stephen the new monarch, believing that he would grant the city new rights and privileges in return. Henry of Blois delivered the support of the church to Stephen: Stephen was able to advance to Winchester, where Roger, Bishop of Salisbury and Lord Chancellor, instructed the royal treasury to be handed over to Stephen. On 15 December, Henry delivered an agreement under which Stephen would grant extensive freedoms and liberties to the church, in exchange for the Archbishop of Canterbury and the Papal Legate supporting his succession to the throne. There was the slight problem of the religious oath that Stephen had taken to support the Empress Matilda, but Henry convincingly argued that the late King had been wrong to insist that his court take the oath.

Furthermore, the late King had only insisted on that oath to protect the stability of the kingdom, and in light of the chaos that might now ensue, Stephen would be justified in ignoring it. Henry was also able to persuade Hugh Bigod, the late King's royal steward, to swear that the King had changed his mind about the succession on his deathbed, nominating Stephen instead. Stephen's coronation was held a week later at Westminster Abbey on 22 December.

Meanwhile, the Norman nobility gathered at Le Neubourg to discuss declaring Theobald king, probably following the news that Stephen was gathering support in England. The Normans argued that the count, as the more senior grandson of William the Conqueror, had the most valid claim over the kingdom and the duchy, and was certainly preferable to Matilda.

Theobald met with the Norman barons and Robert of Gloucester at Lisieux on 21 December. Their discussions were interrupted by the sudden news from England that Stephen's coronation was to occur the next day. Theobald then agreed to the Normans' proposal that he be made king, only to find that his former support immediately ebbed away: the barons were not prepared to support the division of England and Normandy by opposing Stephen, who subsequently financially compensated Theobald, who in return remained in Blois and supported his brother's succession.

Stephen's new Anglo-Norman kingdom had been shaped by the Norman conquest of England in 1066, followed by the Norman expansion into south Wales over the coming years. Both the kingdom and duchy were dominated by a small number of major barons who owned lands on both sides of the English Channel, with the lesser barons beneath them usually having more localised holdings. The extent to which lands and positions should be passed down through hereditary right or by the gift of the King was still uncertain, and tensions concerning this issue had grown during the reign of Henry I. Certainly lands in Normandy, passed by hereditary right, were usually considered more important to major barons than those in England, where their possession was less certain. Henry had increased the authority and capabilities of the central royal administration, often bringing in "new men" to fulfil key positions rather than using the established nobility. In the process he had been able to maximise revenues and contain expenditures, resulting in a healthy surplus and a famously large treasury, but also increasing political tensions.

Stephen had to intervene in the north of England immediately after his coronation. David I of Scotland invaded the north on the news of Henry's death, taking Carlisle, Newcastle and other key strongholds. Northern England was a disputed territory at this time, with the Scottish kings laying a traditional claim to Cumberland, and David also claiming Northumbria by virtue of his marriage to the daughter of Waltheof, Earl of Northumbria. Stephen rapidly marched north with an army and met David at Durham. An agreement was made under which David would return most of the territory he had taken, with the exception of Carlisle. In return, Stephen confirmed the English possessions of David's son Henry, including the Earldom of Huntingdon.

Returning south, Stephen held his first royal court at Easter 1136. A wide range of nobles gathered at Westminster for the event, including many of the Anglo-Norman barons and most of the higher officials of the church. Stephen issued a new royal charter, confirming the promises he had made to the church, promising to reverse Henry I's policies on the royal forests and to reform any abuses of the royal legal system. He portrayed himself as the natural successor to Henry's policies, and reconfirmed the existing seven earldoms in the kingdom on their existing holders. The Easter court was a lavish event, and a large amount of money was spent on the event itself, clothes and gifts. Stephen gave out grants of land and favours to those present and endowed numerous church foundations with land and privileges. His accession to the throne still needed to be ratified by the Pope, however, and Henry of Blois appears to have been responsible for ensuring that testimonials of support were sent both from Stephen's brother Theobald and from the French king Louis VI, to whom Stephen represented a useful balance to Angevin power in the north of France. Pope Innocent II confirmed Stephen as king by letter later that year, and Stephen's advisers circulated copies widely around England to demonstrate his legitimacy.

Troubles continued across Stephen's kingdom. After the Welsh victory at the battle of Llwchwr in January 1136 and the successful ambush of Richard Fitz Gilbert de Clare in April, south Wales rose in rebellion, starting in east Glamorgan and rapidly spreading across the rest of south Wales during 1137. Owain Gwynedd and Gruffydd ap Rhys successfully captured considerable territories, including Carmarthen Castle. Stephen responded by sending Richard's brother Baldwin and the Marcher Lord Robert Fitz Harold of Ewyas into Wales to pacify the region. Neither mission was particularly successful, and by the end of 1137 the King appears to have abandoned attempts to put down the rebellion. Historian David Crouch suggests that Stephen effectively "bowed out of Wales" around this time to concentrate on his other problems. Meanwhile, he had put down two revolts in the south-west led by Baldwin de Redvers and Robert of Bampton; Baldwin was released after his capture and travelled to Normandy, where he became an increasingly vocal critic of the King.

The security of Normandy was also a concern. Geoffrey of Anjou invaded in early 1136 and, after a temporary truce, invaded later the same year, raiding and burning estates rather than trying to hold the territory. Events in England meant that Stephen was unable to travel to Normandy himself, so Waleran de Beaumont, appointed by Stephen as the lieutenant of Normandy, and Theobald led the efforts to defend the duchy. Stephen himself only returned to the duchy in 1137, where he met with Louis VI and Theobald to agree to an informal regional alliance, probably brokered by Henry, to counter the growing Angevin power in the region. As part of this deal, Louis recognised Stephen's son Eustace as Duke of Normandy in exchange for Eustace giving fealty to the French King. Stephen was less successful, however, in regaining the Argentan province along the Normandy and Anjou border, which Geoffrey had taken at the end of 1135. Stephen formed an army to retake it, but the frictions between his Flemish mercenary forces led by William of Ypres and the local Norman barons resulted in a battle between the two halves of his army. The Norman forces then deserted Stephen, forcing the King to give up his campaign. He agreed to another truce with Geoffrey, promising to pay him 2,000 marks a year in exchange for peace along the Norman borders.

In the years following his succession, Stephen's relationship with the church became gradually more complex. The royal charter of 1136 had promised to review the ownership of all the lands that had been taken by the crown from the church since 1087, but these estates were now typically owned by nobles. Henry of Blois's claims, in his role as Abbot of Glastonbury, to extensive lands in Devon resulted in considerable local unrest. In 1136, Archbishop of Canterbury William de Corbeil died. Stephen responded by seizing his personal wealth, which caused some discontent amongst the senior clergy. Henry wanted to succeed to the post, but Stephen instead supported Theobald of Bec, who was eventually appointed. The papacy named Henry papal legate, possibly as consolation for not receiving Canterbury.

Stephen's first few years as king can be interpreted in different ways. He stabilised the northern border with Scotland, contained Geoffrey's attacks on Normandy, was at peace with Louis VI, enjoyed good relations with the church and had the broad support of his barons. There were significant underlying problems, nonetheless. The north of England was now controlled by David and Prince Henry, Stephen had abandoned Wales, the fighting in Normandy had considerably destabilised the duchy, and an increasing number of barons felt that Stephen had given them neither the lands nor the titles they felt they deserved or were owed. Stephen was also rapidly running out of money: Henry's considerable treasury had been emptied by 1138 due to the costs of running Stephen's more lavish court and the need to raise and maintain his mercenary armies fighting in England and Normandy.

Stephen was attacked on several fronts during 1138. First, Robert, Earl of Gloucester, rebelled against the King, starting the descent into civil war in England. An illegitimate son of Henry I and the half-brother of the Empress Matilda, Robert was one of the most powerful Anglo-Norman barons, controlling estates in Normandy. He was known for his qualities as a statesman, his military experience, and leadership ability. Robert had tried to convince Theobald to take the throne in 1135; he did not attend Stephen's first court in 1136 and it took several summons to convince him to attend court at Oxford later that year. In 1138, Robert renounced his fealty to Stephen and declared his support for Matilda, triggering a major regional rebellion in Kent and across the south-west of England, although Robert himself remained in Normandy. In France, Geoffrey of Anjou took advantage of the situation by re-invading Normandy. David of Scotland also invaded the north of England once again, announcing that he was supporting the claim of his niece the Empress Matilda to the throne, pushing south into Yorkshire.

Anglo-Norman warfare during the reign of Stephen was characterised by attritional military campaigns, in which commanders tried to seize key enemy castles in order to allow them to take control of their adversaries' territory and ultimately win a slow, strategic victory. The armies of the period centred on bodies of mounted, armoured knights, supported by infantry and crossbowmen. These forces were either feudal levies, drawn up by local nobles for a limited period of service during a campaign, or, increasingly, mercenaries, who were expensive but more flexible and often more skilled. These armies, however, were ill-suited to besieging castles, whether the older motte-and-bailey designs or the newer, stone-built keeps. Existing siege engines were significantly less powerful than the later trebuchet designs, giving defenders a substantial advantage over attackers. As a result, slow sieges to starve defenders out, or mining operations to undermine walls, tended to be preferred by commanders over direct assaults. Occasionally pitched battles were fought between armies but these were considered highly risky endeavours and were usually avoided by prudent commanders. The cost of warfare had risen considerably in the first part of the 12th century, and adequate supplies of ready cash were increasingly proving important in the success of campaigns.

Stephen's personal qualities as a military leader focused on his skill in personal combat, his capabilities in siege warfare and a remarkable ability to move military forces quickly over relatively long distances. In response to the revolts and invasions, he rapidly undertook several military campaigns, focusing primarily on England rather than Normandy. His wife Matilda was sent to Kent with ships and resources from Boulogne, with the task of retaking the key port of Dover, under Robert's control. A small number of Stephen's household knights were sent north to help the fight against the Scots, where David's forces were defeated later that year at the battle of the Standard in August by the forces of Thurstan, the Archbishop of York. Despite this victory, however, David still occupied most of the north. Stephen himself went west in an attempt to regain control of Gloucestershire, first striking north into the Welsh Marches, taking Hereford and Shrewsbury, before heading south to Bath. The town of Bristol itself proved too strong for him, and Stephen contented himself with raiding and pillaging the surrounding area. The rebels appear to have expected Robert to intervene with support that year, but he remained in Normandy throughout, trying to persuade the Empress Matilda to invade England herself. Dover finally surrendered to the queen's forces later in the year.

Stephen's military campaign in England had progressed well, and historian David Crouch describes it as "a military achievement of the first rank". The King took the opportunity of his military advantage to forge a peace agreement with Scotland. Stephen's wife Matilda was sent to negotiate another agreement between Stephen and David, called the treaty of Durham; Northumbria and Cumbria would effectively be granted to David and his son Henry, in exchange for their fealty and future peace along the border. Unfortunately, the powerful Ranulf I, Earl of Chester, considered himself to hold the traditional rights to Carlisle and Cumberland and was extremely displeased to see them being given to the Scots. Nonetheless, Stephen could now focus his attention on the anticipated invasion of England by Robert and Matilda's forces.

Stephen prepared for the Angevin invasion by creating a number of additional earldoms. Only a handful of earldoms had existed under Henry I and these had been largely symbolic in nature. Stephen created many more, filling them with men he considered to be loyal, capable military commanders, and in the more vulnerable parts of the country assigning them new lands and additional executive powers. He appears to have had several objectives in mind, including both ensuring the loyalty of his key supporters by granting them these honours, and improving his defences in key parts of the kingdom. Stephen was heavily influenced by his principal advisor, Waleran de Beaumont, the twin brother of Robert of Leicester. The Beaumont twins and their younger brother and cousins received the majority of these new earldoms. From 1138 onwards, Stephen gave them the earldoms of Worcester, Leicester, Hereford, Warwick and Pembroke, which‚Äîespecially when combined with the possessions of Stephen's new ally, Prince Henry, in Cumberland and Northumbria‚Äîcreated a wide block of territory to act as a buffer zone between the troubled south-west, Chester, and the rest of the kingdom. With their new lands, the power of the Beamounts grew to the point where David Crouch suggests that it became "dangerous to be anything other than a friend of Waleran" at Stephen's court.

Stephen took steps to remove a group of bishops he regarded as a threat to his rule. The royal administration under Henry I had been headed by Roger, the Bishop of Salisbury, supported by Roger's nephews, Alexander and Nigel, the Bishops of Lincoln and Ely respectively, and Roger's son, Lord Chancellor Roger le Poer. These bishops were powerful landowners as well as ecclesiastical rulers, and they had begun to build new castles and increase the size of their military forces, leading Stephen to suspect that they were about to defect to the Empress Matilda. Roger and his family were also enemies of Waleran, who disliked their control of the royal administration. In June 1139, Stephen held his court in Oxford, where a fight between Alan of Brittany and Roger's men broke out, an incident probably deliberately created by Stephen. Stephen responded by demanding that Roger and the other bishops surrender all of their castles in England. This threat was backed up by the arrest of the bishops, with the exception of Nigel who had taken refuge in Devizes Castle; the bishop only surrendered after Stephen besieged the castle and threatened to execute Roger le Poer. The remaining castles were then surrendered to the King.

Stephen's brother, Henry of Blois, was alarmed by this, both as a matter of principle, since Stephen had previously agreed in 1135 to respect the freedoms of the church, and more pragmatically because he himself had recently built six castles and had no desire to be treated in the same way. As the papal legate, he summoned the King to appear before an ecclesiastical council to answer for the arrests and seizure of property. Henry asserted the Church's right to investigate and judge all charges against members of the clergy. Stephen sent Aubrey de Vere II as his spokesman to the council, who argued that Roger of Salisbury had been arrested not as a bishop, but rather in his role as a baron who had been preparing to change his support to the Empress Matilda. The King was supported by Hugh of Amiens, Archbishop of Rouen, who challenged the bishops to show how canon law entitled them to build or hold castles. Aubrey threatened that Stephen would complain to the pope that he was being harassed by the English church, and the council let the matter rest following an unsuccessful appeal to Rome. The incident successfully removed any military threat from the bishops, but it may have damaged Stephen's relationship with the senior clergy, and in particular with his brother Henry.

The Angevin invasion finally arrived in 1139. Baldwin de Redvers crossed over from Normandy to Wareham in August in an initial attempt to capture a port to receive the Empress Matilda's invading army, but Stephen's forces forced him to retreat into the south-west. The following month, however, the Empress was invited by the Dowager Queen Adeliza to land at Arundel instead, and on 30 September Robert of Gloucester and the Empress arrived in England with 140 knights. The Empress stayed at Arundel Castle, whilst Robert marched north-west to Wallingford and Bristol, hoping to raise support for the rebellion and to link up with Miles of Gloucester, a capable military leader who took the opportunity to renounce his fealty to the King. Stephen promptly moved south, besieging Arundel and trapping Matilda inside the castle.

Stephen then agreed to a truce proposed by his brother, Henry; the full details of the truce are not known, but the results were that Stephen first released Matilda from the siege and then allowed her and her household of knights to be escorted to the south-west, where they were reunited with Robert of Gloucester. The reasoning behind Stephen's decision to release his rival remains unclear. Contemporary chroniclers suggested that Henry argued that it would be in Stephen's own best interests to release the Empress and concentrate instead on attacking Robert, and Stephen may have seen Robert, not the Empress, as his main opponent at this point in the conflict. He also faced a military dilemma at Arundel‚Äîthe castle was considered almost impregnable, and he may have been worried that he was tying down his army in the south whilst Robert roamed freely in the west. Another theory is that Stephen released Matilda out of a sense of chivalry; he was certainly known for having a generous, courteous personality and women were not normally expected to be targeted in Anglo-Norman warfare.

Having released the Empress, Stephen focused on pacifying the south-west of England. Although there had been few new defections to the Empress, his enemies now controlled a compact block of territory stretching out from Gloucester and Bristol south-west into Devon and Cornwall, west into the Welsh Marches and east as far as Oxford and Wallingford, threatening London. Stephen started by attacking Wallingford Castle, held by the Empress's childhood friend Brien FitzCount, only to find it too well defended. He then left behind some forces to blockade the castle and continued west into Wiltshire to attack Trowbridge Castle, taking the castles of South Cerney and Malmesbury en route. Meanwhile, Miles of Gloucester marched east, attacking Stephen's rearguard forces at Wallingford and threatening an advance on London. Stephen was forced to give up his western campaign, returning east to stabilise the situation and protect his capital.
At the start of 1140, Nigel, Bishop of Ely, whose castles Stephen had confiscated the previous year, rebelled against Stephen as well. Nigel hoped to seize East Anglia and established his base of operations in the Isle of Ely, then surrounded by protective fenland. Stephen responded quickly, taking an army into the fens and using boats lashed together to form a causeway that allowed him to make a surprise attack on the isle. Nigel escaped to Gloucester, but his men and castle were captured, and order was temporarily restored in the east. Robert of Gloucester's men retook some of the territory that Stephen had taken in his 1139 campaign. In an effort to negotiate a truce, Henry of Blois held a peace conference at Bath, to which Stephen sent his wife. The conference collapsed over the insistence by Henry and the clergy that they should set the terms of any peace deal, which Stephen found unacceptable.

Ranulf of Chester remained upset over Stephen's gift of the north of England to Prince Henry. Ranulf devised a plan for dealing with the problem by ambushing Henry whilst the prince was travelling back from Stephen's court to Scotland after Christmas. Stephen responded to rumours of this plan by escorting Henry himself north, but this gesture proved the final straw for Ranulf. Ranulf had previously claimed that he had the rights to Lincoln Castle, held by Stephen, and under the guise of a social visit, Ranulf seized the fortification in a surprise attack. Stephen marched north to Lincoln and agreed to a truce with Ranulf, probably to keep him from joining the Empress's faction, under which Ranulf would be allowed to keep the castle. Stephen returned to London but received news that Ranulf, his brother and their family were relaxing in Lincoln Castle with a minimal guard force, a ripe target for a surprise attack of his own. Abandoning the deal he had just made, Stephen gathered his army again and sped north, but not quite fast enough‚ÄîRanulf escaped Lincoln and declared his support for the Empress. Stephen was forced to place the castle under siege.

While Stephen and his army besieged Lincoln Castle at the start of 1141, Robert of Gloucester and Ranulf of Chester advanced on the King's position with a somewhat larger force. When the news reached Stephen, he held a council to decide whether to give battle or to withdraw and gather additional soldiers: Stephen decided to fight, resulting in the Battle of Lincoln on 2 February 1141. The King commanded the centre of his army, with Alan of Brittany on his right and William of Aumale on his left. Robert and Ranulf's forces had superiority in cavalry and Stephen dismounted many of his own knights to form a solid infantry block; he joined them himself, fighting on foot in the battle. Stephen was not a gifted public speaker, and delegated the pre-battle speech to Baldwin of Clare, who delivered a rousing declaration. After an initial success in which William's forces destroyed the Angevins' Welsh infantry, the battle went badly for Stephen. Robert and Ranulf's cavalry encircled Stephen's centre, and the King found himself surrounded by the enemy army. Many of his supporters, including Waleran de Beaumont and William of Ypres, fled from the field at this point but Stephen fought on, defending himself first with his sword and then, when that broke, with a borrowed battle axe. Finally, he was overwhelmed by Robert's men and taken away from the field in custody.

Robert took Stephen back to Gloucester, where the King met with the Empress Matilda, and was then moved to Bristol Castle, traditionally used for holding high-status prisoners. He was initially left confined in relatively good conditions, but his security was later tightened and he was kept in chains. The Empress now began to take the necessary steps to have herself crowned queen in his place, which would require the agreement of the church and her coronation at Westminster. Stephen's brother Henry summoned a council at Winchester before Easter in his capacity as papal legate to consider the clergy's view. He had made a private deal with the Empress Matilda that he would deliver the support of the church, if she agreed to give him control over church business in England. Henry handed over the royal treasury, rather depleted except for Stephen's crown, to the Empress, and excommunicated many of Stephen's supporters who refused to switch sides. Archbishop Theobald of Canterbury was unwilling to declare Matilda queen so rapidly, however, and a delegation of clergy and nobles, headed by Theobald, travelled to see Stephen in Bristol and consult about their moral dilemma: should they abandon their oaths of fealty to the King? Stephen agreed that, given the situation, he was prepared to release his subjects from their oath of fealty to him, and the clergy gathered again in Winchester after Easter to declare the Empress "Lady of England and Normandy" as a precursor to her coronation. When Matilda advanced to London in an effort to stage her coronation in June, though, she faced an uprising by the local citizens in support of Stephen that forced her to flee to Oxford, uncrowned.

Once news of Stephen's capture reached him, Geoffrey of Anjou invaded Normandy again and, in the absence of Waleran of Beaumont, who was still fighting in England, Geoffrey took all the duchy south of the river Seine and east of the river Risle. No help was forthcoming from Stephen's brother Theobald this time either, who appears to have been preoccupied with his own problems with France‚Äîthe new French king, Louis VII, had rejected his father's regional alliance, improving relations with Anjou and taking a more bellicose line with Theobald, which would result in war the following year. Geoffrey's success in Normandy and Stephen's weakness in England began to influence the loyalty of many Anglo-Norman barons, who feared losing their lands in England to Robert and the Empress, and their possessions in Normandy to Geoffrey. Many started to leave Stephen's faction. His friend and advisor Waleran was one of those who decided to defect in mid-1141, crossing into Normandy to secure his ancestral possessions by allying himself with the Angevins, and bringing Worcestershire into the Empress's camp. Waleran's twin brother, Robert of Leicester, effectively withdrew from fighting in the conflict at the same time. Other supporters of the Empress were restored in their former strongholds, such as Bishop Nigel of Ely, or received new earldoms in the west of England. The royal control over the minting of coins broke down, leading to coins being struck by local barons and bishops across the country.
Stephen's wife Matilda played a critical part in keeping the King's cause alive during his captivity. Queen Matilda gathered Stephen's remaining lieutenants around her and the royal family in the south-east, advancing into London when the population rejected the Empress. Stephen's long-standing commander William of Ypres remained with the Queen in London; William Martel, the royal steward, commanded operations from Sherborne in Dorset, and Faramus of Boulogne ran the royal household. The Queen appears to have generated genuine sympathy and support from Stephen's more loyal followers. Henry's alliance with the Empress proved short-lived, as they soon fell out over political patronage and ecclesiastical policy; the bishop met the Queen at Guildford and transferred his support to her.

The King's eventual release resulted from the Angevin defeat at the rout of Winchester. Robert of Gloucester and the Empress besieged Henry in the city of Winchester in July. Queen Matilda and William of Ypres then encircled the Angevin forces with their own army, reinforced with fresh troops from London. In the subsequent battle the Empress's forces were defeated and Robert of Gloucester himself was taken prisoner. Further negotiations attempted to deliver a general peace agreement but the Queen was unwilling to offer any compromise to the Empress, and Robert refused to accept any offer to encourage him to change sides to Stephen. Instead, in November the two sides simply exchanged Robert and the King, with Stephen releasing Robert on 1 November 1141. Stephen began re-establishing his authority. Henry held another church council, which this time reaffirmed Stephen's legitimacy to rule, and a fresh coronation of Stephen and Matilda occurred at Christmas 1141.

At the beginning of 1142 Stephen fell ill, and by Easter rumours had begun to circulate that he had died. Possibly this illness was the result of his imprisonment the previous year, but he finally recovered and travelled north to raise new forces and to successfully convince Ranulf of Chester to change sides once again. Stephen then spent the summer attacking some of the new Angevin castles built the previous year, including Cirencester, Bampton and Wareham. In September, he spotted an opportunity to seize the Empress Matilda herself in Oxford. Oxford was a secure town, protected by walls and the river Isis, but Stephen led a sudden attack across the river, leading the charge and swimming part of the way. Once on the other side, the King and his men stormed into the town, trapping the Empress in the castle. Oxford Castle, however, was a powerful fortress and, rather than storming it, Stephen had to settle down for a long siege, albeit secure in the knowledge that Matilda was now surrounded. Just before Christmas, the Empress left the castle unobserved, crossed the icy river on foot and made her escape to Wallingford. The garrison surrendered shortly afterwards, but Stephen had lost an opportunity to capture his principal opponent.

The war between the two sides in England reached a stalemate in the mid-1140s, while Geoffrey of Anjou consolidated his hold on power in Normandy. 1143 started precariously for Stephen when he was besieged by Robert of Gloucester at Wilton Castle, an assembly point for royal forces in Herefordshire. Stephen attempted to break out and escape, resulting in the battle of Wilton. Once again, the Angevin cavalry proved too strong, and for a moment it appeared that Stephen might be captured for a second time. On this occasion, however, William Martel, Stephen's steward, made a fierce rear guard effort, allowing Stephen to escape from the battlefield. Stephen valued William's loyalty sufficiently to agree to exchange Sherborne Castle for his safe release‚Äîthis was one of the few instances where Stephen was prepared to give up a castle to ransom one of his men.

In late 1143, Stephen faced a new threat in the east, when Geoffrey de Mandeville, Earl of Essex, rose up in rebellion against him in East Anglia. The King had disliked the Earl for several years, and provoked the conflict by summoning Geoffrey to court, where the King arrested him. He threatened to execute Geoffrey unless the Earl handed over his various castles, including the Tower of London, Saffron Walden and Pleshey, all important fortifications because they were in, or close to, London. Geoffrey gave in, but once free he headed north-east into the Fens to the Isle of Ely, from where he began a military campaign against Cambridge, with the intention of progressing south towards London. With all of his other problems and with Hugh Bigod, 1st Earl of Norfolk, in open revolt in Norfolk, Stephen lacked the resources to track Geoffrey down in the Fens and made do with building a screen of castles between Ely and London, including Burwell Castle.

For a period, the situation continued to worsen. Ranulf of Chester revolted once again in the summer of 1144, splitting up Stephen's Honour of Lancaster between himself and Prince Henry. In the west, Robert of Gloucester and his followers continued to raid the surrounding royalist territories, and Wallingford Castle remained a secure Angevin stronghold, too close to London for comfort. Meanwhile, Geoffrey of Anjou finished securing his hold on southern Normandy and in January 1144 he advanced into Rouen, the capital of the duchy, concluding his campaign. Louis VII recognised him as Duke of Normandy shortly after. By this point in the war, Stephen was depending increasingly on his immediate royal household, such as William of Ypres and others, and lacked the support of the major barons who might have been able to provide him with significant additional forces; after the events of 1141, Stephen made little use of his network of earls.

After 1143 the war ground on, but progressing slightly better for Stephen. Miles of Gloucester, one of the most talented Angevin commanders, had died whilst hunting over the previous Christmas, relieving some of the pressure in the west. Geoffrey de Mandeville's rebellion continued until September 1144, when he died during an attack on Burwell. The war in the west progressed better in 1145, with the King recapturing Faringdon Castle in Oxfordshire. In the north, Stephen came to a fresh agreement with Ranulf of Chester, but then in 1146 repeated the ruse he had played on Geoffrey de Mandeville in 1143, first inviting Ranulf to court, before arresting him and threatening to execute him unless he handed over a number of castles, including Lincoln and Coventry. As with Geoffrey, the moment Ranulf was released he immediately rebelled, but the situation was a stalemate: Stephen had few forces in the north with which to prosecute a fresh campaign, whilst Ranulf lacked the castles to support an attack on Stephen. By this point, however, Stephen's practice of inviting barons to court and arresting them had brought him into some disrepute and increasing distrust.

England had suffered extensively from the war by 1147, leading later Victorian historians to call the period of conflict "the Anarchy". The contemporary "Anglo-Saxon Chronicle" recorded how "there was nothing but disturbance and wickedness and robbery". Certainly in many parts of the country, such as Wiltshire, Berkshire, the Thames Valley and East Anglia, the fighting and raiding had caused serious devastation. Numerous "adulterine", or unauthorised, castles had been built as bases for local lords‚Äîthe chronicler Robert of Torigny complained that as many as 1,115 such castles had been built during the conflict, although this was probably an exaggeration as elsewhere he suggested an alternative figure of 126. The previously centralised royal coinage system was fragmented, with Stephen, the Empress and local lords all minting their own coins. The royal forest law had collapsed in large parts of the country. Some parts of the country, though, were barely touched by the conflict‚Äîfor example, Stephen's lands in the south-east and the Angevin heartlands around Gloucester and Bristol were largely unaffected, and David I ruled his territories in the north of England effectively. Stephen's overall income from his estates, however, declined seriously during the conflict, particularly after 1141, and royal control over the minting of new coins remained limited outside of the south-east and East Anglia. With Stephen often based in the south-east, increasingly Westminster, rather than the older site of Winchester, was used as the centre of royal government.

The character of the conflict in England gradually began to shift; as historian Frank Barlow suggests, by the late 1140s "the civil war was over", barring the occasional outbreak of fighting. In 1147 Robert of Gloucester died peacefully, and the next year the Empress Matilda left south-west England for Normandy, both of which contributed to reducing the tempo of the war. The Second Crusade was announced, and many Angevin supporters, including Waleran of Beaumont, joined it, leaving the region for several years. Many of the barons were making individual peace agreements with each other to secure their lands and war gains. Geoffrey and Matilda's son, the future King Henry II of England, mounted a small mercenary invasion of England in 1147 but the expedition failed, not least because Henry lacked the funds to pay his men. Surprisingly, Stephen himself ended up paying their costs, allowing Henry to return home safely; his reasons for doing so are unclear. One potential explanation is his general courtesy to a member of his extended family; another is that he was starting to consider how to end the war peacefully, and saw this as a way of building a relationship with Henry.

The young Henry FitzEmpress returned to England again in 1149, this time planning to form a northern alliance with Ranulf of Chester. The Angevin plan involved Ranulf agreeing to give up his claim to Carlisle, held by the Scots, in return for being given the rights to the whole of the Honour of Lancaster; Ranulf would give homage to both David and Henry FitzEmpress, with Henry having seniority. Following this peace agreement, Henry and Ranulf agreed to attack York, probably with help from the Scots. Stephen marched rapidly north to York and the planned attack disintegrated, leaving Henry to return to Normandy, where he was declared duke by his father.

Although still young, Henry was increasingly gaining a reputation as an energetic and capable leader. His prestige and power increased further when he unexpectedly married the attractive Eleanor, Duchess of Aquitaine, the recently divorced wife of Louis VII, in 1152. The marriage made Henry the future ruler of a huge swathe of territory across France.

In the final years of the war, Stephen began to focus on the issue of his family and the succession. He wanted to confirm his eldest son, Eustace, as his successor, although chroniclers recorded that Eustace was infamous for levying heavy taxes and extorting money from those on his lands. Stephen's second son, William, was married to the extremely wealthy heiress Isabel de Warenne. In 1148, Stephen built the Cluniac Faversham Abbey as a resting place for his family. Both Stephen's wife, Queen Matilda, and his older brother Theobald died in 1152.

Stephen's relationship with the church deteriorated badly towards the end of his reign. The reforming movement within the church, which advocated greater autonomy from royal authority for the clergy, had continued to grow, while new voices such as the Cistercians had gained additional prestige within the monastic orders, eclipsing older orders such as the Cluniacs. Stephen's dispute with the church had its origins in 1140, when Archbishop Thurstan of York died. An argument then broke out between a group of reformers based in York and backed by Bernard of Clairvaux, the head of the Cistercian order, who preferred William of Rievaulx as the new archbishop, and Stephen and his brother Henry, who preferred various Blois family relatives. The row between Henry and Bernard grew increasingly personal, and Henry used his authority as legate to appoint his nephew William of York to the post in 1144 only to find that, when Pope Innocent II died in 1145, Bernard was able to get the appointment rejected by Rome. Bernard then convinced Pope Eugene III to overturn Henry's decision altogether in 1147, deposing William, and appointing Henry Murdac as archbishop instead.

Stephen was furious over what he saw as potentially precedent-setting papal interference in his royal authority, and initially refused to allow Murdac into England. When Theobald, the Archbishop of Canterbury, went to consult with the Pope on the matter against Stephen's wishes, the King refused to allow him back into England either, and seized his estates. Stephen also cut his links to the Cistercian order, and turned instead to the Cluniacs, of which Henry was a member.

Nonetheless, the pressure on Stephen to get Eustace confirmed as his legitimate heir continued to grow. The King gave Eustace the County of Boulogne in 1147, but it remained unclear whether Eustace would inherit England. Stephen's preferred option was to have Eustace crowned while he himself was still alive, as was the custom in France, but this was not the normal practice in England, and Celestine II, during his brief tenure as pope between 1143 and 1144, had banned any change to this practice. Since the only person who could crown Eustace was Archbishop Theobald, who refused to do so without agreement from the current pope, Eugene III, the matter reached an impasse. At the end of 1148, Stephen and Theobald came to a temporary compromise that allowed Theobald to return to England. Theobald was appointed a papal legate in 1151, adding to his authority. Stephen then made a fresh attempt to have Eustace crowned at Easter 1152, gathering his nobles to swear fealty to Eustace, and then insisting that Theobald and his bishops anoint him king. When Theobald refused yet again, Stephen and Eustace imprisoned both him and the bishops and refused to release them unless they agreed to crown Eustace. Theobald escaped again into temporary exile in Flanders, pursued to the coast by Stephen's knights, marking a low point in Stephen's relationship with the church.

Henry FitzEmpress returned to England again at the start of 1153 with a small army, supported in the north and east of England by Ranulf of Chester and Hugh Bigod. Stephen's castle at Malmesbury was besieged by Henry's forces, and the King responded by marching west with an army to relieve it. He unsuccessfully attempted to force Henry's smaller army to fight a decisive battle along the river Avon. In the face of the increasingly wintry weather, Stephen agreed to a temporary truce and returned to London, leaving Henry to travel north through the Midlands where the powerful Robert de Beaumont, Earl of Leicester, announced his support for the Angevin cause. Despite only modest military successes, Henry and his allies now controlled the south-west, the Midlands and much of the north of England.

Over the summer, Stephen intensified the long-running siege of Wallingford Castle in a final attempt to take this major Angevin stronghold. The fall of Wallingford appeared imminent and Henry marched south in an attempt to relieve the siege, arriving with a small army and placing Stephen's besieging forces under siege themselves. Upon news of this, Stephen gathered up a large force and marched from Oxford, and the two sides confronted each other across the River Thames at Wallingford in July. By this point in the war, the barons on both sides seem to have been eager to avoid an open battle. As a result, instead of a battle ensuing, members of the church brokered a truce, to the annoyance of both Stephen and Henry.

In the aftermath of Wallingford, Stephen and Henry spoke together privately about a potential end to the war; Stephen's son Eustace, however, was furious about the peaceful outcome at Wallingford. He left his father and returned home to Cambridge to gather more funds for a fresh campaign, where he fell ill and died the next month. Eustace's death removed an obvious claimant to the throne and was politically convenient for those seeking a permanent peace in England. It is possible, however, that Stephen had already begun to consider passing over Eustace's claim; historian Edmund King observes that Eustace's claim to the throne was not mentioned in the discussions at Wallingford, for example, and this may have added to his anger.

Fighting continued after Wallingford, but in a rather half-hearted fashion. Stephen lost the towns of Oxford and Stamford to Henry while the King was diverted fighting Hugh Bigod in the east of England, but Nottingham Castle survived an Angevin attempt to capture it. Meanwhile, Stephen's brother Henry of Blois and Archbishop Theobald of Canterbury were for once unified in an effort to broker a permanent peace between the two sides, putting pressure on Stephen to accept a deal. The armies of Stephen and Henry FitzEmpress met again at Winchester, where the two leaders would ratify the terms of a permanent peace in November. Stephen announced the Treaty of Winchester in Winchester Cathedral: he recognised Henry FitzEmpress as his adopted son and successor, in return for Henry doing homage to him; Stephen promised to listen to Henry's advice, but retained all his royal powers; Stephen's remaining son, William, would do homage to Henry and renounce his claim to the throne, in exchange for promises of the security of his lands; key royal castles would be held on Henry's behalf by guarantors, whilst Stephen would have access to Henry's castles; and the numerous foreign mercenaries would be demobilised and sent home. Stephen and Henry sealed the treaty with a kiss of peace in the cathedral.

Stephen's decision to recognise Henry as his heir was, at the time, not necessarily a final solution to the civil war. Despite the issuing of new currency and administrative reforms, Stephen might potentially have lived for many more years, whilst Henry's position on the continent was far from secure. Although Stephen's son William was young and unprepared to challenge Henry for the throne in 1153, the situation could well have shifted in subsequent years‚Äîthere were widespread rumours during 1154 that William planned to assassinate Henry, for example. Historian Graham White describes the treaty of Winchester as a "precarious peace", in line with the judgement of most modern historians that the situation in late 1153 was still uncertain and unpredictable.

Certainly many problems remained to be resolved, including re-establishing royal authority over the provinces and resolving the complex issue of which barons should control the contested lands and estates after the long civil war. Stephen burst into activity in early 1154, travelling around the kingdom extensively. He began issuing royal writs for the south-west of England once again and travelled to York where he held a major court in an attempt to impress upon the northern barons that royal authority was being reasserted. After a busy summer in 1154, however, Stephen travelled to Dover to meet Thierry, Count of Flanders; some historians believe that the King was already ill and preparing to settle his family affairs. Stephen fell ill with a stomach disease and died on 25 October at the local priory, being buried at Faversham Abbey with his wife Matilda and son Eustace.

After Stephen's death, Henry II succeeded to the throne of England. Henry vigorously re-established royal authority in the aftermath of the civil war, dismantling castles and increasing revenues, although several of these trends had begun under Stephen. The destruction of castles under Henry was not as dramatic as once thought, and although he restored royal revenues, the economy of England remained broadly unchanged under both rulers. Stephen's son William was confirmed as the Earl of Surrey by Henry, and prospered under the new regime, with the occasional point of tension with Henry. Stephen's daughter Marie I, Countess of Boulogne, also survived her father; she had been placed in a convent by Stephen, but after his death she left and married. Stephen's middle son, Baldwin, and second daughter, Matilda, had died before 1147 and were buried at Holy Trinity Priory, Aldgate. Stephen probably had three illegitimate sons, Gervase, Abbot of Westminster, Ralph and Americ, by his mistress Damette; Gervase became abbot in 1138, but after his father's death he was removed by Henry in 1157 and died shortly afterwards.

Much of the modern history of Stephen's reign is based on accounts of chroniclers who lived in, or close to, the middle of the 12th century, forming a relatively rich account of the period. All of the main chronicler accounts carry significant regional biases in how they portray the disparate events. Several of the key chronicles were written in the south-west of England, including the "Gesta Stephani", or "Acts of Stephen", and William of Malmesbury's "Historia Novella", or "New History". In Normandy, Orderic Vitalis wrote his "Ecclesiastical History", covering Stephen's reign until 1141, and Robert of Torigni wrote a later history of the rest of the period. Henry of Huntingdon, who lived in the east of England, produced the "Historia Anglorum" that provides a regional account of the reign. The "Anglo-Saxon Chronicle" was past its prime by the time of Stephen, but is remembered for its striking account of conditions during "the Anarchy". Most of the chronicles carry some bias for or against Stephen, Robert of Gloucester or other key figures in the conflict. Those writing for the church after the events of Stephen's later reign, such as John of Salisbury for example, paint the King as a tyrant due to his argument with the Archbishop of Canterbury; by contrast, clerics in Durham regarded Stephen as a saviour, due to his contribution to the defeat of the Scots at the battle of the Standard. Later chronicles written during the reign of Henry II were generally more negative: Walter Map, for example, described Stephen as "a fine knight, but in other respects almost a fool." A number of charters were issued during Stephen's reign, often giving details of current events or daily routine, and these have become widely used as sources by modern historians.

Historians in the "Whiggish" tradition that emerged during the Victorian era traced a progressive and universalist course of political and economic development in England over the medieval period. William Stubbs focused on these constitutional aspects of Stephen's reign in his 1874 volume the "Constitutional History of England", beginning an enduring interest in Stephen and his reign. Stubbs' analysis, focusing on the disorder of the period, influenced his student John Round to coin the term "the Anarchy" to describe the period, a label that, whilst sometimes critiqued, continues to be used today. The late-Victorian scholar Frederic William Maitland also introduced the possibility that Stephen's reign marked a turning point in English legal history‚Äîthe so-called "tenurial crisis".

Stephen remains a popular subject for historical study: David Crouch suggests that after King John he is "arguably the most written-about medieval king of England". Modern historians vary in their assessments of Stephen as a king. Historian R. H. C. Davis's influential biography paints a picture of a weak king: a capable military leader in the field, full of activity and pleasant, but "beneath the surface¬†... mistrustful and sly", with poor strategic judgement that ultimately undermined his reign. Stephen's lack of sound policy judgement and his mishandling of international affairs, leading to the loss of Normandy and his consequent inability to win the civil war in England, is also highlighted by another of his biographers, David Crouch. Historian and biographer Edmund King, whilst painting a slightly more positive picture than Davis, also concludes that Stephen, while a stoic, pious and genial leader, was also rarely, if ever, his own man, usually relying upon stronger characters such as his brother or wife. Historian Keith Stringer provides a more positive portrayal of Stephen, arguing that his ultimate failure as king was the result of external pressures on the Norman state, rather than the result of personal failings.

Stephen and his reign have been occasionally used in historical fiction. Stephen and his supporters appear in Ellis Peters' historical detective series "The Cadfael Chronicles", set between 1137 and 1145. Peters' depiction of Stephen's reign is an essentially local narrative, focused on the town of Shrewsbury and its environs. Peters paints Stephen as a tolerant man and a reasonable ruler, despite his execution of the Shrewsbury defenders after the taking of the city in 1138. In contrast, he is depicted unsympathetically in both Ken Follett's historical novel "The Pillars of the Earth" and the TV mini-series adapted from it.

Stephen of Blois married Matilda of Boulogne in 1125. They had five children:

King Stephen's illegitimate children by his mistress Damette included:



</doc>
<doc id="28954" url="https://en.wikipedia.org/wiki?curid=28954" title="Space Battleship Yamato">
Space Battleship Yamato

It is one of the most influential anime series in Japan due to its theme and story, marking a turn towards more complex serious works and influencing works such as "Mobile Suit Gundam", "Neon Genesis Evangelion" and "Super Dimension Fortress Macross" as well as video games such as "Space Invaders". Hideaki Anno has ranked "Yamato" as his favorite anime and credited it with sparking his interest in anime. "Yamato" was also the first anime series or movie to win the Seiun Award, a feat not repeated until the film "Nausica√§ of the Valley of the Wind" (1984).

Conceived in 1973 by producer Yoshinobu Nishizaki, the project underwent heavy revisions. Originally intended to be an outer-space variation on "Lord of the Flies", the project at first was titled "Asteroid Ship Icarus" and had a multinational teenage crew journeying through space in a hollowed-out asteroid in search of the planet Iscandar. There was to be much discord among the crew with many of them acting purely out of self-interest and for personal gain. The enemy aliens were originally called Rajendora.

In the year 2199, an alien race known as the Gamilas (Gamilons in the English "Star Blazers" dub) unleash radioactive meteorite bombs on Earth, rendering the planet's surface uninhabitable. Humanity has retreated into deep underground cities, but the radioactivity is slowly affecting them as well, with humanity's extinction estimated in one year. Earth has a space fleet, but they do not yet have interstellar capability, and they are hopelessly outclassed by the Gamilas. All seems lost until a message capsule from a mysterious crashed spaceship is retrieved on Mars. The capsule yields blueprints for a faster-than-light engine and an offering of help from Queen Starsha of the planet Iscandar in the Large Magellanic Cloud. She says that her planet has a device, the Cosmo-Cleaner D (Cosmo DNA), which can cleanse Earth of its radiation damage.

The inhabitants of Earth secretly build a massive spaceship inside the ruins of the gigantic Japanese battleship "Yamato" which lies exposed at the former bottom of the ocean location where she was sunk in World War II. This becomes the "Space Battleship Yamato" for which the story is titled. In the English "Star Blazers" dub, the ship is noted as being the historical "Yamato", but is then renamed the "Argo" (after the ship of Jason and the Argonauts).

Using Starsha's blueprints, they equip the new ship with a space warp drive, called the "wave motion engine", and in an apparently unexpected move, weaponize the technology to create a new, incredibly powerful weapon at the bow called the "Wave Motion Gun". The is capable of converting tachyon particles which travel faster than light and enables the "Yamato" to "ride" the wave of tachyons and travel faster than light. The , also called the Dimensional Wave Motion Explosive Compression Emitter, is the "trump card" of the Yamato that functions by connecting the Wave Motion Engine to the enormous firing gate at the ship's bow, enabling the tachyon energy power of the engine to be fired in a stream directly forwards. Enormously powerful, it can vaporize a fleet of enemy ships‚Äîor a small continent (as seen in the first season, fifth episode)‚Äîwith one shot; however, it takes a brief but critical period to charge before firing.

A crew of 114 departs for Iscandar in the "Yamato" to retrieve the radiation-removing device and return to Earth within the one-year deadline. Along the way, they discover the motives of their blue-skinned adversaries: the planet Gamilas, sister planet to Iscandar, is dying; and its leader, Lord Desslar (Desslok in the "Star Blazers" dub), is trying to irradiate Earth enough for his people to move there, at the expense of the "barbarians" he considers humanity to be.

The first season contained 26 episodes, following the "Yamato"s voyage out of the Milky Way Galaxy and back again. A continuing story, it features the declining health of "Yamato"s Captain Okita (Avatar in the "Star Blazers" dub), and the transformation of the brash young orphan Susumu Kodai (Derek Wildstar) into a mature officer, as well as his budding romance with female crewmember Yuki Mori (Nova Forrester). The foreign edits tend to play up the individual characters, while the Japanese original is often more focused on the ship itself. In a speech at the 1995 Anime Expo, series episode director Noboru Ishiguro said low ratings and high production expenses forced producer Yoshinobu Nishizaki to trim down the episode count from the original 39 episodes to only 26. The 13 episodes would have introduced Captain Harlock as a new series character.

The series was condensed into a 130-minute-long movie by combining elements from a few key episodes of the first season. Additional animation was created for the movie (such as the scenes on Iscandar) or recycled from the series' test footage (such as the opening sequence). The movie, which was released in Japan on August 6, 1977, was edited down further and dubbed into English in 1978; entitled "Space Cruiser Yamato" or simply "Space Cruiser", it was only given a limited theatrical release in Europe and Latin America, where it was called ' ("Star Patrol", in Brazilian Portuguese) or ' ("Starship Intrepid", in Spanish), though it was later released on video in most countries.


The success of the "Yamato" movie in Japan eclipsed that of the local release of "Star Wars", leading to the production of a second movie that would end the story. Also going by the name " Yamato", "Farewell to Space Battleship Yamato", set in the year 2201, shows the "Yamato" crew going up against the White Comet Empire, a mobile city fortress called Gatlantis, from the Andromeda Galaxy. A titanic space battle results in the crew going out on a suicide mission to save humanity. The film has been considered as a non-canonical, alternate timeline.

Viewer dissatisfaction with the ending of "Farewell to Space Battleship Yamato" prompted the production of a second "Yamato" television season which retconned the film and presented a slightly different plot against Z≈çdah (Prince Zordar in the "Star Blazers" dub) and his Comet Empire, and ended without killing off the "Yamato" or its primary characters. Like "Farewell", the story is set in the year 2201, and expands the film story to 26 episodes. This second season featured additional plots such as a love story between Teresa (Trelaina) and "Yamato" crew member Daisuke Shima (Mark Venture), and an onboard antagonism between Kodai and Saito (Knox), leader of a group of space marines.

Footage from "Farewell to Space Battleship Yamato" was reused in the second season, particularly in the opening titles. The sequence of the "Yamato" launching from water was also reused in two of the subsequent movies.

The television movie "Yamato: The New Voyage" (aka "Yamato: The New Journey"), came next, featuring a new enemy, the Black Nebula Empire. The story opens in late 2201. In the film, later modified into a theatrical movie, Desslar sees his homeworld, Gamilas, destroyed by the grey-skinned aliens, and its twin planet Iscandar next in line for invasion. He finds an eventual ally in the "Yamato", then on a training mission under deputy captain Kodai.

The theatrical movie "Be Forever Yamato", set in the year 2202, sees the Black Nebula Empire launch a powerful weapon at Earth, a hyperon bomb which will annihilate humanity if they resist a full-scale invasion. The "Yamato", under new captain, Yamanami, travels to the aliens' home galaxy only to discover what appears to be a future Earth‚Äîdefeated and ruled by the enemy. Appearing in this film is Sasha, the daughter of Queen Starsha of Iscandar and Mamoru Kodai (Susumu's older brother).

Following these movies, a third season of the television series was produced, broadcast on Japanese television in 1980. Its date was not mentioned in the broadcast, but design documents, as well as anime industry publications, cited the year 2205. In the story, the Sun is hit by a stray proton missile from a nearby battle between forces of the Galman Empire and Bolar Federation. This missile greatly accelerates nuclear fusion in the Sun, and humanity must either evacuate to a new home or find a means of preventing a supernova. During the course of the story, it is learned that the people of the Galman Empire are actually the forebears of Desslar and the Gamilas race. Desslar and the remnants of his space fleet have found and liberated Galman from the Bolar Federation. Originally conceived as a 52-episode story, funding cuts meant the season had to be truncated to 25 episodes, with a corresponding loss of overall story development. This third season was adapted into English several years after the original "Star Blazers" run and, to the dissatisfaction of fans, used different voice actors than did the earlier seasons.

Premiering in Japanese theaters on March 19, 1983, "Final Yamato" reunites the crew one more time to combat the threat of the Denguilu, a militaristic alien civilization that intends to use the water planet, Aquarius, to flood Earth and resettle there (having lost their home planet to a galactic collision). Captain Okita, who was found to be in cryogenic sleep since the first season, returns to command the "Yamato" and sacrifices himself to stop the Denguili's plan. Susumu and Yuki also get married.

The story is set in the year 2203, contradicting earlier assumptions that its predecessor, "Yamato III", took place in 2205. Having a running time of 163 minutes, "Final Yamato" holds the record of being the longest animated film ever made, a record which has yet to be surpassed as of 2019.

In the mid-1990s, Nishizaki attempted to create a sequel to "Yamato", set hundreds of years after the original. "Yamato 2520" was to chronicle the adventures of the eighteenth starship to bear the name, and its battle against the Seiren Federation. Much of the continuity established in the original series (including the destruction of Earth's moon) is ignored in this sequel.

In place of Leiji Matsumoto, American artist Syd Mead ("‚àÄ Gundam", "Blade Runner", "Tron" and "") provided the conceptual art.

Due to the bankruptcy of Nishizaki's company Office Academy (former Academy Productions), and legal disputes with Matsumoto over the ownership of the "Yamato" copyrights, the series was never finished and only three episodes were produced.

 is a graphic novel comic created by the animator Leiji Matsumoto. For a time it was streaming online. However this has since stopped.

In March 2002, a Tokyo court ruled that Yoshinobu Nishizaki legally owned the "Yamato" copyrights. Nishizaki and Matsumoto eventually settled, and Nishizaki pushed ahead with developing a new Yamato television series. Project proposals for a 26-episode television series were drawn up in early 2004, but no further work was done with Tohoku Shinsha not backing the project. American series expert Tim Eldred was able to secure a complete package of art, mecha designs, and story outline at an auction over Japanese store Mandarake in April 2014.

Set 20 years after "Final Yamato", the series would have shown Susumu Kodai leading a salvage operation for the remains of the Yamato. The ship is rebuilt as the Earth Defense Force builds a second Space Battleship Yamato to combat the Balbard Empire, an alien race that has erected a massive honeycombed cage called Ru Sak Gar, over Earth in a bid to stop the human race's spacefaring efforts. A feature film to be released after the series ended would have featured the original space battleship fighting the Balbards' attempt to launch a black hole at Earth. Kodai, Yuki, and Sanada are the only original series characters who would have returned in the series.

 is the second original animated video based on Space Battleship Yamato

The story begins in 3199, when a mighty enemy attacks the Milky Way from a neighbouring galaxy, and defeats the Milky Way Alliance, reducing them to just six fleets. After the Alliance headquarters is destroyed, and when the collapse of the central Milky Way Alliance is imminent, the Great Yamato "Zero" embarks on a mission to assist the Milky Way Alliance in one last great battle.

Although New Space Battleship Yamato was sent to the discard pile, Nishizaki began work on a new movie titled , set after the original series, while Matsumoto planned a new "Yamato series". However, additional legal conflicts stalled both projects until August 2008, when Nishizaki announced plans for the release of his film on December 12, 2009.

Set 17 years after the events of "Final Yamato", "Resurrection" brings together some members of the "Yamato" crew, who lead Earth's inhabitants to resettle in a far-flung star system after a black hole is discovered, which will destroy the solar system in three months.

Released on December 1, 2010, "Space Battleship Yamato" is the franchise's first live-action film. Directed by Takashi Yamazaki, the movie stars Takuya Kimura as Susumu Kodai and Meisa Kuroki as Yuki. It was revealed originally that the plot would be based on that of the 1974 series. However, an official trailer released during June 2010 on Japanese television has also shown elements from the series' second season (1978).

Debuting in Japanese cinemas on April 7, 2012, "2199" is a remake of the 1974 series. Yutaka Izubuchi serves as supervising director, with character designs by Nobuteru Yuki, and Junichiro Tamamori and Makoto Kobayashi in charge of mecha and conceptual designs. The series is a joint project of Xebec and AIC. Hideaki Anno designed the new series' opening sequence.

The sequel to the first remake heptalogy, and debuting in Japanese Cinemas on February 25, 2017, "2202" is a remake of the second series, with Nobuyoshi Habara as director and Harutoshi Fukui as writer. Most of the staff and original cast from the first remake were brought back to the project. It is animated by Xebec.

With the retelling of " Yamato" as the open-ended "Yamato II" television series (ending in late 2201), " Yamato" was redesignated as a discardable, alternate timeline. The follow-on film, "Yamato: New Journey", took place in late 2201; and its successor, "Be Forever Yamato", in early 2202. "Yamato III" was commonly believed to be set in 2205 (several printed publications used this date, although it was never stated in the show's broadcast). But the following film, "Final Yamato", was set in 2203. The opening narration of "Final" mentioned the Bolar/Galman conflict, implying that the date for "Yamato III" was to be regarded as some time between 2202 and 2203 (making for an unrealistic and compressed timeline).

It is not known if this change was due to the lackluster response to "Yamato III", the production staff's dissatisfaction with the truncated series (additionally, Nishizaki and Matsumoto had limited involvement with it), or a mere oversight.

In 2220, the ship is rebuilt following the events of "Final Yamato". The new captain of the ship is Susumu Kodai, who was the main character in the previous movies. This told in "Space Battleship Yamato: Resurrection" that it is set 17 years after "Final Yamato".

Space Battleship Yamato was a 1985 Japanese exclusive Laserdisc video game designed by Taito which was based on the television series of the same name. "Game Machine" listed "Space Battleship Yamato" on their August 1, 1985 issue as being the second most-successful upright arcade unit of the year.

The "Space Battleship Yamato" series generally involves themes of brave sacrifice, noble enemies, and respect for heroes lost in the line of duty. This can be seen as early as the second episode of the first season, which recounts the defeat of the original battleship "Yamato" while sailors and pilots from both sides salute her as she sinks (this scene was cut from the English dub, but later included on the "Star Blazers" DVD release). The movies spend much time showing the crew visiting monuments to previous missions and recalling the bravery of their fallen comrades. Desslar, the enemy defeated in the first season and left without a home or a people, recognizes that his foes are fighting for the same things he fought for and, eventually, becomes Earth's most important ally.

For many years, English-language releases of the anime bore the title "Space Cruiser Yamato". This romanization has appeared in Japanese publications because Nishizaki, a sailing enthusiast who owned a cruiser yacht, ordered that this translation be used out of love for his boat. However, in reference to naval nomenclature, it is technically inaccurate, as means "battleship" and not "cruiser" (which in Japanese would be ). Leiji Matsumoto's manga adaptation was titled "Cosmoship Yamato". Today, "Yamato" releases, including the Voyager Entertainment DVD, are marketed either as "Star Blazers" or "Space Battleship Yamato".

"Star Blazers" (1979) is a heavily edited dubbed version for the United States market produced by Westchester Film Corporation. Voyager Entertainment released DVD volumes and comic adaptations of the anime years later.



</doc>
<doc id="28957" url="https://en.wikipedia.org/wiki?curid=28957" title="Southern blot">
Southern blot

A Southern blot is a method used in molecular biology for detection of a specific DNA sequence in DNA samples. Southern blotting combines transfer of electrophoresis-separated DNA fragments to a filter membrane and subsequent fragment detection by probe hybridization.

The method is named after the British biologist Edwin Southern, who first published it in 1975. Other blotting methods (i.e., western blot, northern blot, eastern blot, southwestern blot) that employ similar principles, but using RNA or protein, have later been named in reference to Edwin Southern's name. As the label is eponymous, Southern is capitalised, as is conventional of proper nouns. The names for other blotting methods may follow this convention, by analogy.


Hybridization of the probe to a specific DNA fragment on the filter membrane indicates that this fragment contains DNA sequence that is complementary to the probe.
The transfer step of the DNA from the electrophoresis gel to a membrane permits easy binding of the labeled hybridization probe to the size-fractionated DNA. It also allows for the fixation of the target-probe hybrids, required for analysis by autoradiography or other detection methods.
Southern blots performed with restriction enzyme-digested genomic DNA may be used to determine the number of sequences (e.g., gene copies) in a genome. A probe that hybridizes only to a single DNA segment that has not been cut by the restriction enzyme will produce a single band on a Southern blot, whereas multiple bands will likely be observed when the probe hybridizes to several highly similar sequences (e.g., those that may be the result of sequence duplication). Modification of the hybridization conditions (for example, increasing the hybridization temperature or decreasing salt concentration) may be used to increase specificity and decrease hybridization of the probe to sequences that are less than 100% similar.

Southern blotting transfer may be used for homology-based cloning on the basis of amino acid sequence of the protein product of the target gene. Oligonucleotides are designed so that they are similar to the target sequence. The oligonucleotides are chemically synthesized, radiolabeled, and used to screen a DNA library, or other collections of cloned DNA fragments. Sequences that hybridize with the hybridization probe are further analysed, for example, to obtain the full length sequence of the targeted gene.

Southern blotting can also be used to identify methylated sites in particular genes. Particularly useful are the restriction nucleases "MspI" and "HpaII", both of which recognize and cleave within the same sequence. However, "HpaII" requires that a C within that site be methylated, whereas "MspI" cleaves only DNA unmethylated at that site. Therefore, any methylated sites within a sequence analyzed with a particular probe will be cleaved by the former, but not the latter, enzyme.




</doc>
<doc id="28961" url="https://en.wikipedia.org/wiki?curid=28961" title="Standard-gauge railway">
Standard-gauge railway

A standard-gauge railway is a railway with a track gauge of . The standard gauge is also called Stephenson gauge after George Stephenson, International gauge, UIC gauge, uniform gauge, normal gauge and European gauge in Europe. It is the most widely used railway track gauge across the world, with approximately 55% of the lines in the world using it. All high-speed rail lines use standard gauge except those in Russia, Finland, Portugal and Uzbekistan. The distance between the inside edges of the rails is defined to be 1435¬†mm except in the United States and on some heritage British lines, where it is still defined in U.S. customary units as exactly "four feet eight and one half inches".

As railways developed and expanded, one of the key issues was the track gauge (the distance, or width, between the inner sides of the rails) to be used. Different railways used different gauges, and where rails of different gauge met ‚Äì a "gauge break" ‚Äì loads had to be unloaded from one set of rail cars and re-loaded onto another, a time-consuming and expensive process. The result was the adoption throughout a large part of the world of a "standard gauge" of , allowing interconnectivity and interoperability.

A popular legend that has been around since at least 1937 traces the origin of the gauge even further back than the coalfields of northern England, pointing to the evidence of rutted roads marked by chariot wheels dating from the Roman Empire. Snopes categorised this legend as "false", but commented that "it is perhaps more fairly labelled as 'True, but for trivial and unremarkable reasons. The historical tendency to place the wheels of horse-drawn vehicles approximately apart probably derives from the width needed to fit a carthorse in between the shafts. Research however has been undertaken which supports the hypothesis that "the origin of the standard gauge of the railway might result from an interval of wheel ruts of prehistoric ancient carriages".

In addition, while road-travelling vehicles are typically measured from the outermost portions of the wheel rims (and there is some evidence that the first railways were measured in this way as well), it became apparent that for vehicles travelling on rails it was better to have the wheel flanges located "inside" the rails, and thus the distance measured on the inside of the wheels (and, by extension, the inside faces of the rail heads) was the important one.

There was never a standard gauge for horse railways, but there were rough groupings: in the north of England none was less than . Wylam colliery's system, built before 1763, was , as was John Blenkinsop's Middleton Railway; the old plateway was relaid to so that Blenkinsop's engine could be used. Others were (in Beamish) or (in Bigges Main (in Wallsend), Kenton, and Coxlodge).

The English railway pioneer George Stephenson spent much of his early engineering career working for the coal mines of County Durham. He favoured for wagonways in Northumberland and Durham, and used it on his Killingworth line. The Hetton and Springwell wagonways also used this gauge.

Stephenson's Stockton and Darlington railway (S&DR) was built primarily to transport coal from mines near Shildon to the port at Stockton-on-Tees. The initial gauge of was set to accommodate the existing gauge of hundreds of horse-drawn chaldron wagons that were already in use on the wagonways in the mines. The railway used this gauge for 15 years before a change was made to the in gauge. The historic Mount Washington Cog Railway, the world's first mountain-climbing rack railway, is still in operation in the 21st century, and has used the earlier gauge since its inauguration in 1868.

George Stephenson used the gauge (including a belated extra of free movement to reduce binding on curves) for the Liverpool and Manchester Railway, authorised in 1826 and opened 30 September 1830. The success of this project led to Stephenson and his son Robert being employed to engineer several other larger railway projects. Thus the gauge became widespread and dominant in Britain. Robert was reported to have said that if he had had a second chance to choose a standard gauge, he would have chosen one wider than . "I would take a few inches more, but a very few".

During the "gauge war" with the Great Western Railway, standard gauge was called narrow gauge, in contrast to the Great Western's broad gauge. The modern use of the term "narrow gauge" for gauges less than standard did not arise for many years, until the first such locomotive-hauled passenger railway, the Ffestiniog Railway was built.

In 1845, in the United Kingdom of Great Britain and Ireland, a Royal Commission on Railway Gauges reported in favour of a standard gauge. The subsequent Gauge Act ruled that new passenger-carrying railways in Great Britain should be built to a standard gauge of , and those in Ireland to a new standard gauge of . In Great Britain, Stephenson's gauge was chosen on the grounds that existing lines of this gauge were eight times longer than those of the rival (later ) gauge adopted principally by the Great Western Railway. It allowed the broad-gauge companies in Great Britain to continue with their tracks and expand their networks within the "Limits of Deviation" and the exceptions defined in the Act. After an intervening period of mixed-gauge operation (tracks were laid with three rails), the Great Western Railway finally completed the conversion of its network to standard gauge in 1892. In North East England, some early lines in colliery (coal mining) areas were , while in Scotland some early lines were . All these lines had been widened to standard gauge by 1846. The British gauges converged starting from 1846 as the advantages of equipment interchange became increasingly apparent. By the 1890s, the entire network was converted to standard gauge.

The Royal Commission made no comment about small lines narrower than standard gauge (to be called "narrow gauge"), such as the Ffestiniog Railway. Thus it permitted a future multiplicity of narrow gauges in the UK. It also made no comments about future gauges in British colonies, which allowed various gauges to be adopted across the colonies.

Parts of the United States, mainly in the Northeast, adopted the same gauge, because some early trains were purchased from Britain. The American gauges converged, as the advantages of equipment interchange became increasingly apparent. Notably, all the broad gauge track in the South was converted to "almost standard" gauge over the course of two days beginning on 31 May 1886. "See" Track gauge in the United States.

In continental Europe, France and Belgium adopted a gauge (measured between the midpoints of each rail's profile) for their early railways. The gauge between the interior edges of the rails (the measurement adopted from 1844) differed slightly between countries, and even between networks within a country (for example, to in France).
The first tracks in Austria and in the Netherlands had other gauges ( in Austria for the Donau Moldau linen and in the Netherlands for the Hollandsche IJzeren Spoorweg-Maatschappij), but for interoperability reasons (the first rail service between Paris and Berlin began in 1849, first Chaix timetable) Germany adopted standard gauges, as did most other European countries.

The modern method of measuring rail gauge was agreed in the first Berne rail convention of 1886, according to the "Revue g√©n√©rale des chemins de fer, July 1928".






Several states in the United States had laws requiring road vehicles to have a consistent gauge to allow them to follow ruts in the road. Those gauges were similar to railway standard gauge.




</doc>
<doc id="28962" url="https://en.wikipedia.org/wiki?curid=28962" title="Sodium laureth sulfate">
Sodium laureth sulfate

Sodium laureth sulfate (SLES), an accepted contraction of sodium lauryl ether sulfate (SLES), is an anionic detergent and surfactant found in many personal care products (soaps, shampoos, toothpaste, etc.). SLES is an inexpensive and very effective foaming agent. SLES, sodium lauryl sulfate (SLS), ammonium lauryl sulfate (ALS), and sodium pareth sulfate are surfactants that are used in many cosmetic products for their cleaning and emulsifying properties. It is derived from palm kernel oil or coconut oil.

Its chemical formula is . Sometimes the number represented by "n" is specified in the name, for example laureth-2 sulfate. The product is heterogeneous in the number of ethoxyl groups, where "n" is the mean. Laureth-3 sulfate is common in commercial products.

SLES is prepared by ethoxylation of dodecyl alcohol, which is produced industrially from palm kernel oil or coconut oil. The resulting ethoxylate is converted to a half ester of sulfuric acid, which is neutralized by conversion to the sodium salt. The related surfactant sodium lauryl sulfate (also known as sodium dodecyl sulfate or SDS) is produced similarly, but without the ethoxylation step. SLS and ammonium lauryl sulfate (ALS) are commonly used alternatives to SLES in consumer products.

Tests in the US indicate that it is safe for consumer use. The Australian government's Department of Health and Ageing and its National Industrial Chemicals Notification and Assessment Scheme (NICNAS) have determined SLES does not react with DNA.

Like many other detergents, SLES is an irritant. It has also been shown that SLES causes eye or skin irritation in experiments conducted on animals and humans. The related surfactant SLS is a known irritant.

Some products containing SLES contain traces (up to 300 ppm) of 1,4-dioxane, which is formed as a by-product during the ethoxylation step of its production. 1,4-Dioxane is classified by the International Agency for Research on Cancer as a Group 2B carcinogen: "possibly carcinogenic to humans". The United States Food and Drug Administration (FDA) recommends that these levels be monitored, and encourages manufacturers to remove 1,4-dioxane, though it is not required by federal law.



</doc>
<doc id="28965" url="https://en.wikipedia.org/wiki?curid=28965" title="Saraswati River (disambiguation)">
Saraswati River (disambiguation)

The Sarasvati River was one of the Rigvedic rivers that played an important role in the Vedic religion.

Saraswati River may also refer to:


</doc>
<doc id="28967" url="https://en.wikipedia.org/wiki?curid=28967" title="Simpson Desert">
Simpson Desert

The Simpson Desert is a large area of dry, red sandy plain and dunes in Northern Territory, South Australia and Queensland in central Australia. It is the fourth-largest Australian desert, with an area of .

The desert is underlain by the Great Artesian Basin, one of the largest inland drainage areas in the world. Water from the basin rises to the surface at numerous natural springs, including Dalhousie Springs, and at bores drilled along stock routes, or during petroleum exploration. As a result of exploitation by such bores, the flow of water to springs has been steadily decreasing in recent years. It is also part of the Lake Eyre basin.

The Simpson Desert is an erg that contains the world's longest parallel sand dunes. These north-south oriented dunes are static, held in position by vegetation. They vary in height from in the west to around on the eastern side. The largest dune, Nappanerica or Big Red, is in height.

The Wangkangurru people lived in the Simpson Desert using hand-dug wells called mikiri from long before European colonisation up until the Federation Drought.

The explorer Charles Sturt, who visited the region from 1844‚Äì1846, was the first European to see the desert. In 1880 Augustus Poeppel, a surveyor with the South Australian Survey Department determined the border between Queensland and South Australia to the west of Haddon Corner and in doing so marked the corner point where the States of Queensland and South Australia meet the Northern Territory. After he returned to Adelaide, it was discovered that the links in his surveyor's chain had stretched. Poeppel‚Äôs border post was too far west by 300 metres. In 1884, surveyor Larry Wells moved the post to its proper position on the eastern bank of Lake Poeppel. The tri-state border is now known as Poeppel Corner. In January 1886 surveyor David Lindsay ventured into the desert from the western edge, in the process discovering and documenting, with the help of a Wangkangurru Aboriginal man, nine native wells and travelling as far east as the Queensland/Northern Territory border.

In 1936 Ted Colson became the first non-indigenous person to cross the desert in its entirety, riding camels. The name Simpson Desert was coined by Cecil Madigan, after Alfred Allen Simpson, an Australian industrialist, philanthropist, geographer, and president of the South Australian branch of the Royal Geographical Society of Australasia. Mr Simpson was the owner of the Simpson washing machine company.

In 1984, Dennis Bartel was the first white man to successfully walk solo and unsupported west-to-east across the Simpson, 390¬†km in 24 days, relying on old Aboriginal wells for water. In 2006 Lucas Trihey was the first non-indigenous person to walk across the desert through the geographical centre away from vehicle tracks and unsupported. He carried all his equipment in a two-wheeled cart and crossed from East Bore on the western edge of the desert to Birdsville in the east. In 2008, Michael Giacometti completed the first, and only, east-to-west walk across the Simpson Desert. Starting at Bedourie in Queensland, he walked solo and unsupported, towing all his equipment, food and water in a two-wheeled cart to Old Andado homestead. Also in 2008, Belgian Louis-Philippe Loncke became the first non-indigenous person to complete a north-south crossing of the desert on foot and unsupported and through the geographical centre.

In 2016, explorer Sebastian Copeland and partner Mark George completed the longest unsupported latitudinal crossing (west-to-east across the dunes) of the Simpson They linked the Madigan Line, Colson Track and French Line for the first time, walking from Old Andado homestead to Birdsville, a distance of in 26 days.

In 1967, the Queensland Government established the Munga-Thirri National Park, formerly known as the Simpson Desert National Park

No maintained roads cross the desert. The Donohue Highway is an unpaved outback track passing from near Boulia towards the Northern Territory border in the north of the desert. There are tracks that were created during seismic surveys in the search for gas and oil during the 1960s and 1970s. These include the French Line, the Rig Road, and the QAA Line. Such tracks are still navigable by well-equipped four-wheel-drive vehicles which must carry extra fuel and water. Towns providing access to the South Australian edge of the Simpson Desert include Innamincka to the south and Oodnadatta to the southwest; and from the eastern (Queensland) side include Birdsville, Bedourie, Thargomindah and Windorah. Last fuel on the western side is at the Mount Dare hotel and store. Before 1980, a section of the Commonwealth Railways Central Australian line passed along the western side of the Simpson Desert.

The desert is popular with tourists, particularly in winter, and popular landmarks include the ruins and mound springs at Dalhousie Springs, Purnie Bore wetlands, Approdinna Attora Knoll and Poeppel Corner (where Queensland, South Australia and Northern Territory meet). Because of the excessive heat and inadequately experienced drivers attempting to access the desert in the past, the Department of Environment and Natural Resources has decided since 2008-2009 to close the Simpson Desert during the summer ‚Äî to save unprepared "adventurers" from themselves.

The area has an extremely hot, dry desert climate. Rainfall is minimal, averaging only about 150¬†mm per year and falling mainly in summer. Temperatures in summer can approach 50¬†¬∞C and large sand storms are common. Winters are generally cool, however, heatwaves even in the middle of July are not unheard of.

Some of the heaviest rain in decades occurred during 2009-2010, and has seen the Simpson Desert burst into life and colour. In early March 2010, Birdsville recorded more rain in 24 hours than is usual in a whole year. Rain inundated Queensland‚Äôs north-west and Gulf regions. In total, 17 million megalitres of water entered the State‚Äôs western river systems leading to Lake Eyre. In 2010, researchers uncovered the courses of ancient river systems under the desert.

The Simpson Desert is also a large part of the World Wildlife Fund ecoregion of the same name which consists of the Channel Country and the Simpson Strzelecki Dunefields bioregions of the Interim Biogeographic Regionalisation for Australia (IBRA).

The flora of the Simpson Desert ecoregion is limited to drought-resistant shrubs and grasses especially "Zygochloa paradoxa" grass that holds the dunes together and the spinifex and other tough grasses of sides slopes and sandy desert floor between the dunes. The Channel Country section of the ecoregion lies to the northeast of the desert proper around the towns of Bedourie and Windorah in Queensland, and consists of low hills covered with Mitchell grass cut through with rivers lined with coolabah trees. The ecoregion also includes areas of rocky upland and seasonally wet clay and salt pans, particularly Lake Eyre, the centre of one of the largest inland drainage systems in the world, including the Georgina and Diamantina Rivers.

Wildlife adapted to this hot, dry environment and seasonal flooding includes the water-holding frog ("Litoria platycephala") and a number of reptiles that inhabit the desert grasses. Endemic mammals of the desert include the kowari ("Dasycercus byrnei") while birds include the grey grasswren ("Amytornis barbatus") and Eyrean grasswren ("Amytornis goyderi"). Lake Eyre and the other seasonal wetlands are important habitats for fish and birds, especially as a breeding ground for waterbirds while the rivers are home to birds, bats and frogs. The seasonal wetlands of the ecoregion include Lake Eyre and the Coongie Lakes as well as the swamps that emerge when Cooper Creek, Strzelecki Creek and the Diamantina River are in flood. The birds that use these wetlands include the freckled duck ("Stictonetta naevosa"), musk duck ("Biziura lobata"), silver gull ("Larus novaehollandiae"), Australian pelican ("Pelecanus conspicillatus"), great egret ("Ardea alba"), glossy ibis ("Plegadis falcinellus"), and banded stilt ("Cladorhynchus leucocephalus"). Finally the mound springs of the Great Artesian Basin are important habitat for a number of plants, fish, snails and other invertebrates.

Native vegetation is largely intact as the desert is uninhabitable. Therefore, habitats are not threatened by agriculture, but are damaged by introduced species, particularly rabbits and feral camels. The only human activity in the desert proper has been the construction of the gas pipelines, while the country on its fringes has been used for cattle grazing and contains towns such as Innamincka. Mound springs and other waterholes are vulnerable to overuse and damage. Protected areas of the ecoregion include the Simpson Desert, Goneaway, Lochern, Bladensburg, Witjira and Kati Thanda-Lake Eyre National Parks as well as the Munga-Thirri‚ÄîSimpson Desert Conservation Park, Innamincka Regional Reserve and Munga-Thirri‚ÄìSimpson Desert Regional Reserve. Ethabuka Reserve is a nature reserve in the north of the desert owned and managed by Bush Heritage Australia.

The extensive dunefields of the Simpson Desert display a range of colours from brilliant white to dark red and include pinks and oranges.

The sand ridges have a trend of SSE-NNW and continue parallel for kilometres. This pattern is seen throughout the deserts of Australia. Some of the ridges continue unbroken for up to 200¬†km. The height and the spacing between the ridges are directly related. Where there are 5-6 ridges in a kilometre, the height is around 15 metres but when there is one or two ridges per kilometre the height jumps to 35‚Äì38 metres. In cross section, the lee side is the eastern slope with an incline of 34-38 degrees, while the stoss side is the western slope with an incline of only 10-20 degrees. In cross section, the cross beds are planar with foresets alternating between east and west. The foresets have incline angles of 10-30 degrees.

The sand is predominately made up of quartz grains. The grains are rounded and sub angular. They range in size from 0.05¬†mm to 1.2¬†mm with 0.5¬†mm being the average size for the crests and 0.3¬†mm being the average size on the dune flanks. The active crests have sand sediment but on the inter-dunes, the sediment is not as well sorted. The sediment varies in color from pink to brick red, but by the rivers and playas the sediment color is light grey. The progression of the color from grey to red is due to the release of iron oxide from the sediment when weathered.



</doc>
<doc id="28968" url="https://en.wikipedia.org/wiki?curid=28968" title="Skycar">
Skycar

Skycar may refer to:



</doc>
<doc id="28969" url="https://en.wikipedia.org/wiki?curid=28969" title="Silesian Voivodeship">
Silesian Voivodeship

Silesian Voivodeship, or Silesia Province ( ; ; , ) is a voivodeship, or province, in southern Poland, centered on the historic region known as Upper Silesia ("), with Katowice serving as its capital. 

Despite the Silesian Voivodeship's name, most of the historic Silesia region lies outside the present Silesian Voivodeship ‚Äî divided among Lubusz, Lower Silesian, and Opole Voivodeships ‚Äî while the eastern half of Silesian Voivodeship (and, notably, Czƒôstochowa in the north) was historically part of Lesser Poland.

The Voivodeship was created on 1 January 1999 out of the former Katowice, Czƒôstochowa and Bielsko-Bia≈Ça Voivodeships, pursuant to the Polish local government reforms adopted in 1998. 

It is the most densely populated voivodeship in Poland and within the area of 12,300 square kilometres, there are almost 5 million inhabitants. It is also the largest urbanised area in Central and Eastern Europe. In relation to economy, over 13% of Poland's Gross Domestic Product (GDP) is generated here, making the Silesian Voivodeship one of the wealthiest provinces in the country.

For the first time Silesian Voivodeship was appointed in Second Polish Republic. It had much wider range of power autonomy, than other contemporary Polish voivodeships and it covered all historical lands of Upper Silesia, which ended up in the Interwar period Poland (among them: Katowice (Kattowitz), Rybnik (Rybnik), Pszczyna (Ple√ü), Wodzis≈Çaw (Loslau), ≈ªory (Sohrau), Miko≈Ç√≥w (Nikolai), Tychy (Tichau), Kr√≥lewska Huta (K√∂nigsh√ºtte), Tarnowskie G√≥ry (Tarnowitz), Miasteczko ≈ölƒÖskie (Georgenberg), Wo≈∫niki (Woischnik), Lubliniec (Lublinitz), Cieszyn (Teschen), Skocz√≥w (Skotschau), Bielsko (Bielitz)). This Voivodeship did not include ‚Äì as opposed to the present one ‚Äì lands and cities of old pre-Partition Polish‚ÄìLithuanian Commonwealth. Among the last ones the Southern part was included in Krak√≥w Voivodeship ≈ªywiec (Saybusch), Wilamowice (Wilmesau), Bia≈Ça Krakowska (Biala) and Jaworzno), and the North Western part Bƒôdzin (Bendzin), DƒÖbrowa G√≥rnicza (Dombrowa), Sosnowiec (Sosnowitz), Czƒôstochowa (Tschenstochau), Myszk√≥w, Szczekociny (Schtschekotzin), Zawiercie, S≈Çawk√≥w) belonged to Kielce Voivodeship.

After aggression of Nazi Germany (Invasion of Poland), on 8 October 1939, Hitler published a decree "About division and administration of Eastern Territories". A Silesian Province (") was created, with a seat in Breslau (Wroc≈Çaw). It consisted of four districts: Kattowitz, Oppeln, Breslau and Liegnitz.

The following counties were included in Kattowitz District: Kattowitz, K√∂nigsh√ºtte, Tarnowitz, Beuthen Hindenburg, Gleiwitz, Freistadt, Teschen, Biala, Bielitz, Saybusch, Ple√ü, Sosnowitz, Bendzin and parts of the following counties: Kranau, Olkusch, Riebnich and Wadowitz. However, according to Hitler's decree from 12 October 1939 about establishing General Government ("Generalgouvernement"), Tschenstochau (Czƒôstochowa) belonged to GG.

In 1941 the Silesian Province ("") underwent new administrative division and as a result Upper Silesian Province was created ("Provinz Oberschlesien"):

After the War during 1945‚Äì1950 there existed a Silesian Voivodeship, commonly known as ≈ölƒÖsko-DƒÖbrowskie Voivodeship, which included a major part of today's Silesian Voivodeship. In 1950 ≈ölƒÖsko-DƒÖbrowskie Voivodeship was divided into Opole and Katowice Voivodeships. The latter one had borders similar to the borders of modern Silesian Voivodeship.

The present Silesian Voivodeship was formed in 1999 from the following voivodeships of the previous administrative division:

The Silesian Voivodeship borders both the Moravian-Silesian Region (Czech Republic), ≈Ωilina Region (Slovakia) to the south. It is also bordered by four other Polish voivodeships: those of Opole (to the west), ≈Å√≥d≈∫ (to the north), ≈öwiƒôtokrzyskie (to the north-east), and Lesser Poland (to the east).

The region includes the Silesian Upland (') in the centre and north-west, and the Krakowsko-Czƒôstochowska Upland (') in the north-east. The southern border is formed by the Beskidy Mountains (Beskid ≈ölƒÖski and Beskid ≈ªywiecki).

The current administrative unit of Silesian Voivodeship is just a fraction of the historical Silesia which is within the borders of today's Poland (there are also fragments of Silesia in the Czech Republic and Germany). Other parts of today's Polish Silesia are administered as the Opole, the Lower Silesian Voivodeships and the Lubusz Voivodeship. On the other hand, a large part of the current administrative unit of the Silesian Voivodeship is not part of historical Silesia (e.g., Czƒôstochowa, Zawiercie, Myszk√≥w, Jaworzno, Sosnowiec, ≈ªywiec, DƒÖbrowa G√≥rnicza, Bƒôdzin and east part of Bielsko-Bia≈Ça, which are historically parts of Lesser Poland).

Silesian Voivodeship has the highest population density in the country (379 people per square kilometre, compared to the national average of 124). The region's considerable industrialisation gives it the lowest unemployment rate nationally (6.2%). The Silesian region is the most industrialized and the most urbanized region in Poland: 78% of its population live in towns and cities.

Both northern and southern part of the voivodeship is surrounded by a green belt. Bielsko-Bia≈Ça is enveloped by the Beskidy Mountains which are popular with winter sports fans. It offers over 150 ski lifts and 200 kilometres of ski routes. More and more slopes are illuminated and equipped with artificial snow generators. Szczyrk, Brenna, Wis≈Ça and Ustro≈Ñ are the most popular winter mountain resorts. Rock climbing sites can be found in Jura Krakowsko-Czestochowska. The ruins of castles forming the Eagle Nests Trail are a famous attraction of the region. Often visited is the Black Madonna's Jasna G√≥ra Sanctuary in Czƒôstochowa ‚Äì the annual destination of over 4 million pilgrims from all over the world. In south-western part of the voivodeship are parks, palaces and old monastery (Rudy Raciborskie, Wodzis≈Çaw ≈ölƒÖski). Along Oder River are interesting natural reserve and at summer places for swimming.

With its more than two centuries of industrialisation history, region has a number of technical heritage memorials. These include narrow and standard gauge railways, coal and silver mines, shafts and its equipment from 19th and 20th century.

Due to its industrial and urban nature, the voivodeship has many cities and large towns. Of Poland's 40 most-populous cities, 12 are in Silesian Voivodeship. 19 of the cities in the voivodeship have the legal status of "city-county" (see powiat). In all it has 71 cities and towns (with legal city rights), listed below in descending order of population (as of 2019):
The gross domestic product (GDP) of the province was 61 billion ‚Ç¨ in 2018, accounting for 12.3% of the Polish economic output. GDP per capita adjusted for purchasing power was 22,200 ‚Ç¨ or 74% of the EU27 average in the same year. The GDP per employee was 83% of the EU average. Silesia Voivodship is the province with the fourth highest GDP per capita in Poland.

The Silesian voivodship is predominantly an industrial region. Most of the
mining is derived from one of the world's largest bituminous coalfields of the Upper Silesian Industrial District (') and the Rybnik Coal District (') with its major cities Rybnik, Jastrzƒôbie Zdr√≥j, ≈ªory and Wodzis≈Çaw ≈ölƒÖski. Lead and zinc can be found near Bytom, Zawiercie and Tarnowskie G√≥ry; iron ore and raw materials for building ‚Äì near Czƒôstochowa. The most important regional industries are: mining, iron, lead and zinc metallurgy, power industry, engineering, automobile, chemical, building materials and textile. In the past, the Silesian economy was determined by coal mining. Now, considering the investment volume, car manufacturing is becoming more and more important. The most profitable company in the region is Fiat Auto-Poland S.A. in Bielsko-Bia≈Ça with a revenue of PLN 6.2 billion in 1997. Recently a new car factory has been opened by GM Opel in Gliwice. There are two Special Economic Zones in the area: Katowice and Czƒôstochowa. The voivodship's economy consists of about 323,000, mostly small and medium-sized, enterprises employing over 3 million people. The biggest Polish steel-works "Huta Katowice" is situated in DƒÖbrowa G√≥rnicza.

The unemployment rate stood at 3.9% in 2017 and was lower than the national average.

Katowice International Airport (in Tarnowskie G√≥ry County) is used for domestic and international flights, Other Nearby Airports are John Paul II International Airport Krak√≥w-Balice and Warsaw Fr√©d√©ric Chopin Airport. The Silesian agglomeration railway network has the largest concentration in the country. The voivodship capital enjoys good railway and road connections with Gda≈Ñsk (motorway A1) and Ostrava (motorway A1), Krak√≥w (motorway A4), Wroc≈Çaw (motorway A4), ≈Å√≥d≈∫ (motorway A1) and Warsaw. It is also the crossing point for many international routes like E40 connecting Calais, Brussels, Cologne, Dresden, Wroc≈Çaw, Krak√≥w and Kiev and E75 from Scandinavia to the Balkans. A relatively short distance to Vienna facilitates cross-border co-operation and may positively influence the process of European integration.
Linia Hutnicza Szerokotorowa (known by its acronym "LHS", English: "Broad gauge metallurgy line") in S≈Çawk√≥w is the longest broad gauge railway line in Poland. The line runs on a single track for almost 400¬†km from the Polish-Ukrainian border, crossing it just east of Hrubiesz√≥w. It is the westernmost broad gauge railway line in Europe that is connected to the broad gauge rail system of the countries of the former Soviet Union.

There are eleven public universities in the voivodship. The biggest university is the University of Silesia in Katowice, with 43,000 students. The region's capital boasts the Medical University, The Karol Adamiecki University of Economics in Katowice, the University of Music in Katowice, the Physical Education Academy and the Academy of Fine Arts. Czƒôstochowa is the seat of the Czƒôstochowa University of Technology and Pedagogic University. The Silesian University of Technology in Gliwice is nationally renowned. Bielsko-Bia≈Ça is home of the Technical-Humanistic Academy. In addition, 17 new private schools have been established in the region.

There are over 300,000 people currently studying in the Voivodeship.

The Silesian voivodeship's government is headed by the province's ' (governor) who is appointed by the Polish Prime Minister. The ' is then assisted in performing his duties by the voivodeship's marshal, who is the appointed speaker for the voivodeship's executive and is elected by the ' (provincial assembly). The current ' of Silesia is Jaros≈Çaw Wieczorek, whilst the present marshal is Wojciech Sa≈Çuga.

The Sejmik of Silesia consists of 48 members.
Silesian Voivodeship is divided into 36 counties (powiats). These include 19 city counties (far more than any other voivodeship) and 17 land counties. The counties are further divided into 167 gminas.

The counties are listed in the following table (ordering within categories is by decreasing population).

Protected areas in Silesian Voivodeship include eight areas designated as Landscape Parks:




</doc>
<doc id="28970" url="https://en.wikipedia.org/wiki?curid=28970" title="SECD machine">
SECD machine

The SECD machine is a highly influential ("See: #Landin's contribution") virtual machine and abstract machine intended as a target for functional programming language compilers. The letters stand for Stack, Environment, Control, Dump -- the internal registers of the machine. The registers Stack, Control, and Dump point to (some realisations of) stacks, and Environment points to (some realisation of) an associative array. 

The machine was the first to be specifically designed to evaluate lambda calculus expressions. It was originally described by Peter J. Landin in "The Mechanical Evaluation of Expressions" in 1964. The description published by Landin was fairly abstract, and left many implementation choices open (like an operational semantics). Hence the SECD machine is often presented in a more detailed form, such as Peter Henderson's Lispkit Lisp compiler, which has been distributed since 1980. Since then it has been used as the target for several other experimental compilers.

In 1989 researchers at the University of Calgary worked on a hardware implementation of the machine.

D. A. Turner (2012) points out that "The Revised Report on Algol 60" (Naur 1963) specifies a procedure call by a copying rule which avoids variable capture with a systematic change of identifiers. This method works in the Algol 60 implementation, but in a functional programming language where functions are first-class citizens, a free variable on a call stack might be dereferenced in error.

Turner notes that Landin solved this with his SECD machine, in which a function is represented by a closure in the heap instead.

When evaluation of an expression begins, the expression is loaded as the only element of control codice_1. The environment codice_2, stack codice_3 and dump codice_4 begin empty.

During evaluation of codice_1 it is converted to reverse Polish notation (RPN) with codice_6 (for apply) being the only operator. For example, the expression codice_7 (a single list element) is changed to the list codice_8.

Evaluation of codice_1 proceeds similarly to other RPN expressions. If the first item in codice_1 is a value, it is pushed onto the stack codice_3. More exactly, if the item is an identifier, the value pushed onto the stack will be the binding for that identifier in the current environment codice_2. If the item is an abstraction, a closure is constructed to preserve the bindings of its free variables (which are in codice_2), and it is this closure which is pushed onto the stack.

If the item is codice_6, two values are popped off the stack and the application done (first applied to second). If the result of the application is a value, it is pushed onto the stack.

If the application is of an abstraction to a value, however, it will result in a lambda calculus expression which may itself be an application (rather than a value), and so cannot be pushed onto the stack. In this case, the current contents of codice_3, codice_2, and codice_1 are pushed onto the dump codice_4 (which is a stack of these triples), codice_3 is reinitialised to empty, and codice_1 is reinitialised to the application result with codice_2 containing the environment for the free variables of this expression, augmented with the binding that resulted from the application. Evaluation then proceeds as above.

Completed evaluation is indicated by codice_1 being empty, in which case the result will be on the stack codice_3. The last saved evaluation state on codice_4 is then popped, and the result of the completed evaluation is pushed onto the stack contents restored from codice_4. Evaluation of the restored state then continues as above.

If codice_1 and codice_4 are both empty, overall evaluation has completed with the result on the stack codice_3.

The SECD machine is stack-based. Functions take their arguments from the stack. The arguments to built-in instructions are encoded immediately after them in the instruction stream.

Like all internal data-structures, the stack is a list, with the codice_3 register pointing at the list's "head" or beginning. Due to the list structure, the stack need not be a continuous block of memory, so stack space is available as long as there is a single free memory cell. Even when all cells have been used, garbage collection may yield additional free memory. Obviously, specific implementations of the SECD structure can implement the stack as a canonical stack structure, so improving the overall efficiency of the virtual machine, provided that a strict bound be put on the dimension of the stack.

The codice_1 register points at the head of the code or instruction list that will be evaluated. Once the instruction there has been executed, the codice_1 is pointed at the next instruction in the list‚Äîit is similar to an "instruction pointer" (or program counter) in conventional machines, except that subsequent instructions are always specified during execution and are not by default contained in subsequent memory locations, as it is the case with the conventional machines.

The current variable environment is managed by the codice_2 register, which points at a list of lists. Each individual list represents one environment level: the parameters of the current function are in the head of the list, variables that are free in the current function, but bound by a surrounding function, are in other elements of codice_2.

The dump, at whose head the codice_4 register points, is used as temporary storage for values of the other registers, for example during function calls. It can be likened to the return stack of other machines.

The memory organization of the SECD machine is similar to the model used by most functional language interpreters: a number of memory cells, each of which can hold either an "atom" (a simple value, for example "13"), or represent an empty or non-empty list. In the latter case, the cell holds two pointers to other cells, one representing the first element, the other representing the list except for the first element. The two pointers are traditionally named "car" and "cdr" respectively‚Äîbut the more modern terms "head" and "tail" are often used instead. The different types of values that a cell can hold are distinguished by a "tag". Often different types of atoms (integers, strings, etc.) are distinguished as well.

So, a list holding the numbers "1", "2", and "3", usually written as codice_35, might be represented as follows:

The memory cells 3 to 5 do not belong to our list, the cells of which can be distributed randomly over the memory. Cell 2 is the head of the list, it points to cell 1 which holds the first element's value, and the list containing only "2" and "3" (beginning at cell 6). Cell 6 points at a cell holding 2 and at cell 7, which represents the list containing only "3". It does so by pointing at cell 8 containing the value "3", and pointing at an empty list ("nil") as cdr. In the SECD machine, cell 0 always implicitly represents the empty list, so no special tag value is needed to signal an empty list (everything needing that can simply point to cell 0).

The principle that the cdr in a list cell must point at another list is just a convention. If both car and cdr point at atoms, that will yield a pair, usually written like codice_36


A number of additional instructions for basic functions like car, cdr, list construction, integer addition, I/O, etc. exist. They all take any necessary parameters from the stack.




</doc>
<doc id="28971" url="https://en.wikipedia.org/wiki?curid=28971" title="Stratego">
Stratego

Stratego ( ) is a strategy board game for two players on a board of 10√ó10 squares. Each player controls 40 pieces representing individual officer and soldier ranks in an army. The pieces have Napoleonic insignia. The objective of the game is to find and capture the opponent's "Flag", or to capture so many enemy pieces that the opponent cannot make any further moves. "Stratego" has simple enough rules for young children to play but a depth of strategy that is also appealing to adults. The game is a slightly modified copy of an early 20th century French game named "L'Attaque". It has been in production in Europe since World War II and the United States since 1961. There are now two- and four-handed versions, versions with 10, 30 or 40 pieces per player, and boards with smaller sizes (number of spaces). There are also variant pieces and different .

The International Stratego Federation, the game's governing body, sponsors an annual Stratego World Championship.

"Stratego" is from the French or Greek "strategos" (var. "strategus") for leader of an ancient (especially Greek) army; first general.

The name "Stratego" was first registered in 1942 in the Netherlands. The United States trademark was filed in 1958 and registered in 1960 to Jacques Johan Mogendorff and is presently owned by Jumbo Games as successors to Hausemann and Hotte, headquartered in the Netherlands. It has been licensed to manufacturers like Milton Bradley, Hasbro and others, as well as retailers like Barnes & Noble, Target stores, etc.

The game box contents are a set of 40 gold-embossed red playing pieces, a set of 40 silver-embossed blue playing pieces, a glossy folding rectangular cardboard playing board imprinted with a 10√ó10 grid of spaces, and instructions printed in English on the underside of the box top. The early sets featured painted wood pieces, later sets colored plastic. The pieces are small and roughly rectangular, tall and wide, and unweighted. More modern versions first introduced in Europe have cylindrical castle-shaped pieces. Some versions have a cardboard privacy screen to assist setup. A few versions have wooden boxes or boards.

Typically, color is chosen by lot: one player uses red pieces, and the other uses blue pieces. Before the start of the game, players arrange their 40 pieces in a 4√ó10 configuration at either end of the board. The ranks are printed on one side only and placed so that the players cannot identify the opponent's pieces. Players may not place pieces in the lakes or the 12 squares in the center of the board. Such pre-play distinguishes the fundamental strategy of particular players, and influences the outcome of the game.

Players alternate moving; red moves first. Each player moves one piece per turn. A player must move a piece in his turn; there is no "pass" (like that in the game of Go).

Two zones in the middle of the board, each 2√ó2, cannot be entered by either player's pieces at any time. They are shown as lakes on the battlefield and serve as choke points to make frontal assaults less direct.

The game can be won by capturing the opponent's "Flag", or all of his moveable pieces. It is possible to have ranked pieces that are not moveable because they are trapped behind "bomb"s.

The average game has 381 moves. The number of legal positions is 10. The number of possible games is 10. "Stratego" has many more moves and substantially greater complexity than other familiar games like chess and backgammon; however, unlike those games where a single bad move at any point may result in loss of the game, most moves in "Stratego" are inconsequential.

All movable pieces, with the exception of the "Scout", may move only one step to any adjacent space vertically or horizontally (but not diagonally). A piece may not move onto a space occupied by a like-color piece. "Bomb" and "Flag" pieces are not moveable. The "Scout" may move any number of spaces in a straight line (such as the rook in chess). In the older versions of "Stratego" the "Scout" could not move and strike in the same turn; in newer versions this was allowed. Even before that, sanctioned play usually amended the original "Scout" movement to allow moving and striking in the same turn because it facilitates gameplay. No piece can move back and forth between the same two spaces for more than three consecutive turns (two square rule), nor can a piece endlessly chase a piece it has no hope of capturing (more square rule).

When the player wants to attack, they move their piece onto a square occupied by an opposing piece. Both players then reveal their piece's rank; the weaker piece (see exceptions below) is removed from the board. If the engaging pieces are of equal rank, both are removed. A piece may not move onto a square already occupied unless it attacks. Two pieces have special attack powers. One special piece is the "Bomb" which only "Miners" can defuse. It immediately eliminates any other piece striking it, without itself being destroyed. Each player also has one "Spy", which succeeds only if it attacks the "Marshal" or the "Flag". If the "Spy" attacks any other piece, or is attacked by any piece (including the "Marshal"), the "Spy" is defeated. The original rules contained a provision that following a strike, the winning piece immediately occupies the space vacated by the losing piece. This makes sense when the winning piece belongs to the player on move, but no sense when the winning piece belongs to the player not on move. The latter part of the rule has been quietly ignored in most play.

Competitive play does not include recording the game, unlike chess. The game is fast-paced, no standard notation exists, and players keep their initial setups secret, so recording games is impractical. However, digital interfaces like web-based gaming interfaces, may have a facility for recording, replaying and downloading the game. Those interfaces use an algebraic-style notation that numbers the rows ('ranks') 1 to 10 from bottom to top and the columns ('files') A to J from left to right. Alternately, a few interfaces designate the files as A to K, omitting 'I'. Moves are recorded as source square followed by destination square separated by a "-" (move) or "x" (strike). Revealed pieces on strikes precede the square designation, and may be by either rank name or rank number for brevity, for example "major B2xcaptain B3". The bottom half of the board is by default considered to be the 'red' side, and the top half the 'blue' side.

No published compilation of recorded exemplary games (akin to master games in chess) exists.

Unlike chess, "Stratego" is a game of incomplete information. In addition to calculated sequences of moves, this gives rise to aspects of battle psychology like concealment, bluffing, lying in wait and guessing. No exposition of the strategy of "Stratego", either set up or game play, has been published.

There are seven immobile pieces ‚Äì six "Bombs" and one "Flag" ‚Äì and 33 mobile pieces per player. They can move to the adjacent square in horizontal or vertical direction, with exception of the "Scout", which moves any distance. From highest rank to lowest the pieces are:
The higher ranked piece always captures the lower, except when stated otherwise.

Some versions (primarily those released since 2000) make 10 (the "Marshal") the highest rank with the "Spy" ranked 1, while others (versions prior to 2000, as well as the Nostalgia version released in 2002) have the "Marshal" piece ranked at 1 and the "Spy" designated S. The European version depicts pieces with the lower number to be higher ranked, whereas the American version depicts pieces with the higher number to be higher ranked.

Variant versions of the game have a few different pieces with different rules of movement, like the "Cannon", "Archer" (possibly a different name for the "Cannon"), "Spotter", "Infiltrator", "Corporal" and "Cavalry Captain". In one version, mobile pieces are allowed to "carry" the "Flag". In some variants like "Stratego Waterloo" and "Fire and Ice Stratego", all or most of the pieces have substantially different moves; these are essentially different games.

Japanese Military Chess () has been sold and played since as early as 1895, although it is unknown by whom and when it was invented.Dr. Chiristian Junghans reported this game in "Monstshefte" magazine in Germany in 1905. It seems, only after reading his article, Julie Berg took out a patent on a war game in London and Paris in 1907. Similarly, Hermance Edan took a patent for "L'attaque" game in 1909 and sold them in 1910. 
The main differences between Japanese Military Chess and Stratego are:

In nearly its present form "Stratego" appeared in France from La Samaritaine in 1910, and then in Britain before World War I, as a game called "L'attaque". Historian and game collector Thierry Depaulis writes:

It was in fact designed by a lady, Mademoiselle Hermance Edan, who filed a patent for a ""jeu de bataille avec pi√®ces mobiles sur damier"" (a battle game with mobile pieces on a gameboard) on 1908-11-26. The patent was released by the French Patent Office in 1909 (patent #396.795). Hermance Edan had given no name to her game but a French manufacturer named Au Jeu Retrouv√© was selling the game as "L'Attaque" as early as 1910.

Depaulis further notes that the 1910 version was played with 36 pieces per player on a 9√ó10 board and the armies were divided into red and blue colors. The rules of "L'attaque" were basically the same as for the game we know as "Stratego". It featured standing cardboard rectangular pieces, color printed with soldiers who wore contemporary (to 1900) uniforms, not Napoleonic uniforms. In papers of her estate, Ms. Edan states that she developed the game in the 1880s.

"L'attaque" was later produced in England by game maker H.P. Gibson and Sons, who bought the rights to the game in 1925, until the 1970s, at least, retaining the French name at least to begin with.



"Stratego" was created by some time before 1942. The name was registered as a trademark in 1942 by the Dutch company Van Perlstein & Roeper Bosch N.V. (which also produced the first edition of "Monopoly"). After WW2, Mogendorff licensed Stratego to Smeets and Schippers, a Dutch company, in 1946. Hausemann and Hotte acquired a license in 1958 for European distribution, and in 1959 for global distribution. After Mogendorff's death in 1961, Hausemann and Hotte purchased the trademark from his heirs, and sublicensed it to Milton Bradley (which was acquired by Hasbro in 1984) in 1961 for United States distribution. In 2009, Hausemann and Hotte was succeeded by Koninklijke Jumbo B.V. in the Netherlands.

The modern game of "Stratego", with its Napoleonic imagery, was originally manufactured in the Netherlands. Pieces were originally made of printed cardboard and inserted in metal clip stands. After World War II, painted wood pieces became standard. Starting in the early 1960s all versions switched to plastic pieces. The change from wood to plastic was made for economical reasons, as was the case with many products during that period, but with Stratego the change also served a structural function: Unlike the wooden pieces, the plastic pieces were designed with a small base. The wooden pieces had none, often resulting in pieces tipping over. This was disastrous for that player, since it often immediately revealed the piece's rank, as well as unleashing a literal domino effect by having a falling piece knock over other pieces. European versions introduced cylindrical castle-shaped pieces that proved to be popular. American editions later introduced new rectangular pieces with a more stable base and colorful stickers, not images directly imprinted on the plastic.

European versions of the game give the "Marshal" the highest number (10), while the initial American versions give the "Marshal" the lowest number (1) to show the highest value (i.e. it is the #1 or most powerful tile). More recent American versions of the game, which adopted the European system, caused considerable complaint among American players who grew up in the 1960s and 1970s. This may have been a factor in the release of a Nostalgic edition, in a wooden box, reproducing the Classic edition of the early 1970s.

"Electronic Stratego" was introduced by Milton Bradley in 1982. It has features that make many aspects of the game strikingly different from those of classic "Stratego". Each type of playing piece in "Electronic Stratego" has a unique series of bumps on its bottom that are read by the game's battery-operated touch-sensitive "board". When attacking another piece a player hits their Strike button, presses their piece and then the targeted piece: the game either rewards a successful attack or punishes a failed strike with an appropriate bit of music. In this way the players never know for certain the rank of the piece that wins the attack, only whether the attack wins, fails, or ties (similar to the role of the referee in the Chinese game of "Luzhanqi"). Instead of choosing to move a piece, a player can opt to "probe" an opposing piece by hitting the Probe button and pressing down on the enemy piece: the game then beeps out a rough approximation of the strength of that piece. There are no "Bomb" pieces: "Bombs" are set using pegs placed on a touch-sensitive "peg board" that is closed from view prior to the start of the game. Hence, it is possible for a player to have their piece occupying a square with a bomb on it. If an opposing piece lands on the seemingly empty square, the game plays the sound of an explosion and that piece is removed from play. As in classic "Stratego", only a "Miner" can remove a "Bomb" from play. A player who successfully captures the opposing "Flag" is rewarded with a triumphant bit of music from the "1812 Overture".

In the late 1990s, the Jumbo Company released several European variants, including a three- and four-player version, and a new "Cannon" piece (which jumps two squares to capture any piece, but loses to any attack against it). It also included some alternate rules such as "Barrage" (a quicker two-player game with fewer pieces) and "Reserves" (reinforcements in the three- and four-player games). The four-player version appeared in America in 1997.

Starting in the 2000s, Hasbro, under its Milton Bradley label, released a series of popular media-themed Stratego editions.

Besides themed variants with substantially different rules, current production includes three slightly different editions: sets with classic (1961) piece numbering (highest rank=1), sets with European piece numbering (highest rank=10), and sets that allow substitution of one or two variant pieces such as "Cannons", usually in place of scouts. Sets produced since 1970 or so have uniformly adopted the rule that scouts can move and strike in the same turn.

Accolade first introduced a Microsoft DOS-based "Stratego" AI in 1990, but it was not even so good as a rank beginner human player, lacking any apparent strategic conception and making many tactical blunders. Modern AIs exist and compete in various tournaments including the Computer Stratego World Championship, but are currently no better than an intermediate level human player.

A digital Stratego CD-ROM was introduced by Hasbro Interactive in 1998 for Windows 95/98. It included all the games of Ultimate Stratego as well as classic Stratego, and was designed to be used over an LAN, modem-to-modem, or over the internet.

In 2013, Jumbo, together with Keesing Games launched stratego.com, which is free to play online with other players. Since its launch, the site has come to have the largest stratego player base.

"Stratego" and its predecessor "L'Attaque" have spawned several derivative games, notably two 20th century Chinese games, "Game of the fighting animals" ("Dou Shou Qi") also known as Jungle or "Animal Chess", and Land Battle Chess (Lu Zhan Qi).

The game Jungle also has pieces (but of animals rather than soldiers) with different ranks and pieces with higher rank capture the pieces with lower rank. The board, with two lakes in the middle, is also remarkably similar to that in "Stratego". The major differences between the two games is that in Jungle, the pieces are not hidden from the opponent, and the initial setup is fixed. According to historian R.C. Bell, this game is 20th century, and cannot have been a predecessor of "L'Attaque" or "Stratego". 

A modern, more elaborate, Chinese game known as Land Battle Chess (Lu Zhan Qi) or Army Chess (Lu Zhan Jun Qi) is a descendant of Jungle, and a cousin of Stratego: It is played on a 5√ó13 board with two un-occupiable spaces in the middle and each player has 25 playing pieces. The initial setup is not fixed, both players keep their pieces hidden from their opponent, and the objective is to capture the enemy's flag.[2] Lu Zhan Jun Qi's basic gameplay is similar, though differences include "missile" pieces and a xiangqi-style board layout with the addition of railroads and defensive "camps". A third player is also typically used as a neutral referee to decide battles between pieces without revealing their identities. An expanded version of the Land Battle Chess game also exists, adding naval and aircraft pieces and is known as Sea-Land-Air Battle Chess (Hai Lu Kong Zhan Qi).[3]

A capture the flag game called "Stratego" and loosely based on the board game is played at summer camps. In this game, two teams of thirty to sixty players are assigned ranks by distribution of coloured objects such as pinnies or glowsticks, the colours representing rank, not team. Players can tag and capture lower-ranked opponents, with the exception that the lowest rank captures the highest. Players who do not know their teammates may not be able to tell which team other players are on, creating incomplete information and opportunities for bluffing.

Unlike the vast literature for chess, checkers and backgammon, as of 2019, there is a single book, "Stratego: From Beginner To Winner", written by Richard Ratcliffe and published by Steel City Press.

The game remains in production, with new versions continuing to appear every few years. These are a few of the notable ones. In addition, the first U.S. edition (1961) Milton Bradley set, and a special edition 1963 set called "Stratego Fine", had wooden pieces. The 1961 wood pieces had a design that looked like vines scaling a castle wall on the back. But note that later 1961 production featured plastic pieces (not true first editions). All other regular edition sets had plastic pieces. A few special editions as noted below had wooden or metal pieces.

These have 10√ó10 boards, 40 pieces per side with classic pieces and rules of movement.

Official Modern Version: Also known as Stratego Original. Redesigned pieces and game art. The pieces now use stickers attached to new "castle-like" plastic pieces. The stickers must be applied by the player after purchase, though the box does not mention any assembly being required. Rank numbering is reversed in European style (higher numbers equals higher rank). Comes with an optional alternate piece, the "Infiltrator".

Stratego 50th Anniversary 1997 by Spin Master comes in both a book style box and a cookie tin like metal box, with original artwork, pieces and gameplay. Optional "Cannons" (2 per player) playing pieces.

Nostalgia Game Series Edition: Released 2002. Traditional stamped plastic pieces, although the metallic paint is dull and less reflective than some older versions, and the pieces are not engraved as some previous editions were. Wooden box, traditional board and piece numbering.

Library Edition: Hasbro's Library Series puts what appears to be the classic Stratego of the Nostalgia Edition into a compact, book-like design. The wooden box approximates the size of a book and is made to fit in a bookcase in one's library. In this version, the scout may not move and strike in the same turn.

Michael Graves Design Stratego by Milton Bradley introduced in 2002 and sold exclusively through Target Stores. It features a finished wood box, wooden pedestal board, and closed black and white roughly wedge-
shaped plastic pieces. Limited production, no longer available.

Stratego Onyx: Introduced in 2008, Stratego Onyx was sold exclusively by Barnes & Noble. It includes foil stamped wooden game pieces and a raised gameboard with a decorative wooden frame. One-time production, no longer available.

Franklin Mint Civil War Collector's Edition: In the mid-1990s, Franklin Mint created a luxury version of Stratego with a Civil War theme and gold- and silver-plated pieces. Due to a last-minute licensing problem, the set was never officially released and offered for sale. The only remaining copies are those sent to the company's retail stores for display.

These have substantially different configurations and rules.

Ultimate Stratego: No longer in production, this version can still be found at some online stores and specialty gaming stores. This version is a variant of traditional "Stratego" and can accommodate up to 4 players simultaneously. The "Ultimate Stratego" board game contained four different Stratego versions: "Ultimate Lightning", "Alliance Campaign", "Alliance Lightning" and "Ultimate Campaign".

Science Fiction Version: Jumbo B.V. / Spin Master version of "Stratego", common in North American department stores. The game has a futuristic science fiction theme. Played on a smaller 8√ó10 board, with 30 pieces per player. Features unique "Spotter" playing pieces.

Stratego Waterloo: For the bicentenary of the Battle of Waterloo in June 2015, the Dutch publishing group Jumbo published "Stratego Waterloo". Instead of using ranks, the different historical units that had actually fought at the battle were added as "Pawns" (Old Guard, 95th Rifles...) ‚Äì each with their own strengths and weaknesses. The "Pawns" are divided into light infantry, line infantry, light cavalry, heavy cavalry, artillery, commanders and commanders-in-chief (Wellington and Napoleon). Instead of capturing the "Flag", the players must get two of their pawns on the lines of communication of their opponent.

Stratego Conquest: 1996, two- to four-handed game played on world map; alternate pieces cannons and cavalry

Stratego Fortress: A 3D version of "Stratego" featuring a 3-level fortress and mystical themed pieces and maneuvers

Fire and Ice Stratego: The Hasbro version called Fire and Ice Stratego has different pieces and rules of movement. The game features a smaller 8√ó10 board and each player has 30 magical and mythological themed pieces with special powers.

Hertog Jan, a Dutch brand of beer, released "Stratego Tournament", a promotional version of "Stratego" with variant rules. It includes substantially fewer pieces, including only one Bomb and no Miners. Since each side has only about 18 pieces, the pieces are far more mobile. The scout in this version is allowed to move three squares in any combination of directions (including L-shapes) and there is a new piece called the "Archer", which is defeated by anything, but can defeat any piece other than the "Bomb" by shooting it from a two-square distance, in direct orthogonal, or straight, directions only. If one player is unable to move any more of his or her pieces, the game results in a tie because neither player's "Flag" was captured.

These variants are produced by the company with pop culture themed pieces.

Produced by Avalon Hill:

Produced by USAopoly:
"Stratego" is a very competitive game and this competition has increased over the years. There are now many "Stratego" competitions held throughout the world.
The game is particularly popular in the Netherlands, Germany, Greece, and Belgium, where regular world and national championships are organized. The international "Stratego" scene has, more recently, been dominated by players from the Netherlands. Stratego World Championships have been held since 1997 and continue to be held yearly around August; the latest was 2019.

Competitive "Stratego" competitions are now held in all four versions of the game:





World Championships

Other tournaments






</doc>
<doc id="28972" url="https://en.wikipedia.org/wiki?curid=28972" title="Sindh">
Sindh

Sindh (; ; , ) is one of the four provinces of Pakistan. Located in the southeast of the country, it is the home of the Sindhi people. Sindh is the third largest province of Pakistan by area, and second largest province by population after Punjab. Sindh is bordered by Balochistan province to the west, and Punjab province to the north. Sindh also borders the Indian states of Gujarat and Rajasthan to the east, and Arabian Sea to the south. Sindh's landscape consists mostly of alluvial plains flanking the Indus River, the Thar desert in the eastern portion of the province closest to the border with India, and the Kirthar Mountains in the western part of Sindh.

Sindh has Pakistan's second largest economy, while its provincial capital Karachi is Pakistan's largest city and financial hub, and hosts the headquarters of several multinational banks. Sindh is home to a large portion of Pakistan's industrial sector and contains two of Pakistan's commercial seaports, Port Bin Qasim and the Karachi Port. The remainder of Sindh has an agriculture based economy, and produces fruits, food consumer items, and vegetables for the consumption of other parts of the country.

Sindh is known for its distinct culture which is strongly influenced by Sufism, an important marker of Sindhi identity for both Hindus (Sindh has Pakistan's highest percentage of Hindu residents) and Muslims in the province. Several important Sufi shrines are located throughout the province which attract millions of annual devotees.

Sindh is home to two UNESCO World Heritage Sites ‚Äì the Historical Monuments at Makli, and the Archaeological Ruins at Mohenjodaro.

The word "Sindh" is derived from the Sanskrit term "Sindhu" (literally meaning "river"), which is a reference to Indus River.

Southworth suggests that the name "Sindhu" is in turn derived from "Cintu", the Proto-Dravidian word for date palm, a tree commonly found in Sindh.

The official spelling "Sind" (from the Perso-Arabic pronunciation ) was discontinued in 1988 by an amendment passed in Sindh Assembly.

The Greeks who conquered Sindh in 325 BC under the command of Alexander the Great rendered it as "Ind√≥s", hence the modern "Indus". The ancient Iranians referred to everything east of the river Indus as "hind".

Sindh's first known village settlements date as far back as 7000 BC. Permanent settlements at Mehrgarh, currently in Balochistan, to the west expanded into Sindh. This culture blossomed over several millennia and gave rise to the Indus Valley Civilization around 3000 BC. The Indus Valley Civilization rivalled the contemporary civilizations of Ancient Egypt and Mesopotamia in size and scope, numbering nearly half a million inhabitants at its height with well-planned grid cities and sewer systems.

The primitive village communities in Balochistan were still struggling against a difficult highland environment, a highly cultured people were trying to assert themselves at Kot Diji. This was one of the most developed urban civilizations of the ancient world. It flourished between the 25th and 15th centuries BC in the Indus valley sites of Mohenjo Daro and Harappa. The people had a high standard of art and craftsmanship and a well-developed system of quasi-pictographic writing which remains un-deciphered. The ruins of the well planned towns, the brick buildings of the common people, roads, public baths and the covered drainage system suggest a highly organized community.

According to some accounts, there is no evidence of large palaces or burial grounds for the elite. The grand and presumably holy site might have been the great bath, which is built upon an artificially created elevation. This civilization collapsed around 1700 BC for reasons uncertain; the cause is hotly debated and may have been a massive earthquake, which dried up the Ghaggar River. Skeletons discovered in the ruins of Moan Jo Daro ("mount of dead") were thought to indicate that the city was suddenly attacked and the population was wiped out, but further examinations showed that the marks on the skeletons were due to erosion and not of violence.

The ancient city of Roruka, identified with modern Aror/Rohri, was capital of the Sauvira Kingdom, and finds mentioned early Buddhist literature as a major trading center. Sindh finds mention in the Hindu epic "Mahabharata" as being part of Bharatvarsha. Sindh was conquered by the Persian Achaemenid Empire in the 6th century BC. In the late 4th century BC, Sindh was conquered by a mixed army led by Macedonian Greeks under Alexander the Great. The region remained under control of Greek satraps for only a few decades. After Alexander's death, there was a brief period of Seleucid rule, before Sindh was traded to the Mauryan Empire led by Chandragupta in 305 BC. During the rule of the Mauryan Emperor Ashoka, the Buddhist religion spread to Sindh.

Mauryan rule ended in 185 BC with the overthrow of the last king by the Shunga Dynasty. In the disorder that followed, Greek rule returned when Demetrius I of Bactria led a Greco-Bactrian invasion of India and annexed most of the northwestern lands, including Sindh. Demetrius was later defeated and killed by a usurper, but his descendants continued to rule Sindh and other lands as the Indo-Greek Kingdom. Under the reign of Menander I, many Indo-Greeks followed his example and converted to Buddhism.

In the late 2nd century BC, Scythian tribes shattered the Greco-Bactrian empire and invaded the Indo-Greek lands. Unable to take the Punjab region, they invaded South Asia through Sindh, where they became known as Indo-Scythians (later Western Satraps). By the 1st century AD, the Kushan Empire annexed Sindh. Kushans under Kanishka were great patrons of Buddhism and sponsored many building projects for local beliefs. Ahirs were also found in large numbers in Sindh. Abiria country of Abhira tribe was in southern Sindh.

The Kushan Empire was defeated in the mid 3rd century AD by the Sassanid Empire of Persia, who installed vassals known as the Kushanshahs in these far eastern territories. These rulers were defeated by the Kidarites in the late 4th century.

It then came under the Gupta Empire after dealing with the Kidarites. By the late 5th century, attacks by Hephthalite tribes known as the Indo-Hephthalites or "Hunas" (Huns) broke through the Gupta Empire's northwestern borders and overran much of northwestern India. Concurrently, Ror dynasty ruled parts of the region for several centuries.

Afterwards, Sindh came under the rule of Emperor Harshavardhan, then the Rai Dynasty around 478. The Rais were overthrown by Chachar of Alor around 632. The Brahman dynasty ruled a vast territory that stretched from Multan in the north to the Rann of Kutch, Alor was their capital.

The connection between the Sindh and Islam was established by the initial Muslim missions during the Rashidun Caliphate. Al-Hakim ibn Jabalah al-Abdi, who attacked Makran in the year AD 649, was an early partisan of Ali ibn Abu Talib. During the caliphate of Ali, many Jats of Sindh had come under the influence of Shi'ism and some even participated in the Battle of Camel and died fighting for Ali. Under the Umayyads (661 ‚Äì 750 AD), many Shias sought asylum in the region of Sindh, to live in relative peace in the remote area. Ziyad Hindi is one of those refugees.

Muhammad Ali Jinnah claimed that the Pakistan movement started when the first Muslim put his foot on the soil of Sindh, the Gateway of Islam in India.

In 712, Muhammad bin Qasim conquered the Sindh and Indus Valley, bringing South Asian societies into contact with Islam. Dahir was an unpopular Hindu king that ruled over a Buddhist majority and that Chach of Alor and his kin were regarded as usurpers of the earlier Buddhist Rai Dynasty, a view questioned by those who note the diffuse and blurred nature of Hindu and Buddhist practices in the region, especially that of the royalty to be patrons of both and those who believe that Chach may have been a Buddhist. The forces of Muhammad bin Qasim defeated Raja Dahir in alliance with the Hindu Jats and other regional governors.

In 711 AD, Muhammad bin Qasim led an Umayyad force of 20,000 cavalry and 5 catapults. Muhammad bin Qasim defeated the Raja Dahir and captured the cities of Alor, Multan and Debal. Sindh became the easternmost State of the Umayyad Caliphate and was referred to as "Sind" on Arab maps, with lands further east known as "Hind". Muhammad bin Qasim built the city of Mansura as his capital; the city then produced famous historical figures such as Abu Mashar Sindhi, Abu Ata al-Sindhi, Abu Raja Sindhi and Sind ibn Ali. At the port city of Debal, most of the Bawarij embraced Islam and became known as Sindhi Sailors, who were renowned for their navigation, geography and languages. After Bin Qasim left, the Umayyads ruled Sindh through the Habbari dynasty.

By the year 750, Debal (modern Karachi) was second only to Basra; Sindhi sailors from the port city of Debal voyaged to Basra, Bushehr, Musqat, Aden, Kilwa, Zanzibar, Sofala, Malabar, Sri Lanka and Java (where Sindhi merchants were known as the Santri). During the power struggle between the Umayyads and the Abbasids. The Habbari Dynasty became semi independent and was eliminated and Mansura was invaded by Sultan Mahmud Ghaznavi. Sindh then became an easternmost State of the Abbasid Caliphate ruled by the Soomro Dynasty until the Siege of Baghdad (1258). Mansura was the first capital of the Soomra Dynasty and the last of the Habbari dynasty. Muslim geographers, historians and travelers such as al-Masudi, Ibn Hawqal, Istakhri, Ahmed ibn Sahl al-Balkhi, al-Tabari, Baladhuri, Nizami, al-Biruni, Saadi Shirazi, Ibn Battutah and Katip √áelebi wrote about or visited the region, sometimes using the name "Sindh" for the entire area from the Arabian Sea to the Hindu Kush.

When Sindh was under the Arab Umayyad Caliphate, the Arab Habbari dynasty was in control. The Umayyads appointed Aziz al Habbari as the governor of Sindh. Habbaris ruled Sindh until Sultan Mahmud Ghaznavi defeated the Habbaris in 1024. Sultan Mahmud Ghaznavi viewed the Abbasid Caliphate to be the caliphs thus he removed the remaining influence of the Umayyad Caliphate in the region and Sindh fell to Abbasid control following the defeat of the Habbaris. The Abbasid Caliphate then appointed Al Khafif from Samarra; 'Soomro' means 'of Samarra' in Sindhi. The new governor of Sindh was to create a better, stronger and stable government. Once he became the governor, he allotted several key positions to his family and friends; thus Al-Khafif or Sardar Khafif Soomro formed the Soomro Dynasty in Sindh; and became its first ruler. Until the Siege of Baghdad (1258) the Soomro dynasty was the Abbasid Caliphate's functionary in Sindh, but after that it became independent.

When the Soomro dynasty lost ties with the Abbasid Caliphate after the Siege of Baghdad (1258,) the Soomra ruler Dodo-I established their rule from the shores of the Arabian Sea to the Punjab in the north and in the east to Rajasthan and in the west to Pakistani Balochistan. The Soomros were one of the first indigenous Muslim dynasties in Sindh of Parmar Rajput origin. They were the first Muslims to translate the Quran into the Sindhi language. The Soomros created a chivalrous culture in Sindh, which eventually facilitated their rule centred at Mansura. It was later abandoned due to changes in the course of the Puran River; they ruled for the next 95 years until 1351. During this period, Kutch was ruled by the Samma Dynasty, who enjoyed good relations with the Soomras in Sindh. Since the Soomro Dynasty lost its support from the Abbasid Caliphate, the Sultans of Delhi wanted a piece of Sindh. The Soomros successfully defended their kingdom for about 36 years, but their dynasties soon fell to the might of the Sultanate of Delhi's massive armies such as the Tughluks and the Khaljis.

In 1339 Jam Unar founded a Sindhi Muslim Rajput Samma Dynasty and challenged the Sultans of Delhi. He used the title of the "Sultan of Sindh". The Samma tribe reached its peak during the reign of Jam Nizamuddin II (also known by the nickname J√°m Nind√≥). During his reign from 1461 to 1509, Nind√≥ greatly expanded the new capital of Thatta and its Makli hills, which replaced Debal. He patronized Sindhi art, architecture and culture. The Samma had left behind a popular legacy especially in architecture, music and art. Important court figures included the poet Kazi Kadal, Sardar Darya Khan, Moltus Khan, Makhdoom Bilawal and the theologian Kazi Kaadan. However, Thatta was a port city; unlike garrison towns, it could not mobilize large armies against the Arghun and Tarkhan Mongol invaders, who killed many regional Sindhi Mirs and Amirs loyal to the Samma. Some parts of Sindh still remained under the Sultans of Delhi and the ruthless Arghuns and the Tarkhans sacked Thatta during the rule of Jam Ferozudin.

According to Dr. Akhtar Baloch, Professor at University of Karachi, and Nadeem Wagan, General Manager at HANDS, the Balochi migrated from Balochistan during the Little Ice Age and settled in Sindh and Punjab. The Little Ice Age is conventionally defined as a period extending from the sixteenth to the nineteenth centuries, or alternatively, from about 1300 to about 1850. According to Professor Baloch, the climate of Balochistan was very cold during this epoch and the region was uninhabitable during the winters so the Baloch people emigrated in waves to Sindh and Punjab.

In the year 1524, the few remaining Sindhi Amirs welcomed the Mughal Empire and Babur dispatched his forces to rally the Arghuns and the Tarkhans, branches of a Turkic dynasty. In the coming centuries, Sindh became a region loyal to the Mughals, a network of forts manned by cavalry and musketeers further extended Mughal power in Sindh. In 1540 a mutiny by Sher Shah Suri forced the Mughal Emperor Humayun to withdraw to Sindh, where he joined the Sindhi Emir Hussein Umrani. In 1541 Humayun married Hamida Banu Begum, who gave birth to the infant Akbar at Umarkot in the year 1542.

During the reign of Akbar the Great, Sindh produced scholars and others such as Mir Ahmed Nasrallah Thattvi, Tahir Muhammad Thattvi and Mir Ali Sir Thattvi and the Mughal chronicler Abu'l-Fazl ibn Mubarak and his brother the poet Faizi was a descendant of a Sindhi Shaikh family from Rel, Siwistan in Sindh. Abu'l-Fazl ibn Mubarak was the author of "Akbarnama" (an official biographical account of Akbar) and the "Ain-i-Akbari" (a detailed document recording the administration of the Mughal Empire).

Shah Jahan carved a subah (imperial province), covering Sindh, called Thatta after its capital, out of Multan, further bordering on the Ajmer and Gujarat subahs as well as the rival Persian Safavid empire.

During the Mughal period, Sindhi literature began to flourish and historical figures such as Shah Abdul Latif Bhittai, Sulatn-al-Aoliya Muhammad Zaman and Sachal Sarmast became prominent throughout the land. In 1603 Shah Jahan visited the State of Sindh; at Thatta, he was generously welcomed by the locals after the death of his father Jahangir. Shah Jahan ordered the construction of the Shahjahan Mosque, which was completed during the early years of his rule under the supervision of Mirza Ghazi Beg. During his reign, in 1659 in the Mughal Empire, Muhammad Salih Tahtawi of Thatta created a seamless celestial globe with Arabic and Persian inscriptions using a wax casting method.

Sindh was home to several wealthy merchant-rulers such as Mir Bejar of Sindh, whose great wealth had attracted the close ties with the Sultan bin Ahmad of Oman.

In the year 1701, the Kalhora Nawabs were authorized in a firman by the Mughal Emperor Aurangzeb to administer subah Sindh.

From 1752 to 1762, Marathas collected Chauth or tributes from Sindh. Maratha power was decimated in the entire region after the Third Battle of Panipat in 1761. In 1762, Mian Ghulam Shah Kalhoro brought stability in Sindh, he reorganized and independently defeated the Marathas and their prominent vassal the "Rao of Kuch" in the Thar Desert and returned victoriously.

After the Sikhs annexed Multan, the Kalhora Dynasty supported counterattacks against the Sikhs and defined their borders.

In 1783 a firman which designated Mir Fateh Ali Khan Talpur as the new "Nawab of Sindh", and mediated peace particularly after the Battle of Halani and the defeat of the ruling Kalhora by the Talpur Baloch tribes.

The Talpur dynasty was established by members of the Talpur tribe. The Talpur tribes migrated from Dera Ghazi Khan in Punjab to Sindh on the invitation of Kalhora to help them organize unruly Baloch tribes living in Sindh. Talpurs, who learned the Sindhi language, settled in northern Sindh. Very soon they united all the Baloch tribes of Sindh and formed a confederacy against the Kalhora Dynasty.

Four branches of the dynasty were established following the defeat of the Kalhora dynasty at the Battle of Halani in 1743: one ruled lower Sindh from the city of Hyderabad, another ruled over upper Sindh from the city of Khairpur, a third ruled around the eastern city of Mirpur Khas, and a fourth was based in Tando Muhammad Khan. The Talpurs were ethnically Baloch, and Shia by faith. They ruled from 1783, until 1843, when they were in turn defeated by the British at the Battle of Miani and Battle of Dubbo. The northern Khairpur branch of the Talpur dynasty, however, continued to maintain a degree of sovereignty during British rule as the princely state of Khairpur, whose ruler elected to join the new Dominion of Pakistan in October 1947 as an autonomous region, before being fully amalgamated in the West Pakistan in 1955.

In 1802, when Mir Ghulam Ali Khan Talpur succeeded as the Talpur Nawab, internal tensions broke out in the state. As a result, the following year the Maratha Empire declared war on Sindh and Berar Subah, during which Arthur Wellesley took a leading role causing much early suspicion between the Emirs of Sindh and the British Empire. The British East India Company made its first contacts in the Sindhi port city of Thatta, which according to a report was:
"a city as large as London containing 50,000 houses which were made of stone and mortar with large verandahs some three or four stories high ... the city has 3,000 looms ... the textiles of Sindh were the flower of the whole produce of the East, the international commerce of Sindh gave it a place among that of Nations, Thatta has 400 schools and 4,000 Dhows at its docks, the city is guarded by well armed Sepoys".

British and Bengal Presidency forces under General Charles James Napier arrived in Sindh in the mid-19th century and conquered Sindh in February 1843. The Baloch coalition led by Talpur under Mir Nasir Khan Talpur was defeated at the Battle of Miani during which 5,000 Talpur Baloch were killed. Shortly afterwards, Hoshu Sheedi commanded another army at the Battle of Dubbo, where 5,000 Baloch were killed.

The first Agha Khan (was escaping persecution from Persia and looking for a foothold in the British Raj) he helped the British in their conquest of Sindh. As a result, he was granted a lifetime pension.

A British journal by Thomas Postans mentions the captive Sindhi Amirs: "The Amirs as being the prisoners of 'Her Majesty'... they are maintained in strict seclusion; they are described as Broken-Hearted and Miserable men, maintaining much of the dignity of fallen greatness, and without any querulous or angry complaining at this unlivable source of sorrow, refusing to be comforted". Within weeks, Charles Napier and his forces occupied Sindh.

After 1853 the British divided Sindh into districts and later made it part of British India's Bombay Presidency.

In the year 1868, the Bombay Presidency assigned "Narayan Jagannath Vaidya" to replace the Abjad used in Sindhi, with the "Khudabadi script". The script was decreed a standard script by the Bombay Presidency thus inciting anarchy in the Muslim majority region. A powerful unrest followed, after which Twelve Martial Laws were imposed by the British authorities.

The Bombay Presidency caused the rise of rebels such as Sibghatullah Shah Rashidi pioneered the Sindhi Muslim Hur Movement against the British Raj. He was hanged on 20March 1943 in Hyderabad, Sindh. His burial place is not known.

During the British period, railways, printing presses and bridges were introduced in the province. Writers like Mirza Kalich Beg compiled and traced the literary history of Sindh.

Although Sindh had a culture of religious syncretism, communal harmony and tolerance due to Sindh's strong Sufi culture in which both Sindhi Muslims and Sindhi Hindus partook, the mostly Muslim peasantry was oppressed by the Hindu moneylending class and also by the landed Muslim elite. Sindhi Muslims eventually demanded the separation of Sindh from the Bombay Presidency, a move opposed by Sindhi Hindus.

By 1936 Sindh was separated from the Bombay Presidency. Elections in 1937 resulted in local Sindhi Muslim parties winning the bulk of seats. By the mid-1940s the Muslim League gained a foothold in the province and after winning over the support of local Sufi "pirs", it didn't take long for the overwhelming majority of Sindhi Muslims to campaign for the creation of Pakistan.

Sindh has the 2nd highest Human Development Index out of all of Pakistan's provinces at 0.628. The 2017 Census of Pakistan indicated a population of 47.9 million.

The major ethnic group of the province is the Sindhis, but there is also a significant presence of other groups. Sindhis of Baloch origin make up about 30% of the total Sindhi population (although they speak Sindhi Saraiki as their native tongue), while Urdu-speaking Muhajirs make up over 19% of the total population of the province, while Punjabi are 10% and Pashtuns represent 7%. In August 1947, before the partition of India, the total population of Sindh was 3,887,070 out of which 2,832,000 were Muslims and 1,015,000 were Hindus

Islam in Sindh has a strong Sufi ethos with numerous Muslim saints and mystics, such as the Sufi poet Shah Abdul Latif Bhittai, having lived in Sindh historically. One popular legend which highlights the strong Sufi presence in Sindh is that 125,000 Sufi saints and mystics are buried on Makli Hill near Thatta. The development of Sufism in Sindh was similar to the development of Sufism in other parts of the Muslim world. In the 16th century two Sufi tareeqat (orders) ‚Äì Qadria and Naqshbandia ‚Äì were introduced in Sindh. Sufism continues to play an important role in the daily lives of Sindhis.

Sindh also has Pakistan's highest percentage of Hindu residents, with 8.9% of Sindh's population overall, and 13.56% of Sindh's rural population, classifying itself as Hindu, and a majority of residents in Tharparkar District identifying themselves as Hindu. The communal harmony between Sindhi Muslims and Hindus is an example of Sindh's pluralistic and tolerant Sufi culture.

There are approximately 10,000 Sikhs in Sindh.

According to the 2017 census, the most widely spoken language in the province is Sindhi, the first language of % of the population. It is followed by Urdu (%), Pashto (%), Punjabi (%), Saraiki (%) and Balochi (2%).

Other languages with substantial numbers of speakers include Kutchi and Gujarati. Other minority languages include Aer, Bagri, Bhaya, Brahui, Dhatki, Ghera, Goaria, Gurgula, Jadgali, Jandavra, Jogi, Kabutra, Kachi Koli, Parkari Koli, Wadiyari Koli, Loarki, Marwari, Sansi, and Vaghri.

According to the 1998 census, 7.3% of people Karachi's residents are Sindhi-speaking. However, since the last few decades, every year thousands of Sindhi speaking from the rural areas are moving and settling to the Karachi due to which population of the Sindhis is increasing drastically. Karachi is 40% populated by Muhajirs who speak Urdu. Other immigrant communities in Karachi are Pashtuns from Khyber Pakhtunkhwa, Punjabis from Punjab and other linguistic groups from various regions of Pakistan.

Sindh is in the western corner of South Asia, bordering the Iranian plateau in the west. Geographically it is the third largest province of Pakistan, stretching about from north to south and (extreme) or (average) from east to west, with an area of of Pakistani territory. Sindh is bounded by the Thar Desert to the east, the Kirthar Mountains to the west and the Arabian Sea and Rann of Kutch to the south. In the centre is a fertile plain along the Indus River.

The province is mostly arid with scant vegetation except for the irrigated Indus Valley. The dwarf palm, "Acacia Rupestris" (kher), and "Tecomella undulata" (lohirro) trees are typical of the western hill region. In the Indus valley, the "Acacia nilotica" (babul) (babbur) is the most dominant and occurs in thick forests along the Indus banks. The "Azadirachta indica" (neem) (nim), "Zizyphys vulgaris" (bir) (ber), "Tamarix orientalis" (jujuba lai) and "Capparis aphylla" (kirir) are among the more common trees.

Mango, date palms and the more recently introduced banana, guava, orange and chiku are the typical fruit-bearing trees. The coastal strip and the creeks abound in semi-aquatic and aquatic plants and the inshore Indus delta islands have forests of "Avicennia tomentosa" (timmer) and "Ceriops candolleana" (chaunir) trees. Water lilies grow in abundance in the numerous lake and ponds, particularly in the lower Sindh region.

Among the wild animals, the Sindh ibex (sareh), blackbuck, wild sheep (Urial or gadh) and wild bear are found in the western rocky range. The leopard is now rare and the Asiatic cheetah extinct. The Pirrang (large tiger cat or fishing cat) of the eastern desert region is also disappearing. Deer occur in the lower rocky plains and in the eastern region, as do the striped hyena (charakh), jackal, fox, porcupine, common gray mongoose and hedgehog. The Sindhi phekari, red lynx or Caracal cat, is found in some areas. Phartho (hog deer) and wild bear occur, particularly in the central inundation belt. There are bats, lizards and reptiles, including the cobra, lundi (viper) and the mysterious Sindh krait of the Thar region, which is supposed to suck the victim's breath in his sleep.
Some unusual sightings of Asian cheetah occurred in 2003 near the Balochistan border in Kirthar Mountains. The rare houbara bustard find Sindh's warm climate suitable to rest and mate. Unfortunately, it is hunted by locals and foreigners.

Crocodiles are rare and inhabit only the backwaters of the Indus, eastern Nara channel and Karachi backwater. Besides a large variety of marine fish, the plumbeous dolphin, the beaked dolphin, rorqual or blue whale and skates frequent the seas along the Sindh coast. The Pallo (Sable fish), a marine fish, ascends the Indus annually from February to April to spawn. The Indus river dolphin is among the most endangered species in Pakistan and is found in the part of the Indus river in northern Sindh. Hog deer and wild bear occur, particularly in the central inundation belt.

Although Sindh has a semi arid climate, through its coastal and riverine forests, its huge fresh water lakes and mountains and deserts, Sindh supports a large amount of varied wildlife. Due to the semi-arid climate of Sindh the left out forests support an average population of jackals and snakes. The national parks established by the Government of Pakistan in collaboration with many organizations such as World Wide Fund for Nature and Sindh Wildlife Department support a huge variety of animals and birds. The Kirthar National Park in the Kirthar range spreads over more than 3000¬†km of desert, stunted tree forests and a lake. The KNP supports Sindh ibex, wild sheep (urial) and black bear along with the rare leopard. There are also occasional sightings of The Sindhi phekari, ped lynx or Caracal cat. There is a project to introduce tigers and Asian elephants too in KNP near the huge Hub Dam Lake. Between July and November when the monsoon winds blow onshore from the ocean, giant olive ridley turtles lay their eggs along the seaward side. The turtles are protected species. After the mothers lay and leave them buried under the sands the SWD and WWF officials take the eggs and protect them until they are hatched to keep them from predators.

Sindh lies in a tropical to subtropical region; it is hot in the summer and mild to warm in winter. Temperatures frequently rise above between May and August, and the minimum average temperature of occurs during December and January in the northern and higher elevated regions. The annual rainfall averages about seven inches, falling mainly during July and August. The southwest monsoon wind begins in mid-February and continues until the end of September, whereas the cool northerly wind blows during the winter months from October to January.

Sindh lies between the two monsoons‚Äîthe southwest monsoon from the Indian Ocean and the northeast or retreating monsoon, deflected towards it by the Himalayan mountains‚Äîand escapes the influence of both. The region's scarcity of rainfall is compensated by the inundation of the Indus twice a year, caused by the spring and summer melting of Himalayan snow and by rainfall in the monsoon season.

Sindh is divided into three climatic regions: Siro (the upper region, centred on Jacobabad), Wicholo (the middle region, centred on Hyderabad), and Lar (the lower region, centred on Karachi). The thermal equator passes through upper Sindh, where the air is generally very dry. Central Sindh's temperatures are generally lower than those of upper Sindh but higher than those of lower Sindh. Dry hot days and cool nights are typical during the summer. Central Sindh's maximum temperature typically reaches . Lower Sindh has a damper and humid maritime climate affected by the southwestern winds in summer and northeastern winds in winter, with lower rainfall than Central Sindh. Lower Sindh's maximum temperature reaches about . In the Kirthar range at and higher at Gorakh Hill and other peaks in Dadu District, temperatures near freezing have been recorded and brief snowfall is received in the winters.

The Provincial Assembly of Sindh is a unicameral and consists of 168 seats, of which 5% are reserved for non-Muslims and 17% for women. The provincial capital of Sindh is Karachi. The provincial government is led by Chief Minister who is directly elected by the popular and landslide votes; the Governor serves as a ceremonial representative nominated and appointed by the President of Pakistan. The administrative boss of the province who is in charge of the bureaucracy is the Chief Secretary Sindh, who is appointed by the Prime Minister of Pakistan. Most of the influential Sindhi tribes in the province are involved in Pakistan's politics.

In addition, Sindh's politics leans towards the left-wing and its political culture serves as a dominant place for the left-wing spectrum in the country. The province's trend towards the Pakistan Peoples Party and away from the Pakistan Muslim League (N) can be seen in nationwide general elections, in which, Sindh is a stronghold of the Pakistan Peoples Party (PPP). The PML(N) has a limited support due to its centre-right agenda.

In metropolitan cities such as Karachi and Hyderabad, the MQM (another left-wing party with the support of "Muhajirs") has a considerable vote bank and support. Minor leftist parties such as People's Movement also found support in rural areas of the province.

In 2008, after the public elections, the new government decided to restore the structure of Divisions of all provinces. In Sindh after the lapse of the Local Governments Bodies term in 2010 the Divisional Commissioners system was to be restored.
In July 2011, following excessive violence in the city of Karachi and after the political split between the ruling PPP and the majority party in Sindh, the MQM and after the resignation of the MQM Governor of Sindh, PPP and the Government of Sindh decided to restore the commissionerate system in the province. As a consequence, the five divisions of Sindh were restored ‚Äì namely Karachi, Hyderabad, Sukkur, Mirpurkhas and Larkana with their respective districts. Subsequently, two new divisions have been added in Sindh, Banbore and Nawab Shah/Shaheed Benazirabad division.

Karachi district has been de-merged into its five original constituent districts: Karachi East, Karachi West, Karachi Central, Karachi South and Malir. Recently Korangi has been upgraded to the status of the sixth district of Karachi. These six districts form the Karachi Division now.

Sindh has the second largest economy in Pakistan. A 2016 study commissioned by Pakistan Ministry of Planning found that urban Sindh and northern Punjab province are the most prosperous regions in Pakistan. Its GDP per capita was $1,400 in 2010 which is 50 percent more than the rest of the nation or 35 percent more than the national average. Historically, Sindh's contribution to Pakistan's GDP has been between 30% to 32.7%. Its share in the service sector has ranged from 21% to 27.8% and in the agriculture sector from 21.4% to 27.7%. Performance wise, its best sector is the manufacturing sector, where its share has ranged from 36.7% to 46.5%.

Endowed with coastal access, Sindh is a major centre of economic activity in Pakistan and has a highly diversified economy ranging from heavy industry and finance centred in Karachi to a substantial agricultural base along the Indus. Manufacturing includes machine products, cement, plastics, and other goods.

Agriculture is very important in Sindh with cotton, rice, wheat, sugar cane, dates, bananas, and mangoes as the most important crops. The largest and finer quality of rice is produced in Larkano district.

The following is a chart of the education market of Sindh estimated by the government in 1998:
Major public and private educational institutes in Sindh include:

The rich culture, art and architectural landscape of Sindh have fascinated historians. The culture, folktales, art and music of Sindh form a mosaic of human history.

Sindh has a rich heritage of traditional handicraft that has evolved over the centuries. Perhaps the most professed exposition of Sindhi culture is in the handicrafts of Hala, a town some 30 kilometres from Hyderabad. Hala's artisans manufacture high-quality and impressively priced wooden handicrafts, textiles, paintings, handmade paper products, and blue pottery. Lacquered wood works known as Jandi, painting on wood, tiles, and pottery known as Kashi, hand weaved textiles including "khadi", "susi", and "ajraks" are synonymous with Sindhi culture preserved in Hala's handicraft.

The work of Sindhi artisans was sold in ancient markets of Damascus, Baghdad, Basra, Istanbul, Cairo and Samarkand. Referring to the lacquer work on wood locally known as Jandi, T. Posten (an English traveller who visited Sindh in the early 19th century) asserted that the articles of Hala could be compared with exquisite specimens of China. Technological improvements such as the spinning wheel (charkha) and treadle (pai-chah) in the weaver's loom were gradually introduced and the processes of designing, dyeing and printing by block were refined. The refined, lightweight, colourful, washable fabrics from Hala became a luxury for people used to the woollens and linens of the age.

Non-governmental organisations (NGOs) such as the World Wildlife Fund, Pakistan, play an important role to promote the culture of Sindh. They provide training to women artisans in Sindh so they get a source of income. They promote their products under the name of "Crafts Forever". Many women in rural Sindh are skilled in the production of caps. Sindhi caps are manufactured commercially on a small scale at New Saeedabad and Hala New. Sindhi people began celebrating Sindhi Topi Day on 6 December 2009, to preserve the historical culture of Sindh by wearing Ajrak and Sindhi topi.

Tourist sites include the ruins of Mohenjo-daro near the city of Larkana, Runi Kot, Kot Deji, the Jain temples of Nangar Parker and the historic temple of Sadhu Bela, Sukkur. Islamic architecture is quite prominent in the province; its numerous mausoleums include the ancient Shahbaz Qalander mausoleum.





</doc>
<doc id="28975" url="https://en.wikipedia.org/wiki?curid=28975" title="Super Bowl III">
Super Bowl III

Super Bowl III was the third AFL‚ÄìNFL Championship Game in professional American football, and the first to officially bear the trademark name "Super Bowl". Played on January 12, 1969, at the Orange Bowl in Miami, Florida, the game is regarded as one of the greatest upsets in both American football history and in the recorded history of sports. The 18-point underdog American Football League (AFL) champion New York Jets defeated the National Football League (NFL) champion Baltimore Colts by a score of 16‚Äì7.

This was the first Super Bowl victory for the AFL. Before the game, most sports writers and fans believed that AFL teams were less talented than NFL clubs, and expected the Colts to defeat the Jets by a wide margin. Baltimore posted a 13‚Äì1 record in the regular season and shut out the Cleveland Browns 34‚Äì0 in the NFL Championship Game. The Jets were 11‚Äì3 in the regular season, and defeated the Oakland Raiders 27‚Äì23 in the AFL Championship Game.

Jets quarterback Joe Namath famously made an appearance three days before the Super Bowl at the Miami Touchdown Club and personally guaranteed his team's victory. His team backed up his words by controlling most of the game, building a 16‚Äì0 lead by the fourth quarter off of a touchdown run by Matt Snell and three field goals by Jim Turner. Colts quarterback Earl Morrall threw three interceptions before being replaced by Johnny Unitas, who then led Baltimore to its only touchdown, during the last few minutes of the game. With the victory, the Jets were the only winning team to score only one touchdown (either offensive, defensive, or special teams) until the New England Patriots in Super Bowl LIII. Namath, who completed 17 out of 28 passes for 206 yards, was named as the Super Bowl's most valuable player, making him the first player in Super Bowl history to be declared MVP without personally scoring or throwing for a touchdown.

The game was awarded to Miami on May 14, 1968, at the owners meetings held in Atlanta.

The National Football League (NFL) had dominated professional football from its origins after World War I. Rival leagues had crumbled or merged with it, and when the American Football League (AFL) began to play in 1960, it was the fourth to hold that similar name to challenge the older NFL. Unlike its earlier namesakes, however, this AFL was able to command sufficient financial resources to survive; one factor in this was becoming the first league to sign a television contract‚Äîpreviously, individual franchises had signed agreements with networks to televise games. The junior league proved successful enough, in fact, to make attractive offers to players. After the 1964 season, in fact, there had been a well-publicized bidding war which culminated with the signing, by the AFL's New York Jets (formerly New York Titans), of Alabama quarterback Joe Namath for an unprecedented contract. Fearing that bidding wars over players would become the norm, greatly increasing labor costs, NFL owners, ostensibly led by league Commissioner Pete Rozelle, obtained a merger agreement with the AFL in June 1966, which provided for a common draft, interleague play in the pre-season, a world championship game to follow each season, and the integration of the two leagues into one in a way to be agreed at a future date. As the two leagues had an unequal number of teams (under the new merger agreement, the NFL expanded to sixteen in , and the AFL to ten in 1968), realignment was advocated by some owners, but was opposed. Eventually, three NFL teams (Cleveland Browns, Pittsburgh Steelers, and the Baltimore Colts) agreed to move over to join the ten AFL franchises in the American Football Conference.

Despite the ongoing merger, it was a commonly held view that the NFL was a far superior league. This was seemingly confirmed by the results of the first two interleague championship games, in January 1967 and 1968, in which the NFL champion Green Bay Packers, coached by the legendary Vince Lombardi, easily defeated the AFL's Kansas City Chiefs and Oakland Raiders. Although publicized as the inter-league championship games, it was not until later that the moniker for this championship contest between the now two conferences (National and American) began having the nickname of "Super Bowl" applied to it by the media and later began being counted by using Roman numerals, the creation of the term being credited to the founder of the AFL, Lamar Hunt.

The Baltimore Colts had won the 1958 and 1959 NFL championships under Coach Weeb Ewbank. In the following years, however, the Colts failed to make the playoffs, and the Colts dismissed Ewbank after a 7‚Äì7 record in 1962. He was soon hired by New York's new AFL franchise, which had just changed its name from the Titans to the Jets. In Ewbank's place, Baltimore hired an untested young head coach, Don Shula, who would also go on to become one of the game's greatest coaches. The Colts did well under Shula, despite losing to the Cleveland Browns in the 1964 NFL Championship Game and, in 1965, losing in overtime to the Green Bay Packers in a tie-breaker game to decide the NFL Western Conference title. The Colts finished a distant second in the West to the Packers in 1966, and in 1967, with the NFL realigned into four divisions of four teams each, went undefeated with two ties through their first 13 games, but lost the game and the Coastal Division championship to the Los Angeles Rams on the final Sunday of the season‚Äîunder newly instituted tiebreakers procedures, L.A. won the division championship as it had better net points in the two games the teams played (the Rams win and an earlier tie). The Colts finished 11‚Äì1‚Äì2, tied for the best record in the league, but were excluded from the playoffs. In 1968, Shula and the Colts were considered a favorite to win the NFL championship again, which carried with it an automatic berth what was now becoming popularly known as the "Super Bowl" against the champion of the younger AFL. The NFL champion, in both cases the Green Bay Packers, had easily won the first two Super Bowls (1967 and 1968) over the AFL winner, establishing for a while then the superiority of the older NFL circuit.

Baltimore's quest for a championship seemed doomed from the start when long-time starting quarterback Johnny Unitas suffered a pre-season injury to his throwing arm and was replaced by Earl Morrall, a veteran who had started inconsistently over the course of his 12 seasons with four teams. But Morrall would go on to have the best year of his career, leading the league in passer rating (93.2) during the regular season. His performance was so impressive that Colts coach Don Shula decided to keep Morrall in the starting lineup after Unitas was healthy enough to play. The Colts had won ten games in a row, including four shutouts, and finished the season with an NFL-best 13‚Äì1 record. In those ten games, they had allowed only seven touchdowns. Then, the Colts avenged their sole regular-season loss against the Cleveland Browns by crushing them 34‚Äì0 in the NFL Championship Game.

The Colts offense ranked second in the NFL in points scored (402). Wide receivers Jimmy Orr (29 receptions, 743 yards, 6 touchdowns) and Willie Richardson (37 receptions, 698 yards, 8 touchdowns) provided Baltimore with two deep threats, with Orr averaging 25.6 yards per catch, and Richardson averaging 18.9. Tight end John Mackey also recorded 45 receptions for 644 yards and 5 touchdowns. Pro Bowl running back Tom Matte was the team's top rusher with 662 yards and 9 touchdowns. He also caught 25 passes for 275 yards and another touchdown. Running backs Terry Cole and Jerry Hill combined for 778 rushing yards and 236 receiving yards.

The Colts defense led the NFL in fewest points allowed (144, tying the then all-time league record), and ranked third in total rushing yards allowed (1,339). Bubba Smith, a 6'7" 295-pound defensive end considered the NFL's best pass rusher, anchored the line. Linebacker Mike Curtis was considered one of the top linebackers in the NFL. Baltimore's secondary consisted of defensive backs Bobby Boyd (8 interceptions), Rick Volk (6 interceptions), Lenny Lyles (5 interceptions), and Jerry Logan (3 interceptions). The Colts were the only NFL team to routinely play a zone defense. That gave them an advantage in the NFL because the other NFL teams were inexperienced against a zone defense. (This would not give them an advantage over the upstart New York Jets, however, because zone defenses were common in the AFL and the Jets knew how to attack them.)

The New York Jets, led by head coach Weeb Ewbank (who was the head coach of the Colts when they won the famous 1958 NFL Championship game and later the '59 title also), finished the season with an 11‚Äì3 regular season record (one of the losses was to the Oakland Raiders in the infamous "Heidi Game") and had to rally to defeat those same Raiders, 27‚Äì23, in a thrilling AFL Championship Game.

Jets quarterback Joe Namath threw for 3,147 yards during the regular season and completed 49.2 percent of his passes, but threw more interceptions (17) than touchdowns (15). Still, he led the offense effectively enough for them to finish the regular season with more total points scored (419) than Baltimore, and finished fourth in completion percentage, fifth in touchdown passes, and third in passing yards as one of only three quarterbacks to pass for over 3,000 yards in the AFL that season. More importantly, Namath usually found ways to win. For example, late in the fourth quarter of the AFL championship game, Namath threw an interception that allowed the Raiders to take the lead. But he then made up for his mistake by completing 3 consecutive passes on the ensuing drive, advancing the ball 68 yards in just 55 seconds to score a touchdown to regain the lead for New York. Future Hall of Fame wide receiver Don Maynard caught the game-winning pass in the end zone but strained his hamstring on the play.

The Jets had a number of offensive weapons that Namath used. Maynard had the best season of his career, catching 57 passes for 1,297 yards (an average of 22.8 yards per catch) and 10 touchdowns. Wide receiver George Sauer Jr. recorded 66 receptions for 1,141 yards and 3 touchdowns. The Jets rushing attack was also effective. Fullback Matt Snell, a power runner, was the top rusher on the team with 747 yards and 6 touchdowns, while elusive halfback Emerson Boozer contributed 441 yards and 5 touchdowns. Meanwhile, kicker Jim Turner made 34 field goals and 43 extra points for a combined total of 145 points.

The Jets defense led the AFL in total rushing yards allowed (1,195). Gerry Philbin, Paul Rochester, John Elliott, and Verlon Biggs anchored the defensive line. The Jets linebacking core was led by middle linebacker Al Atkinson. The secondary was led by defensive backs Johnny Sample (a former Colt who played on their 1958 NFL Championship team) who recorded 7 interceptions, and Jim Hudson, who recorded 5.

Several of the Jets' players had been cut by NFL teams. Maynard had been cut by the New York Giants after they lost the 1958 NFL Championship Game to the Colts. "I kept a little bitterness in me," he says. Sample had been cut by the Colts. "I was almost in a frenzy by the time the game arrived," he says. "I held a private grudge against the Colts. I was really ready for that game. All of us were." Offensive tackle Winston Hill had been cut five years earlier by the Colts as a rookie in training camp. "Ordell Braase kept making me look bad in practice," he says. Hill would be blocking Braase in Super Bowl III.

At an all-night party to celebrate the Jets victory over the Raiders at Namath's nightclub, Bachelors III, Namath poured champagne over Johnny Carson as the talk show host commented, "First time I ever knew you to waste the stuff."

The Colts advanced to the Super Bowl with two dominating wins. First, they jumped to a 21‚Äì0 fourth quarter lead against the Minnesota Vikings and easily held off their meager comeback attempt in the final period for a 24‚Äì14 win.

Then they faced the Cleveland Browns, who had defeated them in week 5 of the regular season. But in this game, they proved to be no challenge as Baltimore held them to just 173 total yards and only allowed them to cross midfield twice in the entire game. Matte scored three of the Colts four rushing touchdowns as the team won easily, 34-0.

Meanwhile, New York in the AFL championship game faced a red hot Oakland Raiders team who had just defeated the Kansas City Chiefs 41‚Äì6 one week earlier, with quarterback Daryle Lamonica throwing five touchdown passes. The championship game was close and hard fought the whole way through, with both teams trading scores at a relatively even pace. The momentum seemed to swing in the Raiders' favor when George Atkinson picked off a pass from Namath and returned it 32 yards to the Jets 5-yard line, setting up a touchdown that gave Oakland their first lead of the game at 23‚Äì20 with 8:18 left in regulation. But Namath quickly led the team back, completing a 10-yard pass to Sauer and a 52-yard pass to Maynard on the Raiders' six-yard line. On the next play, his six-yard touchdown pass to Maynard gave them a 27‚Äì23 lead they would never relinquish. Oakland's final three possessions of the game would result in a turnover on downs, a lost fumble, and time expiring in the game.

After the Jets' AFL championship victory, Namath stated to "The New York Times" sportswriter Dave Anderson, "There are five quarterbacks in the AFL who are better than Morrall." The five were himself, his backup Babe Parilli, Lamonica, John Hadl of the San Diego Chargers, and Bob Griese of the Miami Dolphins. Namath added, "You put Babe Parilli with Baltimore instead of Morrall and Baltimore might be better. Babe throws better than Morrall."

Despite the Jets' accomplishments, AFL teams were generally not regarded as having the same caliber of talent as NFL teams. However, three days before the game, an intoxicated Namath appeared at the Miami Touchdown Club and boldly predicted to the audience, "We're gonna win the game. I guarantee it". Coach Ewbank later joked that he "could have shot" Namath for the statement. Namath made his famous "guarantee" in response to a rowdy Colts supporter at the club, who boasted the Colts would easily defeat the Jets. Namath said he never intended to make such a public prediction, and would not have done so if he had not been confronted by the fan. Sportswriter Dave Anderson did not think that the remark was notable because, he recalled, Namath had said similar things during the week ("I know we're gonna win" for example), but an article by Luther Evans of the "Miami Herald" made the statement famous. Namath's comments and subsequent performance in the game itself are one of the more famous instances in NFL lore.
The Colts, linebacker Curtis recalled, "sort of laughed at" Namath's guarantee. The team did not adjust the defense it had used during the season against the Jets because "that should be good enough," Curtis said. The AFL champions shared the confident feelings of their quarterback. According to Matt Snell, all of the Jets, not just Namath, were insulted and angry that they were 18-point underdogs. Most of the Jets considered the Raiders, whom they barely beat (27‚Äì23) in the AFL title game, a better team than the Colts. Indeed, watching films of the Colts and in preparation for the game, Jets coaching staff and offensive players noted that their offense was particularly suited against the Colts defense. The Colts defensive schemes relied on frequent blitzing, which covered up weak points in pass coverage. The Jets had an automatic contingency for such blitzes by short passing to uncovered tight ends or backs. After a film session the Wednesday prior to the game, Jets tight end Pete Lammons, a Texas native, was heard to drawl, "Damn, y'all, we gotta stop watching these films. We gonna get overconfident".

The game was broadcast in the United States by NBC Sports ‚Äì at the time, still a "Service of NBC News" ‚Äì with Curt Gowdy handling the play-by-play duties and joined by color commentators Al DeRogatis and Kyle Rote in the broadcast booth. Also helping with NBC's coverage were Jim Simpson (reporting from the sidelines) and Pat Summerall, on loan from CBS (helping conduct player interviews for the pregame show, along with Rote). In an interview later done with NFL Films, Gowdy called it the most memorable game he ever called because of its historical significance.

While the Orange Bowl was sold out for the game, the live telecast was not shown in Miami due to both leagues' unconditional blackout rules at the time.

This game is thought to be the earliest surviving Super Bowl game preserved on videotape in its entirety, save for a portion of the Colts' fourth quarter scoring drive. The original NBC broadcast was aired as part of the NFL Network "Super Bowl Classics" series.

"Mr. Football" was the title of the pregame show, which featured marching bands playing "Mr. Touchdown U.S.A." as people in walking footballs representing all NFL and AFL teams except the Jets and Colts were paraded, after which performers representing a Jets player and a Colts player appeared on top of a large, multi-layered, smoke topped cake. Astronauts of the Apollo 8 mission (Frank Borman, Jim Lovell, and William Anders), the first manned flight around the Moon, which had returned to Earth just 18 days prior to the game, then led the Pledge of Allegiance. Lloyd Geisler, first trumpeter of the Washington National Symphony Orchestra, performed the national anthem. The Florida A&M University band was featured during the "America Thanks" halftime show.

New York entered the game with their primary deep threat, wide receiver Don Maynard, playing with a pulled hamstring. But his 112-yard, two touchdown performance against the Oakland Raiders in the AFL championship game made the Colts defense pay special attention to him, not realizing he was injured. Using Maynard as a decoy‚Äîhe had no receptions in the game‚ÄîJoe Namath was able to take advantage of single coverage on wide receiver George Sauer Jr.. (After studying the Colts' zone defense, Ewbank had told his receivers, "Find the dead spots in the zone, hook up, and Joe will hit you.") The Jets had a conservative game plan, emphasizing the run as well as short high-percentage passes to minimize interceptions. Meanwhile, with the help of many fortunate plays, the Jets defense kept the Colts offense from scoring for most of the game. Also, Baltimore had a distinctly older group of players with 10+ years experience (Braase, Shinnick, Lyles, Boyd) on their defense's right side versus New York's younger, bigger left offensive side (Hill, Talamini, Schmitt, Sauer)--and back Snell when running left behind left tackle Hill, who thoroughly defeated defensive end Braase.

Namath recalled that he did not become "dead serious" until, on the sideline before the game, he saw Unitas. The Jets, led by captains Namath and Johnny Sample, and Colts, led by captains Preston Pearson, Unitas, and Lyles, met at midfield where referee Tom Bell announced that the Jets had won the coin toss and had elected to receive the football. The coin toss had been conducted an hour prior to kickoff but this was done for the benefit of the spectators. Colts kicker Lou Michaels kicked the ball off to Earl Christy who returned the ball 25 yards to the Jets' 23-yard line. Namath handed the ball off to Snell on first down who carried it 3 yards. On second down, Snell carried the ball for 9 yards, earning the Jets their first first down of the game. Colts' free safety Rick Volk sustained a concussion when he tackled Snell and was subsequently lost for the game. On the ensuing play, Emerson Boozer lost four yards when he was tackled behind the line of scrimmage by Don Shinnick. Namath threw his first pass to Snell that gained 9 yards on 2nd and 14, but a 2-yard loss by Snell on the following play forced the Jets to punt the ball. The Jets noticed, however, from watching film the predictability of the Colts' defense based on how players lined up. Instead of calling plays in the huddle, Namath usually gave formations to his team and operated from the line of scrimmage. Center John Schmitt recalled that the Colts were "in shock" and "it drove them crazy ... no matter what [the Colts] did, [Snell] would run it the other way".
The Colts began their first offensive series on their own 27-yard line. Quarterback Earl Morrall completed a 19-yard pass to tight end John Mackey and then running back Tom Matte ran for 10 yards to place the ball on the Jets' 44-yard line. Jerry Hill's runs of 7 and 5 yards picked up another Colts first down, then Morrall's pass to tight end Tom Mitchell gained 15 yards on third and thirteen and saw the ball placed at the Jets' 19-yard line. In scoring position, Morrall attempted to score quickly against a reeling Jets defense. Receiver Willie Richardson dropped Morrall's pass on first down followed by an incompletion on second down after Mitchell was overthrown. On third down, none of his receivers were open and Morrall was tackled at the line of scrimmage by Al Atkinson. Michaels was brought out to attempt a 27-yard field goal, but it was wide left. "You could almost feel the steam go out of them", said Snell.

The Jets did not only rely on Snell; Namath said "if they're going to blitz, then we're going to throw". Shula said that Namath "beat our blitz" with his fast release, which let him quickly dump the football off to a receiver. On the Jets' second possession, Namath threw deep to Maynard, who, despite his pulled hamstring, was open by a step. The ball was overthrown, but this one play helped change the outcome of the game. Fearing the speedy Maynard, the Colts decided to rotate their zone defense to help cover Maynard, leaving Sauer covered one-on-one by Lenny Lyles, helping Sauer catch 8 passes for 133 yards, including a crucial third quarter 39-yard reception that kept a scoring drive alive. The Jets kept rushing Snell to their strong left, rushing off tackle with Boozer blocking the linebacker, and gained first down after first down as the Colts defense gave ground. The Colts defense was more concerned about Maynard, the passing game, and the deep threat of a Namath to Maynard touchdown. Although the Colts were unaware of Maynard's injury, the Jets were aware that Lyles had been weakened by tonsillitis all week, causing them great glee when they saw the one-on-one matchup with Sauer.

With less than two minutes left in the period, Colts punter David Lee booted a 51-yard kick that pinned the Jets back at their own 4-yard line. Three plays later, Sauer caught a 3-yard pass from Namath, but fumbled while being tackled by Lyles, and Baltimore linebacker Ron Porter recovered it at New York's 12-yard line.

However, on third down (the second play of the second quarter), Morrall's pass was tipped by Jets linebacker Al Atkinson, bounced crazily, high into the air off tight end Tom Mitchell, and was intercepted by Jets cornerback Randy Beverly in the end zone for a touchback. "That was the game in a nutshell," says Matte. Starting from their own 20-yard line, Snell rushed on the next 4 plays, advancing the ball 26 yards. The Jets would have success all day running off left tackle behind the blocking of Winston Hill, who, according to Snell, was overpowering 36-year-old defensive end Ordell Braase, the man who had tormented the rookie Hill in Colts' training camp. Said Snell, "Braase pretty much faded out." Namath later completed 3 consecutive passes, moving the ball to the Colts 23-yard line. Boozer gained just 2 yards on the next play, but Snell followed it up with a 12-yard reception at the 9-yard line and a 5-yard run to the 4-yard line, and capped the drive with a 4-yard touchdown run, once again off left tackle. The score gave the Jets a 7‚Äì0 lead, and marked the first time in history that an AFL team led in the Super Bowl.

On Baltimore's ensuing drive, a 30-yard completion from Morrall to running back Tom Matte helped the Colts advance to the New York 42-yard line, but they once again failed to score as Jets cornerback Johnny Sample broke up Morrall's third down pass and Michaels missed his second field goal attempt, this time from 46 yards. Two plays after the Jets took over following the missed field goal, Namath's 36-yard completion to Sauer enabled New York to eventually reach the Baltimore 32-yard line. But Namath then threw two incompletions, and was sacked on third down by Colts linebacker Dennis Gaubatz for a 2-yard loss. New York kicker Jim Turner tried to salvage the drive with a 41-yard field goal attempt, but he missed.

On their next possession, Baltimore went from their own 20-yard line to New York's 15-yard line in three plays, aided by Matte's 58-yard run. However, with 2 minutes left in the half, Morrall was intercepted again, by Sample at the Jets' 2-yard line, deflating the Colts considerably. The Jets then were forced to punt on their ensuing drive, and the Colts advanced the ball to New York's 41-yard line. What followed is one of the most famous plays in Super Bowl history. Baltimore tried a flea flicker play, which had a huge impact on the momentum of the game. Matte ran off right tackle after taking a handoff, then pitched the ball back to Morrall. The play completely fooled the NBC Camera Crew, and the Jets defense, leaving receiver Jimmy Orr wide open near the end zone. However, Morrall failed to spot him and instead threw a pass intended for running back Jerry Hill that was intercepted by Jets safety Jim Hudson as time expired, maintaining the Jets' 7‚Äì0 lead at halftime. Earlier in the season, against the Atlanta Falcons, on the same play, Morrall had completed the same pass for a touchdown to Orr, the play's intended target. "I was the primary receiver," Orr said later. "Earl said he just didn't see me. I was open from here to Tampa." "I'm just a lineman, but I looked up and saw Jimmy open," added center Bill Curry. "I don't know what happened." Some speculated that Morrall couldn't see Orr because the Florida A&M marching band (in blue uniforms similar to the Colts) was gathering behind the end zone for the halftime show.

The third quarter belonged to the Jets, who controlled the ball for all but three minutes of the period. Baltimore ran only seven offensive plays all quarter, gaining only 11 yards. Matte lost a fumble on the first play from scrimmage in the second half, yet another demoralizing event, which was recovered by linebacker Ralph Baker on the Colts 33-yard line, leading to Turner's 32-yard field goal to increase the Jets' lead, 10‚Äì0. Then after forcing the Colts to punt again, Namath completed 4 passes for 40 yards to set up Turner's 30-yard field goal to increase the lead, 13‚Äì0. On that drive, Namath temporarily went out of the game after injuring his right thumb, and was replaced by backup quarterback Babe Parilli for a few plays. Namath returned by the end of the third quarter, but the Jets would not run a pass play for the entire fourth quarter.

Matt Snell said, "By this time, the Colts were pressing. You saw the frustration and worry on all their faces." After Turner's second field goal, with 4 minutes left in the third quarter, Colts head coach Don Shula took Morrall out of the game and put in the sore-armed Johnny Unitas to see if he could provide a spark to Baltimore's offense. Unitas could not get the Colts offense moving on their next drive and they were forced to punt again after 3 plays.

Aided by a 39-yard pass from Namath to Sauer, the Jets drove all the way to the Colts 2-yard line. Baltimore's defense would not quit, and kept them out of the end zone. Turner kicked his third field goal early in the final period to make the score 16‚Äì0.

The Colts' inability to score made Namath so confident by the fourth quarter, that he told Ewbank that he preferred to run out the clock instead of playing aggressively. Namath did not throw any passes in the quarter. On Baltimore's next possession, they managed to drive all the way to the Jets' 25-yard line. However, Beverly ended the drive by intercepting a pass from Unitas in the end zone, the Jets' fourth interception of the game. New York then drove to the Colts 35-yard line with seven consecutive running plays, but ended up with no points after Turner missed a 42-yard field goal attempt.

Unitas started out the next drive with three incomplete passes, but completed a key 17-yard pass to Orr on fourth down. Ten plays later, aided by three Jets penalties, Baltimore finally scored a touchdown on a 1-yard run by Hill to cut their deficit to 16‚Äì7, but with only 3:19 left in the game. The Colts then recovered an onside kick and drove to the Jets 19-yard line with 3 consecutive completions by Unitas, but his next 3 passes fell incomplete. Instead of kicking a field goal and attempting another onside kick (which would have been necessary in the end), they opted to throw on 4th down, and the pass fell incomplete, turning the ball over on downs. That ended any chance of a Baltimore comeback, as the Jets ran the ball for six plays before being forced to punt.
When the Colts got the ball back, only 8 seconds remained in the game. The Colts then attempted two more passes before the game ended. Matt Snell said, "Leaving the field, I saw the Colts were exhausted and in a state of shock. I don't remember any Colt coming over to congratulate me". As he ran off the field, Namath, in a spontaneous show of defiance held up his index finger, signaling "number one"; "the only time I ever did that in my life", he said.

Namath finished the game having completed 17 of his 28 passes. He is the only quarterback to win Super Bowl MVP without throwing a touchdown pass. Snell rushed for 121 yards on 30 carries with a touchdown, and caught 4 passes for 40 yards. Sauer caught eight passes for 133 yards. Beverly became the first player in Super Bowl history to record two interceptions. Morrall had a terrible game‚Äîjust 6 of 17 completions for 71 yards, with 3 interceptions. Through 51 games, he had the third worst passer rating in Super Bowl history, with a 9.3, one of only 3 ratings below 10. Despite not being put into the game until late in the third quarter, Unitas finished with more pass completions (11) and passing yards (110) than Morrall, but he also threw one interception. Matte was the Colts' top rusher with 116 yards on just 11 carries, an average of 10.5 yards per run, and caught 2 passes for 30 yards. The Colts were minus-4 in turnovers throwing four interceptions, all of which were deep in Jet territory.

When Sal Marchiano asked Namath in the locker room if he was the "king of the hill", Namath replied "No, no, we're king of the hill. We got the team, brother". Morrall later said, "I thought we would win handily. We'd only lost twice in our last 30 games. I'm still not sure what happened that day at the Orange Bowl, however; it's still hard to account for." Snell wrote, "The most distinct image I have from that whole game is of Ordell Braase and some other guys‚Äînot so much Mike Curtis--having a bewildered look".

Sources: NFL.com Super Bowl III, Super Bowl III Play Finder NYJ, Super Bowl III Play Finder Bal

Completions/Attempts
Carries
Long gain
Receptions
Times targeted

The following records were set or tied in Super Bowl III, according to the official NFL.com boxscore and the Pro-Football-Reference.com game summary. Some records have to meet NFL minimum number of attempts to be recognized. The minimums are shown (in parenthesis).


Turnovers are defined as the number of times losing the ball on interceptions and fumbles.

Source:


Unlike the first two Super Bowls, officials wore their standard uniform. The AFL switched to the NFL uniform for 1968 in anticipation of the 1970 merger.

Jack Reader became the first official to work two Super Bowls. He was the only official to work two prior to the merger. He was promoted to referee in 1969.

The following season, 1969, would be the last one before the AFL-NFL merger. The AFL's Kansas City Chiefs would go on to defeat the NFL's Minnesota Vikings in Super Bowl IV. That victory by the AFL squared the Super Bowl series with the NFL at two games apiece before the two leagues merged into one.

As part of the merger, the Colts were one of three NFL teams that moved to the newly formed American Football Conference (AFC) with the Jets and the other AFL teams (the other two were the Cleveland Browns and Pittsburgh Steelers, who were first division rivals in the AFC Central and later the AFC North, with a three-year gap that resulted from the Browns' controversial relocation to Baltimore interrupting it). The former Super Bowl III combatants became divisional rivals in the AFC East until the 2002 realignment shifted the Colts, who had moved to Indianapolis in 1984, to the new AFC South. The teams would however not meet in the playoffs until the 2002 season. And being in the same conference, they can no longer meet in a Super Bowl rematch unless the NFL radically changes its conference alignment or its playoff structure.

The Jets have never gone back to the Super Bowl since the merger, only reaching as far as the AFC Championship Game in the 1982, 1998, 2009 and 2010 seasons. On the other hand, the Colts won Super Bowl V (1970), then after relocating to Indianapolis they won Super Bowl XLI (2006) and lost Super Bowl XLIV (2009).

However, teams representing Baltimore and New York have contested one Super Bowl since the merger: Super Bowl XXXV between the Jets' crosstown rival (the Giants) and Baltimore's replacement team (the Ravens), with the latter contest being won by Baltimore.

This was the first of three occasions in which a team from New York defeated one from Baltimore in postseason play during 1969, with the Knicks eliminating the Bullets in the NBA playoffs, and the Mets upsetting the heavily-favored Orioles in the World Series, being the other two.

This was the last postseason victory for the Jets until they beat the Cincinnati Bengals in the 1982‚Äì83 playoffs.




</doc>
<doc id="28976" url="https://en.wikipedia.org/wiki?curid=28976" title="Super Bowl XX">
Super Bowl XX

Super Bowl XX was an American football game between the National Football Conference (NFC) champion Chicago Bears and the American Football Conference (AFC) champion New England Patriots to decide the National Football League (NFL) champion for the 1985 season. The Bears defeated the Patriots by the score of 46‚Äì10, capturing their first NFL championship since 1963, three years prior to the birth of the Super Bowl. Super Bowl XX was played on January 26, 1986 at the Louisiana Superdome in New Orleans.

This was the fourth Super Bowl and, to date, the last time in which both teams made their Super Bowl debuts. The Bears entered the game after becoming the second team in NFL history to win 15 regular season games. With their then-revolutionary 46 defense, Chicago led the league in several defensive categories, outscored their opponents with a staggering margin of 456‚Äì198, and recorded two postseason shutouts. The Patriots were considered a Cinderella team during the 1985 season, and posted an 11‚Äì5 regular season record, but entered the playoffs as a wild card because of tiebreakers. But defying the odds, New England posted three road playoff wins to advance to Super Bowl XX.

In their victory over the Patriots, the Bears set or tied Super Bowl records for sacks (seven), fewest rushing yards allowed (seven), and margin of victory (36 points). At the time, New England broke the record for the quickest lead in Super Bowl history, with Tony Franklin's 36-yard field goal 1:19 into the first quarter after a Chicago fumble. But the Patriots were eventually held to negative yardage (‚àí19) throughout the entire first half, and finished with just 123 total yards from scrimmage, the second lowest total yards in Super Bowl history, behind the Minnesota Vikings (119 total yards) in Super Bowl IX. Bears defensive end Richard Dent, who had 1.5 quarterback sacks, forced two fumbles, and blocked a pass, was named the game's Most Valuable Player (MVP).

The telecast of the game on NBC was watched by an estimated 92.57 million viewers. To commemorate the 20th Super Bowl, all previous Super Bowl MVPs were honored during the pregame ceremonies.

NFL owners awarded the hosting of Super Bowl XX to New Orleans, Louisiana on December 14, 1982, at an owners meeting held in Dallas. This was the sixth time that New Orleans hosted the Super Bowl. Tulane Stadium was the site of Super Bowls IV, VI, and IX; while the Louisiana Superdome previously hosted XII and XV.

As of 2019, Super Bowl XX remains the last Super Bowl to feature two teams both making their first appearance in the game. It was the fourth overall following Super Bowl I, Super Bowl III, and Super Bowl XVI. Absent further expansion of the NFL, any future Super Bowl that would have such a combination would have to have the Detroit Lions playing either the Cleveland Browns, Houston Texans, or Jacksonville Jaguars in the game. All 16 NFC teams have played in an NFL championship game (Detroit last made an NFL championship game in the pre-merger era); only the three AFC franchises that began play since 1995 (the technicalities of the Browns franchise relocating means this version began in 1999) have yet to reach a league championship game.

The nation's recognition of the Bears' accomplishment was overshadowed by STS 51-L two days later, an event which caused the cancellation of the Bears' post-Super Bowl White House visit. Jim McMahon drew controversy after Super Bowl XXXI by wearing a Bears jersey to the Green Bay Packers' visit following their championship, owing to his first official visit never having happened at the time. Twenty-five years after the championship, surviving members of the team would be invited to the White House in 2011 by President Barack Obama, a Chicago native and Bears fan.

Under head coach Mike Ditka, who won the 1985 NFL Coach of the Year Award, the Bears went 15‚Äì1 in the regular season, becoming the second NFL team to win 15 regular season games, while outscoring their opponents with a staggering margin of 456‚Äì198.

The Bears' defense, the "46 defense", allowed the fewest points (198), fewest total yards (4,135), and fewest rushing of any team during the regular season (1,319). They also led the league in interceptions (34) and ranked third in sacks (64).

Pro Bowl quarterback Jim McMahon provided the team with a solid passing attack, throwing for 2,392 yards and 15 touchdowns, while also rushing for 252 yards and three touchdowns. Running back Walter Payton, who was then the NFL's all-time leading rusher with 14,860 yards, rushed for 1,551 yards. He also caught 49 passes for 500 yards, and scored 11 touchdowns. Linebacker Mike Singletary won the NFL Defensive Player of the Year Award by recording three sacks, three fumble recoveries, and one interception.

But one of the most distinguishable players on defense was a large rookie lineman named William "The Refrigerator" Perry. Perry came into training camp before the season weighing over 380 pounds. But after Bears defensive coordinator Buddy Ryan told the press that the team "wasted" their first round draft pick on him, Perry lost some weight and ended up being an effective defensive tackle, finishing the season with 5 sacks. He got even more attention when Ditka started putting him in the game at the fullback position during offensive plays near the opponent's goal line. During the regular season, Perry rushed for 2 touchdowns, caught a pass for another touchdown, and was frequently a lead blocker for Payton during goal line plays.

The Bears "46 defense" also had the following impact players: On the defensive line, Pro Bowler and future Hall of Famer Richard Dent led the NFL in sacks with 17, while Pro Bowler and future Hall of Famer Dan Hampton recorded 6.5 sacks, and nose tackle Steve McMichael compiled 8. In addition to Singletary, linebacker Otis Wilson had 10.5 sacks and 3 interceptions, while Wilber Marshall recorded 4 interceptions. In the secondary, defensive back Leslie Frazier had 6 interceptions, Mike Richardson recorded 4 interceptions, Dave Duerson had 5 interceptions, and Gary Fencik recorded 5 interceptions and 118 tackles.

Chicago's main offensive weapon was Payton and the running game. A big reason for Payton's success was fullback Matt Suhey as the primary lead blocker. Suhey was also a good ball carrier, rushing for 471 yards and catching 33 passes for 295 yards. The team's rushing was also aided by Pro Bowlers Jim Covert and Jay Hilgenberg and the rest of the Bears' offensive line including Mark Bortz, Keith Van Horne, and Tom Thayer.

In their passing game, the Bears' primary deep threat was wide receiver Willie Gault, who caught 33 passes for 704 yards, an average of 21.3 yards per catch, and returned 22 kickoffs for 557 yards and a touchdown. Tight end Emery Moorehead was another key contributor, catching 35 passes for 481 yards. Wide receiver Dennis McKinnon was another passing weapon, recording 31 receptions, 555 yards, and 7 touchdowns. On special teams, Kevin Butler set a rookie scoring record with 144 points, making 31 of 37 field goals (83%) and 51 of 51 extra points.

Meanwhile, the players brought their characterizations to the national stage with "The Super Bowl Shuffle", a rap song the Bears recorded during the season. Even though it was in essence a novelty song, it actually peaked at #41 on the Billboard charts and received a Grammy nomination for best R&B song by a group.

The Patriots were a Cinderella team during the 1985 season because many sports writers and fans thought they were lucky to make the playoffs at all. New England began the season losing three of their first five games, but won six consecutive games to finish with an 11‚Äì5 record. However, the 11‚Äì5 mark only earned them third place in the AFC East behind the Miami Dolphins and the New York Jets.

Quarterback Tony Eason, in his third year in the NFL, was inconsistent during the regular season, completing 168 out of 299 passes for 2,156 yards and 11 touchdowns, but also 17 interceptions. His backup, Steve Grogan, was considered one of the best reserve quarterbacks in the league. Grogan was the starter in six of the Patriots' games, and finished the regular season with 85 out of 156 completions for 1,311 yards, 7 touchdowns, and 5 interceptions.

Wide receiver Stanley Morgan provided the team with a good deep threat, catching 39 passes for 760 yards and 5 touchdowns. On the other side of the field, multi-talented wide receiver Irving Fryar was equally effective, catching 39 passes for 670 yards, while also rushing for 27 yards, gaining another 559 yards returning punts and kickoffs, and scoring 10 touchdowns. But like the Bears, the Patriots' main strength on offense was their rushing attack. Halfback Craig James rushed for 1,227 yards, caught 27 passes for 370 yards, and scored 7 touchdowns. Fullback Tony Collins rushed for 657 yards, recorded a team-leading 52 receptions for 549 yards, and scored 5 touchdowns. The Patriots also had an outstanding offensive line, led by Pro Bowl tackle Brian Holloway and future Hall of Fame guard John Hannah.

New England's defense ranked 5th in the league in fewest yards allowed (5,048). Pro Bowl linebacker Andre Tippett led the AFC with 16.5 sacks and recovered 3 fumbles. Pro Bowl linebacker Steve Nelson was also a big defensive weapon, excelling at pass coverage and run stopping. Also, the Patriots' secondary only gave up 14 touchdown passes during the season, second fewest in the league. Pro Bowl defensive back Raymond Clayborn recorded 6 interceptions for 80 return yards and 1 touchdown, while Pro Bowler Fred Marion had 7 interceptions for 189 return yards.

In the playoffs, the Patriots qualified as the AFC's second wild card.

But the Patriots, under head coach Raymond Berry, defied the odds, beating the New York Jets 26‚Äì14, Los Angeles Raiders 27‚Äì20, and the Dolphins 31‚Äì14 ‚Äì all on the road ‚Äì to make it to the Super Bowl. The win against Miami had been especially surprising, not only because Miami was the only team to beat Chicago in the season, but also because New England had not won in the Orange Bowl (Miami's then-home field) since 1966, the Dolphins' first season (then in the AFL). The Patriots had lost to Miami there 18 consecutive times, including a 30‚Äì27 loss in their 15th game of the season. But New England dominated the Dolphins in the AFC Championship Game, recording two interceptions from quarterback Dan Marino and recovering 4 fumbles. New England remains the only team to finish third in their division and qualify for the Super Bowl in the same season.

Meanwhile, the Bears became the first and only team in NFL history to shut out both of their opponents in the playoffs, beating the New York Giants 21‚Äì0 and the Los Angeles Rams 24‚Äì0.

Much of the Super Bowl pregame hype centered on Bears quarterback Jim McMahon. First, he was fined by the NFL during the playoffs for a violation of the league's dress code, wearing a head band from Adidas. He then started to wear a head band where he hand-wrote "Rozelle", after then-league commissioner Pete Rozelle.

McMahon suffered a strained glute as the result of a hit taken in the NFC Championship Game and flew his acupuncturist into New Orleans to get treatment. During practice four days before the Super Bowl, he wore a headband reading "Acupuncture". During a Bears practice before the Super Bowl, McMahon mooned a helicopter that was hovering over the practice.

Another anecdote involving McMahon during the Super Bowl anticipation involved WDSU sports anchor Buddy Diliberto reporting a quote attributed to McMahon, where he had allegedly referred to the women of New Orleans as "sluts" on a local morning sports talk show. This caused wide controversy among the women of New Orleans and McMahon began receiving calls from irate fans in his hotel. A groggy McMahon, who had not been able to sleep well because of all the calls he had gotten, was confronted by Mike Ditka later that morning and denied making the statement, saying he would not have even been awake to make the comment when he was said to have done so. He was supported in his claim by WLS reporter Les Grobstein, who was present when the alleged statements were made. WDSU would later retract the statement, have an on-air apology read by the station's general manager during the noon newscast on January 23, and suspended Diliberto.

The NBC telecast of the game, with play-by-play announcer Dick Enberg and color commentators Merlin Olsen and Bob Griese (who was not in the booth with Enberg and Olsen), garnered the third highest Nielsen rating of any Super Bowl to date at 48.3, but it ended up being the first Super Bowl to garner over 90 million viewers, the highest ever at that time. While Dick Enberg, Merlin Olsen and Bob Griese called the game, Bob Costas and his "NFL '85" castmates, Ahmad Rashad and Pete Axthelm anchored the pregame, halftime and postgame coverage. Other contributors included Charlie Jones (recapping Super Bowl I), Larry King (interviewing Mike Ditka and Raymond Berry), and Bill Macatee (profiling Patriots owner Billy Sullivan and his family). Also, the pregame coverage included what became known as "the silent minute"; a 60-second countdown over a black screen (a concept devised by then-NBC Sports executive Michael Weisman); a skit featuring comedian Rodney Dangerfield and an interview by "NBC Nightly News" anchor Tom Brokaw of United States President Ronald Reagan at the White House (this would not become a regular Super Bowl pregame feature until Super Bowl XLIII, when "Today show" host Matt Lauer interviewed U.S. President Barack Obama).

"The Last Precinct" debuted on NBC after the game.

Super Bowl XX was simulcast in Canada on CTV and also broadcast on Channel 4 in the United Kingdom, and Canal 5 (Mexico) on Mexico, with play-by-play announcers To√±o de Vald√©s, Enrique Burak and color commentator Pepe Segarra.

Super Bowl XX is featured on "NFL's Greatest Games" under the title "Super Bears" with narration by Don LaFontaine.

The national radio broadcast was aired by NBC Radio, which outbid CBS Radio for the nationwide NFL contract in March 1985. Don Criqui was the play-by-play announcer, with Bob Trumpy as the color analyst. WGN-AM carried the game in the Chicago area (and thanks to WGN's 50,000-watt clear-channel signal, to much of the continental United States), with Wayne Larrivee on play-by-play, and Jim Hart and Dick Butkus providing commentary. WEEI carried the game in the Boston area, with John Carlson and Jon Morris on the call.

This was the first year that the NFL itself implemented the pregame entertainment. The pregame entertainment show began after the players left the field and ended with kick-off. Lesslee Fitzmorris created and directed the show. To celebrate the 20th Super Bowl game, the Most Valuable Players of the previous Super Bowls were featured during the pregame festivities. The number one song of the year coupled with video plays from each Super Bowl accompanied the presentation of each player. Performers formed the score of each championship game. The show concluded with the question of who would be the next Super Bowl Champions. This would start a tradition occurring every ten years (in Super Bowls XXX, XL and 50) in which past Super Bowl MVPs would be honored before the game.

After trumpeter Wynton Marsalis performed the national anthem, Bart Starr, MVP of Super Bowl I and Super Bowl II, tossed the coin.

The performance event group Up with People performed during the halftime show titled "Beat of the Future". Up with People dancers portrayed various scenes into the future. This was the last Super Bowl to feature Up with People as a halftime show, though they later performed in the Super Bowl XXV pregame show. The halftime show was dedicated to the memory of Dr. Martin Luther King Jr. (the first observance of Martin Luther King Jr. Day had been held the previous Monday).

The Patriots took the then-quickest lead in Super Bowl history after linebacker Larry McGrew recovered a fumble from Walter Payton at the Chicago 19-yard line on the second play of the game (the Bears themselves would break this record in Super Bowl XLI when Devin Hester ran back the opening kickoff for a touchdown). Bears quarterback Jim McMahon took responsibility for this fumble after the game, saying he had called the wrong play. This set up Tony Franklin's 36-yard field goal 1:19 into the first quarter after three incomplete passes by Tony Eason (during the first of those three, starting tight end Lin Dawson went down with torn ligaments in his knee). "I looked up at the message board", said Chicago linebacker Mike Singletary, "and it said that 15 of the 19 teams that scored first won the game. I thought, yeah, but none of those 15 had ever played the Bears." Chicago struck back with a 7-play, 59-yard drive, featuring a 43-yard pass completion from McMahon to wide receiver Willie Gault, to set up a field goal from Kevin Butler, tying the score at 3‚Äì3.

After both teams traded punts, Richard Dent and linebacker Wilber Marshall shared a sack on Eason, forcing a fumble that lineman Dan Hampton recovered on the Patriots 13-yard line. Chicago then drove to the 3-yard line, but had to settle for another field goal from Butler after rookie defensive lineman William "The Refrigerator" Perry was tackled (and technically sacked) for a 1-yard loss while trying to throw his first NFL pass on a halfback option play. On the Patriots' ensuing drive, Dent forced running back Craig James to fumble, which was recovered by Singletary at the 13-yard line. Two plays later, Bears fullback Matt Suhey scored on an 11-yard touchdown run to increase the lead to 13‚Äì3.

New England took the ensuing kickoff and ran one play before the first quarter ended, which resulted in positive yardage for the first time in the game (a 3-yard run by James).

After an incomplete pass and a 4-yard loss, the Patriots had to send in punter Rich Camarillo again, and receiver Keith Ortego returned the ball 12 yards to the 41-yard line. The Bears subsequently drove 59 yards in 10 plays, featuring a 24-yard reception by Suhey, to score on McMahon's 2-yard touchdown run to increase their lead, 20‚Äì3. After the ensuing kickoff, New England lost 13 yards in 3 plays and had to punt again, but got the ball back with great field position when defensive back Raymond Clayborn recovered a fumble from Suhey at their own 46-yard line. On the punt, Ortego forgot what the play call was for the punt return, and the ensuing chaos resulted in him being penalized for running after a fair catch and teammate Leslie Frazier suffering a knee injury, which ended his career.

Patriots head coach Raymond Berry then replaced Eason with Steve Grogan, who had spent the previous week hoping he would have the opportunity to step onto the NFL's biggest stage. "I probably won't get a chance", he had told reporters a few days before the game. "I just hope I can figure out some way to get on the field. I could come in on the punt-block team and stand behind the line and wave my arms, or something." But on his first drive, Grogan could only lead them to the 37-yard line, and they decided to punt rather than risk a 55-yard field goal attempt. The Bears then marched 72 yards in 11 plays, moving the ball inside the Patriots' 10-yard line. New England kept them out of the end zone, but Butler kicked his third field goal on the last play of the half to give Chicago a 23‚Äì3 halftime lead.
The end of the first half was controversial. With 21 seconds left, McMahon scrambled to the Patriots' 3-yard line and was stopped inbounds. With the clock ticking down, players from both teams were fighting, and the Bears were forced to snap the ball before the officials formally put it back into play, allowing McMahon to throw the ball out of bounds and stop the clock with three seconds left. The Bears were penalized five yards for delay of game, but according to NFL rules, 10 seconds should have also been run off the clock during such a deliberate clock-stopping attempt in the final two minutes of a half. In addition, a flag should have been thrown for fighting (also according to NFL rules). This would have likely resulted in offsetting penalties, which would still allow for a field goal attempt. Meanwhile, the non-call on the illegal snap was promptly acknowledged by the officials and reported by NBC sportscasters during halftime, but the resulting three points were not taken away from the Bears (because of this instance, the NFL instructed officials to strictly enforce the 10-second run-off rule at the start of the 1986 season).

The Bears had dominated New England in the first half, holding them to 21 offensive plays (only four of which resulted in positive yardage), ‚àí19 total offensive yards, two pass completions, one first down, and 3 points. While Eason was in the game, the totals were six possessions, one play of positive yardage out of 15 plays, no first downs, 3 points, 3 punts, 2 turnovers, no pass completions, and -36 yards of total offense. Meanwhile, Chicago gained 236 yards and scored 23 points themselves.

After the Patriots received the second-half kickoff, they managed to get one first down, but then had to punt after Grogan was sacked twice. Camarillo, who punted four times in the first half, managed to pin the Bears back at their own 4-yard line with a then-Super Bowl record 62-yard punt. But the Patriots' defense still had no ability to stop Chicago's offense. On their very first play, McMahon faked a handoff to Payton, then threw a 60-yard completion to Gault. Eight plays later, McMahon finished the Super Bowl-record 96-yard drive with a 1-yard touchdown run to increase the Bears' lead to 30‚Äì3. On New England's second drive of the quarter, Chicago cornerback Reggie Phillips (who replaced Frazier) intercepted a pass from Grogan and returned it 28 yards for a touchdown to increase the lead to 37‚Äì3.

On the second play of their ensuing possession, the Patriots turned the ball over again, when receiver Cedric Jones lost a fumble after catching a 19-yard pass from Grogan, and Wilber Marshall returned the fumble 13 yards to New England's 37-yard line. A few plays later, McMahon's 27-yard completion to receiver Dennis Gentry moved the ball to the 1-yard line, setting up perhaps the most memorable moment of the game. William "The Refrigerator" Perry was brought on to score on offense, as he had done twice in the regular season. His touchdown (while running over Patriots linebacker Larry McGrew in the process) made the score 44‚Äì3. The Bears' 21 points in the third quarter is still a record for the most points scored in that period, and their 41-point lead remains the record for widest margin after three quarters in a Super Bowl.

Perry's surprise touchdown cost Las Vegas sports books hundreds of thousands of dollars in losses from prop bets.

The Patriots finally scored a touchdown early in the fourth quarter, advancing the ball 76 yards in 12 plays and scoring on an 8-yard fourth-down pass from Grogan to receiver Irving Fryar. But the Bears' defense dominated New England for the rest of the game, forcing another fumble, another interception, and defensive lineman Henry Waechter's sack on Grogan in the end zone for a safety to make the final score 46‚Äì10.

One oddity in the Bears' victory was that Walter Payton had a relatively poor performance running the ball and never scored a touchdown in Super Bowl XX, his only Super Bowl appearance during his Hall of Fame career. Many people including Mike Ditka have claimed that the reason for this was due to the fact that the Patriots' defensive scheme was centered on stopping Payton. Although Payton was ultimately the Bears' leading rusher during the game, the Patriots' defense held him to only 61 yards on 22 carries, with his longest run being only 7 yards. He was given several opportunities to score near the goal line, but New England stopped him every time before he reached the end zone (such as his 2-yard loss from the New England 3-yard line a few plays before Butler's second field goal, and his 2-yard run from the 4-yard line right before McMahon's first rushing touchdown). Thus, Chicago head coach Mike Ditka opted to go for other plays to counter the Patriots' defense. Ditka has since stated that his biggest regret of his career was not creating a scoring opportunity for Payton during the game.

McMahon, who completed 12 out of 20 passes for 256 yards, became the first quarterback in a Super Bowl to score 2 rushing touchdowns. Bears receiver Willie Gault finished the game with 129 receiving yards on just 4 receptions, an average of 32.3 yards per catch. He also returned 4 kickoffs for 49 yards. Suhey had 11 carries for 52 yards and a touchdown, and caught a pass for 24 yards. Singletary tied a Super Bowl record with 2 fumble recoveries.

Eason became the first Super Bowl starting quarterback to fail to complete a pass, going 0 for 6 attempts. Grogan completed 17 out of 30 passes for 177 yards and 1 touchdown, with 2 interceptions. Although fullback Tony Collins was the Patriots' leading rusher, he was limited to just 4 yards on 3 carries, and caught 2 passes for 19 yards. New England receiver Stephen Starring returned 7 kickoffs for 153 yards and caught 2 passes for 39 yards. The Patriots, as a team, only recorded 123 total offensive yards, the second-lowest total in Super Bowl history.

Sources: NFL.com Super Bowl XX, USA Today Super Bowl XX Play by Play, Super Bowl XX Play Finder Chi, Super Bowl XX Play Finder NE

Completions/attempts
Carries
Long gain
Receptions
Times targeted

The following records were set in Super Bowl XX, according to the official NFL.com boxscore and the Pro-Football-Reference.com game summary.

Source:




</doc>
<doc id="28977" url="https://en.wikipedia.org/wiki?curid=28977" title="Salute">
Salute

A salute is a gesture or other action used to display respect. When saluting a person, as distinct from a flag or a National Anthem or other symbolic melody, the gaze must be towards that person, also when returning a salute. Thus, the respectable salute includes a greeting. Not looking at the person, as with most gestured greetings, is likely to be interpreted as disrespect or an eye deficiency. Salutes are primarily associated with armed forces and law enforcement, but other organizations, such as girl guides, and boy scouts and other civilians also use salutes.

In military traditions of various times and places, there have been numerous methods of performing salutes, using hand gestures, cannon or rifle shots, hoisting of flags, removal of headgear, or other means of showing respect or deference. In the Commonwealth of Nations, only commissioned officers are saluted, and the salute is to the commission they carry from their respective commanders-in-chief representing the Monarch, not the officers themselves.

Hand salutes are normally carried out by bringing the right hand to the head in some way, the precise manner varying between different countries. The British Army's salute is almost identical to the French salute, with the palm facing outward. The customary salute in the Polish Armed Forces is the two-fingers salute, a variation of the British military salute with only two fingers extended. In the Russian military, the right hand, palm down, is brought to the right temple, almost, but not quite, touching; the head has to be covered. In the Hellenic Army salute, the palm is facing down and the fingers point to the coat of arms.

In the United States Navy, United States Marine Corps, United States Coast Guard, United States Public Health Service Commissioned Corps, Colombian Army and Ecuadorian Army, as well as in all branches of the French Armed Forces, Spanish Armed Forces, British Armed Forces (with the exception of the Blues and Royals), Canadian Forces, Danish Armed Forces, Hellenic Armed Forces, Italian Armed Forces, Norwegian Armed Forces, Polish Armed Forces, Irish Defence Forces, Australian Defence Force, South African National Defence Force, Swedish Defence Forces, Finnish Defence Forces, Turkish Armed Forces, Portuguese Armed Forces and Russian and all former Soviet republic forces, hand salutes are only given when a cover (protection for the head, usually a hat) is worn.

If there is a reason not to salute with the right hand, due for example to performing an activity that should not be interrupted, or injury, an equivalent left-hand salute is sometimes performed. A right-handed boatswain's mate piping an officer aboard may salute with their left hand.

When the presence of enemy snipers is suspected, military salutes are generally forbidden, since the enemy may use them to recognize officers as valuable targets.

According to some modern military manuals, the modern Western salute originated in France when knights greeted each other to show friendly intentions by raising their visors to show their faces, using a salute. Others also note that the raising of one's visor was a way to identify oneself saying "This is who I am, and I am not afraid." Medieval visors were, to this end, equipped with a protruding spike that allowed the visor to be raised using a saluting motion.

The US Army Quartermaster School provides another explanation of the origin of the hand salute: that it was a long-established military courtesy for subordinates to remove their headgear in the presence of superiors. As late as the American Revolution, a British Army soldier saluted by removing his hat. With the advent of increasingly cumbersome headgear in the 18th and 19th centuries, however, the act of removing one's hat was gradually converted into the simpler gesture of grasping or touching the visor and issuing a courteous salutation.

As early as 1745, a British order book stated that: "The men are ordered not to pull off their hats when they pass an officer, or to speak to them, but only to clap up their hands to their hats and bow as they pass." Over time, it became conventionalized into something resembling the modern hand salute. In the Austrian Army the practice of making a hand salute replaced that of removing the headdress in 1790, although officers wearing cocked hats continued to remove them when greeting superiors until 1868.

The naval salute, with the palm downwards is said to have evolved because the palms of naval ratings, particularly deckhands, were often dirty through working with lines and was deemed insulting to present a dirty palm to an officer; thus the palm was turned downwards. During the Napoleonic Wars, British crews saluted officers by touching a clenched fist to the brow as though grasping a hat-brim between fingers and thumb.

When carrying a sword, still done on ceremonial occasions, European military forces and their cultural descendants use a two-step gesture. The sword is first raised, in the right hand, to the level of and close to the front of the neck. The blade is inclined forward and up 30 degrees from the vertical; the true edge is to the left. Then the sword is slashed downward to a position with the point close to the ground in front of the right foot. The blade is inclined down and forward with the true edge to the left. This gesture originated in the Crusades. The hilt of a sword formed a cross with the blade, so if a crucifix was not available, a Crusader could kiss the hilt of his sword when praying, before entering battle, for oaths and vows, and so on. The lowering of the point to the ground is a traditional act of submission.

In fencing, the fencers salute each other before putting their masks on to begin a bout. There are several methods of doing this, but the most common is to bring the sword in front of the face so that the blade is pointing up in front of the nose. The fencers also salute the referee and the audience.

When armed with a rifle, two methods are available when saluting. The usual method is called "present arms"; the rifle is brought to the vertical, muzzle up, in front of center of the chest with the trigger away from the body. The hands hold the stock close to the positions they would have if the rifle were being fired, though the trigger is not touched. Less formal salutes include the "order arms salute" and the "shoulder arms salutes." These are most often given by a sentry to a low-ranking superior who does not rate the full "present arms" salute. In the "order arms salute," the rifle rests on its butt by the sentry's right foot, held near the muzzle by the sentry's right hand, and does not move. The sentry brings his flattened left hand across his body and touches the rifle near its muzzle. When the rifle is being carried on the shoulder, a similar gesture is used in which the flattened free hand is brought across the body to touch the rifle near the rear of the receiver.

A different type of salute with a rifle is a ritual firing performed during military funerals, known as a three-volley salute. In this ceremonial act, an odd number of rifleman fire three blank cartridges in unison into the air over the casket. This originates from an old European tradition wherein a battle was halted to remove the dead and wounded, then three shots were fired to signal readiness to re-engage.

The custom of firing cannon salutes originated in the Royal Navy. When a cannon was fired, it partially disarmed the ship until reloaded, so needlessly firing a cannon showed respect and trust. As a matter of courtesy a warship would fire her guns harmlessly out to sea, to show that she had no hostile intent. At first, ships were required to fire seven guns, and forts, with their more numerous guns and a larger supply of gunpowder, to fire 21 times. Later, as the quality of gunpowder improved, the British increased the number of shots required from ships to match the forts.

The system of odd-numbered rounds originated from Samuel Pepys, Secretary to the Navy in the Restoration, as a way of economising on the use of powder, the rule until that time having been that all guns had to be fired. Odd numbers were chosen, as even numbers indicated a death.

As naval customs evolved, the 21-gun salute came to be reserved for heads of state, with fewer rounds used to salute lower-ranking officials. Today, In the US Armed Forces, heads of government and cabinet ministers (e.g., the Vice President, U.S. cabinet members, and service secretaries), and military officers with five-star rank receive 19 rounds; four-stars receive 17 rounds; three-stars receive 15; two-stars receive 13; and a one-star general or admiral receives 11. These same standards are currently adhered to by ground-based saluting batteries.

Multiples of 21-gun salutes may be fired for particularly important celebrations. In monarchies this is often done at births of members of the royal family of the country and other official celebrations associated with the royal family.

A specialty platoon of the 3rd US Infantry Regiment (The Old Guard), the Presidential Salute Battery is based at Fort Myer, Virginia. The Guns Platoon (as it is known for short) has the task of rendering military honors in the National Capital Region, including armed forces full-honors funerals; state funerals; presidential inaugurations; full-honors wreath ceremonies at the Tomb of the Unknowns in Arlington National Cemetery; state arrivals at the White House and Pentagon, and retirement ceremonies for general-grade officers in the Military District of Washington, which are normally conducted at Fort Myer.

The Presidential Salute Battery also participates in A Capitol Fourth, the Washington Independence Day celebration; the guns accompany the National Symphony Orchestra in performing the "1812 Overture".

The platoon maintains its battery of ten ceremonially-modified World War II-vintage M-5 anti-tank guns at the Old Guard regimental motor pool.

A ceremonial or celebratory form of aerial salute is the flypast (known as a "flyover" in the United States), which often follows major parades such as the annual Trooping the Colour in the United Kingdom and the French Bastille Day military parade ("d√©fil√© du 14 juillet"). It is seen in other countries as well, notably Singapore and Canada. In Singapore, the Republic of Singapore Air Force usually conducts aerial salutes during the annual National Day Parade and major state events, such as during the funeral of Lee Kuan Yew.

Gun salute by aircraft, primarily displayed during funerals, began with simple flypasts during World War I and have evolved into the missing man formation, where either a formation of aircraft is conspicuously missing an element or a single aircraft abruptly leaves a formation.

A casual salute by an aircraft, somewhat akin to waving to a friend, is the custom of "waggling" the wings by partially rolling the aircraft first to one side, and then the other.

In both countries, the right-hand salute is generally identical to, and drawn from the traditions of, the British armed forces. The salute of the Australian or New Zealand Army is best described as the right arm taking the path of the longest way up and then the shortest way down. Similar in many ways, the salute of the Royal Australian Air Force and Royal New Zealand Air Force takes the longest way up and the shortest way down. The Royal Australian Navy and Royal New Zealand Navy, however, take the shortest way up, palm down, and the shortest way down. The action of the arm rotating up is slower than the action of the conclusion of the salute which is the arm being quickly "snapped" down to the saluter's side. Junior members are required to salute first and the senior member is obliged to return the compliment. Protocol dictates that the Monarch, members of the Royal Family, the Governor-General and State Governors are to be saluted at all times by all ranks. Except where a Drill Manual (or parade) protocol dictates otherwise, the duration of the salute is timed at three beats of the quick-time march (approximately 1.5 seconds), timed from the moment the senior member first returns it. In situations where cover (or "headdress", as it is called in the Australian Army) is not being worn, the salute is given verbally; the junior party (or at least the senior member thereof) will first come to attention, then offer the salute "Good morning/afternoon Your Majesty/Your Royal Highness/Prime Minister/Your Grace/Sir/Ma'am", etc., as the case may be. It is this, rather than the act of standing to attention, which indicates that a salute is being offered. If either party consists of two or more members, all will come to attention, but only the most senior member of the party will offer (or return) the physical or verbal salute. The party which is wearing headdress must always offer, or respond with, a full salute. However, within the Forward Edge of the Battle Area (FEBA) no salutes of any kind are given, under any circumstances; it is always sensible to assume that there are snipers in the area who may see or overhear. In this case, parties personally known to each other are addressed familiarly by their first or given names, regardless of rank; senior officers are addressed as one might address a stranger, courteously, but without any naming or mark of respect.

Since 1917, the British Army's salute has been given with the right hand palm facing forwards with the fingers almost touching the cap or beret. Before 1917, for Other Ranks (i.e. not officers) the salute was given with whichever hand was furthest from the person being saluted, whether that was the right or the left. Officers always saluted with the right hand (as the left, in theory, would always be required to hold the scabbard of their sword) The salute is given to acknowledge the Queen's commission. A salute may not be given unless a soldier is wearing his regimental headdress, for example a beret, caubeen, Tam o' Shanter, Glengarry, field service cap or peaked cap. This does not apply to members of The Blues and Royals (RHG/1stD) The Household Cavalry who, after The Battle of Warburg were allowed to salute without headdress. If a soldier or officer is not wearing headdress then he or she must come to attention instead of giving/returning the salute. The subordinate salutes first and maintains the salute until the superior has responded in kind.

There is a widespread though erroneous belief that it is statutory for "all ranks to salute a bearer of the Victoria Cross". There is no official requirement that appears in the official Warrant of the VC, nor in Queen's Regulations and Orders, but tradition dictates that this occurs and as such the Chiefs of Staff will salute a Private awarded either a VC or George Cross.

The custom of saluting commissioned officers relates wholly to the commission given by Her Majesty the Queen to that officer, not the person. Therefore, when a subordinate airman salutes an officer, he is indirectly acknowledging Her Majesty as Head of State. A salute returned by the officer is on behalf of the Queen.

The RAF salute is similar to the British Army, the hand is brought upwards in a circular motion out from the body, it is stopped 1 inch (25¬†mm) to the rear and to the right of the right eye, the elbow and wrist are kept inline with the shoulder. The hand is then brought straight down back to the position of attention, this movement is completed to the timing "UP TWO-THREE DOWN"

The Naval salute differs in that the palm of the hand faces down towards the shoulder. This dates back to the days of sailing ships, when tar and pitch were used to seal a ship's timbers from seawater. To protect their hands, officers wore white gloves and it was considered most undignified to present a dirty palm in the salute, so the hand was turned through 90 degrees. A common story is that Queen Victoria, having been saluted by an individual with a dirty palm, decreed that in future sailors of the fleet would salute palm down, with the palm facing the ground.

The Royal Marines follow the British Army and salute with the right hand palm facing forward.

In the British Empire (originally in the maritime and hinterland sphere of influence of the East India Company, HEIC, later transformed into crown territories), mainly in British India, the numbers of guns fired as a "gun salute" to the ruler of a so-called princely state became a politically highly significant indicator of his status, not governed by objective rules, but awarded (and in various cases increased) by the British paramount power, roughly reflecting his state's socio-economic, political and/or military weight, but also as a prestigious reward for loyalty to the Raj, in classes (always odd numbers) from three to twenty-one (seven lacking), for the "vassal" indigenous rulers (normally hereditary with a throne, sometimes raised as a personal distinction for an individual ruling prince). Two sovereign monarchies officially outside the Empire were granted a higher honor: thirty-one guns for the royal houses of Afghanistan (under British and Russian influence), and Siam (which was then ruled by the Rattanakosin Kingdom).

In addition, the right to style himself "Highness" (Majesty, which since its Roman origin expresses the sovereign authority of the state, was denied to all "vassals"), a title of great importance in international relations, was formally restricted to rulers of relatively high salute ranks (originally only those with eleven guns or more, later also those with nine guns).

Much as the British salute, described above, the Canadian military salutes to demonstrate a mark of respect and courtesy for the commissioned ranks. When in uniform and not wearing headdress one does not salute. Instead, compliments shall be paid by standing at attention. If on the march, arms shall be swung and the head turned to the left or right as required.

On Remembrance Day, 2009, The Prince of Wales attended the national ceremony in Ottawa with Governor General Micha√´lle Jean‚Äîboth wearing Canadian military dress. CBC live television coverage of the event noted that, when Prince Charles saluted, he performed the Canadian form of the salute with a cupped hand (the British "naval salute"‚Äîappropriate, as he did his military service as an officer in the Royal Navy), adopted by all elements of the Canadian Forces after unification in 1968, rather than the British (Army) form with the palm facing forward.

In the Danish military, there are two types of military salutes. The first type is employed by the Royal Danish Navy, Royal Danish Air Force, and Guard Hussar Regiment Mounted Squadron, and is the same as the one used by the U.S. The second is employed by the Royal Danish Army, and goes as follows: Raise the right arm forward, as to have upper arm 90 degrees from the body. Move the right hand to the temple, and have it parallel to the ground.

The French military salutes to demonstrate a mark of respect, fraternity and courtesy for all soldiers; subordinates salute superiors and every salute is given back. Salutes are not performed if a member is not wearing a headdress or if he is holding a weapon. The French salute, as the original template, is performed with a flat hand, palm facing forwards; the upper arm is horizontal and the tips of the fingers come near the corner of the eyes. The hand, unlike the British salute, remains at a 45 degree angle in line with the lower arm. The five fingers are lined together It perfectly mirrors the gesture made when knights greeted each other to show friendly intentions by raising their visors to show their faces. A crisp tension may be given when the salute is taken or broken. However some "creative" salutes are in use in certain mounted (cavalry) units. The fingers can be spread out with only the right thumb brushing the temple, or the hand can be cocked vertically along the cheek, with the little finger detached or not. These unusual regimental salutes are mannerisms which are lost during official ceremonies. A salute is never given with a bare head or holding a weapon. A civilian (even if he has a hat) never salutes, but a nod to a patrolling soldier is generally appreciated.

In the German Bundeswehr, the salute is performed with a flat hand, with the thumb resting on the index finger. The hand is slightly tilted to the front so that the thumb can not be seen. The upper arm is horizontal and the fingers point to the temple but do not touch it or the headgear. Every soldier saluting another uniformed soldier is entitled to be saluted in return. Soldiers below the rank of Feldwebel are not permitted to speak while saluting. Since the creation of the Bundeswehr, soldiers are required to salute with and without headgear. Originally, in the Reichswehr it was not permitted to perform the salute when the soldier is not wearing uniform headgear. In the Wehrmacht, the traditional military salute was required when wearing headgear, but the Nazi salute was performed when not wearing headgear. The Wehrmacht eventually fully adopted the Nazi salute following the 20 July Plot. East German National People's Army followed the Reichswehr protocol.

In India, the three forces have different salutes with the Indian Army and the Indian Navy following the British tradition. In the Indian army, the salute is performed by keeping the open palm forward, with fingers and thumb together and middle finger almost touching the hatband or right eyebrow. This is often accompanied by the regimental salutation, e.g.:"Sat Sri Akal" in the Sikh Regiment. The Navy salute has the palm facing towards the ground at a 90-degree angle. The Indian Air Force salute involves the right arm being sharply raised from the front by the shortest possible way, with the plane of the palm at 45-degree angle to the forehead.

In Indonesia, executing a salute has its regulations. For members who are part of a uniformed institution and wearing a uniform will implement a gesture of salute according to the regulations of the institution the member is part of. In this case, personnel of the TNI and Indonesian National Police are to implement a hand salute by forming the right hand up making an angle of 90 degrees and is bent 45 degrees, fingers are pressed together and placed near the temple of the right eye, palm facing down. For personnel wearing a headdress is to place the tip of the right index finger touching the front right tip of the headdress. 
Other uniformed organizations/institutions which are not part of the military/police will implement a hand salute as done by members of the military/police.

The command for this gesture in Indonesian is "Hormat, Gerak!". Military and police personnel armed with a rifle during a ceremony will implement a Present arms while personnel unarmed will execute the hand salute.

This is done during the raising and/or lowering of the national flag, rendition or singing of the national anthem, and when saluting to a person or object worth saluting.

In the Israel Defense Forces, saluting is normally reserved for special ceremonies. Unlike in the US Army, saluting is not a constant part of day to day barracks life.

The hand salute is still performed according to the army "Infantry Training - Formal Instructions" regulation, chapter II, section 12 (1939) ""The salute is completed sharply ... bringing the right hand vigorously to the visor of the headdress, with the tip of the fingers over the right eye; the hand in line with the forearm, with the palm facing downwards, the fingers joined and stretched, the index finger in contact with the edge of the visor; horizontal arm, forearm naturally inclined"". The air force and navy use the same procedure, with the single exception of the navy boatswains that salute left-handed while giving the traditional "pipe aboard", as their right hand is used to hold the Boatswain's call.

When given individually, the salute is given by inferior to superior ranks and is held until returned, and by word of command when given by a formed unit. For personnel not wearing hats, holding weapons or with otherwise encumbered hands, the salute is given by coming to attention. During marching armed parades only the officer in command salute for the whole unit briefly bringing the flat of his sword to his face if in full dress, or giving the standard hand-salute if in combat uniform. During flag-rising and flag-lowering armed parades all officers and senior NCOs hand-salutes the flag, while other ranks present arms, and the whole unit sings the National anthem. Flag parties gives salute by slightly inclining the flag only, with the flag-bearer and the escort not giving individual salutes.

In Japan, the angle of salute depends on the branch. In the Ground and the Air Self-Defense Forces, the salute is 90 degrees under the armpit like the U.S. Armed Forces. In the Maritime Self-Defense Forces, the salute is a 45 degree angle because of the narrowness of a ship's interior spaces. To prevent a member's elbow from hitting other members, subordinates may be given approval to not salute in a corridor inside the ship. Furthermore, in all the branches, if a member is not wearing their cap, then they should salute by bowing 10 degrees.

In Pakistan, the salute is generally identical to that of British armed forces. Salute is given with the right hand palm facing forward and fingers slightly touching the right side of the forehead, but not on the forehead. The salute must be performed by the lower rank officials to the higher rank officials under all conditions except when the higher rank official is not in uniform or if the lower rank official is the driver and the vehicle is in motion. The salute is sometimes also performed by left hand if the right hand of the person is completely occupied.

Military personnel of the People's Liberation Army salute palm-down, similar to the Royal Navy or US Military salutes.

In Polish military forces, military men use two fingers to salute, and when they wear headdress (including helmet) because soldiers are supposed to salute to the Coat of Arms on the military headdress, out of respect to the national symbols (This is called the Two-finger salute). There are some exceptions in Polish regulations when salute is not demonstrated, for instance after proclaiming alert in military unit area. As above, salute is marking respect for higher rank or command..

Salutes are similar to those of the Royal Navy. The official instruction for stationary salute states: "The right hand is quickly raised straight up to the headgear. The fingers straight but not stiff next to each other, the little finger edge facing forward. One or two finger tips lightly resting against the right part of the head gear (visor), so that the hand does not obstruct the eye. The wrist straight, the elbow angled forward and slightly lower than the shoulder." Salutes to persons are normally not made when further away than 30 m. Hand salutes are performed only when carrying head gear, if bare headed (normally only indoors) a swift turning of the head towards the person that is being saluted is made instead. The same applies if the right hand is carrying any item that cannot easily be transferred to the left hand. During inspections and when on guard duty, the salute is made by coming to attention. Drivers of moving vehicles never salute. In formations, only the commander salutes.

Swiss soldiers are required to salute any higher-ranking military personnel whenever they encounter them. When the soldier announces to a higher-ranking person he has to state the superior's rank, his rank and his name. When a military formation encounters a superior, it has to state the name of the formation. The salute is given like that of the British navy with the palm pointing towards the shoulder, the tips of the fingers pointing towards the temple.

Within the Turkish military hand salutes are only given when a cover (protection for the head, usually a hat) is worn.

If head is not covered or when the personnel is carrying a rifle on the shoulder the "head salute" is performed by nodding the head forward slightly while maintaining erect posture.

The salute (hand or head) must be performed first by the lower ranking personnel to the higher ranking personnel, and higher official is expected to return the salute, under all conditions except:

Despite of the rank, casket of a martyr personnel while in transport or on stand has to be saluted by all ranks of personnel.


Within United States' military, the salute is a courteous exchange of greetings, with the HDB individual salute, the head and eyes are turned toward the Colors or person saluted. Military personnel in uniform are required to salute when they meet and recognize persons entitled to a salute, except when it is inappropriate or impractical (in public conveyances such as planes and buses, in public places such as inside theaters, or when driving a vehicle).

It is believed that the U.S. military's salute was influenced by British military, although differs slightly, in that the palm of the hand faces down towards the shoulder. This difference may date back to the days of sailing ships, when tar and pitch were used to seal the timber from seawater. During such times, was considered undignified to present a dirty palm in the salute, so the hand was turned through 90 degrees.

Specifically, a proper salute goes as follows: Raise the right hand sharply, fingers and thumb extended and joined, palm facing down, and place the tip of the right forefinger on the rim of the visor, slightly to the right of the eye. The outer edge of the hand is barely canted downward so that neither the back of the hand nor the palm is clearly visible from the front. The hand and wrist are straight, the elbow inclined slightly forward, and the upper arm is horizontal.

The United States Army and United States Air Force give salutes both covered and uncovered, but saluting indoors is forbidden except when formally reporting to a superior officer or during an indoor ceremony. When outdoors, a cover is to be worn at all times when wearing Army Combat Uniforms, but is not required when wearing physical training (PT) gear.


State Defense Forces (SDF; also known as state military, state guards, state militias, or state military reserves) in the United States are military units that operate under the sole authority of a state government. State defense forces are authorized by state and federal law and are under the command of the governor of each state.

State defense forces soldiers are subject to the Uniform Code of Military Justice. They are also subject to their state military laws and regulations and render the same customs and courtesies as active duty, Reserve and National Guard personnel.

The Zogist salute is a military salute that was instituted by Zog I of Albania. It is a gesture whereby the right hand is placed over the heart, with the palm facing downwards. It was first widely used by Zog's personal police force and was later adopted by the Royal Albanian Army.

In Mexico, a salute similar to the Zogist one is rendered by Mexican civilians during the playing of the Mexican national anthem.

Most police forces salute similar to the Canadian Armed Forces standard, with the exception of the Royal Canadian Mounted Police and the Royal Newfoundland Constabulary, which follow the British Army standard of saluting with the full palm facing forward, touching the brim of the hat, if worn.

Similar salutes are used by guards of honour for non-police services (e.g. Toronto Fire Services, Toronto Transit Commission) during funerals or ceremonial events.

All uniform branches of the Hong Kong (Police, Police Auxiliary, Police Pipeband, Fire (including Ambulance service members), Immigration, Customs, Correctional Services, Government Flying Service, Civil Aid Service) salute according to British Army traditions. Personnel stationed with the People's Liberation Army in Hong Kong salute using the Chinese military standards and similar to those used by the Royal Navy.

Non-government organizations like Hong Kong Air Cadet Corps, Hong Kong Adventure Corps, the Boys' Brigade, Hong Kong, Hong Kong Sea Cadet Corps and St. John Ambulance all follow the same military salutes due to their ties with the British Armed Forces.

In the United States, civilian military auxiliaries such as the Civil Air Patrol are required to salute all commissioned and warrant officers of higher rank and return the salute of those with lower ranks of the U.S. Uniformed Services (Army, Navy, Air Force, Marine Corps, Coast Guard, U.S. Public Health Service, National Oceanic and Atmospheric Administration Commissioned Corps) senior in rank to them, as well as all friendly foreign officers, though military members are not required to reciprocate (they may salute voluntarily if they choose). CAP officers are required to salute one another though this is not uniformly observed throughout the CAP. Cadets are required to salute all senior members and military/uniformed services personnel.

The U.S. Coast Guard Auxiliary requires its members to salute all commissioned and warrant officers of higher rank and return the salute of those with lower ranks; since Auxiliarists hold "office" rather than "grade" (indicated by modified military insignia), all Auxiliarists are required to perform this courtesy. Saluting between Auxiliarists is not usually the custom, but is not out of protocol to do so. When operating in direct support of the USCG, or when on military installations in general, Auxiliarists usually wear "member" insignia unless specified otherwise by the officer/NCO in charge.

In most countries, civilians have their own form of salutes.

The same salute of the United States was instituted in Albania as the "Zog salute" by King Zog I.

In Indonesia, executing a salute is also regulated for civilians according to the Constitution of Indonesia. The salute gesture for Civilians in civilian clothing are to stand upright in their respective positions with perfect posture, straightening their arms down, clenching palms, and thumbs facing forward against the thighs with a straight ahead gaze. Members of a uniformed organization/institution which are not part of the military/police such as Fire fighters, traffic wardens, municipal policemen, immigration officers, customs officers, Search and Rescue personnel, scouts, school students, etc. in uniform will implement a hand salute as done by members of the military/police.

This is done during the raising and/or lowering of the national flag, rendition or singing of the national anthem, and when saluting to a person or object worth saluting.

In Iran a salute similar to the United States is given. In ancient times a salute would be given by raising a flat hand in front of the chest with the thumb facing the saluters face.

In Latin America, except in Mexico, a salute similar to the United States flag salute is used, with the hand over the heart.

In the Philippines, civilians salute to the national flag during flag raising and upon hearing the Philippine National Anthem by standing at attention and doing the same hand-to-heart salute as their American, Italian, Nigerian, and South African counterparts. People wearing hats or caps must bare their heads and hold the headwear over their heart; this rule however exempts those who wear headgear or headwear for religious purposes/reasons. Members of the Armed Forces of the Philippines, the Philippine National Police, Philippine Coast Guard, security guards, Boy Scouts of the Philippines, Girl Scouts of the Philippines, including citizens military training, and sometimes airline pilots and civilian ship crews, meanwhile do the traditional military salutes if they are in uniform on duty; off-duty personnel do the hand-to-heart salutes. During the Martial Law years from 1972‚Äì1981 up to the 1986 EDSA Revolution, the "raised clenched fist" salute was done during the singing and playing of the National Anthem by some groups.

People whose faith or religious beliefs prohibit them from singing the anthem or reciting the patriotic pledge such as Jehovah's Witnesses are exempted from doing the salutes but are still required to show full respect when the anthem is being sung or played on record by standing at attention and not engaging in disruptive activities.

Boy Scouts and Girl Scouts meanwhile have their own form of salutes.

Thailand also has the same rule like Indonesia wherein all persons present regardless of nationality are expected to stand at attention and respectfully during the flag raising and lowering and upon hearing the Thai National Anthem every 8:00¬†a.m. and 6:00¬†p.m. or hearing the Sansoen Phra Barami. The L√®se majest√© in Thailand says that it is a serious criminal offense to dishonor the flag of Thailand or National/Royal Anthem.

The Roman salute is a gesture in which the arm is held out forward straight, with palm down and fingers extended straight and touching. Sometimes the arm is raised upward at an angle, sometimes it is held out parallel to the ground. A well known symbol of Fascism, it is commonly perceived to be based on a classical Roman custom. but no known Roman work of art displays this salute, and no known Roman text describes it.

Beginning with Jacques-Louis David's painting "The Oath of the Horatii" (1784), an association of the gesture with Roman republican and imperial culture emerged through 18th-century French art. The association with ancient Roman traditions was further developed in France during the Napoleonic era and again in popular culture through late 19th- and early 20th-century plays and films. These include the epic "Cabiria" (1914), whose screenplay was attributed to Italian nationalist Gabriele d'Annunzio. In a case of life imitating art, d'Annunzio appropriated the salute as a neo-imperial ritual when he led the occupation of Fiume in 1919. It was soon adopted by the Italian Fascist party, whose use of the salute inspired the Nazi party salute. However, the armed forces ("Wehrmacht") of the Third Reich used a German form of the military salute until, in the wake of the July 20 plot on Hitler's life in 1944, the Nazi salute or "Hitlergruss" was imposed on them.

The Bellamy salute was a similar gesture and was the civilian salute of the United States from 1892 to 1942.

In Germany showing the Roman salute is today prohibited by law. Those rendering similar salutes, for example raising the left instead of the right hand, or raising only three fingers, are liable to prosecution. The punishment derives from ¬ß 86a of the German Criminal Code and can be up to three years imprisonment or a fine (in minor cases).

According to SOPs (standard operating procedures) of most airlines, the ground crew that handles departure of an aircraft from a gate (such handling normally includes: disconnecting of required for engine start pneumatic generators or aircraft power and ventilation utilities, aircraft push-back, icing inspection, etc.) is required to salute the captain before the aircraft is released for taxi. Captain normally returns the salute. Since a large percentage of airline pilots are ex-military pilots, this practice was transferred to the airline industry from the military. Exactly the same saluting practice is appropriate to most military aircraft operations, including Air Force, Navy and Army.

In Islam raising the index finger signifies the Tawhƒ´d (ÿ™ŸéŸàŸíÿ≠ŸêŸäÿØ), which denotes the indivisible oneness of God. It is used to express the unity of God ("there is no god but God"). The gesture has recently become widespread among supporters of Islamism, particularly members of ISIS, though its use does not necessarily signal extremism.

In Arabic, the index or fore finger is called musabbi·∏•a (ŸÖŸèÿ≥Ÿéÿ®ŸêŸëÿ≠ÿ©), mostly used with the definite article: al-musabbi·∏•a (ÿßŸÑŸíŸÖŸèÿ≥Ÿéÿ®ŸêŸëÿ≠ÿ©). Sometimes also as-sabbƒÅ·∏•a (ÿßŸÑÿ≥ŸéŸëÿ®Ÿëÿßÿ≠ÿ©) is used. The Arabic verb ÿ≥Ÿéÿ®ŸéŸëÿ≠Ÿé (sabba·∏•a), which has the same root as the Arabic word for index finger, means to praise or glorify God by saying: "Sub·∏•ƒÅna AllƒÅh" (ÿ≥Ÿèÿ®Ÿíÿ≠ÿßŸÜŸé ÿßŸÑŸÑŸá).

The raised clenched fist, symbolizing unity in struggle, was popularized in the 19th century by the socialist, communist and anarchist movements, and is still used today.

In the United States, the raised fist was associated with the Black Power movement, symbolized in the 1968 Olympics Black Power salute; a clenched-fist salute is also proper in many African nations, including South Africa. However, the two salutes are somewhat different: in the Black Power salute, the arm is held straight, while in the salute of leftist movements the arm is bent slightly at the elbow.

Many different gestures are used throughout the world as simple greetings. In Western cultures the handshake is very common, though it has numerous subtle variations in the strength of grip, the vigour of the shake, the dominant position of one hand over the other, and whether or not the left hand is used.

Historically, when men normally wore hats out of doors, male greetings to people they knew, and sometimes those they did not, involved touching, raising slightly ("tipping"), or removing their hat in a variety of gestures, see hat tip. This basic gesture remained normal in very many situations from the Middle Ages until men typically ceased wearing hats in the mid-20th century. Hat-raising began with an element of recognition of superiority, where only the socially inferior party might perform it, but gradually lost this element; King Louis XIV of France made a point of at least touching his hat to all women he encountered. However the gesture was never used by women, for whom their head-covering included considerations of modesty. When a man was not wearing a hat he might touch his hair to the side of the front of his head to replicate a hat tipping gesture. This was typically performed by lower-class men to social superiors, such as peasants to the land-owner, and is known as "tugging the forelock", which still sometimes occurs as a metaphor for submissive behaviour.

In Europe, the formal style of upper-class greeting used by a man to a woman in the Early Modern Period was to hold the woman's presented hand (usually the right) with his right hand and kiss it while bowing, see hand-kissing and kissing hands. This style has not been widespread for a century or more. In cases of a low degree of intimacy, the hand is held but not kissed. The ultra-formal style, with the man's right knee on the floor, is now only used in marriage proposals, as a romantic gesture.

The Arabic term "salaam" (literally "peace", from the spoken greeting that accompanies the gesture), refers to the practice of placing the right palm on the heart, before and after a handshake.

A Chinese greeting, Bao Quan Li (Êä±Êã≥Á§º or "fist wrapping rite"), features the right fist placed in the palm of the left hand and both shaken back and forth two or three times; it may be accompanied by a head nod or bow. The gesture may be used on meeting and parting, and when offering thanks or apologies.

In India, it is common to see the Namaste greeting (or "Sat Sri Akal" for Sikhs) where the palms of the hands are pressed together and held near the heart with the head gently bowed.

Adab, meaning respect and politeness, is a hand gesture used as a Muslim greeting of south Asian Muslims, especially of Urdu-speaking communities of Uttar Pradesh, Hyderabadi Muslims, Bengali Muslims and Muhajir people of Pakistan. The gesture involves raising the right hand towards the face with palm inwards such that it is in front of the eyes and the finger tips are almost touching the forehead, as the upper torso is bent forward. It is typical for the person to say ""adab arz hai"", or just ""adab"". It is often answered with the same or the word ""Tasleem"" is said as an answer or sometimes it is answered with a facial gesture of acceptance.

In Indonesia, a nation with a huge variety of cultures and religions, many greetings are expressed, from the formalized greeting of the highly stratified and hierarchical Javanese to the more egalitarian and practical greetings of outer islands. Javanese, Batak and other ethnicities currently or formerly involved in the armed forces will salute a Government-employed superior, and follow with a deep bow from the waist or short nod of the head and a passing, loose handshake. Hand position is highly important; the superior's hand must be higher than the inferior's.
Muslim men will clasp both hands, palms together at the chest and utter the correct Islamic "slametan" (greeting) phrase, which may be followed by cheek-to-cheek contact, a quick hug or loose handshake. Pious Muslim women rotate their hands from a vertical to perpendicular prayer-like position in order to barely touch the finger tips of the male greeter and may opt out of the cheek-to-cheek contact.
If the male is an "Abdi Dalem" royal servant, courtier or particularly "peko-peko" (taken directly from Japanese to mean obsequious) or even a highly formal individual, he will retreat backwards with head downcast, the left arm crossed against the chest and the right arm hanging down, never showing his side or back to his superior. His head must always be lower than that of his superior.
Younger Muslim males and females will clasp their elder's or superior's outstretched hand to the forehead as a sign of respect and obeisance.
If a manual worker or a person with obviously dirty hands salutes or greets an elder or superior, he will show deference to his superior and avoid contact by bowing, touching the right forehead in a very quick salute or a distant "slamet" gesture.

The traditional Javanese "Sungkem" involves clasping the palms of both hands together, aligning the thumbs with the nose, turning the head downwards and bowing deeply, bending from the knees. In a royal presence, the one performing "sungkem" would kneel at the base of the throne.

A gesture called a "wai" is used in Thailand, where the hands are placed together palm to palm, approximately at nose level, while bowing. The "wai" is similar in form to the gesture referred to by the Japanese term "gassho" by Buddhists. In Thailand, the men and women would usually press two palms together and bow a little while saying "Sawadee ka" (female speaker) or "Sawadee krap" (male speaker).

Some cultures use hugs and kisses (regardless of the sex of the greeters), but those gestures show an existing degree of intimacy and are not used between total strangers. All of these gestures are being supplemented or completely displaced by the handshake in areas with large amounts of business contact with the West.

These bows indicate respect and acknowledgment of social rank, but do not necessarily imply obeisance.

An "obeisance" is a gesture not only of respect but also of submission. Such gestures are rarer in cultures that do not have strong class structures; citizens of the Western World, for example, often react with hostility to the idea of bowing to an authority figure. However, even in Western societies, those retaining vestiges of once rigid social hierarchy may retain the practice on formal occasions. Two examples in England are royal court protocol and the start and end of sittings of courts of justice. The distinction between a formally polite greeting and an obeisance is often hard to make; for example, "proskynesis" (from the words œÄœÅœåœÇ "pros" (towards) and Œ∫œÖŒΩŒ≠œâ "kyneo" (to kiss)) is described by the Greek researcher Herodotus of Halicarnassus, who lived in the 5th century BC in his "Histories" 1.134:

After his conquest of Persia, Alexander the Great introduced Persian etiquette into his own court, including the practice of proskynesis. Visitors, depending on their ranks, would have to prostrate themselves, bow to, kneel in front of, or kiss the king. His Greek countrymen objected to this practice, as they considered these rituals only suitable to the gods.

In countries with recognized social classes, bowing to nobility and royalty is customary. Standing bows of obeisance all involve bending forward from the waist with the eyes downcast, though variations in the placement of the arms and feet are seen. In western European cultures, women do not bow, they "curtsey" (a contraction of "courtesy" that became its own word), a movement in which one foot is moved back and the entire body lowered to a crouch while the head is bowed.

The European formal greeting used from men to women can be transformed into an obeisance gesture by holding the suzerain's hand with both hands. This kind of respect is due to kings, princes, sovereigns (in their kingdoms), archbishops (in their metropolitan province) or the Pope (everywhere). In ultra-formal ceremonies (a coronation, oath of allegiance or episcopal inauguration) the right knee shall touch the ground.

In South Asia traditions, obeisance also involves prostrating oneself before a king.

Many religious believers kneel in prayer, and some (Roman Catholics, and Anglicans) "genuflect", bending one knee to touch the ground, at various points during religious services; the Orthodox Christian equivalent is a deep bow from the waist, and as an especially solemn obeisance the Orthodox make prostrations, bending down on both knees and touching the forehead to the floor. Roman Catholics also employ prostrations on Good Friday and at ordinations. During Islamic prayer, a kneeling bow called "sajdah" is used, with forehead, nose, hands, knees, and toes all touching the ground. Jews bow from the waist many times during prayer. Four times during the Yom Kippur service, and once on each day of Rosh Hashanah, many Jews will kneel and then prostrate. With the Salvation Army, when becoming a soldier, at a christening or other official event, underneath the flag, a salute is often used. This involves holding the hand, palm forwards, with all the fingers held in a clenched fist position. The index finger is left raised pointing towards God, and the hand is often held at chest height, in a similar position to that of Girl Guides.

Hand salutes similar to those used in the military are rendered by the Drum Major of a marching band or drum corps just prior to beginning their performance (after the show announcer asks if the group is ready), following completion of the performance and at other appropriate times. In all cases the salute is rendered to the audience.

The classic "corps style" salute is often known as the "punch" type, where the saluting party will first punch their right arm straight forward from their body, arm parallel to the ground, hand in a fist, followed by the more traditional salute position with the right hand, left arm akimbo. Dropping the salute typically entails snapping the saluting hand to the side and clenching the fist, then dropping both arms to the sides.

In the US, a Drum Major carrying a large baton or mace will often salute by bringing the right hand, holding the mace with the head upward, to the left shoulder.

There are occasional, more flamboyant variations, such as the windmill action of the saluting arm given by the Madison Scouts drum major, or the running of the saluting hand around the brim of the hat worn by the Cavaliers drum major.

In the United Kingdom and the Commonwealth, civilians are not expected to salute. In the United Kingdom, certain civilians, such as officers of HM Revenue and Customs, salute the quarterdeck of Royal Navy vessels on boarding.

In the past most gentlemen in Britain wore hats, and it is customary to tip the hat to a lady in salutation.

In the United States, civilians may salute the national flag by placing their right hand over their heart or by standing at attention during the playing of the national anthem or while reciting the U.S. Pledge of Allegiance, or when the flag is passing by, as in a parade. Men and boys remove their hats and other headgear during the salute; religious headdress (and military headdress worn by veterans in uniform, who are otherwise civilians) are exempt. The nature of the headgear determines whether it is held in the left or right hand, tucked under the left arm, etc. However, if it is held in the right hand, the headgear is not held over the heart but the hand is placed in the same position it would be if it were not holding anything.

The Defense Authorization Act of 2009, signed by President Bush, contained a provision that gave veterans and active-duty service members not in uniform the right to salute during the playing of the national anthem. Previous legislation authorized saluting when not in uniform during the raising, lowering and passing of the flag. However, because a salute is a form of communication protected by the Free Speech clause of the First Amendment, legislative authorization is not technically required for any civilian‚Äîveteran or non-veteran‚Äîto salute the U.S. flag. Whatever the legal status, to salute wrongly is disapproved by veterans' organizations. Civilians in some other countries, like Italy, South Africa, Afghanistan, Bosnia and Herzegovina, South Korea, Croatia, Poland, Kazakhstan, and Nigeria also render the same civilian salute as their U.S. counterparts when hearing their respective national anthems.

Many artefacts of popular culture have created military salutes for fictional purposes, more often than not with a cynical or sarcastic purpose.

In his 1953 comic book album "Le Dictateur et le Champignon", which is part of the "Spirou et Fantasio" series, Belgian artist Franquin creates a silly salute, used in a fictional Latin American country named Palombia. When saluting, subordinates of General Zantas must raise their hands over their heads, with the palm facing forward, then point to the top of their heads with their thumbs. Franquin repeats this idea in his 1957 comic book album "Z comme Zorglub", another episode of the "Spirou et Fantasio" series. Here, almighty science wizard Zorglub's conscripted soldiers salute their leader by pointing to their heads with their index fingers to cynically underline how much of a genius they consider him to be.

In the Marvel Comics universe members of the organisation Hydra salute in a similar way to a fascist salute but instead raise both hands with fists clenched. This is also accompanied by chanting "Hail Hydra".

In the 1987 parodic science fiction film "Spaceballs", directed by Mel Brooks, all subordinates of Supreme leader President Skroob salute him by first bending their forearms over their opposed hands as though they are about to give him the arm of honor salute, but at the last moment, use their raised hands to wave him good bye, rather than showing him the middle finger.

In the manga "Attack on Titan" the members of the armed forces (and sometimes civilians in a show of respect towards military) salute by bending their arms and placing their clenched fist over their hearts. The gesture, known as "offering hearts" is meant to demonstrate that the soldiers are willing to give their bodies and lives to protect humanity and to ensure its survival.

In the BBC TV science fiction comedy "Red Dwarf", Arnold J. Rimmer continually performs an elaborate special salute that he has invented for the Space Corps, in spite of the fact that he is not a member of the Corps. It consists of extending the hand out in front of the body, palm down and rotating it about the wrist five times (to represent the five rings of the Space Corps) followed by bringing the hand close to the head with the palm facing out.




</doc>
<doc id="28979" url="https://en.wikipedia.org/wiki?curid=28979" title="Hyoscine">
Hyoscine

Hyoscine, also known as scopolamine, is a medication used to treat motion sickness and postoperative nausea and vomiting. It is also sometimes used before surgery to decrease saliva. When used by injection, effects begin after about 20 minutes and last for up to 8 hours. It may also be used by mouth and as a transdermal patch.
Common side effects include sleepiness, blurred vision, dilated pupils, and dry mouth. It is not recommended in people with angle-closure glaucoma or bowel obstruction. Whether use during pregnancy is safe is unclear, but its use appears to be safe during breastfeeding. Hyoscine is in the antimuscarinic family of medications and works by blocking some of the effects of acetylcholine within the nervous system.
Hyoscine was first written about in 1881 and started to be used for anesthesia around 1900. It is on the World Health Organization's List of Essential Medicines. Hyoscine is the main active component produced by certain plants of the nightshade family, which historically have been used as psychoactive drugs due to their hallucinogenic effects. The name "scopolamine" is derived from one type of nightshade known as "Scopolia", while the name "hyoscine" is derived from another type known as "Hyoscyamus niger".

Hyoscine has a number of uses in medicine, where it is used to treat:
It is sometimes used as a premedication, (especially to reduce respiratory tract secretions) in surgery, mostly commonly by injection.

Hyoscine enters breast milk by secretion. Although no human studies exist to document the safety of hyoscine while nursing, the manufacturer recommends that caution be taken if hyoscine is administered to a breastfeeding woman.

The likelihood of experiencing adverse effects from hyoscine is increased in the elderly relative to younger people. This phenomenon is especially true for older people who are also on several other medications. Hyoscine use should be avoided in this age group because of these potent anticholinergic adverse effects, which have also been linked to an increased risk for dementia.

Adverse effect incidence:

Uncommon (0.1‚Äì1% incidence) adverse effects include:

Rare (<0.1% incidence) adverse effects include:

Unknown frequency adverse effects include:

Physostigmine, a cholinergic drug that readily crosses the blood-brain barrier, has been used as an antidote to treat the central nervous system depression symptoms of a hyoscine overdose. Other than this supportive treatment, gastric lavage and induced emesis (vomiting) are usually recommended as treatments for oral overdoses. The symptoms of overdose include:

Due to interactions with metabolism of other drugs, hyoscine can cause significant unwanted side effects when taken with other medications. Specific attention should be paid to other medications in the same pharmacologic class as hyoscine, also known as anticholinergics. These medications could potentially interact with the metabolism of hyoscine: analgesics/pain medications, ethanol, zolpidem, thiazide diuretics, buprenorphine, anticholinergic drugs such as tiotropium, etc.

Hyoscine can be taken by mouth, subcutaneously, ophthalmically, and intravenously, as well as via a transdermal patch. The transdermal patch ("e.g.," Transderm Sc≈çp) for prevention of nausea and motion sickness employs hyoscine base, and is effective for up to three days. The oral, ophthalmic, and intravenous forms have shorter half-lives and are usually found in the form hyoscine hydrobromide (for example in Scopace, soluble tablets or Donnatal).

NASA is currently developing a nasal administration method. With a precise dosage, the NASA spray formulation has been shown to work faster and more reliably than the oral form.

The muscarinic antagonism of scopolamine remains the standard method for inducing cognitive deficits in animals and in healthy volunteers. Thus, it used as a relevant preclinical model for pharmacological profiling of new therapeutics.
Scopolamine is a nonspecific muscarinic antagonist at all four muscarinic acetylcholine receptors (M1, M2, M3, and M4).,

Hyoscine is among the secondary metabolites of plants from Solanaceae (nightshade) family of plants, such as henbane ("Hyoscyamus niger"), jimson weed ("Datura"), angel's trumpets ("Brugmansia"), and corkwood ("Duboisia").

The biosynthesis of hyoscine begins with the decarboxylation of L-ornithine to putrescine by ornithine decarboxylase. Putrescine is methylated to N-methylputrescine by putrescine N-methyltransferase.

A putrescine oxidase that specifically recognizes methylated putrescine catalyzes the deamination of this compound to 4-methylaminobutanal, which then undergoes a spontaneous ring formation to N-methyl-pyrrolium cation. In the next step, the pyrrolium cation condenses with acetoacetic acid yielding hygrine. No enzymatic activity could be demonstrated to catalyze this reaction. Hygrine further rearranges to tropinone.

Subsequently, tropinone reductase I converts tropinone to tropine, which condenses with phenylalanine-derived phenyllactate to littorine. A cytochrome P450 classified as Cyp80F1 oxidizes and rearranges littorine to hyoscyamine aldehyde. In the final step, hyoscyamine undergoes epoxidation catalyzed by 6beta-hydroxyhyoscyamine epoxidase yielding hyoscine.
One of the earlier alkaloids isolated from plant sources, hyoscine has been in use in its purified forms (such as various salts, including hydrochloride, hydrobromide, hydroiodide, and sulfate), since its isolation by the German scientist Albert Ladenburg in 1880, and as various preparations from its plant-based form since antiquity and perhaps prehistoric times. Following the description of the structure and activity of hyoscine by Ladenburg, the search for synthetic analogues, and methods for total synthesis, of hyoscine and/or atropine in the 1930s and 1940s resulted in the discovery of diphenhydramine, an early antihistamine and the prototype of its chemical subclass of these drugs, and pethidine, the first fully synthetic opioid analgesic, known as Dolantin and Demerol amongst many other trade names.

In 1899, a Dr. Schneiderlin recommended the use of hyoscine and morphine for surgical anaesthesia, and it started to be used sporadically for that purpose. The use of this combination in obstetric anesthesiology was first proposed by Richard von Steinbuchel in 1902 and was picked up and further developed by Carl Gauss in Freiburg, Germany starting in 1903. The method came to be known as "D√§mmerschlaf" ("twilight sleep") or the "Freiburg method". It spread rather slowly, and different clinics experimented with different dosages and ingredients; in 1915, the "Canadian Medical Association Journal" reported, "the method [was] really still in a state of development". It remained widely used in the US until the 1960s, when growing chemophobia and a desire for more natural childbirth led to its abandonment.

Hyoscine hydrobromide is the international nonproprietary name, and scopolamine hydrobromide is the United States Adopted Name. Other names include "levo"-duboisine, devil's breath, and "burundanga".

A bush medicine developed by Aboriginal peoples of the eastern states of Australia from the soft corkwood tree ("Duboisia myoporoides") was used by the Allies in World War II to stop soldiers from getting seasick when they sailed across the English Channel on their way to France during the Invasion of Normandy. Later, the same substance was found to be usable in the production of scopolamine and hyoscyamine, which are used in eye surgery, and a multimillion dollar industry was built in Queensland based on this substance.

While it has been occasionally used recreationally for its hallucinogenic properties, the experiences are often unpleasant, mentally and physically. It is also physically dangerous and formally classified as a deliriant drug, so repeated use is rare. In June 2008, more than 20 people were hospitalized with psychosis in Norway after ingesting counterfeit rohypnol tablets containing hyoscine. In January 2018, 9 individuals were hospitalized in Perth, Western Australia, after reportedly ingesting hyoscine.

Historically, the various plants that produce hyoscine have been used psychoactively for spiritual purposes. When entheogenic preparations of these plants were used, hyoscine was considered to be the main psychoactive compound and was largely responsible for the hallucinogenic effects, particularly when the preparation was made into a topical ointment (most notably flying ointment). Hyoscine is reported to be the only active alkaloid within these plants that can effectively be absorbed through the skin to cause effects. Different recipes for these ointments were explored in European witchcraft at least as far back as the Early Modern period and included multiple ingredients to help with the transdermal absorption of hyoscine (such as animal fat), as well as other possible ingredients to counteract its noxious and dysphoric effects.

The effects of hyoscine were studied for use as a truth serum in interrogations in the early 20th century, but because of the side effects, investigations were dropped. In 2009, the Czechoslovak state security secret police were proven to have used hyoscine at least three times to obtain confessions from alleged antistate dissidents.

Claims that hyoscine is commonly used in crime have been described as "exaggerated" or even implausible. Powdered hyoscine, in a form referred to as "devil's breath", does not brainwash or control people into being defrauded by their attackers; these alleged effects are most likely urban legends. Nevertheless, the drug is known to produce loss of memory following exposure and sleepiness, similar to the effect of benzodiazepines or alcohol poisoning.

A travel advisory published by the United States Department of State in 2012 stated: "One common and particularly dangerous method that criminals use in order to rob a victim is through the use of drugs. The most common [in Colombia] has been hyoscine. Unofficial estimates put the number of annual hyoscine incidents in Colombia at approximately 50,000. Hyoscine can render a victim unconscious for 24 hours or more. In large doses, it can cause respiratory failure and death. It is most often administered in liquid or powder form in foods and beverages. The majority of these incidents occur in night clubs and bars, and usually men, perceived to be wealthy, are targeted by young, attractive women. It is recommended that, to avoid becoming a victim of hyoscine, a person should never accept food or beverages offered by strangers or new acquaintances, nor leave food or beverages unattended in their presence. Victims of hyoscine or other drugs should seek immediate medical attention."

Beside robberies, it is also allegedly involved in express kidnappings and sexual assault. The Hospital Cl√≠nic in Barcelona introduced a protocol in 2008 to help medical workers identify cases, while Madrid hospitals adopted a similar working document in February 2015. Hospital Cl√≠nic has found little scientific evidence to support this use and relies on the victims' stories to reach any conclusion. Although poisoning by hyoscine appears quite often in the media as an aid for raping, kidnapping, killing, or robbery, the effects of this drug and the way it is applied by criminals (transdermal injection, on playing cards and papers, etc.) are often exaggerated, especially skin exposure, as the dose that can be absorbed by the skin is too low to have any effect. Hyoscine transdermal patches must be used for hours to days.

The name "burundanga" derives from being an extract of the "Brugmansia" plant.

Between 1998 and 2004, 13% of emergency-room admissions for "poisoning with criminal intentions" in a clinic of Bogot√°, Colombia have been attributed to hyoscine, and 44% to benzodiazepines. Most commonly, the person has been poisoned by a robber who gave the victim a scopolamine-laced beverage, in the hope that the victim would become unconscious or unable to effectively resist the robbery.

Hyoscine is used as a research tool to study memory encoding. Initially, in human trials, relatively low doses of the muscarinic receptor antagonist, scopolamine, were found to induce temporary cognitive defects. Since then, scopolamine has become a standard drug for experimentally inducing cognitive defects in animals. Results in primates suggest that acetylcholine is involved in the encoding of new information into long-term memory.

Hyoscine produces detrimental effects on short-term memory, memory acquisition, learning, visual recognition memory, visuospatial praxis, visuospatial memory, visuoperceptual function, verbal recall, and psychomotor speed. It does not seem to impair recognition and memory retrieval, though. Acetylcholine projections in hippocampal neurons, which are vital in mediating long-term potentiation, are inhibited by scopolamine. Hyoscine also inhibits cholinergic-mediated glutamate release in hippocampal neurons, which assist in depolarization, potentiation of action potential, and synaptic suppression. Hyoscine's effects on acetylcholine and glutamate release in the hippocampus favor retrieval-dominant cognitive functioning. Hyoscine has been used to model the defects in cholinergic function for models of Alzheimer's, dementia, fragile X syndrome, and Down syndrome.

Hyoscine has also been investigated as a rapid-onset antidepressant, with a number of small studies finding positive results.




</doc>
<doc id="28981" url="https://en.wikipedia.org/wiki?curid=28981" title="Society for Creative Anachronism">
Society for Creative Anachronism

The Society for Creative Anachronism (SCA) is an international living history group with the aim of studying and recreating mainly Medieval European cultures and their histories before the 17th century. A quip often used within the SCA describes it as a group devoted to the Middle Ages "as they ought to have been", choosing to "selectively recreate the culture, choosing elements of the culture that interest and attract us". Founded in 1966, the non-profit educational corporation has over 30,000 paid members with about 60,000 total participants in the society (including members and non-member participants).

The SCA's roots can be traced to a backyard party of a UC Berkeley medieval studies graduate, the author Diana Paxson, in Berkeley, California, on May Day in 1966. The party began with a "Grand Tournament" in which the participants wore helmets, fencing masks, and usually some semblance of a costume, and sparred with each other using weapons such as plywood swords, padded maces, and fencing foils. It ended with a parade down Telegraph Avenue with everyone singing "Greensleeves". It was styled as a "protest against the 20th century". The SCA still measures dates within the society from the date of that party, calling the system "Anno Societatis" (Latin for "in the Year of the Society"). For example, 2009 May 1 to 2010 April 30 was A.S. XLIV (44). The name Berkeley Society for Creative Anachronism was coined by science fiction author Marion Zimmer Bradley, an early participant, when the nascent group needed an official name in order to reserve a park for a tournament. "Berkeley" was dropped as the group expanded.

Three more co-founders are mentioned by Douglas Martin in the "New York Times" obituaries of August 3, 2001 (p.A23, "Poul Anderson, Science Fiction Novelist, Dies at 74"): "[Anderson and Karen Kruse] moved to San Francisco and were married...They and their daughter, Astrid...founded the Society for Creative Anachronism, which...has spread nationwide."
In 1968, Bradley moved to Staten Island, New York and founded the Kingdom of the East, holding a tournament that summer to determine the first Eastern King of the SCA. That September, a tournament was held at the 26th World Science Fiction Convention, which was in Berkeley that year. The SCA had produced a book for the convention called "A Handbook for the Current Middle Ages", which was a how-to book for people wanting to start their own SCA chapters. Convention goers purchased the book and the idea spread. Soon, other local chapters began to form. In October 1968, the SCA was incorporated as a 501(c)(3) non-profit corporation in California. By the end of 1969, the SCA's three original kingdoms had been established: West Kingdom, East, and Middle. All SCA kingdoms trace their roots to these original three. The number of SCA kingdoms has continued to grow by the expansion and division of existing kingdoms; for example, the kingdoms now called the Outlands, Artemisia, Ansteorra, Gleann Abhann, Meridies, and Trimaris all are made up of lands originally belonging to the fourth kingdom, Atenveldt, which began as a branch of the West Kingdom.

In 2012, SCA agreed to pay $1.3 million to settle a lawsuit brought on behalf of 11 victims of child sexual abuse. The abuse was committed in Pennsylvania at the private residence of Ben Schragger, who pleaded guilty to criminal charges in 2004. Schragger was a member of SCA at the time of the abuse. His membership was suspended on his arrest and permanently revoked after his plea. The lawsuit contended that the SCA had not conducted a background check on Schragger, though at the time the organization did not perform background checks in general and there is no legal requirement to do so.

The SCA engages in a broad range of activities, including SCA armoured combat, SCA fencing, archery, equestrian activities, feasting, medieval dance and recreating medieval arts and sciences, including a broad range of crafts as well as medieval music and theatre. Other activities include the study and practice of heraldry and scribal arts (calligraphy and illumination). Members are afforded opportunities to register a medieval personal name and coat of arms (often colloquially called a "device" in SCA parlance). SCA scribes produce illuminated scrolls to be given by SCA royalty as awards for various achievements.

Most local groups in the SCA hold weekly fighter practices, and many also hold regular archery practices, dance practices, A&S (Arts & Science) nights and other regular gatherings. Some kingdoms and regions also have occasional war practices, where fighters practice formations and group tactics in preparation for large scale "war" events.

The research and approach by members of the SCA toward the recreation of history has led to new discoveries about medieval life.

Some local groups participate in nearby Renaissance fairs, though the main focus of activity is organized through the SCA's own events. Each kingdom in the SCA runs its own schedule of events which are announced in the kingdom newsletter (and usually posted on the kingdom web site), but some of the largest SCA-sanctioned events, called "wars", attract members from many kingdoms. Pennsic War, fought annually between the East Kingdom and Middle Kingdom, is the biggest event in the SCA. The Estrella War has been held for over thirty years, mainly between two large regional SCA groups: the Kingdom of Atenveldt and the Kingdom of the Outlands. Most Estrella wars are held near Phoenix, Arizona in late February and last around 7‚Äì9 days. Several thousand people attend each year, some from as far as Sweden, Germany, France, Italy, Greece, and Australia. Other annual SCA wars include Gulf Wars in Gleann Abhann (formerly Meridies), Great Western War in Caid, War of the Lillies in Calontir and others. Other annual or semi-annual Kingdom-level events held analogously by most or all SCA kingdoms include Crown Tournament, Coronation, Kingdom Arts and Sciences competition and Queen's Prize. Additionally, most baronies in the SCA have their own traditional annual events such as Baronial Arts and Sciences competition, a championship tournament, and often a Yule or Twelfth Night feast. Various SCA groups also sometimes host collegia or symposia, where members gather for a raft of classes on various medieval arts and sciences and other SCA-related topics.

The minimum standard for attendance at an SCA event is "an attempt at pre-17th century clothing", and there is a general goal of maintaining a historical atmosphere. However, SCA members will use modern elements when necessary for personal comfort, medical needs, or to promote safety (e.g. wearing prescription eye-wear, using rattan for swords or shear thickening substances for padding). Unlike some other living history groups, most SCA gatherings do not reenact a specific time or place in history, leaving members free to dress as any culture within the SCA's time period.

The SCA produces two quarterly publications, "The Compleat Anachronist" and "Tournaments Illuminated", and each kingdom publishes a monthly newsletter.

"The Compleat Anachronist" is a quarterly monographic series, each issue focused on a specific topic related to the period of circa 600‚Äì1600 in European history. Issues are written by SCA members and have covered a wide range of topics.

"Tournaments Illuminated" is a quarterly magazine, each issue covering a range of topics and including several features such as news, a humor column, book reviews, war reports and various articles on SCA-related topics of interest.

The SCA is incorporated as a 501(c)(3) non-profit corporation in California, with its current headquarters in the city of Milpitas. It is headed by a board of directors, each of whom is nominated by the membership of the SCA, selected by sitting directors, and elected to serve for 3.5 years. Each director serves as an ombudsman for various kingdoms and society officers. The BoD, as it is called, is responsible for handling the corporate affairs of the SCA and is also in charge of certain disciplinary actions, such as revoking the membership status of participants who have broken Corpora regulations or modern law while participating in SCA activities.
Because the SCA now has groups all over the world, it has also been incorporated in other countries, e.g. SCAA in Australia, SCANZ in New Zealand, SKA Nordmark in Sweden, SKA in Finland, and the UK CIC which covers both the UK and Ireland. These affiliated bodies work with the US board of directors with regards to societal issues, but make all decisions affected by local law independently of the US parent body. Although they agree to work in unity with the US SCA board of directors, they are autonomous and are not bound by any ruling of the US body.

The SCA is divided into administrative regions which it calls "kingdoms". Smaller branches within those kingdoms include "Principalities", "Regions", "Baronies", and "Provinces", and local chapters are known as "Cantons", "Ridings", "Shires", "Colleges", "Strongholds", and "Ports". Kingdoms, Principalities, and Baronies have ceremonial rulers who preside over activities and issue awards to individuals and groups. Colleges, Strongholds, and Ports are local chapters (like a shire) that are associated with an institution, such as a school, military base, or even a military ship at sea.

All SCA branches are organized in descending order as follows:


Groups are active all over the United States, Canada, Europe, Australia, South Africa, and New Zealand, with scattered groups elsewhere, including China, Panama and Thailand. At one time there was even a group on the aircraft carrier USS "Nimitz", known as the "Shire of Curragh Mor" (anglicized Irish for "Big Boat"), and the shire's arms played on the "Nimitz's" ship's badge. There is also an active chapter in South Korea, the Stronghold of Warrior's Gate, with a mix of active duty military personnel from the several services and military-connected civilians. There are also non-territorial, usually called "households", which are not part of the Society's formal organization, the largest of which is the Mongol Empire-themed Great Dark Horde.

The twenty SCA Kingdoms and the geographic areas they cover are (in order of founding)


The Society as a whole, each kingdom, and each local group within a kingdom, all have a standard group of officers with titles loosely based on medieval equivalents.

Members of the SCA study and take part in a variety of activities, including combat and chivalry, archery, heraldry, equestrian activities, costuming, cooking, metalwork, woodworking, leathercrafting, music, dance, calligraphy, fiber arts, and others as practiced during the member's time period.

To aid historical recreation, participants in the SCA create historically plausible characters known individually as "personae". To new members, a persona can simply be a costume and a name used for weekend events, while other members may study and create an elaborate personal history. The goal of a well-crafted persona is a historically accurate person who might have lived in a particular historical time and place. The SCA has onomastic students who assist members in creating an appropriate persona name. The SCA rules state that: "We allow elements and patterns for personal names from beyond Europe, but we require them to be from cultures that were known to medieval and Renaissance Europeans or whose members might reasonably have traveled to Europe". So, while less common, there are members with Saracen, Chinese, Japanese or Native American personas.

In addition, claiming to be a specific historical individual, especially a very familiar one (e.g. Genghis Khan, Julius Caesar, Henry Plantagenet, Queen Elizabeth I), is not permitted. Likewise, one is not allowed to claim the "persona" of a fellow SCA member, alive or dead. Nor is one allowed to take on the persona of a sufficiently familiar fictional character (e.g. Robin of Locksley/Robin Hood).

A major dimension to the SCA is its internal system of Heraldry. Any member of the society may apply to register a name and device for their persona, which are checked by the heralds for uniqueness and period authenticity, before being blazoned and recorded in the society's Armorial. The system has evolved since the formation of the society; and now has three Sovereigns of Arms, with "Principal Heralds" for each Kingdom, who oversee deputy officers for matters such as heraldic education and processing registrations, and local officers (generally one for each local chapter) who assist the local participants. In addition to design of arms, heralds in the Society also provide services such as voice heraldry (similar to a master of ceremonies) at tournaments and official functions, and organizing tournament brackets or "lists."

The SCA has ceremonial rulers chosen by winning tournaments (Kings/Queens, Princes/Princesses) in SCA armoured combat. Barons and Baronesses are appointed by Royalty, although some baronies hold elections or competitions to choose their preferred Baron and/or Baroness. One of the primary functions of state for reigning monarchs is to recognize participant achievement through awards. Most awards denote excellence in a specific pursuit such as local service, arts and sciences, and combat. Some awards change the precedence and title of the recipient, giving him or her the privilege of being known as "Lord"/"Lady", "Baron", "Duchess", "Master", and so forth. High level awards are often given with the consultation of the other people who have received the award, such as peerages and consulting orders. The Crown has some authority over other matters relating to leadership, but the extent of this varies from kingdom to kingdom.

Each SCA kingdom is "ruled" by a king and queen chosen by winning a Crown Tournament in armored combat. Corpora require this to be held as a "properly constituted armored combat" tournament. The winner of the Crown Tournament and his/her Consort are styled "Crown Prince and Princess" and serve an advisory period (three to six months, depending upon the scheduling of the Crown Tournament) under the current King and Queen prior to acceding to the throne and ruling in their turn.

This selection method is not based on how actual medieval monarchs were chosen, as there is no record of one being selected by Tournament combat in this manner. There are, however, literary and historical bases for the custom, most famously the tournament in Sir Walter Scott's "Ivanhoe". In the Middle Ages, there were a number of different "mock king" games, some of which involved some form of combat, such as King of the Mountain or the King of Archers. In the 17th century the Cotswold Games were developed, the winner of which was declared to be "king". Also, the medieval sagas contain accounts of uniting petty kingdoms under a single king through "actual" combat.

The SCA's first event did not choose a "king". Fighters vied for the right to declare their ladies (only men fought at the first event) "fairest", later called the "Queen of Love and Beauty".

The highest ranking titles in the SCA belong to the royalty, followed by the former royalty. Former kings and queens become counts and countesses (dukes and duchesses if they have reigned more than once), and former princes and princesses of Principalities become viscounts and viscountesses. This system is not historically based, but was developed out of practical necessity early in the Society's history.

Directly beneath this "landed" nobility (current and former royalty) rank the highest awards, the Peerages. The SCA has four orders of peerage: the Order of the Chivalry, awarded for skill at arms in Armored Combat; the Order of the Laurel, awarded for skill in the arts and sciences; the Order of the Pelican, awarded for outstanding service to the Society; and the Order of the Masters of Defense, awarded for skill at arms in Rapier Combat. In Several of the Kingdoms the Order of the Rose, made up of former Consorts, is considered a peerage equal to the other four.

Peerages are bestowed by the Crown (the Sovereign and Consort) of a Kingdom. In most cases, this is done with the consent of the members of a given peerage, often at their suggestion. The Society's Bylaws state that "the Crown may elevate subjects to the Peerage by granting membership in one of the Orders conferring a Patent of Arms, after consultation with the members of the Order within the Kingdom, and in accordance with the laws and customs of the kingdom. Restriction: to advance a candidate to the Order of the Chivalry, a Knight of the Society (usually the King) must bestow the accolade".

In May 1999, "The Onion" ran a front-page article headlined "Society for Creative Anachronism Seizes Control of Russia" featuring photos of actual SCA participants from the Barony of Jaravellir (Madison, Wisconsin).

In "Number of the Beast" (1980), Robert A. Heinlein portrayed an SCA tournament where live weapons were used and the battles actually fought to the "death". The defeated combatants were either transported to an alternate reality where medical technology was advanced enough that they could be revived from any wound or transported to the alternate reality that was Valhalla. The contestants' desires were placed in sealed envelopes prior to the tournament, which were destroyed if the competitor won and obeyed if a competitor lost.

In "Ariel" (1983), a post-apocalyptic fantasy by Steven R. Boyett, technology suddenly stops working and sorcery and sword fight take over. Several characters who are former SCA members attribute their survival to their SCA experience.

The fantasy 1986 novel "The Folk of the Air" by Peter S. Beagle was written after the author attended a few early SCA events "circa" 1968; but he has repeatedly stated that he then studiously avoided any contact with the actual SCA itself for almost two decades, so that his description of a fictitious "League for Archaic Pleasures" would not be "contaminated" by contact with the actual real-life organization.

Members of the SCA are given pivotal roles in S. M. Stirling's Emberverse series, where their skills in pre-industrial technology and warfare become invaluable in helping humanity adapt when all modern technology (including firearms) ceases working.

The novel "Murder at the War" ("Knightfall" in paperback edition) by Mary Monica Pulver is a murder mystery set entirely at the SCA's largest annual event, Pennsic War.

In David Weber's 1996 science fiction novel "Honor Among Enemies", main character Honor Harrington mentions that her uncle is a member of the SCA and that he taught her to shoot from the hip (the time the SCA covers having been moved up to the 19th century in the future era in which the novel is set, to include cowboy and Civil War reenactors).

In his conclusion to the "Space Odyssey" series, "" (1997), Arthur C. Clarke portrays the SCA as still being active in the year 3001.

In Christopher Stasheff's "Warlock" series the inhabitants of the planet Gramarye are revealed to be descended from SCA participants. A prequel, "Escape Velocity", describes how the SCAdians first came to Gramarye, and how lands were assigned to the royal peers.

In John Ringo's "The Council Wars" science-fiction series, characters with SCA or SCA-like experience help their society recover from a catastrophic loss of technology.




</doc>
<doc id="28982" url="https://en.wikipedia.org/wiki?curid=28982" title="Snowball Earth">
Snowball Earth

The Snowball Earth hypothesis proposes that during one or more of Earth's icehouse climates, Earth's surface became entirely or nearly entirely frozen, sometime earlier than 650 Mya (million years ago) during the Cryogenian period. Proponents of the hypothesis argue that it best explains sedimentary deposits generally regarded as of glacial origin at tropical palaeolatitudes and other enigmatic features in the geological record. Opponents of the hypothesis contest the implications of the geological evidence for global glaciation and the geophysical feasibility of an ice- or slush-covered ocean and emphasize the difficulty of escaping an all-frozen condition. A number of unanswered questions remain, including whether the Earth was a full snowball, or a "slushball" with a thin equatorial band of open (or seasonally open) water.

The snowball-Earth episodes are proposed to have occurred before the sudden radiation of multicellular bioforms known as the Cambrian explosion. The most recent snowball episode may have triggered the evolution of multicellularity. Another, much earlier and longer snowball episode, the Huronian glaciation, which would have occurred 2400 to 2100¬†Mya, may have been triggered by the first appearance of oxygen in the atmosphere, the "Great Oxygenation Event".

Long before the idea of a global glaciation was established, a series of discoveries began to accumulate evidence for ancient Precambrian glaciations. The first of these discoveries was published in 1871 by J. Thomson who found ancient glacier-reworked material (tillite) in Islay, Scotland. Similar findings followed in Australia (1884) and India (1887). A fourth and very illustrative finding that came to be known as "Reusch's Moraine" was reported by Hans Reusch in northern Norway in 1891. Many other findings followed, but their understanding was hampered by the rejection of continental drift.

Sir Douglas Mawson (1882‚Äì1958), an Australian geologist and Antarctic explorer, spent much of his career studying the Neoproterozoic stratigraphy of South Australia, where he identified thick and extensive glacial sediments and late in his career speculated about the possibility of global glaciation.

Mawson's ideas of global glaciation, however, were based on the mistaken assumption that the geographic position of Australia, and those of other continents where low-latitude glacial deposits are found, have remained constant through time. With the advancement of the continental drift hypothesis, and eventually plate tectonic theory, came an easier explanation for the glaciogenic sediments‚Äîthey were deposited at a time when the continents were at higher latitudes.

In 1964, the idea of global-scale glaciation reemerged when W. Brian Harland published a paper in which he presented palaeomagnetic data showing that glacial tillites in Svalbard and Greenland were deposited at tropical latitudes. From this palaeomagnetic data, and the sedimentological evidence that the glacial sediments interrupt successions of rocks commonly associated with tropical to temperate latitudes, he argued for an ice age that was so extreme that it resulted in the deposition of marine glacial rocks in the tropics.

In the 1960s, Mikhail Budyko, a Soviet climatologist, developed a simple energy-balance climate model to investigate the effect of ice cover on global climate. Using this model, Budyko found that if ice sheets advanced far enough out of the polar regions, a feedback loop ensued where the increased reflectiveness (albedo) of the ice led to further cooling and the formation of more ice, until the entire Earth was covered in ice and stabilized in a new ice-covered equilibrium.

While Budyko's model showed that this ice-albedo stability could happen, he concluded that it had in fact never happened, because his model offered no way to escape from such a feedback loop. In 1971, Aron Faegre, an American physicist, showed that a similar energy-balance model predicted three stable global climates, one of which was snowball earth.

This model introduced Edward Norton Lorenz's concept of intransitivity indicating that there could be a major jump from one climate to another, including to snowball earth.

The term "snowball Earth" was coined by Joseph Kirschvink in a short paper published in 1992 within a lengthy volume concerning the biology of the Proterozoic eon. The major contributions from this work were: (1) the recognition that the presence of banded iron formations is consistent with such a global glacial episode, and (2) the introduction of a mechanism by which to escape from a completely ice-covered Earth‚Äîspecifically, the accumulation of CO from volcanic outgassing leading to an ultra-greenhouse effect.

Franklyn Van Houten's discovery of a consistent geological pattern in which lake levels rose and fell is now known as the "Van Houten cycle". His studies of phosphorus deposits and banded iron formations in sedimentary rocks made him an early adherent of the "snowball Earth" hypothesis postulating that the planet's surface froze more than 650 million years ago.

Interest in the notion of a snowball Earth increased dramatically after Paul F. Hoffman and his co-workers applied Kirschvink's ideas to a succession of Neoproterozoic sedimentary rocks in Namibia and elaborated upon the hypothesis in the journal "Science" in 1998 by incorporating such observations as the occurrence of cap carbonates.

In 2010, Francis MacDonald reported evidence that Rodinia was at equatorial latitude during the Cryogenian period with glacial ice at or below sea level, and that the associated Sturtian glaciation was global.

The snowball Earth hypothesis was originally devised to explain geological evidence for the apparent presence of glaciers at tropical latitudes. According to modelling, an ice‚Äìalbedo feedback would result in glacial ice rapidly advancing to the equator once the glaciers spread to within 25¬∞ to 30¬∞ of the equator. Therefore, the presence of glacial deposits within the tropics suggests global ice cover.

Critical to an assessment of the validity of the theory, therefore, is an understanding of the reliability and significance of the evidence that led to the belief that ice ever reached the tropics. This evidence must prove two things:

During a period of global glaciation, it must also be demonstrated that glaciers were active at different global locations at the same time, and that no other deposits of the same age are in existence.

This last point is very difficult to prove. Before the Ediacaran, the biostratigraphic markers usually used to correlate rocks are absent; therefore there is no way to prove that rocks in different places across the globe were deposited at precisely the same time. The best that can be done is to estimate the age of the rocks using radiometric methods, which are rarely accurate to better than a million years or so.

The first two points are often the source of contention on a case-to-case basis. Many glacial features can also be created by non-glacial means, and estimating the approximate latitudes of landmasses even as recently as can be riddled with difficulties.

The snowball Earth hypothesis was first posited to explain what were then considered to be glacial deposits near the equator. Since tectonic plates move slowly over time, ascertaining their position at a given point in Earth's long history is not easy. In addition to considerations of how the recognizable landmasses could have fit together, the latitude at which a rock was deposited can be constrained by palaeomagnetism.

When sedimentary rocks form, magnetic minerals within them tend to align themselves with the Earth's magnetic field. Through the precise measurement of this palaeomagnetism, it is possible to estimate the latitude (but not the longitude) where the rock matrix was formed. Palaeomagnetic measurements have indicated that some sediments of glacial origin in the Neoproterozoic rock record were deposited within 10 degrees of the equator, although the accuracy of this reconstruction is in question. This palaeomagnetic location of apparently glacial sediments (such as dropstones) has been taken to suggest that glaciers extended from land to sea level in tropical latitudes at the time the sediments were deposited. It is not clear whether this implies a global glaciation, or the existence of localized, possibly land-locked, glacial regimes. Others have even suggested that most data do not constrain any glacial deposits to within 25¬∞ of the equator.

Skeptics suggest that the palaeomagnetic data could be corrupted if Earth's ancient magnetic field was substantially different from today's. Depending on the rate of cooling of Earth's core, it is possible that during the Proterozoic, the magnetic field did not approximate a simple dipolar distribution, with north and south magnetic poles roughly aligning with the planet's axis as they do today. Instead, a hotter core may have circulated more vigorously and given rise to 4, 8 or more poles. Palaeomagnetic data would then have to be re-interpreted, as the sedimentary minerals could have aligned pointing to a 'West Pole' rather than the North Pole. Alternatively, Earth's dipolar field could have been oriented such that the poles were close to the equator. This hypothesis has been posited to explain the extraordinarily rapid motion of the magnetic poles implied by the Ediacaran palaeomagnetic record; the alleged motion of the north pole would occur around the same time as the Gaskiers glaciation.

Another weakness of reliance on palaeomagnetic data is the difficulty in determining whether the magnetic signal recorded is original, or whether it has been reset by later activity. For example, a mountain-building orogeny releases hot water as a by-product of metamorphic reactions; this water can circulate to rocks thousands of kilometers away and reset their magnetic signature. This makes the authenticity of rocks older than a few million years difficult to determine without painstaking mineralogical observations. Moreover, further evidence is accumulating that large-scale remagnetization events have taken place which may necessitate revision of the estimated positions of the palaeomagnetic poles.

There is currently only one deposit, the Elatina deposit of Australia, that was indubitably deposited at low latitudes; its depositional date is well-constrained, and the signal is demonstrably original.

Sedimentary rocks that are deposited by glaciers have distinctive features that enable their identification. Long before the advent of the "snowball Earth" hypothesis many Neoproterozoic sediments had been interpreted as having a glacial origin, including some apparently at tropical latitudes at the time of their deposition. However, it is worth remembering that many sedimentary features traditionally associated with glaciers can also be formed by other means. Thus the glacial origin of many of the key occurrences for snowball Earth has been contested.
As of 2007, there was only one "very reliable"‚Äîstill challenged‚Äîdatum point identifying tropical tillites, which makes statements of equatorial ice cover somewhat presumptuous. However, evidence of sea-level glaciation in the tropics during the Sturtian is accumulating.
Evidence of possible glacial origin of sediment includes:

It appears that some deposits formed during the snowball period could only have formed in the presence of an active hydrological cycle. Bands of glacial deposits up to 5,500 meters thick, separated by small (meters) bands of non-glacial sediments, demonstrate that glaciers melted and re-formed repeatedly for tens of millions of years; solid oceans would not permit this scale of deposition. It is considered possible that ice streams such as seen in Antarctica today could have caused these sequences.
Further, sedimentary features that could only form in open water (for example: wave-formed ripples, far-traveled ice-rafted debris and indicators of photosynthetic activity) can be found throughout sediments dating from the snowball-Earth periods. While these may represent "oases" of meltwater on a completely frozen Earth, computer modelling suggests that large areas of the ocean must have remained ice-free; arguing that a "hard" snowball is not plausible in terms of energy balance and general circulation models.

There are two stable isotopes of carbon in sea water: carbon-12 (C) and the rare carbon-13 (C), which makes up about 1.109 percent of carbon atoms.

Biochemical processes, of which photosynthesis is one, tend to preferentially incorporate the lighter C isotope. Thus ocean-dwelling photosynthesizers, both protists and algae, tend to be very slightly depleted in C, relative to the abundance found in the primary volcanic sources of Earth's carbon. Therefore, an ocean with photosynthetic life will have a lower C/C ratio within organic remains, and a higher ratio in corresponding ocean water. The organic component of the lithified sediments will remain very slightly, but measurably, depleted in C.

During the proposed episode of snowball Earth, there are rapid and extreme negative excursions in the ratio of C to C. Close analysis of the timing of C 'spikes' in deposits across the globe allows the recognition of four, possibly five, glacial events in the late Neoproterozoic.

Banded iron formations (BIF) are sedimentary rocks of layered iron oxide and iron-poor chert. In the presence of oxygen, iron naturally rusts and becomes insoluble in water. The banded iron formations are commonly very old and their deposition is often related to the oxidation of the Earth's atmosphere during the Palaeoproterozoic era, when dissolved iron in the ocean came in contact with photosynthetically produced oxygen and precipitated out as iron oxide.

The bands were produced at the tipping point between an anoxic and an oxygenated ocean. Since today's atmosphere is oxygen-rich (nearly 21% by volume) and in contact with the oceans, it is not possible to accumulate enough iron oxide to deposit a banded formation. The only extensive iron formations that were deposited after the Palaeoproterozoic (after 1.8 billion years ago) are associated with Cryogenian glacial deposits.

For such iron-rich rocks to be deposited there would have to be anoxia in the ocean, so that much dissolved iron (as ferrous oxide) could accumulate before it met an oxidant that would precipitate it as ferric oxide. For the ocean to become anoxic it must have limited gas exchange with the oxygenated atmosphere. Proponents of the hypothesis argue that the reappearance of BIF in the sedimentary record is a result of limited oxygen levels in an ocean sealed by sea-ice, while opponents suggest that the rarity of the BIF deposits may indicate that they formed in inland seas.

Being isolated from the oceans, such lakes could have been stagnant and anoxic at depth, much like today's Black Sea; a sufficient input of iron could provide the necessary conditions for BIF formation. A further difficulty in suggesting that BIFs marked the end of the glaciation is that they are found interbedded with glacial sediments. BIFs are also strikingly absent during the Marinoan glaciation.

Around the top of Neoproterozoic glacial deposits there is commonly a sharp transition into a chemically precipitated sedimentary limestone or dolomite metres to tens of metres thick. These cap carbonates sometimes occur in sedimentary successions that have no other carbonate rocks, suggesting that their deposition is result of a profound aberration in ocean chemistry.
These cap carbonates have unusual chemical composition, as well as strange sedimentary structures that are often interpreted as large ripples.
The formation of such sedimentary rocks could be caused by a large influx of positively charged ions, as would be produced by rapid weathering during the extreme greenhouse following a snowball Earth event. The isotopic signature of the cap carbonates is near ‚àí5¬†‚Ä∞, consistent with the value of the mantle‚Äîsuch a low value is usually/could be taken to signify an absence of life, since photosynthesis usually acts to raise the value; alternatively the release of methane deposits could have lowered it from a higher value, and counterbalance the effects of photosynthesis.

The precise mechanism involved in the formation of cap carbonates is not clear, but the most cited explanation suggests that at the melting of a snowball Earth, water would dissolve the abundant from the atmosphere to form carbonic acid, which would fall as acid rain. This would weather exposed silicate and carbonate rock (including readily attacked glacial debris), releasing large amounts of calcium, which when washed into the ocean would form distinctively textured layers of carbonate sedimentary rock. Such an abiotic "cap carbonate" sediment can be found on top of the glacial till that gave rise to the snowball Earth hypothesis.

However, there are some problems with the designation of a glacial origin to cap carbonates. Firstly, the high carbon dioxide concentration in the atmosphere would cause the oceans to become acidic, and dissolve any carbonates contained within‚Äîstarkly at odds with the deposition of cap carbonates. Further, the thickness of some cap carbonates is far above what could reasonably be produced in the relatively quick deglaciations. The cause is further weakened by the lack of cap carbonates above many sequences of clear glacial origin at a similar time and the occurrence of similar carbonates within the sequences of proposed glacial origin. An alternative mechanism, which may have produced the Doushantuo cap carbonate at least, is the rapid, widespread release of methane. This accounts for incredibly low‚Äîas low as ‚àí48¬†‚Ä∞‚Äî values‚Äîas well as unusual sedimentary features which appear to have been formed by the flow of gas through the sediments.

Isotopes of the element boron suggest that the pH of the oceans dropped dramatically before and after the Marinoan glaciation.
This may indicate a buildup of carbon dioxide in the atmosphere, some of which would dissolve into the oceans to form carbonic acid. Although the boron variations may be evidence of extreme climate change, they need not imply a global glaciation.

Earth's surface is very depleted in the element iridium, which primarily resides in the Earth's core. The only significant source of the element at the surface is cosmic particles that reach Earth. During a snowball Earth, iridium would accumulate on the ice sheets, and when the ice melted the resulting layer of sediment would be rich in iridium. An iridium anomaly has been discovered at the base of the cap carbonate formations, and has been used to suggest that the glacial episode lasted for at least 3 million years, but this does not necessarily imply a "global" extent to the glaciation; indeed, a similar anomaly could be explained by the impact of a large meteorite.

Using the ratio of mobile cations to those that remain in soils during chemical weathering (the chemical index of alteration), it has been shown that chemical weathering varied in a cyclic fashion within a glacial succession, increasing during interglacial periods and decreasing during cold and arid glacial periods. This pattern, if a true reflection of events, suggests that the "snowball Earths" bore a stronger resemblance to Pleistocene ice age cycles than to a completely frozen Earth.

In addition, glacial sediments of the Port Askaig Tillite Formation in Scotland clearly show interbedded cycles of glacial and shallow marine sediments. The significance of these deposits is highly reliant upon their dating. Glacial sediments are difficult to date, and the closest dated bed to the Portaskaig group is 8¬†km stratigraphically above the beds of interest. Its dating to 600¬†Ma means the beds can be tentatively correlated to the Sturtian glaciation, but they may represent the advance or retreat of a snowball Earth.

The initiation of a snowball Earth event would involve some initial cooling mechanism, which would result in an increase in Earth's coverage of snow and ice. The increase in Earth's coverage of snow and ice would in turn increase Earth's albedo, which would result in positive feedback for cooling. If enough snow and ice accumulates, run-away cooling would result. This positive feedback is facilitated by an equatorial continental distribution, which would allow ice to accumulate in the regions closer to the equator, where solar radiation is most direct.

Many possible triggering mechanisms could account for the beginning of a snowball Earth, such as the eruption of a supervolcano, a reduction in the atmospheric concentration of greenhouse gases such as methane and/or carbon dioxide, changes in Solar energy output, or perturbations of Earth's orbit. Regardless of the trigger, initial cooling results in an increase in the area of Earth's surface covered by ice and snow, and the additional ice and snow reflects more Solar energy back to space, further cooling Earth and further increasing the area of Earth's surface covered by ice and snow. This positive feedback loop could eventually produce a frozen equator as cold as modern Antarctica.

Global warming associated with large accumulations of carbon dioxide in the atmosphere over millions of years, emitted primarily by volcanic activity, is the proposed trigger for melting a snowball Earth. Due to positive feedback for melting, the eventual melting of the snow and ice covering most of Earth's surface would require as little as a millennium.

A tropical distribution of the continents is, perhaps counter-intuitively, necessary to allow the initiation of a snowball Earth.
Firstly, tropical continents are more reflective than open ocean, and so absorb less of the Sun's heat: most absorption of Solar energy on Earth today occurs in tropical oceans.

Further, tropical continents are subject to more rainfall, which leads to increased river discharge‚Äîand erosion.
When exposed to air, silicate rocks undergo weathering reactions which remove carbon dioxide from the atmosphere. These reactions proceed in the general form: Rock-forming mineral + CO + HO ‚Üí cations + bicarbonate + SiO. An example of such a reaction is the weathering of wollastonite:

The released calcium cations react with the dissolved bicarbonate in the ocean to form calcium carbonate as a chemically precipitated sedimentary rock. This transfers carbon dioxide, a greenhouse gas, from the air into the geosphere, and, in steady-state on geologic time scales, offsets the carbon dioxide emitted from volcanoes into the atmosphere.

As of 2003, a precise continental distribution during the Neoproterozoic was difficult to establish because there were too few suitable sediments for analysis. Some reconstructions point towards polar continents‚Äîwhich have been a feature of all other major glaciations, providing a point upon which ice can nucleate. Changes in ocean circulation patterns may then have provided the trigger of snowball Earth.

Additional factors that may have contributed to the onset of the Neoproterozoic snowball include the introduction of atmospheric free oxygen, which may have reached sufficient quantities to react with methane in the atmosphere, oxidizing it to carbon dioxide, a much weaker greenhouse gas, and a younger‚Äîthus fainter‚ÄîSun, which would have emitted 6 percent less radiation in the Neoproterozoic.

Normally, as Earth gets colder due to natural climatic fluctuations and changes in incoming solar radiation, the cooling slows these weathering reactions. As a result, less carbon dioxide is removed from the atmosphere and Earth warms as this greenhouse gas accumulates‚Äîthis 'negative feedback' process limits the magnitude of cooling. During the Cryogenian period, however, Earth's continents were all at tropical latitudes, which made this moderating process less effective, as high weathering rates continued on land even as Earth cooled. This let ice advance beyond the polar regions. Once ice advanced to within 30¬∞ of the equator, a positive feedback could ensue such that the increased reflectiveness (albedo) of the ice led to further cooling and the formation of more ice, until the whole Earth is ice-covered.

Polar continents, due to low rates of evaporation, are too dry to allow substantial carbon deposition‚Äîrestricting the amount of atmospheric carbon dioxide that can be removed from the carbon cycle. A gradual rise of the proportion of the isotope carbon-13 relative to carbon-12 in sediments pre-dating "global" glaciation indicates that draw-down before snowball Earths was a slow and continuous process.

The start of snowball Earths are always marked by a sharp downturn in the Œ¥C value of sediments, a hallmark that may be attributed to a crash in biological productivity as a result of the cold temperatures and ice-covered oceans.

In January 2016, Gernon et al. proposed a "shallow-ridge hypothesis" involving the breakup of the supercontinent Rodinia, linking the eruption and rapid alteration of hyaloclastites along shallow ridges to massive increases in alkalinity in an ocean with thick ice cover. Gernon et al. demonstrated that the increase in alkalinity over the course of glaciation is sufficient to explain the thickness of cap carbonates formed in the aftermath of Snowball Earth events.

Global temperature fell so low that the equator was as cold as modern-day Antarctica. This low temperature was maintained by the high albedo of the ice sheets, which reflected most incoming solar energy into space. A lack of heat-retaining clouds, caused by water vapor freezing out of the atmosphere, amplified this effect.

The carbon dioxide levels necessary to thaw Earth have been estimated as being 350 times what they are today, about 13% of the atmosphere. Since the Earth was almost completely covered with ice, carbon dioxide could not be withdrawn from the atmosphere by release of alkaline metal ions weathering out of siliceous rocks. Over 4 to 30 million years, enough and methane, mainly emitted by volcanoes but also produced by microbes converting organic carbon trapped under the ice into the gas, would accumulate to finally cause enough greenhouse effect to make surface ice melt in the tropics until a band of permanently ice-free land and water developed; this would be darker than the ice, and thus absorb more energy from the Sun‚Äîinitiating a "positive feedback".

Destabilization of substantial deposits of methane hydrates locked up in low-latitude permafrost may also have acted as a trigger and/or strong positive feedback for deglaciation and warming.

On the continents, the melting of glaciers would release massive amounts of glacial deposit, which would erode and weather. The resulting sediments supplied to the ocean would be high in nutrients such as phosphorus, which combined with the abundance of would trigger a cyanobacteria population explosion, which would cause a relatively rapid reoxygenation of the atmosphere, which may have contributed to the rise of the Ediacaran biota and the subsequent Cambrian explosion‚Äîa higher oxygen concentration allowing large multicellular lifeforms to develop. Although the positive feedback loop would melt the ice in geological short order, perhaps less than 1,000 years, replenishment of atmospheric oxygen and depletion of the levels would take further millennia.

It is possible that carbon dioxide levels fell enough for Earth to freeze again; this cycle may have repeated until the continents had drifted to more polar latitudes.

More recent evidence suggests that with colder oceanic temperatures, the resulting higher ability of the oceans to dissolve gases led to the carbon content of sea water being more quickly oxidized to carbon dioxide. This leads directly to an increase of atmospheric carbon dioxide, enhanced greenhouse warming of Earth's surface, and the prevention of a total snowball state.

During millions of years, cryoconite would have accumulated on and inside the ice. Psychrophilic microorganisms, volcanic ash and dust from ice-free locations would settle on ice covering several million square kilometers. Once the ice started to melt, these layers would become visible and color the icy surfaces dark, helping to accelerate the process.

Ultraviolet light from the Sun would also produce hydrogen peroxide (HO) when it hits water molecules. Normally hydrogen peroxide is broken down by sunlight, but some would have been trapped inside the ice. When the glaciers started to melt, it would have been released in both the ocean and the atmosphere, where it was split into water and oxygen molecules, leading to an increase in atmospheric oxygen.

While the presence of glaciers is not disputed, the idea that the entire planet was covered in ice is more contentious, leading some scientists to posit a "slushball Earth", in which a band of ice-free, or ice-thin, waters remains around the equator, allowing for a continued hydrologic cycle.

This hypothesis appeals to scientists who observe certain features of the sedimentary record that can only be formed under open water, or rapidly moving ice (which would require somewhere ice-free to move to). Recent research observed geochemical cyclicity in clastic rocks, showing that the "snowball" periods were punctuated by warm spells, similar to ice age cycles in recent Earth history. Attempts to construct computer models of a snowball Earth have also struggled to accommodate global ice cover without fundamental changes in the laws and constants which govern the planet.

A less extreme snowball Earth hypothesis involves continually evolving continental configurations and changes in ocean circulation. Synthesised evidence has produced models indicating a "slushball Earth", where the stratigraphic record does not permit postulating complete global glaciations. Kirschivink's original hypothesis had recognised that warm tropical puddles would be expected to exist in a snowball earth.

The snowball Earth hypothesis does not explain the alternation of glacial and interglacial events, nor the oscillation of glacial sheet margins.

The argument against the hypothesis is evidence of fluctuation in ice cover and melting during "snowball Earth" deposits. Evidence for such melting comes from evidence of glacial dropstones, geochemical evidence of climate cyclicity, and interbedded glacial and shallow marine sediments. A longer record from Oman, constrained to 13¬∞N, covers the period from 712 to 545 million years ago‚Äîa time span containing the Sturtian and Marinoan glaciations‚Äîand shows both glacial and ice-free deposition.

There have been difficulties in recreating a snowball Earth with global climate models. Simple GCMs with mixed-layer oceans can be made to freeze to the equator; a more sophisticated model with a full dynamic ocean (though only a primitive sea ice model) failed to form sea ice to the equator. In addition, the levels of necessary to melt a global ice cover have been calculated to be 130,000 ppm, which is considered by to be unreasonably large.

Strontium isotopic data have been found to be at odds with proposed snowball Earth models of silicate weathering shutdown during glaciation and rapid rates immediately post-glaciation. Therefore, methane release from permafrost during marine transgression was proposed to be the source of the large measured carbon excursion in the time immediately after glaciation.

Nick Eyles suggests that the Neoproterozoic Snowball Earth was in fact no different from any other glaciation in Earth's history, and that efforts to find a single cause are likely to end in failure. The "Zipper rift" hypothesis proposes two pulses of continental "unzipping"‚Äîfirst, the breakup of the supercontinent Rodinia, forming the proto-Pacific Ocean; then the splitting of the continent Baltica from Laurentia, forming the proto-Atlantic‚Äîcoincided with the glaciated periods.
The associated tectonic uplift would form high plateaus, just as the East African Rift is responsible for high topography; this high ground could then host glaciers.

Banded iron formations have been taken as unavoidable evidence for global ice cover, since they require dissolved iron ions and anoxic waters to form; however, the limited extent of the Neoproterozoic banded iron deposits means that they may not have formed in frozen oceans, but instead in inland seas. Such seas can experience a wide range of chemistries; high rates of evaporation could concentrate iron ions, and a periodic lack of circulation could allow anoxic bottom water to form.

Continental rifting, with associated subsidence, tends to produce such landlocked water bodies. This rifting, and associated subsidence, would produce the space for the fast deposition of sediments, negating the need for an immense and rapid melting to raise the global sea levels.

A competing hypothesis to explain the presence of ice on the equatorial continents was that Earth's axial tilt was quite high, in the vicinity of 60¬∞, which would place Earth's land in high "latitudes", although supporting evidence is scarce. A less extreme possibility would be that it was merely Earth's magnetic pole that wandered to this inclination, as the magnetic readings which suggested ice-filled continents depend on the magnetic and rotational poles being relatively similar. In either of these two situations, the freeze would be limited to relatively small areas, as is the case today; severe changes to Earth's climate are not necessary.

The evidence for low-latitude glacial deposits during the supposed snowball Earth episodes has been reinterpreted via the concept of inertial interchange true polar wander (IITPW).
This hypothesis, created to explain palaeomagnetic data, suggests that Earth's orientation relative to its axis of rotation shifted one or more times during the general time-frame attributed to snowball Earth. This could feasibly produce the same distribution of glacial deposits without requiring any of them to have been deposited at equatorial latitude. While the physics behind the proposition is sound, the removal of one flawed data point from the original study rendered the application of the concept in these circumstances unwarranted.

Several alternative explanations for the evidence have been proposed.

A tremendous glaciation would curtail photosynthetic life on Earth, thus depleting atmospheric oxygen, and thereby allowing non-oxidized iron-rich rocks to form.

Detractors argue that this kind of glaciation would have made life extinct entirely. However, microfossils such as stromatolites and oncolites prove that, in shallow marine environments at least, life did not suffer any perturbation. Instead life developed a trophic complexity and survived the cold period unscathed. Proponents counter that it may have been possible for life to survive in these ways:

However, organisms and ecosystems, as far as it can be determined by the fossil record, do not appear to have undergone the significant change that would be expected by a mass extinction. With the advent of more precise dating, a phytoplankton extinction event which had been associated with snowball Earth was shown to precede glaciations by 16¬†million years. Even if life were to cling on in all the ecological refuges listed above, a whole-Earth glaciation would result in a biota with a noticeably different diversity and composition. This change in diversity and composition has not yet been observed‚Äîin fact, the organisms which should be most susceptible to climatic variation emerge unscathed from the snowball Earth.

A snowball Earth has profound implications in the history of life on Earth. While many refugia have been postulated, global ice cover would certainly have ravaged ecosystems dependent on sunlight. Geochemical evidence from rocks associated with low-latitude glacial deposits have been interpreted to show a crash in oceanic life during the glacials.

Because about half of the oceans' water was frozen solid as ice, the remaining water would be twice as salty as it is today, lowering its freezing point. When the ice sheet melted, it would cover the oceans with a layer of hot freshwater up to 2 kilometres thick. Only after the hot surface water mixed with the colder and deeper saltwater did the sea return to a warmer and less salty state.

The melting of the ice may have presented many new opportunities for diversification, and may indeed have driven the rapid evolution which took place at the end of the Cryogenian period.

The Neoproterozoic was a time of remarkable diversification of multicellular organisms, including animals. Organism size and complexity increased considerably after the end of the snowball glaciations. This development of multicellular organisms may have been the result of increased evolutionary pressures resulting from multiple icehouse-hothouse cycles; in this sense, snowball Earth episodes may have "pumped" evolution. Alternatively, fluctuating nutrient levels and rising oxygen may have played a part. Another major glacial episode may have ended just a few million years before the Cambrian explosion.

One hypothesis which has been gaining currency in recent years: that early snowball Earths did not so much "affect" the evolution of life on Earth as result from it. In fact the two hypotheses are not mutually exclusive. The idea is that Earth's life forms affect the global carbon cycle and so major evolutionary events alter the carbon cycle, redistributing carbon within various reservoirs within the biosphere system and in the process temporarily lowering the atmospheric (greenhouse) carbon reservoir until the revised biosphere system settled into a new state. The Snowball I episode (of the Huronian glaciation 2.4 to 2.1 billion years) and Snowball II (of the Precambrian's Cryogenian between 580‚Äì850 million years and which itself had a number of distinct episodes) are respectively thought to be caused by the evolution of oxygenic photosynthesis and then the rise of more advanced multicellular animal life and life's colonization of the land.

Global ice cover, if it existed, may‚Äîin concert with geothermal heating‚Äîhave led to a lively, well mixed ocean with great vertical convective circulation.

There were three or four significant ice ages during the late Neoproterozoic. Of these, the Marinoan was the most significant, and the Sturtian glaciations were also truly widespread. Even the leading snowball proponent Hoffman agrees that the 350 thousand-year-long Gaskiers glaciation did not lead to global glaciation, although it was probably as intense as the late Ordovician glaciation. The status of the Kaigas "glaciation" or "cooling event" is currently unclear; some scientists do not recognise it as a glacial, others suspect that it may reflect poorly dated strata of Sturtian association, and others believe it may indeed be a third ice age. It was certainly less significant than the Sturtian or Marinoan glaciations, and probably not global in extent. Emerging evidence suggests that the Earth underwent a number of glaciations during the Neoproterozoic, which would stand strongly at odds with the snowball hypothesis.

The snowball Earth hypothesis has been invoked to explain glacial deposits in the Huronian Supergroup of Canada, though the palaeomagnetic evidence that suggests ice sheets at low latitudes is contested. The glacial sediments of the Makganyene formation of South Africa are slightly younger than the Huronian glacial deposits (~2.25 billion years old) and were deposited at tropical latitudes. It has been proposed that rise of free oxygen that occurred during the Great Oxygenation Event removed methane in the atmosphere through oxidation. As the Sun was notably weaker at the time, Earth's climate may have relied on methane, a powerful greenhouse gas, to maintain surface temperatures above freezing.

In the absence of this methane greenhouse, temperatures plunged and a snowball event could have occurred.

Before the theory of continental drift, glacial deposits in Carboniferous strata in tropical continental areas such as India and South America led to speculation that the Karoo Ice Age glaciation reached into the tropics. However, a continental reconstruction shows that ice was in fact constrained to the polar parts of the supercontinent Gondwana.





</doc>
<doc id="28984" url="https://en.wikipedia.org/wiki?curid=28984" title="S.S. Lazio">
S.S. Lazio

Societ√† Sportiva Lazio (; "Lazio Sport Club"), commonly referred to as Lazio (), is an Italian professional sports club based in Rome, most known for its football activity. The society, founded in 1900, plays in the Serie A and have spent most of their history in the top tier of Italian football. Lazio have been Italian champions twice (1974, 2000), and have won the Coppa Italia seven times, the Supercoppa Italiana five times, and both the UEFA Cup Winners' Cup and UEFA Super Cup on one occasion.

The club had their first major success in 1958, winning the domestic cup. In 1974, they won their first Serie A title. The 1990s have been the most successful period in Lazio's history, seeing them win the UEFA Cup Winners' Cup and UEFA Super Cup in 1999, the Serie A title in 2000, and reaching their first UEFA Cup final in 1998. Due to a severe economic crisis in 2002 that forced president Sergio Cragnotti out of the club along with several star players being sold, Lazio's success in the league declined. In spite of the lower funds, the club has won four Coppa Italia titles since then; in 2004, 2009, 2013 and 2019. Current president Claudio Lotito took charge of the club in 2004 after two years of a vacuum after Cragnotti's departure.

Lazio's traditional kit colours are sky blue shirts and white shorts with white socks; the colours are reminiscent of Rome's ancient Hellenic legacy. Sky blue socks have also been interchangeably used as home colours. Their home is the 70,634 capacity Stadio Olimpico in Rome, which they share with A.S. Roma. Lazio have a long-standing rivalry with Roma, with whom they have contested the "Derby della Capitale" (in English "Derby of the capital city" or Rome derby) since 1929.

Despite initially not having any parent‚Äìsubsidiary relation with the male and female professional team (that was incorporated as S.S. Lazio S.p.A.), the founding of Societ√† Sportiva Lazio allowed for the club that participates in over 40 sports disciplines in total, more than any other sports association in the world.

"Societ√† Podistica Lazio" was founded on 9 January 1900 in the Prati district of Rome. Until 1910, the club played at an amateur level until it officially joined the league competition in 1912 as soon as the Italian Football Federation began organising championships in the center and south of Italy, and reached the final of the national championship playoff three times, but never won, losing in 1913 to Pro Vercelli, in 1914 to Casale and in 1923 to Genoa 1893.

In 1927, Lazio was the only major Roman club which resisted the Fascist regime's attempts to merge all the city's teams into what would become A.S. Roma the same year.

The club played in the first organised Serie A in 1929 and, led by legendary Italian striker Silvio Piola, achieved a second-place finish in 1937¬†‚Äì its highest pre-war result.

The 1950s produced a mix of mid and upper table results with a Coppa Italia win in 1958. Lazio was relegated for the first time in 1961 to the Serie B, but returned in the top flight two years later. After a number of mid-table placements, another relegation followed in 1970‚Äì71. Back to Serie A in 1972‚Äì73, Lazio immediately emerged as surprise challengers for the "Scudetto" to Milan and Juventus in 1972‚Äì73, only losing out on the final day of the season, with a team comprising captain Giuseppe Wilson, as well as midfielders Luciano Re Cecconi and Mario Frustalupi, striker Giorgio Chinaglia, and head coach Tommaso Maestrelli. Lazio improved such successes the following season, ensuring its first title in 1973‚Äì74. However, tragic deaths of Re Cecconi and "Scudetto" trainer Maestrelli, as well as the departure of Chinaglia, would be a triple blow for Lazio. The emergence of Bruno Giordano during this period provided some relief as he finished League top scorer in 1979, when Lazio finished eighth.

Lazio were forcibly relegated to Serie B in 1980 due to a remarkable scandal concerning illegal bets on their own matches, along with Milan. They remained in Italy's second division for three seasons in what would mark the darkest period in Lazio's history. They would return in 1983 and manage a last-day escape from relegation the following season. The 1984‚Äì85 season would prove harrowing, with a pitiful 15 points and bottom place finish.

In 1986, Lazio was hit with a nine-point deduction (a true deathblow back in the day of the two-point win) for a betting scandal involving player Claudio Vinazzani. An epic struggle against relegation followed the same season in Serie B, with the club led by trainer Eugenio Fascetti only avoiding relegation to the Serie C after play-off wins over Taranto and Campobasso. This would prove a turning point in the club's history, with Lazio returning to Serie A in 1988 and, under the careful financial management of Gianmarco Calleri, the consolidation of the club's position as a solid top-flight club.

The arrival of Sergio Cragnotti in 1992 changed the club's history due to his long-term investments in new players to make the team a "Scudetto" competitor. A notable early transfer during his tenure was the capture of English midfielder Paul Gascoigne from Tottenham Hotspur for ¬£5.5¬†million. Gascoigne's transfer to Lazio is credited with the increase of interest in Serie A in the United Kingdom during the 1990s. Cragnotti repeatedly broke transfer records in pursuit of players who were considered major stars¬†‚Äì Juan Sebasti√°n Ver√≥n for ¬£18¬†million, Christian Vieri for ¬£19¬†million and breaking the world transfer record, albeit only for a matter of weeks, to sign Hern√°n Crespo from Parma for ¬£35¬†million.

Lazio were Serie A runners-up in 1995, third in 1996 and fourth in 1997, then losing the championship just by one point to Milan on the last championship's match in 1999 before, with the likes of Sini≈°a Mihajloviƒá, Alessandro Nesta, Marcelo Salas and Pavel Nedvƒõd in the side, winning its second "Scudetto" in 2000, as well as the Coppa Italia double with Sven-G√∂ran Eriksson (1997‚Äì2001) as manager.

Lazio had two more Coppa Italia triumphs in 1998 and 2004, as well as the last ever UEFA Cup Winners' Cup in 1999. They also reached the UEFA Cup, but lost 0‚Äì3 against Internazionale.

In addition, Lazio won the Supercoppa Italiana twice and defeated Manchester United in 1999 to win the UEFA Super Cup.

In 2000, Lazio became also the first Italian football club to be quoted on the Italian Piazza Affari stock market.

With money running out, however, Lazio's results slowly worsened in the years. In 2002, a financial scandal involving Cragnotti and his food products multinational Cirio forced him to leave the club, and Lazio was controlled until 2004 by caretaker financial managers and a bank pool. This forced the club to sell their star players and even fan favourite captain Alessandro Nesta. In 2004, entrepreneur Claudio Lotito acquired the majority of the club.

In 2006, the club qualified to the 2006‚Äì07 UEFA Cup under coach Delio Rossi. The club, however, was excluded from European competitions due to their involvement in a match-fixing scandal.

In the 2006‚Äì07 season, despite a later-reduced points deduction, Lazio achieved a third-place finish, thus gaining qualification to the UEFA Champions League third qualifying round, where they defeated Dinamo Bucure»ôti to reach the group phase, and ended fourth place in the group composed of Real Madrid, Werder Bremen and Olympiacos. Things in the league did not go much better, with the team spending most of the season in the bottom half of the table, sparking the protests of the fans, and eventually ending the Serie A season in 12th place. In the 2008‚Äì09 season, Lazio won their fifth Coppa Italia, beating Sampdoria in the final.

Lazio started the 2009‚Äì10 season playing the Supercoppa Italiana against Inter in Beijing and winning the match 2‚Äì1, with goals from Matuzal√©m and Tommaso Rocchi.

Lazio won the 2012‚Äì13 Coppa Italia 1‚Äì0 over rivals Roma with the lone goal coming from Senad Luliƒá. Lazio won the 2018‚Äì19 Coppa Italia 2‚Äì0 over Atalanta, winning their seventh title overall.

On 22 December 2019, Lazio won their fifth Supercoppa Italiana title, following a 3‚Äì1 victory over Juventus. Lazio mounted an unexpected title challenge during the 2019‚Äì20 Serie A season. A poor run of form following the restart of the Serie A campaign after the COVID-19 suspension saw Lazio fall out of the title race. On 24 July 2020, Lazio qualified for the Champions League for the first time in 12 years after securing a top 4 finish. 

Lazio's colours of white and sky blue were inspired by the national emblem of Greece, due to the fact that Lazio is a mixed sports club this was chosen in recognition of the fact that the Ancient Olympic Games and along with it the sporting tradition in Europe is linked to Greece.

Originally, Lazio wore a shirt which was divided into white and sky blue quarters, with black shorts and socks. After a while of wearing a plain white shirt very early on, Lazio reverted to the colours which they wear today. Some seasons Lazio have used a sky blue and white shirt with stripes, but usually it is sky blue with a white trim, with the white shorts and socks. The club's colours have led to their Italian nickname of "Biancocelesti".

Lazio's traditional club badge and symbol is the eagle, which was chosen by founding member Luigi Bigiarelli. It is an acknowledgment to the emblem of Zeus (the god of sky and thunder in Greek mythology) commonly known as the Aquila; Lazio's use of the symbol has led to two of their nicknames; "le Aquile" ("the Eagles") and "Aquilotti" ("Eaglets"). The current club badge features a golden eagle above a white shield with a blue border; inside the shield is the club's name and a smaller tripartite shield with the colours of the club.

Stadio Olimpico, located on the Foro Italico, is the major stadium of Rome. It is the home of the Italy national football team as well as of both local teams Lazio and Roma. It was opened in 1937 and after its latest renovation in 2008, the stadium has a capacity of 70,634 seats. It was the site of the 1960 Summer Olympics, but has also served as the location of the 1987 World Athletics Championships, the 1980 European Championship final, the 1990 World Cup and the Champions League Final in 1996 and 2009.

Also on the Foro Italico lies the Stadio dei Marmi, or "marble stadium", which was built in 1932 and designed by Enrico Del Debbio. It has tiers topped by 60 white marble statues that were gifts from Italian cities in commemoration of 60 athletes.

During the 1989‚Äì90 season, Lazio and Roma played their games at the Stadio Flaminio of Rome, located in the district Flaminio, because of the renovation works carried out at the Stadio Olimpico.
In June 2018, Lazio President Claudio Lotito stated that "Lazio should be granted the same favour and treatment as Roma ‚Äì the ability to also build a new stadium. He also added that ‚ÄúLazio‚Äôs stadium will be built before Roma‚Äôs stadium."

In June 2019, Lazio President Claudio Lotito was set to present the designs of a potential future stadium for Lazio, named the Stadio delle Aquile. However, this did not occur for reasons unknown.

Lazio is the sixth-most supported football club in Italy and the second in Rome, with around 2% of Italian football fans supporting the club (according to "La Repubblica's" research of August 2008). Historically, the largest section of Lazio supporters in the city of Rome has come from the far northern section, creating an arch-like shape across Rome with affluent areas such as Parioli, Prati, Flaminio, Cassia and Monte Mario.

Founded in 1987, "Irriducibili Lazio" were the club's biggest ultras group for over 30 years. They typically create traditional Italian ultra displays during the "Derby della Capitale" (Rome Derby), the match between Lazio and their main rivals, Roma. It is amongst the most heated and emotional footballing rivalries in the world, such as where Lazio fan Vincenzo Paparelli was killed at one of the derby games during the 1979‚Äì80 season after being hit in the eye by an emergency rocket thrown by a Roma fan. A minority of Lazio's ultras used to use swastikas and fascist symbols on their banners, and they have displayed racist behaviour in several occasions during the derbies. Most notably, at a derby of the season 1998‚Äì99, laziali unfurled a 50-metre banner around the Curva Nord that read, "Auschwitz is your town, the ovens are your houses". Black players of Roma have often been receivers of racist and offensive behaviour. After 33 years, the Irriducibili disbanded on 27 February 2020, citing "too much blood, too many banning orders, too many arrests." Lazio's ultras now go by the name "Ultras Lazio". Lazio also have a strong rivalry with Napoli and Livorno, as well as with Pescara and Atalanta. The club also maintains strong competitive rivalries with Fiorentina, Juventus, and Milan.

Conversely, the ultras have friendly relationships with Internazionale, Triestina, and Hellas Verona. Internationally, Lazio's fans maintain a long-standing strong friendship with the supporters of the Bulgarian club Levski Sofia and as such, Lazio were invited to participate in the centenary football match honouring the birthday of the Bulgarian club.

12¬†‚Äì Since the 2003‚Äì04 season, Curva Nord of Stadio Olimpico, as a sign of recognition towards the Curva Nord, is considered the 12th man in the field.

The following managers have all won at least one trophy when in charge of Lazio:







Giuseppe Favalli holds Lazio's official appearance record, having made 401 over the course of 16 years from 1992 until 2004. The record for total appearances by a goalkeeper is held by Luca Marchegiani, with 339 appearances, while the record for most league appearances is held by Aldo Puccinelli with 339.

The all-time leading goalscorer for Lazio is Silvio Piola, with 149 goals scored. Piola, who played also with Pro Vercelli, Torino, Juventus and Novara, is also the highest goalscorer in Serie A history, with 274 goals. Simone Inzaghi is the all-time top goalscorer in the European Competitions, with 20 goals. He is also one of the five players who scored four goals in a single UEFA Champions League match.

Officially, Lazio's highest home attendance is approximately 80,000 for a Serie A match against Foggia on 12 May 1974, the match that awarded to Lazio their first "Scudetto". This is also the record for the Stadio Olimpico, including matches held by Roma and the Italy national football team.

In 1998, during Sergio Cragnotti's period in charge as the chairman, Societ√† Sportiva Lazio S.p.A. became a listed company: Lazio were the first Italian club to do so. However, Cragnotti resigned as chairman in 2001, after a "huge hole in the budget" of the club.

Claudio Lotito, the current chairman of Lazio, purchased the club from Cragnotti in 2004, but owned just 26.969% of shares as the largest shareholders at that time. It was followed by banking group Capitalia (and its subsidiaries Mediocredito Centrale, Banca di Roma and Banco di Sicilia) as the second largest shareholders for 17.717%. Capitalia also hold 49% stake of Italpetroli (via Capitalia's subsidiary Banca di Roma), the parent company of city rival Roma (via Italpetroli's subsidiary "Roma 2000"). Lotito later purchased the minority stake from Capitalia.

, Claudio Lotito owns just over two-thirds of the shares of Lazio. Lazio is one of only three Italian clubs listed on the Borsa Italiana, the others being Juventus and Roma. In the past, Lazio was the only one with a single primary share holder (Lotito). However, following several capital increases by Roma and Juventus, they also are significantly owned by a shareholder. According to The Football Money League, published by consultants Deloitte, in the 2004‚Äì05 season, Lazio was the 20th highest earning football club in the world with an estimated revenue of ‚Ç¨83 million; the 2005 ranking of the club was 15th. However, in 2016 ranking (the rank used data in 2014‚Äì15 season), Lazio was not in the top 20.

Lazio was one of the few clubs that self-sustain from the financial support of a shareholder, and also consistently make an aggregate profit after every season. Unlike Inter Milan, Roma and Milan, who were sanctioned by UEFA due to breaches of Financial Fair Play, Lazio passed the regulations held by the administrative body with the high achievements. Lotito also received a prize that joint awarded by and DGS Sport&Cultura, due to Lazio's financial health.

In 2017, the club renewed their sponsorship deal with shirt manufacturer Macron. It is worth ‚Ç¨16 million a season, plus variables of about ‚Ç¨9 million stemming from league and European competition finishes.





</doc>
<doc id="28990" url="https://en.wikipedia.org/wiki?curid=28990" title="Ninian">
Ninian

Ninian is a Christian saint first mentioned in the 8th century as being an early missionary among the Pictish peoples of what is now Scotland. For this reason he is known as the Apostle to the Southern Picts, and there are numerous dedications to him in those parts of Scotland with a Pictish heritage, throughout the Scottish Lowlands, and in parts of Northern England with a Northumbrian heritage. In Scotland, Ninian is also known as Ringan, and as Trynnian in Northern England.

Ninian's major shrine was at Whithorn in Galloway, where he is associated with the Candida Casa (Latin for 'White House'). Nothing is known about his teachings, and there is no unchallenged authority for information about his life.

The nature of Ninian's identity is uncertain, and historians have identified the name "Ninian" with other historical figures. A popular hypothesis proposed by Thomas Owen Clancy, a researcher and professor of Celtic studies, posits that Ninian can be identified with three other historical figures: Saint Finnian of Moville, Saint Finnian of Clonard, and Saint Finbarr of Cork. Linguistic variations across the territories associated with each saint have provided evidence that the Ninian preserved in literary tradition originated from this individual. This article discusses the particulars and origins of what has come to be known as the "traditional" stories of Saint Ninian.

The Southern Picts, for whom Ninian is held to be the apostle, are the Picts south of the mountains known as the Mounth, which cross Scotland north of the Firths of Clyde and Forth. That they had once been Christian is known from a 5th-century mention of them by Saint Patrick in his "Letter to Coroticus", where he refers to them as 'apostate Picts'. Patrick could not have been referring to the Northern Picts who were converted by Saint Columba in the 6th century because they were not yet Christian, and thus could not be called 'apostate'. Northumbria had established a bishopric among the Southern Picts at Abercorn in 681, under Bishop Trumwine. This effort was abandoned shortly after the Picts defeated the Northumbrians at the Battle of Dun Nechtain in 685.

Christianity had flourished in Galloway in the 6th century. By the time of Bede's account in 731, the Northumbrians had enjoyed an unbroken relationship with Galloway for a century or longer, beginning with the Northumbrian predecessor state of Bernicia. The full nature of the relationship is uncertain. Also at this time, Northumbria was establishing bishoprics in its sphere of influence, to be subordinate to the Northumbrian Archbishop of York. One such bishopric was established at Whithorn in 731, and Bede's account serves to support the legitimacy of the new Northumbrian bishopric. The Bernician name "hwit √¶rn" is Old English for the Latin "candida casa", or 'white house' in modern English, and it has survived as the modern name of Whithorn.

There is as yet no unchallenged connection of the historical record to the person who was Bede's Ninian. However, the unlikelihood that the reputable historian Bede invented Ninian without some basis in the historical record, combined with an increased knowledge of Ireland's early saints and Whithorn's early Christian connections, has led to serious scholarly efforts to find Bede's basis. James Henthorn Todd, in his 1855 publication of the "Leabhar Imuinn" (The Book of Hymns of the Ancient Church of Ireland), suggested that it was Finnian of Moville, and that view has gained traction among modern scholars. 

The earliest mention of Ninian of Whithorn is in a short passage of "The Ecclesiastical History of the English People" by the Northumbrian monk Bede in c. 731. The 9th-century poem "Miracula Nyniae Episcopi" records some of the miracles attributed to him. A "Life of Saint Ninian" ("Vita Sancti Niniani") was written around 1160 by Ailred of Rievaulx, and in 1639 James Ussher discusses Ninian in his "Brittanicarum Ecclesiarum Antiquitates". These are the sources of information about Ninian of Whithorn, and all provide seemingly innocuous personal details about his life. However, there is no unchallenged historical evidence to support any of their stories, and all sources had political and religious agendas that were served by their accounts of Saint Ninian (discussed below).

Tradition holds that Ninian was a Briton who had studied in Rome, that he established an episcopal see at the "Candida Casa" in Whithorn, that he named the see for Saint Martin of Tours, that he converted the southern Picts to Christianity, and that he is buried at Whithorn. Variations of the story add that he had actually met Saint Martin, that his father was a Christian king, and that he was buried in a stone sarcophagus near the altar of his church. Further variations assert that he left for Ireland, and died there in 432. Dates for his birth are derived from the traditional mention of Saint Martin, who died in 397.

Bede says that Ninian (whose name he only renders in the ablative case "Nynia") was a Briton who had been instructed in Rome; that he made his church of stone, which was unusual among the Britons; that his episcopal see was named after Saint Martin of Tours; that he preached to and converted the southern Picts; that his base was in a place called " Ad Candidam Casam", which was in the province of the Bernicians; and that he was buried there, along with many other saints.

Bede's information is minimal and he does not claim it as fact, asserting only that he is passing on "traditional" information. He provides the first historical reference to Saint Ninian, in a passing reference contained in the final part of a single paragraph.

Leaving aside the tales regarding miracles, in the "Vita Sancti Niniani" Aelred includes the following incidental information regarding Saint Ninian: that his father was a Christian king; that he was consecrated a bishop in Rome and that he met Saint Martin in Tours; that Saint Martin sent masons with him on his homeward journey, at his request; that these masons built a church of stone, situated on the shore, and that on learning of Saint Martin's death, Ninian dedicated the church to him; that a certain rich and powerful "King Tuduvallus" was converted by him; that he died after having converted the Picts and returned home, being buried in a stone sarcophagus near the altar of his church; and that he had once travelled with his brother, named "Plebia".

Aelred said that in addition to finding information about Ninian in Bede, he took much additional information for his "Life of S. Ninian" from a source written in a "barbarous language"; there is no further information about this text. Aelred wrote his "Life of S. Ninian" sometime after spending ten years at the Scottish court and thus had close connections both to the Scottish royal family and to Fergus of Galloway (who would resurrect the Bishopric of Galloway), all of whom would have been pleased to have a manuscript with such a glowing description of a Galwegian and Scottish saint. His work is what Thomas Heffernan refers to as a "sacred biography," probably intended for a politically ambitious audience.

Ussher wrote that Ninian left Candida Casa for "Cluayn-coner" in Ireland, and eventually died in Ireland; that his mother was a Spanish princess; that his father wished to regain him after having assented to his training for an ecclesiastical state; that a bell comes from heaven to call together his disciples; that a wooden church was raised by him, with beams delivered by stags; and that a harper with no experience at architecture was the builder of the church. He adds that a smith and his son, named respectively "Terna" and "Wyn", witnessed a miracle by Ninian and that the saint was granted lands to be called "Wytterna".

In addition, Skene attributes the "traditional" date of Ninian's death (16 September 432) ultimately to Ussher's "Life of Ninian," noting that the date is "without authority."

Ussher's contribution is often disparaged, as he both invented fictitious histories and misquoted legitimate manuscripts to suit his own purposes. Still, he had access to legitimate manuscripts, and he has contributed to some versions of the traditional stories.

Others who wrote of Saint Ninian used the accounts of Bede, Aelred, or Ussher, or used derivatives of them in combination with information from various manuscripts. This includes John Capgrave (1393‚Äì1464), John of Tinmouth (fl. c. 1366), John Colgan (d. c. 1657), and many others, up to the present day.

The anonymously written 8th century hagiographic "Miracula Nynie Episcopi" ("Miracles of Bishop Ninian") is discounted as a non-historical account, and copies are not widely extant.
Dedications to Saint Ninian are expressions of respect for the good works that are attributed to him, and the authenticity of the stories about him are not relevant to that point. Almost all of the dedications have their origins in the medieval era, after Aelred wrote his account.

The dedications are found throughout the lands of the ancient Picts of Scotland, throughout Scotland south of the Firths of Clyde and Forth, in Orkney and Shetland, and in parts of northern England. Ss Ninian and Triduana's Church, Edinburgh is a Roman Catholic church dedicated to Ninian.

Dedications on the Isle of Man date from the time of medieval Scottish dominance, and are not natively inspired.

There are dedications to St. Ninian in East Donegal and Belfast; and a spot formerly on the shore of Belfast Lough was traditionally known as St. Ninian's point, where the missionary reputedly landed after a voyage from Scotland. These connections reflect a strong Ulster-Scots heritage in both areas of Ulster.

There are also dedications elsewhere in the world where there is a Scottish heritage, such as Nova Scotia. St. Ninian's Cathedral is located in Antigonish, Nova Scotia.

There is a noticeable lack of dedications in the Scottish Highlands and Isles.

In Scotland the date 16 September is celebrated as St. Ninian's Day.

In the modern era, Ss Ninian and Triduana's Church, Edinburgh is a Roman Catholic church constructed in 1932 that is dedicated to St. Ninian.

St Martin and St Ninian is a Catholic church in Whithorn, Wigtownshire constructed in 1959‚Äì60 in the Diocese of Galloway. The architect was Harry Stuart Goodhart-Rendel, (1887‚Äì1859). 




</doc>
<doc id="28994" url="https://en.wikipedia.org/wiki?curid=28994" title="Standard Generalized Markup Language">
Standard Generalized Markup Language

The Standard Generalized Markup Language (SGML; ISO 8879:1986) is a standard for defining generalized markup languages for documents. ISO 8879 Annex A.1 states that generalized markup is "based on two postulates":

DocBook SGML and LinuxDoc are examples which were used almost exclusively with actual SGML tools.

SGML is an ISO standard: "ISO 8879:1986 Information processing¬†‚Äì Text and office systems¬†‚Äì Standard Generalized Markup Language (SGML)", of which there are three versions:


SGML is part of a trio of enabling ISO standards for electronic documents developed by ISO/IEC JTC1/SC34 (ISO/IEC Joint Technical Committee 1, Subcommittee 34¬†‚Äì Document description and processing languages) :


SGML is supported by various technical reports, in particular


SGML descended from IBM's Generalized Markup Language (GML), which Charles Goldfarb, Edward Mosher, and Raymond Lorie developed in the 1960s. Goldfarb, editor of the international standard, coined the "GML" term using their surname initials. Goldfarb also wrote the definitive work on SGML syntax in "The SGML Handbook". The syntax of SGML is closer to the COCOA format. As a document markup language, SGML was originally designed to enable the sharing of machine-readable large-project documents in government, law, and industry. Many such documents must remain readable for several decades‚Äîa long time in the information technology field. SGML also was extensively applied by the military, and the aerospace, technical reference, and industrial publishing industries. The advent of the XML profile has made SGML suitable for widespread application for small-scale, general-purpose use.

SGML (ENR+WWW) defines two kinds of validity. According to the revised Terms and Definitions of ISO 8879 (from the public draft):
A conforming SGML document must be either a type-valid SGML document, a tag-valid SGML document, or both. Note: A user may wish to enforce additional constraints on a document, such as whether a document instance is integrally-stored or free of entity references.
A type-valid SGML document is defined by the standard as
An SGML document in which, for each document instance, there is an associated document type declaration (DTD) to whose DTD that instance conforms.
A tag-valid SGML document is defined by the standard as
An SGML document, all of whose document instances are fully tagged. There need not be a document type declaration associated with any of the instances. Note: If there is a document type declaration, the instance can be parsed with or without reference to it.
"Tag-validity" was introduced in SGML (ENR+WWW) to support XML which allows documents with no DOCTYPE declaration but which can be parsed without a grammar, or documents which have a DOCTYPE declaration that makes no XML Infoset contributions to the document. The standard calls this "fully tagged". "Integrally stored" reflects the XML requirement that elements end in the same entity in which they started. "Reference-free" reflects the HTML requirement that entity references are for special characters and do not contain markup. SGML validity commentary, especially commentary that was made before 1997 or that is unaware of SGML (ENR+WWW), covers "type-validity" only.

The SGML emphasis on validity supports the requirement for generalized markup that "markup should be rigorous." (ISO 8879 A.1)

An SGML document may have three parts:

An SGML document may be composed from many entities (discrete pieces of text). In SGML, the entities and element types used in the document may be specified with a DTD, the different character sets, features, delimiter sets, and keywords are specified in the SGML Declaration to create the "concrete syntax" of the document.

Although full SGML allows implicit markup and some other kinds of tags, the XML specification (s4.3.1) states:

For introductory information on a basic, modern SGML syntax, see XML. The following material concentrates on features not in XML and is not a comprehensive summary of SGML syntax.

SGML generalizes and supports a wide range of markup languages as found in the mid 1980s. These ranged from terse Wiki-like syntaxes to RTF-like bracketed languages to HTML-like matching-tag languages. SGML did this by a relatively simple default "reference concrete syntax" augmented with a large number of optional features that could be enabled in the SGML Declaration. Not every SGML parser can necessarily process every SGML document. Because each processor's "System Declaration" can be compared to the document's "SGML Declaration" it is always possible to know whether a document is supported by a particular processor.

Many SGML features relate to markup minimization. Other features relate to concurrent (parallel) markup (CONCUR), to linking processing attributes (LINK), and to embedding SGML documents within SGML documents (SUBDOC).

The notion of customizable features was not appropriate for Web use, so one goal of XML was to minimize optional features. However, XML's well-formedness rules cannot support Wiki-like languages, leaving them unstandardized and difficult to integrate with non-text information systems.

The usual (default) SGML "concrete syntax" resembles this example, which is the default HTML concrete syntax:
<QUOTE TYPE="example">
</QUOTE>
SGML provides an "abstract syntax" that can be implemented in many different types of "concrete syntax". Although the markup norm is using angle brackets as start- and end- tag delimiters in an SGML document (per the standard-defined "reference concrete syntax"), it is possible to use other characters‚Äîprovided a suitable "concrete syntax" is defined in the document's SGML declaration. For example, an SGML interpreter might be programmed to parse GML, wherein the tags are delimited with a left colon and a right full stop, thus, an ":e" prefix denotes an end tag: codice_1. According to the reference syntax, letter-case (upper- or lower-) is not distinguished in tag names, thus the three tags: (i) codice_2, (ii) codice_3, and (iii) codice_4 are equivalent. ("NOTE:" A concrete syntax might "change" this rule via the NAMECASE NAMING declarations).

SGML has features for reducing the number of characters required to mark up a document, which must be enabled in the SGML Declaration. SGML processors need not support every available feature, thus allowing applications to tolerate many types of inadvertent markup omissions; however, SGML systems usually are intolerant of invalid structures. XML is intolerant of syntax omissions, and does not require a DTD for checking well-formedness.

Both start tags and end tags may be omitted from a document instance, provided:

For example, if OMITTAG YES is specified in the SGML Declaration (enabling the OMITTAG feature), and the DTD includes the following declarations:

<!ELEMENT chapter - - (title, section+)>
<!ELEMENT title o o (#PCDATA)>
<!ELEMENT section - - (title, subsection+)>
then this excerpt:
<chapter>Introduction to SGML
<section>The SGML Declaration
<subsection>

which omits two tags and two tags, would represent valid markup.

Omitting tags is optional¬†‚Äì the same excerpt could be tagged like this:
<chapter><title>Introduction to SGML</title>
<section><title>The SGML Declaration</title>
<subsection>

and would still represent valid markup.
Note: The OMITTAG feature is unrelated to the tagging of elements whose declared content is codice_6 as defined in the DTD:

<!ELEMENT image - o EMPTY>
Elements defined like this have no end tag, and specifying one in the document instance would result in invalid markup. This is syntactically different than XML empty elements in this regard.

Tags can be replaced with delimiter strings, for a terser markup, via the SHORTREF feature. This markup style is now associated with wiki markup, e.g. wherein two equals-signs (==), at the start of a line, are the "heading start-tag", and two equals signs (==) after that are the "heading end-tag".

SGML markup languages whose concrete syntax enables the SHORTTAG VALUE feature, do not require attribute values containing only alphanumeric characters to be enclosed within quotation marks‚Äîeither double codice_7 (LIT) or single codice_8 (LITA)‚Äîso that the previous markup example could be written:
<QUOTE TYPE=example>
</QUOTE>
One feature of SGML markup languages is the "presumptuous empty tagging", such that the empty end tag codice_9 in codice_10 "inherits" its value from the nearest previous full start tag, which, in this example, is codice_11 (in other words, it closes the most recently opened item). The expression is thus equivalent to codice_12.

Another feature is the "NET" (Null End Tag) construction: codice_13, which is structurally equivalent to codice_12.

Additionally, the SHORTTAG NETENABL IMMEDNET feature allows shortening tags surrounding an empty text value, but forbids shortening full tags:
<QUOTE></QUOTE>
can be written as
<QUOTE// <!-- not a typo! -->
wherein the first slash ( / ) stands for the NET-enabling "start-tag close" (NESTC), and the second slash stands for the NET. NOTE: XML defines NESTC with a /, and NET with an > (angled bracket)‚Äîhence the corresponding construct in XML appears as <QUOTE/>.

The third feature is 'text on the same line', allowing a markup item to be ended with a line-end; especially useful for headings and such, requiring using either SHORTREF or DATATAG minimization. For example, if the DTD includes the following declarations:
<!ELEMENT lines (line*)>
<!ELEMENT line O - (#PCDATA)>
<!ENTITY line-tagc "</line>">
<!SHORTREF one-line "&#RE;&#RS;" line-tagc>
<!USEMAP one-line line>
(and "&#RE;&#RS;" is a short-reference delimiter in the concrete syntax), then:
<lines>
first line
second line
</lines>
is equivalent to:
<lines>
<line>first line</line>
<line>second line</line>
</lines>
SGML has many features that defied convenient description with the popular formal automata theory and the contemporary parser technology of the 1980s and the 1990s. The standard warns in Annex H:

A report on an early implementation of a parser for basic SGML, the Amsterdam SGML Parser, notes and specifies various differences.

There appears to be no definitive classification of full SGML against a known class of formal grammar. Plausible classes may include tree-adjoining grammars and adaptive grammars.

XML is described as being generally parsable like a two-level grammar for non-validated XML and a Conway-style pipeline of coroutines (lexer, parser, validator) for valid XML. The SGML productions in the ISO standard are reported to be LL(3) or LL(4). XML-class subsets are reported to be expressible using a W-grammar. According to one paper, and probably considered at an "information set" or parse tree level rather than a character or delimiter level:

The SGML standard does not define SGML with formal data structures, such as parse trees; however, an SGML document is constructed of a rooted directed acyclic graph (RDAG) of physical storage units known as "entities", which is parsed into a RDAG of structural units known as "elements". The physical graph is loosely characterized as an "entity tree", but entities might appear multiple times. Moreover, the structure graph is also loosely characterized as an "element tree", but the ID/IDREF markup allows arbitrary arcs.

The results of parsing can also be understood as a data tree in different notations; where the document is the root node, and entities in other notations (text, graphics) are child nodes. SGML provides apparatus for linking to and annotating external non-SGML entities.

The SGML standard describes it in terms of "maps" and "recognition modes" (s9.6.1). Each entity, and each element, can have an associated "notation" or "declared content type", which determines the kinds of references and tags which will be recognized in that entity and element. Also, each element can have an associated "delimiter map" (and "short reference map"), which determines which characters are treated as delimiters in context. The SGML standard characterizes parsing as a state machine switching between recognition modes. During parsing, there is a stack of maps that configure the scanner, while the tokenizer relates to the recognition modes.

Parsing involves traversing the dynamically-retrieved entity graph, finding/implying tags and the element structure, and validating those tags against the grammar. An unusual aspect of SGML is that the grammar (DTD) is used both passively¬†‚Äî to "recognize" lexical structures, and actively¬†‚Äî to "generate" missing structures and tags that the DTD has declared optional. End- and start- tags can be omitted, because they can be inferred. Loosely, a series of tags can be omitted only if there is a single, possible path in the grammar to imply them. It was this active use of grammars that made concrete SGML parsing difficult to formally characterize.

SGML uses the term "validation" for both recognition and generation. XML does not use the grammar (DTD) to change delimiter maps or to inform the parse modes, and does not allow tag omission; consequently, XML validation of elements is not active in the sense that SGML validation is active. SGML "without" a DTD (e.g. simple XML), is a grammar or a language; SGML "with" a DTD is a metalanguage. SGML with an SGML declaration is, perhaps, a meta-metalanguage, since it is a metalanguage whose declaration mechanism "is" a metalanguage.

SGML has an abstract syntax implemented by many possible concrete syntaxes; however, this is not the same usage as in an abstract syntax tree and as in a concrete syntax tree. In the SGML usage, a concrete syntax is a set of specific delimiters, while the abstract syntax is the set of names for the delimiters. The XML Infoset corresponds more to the programming language notion of abstract syntax introduced by John McCarthy.

The W3C XML (Extensible Markup Language) is a profile (subset) of SGML designed to ease the implementation of the parser compared to a full SGML parser, primarily for use on the World Wide Web. In addition to disabling many SGML options present in the reference syntax (such as omitting tags and nested subdocuments) XML adds a number of additional restrictions on the kinds of SGML syntax. For example, despite enabling SGML shortened tag forms, XML does not allow unclosed start or end tags. It also relied on many of the additions made by the WebSGML Annex. XML currently is more widely used than full SGML. XML has lightweight internationalization based on Unicode. Applications of XML include XHTML, XQuery, XSLT, XForms, XPointer, JSP, SVG, RSS, Atom, XML-RPC, RDF/XML, and SOAP.

While HTML was developed partially independently and in parallel with SGML, its creator, Tim Berners-Lee, intended it to be an application of SGML. The design of HTML (Hyper Text Markup Language) was therefore inspired by SGML tagging, but, since no clear expansion and parsing guidelines were established, most actual HTML documents are not valid SGML documents. Later, HTML was reformulated (version 2.0) to be more of an SGML application; however, the HTML markup language has many legacy- and exception-handling features that differ from SGML's requirements. HTML 4 is an SGML application that fully conforms to ISO 8879¬†‚Äì SGML.

The charter for the 2006 revival of the World Wide Web Consortium HTML Working Group says, "the Group will not assume that an SGML parser is used for 'classic HTML'". Although HTML syntax closely resembles SGML syntax with the default "reference concrete syntax", HTML5 abandons any attempt to define HTML as an SGML application, explicitly defining its own parsing rules, which more closely match existing implementations and documents. It does, however, define an alternative XHTML serialization, which conforms to XML and therefore to SGML as well.

The second edition of the "Oxford English Dictionary" (OED) is entirely marked up with an SGML-based markup language using the LEXX.

The third edition is marked up as XML.

Other document markup languages are partly related to SGML and XML, but‚Äîbecause they cannot be parsed or validated or other-wise processed using standard SGML and XML tools‚Äîthey are not considered either SGML or XML languages; the Z Format markup language for typesetting and documentation is an example.

Several modern programming languages support tags as primitive token types, or now support Unicode and regular expression pattern-matching. An example is the Scala programming language.

Document markup languages defined using SGML are called "applications" by the standard; many pre-XML SGML applications were proprietary property of the organizations which developed them, and thus unavailable in the World Wide Web. The following list is of pre-XML SGML applications.


Significant open-source implementations of SGML have included:

SP and Jade, the associated DSSSL processors, are maintained by the OpenJade project, and are common parts of Linux distributions. A general archive of SGML software and materials resides at SUNET. The original HTML parser class, in Sun System's implementation of Java, is a limited-features SGML parser, using SGML terminology and concepts.



</doc>
<doc id="29000" url="https://en.wikipedia.org/wiki?curid=29000" title="Speciation">
Speciation

Speciation is the evolutionary process by which populations evolve to become distinct species. The biologist Orator F. Cook coined the term in 1906 for cladogenesis, the splitting of lineages, as opposed to anagenesis, phyletic evolution within lineages. Charles Darwin was the first to describe the role of natural selection in speciation in his 1859 book "On the Origin of Species". He also identified sexual selection as a likely mechanism, but found it problematic.

There are four geographic modes of speciation in nature, based on the extent to which speciating populations are isolated from one another: allopatric, peripatric, parapatric, and sympatric. Speciation may also be induced artificially, through animal husbandry, agriculture, or laboratory experiments. Whether genetic drift is a minor or major contributor to speciation is the subject matter of much ongoing discussion.

Rapid sympatric speciation can take place through polyploidy, such as by doubling of chromosome number; the result is progeny which are immediately reproductively isolated from the parent population. New species can also be created through hybridisation followed, if the hybrid is favoured by natural selection, by reproductive isolation.

In addressing the question of the origin of species, there are two key issues: (1) what are the evolutionary mechanisms of speciation, and (2) what accounts for the separateness and individuality of species in the biota? Since Charles Darwin's time, efforts to understand the nature of species have primarily focused on the first aspect, and it is now widely agreed that the critical factor behind the origin of new species is reproductive isolation. Next we focus on the second aspect of the origin of species.

In "On the Origin of Species" (1859), Darwin interpreted biological evolution in terms of natural selection, but was perplexed by the clustering of organisms into species. Chapter 6 of Darwin's book is entitled "Difficulties of the Theory." In discussing these "difficulties" he noted "Firstly, why, if species have descended from other species by insensibly fine gradations, do we not everywhere see innumerable transitional forms? Why is not all nature in confusion instead of the species being, as we see them, well defined?" This dilemma can be referred to as the absence or rarity of transitional varieties in habitat space.

Another dilemma, related to the first one, is the absence or rarity of transitional varieties in time. Darwin pointed out that by the theory of natural selection "innumerable transitional forms must have existed," and wondered "why do we not find them embedded in countless numbers in the crust of the earth." That clearly defined species actually do exist in nature in both space and time implies that some fundamental feature of natural selection operates to generate and maintain species.

It has been argued that the resolution of Darwin's first dilemma lies in the fact that out-crossing sexual reproduction has an intrinsic cost of rarity. The cost of rarity arises as follows. If, on a resource gradient, a large number of separate species evolve, each exquisitely adapted to a very narrow band on that gradient, each species will, of necessity, consist of very few members. Finding a mate under these circumstances may present difficulties when many of the individuals in the neighborhood belong to other species. Under these circumstances, if any species' population size happens, by chance, to increase (at the expense of one or other of its neighboring species, if the environment is saturated), this will immediately make it easier for its members to find sexual partners. The members of the neighboring species, whose population sizes have decreased, experience greater difficulty in finding mates, and therefore form pairs less frequently than the larger species. This has a snowball effect, with large species growing at the expense of the smaller, rarer species, eventually driving them to extinction. Eventually, only a few species remain, each distinctly different from the other. The cost of rarity not only involves the costs of failure to find a mate, but also indirect costs such as the cost of communication in seeking out a partner at low population densities.

Rarity brings with it other costs. Rare and unusual features are very seldom advantageous. In most instances, they indicate a (non-silent) mutation, which is almost certain to be deleterious. It therefore behooves sexual creatures to avoid mates sporting rare or unusual features (koinophilia). Sexual populations therefore rapidly shed rare or peripheral phenotypic features, thus canalizing the entire external appearance, as illustrated in the accompanying illustration of the African pygmy kingfisher, "Ispidina picta". This uniformity of all the adult members of a sexual species has stimulated the proliferation of field guides on birds, mammals, reptiles, insects, and many other taxa, in which a species can be described with a single illustration (or two, in the case of sexual dimorphism). Once a population has become as homogeneous in appearance as is typical of most species (and is illustrated in the photograph of the African pygmy kingfisher), its members will avoid mating with members of other populations that look different from themselves. Thus, the avoidance of mates displaying rare and unusual phenotypic features inevitably leads to reproductive isolation, one of the hallmarks of speciation.

In the contrasting case of organisms that reproduce asexually, there is no cost of rarity; consequently, there are only benefits to fine-scale adaptation. Thus, asexual organisms very frequently show the continuous variation in form (often in many different directions) that Darwin expected evolution to produce, making their classification into "species" (more correctly, morphospecies) very difficult.

All forms of natural speciation have taken place over the course of evolution; however, debate persists as to the relative importance of each mechanism in driving biodiversity.

One example of natural speciation is the diversity of the three-spined stickleback, a marine fish that, after the last glacial period, has undergone speciation into new freshwater colonies in isolated lakes and streams. Over an estimated 10,000 generations, the sticklebacks show structural differences that are greater than those seen between different genera of fish including variations in fins, changes in the number or size of their bony plates, variable jaw structure, and color differences.

During allopatric (from the ancient Greek "allos", "other" + "patrƒÅ", "fatherland") speciation, a population splits into two geographically isolated populations (for example, by habitat fragmentation due to geographical change such as mountain formation). The isolated populations then undergo genotypic or phenotypic divergence as: (a) they become subjected to dissimilar selective pressures; (b) they independently undergo genetic drift; (c) different mutations arise in the two populations. When the populations come back into contact, they have evolved such that they are reproductively isolated and are no longer capable of exchanging genes. Island genetics is the term associated with the tendency of small, isolated genetic pools to produce unusual traits. Examples include insular dwarfism and the radical changes among certain famous island chains, for example on Komodo. The Gal√°pagos Islands are particularly famous for their influence on Charles Darwin. During his five weeks there he heard that Gal√°pagos tortoises could be identified by island, and noticed that finches differed from one island to another, but it was only nine months later that he reflected that such facts could show that species were changeable. When he returned to England, his speculation on evolution deepened after experts informed him that these were separate species, not just varieties, and famously that other differing Gal√°pagos birds were all species of finches. Though the finches were less important for Darwin, more recent research has shown the birds now known as Darwin's finches to be a classic case of adaptive evolutionary radiation.

In peripatric speciation, a subform of allopatric speciation, new species are formed in isolated, smaller peripheral populations that are prevented from exchanging genes with the main population. It is related to the concept of a founder effect, since small populations often undergo bottlenecks. Genetic drift is often proposed to play a significant role in peripatric speciation.

Case studies include Mayr's investigation of bird fauna; the Australian bird "Petroica multicolor"; and reproductive isolation in populations of "Drosophila" subject to population bottlenecking.

In parapatric speciation, there is only partial separation of the zones of two diverging populations afforded by geography; individuals of each species may come in contact or cross habitats from time to time, but reduced fitness of the heterozygote leads to selection for behaviours or mechanisms that prevent their interbreeding. Parapatric speciation is modelled on continuous variation within a "single," connected habitat acting as a source of natural selection rather than the effects of isolation of habitats produced in peripatric and allopatric speciation.

Parapatric speciation may be associated with differential landscape-dependent selection. Even if there is a gene flow between two populations, strong differential selection may impede assimilation and different species may eventually develop. Habitat differences may be more important in the development of reproductive isolation than the isolation time. Caucasian rock lizards "Darevskia rudis", "D. valentini" and "D. portschinskii" all hybridize with each other in their hybrid zone; however, hybridization is stronger between "D. portschinskii" and "D. rudis", which separated earlier but live in similar habitats than between "D. valentini" and two other species, which separated later but live in climatically different habitats.

Ecologists refer to parapatric and peripatric speciation in terms of ecological niches. A niche must be available in order for a new species to be successful. Ring species such as "Larus" gulls have been claimed to illustrate speciation in progress, though the situation may be more complex. The grass "Anthoxanthum odoratum" may be starting parapatric speciation in areas of mine contamination.

Sympatric speciation is the formation of two or more descendant species from a single ancestral species all occupying the same geographic location.

Often-cited examples of sympatric speciation are found in insects that become dependent on different host plants in the same area.

The best known example of sympatric speciation is that of the cichlids of East Africa inhabiting the Rift Valley lakes, particularly Lake Victoria, Lake Malawi and Lake Tanganyika. There are over 800 described species, and according to estimates, there could be well over 1,600 species in the region. Their evolution is cited as an example of both natural and sexual selection. A 2008 study suggests that sympatric speciation has occurred in Tennessee cave salamanders. Sympatric speciation driven by ecological factors may also account for the extraordinary diversity of crustaceans living in the depths of Siberia's Lake Baikal.

Budding speciation has been proposed as a particular form of sympatric speciation, whereby small groups of individuals become progressively more isolated from the ancestral stock by breeding preferentially with one another. This type of speciation would be driven by the conjunction of various advantages of inbreeding such as the expression of advantageous recessive phenotypes, reducing the recombination load, and reducing the cost of sex.

The hawthorn fly ("Rhagoletis pomonella"), also known as the apple maggot fly, appears to be undergoing sympatric speciation. Different populations of hawthorn fly feed on different fruits. A distinct population emerged in North America in the 19th century some time after apples, a non-native species, were introduced. This apple-feeding population normally feeds only on apples and not on the historically preferred fruit of hawthorns. The current hawthorn feeding population does not normally feed on apples. Some evidence, such as that six out of thirteen allozyme loci are different, that hawthorn flies mature later in the season and take longer to mature than apple flies; and that there is little evidence of interbreeding (researchers have documented a 4‚Äì6% hybridization rate) suggests that sympatric speciation is occurring.

Reinforcement, sometimes referred to as the Wallace effect, is the process by which natural selection increases reproductive isolation. It may occur after two populations of the same species are separated and then come back into contact. If their reproductive isolation was complete, then they will have already developed into two separate incompatible species. If their reproductive isolation is incomplete, then further mating between the populations will produce hybrids, which may or may not be fertile. If the hybrids are infertile, or fertile but less fit than their ancestors, then there will be further reproductive isolation and speciation has essentially occurred (e.g., as in horses and donkeys).

The reasoning behind this is that if the parents of the hybrid offspring each have naturally selected traits for their own certain environments, the hybrid offspring will bear traits from both, therefore would not fit either ecological niche as well as either parent. The low fitness of the hybrids would cause selection to favor assortative mating, which would control hybridization. This is sometimes called the Wallace effect after the evolutionary biologist Alfred Russel Wallace who suggested in the late 19th century that it might be an important factor in speciation. 
Conversely, if the hybrid offspring are more fit than their ancestors, then the populations will merge back into the same species within the area they are in contact.

Reinforcement favoring reproductive isolation is required for both parapatric and sympatric speciation. Without reinforcement, the geographic area of contact between different forms of the same species, called their "hybrid zone," will not develop into a boundary between the different species. Hybrid zones are regions where diverged populations meet and interbreed. Hybrid offspring are very common in these regions, which are usually created by diverged species coming into secondary contact. Without reinforcement, the two species would have uncontrollable inbreeding. Reinforcement may be induced in artificial selection experiments as described below.

Ecological selection is "the interaction of individuals with their environment during resource acquisition". Natural selection is inherently involved in the process of speciation, whereby, "under ecological speciation, populations in different environments, or populations exploiting different resources, experience contrasting natural selection pressures on the traits that directly or indirectly bring about the evolution of reproductive isolation". Evidence for the role ecology plays in the process of speciation exists. Studies of stickleback populations support ecologically-linked speciation arising as a by-product, alongside numerous studies of parallel speciation, where isolation evolves between independent populations of species adapting to contrasting environments than between independent populations adapting to similar environments. Ecological speciation occurs with much of the evidence, "...accumulated from top-down studies of adaptation and reproductive isolation".

It is widely appreciated that sexual selection could drive speciation in many clades, independently of natural selection. However the term "speciation", in this context, tends to be used in two different, but not mutually exclusive senses. The first and most commonly used sense refers to the "birth" of new species. That is, the splitting of an existing species into two separate species, or the budding off of a new species from a parent species, both driven by a biological "fashion fad" (a preference for a feature, or features, in one or both sexes, that do not necessarily have any adaptive qualities). In the second sense, "speciation" refers to the wide-spread tendency of sexual creatures to be grouped into clearly defined species, rather than forming a continuum of phenotypes both in time and space ‚Äì which would be the more obvious or logical consequence of natural selection. This was indeed recognized by Darwin as problematic, and included in his "On the Origin of Species" (1859), under the heading "Difficulties with the Theory". There are several suggestions as to how mate choice might play a significant role in resolving . If speciation takes place in the absence of natural selection, it might be referred to as nonecological speciation. 

New species have been created by animal husbandry, but the dates and methods of the initiation of such species are not clear. Often, the domestic counterpart of the wild ancestor can still interbreed and produce fertile offspring as in the case of domestic cattle, that can be considered the same species as several varieties of wild ox, gaur, yak, etc., or domestic sheep that can interbreed with the mouflon.

The best-documented creations of new species in the laboratory were performed in the late 1980s. William R. Rice and George W. Salt bred "Drosophila melanogaster" fruit flies using a maze with three different choices of habitat such as light/dark and wet/dry. Each generation was placed into the maze, and the groups of flies that came out of two of the eight exits were set apart to breed with each other in their respective groups. After thirty-five generations, the two groups and their offspring were isolated reproductively because of their strong habitat preferences: they mated only within the areas they preferred, and so did not mate with flies that preferred the other areas. The history of such attempts is described by Rice and Elen E. Hostert (1993).
Diane Dodd used a laboratory experiment to show how reproductive isolation can develop in "Drosophila pseudoobscura" fruit flies after several generations by placing them in different media, starch- and maltose-based media.

Dodd's experiment has been easy for many others to replicate, including with other kinds of fruit flies and foods. Research in 2005 has shown that this rapid evolution of reproductive isolation may in fact be a relic of infection by "Wolbachia" bacteria.

Alternatively, these observations are consistent with the notion that sexual creatures are inherently reluctant to mate with individuals whose appearance or behavior is different from the norm. The risk that such deviations are due to heritable maladaptations is very high. Thus, if a sexual creature, unable to predict natural selection's future direction, is conditioned to produce the fittest offspring possible, it will avoid mates with unusual habits or features. Sexual creatures will then inevitably tend to group themselves into reproductively isolated species.

Few speciation genes have been found. They usually involve the reinforcement process of late stages of speciation. In 2008, a speciation gene causing reproductive isolation was reported. It causes hybrid sterility between related subspecies. The order of speciation of three groups from a common ancestor may be unclear or unknown; a collection of three such species is referred to as a "trichotomy."

Polyploidy is a mechanism that has caused many rapid speciation events in sympatry because offspring of, for example, tetraploid x diploid matings often result in triploid sterile progeny. However, not all polyploids are reproductively isolated from their parental plants, and gene flow may still occur for example through triploid hybrid x diploid matings that produce tetraploids, or matings between meiotically unreduced gametes from diploids and gametes from tetraploids (see also hybrid speciation).

It has been suggested that many of the existing plant and most animal species have undergone an event of polyploidization in their evolutionary history. Reproduction of successful polyploid species is sometimes asexual, by parthenogenesis or apomixis, as for unknown reasons many asexual organisms are polyploid. Rare instances of polyploid mammals are known, but most often result in prenatal death.

Hybridization between two different species sometimes leads to a distinct phenotype. This phenotype can also be fitter than the parental lineage and as such natural selection may then favor these individuals. Eventually, if reproductive isolation is achieved, it may lead to a separate species. However, reproductive isolation between hybrids and their parents is particularly difficult to achieve and thus hybrid speciation is considered an extremely rare event. The Mariana mallard is thought to have arisen from hybrid speciation.

Hybridization is an important means of speciation in plants, since polyploidy (having more than two copies of each chromosome) is tolerated in plants more readily than in animals. Polyploidy is important in hybrids as it allows reproduction, with the two different sets of chromosomes each being able to pair with an identical partner during meiosis. Polyploids also have more genetic diversity, which allows them to avoid inbreeding depression in small populations.

Hybridization without change in chromosome number is called homoploid hybrid speciation. It is considered very rare but has been shown in "Heliconius" butterflies and sunflowers. Polyploid speciation, which involves changes in chromosome number, is a more common phenomenon, especially in plant species.

Theodosius Dobzhansky, who studied fruit flies in the early days of genetic research in 1930s, speculated that parts of chromosomes that switch from one location to another might cause a species to split into two different species. He mapped out how it might be possible for sections of chromosomes to relocate themselves in a genome. Those mobile sections can cause sterility in inter-species hybrids, which can act as a speciation pressure. In theory, his idea was sound, but scientists long debated whether it actually happened in nature. Eventually a competing theory involving the gradual accumulation of mutations was shown to occur in nature so often that geneticists largely dismissed the moving gene hypothesis. However, 2006 research shows that jumping of a gene from one chromosome to another can contribute to the birth of new species. This validates the reproductive isolation mechanism, a key component of speciation.

There is debate as to the rate at which speciation events occur over geologic time. While some evolutionary biologists claim that speciation events have remained relatively constant and gradual over time (known as "Phyletic gradualism" ‚Äì see diagram), some palaeontologists such as Niles Eldredge and Stephen Jay Gould have argued that species usually remain unchanged over long stretches of time, and that speciation occurs only over relatively brief intervals, a view known as "punctuated equilibrium". (See diagram, and .)

Evolution can be extremely rapid, as shown in the creation of domesticated animals and plants in a very short geological space of time, spanning only a few tens of thousands of years. Maize ("Zea mays"), for instance, was created in Mexico in only a few thousand years, starting about 7,000 to 12,000 years ago. This raises the question of why the long term rate of evolution is far slower than is theoretically possible.

Evolution is imposed on species or groups. It is not planned or striven for in some Lamarckist way. The mutations on which the process depends are random events, and, except for the "silent mutations" which do not affect the functionality or appearance of the carrier, are thus usually disadvantageous, and their chance of proving to be useful in the future is vanishingly small. Therefore, while a species or group might benefit from being able to adapt to a new environment by accumulating a wide range of genetic variation, this is to the detriment of the "individuals" who have to carry these mutations until a small, unpredictable minority of them ultimately contributes to such an adaptation. Thus, the "capability" to evolve would require group selection, a concept discredited by (for example) George C. Williams, John Maynard Smith and Richard Dawkins as selectively disadvantageous to the individual.

The resolution to Darwin's second dilemma might thus come about as follows:

If sexual individuals are disadvantaged by passing mutations on to their offspring, they will avoid mutant mates with strange or unusual characteristics. Mutations that affect the external appearance of their carriers will then rarely be passed on to the next and subsequent generations. They would therefore seldom be tested by natural selection. Evolution is, therefore, effectively halted or slowed down considerably. The only mutations that can accumulate in a population, on this punctuated equilibrium view, are ones that have no noticeable effect on the outward appearance and functionality of their bearers (i.e., they are "silent" or "neutral mutations," which can be, and are, used to trace the relatedness and age of populations and species.)
This argument implies that evolution can only occur if mutant mates cannot be avoided, as a result of a severe scarcity of potential mates. This is most likely to occur in small, isolated communities. These occur most commonly on small islands, in remote valleys, lakes, river systems, or caves, or during the aftermath of a mass extinction. Under these circumstances, not only is the choice of mates severely restricted but population bottlenecks, founder effects, genetic drift and inbreeding cause rapid, random changes in the isolated population's genetic composition. Furthermore, hybridization with a related species trapped in the same isolate might introduce additional genetic changes. If an isolated population such as this survives its genetic upheavals, and subsequently expands into an unoccupied niche, or into a niche in which it has an advantage over its competitors, a new species, or subspecies, will have come in being. In geological terms, this will be an abrupt event. A resumption of avoiding mutant mates will thereafter result, once again, in evolutionary stagnation.

In apparent confirmation of this punctuated equilibrium view of evolution, the fossil record of an evolutionary progression typically consists of species that suddenly appear, and ultimately disappear, hundreds of thousands or millions of years later, without any change in external appearance. Graphically, these fossil species are represented by lines parallel with the time axis, whose lengths depict how long each of them existed. The fact that the lines remain parallel with the time axis illustrates the unchanging appearance of each of the fossil species depicted on the graph. During each species' existence new species appear at random intervals, each also lasting many hundreds of thousands of years before disappearing without a change in appearance. The exact relatedness of these concurrent species is generally impossible to determine. This is illustrated in the diagram depicting the distribution of hominin species through time since the hominins separated from the line that led to the evolution of our closest living primate relatives, the chimpanzees.

For similar evolutionary time lines see, for instance, the paleontological list of African dinosaurs, Asian dinosaurs, the Lampriformes and Amiiformes.



</doc>
<doc id="29004" url="https://en.wikipedia.org/wiki?curid=29004" title="SQL">
SQL

SQL ( "S-Q-L", "sequel"; Structured Query Language) is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS). It is particularly useful in handling structured data, i.e. data incorporating relations among entities and variables.

SQL offers two main advantages over older read‚Äìwrite APIs such as ISAM or VSAM. Firstly, it introduced the concept of accessing many records with one single command. Secondly, it eliminates the need to specify "how" to reach a record, e.g. with or without an index.

Originally based upon relational algebra and tuple relational calculus, SQL consists of many types of statements, which may be informally classed as sublanguages, commonly: a data query language (DQL), a data definition language (DDL), a data control language (DCL), and a data manipulation language (DML). The scope of SQL includes data query, data manipulation (insert, update and delete), data definition (schema creation and modification), and data access control. Although SQL is essentially a declarative language (4GL), it also includes procedural elements.

SQL was one of the first commercial languages to utilize Edgar F. Codd‚Äôs relational model. The model was described in his influential 1970 paper, "A Relational Model of Data for Large Shared Data Banks". Despite not entirely adhering to the relational model as described by Codd, it became the most widely used database language.

SQL became a standard of the American National Standards Institute (ANSI) in 1986, and of the International Organization for Standardization (ISO) in 1987. Since then the standard has been revised to include a larger set of features. Despite the existence of standards, most SQL code requires at least some changes before being ported to different database systems.

SQL was initially developed at IBM by Donald D. Chamberlin and Raymond F. Boyce after learning about the relational model from Edgar F. Codd in the early 1970s. This version, initially called "SEQUEL" ("Structured English Query Language"), was designed to manipulate and retrieve data stored in IBM's original quasi-relational database management system, System R, which a group at IBM San Jose Research Laboratory had developed during the 1970s.

Chamberlin and Boyce's first attempt of a relational database language was Square, but it was difficult to use due to subscript notation. After moving to the San Jose Research Laboratory in 1973, they began work on SEQUEL. The acronym SEQUEL was later changed to SQL because "SEQUEL" was a trademark of the UK-based Hawker Siddeley Dynamics Engineering Limited company.

After testing SQL at customer test sites to determine the usefulness and practicality of the system, IBM began developing commercial products based on their System R prototype including System/38, SQL/DS, and DB2, which were commercially available in 1979, 1981, and 1983, respectively.

In the late 1970s, Relational Software, Inc. (now Oracle Corporation) saw the potential of the concepts described by Codd, Chamberlin, and Boyce, and developed their own SQL-based RDMS with aspirations of selling it to the U.S. Navy, Central Intelligence Agency, and other U.S. government agencies. In June 1979, Relational Software, Inc. introduced the first commercially available implementation of SQL, Oracle V2 (Version2) for VAX computers.

By 1986, ANSI and ISO standard groups officially adopted the standard "Database Language SQL" language definition. New versions of the standard were published in 1989, 1992, 1996, 1999, 2003, 2006, 2008, 2011 and, most recently, 2016.

</math>

The SQL language is subdivided into several language elements, including:


SQL is designed for a specific purpose: to query data contained in a relational database. SQL is a set-based, declarative programming language, not an imperative programming language like C or BASIC. However, extensions to Standard SQL add procedural programming language functionality, such as control-of-flow constructs. These include:

In addition to the standard SQL/PSM extensions and proprietary SQL extensions, procedural and object-oriented programmability is available on many SQL platforms via DBMS integration with other languages. The SQL standard defines SQL/JRT extensions (SQL Routines and Types for the Java Programming Language) to support Java code in SQL databases. Microsoft SQL Server 2005 uses the SQLCLR (SQL Server Common Language Runtime) to host managed .NET assemblies in the database, while prior versions of SQL Server were restricted to unmanaged extended stored procedures primarily written in C. PostgreSQL lets users write functions in a wide variety of languages‚Äîincluding Perl, Python, Tcl, JavaScript (PL/V8) and C.

SQL implementations are incompatible between vendors and do not necessarily completely follow standards. In particular date and time syntax, string concatenation, codice_1s, and comparison case sensitivity vary from vendor to vendor. Particular exceptions are PostgreSQL and Mimer SQL which strive for standards compliance, though PostgreSQL does not adhere to the standard in how folding of unquoted names is done. The folding of unquoted names to lower case in PostgreSQL is incompatible with the SQL standard, which says that unquoted names should be folded to upper case. Thus, codice_2 should be equivalent to codice_3 not codice_4 according to the standard.

Popular implementations of SQL commonly omit support for basic features of Standard SQL, such as the codice_5 or codice_6 data types. The most obvious such examples, and incidentally the most popular commercial and proprietary SQL DBMSs, are Oracle (whose codice_5 behaves as codice_8, and lacks a codice_6 type) and MS SQL Server (before the 2008 version). As a result, SQL code can rarely be ported between database systems without modifications.
There are several reasons for this lack of portability between database systems:

SQL was adopted as a standard by the American National Standards Institute (ANSI) in 1986 as SQL-86 and the International Organization for Standardization (ISO) in 1987. It is maintained by "ISO/IEC JTC 1, Information technology, Subcommittee SC 32, Data management and interchange".

Until 1996, the National Institute of Standards and Technology (NIST) data management standards program certified SQL DBMS compliance with the SQL standard. Vendors now self-certify the compliance of their products.

The original standard declared that the official pronunciation for "SQL" was an initialism: ("ess cue el"). Regardless, many English-speaking database professionals (including Donald Chamberlin himself) use the acronym-like pronunciation of ("sequel"), mirroring the language's pre-release development name, "SEQUEL".<br> The SQL standard has gone through a number of revisions:
The standard is commonly denoted by the pattern: "ISO/IEC 9075-n:yyyy Part n: title", or, as a shortcut, "ISO/IEC 9075".

"ISO/IEC 9075" is complemented by "ISO/IEC 13249: SQL Multimedia and Application Packages" (SQL/MM), which defines SQL based interfaces and packages to widely spread applications like video, audio and spatial data.
Interested parties may purchase SQL standards documents from ISO, IEC or ANSI. A draft of SQL:2008 is freely available as a zip archive.
The SQL standard is divided into ten parts. There are gaps in the numbering due to the withdrawal of outdated parts.
ISO/IEC 9075 is complemented by ISO/IEC 13249 "SQL Multimedia and Application Packages". This closely related but separate standard is developed by the same committee. It defines interfaces and packages based on SQL. The aim is a unified access to typical database applications like text, pictures, data mining or spatial data.

ISO/IEC 9075 is also accompanied by a series of Technical Reports, published as ISO/IEC TR 19075 in 8 parts. These Technical Reports explain the justification for and usage of some features of SQL, giving examples where appropriate. The Technical Reports are non-normative; if there is any discrepancy from 9075, the text in 9075 holds. Currently available 19075 Technical Reports are:

A distinction should be made between alternatives to SQL as a language, and alternatives to the relational model itself. Below are proposed relational alternatives to the SQL language. See navigational database and NoSQL for alternatives to the relational model.

Distributed Relational Database Architecture (DRDA) was designed by a work group within IBM in the period 1988 to 1994. DRDA enables network connected relational databases to cooperate to fulfill SQL requests.

An interactive user or program can issue SQL statements to a local RDB and receive tables of data and status indicators in reply from remote RDBs. SQL statements can also be compiled and stored in remote RDBs as packages and then invoked by package name. This is important for the efficient operation of application programs that issue complex, high-frequency queries. It is especially important when the tables to be accessed are located in remote systems.

The messages, protocols, and structural components of DRDA are defined by the Distributed Data Management Architecture.

SQL deviates in several ways from its theoretical foundation, the relational model and its tuple calculus. In that model, a table is a set of tuples, while in SQL, tables and query results are lists of rows: the same row may occur multiple times, and the order of rows can be employed in queries (e.g. in the LIMIT clause).

Critics argue that SQL should be replaced with a language that returns strictly to the original foundation: for example, see "The Third Manifesto". However, no known proof exists that such uniqueness cannot be added to SQL itself, or at least a variation of SQL. In other words, it's quite possible that SQL can be "fixed" or at least improved in this regard such that the industry may not have to switch to a completely different query language to obtain uniqueness. Debate on this remains open.

Chamberlin discusses four historical criticisms of SQL in a 2012 paper:

Early specifications did not support major features, such as primary keys. Result sets could not be named, and sub-queries had not been defined. These were added in 1992.

The concept of Null is subject of some debates. The Null marker indicates that there is no value, even no 0 for an integer column or a string of length 0 for a text column. The concept of Nulls enforces the 3-valued-logic in SQL, which is a concrete implementation of the general 3 valued logic. 
Another popular criticism is that it allows duplicate rows, making integration with languages such as Python, whose data types might make it difficult to accurately represent the data, difficult in terms of parsing and by the absence of modularity.

This can be avoided declaring a unique constraint with one or more fields that identifies uniquely a row in the table. That constraint could also become the primary key of the table.

In a similar sense to Object-relational impedance mismatch, there is a mismatch between the declarative SQL language and the procedural languages that SQL is typically embedded in.

Main data integrity categories of each RDBMS.
Establishes that within the table the primary key has a unique value for each row, checking the uniqueness of the value of the primary key avoiding that there are duplicated rows in a table.
Restricts the type, format, and value range that applies to valid entries for a column within a table
Makes rows in a table that are being used by other records impossible to delete
Other specific rules not included above apply

The SQL standard defines three kinds of data types:


"Constructed types" are one of ARRAY, MULTISET, REF(erence), or ROW. <br>"User-defined types" are comparable to classes in object-oriented language with their own constructors, observers, mutators, methods, inheritance, overloading, overwriting, interfaces, and so on.


The ISO/IEC Information Technology Task Force publishes publicly available standards including SQL. Technical Corrigenda (corrections) and Technical Reports (discussion documents) are published there.

SQL -- Part 1: Framework (SQL/Framework)

Formal SQL standards are available from ISO and ANSI for a fee. For informative use, as opposed to strict standards compliance, late drafts often suffice.




</doc>
<doc id="29005" url="https://en.wikipedia.org/wiki?curid=29005" title="Strait of Hormuz">
Strait of Hormuz

The Strait of Hormuz ( "Tangeh-ye Hormoz" "Ma·∏çƒ´q Hurmuz" ) is a strait between the Persian Gulf and the Gulf of Oman. It provides the only sea passage from the Persian Gulf to the open ocean and is one of the world's most strategically important choke points. On the north coast lies Iran, and on the south coast the United Arab Emirates and Musandam, an exclave of Oman. The strait is about long, with a width varying from about to .

A third of the world's liquefied natural gas and almost 25% of total global oil consumption passes through the strait, making it a highly important strategic location for international trade.

The opening to the Persian Gulf was described, but not given a name, in the "Periplus of the Erythraean Sea", a 1st-century mariner's guide:

In the 10th17th centuries AD, the Kingdom of Ormus, which seems to have given the strait its name, was located here. Scholars, historians and linguists derive the name "Ormuz" from the local Persian word "Hur-mogh" meaning date palm. In the local dialects of Hurmoz and Minab this strait is still called Hurmogh and has the aforementioned meaning.
The resemblance of this word with the name of the Zoroastrian god "Hormoz" (a variant of "Ahura Mazda") has resulted in the popular belief that these words are related.

Jodocus Hondius labels the Strait "Basora fretum" ("Strait of Basra") on his .

To reduce the risk of collision, ships moving through the Strait follow a Traffic Separation Scheme (TSS): inbound ships use one lane, outbound ships another, each lane being two miles wide. The lanes are separated by a two-mile-wide "median".

To traverse the Strait, ships pass through the territorial waters of Iran and Oman under the transit passage provisions of the United Nations Convention on the Law of the Sea.
Although not all countries have ratified the convention, most countries, including the U.S., accept these customary navigation rules as codified in the Convention.

In April 1959 Iran altered the legal status of the strait by expanding its territorial sea to and declaring that it would recognize only transit by innocent passage through the newly expanded area. In July 1972, Oman also expanded its territorial sea to by decree. Thus, by mid-1972, the Strait of Hormuz was completely "closed" by the combined territorial waters of Iran and Oman. During the 1970s, neither Iran or Oman attempted to impede the passage of warships through the strait, but in the 1980s, both countries asserted claims that were different from customary (old) law. Upon ratifying UNCLOS in August 1989, Oman submitted declarations confirming its 1981 royal decree that only innocent passage is permitted through its territorial sea. The declarations further asserted that prior permission was required before foreign warships could pass through Omani territorial waters. Upon signing the convention in December 1982, Iran entered a declaration stating ‚Äúthat only states parties to the Law of the Sea Convention shall be entitled to benefit from the contractual rights created therein‚Äù, including ‚Äúthe right of transit passage through straits used for international navigation‚Äù. In May 1993, Iran enacted a comprehensive law on maritime areas, several provisions of which conflict with UNCLOS provisions, including a requirement that warships, submarines, and nuclear-powered ships obtain permission before exercising innocent passage through Iran's territorial waters.The United States does not recognize any of the claims by Oman and Iran and has contested each of them.

Oman has a radar site Link Quality Indicator (LQI) to monitor the TSS in the Strait of Hormuz. This site is on a small island on the peak of Musandam Governorate.

A 2007 report from the Center for Strategic and International Studies also stated that 17 million barrels passed out of the Persian Gulf daily, but that oil flows through the Strait accounted for roughly 40% of all world-traded oil.

According to the U.S. Energy Information Administration, in 2011, an average of 14 tankers per day passed out of the Persian Gulf through the Strait carrying of crude oil. This was said to represent 35% of the world's seaborne oil shipments and 20% of oil traded worldwide. The report stated that more than 85% of these crude oil exports went to Asian markets, with Japan, India, South Korea and China the largest destinations. In 2018 alone, 21 million barrels a day were passing through the strait - this means $1.17 billion worth of oil a day, at September 2019 prices.

The Tanker War phase of the Iran‚ÄìIraq War started when Iraq attacked the oil terminal and oil tankers at Iran's Kharg Island in early 1984. Saddam Hussein's aim in attacking Iranian shipping was, among other things, to provoke the Iranians to retaliate with extreme measures, such as closing the Strait of Hormuz to all maritime traffic, thereby bringing American intervention. Iran limited the retaliatory attacks to Iraqi shipping, leaving the strait open.

On 18 April 1988, the U.S. Navy waged a one-day battle against Iranian forces in and around the strait. The battle, dubbed Operation Praying Mantis by the U.S., was launched in retaliation for the USS "Samuel B. Roberts" striking a mine laid in the channel by Iran on 14 April. U.S. forces sank one frigate, one gunboat, and up to six armed speedboats, as well as seriously damaging a second frigate.

On 3 July 1988, 290 people were killed when an Iran Air Airbus A300 was shot down over the strait by the United States Navy guided missile cruiser USS "Vincennes" (CG-49) when it was wrongly identified as a jet fighter.

On 8 January 2007, the nuclear submarine USS "Newport News", traveling submerged, struck , a 300,000-ton Japanese-flagged very large crude tanker, south of the strait. There were no injuries, and no oil leaked from the tanker.

A series of naval stand-offs between Iranian speedboats and U.S. warships in the Strait of Hormuz occurred in December 2007 and January 2008. U.S. officials accused Iran of harassing and provoking their naval vessels, but Iranian officials denied the allegations. On 14 January 2008, U.S. Navy officials appeared to contradict the Pentagon version of the 16 January event, in which the Pentagon had reported that U.S. vessels had almost fired on approaching Iranian boats. The Navy's regional commander, Vice Admiral Kevin Cosgriff, said the Iranians had "neither anti-ship missiles nor torpedoes" and he "wouldn't characterize the posture of the US 5th Fleet as afraid of these small boats".

On 29 June 2008, the commander of Iran's Revolutionary Guard, Mohammad Ali Jafari, said that if either Israel or the United States attacked Iran, it would seal off the Strait of Hormuz to wreak havoc in the oil markets. This followed more ambiguous threats from Iran's oil minister and other government officials that an attack on Iran would result in turmoil in the world's oil supply.

Vice Admiral Kevin Cosgriff, commander of the U.S. 5th Fleet stationed in Bahrain across the Persian Gulf from Iran, warned that such Iranian action would be considered an act of war, and the U.S. would not allow Iran to hold hostage nearly a third of the world's oil supply.

On 8 July 2008, Ali Shirazi, a mid-level clerical aide to Iran's Supreme Leader Ayatollah Ali Khamenei, was quoted by the student news agency ISNA as telling the Revolutionary Guards, "The Zionist regime is pressuring White House officials to attack Iran. If they commit such a stupidity, Tel Aviv and U.S. shipping in the Persian Gulf will be Iran's first targets and they will be burned."

In the last week of July 2008, in the Operation Brimstone, dozens of U.S. and foreign naval ships came to undergo joint exercises for possible military activity in the shallow waters off the coast of Iran.

As of 11 August 2008, more than 40 U.S. and allied ships reportedly were en route to the Strait of Hormuz. One U.S. carrier battle group from Japan would complement the two which are already in the Persian Gulf, for a total of five battle groups, not including the submarines.

On 20 March 2009, United States Navy collided with the in the strait. The collision, which slightly injured 15 sailors aboard "Hartford", ruptured a fuel tank aboard "New Orleans", spilling of marine diesel fuel.

On 27 December 2011, Iranian Vice President Mohammad-Reza Rahimi threatened to cut off oil supply from the Strait of Hormuz should economic sanctions limit, or cut off, Iranian oil exports. A U.S. Fifth Fleet spokeswoman said in response that the Fleet was "always ready to counter malevolent actions", whilst Admiral Habibollah Sayyari of the Iranian navy claimed that cutting off oil shipments would be "easy". Despite an initial 2% rise in oil prices, oil markets ultimately did not react significantly to the Iranian threat, with oil analyst Thorbjoern Bak Jensen of Global Risk Management concluding that "they cannot stop the flow for a longer period due to the amount of U.S. hardware in the area".
On 3 January 2012, Iran threatened to take action if the U.S. Navy moves an aircraft carrier back into the Persian Gulf. Iranian Army chief Ataollah Salehi said the United States had moved an aircraft carrier out of the Persian Gulf because of Iran's naval exercises, and Iran would take action if the ship returned. "Iran will not repeat its warning...the enemy's carrier has been moved to the Gulf of Oman because of our drill. I recommend and emphasize to the American carrier not to return to the Persian Gulf", he said.

The U.S. Navy spokesman Commander Bill Speaks quickly responded that deployment of U.S. military assets would continue as has been the custom stating: "The U.S. Navy operates under international maritime conventions to maintain a constant state of high vigilance in order to ensure the continued, safe flow of maritime traffic in waterways critical to global commerce."

While earlier statements from Iran had little effect on global oil markets, coupled with the new sanctions, these comments from Iran are driving crude futures higher, up over 4%. Pressure on prices reflect a combination of uncertainty driven further by China's recent response ‚Äì reducing oil January 2012 purchases from Iran by 50% compared to those made in 2011.

The U.S. led sanctions may be "beginning to bite" as Iranian currency has recently lost some 12% of its value. Further pressure on Iranian currency was added by French Foreign Minister Alain Jupp√© who was quoted as calling for more "strict sanctions" and urged EU countries to follow the US in freezing Iranian central bank assets and imposing an embargo on oil exports.

On 7 January 2012, the British government announced that it would be sending the Type 45 destroyer to the Persian Gulf. "Daring", which is the lead ship of her class is one of the "most advanced warships" in the world, and will undertake its first mission in the Persian Gulf. The British Government however have said that this move has been long-planned, as "Daring" will replace another Armilla patrol frigate.
On 9 January 2012, Iranian Defense Minister Ahmad Vahidi denied that Iran had ever claimed that it would close the Strait of Hormuz, saying that "the Islamic Republic of Iran is the most important provider of security in the strait... if one threatens the security of the Persian Gulf, then all are threatened."

The Iranian Foreign Ministry confirmed on 16 January 2012 that it has received a letter from the United States concerning the Strait of Hormuz, "via three different channels." Authorities were considering whether to reply, although the contents of the letter were not divulged. The United States had previously announced its intention to warn Iran that closing the Strait of Hormuz is a "red line" that would provoke an American response. Gen. Martin E. Dempsey, the chairman of the Joint Chiefs of Staff, said this past weekend that the United States would "take action and re-open the strait,‚Äù which could be accomplished only by military means, including minesweepers, warship escorts and potentially airstrikes. Defense Secretary Leon E. Panetta told troops in Texas that the United States would not tolerate Iran's closing of the strait. Nevertheless, Iran continued to discuss the impact of shutting the Strait on world oil markets, saying that any disruption of supply would cause a shock to markets that "no country" could manage.

By 23 January, a flotilla had been established by countries opposing Iran's threats to close the Hormuz Strait. These ships operated in the Persian Gulf and Arabian Sea off the coast of Iran. The flotilla included three American aircraft carriers (the , the and ) and three destroyers (, , ), seven British warships, including the destroyer and a number of Type 23 frigates (, , and ), and a French warship, the frigate "La Motte-Picquet" .

On 24 January, tensions rose further after the European Union imposed sanctions on Iranian oil. A senior member of Iran's parliament said that the Islamic Republic would close the entry point to the Persian Gulf if new sanctions block its oil exports. "If any disruption happens regarding the sale of Iranian oil, the Strait of Hormuz will definitely be closed," Mohammad Kossari, deputy head of parliament's foreign affairs and national security committee, told the semi-official Fars News Agency.

On April 28, 2015, IRGCN patrol boats contacted the Marshall Islands-flagged container ship "Maersk Tigris", which was westbound through the strait, and directed the ship to proceed further into Iranian territorial waters, according to a spokesman for the U.S. Defense Department. When the ship's master declined, one of the Iranian craft fired shots across the bridge of "Maersk Tigris". The captain complied and proceeded into Iranian waters near Larak Island. The US Navy sent aircraft and a destroyer, USS "Farragut", to monitor the situation.

Maersk says they have agreed to pay an Iranian company $163,000 over a dispute about 10 container boxes transported to Dubai in 2005. The court ruling allegedly ordered a fine of $3.6 million.

In July 2018, Iran again made threats to close the strait. Citing looming American sanctions after the U.S withdrew from the JCPOA deal earlier in the year. Iran's Revolutionary Guards reported they were ready to carry out the action if required.

In August 2018, Iran test-fired a ballistic missile for the first time in 2018. According to the officials, the anti-ship Fateh-110 Mod 3 flew over 100 miles on a flight path over the Strait of Hormuz to a test range in the Iranian desert. ‚ÄúIt was shore-to-shore,‚Äù said one U.S. official describing the launch, who like the others requested anonymity to discuss sensitive information.

On April 22, 2019, the U.S. ended the oil waivers, which allowed some of Iran‚Äôs customers to import Iranian oil, without risking financial penalties as part of the U.S. economic sanctions against Iran. Again, this had implication playing out in the Strait of Hormuz, as Iranian threats of Strait closure was put forward in April 2019. 

Aljazeera quoted Major-General Mohammad Baqeri of the Iranian Armed Forces, stating ‚ÄúWe are not after closing the Strait of Hormuz but if the hostility of the enemies increases, we will be able to do so.‚Äù Baqeri is also quoted for stating ‚ÄúIf our oil does not pass, the oil of others shall not pass the Strait of Hormuz either.‚Äù 

On the morning of June 13, 2019, the oil tankers "Front Altair" and "Kokuka Courageous" were both rocked by explosions shortly before dawn, the crew of the latter reported seeing a flying object strike the ship; the crew were rescued by the destroyer while the crew of the "Front Altair" were rescued by Iranian ships. That afternoon, U.S. secretary of state Mike Pompeo issued a statement accusing Iran of the attacks. Iran subsequently denied the accusations, calling the incident a false-flag attack.

In July 2019, a Stena Bulk Tanker, "Stena Impero", sailing under a British flag, was boarded and captured by Iranian forces. The spokesman for Iran's Guardian Council, Abbas Ali Kadkhodaei, was quoted as describing the seizure as a "reciprocal action." This was presumed to be in reference to the , "Grace 1", bound for Syria in Gibraltar a few days prior.

In 2020, France deployed about 600 troops at sea and in the air under the CTF474 to protect maritime trade, regional business, and to ease local tensions. Since the first week of April 2020, the operation combines the Dutch frigate Ruyter, the French frigate Forbin, and one french airplane ATLANTIC2 (ATL2).

In May 2020, Iran launched missiles at one of their own ships in a friendly fire accident, killing 19 sailors.

Iran have threatened to close of the Strait of Hormuz on multiple occasions, most notably in 2008, 2012, 2018 and 2019. Traditionally, the motivations of the threats have been a response to U.S. provocations, and a number of economic sanctions posed on Iran by the U.S, targeting both the Iranian oil market, as well as other economic sectors.). 

It is widely acknowledged, that even a partial closure of the Strait would wreak havoc on the global oil markets and pose a severe threat to energy security. Additionally, a closing of the Strait would also have severe consequences for Iran itself. Economically, Iran would face consequences in terms of their own dependency on oil revenues and commerce through the Strait, such as medical products and food. In terms of the international opinion, the threat of closing the Strait can severely damage Iran‚Äôs relations to states who are engaging with them economically. If Iran is to block the maritime traffic through the Strait, the violation of international norms and damage to the global economy would likely end up in international support to the U.S. acting against Iran. Iran‚Äôs use of its territorial advantages in the Strait of Hormuz is therefore more effective as a threat, than if a complete or partial closure of the Strait is actually executed.

Iran have a number of options regarding the threats of blocking the Strait of Hormuz. (1) A full closure of the Strait, which is an immense threat to global oil markets, and would likely result in a significant rise in oil prices. (2) Harassment of tanker traffic and damage to the infrastructure, as it was seen in the Iran-Iraq war in the 1980s. Again, this action would be a risk to energy-security, and the steady flow of oil through the strait. (3) At last, Iran can continue their threats of Strait closure as responses to U.S sanctions, or conduct more naval exercises, displaying their naval capabilities. 

Millennium Challenge 2002 was a major war game exercise conducted by the United States armed forces in 2002. According to a 2012 article in The Christian Science Monitor, it simulated an attempt by Iran to close the strait. The assumptions and results were controversial.

A 2008 article in "International Security" contended that Iran could seal off or impede traffic in the Strait for a month, and an attempt by the U.S. to reopen it would be likely to escalate the conflict. In a later issue, however, the journal published a response which questioned some key assumptions and suggested a much shorter timeline for re-opening.

In December 2011, Iran's navy began a ten-day exercise in international waters along the strait. The Iranian Navy Commander, Rear Admiral Habibollah Sayyari, stated that the strait would not be closed during the exercise; Iranian forces could easily accomplish that but such a decision must be made at a political level.

Captain John Kirby, a Pentagon spokesman, was quoted in a December 2011 Reuters article: "Efforts to increase tension in that part of the world are unhelpful and counter-productive. For our part, we are comfortable that we have in the region sufficient capabilities to honor our commitments to our friends and partners, as well as the international community." In the same article, Suzanne Maloney, an Iran expert at the Brookings Institution, said, "The expectation is that the U.S. military could address any Iranian threat relatively quickly."

General Martin Dempsey, Chairman of the Joint Chiefs of Staff, said in January 2012 that Iran "has invested in capabilities that could, in fact, for a period of time block the Strait of Hormuz." He also stated, "We've invested in capabilities to ensure that if that happens, we can defeat that."

In May 2012, a learned article concluded that both the UNCLOS and the 1958 Convention on the High Seas would be violated if Iran followed through on its threat to block passage through the Straits of vessels such as oil tankers, and that the act of passage bears no relation in law to the imposition of economic sanctions. The coastal state is limited in its powers to prevent passage: 1) if threat or actual use of force against its sovereignty, its territorial integrity, or its political independence; or 2) the vessel in any other way violates the principles of international law such as embodied in the Charter of the United Nations.

If Iran were to follow through on its threats to completely or partially close of the Strait, one of the world‚Äôs most strategically important maritime choke points, the most effective way would be through the use of its anti-access/area-denial capabilities. These capabilities are meant to prevent advanced navies or other opponents to operate in the Strait and the Persian Gulf, and would be of particular concern to the U.S. In 2016, it was assessed that Iran‚Äôs military strength was weak, even compared to regional rivals. However, since the 1979 Iranian Revolution, a significant part of Iran‚Äôs military spending has been allocated to the asymmetric warfare approach of its naval capabilities, the anti-access/ anti-denial (A2/AD) systems. The Armed Forces of the Islamic Republic of Iran have a number of these capabilities available, and in short reach of the Strait. Examples of these are as follows: 

‚Ä¢ Coastal air defences, long-range artillery and anti-ship missiles.

‚Ä¢ Kilo-class submarines and midget submarines.

‚Ä¢ A significant fleet of small boats and manpower available, which can be used to manoeuvre around larger vessels in swarming (military) tactics. These small attack crafts can be armed with machine guns, torpedoes and anti-ship missiles. 

‚Ä¢ Naval mine-laying capabilities. Iran possess and produces a variety of naval mines, e.g. bottom-moored contact mines; moored and bottom-influence mines; drifting mines and remotely controlled mines. In 2010, Iran was estimated to have at least 2,000 moored and drifting contact mines from Soviet, Western and Iranian sources.
Iran‚Äôs fleet of small vessels, speedboats and submarines can be used for its rapid and covert mine-laying capabilities.

However, Iran‚Äôs anti-access/area-denial capabilities are filled with operational difficulties, and the use of these tactics would prompt a military response from the U.S. 

The United States Navy and United States Air Force in the Gulf region is far stronger than that of Iran, and while an Iranian attempt to close of the Strait can cause damage, the U.S. is able to defeat it. A key interest to the U.S. in the Persian Gulf, is the free flow of oil and natural gas through the Strait of Hormuz. This is why the U.S relies on a substantial navy and air-force presence, which secures the traffic through the Strait, and are prepared to counter Iranian attempts to blockade it. Most notably, this includes the United States Fifth Fleet based in Bahrain, and the Al Udeid Air Base, housing troops from the United States Air Force 

In June 2012, Saudi Arabia reopened the Iraq Pipeline through Saudi Arabia (IPSA), which was confiscated from Iraq in 2001 and travels from Iraq across Saudi Arabia to a Red Sea port. It will have a capacity of 1.65 million barrels per day.

In July 2012, the UAE began using the new Habshan‚ÄìFujairah oil pipeline from the Habshan fields in Abu Dhabi to the Fujairah oil terminal on the Gulf of Oman, effectively bypassing the Strait of Hormuz. It has a maximum capacity of around 2 million barrels per day, over three-quarters of the UAE's 2012 production rate. The UAE is also increasing Fujairah's storage and off-loading capacities. The UAE is building the world's largest crude oil storage facility in Fujairah with a capacity of holding 14 million barrels to enhance Fujairah's growth as a global oil and trading hub. The Habshan ‚Äì Fujairah route secures the UAE's energy security and has the advantage of being a ground oil pipeline transportation which is considered the cheapest form of oil transportation and also reduces insurance costs as oil tankers would no longer enter the Persian Gulf.

In a July 2012 "Foreign Policy" article, Gal Luft compared Iran and the Strait of Hormuz to the Ottoman Empire and the Dardanelles, a choke point for shipments of Russian grain a century ago. He indicated that tensions involving the Strait of Hormuz are leading those currently dependent on shipments from the Persian Gulf to find alternative shipping capabilities. He stated that Saudi Arabia was considering building new pipelines to Oman and Yemen, and that Iraq might revive the disused Iraq‚ÄìSyria pipeline to ship crude to the Mediterranean. Luft stated that reducing Hormuz traffic "presents the West with a new opportunity to augment its current Iran containment strategy."






</doc>
<doc id="29006" url="https://en.wikipedia.org/wiki?curid=29006" title="Space telescope">
Space telescope

A space telescope or space observatory is a telescope located in outer space to observe distant planets, galaxies and other astronomical objects. Space telescopes avoid the filtering of ultraviolet frequencies, X-rays and gamma rays; the distortion (scintillation) of electromagnetic radiation; as well as light pollution which ground-based observatories encounter.

Suggested by Lyman Spitzer in 1946, the first operational space telescopes were the American Orbiting Astronomical Observatory, OAO-2 launched in 1968, and the Soviet Orion 1 ultraviolet telescope aboard space station Salyut 1 in 1971.

Space telescopes are distinct from Earth observation satellites, that point toward Earth for satellite imaging, applied for espionage, weather analysis and other types of information gathering. Space observatories are divided into two types: Astronomical survey satellites to map the entire sky, and satellites which focus on selected astronomical objects or parts of the sky and beyond.

Wilhelm Beer and Johann Heinrich M√§dler in 1837 discussed the advantages of an observatory on the Moon. In 1946, American theoretical astrophysicist Lyman Spitzer proposed a telescope in space, 11 years before the Soviet Union launched the first satellite, "Sputnik 1". Spitzer's proposal called for a large telescope that would not be hindered by Earth's atmosphere. After lobbying in the 1960s and 70s for such a system to be built, Spitzer's vision ultimately materialized into the Hubble Space Telescope, which was launched on April 24, 1990 by the Space Shuttle "Discovery" (STS-31).

Performing astronomy from ground-based observatories on Earth is limited by the filtering and distortion of electromagnetic radiation (scintillation or twinkling) due to the atmosphere. A telescope orbiting Earth outside the atmosphere is subject neither to twinkling nor to light pollution from artificial light sources on Earth. As a result, the angular resolution of space telescopes is often much higher than a ground-based telescope with a similar aperture. Many larger terrestrial telescopes, however, reduce atmospheric effects with adaptive optics.
Space-based astronomy is more important for frequency ranges which are outside the optical window and the radio window, the only two wavelength ranges of the electromagnetic spectrum that are not severely attenuated by the atmosphere. For example, X-ray astronomy is nearly impossible when done from Earth, and has reached its current importance in astronomy only due to orbiting X-ray telescopes such as the Chandra observatory and the XMM-Newton observatory. Infrared and ultraviolet are also largely blocked.

Space telescopes are much more expensive to build than ground-based telescopes. Due to their location, space telescopes are also extremely difficult to maintain. The Hubble Space Telescope was serviced by the Space Shuttle, but most space telescopes cannot be serviced at all.

Satellites have been launched and operated by NASA, ISRO, ESA, CNSA, JAXA and the Soviet space program later succeeded by Roscosmos of Russia. As of 2018, many space observatories have already completed their missions, while others continue operating on extended time. However, the future availability of space telescopes and observatories depends on timely and sufficient funding. While future space observatories are planned by NASA, JAXA and the CNSA, scientists fear that there would be gaps in coverage that would not be covered immediately by future projects and this would affect research in fundamental science.




</doc>
<doc id="29007" url="https://en.wikipedia.org/wiki?curid=29007" title="Saint David">
Saint David

Saint David (; ; ) was a Welsh bishop of Mynyw (now St Davids) during the 6th century. He is the patron saint of Wales. David was a native of Wales, and a relatively large amount of information is known about his life. His birth date, however, is uncertain: suggestions range from 462 to 512. He is traditionally believed to be the son of Saint Non and the grandson of Ceredig ap Cunedda, king of Ceredigion. The Welsh annals placed his death 569 years after the birth of Christ, but Phillimore's dating revised this to 601.

Many of the traditional tales about David are found in the "Buchedd Dewi" ("Life of David"), a hagiography written by Rhygyfarch in the late 11th century. Rhygyfarch claimed it was based on documents found in the cathedral archives. Modern historians are sceptical of some of its claims: one of Rhygyfarch's aims was to establish some independence for the Welsh church, which had refused the Roman rite until the 8th century and now sought a metropolitan status equal to that of Canterbury. (This may apply to the supposed pilgrimage to Jerusalem where he is said to have been anointed as an archbishop by the patriarch).

The tradition that he was born at Henfynyw (Vetus-Menevia) in Ceredigion is not improbable. He became renowned as a teacher and preacher, founding monastic settlements and churches in Wales, Dumnonia, and Brittany. St David's Cathedral stands on the site of the monastery he founded in the Glyn Rhosyn valley of Pembrokeshire. Around 550, he attended the Synod of Brefi, where his eloquence in opposing Pelagianism caused his fellow monks to elect him primate of the region. As such he presided over the synod of Caerleon (the "Synod of Victory") around 569.

His best-known miracle is said to have taken place when he was preaching in the middle of a large crowd at the Synod of Brefi: the village of Llanddewi Brefi stands on the spot where the ground on which he stood is reputed to have risen up to form a small hill. A white dove, which became his emblem, was seen settling on his shoulder. John Davies notes that one can scarcely "conceive of any miracle more superfluous" in that part of Wales than the creation of a new hill. David is said to have denounced Pelagianism during this incident and he was declared archbishop by popular acclaim according to Rhygyfarch, bringing about the retirement of Dubricius. St David's metropolitan status as an archbishopric was later supported by Bernard, Bishop of St David's, Geoffrey of Monmouth and Gerald of Wales.

The Monastic Rule of David prescribed that monks had to pull the plough themselves without draught animals, and must drink only water and eat only bread with salt and herbs. The monks spent their evenings in prayer, reading and writing. No personal possessions were allowed: even to say "my book" was considered an offence. He lived a simple life and practised asceticism, teaching his followers to refrain from eating meat and drinking beer. His symbol, also the symbol of Wales, is the leek (this inspires a reference in Shakespeare's Henry V, Act V scene 1) :

"Fluellen: "If your Majesty is remembered of it, the Welshmen did good service in a garden where leeks did grow, wearing leeks in their Monmouth caps, which your Majesty knows, to this hour is an honourable badge of the service, and I do believe, your Majesty takes no scorn to wear the leek upon Saint Tavy's day". King Henry: "I wear it for a memorable honour; for I am Welsh, you know, good countryman"."

Rhigyfarch counted Glastonbury Abbey among the churches David founded. Around forty years later William of Malmesbury, believing the Abbey older, said that David visited Glastonbury only to rededicate the Abbey and to donate a travelling altar including a great sapphire. He had had a vision of Jesus who said that "the church had been dedicated long ago by Himself in honour of His Mother, and it was not seemly that it should be re-dedicated by human hands". So David instead commissioned an extension to be built to the abbey, east of the Old Church. (The dimensions of this extension given by William were verified archaeologically in 1921). One manuscript indicates that a sapphire altar was among the items Henry VIII of England confiscated from the abbey during the Dissolution of the Monasteries a thousand years later.

Though the exact date of his death is not certain, tradition holds that it was on 1 March, which is the date now marked as Saint David's Day. The two most common years given for his death are 601 and 589. The monastery is said to have been "filled with angels as Christ received his soul". His last words to his followers were in a sermon on the previous Sunday. The Welsh Life of St David gives these as, "Arglwyddi, brodyr, a chwiorydd, Byddwch lawen a chadwch eich ffyd a'ch credd, a gwnewch y petheu bychain a glywsoch ac y welsoch gennyf i. A mwynhau a gerdaf y fford yd aeth an tadeu idi", which translates as, "Lords, brothers and sisters, Be joyful, and keep your faith and your creed, and do the little things that you have seen me do and heard about. And as for me, I will walk the path that our fathers have trod before us." "Do ye the little things in life" ("Gwnewch y pethau bychain mewn bywyd") is today a very well known phrase in Welsh. The same passage states that he died on a Tuesday, from which attempts have been made to calculate the year of his death.

David was buried at St David's Cathedral at St Davids, Pembrokeshire, where his shrine was a popular place of pilgrimage throughout the Middle Ages. During the 10th and 11th centuries the Cathedral was regularly raided by Vikings, who removed the shrine from the church and stripped off the precious metal adornments. In 1275 a new shrine was constructed, the ruined base of which remains to this day (see photo), which was originally surmounted by an ornamental wooden canopy with murals of David, Patrick and Denis. The relics of David and Justinian of Ramsey Island were kept in a portable casket on the stone base of the shrine. It was at this shrine that Edward I came to pray in 1284. During the reformation Bishop Barlow (1536‚Äì48), a staunch Protestant, stripped the shrine of its jewels and confiscated the relics of David and Justinian.

David was officially recognised at the Holy See by Pope Callixtus II in 1120, thanks to the work of Bernard, Bishop of St David's. Music for his Liturgy of the Hours has been edited by O. T. Edwards in "Matins, Lauds and Vespers for St David's Day: the Medieval Office of the Welsh Patron Saint in National Library of Wales MS 20541 E" (Cambridge, 1990). David was also canonized by the Eastern Orthodox Church at an unknown date.

Over 50 churches in South Wales were dedicated to him in pre-Reformation days.

In the 2004 edition of the Roman Martyrology, David is listed under 1 March with the Latin name "D√°vus". He is recognised as bishop of Menevia in Wales who governed his monastery following the example of the Eastern Fathers. Through his leadership, many monks went forth to evangelise Wales, Ireland, Cornwall and Armorica (Brittany and surrounding provinces).

The restored Shrine of Saint David was unveiled and rededicated by the Right Reverend Wyn Evans, Bishop of St David's, at a Choral Eucharist on Saint David's Day, 2012.

A broadside ballad published around 1630 claimed that the Welsh wore a leek in their hats to commemorate a battle fought on St David's Day. So as to recognise friend from foe, the Welsh had pulled up leeks from a garden and put them in their hats, before going on to win the battle.

Saint David is usually represented standing on a hill with a dove on his shoulder.

David's popularity in Wales is shown by the "Armes Prydein" of around 930, a popular poem which prophesied that in the future, when all might seem lost, the "Cymry" (Welsh people) would unite behind the standard of David to defeat the English; "A lluman gl√¢n Dewi a ddyrchafant" ("And they will raise the pure banner of Dewi").

David is said to have played a role in spreading Christianity on the continent, inspiring numerous place names in Brittany including Saint-Divy, Saint-Yvi and Landivy.

David's life and teachings have inspired a choral work by Welsh composer Karl Jenkins, "Dewi Sant". It is a seven-movement work best known for the classical crossover series Adiemus, which intersperses movements reflecting the themes of David's last sermon with those drawing from three Psalms. An oratorio by another Welsh composer Arwel Hughes, also entitled "Dewi Sant", was composed in 1950.

Saint David is also thought to be associated with corpse candles, lights that would warn of the imminent death of a member of the community. The story goes that David prayed for his people to have some warning of their death, so that they could prepare themselves. In a vision, David's wish was granted and told that from then on, people who lived in the land of Dewi Sant (Saint David) "would be forewarned by the dim light of mysterious tapers when and where the death might be expected". The colour and size of the tapers indicated whether the person to die would be a woman, man, or child.






</doc>
