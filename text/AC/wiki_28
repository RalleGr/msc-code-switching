<doc id="26743" url="https://en.wikipedia.org/wiki?curid=26743" title="Sigmund Freud">
Sigmund Freud

Sigmund Freud ( ; ; born Sigismund Schlomo Freud; 6 May 1856 – 23 September 1939) was an Austrian neurologist and the founder of psychoanalysis, a clinical method for treating psychopathology through dialogue between a patient and a psychoanalyst.

Freud was born to Galician Jewish parents in the Moravian town of Freiberg, in the Austrian Empire. He qualified as a doctor of medicine in 1881 at the University of Vienna. Upon completing his habilitation in 1885, he was appointed a docent in neuropathology and became an affiliated professor in 1902. Freud lived and worked in Vienna, having set up his clinical practice there in 1886. In 1938, Freud left Austria to escape Nazi persecution. He died in exile in the United Kingdom in 1939.

In founding psychoanalysis, Freud developed therapeutic techniques such as the use of free association and discovered transference, establishing its central role in the analytic process. Freud's redefinition of sexuality to include its infantile forms led him to formulate the Oedipus complex as the central tenet of psychoanalytical theory. His analysis of dreams as wish-fulfillments provided him with models for the clinical analysis of symptom formation and the underlying mechanisms of repression. On this basis Freud elaborated his theory of the unconscious and went on to develop a model of psychic structure comprising id, ego and super-ego. Freud postulated the existence of libido, a sexualised energy with which mental processes and structures are invested and which generates erotic attachments, and a death drive, the source of compulsive repetition, hate, aggression and neurotic guilt. In his later works, Freud developed a wide-ranging interpretation and critique of religion and culture.

Though in overall decline as a diagnostic and clinical practice, psychoanalysis remains influential within psychology, psychiatry, and psychotherapy, and across the humanities. It thus continues to generate extensive and highly contested debate with regard to its therapeutic efficacy, its scientific status, and whether it advances or is detrimental to the feminist cause. Nonetheless, Freud's work has suffused contemporary Western thought and popular culture. 1940 poetic tribute to Freud describes him as having created "a whole climate of opinion / under whom we conduct our different lives."

Freud was born to Jewish parents in the Moravian town of Freiberg, in the Austrian Empire (later Příbor, Czech Republic), the first of eight children. Both of his parents were from Galicia, a province straddling modern-day West Ukraine and Poland. His father, Jakob Freud (1815–1896), a wool merchant, had two sons, Emanuel (1833–1914) and Philipp (1836–1911), by his first marriage. Jakob's family were Hasidic Jews, and although Jakob himself had moved away from the tradition, he came to be known for his Torah study. He and Freud's mother, Amalia Nathansohn, who was 20 years younger and his third wife, were married by Rabbi Isaac Noah Mannheimer on 29 July 1855. They were struggling financially and living in a rented room, in a locksmith's house at Schlossergasse 117 when their son Sigmund was born. He was born with a caul, which his mother saw as a positive omen for the boy's future.

In 1859, the Freud family left Freiberg. Freud's half brothers emigrated to Manchester, England, parting him from the "inseparable" playmate of his early childhood, Emanuel's son, John. Jakob Freud took his wife and two children (Freud's sister, Anna, was born in 1858; a brother, Julius born in 1857, had died in infancy) firstly to Leipzig and then in 1860 to Vienna where four sisters and a brother were born: Rosa (b. 1860), Marie (b. 1861), Adolfine (b. 1862), Paula (b. 1864), Alexander (b. 1866). In 1865, the nine-year-old Freud entered the "Leopoldstädter Kommunal-Realgymnasium", a prominent high school. He proved to be an outstanding pupil and graduated from the Matura in 1873 with honors. He loved literature and was proficient in German, French, Italian, Spanish, English, Hebrew, Latin and Greek.

Freud entered the University of Vienna at age 17. He had planned to study law, but joined the medical faculty at the university, where his studies included philosophy under Franz Brentano, physiology under Ernst Brücke, and zoology under Darwinist professor Carl Claus. In 1876, Freud spent four weeks at Claus's zoological research station in Trieste, dissecting hundreds of eels in an inconclusive search for their male reproductive organs. In 1877 Freud moved to Ernst Brücke's physiology laboratory where he spent six years comparing the brains of humans and other vertebrates with those of frogs and invertebrates such as crayfish and lampreys. His research work on the biology of nervous tissue proved seminal for the subsequent discovery of the neuron in the 1890s. Freud's research work was interrupted in 1879 by the obligation to undertake a year's compulsory military service. The lengthy downtimes enabled him to complete a commission to translate four essays from John Stuart Mill's collected works. He graduated with an MD in March 1881.

In 1882, Freud began his medical career at the Vienna General Hospital. His research work in cerebral anatomy led to the publication of an influential paper on the palliative effects of cocaine in 1884 and his work on aphasia would form the basis of his first book "On the Aphasias: a Critical Study", published in 1891. Over a three-year period, Freud worked in various departments of the hospital. His time spent in Theodor Meynert's psychiatric clinic and as a locum in a local asylum led to an increased interest in clinical work. His substantial body of published research led to his appointment as a university lecturer or docent in neuropathology in 1885, a non-salaried post but one which entitled him to give lectures at the University of Vienna.

In 1886, Freud resigned his hospital post and entered private practice specializing in "nervous disorders". The same year he married Martha Bernays, the granddaughter of Isaac Bernays, a chief rabbi in Hamburg. They had six children: Mathilde (b. 1887), Jean-Martin (b. 1889), Oliver (b. 1891), Ernst (b. 1892), Sophie (b. 1893), and Anna (b. 1895). From 1891 until they left Vienna in 1938, Freud and his family lived in an apartment at Berggasse 19, near Innere Stadt, a historical district of Vienna.
In 1896, Minna Bernays, Martha Freud's sister, became a permanent member of the Freud household after the death of her fiancé. The close relationship she formed with Freud led to rumours, started by Carl Jung, of an affair. The discovery of a Swiss hotel guest-book entry for 13 August 1898, signed by Freud whilst travelling with his sister-in-law, has been presented as evidence of the affair.

Freud began smoking tobacco at age 24; initially a cigarette smoker, he became a cigar smoker. He believed smoking enhanced his capacity to work and that he could exercise self-control in moderating it. Despite health warnings from colleague Wilhelm Fliess, he remained a smoker, eventually suffering a buccal cancer. Freud suggested to Fliess in 1897 that addictions, including that to tobacco, were substitutes for masturbation, "the one great habit."

Freud had greatly admired his philosophy tutor, Brentano, who was known for his theories of perception and introspection. Brentano discussed the possible existence of the unconscious mind in his "Psychology from an Empirical Standpoint" (1874). Although Brentano denied its existence, his discussion of the unconscious probably helped introduce Freud to the concept. Freud owned and made use of Charles Darwin's major evolutionary writings, and was also influenced by Eduard von Hartmann's "The Philosophy of the Unconscious" (1869). Other texts of importance to Freud were by Fechner and Herbart with the latter's "Psychology as Science" arguably considered to be of underrated significance in this respect. Freud also drew on the work of Theodor Lipps who was one of the main contemporary theorists of the concepts of the unconscious and empathy.

Though Freud was reluctant to associate his psychoanalytic insights with prior philosophical theories, attention has been drawn to analogies between his work and that of both Schopenhauer and Nietzsche, both of whom he claimed not to have read until late in life. One historian concluded, based on Freud's correspondence with his adolescent friend Eduard Silberstein, that Freud read Nietzsche's "The Birth of Tragedy" and the first two of the "Untimely Meditations" when he was seventeen. In 1900, the year of Nietzsche's death, Freud bought his collected works; he told his friend, Fliess, that he hoped to find in Nietzsche's works "the words for much that remains mute in me." Later, he said he had not yet opened them. Freud came to treat Nietzsche's writings "as texts to be resisted far more than to be studied." His interest in philosophy declined after he had decided on a career in neurology.

Freud read William Shakespeare in English throughout his life, and it has been suggested that his understanding of human psychology may have been partially derived from Shakespeare's plays.

Freud's Jewish origins and his allegiance to his secular Jewish identity were of significant influence in the formation of his intellectual and moral outlook, especially with respect to his intellectual non-conformism, as he was the first to point out in his "Autobiographical Study". They would also have a substantial effect on the content of psychoanalytic ideas, particularly in respect of their common concerns with depth interpretation and "the bounding of desire by law".

In October 1885, Freud went to Paris on a three-month fellowship to study with Jean-Martin Charcot, a renowned neurologist who was conducting scientific research into hypnosis. He was later to recall the experience of this stay as catalytic in turning him toward the practice of medical psychopathology and away from a less financially promising career in neurology research. Charcot specialized in the study of hysteria and susceptibility to hypnosis, which he frequently demonstrated with patients on stage in front of an audience.

Once he had set up in private practice back in Vienna in 1886, Freud began using hypnosis in his clinical work. He adopted the approach of his friend and collaborator, Josef Breuer, in a type of hypnosis which was different from the French methods he had studied, in that it did not use suggestion. The treatment of one particular patient of Breuer's proved to be transformative for Freud's clinical practice. Described as Anna O., she was invited to talk about her symptoms while under hypnosis (she would coin the phrase "talking cure" for her treatment). In the course of talking in this way, her symptoms became reduced in severity as she retrieved memories of traumatic incidents associated with their onset.

The inconsistent results of Freud's early clinical work eventually led him to abandon hypnosis, having concluded that more consistent and effective symptom relief could be achieved by encouraging patients to talk freely, without censorship or inhibition, about whatever ideas or memories occurred to them. In conjunction with this procedure, which he called "free association", Freud found that patients' dreams could be fruitfully analyzed to reveal the complex structuring of unconscious material and to demonstrate the psychic action of repression which, he had concluded, underlay symptom formation. By 1896 he was using the term "psychoanalysis" to refer to his new clinical method and the theories on which it was based.

Freud's development of these new theories took place during a period in which he experienced heart irregularities, disturbing dreams and periods of depression, a "neurasthenia" which he linked to the death of his father in 1896 and which prompted a "self-analysis" of his own dreams and memories of childhood. His explorations of his feelings of hostility to his father and rivalrous jealousy over his mother's affections led him to fundamentally revise his theory of the origin of the neuroses.

On the basis of his early clinical work, Freud had postulated that unconscious memories of sexual molestation in early childhood were a necessary precondition for the psychoneuroses (hysteria and obsessional neurosis), a formulation now known as Freud's seduction theory. In the light of his self-analysis, Freud abandoned the theory that every neurosis can be traced back to the effects of infantile sexual abuse, now arguing that infantile sexual scenarios still had a causative function, but it did not matter whether they were real or imagined and that in either case they became pathogenic only when acting as repressed memories.

This transition from the theory of infantile sexual trauma as a general explanation of how all neuroses originate to one that presupposes an autonomous infantile sexuality provided the basis for Freud's subsequent formulation of the theory of the Oedipus complex.

Freud described the evolution of his clinical method and set out his theory of the psychogenetic origins of hysteria, demonstrated in a number of case histories, in "Studies on Hysteria" published in 1895 (co-authored with Josef Breuer). In 1899 he published "The Interpretation of Dreams" in which, following a critical review of existing theory, Freud gives detailed interpretations of his own and his patients' dreams in terms of wish-fulfillments made subject to the repression and censorship of the "dream work". He then sets out the theoretical model of mental structure (the unconscious, pre-conscious and conscious) on which this account is based. An abridged version, "On Dreams", was published in 1901. In works which would win him a more general readership, Freud applied his theories outside the clinical setting in "The Psychopathology of Everyday Life" (1901) and "Jokes and their Relation to the Unconscious" (1905). In "Three Essays on the Theory of Sexuality", published in 1905, Freud elaborates his theory of infantile sexuality, describing its "polymorphous perverse" forms and the functioning of the "drives", to which it gives rise, in the formation of sexual identity. The same year he published "Fragment of an Analysis of a Case of Hysteria", which became one of his more famous and controversial case studies.

During this formative period of his work, Freud valued and came to rely on the intellectual and emotional support of his friend Wilhelm Fliess, a Berlin-based ear, nose and throat specialist whom he had first met 1887. Both men saw themselves as isolated from the prevailing clinical and theoretical mainstream because of their ambitions to develop radical new theories of sexuality. Fliess developed highly eccentric theories of human biorhythms and a nasogenital connection which are today considered pseudoscientific. He shared Freud's views on the importance of certain aspects of sexuality – masturbation, coitus interruptus, and the use of condoms – in the etiology of what were then called the "actual neuroses," primarily neurasthenia and certain physically manifested anxiety symptoms. They maintained an extensive correspondence from which Freud drew on Fliess's speculations on infantile sexuality and bisexuality to elaborate and revise his own ideas. His first attempt at a systematic theory of the mind, his "Project for a Scientific Psychology" was developed as a metapsychology with Fliess as interlocutor. However, Freud's efforts to build a bridge between neurology and psychology were eventually abandoned after they had reached an impasse, as his letters to Fliess reveal, though some ideas of the "Project" were to be taken up again in the concluding chapter of "The Interpretation of Dreams".

Freud had Fliess repeatedly operate on his nose and sinuses to treat "nasal reflex neurosis", and subsequently referred his patient Emma Eckstein to him. According to Freud, her history of symptoms included severe leg pains with consequent restricted mobility, as well as stomach and menstrual pains. These pains were, according to Fliess's theories, caused by habitual masturbation which, as the tissue of the nose and genitalia were linked, was curable by removal of part of the middle turbinate. Fliess's surgery proved disastrous, resulting in profuse, recurrent nasal bleeding – he had left a half-metre of gauze in Eckstein's nasal cavity – the subsequent removal of which left her permanently disfigured. At first, though aware of Fliess's culpability – Freud fled from the remedial surgery in horror – he could only bring himself to delicately intimate in his correspondence to Fliess the nature of his disastrous role and in subsequent letters maintained a tactful silence on the matter or else returned to the face-saving topic of Eckstein's hysteria. Freud ultimately, in light of Eckstein's history of adolescent self-cutting and irregular nasal (and menstrual) bleeding, concluded that Fliess was "completely without blame", as Eckstein's post-operative haemorrhages were hysterical "wish-bleedings" linked to "an old wish to be loved in her illness" and triggered as a means of "rearousing [Freud's] affection". Eckstein nonetheless continued her analysis with Freud. She was restored to full mobility and went on to practice psychoanalysis herself.

Freud, who had called Fliess "the Kepler of biology", later concluded that a combination of a homoerotic attachment and the residue of his "specifically Jewish mysticism" lay behind his loyalty to his Jewish friend and his consequent over-estimation of both his theoretical and clinical work. Their friendship came to an acrimonious end with Fliess angry at Freud's unwillingness to endorse his general theory of sexual periodicity and accusing him of collusion in the plagiarism of his work. After Fliess failed to respond to Freud's offer of collaboration over publication of his "Three Essays on the Theory of Sexuality" in 1906, their relationship came to an end.

In 1902, Freud at last realised his long-standing ambition to be made a university professor. The title "professor extraordinarius" was important to Freud for the recognition and prestige it conferred, there being no salary or teaching duties attached to the post (he would be granted the enhanced status of "professor ordinarius" in 1920). Despite support from the university, his appointment had been blocked in successive years by the political authorities and it was secured only with the intervention of one of his more influential ex-patients, a Baroness Marie Ferstel, who (supposedly) had to bribe the minister of education with a valuable painting.

With his prestige thus enhanced, Freud continued with the regular series of lectures on his work which, since the mid-1880s as a docent of Vienna University, he had been delivering to small audiences every Saturday evening at the lecture hall of the university's psychiatric clinic.

From the autumn of 1902, a number of Viennese physicians who had expressed interest in Freud's work were invited to meet at his apartment every Wednesday afternoon to discuss issues relating to psychology and neuropathology. This group was called the Wednesday Psychological Society ("Psychologische Mittwochs-Gesellschaft") and it marked the beginnings of the worldwide psychoanalytic movement.

Freud founded this discussion group at the suggestion of the physician Wilhelm Stekel. Stekel had studied medicine at the University of Vienna under Richard von Krafft-Ebing. His conversion to psychoanalysis is variously attributed to his successful treatment by Freud for a sexual problem or as a result of his reading "The Interpretation of Dreams", to which he subsequently gave a positive review in the Viennese daily newspaper "Neues Wiener Tagblatt".

The other three original members whom Freud invited to attend, Alfred Adler, Max Kahane, and Rudolf Reitler, were also physicians and all five were Jewish by birth. Both Kahane and Reitler were childhood friends of Freud. Kahane had attended the same secondary school and both he and Reitler went to university with Freud. They had kept abreast of Freud's developing ideas through their attendance at his Saturday evening lectures. In 1901, Kahane, who first introduced Stekel to Freud's work, had opened an out-patient psychotherapy institute of which he was the director in Bauernmarkt, in Vienna. In the same year, his medical textbook, "Outline of Internal Medicine for Students and Practicing Physicians," was published. In it, he provided an outline of Freud's psychoanalytic method. Kahane broke with Freud and left the Wednesday Psychological Society in 1907 for unknown reasons and in 1923 committed suicide. Reitler was the director of an establishment providing thermal cures in Dorotheergasse which had been founded in 1901. He died prematurely in 1917. Adler, regarded as the most formidable intellect among the early Freud circle, was a socialist who in 1898 had written a health manual for the tailoring trade. He was particularly interested in the potential social impact of psychiatry.

Max Graf, a Viennese musicologist and father of "Little Hans", who had first encountered Freud in 1900 and joined the Wednesday group soon after its initial inception, described the ritual and atmosphere of the early meetings of the society:

The gatherings followed a definite ritual. First one of the members would present a paper. Then, black coffee and cakes were served; cigar and cigarettes were on the table and were consumed in great quantities. After a social quarter of an hour, the discussion would begin. The last and decisive word was always spoken by Freud himself. There was the atmosphere of the foundation of a religion in that room. Freud himself was its new prophet who made the heretofore prevailing methods of psychological investigation appear superficial.
By 1906, the group had grown to sixteen members, including Otto Rank, who was employed as the group's paid secretary. In the same year, Freud began a correspondence with Carl Gustav Jung who was by then already an academically acclaimed researcher into word-association and the Galvanic Skin Response, and a lecturer at Zurich University, although still only an assistant to Eugen Bleuler at the Burghölzli Mental Hospital in Zürich. In March 1907, Jung and Ludwig Binswanger, also a Swiss psychiatrist, travelled to Vienna to visit Freud and attend the discussion group. Thereafter, they established a small psychoanalytic group in Zürich. In 1908, reflecting its growing institutional status, the Wednesday group was reconstituted as the Vienna Psychoanalytic Society with Freud as president, a position he relinquished in 1910 in favor of Adler in the hope of neutralizing his increasingly critical standpoint.

The first woman member, Margarete Hilferding, joined the Society in 1910 and the following year she was joined by Tatiana Rosenthal and Sabina Spielrein who were both Russian psychiatrists and graduates of the Zürich University medical school. Prior to the completion of her studies, Spielrein had been a patient of Jung at the Burghölzli and the clinical and personal details of their relationship became the subject of an extensive correspondence between Freud and Jung. Both women would go on to make important contributions to the work of the Russian Psychoanalytic Society founded in 1910.

Freud's early followers met together formally for the first time at the Hotel Bristol, Salzburg on 27 April 1908. This meeting, which was retrospectively deemed to be the first International Psychoanalytic Congress, was convened at the suggestion of Ernest Jones, then a London-based neurologist who had discovered Freud's writings and begun applying psychoanalytic methods in his clinical work. Jones had met Jung at a conference the previous year and they met up again in Zürich to organize the Congress. There were, as Jones records, "forty-two present, half of whom were or became practicing analysts." In addition to Jones and the Viennese and Zürich contingents accompanying Freud and Jung, also present and notable for their subsequent importance in the psychoanalytic movement were Karl Abraham and Max Eitingon from Berlin, Sándor Ferenczi from Budapest and the New York-based Abraham Brill.

Important decisions were taken at the Congress with a view to advancing the impact of Freud's work. A journal, the "Jahrbuch für psychoanalytische und psychopathologishe Forschungen", was launched in 1909 under the editorship of Jung. This was followed in 1910 by the monthly "Zentralblatt für Psychoanalyse" edited by Adler and Stekel, in 1911 by "Imago", a journal devoted to the application of psychoanalysis to the field of cultural and literary studies edited by Rank and in 1913 by the "Internationale Zeitschrift für Psychoanalyse", also edited by Rank. Plans for an international association of psychoanalysts were put in place and these were implemented at the Nuremberg Congress of 1910 where Jung was elected, with Freud's support, as its first president.

Freud turned to Brill and Jones to further his ambition to spread the psychoanalytic cause in the English-speaking world. Both were invited to Vienna following the Salzburg Congress and a division of labour was agreed with Brill given the translation rights for Freud's works, and Jones, who was to take up a post at the University of Toronto later in the year, tasked with establishing a platform for Freudian ideas in North American academic and medical life. Jones's advocacy prepared the way for Freud's visit to the United States, accompanied by Jung and Ferenczi, in September 1909 at the invitation of Stanley Hall, president of Clark University, Worcester, Massachusetts, where he gave five lectures on psychoanalysis.

The event, at which Freud was awarded an Honorary Doctorate, marked the first public recognition of Freud's work and attracted widespread media interest. Freud's audience included the distinguished neurologist and psychiatrist James Jackson Putnam, Professor of Diseases of the Nervous System at Harvard, who invited Freud to his country retreat where they held extensive discussions over a period of four days. Putnam's subsequent public endorsement of Freud's work represented a significant breakthrough for the psychoanalytic cause in the United States. When Putnam and Jones organised the founding of the American Psychoanalytic Association in May 1911 they were elected president and secretary respectively. Brill founded the New York Psychoanalytic Society the same year. His English translations of Freud's work began to appear from 1909.

Some of Freud's followers subsequently withdrew from the International Psychoanalytical Association (IPA) and founded their own schools.

From 1909, Adler's views on topics such as neurosis began to differ markedly from those held by Freud. As Adler's position appeared increasingly incompatible with Freudianism, a series of confrontations between their respective viewpoints took place at the meetings of the Viennese Psychoanalytic Society in January and February 1911. In February 1911, Adler, then the president of the society, resigned his position. At this time, Stekel also resigned his position as vice president of the society. Adler finally left the Freudian group altogether in June 1911 to found his own organization with nine other members who had also resigned from the group. This new formation was initially called "Society for Free Psychoanalysis" but it was soon renamed the "Society for Individual Psychology". In the period after World War I, Adler became increasingly associated with a psychological position he devised called individual psychology.

In 1912, Jung published "Wandlungen und Symbole der Libido" (published in English in 1916 as "Psychology of the Unconscious") making it clear that his views were taking a direction quite different from those of Freud. To distinguish his system from psychoanalysis, Jung called it analytical psychology. Anticipating the final breakdown of the relationship between Freud and Jung, Ernest Jones initiated the formation of a Secret Committee of loyalists charged with safeguarding the theoretical coherence and institutional legacy of the psychoanalytic movement. Formed in the autumn of 1912, the Committee comprised Freud, Jones, Abraham, Ferenczi, Rank, and Hanns Sachs. Max Eitingon joined the committee in 1919. Each member pledged himself not to make any public departure from the fundamental tenets of psychoanalytic theory before he had discussed his views with the others. After this development, Jung recognised that his position was untenable and resigned as editor of the "Jarhbuch" and then as president of the IPA in April 1914. The Zürich Society withdrew from the IPA the following July.

Later the same year, Freud published a paper entitled "", the German original being first published in the "Jahrbuch", giving his view on the birth and evolution of the psychoanalytic movement and the withdrawal of Adler and Jung from it.

The final defection from Freud's inner circle occurred following the publication in 1924 of Rank's "The Trauma of Birth" which other members of the committee read as, in effect, abandoning the Oedipus Complex as the central tenet of psychoanalytic theory. Abraham and Jones became increasingly forceful critics of Rank and though he and Freud were reluctant to end their close and long-standing relationship the break finally came in 1926 when Rank resigned from his official posts in the IPA and left Vienna for Paris. His place on the Committee was taken by Anna Freud. Rank eventually settled in the United States where his revisions of Freudian theory were to influence a new generation of therapists uncomfortable with the orthodoxies of the IPA.

After the founding of the IPA in 1910, an international network of psychoanalytical societies, training institutes and clinics became well established and a regular schedule of biannual Congresses commenced after the end of World War I to coordinate their activities.

Abraham and Eitingon founded the Berlin Psychoanalytic Society in 1910 and then the Berlin Psychoanalytic Institute and the Poliklinik in 1920. The Poliklinik's innovations of free treatment, and child analysis and the Berlin Institute's standardisation of psychoanalytic training had a major influence on the wider psychoanalytic movement. In 1927 Ernst Simmel founded the Schloss Tegel Sanatorium on the outskirts of Berlin, the first such establishment to provide psychoanalytic treatment in an institutional framework. Freud organised a fund to help finance its activities and his architect son, Ernst, was commissioned to refurbish the building. It was forced to close in 1931 for economic reasons.

The 1910 Moscow Psychoanalytic Society became the Russian Psychoanalytic Society and Institute in 1922. Freud's Russian followers were the first to benefit from translations of his work, the 1904 Russian translation of "The Interpretation of Dreams" appearing nine years before Brill's English edition. The Russian Institute was unique in receiving state support for its activities, including publication of translations of Freud's works. Support was abruptly annulled in 1924, when Joseph Stalin came to power, after which psychoanalysis was denounced on ideological grounds.

After helping found the American Psychoanalytic Association in 1911, Ernest Jones returned to Britain from Canada in 1913 and founded the London Psychoanalytic Society the same year. In 1919, he dissolved this organisation and, with its core membership purged of Jungian adherents, founded the British Psychoanalytical Society, serving as its president until 1944. The Institute of Psychoanalysis was established 1924 and the London Clinic of Psychoanalysis established in 1926, both under Jones's directorship.

The Vienna Ambulatorium (Clinic) was established in 1922 and the Vienna Psychoanalytic Institute was founded in 1924 under the directorship of Helene Deutsch. Ferenczi founded the Budapest Psychoanalytic Institute in 1913 and a clinic in 1929.

Psychoanalytic societies and institutes were established in Switzerland (1919), France (1926), Italy (1932), the Netherlands (1933), Norway (1933) and in Palestine (Jerusalem, 1933) by Eitingon, who had fled Berlin after Adolf Hitler came to power. The New York Psychoanalytic Institute was founded in 1931.

The 1922 Berlin Congress was the last Freud attended. By this time his speech had become seriously impaired by the prosthetic device he needed as a result of a series of operations on his cancerous jaw. He kept abreast of developments through a regular correspondence with his principal followers and via the circular letters and meetings of the Secret Committee which he continued to attend.

The Committee continued to function until 1927 by which time institutional developments within the IPA, such as the establishment of the International Training Commission, had addressed concerns about the transmission of psychoanalytic theory and practice. There remained, however, significant differences over the issue of lay analysis – i.e. the acceptance of non-medically qualified candidates for psychoanalytic training. Freud set out his case in favour in 1926 in his "The Question of Lay Analysis". He was resolutely opposed by the American societies who expressed concerns over professional standards and the risk of litigation (though child analysts were made exempt). These concerns were also shared by some of his European colleagues. Eventually an agreement was reached allowing societies autonomy in setting criteria for candidature.

In 1930 Freud was awarded the Goethe Prize in recognition of his contributions to psychology and to German literary culture.

Freud used pseudonyms in his case histories. Some patients known by pseudonyms were Cäcilie M. (Anna von Lieben); Dora (Ida Bauer, 1882–1945); Frau Emmy von N. (Fanny Moser); Fräulein Elisabeth von R. (Ilona Weiss); Fräulein Katharina (Aurelia Kronich); Fräulein Lucy R.; (Herbert Graf, 1903–1973); Rat Man (Ernst Lanzer, 1878–1914); Enos Fingy (Joshua Wild, 1878–1920); and Wolf Man (Sergei Pankejeff, 1887–1979). Other famous patients included Prince Pedro Augusto of Brazil (1866–1934); H.D. (1886–1961); Emma Eckstein (1865–1924); Gustav Mahler (1860–1911), with whom Freud had only a single, extended consultation; Princess Marie Bonaparte; Edith Banfield Jackson (1895–1977); and Albert Hirst (1887–1974).

In February 1923, Freud detected a leukoplakia, a benign growth associated with heavy smoking, on his mouth. He initially kept this secret, but in April 1923 he informed Ernest Jones, telling him that the growth had been removed. Freud consulted the dermatologist Maximilian Steiner, who advised him to quit smoking but lied about the growth's seriousness, minimizing its importance. Freud later saw Felix Deutsch, who saw that the growth was cancerous; he identified it to Freud using the euphemism "a bad leukoplakia" instead of the technical diagnosis epithelioma. Deutsch advised Freud to stop smoking and have the growth excised. Freud was treated by Marcus Hajek, a rhinologist whose competence he had previously questioned. Hajek performed an unnecessary cosmetic surgery in his clinic's outpatient department. Freud bled during and after the operation, and may narrowly have escaped death. Freud subsequently saw Deutsch again. Deutsch saw that further surgery would be required, but did not tell Freud he had cancer because he was worried that Freud might wish to commit suicide.

In January 1933, the Nazi Party took control of Germany, and Freud's books were prominent among those they burned and destroyed. Freud remarked to Ernest Jones: "What progress we are making. In the Middle Ages they would have burned me. Now, they are content with burning my books." Freud continued to underestimate the growing Nazi threat and remained determined to stay in Vienna, even following the Anschluss of 13 March 1938, in which Nazi Germany annexed Austria, and the outbreaks of violent antisemitism that ensued. Jones, the then president of the International Psychoanalytical Association (IPA), flew into Vienna from London via Prague on 15 March determined to get Freud to change his mind and seek exile in Britain. This prospect and the shock of the arrest and interrogation of Anna Freud by the Gestapo finally convinced Freud it was time to leave Austria. Jones left for London the following week with a list provided by Freud of the party of émigrés for whom immigration permits would be required. Back in London, Jones used his personal acquaintance with the Home Secretary, Sir Samuel Hoare, to expedite the granting of permits. There were seventeen in all and work permits were provided where relevant. Jones also used his influence in scientific circles, persuading the president of the Royal Society, Sir William Bragg, to write to the Foreign Secretary Lord Halifax, requesting to good effect that diplomatic pressure be applied in Berlin and Vienna on Freud's behalf. Freud also had support from American diplomats, notably his ex-patient and American ambassador to France, William Bullitt. Bullitt alerted U.S. President Roosevelt to the increased dangers facing the Freuds, resulting in the American consul-general in Vienna, John Cooper Wiley, arranging regular monitoring of Berggasse 19. He also intervened by phone call during the Gestapo interrogation of Anna Freud.

The departure from Vienna began in stages throughout April and May 1938. Freud's grandson Ernst Halberstadt and Freud's son Martin's wife and children left for Paris in April. Freud's sister-in-law, Minna Bernays, left for London on 5 May, Martin Freud the following week and Freud's daughter Mathilde and her husband, Robert Hollitscher, on 24 May.

By the end of the month, arrangements for Freud's own departure for London had become stalled, mired in a legally tortuous and financially extortionate process of negotiation with the Nazi authorities. Under regulations imposed on its Jewish population by the new Nazi regime, a Kommissar was appointed to manage Freud's assets and those of the IPA whose headquarters were nearby Freud's home. Freud was allocated to Dr. Anton Sauerwald, who had studied chemistry at Vienna University under Professor Josef Herzig, an old friend of Freud's. Sauerwald read Freud's books to further learn about him and became sympathetic towards his situation. Though required to disclose details of all Freud's bank accounts to his superiors and to arrange the destruction of the historic library of books housed in the offices of the IPA, Sauerwald did neither. Instead he removed evidence of Freud's foreign bank accounts to his own safe-keeping and arranged the storage of the IPA library in the Austrian National Library where it remained until the end of the war.

Though Sauerwald's intervention lessened the financial burden of the "flight" tax on Freud's declared assets, other substantial charges were levied in relation to the debts of the IPA and the valuable collection of antiquities Freud possessed. Unable to access his own accounts, Freud turned to Princess Marie Bonaparte, the most eminent and wealthy of his French followers, who had travelled to Vienna to offer her support and it was she who made the necessary funds available. This allowed Sauerwald to sign the necessary exit visas for Freud, his wife Martha and daughter Anna. They left Vienna on the Orient Express on 4 June, accompanied by their housekeeper and a doctor, arriving in Paris the following day where they stayed as guests of Marie Bonaparte before travelling overnight to London arriving at London Victoria station on 6 June.

Among those soon to call on Freud to pay their respects were Salvador Dalí, Stefan Zweig, Leonard Woolf, Virginia Woolf and H. G. Wells. Representatives of the Royal Society called with the Society's Charter for Freud, who had been elected a Foreign Member in 1936, to sign himself into membership. Marie Bonaparte arrived towards the end of June to discuss the fate of Freud's four elderly sisters left behind in Vienna. Her subsequent attempts to get them exit visas failed and they would all die in Nazi concentration camps.
In early 1939 Sauerwald arrived in London in mysterious circumstances where he met Freud's brother Alexander. He was tried and imprisoned in 1945 by an Austrian court for his activities as a Nazi Party official. Responding to a plea from his wife, Anna Freud wrote to confirm that Sauerwald "used his office as our appointed commissar in such a manner as to protect my father". Her intervention helped secure his release from jail in 1947.

In the Freuds' new home, 20 Maresfield Gardens, Hampstead, North London, Freud's Vienna consulting room was recreated in faithful detail. He continued to see patients there until the terminal stages of his illness. He also worked on his last books, "Moses and Monotheism", published in German in 1938 and in English the following year and the uncompleted "An Outline of Psychoanalysis" which was published posthumously.

By mid-September 1939, Freud's cancer of the jaw was causing him increasingly severe pain and had been declared to be inoperable. The last book he read, Balzac's "La Peau de chagrin", prompted reflections on his own increasing frailty and a few days later he turned to his doctor, friend and fellow refugee, Max Schur, reminding him that they had previously discussed the terminal stages of his illness: "Schur, you remember our 'contract' not to leave me in the lurch when the time had come. Now it is nothing but torture and makes no sense." When Schur replied that he had not forgotten, Freud said, "I thank you," and then "Talk it over with Anna, and if she thinks it's right, then make an end of it." Anna Freud wanted to postpone her father's death, but Schur convinced her it was pointless to keep him alive and on 21 and 22 September administered doses of morphine that resulted in Freud's death around 3 am on 23 September 1939. However, discrepancies in the various accounts Schur gave of his role in Freud's final hours, which have in turn led to inconsistencies between Freud's main biographers, has led to further research and a revised account. This proposes that Schur was absent from Freud's deathbed when a third and final dose of morphine was administered by Dr Josephine Stross, a colleague of Anna Freud, leading to Freud's death around midnight on 23 September 1939.

Three days after his death Freud's body was cremated at the Golders Green Crematorium in North London, with Harrods acting as funeral directors, on the instructions of his son, Ernst. Funeral orations were given by Ernest Jones and the Austrian author Stefan Zweig. Freud's ashes were later placed in the crematorium's "Ernest George Columbarium" (see "Freud Corner"). They rest on a plinth designed by his son, Ernst, in a sealed ancient Greek bell krater painted with Dionysian scenes that Freud had received as a gift from Marie Bonaparte and which he had kept in his study in Vienna for many years. After his wife, Martha, died in 1951, her ashes were also placed in the urn.

Freud began his study of medicine at the University of Vienna in 1873. He took almost nine years to complete his studies, due to his interest in neurophysiological research, specifically investigation of the sexual anatomy of eels and the physiology of the fish nervous system, and because of his interest in studying philosophy with Franz Brentano. He entered private practice in neurology for financial reasons, receiving his M.D. degree in 1881 at the age of 25. Amongst his principal concerns in the 1880s was the anatomy of the brain, specifically the medulla oblongata. He intervened in the important debates about aphasia with his monograph of 1891, "Zur Auffassung der Aphasien", in which he coined the term agnosia and counselled against a too locationist view of the explanation of neurological deficits. Like his contemporary Eugen Bleuler, he emphasized brain function rather than brain structure.

Freud was also an early researcher in the field of cerebral palsy, which was then known as "cerebral paralysis". He published several medical papers on the topic, and showed that the disease existed long before other researchers of the period began to notice and study it. He also suggested that William John Little, the man who first identified cerebral palsy, was wrong about lack of oxygen during birth being a cause. Instead, he suggested that complications in birth were only a symptom.

Freud hoped that his research would provide a solid scientific basis for his therapeutic technique. The goal of Freudian therapy, or psychoanalysis, was to bring repressed thoughts and feelings into consciousness in order to free the patient from suffering repetitive distorted emotions.

Classically, the bringing of unconscious thoughts and feelings to consciousness is brought about by encouraging a patient to talk about dreams and engage in free association, in which patients report their thoughts without reservation and make no attempt to concentrate while doing so. Another important element of psychoanalysis is transference, the process by which patients displace onto their analysts feelings and ideas which derive from previous figures in their lives. Transference was first seen as a regrettable phenomenon that interfered with the recovery of repressed memories and disturbed patients' objectivity, but by 1912, Freud had come to see it as an essential part of the therapeutic process.

The origin of Freud's early work with psychoanalysis can be linked to Josef Breuer. Freud credited Breuer with opening the way to the discovery of the psychoanalytical method by his treatment of the case of Anna O. In November 1880, Breuer was called in to treat a highly intelligent 21-year-old woman (Bertha Pappenheim) for a persistent cough that he diagnosed as hysterical. He found that while nursing her dying father, she had developed a number of transitory symptoms, including visual disorders and paralysis and contractures of limbs, which he also diagnosed as hysterical. Breuer began to see his patient almost every day as the symptoms increased and became more persistent, and observed that she entered states of "absence". He found that when, with his encouragement, she told fantasy stories in her evening states of "absence" her condition improved, and most of her symptoms had disappeared by April 1881. Following the death of her father in that month her condition deteriorated again. Breuer recorded that some of the symptoms eventually remitted spontaneously, and that full recovery was achieved by inducing her to recall events that had precipitated the occurrence of a specific symptom. In the years immediately following Breuer's treatment, Anna O. spent three short periods in sanatoria with the diagnosis "hysteria" with "somatic symptoms", and some authors have challenged Breuer's published account of a cure. Richard Skues rejects this interpretation, which he sees as stemming from both Freudian and anti-psychoanalytical revisionism, that regards both Breuer's narrative of the case as unreliable and his treatment of Anna O. as a failure. Psychologist Frank Sulloway contends that "Freud's case histories are rampant with censorship, distortions, highly dubious 'reconstructions,' and exaggerated claims."

In the early 1890s, Freud used a form of treatment based on the one that Breuer had described to him, modified by what he called his "pressure technique" and his newly developed analytic technique of interpretation and reconstruction. According to Freud's later accounts of this period, as a result of his use of this procedure most of his patients in the mid-1890s reported early childhood sexual abuse. He believed these accounts, which he used as the basis for his seduction theory, but then he came to believe that they were fantasies. He explained these at first as having the function of "fending off" memories of infantile masturbation, but in later years he wrote that they represented Oedipal fantasies, stemming from innate drives that are sexual and destructive in nature.

Another version of events focuses on Freud's proposing that unconscious memories of infantile sexual abuse were at the root of the psychoneuroses in letters to Fliess in October 1895, before he reported that he had actually discovered such abuse among his patients. In the first half of 1896, Freud published three papers, which led to his seduction theory, stating that he had uncovered, in all of his current patients, deeply repressed memories of sexual abuse in early childhood. In these papers, Freud recorded that his patients were not consciously aware of these memories, and must therefore be present as "unconscious memories" if they were to result in hysterical symptoms or obsessional neurosis. The patients were subjected to considerable pressure to "reproduce" infantile sexual abuse "scenes" that Freud was convinced had been repressed into the unconscious. Patients were generally unconvinced that their experiences of Freud's clinical procedure indicated actual sexual abuse. He reported that even after a supposed "reproduction" of sexual scenes the patients assured him emphatically of their disbelief.

As well as his pressure technique, Freud's clinical procedures involved analytic inference and the symbolic interpretation of symptoms to trace back to memories of infantile sexual abuse. His claim of one hundred percent confirmation of his theory only served to reinforce previously expressed reservations from his colleagues about the validity of findings obtained through his suggestive techniques. Freud subsequently showed inconsistency as to whether his seduction theory was still compatible with his later findings. In an addendum to "The Aetiology of Hysteria" he stated: "All this is true [the sexual abuse of children]; but it must be remembered that at the time I wrote it I had not yet freed myself from my overvaluation of reality and my low valuation of phantasy". Some years later Freud explicitly rejected the claim of his colleague Ferenczi that his patients' reports of sexual molestation were actual memories instead of fantasies, and he tried to dissuade Ferenczi from making his views public. Karin Ahbel-Rappe concludes in her study "'I no longer believe': did Freud abandon the seduction theory?": "Freud marked out and started down a trail of investigation into the nature of the experience of infantile incest and its impact on the human psyche, and then abandoned this direction for the most part."

As a medical researcher, Freud was an early user and proponent of cocaine as a stimulant as well as analgesic. He believed that cocaine was a cure for many mental and physical problems, and in his 1884 paper "On Coca" he extolled its virtues. Between 1883 and 1887 he wrote several articles recommending medical applications, including its use as an antidepressant. He narrowly missed out on obtaining scientific priority for discovering its anesthetic properties of which he was aware but had mentioned only in passing. (Karl Koller, a colleague of Freud's in Vienna, received that distinction in 1884 after reporting to a medical society the ways cocaine could be used in delicate eye surgery.) Freud also recommended cocaine as a cure for morphine addiction. He had introduced cocaine to his friend Ernst von Fleischl-Marxow, who had become addicted to morphine taken to relieve years of excruciating nerve pain resulting from an infection acquired after injuring himself while performing an autopsy. His claim that Fleischl-Marxow was cured of his addiction was premature, though he never acknowledged that he had been at fault. Fleischl-Marxow developed an acute case of "cocaine psychosis", and soon returned to using morphine, dying a few years later still suffering from intolerable pain.

The application as an anesthetic turned out to be one of the few safe uses of cocaine, and as reports of addiction and overdose began to filter in from many places in the world, Freud's medical reputation became somewhat tarnished. After the "Cocaine Episode" Freud ceased to publicly recommend use of the drug, but continued to take it himself occasionally for depression, migraine and nasal inflammation during the early 1890s, before discontinuing its use in 1896.

The concept of the unconscious was central to Freud's account of the mind. Freud believed that while poets and thinkers had long known of the existence of the unconscious, he had ensured that it received scientific recognition in the field of psychology.

Freud states explicitly that his concept of the unconscious as he first formulated it was based on the theory of repression. He postulated a cycle in which ideas are repressed, but remain in the mind, removed from consciousness yet operative, then reappear in consciousness under certain circumstances. The postulate was based upon the investigation of cases of hysteria, which revealed instances of behaviour in patients that could not be explained without reference to ideas or thoughts of which they had no awareness and which analysis revealed were linked to the (real or imagined) repressed sexual scenarios of childhood. In his later re-formulations of the concept of repression in his 1915 paper 'Repression' ("Standard Edition" XIV) Freud introduced the distinction in the unconscious between primary repression linked to the universal taboo on incest ('innately present originally') and repression ('after expulsion') that was a product of an individual's life history ('acquired in the course of the ego's development') in which something that was at one point conscious is rejected or eliminated from consciousness.

In his account of the development and modification of his theory of unconscious mental processes he sets out in his 1915 paper 'The Unconscious' ("Standard Edition" XIV), Freud identifies the three perspectives he employs: the dynamic, the economic and the topographical.

The dynamic perspective concerns firstly the constitution of the unconscious by repression and secondly the process of "censorship" which maintains unwanted, anxiety inducing thoughts as such. Here Freud is drawing on observations from his earliest clinical work in the treatment of hysteria.

In the economic perspective the focus is upon the trajectories of the repressed contents "the vicissitudes of sexual impulses" as they undergo complex transformations in the process of both symptom formation and normal unconscious thought such as dreams and slips of the tongue. These were topics Freud explored in detail in "The Interpretation of Dreams" and "The Psychopathology of Everyday Life."

Whereas both these former perspectives focus on the unconscious as it is about to enter consciousness, the topographical perspective represents a shift in which the systemic properties of the unconscious, its characteristic processes and modes of operation such as condensation and displacement, are placed in the foreground.

This "first topography" presents a model of psychic structure comprising three systems:


In his later work, notably in "The Ego and the Id" (1923), a second topography is introduced comprising id, ego and super-ego, which is superimposed on the first without replacing it. In this later formulation of the concept of the unconscious the id comprises a reservoir of instincts or drives a portion of them being hereditary or innate, a portion repressed or acquired. As such, from the economic perspective, the id is the prime source of psychical energy and from the dynamic perspective it conflicts with the ego and the super-ego which, genetically speaking, are diversifications of the id.

Freud believed the function of dreams is to preserve sleep by representing as fulfilled wishes that would otherwise awaken the dreamer.

In Freud's theory dreams are instigated by the daily occurrences and thoughts of everyday life. In what Freud called the "dream-work", these "secondary process" thoughts ("word presentations"), governed by the rules of language and the reality principle, become subject to the "primary process" of unconscious thought ("thing presentations") governed by the pleasure principle, wish gratification and the repressed sexual scenarios of childhood. Because of the disturbing nature of the latter and other repressed thoughts and desires which may have become linked to them, the dream-work operates a censorship function, disguising by distortion, displacement and condensation the repressed thoughts so as to preserve sleep.

In the clinical setting Freud encouraged free association to the dream's manifest content, as recounted in the dream narrative, so as to facilitate interpretative work on its latent content – the repressed thoughts and fantasies – and also on the underlying mechanisms and structures operative in the dream-work. As Freud developed his theoretical work on dreams he went beyond his theory of dreams as wish-fulfillments to arrive at an emphasis on dreams as "nothing other than a particular form of thinking... It is the dream-work that creates that form, and it alone is the essence of dreaming".

Freud's theory of psychosexual development proposes that, following on from the initial polymorphous perversity of infantile sexuality, the sexual "drives" pass through the distinct developmental phases of the oral, the anal, and the phallic. Though these phases then give way to a latency stage of reduced sexual interest and activity (from the age of five to puberty, approximately), they leave, to a greater or lesser extent, a "perverse" and bisexual residue which persists during the formation of adult genital sexuality. Freud argued that neurosis or perversion could be explained in terms of fixation or regression to these phases whereas adult character and cultural creativity could achieve a sublimation of their perverse residue.

After Freud's later development of the theory of the Oedipus complex this normative developmental trajectory becomes formulated in terms of the child's renunciation of incestuous desires under the fantasised threat of (or phantasised fact of, in the case of the girl) castration. The "dissolution" of the Oedipus complex is then achieved when the child's rivalrous identification with the parental figure is transformed into the pacifying identifications of the Ego ideal which assume both similarity and difference and acknowledge the separateness and autonomy of the other.

Freud hoped to prove that his model was universally valid and turned to ancient mythology and contemporary ethnography for comparative material arguing that totemism reflected a ritualized enactment of a tribal Oedipal conflict.

Freud proposed that the human psyche could be divided into three parts: Id, ego and super-ego. Freud discussed this model in the 1920 essay "Beyond the Pleasure Principle", and fully elaborated upon it in "The Ego and the Id" (1923), in which he developed it as an alternative to his previous topographic schema (i.e., conscious, unconscious and preconscious). The id is the completely unconscious, impulsive, childlike portion of the psyche that operates on the "pleasure principle" and is the source of basic impulses and drives; it seeks immediate pleasure and gratification.

Freud acknowledged that his use of the term "Id" ("das Es", "the It") derives from the writings of Georg Groddeck. The super-ego is the moral component of the psyche, which takes into account no special circumstances in which the morally right thing may not be right for a given situation. The rational ego attempts to exact a balance between the impractical hedonism of the id and the equally impractical moralism of the super-ego; it is the part of the psyche that is usually reflected most directly in a person's actions. When overburdened or threatened by its tasks, it may employ defence mechanisms including denial, repression, undoing, rationalization, and displacement. This concept is usually represented by the "Iceberg Model". This model represents the roles the id, ego, and super- ego play in relation to conscious and unconscious thought.

Freud compared the relationship between the ego and the id to that between a charioteer and his horses: the horses provide the energy and drive, while the charioteer provides direction.

Freud believed that the human psyche is subject to two conflicting drives: the life drive or libido and the death drive. The life drive was also termed "Eros" and the death drive "Thanatos", although Freud did not use the latter term; "Thanatos" was introduced in this context by Paul Federn. Freud hypothesized that libido is a form of mental energy with which processes, structures and object-representations are invested.

In "Beyond the Pleasure Principle" (1920), Freud inferred the existence of a death drive. Its premise was a regulatory principle that has been described as "the principle of psychic inertia", "the Nirvana principle", and "the conservatism of instinct". Its background was Freud's earlier "Project for a Scientific Psychology", where he had defined the principle governing the mental apparatus as its tendency to divest itself of quantity or to reduce tension to zero. Freud had been obliged to abandon that definition, since it proved adequate only to the most rudimentary kinds of mental functioning, and replaced the idea that the apparatus tends toward a level of zero tension with the idea that it tends toward a minimum level of tension.

Freud in effect readopted the original definition in "Beyond the Pleasure Principle", this time applying it to a different principle. He asserted that on certain occasions the mind acts as though it could eliminate tension entirely, or in effect to reduce itself to a state of extinction; his key evidence for this was the existence of the compulsion to repeat. Examples of such repetition included the dream life of traumatic neurotics and children's play. In the phenomenon of repetition, Freud saw a psychic trend to work over earlier impressions, to master them and derive pleasure from them, a trend was prior to the pleasure principle but not opposed to it. In addition to that trend, there was also a principle at work that was opposed to, and thus "beyond" the pleasure principle. If repetition is a necessary element in the binding of energy or adaptation, when carried to inordinate lengths it becomes a means of abandoning adaptations and reinstating earlier or less evolved psychic positions. By combining this idea with the hypothesis that all repetition is a form of discharge, Freud reached the conclusion that the compulsion to repeat is an effort to restore a state that is both historically primitive and marked by the total draining of energy: death. Such an explanation has been defined by some scholars as "metaphysical biology".

In his 1917 essay "Mourning and Melancholia", Freud drew a distinction between mourning, painful but an inevitable part of life, and "melancholia", his term for pathological refusal of a mourner to "decathect" from the lost one. Freud claimed that, in normal mourning, the ego was responsible for narcissistically detaching the libido from the lost one as a means of self-preservation, but that in "melancholia", prior ambivalence towards the lost one prevents this from occurring. Suicide, Freud hypothesized, could result in extreme cases, when unconscious feelings of conflict became directed against the mourner's own ego.

Initiating what became the first debate within psychoanalysis on femininity, Karen Horney of the Berlin Institute set out to challenge Freud's account of the development of feminine sexuality. Rejecting Freud's theories of the feminine castration complex and penis envy, Horney argued for a primary femininity and penis envy as a defensive formation rather than arising from the fact, or "injury", of biological asymmetry as Freud held. Horney had the influential support of Melanie Klein and Ernest Jones who coined the term "phallocentrism" in his critique of Freud's position.

In defending Freud against this critique, feminist scholar Jacqueline Rose has argued that it presupposes a more normative account of female sexual development than that given by Freud. She notes that Freud moved from a description of the little girl stuck with her 'inferiority' or 'injury' in the face of the anatomy of the little boy to an account in his later work which explicitly describes the process of becoming 'feminine' as an 'injury' or 'catastrophe' for the complexity of her earlier psychic and sexual life.

According to Freud, "Elimination of clitoral sexuality is a necessary precondition for the development of femininity, since it is immature and masculine in its nature." Freud postulated the concept of "vaginal orgasm" as separate from clitoral orgasm, achieved by external stimulation of the clitoris. In 1905, he stated that clitoral orgasms are purely an adolescent phenomenon and that, upon reaching puberty, the proper response of mature women is a change-over to vaginal orgasms, meaning orgasms without any clitoral stimulation. This theory has been criticized on the grounds that Freud provided no evidence for this basic assumption, and because it made many women feel inadequate when they could not achieve orgasm via vaginal intercourse alone.

Freud regarded the monotheistic God as an illusion based upon the infantile emotional need for a powerful, supernatural pater familias. He maintained that religion – once necessary to restrain man's violent nature in the early stages of civilization – in modern times, can be set aside in favor of reason and science. "Obsessive Actions and Religious Practices" (1907) notes the likeness between faith (religious belief) and neurotic obsession. "Totem and Taboo" (1913) proposes that society and religion begin with the patricide and eating of the powerful paternal figure, who then becomes a revered collective memory. These arguments were further developed in "The Future of an Illusion" (1927) in which Freud argued that religious belief serves the function of psychological consolation. Freud argues the belief of a supernatural protector serves as a buffer from man's "fear of nature" just as the belief in an afterlife serves as a buffer from man's fear of death. The core idea of the work is that all of religious belief can be explained through its function to society, not for its relation to the truth. This is why, according to Freud, religious beliefs are "illusions". In "Civilization and Its Discontents" (1930), he quotes his friend Romain Rolland, who described religion as an "oceanic sensation", but says he never experienced this feeling. "Moses and Monotheism" (1937) proposes that Moses was the tribal pater familias, killed by the Jews, who psychologically coped with the patricide with a reaction formation conducive to their establishing monotheist Judaism; analogously, he described the Roman Catholic rite of Holy Communion as cultural evidence of the killing and devouring of the sacred father.

Moreover, he perceived religion, with its suppression of violence, as mediator of the societal and personal, the public and the private, conflicts between Eros and Thanatos, the forces of life and death. Later works indicate Freud's pessimism about the future of civilization, which he noted in the 1931 edition of "Civilization and its Discontents".

In a footnote of his 1909 work, "Analysis of a Phobia in a Five year old Boy", Freud theorized that the universal fear of castration was provoked in the uncircumcised when they perceived circumcision and that this was "the deepest unconscious root of anti-Semitism".

Freud's legacy, though a highly contested area of controversy, was described by Stephen Frosh as "one of the strongest influences on twentieth-century thought, its impact comparable only to that of Darwinism and Marxism." Henri Ellenberger stated that its range of influence permeated "all the fields of culture ... so far as to change our way of life and concept of man."

Though not the first methodology in the practice of individual verbal psychotherapy, Freud's psychoanalytic system came to dominate the field from early in the twentieth century, forming the basis for many later variants. While these systems have adopted different theories and techniques, all have followed Freud by attempting to achieve psychic and behavioral change through having patients talk about their difficulties. Psychoanalysis is not as influential as it once was in Europe and the United States, though in some parts of the world, notably Latin America, its influence in the later 20th century expanded substantially. Psychoanalysis also remains influential within many contemporary schools of psychotherapy and has led to innovative therapeutic work in schools and with families and groups. There is a substantial body of research which demonstrates the efficacy of the clinical methods of psychoanalysis and of related psychodynamic therapies in treating a wide range of psychological disorders.

The neo-Freudians, a group including Alfred Adler, Otto Rank, Karen Horney, Harry Stack Sullivan and Erich Fromm, rejected Freud's theory of instinctual drive, emphasized interpersonal relations and self-assertiveness, and made modifications to therapeutic practice that reflected these theoretical shifts. Adler originated the approach, although his influence was indirect due to his inability to systematically formulate his ideas. Neo-Freudian analysis places more emphasis on the patient's relationship with the analyst and less on exploration of the unconscious.

Carl Jung believed that the collective unconscious, which reflects the cosmic order and the history of the human species, is the most important part of the mind. It contains archetypes, which are manifested in symbols that appear in dreams, disturbed states of mind, and various products of culture. Jungians are less interested in infantile development and psychological conflict between wishes and the forces that frustrate them than in integration between different parts of the person. The object of Jungian therapy was to mend such splits. Jung focused in particular on problems of middle and later life. His objective was to allow people to experience the split-off aspects of themselves, such as the anima (a man's suppressed female self), the animus (a woman's suppressed male self), or the shadow (an inferior self-image), and thereby attain wisdom.

Jacques Lacan approached psychoanalysis through linguistics and literature. Lacan believed Freud's essential work had been done prior to 1905 and concerned the interpretation of dreams, neurotic symptoms, and slips, which had been based on a revolutionary way of understanding language and its relation to experience and subjectivity, and that ego psychology and object relations theory were based upon misreadings of Freud's work. For Lacan, the determinative dimension of human experience is neither the self (as in ego psychology) nor relations with others (as in object relations theory), but language. Lacan saw desire as more important than need and considered it necessarily ungratifiable.

Wilhelm Reich developed ideas that Freud had developed at the beginning of his psychoanalytic investigation but then superseded but never finally discarded. These were the concept of the Actualneurosis and a theory of anxiety based upon the idea of dammed-up libido. In Freud's original view, what really happened to a person (the "actual") determined the resulting neurotic disposition. Freud applied that idea both to infants and to adults. In the former case, seductions were sought as the causes of later neuroses and in the latter incomplete sexual release. Unlike Freud, Reich retained the idea that actual experience, especially sexual experience, was of key significance. By the 1920s, Reich had "taken Freud's original ideas about sexual release to the point of specifying the orgasm as the criteria of healthy function." Reich was also "developing his ideas about character into a form that would later take shape, first as "muscular armour", and eventually as a transducer of universal biological energy, the "orgone"."

Fritz Perls, who helped to develop Gestalt therapy, was influenced by Reich, Jung and Freud. The key idea of gestalt therapy is that Freud overlooked the structure of awareness, "an active process that moves toward the construction of organized meaningful wholes... between an organism and its environment." These wholes, called "gestalts", are "patterns involving all the layers of organismic function – thought, feeling, and activity." Neurosis is seen as splitting in the formation of gestalts, and anxiety as the organism sensing "the struggle towards its creative unification." Gestalt therapy attempts to cure patients through placing them in contact with "immediate organismic needs." Perls rejected the verbal approach of classical psychoanalysis; talking in gestalt therapy serves the purpose of self-expression rather than gaining self-knowledge. Gestalt therapy usually takes place in groups, and in concentrated "workshops" rather than being spread out over a long period of time; it has been extended into new forms of communal living.

Arthur Janov's primal therapy, which has been an influential post-Freudian psychotherapy, resembles psychoanalytic therapy in its emphasis on early childhood experience, but has also differences with it. While Janov's theory is akin to Freud's early idea of Actualneurosis, he does not have a dynamic psychology but a nature psychology like that of Reich or Perls, in which need is primary while wish is derivative and dispensable when need is met. Despite its surface similarity to Freud's ideas, Janov's theory lacks a strictly psychological account of the unconscious and belief in infantile sexuality. While for Freud there was a hierarchy of danger situations, for Janov the key event in the child's life is awareness that the parents do not love it. Janov writes in "The Primal Scream" (1970) that primal therapy has in some ways returned to Freud's early ideas and techniques.

Ellen Bass and Laura Davis, co-authors of "The Courage to Heal" (1988), are described as "champions of survivorship" by Frederick Crews, who considers Freud the key influence upon them, although in his view they are indebted not to classic psychoanalysis but to "the pre-psychoanalytic Freud... who supposedly took pity on his hysterical patients, found that they were all harboring memories of early abuse... and cured them by unknotting their repression." Crews sees Freud as having anticipated the recovered memory movement by emphasizing "mechanical cause-and-effect relations between symptomatology and the premature stimulation of one body zone or another", and with pioneering its "technique of thematically matching a patient's symptom with a sexually symmetrical 'memory.'" Crews believes that Freud's confidence in accurate recall of early memories anticipates the theories of recovered memory therapists such as Lenore Terr, which in his view have led to people being wrongfully imprisoned or involved in litigation.

Research projects designed to test Freud's theories empirically have led to a vast literature on the topic. American psychologists began to attempt to study repression in the experimental laboratory around 1930. In 1934, when the psychologist Saul Rosenzweig sent Freud reprints of his attempts to study repression, Freud responded with a dismissive letter stating that "the wealth of reliable observations" on which psychoanalytic assertions were based made them "independent of experimental verification." Seymour Fisher and Roger P. Greenberg concluded in 1977 that some of Freud's concepts were supported by empirical evidence. Their analysis of research literature supported Freud's concepts of oral and anal personality constellations, his account of the role of Oedipal factors in certain aspects of male personality functioning, his formulations about the relatively greater concern about loss of love in women's as compared to men's personality economy, and his views about the instigating effects of homosexual anxieties on the formation of paranoid delusions. They also found limited and equivocal support for Freud's theories about the development of homosexuality. They found that several of Freud's other theories, including his portrayal of dreams as primarily containers of secret, unconscious wishes, as well as some of his views about the psychodynamics of women, were either not supported or contradicted by research. Reviewing the issues again in 1996, they concluded that much experimental data relevant to Freud's work exists, and supports some of his major ideas and theories.

Other viewpoints include those of Hans Eysenck, who writes in "Decline and Fall of the Freudian Empire" (1985) that Freud set back the study of psychology and psychiatry "by something like fifty years or more", and Malcolm Macmillan, who concludes in "Freud Evaluated" (1991) that "Freud's method is not capable of yielding objective data about mental processes". Morris Eagle states that it has been "demonstrated quite conclusively that because of the epistemologically contaminated status of clinical data derived from the clinical situation, such data have questionable probative value in the testing of psychoanalytic hypotheses". Richard Webster, in "Why Freud Was Wrong" (1995), described psychoanalysis as perhaps the most complex and successful pseudoscience in history. Crews believes that psychoanalysis has no scientific or therapeutic merit.

I.B. Cohen regards Freud's "Interpretation of Dreams" as a revolutionary work of science, the last such work to be published in book form.
In contrast Allan Hobson believes that Freud, by rhetorically discrediting 19th century investigators of dreams such as Alfred Maury and the Marquis de Hervey de Saint-Denis at a time when study of the physiology of the brain was only beginning, interrupted the development of scientific dream theory for half a century. The dream researcher G. William Domhoff has disputed claims of Freudian dream theory being validated.

The philosopher Karl Popper, who argued that all proper scientific theories must be potentially falsifiable, claimed that Freud's Psychoanalytic Theories were presented in unfalsifiable form, meaning that no experiment could ever disprove them. The philosopher Adolf Grünbaum argues in "The Foundations of Psychoanalysis" (1984) that Popper was mistaken and that many of Freud's theories are empirically testable, a position with which others such as Eysenck agree. The philosopher Roger Scruton, writing in "Sexual Desire" (1986), also rejected Popper's arguments, pointing to the theory of repression as an example of a Freudian theory that does have testable consequences. Scruton nevertheless concluded that psychoanalysis is not genuinely scientific, on the grounds that it involves an unacceptable dependence on metaphor. The philosopher Donald Levy agrees with Grünbaum that Freud's theories are falsifiable but disputes Grünbaum's contention that therapeutic success is only the empirical basis on which they stand or fall, arguing that a much wider range of empirical evidence can be adduced if clinical case material is taken into consideration.

In a study of psychoanalysis in the United States, Nathan Hale reported on the "decline of psychoanalysis in psychiatry" during the years 1965–1985. The continuation of this trend was noted by Alan Stone: "As academic psychology becomes more 'scientific' and psychiatry more biological, psychoanalysis is being brushed aside." Paul Stepansky, while noting that psychoanalysis remains influential in the humanities, records the "vanishingly small number of psychiatric residents who choose to pursue psychoanalytic training" and the "nonanalytic backgrounds of psychiatric chairpersons at major universities" among the evidence he cites for his conclusion that "Such historical trends attest to the marginalisation of psychoanalysis within American psychiatry." Nonetheless Freud was ranked as the third most cited psychologist of the 20th century, according to a "Review of General Psychology" survey of American psychologists and psychology texts, published in 2002. It is also claimed that in moving beyond the "orthodoxy of the not so distant past... new ideas and new research has led to an intense reawakening of interest in psychoanalysis from neighbouring disciplines ranging from the humanities to neuroscience and including the non-analytic therapies".

Research in the emerging field of neuropsychoanalysis, founded by neuroscientist and psychoanalyst Mark Solms, has proved controversial with some psychoanalysts criticising the very concept itself. Solms and his colleagues have argued for neuro-scientific findings being "broadly consistent" with Freudian theories pointing out brain structures relating to Freudian concepts such as libido, drives, the unconscious, and repression. Neuroscientists who have endorsed Freud's work include David Eagleman who believes that Freud "transformed psychiatry" by providing " the first exploration of the way in which hidden states of the brain participate in driving thought and behavior" and Nobel laureate Eric Kandel who argues that "psychoanalysis still represents the most coherent and intellectually satisfying view of the mind."

Psychoanalysis has been interpreted as both radical and conservative. By the 1940s, it had come to be seen as conservative by the European and American intellectual community. Critics outside the psychoanalytic movement, whether on the political left or right, saw Freud as a conservative. Fromm had argued that several aspects of psychoanalytic theory served the interests of political reaction in his "The Fear of Freedom" (1942), an assessment confirmed by sympathetic writers on the right. In "" (1959), Philip Rieff portrayed Freud as a man who urged men to make the best of an inevitably unhappy fate, and admirable for that reason. In the 1950s, Herbert Marcuse challenged the then prevailing interpretation of Freud as a conservative in "Eros and Civilization" (1955), as did Lionel Trilling in "Freud and the Crisis of Our Culture" and Norman O. Brown in "Life Against Death" (1959). "Eros and Civilization" helped make the idea that Freud and Karl Marx were addressing similar questions from different perspectives credible to the left. Marcuse criticized neo-Freudian revisionism for discarding seemingly pessimistic theories such as the death instinct, arguing that they could be turned in a utopian direction. Freud's theories also influenced the Frankfurt School and critical theory as a whole.

Freud has been compared to Marx by Reich, who saw Freud's importance for psychiatry as parallel to that of Marx for economics, and by Paul Robinson, who sees Freud as a revolutionary whose contributions to twentieth century thought are comparable in importance to Marx's contributions to nineteenth century thought. Fromm calls Freud, Marx, and Einstein the "architects of the modern age", but rejects the idea that Marx and Freud were equally significant, arguing that Marx was both far more historically important and a finer thinker. Fromm nevertheless credits Freud with permanently changing the way human nature is understood. Gilles Deleuze and Félix Guattari write in "Anti-Oedipus" (1972) that psychoanalysis resembles the Russian Revolution in that it became corrupted almost from the beginning. They believe this began with Freud's development of the theory of the Oedipus complex, which they see as idealist.

Jean-Paul Sartre critiques Freud's theory of the unconscious in "Being and Nothingness" (1943), claiming that consciousness is essentially self-conscious. Sartre also attempts to adapt some of Freud's ideas to his own account of human life, and thereby develop an "existential psychoanalysis" in which causal categories are replaced by teleological categories. Maurice Merleau-Ponty considers Freud to be one of the anticipators of phenomenology, while Theodor W. Adorno considers Edmund Husserl, the founder of phenomenology, to be Freud's philosophical opposite, writing that Husserl's polemic against psychologism could have been directed against psychoanalysis. Paul Ricœur sees Freud as one of the three "masters of suspicion", alongside Marx and Nietzsche, for their unmasking 'the lies and illusions of consciousness'. Ricœur and Jürgen Habermas have helped create a "hermeneutic version of Freud", one which "claimed him as the most significant progenitor of the shift from an objectifying, empiricist understanding of the human realm to one stressing subjectivity and interpretation." Louis Althusser drew on Freud's concept of overdetermination for his reinterpretation of Marx's "Capital". Jean-François Lyotard developed a theory of the unconscious that reverses Freud's account of the dream-work: for Lyotard, the unconscious is a force whose intensity is manifest via disfiguration rather than condensation. Jacques Derrida finds Freud to be both a late figure in the history of western metaphysics and, with Nietzsche and Heidegger, a precursor of his own brand of radicalism.

Several scholars see Freud as parallel to Plato, writing that they hold nearly the same theory of dreams and have similar theories of the tripartite structure of the human soul or personality, even if the hierarchy between the parts of the soul is almost reversed. Ernest Gellner argues that Freud's theories are an inversion of Plato's. Whereas Plato saw a hierarchy inherent in the nature of reality, and relied upon it to validate norms, Freud was a naturalist who could not follow such an approach. Both men's theories drew a parallel between the structure of the human mind and that of society, but while Plato wanted to strengthen the super-ego, which corresponded to the aristocracy, Freud wanted to strengthen the ego, which corresponded to the middle class. Paul Vitz compares Freudian psychoanalysis to Thomism, noting St. Thomas's belief in the existence of an "unconscious consciousness" and his "frequent use of the word and concept 'libido' – sometimes in a more specific sense than Freud, but always in a manner in agreement with the Freudian use." Vitz suggests that Freud may have been unaware his theory of the unconscious was reminiscent of Aquinas.

The poem "In Memory of Sigmund Freud" was published by British poet W. H. Auden in his 1940 collection "Another Time". Auden describes Freud as having created "a whole climate of opinion / under whom we conduct our different lives."
Literary critic Harold Bloom has been influenced by Freud. Camille Paglia has also been influenced by Freud, whom she calls "Nietzsche's heir" and one of the greatest sexual psychologists in literature, but has rejected the scientific status of his work in her "Sexual Personae" (1990), writing, "Freud has no rivals among his successors because they think he wrote science, when in fact he wrote art."

The decline in Freud's reputation has been attributed partly to the revival of feminism. Simone de Beauvoir criticizes psychoanalysis from an existentialist standpoint in "The Second Sex" (1949), arguing that Freud saw an "original superiority" in the male that is in reality socially induced. Betty Friedan criticizes Freud and what she considered his Victorian view of women in "The Feminine Mystique" (1963). Freud's concept of penis envy was attacked by Kate Millett, who in "Sexual Politics" (1970) accused him of confusion and oversights. Naomi Weisstein writes that Freud and his followers erroneously thought his "years of intensive clinical experience" added up to scientific rigor.

Freud is also criticized by Shulamith Firestone and Eva Figes. In "The Dialectic of Sex" (1970), Firestone argues that Freud was a "poet" who produced metaphors rather than literal truths; in her view, Freud, like feminists, recognized that sexuality was the crucial problem of modern life, but ignored the social context and failed to question society itself. Firestone interprets Freud's "metaphors" in terms of the facts of power within the family. Figes tries in "Patriarchal Attitudes" (1970) to place Freud within a "history of ideas". Juliet Mitchell defends Freud against his feminist critics in "Psychoanalysis and Feminism" (1974), accusing them of misreading him and misunderstanding the implications of psychoanalytic theory for feminism. Mitchell helped introduce English-speaking feminists to Lacan. Mitchell is criticized by Jane Gallop in "The Daughter's Seduction" (1982). Gallop compliments Mitchell for her criticism of feminist discussions of Freud, but finds her treatment of Lacanian theory lacking.

Some French feminists, among them Julia Kristeva and Luce Irigaray, have been influenced by Freud as interpreted by Lacan. Irigaray has produced a theoretical challenge to Freud and Lacan, using their theories against them to put forward a "psychoanalytic explanation for theoretical bias". Irigaray, who claims that "the cultural unconscious only recognizes the male sex", describes how this affects "accounts of the psychology of women".

Psychologist Carol Gilligan writes that "The penchant of developmental theorists to project a masculine image, and one that appears frightening to women, goes back at least to Freud." She sees Freud's criticism of women's sense of justice reappearing in the work of Jean Piaget and Lawrence Kohlberg. Gilligan notes that Nancy Chodorow, in contrast to Freud, attributes sexual difference not to anatomy but to the fact that male and female children have different early social environments. Chodorow, writing against the masculine bias of psychoanalysis, "replaces Freud's negative and derivative description of female psychology with a positive and direct account of her own."

Toril Moi has developed a feminist perspective on psychoanalysis proposing that it is a discourse that "attempts to understand the psychic consequences of three universal traumas: the fact that there are others, the fact of sexual difference, and the fact of death". She replaces Freud's term of castration with Stanley Cavell's concept of "victimization" which is a more universal term that applies equally to both sexes. Moi regards this concept of human finitude as a suitable replacement for both castration and sexual difference as the traumatic "discovery of our separate, sexed, mortal existence" and how both men and women come to terms with it.

Sigmund Freud is the subject of three major films or TV series, the first of which was 1962's" " starring Montgomery Clift as Freud, directed by John Huston from a revision of a script by an uncredited Jean-Paul Sartre. The film is focused on Freud's early life from 1885 to 1890, and combines multiple case studies of Freud into single ones, and multiple friends of his into single characters.

In 1984, the BBC produced the 6-episode mini-series "Freud: the Life of a Dream" starring David Suchet in the lead role.

The stage play "The Talking Cure" and subsequent film "A Dangerous Method" focus on the conflict between Freud and Carl Jung. Both are written by Christopher Hampton and are partly based on the non-fiction book "A Most Dangerous Method" by John Kerr. Viggo Mortensen plays Freud and Michael Fassbender plays Jung. The play is a reworking of an earlier unfilmed screenplay.

More fanciful employments of Freud in fiction are "The Seven-Per-Cent Solution" by Nicholas Meyer, which centers on an encounter between Freud and the fictional detective Sherlock Holmes, with a main part of the plot seeing Freud helping Holmes overcome his cocaine addiction. Similarly, the 2020 Austrian-German series "Freud" involves a young Freud solving murder mysteries. The series has been criticized for having Freud be helped by a medium with real paranormal powers, when in reality Freud was quite skeptical of the paranormal.

Mark St. Germain's 2009 play "Freud's Last Session" imagines a meeting between C. S. Lewis, aged 40, and Freud, aged 83, at Freud's house in Hampstead, London, in 1939, as the Second World War is about to break out. The play is focused on the two men discussing religion and whether it should be seen as a sign of neurosis. The play is inspired by the 2003 non-fiction book "The Question of God: C.S. Lewis and Sigmund Freud Debate God, Love, Sex, and the Meaning of Life" by Armand Nicholi which also inspired a four-part non-fiction PBS series. (Although, no such meeting took place, June Flewett, who as a teenager stayed with C.S. Lewis and his brother during the wartime London air-raids, later married Freud's grandson Clement Freud.)

Freud is employed to more comic effect in the 1983 film "Lovesick" in which Alec Guinness plays Freud's ghost who gives love advice to a modern psychiatrist played by Dudley Moore. Freud also appears in the time-travel comedy "Bill & Ted's Excellent Adventure".

Canadian author Kim Morrissey's stage play about the Dora case "Dora: A Case of Hysteria" attempts to thoroughly debunk Freud's approach to the case. French playwright Hélène Cixous' 1976 "Portrait of Dora" is also critical of Freud's approach, though less acerbically.





"The Standard Edition of the Complete Psychological Works of Sigmund Freud". Translated from the German under the general editorship of James Strachey, in collaboration with Anna Freud, assisted by Alix Strachey, Alan Tyson, and Angela Richards. 24 volumes, London: Hogarth Press and the Institute of Psycho-Analysis, 1953–1974.




</doc>
<doc id="26746" url="https://en.wikipedia.org/wiki?curid=26746" title="South Dakota">
South Dakota

South Dakota () is a U.S. state in the Midwestern region of the United States. It is named after the Lakota and Dakota Sioux Native American tribes, who comprise a large portion of the population and historically dominated the territory. South Dakota is the seventeenth largest by area, but the fifth smallest by population and the 5th least densely populated of the 50 United States. As the southern part of the former Dakota Territory, South Dakota became a state on November 2, 1889, simultaneously with North Dakota. It was either the 39th or 40th state admitted to the union. Before signing the statehood papers, President Benjamin Harrison shuffled the papers so that no one could tell which became a state first. Pierre is the state capital and Sioux Falls, with a population of about 187,200, is South Dakota's largest city.

South Dakota is bordered by the states of North Dakota (to the north), Minnesota (to the east), Iowa (to the southeast), Nebraska (to the south), Wyoming (to the west), and Montana (to the northwest). The state is bisected by the Missouri River, dividing South Dakota into two geographically and socially distinct halves, known to residents as "East River" and "West River".

Eastern South Dakota is home to most of the state's population, and the area's fertile soil is used to grow a variety of crops. West of the Missouri River, ranching is the predominant agricultural activity, and the economy is more dependent on tourism and defense spending. Most of the Native American reservations are in West River. The Black Hills, a group of low pine-covered mountains sacred to the Sioux, are in the southwest part of the state. Mount Rushmore, a major tourist destination, is there. South Dakota has a temperate continental climate, with four distinct seasons and precipitation ranging from moderate in the east to semi-arid in the west. The state's ecology features species typical of a North American grassland biome.

Humans have inhabited the area for several millennia, with the Sioux becoming dominant by the early 19th century. In the late 19th century, European-American settlement intensified after a gold rush in the Black Hills and the construction of railroads from the east. Encroaching miners and settlers triggered a number of Indian wars, ending with the Wounded Knee Massacre in 1890. Key events in the 20th century included the Dust Bowl and Great Depression, increased federal spending during the 1940s and 1950s for agriculture and defense, and an industrialization of agriculture that has reduced family farming.

While several Democrats have represented South Dakota for multiple terms in both chambers of Congress, the state government is largely controlled by the Republican Party, whose nominees have carried South Dakota in each of the last 13 presidential elections. Historically dominated by an agricultural economy and a rural lifestyle, South Dakota has recently sought to diversify its economy in areas to attract and retain residents. South Dakota's history and rural character still strongly influence the state's culture.

South Dakota is in the north-central United States, and is considered a part of the Midwest by the U.S. Census Bureau; it is also part of the Great Plains region. The culture, economy, and geography of western South Dakota have more in common with the West than the Midwest. South Dakota has a total area of , making the state the 17th largest in the Union.

Black Elk Peak, formerly named Harney Peak, with an elevation of , is the state's highest point, while the shoreline of Big Stone Lake is the lowest, with an elevation of . South Dakota is bordered to the north by North Dakota; to the south by Nebraska; to the east by Iowa and Minnesota; and to the west by Wyoming and Montana. The geographical center of the U.S. is west of Castle Rock in Butte County. The North American continental pole of inaccessibility is between Allen and Kyle, from the nearest coastline.

The Missouri River is the largest and longest river in the United States. Other major South Dakota rivers include the Cheyenne, James, Big Sioux, and White Rivers. Eastern South Dakota has many natural lakes, mostly created by periods of glaciation. Additionally, dams on the Missouri River create four large reservoirs: Lake Oahe, Lake Sharpe, Lake Francis Case, and Lewis and Clark Lake.

South Dakota can generally be divided into three regions: eastern South Dakota, western South Dakota, and the Black Hills. The Missouri River serves as a boundary in terms of geographic, social, and political differences between eastern and western South Dakota. The geography of the Black Hills, long considered sacred by Native Americans, differs from its surroundings to such an extent it can be considered separate from the rest of western South Dakota. At times the Black Hills are combined with the rest of western South Dakota, and people often refer to the resulting two regions divided by the Missouri River as West River and East River.
Eastern South Dakota generally features higher precipitation and lower topography than the western part of the state. Smaller geographic regions of this area include the Coteau des Prairies, the Dissected Till Plains, and the James River Valley. The Coteau des Prairies is a plateau bordered on the east by the Minnesota River Valley and on the west by the James River Basin. Further west, the James River Basin is mostly low, flat, highly eroded land, following the flow of the James River through South Dakota from north to south. The Dissected Till Plains, an area of rolling hills and fertile soil that covers much of Iowa and Nebraska, extends into the southeastern corner of South Dakota. Layers deposited during the Pleistocene epoch, starting around two million years ago, cover most of eastern South Dakota. These are the youngest rock and sediment layers in the state, the product of several successive periods of glaciation which deposited a large amount of rocks and soil, known as till, over the area.

The Great Plains cover most of the western two-thirds of South Dakota. West of the Missouri River the landscape becomes more arid and rugged, consisting of rolling hills, plains, ravines, and steep flat-topped hills called buttes. In the south, east of the Black Hills, lie the South Dakota Badlands. Erosion from the Black Hills, marine skeletons which fell to the bottom of a large shallow sea that once covered the area, and volcanic material all contribute to the geology of this area.
The Black Hills are in the southwestern part of South Dakota and extend into Wyoming. This range of low mountains covers , with peaks that rise from 2,000 to 4,000 feet (600 to 1,200 m) above their bases. The Black Hills are the location of Black Elk Peak (7,242 ft or 2,207 m above sea level), the highest point in South Dakota and also the highest point in the United States east of the Rocky Mountains. Two-billion-year-old Precambrian formations, the oldest rocks in the state, form the central core of the Black Hills. Formations from the Paleozoic Era form the outer ring of the Black Hills; these were created between roughly 540 and 250 million years ago. This area features rocks such as limestone, which were deposited here when the area formed the shoreline of an ancient inland sea.

Much of South Dakota (except for the Black Hills area) is dominated by a temperate grassland biome. Although grasses and crops cover most of this region, deciduous trees such as cottonwoods, elms, and willows are common near rivers and in shelter belts.
Mammals in this area include bison, deer, pronghorn, coyotes, and prairie dogs. The state bird, the ring-necked pheasant, has adapted well to the area after being introduced from China. Growing populations of bald eagles are spread throughout the state, especially near the Missouri River. Rivers and lakes of the grasslands support populations of walleye, carp, pike, bass, and other species. The Missouri River also contains the pre-historic paddlefish.

Due to a higher elevation and level of precipitation, the Black Hills ecology differs significantly from that of the plains. The mountains are thickly blanketed by various types of pines, including ponderosa and lodgepole pines, as well as spruces. Black Hills mammals include deer, elk (wapiti), bighorn sheep, mountain goats, pine marten, and mountain lions, while the streams and lakes contain several species of trout.

South Dakota has a continental climate with four distinct seasons, ranging from cold, dry winters to warm and semi-humid summers. During the summers, the state's average high temperature is often close to , although it cools to near at night. It is not unusual for South Dakota to have severe hot, dry spells in the summer with the temperature climbing above several times a year. Winters are cold with January high temperatures averaging below freezing and low temperatures averaging below in most of the state. The highest recorded temperature is at Usta on July 15, 2006 and the lowest recorded temperature is at McIntosh on February 17, 1936.

Average annual precipitation in South Dakota ranges from semi-arid conditions in the northwestern part of the state (around ) to semi-humid around the southeast portion of the state (around ), although a small area centered on Lead in the Black Hills has the highest precipitation at nearly per year.

South Dakota summers bring frequent, sometimes severe, thunderstorms with high winds, thunder, and hail. The state's eastern part is often considered part of Tornado Alley, and South Dakota experiences an average of 30 tornadoes each year. Severe blizzards and ice storms often occur during winter.

South Dakota has several sites administered by the National Park Service. Two national parks have been established in South Dakota, both in the state's southwestern part. Wind Cave National Park, established in 1903 in the Black Hills, has an extensive cave network as well as a large herd of bison. Badlands National Park was created in 1978. The park features an eroded, brightly colored landscape surrounded by semi-arid grasslands. Mount Rushmore National Memorial in the Black Hills was established in 1925. The sculpture of four U.S. Presidents was carved into the mountainside by sculptor Gutzon Borglum.

Other areas managed by the National Park Service include Jewel Cave National Monument near Custer, the Lewis and Clark National Historic Trail, the Minuteman Missile National Historic Site, which features a decommissioned nuclear missile silo and a separate missile control area several miles away, and the Missouri National Recreational River. The Crazy Horse Memorial is a large mountainside sculpture near Mount Rushmore being built with private funds. The Mammoth Site near Hot Springs is another privately owned attraction in the Black Hills. A working paleontological dig, the site has one of the world's largest concentrations of mammoth remains.

Humans have lived in what is today South Dakota for several thousand years. The first inhabitants were Paleoindian hunter-gatherers, and disappeared from the area around 5000 BC. Between 500 AD and 800 AD, a semi-nomadic people known as the Mound Builders lived in central and eastern South Dakota. In the 14th century, the Crow Creek Massacre occurred, in which several hundred men, women, and children were killed near the Missouri River.

By 1500, the Arikara (or Ree) had settled in much of the Missouri River valley. European contact with the area began in 1743, when the LaVérendrye brothers explored the region. The LaVérendrye group buried a plate near the site of modern-day Pierre, claiming the region for France as part of greater Louisiana. In 1762 the entire region became part of the Spanish Louisiana until 1802. By the early 19th century, the Sioux had largely replaced the Arikara as the dominant group in the area.

In 1803, the United States purchased the Louisiana Territory, an area that included most of South Dakota, from Napoleon Bonaparte, and President Thomas Jefferson organized a group commonly referred to as the "Lewis and Clark Expedition" to explore the region. In 1817, an American fur trading post was set up at present-day Fort Pierre, beginning continuous American settlement of the area. In 1855, the U.S. Army bought Fort Pierre but abandoned it in 1857 in favor of Fort Randall to the south. Settlement by Americans and Europeans was by this time increasing rapidly, and in 1858 the Yankton Sioux signed the 1858 Treaty, ceding most of present-day eastern South Dakota to the United States.
Land speculators founded two of eastern South Dakota's largest present-day cities: Sioux Falls in 1856 and Yankton in 1859. In 1861, the Dakota Territory was established by the United States government (this initially included North Dakota, South Dakota, and parts of Montana and Wyoming). Settlement of the area, mostly by people from the eastern United States as well as western and northern Europe, increased rapidly, especially after the completion of an eastern railway link to Yankton in 1873.

In 1874, gold was discovered in the Black Hills during a military expedition led by George A. Custer and miners and explorers began illegally entering land promised to the Lakota. Custer's expedition took place despite the fact that the Sioux had been granted the entire western half of present-day South Dakota (West River) in 1868 by the Treaty of Laramie as part of the Great Sioux Reservation. The Sioux declined to grant mining rights or land in the Black Hills, and war broke out after the U.S. failed to stop white miners and settlers from entering the region. Eventually the U.S. won and broke up the Great Sioux Reservation into five reservations, settling the Lakota there. In 1980 the Supreme Court and Congress ordered compensation but the Lakota still refuse to accept it, insisting on return of their land.
A growing population and political concerns (admitting two states meant having four new senators for the Republican Party) caused Dakota Territory to be divided in half and President Benjamin Harrison signed proclamations formally admitting South Dakota and North Dakota to the union on November 2, 1889. Harrison had the papers shuffled to obscure which one was signed first and the order went unrecorded.

On December 29, 1890, the Wounded Knee Massacre occurred on the Pine Ridge Indian Reservation. Commonly cited as the last major armed conflict between the United States and the Lakota Sioux Nation, the massacre resulted in the deaths of at least 146 Sioux, many of them women and children. 31 U.S. soldiers were also killed in the conflict.
During the 1930s, several economic and climatic conditions combined with disastrous results for South Dakota. A lack of rainfall, extremely high temperatures and inappropriate cultivation techniques produced what was known as the Dust Bowl in South Dakota and several other plains states. Fertile topsoil was blown away in massive dust storms, and several harvests were completely ruined. The experiences of the Dust Bowl, coupled with local bank foreclosures and the general economic effects of the Great Depression, resulted in many South Dakotans leaving the state. The population of South Dakota declined by more than 7% between 1930 and 1940.

Economic stability returned with the U.S. entry into World War II in 1941, when demand for the state's agricultural and industrial products grew as the nation mobilized for war. In 1944, the Pick–Sloan Plan was passed as part of the Flood Control Act of 1944 by the U.S. Congress, resulting in the construction of six large dams on the Missouri River, four of which are at least partially in South Dakota. Flood control, hydroelectricity, and recreational opportunities such as boating and fishing are provided by the dams and their reservoirs.

In recent decades, South Dakota has been transformed from a state dominated by agriculture to one with a more diversified economy. The tourism industry has grown considerably since the completion of the interstate system in the 1960s, with the Black Hills becoming more important as a destination. The financial service industry began to grow in the state as well, with Citibank moving its credit card operations from New York to Sioux Falls in 1981, a move that has been followed by several other financial companies. South Dakota was the first state to eliminate caps on interest rates.

In 2007, the site of the recently closed Homestake gold mine near Lead was chosen as the location of a new underground research facility, the Deep Underground Science and Engineering Laboratory. Despite a growing state population and recent economic development, many rural areas have been struggling over the past 50 years with locally declining populations and the emigration of educated young adults to larger South Dakota cities, such as Rapid City or Sioux Falls, or to other states. Mechanization and consolidation of agriculture has contributed greatly to the declining number of smaller family farms and the resulting economic and demographic challenges facing rural towns.

The United States Census Bureau estimates the population of South Dakota was 884,659 on July 1, 2019, an 8.66% increase since the 2010 United States Census, only North Dakota, Alaska, Vermont, and Wyoming have fewer residents.

As of 2019, South Dakota had an estimated population of 884,659, an increase of 70,479, or 8.66%, since the year 2010. 7.3% of South Dakota's population was reported as under 5, 24% under 18, and 14.3% were 65 or older. Females made up approximately 50.2% of the population. As of the 2000 census, South Dakota ranked fifth-lowest in the nation in population and population density.

Of the people residing in South Dakota, 65.7% were born in South Dakota, 31.4% were born in another U.S. state, 0.6% were born in Puerto Rico, U.S. Island areas, or born abroad to American parent(s), and 2.3% were born in another country.

The center of population of South Dakota is in Buffalo County, in the unincorporated county seat of Gann Valley.

According to the 2010 Census, the racial composition of the population was:

Ethnically, 2.7% of South Dakota's population was of Hispanic, Latino, or Spanish origin (they may be of any race).

As of 2011, 25.4% of South Dakota's population younger than age 1 were minorities, meaning they had at least one parent who was not non-Hispanic white.

As of 2000, the five largest ancestry groups in South Dakota are German (40.7%), Norwegian (15.3%), Irish (10.4%), Native American (8.3%), and English (7.1%).

German Americans are the largest ancestry group in most parts of the state, especially in East River (east of the Missouri River), although there are also large Scandinavian-descended populations in some counties. South Dakota has the nation's largest population of Hutterites, a communal Anabaptist group which emigrated in 1874 from Europe, primarily from German-speaking areas.
American Indians, largely Lakota, Dakota, and Nakota (Sioux), are predominant in several counties and constitute 20 per cent of the population in West River. The seven large Indian reservations in the state occupy an area much diminished from their former Great Sioux Reservation of West River, which the federal government had once allocated to the Sioux tribes. South Dakota has the third-highest proportion of Native Americans of any state, behind Alaska and New Mexico.

Five of the state's counties are wholly within the boundaries of sovereign Indian reservations. Because of the limitations of climate and land, and isolation from urban areas with more employment opportunities, living standards on many South Dakota reservations are often far below the national average; Ziebach County ranked as the poorest county in the nation in 2009. The unemployment rate in Fort Thompson, on the Crow Creek Reservation, is 70%, and 21% of households lack plumbing or basic kitchen appliances. A 1995 study by the U.S. Census Bureau found 58% of homes on the Pine Ridge Indian Reservation did not have a telephone. The reservations' isolation also inhibits their ability to generate revenue from gaming casinos, an avenue that has proved profitable for many tribes closer to urban centers.

In 1995 the legislature passed a law to make English the "common language" of the state. Since 2019, ""the language of the Great Sioux Nation, three dialects, Dakota, Lakota, and Nakota"" is an official language. As of the 2000 census, 1.90% of the population age5 or older speak German at home, while 1.51% speak Lakota or Dakota, and 1.43% Spanish. As of 2010, 93.46% (692,504) of South Dakota residents age5 and older spoke English as their primary language. 6.54% of the population spoke a language other than English. 2.06% (15,292) of the population spoke Spanish, 1.39% (10,282) spoke Dakota, and 1.37% (10,140) spoke German. Other languages spoken included Vietnamese (0.16%), Chinese (0.12%), and Russian (0.10%).

Over the last several decades, the population in many rural areas has declined in South Dakota, in common with other Great Plains states. The change has been characterized as "rural flight" as family farming has declined. Young people have moved to cities for other employment. This trend has continued in recent years, with 30 of South Dakota's counties losing population between the 1990 and the 2000 census. During that time, nine counties had a population loss of greater than 10%, with Harding County, in the northwest corner of the state, losing nearly 19% of its population. Low birth rates and a lack of younger immigration has caused the median age of many of these counties to increase. In 24 counties, at least 20% of the population is over the age of 65, compared with a national rate of 12.8%.

The effect of rural flight has not been spread evenly through South Dakota, however. Although most rural counties and small towns have lost population, the Sioux Falls area, the larger counties along Interstate 29, the Black Hills, and many Indian reservations have all gained population. As the reservations have exercised more sovereignty, some Sioux have returned to them from urban areas. Lincoln County near Sioux Falls was the seventh fastest-growing county (by percentage) in the United States in 2010. The growth in these areas has compensated for losses in the rest of the state. South Dakota's total population continues to increase steadily, albeit at a slower rate than the national average.

The largest denominations by number of adherents in 2010 were the Roman Catholic Church with 148,883 members; the Evangelical Lutheran Church in America (ELCA) with 112,649 members; and the United Methodist Church (UMC) with 36,020 members. (The ELCA and UMC are specific denominations within the broader terms 'Lutheran' and 'Methodist', respectively.)
The results of a 2001 survey, in which South Dakotans were asked to identify their religion, include:

The current-dollar gross state product of South Dakota was $39.8 billion as of 2010, the fifth-smallest total state output in the U.S. The per capita personal income was $38,865 in 2010, ranked 25th in the U.S., and 12.5% of the population was below the poverty line in 2008.
CNBC's list of "Top States for Business for 2010" has recognized South Dakota as the seventh best state in the nation. In July 2011, the state's unemployment rate was 4.7%.

The service industry is the largest economic contributor in South Dakota. This sector includes the retail, finance, and health care industries. Citibank, which was the largest bank holding company in the United States at one time, established national banking operations in South Dakota in 1981 to take advantage of favorable banking regulations. Government spending is another important segment of the state's economy, providing over ten percent of the gross state product. Ellsworth Air Force Base, near Rapid City, is the second-largest single employer in the state.
Agriculture has historically been a key component of the South Dakota economy. Although other industries have expanded rapidly in recent decades, agricultural production is still very important to the state's economy, especially in rural areas. The five most valuable agricultural products in South Dakota are cattle, corn (maize), soybeans, wheat, and hogs. Agriculture-related industries such as meat packing and ethanol production also have a considerable economic impact on the state. South Dakota is the sixth leading ethanol-producing state in the nation.

Another important sector in South Dakota's economy is tourism. Many travel to view the attractions of the state, particularly those in the Black Hills region, such as historic Deadwood, Mount Rushmore, and the nearby state and national parks. One of the largest tourist events in the state is the annual Sturgis Motorcycle Rally. The five-day event drew over 739,000 attendees in 2015; significant considering the state has a total population of 850,000. In 2006, tourism provided an estimated 33,000 jobs in the state and contributed over two billion dollars to the economy of South Dakota.

South Dakota has of highways, roads, and streets, along with of interstate highways. Two major interstates pass through South Dakota: Interstate 90, which runs east and west through the southern half of the state; and Interstate 29, running north and south in the eastern portion of the state. The I-29 corridor features generally higher rates of population and economic growth than areas in eastern South Dakota further from the interstate.

Also in the state are the shorter Interstates 190, a spur into central Rapid City, and 229, a loop around southern and eastern Sioux Falls. Several major U.S. highways pass through the state. U.S. routes 12, 14, 16, 18 and 212 travel east and west, while U.S. routes 81, 83, 85 and 281 run north and south. South Dakota and Montana are the only states sharing a land border which is not traversed by a paved road.

South Dakota contains two National Scenic Byways. The Peter Norbeck National Scenic Byway is in the Black Hills, while the Native American Scenic Byway runs along the Missouri River in the north-central part of the state. Other scenic byways include the Badlands Loop Scenic Byway, the Spearfish Canyon Scenic Byway, and the Wildlife Loop Road Scenic Byway.

Railroads have played an important role in South Dakota transportation since the mid-19th century. Some of railroad track were built in South Dakota during the late 19th century and early 20th century, but only are active. BNSF Railway is the largest railroad in South Dakota; the Rapid City, Pierre and Eastern Railroad (formerly the Dakota, Minnesota and Eastern) is the state's other major carrier. Other state carriers include Dakota Southern Railway, Dakota and Iowa Railroad, Ellis and Eastern Railroad, Sunflour Railroad, Canadian Pacific Railway, and the Sisseton Milbank Railroad. Rail transportation in the state is mostly freight, but there are two passenger heritage railroads: the Black Hills Central and the Prairie Village, Herman, and Milwaukee. However, South Dakota is one of the two contiguous states that lack Amtrak service. (South Dakota is the only contiguous state that "never had" Amtrak—Wyoming used to be served by the San Francisco Zephyr and the Pioneer.)

South Dakota's largest commercial airports in terms of passenger traffic are the Sioux Falls Regional Airport and Rapid City Regional Airport. Delta Air Lines, Frontier Airlines, and Allegiant Airlines, as well as commuter airlines using the brand affiliation with major airlines serve the two largest airports. Several other cities in the state also have commercial air service: Aberdeen Regional Airport, Pierre Regional Airport, and Watertown Regional Airport, some of which is subsidized by the Essential Air Service program.

Like other U.S. states, the structure of the government of South Dakota follows the same separation of powers as the federal government, with executive, legislative, and judicial branches. The structure of the state government is laid out in the Constitution of South Dakota, the highest law in the state. The constitution may be amended by a majority vote of both houses of the legislature, or by voter initiative.

The Governor of South Dakota occupies the executive branch of the state government. The current governor is Kristi Noem, a Republican. The state constitution gives the governor the power to sign into law or veto bills passed by the state legislature, to serve as commander-in-chief of the South Dakota National Guard, to appoint a cabinet, and to commute criminal sentences or to pardon those convicted of crimes. The governor serves for a four-year term, and may not serve more than two consecutive terms.

The state legislature is made up of two bodies, the Senate, which has 35 members, and the House of Representatives, with 70 members. South Dakota is divided into 35 legislative districts, with voters electing two representatives and one senator per district. The legislature meets for an annual session which begins on the second Tuesday in January and lasts for 30 days; it also meets if a special session is called by the governor.

The judicial branch is made up of several levels. The state supreme court, with four justices and a chief justice, is the highest court in the state. Below the supreme court are the circuit courts; 41 circuit judges serve in seven judicial circuits in the state. Below the circuit courts are the magistrate courts, which deal with lesser criminal and civil actions.

As of 2005, South Dakota has the lowest per capita total state tax rate in the United States. The state does not levy personal or corporate income taxes, inheritance taxes, or taxes on intangible personal property. The state sales tax rate is 4.5 percent. Various localities have local levies so in some areas the rate is six percent. The state sales tax does not apply to sales to Indians on Indian reservations, but many reservations have a compact with the state. Businesses on the reservation collect the tax and the state refunds to the Indian Tribes the percentage of sales tax collections relating to the ratio of Indian population to total population in the county or area affected. Ad valorem property taxes are local taxes and are a large source of funding for school systems, counties, municipalities and other local government units. The South Dakota Special Tax Division regulates some taxes including cigarette and alcohol-related taxes.

South Dakota is represented at the federal level by Senator John Thune, Senator Mike Rounds, and Representative Dusty Johnson. All three are Republicans. South Dakota is one of seven states with only one seat in the U.S. House of Representatives. In United States presidential elections, South Dakota is allotted three of 538 votes in the Electoral College. As in all other states except Maine and neighboring Nebraska, South Dakota's electoral votes are granted in a winner-take-all system.

South Dakota politics are generally dominated by the Republican Party. Since statehood, Republicans have carried the state's electoral votes in all but five presidential elections: 1896, 1912 (by Theodore Roosevelt's Progressive Party), 1932, 1936 and 1964. (Not to mention Democrat George McGovern—a native South Dakotan—who failed in 1972.) Only Alaska has been carried fewer times by a Democrat. Additionally, a Democrat has not won the governorship since 1974. As of 2016, Republicans hold a 15% voter registration advantage over Democrats and hold large majorities in both the state House and the state Senate.

Despite the state's general Republican and conservative leanings, Democrats have found success in various statewide elections, most notably in those involving South Dakota's congressional representatives in Washington. American Indians have been becoming more active in state and county electoral politics. In the 2002 election, American Indian voting carried Tim Johnson as the Democratic candidate by a margin of 532 votes. Until his electoral defeat in 2004, Senator Tom Daschle was the Senate minority leader (and briefly its majority leader during Democratic control of the Senate in 2001–02).

In 2016, South Dakota voted for Republican nominee Donald Trump over Democratic nominee Hillary Clinton by a margin of 30%. In 2018, Republican congresswoman Kristi Noem defeated Democrat Billie Sutton in the gubernatorial election, and Republican Dusty Johnson defeated Democrat Tim Bjorkman for the state's at-large seat in the U.S. House of Representatives.

Contemporary political issues in South Dakota include the costs and benefits of the state lottery, South Dakota's relatively low rankings in education spending (particularly teacher pay—recently the State Sales Tax was increased from 4% to 4.5% to finance an increase in teacher pay), and recent legislative and electoral attempts to ban abortion in the state.

A Republican supported bill passed in March 2019 requires that all public schools display "In God We Trust" in a prominent location.

South Dakota's culture reflects the state's American Indian, rural, Western, and European roots. A number of annual events celebrating the state's ethnic and historical heritage take place around the state, such as Days of '76 in Deadwood, Czech Days in Tabor, and the annual St. Patrick's Day and Cinco de Mayo festivities in Sioux Falls. The various tribes hold many annual pow wows at their reservations throughout the state, to which non-Native Americans are sometimes invited. Custer State Park holds an annual Buffalo Roundup, in which volunteers on horseback gather the park's herd of around 1,500 bison.

Black Elk (Lakota) was a medicine man and heyokha, whose life spanned the transition to reservations. His accounts of the 19th-century Indian Wars and Ghost Dance movement, and his deep thoughts on personal visions and Native American religion, form the basis of the book "Black Elk Speaks", first published in 1932. (Among several editions, a premier annotated edition was published in 2008.) Paul Goble, an award-winning children's book author and illustrator, has been based in the Black Hills since 1977.

Laura Ingalls Wilder, whose semi-autobiographical books are based on her experiences as a child and young adult on the frontier, is one of South Dakota's best-known writers. She drew from her life growing up on a homestead near De Smet as the basis for five of her novels: "By the Shores of Silver Lake", "The Long Winter", "Little Town on the Prairie", "These Happy Golden Years", and "The First Four Years". These gained renewed popularity in the United States when "Little House on the Prairie" was adapted and produced as a television series in 1974. Wilder's daughter, Rose Wilder Lane, who became a well-known writer in her own right, was born near De Smet in 1886.

South Dakota has also produced several notable artists. Harvey Dunn grew up on a homestead near Manchester in the late 19th century. While Dunn worked most of his career as a commercial illustrator, his most famous works showed various scenes of frontier life; he completed these near the end of his career. Oscar Howe (Crow) was born on the Crow Creek Indian Reservation and won fame for his watercolor paintings. Howe was one of the first Native American painters to adopt techniques and style heavily influenced by the mid-20th century abstraction movement, rather than relying on traditional Native American styles. Terry Redlin, originally from Watertown, is an accomplished painter of rural and wildlife scenes. Many of his works are on display at the Redlin Art Center in Watertown.

Sioux Falls is the largest city in South Dakota, with a 2010 population of 153,888,
and a metropolitan area population of 238,122.
The city, founded in 1856, is in the southeast corner of the state. Retail, finance, and healthcare have assumed greater importance in Sioux Falls, where the economy was originally centered on agri-business and quarrying.

Rapid City, with a 2010 population of 67,956, and a metropolitan area population of 124,766, is the second-largest city in the state. It is on the eastern edge of the Black Hills, and was founded in 1876. Rapid City's economy is largely based on tourism and defense spending, because of the proximity of many tourist attractions in the Black Hills and Ellsworth Air Force Base.

The next eight largest cities in the state, in order of descending 2010 population, are Aberdeen (26,091), Brookings (22,056), Watertown (21,482), Mitchell (15,254), Yankton (14,454), Pierre (13,646), Huron (12,592), and Vermillion (10,571). Pierre is the state capital, and Brookings and Vermillion are the locations of the state's two largest universities (South Dakota State University and University of South Dakota, respectively). With a population of about 14,000, Pierre is the second smallest state capital in the United States. Of the ten largest cities in the state, only Rapid City is west of the Missouri River.

South Dakota's first newspaper, the "Dakota Democrat", began publishing in Yankton in 1858. Today, the state's largest newspaper is the Sioux Falls "Argus Leader", with a Sunday circulation of 63,701 and a weekday circulation of 44,334. The "Rapid City Journal", with a Sunday circulation of 32,638 and a weekday circulation of 27,827, is South Dakota's second largest newspaper. The next four largest newspapers in the state are the Aberdeen "American News", the "Watertown Public Opinion", the "Huron Plainsman", and the "Brookings Register". In 1981, Tim Giago founded the "Lakota Times" as a newspaper for the local American Indian community on the Pine Ridge Indian Reservation. The newspaper, now published in New York and known as "Indian Country Today", is available in every state in the country. The "Sioux City Journal" also covers parts of South Dakota.

There are nine television stations broadcasting in South Dakota; South Dakota Public Television broadcasts from a number of locations around the state, while the other stations broadcast from Sioux Falls or Rapid City. The two largest television media markets in South Dakota are Sioux Falls-Mitchell, with a viewership of 246,020, and Rapid City, with a viewership of 91,070. The two markets rank as 114th and 177th largest in the United States, respectively. The state's first television station, KELO-TV, began airing in Sioux Falls in 1953. Among KELO's early programs was "Captain 11", an afternoon children's program. "Captain 11" ran from 1955 until 1996, making it the nation's longest continuously running children's television program.

A number of South Dakotans are famous for their work in television and publishing. Former NBC Nightly News anchor and author Tom Brokaw is from Webster and Yankton, "USA Today" founder Al Neuharth was from Eureka and Alpena, gameshow host Bob Barker spent much of his childhood in Mission, and entertainment news hosts Pat O'Brien and Mary Hart are from Sioux Falls.

As of 2006, South Dakota has a total primary and secondary school enrollment of 136,872, with 120,278 of these students being educated in the public school system. There are 703 public schools in 168 school districts, giving South Dakota the highest number of schools per capita in the United States. The current high school graduation rate is 89.9%, and the average ACT score is 21.8, slightly above the national average of 21.1. 89.8% of the adult population has earned at least a high school diploma, and 25.8% has earned a bachelor's degree or higher. South Dakota's 2008 average public school teacher salary of $36,674 was the lowest in the nation (national average was $52,308). In 2007 South Dakota passed legislation modeled after Montana's Indian Education for All Act (1999), mandating education about Native American tribal history, culture, and heritage in all the schools, from pre-school through college, in an effort to increase knowledge and appreciation about Indian culture among all residents of the state, as well as to reinforce Indian students' understanding of their own cultures' contributions.

The South Dakota Board of Regents, whose members are appointed by the governor, controls the six public universities in the state. South Dakota State University (SDSU), in Brookings, is the state's largest university, with an enrollment of 12,831. The University of South Dakota (USD), in Vermillion, is the state's oldest university, and has South Dakota's only law school and medical school. South Dakota also has several private universities, the largest of which is Augustana University in Sioux Falls.

Because of its low population, South Dakota does not host any major league professional sports franchises. The state has minor league and independent league teams, all of which play in Sioux Falls or Rapid City. Sioux Falls is home to four teams: the Sioux Falls Canaries (baseball), the Sioux Falls Skyforce (basketball), the Sioux Falls Stampede (hockey), and the Sioux Falls Storm (indoor American football). The Canaries play in the American Association, and their home field is Sioux Falls Stadium. The Skyforce play in the NBA G League, and are owned by the NBA's Miami Heat. They play at the Sanford Pentagon. The Stampede and Storm share the Denny Sanford Premier Center. The Stampede play in the USHL, and the Storm play in the IFL. Rapid City has a hockey team named the Rapid City Rush that plays in the ECHL. The Rush began their inaugural season in 2008 at the Rushmore Plaza Civic Center.

Universities in South Dakota host a variety of sports programs. For many years, South Dakota was one of the only states in the country without an NCAA DivisionI football or basketball team. However, several years ago SDSU decided to move their teams from DivisionII to DivisionI, a move followed by the University of South Dakota. Other universities in the state compete at the NCAA's Division II or III levels, or in the NAIA.

Famous South Dakota athletes include Billy Mills, Mike Miller, Mark Ellis, Becky Hammon, Brock Lesnar, Chad Greenway, and Adam Vinatieri. Mills is from the town of Pine Ridge and competed at the 1964 Summer Olympic Games in Tokyo, becoming the only American to win a gold medal in the 10,000-meter event. Miller, of Mitchell, is a two-time NBA champion who played college basketball at the University of Florida, leading them to the 2000 NCAA Championship game his sophomore year, and won the 2001 NBA rookie of the year award. Ellis, of Rapid City, played for the University of Florida and four MLB teams before retiring in 2015. Hammon, of Rapid City, played for the WNBA's New York Liberty and San Antonio Silver Stars before becoming an assistant coach for the NBA's San Antonio Spurs in 2014. Lesnar, of Webster, is a former heavy-weight champion in the UFC and WWE. Vinatieri is an NFL placekicker who grew up in Rapid City and attended SDSU.

Fishing and hunting are popular outdoor activities in South Dakota. Fishing contributes over $224 million to South Dakota's economy, and hunting contributes over $303 million. In 2007, over 275,000 hunting licences and 175,000 fishing licences were sold in the state; around half of the hunting licences and over two-thirds of the fishing licences were purchased by South Dakotans. Popular species of game include pheasants, white-tailed deer, mule deer, and turkeys, as well as waterfowl such as Canada geese, snow geese, and mallards. Targets of anglers include walleye in the eastern glacial lakes and Missouri River reservoirs, Chinook salmon in Lake Oahe, and trout in the Black Hills.

Other sports, such as cycling and running, are also popular in the state. In 1991, the state opened the George S. Mickelson Trail, a rail trail in the Black Hills. Besides being used by cyclists, the trail is also the site of a portion of the annual Mount Rushmore marathon; the marathon's entire course is at an elevation of over 4,000 feet (1,200 m). Other events in the state include the Tour de Kota, a , six-day cycling event that covers much of eastern and central South Dakota, and the annual Sturgis Motorcycle Rally, which draws hundreds of thousands of participants from around the United States.

Some of South Dakota's official state symbols include:





</doc>
<doc id="26748" url="https://en.wikipedia.org/wiki?curid=26748" title="Switzerland">
Switzerland

Switzerland, officially the Swiss Confederation, is a country situated in the confluence of Western, Central, and Southern Europe. It is a federal republic composed of 26 cantons, with federal authorities based in Bern. Switzerland is a landlocked country bordered by Italy to the south, France to the west, Germany to the north, and Austria and Liechtenstein to the east. It is geographically divided among the Swiss Plateau, the Alps, and the Jura, spanning a total area of , and land area of . While the Alps occupy the greater part of the territory, the Swiss population of approximately 8.5 million is concentrated mostly on the plateau, where the largest cities and economic centres are located, among them Zürich, Geneva and Basel, where multiple international organisations are domiciled (such as FIFA, the UN's second-largest Office, and the Bank for International Settlements) and where the main international airports of Switzerland are.

The establishment of the Old Swiss Confederacy dates to the late medieval period, resulting from a series of military successes against Austria and Burgundy. Swiss independence from the Holy Roman Empire was formally recognized in the Peace of Westphalia in 1648. The Federal Charter of 1291 is considered the founding document of Switzerland which is celebrated on Swiss National Day. Since the Reformation of the 16th century, Switzerland has maintained a strong policy of armed neutrality; it has not fought an international war since 1815 and did not join the United Nations until 2002. Nevertheless, it pursues an active foreign policy and is frequently involved in peace-building processes around the world. Switzerland is the birthplace of the Red Cross, one of the world's oldest and best known humanitarian organisations, and is home to numerous international organisations, including the United Nations Office at Geneva, which is its second-largest in the world. It is a founding member of the European Free Trade Association, but notably not part of the European Union, the European Economic Area or the Eurozone. However, it participates in the Schengen Area and the European Single Market through bilateral treaties.

Switzerland occupies the crossroads of Germanic and Romance Europe, as reflected in its four main linguistic and cultural regions: German, French, Italian and Romansh. Although the majority of the population are German-speaking, Swiss national identity is rooted in a common historical background, shared values such as federalism and direct democracy, and Alpine symbolism. Due to its linguistic diversity, Switzerland is known by a variety of native names: "Schweiz" (German); "Suisse" (French); "Svizzera" (Italian); and "Svizra" (Romansh). On coins and stamps, the Latin name, "Confoederatio Helvetica" – frequently shortened to "Helvetia" – is used instead of the four national languages.

The sovereign state is one of the most developed countries in the world, with the highest nominal wealth per adult and the eighth-highest per capita gross domestic product. It ranks at or near the top in several international metrics, including economic competitiveness and human development. Zürich, Geneva and Basel have been ranked among the top ten cities in the world in terms of quality of life, with Zürich ranked second globally. In 2019, IMD placed Switzerland first in attracting skilled workers. World Economic Forum ranks it the 5th most competitive country globally.

The English name "Switzerland" is a compound containing "Switzer", an obsolete term for the Swiss, which was in use during the 16th to 19th centuries. The English adjective "Swiss" is a loan from French ', also in use since the 16th century. The name "Switzer" is from the Alemannic ', in origin an inhabitant of "Schwyz" and its associated territory, one of the Waldstätte cantons which formed the nucleus of the Old Swiss Confederacy. The Swiss began to adopt the name for themselves after the Swabian War of 1499, used alongside the term for "Confederates", "Eidgenossen" (literally: "comrades by oath"), used since the 14th century. The data code for Switzerland, CH, is derived from Latin "Confoederatio Helvetica" ().

The toponym "Schwyz" itself was first attested in 972, as Old High German ', ultimately perhaps related to ' ‘to burn’ (cf. Old Norse "svíða" ‘to singe, burn’), referring to the area of forest that was burned and cleared to build. The name was extended to the area dominated by the canton, and after the Swabian War of 1499 gradually came to be used for the entire Confederation.
The Swiss German name of the country, ', is homophonous to that of the canton and the settlement, but distinguished by the use of the definite article (' for the Confederation, but simply "" for the canton and the town). The long [iː] of Swiss German is historically and still often today spelled rather than , preserving the original identity of the two names even in writing.

The Latin name "Confoederatio Helvetica" was neologized and introduced gradually after the formation of the federal state in 1848, harking back to the Napoleonic Helvetic Republic, appearing on coins from 1879, inscribed on the Federal Palace in 1902 and after 1948 used in the official seal. (for example, the ISO banking code "CHF" for the Swiss franc, and the country top-level domain ".ch", are both taken from the state's Latin name). "Helvetica" is derived from the "Helvetii", a Gaulish tribe living on the Swiss plateau before the Roman era.

"Helvetia" appears as a national personification of the Swiss confederacy in the 17th century with a 1672 play by Johann Caspar Weissenbach.

Switzerland has existed as a state in its present form since the adoption of the Swiss Federal Constitution in 1848. The precursors of Switzerland established a protective alliance at the end of the 13th century (1291), forming a loose confederation of states which persisted for centuries.

The oldest traces of hominid existence in Switzerland date back about 150,000 years. The oldest known farming settlements in Switzerland, which were found at Gächlingen, have been dated to around 5300 BC.

The earliest known cultural tribes of the area were members of the Hallstatt and La Tène cultures, named after the archaeological site of La Tène on the north side of Lake Neuchâtel. La Tène culture developed and flourished during the late Iron Age from around 450 BC, possibly under some influence from the Greek and Etruscan civilisations. One of the most important tribal groups in the Swiss region was the Helvetii. Steadily harassed by the Germanic tribes, in 58 BC the Helvetii decided to abandon the Swiss plateau and migrate to western Gallia, but Julius Caesar's armies pursued and defeated them at the Battle of Bibracte, in today's eastern France, forcing the tribe to move back to its original homeland. In 15 BC, Tiberius, who would one day become the second Roman emperor, and his brother Drusus, conquered the Alps, integrating them into the Roman Empire. The area occupied by the Helvetii—the namesakes of the later "Confoederatio Helvetica"—first became part of Rome's Gallia Belgica province and then of its Germania Superior province, while the eastern portion of modern Switzerland was integrated into the Roman province of Raetia. Sometime around the start of the Common Era, the Romans maintained a large legionary camp called Vindonissa, now a ruin at the confluence of the Aare and Reuss rivers, near the town of Windisch, an outskirt of Brugg.

The first and second century AD was an age of prosperity for the population living on the Swiss plateau. Several towns, like Aventicum, Iulia Equestris and Augusta Raurica, reached a remarkable size, while hundreds of agricultural estates (Villae rusticae) were founded in the countryside.

Around 260 AD, the fall of the Agri Decumates territory north of the Rhine transformed today's Switzerland into a frontier land of the Empire. Repeated raids by the Alamanni tribes provoked the ruin of the Roman towns and economy, forcing the population to find shelter near Roman fortresses, like the Castrum Rauracense near Augusta Raurica. The Empire built another line of defence at the north border (the so-called Donau-Iller-Rhine-Limes), but at the end of the fourth century the increased Germanic pressure forced the Romans to abandon the linear defence concept, and the Swiss plateau was finally open to the settlement of Germanic tribes.

In the Early Middle Ages, from the end of the 4th century, the western extent of modern-day Switzerland was part of the territory of the Kings of the Burgundians. The Alemanni settled the Swiss plateau in the 5th century and the valleys of the Alps in the 8th century, forming Alemannia. Modern-day Switzerland was therefore then divided between the kingdoms of Alemannia and Burgundy. The entire region became part of the expanding Frankish Empire in the 6th century, following Clovis I's victory over the Alemanni at Tolbiac in 504 AD, and later Frankish domination of the Burgundians.

Throughout the rest of the 6th, 7th and 8th centuries the Swiss regions continued under Frankish hegemony (Merovingian and Carolingian dynasties). But after its extension under Charlemagne, the Frankish Empire was divided by the Treaty of Verdun in 843. The territories of present-day Switzerland became divided into Middle Francia and East Francia until they were reunified under the Holy Roman Empire around 1000 AD.

By 1200, the Swiss plateau comprised the dominions of the houses of Savoy, Zähringer, Habsburg, and Kyburg. Some regions (Uri, Schwyz, Unterwalden, later known as "Waldstätten") were accorded the Imperial immediacy to grant the empire direct control over the mountain passes. With the extinction of its male line in 1263 the Kyburg dynasty fell in AD 1264; then the Habsburgs under King Rudolph I (Holy Roman Emperor in 1273) laid claim to the Kyburg lands and annexed them extending their territory to the eastern Swiss plateau.

A female who died in about 200 BC was found buried in a carved tree trunk during a construction project at the Kern school complex in March 2017 in Aussersihl. Archaeologists revealed that she was approximately 40 years old when she died and likely carried out little physical labor when she was alive. A sheepskin coat, a belt chain, a fancy wool dress, a scarf and a pendant made of glass and amber beads were also discovered with the woman.

The Old Swiss Confederacy was an alliance among the valley communities of the central Alps. The Confederacy, governed by nobles and patricians of various cantons, facilitated management of common interests and ensured peace on the important mountain trade routes. The Federal Charter of 1291 agreed between the rural communes of Uri, Schwyz, and Unterwalden is considered the confederacy's founding document, even though similar alliances are likely to have existed decades earlier.

By 1353, the three original cantons had joined with the cantons of Glarus and Zug and the Lucerne, Zürich and Bern city states to form the "Old Confederacy" of eight states that existed until the end of the 15th century. The expansion led to increased power and wealth for the confederation. By 1460, the confederates controlled most of the territory south and west of the Rhine to the Alps and the Jura mountains, particularly after victories against the Habsburgs (Battle of Sempach, Battle of Näfels), over Charles the Bold of Burgundy during the 1470s, and the success of the Swiss mercenaries. The Swiss victory in the Swabian War against the Swabian League of Emperor Maximilian I in 1499 amounted to "de facto" independence within the Holy Roman Empire. In 1501, Basel and Schaffhausen joined the Old Swiss Confederacy.

The Old Swiss Confederacy had acquired a reputation of invincibility during these earlier wars, but expansion of the confederation suffered a setback in 1515 with the Swiss defeat in the Battle of Marignano. This ended the so-called "heroic" epoch of Swiss history. The success of Zwingli's Reformation in some cantons led to inter-cantonal religious conflicts in 1529 and 1531 (Wars of Kappel). It was not until more than one hundred years after these internal wars that, in 1648, under the Peace of Westphalia, European countries recognised Switzerland's independence from the Holy Roman Empire and its neutrality.

During the Early Modern period of Swiss history, the growing authoritarianism of the patriciate families combined with a financial crisis in the wake of the Thirty Years' War led to the Swiss peasant war of 1653. In the background to this struggle, the conflict between Catholic and Protestant cantons persisted, erupting in further violence at the First War of Villmergen, in 1656, and the Toggenburg War (or Second War of Villmergen), in 1712.

In 1798, the revolutionary French government conquered Switzerland and imposed a new unified constitution. This centralised the government of the country, effectively abolishing the cantons: moreover, Mülhausen joined France and the Valtellina valley became part of the Cisalpine Republic, separating from Switzerland. The new regime, known as the Helvetic Republic, was highly unpopular. It had been imposed by a foreign invading army and destroyed centuries of tradition, making Switzerland nothing more than a French satellite state. The fierce French suppression of the Nidwalden Revolt in September 1798 was an example of the oppressive presence of the French Army and the local population's resistance to the occupation.

When war broke out between France and its rivals, Russian and Austrian forces invaded Switzerland. The Swiss refused to fight alongside the French in the name of the Helvetic Republic. In 1803 Napoleon organised a meeting of the leading Swiss politicians from both sides in Paris. The result was the Act of Mediation which largely restored Swiss autonomy and introduced a Confederation of 19 cantons. Henceforth, much of Swiss politics would concern balancing the cantons' tradition of self-rule with the need for a central government.

In 1815 the Congress of Vienna fully re-established Swiss independence and the European powers agreed to permanently recognise Swiss neutrality. Swiss troops still served foreign governments until 1860 when they fought in the Siege of Gaeta. The treaty also allowed Switzerland to increase its territory, with the admission of the cantons of Valais, Neuchâtel and Geneva. Switzerland's borders have not changed since, except for some minor adjustments.

The restoration of power to the patriciate was only temporary. After a period of unrest with repeated violent clashes, such as the Züriputsch of 1839, civil war (the "Sonderbundskrieg") broke out in 1847 when some Catholic cantons tried to set up a separate alliance (the "Sonderbund"). The war lasted for less than a month, causing fewer than 100 casualties, most of which were through friendly fire. Yet however minor the Sonderbundskrieg appears compared with other European riots and wars in the 19th century, it nevertheless had a major impact on both the psychology and the society of the Swiss and of Switzerland.

The war convinced most Swiss of the need for unity and strength towards its European neighbours. Swiss people from all strata of society, whether Catholic or Protestant, from the liberal or conservative current, realised that the cantons would profit more if their economic and religious interests were merged.

Thus, while the rest of Europe saw revolutionary uprisings, the Swiss drew up a constitution which provided for a federal layout, much of it inspired by the American example. This constitution provided for a central authority while leaving the cantons the right to self-government on local issues. Giving credit to those who favoured the power of the cantons (the Sonderbund Kantone), the national assembly was divided between an upper house (the Council of States, two representatives per canton) and a lower house (the National Council, with representatives elected from across the country). Referendums were made mandatory for any amendment of this constitution. This new constitution also brought a legal end to nobility in Switzerland.
A system of single weights and measures was introduced and in 1850 the Swiss franc became the Swiss single currency. Article 11 of the constitution forbade sending troops to serve abroad, with the exception of serving the Holy See, though the Swiss were still obliged to serve Francis II of the Two Sicilies with Swiss Guards present at the Siege of Gaeta in 1860, marking the end of foreign service.

An important clause of the constitution was that it could be re-written completely if this was deemed necessary, thus enabling it to evolve as a whole rather than being modified one amendment at a time.

This need soon proved itself when the rise in population and the Industrial Revolution that followed led to calls to modify the constitution accordingly. An early draft was rejected by the population in 1872 but modifications led to its acceptance in 1874. It introduced the facultative referendum for laws at the federal level. It also established federal responsibility for defence, trade, and legal matters.

In 1891, the constitution was revised with unusually strong elements of direct democracy, which remain unique even today.

Switzerland was not invaded during either of the world wars. During World War I, Switzerland was home to Vladimir Illych Ulyanov (Vladimir Lenin) and he remained there until 1917. Swiss neutrality was seriously questioned by the Grimm–Hoffmann affair in 1917, but that was short-lived. In 1920, Switzerland joined the League of Nations, which was based in Geneva, on condition that it was exempt from any military requirements.

During World War II, detailed invasion plans were drawn up by the Germans, but Switzerland was never attacked. Switzerland was able to remain independent through a combination of military deterrence, concessions to Germany, and good fortune as larger events during the war delayed an invasion. Under General Henri Guisan, appointed the commander-in-chief for the duration of the war, a general mobilisation of the armed forces was ordered. The Swiss military strategy was changed from one of static defence at the borders to protect the economic heartland, to one of organised long-term attrition and withdrawal to strong, well-stockpiled positions high in the Alps known as the Reduit. Switzerland was an important base for espionage by both sides in the conflict and often mediated communications between the Axis and Allied powers.

Switzerland's trade was blockaded by both the Allies and by the Axis. Economic cooperation and extension of credit to the Third Reich varied according to the perceived likelihood of invasion and the availability of other trading partners. Concessions reached a peak after a crucial rail link through Vichy France was severed in 1942, leaving Switzerland (together with Liechtenstein) entirely isolated from the wider world by Axis controlled territory. Over the course of the war, Switzerland interned over 300,000 refugees and the International Red Cross, based in Geneva, played an important part during the conflict. Strict immigration and asylum policies as well as the financial relationships with Nazi Germany raised controversy, but not until the end of the 20th century.

During the war, the Swiss Air Force engaged aircraft of both sides, shooting down 11 intruding Luftwaffe planes in May and June 1940, then forcing down other intruders after a change of policy following threats from Germany. Over 100 Allied bombers and their crews were interned during the war. Between 1940 and 1945, Switzerland was bombed by the Allies causing fatalities and property damage. Among the cities and towns bombed were Basel, Brusio, Chiasso, Cornol, Geneva, Koblenz, Niederweningen, Rafz, Renens, Samedan, Schaffhausen, Stein am Rhein, Tägerwilen, Thayngen, Vals, and Zürich. Allied forces explained the bombings, which violated the 96th Article of War, resulted from navigation errors, equipment failure, weather conditions, and errors made by bomber pilots. The Swiss expressed fear and concern that the bombings were intended to put pressure on Switzerland to end economic cooperation and neutrality with Nazi Germany. Court-martial proceedings took place in England and the U.S. Government paid 62,176,433.06 in Swiss francs for reparations of the bombings.

Switzerland's attitude towards refugees was complicated and controversial; over the course of the war it admitted as many as 300,000 refugees while refusing tens of thousands more, including Jews who were severely persecuted by the Nazis.

After the war, the Swiss government exported credits through the charitable fund known as the Schweizerspende and also donated to the Marshall Plan to help Europe's recovery, efforts that ultimately benefited the Swiss economy.

During the Cold War, Swiss authorities considered the construction of a Swiss nuclear bomb. Leading nuclear physicists at the Federal Institute of Technology Zürich such as Paul Scherrer made this a realistic possibility. In 1988, the Paul Scherrer Institute was founded in his name to explore the therapeutic uses of neutron scattering technologies. Financial problems with the defence budget and ethical considerations prevented the substantial funds from being allocated, and the Nuclear Non-Proliferation Treaty of 1968 was seen as a valid alternative. All remaining plans for building nuclear weapons were dropped by 1988.

Switzerland was the last Western republic to grant women the right to vote. Some Swiss cantons approved this in 1959, while at the federal level it was achieved in 1971 and, after resistance, in the last canton Appenzell Innerrhoden (one of only two remaining "Landsgemeinde", along with Glarus) in 1990. After obtaining suffrage at the federal level, women quickly rose in political significance, with the first woman on the seven member Federal Council executive being Elisabeth Kopp, who served from 1984 to 1989, and the first female president being Ruth Dreifuss in 1999.

Switzerland joined the Council of Europe in 1963. In 1979 areas from the canton of Bern attained independence from the Bernese, forming the new canton of Jura. On 18 April 1999 the Swiss population and the cantons voted in favour of a completely revised federal constitution.

In 2002 Switzerland became a full member of the United Nations, leaving the Vatican City as the last widely recognised state without full UN membership. Switzerland is a founding member of the EFTA, but is not a member of the European Economic Area. An application for membership in the European Union was sent in May 1992, but not advanced since the EEA was rejected in December 1992 when Switzerland was the only country to launch a referendum on the EEA. There have since been several referendums on the EU issue; due to opposition from the citizens, the membership application has been withdrawn. Nonetheless, Swiss law is gradually being adjusted to conform with that of the EU, and the government has signed a number of bilateral agreements with the European Union. Switzerland, together with Liechtenstein, has been completely surrounded by the EU since Austria's entry in 1995. On 5 June 2005, Swiss voters agreed by a 55% majority to join the Schengen treaty, a result that was regarded by EU commentators as a sign of support by Switzerland, a country that is traditionally perceived as independent and reluctant to enter supranational bodies.

Extending across the north and south side of the Alps in west-central Europe, Switzerland encompasses a great diversity of landscapes and climates on a limited area of . The population is about 8 million, resulting in an average population density of around 195 people per square kilometre (500/sq mi). The more mountainous southern half of the country is far more sparsely populated than the northern half. In the largest Canton of Graubünden, lying entirely in the Alps, population density falls to 27 /km² (70 /sq mi).

Switzerland lies between latitudes 45° and 48° N, and longitudes 5° and 11° E. It contains three basic topographical areas: the Swiss Alps to the south, the Swiss Plateau or Central Plateau, and the Jura mountains on the west. The Alps are a high mountain range running across the central-south of the country, constituting about 60% of the country's total area. The majority of the Swiss population live in the Swiss Plateau. Among the high valleys of the Swiss Alps many glaciers are found, totalling an area of . From these originate the headwaters of several major rivers, such as the Rhine, Inn, Ticino and Rhône, which flow in the four cardinal directions into the whole of Europe. The hydrographic network includes several of the largest bodies of freshwater in Central and Western Europe, among which are included Lake Geneva (also called le Lac Léman in French), Lake Constance (known as Bodensee in German) and Lake Maggiore. Switzerland has more than 1500 lakes, and contains 6% of Europe's stock of fresh water. Lakes and glaciers cover about 6% of the national territory. The largest lake is Lake Geneva, in western Switzerland shared with France. The Rhône is both the main source and outflow of Lake Geneva. Lake Constance is the second largest Swiss lake and, like the Lake Geneva, an intermediate step by the Rhine at the border to Austria and Germany. While the Rhône flows into the Mediterranean Sea at the French Camargue region and the Rhine flows into the North Sea at Rotterdam in the Netherlands, about apart, both springs are only about apart from each other in the Swiss Alps.

Forty-eight of Switzerland's mountains are above sea in altitude or higher. At , Monte Rosa is the highest, although the Matterhorn () is often regarded as the most famous. Both are located within the Pennine Alps in the canton of Valais, on the border with Italy. The section of the Bernese Alps above the deep glacial Lauterbrunnen valley, containing 72 waterfalls, is well known for the Jungfrau () Eiger and Mönch, and the many picturesque valleys in the region. In the southeast the long Engadin Valley, encompassing the St. Moritz area in canton of Graubünden, is also well known; the highest peak in the neighbouring Bernina Alps is Piz Bernina ().

The more populous northern part of the country, constituting about 30% of the country's total area, is called the Swiss Plateau. It has greater open and hilly landscapes, partly forested, partly open pastures, usually with grazing herds, or vegetables and fruit fields, but it is still hilly. There are large lakes found here and the biggest Swiss cities are in this area of the country.

Within Switzerland there are two small enclaves: Büsingen belongs to Germany, Campione d'Italia belongs to Italy. Switzerland has no exclaves in other countries.

The Swiss climate is generally temperate, but can vary greatly between the localities, from glacial conditions on the mountaintops to the often pleasant near Mediterranean climate at Switzerland's southern tip. There are some valley areas in the southern part of Switzerland where some cold-hardy palm trees are found. Summers tend to be warm and humid at times with periodic rainfall so they are ideal for pastures and grazing. The less humid winters in the mountains may see long intervals of stable conditions for weeks, while the lower lands tend to suffer from inversion, during these periods, thus seeing no sun for weeks.

A weather phenomenon known as the föhn (with an identical effect to the chinook wind) can occur at all times of the year and is characterised by an unexpectedly warm wind, bringing air of very low relative humidity to the north of the Alps during rainfall periods on the southern face of the Alps. This works both ways across the alps but is more efficient if blowing from the south due to the steeper step for oncoming wind from the south. Valleys running south to north trigger the best effect.
The driest conditions persist in all inner alpine valleys that receive less rain because arriving clouds lose a lot of their content while crossing the mountains before reaching these areas. Large alpine areas such as Graubünden remain drier than pre-alpine areas and as in the main valley of the Valais wine grapes are grown there.

The wettest conditions persist in the high Alps and in the Ticino canton which has much sun yet heavy bursts of rain from time to time. Precipitation tends to be spread moderately throughout the year with a peak in summer. Autumn is the driest season, winter receives less precipitation than summer, yet the weather patterns in Switzerland are not in a stable climate system and can be variable from year to year with no strict and predictable periods.

Switzerland's ecosystems can be particularly fragile, because the many delicate valleys separated by high mountains often form unique ecologies. The mountainous regions themselves are also vulnerable, with a rich range of plants not found at other altitudes, and experience some pressure from visitors and grazing. The climatic, geological and topographical conditions of the alpine region make for a very fragile ecosystem that is particularly sensitive to climate change. Nevertheless, according to the 2014 Environmental Performance Index, Switzerland ranks first among 132 nations in safeguarding the environment, due to its high scores on environmental public health, its heavy reliance on renewable sources of energy (hydropower and geothermal energy), and its control of greenhouse gas emissions.

However, access to biocapacity in Switzerland is far lower than world average. In 2016, Switzerland had 1.0 global hectares of biocapacity per person within its territory, 40 percent less than world average of 1.6 global hectares per person. In contrast, in 2016, they used 4.6 global hectares of biocapacity - their ecological footprint of consumption. This means they used about 4.6 times as much biocapacity as Switzerland contains. The remainder comes from imports and overusing the global commons (such as the atmosphere through greenhouse gas emissions). As a result, Switzerland is running a biocapacity deficit.

The Federal Constitution adopted in 1848 is the legal foundation of the modern federal state. A new Swiss Constitution was adopted in 1999, but did not introduce notable changes to the federal structure. It outlines basic and political rights of individuals and citizen participation in public affairs, divides the powers between the Confederation and the cantons and defines federal jurisdiction and authority. There are three main governing bodies on the federal level: the bicameral parliament (legislative), the Federal Council (executive) and the Federal Court (judicial).

The Swiss Parliament consists of two houses: the Council of States which has 46 representatives (two from each canton and one from each half-canton) who are elected under a system determined by each canton, and the National Council, which consists of 200 members who are elected under a system of proportional representation, depending on the population of each canton. Members of both houses serve for 4 years and only serve as members of parliament part-time (so-called "Milizsystem" or citizen legislature). When both houses are in joint session, they are known collectively as the Federal Assembly. Through referendums, citizens may challenge any law passed by parliament and through initiatives, introduce amendments to the federal constitution, thus making Switzerland a direct democracy.

The Federal Council constitutes the federal government, directs the federal administration and serves as collective Head of State. It is a collegial body of seven members, elected for a four-year mandate by the Federal Assembly which also exercises oversight over the Council. The President of the Confederation is elected by the Assembly from among the seven members, traditionally in rotation and for a one-year term; the President chairs the government and assumes representative functions. However, the president is a "primus inter pares" with no additional powers, and remains the head of a department within the administration.

The Swiss government has been a coalition of the four major political parties since 1959, each party having a number of seats that roughly reflects its share of electorate and representation in the federal parliament.
The classic distribution of 2 CVP/PDC, 2 SPS/PSS, 2 FDP/PRD and 1 SVP/UDC as it stood from 1959 to 2003 was known as the "magic formula". Following the 2015 Federal Council elections, the seven seats in the Federal Council were distributed as follows:

The function of the Federal Supreme Court is to hear appeals against rulings of cantonal or federal courts. The judges are elected by the Federal Assembly for six-year terms.

Direct democracy and federalism are hallmarks of the Swiss political system. Swiss citizens are subject to three legal jurisdictions: the municipality, canton and federal levels. The 1848 and 1999 Swiss Constitutions define a system of direct democracy (sometimes called half-direct or representative direct democracy because it is aided by the more commonplace institutions of a representative democracy). The instruments of this system at the federal level, known as popular rights (, , ), include the right to submit a federal initiative and a referendum, both of which may overturn parliamentary decisions.

By calling a federal referendum, a group of citizens may challenge a law passed by parliament, if they gather 50,000 signatures against the law within 100 days. If so, a national vote is scheduled where voters decide by a simple majority whether to accept or reject the law. Any 8 cantons together can also call a constitutional referendum on a federal law.

Similarly, the federal "constitutional initiative" allows citizens to put a constitutional amendment to a national vote, if 100,000 voters sign the proposed amendment within 18 months. The Federal Council and the Federal Assembly can supplement the proposed amendment with a counter-proposal, and then voters must indicate a preference on the ballot in case both proposals are accepted. Constitutional amendments, whether introduced by initiative or in parliament, must be accepted by a double majority of the national popular vote and the cantonal popular votes.

The Swiss Confederation consists of 26 cantons:
<nowiki>*</nowiki>

The cantons are federated states, have a permanent constitutional status and, in comparison with the situation in other countries, a high degree of independence. Under the Federal Constitution, all 26 cantons are equal in status, except that 6 (referred to often as the half-cantons) are represented by only one councillor (instead of two) in the Council of States and have only half a cantonal vote with respect to the required cantonal majority in referendums on constitutional amendments. Each canton has its own constitution, and its own parliament, government, police and courts. However, there are considerable differences between the individual cantons, most particularly in terms of population and geographical area. Their populations vary between 16,003 (Appenzell Innerrhoden) and 1,487,969 (Zürich), and their area between (Basel-Stadt) and (Grisons).

The cantons comprise a total of 2,222 municipalities as of 2018.

Traditionally, Switzerland avoids alliances that might entail military, political, or direct economic action and has been neutral since the end of its expansion in 1515. Its policy of neutrality was internationally recognised at the Congress of Vienna in 1815. Only in 2002 did Switzerland become a full member of the United Nations and it was the first state to join it by referendum. Switzerland maintains diplomatic relations with almost all countries and historically has served as an intermediary between other states. Switzerland is not a member of the European Union; the Swiss people have consistently rejected membership since the early 1990s. However, Switzerland does participate in the Schengen Area. Swiss neutrality has been questioned at times.

Many international institutions have their seats in Switzerland, in part because of its policy of neutrality. Geneva is the birthplace of the Red Cross and Red Crescent Movement, the Geneva Conventions and, since 2006, hosts the United Nations Human Rights Council. Even though Switzerland is one of the most recent countries to have joined the United Nations, the Palace of Nations in Geneva is the second biggest centre for the United Nations after New York, and Switzerland was a founding member and home to the League of Nations.

Apart from the United Nations headquarters, the Swiss Confederation is host to many UN agencies, like the World Health Organization (WHO), the International Labour Organization (ILO), the International Telecommunication Union (ITU), the United Nations High Commissioner for Refugees (UNHCR) and about 200 other international organisations, including the World Trade Organization and the World Intellectual Property Organization. The annual meetings of the World Economic Forum in Davos bring together top international business and political leaders from Switzerland and foreign countries to discuss important issues facing the world, including health and the environment. Additionally the headquarters of the Bank for International Settlements (BIS) are located in Basel since 1930.

Furthermore, many sport federations and organisations are located throughout the country, such as the International Handball Federation in Basel, the
International Basketball Federation in Geneva, the Union of European Football Associations (UEFA) in Nyon, the International Federation of Association Football (FIFA) and the International Ice Hockey Federation both in Zürich, the International Cycling Union in Aigle, and the International Olympic Committee in Lausanne.

The Swiss Armed Forces, including the Land Forces and the Air Force, are composed mostly of conscripts, male citizens aged from 20 to 34 (in special cases up to 50) years. Being a landlocked country, Switzerland has no navy; however, on lakes bordering neighbouring countries, armed military patrol boats are used. Swiss citizens are prohibited from serving in foreign armies, except for the Swiss Guards of the Vatican, or if they are dual citizens of a foreign country and reside there.

The structure of the Swiss militia system stipulates that the soldiers keep their Army issued equipment, including all personal weapons, at home. Some organisations and political parties find this practice controversial. Women can serve voluntarily. Men usually receive military conscription orders for training at the age of 18. About two thirds of the young Swiss are found suited for service; for those found unsuited, various forms of alternative service exist. Annually, approximately 20,000 persons are trained in recruit centres for a duration from 18 to 21 weeks. The reform "Army XXI" was adopted by popular vote in 2003, it replaced the previous model "Army 95", reducing the effectives from 400,000 to about 200,000. Of those, 120,000 are active in periodic Army training and 80,000 are non-training reserves.
Overall, three general mobilisations have been declared to ensure the integrity and neutrality of Switzerland. The first one was held on the occasion of the Franco-Prussian War of 1870–71. The second was in response to the outbreak of the First World War in August 1914. The third mobilisation of the army took place in September 1939 in response to the German attack on Poland; Henri Guisan was elected as the General-in-Chief.

Because of its neutrality policy, the Swiss army does not currently take part in armed conflicts in other countries, but is part of some peacekeeping missions around the world. Since 2000 the armed force department has also maintained the Onyx intelligence gathering system to monitor satellite communications. Switzerland decided not to sign the Nuclear Weapon Ban Treaty.

Following the end of the Cold War there have been a number of attempts to curb military activity or even abolish the armed forces altogether. A notable referendum on the subject, launched by an anti-militarist group, was held on 26 November 1989. It was defeated with about two thirds of the voters against the proposal. A similar referendum, called for before, but held shortly after the 11 September attacks in the US, was defeated by over 78% of voters.

Gun politics in Switzerland are unique in Europe in that 29% of citizens are legally armed. The large majority of firearms kept at home are issued by the Swiss army, but ammunition is no longer issued.

Until 1848 the rather loosely coupled Confederation did not know a central political organisation, but representatives, mayors, and "Landammänner" met several times a year at the capital of the "Lieu" presiding the Confederal Diet for one year.
Until 1500 the legates met most of the time in Lucerne, but also in Zürich, Baden, Bern, Schwyz etc., but sometimes also at places outside of the confederation, such as Constance. From the Swabian War in 1499 onwards until Reformation, most conferences met in Zurich. Afterwards the town hall at Baden, where the annual accounts of the common people had been held regularly since 1426, became the most frequent, but not the sole place of assembly. After 1712 Frauenfeld gradually dissolved Baden. From 1526, the Catholic conferences were held mostly in Lucerne, the Protestant conferences from 1528 mostly in Aarau, the one for the legitimation of the French Ambassador in Solothurn. At the same time the syndicate for the "Ennetbirgischen Vogteien" located in the present Ticino met from 1513 in Lugano and Locarno.

After the Helvetic Republic and during the Mediation from 1803 until 1815 the Confederal Diet of the 19 "Lieus" met at the capitals of the "directoral cantons" Fribourg, Berne, Basel, Zurich, Lucerne and Solothurn.

After the Long Diet from 6 April 1814 to 31 August 1815 took place in Zurich to replace the constitution and the enhancement of the Confederation to 22 cantons by the admission of the cantons of Valais, Neuchâtel and Geneva to full members, the directoral cantons of Lucerne, Zurich and Berne took over the diet in two-year turns.

In 1848, the federal constitution provided that details concerning the federal institutions, such as their locations, should be taken care of by the Federal Assembly (BV 1848 Art. 108). Thus on 28 November 1848, the Federal Assembly voted in majority to locate the seat of government in Berne. And, as a prototypical federal compromise, to assign other federal institutions, such as the Federal Polytechnical School (1854, the later ETH) to Zurich, and other institutions to Lucerne, such as the later SUVA (1912) and the Federal Insurance Court (1917). In 1875, a law (RS 112) fixed the compensations owed by the city of Bern for the federal seat. According to these living fundamental federalistic feelings further federal institutions were subsequently attributed to Lausanne (Federal Supreme Court in 1872, and EPFL in 1969), Bellinzona (Federal Criminal Court, 2004), and St. Gallen (Federal Administrative Court and Federal Patent Court, 2012).

The 1999 new constitution, however, does not contain anything concerning any Federal City. In 2002 a tripartite committee has been asked by the Swiss Federal Council to prepare the "creation of a federal law on the status of Bern as a Federal City", and to evaluate the positive and negative aspects for the city and the canton of Bern if this status were awarded. After a first report the work of this committee was suspended in 2004 by the Swiss Federal Council, and work on this subject has not resumed since.

Thus as of today, no city in Switzerland has the official status either of capital or of Federal City, nevertheless Berne is commonly referred to as "Federal City" (, , ).

Switzerland has a stable, prosperous and high-tech economy and enjoys great wealth, being ranked as the wealthiest country in the world per capita in multiple rankings, while at the same time being one the least corrupt countries in the world. It has the world's twentieth largest economy by nominal GDP and the thirty-eighth largest by purchasing power parity. It is the seventeenth largest exporter. Zürich and Geneva are regarded as global cities, ranked as Alpha and Beta respectively. Basel is the capital of the pharmaceutical industry in Switzerland. With its world-class companies, Novartis and Roche, and many other players, it is also one of the world's most important centres for the life sciences industry.

Switzerland has the highest European rating in the Index of Economic Freedom 2010, while also providing large coverage through public services. The nominal per capita GDP is higher than those of the larger Western and Central European economies and Japan. In terms of GDP per capita adjusted for purchasing power, Switzerland was ranked 5th in the world in 2018 by World Bank and estimated at 9th by the IMF in 2020, as well as 11th by the CIA World Factbook in 2017. 

The World Economic Forum's Global Competitiveness Report currently ranks Switzerland's economy as the most competitive in the world, while ranked by the European Union as Europe's most innovative country. It is a relatively easy place to do business, currently ranking 20th of 189 countries in the Ease of Doing Business Index. The slow growth Switzerland experienced in the 1990s and the early 2000s has brought greater support for economic reforms and harmonisation with the European Union. 

For much of the 20th century, Switzerland was the wealthiest country in Europe by a considerable margin (by GDP – per capita). Switzerland also has one of the world's largest account balances as a percentage of GDP. In 2018, the canton of Basel-City had the highest GDP per capita in the country, ahead of the cantons of Zug and Geneva. According to Credit Suisse, only about 37% of residents own their own homes, one of the lowest rates of home ownership in Europe. Housing and food price levels were 171% and 145% of the EU-25 index in 2007, compared to 113% and 104% in Germany.

Switzerland is home to several large multinational corporations. The largest Swiss companies by revenue are Glencore, Gunvor, Nestlé, Novartis, Hoffmann-La Roche, ABB, Mercuria Energy Group and Adecco. Also, notable are UBS AG, Zurich Financial Services, Credit Suisse, Barry Callebaut, Swiss Re, Tetra Pak, The Swatch Group and Swiss International Air Lines. Switzerland is ranked as having one of the most powerful economies in the world.

Switzerland's most important economic sector is manufacturing. Manufacturing consists largely of the production of specialist chemicals, health and pharmaceutical goods, scientific and precision measuring instruments and musical instruments. The largest exported goods are chemicals (34% of exported goods), machines/electronics (20.9%), and precision instruments/watches (16.9%). Exported services amount to a third of exports. The service sector – especially banking and insurance, tourism, and international organisations – is another important industry for Switzerland. 

Agricultural protectionism—a rare exception to Switzerland's free trade policies—has contributed to high food prices. Product market liberalisation is lagging behind many EU countries according to the OECD. Nevertheless, domestic purchasing power is one of the best in the world. Apart from agriculture, economic and trade barriers between the European Union and Switzerland are minimal and Switzerland has free trade agreements worldwide. Switzerland is a member of the European Free Trade Association (EFTA).

Switzerland has an overwhelmingly private sector economy and low tax rates by Western World standards; overall taxation is one of the smallest of developed countries. The Swiss Federal budget had a size of 62.8 billion Swiss francs in 2010, which is an equivalent 11.35% of the country's GDP in that year; however, the regional (canton) budgets and the budgets of the municipalities are not counted as part of the federal budget and the total rate of government spending is closer to 33.8% of GDP. The main sources of income for the federal government are the value-added tax (33%) and the direct federal tax (29%) and the main expenditure is located in the areas of social welfare and finance & tax. The expenditures of the Swiss Confederation have been growing from 7% of GDP in 1960 to 9.7% in 1990 and to 10.7% in 2010. While the sectors social welfare and finance & tax have been growing from 35% in 1990 to 48.2% in 2010, a significant reduction of expenditures has been occurring in the sectors of agriculture and national defence; from 26.5% in to 12.4% (estimation for the year 2015).

Slightly more than 5 million people work in Switzerland; about 25% of employees belonged to a trade union in 2004. Switzerland has a more flexible job market than neighbouring countries and the unemployment rate is very low. The unemployment rate increased from a low of 1.7% in June 2000 to a peak of 4.4% in December 2009. The unemployment rate decreased to 3.2% in 2014 and held steady at that level for several years, before further dropping to 2.5% in 2018 and 2.3% in 2019. Population growth from net immigration is quite high, at 0.52% of population in 2004, increased in the following years before falling to 0.54% again in 2017. The foreign citizen population was 28.9% in 2015, about the same as in Australia. GDP per hour worked is the world's 16th highest, at 49.46 international dollars in 2012.

In 2016, median monthly gross salary in Switzerland was 6,502 francs per month (equivalent to US$6,597 per month), is just enough to cover the high cost of living. After rent, taxes and social security contributions, plus spending on goods and services, the average household has about 15% of its gross income left for savings. Though 61% of the population made less than the average income, income inequality is relatively low with a Gini coefficient of 29.7, placing Switzerland among the top 20 countries for income equality. 

About 8.2% of the population live below the national poverty line, defined in Switzerland as earning less than CHF3,990 per month for a household of two adults and two children, and a further 15% are at risk of poverty. Single-parent families, those with no post-compulsory education and those who are out of work are among the most likely to be living below the poverty line. Although getting a job is considered a way out of poverty, among the gainfully employed, some 4.3% are considered working poor. One in ten jobs in Switzerland is considered low-paid and roughly 12% of Swiss workers hold such jobs, many of them women and foreigners.

Education in Switzerland is very diverse because the constitution of Switzerland delegates the authority for the school system to the cantons. There are both public and private schools, including many private international schools. The minimum age for primary school is about six years in all cantons, but most cantons provide a free "children's school" starting at four or five years old. Primary school continues until grade four, five or six, depending on the school. Traditionally, the first foreign language in school was always one of the other national languages, although recently (2000) English was introduced first in a few cantons.

At the end of primary school (or at the beginning of secondary school), pupils are separated according to their capacities in several (often three) sections. The fastest learners are taught advanced classes to be prepared for further studies and the matura, while students who assimilate a little more slowly receive an education more adapted to their needs.

There are 12 universities in Switzerland, ten of which are maintained at cantonal level and usually offer a range of non-technical subjects. The first university in Switzerland was founded in 1460 in Basel (with a faculty of medicine) and has a tradition of chemical and medical research in Switzerland. It is listed 87th on the 2019 Academic Ranking of World Universities. The largest university in Switzerland is the University of Zurich with nearly 25,000 students.The Swiss Federal Institute of Technology Zurich (ETHZ) and the University of Zurich are listed 20th and 54th respectively, on the 2015 Academic Ranking of World Universities.

The two institutes sponsored by the federal government are the Swiss Federal Institute of Technology Zurich (ETHZ) in Zürich, founded 1855 and the EPFL in Lausanne, founded 1969 as such, which was formerly an institute associated with the University of Lausanne.

In addition, there are various Universities of Applied Sciences. In business and management studies, the University of St. Gallen, (HSG) is ranked 329th in the world according to QS World University Rankings and the International Institute for Management Development (IMD), was ranked first in open programmes worldwide by the "Financial Times." Switzerland has the second highest rate (almost 18% in 2003) of foreign students in tertiary education, after Australia (slightly over 18%).

As might befit a country that plays home to innumerable international organisations, the Graduate Institute of International and Development Studies, located in Geneva, is not only continental Europe's oldest graduate school of international and development studies, but also widely believed to be one of its most prestigious.

Many Nobel Prize laureates have been Swiss scientists. They include the world-famous physicist Albert Einstein in the field of physics, who developed his special relativity while working in Bern. More recently Vladimir Prelog, Heinrich Rohrer, Richard Ernst, Edmond Fischer, Rolf Zinkernagel, Kurt Wüthrich and Jacques Dubochet received Nobel Prizes in the sciences. In total, 114 Nobel Prize winners in all fields stand in relation to Switzerland and the Nobel Peace Prize has been awarded nine times to organisations residing in Switzerland.

Geneva and the nearby French department of Ain co-host the world's largest laboratory, CERN, dedicated to particle physics research. Another important research centre is the Paul Scherrer Institute. Notable inventions include lysergic acid diethylamide (LSD), diazepam (Valium), the scanning tunnelling microscope (Nobel prize) and Velcro. Some technologies enabled the exploration of new worlds such as the pressurised balloon of Auguste Piccard and the Bathyscaphe which permitted Jacques Piccard to reach the deepest point of the world's oceans.

Switzerland Space Agency, the Swiss Space Office, has been involved in various space technologies and programmes. In addition it was one of the 10 founders of the European Space Agency in 1975 and is the seventh largest contributor to the ESA budget. In the private sector, several companies are implicated in the space industry such as Oerlikon Space or Maxon Motors who provide spacecraft structures.

Switzerland voted against membership in the European Economic Area in a referendum in December 1992 and has since maintained and developed its relationships with the European Union (EU) and European countries through bilateral agreements. In March 2001, the Swiss people refused in a popular vote to start accession negotiations with the EU. In recent years, the Swiss have brought their economic practices largely into conformity with those of the EU in many ways, in an effort to enhance their international competitiveness. The economy grew at 3% in 2010, 1.9% in 2011, and 1% in 2012. EU membership was a long-term objective of the Swiss government, but there was and remains considerable popular sentiment against membership, which is opposed by the conservative SVP party, the largest party in the National Council, and not currently supported or proposed by several other political parties. The application for membership of the EU was formally withdrawn in 2016, having long been frozen. The western French-speaking areas and the urban regions of the rest of the country tend to be more pro-EU, nonetheless with far from a significant share of the population.
The government has established an Integration Office under the Department of Foreign Affairs and the Department of Economic Affairs. To minimise the negative consequences of Switzerland's isolation from the rest of Europe, Bern and Brussels signed seven bilateral agreements to further liberalise trade ties. These agreements were signed in 1999 and took effect in 2001. This first series of bilateral agreements included the free movement of persons. A second series covering nine areas was signed in 2004 and has since been ratified, which includes the Schengen Treaty and the Dublin Convention besides others. They continue to discuss further areas for cooperation.

In 2006, Switzerland approved 1 billion francs of supportive investment in the poorer Southern and Central European countries in support of cooperation and positive ties to the EU as a whole. A further referendum will be needed to approve 300 million francs to support Romania and Bulgaria and their recent admission. The Swiss have also been under EU and sometimes international pressure to reduce banking secrecy and to raise tax rates to parity with the EU. Preparatory discussions are being opened in four new areas: opening up the electricity market, participation in the European GNSS project Galileo, cooperating with the European centre for disease prevention and recognising certificates of origin for food products.

On 27 November 2008, the interior and justice ministers of European Union in Brussels announced Switzerland's accession to the Schengen passport-free zone from 12 December 2008. The land border checkpoints will remain in place only for goods movements, but should not run controls on people, though people entering the country had their passports checked until 29 March 2009 if they originated from a Schengen nation.

On 9 February 2014, Swiss voters narrowly approved by 50.3% a ballot initiative launched by the national conservative Swiss People's Party (SVP/UDC) to restrict immigration, and thus reintroducing a quota system on the influx of foreigners. This initiative was mostly backed by rural (57.6% approvals) and suburban agglomerations (51.2% approvals), and isolated towns (51.3% approvals) of Switzerland as well as by a strong majority (69.2% approval) in the canton of Ticino, while metropolitan centres (58.5% rejection) and the French-speaking part (58.5% rejection) of Switzerland rather rejected it. Some news commentators claim that this proposal "de facto" contradicts the bilateral agreements on the free movement of persons from these respective countries.

In December 2016, a compromise with the European Union was attained effectively canceling quotas on EU citizens but still allowing for favourable treatment of Swiss-based job applicants.

Electricity generated in Switzerland is 56% from hydroelectricity and 39% from nuclear power, resulting in a nearly CO-free electricity-generating network. On 18 May 2003, two anti-nuclear initiatives were turned down: "Moratorium Plus", aimed at forbidding the building of new nuclear power plants (41.6% supported and 58.4% opposed), and Electricity Without Nuclear (33.7% supported and 66.3% opposed) after a previous moratorium expired in 2000. However, as a reaction to the Fukushima nuclear disaster, the Swiss government announced in 2011 that it plans to end its use of nuclear energy in the next 2 or 3 decades. In November 2016, Swiss voters rejected a proposal by the Green Party to accelerate the phaseout of nuclear power (45.8% supported and 54.2% opposed). The Swiss Federal Office of Energy (SFOE) is the office responsible for all questions relating to energy supply and energy use within the Federal Department of Environment, Transport, Energy and Communications (DETEC). The agency is supporting the 2000-watt society initiative to cut the nation's energy use by more than half by the year 2050.
The most dense rail network in Europe of carries over 596 million passengers annually (as of 2015). In 2015, each Swiss resident travelled on average by rail, which makes them the keenest rail users. Virtually 100% of the network is electrified. The vast majority (60%) of the network is operated by the Swiss Federal Railways (SBB CFF FFS). Besides the second largest standard gauge railway company BLS AG two railways companies operating on narrow gauge networks are the Rhaetian Railway (RhB) in the southeastern canton of Graubünden, which includes some World Heritage lines, and the Matterhorn Gotthard Bahn (MGB), which co-operates together with RhB the Glacier Express between Zermatt and St. Moritz/Davos. On 31 May 2016 the world's longest and deepest railway tunnel and the first flat, low-level route through the Alps, the Gotthard Base Tunnel, opened as the largest part of the New Railway Link through the Alps (NRLA) project after 17 years of realization. It started its daily business for passenger transport on 11 December 2016 replacing the old, mountainous, scenic route over and through the St Gotthard Massif.

Switzerland has a publicly managed road network without road tolls that is financed by highway permits as well as vehicle and gasoline taxes. The Swiss autobahn/autoroute system requires the purchase of a vignette (toll sticker)—which costs 40 Swiss francs—for one calendar year in order to use its roadways, for both passenger cars and trucks. The Swiss autobahn/autoroute network has a total length of (as of 2000) and has, by an area of , also one of the highest motorway densities in the world. Zurich Airport is Switzerland's largest international flight gateway, which handled 22.8 million passengers in 2012. The other international airports are Geneva Airport (13.9 million passengers in 2012), EuroAirport Basel Mulhouse Freiburg which is located in France, Bern Airport, Lugano Airport, St. Gallen-Altenrhein Airport and Sion Airport. Swiss International Air Lines is the flag carrier of Switzerland. Its main hub is Zürich, but it is legally domiciled in Basel.

Switzerland has one of the best environmental records among nations in the developed world; it was one of the countries to sign the Kyoto Protocol in 1998 and ratified it in 2003. With Mexico and the Republic of Korea it forms the Environmental Integrity Group (EIG). The country is heavily active in recycling and anti-littering regulations and is one of the top recyclers in the world, with 66% to 96% of recyclable materials being recycled, depending on the area of the country. The 2014 Global Green Economy Index ranked Switzerland among the top 10 green economies in the world.

Switzerland developed an efficient system to recycle most recyclable materials. Publicly organised collection by volunteers and economical railway transport logistics started as early as 1865 under the leadership of the notable industrialist Hans Caspar Escher (Escher Wyss AG) when the first modern Swiss paper manufacturing plant was built in Biberist.

Switzerland also has an economic system for garbage disposal, which is based mostly on recycling and energy-producing incinerators due to a strong political will to protect the environment. As in other European countries, the illegal disposal of garbage is not tolerated at all and heavily fined. In almost all Swiss municipalities, stickers or dedicated garbage bags need to be purchased that allow for identification of disposable garbage.

In 2018, Switzerland's population slightly exceeded 8.5 million. In common with other developed countries, the Swiss population increased rapidly during the industrial era, quadrupling between 1800 and 1990 and has continued to grow. Like most of Europe, Switzerland faces an ageing population, albeit with consistent annual growth projected into 2035, due mostly to immigration and a fertility rate close to replacement level. Switzerland subsequently has one of the oldest populations in the world, with the average age of 42.5 years.

, resident foreigners make up 25.2% of the population, one of the largest proportions in the developed world. Most of these (64%) were from European Union or EFTA countries. Italians were the largest single group of foreigners, with 15.6% of total foreign population, followed closely by Germans (15.2%), immigrants from Portugal (12.7%), France (5.6%), Serbia (5.3%), Turkey (3.8%), Spain (3.7%), and Austria (2%). Immigrants from Sri Lanka, most of them former Tamil refugees, were the largest group among people of Asian origin (6.3%).

Additionally, the figures from 2012 show that 34.7% of the permanent resident population aged 15 or over in Switzerland (around 2.33 million), had an immigrant background. A third of this population (853,000) held Swiss citizenship. Four fifths of persons with an immigration background were themselves immigrants (first generation foreigners and native-born and naturalised Swiss citizens), whereas one fifth were born in Switzerland (second generation foreigners and native-born and naturalised Swiss citizens).

In the 2000s, domestic and international institutions expressed concern about what was perceived as an increase in xenophobia, particularly in some political campaigns. In reply to one critical report, the Federal Council noted that "racism unfortunately is present in Switzerland", but stated that the high proportion of foreign citizens in the country, as well as the generally unproblematic integration of foreigners, underlined Switzerland's openness. 
Follow-up study conducted in 2018 found that 59% considered racism a serious problem in Switzerland. 

The proportion of the population that has reported being targeted by racial discrimination has increased in recent years, from 10% in 2014 to almost 17% in 2018, according to the Federal Statistical Office. 

Switzerland has four national languages: mainly German (spoken by 62.8% of the population in 2016); French (22.9%) in the west; and Italian (8.2%) in the south. The fourth national language, Romansh (0.5%), is a Romance language spoken locally in the southeastern trilingual canton of Grisons, and is designated by Article 4 of the Federal Constitution as a national language along with German, French, and Italian, and in Article 70 as an official language if the authorities communicate with persons who speak Romansh. However, federal laws and other official acts do not need to be decreed in Romansh.

In 2016, the languages most spoken at home among permanent residents aged 15 and older were Swiss German (59.4%), French (23.5%), Standard German (10.6%), and Italian (8.5%). Other languages spoken at home included English (5.0%), Portuguese (3.8%), Albanian (3.0%), Spanish (2.6%) and Serbian and Croatian (2.5%). 6.9% reported speaking another language at home. In 2014 almost two-thirds (64.4%) of the permanent resident population indicated speaking more than one language regularly.

The federal government is obliged to communicate in the official languages, and in the federal parliament simultaneous translation is provided from and into German, French and Italian.

Aside from the official forms of their respective languages, the four linguistic regions of Switzerland also have their local dialectal forms. The role played by dialects in each linguistic region varies dramatically: in the German-speaking regions, Swiss German dialects have become ever more prevalent since the second half of the 20th century, especially in the media, such as radio and television, and are used as an everyday language for many, while the Swiss variety of Standard German is almost always used instead of dialect for written communication (c.f. diglossic usage of a language). Conversely, in the French-speaking regions the local dialects have almost disappeared (only 6.3% of the population of Valais, 3.9% of Fribourg, and 3.1% of Jura still spoke dialects at the end of the 20th century), while in the Italian-speaking regions dialects are mostly limited to family settings and casual conversation.

The principal official languages (German, French, and Italian) have terms, not used outside of Switzerland, known as Helvetisms. German Helvetisms are, roughly speaking, a large group of words typical of Swiss Standard German, which do not appear either in Standard German, nor in other German dialects. These include terms from Switzerland's surrounding language cultures (German "Billett" from French), from similar terms in another language (Italian "azione" used not only as "act" but also as "discount" from German "Aktion"). The French spoken in Switzerland has similar terms, which are equally known as Helvetisms. The most frequent characteristics of Helvetisms are in vocabulary, phrases, and pronunciation, but certain Helvetisms denote themselves as special in syntax and orthography likewise. Duden, the comprehensive German dictionary, contains about 3000 Helvetisms. Current French dictionaries, such as the Petit Larousse, include several hundred Helvetisms.

Learning one of the other national languages at school is compulsory for all Swiss pupils, so many Swiss are supposed to be at least bilingual, especially those belonging to linguistic minority groups.

Swiss residents are universally required to buy health insurance from private insurance companies, which in turn are required to accept every applicant. While the cost of the system is among the highest, it compares well with other European countries in terms of health outcomes; patients have been reported as being, in general, highly satisfied with it. In 2012, life expectancy at birth was 80.4 years for men and 84.7 years for women — the highest in the world. However, spending on health is particularly high at 11.4% of GDP (2010), on par with Germany and France (11.6%) and other European countries, but notably less than spending in the USA (17.6%). From 1990, a steady increase can be observed, reflecting the high costs of the services provided. With an ageing population and new healthcare technologies, health spending will likely continue to rise. Drug use is comparable to other developed countries with 14% of men and 6.5% of women between 20-24 saying they had consumed cannabis in the past 30 days and 5 Swiss cities were listed among the top 10 European cities for cocaine use as measured in wastewater.

Between two thirds and three quarters of the population live in urban areas. Switzerland has gone from a largely rural country to an urban one in just 70 years. Since 1935 urban development has claimed as much of the Swiss landscape as it did during the previous 2,000 years. This urban sprawl does not only affect the plateau but also the Jura and the Alpine foothills and there are growing concerns about land use. However, from the beginning of the 21st century, the population growth in urban areas is higher than in the countryside.

Switzerland has a dense network of towns, where large, medium and small towns are complementary. The plateau is very densely populated with about 450 people per km and the landscape continually shows signs of human presence. The weight of the largest metropolitan areas, which are Zürich, Geneva–Lausanne, Basel and Bern tend to increase. In international comparison the importance of these urban areas is stronger than their number of inhabitants suggests. In addition the three main centres of Zürich, Geneva and Basel are recognised for their particularly great quality of life.

Switzerland has no official state religion, though most of the cantons (except Geneva and Neuchâtel) recognise official churches, which are either the Roman Catholic Church or the Swiss Reformed Church. These churches, and in some cantons also the Old Catholic Church and Jewish congregations, are financed by official taxation of adherents.

Christianity is the predominant religion of Switzerland (about 67% of resident population in 2016-2018 and 75% of Swiss citizens), divided between the Roman Catholic Church (35.8% of the population), the Swiss Reformed Church (23.8%), further Protestant churches (2.2%), Eastern Orthodoxy (2.5%), and other Christian denominations (2.2%). Immigration has established Islam (5.3%) as a sizeable minority religion.

26.3% of Swiss permanent residents are not affiliated with any religious community (Atheism, Agnosticism, and others).

As of the 2000 census other Christian minority communities included Neo-Pietism (0.44%), Pentecostalism (0.28%, mostly incorporated in the Schweizer Pfingstmission), Methodism (0.13%), the New Apostolic Church (0.45%), Jehovah's Witnesses (0.28%), other Protestant denominations (0.20%), the Old Catholic Church (0.18%), other Christian denominations (0.20%). Non-Christian religions are Hinduism (0.38%), Buddhism (0.29%), Judaism (0.25%) and others (0.11%); 4.3% did not make a statement.

The country was historically about evenly balanced between Catholic and Protestant, with a complex patchwork of majorities over most of the country. Switzerland played an exceptional role during the Reformation as it became home to many reformers. Geneva converted to Protestantism in 1536, just before John Calvin arrived there. In 1541, he founded the "Republic of Geneva" on his own ideals. It became known internationally as the "Protestant Rome", and housed such reformers as Theodore Beza, William Farel or Pierre Viret. Zürich became another stronghold around the same time, with Huldrych Zwingli and Heinrich Bullinger taking the lead there. Anabaptists Felix Manz and Conrad Grebel also operated there. They were later joined by the fleeing Peter Martyr Vermigli and Hans Denck. Other centres included Basel (Andreas Karlstadt and Johannes Oecolampadius), Berne (Berchtold Haller and Niklaus Manuel), and St. Gallen (Joachim Vadian). One canton, Appenzell, was officially divided into Catholic and Protestant sections in 1597. The larger cities and their cantons (Bern, Geneva, Lausanne, Zürich and Basel) used to be predominantly Protestant. Central Switzerland, the Valais, the Ticino, Appenzell Innerrhodes, the Jura, and Fribourg are traditionally Catholic. The Swiss Constitution of 1848, under the recent impression of the clashes of Catholic vs. Protestant cantons that culminated in the Sonderbundskrieg, consciously defines a consociational state, allowing the peaceful co-existence of Catholics and Protestants. A 1980 initiative calling for the complete separation of church and state was rejected by 78.9% of the voters. Some traditionally Protestant cantons and cities nowadays have a slight Catholic majority, not because they were growing in members, quite the contrary, but only because since about 1970 a steadily growing minority became not affiliated with any church or other religious body (21.4% in Switzerland, 2012) especially in traditionally Protestant regions, such as Basel-City (42%), canton of Neuchâtel (38%), canton of Geneva (35%), canton of Vaud (26%), or Zürich city (city: >25%; canton: 23%).

Three of Europe's major languages are official in Switzerland. Swiss culture is characterised by diversity, which is reflected in a wide range of traditional customs. A region may be in some ways strongly culturally connected to the neighbouring country that shares its language, the country itself being rooted in western European culture. The linguistically isolated Romansh culture in Graubünden in eastern Switzerland constitutes an exception, it survives only in the upper valleys of the Rhine and the Inn and strives to maintain its rare linguistic tradition.

Switzerland is home to many notable contributors to literature, art, architecture, music and sciences. In addition the country attracted a number of creative persons during time of unrest or war in Europe.
Some 1000 museums are distributed through the country; the number has more than tripled since 1950. Among the most important cultural performances held annually are the Paléo Festival, Lucerne Festival, the Montreux Jazz Festival, the Locarno International Film Festival and the Art Basel.

Alpine symbolism has played an essential role in shaping the history of the country and the Swiss national identity. Nowadays some concentrated mountain areas have a strong highly energetic ski resort culture in winter, and a hiking () or Mountain biking culture in summer. Other areas throughout the year have a recreational culture that caters to tourism, yet the quieter seasons are spring and autumn when there are fewer visitors. A traditional farmer and herder culture also predominates in many areas and small farms are omnipresent outside the towns. Folk art is kept alive in organisations all over the country. In Switzerland it is mostly expressed in music, dance, poetry, wood carving and embroidery. The alphorn, a trumpet-like musical instrument made of wood, has become alongside yodeling and the accordion an epitome of traditional Swiss music.

As the Confederation, from its foundation in 1291, was almost exclusively composed of German-speaking regions, the earliest forms of literature are in German. In the 18th century, French became the fashionable language in Bern and elsewhere, while the influence of the French-speaking allies and subject lands was more marked than before.

Among the classic authors of Swiss German literature are Jeremias Gotthelf (1797–1854) and Gottfried Keller (1819–1890). The undisputed giants of 20th-century Swiss literature are Max Frisch (1911–91) and Friedrich Dürrenmatt (1921–90), whose repertoire includes "Die Physiker" (The Physicists) and "Das Versprechen" (), released in 2001 as a Hollywood film.

Famous French-speaking writers were Jean-Jacques Rousseau (1712–1778) and Germaine de Staël (1766–1817). More recent authors include Charles Ferdinand Ramuz (1878–1947), whose novels describe the lives of peasants and mountain dwellers, set in a harsh environment and Blaise Cendrars (born Frédéric Sauser, 1887–1961). Italian and Romansh-speaking authors also contributed to the Swiss literary landscape, but generally in more modest ways given their small number.

Probably the most famous Swiss literary creation, "Heidi", the story of an orphan girl who lives with her grandfather in the Alps, is one of the most popular children's books ever and has come to be a symbol of Switzerland. Her creator, Johanna Spyri (1827–1901), wrote a number of other books on similar themes.

The freedom of the press and the right to free expression is guaranteed in the federal constitution of Switzerland. The Swiss News Agency (SNA) broadcasts information around-the-clock in three of the four national languages—on politics, economics, society and culture. The SNA supplies almost all Swiss media and a couple dozen foreign media services with its news.

Switzerland has historically boasted the greatest number of newspaper titles published in proportion to its population and size. The most influential newspapers are the German-language "Tages-Anzeiger" and "Neue Zürcher Zeitung" NZZ, and the French-language "Le Temps", but almost every city has at least one local newspaper. The cultural diversity accounts for a variety of newspapers.

The government exerts greater control over broadcast media than print media, especially due to finance and licensing. The Swiss Broadcasting Corporation, whose name was recently changed to SRG SSR, is charged with the production and broadcast of radio and television programmes. SRG SSR studios are distributed throughout the various language regions. Radio content is produced in six central and four regional studios while the television programmes are produced in Geneva, Zürich, Basel, and Lugano. An extensive cable network also allows most Swiss to access the programmes from neighbouring countries.

Skiing, snowboarding and mountaineering are among the most popular sports in Switzerland, the nature of the country being particularly suited for such activities. Winter sports are practised by the natives and tourists since the second half of the 19th century with the invention of bobsleigh in St. Moritz. The first world ski championships were held in Mürren (1931) and St. Moritz (1934). The latter town hosted the second Winter Olympic Games in 1928 and the fifth edition in 1948. Among the most successful skiers and world champions are Pirmin Zurbriggen and Didier Cuche.

The most prominently watched sports in Switzerland are football, ice hockey, Alpine skiing, "Schwingen", and tennis.

The headquarters of the international football's and ice hockey's governing bodies, the International Federation of Association Football (FIFA) and International Ice Hockey Federation (IIHF), are located in Zürich. Actually many other headquarters of international sports federations are located in Switzerland. For example, the International Olympic Committee (IOC), IOC's Olympic Museum and the Court of Arbitration for Sport (CAS) are located in Lausanne.

Switzerland hosted the 1954 FIFA World Cup, and was the joint host, with Austria, of the UEFA Euro 2008 tournament. The Swiss Super League is the nation's professional football club league. Europe's highest football pitch, at above sea level, is located in Switzerland and is named the "Ottmar Hitzfeld Stadium".
Many Swiss also follow ice hockey and support one of the 12 teams of the National League, which is the most attended league in Europe. In 2009, Switzerland hosted the IIHF World Championship for the 10th time. It also became World Vice-Champion in 2013 and 2018. The numerous lakes make Switzerland an attractive place for sailing. The largest, Lake Geneva, is the home of the sailing team Alinghi which was the first European team to win the America's Cup in 2003 and which successfully defended the title in 2007. Tennis has become an increasingly popular sport, and Swiss players such as Martina Hingis, Roger Federer, and Stanislas Wawrinka have won multiple Grand Slams.

Motorsport racecourses and events were banned in Switzerland following the 1955 Le Mans disaster with exception to events such as Hillclimbing. During this period, the country still produced successful racing drivers such as Clay Regazzoni, Sébastien Buemi, Jo Siffert, Dominique Aegerter, successful World Touring Car Championship driver Alain Menu, 2014 24 Hours of Le Mans winner Marcel Fässler and 2015 24 Hours Nürburgring winner Nico Müller. Switzerland also won the A1GP World Cup of Motorsport in 2007–08 with driver Neel Jani. Swiss motorcycle racer Thomas Lüthi won the 2005 MotoGP World Championship in the 125cc category. In June 2007 the Swiss National Council, one house of the Federal Assembly of Switzerland, voted to overturn the ban, however the other house, the Swiss Council of States rejected the change and the ban remains in place.

Traditional sports include Swiss wrestling or "Schwingen". It is an old tradition from the rural central cantons and considered the national sport by some. Hornussen is another indigenous Swiss sport, which is like a cross between baseball and golf. Steinstossen is the Swiss variant of stone put, a competition in throwing a heavy stone. Practised only among the alpine population since prehistoric times, it is recorded to have taken place in Basel in the 13th century. It is also central to the Unspunnenfest, first held in 1805, with its symbol the 83.5 stone named "Unspunnenstein".

The cuisine of Switzerland is multifaceted. While some dishes such as fondue, raclette or rösti are omnipresent through the country, each region developed its own gastronomy according to the differences of climate and languages. Traditional Swiss cuisine uses ingredients similar to those in other European countries, as well as unique dairy products and cheeses such as Gruyère or Emmental, produced in the valleys of Gruyères and Emmental. The number of fine-dining establishments is high, particularly in western Switzerland.

Chocolate has been made in Switzerland since the 18th century but it gained its reputation at the end of the 19th century with the invention of modern techniques such as conching and tempering which enabled its production on a high quality level. Also a breakthrough was the invention of solid milk chocolate in 1875 by Daniel Peter. The Swiss are the world's largest consumers of chocolate.

Due to the popularisation of processed foods at the end of the 19th century, Swiss health food pioneer Maximilian Bircher-Benner created the first nutrition-based therapy in form of the well-known rolled oats cereal dish, called Birchermüesli.

The most popular alcoholic drink in Switzerland is wine. Switzerland is notable for the variety of grapes grown because of the large variations in terroirs, with their specific mixes of soil, air, altitude and light. Swiss wine is produced mainly in Valais, Vaud (Lavaux), Geneva and Ticino, with a small majority of white wines. Vineyards have been cultivated in Switzerland since the Roman era, even though certain traces can be found of a more ancient origin. The most widespread varieties are the Chasselas (called Fendant in Valais) and Pinot noir. The Merlot is the main variety produced in Ticino.




</doc>
<doc id="26750" url="https://en.wikipedia.org/wiki?curid=26750" title="Sri Lanka">
Sri Lanka

Sri Lanka (, ; '; "Ilaṅkai"), officially the Democratic Socialist Republic of Sri Lanka, is an island country in South Asia, located in the Indian Ocean southwest of the Bay of Bengal and southeast of the Arabian Sea. It is geographically separated from the Indian subcontinent by the Gulf of Mannar and the Palk Strait. Sri Jayawardenepura Kotte is its legislative capital, and Colombo is its largest city and centre of commerce.

Sri Lanka's documented history spans 3,000 years, with evidence of prehistoric human settlements dating back at least 125,000 years. It has a rich cultural heritage, and the first known Buddhist writings of Sri Lanka, the Pāli Canon, date back to the Fourth Buddhist council in 29 BC. Its geographic location and deep harbours made it of great strategic importance from the time of the ancient Silk Road through to the modern Maritime Silk Road. Its location as a major trading hub made it known to both the far East as well as the European continent from as far back as the Anuradhapura period. The country's trade in luxury goods and spices attracted traders of many nations, creating Sri Lanka's diverse population. During a period of great political crisis the Portuguese, whose arrival in Sri Lanka was largely accidental, sought to control the island's maritime regions and its lucrative external trade. The Portuguese possessions were later taken over by the Dutch. The Dutch possessions were then taken by the British, who later extended their control over the whole island, colonising it from 1815 to 1948. Resistance to the British was immediate. A national movement for political independence arose in the early 20th century, and in 1948, Ceylon became a republic and adopted its current name in 1972. Sri Lanka's recent history has been marred by a 26-year civil war, which ended decisively when the Sri Lanka Armed Forces defeated the Liberation Tigers of Tamil Eelam in 2009.

Sri Lanka's current constitution stipulates it as a republic and unitary state governed by a semi-presidential system. It has had a long history of international engagement, as a founding member of the South Asian Association for Regional Cooperation (SAARC), and a member of the United Nations, the Commonwealth of Nations, the G77, and the Non-Aligned Movement. Sri Lanka is rated "high" on the Human Development Index (HDI), with its HDI rating and per capita income the highest among South Asian nations. The Sri Lankan constitution accords Buddhism the "foremost place", and although it does not identify it as a state religion, Buddhism is given special privileges in the Sri Lankan constitution.

Sri Lanka is home to many cultures, languages and ethnicities. The majority of the population are from the Sinhalese ethnicity, while a large minority of Tamils have also played an influential role in the island's history. Moors, Burghers, Malays, Chinese, and the indigenous Vedda are also established groups.

In antiquity, Sri Lanka was known to travellers by a variety of names. According to the "Mahavamsa", the legendary Prince Vijaya named the land Tambapanni ('copper-red hands' or 'copper-red earth'), because his followers' hands were reddened by the red soil of the area. In Hindu mythology, such as the Ramayana, the island was referred to as "Lankā" ('Island'). The Tamil term Eelam () was used to designate the whole island in Sangam literature. The island was known under Chola rule as "Mummudi Cholamandalam" ('realm of the three crowned Cholas').

Ancient Greek geographers called it "Taprobanā" () or "Taprobanē" () from the word "Tambapanni". The Persians and Arabs referred to it as "Sarandīb" (the origin of the word "serendipity") from Sanskrit "Siṃhaladvīpaḥ". "Ceilão", the name given to Sri Lanka by the Portuguese Empire when it arrived in 1505, was transliterated into English as "Ceylon". As a British crown colony, the island was known as Ceylon; it achieved independence as the Dominion of Ceylon in 1948.

The country is now known in Sinhala as ' () and in Tamil as ' (, ). In 1972, its formal name was changed to "Free, Sovereign and Independent Republic of Sri Lanka". Later, in 1978, it was changed to the "Democratic Socialist Republic of Sri Lanka". As the name Ceylon still appears in the names of a number of organisations, the Sri Lankan government announced in 2011 a plan to rename all those over which it has authority.

The Pre-history of Sri Lanka goes back 125,000 years and possibly even as far back as 500,000 years. Among the Paleolithic human settlements discovered in Sri Lanka, Pahiyangala (named after the Chinese traveller monk Faxian), which dates back to 37,000 BP, Batadombalena (28,500 BP) and Belilena (12,000 BP) are the most important. In these caves, archaeologists have found the remains of anatomically modern humans which they have named Balangoda Man, and other evidence suggesting that they may have engaged in agriculture and kept domestic dogs for driving game.

One of the first written references to the island is found in the Indian epic Ramayana, which provides details of a kingdom named "Lanka" that was created by the divine sculptor Vishwakarma for Kubera, the lord of wealth. It is said that Kubera was overthrown by his demon stepbrother Ravana, the powerful emperor who built a mythical flying machine. The modern city of Wariyapola is described as Ravana's airport.

Early inhabitants of Sri Lanka were probably ancestors of the Vedda people, an indigenous people numbering approximately 2,500 living in modern-day Sri Lanka. The 19th-century Irish historian James Emerson Tennent theorised that Galle, a city in southern Sri Lanka, was the ancient seaport of Tarshish from which King Solomon is said to have drawn ivory, peacocks, and other valuables.

According to the "Mahāvamsa", a Sinhalese chronicle written in Pāḷi, the original inhabitants of Sri Lanka are said to be the Yakshas and Nagas. Ancient cemeteries that were used before 600 BC and other signs of advanced civilisation have also been discovered in Sri Lanka. Sinhalese history traditionally starts in 543 BC with the arrival of Prince Vijaya, a semi-legendary prince who sailed with 700 followers to Sri Lanka, after being expelled from Vanga Kingdom. He established the Kingdom of Tambapanni. Vijaya (Singha) is the first of the approximately 189 monarchs of Sri Lanka described in chronicles such as the "Dipavamsa", "Mahāvaṃsa", "Cūḷavaṃsa", and "Rājāvaliya".

The Anuradhapura period began with the establishment of the Anuradhapura Kingdom in 380 BC during the reign of Pandukabhaya. Thereafter, Anuradhapura served as the capital city of the country for nearly 1,400 years. Ancient Sri Lankans excelled at building certain types of structures such as tanks, dagobas and palaces. Society underwent a major transformation during the reign of Devanampiya Tissa, with the arrival of Buddhism from India. In 250 BC, Mahinda, a bhikkhu and the son of the Mauryan Emperor Ashoka arrived in Mihintale carrying the message of Buddhism. His mission won over the monarch, who embraced the faith and propagated it throughout the Sinhalese population.

Succeeding kingdoms of Sri Lanka would maintain many Buddhist schools and monasteries and support the propagation of Buddhism into other countries in Southeast Asia. Sri Lankan Bhikkhus studied in India's famous ancient Buddhist University of Nalanda, which was destroyed by Bakhtiyar Khilji. It is probable that many of the scriptures from Nalanda are preserved in Sri Lanka's many monasteries and that the written form of the Tipitaka, including Sinhalese Buddhist literature, were part of the University of Nalanda. In 245 BC, bhikkhuni Sangamitta arrived with the Jaya Sri Maha Bodhi tree, which is considered to be a sapling from the historical Bodhi tree under which Gautama Buddha became enlightened. It is considered the oldest human-planted tree (with a continuous historical record) in the world.

Sri Lanka experienced the first of many foreign invasions during the reign of Suratissa, who was defeated by two horse traders named Sena and Guttika from South India. The next invasion came in 205 BC by a Chola named Elara, who overthrew Asela and ruled the country for 44 years. Dutugemunu, the eldest son of the southern regional sub-king, Kavan Tissa, defeated Elara in the Battle of Vijithapura. During its two and a half millennia of existence, the Sinhala Kingdom was invaded at least eight times by neighbouring South Indian dynasties. These invaders were all subsequently driven back. There also were incursions by the kingdoms of Kalinga (modern Odisha) and from the Malay Peninsula as well.

The Fourth Buddhist council of Theravada Buddhism was held at the Anuradhapura Maha Viharaya in Sri Lanka under the patronage of Valagamba of Anuradhapura in 25 BC. The council was held in response to a year in which the harvests in Sri Lanka were particularly poor and many Buddhist monks subsequently died of starvation. Because the Pāli Canon was at that time oral literature maintained in several recensions by "dhammabhāṇaka"s (dharma reciters), the surviving monks recognised the danger of not writing it down so that even if some of the monks whose duty it was to study and remember parts of the canon for later generations died, the teachings would not be lost. After the council, palm-leaf manuscripts containing the completed canon were taken to other countries such as Burma, Thailand, Cambodia and Laos.

Sri Lanka was the first Asian country known to have a female ruler: Anula of Anuradhapura (r. 47–42 BC). Sri Lankan monarchs undertook some remarkable construction projects such as Sigiriya, the so-called "Fortress in the Sky", built during the reign of Kashyapa I of Anuradhapura, who ruled between 477 and 495. The Sigiriya rock fortress is surrounded by an extensive network of ramparts and moats. Inside this protective enclosure were gardens, ponds, pavilions, palaces and other structures.

In AD 993, the invasion of Chola emperor Rajaraja I forced Sinhalese ruler Mahinda V to flee to the southern part of Sri Lanka. Taking advantage of this situation, Rajendra I, son of Rajaraja I, launched a large invasion in 1017. Mahinda V was captured and taken to India, and the Cholas sacked the city of Anuradhapura causing the fall of Anuradhapura Kingdom. Subsequently, they moved the capital to Polonnaruwa.

Following a seventeen-year-long campaign, Vijayabahu I successfully drove the Chola out of Sri Lanka in 1070, reuniting the country for the first time in over a century. Upon his request, ordained monks were sent from Burma to Sri Lanka to re-establish Buddhism, which had almost disappeared from the country during the Chola reign. During the medieval period, Sri Lanka was divided into three sub-territories, namely Ruhunu, Pihiti and Maya.
Sri Lanka's irrigation system was extensively expanded during the reign of Parākramabāhu the Great in the 12th century. This period is considered as a time when Sri Lanka was at the height of its power. He built 1,470 reservoirs – the highest number by any ruler in Sri Lanka's history – repaired 165 dams, 3,910 canals, 163 major reservoirs, and 2,376 mini-reservoirs. His most famous construction is the Parakrama Samudra, the largest irrigation project of medieval Sri Lanka. Parākramabāhu's reign is memorable for two major campaigns – in the south of India as part of a Pandyan war of succession, and a punitive strike against the kings of Ramanna (Burma) for various perceived insults to Sri Lanka.

After his demise, Sri Lanka gradually decayed in power. In 1215, Kalinga Magha, an invader with uncertain origins, identified as the founder of the Jaffna kingdom, invaded and captured the Kingdom of Polonnaruwa. He sailed from Kalinga 690 nautical miles on 100 large ships with a 24,000 strong army. Unlike previous invaders, he looted, ransacked, and destroyed everything in the ancient Anuradhapura and Polonnaruwa Kingdoms beyond recovery. His priorities in ruling were to extract as much as possible from the land and overturn as many of the traditions of Rajarata as possible. His reign saw the massive migration of native Sinhalese people to the south and west of Sri Lanka and into the mountainous interior, in a bid to escape his power.

Sri Lanka never really recovered from the impact of Kalinga Magha's invasion. King Vijayabâhu III, who led the resistance, brought the kingdom to Dambadeniya. The north, in the meanwhile, eventually evolved into the Jaffna kingdom. The Jaffna kingdom never came under the rule of any kingdom of the south except on one occasion; in 1450, following the conquest led by King Parâkramabâhu VI's adopted son, Prince Sapumal. He ruled the north from 1450 to 1467.

The next three centuries starting from 1215 were marked by shifting collections of kingdoms in south and central Sri Lanka, including Dambadeniya, Yapahuwa, Gampola, Raigama, Kotte, Sitawaka, and finally, Kandy. Chinese admiral Zheng He and his naval expeditionary force landed at Galle, Sri Lanka in 1409 and fought with the local King Vira Alakesvara of Gampola. Zheng He captured King Vira Alakesvara and later released him. Zheng He erected the Galle Trilingual Inscription, a stone tablet written in three languages (Chinese, Tamil, and Persian), to commemorate his visit. The stele was discovered in 1911 and is now preserved in the Colombo National Museum.

The early modern period of Sri Lanka begins with the arrival of Portuguese soldier and explorer Lourenço de Almeida in 1505. In 1517, the Portuguese built a fort at the port city of Colombo and gradually extended their control over the coastal areas. In 1592, after decades of intermittent warfare with the Portuguese, Vimaladharmasuriya I moved his kingdom to the inland city of Kandy, a location he thought more secure from attack. In 1619, succumbing to attacks by the Portuguese, the independent existence of the Jaffna kingdom came to an end.

During the reign of the Rajasinghe II, Dutch explorers arrived on the island. In 1638, the king signed a treaty with the Dutch East India Company to get rid of the Portuguese who ruled most of the coastal areas. The following Dutch–Portuguese War resulted in a Dutch victory, with Colombo falling into Dutch hands by 1656. The Dutch remained in the areas they had captured, thereby violating the treaty they had signed in 1638. The Burgher people, a distinct ethnic group, emerged as a result of intermingling between the Dutch and native Sri Lankans in this period.

The Kingdom of Kandy was the last independent monarchy of Sri Lanka. In 1595, Vimaladharmasurya brought the sacred Tooth Relic – the traditional symbol of royal and religious authority amongst the Sinhalese – to Kandy, and built the Temple of the Tooth. In spite of on-going intermittent warfare with Europeans, the kingdom survived. Later, a crisis of succession emerged in Kandy upon King Vira Narendrasinha's death in 1739. He was married to a Telugu-speaking Nayakkar princess from south India and was childless by her. Eventually, with the support of bhikku Weliwita Sarankara, the crown passed to the brother of one of Narendrasinha's princesses, overlooking the right of ""Unambuwe Bandara"", Narendrasinha's own son by a Sinhalese concubine. The new king was crowned Sri Vijaya Rajasinha later that year. Kings of the Nayakkar dynasty launched several attacks on Dutch controlled areas, which proved to be unsuccessful.

During the Napoleonic Wars, fearing that French control of the Netherlands might deliver Sri Lanka to the French, Great Britain occupied the coastal areas of the island (which they called Ceylon) with little difficulty in 1796. Two years later, in 1798, Sri Rajadhi Rajasinha, third of the four Nayakkar kings of Sri Lanka, died of a fever. Following his death, a nephew of Rajadhi Rajasinha, eighteen-year-old Kannasamy, was crowned. The young king, called Sri Vikrama Rajasinha, faced a British invasion in 1803 but successfully retaliated. The First Kandyan War ended in a stalemate.

By then the entire coastal area was under the East India Company as a result of the Treaty of Amiens. On 14 February 1815, Kandy was occupied by the British in the second Kandyan War, ending Sri Lanka's independence. Sri Vikrama Rajasinha, the last native monarch of Sri Lanka, was exiled to India. The Kandyan Convention formally ceded the entire country to the British Empire. Attempts by Sri Lankan noblemen to undermine British power in 1818 during the Uva Rebellion were thwarted by Governor Robert Brownrigg.

The beginning of the modern period of Sri Lanka is marked by the Colebrooke-Cameron reforms of 1833. They introduced a utilitarian and liberal political culture to the country based on the rule of law and amalgamated the Kandyan and maritime provinces as a single unit of government. An executive council and a legislative council were established, later becoming the foundation of a representative legislature. By this time, experiments with coffee plantations were largely successful.

Soon, coffee became the primary commodity export of Sri Lanka. Falling coffee prices as a result of the depression of 1847 stalled economic development and prompted the governor to introduce a series of taxes on firearms, dogs, shops, boats, etc., and to reintroduce a form of "rajakariya", requiring six days free labour on roads or payment of a cash equivalent. These harsh measures antagonised the locals, and another rebellion broke out in 1848. A devastating leaf disease, "Hemileia vastatrix", struck the coffee plantations in 1869, destroying the entire industry within fifteen years. The British quickly found a replacement: abandoning coffee, they began cultivating tea instead. Tea production in Sri Lanka thrived in the following decades.
By the end of the 19th century, a new educated social class transcending race and caste arose through British attempts to staff the Ceylon Civil Service and the legal, educational, and medical professions. New leaders represented the various ethnic groups of the population in the Ceylon Legislative Council on a communal basis. Buddhist and Hindu revivalism reacted against Christian missionary activities. The first two decades in the 20th century are noted by the unique harmony among Sinhalese and Tamil political leadership, which has since been lost.

In 1919, major Sinhalese and Tamil political organisations united to form the Ceylon National Congress, under the leadership of Ponnambalam Arunachalam, pressing colonial masters for more constitutional reforms. But without massive popular support, and with the governor's encouragement for "communal representation" by creating a "Colombo seat" that dangled between Sinhalese and Tamils, the Congress lost momentum towards the mid-1920s.

The Donoughmore reforms of 1931 repudiated the communal representation and introduced universal suffrage (which stood at 4% before the reforms). This step was strongly criticised by the Tamil political leadership, who realised that they would be reduced to a minority in the newly created State Council of Ceylon, which succeeded the legislative council. In 1937, Tamil leader G. G. Ponnambalam demanded a 50–50 representation (50% for the Sinhalese and 50% for other ethnic groups) in the State Council. However, this demand was not met by the Soulbury reforms of 1944–45.

The Soulbury constitution ushered in dominion status, with independence proclaimed on 4 February 1948. D. S. Senanayake became the first Prime Minister of Ceylon. Prominent Tamil leaders including Ponnambalam and Arunachalam Mahadeva joined his cabinet. The British Royal Navy remained stationed at Trincomalee until 1956. A countrywide popular demonstration against withdrawal of the rice ration, known as Hartal 1953, resulted in the resignation of prime minister Dudley Senanayake.

S. W. R. D. Bandaranaike was elected prime minister in 1956. His three-year rule had a profound impact through his self-proclaimed role of "defender of the besieged Sinhalese culture". He introduced the controversial Sinhala Only Act, recognising Sinhala as the only official language of the government. Although partially reversed in 1958, the bill posed a grave concern for the Tamil community, which perceived in it a threat to their language and culture.

The Federal Party (FP) launched a movement of non-violent resistance (satyagraha) against the bill, which prompted Bandaranaike to reach an agreement with S. J. V. Chelvanayakam, leader of the FP, to resolve the looming ethnic conflict. The pact proved ineffective in the face of ongoing protests by opposition and the Buddhist clergy. The bill, together with various government colonisation schemes, contributed much towards the political rancour between Sinhalese and Tamil political leaders. 

Bandaranaike was assassinated by an extremist Buddhist monk in 1959. Sirimavo Bandaranaike, the widow of Bandaranaike, took office as prime minister in 1960 and withstood an attempted coup d'état in 1962. During her second term as prime minister, the government instituted socialist economic policies, strengthening ties with the Soviet Union and China while promoting a policy of non-alignment. In 1971, Ceylon experienced a Marxist insurrection, which was quickly suppressed. In 1972, the country became a republic named Sri Lanka, repudiating its dominion status. Prolonged minority grievances and the use of communal emotionalism as an election campaign weapon by both Sinhalese and Tamil leaders abetted a fledgling Tamil militancy in the north during the 1970s. The policy of standardisation by the Sirimavo government to rectify disparities created in university enrolment, which was in essence an affirmative action to assist geographically disadvantaged students to obtain tertiary education, resulted in reducing the proportion of Tamil students at university level and acted as the immediate catalyst for the rise of militancy. The assassination of Jaffna Mayor Alfred Duraiyappah in 1975 by the Liberation Tigers of Tamil Eelam (LTTE) marked a crisis point.

The government of J. R. Jayawardene swept to power in 1977, defeating the largely unpopular United Front government. Jayawardene introduced a new constitution, together with a free-market economy and a powerful executive presidency modelled after France. It made Sri Lanka the first South Asian country to liberalise its economy. Beginning in 1983, ethnic tensions were manifested in an on-and-off insurgency against the government by the LTTE. An LTTE attack on 13 soldiers resulted in the anti-Tamil race riots in July 1983, allegedly backed by Sinhalese hard-line ministers, which resulted in more than 150,000 Tamil civilians fleeing the island, seeking asylum in other countries.

Lapses in foreign policy resulted in India strengthening the LTTE by providing arms and training. In 1987, the Indo-Sri Lanka Accord was signed, and the Indian Peace Keeping Force was deployed in northern Sri Lanka to stabilise the region by neutralising the LTTE. The same year, the Janatha Vimukthi Peramuna launched its second insurrection in southern Sri Lanka, necessitating redeployment of the Indian Peace Keeping Force in 1990. In October 1990, the LTTE expelled Sri Lankan Muslims from northern Sri Lanka. From 1985 to 2006, the Sri Lankan government and Tamil insurgents held four rounds of peace talks. In 2002, the Sri Lankan government and LTTE signed a ceasefire agreement. Both LTTE and the government resumed fighting in 2006, and the government officially backed out of the ceasefire in 2008. In 2009, under the presidency of Mahinda Rajapaksa, the Sri Lanka Armed Forces defeated the LTTE and re-established control of the entire country by the Sri Lankan Government. Overall, between 60,000 and 100,000 people were killed during the 26 years of conflict. Following the LTTE's defeat, the Tamil National Alliance, the largest Tamil political party in Sri Lanka, dropped its demand for a separate state in favour of a federal solution.The final stages of the war left some 294,000 people displaced. Up to 40,000 Tamil civilians may have been killed in the final phases of the Sri Lankan civil war, according to an expert panel convened by UN Secretary General Ban Ki-moon. The UN Human Rights Council documented over 12,000 named individuals who disappeared following detention by security forces in Sri Lanka, the second highest figure in the world since the Working Group came into being in 1980. In March 2009, 378 people had been killed in one day and at least another 1,212 injured. The UN described the situation as a "bloodbath", and one that its Colombo office had been warning against for some time. Their spokesperson Gordon Weiss said that over 100 children had been killed over the weekend in the "large-scale killing of civilians".

In May 2010, President Rajapaksa appointed the Lessons Learnt and Reconciliation Commission (LLRC) to assess the conflict between the time of the ceasefire agreement in 2002 and the defeat of the LTTE in 2009. By 2014, Sri Lanka emerged from its 26-year war to become one of the fastest-growing economies of the world, which has since slowed.

Sri Lanka lies on the Indian Plate, a major tectonic plate that was formerly part of the Indo-Australian Plate. It is in the Indian Ocean southwest of the Bay of Bengal, between latitudes 5° and 10° N, and longitudes 79° and 82° E. Sri Lanka is separated from the mainland portion of the Indian subcontinent by the Gulf of Mannar and Palk Strait. According to Hindu mythology, a land bridge existed between the Indian mainland and Sri Lanka. It now amounts to only a chain of limestone shoals remaining above sea level. Legends claim that it was passable on foot up to 1480 AD, until cyclones deepened the channel. Portions are still as shallow as , hindering navigation. The island consists mostly of flat to rolling coastal plains, with mountains rising only in the south-central part. The highest point is Pidurutalagala, reaching above sea level. 

Sri Lanka has 103 rivers. The longest of these is the Mahaweli River, extending . These waterways give rise to 51 natural waterfalls of or more. The highest is Bambarakanda Falls, with a height of . Sri Lanka's coastline is long. Sri Lanka claims an exclusive economic zone extending 200 nautical miles, which is approximately 6.7 times Sri Lanka's land area. The coastline and adjacent waters support highly productive marine ecosystems such as fringing coral reefs and shallow beds of coastal and estuarine seagrasses.

Sri Lanka has 45 estuaries and 40 lagoons. Sri Lanka's mangrove ecosystem spans over 7,000 hectares and played a vital role in buffering the force of the waves in the 2004 Indian Ocean tsunami. The island is rich in minerals such as ilmenite, feldspar, graphite, silica, kaolin, mica and thorium. Existence of petroleum and gas in the Gulf of Mannar has also been confirmed, and the extraction of recoverable quantities is underway.

The climate is tropical and warm, because of moderating effects of ocean winds. Mean temperatures range from in the central highlands, where frost may occur for several days in the winter, to a maximum of in other low-altitude areas. Average yearly temperatures range from to nearly . Day and night temperatures may vary by to .

Rainfall pattern is influenced by monsoon winds from the Indian Ocean and Bay of Bengal. The "wet zone" and some of the windward slopes of the central highlands receive up to of rain each year, but the leeward slopes in the east and northeast receive little rain. Most of the east, southeast, and northern parts of Sri Lanka comprise the "dry zone", which receives between of rain annually. The arid northwest and southeast coasts receive the least amount of rain at per year. Periodic squalls occur and sometimes tropical cyclones bring overcast skies and rains to the southwest, northeast, and eastern parts of the island. Humidity is typically higher in the southwest and mountainous areas and depends on the seasonal patterns of rainfall. An increase in average rainfall coupled with heavier rainfall events has resulted in recurrent flooding and related damages to infrastructure, utility supply and the urban economy.

Lying within the Indomalayan realm, Sri Lanka is one of 25 biodiversity hotspots in the world. Although the country is relatively small in size, it has the highest biodiversity density in Asia. A remarkably high proportion of the species among its flora and fauna, 27% of the 3,210 flowering plants and 22% of the mammals, are endemic. Sri Lanka has declared 24 wildlife reserves, which are home to a wide range of native species such as elephants, leopards, sloth bears, the unique small loris, a variety of deer, and the purple-faced langur.

Flowering acacias flourish on the arid Jaffna Peninsula. Among the trees of the dry-land forests are valuable species such as satinwood, ebony, ironwood, mahogany and teak. The wet zone is a tropical evergreen forest with tall trees, broad foliage, and a dense undergrowth of vines and creepers. Subtropical evergreen forests resembling those of temperate climates flourish in the higher altitudes.
Yala National Park in the southeast protects herds of elephant, deer, and peacocks. The Wilpattu National Park in the northwest, the largest national park, preserves the habitats of many water birds such as storks, pelicans, ibis, and spoonbills. The island has four biosphere reserves: Bundala, Hurulu Forest Reserve, the Kanneliya-Dediyagala-Nakiyadeniya, and Sinharaja. 

Sinharaja is home to 26 endemic birds and 20 rainforest species, including the elusive red-faced malkoha, the green-billed coucal and the Sri Lanka blue magpie. The untapped genetic potential of Sinharaja flora is enormous. Of the 211 woody trees and lianas within the reserve, 139 (66%) are endemic. The total vegetation density, including trees, shrubs, herbs and seedlings, has been estimated at 240,000 individuals per hectare. The Minneriya National Park borders the Minneriya Tank, which is an important source of water for elephants inhabiting the surrounding forests. Dubbed "The Gathering", the congregation of elephants can be seen on the tank-bed in the late dry season (August to October) as the surrounding water sources steadily disappear. The park also encompasses a range of micro-habitats which include classic dry zone tropical monsoonal evergreen forest, thick stands of giant bamboo, hilly pastures (patanas), and grasslands (talawas).

Sri Lanka is home to over 250 types of resident birds, with several bird sanctuaries including Kumana. During the Mahaweli Program of the 1970s and 1980s in northern Sri Lanka, the government set aside four areas of land totalling as national parks. Sri Lanka's forest cover, which was around 49% in 1920, had fallen to approximately 24% by 2009.

Sri Lanka is a democratic republic and a unitary state which is governed by a semi-presidential system, with a mixture of a presidential system and a parliamentary system. Sri Lanka is the oldest democracy in Asia. Most provisions of the constitution can be amended by a two-thirds majority in parliament. The amendment of certain basic features such as the clauses on language, religion, and reference to Sri Lanka as a unitary state require both a two-thirds majority and approval in a nationwide referendum.

In common with many democracies, the Sri Lankan government has three branches:

The current political culture in Sri Lanka is a contest between two rival coalitions led by the centre-leftist and progressivist United People's Freedom Alliance (UPFA), an offspring of Sri Lanka Freedom Party (SLFP), and the comparatively right-wing and pro-capitalist United National Party (UNP). Sri Lanka is essentially a multi-party democracy with many smaller Buddhist, socialist and Tamil nationalist political parties. As of July 2011, the number of registered political parties in the country is 67. Of these, the Lanka Sama Samaja Party (LSSP), established in 1935, is the oldest.

The UNP, established by D. S. Senanayake in 1946, was until recently the largest single political party. It is the only political group which had representation in all parliaments since independence. SLFP was founded by S. W. R. D. Bandaranaike in July 1951. SLFP registered its first victory in 1956, defeating the ruling UNP in 1956 Parliamentary election. Following the parliamentary election in July 1960, Sirimavo Bandaranaike became the prime minister and the world's first elected female head of government.

G. G. Ponnambalam, the Tamil nationalist counterpart of S. W. R. D. Bandaranaike, founded the All Ceylon Tamil Congress (ACTC) in 1944. Objecting to Ponnambalam's cooperation with D. S. Senanayake, a dissident group led by S.J.V. Chelvanayakam broke away in 1949 and formed the Illankai Tamil Arasu Kachchi (ITAK), also known as the Federal Party, becoming the main Tamil political party in Sri Lanka for next two decades. The Federal Party advocated a more aggressive stance toward the Sinhalese. With the constitutional reforms of 1972, the ACTC and ITAK created the Tamil United Front (later Tamil United Liberation Front). Following a period of turbulence as Tamil militants rose to power in the late 1970s, these Tamil political parties were succeeded in October 2001 by the Tamil National Alliance. Janatha Vimukthi Peramuna, a Marxist–Leninist political party founded by Rohana Wijeweera in 1965, serves as a third force in the current political context. It endorses leftist policies which are more radical than the traditionalist leftist politics of the LSSP and the Communist Party. Founded in 1981, the Sri Lanka Muslim Congress is the largest Muslim political party in Sri Lanka.

For administrative purposes, Sri Lanka is divided into nine provinces and twenty-five districts.

Provinces
There have been provinces in Sri Lanka since the 19th century, but they had no legal status until 1987 when the 13th Amendment to the 1978 constitution established provincial councils after several decades of increasing demand for a decentralisation of the government. Each provincial council is an autonomous body not under the authority of any ministry. Some of its functions had been undertaken by central government ministries, departments, corporations, and statutory authorities, but authority over land and police is not as a rule given to provincial councils. Between 1989 and 2006, the Northern and Eastern provinces were temporarily merged to form the North-East Province. Prior to 1987, all administrative tasks for the provinces were handled by a district-based civil service which had been in place since colonial times. Now each province is administered by a directly elected provincial council:

Districts and local authorities
Each district is administered under a district secretariat. The districts are further subdivided into 256 divisional secretariats, and these to approximately 14,008 Grama Niladhari divisions. The districts are known in Sinhala as "disa" and in Tamil as "māwaddam". Originally, a "disa" (usually rendered into English as Dissavony) was a duchy, notably Matale and Uva. 

There are three other types of local authorities: municipal councils (18), urban councils (13) and pradeshiya sabha, also called pradesha sabhai (256). Local authorities were originally based on feudal counties named "korale" and "rata", and were formerly known as "D.R.O. divisions" after the divisional revenue officer. Later the D.R.O.s became "assistant government agents," and the divisions were known as "A.G.A. divisions". These divisional secretariats are currently administered by a divisional secretary.

Sri Lanka is a founding member of the Non-Aligned Movement (NAM). While ensuring that it maintains its independence, Sri Lanka has cultivated relations with India. Sri Lanka became a member of the United Nations in 1955. Today, it is also a member of the Commonwealth, the SAARC, the World Bank, the International Monetary Fund, the Asian Development Bank, and the Colombo Plan.

The United National Party has traditionally favoured links with the West, while the Sri Lanka Freedom Party has favoured links with the East. Sri Lankan Finance Minister J. R. Jayewardene, together with then Australian Foreign Minister Sir Percy Spencer, proposed the Colombo Plan at the Commonwealth Foreign Minister's Conference held in Colombo in 1950. At the San Francisco Peace Conference in 1951, while many countries were reluctant, Sri Lanka argued for a free Japan and refused to accept payment of reparations for World War II damage because it believed it would harm Japan's economy. Sri Lanka-China relations started as soon as the People's Republic of China was formed in 1949. The two countries signed an important Rice-Rubber Pact in 1952. Sri Lanka played a vital role at the Asian–African Conference in 1955, which was an important step in the crystallisation of the NAM.

The Bandaranaike government of 1956 significantly changed the pro-western policies set by the previous UNP government. It recognised Cuba under Fidel Castro in 1959. Shortly afterward, Cuba's revolutionary Che Guevara paid a visit to Sri Lanka. The Sirima-Shastri Pact of 1964 and Sirima-Gandhi Pact of 1974 were signed between Sri Lankan and Indian leaders in an attempt to solve the long-standing dispute over the status of plantation workers of Indian origin. In 1974, Kachchatheevu, a small island in Palk Strait, was formally ceded to Sri Lanka. By this time, Sri Lanka was strongly involved in the NAM, and the fifth NAM summit was held in Colombo in 1976. The relationship between Sri Lanka and India became tense under the government of J. R. Jayawardene. As a result, India intervened in the Sri Lankan Civil War and subsequently deployed an Indian Peace Keeping Force in 1987. In the present, Sri Lanka enjoys extensive relations with China, Russia, and Pakistan.

The Sri Lanka Armed Forces, comprising the Sri Lanka Army, the Sri Lanka Navy, and the Sri Lanka Air Force, come under the purview of the Ministry of Defence. The total strength of the three services is around 346,000 personnel, with nearly 36,000 reserves. Sri Lanka has not enforced military conscription. Paramilitary units include the Special Task Force, the Civil Security Force, and the Sri Lanka Coast Guard.

Since independence in 1948, the primary focus of the armed forces has been internal security, crushing three major insurgencies, two by Marxist militants of the JVP and a 26-year-long conflict with the LTTE. The armed forces have been in a continuous mobilised state for the last 30 years. The Sri Lankan Armed Forces have engaged in United Nations peacekeeping operations since the early 1960s, contributing forces to permanent contingents deployed in several UN peacekeeping missions in Chad, Lebanon, and Haiti.

According to the International Monetary Fund, Sri Lanka's GDP in terms of purchasing power parity is the second most highest in the South Asian region in terms of per capita income. In the 19th and 20th centuries, Sri Lanka became a plantation economy famous for its production and export of cinnamon, rubber, and Ceylon tea, which remains a trademark national export. The development of modern ports under British rule raised the strategic importance of the island as a centre of trade. From 1948 to 1977, socialism strongly influenced the government's economic policies. Colonial plantations were dismantled, industries were nationalised, and a welfare state established. In 1977, the free market economy was introduced to the country incorporating privatisation, deregulation, and the promotion of private enterprise.

While the production and export of tea, rubber, coffee, sugar, and other commodities remain important, industrialisation has increased the importance of food processing, textiles, telecommunications, and finance. The country's main economic sectors are tourism, tea export, clothing, rice production, and other agricultural products. In addition to these economic sectors, overseas employment, especially in the Middle East, contributes substantially in foreign exchange.

, the service sector makes up 60% of GDP, the industrial sector 28%, and the agriculture sector 12%. The private sector accounts for 85% of the economy. China, India and the United States are Sri Lanka's largest trading partners. Economic disparities exist between the provinces with the Western Province contributing 45.1% of the GDP and the Southern Province and the Central Province contributing 10.7% and 10%, respectively. With the end of the war, the Northern Province reported a record 22.9% GDP growth in 2010.
The per capita income of Sri Lanka doubled from 2005 to 2011. During the same period, poverty dropped from 15.2% to 7.6%, unemployment rate dropped from 7.2% to 4.9%, market capitalisation of the Colombo Stock Exchange quadrupled, and the budget deficit doubled. Over 90% of the households in Sri Lanka are electrified; 87% of the population have access to safe drinking water; and 39% have access to pipe-borne water. Income inequality has also dropped in recent years, indicated by a Gini coefficient of 0.36 in 2010. 

The 2011 Global Competitiveness Report, published by the World Economic Forum, described Sri Lanka's economy as transitioning from the factor-driven stage to the efficiency-driven stage and that it ranked 52nd in global competitiveness. Also, out of the 142 countries surveyed, Sri Lanka ranked 45th in health and primary education, 32nd in business sophistication, 42nd in innovation, and 41st in goods market efficiency. In 2016, Sri Lanka ranked 5th in the World Giving Index, registering high levels of contentment and charitable behaviour in its society. In 2010, "The New York Times" placed Sri Lanka at the top of its list of 31 places to visit. S&P Dow Jones Indices classifies Sri Lanka as a frontier market as of 2018. Sri Lanka ranks well above other South Asian countries in the Human Development Index (HDI) with an index of 0.750.

By 2016, the country's debt soared as it was developing its infrastructure to the point of near bankruptcy which required a bailout from the International Monetary Fund (IMF) The IMF had agreed to provide a US$1.5 billion bailout loan in April 2016 after Sri Lanka provided a set of criteria intended to improve its economy. By the fourth quarter of 2016, the debt was estimated to be $64.9 billion. Additional debt had been incurred in the past by state-owned organisations and this was said to be at least $9.5 billion. Since early 2015, domestic debt increased by 12% and external debt by 25%. In November 2016, the IMF reported that the initial disbursement was larger than US$150 million originally planned, a full US$162.6 million (SDR 119.894 million). The agency's evaluation for the first tranche was cautiously optimistic about the future. Under the program Sri Lankan government implemented a new Inland Revenue Act and an automatic fuel pricing formula which were noted by the IMF in its fourth review. In 2018 China agreed to bail out Sri Lanka with a loan of $1.25 billion to deal with foreign debt repayment spikes in 2019 to 2021.

Sri Lanka has roughly 21,670,000 people and an annual population growth rate of 1.14%. The birth rate is 17.6 births per 1,000 people, and the death rate is 6.2 deaths per 1,000 people. Population density is highest in western Sri Lanka, especially in and around the capital. Sinhalese constitute the largest ethnic group in the country, with 74.8% of the total population. Sri Lankan Tamils are the second major ethnic group in the island, with a percentage of 11.2%. Moors comprise 9.2%. There are also small ethnic groups such as the Burghers (of mixed European descent) and Malays from Southeast Asia. Moreover, there is a small population of Vedda people who are believed to be the original indigenous group to inhabit the island.

Sinhala and Tamil are the two official languages. The constitution defines English as the link language. English is widely used for education, scientific and commercial purposes. Members of the Burgher community speak variant forms of Portuguese Creole and Dutch with varying proficiency, while members of the Malay community speak a form of Creole Malay that is unique to the island.

Buddhists comprise 70% of the population, with the Theravada school being predominant. Most Buddhists are of the Sinhalese ethnic group. Buddhism was introduced to Sri Lanka in the 2nd century BCE by Venerable Mahinda. A sapling of the Bodhi Tree under which the Buddha attained enlightenment was brought to Sri Lanka during the same time. The Pāli Canon ("Thripitakaya"), having previously been preserved as an oral tradition, was first committed to writing in Sri Lanka around 30 BCE. Sri Lanka has the longest continuous history of Buddhism of any predominantly Buddhist nation. During periods of decline, the Sri Lankan monastic lineage was revived through contact with Thailand and Burma. Buddhism is given special recognition in the constitution which requires Sri Lankans to "protect and foster the Buddha Sasana".

Hinduism is the second most prevalent religion and predates Buddhism. Today, Hinduism is dominant in Northern, Eastern and Central Sri Lanka. Islam is the third most prevalent religion in the country, having first been brought to the island by Arab traders over the course of many centuries, starting around the 7th century CE. Most Muslims are Sunni who follow the Shafi'i school. Most followers today are believed to be descendants of those Arab traders and the local women they married. Christianity reached the country through Western colonists in the early 16th century. Around 7.4% of the Sri Lankan population are Christians, of whom 82% are Roman Catholics who trace their religious heritage directly to the Portuguese. Tamil Catholics attribute their religious heritage to St. Francis Xavier as well as Portuguese missionaries. The remaining Christians are evenly split between the Anglican Church of Ceylon and other Protestant denominations. There is also a small population of Zoroastrian immigrants from India (Parsis) who settled in Ceylon during the period of British rule, but this community has steadily dwindled in recent years. 

Religion plays a prominent role in the life and culture of Sri Lankans. The Buddhist majority observe Poya Days each month according to the Lunar calendar, and Hindus and Muslims also observe their own holidays. In a 2008 Gallup poll, Sri Lanka was ranked the third most religious country in the world, with 99% of Sri Lankans saying religion was an important part of their daily life.

Sri Lankans have a life expectancy of 77.9 years at birth, which is 10% higher than the world average. The infant mortality rate stands at 8.5 per 1,000 births and the maternal mortality rate at 0.39 per 1,000 births, which is on par with figures from the developed countries. The universal "pro-poor" health care system adopted by the country has contributed much towards these figures. Sri Lanka ranks first among southeast Asian countries with respect to commitment of suicide, with 33 deaths per 100,000 persons. According to the Department of Census and Statistics, poverty, destructive pastimes, and inability to cope with stressful situations are the main causes behind the high suicide rates.

With a literacy rate of 92.5%, Sri Lanka has one of the most literate populations amongst developing nations. Its youth literacy rate stands at 98%, computer literacy rate at 35%, and primary school enrollment rate at over 99%. An education system which dictates 9 years of compulsory schooling for every child is in place. 

The free education system established in 1945 is a result of the initiative of C. W. W. Kannangara and A. Ratnayake. It is one of the few countries in the world that provide universal free education from primary to tertiary stage. Kannangara led the establishment of the Madhya Vidyalayas (central schools) in different parts of the country in order to provide education to Sri Lanka's rural children. In 1942, a special education committee proposed extensive reforms to establish an efficient and quality education system for the people. However, in the 1980s changes to this system separated the administration of schools between the central government and the provincial government. Thus the elite national schools are controlled directly by the ministry of education and the provincial schools by the provincial government. Sri Lanka has approximately 9,675 government schools and 817 private schools and pirivenas.

Sri Lanka has 15 public universities. A lack of responsiveness of the education system to labour market requirements, disparities in access to quality education, lack of an effective linkage between secondary and tertiary education remain major challenges for the education sector. A number of private, degree awarding institutions have emerged in recent times to fill in these gaps, yet the participation at tertiary level education remains at 5.1%. 

Science fiction author Arthur C. Clarke served as chancellor of Moratuwa University from 1979 to 2002.

Sri Lanka has an extensive road network for inland transportation. With more than of paved roads, it has one of the highest road densities in the world ( of paved roads per every of land). The road network consists of 35 A-Grade highways and two controlled-access highways (E01 and E03). A and B grade roads are national (arterial) highways administered by Road Development Authority. C and D grade roads are provincial roads coming under the purview of the Provincial Road Development Authority of the respective province. The other roads are local roads falling under local government authorities.

The railway network, operated by the state-run national railway operator Sri Lanka Railways, spans . Sri Lanka also has three deep-water ports at Colombo, Galle, and Trincomalee, in addition to the newest port being built at Hambantota. 

The Sri Lanka Broadcasting Corporation (formerly Radio Ceylon) is the oldest-running radio station in Asia, established in 1923 by Edward Harper just three years after broadcasting began in Europe. The station broadcasts services in Sinhala, Tamil, English and Hindi. Since the 1980s, many private radio stations have also been introduced. Broadcast television was introduced in 1979 when the Independent Television Network was launched. Initially, all television stations were state-controlled, but private television networks began broadcasting in 1992.

, 51 newspapers (30 Sinhala, 10 Tamil, 11 English) are published and 34 TV stations and 52 radio stations are in operation. In recent years, freedom of the press in Sri Lanka has been alleged by media freedom groups to be amongst the poorest in democratic countries. Alleged abuse of a newspaper editor by a senior government minister achieved international notoriety because of the unsolved murder of the editor's predecessor, Lasantha Wickrematunge, who had been a critic of the government and had presaged his own death in a posthumously published article.

Officially, the constitution of Sri Lanka guarantees human rights as ratified by the United Nations. However, human rights has come under criticism by Amnesty International, Freedom from Torture, Human Rights Watch, and the United States Department of State. British colonial rulers, the LTTE, and the government of Sri Lanka have been accused of violating human rights. A report by an advisory panel to the UN secretary-general accused both the LTTE and the Sri Lankan government of war crimes during final stages of the civil war. Corruption remains a problem in Sri Lanka, and there is little protection for those who stand up against corruption. The 135-year-old Article 365 of the Sri Lankan Penal Code criminalises gay sex and provides for a penalty of up to ten years in prison.

The UN Human Rights Council has documented over 12,000 named individuals who have disappeared after detention by security forces in Sri Lanka, the second highest figure in the world since the Working Group came into being in 1980. The Sri Lankan government confirmed that 6,445 of these died. Allegations of human rights abuses have not ended with the close of the ethnic conflict.

UN Human Rights Commissioner Navanethem Pillay visited Sri Lanka in May 2013. After her visit, she said: "The war may have ended [in Sri Lanka], but in the meantime democracy has been undermined and the rule of law eroded." Pillay spoke about the military's increasing involvement in civilian life and reports of military land grabbing. She also said that, while in Sri Lanka, she had been allowed to go wherever she wanted, but that Sri Lankans who came to meet her were harassed and intimidated by security forces.

In 2012, the UK charity Freedom from Torture reported that it had received 233 referrals of torture survivors from Sri Lanka for clinical treatment or other services provided by the charity. In the same year, the group published "Out of the Silence", which documents evidence of torture in Sri Lanka and demonstrates that the practice has continued long after the end of the civil war in 2009. On 29 July 2020, Human Rights Watch said that the Sri Lanka government has targeted lawyers, human rights defenders, and journalists to suppress criticism against government.

The culture of Sri Lanka is influenced primarily by Buddhism and Hinduism. Sri Lanka is the home to two main traditional cultures: the Sinhalese (centred in Kandy and Anuradhapura) and the Tamil (centred in Jaffna). Tamils co-existed with the Sinhalese people since then, and the early mixing rendered the two ethnic groups almost physically indistinct. Ancient Sri Lanka is marked for its genius in hydraulic engineering and architecture. The British colonial culture has also influenced the locals. The rich cultural traditions shared by all Sri Lankan cultures is the basis of the country's long life expectancy, advanced health standards and high literacy rate.

Dishes include rice and curry, pittu, kiribath, wholemeal roti, string hoppers, wattalapam (a rich pudding of Malay origin made with coconut milk, jaggery, cashews, eggs, and spices including cinnamon and nutmeg), kottu, and appam. Jackfruit may sometimes replace rice. Traditionally food is served on a plantain leaf or lotus leaf. Middle Eastern influences and practices are found in traditional Moor dishes, while Dutch and Portuguese influences are found with the island's Burgher community preserving their culture through traditional dishes such as lamprais (rice cooked in stock and baked in a banana leaf), breudher (Dutch holiday biscuit), and bolo fiado (Portuguese-style layer cake).

In April, Sri Lankans celebrate the Buddhist and Hindu new year festivals. Esala Perahera is a symbolic Buddhist festival consisting of dances and decorated elephants held in Kandy in July and August. Fire dances, whip dances, Kandian dances and various other cultural dances are integral parts of the festival. Christians celebrate Christmas on 25 December to celebrate the birth of Jesus Christ and Easter to celebrate the resurrection of Jesus. Tamils celebrate Thai Pongal and Maha Shivaratri, and Muslims celebrate Hajj and Ramadan.

The movie "Kadawunu Poronduwa" ("The Broken Promise"), produced by S. M. Nayagam of Chitra Kala Movietone, heralded the coming of Sri Lankan cinema in 1947. "Ranmuthu Duwa" ("Island of Treasures") marked the transition cinema from black-and-white to colour. In recent years, movies have featured subjects such as family melodrama, social transformation and the years of conflict between the military and the LTTE. The Sri Lankan cinematic style is similar to Bollywood movies. In 1979, movie attendance rose to an all-time high, but has been in steady decline since then.

An influential filmmaker is Lester James Peiris, who has directed a number of movies which led to global acclaim, including "Rekava" ("Line of Destiny", 1956), "Gamperaliya" ("The Changing Village", 1964), "Nidhanaya" ("The Treasure", 1970) and "Golu Hadawatha" ("Cold Heart", 1968). Sri Lankan-Canadian poet Rienzi Crusz, is the subject of a documentary on his life in Sri Lanka. His work is published in Sinhala and English. Naturalised Canadian Michael Ondaatje is well known for his English-language novels and three films.

The earliest music in Sri Lanka came from theatrical performances such as "Kolam", "Sokari" and "Nadagam". Traditional music instruments such as "Béra", "Thammátama", "Daŭla" and "Răbān" were performed at these dramas. The first music album, "Nurthi", recorded in 1903, was released through Radio Ceylon. Songwriters like Mahagama Sekara and Ananda Samarakoon and musicians such as W. D. Amaradeva, Victor Ratnayake, Nanda Malini and Clarence Wijewardene have contributed much towards the progression of Sri Lankan music. Baila originated among Kaffirs or the Afro-Sinhalese community.
There are three main styles of Sri Lankan classical dance. They are, the Kandyan dances, low country dances and Sabaragamuwa dances. Of these, the Kandyan style is most prominent. It is a sophisticated form of dance that consists of five sub-categories: "Ves dance", "Naiyandi dance", "Udekki dance", "Pantheru dance" and "18 Vannam". An elaborate headdress is worn by the male dancers, and a drum called "Geta Béraya" is used to assist the dancers to keep on rhythm. 

The history of Sri Lankan painting and sculpture can be traced as far back as to the 2nd or 3rd century BC. The earliest mention about the art of painting on Mahavamsa, is to the drawing of a palace on cloth using cinnabar in the 2nd century BC. The chronicles have description of various paintings in relic-chambers of Buddhist stupas and in monastic residence.

Theatre came to the country when a Parsi theatre company from Mumbai introduced "Nurti", a blend of European and Indian theatrical conventions to the Colombo audience in the 19th century. The golden age of Sri Lankan drama and theatre began with the staging of "Maname", a play written by Ediriweera Sarachchandra in 1956. It was followed by a series of popular dramas like "Sinhabāhu", "Pabāvatī", "Mahāsāra", "Muudu Puththu" and "Subha saha Yasa".

Sri Lankan literature spans at least two millennia and is heir to the Aryan literary tradition as embodied in the hymns of the Rigveda. The Pāli Canon, the standard collection of scriptures in the Theravada Buddhist tradition, was written down in Sri Lanka during the Fourth Buddhist council, at the Alulena cave temple, Kegalle, as early as 29 BC. Chronicles such as the "Mahāvamsa", written in the 6th century, provide vivid descriptions of Sri Lankan dynasties. According to the German philosopher Wilhelm Geiger, the chronicles are based on Sinhala Atthakatha (commentary). The oldest surviving prose work is the "Dhampiya-Atuva-Getapadaya", compiled in the 9th century AD. The greatest literary feats of medieval Sri Lanka include "Sandesha Kāvya" (poetic messages) such as "Girā Sandeshaya" (parrot message), "Hansa Sandeshaya" (swan message) and "Salalihini Sandeshaya" (myna message). Poetry including "Kavsilumina", "Kavya-Sekharaya" ("Diadem of Poetry") and proses such as "Saddharma-Ratnāvaliya", "Amāvatura" ("Flood of Nectar") and "Pujāvaliya" are also notable works of this period, which is considered to be the golden age of Sri Lankan literature. The first modern-day novel, "Meena" by Simon de Silva appeared in 1905 and was followed by several revolutionary literary works. Martin Wickramasinghe, the author of "Madol Doova" is considered the iconic figure of Sri Lankan literature.

While the national sport is volleyball, by far the most popular sport in the country is cricket. Rugby union also enjoys extensive popularity, as do association football, netball and tennis. Aquatic sports such as boating, surfing, swimming, kitesurfing and scuba diving attract many Sri Lankans and foreign tourists. There are two styles of martial arts native to Sri Lanka: Cheena di and Angampora.

The Sri Lanka national cricket team achieved considerable success beginning in the 1990s, rising from underdog status to winning the 1996 Cricket World Cup. They also won the 2014 ICC World Twenty20 played in Bangladesh, beating India in the final. In addition, Sri Lanka became the runners-up of the Cricket World Cup in 2007 and 2011, and of the ICC World Twenty20 in 2009 and 2012. Former Sri Lankan off-spinner Muttiah Muralitharan has been rated as the greatest test match bowler ever by "Wisden Cricketers' Almanack", and four Sri Lankan cricketers ranked 2nd (Sangakkara), 4th (Jayasuriya), 5th (Jayawardene) and 11th (Dilshan) highest ODI run scorers of all time, which is the second best by a team. Sri Lanka has won the Asia Cup in 1986, 1997, 2004, 2008 and 2014. Sri Lanka once held highest team score in all three formats of cricket. The country co-hosted the Cricket World Cup in 1996 and 2011, and hosted the 2012 ICC World Twenty20.

Sri Lankans have won two medals at Olympic Games: one silver, by Duncan White at 1948 London Olympics for men's 400 metres hurdles; and one silver by Susanthika Jayasinghe at 2000 Sydney Olympics for women's 200 metres. In 1973, Muhammad Lafir won the World Billiards Championship, the highest feat by a Sri Lankan in a Cue sport. Sri Lanka has also won the Carrom World Championship titles twice in 2012, 2016 and 2018, men's team becoming champions and women's team won second place. 


Government

Overviews and data

History

Maps

Trade


</doc>
<doc id="26751" url="https://en.wikipedia.org/wiki?curid=26751" title="Sun">
Sun

The Sun is the star at the center of the Solar System. It is a nearly perfect sphere of hot plasma, heated to incandescence by nuclear fusion reactions in its core, radiating the energy mainly as light and infrared radiation. It is by far the most important source of energy for life on Earth. Its diameter is about 1.39 million kilometers (864,000 miles), or 109 times that of Earth, and its mass is about 330,000 times that of Earth. It accounts for about 99.86% of the total mass of the Solar System.
Roughly three quarters of the Sun's mass consists of hydrogen (~73%); the rest is mostly helium (~25%), with much smaller quantities of heavier elements, including oxygen, carbon, neon, and iron.

The Sun is a G-type main-sequence star (G2V) based on its spectral class. As such, it is informally and not completely accurately referred to as a yellow dwarf (its light is closer to white than yellow). It formed approximately 4.6 billion years ago from the gravitational collapse of matter within a region of a large molecular cloud. Most of this matter gathered in the center, whereas the rest flattened into an orbiting disk that became the Solar System. The central mass became so hot and dense that it eventually initiated nuclear fusion in its core. It is thought that almost all stars form by this process.

The Sun currently fuses about 600 million tons of hydrogen into helium every second, converting 4 million tons of matter into energy every second as a result. This energy, which can take between 10,000 and 170,000 years to escape from its core, is the source of the Sun's light and heat. When hydrogen fusion in its core has diminished to the point at which the Sun is no longer in hydrostatic equilibrium, its core will undergo a marked increase in density and temperature while its outer layers expand, eventually transforming the Sun into a red giant. It is calculated that the Sun will become sufficiently large to engulf the current orbits of Mercury and Venus, and render Earth uninhabitable – but not for about five billion years. After this, it will shed its outer layers and become a dense type of cooling star known as a white dwarf, and no longer produce energy by fusion, but still glow and give off heat from its previous fusion.

The enormous effect of the Sun on Earth has been recognized since prehistoric times, and the Sun has been regarded by some cultures as a deity. The synodic rotation of Earth and its orbit around the Sun are the basis of solar calendars, one of which is the predominant calendar in use today.

The English word "sun" developed from Old English "sunne". Cognates appear in other Germanic languages, including West Frisian "sinne", Dutch "zon", Low German "Sünn", Standard German "Sonne", Bavarian "Sunna", Old Norse "sunna" and Gothic "sunnō". All these words stem from Proto-Germanic *sunnōn. This is ultimately related to the word for "sun" in other branches of the Indo-European language family, though in most cases a nominative stem with an "l" is found, rather than the genitive stem in "n", as for example in Latin "sōl", Greek ἥλιος "hēlios", Welsh "haul" and Russian солнце "solntse" (pronounced "sontse"), as well as (with *l > "r") Sanskrit स्वर "svár" and Persian خور‎ "xvar". Indeed, the "l"-stem survived in Proto-Germanic as well, as *sōwelan, which gave rise to Gothic "sauil" (alongside "sunnō") and Old Norse prosaic "sól" (alongside poetic "sunna"), and through it the words for "sun" in the modern Scandinavian languages: Swedish and Danish "solen", Icelandic "sólin", etc.

In English, the Greek and Latin words occur in poetry as personifications of the Sun, Helios and Sol , while in science fiction "Sol" may be used as a name for the Sun to distinguish it from others. The term "sol" with a lower-case 's' is used by planetary astronomers for the duration of a solar day on another planet such as Mars.

The principal adjectives for the Sun in English are "sunny" for sunlight and, in technical contexts, "solar" , from Latin "sol" – the latter found in terms such as "solar day", "solar eclipse" and "Solar System" (occasionally "Sol system").
From the Greek "helios" comes the rare adjective "heliac" .

The English weekday name "Sunday" stems from Old English "Sunnandæg" "sun's day", a Germanic interpretation of the Latin phrase "diēs sōlis", itself a translation of the Greek ἡμέρα ἡλίου "hēmera hēliou" "day of the sun".

The Sun is a G-type main-sequence star that comprises about 99.86% of the mass of the Solar System. The Sun has an absolute magnitude of +4.83, estimated to be brighter than about 85% of the stars in the Milky Way, most of which are red dwarfs. The Sun is a Population I, or heavy-element-rich, star. The formation of the Sun may have been triggered by shockwaves from one or more nearby supernovae. This is suggested by a high abundance of heavy elements in the Solar System, such as gold and uranium, relative to the abundances of these elements in so-called Population II, heavy-element-poor, stars. The heavy elements could most plausibly have been produced by endothermic nuclear reactions during a supernova, or by transmutation through neutron absorption within a massive second-generation star.

The Sun is by far the brightest object in the Earth's sky, with an apparent magnitude of −26.74. This is about 13 billion times brighter than the next brightest star, Sirius, which has an apparent magnitude of −1.46. is defined as the mean distance of the Sun's center to Earth's center, though the distance varies as Earth moves from perihelion in January to aphelion in July. At this average distance, light travels from the Sun's horizon to Earth's horizon in about 8 minutes and 19 seconds, while light from the closest points of the Sun and Earth takes about two seconds less. The energy of this sunlight supports almost all life on Earth by photosynthesis, and drives Earth's climate and weather.

The Sun does not have a definite boundary, but its density decreases exponentially with increasing height above the photosphere. For the purpose of measurement, the Sun's radius is considered to be the distance from its center to the edge of the photosphere, the apparent visible surface of the Sun. By this measure, the Sun is a near-perfect sphere with an oblateness estimated at about 9 millionths, which means that its polar diameter differs from its equatorial diameter by only . The tidal effect of the planets is weak and does not significantly affect the shape of the Sun. The Sun rotates faster at its equator than at its poles. This differential rotation is caused by convective motion due to heat transport and the Coriolis force due to the Sun's rotation. In a frame of reference defined by the stars, the rotational period is approximately 25.6 days at the equator and 33.5 days at the poles. Viewed from Earth as it orbits the Sun, the "apparent rotational period" of the Sun at its equator is about 28 days. Viewed from a vantage point above its north pole, the Sun rotates counterclockwise around its axis of spin.

The solar constant is the amount of power that the Sun deposits per unit area that is directly exposed to sunlight. The solar constant is equal to approximately (watts per square meter) at a distance of one astronomical unit (AU) from the Sun (that is, on or near Earth). Sunlight on the surface of Earth is attenuated by Earth's atmosphere, so that less power arrives at the surface (closer to ) in clear conditions when the Sun is near the zenith. Sunlight at the top of Earth's atmosphere is composed (by total energy) of about 50% infrared light, 40% visible light, and 10% ultraviolet light. The atmosphere in particular filters out over 70% of solar ultraviolet, especially at the shorter wavelengths. Solar ultraviolet radiation ionizes Earth's dayside upper atmosphere, creating the electrically conducting ionosphere.

The Sun's color is white, with a CIE color-space index near (0.3, 0.3), when viewed from space or when the Sun is high in the sky. When measuring all the photons emitted, the Sun is emitting more photons in the green portion of the spectrum than any other. When the Sun is low in the sky, atmospheric scattering renders the Sun yellow, red, orange, or magenta. Despite its typical whiteness, most people mentally picture the Sun as yellow; the reasons for this are the subject of debate.
The Sun is a G2V star, with "G2" indicating its surface temperature of approximately 5,778 K (5,505 °C, 9,941 °F), and "V" that it, like most stars, is a main-sequence star. The average luminance of the Sun is about 1.88 giga candela per square metre, but as viewed through Earth's atmosphere, this is lowered to about 1.44 Gcd/m. However, the luminance is not constant across the disk of the Sun (limb darkening).

The Sun is composed primarily of the chemical elements hydrogen and helium. At this time in the Sun's life, they account for 74.9% and 23.8% of the mass of the Sun in the photosphere, respectively. All heavier elements, called "metals" in astronomy, account for less than 2% of the mass, with oxygen (roughly 1% of the Sun's mass), carbon (0.3%), neon (0.2%), and iron (0.2%) being the most abundant.

The Sun's original chemical composition was inherited from the interstellar medium out of which it formed. Originally it would have contained about 71.1% hydrogen, 27.4% helium, and 1.5% heavier elements. The hydrogen and most of the helium in the Sun would have been produced by Big Bang nucleosynthesis in the first 20 minutes of the universe, and the heavier elements were produced by previous generations of stars before the Sun was formed, and spread into the interstellar medium during the final stages of stellar life and by events such as supernovae.

Since the Sun formed, the main fusion process has involved fusing hydrogen into helium. Over the past 4.6 billion years, the amount of helium and its location within the Sun has gradually changed. Within the core, the proportion of helium has increased from about 24% to about 60% due to fusion, and some of the helium and heavy elements have settled from the photosphere towards the center of the Sun because of gravity. The proportions of metals (heavier elements) is unchanged. Heat is transferred outward from the Sun's core by radiation rather than by convection (see Radiative zone below), so the fusion products are not lifted outward by heat; they remain in the core and gradually an inner core of helium has begun to form that cannot be fused because presently the Sun's core is not hot or dense enough to fuse helium. In the current photosphere the helium fraction is reduced, and the metallicity is only 84% of what it was in the protostellar phase (before nuclear fusion in the core started). In the future, helium will continue to accumulate in the core, and in about 5 billion years this gradual build-up will eventually cause the Sun to exit the main sequence and become a red giant.

The chemical composition of the photosphere is normally considered representative of the composition of the primordial Solar System. The solar heavy-element abundances described above are typically measured both using spectroscopy of the Sun's photosphere and by measuring abundances in meteorites that have never been heated to melting temperatures. These meteorites are thought to retain the composition of the protostellar Sun and are thus not affected by settling of heavy elements. The two methods generally agree well.

In the 1970s, much research focused on the abundances of iron-group elements in the Sun. Although significant research was done, until 1978 it was difficult to determine the abundances of some iron-group elements (e.g. cobalt and manganese) via spectrography because of their hyperfine structures.

The first largely complete set of oscillator strengths of singly ionized iron-group elements were made available in the 1960s, and these were subsequently improved. In 1978, the abundances of singly ionized elements of the iron group were derived.

Various authors have considered the existence of a gradient in the isotopic compositions of solar and planetary noble gases, e.g. correlations between isotopic compositions of neon and xenon in the Sun and on the planets.

Prior to 1983, it was thought that the whole Sun has the same composition as the solar atmosphere. In 1983, it was claimed that it was fractionation in the Sun itself that caused the isotopic-composition relationship between the planetary and solar-wind-implanted noble gases.

The structure of the Sun contains the following layers:

The core of the Sun extends from the center to about 20–25% of the solar radius. It has a density of up to (about 150 times the density of water) and a temperature of close to 15.7 million kelvin (K). By contrast, the Sun's surface temperature is approximately . Recent analysis of SOHO mission data favors a faster rotation rate in the core than in the radiative zone above. Through most of the Sun's life, energy has been produced by nuclear fusion in the core region through a series of nuclear reactions called the p–p (proton–proton) chain; this process converts hydrogen into helium. Only 0.8% of the energy generated in the Sun comes from another sequence of fusion reactions called the CNO cycle, though this proportion is expected to increase as the Sun becomes older.

The core is the only region in the Sun that produces an appreciable amount of thermal energy through fusion; 99% of the power is generated within 24% of the Sun's radius, and by 30% of the radius, fusion has stopped nearly entirely. The remainder of the Sun is heated by this energy as it is transferred outwards through many successive layers, finally to the solar photosphere where it escapes into space through radiation (photons) or advection (massive particles).

The proton–proton chain occurs around times each second in the core, converting about 3.7 protons into alpha particles (helium nuclei) every second (out of a total of ~8.9 free protons in the Sun), or about . Fusing four free protons (hydrogen nuclei) into a single alpha particle (helium nucleus) releases around 0.7% of the fused mass as energy, so the Sun releases energy at the mass–energy conversion rate of 4.26 million metric tons per second (which requires 600 metric megatons of hydrogen ), for 384.6 yottawatts (), or 9.192 megatons of TNT per second. The large power output of the Sun is mainly due to the huge size and density of its core (compared to Earth and objects on Earth), with only a fairly small amount of power being generated per cubic metre. Theoretical models of the Sun's interior indicate a maximum power density, or energy production, of approximately 276.5 watts per cubic metre at the center of the core, which is about the same power density as in body of a reptile or inside a compost pile.

The fusion rate in the core is in a self-correcting equilibrium: a slightly higher rate of fusion would cause the core to heat up more and expand slightly against the weight of the outer layers, reducing the density and hence the fusion rate and correcting the perturbation; and a slightly lower rate would cause the core to cool and shrink slightly, increasing the density and increasing the fusion rate and again reverting it to its present rate.

From the core out to about 0.7 solar radii, thermal radiation is the primary means of energy transfer. The temperature drops from approximately 7 million to 2 million kelvins with increasing distance from the core. This temperature gradient is less than the value of the adiabatic lapse rate and hence cannot drive convection, which explains why the transfer of energy through this zone is by radiation instead of thermal convection. Ions of hydrogen and helium emit photons, which travel only a brief distance before being reabsorbed by other ions. The density drops a hundredfold (from 20 g/cm to 0.2 g/cm) from 0.25 solar radii to the 0.7 radii, the top of the radiative zone.

The radiative zone and the convective zone are separated by a transition layer, the tachocline. This is a region where the sharp regime change between the uniform rotation of the radiative zone and the differential rotation of the convection zone results in a large shear between the two—a condition where successive horizontal layers slide past one another. Presently, it is hypothesized (see Solar dynamo) that a magnetic dynamo within this layer generates the Sun's magnetic field.

The Sun's convection zone extends from 0.7 solar radii (500,000 km) to near the surface. In this layer, the solar plasma is not dense enough or hot enough to transfer the heat energy of the interior outward via radiation. Instead, the density of the plasma is low enough to allow convective currents to develop and move the Sun's energy outward towards its surface. Material heated at the tachocline picks up heat and expands, thereby reducing its density and allowing it to rise. As a result, an orderly motion of the mass develops into thermal cells that carry the majority of the heat outward to the Sun's photosphere above. Once the material diffusively and radiatively cools just beneath the photospheric surface, its density increases, and it sinks to the base of the convection zone, where it again picks up heat from the top of the radiative zone and the convective cycle continues. At the photosphere, the temperature has dropped to 5,700 K and the density to only 0.2 g/m (about 1/6,000 the density of air at sea level).

The thermal columns of the convection zone form an imprint on the surface of the Sun giving it a granular appearance called the solar granulation at the smallest scale and supergranulation at larger scales. Turbulent convection in this outer part of the solar interior sustains "small-scale" dynamo action over the near-surface volume of the Sun. The Sun's thermal columns are Bénard cells and take the shape of hexagonal prisms.

The visible surface of the Sun, the photosphere, is the layer below which the Sun becomes opaque to visible light. Photons produced in this layer escape the Sun through the transparent solar atmosphere above it and become solar radiation, sunlight. The change in opacity is due to the decreasing amount of H ions, which absorb visible light easily. Conversely, the visible light we see is produced as electrons react with hydrogen atoms to produce H ions.
The photosphere is tens to hundreds of kilometers thick, and is slightly less opaque than air on Earth. Because the upper part of the photosphere is cooler than the lower part, an image of the Sun appears brighter in the center than on the edge or "limb" of the solar disk, in a phenomenon known as limb darkening. The spectrum of sunlight has approximately the spectrum of a black-body radiating at , interspersed with atomic absorption lines from the tenuous layers above the photosphere. The photosphere has a particle density of ~10 m (about 0.37% of the particle number per volume of Earth's atmosphere at sea level). The photosphere is not fully ionized—the extent of ionization is about 3%, leaving almost all of the hydrogen in atomic form.

During early studies of the optical spectrum of the photosphere, some absorption lines were found that did not correspond to any chemical elements then known on Earth. In 1868, Norman Lockyer hypothesized that these absorption lines were caused by a new element that he dubbed "helium", after the Greek Sun god Helios. Twenty-five years later, helium was isolated on Earth.

During a total solar eclipse, when the disk of the Sun is covered by that of the Moon, parts of the Sun's surrounding atmosphere can be seen. It is composed of four distinct parts: the chromosphere, the transition region, the corona and the heliosphere.

The coolest layer of the Sun is a temperature minimum region extending to about above the photosphere, and has a temperature of about . This part of the Sun is cool enough to allow the existence of simple molecules such as carbon monoxide and water, which can be detected via their absorption spectra.

The chromosphere, transition region, and corona are much hotter than the surface of the Sun. The reason is not well understood, but evidence suggests that Alfvén waves may have enough energy to heat the corona.

Above the temperature minimum layer is a layer about thick, dominated by a spectrum of emission and absorption lines. It is called the "chromosphere" from the Greek root "chroma", meaning color, because the chromosphere is visible as a colored flash at the beginning and end of total solar eclipses. The temperature of the chromosphere increases gradually with altitude, ranging up to around near the top. In the upper part of the chromosphere helium becomes partially ionized.

Above the chromosphere, in a thin (about ) transition region, the temperature rises rapidly from around in the upper chromosphere to coronal temperatures closer to . The temperature increase is facilitated by the full ionization of helium in the transition region, which significantly reduces radiative cooling of the plasma. The transition region does not occur at a well-defined altitude. Rather, it forms a kind of nimbus around chromospheric features such as spicules and filaments, and is in constant, chaotic motion. The transition region is not easily visible from Earth's surface, but is readily observable from space by instruments sensitive to the extreme ultraviolet portion of the spectrum.

The corona is the next layer of the Sun. The low corona, near the surface of the Sun, has a particle density around 10 m to 10 m. The average temperature of the corona and solar wind is about 1,000,000–2,000,000 K; however, in the hottest regions it is 8,000,000–20,000,000 K. Although no complete theory yet exists to account for the temperature of the corona, at least some of its heat is known to be from magnetic reconnection.
The corona is the extended atmosphere of the Sun, which has a volume much larger than the volume enclosed by the Sun's photosphere. A flow of plasma outward from the Sun into interplanetary space is the solar wind.

The heliosphere, the tenuous outermost atmosphere of the Sun, is filled with the solar wind plasma. This outermost layer of the Sun is defined to begin at the distance where the flow of the solar wind becomes "superalfvénic"—that is, where the flow becomes faster than the speed of Alfvén waves, at approximately 20 solar radii (0.1 AU).
Turbulence and dynamic forces in the heliosphere cannot affect the shape of the solar corona within, because the information can only travel at the speed of Alfvén waves. The solar wind travels outward continuously through the heliosphere, forming the solar magnetic field into a spiral shape, until it impacts the heliopause more than from the Sun. In December 2004, the Voyager 1 probe passed through a shock front that is thought to be part of the heliopause. In late 2012 Voyager 1 recorded a marked increase in cosmic ray collisions and a sharp drop in lower energy particles from the solar wind, which suggested that the probe had passed through the heliopause and entered the interstellar medium.

High-energy gamma ray photons initially released with fusion reactions in the core are almost immediately absorbed by the solar plasma of the radiative zone, usually after traveling only a few millimeters. Re-emission happens in a random direction and usually at a slightly lower energy. With this sequence of emissions and absorptions, it takes a long time for radiation to reach the Sun's surface. Estimates of the photon travel time range between 10,000 and 170,000 years. In contrast, it takes only 2.3 seconds for the neutrinos, which account for about 2% of the total energy production of the Sun, to reach the surface. Because energy transport in the Sun is a process that involves photons in thermodynamic equilibrium with matter, the time scale of energy transport in the Sun is longer, on the order of 30,000,000 years. This is the time it would take the Sun to return to a stable state, if the rate of energy generation in its core were suddenly changed.

Neutrinos are also released by the fusion reactions in the core, but, unlike photons, they rarely interact with matter, so almost all are able to escape the Sun immediately. For many years measurements of the number of neutrinos produced in the Sun were lower than theories predicted by a factor of 3. This discrepancy was resolved in 2001 through the discovery of the effects of neutrino oscillation: the Sun emits the number of neutrinos predicted by the theory, but neutrino detectors were missing of them because the neutrinos had changed flavor by the time they were detected.

The Sun has a magnetic field that varies across the surface of the Sun. Its polar field is , whereas the field is typically in features on the Sun called sunspots and in solar prominences.

The magnetic field also varies in time and location. The quasi-periodic 11-year solar cycle is the most prominent variation in which the number and size of sunspots waxes and wanes.<ref name="doi10.1146/annurev-astro-081913-040012"></ref>

Sunspots are visible as dark patches on the Sun's photosphere, and correspond to concentrations of magnetic field where the convective transport of heat is inhibited from the solar interior to the surface. As a result, sunspots are slightly cooler than the surrounding photosphere, so they appear dark. At a typical solar minimum, few sunspots are visible, and occasionally none can be seen at all. Those that do appear are at high solar latitudes. As the solar cycle progresses towards its maximum, sunspots tend to form closer to the solar equator, a phenomenon known as Spörer's law. The largest sunspots can be tens of thousands of kilometers across.

An 11-year sunspot cycle is half of a 22-year Babcock–Leighton dynamo cycle, which corresponds to an oscillatory exchange of energy between toroidal and poloidal solar magnetic fields. At solar-cycle maximum, the external poloidal dipolar magnetic field is near its dynamo-cycle minimum strength, but an internal toroidal quadrupolar field, generated through differential rotation within the tachocline, is near its maximum strength. At this point in the dynamo cycle, buoyant upwelling within the convective zone forces emergence of toroidal magnetic field through the photosphere, giving rise to pairs of sunspots, roughly aligned east–west and having footprints with opposite magnetic polarities. The magnetic polarity of sunspot pairs alternates every solar cycle, a phenomenon known as the Hale cycle.

During the solar cycle's declining phase, energy shifts from the internal toroidal magnetic field to the external poloidal field, and sunspots diminish in number and size. At solar-cycle minimum, the toroidal field is, correspondingly, at minimum strength, sunspots are relatively rare, and the poloidal field is at its maximum strength. With the rise of the next 11-year sunspot cycle, differential rotation shifts magnetic energy back from the poloidal to the toroidal field, but with a polarity that is opposite to the previous cycle. The process carries on continuously, and in an idealized, simplified scenario, each 11-year sunspot cycle corresponds to a change, then, in the overall polarity of the Sun's large-scale magnetic field.

The solar magnetic field extends well beyond the Sun itself. The electrically conducting solar wind plasma carries the Sun's magnetic field into space, forming what is called the interplanetary magnetic field. In an approximation known as ideal magnetohydrodynamics, plasma particles only move along the magnetic field lines. As a result, the outward-flowing solar wind stretches the interplanetary magnetic field outward, forcing it into a roughly radial structure. For a simple dipolar solar magnetic field, with opposite hemispherical polarities on either side of the solar magnetic equator, a thin current sheet is formed in the solar wind. At great distances, the rotation of the Sun twists the dipolar magnetic field and corresponding current sheet into an Archimedean spiral structure called the Parker spiral. The interplanetary magnetic field is much stronger than the dipole component of the solar magnetic field. The Sun's dipole magnetic field of 50–400 μT (at the photosphere) reduces with the inverse-cube of the distance, leading to a predicted magnetic field of 0.1 nT at the distance of Earth. However, according to spacecraft observations the interplanetary field at Earth's location is around 5 nT, about a hundred times greater. The difference is due to magnetic fields generated by electrical currents in the plasma surrounding the Sun.

The Sun's magnetic field leads to many effects that are collectively called solar activity. Solar flares and coronal-mass ejections tend to occur at sunspot groups. Slowly changing high-speed streams of solar wind are emitted from coronal holes at the photospheric surface. Both coronal-mass ejections and high-speed streams of solar wind carry plasma and interplanetary magnetic field outward into the Solar System. The effects of solar activity on Earth include auroras at moderate to high latitudes and the disruption of radio communications and electric power. Solar activity is thought to have played a large role in the formation and evolution of the Solar System.

With solar-cycle modulation of sunspot number comes a corresponding modulation of space weather conditions, including those surrounding Earth where technological systems can be affected.

In December 2019, a new type of solar magnetic explosion was observed, known as forced magnetic reconnection. Previously, in a process called spontaneous magnetic reconnection, it was observed that the solar magnetic field lines diverge explosively and then converge again instantaneously. Forced Magnetic Reconnection was similar, but it was triggered by an explosion in the corona.

Long-term secular change in sunspot number is thought, by some scientists, to be correlated with long-term change in solar irradiance, which, in turn, might influence Earth's long-term climate.
For example, in the 17th century, the solar cycle appeared to have stopped entirely for several decades; few sunspots were observed during a period known as the Maunder minimum. This coincided in time with the era of the Little Ice Age, when Europe experienced unusually cold temperatures. Earlier extended minima have been discovered through analysis of tree rings and appear to have coincided with lower-than-average global temperatures.

A recent theory claims that there are magnetic instabilities in the core of the Sun that cause fluctuations with periods of either 41,000 or 100,000 years. These could provide a better explanation of the ice ages than the Milankovitch cycles.

The Sun today is roughly halfway through the most stable part of its life. It has not changed dramatically for over four billion years, and will remain fairly stable for more than five billion more. However, after hydrogen fusion in its core has stopped, the Sun will undergo dramatic changes, both internally and externally.

The Sun formed about 4.6 billion years ago from the collapse of part of a giant molecular cloud that consisted mostly of hydrogen and helium and that probably gave birth to many other stars. This age is estimated using computer models of stellar evolution and through nucleocosmochronology. The result is consistent with the radiometric date of the oldest Solar System material, at 4.567 billion years ago. Studies of ancient meteorites reveal traces of stable daughter nuclei of short-lived isotopes, such as iron-60, that form only in exploding, short-lived stars. This indicates that one or more supernovae must have occurred near the location where the Sun formed. A shock wave from a nearby supernova would have triggered the formation of the Sun by compressing the matter within the molecular cloud and causing certain regions to collapse under their own gravity. As one fragment of the cloud collapsed it also began to rotate due to conservation of angular momentum and heat up with the increasing pressure. Much of the mass became concentrated in the center, whereas the rest flattened out into a disk that would become the planets and other Solar System bodies. Gravity and pressure within the core of the cloud generated a lot of heat as it accreted more matter from the surrounding disk, eventually triggering nuclear fusion.

HD 162826 and HD 186302 are hypothesized stellar siblings of the Sun, having formed in the same molecular cloud.

The Sun is about halfway through its main-sequence stage, during which nuclear fusion reactions in its core fuse hydrogen into helium. Each second, more than four million tonnes of matter are converted into energy within the Sun's core, producing neutrinos and solar radiation. At this rate, the Sun has so far converted around 100 times the mass of Earth into energy, about 0.03% of the total mass of the Sun. The Sun will spend a total of approximately 10 billion years as a main-sequence star. The Sun is gradually becoming hotter during its time on the main sequence, because the helium atoms in the core occupy less volume than the hydrogen atoms that were fused. The core is therefore shrinking, allowing the outer layers of the Sun to move closer to the center and experience a stronger gravitational force, according to the inverse-square law. This stronger force increases the pressure on the core, which is resisted by a gradual increase in the rate at which fusion occurs. This process speeds up as the core gradually becomes denser. It is estimated that the Sun has become 30% brighter in the last 4.5 billion years. At present, it is increasing in brightness by about 1% every 100 million years.

The Sun does not have enough mass to explode as a supernova. Instead it will exit the main sequence in approximately 5 billion years and start to turn into a red giant. As a red giant, the Sun will grow so large that it will engulf Mercury, Venus, and probably Earth.

Even before it becomes a red giant, the luminosity of the Sun will have nearly doubled, and Earth will receive as much sunlight as Venus receives today. Once the core hydrogen is exhausted in 5.4 billion years, the Sun will expand into a subgiant phase and slowly double in size over about half a billion years. It will then expand more rapidly over about half a billion years until it is over two hundred times larger than today and a couple of thousand times more luminous. This then starts the red-giant-branch phase where the Sun will spend around a billion years and lose around a third of its mass.
After the red-giant branch the Sun has approximately 120 million years of active life left, but much happens. First, the core, full of degenerate helium ignites violently in the helium flash, where it is estimated that 6% of the core, itself 40% of the Sun's mass, will be converted into carbon within a matter of minutes through the triple-alpha process. The Sun then shrinks to around 10 times its current size and 50 times the luminosity, with a temperature a little lower than today. It will then have reached the red clump or horizontal branch, but a star of the Sun's mass does not evolve blueward along the horizontal branch. Instead, it just becomes moderately larger and more luminous over about 100 million years as it continues to react helium in the core.

When the helium is exhausted, the Sun will repeat the expansion it followed when the hydrogen in the core was exhausted, except that this time it all happens faster, and the Sun becomes larger and more luminous. This is the asymptotic-giant-branch phase, and the Sun is alternately reacting hydrogen in a shell or helium in a deeper shell. After about 20 million years on the early asymptotic giant branch, the Sun becomes increasingly unstable, with rapid mass loss and thermal pulses that increase the size and luminosity for a few hundred years every 100,000 years or so. The thermal pulses become larger each time, with the later pulses pushing the luminosity to as much as 5,000 times the current level and the radius to over 1 AU. According to a 2008 model, Earth's orbit is shrinking due to tidal forces (and, eventually, drag from the lower chromosphere), so that it will be engulfed by the Sun near the tip of the red giant branch phase, 3.8 and 1 million years after Mercury and Venus have respectively had the same fate. Models vary depending on the rate and timing of mass loss. Models that have higher mass loss on the red-giant branch produce smaller, less luminous stars at the tip of the asymptotic giant branch, perhaps only 2,000 times the luminosity and less than 200 times the radius. For the Sun, four thermal pulses are predicted before it completely loses its outer envelope and starts to make a planetary nebula. By the end of that phase—lasting approximately 500,000 years—the Sun will only have about half of its current mass.

The post-asymptotic-giant-branch evolution is even faster. The luminosity stays approximately constant as the temperature increases, with the ejected half of the Sun's mass becoming ionized into a planetary nebula as the exposed core reaches 30,000 K. The final naked core, a white dwarf, will have a temperature of over 100,000 K, and contain an estimated 54.05% of the Sun's present day mass. The planetary nebula will disperse in about 10,000 years, but the white dwarf will survive for trillions of years before fading to a hypothetical black dwarf.

The Sun lies close to the inner rim of the Milky Way's Orion Arm, in the Local Interstellar Cloud or the Gould Belt, at a distance of from the Galactic Center.

The Sun is contained within the Local Bubble, a space of rarefied hot gas, possibly produced by the supernova remnant Geminga, or multiple supernovae in subgroup B1 of the Pleiades moving group. The distance between the local arm and the next arm out, the Perseus Arm, is about 6,500 light-years. The Sun, and thus the Solar System, is found in what scientists call the galactic habitable zone.
The "Apex of the Sun's Way", or the solar apex, is the direction that the Sun travels relative to other nearby stars. This motion is towards a point in the constellation Hercules, near the star Vega.

Within of the Sun there are 315 known stars in 227 systems, as of 2000, including 163 single stars. It is estimated that a further 130 systems within this range have not yet been identified. Out to , there may be up to 7,500 stars, of which around 2,600 are known. The number of substellar objects in that volume are expected to be comparable to the number of stars. Of the 50 nearest stellar systems within 17 light-years from Earth (the closest being the red dwarf Proxima Centauri at approximately 4.2 light-years), the Sun ranks fourth in mass.

The Sun orbits the center of the Milky Way, and it is presently moving in the direction of the constellation of Cygnus. A simple model of the motion of a star in the galaxy gives the galactic coordinates , , and as:
where , , and are the respective velocities with respect to the local standard of rest, and are the Oort constants, formula_4 is the angular velocity of galactic rotation for the local standard of rest, formula_5 is the "epicyclic frequency", and ν is the vertical oscillation frequency. For the sun, the present values of , , and are estimated as formula_6 km/s, and estimates for the other constants are  = 15.5 km/s/kpc,  = −12.2 km/s/kpc, κ = 37 km/s/kpc, and ν=74 km/s/kpc. We take and to be zero and is estimated to be 17 parsecs. This model implies that the Sun circulates around a point that is itself going around the galaxy. The period of the Sun's circulation around the point is formula_7. which, using the equivalence that a parsec equals 1 km/s times 0.978 million years, comes to 166 million years, shorter than the time it takes for the point to go around the galaxy. In the () coordinates, the Sun describes an ellipse around the point, whose length in the direction is

and whose width in the direction is

The ratio of length to width of this ellipse, the same for all stars in our neighborhood, is formula_10
The moving point is presently at

The oscillation in the direction takes the Sun

above the galactic plane and the same distance below it, with a period of formula_14 or 83 million years, approximately 2.7 times per orbit. Although formula_15 is 222 million years, the value of formula_16 at the point around which the Sun circulates is

(see Oort constants), corresponding to 235 million years, and this is the time that the point takes to go once around the galaxy. Other stars with the same value of formula_18 have to take the same amount of time to go around the galaxy as the sun and thus remain in the same general vicinity as the Sun.

The Sun's orbit around the Milky Way is perturbed due to the non-uniform mass distribution in Milky Way, such as that in and between the galactic spiral arms. It has been argued that the Sun's passage through the higher density spiral arms often coincides with mass extinctions on Earth, perhaps due to increased impact events. It takes the Solar System about 225–250 million years to complete one orbit through the Milky Way (a "galactic year"), so it is thought to have completed 20–25 orbits during the lifetime of the Sun. The orbital speed of the Solar System about the center of the Milky Way is approximately 251 km/s (156 mi/s). At this speed, it takes around 1,190 years for the Solar System to travel a distance of 1 light-year, or 7 days to travel .

The Milky Way is moving with respect to the cosmic microwave background radiation (CMB) in the direction of the constellation Hydra with a speed of 550 km/s, and the Sun's resultant velocity with respect to the CMB is about 370 km/s in the direction of Crater or Leo.

The Sun is moved by the gravitational pull of the planets. One can think of the barycentre of the Solar System as being stationary (or as moving in a steady motion around the galaxy). The centre of the sun is always within 2.2 solar radii of the barycentre. This motion of the Sun is mainly due to Jupiter, Saturn, Uranus, and Neptune. For some periods of several decades, the motion is rather regular, forming a trefoil pattern, whereas between these periods it appears more chaotic. After 179 years (nine times the synodic period of Jupiter and Saturn) the pattern more or less repeats, but rotated by about 24°. The orbits of the inner planets, including of the Earth, are similarly displaced by the same graviational forces, so the movement of the Sun has little effect on the relative positions of the Earth and the Sun or on solar irradiance on the Earth as a function of time.

The temperature of the photosphere is approximately 6,000 K, whereas the temperature of the corona reaches . The high temperature of the corona shows that it is heated by something other than direct heat conduction from the photosphere.

It is thought that the energy necessary to heat the corona is provided by turbulent motion in the convection zone below the photosphere, and two main mechanisms have been proposed to explain coronal heating. The first is wave heating, in which sound, gravitational or magnetohydrodynamic waves are produced by turbulence in the convection zone. These waves travel upward and dissipate in the corona, depositing their energy in the ambient matter in the form of heat. The other is magnetic heating, in which magnetic energy is continuously built up by photospheric motion and released through magnetic reconnection in the form of large solar flares and myriad similar but smaller events—nanoflares.

Currently, it is unclear whether waves are an efficient heating mechanism. All waves except Alfvén waves have been found to dissipate or refract before reaching the corona. In addition, Alfvén waves do not easily dissipate in the corona. Current research focus has therefore shifted towards flare heating mechanisms.

Theoretical models of the Sun's development suggest that 3.8 to 2.5 billion years ago, during the Archean eon, the Sun was only about 75% as bright as it is today. Such a weak star would not have been able to sustain liquid water on Earth's surface, and thus life should not have been able to develop. However, the geological record demonstrates that Earth has remained at a fairly constant temperature throughout its history, and that the young Earth was somewhat warmer than it is today. One theory among scientists is that the atmosphere of the young Earth contained much larger quantities of greenhouse gases (such as carbon dioxide, methane) than are present today, which trapped enough heat to compensate for the smaller amount of solar energy reaching it.

However, examination of Archaean sediments appears inconsistent with the hypothesis of high greenhouse concentrations. Instead, the moderate temperature range may be explained by a lower surface albedo brought about by less continental area and the "lack of biologically induced cloud condensation nuclei". This would have led to increased absorption of solar energy, thereby compensating for the lower solar output.

The enormous effect of the Sun on Earth has been recognized since prehistoric times, and the Sun has been regarded by some cultures as a solar deity.

The Sun has been an object of veneration in many cultures throughout human history. Humanity's most fundamental understanding of the Sun is as the luminous disk in the sky, whose presence above the horizon creates day and whose absence causes night. In many prehistoric and ancient cultures, the Sun was thought to be a solar deity or other supernatural entity. Worship of the Sun was central to civilizations such as the ancient Egyptians, the Inca of South America and the Aztecs of what is now Mexico. In religions such as Hinduism, the Sun is still considered a god. Many ancient monuments were constructed with solar phenomena in mind; for example, stone megaliths accurately mark the summer or winter solstice (some of the most prominent megaliths are located in Nabta Playa, Egypt; Mnajdra, Malta and at Stonehenge, England); Newgrange, a prehistoric human-built mount in Ireland, was designed to detect the winter solstice; the pyramid of El Castillo at Chichén Itzá in Mexico is designed to cast shadows in the shape of serpents climbing the pyramid at the vernal and autumnal equinoxes.

The Egyptians portrayed the god Ra as being carried across the sky in a solar barque, accompanied by lesser gods, and to the Greeks, he was Helios, carried by a chariot drawn by fiery horses. From the reign of Elagabalus in the late Roman Empire the Sun's birthday was a holiday celebrated as Sol Invictus (literally "Unconquered Sun") soon after the winter solstice, which may have been an antecedent to Christmas. Regarding the fixed stars, the Sun appears from Earth to revolve once a year along the ecliptic through the zodiac, and so Greek astronomers categorized it as one of the seven planets (Greek "planetes", "wanderer"); the naming of the days of the weeks after the seven planets dates to the Roman era.

In the early first millennium BC, Babylonian astronomers observed that the Sun's motion along the ecliptic is not uniform, though they did not know why; it is today known that this is due to the movement of Earth in an elliptic orbit around the Sun, with Earth moving faster when it is nearer to the Sun at perihelion and moving slower when it is farther away at aphelion.

One of the first people to offer a scientific or philosophical explanation for the Sun was the Greek philosopher Anaxagoras. He reasoned that it was not the chariot of Helios, but instead a giant flaming ball of metal even larger than the land of the Peloponnesus and that the Moon reflected the light of the Sun. For teaching this heresy, he was imprisoned by the authorities and sentenced to death, though he was later released through the intervention of Pericles. Eratosthenes estimated the distance between Earth and the Sun in the 3rd century BC as "of stadia myriads 400 and 80000", the translation of which is ambiguous, implying either 4,080,000 stadia (755,000 km) or 804,000,000 stadia (148 to 153 million kilometers or 0.99 to 1.02 AU); the latter value is correct to within a few percent. In the 1st century AD, Ptolemy estimated the distance as 1,210 times the radius of Earth, approximately .

The theory that the Sun is the center around which the planets orbit was first proposed by the ancient Greek Aristarchus of Samos in the 3rd century BC, and later adopted by Seleucus of Seleucia (see Heliocentrism). This view was developed in a more detailed mathematical model of a heliocentric system in the 16th century by Nicolaus Copernicus.

Observations of sunspots were recorded during the Han Dynasty (206 BC–AD 220) by Chinese astronomers, who maintained records of these observations for centuries. Averroes also provided a description of sunspots in the 12th century. The invention of the telescope in the early 17th century permitted detailed observations of sunspots by Thomas Harriot, Galileo Galilei and other astronomers. Galileo posited that sunspots were on the surface of the Sun rather than small objects passing between Earth and the Sun.

Arabic astronomical contributions include Al-Battani's discovery that the direction of the Sun's apogee (the place in the Sun's orbit against the fixed stars where it seems to be moving slowest) is changing. (In modern heliocentric terms, this is caused by a gradual motion of the aphelion of the "Earth's" orbit). Ibn Yunus observed more than 10,000 entries for the Sun's position for many years using a large astrolabe.
From an observation of a transit of Venus in 1032, the Persian astronomer and polymath Ibn Sina concluded that Venus is closer to Earth than the Sun. In 1672 Giovanni Cassini and Jean Richer determined the distance to Mars and were thereby able to calculate the distance to the Sun.

In 1666, Isaac Newton observed the Sun's light using a prism, and showed that it is made up of light of many colors. In 1800, William Herschel discovered infrared radiation beyond the red part of the solar spectrum. The 19th century saw advancement in spectroscopic studies of the Sun; Joseph von Fraunhofer recorded more than 600 absorption lines in the spectrum, the strongest of which are still often referred to as Fraunhofer lines. In the early years of the modern scientific era, the source of the Sun's energy was a significant puzzle. Lord Kelvin suggested that the Sun is a gradually cooling liquid body that is radiating an internal store of heat. Kelvin and Hermann von Helmholtz then proposed a gravitational contraction mechanism to explain the energy output, but the resulting age estimate was only 20 million years, well short of the time span of at least 300 million years suggested by some geological discoveries of that time. In 1890 Joseph Lockyer, who discovered helium in the solar spectrum, proposed a meteoritic hypothesis for the formation and evolution of the Sun.

Not until 1904 was a documented solution offered. Ernest Rutherford suggested that the Sun's output could be maintained by an internal source of heat, and suggested radioactive decay as the source. However, it would be Albert Einstein who would provide the essential clue to the source of the Sun's energy output with his mass–energy equivalence relation . In 1920, Sir Arthur Eddington proposed that the pressures and temperatures at the core of the Sun could produce a nuclear fusion reaction that merged hydrogen (protons) into helium nuclei, resulting in a production of energy from the net change in mass. The preponderance of hydrogen in the Sun was confirmed in 1925 by Cecilia Payne using the ionization theory developed by Meghnad Saha. The theoretical concept of fusion was developed in the 1930s by the astrophysicists Subrahmanyan Chandrasekhar and Hans Bethe. Hans Bethe calculated the details of the two main energy-producing nuclear reactions that power the Sun. In 1957, Margaret Burbidge, Geoffrey Burbidge, William Fowler and Fred Hoyle showed that most of the elements in the universe have been synthesized by nuclear reactions inside stars, some like the Sun.

The first satellites designed for long term observation of the Sun from interplanetary space were NASA's Pioneers 6, 7, 8 and 9, which were launched between 1959 and 1968. These probes orbited the Sun at a distance similar to that of Earth, and made the first detailed measurements of the solar wind and the solar magnetic field. Pioneer 9 operated for a particularly long time, transmitting data until May 1983.

In the 1970s, two Helios spacecraft and the Skylab Apollo Telescope Mount provided scientists with significant new data on solar wind and the solar corona. The Helios 1 and 2 probes were U.S.–German collaborations that studied the solar wind from an orbit carrying the spacecraft inside Mercury's orbit at perihelion. The Skylab space station, launched by NASA in 1973, included a solar observatory module called the Apollo Telescope Mount that was operated by astronauts resident on the station. Skylab made the first time-resolved observations of the solar transition region and of ultraviolet emissions from the solar corona. Discoveries included the first observations of coronal mass ejections, then called "coronal transients", and of coronal holes, now known to be intimately associated with the solar wind.

In 1980, the Solar Maximum Mission was launched by NASA. This spacecraft was designed to observe gamma rays, X-rays and UV radiation from solar flares during a time of high solar activity and solar luminosity. Just a few months after launch, however, an electronics failure caused the probe to go into standby mode, and it spent the next three years in this inactive state. In 1984 Space Shuttle "Challenger" mission STS-41C retrieved the satellite and repaired its electronics before re-releasing it into orbit. The Solar Maximum Mission subsequently acquired thousands of images of the solar corona before re-entering Earth's atmosphere in June 1989.

Launched in 1991, Japan's Yohkoh ("Sunbeam") satellite observed solar flares at X-ray wavelengths. Mission data allowed scientists to identify several different types of flares, and demonstrated that the corona away from regions of peak activity was much more dynamic and active than had previously been supposed. Yohkoh observed an entire solar cycle but went into standby mode when an annular eclipse in 2001 caused it to lose its lock on the Sun. It was destroyed by atmospheric re-entry in 2005.

One of the most important solar missions to date has been the Solar and Heliospheric Observatory, jointly built by the European Space Agency and NASA and launched on 2 December 1995. Originally intended to serve a two-year mission, a mission extension through 2012 was approved in October 2009. It has proven so useful that a follow-on mission, the Solar Dynamics Observatory (SDO), was launched in February 2010. Situated at the Lagrangian point between Earth and the Sun (at which the gravitational pull from both is equal), SOHO has provided a constant view of the Sun at many wavelengths since its launch. Besides its direct solar observation, SOHO has enabled the discovery of a large number of comets, mostly tiny sungrazing comets that incinerate as they pass the Sun.
All these satellites have observed the Sun from the plane of the ecliptic, and so have only observed its equatorial regions in detail. The "Ulysses" probe was launched in 1990 to study the Sun's polar regions. It first traveled to Jupiter, to "slingshot" into an orbit that would take it far above the plane of the ecliptic. Once "Ulysses" was in its scheduled orbit, it began observing the solar wind and magnetic field strength at high solar latitudes, finding that the solar wind from high latitudes was moving at about 750 km/s, which was slower than expected, and that there were large magnetic waves emerging from high latitudes that scattered galactic cosmic rays.

Elemental abundances in the photosphere are well known from spectroscopic studies, but the composition of the interior of the Sun is more poorly understood. A solar wind sample return mission, "Genesis", was designed to allow astronomers to directly measure the composition of solar material.

The Solar Terrestrial Relations Observatory (STEREO) mission was launched in October 2006. Two identical spacecraft were launched into orbits that cause them to (respectively) pull further ahead of and fall gradually behind Earth. This enables stereoscopic imaging of the Sun and solar phenomena, such as coronal mass ejections.

The Parker Solar Probe was launched in 2018 aboard a Delta IV Heavy rocket and will reach a perigee of in 2025, making it the closest-orbiting manmade satellite as the first spacecraft to fly low into the solar corona.

The Indian Space Research Organisation has scheduled the launch of a satellite named "Aditya" for mid 2020. Its main instrument will be a coronagraph for studying the dynamics of the solar corona.

The brightness of the Sun can cause pain from looking at it with the naked eye; however, doing so for brief periods is not hazardous for normal non-dilated eyes. Looking directly at the Sun causes phosphene visual artifacts and temporary partial blindness. It also delivers about 4 milliwatts of sunlight to the retina, slightly heating it and potentially causing damage in eyes that cannot respond properly to the brightness. UV exposure gradually yellows the lens of the eye over a period of years, and is thought to contribute to the formation of cataracts, but this depends on general exposure to solar UV, and not whether one looks directly at the Sun. Long-duration viewing of the direct Sun with the naked eye can begin to cause UV-induced, sunburn-like lesions on the retina after about 100 seconds, particularly under conditions where the UV light from the Sun is intense and well focused; conditions are worsened by young eyes or new lens implants (which admit more UV than aging natural eyes), Sun angles near the zenith, and observing locations at high altitude.

Viewing the Sun through light-concentrating optics such as binoculars may result in permanent damage to the retina without an appropriate filter that blocks UV and substantially dims the sunlight. When using an attenuating filter to view the Sun, the viewer is cautioned to use a filter specifically designed for that use. Some improvised filters that pass UV or IR rays, can actually harm the eye at high brightness levels.
Herschel wedges, also called Solar Diagonals, are effective and inexpensive for small telescopes. The sunlight that is destined for the eyepiece is reflected from an unsilvered surface of a piece of glass. Only a very small fraction of the incident light is reflected. The rest passes through the glass and leaves the instrument. If the glass breaks because of the heat, no light at all is reflected, making the device fail-safe. Simple filters made of darkened glass allow the full intensity of sunlight to pass through if they break, endangering the observer's eyesight. Unfiltered binoculars can deliver hundreds of times as much energy as using the naked eye, possibly causing immediate damage. It is claimed that even brief glances at the midday Sun through an unfiltered telescope can cause permanent damage.

Partial solar eclipses are hazardous to view because the eye's pupil is not adapted to the unusually high visual contrast: the pupil dilates according to the total amount of light in the field of view, "not" by the brightest object in the field. During partial eclipses most sunlight is blocked by the Moon passing in front of the Sun, but the uncovered parts of the photosphere have the same surface brightness as during a normal day. In the overall gloom, the pupil expands from ~2 mm to ~6 mm, and each retinal cell exposed to the solar image receives up to ten times more light than it would looking at the non-eclipsed Sun. This can damage or kill those cells, resulting in small permanent blind spots for the viewer. The hazard is insidious for inexperienced observers and for children, because there is no perception of pain: it is not immediately obvious that one's vision is being destroyed.

During sunrise and sunset, sunlight is attenuated because of Rayleigh scattering and Mie scattering from a particularly long passage through Earth's atmosphere, and the Sun is sometimes faint enough to be viewed comfortably with the naked eye or safely with optics (provided there is no risk of bright sunlight suddenly appearing through a break between clouds). Hazy conditions, atmospheric dust, and high humidity contribute to this atmospheric attenuation.

An optical phenomenon, known as a green flash, can sometimes be seen shortly after sunset or before sunrise. The flash is caused by light from the Sun just below the horizon being bent (usually through a temperature inversion) towards the observer. Light of shorter wavelengths (violet, blue, green) is bent more than that of longer wavelengths (yellow, orange, red) but the violet and blue light is scattered more, leaving light that is perceived as green.

Ultraviolet light from the Sun has antiseptic properties and can be used to sanitize tools and water. It also causes sunburn, and has other biological effects such as the production of vitamin D and sun tanning. It is also the main cause of skin cancer. Ultraviolet light is strongly attenuated by Earth's ozone layer, so that the amount of UV varies greatly with latitude and has been partially responsible for many biological adaptations, including variations in human skin color in different regions of the Earth.

The Sun has eight known planets. This includes four terrestrial planets (Mercury, Venus, Earth, and Mars), two gas giants (Jupiter and Saturn), and two ice giants (Uranus and Neptune). The Solar System also has at least five dwarf planets, an asteroid belt, numerous comets, and a large number of icy bodies which lie beyond the orbit of Neptune.

Solar deities play a major role in many world religions and mythologies. The ancient Sumerians believed that the sun was Utu, the god of justice and twin brother of Inanna, the Queen of Heaven, who was identified as the planet Venus. Later, Utu was identified with the East Semitic god Shamash. Utu was regarded as a helper-deity, who aided those in distress, and, in iconography, he is usually portrayed with a long beard and clutching a saw, which represented his role as the dispenser of justice.

From at least the Fourth Dynasty of Ancient Egypt, the Sun was worshipped as the god Ra, portrayed as a falcon-headed divinity surmounted by the solar disk, and surrounded by a serpent. In the New Empire period, the Sun became identified with the dung beetle, whose spherical ball of dung was identified with the Sun. In the form of the sun disc Aten, the Sun had a brief resurgence during the Amarna Period when it again became the preeminent, if not only, divinity for the Pharaoh Akhenaton.

In Proto-Indo-European religion, the Sun was personified as the goddess "*Sehul". Derivatives of this goddess in Indo-European languages include the Old Norse "Sól", Sanskrit "Surya", Gaulish "Sulis", Lithuanian "Saulė", and Slavic "Solntse". In ancient Greek religion, the sun deity was the male god Helios, but traces of an earlier female solar deity are preserved in Helen of Troy. In later times, Helios was syncretized with Apollo.

In the Bible, mentions the "Sun of Righteousness" (sometimes translated as the "Sun of Justice"), which some Christians have interpreted as a reference to the Messiah (Christ). In ancient Roman culture, Sunday was the day of the sun god. It was adopted as the Sabbath day by Christians who did not have a Jewish background. The symbol of light was a pagan device adopted by Christians, and perhaps the most important one that did not come from Jewish traditions. In paganism, the Sun was a source of life, giving warmth and illumination to mankind. It was the center of a popular cult among Romans, who would stand at dawn to catch the first rays of sunshine as they prayed. The celebration of the winter solstice (which influenced Christmas) was part of the Roman cult of the unconquered Sun (Sol Invictus). Christian churches were built with an orientation so that the congregation faced toward the sunrise in the East.

Tonatiuh, the Aztec god of the sun, was usually depicted holding arrows and a shield and was closely associated with the practice of human sacrifice. The sun goddess Amaterasu is the most important deity in the Shinto religion, and she is believed to be the direct ancestor of all Japanese emperors.




</doc>
<doc id="26752" url="https://en.wikipedia.org/wiki?curid=26752" title="Smiley">
Smiley

A smiley, sometimes referred to as a smiley face, is a basic ideogram that represents a smiling face, which has become part of popular culture worldwide. In modern times, the smiley has mostly been known for its yellow face and has evolved from a simple smiling face to display a range of facial emotions. Drawings of smileys have been traced back to ancient times, often displayed in caves and were often just two eyes and a curved upwards mouth.

The classic form designed by Harvey Ball in 1963 comprises a yellow circle with two black dots representing eyes and a black arc representing the mouth (). On the Internet and in other plain text communication channels, the emoticon form (sometimes also called the smiley-face emoticon) has traditionally been most popular, typically employing a colon and a right parenthesis to form sequences such as codice_1,codice_2,codice_3, codice_4, codice_5, codice_6, or codice_7 that resemble a smiling face when viewed after rotation through 90 degrees. "Smiley" is also sometimes used as a generic term for any emoticon (see Emoji). The smiley has been referenced in nearly all areas of Western culture including music, movies, and art. The smiley has also been associated with late 1980s and early 1990s rave culture.

The plural form "smilies" is commonly used, but the variant spelling "smilie" is not as common as the "y" spelling.

For thousands of years, smiling faces have been used as ideograms and as pictograms. In recent times, the face now known as a smiley has evolved into a well-known image and brand, recognisable for its yellow and black features. It wasn't until the 1900s that the design evolved from a basic eyes and mouth design, into a more recognisable design.

The oldest known smiling face was found by a team of archaeologists led by Nicolò Marchetti of the University of Bologna. Marchetti and his team pieced together fragments of a Hittite pot from approximately 1700 BC that had been found in Karkamış, Turkey. Once the pot had been pieced together, the team noticed that the item had a large smiling face engraved on it, becoming the first item to with such a design to be found. In 1635, a gold smiling face was drawn on the bottom of a legal document in Slovakia, appearing next to lawyer's Jan Ladislaides signature.

The Danish poet and author Johannes V. Jensen was amongst other things famous for experimenting with the form of his writing. In a letter sent to publisher Ernst Bojesen in December 1900, he includes both a happy face and a sad face.

One of the first commercial uses of a smiling face was in 1919, when the Buffalo Steam Roller Company in Buffalo, New York applied stickers on receipts with the word ""thanks"" and a smiling face above it. The face contained a lot of detail, having eyebrows, nose, teeth, chin and facial creases, reminiscent of "man-in-the-moon" style characteristics.

Ingmar Bergman's 1948 film "Port of Call" includes a scene where the unhappy Berit draws a "sad" face closely resembling the modern "frowny", but including a dot for the nose in lipstick on her mirror, before being interrupted. In 1953 and 1958, similar happy faces were used in promotional campaigns for the films "Lili" (1953) and "Gigi" (1958).

In the United States, the first time a combination of yellow and black was used for a smiling face was in late 1962. During the 1960s and early 70s, a number of designers created smiling faces, which were categorised as "happy faces." The WMCA happy face, became synonymous with 1960s culture in New York City. The New York-based radio station used the happy face as part of a competition for listeners. When the station called listeners, any listener who answered their phone "WMCA Good Guys!" was rewarded with a "WMCA good guys" sweatshirt that incorporated the yellow and black happy face into its design. Throughout the 1960s, thousands of these sweatshirts were given away. The features of the WMCA smiley was a yellow face, with black dots as eyes and had a slightly crooked smile. The outline of the face was also not smooth to give it more of a handrawn look. Originally, the yellow and black sweatshirt (sometimes referred to as gold), had WMCA Good Guys written on the front with no smiley face.

According to the Smithsonian Institution, the smiley face as we know it today was created by Harvey Ross Ball, an American graphic artist. In 1963, Ball was employed by State Mutual Life Assurance Company of Worcester, Massachusetts (now known as Hanover Insurance) to create a happy face to raise the morale of the employees. Ball created the design in ten minutes and was paid $45 (). His rendition, with a bright yellow background, dark oval eyes, full smile, and creases at the sides of the mouth, was imprinted on more than fifty million buttons and became familiar around the world. The design is so simple that it is certain that similar versions were produced before 1963, including those cited above. However, Ball’s rendition, as described here, has become the most iconic version. In 1967, Seattle graphic artist George Tenagi drew his own version at the request of advertising agent, David Stern. Tenagi's design was used in an advertising campaign for Seattle-based University Federal Savings & Loan. The ad campaign was inspired by Lee Adams's lyrics in "Put on a Happy Face" from the musical "Bye Bye Birdie". Stern, the man behind this campaign, also later incorporated the Happy Face in his run for Seattle mayor in 1993.

The graphic was further popularized in the early 1970s by Philadelphia brothers Bernard and Murray Spain, who seized upon it in September 1970 in a campaign to sell novelty items. The two produced buttons as well as coffee mugs, t-shirts, bumper stickers and many other items emblazoned with the symbol and the phrase "Have a happy day", which mutated into "Have a nice day". Working with New York button manufacturer NG Slater, some 50 million happy face badges were produced by 1972.

In 1972, Frenchman Franklin Loufrani became the first person to legally trademark the use of a smiley face. He used it to highlight the good news parts of the newspaper "France Soir". He simply called the design "Smiley" and launched The Smiley Company. In 1996 Loufrani's son Nicolas Loufrani took over the family business and built it into a multinational corporation. Nicolas Loufrani was outwardly skeptical of Harvey Ball's claim to creating the first smiley face. While noting that the design that his father came up with and Ball's design were nearly identical, Loufrani argued that the design is so simple that no one person can lay claim to having created it. As evidence for this, Loufrani's website points to early cave paintings found in France (2500 BC) that he claims are the first depictions of a smiley face. Loufrani also points to a 1960 radio ad campaign that reportedly made use of a similar design.
The rights to the Smiley trademark in one hundred countries are owned by the Smiley Company. Its subsidiary SmileyWorld Ltd, in London, headed by Nicolas Loufrani, creates or approves all the Smiley products sold in countries where it holds the trademark. The Smiley brand and logo have significant exposure through licensees in sectors such as clothing, home decoration, perfumery, plush, stationery, publishing, and through promotional campaigns. The Smiley Company is one of the 100 biggest licensing companies in the world, with a turnover of US$167 million in 2012. The first Smiley shop opened in London in the Boxpark shopping centre in December 2011.

In 1997, Franklin Loufrani and Smiley World attempted to acquire trademark rights to the symbol (and even to the word "smiley" itself) in the United States. This brought Loufrani into conflict with Wal-Mart, which had begun prominently featuring a happy face in its "Rolling Back Prices" campaign over a year earlier. Wal-Mart responded first by trying to block Loufrani's application, then later by trying to register the smiley face itself; Loufrani, in turn, sued to stop Wal-Mart's application, and in 2002 after the issue went to court, where it would languish for seven years before a decision.

Wal-Mart began phasing out the smiley face on its vests and its website in 2006. Despite that, Wal-Mart sued an online parodist for alleged "trademark infringement" after he used the symbol (as well as various portmanteaus of "Wal-", such as "Walocaust"). The District Court found in favor of the parodist when in March 2008, the judge concluded that Wal-Mart's smiley face logo was not shown to be "inherently distinctive" and that it "has failed to establish that the smiley face has acquired secondary meaning or that it is otherwise a protectable trademark" under U.S. law.

In June 2010, Wal-Mart and the Smiley Company founded by Loufrani settled their 10-year-old dispute in front of the Chicago federal court. The terms remain confidential. In 2016, Wal-Mart brought back the smiley face on its website, social media profiles, and in selected stores.

The earliest known smiley-like image in a written document was drawn by a Slovak notary to indicate his satisfaction with the state of his town's municipal financial records in 1635. A disputed early use of the smiley in a printed text may have been in Robert Herrick's poem "To Fortune" (1648), which contains the line "Upon my ruins (smiling yet :)". Journalist Levi Stahl has suggested that this may have been an intentional "orthographic joke", while this occurrence is likely merely the colon placed inside parentheses rather than outside of them as is standard typographic practice today -- (smiling yet):. There are citations of similar punctuation in a non-humorous context, even within Herrick's own work. It is likely that the parenthesis was added later by modern editors.

On the Internet, the smiley has become a visual means of conveyance that uses images. The first known mention on the Internet was on September 19, 1982, when Scott Fahlman from Carnegie Mellon University wrote:

Yellow graphical smileys have been used for many different purposes, including use in early 1980s video games. Yahoo! Messenger (from 1998) used smiley symbols in the user list next to each user, and also as an icon for the application. 
In 2001, SmileyWorld launched the website "The official Smiley dictionary", with smileys proposed to replace ASCII emoticons (i.e. emojis). In November 2001, and later, smiley emojis inside the actual chat text was adopted by several chat systems, including Yahoo Messenger.

The smiley is the printable version of characters 1 and 2 of (black-and-white versions of) codepage 437 (1981) of the first IBM PC and all subsequent PC compatible computers. For modern computers, all versions of Microsoft Windows after Windows 95 can use the smiley as part of Windows Glyph List 4, although some computer fonts miss some characters.

The smiley face was included in Unicode's Miscellaneous Symbols from version 1.1 (1993).

Later additions to Unicode included a large number of variants expressing a range of human emotions, in particlar with the addition of the "Emoticons" and "Supplemental Symbols and Pictographs blocks in Unicode versions 6.0 (2010) and 8.0 (2015), respectively.
These were introduced for compatibility with the ad-hoc implementation of emoticons by Japanese telephone carriers in unused ranges of the Shift JIS standard.
This resulted in a de-facto standard in the range with lead bytes 0xF5 to 0xF9. 
KDDI has gone much further than this, and has introduced hundreds more in the space with lead bytes 0xF3 and 0xF4.

The smiley has now become synonymous with culture across the world. It is used for communication, imagery, branding and for topical purposes to display a range of emotions. Beginning in the 1960s, a yellow "happy face" was used by numerous brands in print to demonstrate happiness.

Franklin Loufrani first introduced the word smiley when he designed a smiling face for the newspaper he was working for at the time. The Loufrani design came in 1971, when Loufrani designed a smiley face for the newspaper, "France-Soir". The newspaper used Loufrani's smiley to highlight stories that they defined as ""feel-good news."" This particular smiley went onto form The Smiley Company. "Mad" magazine notably used the smiley a year later in 1972 across their entire front page for the April edition of the magazine. This was one of the first instances that the smiling face had been adapted, with one of the twenty visible smileys pulling a face.

In the United States, there were many instances of smiling faces in the 1900s. However, the first industry to mass adopt the smiley was in comics and cartoons.

As music genres began to create their own cultures from the 1970s onwards, many cultures began to incorporate a smiling face into their culture. In the late 1970s, the American band Dead Kennedys launched their first recording, "California Über Alles". The single cover was a collage aimed to look like that of a Nazi rally prior to World War II. The usual swastika banners used at rallies, was replaced on the single cover with three large smileys. In the UK, the happy face has been associated with psychedelic culture since Ubi Dwyer and the Windsor Free Festival in the 1970s and the electronic dance music culture, particularly with acid house, that emerged during the Second Summer of Love in the late 1980s. The association was cemented when the band Bomb the Bass used an extracted smiley from the comic book series "Watchmen" on the center of its "Beat Dis" hit single.

In the late-1980s, the smiley again became a prominent image within the music industry. It was adopted during the growth of acid house across Europe and the UK in the late 1980s. According to many, this began when DJ, Danny Rampling, used the smiley to celebrate Paul Oakenfold's birthday. This sparked a movement where the smiley moved into various dance genres, becoming a symbol of 1980s dance music.

In 1980, Namco released the now famous Pac-man, a yellow faced cartoon character. In 2008, the video game used the yellow smiley as part of its branding for the game. The smiley appeared throughout the game and also on the cover. The smiley normally appeared on the side of a grenade, which is something that became synonymous with the Battlefield series.

The logo for the "Watchmen" comic book series includes a smiley with blood on top of it. In the film Suicide Squad, the character Deadshot stares into the window of a clothing store. Behind a line of mannequins is a yellow smiley face pin, which had been closely associated to another DC comic character, Comedian.

As part of his early works, graffiti artist Banksy frequently used the smiley in his art. The first of his major works that included a smiley was his Flying Copper portrait, which was completed in 2004. It was during a period when Banksy experimented with working on canvas and paper portraits. He also used the smiley in 2005 to replace the face of the grim reaper. The image became known as ""grin reaper.""

During the London 2012 opening ceremony, early on in the show a number of giant beach balls were released into the audience. All of them were yellow and had a large smiley face on each of them.



</doc>
<doc id="26753" url="https://en.wikipedia.org/wiki?curid=26753" title="Signature">
Signature

A signature (; from , "to sign") is a handwritten (and often stylized) depiction of someone's name, nickname, or even a simple "X" or other mark that a person writes on documents as a proof of identity and intent. The writer of a signature is a signatory or signer. Similar to a handwritten signature, a signature work describes the work as readily identifying its creator. A signature may be confused with an autograph, which is chiefly an artistic signature. This can lead to confusion when people have both an autograph and signature and as such some people in the public eye keep their signatures private whilst fully publishing their autograph.

The traditional function of a signature is to permanently affix to a document a person's uniquely personal, undeniable self-identification as physical evidence of that person's personal witness and certification of the content of all, or a specified part, of the document. For example, the role of a signature in many consumer contracts is not solely to provide evidence of the identity of the contracting party, but also to provide evidence of deliberation and informed consent. In many countries, signatures may be witnessed and recorded in the presence of a notary public to carry additional legal force. On legal documents, an illiterate signatory can make a "mark" (often an "X" but occasionally a personalized symbol), so long as the document is countersigned by a literate witness. In some countries, illiterate people place a thumbprint on legal documents in lieu of a written signature.

In the United States, signatures encompass marks and actions of all sorts that are indicative of identity and intent. The legal rule is that unless a statute specifically prescribes a particular method of making a signature it may be made in any number of ways. These include by a mechanical or rubber stamp facsimile. A signature may be made by the purported signatory; alternatively someone else duly authorized by the signatory, acting in the signer's presence and at the signatory's direction, may make the signature.

Many individuals have much more fanciful signatures than their normal cursive writing, including elaborate ascenders, descenders and exotic flourishes, much as one would find in calligraphic writing. As an example, the final "k" in John Hancock's famous signature on the US Declaration of Independence loops back to underline his name. This kind of flourish is also known as a "paraph".

Paraphe is a term meaning flourish, initial or signature in French. The paraph is used in graphology analyses.

Several cultures whose languages use writing systems other than alphabets do not share the Western notion of signatures per se: the "signing" of one's name results in a written product no different from the result of "writing" one's name in the standard way. For these languages, to write or to sign involves the same written characters. Also see Calligraphy.

Special signature machines, called autopens, are capable of automatically reproducing an individual's signature. These are typically used by people required to sign a lot of printed matter, such as celebrities, heads of state or CEOs. More recently, Members of Congress in the United States have begun having their signature made into a TrueType font file. This allows staff members in the Congressman's office to easily reproduce it on correspondence, legislation, and official documents. In the East Asian languages of Chinese, Japanese, and Korean, people traditionally use stamp-like objects known as "name-seals" with the name carved in "tensho" script ("seal script") in lieu of a handwritten signature. 

A wet signature is a person's name written in their own hand with ink. Some government agencies require that professional persons or official reviewers sign originals and all copies of originals to authenticate that they personally viewed the content. In the United States this is prevalent with architectural and construction plans. Its intent is to prevent mistakes or fraud but the practice is not known to be effective.

Handwriting experts say "it is extremely difficult for anyone to be able to figure out if a signature or other very limited writing sample has been forged,"
High volume review of signatures, to decide if a signature is true or forged, occurs when election offices decide whether to accept absentee ballots arriving from voters,
and possibly when banks decide whether to pay checks.
The highest error rates in signature verification are found with lay people, higher than for computers, which in turn make more errors than experts.

There have been concerns that signature reviews improperly reject ballots from young and minority voters at higher rates than others, with no or limited ability of voters to appeal the rejection.

Researchers have published error rates for computerized signature verification. They compare different systems on a common database of true and false signatures. The best system falsely rejects 10% of true signatures, while it accepts 10% of forgeries. Another system has error rates on both of 14%, and the third-best has error rates of 17%. It is possible to be less stringent and reject fewer true signatures, at the cost of also rejecting fewer forgeries. Computer algorithms:look for a certain number of points of similarity between the compared signatures ... a wide range of algorithms and standards, each particular to that machine's manufacturer, are used to verify signatures. In addition, counties have discretion in managing the settings and implementing manufacturers' guidelines ... there are no statewide standards for automatic signature verification ... most counties do not have a publicly available, written explanation of the signature verification criteria and processes they use.

In an experiment, experts rejected 5% of true signatures and 71% of forgeries. They were doubtful about another 57% of true signatures and 27% of forgeries. If computer verification is adjusted to reflect what experts are sure about, it will wrongly reject 5% of true signatures and wrongly accept 29% of forgeries. If computers were adjusted more strictly, rejecting all signatures which experts have doubts about, the computers would set aside 62% of true signatures, and still wrongly accept 2% of forgeries. Lay people made more mistakes and were doubtful less often, though the study does not report whether their mistakes were to accept more forgeries or reject more true signatures.

Voters with short names are at a disadvantage, since experts make more mistakes on signatures with fewer "turning points and intersections." Participants in this study had 10 true signatures to compare to, which is more than most postal ballot verifications have. A more recent study for the US Department of Justice confirms the probabilistic nature of signature verification, though it does not provide numbers.

In e-mail and newsgroup usage, another type of signature exists which is independent of one's language. Users can set one or more lines of custom text known as a signature block to be automatically appended to their messages. This text usually includes a name, contact information, and sometimes quotations and ASCII art. A shortened form of a signature block, only including one's name, often with some distinguishing prefix, can be used to simply indicate the end of a post or response. Some web sites also allow graphics to be used. Note, however, that this type of signature is not related to electronic signatures or digital signatures, which are more technical in nature and not directly understandable by humans. On Wikipedia, an online wiki-based encyclopedia edited by volunteers, the contributors "sign" their comments on talk pages with their username (only the username holder has the right to digitally affix their signature).

The signature on a painting or other work of art has always been an important item in the assessment of art. Fake signatures are sometimes added to enhance the value of a painting, or are added to a fake painting to support its authenticity. A notorious case was the signature of Johannes Vermeer on the fake "Supper at Emmaus" made by the art-forger Han van Meegeren. However, the fact that painters' signatures often vary over time (particularly in the modern and contemporary periods) might complicate the issue. The signatures of some painters take on an artistic form that may be of less value in determining forgeries.

Under British law, the appearance of signatures (not the names themselves) may be protected under copyright law.

Under United States copyright law, "titles, names [I c...]; mere variations of typographic ornamentation, lettering, or coloring" are not eligible for copyright; however, the appearance of signatures (not the names themselves) may be protected under copyright law.

Uniform Commercial Code §1-201(37) of the United States generally defines signed as "using any symbol executed or adopted with present intention to adopt or accept a writing." The Uniform Commercial Code §3-401(b) for negotiable instruments states "A signature may be made (i) manually or by means of a device or machine, and (ii) by the use of any name, including a trade or assumed name, or by a word, mark, or symbol executed or adopted by a person with present intention to authenticate a writing."




</doc>
<doc id="26754" url="https://en.wikipedia.org/wiki?curid=26754" title="Seal">
Seal

Seal may refer to any of the following:











</doc>
<doc id="26756" url="https://en.wikipedia.org/wiki?curid=26756" title="Sino-Tibetan languages">
Sino-Tibetan languages

Sino-Tibetan, in a few sources also known as Trans-Himalayan, is a family of more than 400 languages, second only to Indo-European in number of native speakers. The vast majority of these are the 1.3 billion native speakers of Chinese languages. Other Sino-Tibetan languages with large numbers of speakers include Burmese (33 million) and the Tibetic languages (six million). Other languages of the family are spoken in the Himalayas, the Southeast Asian Massif, and the eastern edge of the Tibetan Plateau. Most of these have small speech communities in remote mountain areas, and as such are poorly documented.

Several low-level subgroups have been securely reconstructed, but reconstruction of a proto-language for the family as a whole is still at an early stage, so the higher-level structure of Sino-Tibetan remains unclear. Although the family is traditionally presented as divided into Sinitic (i.e. Chinese) and Tibeto-Burman branches, a common origin of the non-Sinitic languages has never been demonstrated. While Chinese linguists generally include Kra–Dai and Hmong–Mien languages within Sino-Tibetan, most other linguists have excluded them since the 1940s.
Several links to other language families have been proposed, but none has broad acceptance.

A genetic relationship between Chinese, Tibetan, Burmese and other languages was first proposed in the early 19th century and is now broadly accepted. The initial focus on languages of civilizations with long literary traditions has been broadened to include less widely spoken languages, some of which have only recently, or never, been written. However, the reconstruction of the family is much less developed than for families such as Indo-European or Austroasiatic. Difficulties have included the great diversity of the languages, the lack of inflection in many of them, and the effects of language contact. In addition, many of the smaller languages are spoken in mountainous areas that are difficult to access, and are often also sensitive border zones.

During the 18th century, several scholars had noticed parallels between Tibetan and Burmese, both languages with extensive literary traditions.
Early in the following century, Brian Houghton Hodgson and others noted that many non-literary languages of the highlands of northeast India and Southeast Asia were also related to these.
The name "Tibeto-Burman" was first applied to this group in 1856 by James Richardson Logan, who added Karen in 1858.
The third volume of the "Linguistic Survey of India", edited by Sten Konow, was devoted to the Tibeto-Burman languages of British India.

Studies of the "Indo-Chinese" languages of Southeast Asia from the mid-19th century by Logan and others revealed that they comprised four families: Tibeto-Burman, Tai, Mon–Khmer and Malayo-Polynesian.
Julius Klaproth had noted in 1823 that Burmese, Tibetan and Chinese all shared common basic vocabulary but that Thai, Mon, and Vietnamese were quite different.
Ernst Kuhn envisaged a group with two branches, Chinese-Siamese and Tibeto-Burman.
August Conrady called this group Indo-Chinese in his influential 1896 classification, though he had doubts about Karen. Conrady's terminology was widely used, but there was uncertainty regarding his exclusion of Vietnamese. Franz Nikolaus Finck in 1909 placed Karen as a third branch of Chinese-Siamese.

Jean Przyluski introduced the French term "sino-tibétain" as the title of his chapter on the group in Meillet and Cohen's "Les langues du monde" in 1924. He divided them into three groups: Tibeto-Burman, Chinese and Tai, and was uncertain about the affinity of Karen and Hmong–Mien. The English translation "Sino-Tibetan" first appeared in a short note by Przyluski and Luce in 1931.

In 1935, the anthropologist Alfred Kroeber started the Sino-Tibetan Philology Project, funded by the Works Project Administration and based at the University of California, Berkeley.
The project was supervised by Robert Shafer until late 1938, and then by Paul K. Benedict.
Under their direction, the staff of 30 non-linguists collated all the available documentation of Sino-Tibetan languages.
The result was eight copies of a 15-volume typescript entitled "Sino-Tibetan Linguistics".
This work was never published, but furnished the data for a series of papers by Shafer, as well as Shafer's five-volume "Introduction to Sino-Tibetan" and Benedict's "Sino-Tibetan, a Conspectus".

Benedict completed the manuscript of his work in 1941, but it was not published until 1972. Instead of building the entire family tree, he set out to reconstruct a Proto-Tibeto-Burman language by comparing five major languages, with occasional comparisons with other languages. He reconstructed a two-way distinction on initial consonants based on voicing, with aspiration conditioned by pre-initial consonants that had been retained in Tibetic but lost in many other languages. Thus, Benedict reconstructed the following initials:

Although the initial consonants of cognates tend to have the same place and manner of articulation, voicing and aspiration is often unpredictable.
This irregularity was attacked by Roy Andrew Miller, though Benedict's supporters attribute it to the effects of prefixes that have been lost and are often unrecoverable.
The issue remains unsolved today.
It was cited together with the lack of reconstructable shared morphology, and evidence that much shared lexical material has been borrowed from Chinese into Tibeto-Burman, by Christopher Beckwith, one of the few scholars still arguing that Chinese is not related to Tibeto-Burman.

Benedict also reconstructed, at least for Tibeto-Burman, prefixes such as the causative "s-", the intransitive "m-", and "r-", "b-" "g-" and "d-" of uncertain function, as well as suffixes "-s", "-t" and "-n".

Old Chinese is by far the oldest recorded Sino-Tibetan language, with inscriptions dating from 1200 BC and a huge body of literature from the first millennium BC, but the Chinese script is not alphabetic. Scholars have sought to reconstruct the phonology of Old Chinese by comparing the obscure descriptions of the sounds of Middle Chinese in medieval dictionaries with phonetic elements in Chinese characters and the rhyming patterns of early poetry. The first complete reconstruction, the "Grammata Serica Recensa" of Bernard Karlgren, was used by Benedict and Shafer.

Karlgren's reconstruction was somewhat unwieldy, with many sounds having a highly non-uniform distribution. Later scholars have revised it by drawing on a range of other sources. Some proposals were based on cognates in other Sino-Tibetan languages, though workers have also found solely Chinese evidence for them. For example, recent reconstructions of Old Chinese have reduced Karlgren's 15 vowels to a six-vowel system originally suggested by Nicholas Bodman. Similarly, Karlgren's *l has been recast as *r, with a different initial interpreted as *l, matching Tibeto-Burman cognates, but also supported by Chinese transcriptions of foreign names. A growing number of scholars believe that Old Chinese did not use tones, and that the tones of Middle Chinese developed from final consonants. One of these, *-s, is believed to be a suffix, with cognates in other Sino-Tibetan languages.
Tibetic has extensive written records from the adoption of writing by the Tibetan Empire in the mid-7th century. The earliest records of Burmese (such as the 12th-century Myazedi inscription) are more limited, but later an extensive literature developed. Both languages are recorded in alphabetic scripts ultimately derived from the Brahmi script of Ancient India. Most comparative work has used the conservative written forms of these languages, following the dictionaries of Jäschke (Tibetan) and Judson (Burmese), though both contain entries from a wide range of periods.

There are also extensive records in Tangut, the language of the Western Xia (1038–1227). Tangut is recorded in a Chinese-inspired logographic script, whose interpretation presents many difficulties, even though multilingual dictionaries have been found.

Gong Hwang-cherng has compared Old Chinese, Tibetic, Burmese and Tangut in an effort to establish sound correspondences between those languages. He found that Tibetic and Burmese correspond to two Old Chinese vowels, *a and *ə. While this has been considered evidence for a separate Tibeto-Burman subgroup, Hill (2014) finds that Burmese has distinct correspondences for Old Chinese rhymes "-ay" : *-aj and "-i" : *-əj, and hence argues that the development *ə > *a occurred independently in Tibetan and Burmese.

The descriptions of non-literary languages used by Shafer and Benedict were often produced by missionaries and colonial administrators of varying linguistic skill.
Most of the smaller Sino-Tibetan languages are spoken in inaccessible mountainous areas, many of which are politically or militarily sensitive and thus closed to investigators.
Until the 1980s, the best-studied areas were Nepal and northern Thailand.
In the 1980s and 1990s, new surveys were published from the Himalayas and southwestern China.
Of particular interest was the discovery of a new branch of the family, the Qiangic languages of western Sichuan and adjacent areas.

Most of the current spread of Sino-Tibetan languages is the result of historical expansions of the three groups with the most speakers – Chinese, Burmese and Tibetic – replacing an unknown number of earlier languages.
These groups also have the longest literary traditions of the family.
The remaining languages are spoken in mountainous areas, along the southern slopes of the Himalayas, the Southeast Asian Massif and the eastern edge of the Tibetan Plateau.

By far the largest branch are the Sinitic languages, with 1.3 billion speakers, most of whom live in the eastern half of China.
The first records of Chinese are oracle bone inscriptions from c. 1200 BC, when Old Chinese was spoken around the middle reaches of the Yellow River.
Chinese has since expanded throughout China, forming a family whose diversity has been compared with the Romance languages.
Diversity is greater in the rugged terrain of southeast China than in the North China Plain.

Burmese is the national language of Myanmar, and the first language of some 33 million people.
Burmese speakers first entered the northern Irrawaddy basin from what is now western Yunnan in the early 9th century, when the Pyu city-states had been weakened by an invasion by Nanzhao.
Other Burmish languages are still spoken in Dehong Prefecture in the far west of Yunnan.
By the 11th century their Pagan Kingdom had expanded over the whole basin.
The oldest texts, such as the Myazedi inscription, date from the early 12th century.

The Tibetic languages are spoken by some 6 million people on the Tibetan Plateau and neighbouring areas in the Himalayas and western Sichuan.
They are descended from Old Tibetan, which was originally spoken in the Yarlung Valley before it was spread by the expansion of the Tibetan Empire in the 7th century.
Although the empire collapsed in the 9th century, Classical Tibetan remained influential as the liturgical language of Tibetan Buddhism.

The remaining languages are spoken in upland areas.
Southernmost are the Karen languages, spoken by 4 million people in the hill country along the Myanmar–Thailand border, with the greatest diversity in the Karen Hills, which are believed to be the homeland of the group.
The highlands stretching from northeast India to northern Myanmar contain over 100 high-diverse Sino-Tibetan languages.
Other Sino-Tibetan languages are found along the southern slopes of the Himalayas, southwest China and northern Thailand.

There have been a range of proposals for the Sino-Tibetan urheimat, reflecting the uncertainty about the classification of the family and its time depth.
James Matisoff (1991) places it in the eastern part of the Tibetan plateau around 4000 BC, with the various groups migrating out down the Yellow, Yangtze, Mekong, Salween and Brahmaputra rivers.
George van Driem (2005) proposes that Sino-Tibetan originated in the Sichuan Basin before 7000 BC, with an early migration into northeast India, and a later migration north of the predecessors of Chinese and Tibetic. Roger Blench and Mark Post (2014) have proposed that the Sino-Tibetan homeland is Northeast India, the area of greatest diversity, around 7000 BC.
Roger Blench (2009) argues that agriculture cannot be reconstructed for Proto-Sino-Tibetan, and that the earliest speakers of Sino-Tibetan were not farmers but highly diverse foragers.

Zhang et al. (2019) performed a computational phylogenetic analysis of 109 Sino-Tibetan languages to suggest a Sino-Tibetan homeland in northern China near the Yellow River basin. The study further suggests that there was an initial major split between the Sinitic languages and the Tibeto-Burman languages approximately 4,200 to 7,800 years ago (with an average of 5,900 years ago), associating this expansion with the Yangshao culture and/or the later Majiayao culture. Sagart et al. (2019) also performed another phylogenetic analysis based on different data and methods to arrive at the same conclusions with respect to the homeland and divergence model, but proposed an earlier root age of approximately 7,200 years ago, associating its origin with the late Cishan and early Yangshao culture.

Several low-level branches of the family, particularly Lolo-Burmese, have been securely reconstructed, but in the absence of a secure reconstruction of a Sino-Tibetan proto-language, the higher-level structure of the family remains unclear.
Thus, a conservative classification of Sino-Tibetan/Tibeto-Burman would posit several dozen small coordinate families and isolates; attempts at subgrouping are either geographic conveniences or hypotheses for further research.

In a survey in the 1937 "Chinese Yearbook", Li Fang-Kuei described the family as consisting of four branches:

Tai and Miao–Yao were included because they shared isolating typology, tone systems and some vocabulary with Chinese. At the time, tone was considered so fundamental to language that tonal typology could be used as the basis for classification. In the Western scholarly community, these languages are no longer included in Sino-Tibetan, with the similarities attributed to diffusion across the Mainland Southeast Asia linguistic area, especially since .
The exclusions of Vietnamese by Kuhn and of Tai and Miao–Yao by Benedict were vindicated in 1954 when André-Georges Haudricourt demonstrated that the tones of Vietnamese were reflexes of final consonants from Proto-Mon–Khmer.

Many Chinese linguists continue to follow Li's classification. However, this arrangement remains problematic. For example, there is disagreement over whether to include the entire Kra–Dai family or just Kam–Tai (Zhuang–Dong excludes the Kra languages), because the Chinese cognates that form the basis of the putative relationship are not found in all branches of the family and have not been reconstructed for the family as a whole. In addition, Kam–Tai itself no longer appears to be a valid node within Kra–Dai.

Benedict overtly excluded Vietnamese (placing it in Mon–Khmer) as well as Hmong–Mien and Kra–Dai (placing them in Austro-Tai).
He otherwise retained the outlines of Conrady's Indo-Chinese classification, though putting Karen in an intermediate position:

Shafer criticized the division of the family into Tibeto-Burman and Sino-Daic branches, which he attributed to the different groups of languages studied by Konow and other scholars in British India on the one hand and by Henri Maspero and other French linguists on the other.
He proposed a detailed classification, with six top-level divisions:

Shafer was sceptical of the inclusion of Daic, but after meeting Maspero in Paris decided to retain it pending a definitive resolution of the question.

James Matisoff abandoned Benedict's Tibeto-Karen hypothesis:

Some more-recent Western scholars, such as Bradley (1997) and La Polla (2003), have retained Matisoff's two primary branches, though differing in the details of Tibeto-Burman. However, Jacques (2006) notes, "comparative work has never been able to put forth evidence for common innovations to all the Tibeto-Burman languages (the Sino-Tibetan languages to the exclusion of Chinese)" and that "it no longer seems justified to treat Chinese as the first branching of the Sino-Tibetan family," because the morphological divide between Chinese and Tibeto-Burman has been bridged by recent reconstructions of Old Chinese.

The internal structure of Sino-Tibetan has been tentatively revised as the following Stammbaum by Matisoff (2015: xxxii, 1123-1127) in the final print release of the "Sino-Tibetan Etymological Dictionary and Thesaurus" (STEDT). Matisoff (2015: xxxi) acknowledges that the position of Chinese as either a sister branch of Tibeto-Burman or a branch within Tibeto-Burman remains an open question.


Sergei Starostin proposed that both the Kiranti languages and Chinese are divergent from a "core" Tibeto-Burman of at least Bodish, Lolo-Burmese, Tamangic, Jinghpaw, Kukish, and Karen (other families were not analysed) in a hypothesis called "Sino-Kiranti". The proposal takes two forms: that Sinitic and Kiranti are themselves a valid node or that the two are not demonstrably close, so that Sino-Tibetan has three primary branches:

Van Driem, like Shafer, rejects a primary split between Chinese and the rest, suggesting that Chinese owes its traditional privileged place in Sino-Tibetan to historical, typological, and cultural, rather than linguistic, criteria. He calls the entire family "Tibeto-Burman", a name he says has historical primacy, but other linguists who reject a privileged position for Chinese nevertheless continue to call the resulting family "Sino-Tibetan".

Like Matisoff, van Driem acknowledges that the relationships of the "Kuki–Naga" languages (Kuki, Mizo, Meitei, etc.), both amongst each other and to the other languages of the family, remain unclear. However, rather than placing them in a geographic grouping, as Matisoff does, van Driem leaves them unclassified.
He has proposed several hypotheses, including the reclassification of Chinese to a Sino-Bodic subgroup:

Van Driem points to two main pieces of evidence establishing a special relationship between Sinitic and Bodic and thus placing Chinese within the Tibeto-Burman family. First, there are a number of parallels between the morphology of Old Chinese and the modern Bodic languages. Second, there is an impressive body of lexical cognates between the Chinese and Bodic languages, represented by the Kirantic language Limbu.

In response, Matisoff notes that the existence of shared lexical material only serves to establish an absolute relationship between two language families, not their relative relationship to one another. Although some cognate sets presented by van Driem are confined to Chinese and Bodic, many others are found in Sino-Tibetan languages generally and thus do not serve as evidence for a special relationship between Chinese and Bodic.

George van Driem (2001) has also proposed a "fallen leaves" model that lists dozens of well-established low-level groups while remaining agnostic about intermediate groupings of these.
In the most recent version (van Driem 2014), 42 groups are identified (with individual languages highlighted in "italics"):
van Driem (2007) also suggested that the Sino-Tibetan language family be renamed "Trans-Himalayan", which he considers to be more neutral.

Roger Blench and Mark W. Post have criticized the applicability of conventional Sino-Tibetan classification schemes to minor languages lacking an extensive written history (unlike Chinese, Tibetic, and Burmese). They find that the evidence for the subclassification or even ST affiliation at all of several minor languages of northeastern India, in particular, is either poor or absent altogether.

In their view, many such languages would for now be best considered unclassified, or "internal isolates" within the family. They propose a provisional classification of the remaining languages:
Following that, because they propose that the three best-known branches may actually be much closer related to each other than they are to "minor" Sino-Tibetan languages, Blench and Post argue that "Sino-Tibetan" or "Tibeto-Burman" are inappropriate names for a family whose earliest divergences led to different languages altogether. They support the proposed name "Trans-Himalayan".

This is the classification scheme proposed in Menghan Zhang, Shi Yan, et al. (2019).

Except for the Chinese, Bai, Karenic, and Mruic languages, the usual word order in Sino-Tibetan languages is object–verb. Most scholars believe this to be the original order, with Chinese, Karen and Bai having acquired subject–verb–object order due to the influence of neighbouring languages in the Mainland Southeast Asia linguistic area.
However, Chinese and Bai differ from almost all other SVO languages in the world in placing relative clauses before the nouns they modify.

Hodgson had in 1849 noted a dichotomy between "pronominalized" (inflecting) languages, stretching across the Himalayas from Himachal Pradesh to eastern Nepal, and "non-pronominalized" (isolating) languages. Konow (1909) explained the pronominalized languages as due to a Munda substratum, with the idea that Indo-Chinese languages were essentially isolating as well as tonal. Maspero later attributed the putative substratum to Indo-Aryan. It was not until Benedict that the inflectional systems of these languages were recognized as (partially) native to the family.
Scholars disagree over the extent to which the agreement system in the various languages can be reconstructed for the proto-language.

In morphosyntactic alignment, many Tibeto-Burman languages have ergative and/or anti-ergative (an argument that is not an actor) case marking. However, the anti-ergative case markings can not be reconstructed at higher levels in the family and are thought to be innovations.

Beyond the traditionally recognized families of Southeast Asia, a number of possible broader relationships have been suggested:

One of these is the "Sino-Caucasian" hypothesis of Sergei Starostin, which posits that the Yeniseian languages and North Caucasian languages form a clade with Sino-Tibetan. The Sino-Caucasian hypothesis has been expanded by others to "Dené–Caucasian" to include the Na-Dené languages of North America, Burushaski, Basque and, occasionally, Etruscan. Edward Sapir had commented on a connection between Na-Dené and Sino-Tibetan. A narrower binary Dené–Yeniseian family has recently been well-received. The validity of the rest of the family, however, is viewed as doubtful or rejected by nearly all historical linguists.

Geoffrey Caveney (2014) suggest that the Sino-Tibetan and Na-Dene languages are related but say that his analysis does not support the Sino-Caucasian or Dene-Caucasian hypothesis.

In contrast, Laurent Sagart proposes a Sino-Austronesian family with Sino-Tibetan and Austronesian (including Kra–Dai as a subbranch) as primary branches. Stanley Starosta has extended this proposal with a further branch called "Yangzian" joining Hmong–Mien and Austroasiatic.





</doc>
<doc id="26757" url="https://en.wikipedia.org/wiki?curid=26757" title="Slavic languages">
Slavic languages

The Slavic languages, also known as the Slavonic languages, are Indo-European languages spoken primarily by the Slavic peoples or their descendants. They are thought to descend from a proto-language called Proto-Slavic, spoken during the Early Middle Ages, which in turn is thought to have descended from the earlier Proto-Balto-Slavic language, linking the Slavic languages to the Baltic languages in a Balto-Slavic group within the Indo-European family.

The Slavic languages are conventionally (that is, also on the basis of extralinguistic features) divided intro three subgroups: East, West, and South, which together constitute more than 20 languages. Of these, 10 have at least one million speakers and official status as the national languages of the countries in which they are predominantly spoken: Russian, Belarusian and Ukrainian (of the East group), Polish, Czech and Slovak (of the West group) and Bulgarian and Macedonian (eastern dialects of the South group), and Serbo-Croatian and Slovene (western dialects of the South group). In addition, Aleksandr Dulichenko recognizes a number of Slavic microlanguages: both isolated ethnolects and peripheral dialects of more well-established Slavic languages.

The current geographic distribution of natively spoken Slavic languages includes Southern Europe, Central Europe, the Balkans, Eastern Europe, and all of the territory of Russia, which includes northern and north-central Asia (though many minority languages of Russia are also still spoken). Furthermore, the diasporas of many Slavic peoples have established isolated minorities of speakers of their languages all over the world. The number of speakers of all Slavic languages together was estimated to be 315 million at the turn of the twenty-first century.

Since the interwar period scholars have conventionally divided Slavic languages, on the basis of geographical and genealogical principle, and with the use of the extralinguistic feature of script, into three main branches, that is, East, West and South. (From the vantage of linguistic features alone, there are only two branches of the Slavic languages, namely North and South. These three conventional branches feature some of the following subbranches:




Some linguists speculate that a North Slavic branch has existed as well. The Old Novgorod dialect may have reflected some idiosyncrasies of this group.
Mutual intelligibility also plays a role in determining the West, East, and South branches. Speakers of languages within the same branch will in most cases be able to understand each other at least partially, but they are generally unable to across branches (which would be comparable to a native English speaker trying to understand any other Germanic language besides Scots).

The most obvious differences between the East, West and South Slavic branches are in the orthography of the standard languages: West Slavic languages (and Western South Slavic languages – Croatian and Slovene) are written in the Latin script, and have had more Western European influence due to their proximity and speakers being historically Roman Catholic, whereas the East Slavic and Eastern South Slavic languages are written in Cyrillic and, with Eastern Orthodox or Uniate faith, have had more Greek influence. East Slavic languages such as Russian have, however, during and after Peter the Great's Europeanization campaign, absorbed many words of Latin, French, German, and Italian origin.

The tripartite division of the Slavic languages does not take into account the spoken dialects of each language. Of these, certain so-called transitional dialects and hybrid dialects often bridge the gaps between different languages, showing similarities that do not stand out when comparing Slavic literary (i.e. standard) languages. For example, Slovak (West Slavic) and Ukrainian (East Slavic) are bridged by the Rusyn language/dialect of Eastern Slovakia and Western Ukraine. Similarly, the Croatian Kajkavian dialect is more similar to Slovene than to the standard Croatian language.

Although the Slavic languages diverged from a common proto-language later than any other group of the Indo-European language family, enough differences exist between the various Slavic dialects and languages to make communication between speakers of different Slavic languages difficult. Within the individual Slavic languages, dialects may vary to a lesser degree, as those of Russian, or to a much greater degree, as those of Slovene.

Slavic languages descend from Proto-Slavic, their immediate parent language, ultimately deriving from Proto-Indo-European, the ancestor language of all Indo-European languages, via a Proto-Balto-Slavic stage. During the Proto-Balto-Slavic period a number of exclusive isoglosses in phonology, morphology, lexis, and syntax developed, which makes Slavic and Baltic the closest related of all the Indo-European branches. The secession of the Balto-Slavic dialect ancestral to Proto-Slavic is estimated on archaeological and glottochronological criteria to have occurred sometime in the period 1500–1000 BCE.

A minority of Baltists maintain the view that the Slavic group of languages differs so radically from the neighboring Baltic group (Lithuanian, Latvian, and the now-extinct Old Prussian), that they could not have shared a parent language after the breakup of the Proto-Indo-European continuum about five millennia ago. Substantial advances in Balto-Slavic accentology that occurred in the last three decades, however, make this view very hard to maintain nowadays, especially when one considers that there was most likely no "Proto-Baltic" language and that West Baltic and East Baltic differ from each other as much as each of them does from Proto-Slavic.

The imposition of Old Church Slavonic on Orthodox Slavs was often at the expense of the vernacular. Says WB Lockwood, a prominent Indo-European linguist, "It (O.C.S) remained in use to modern times but was more and more influenced by the living, evolving languages, so that one distinguishes Bulgarian, Serbian, and Russian varieties. The use of such media hampered the development of the local languages for literary purposes, and when they do appear the first attempts are usually in an artificially mixed style." (148)

Lockwood also notes that these languages have "enriched" themselves by drawing on Church Slavonic for the vocabulary of abstract concepts. The situation in the Catholic countries, where Latin was more important, was different. The Polish Renaissance poet Jan Kochanowski and the Croatian Baroque writers of the 16th century all wrote in their respective vernaculars (though Polish itself had drawn amply on Latin in the same way Russian would eventually draw on Church Slavonic).
Although Church Slavonic hampered vernacular literatures, it fostered Slavonic literary activity and abetted linguistic independence from external influences. Only the Croatian vernacular literary tradition nearly matches Church Slavonic in age. It began with the Vinodol Codex and continued through the Renaissance until the codifications of Croatian in 1830, though much of the literature between 1300 and 1500 was written in much the same mixture of the vernacular and Church Slavonic as prevailed in Russia and elsewhere.

The most important early monument of Croatian literacy is the Baška tablet from the late 11th century. It is a large stone tablet found in the small Church of St. Lucy, Jurandvor on the Croatian island of Krk, containing text written mostly in Čakavian dialect in angular Croatian Glagolitic script. The independence of Dubrovnik facilitated the continuity of the tradition.
More recent foreign influences follow the same general pattern in Slavic languages as elsewhere and are governed by the political relationships of the Slavs. In the 17th century, bourgeois Russian ("delovoi jazyk") absorbed German words through direct contacts between Russians and communities of German settlers in Russia. In the era of Peter the Great, close contacts with France invited countless loan words and calques from French, many of which not only survived but also replaced older Slavonic loans. In the 19th century, Russian influenced most literary Slavic languages by one means or another.

The Proto-Slavic language existed until around AD 500. By the 7th century, it had broken apart into large dialectal zones.

There are no reliable hypotheses about the nature of the subsequent breakups of West and South Slavic. East Slavic is generally thought to converge to one Old East Slavic language, which existed until at least the 12th century.

Linguistic differentiation was accelerated by the dispersion of the Slavic peoples over a large territory, which in Central Europe exceeded the current extent of Slavic-speaking majorities. Written documents of the 9th, 10th, and 11th centuries already display some local linguistic features. For example, the Freising manuscripts show a language that contains some phonetic and lexical elements peculiar to Slovene dialects (e.g. rhotacism, the word "krilatec"). The Freising manuscripts are the first Latin-script continuous text in a Slavic language.

The migration of Slavic speakers into the Balkans in the declining centuries of the Byzantine Empire expanded the area of Slavic speech, but the pre-existing writing (notably Greek) survived in this area. The arrival of the Hungarians in Pannonia in the 9th century interposed non-Slavic speakers between South and West Slavs. Frankish conquests completed the geographical separation between these two groups, also severing the connection between Slavs in Moravia and Lower Austria (Moravians) and those in present-day Styria, Carinthia, East Tyrol in Austria, and in the provinces of modern Slovenia, where the ancestors of the Slovenes settled during first colonisation.
In September 2015, Alexei Kassian and Anna Dybo published, as a part of interdisciplinary study of Slavic ethnogenesis, a lexicostatistical classification of Slavic languages. It was built using qualitative 110-word Swadesh lists that were compiled according to the standards of the Global Lexicostatistical Database project and processed using modern phylogenetic algorithms.

The resulting dated tree complies with the traditional expert views on the Slavic group structure. Kassian-Dybo's tree suggests that Proto-Slavic first diverged into three branches: Eastern, Western and Southern. The Proto-Slavic break-up is dated to around 100 A.D., which correlates with the archaeological assessment of Slavic population in the early 1st millennium A.D. being spread on a large territory and already not being monolithic. Then, in the 5th and 6th centuries A.D., these three Slavic branches almost simultaneously divided into sub-branches, which corresponds to the fast spread of the Slavs through Eastern Europe and the Balkans during the second half of the 1st millennium A.D. (the so-called Slavicization of Europe).

The Slovenian language was excluded from the analysis, as both Ljubljana koine and Literary Slovenian show mixed lexical features of Southern and Western Slavic languages (which could possibly indicate the Western Slavic origin of Slovenian, which for a long time was being influenced on the part of the neighboring Serbo-Croatian dialects), and the quality Swadesh lists were not yet collected for Slovenian dialects. Because of scarcity or unreliability of data, the study also did not cover the so-called Old Novgordian dialect, the Polabian language and some other Slavic lects.

The above Kassian-Dybo's research did not take into account the findings by Russian linguist Andrey Zaliznyak who stated that in the 11th century Novgorod language differed from Kiev language as well as from all other Slavic languages much more than in later centuries, meaning that there was no common Old East Slavic language of Kievan Rus' from which Ukrainian, Russian and Belorusian languages diverged, but that Russian language developed as convergence of Novgorod language and other Russian dialects, whereas Ukrainian and Belorusian were continuation of development of respective Kiev and Polotsk dialects of Kievan Rus'.

Also Russian linguist Sergey Nikolaev, analysing historical development of Slavic dialects’ accent system, concluded that a number of other tribes in Kievan Rus came from different Slavic branches and spoke distant Slavic dialects.

Zaliznyak and Nikolaev's points mean that there was a convergence stage before the divergence or simultaneously, which was not taken into consideration by Kassian-Dybo's research.

Ukrainian linguists (Stepan Smal-Stotsky, Ivan Ohienko, George Shevelov, Yevhen Tymchenko, Vsevolod Hantsov, Olena Kurylo) deny the existence of a common Old East Slavic language at any time in the past. According to them, the dialects of East Slavic tribes evolved gradually from the common Proto-Slavic language without any intermediate stages.

The following is a summary of the main changes from Proto-Indo-European (PIE) leading up to the Common Slavic (CS) period immediately following the Proto-Slavic language (PS).


The Slavic languages are a relatively homogeneous family, compared with other families of Indo-European languages (e.g. Germanic, Romance, and Indo-Iranian). As late as the 10th century AD, the entire Slavic-speaking area still functioned as a single, dialectally differentiated language, termed "Common Slavic". Compared with most other Indo-European languages, the Slavic languages are quite conservative, particularly in terms of morphology (the means of inflecting nouns and verbs to indicate grammatical differences). Most Slavic languages have a rich, fusional morphology that conserves much of the inflectional morphology of Proto-Indo-European.

The following table shows the inventory of consonants of Late Common Slavic:

The sound did not occur in West Slavic, where it had developed to .

This inventory of sounds is quite similar to what is found in most modern Slavic languages. The extensive series of palatal consonants, along with the affricates *ts and *dz, developed through a series of palatalizations that happened during the Proto-Slavic period, from earlier sequences either of velar consonants followed by front vowels (e.g. *ke, *ki, *ge, *gi, *xe, and *xi), or of various consonants followed by *j (e.g. *tj, *dj, *sj, *zj, *rj, *lj, *kj, and *gj, where *j is the palatal approximant (, the sound of the English letter "y" in "yes" or "you").

The biggest change in this inventory results from a further general palatalization occurring near the end of the Common Slavic period, where "all" consonants became palatalized before front vowels. This produced a large number of new palatalized (or "soft") sounds, which formed pairs with the corresponding non-palatalized (or "hard") consonants and absorbed the existing palatalized sounds . These sounds were best preserved in Russian but were lost to varying degrees in other languages (particularly Czech and Slovak). The following table shows the inventory of modern Russian:

This general process of palatalization did not occur in Serbo-Croatian and Slovenian. As a result, the modern consonant inventory of these languages is nearly identical to the Late Common Slavic inventory.

Late Common Slavic tolerated relatively few consonant clusters. However, as a result of the loss of certain formerly present vowels (the weak yers), the modern Slavic languages allow quite complex clusters, as in the Russian word взблеск ("flash"). Also present in many Slavic languages are clusters rarely found cross-linguistically, as in Russian ртуть ("mercury") or Polish mchu ("moss", gen. sg.). The word for "mercury" with the initial "rt-" cluster, for example, is also found in the other East and West Slavic languages, although Slovak retains an epenthetic vowel ("ortuť").

A typical vowel inventory is as follows:

The sound occurs only in some languages (Russian and Belarusian), and even in these languages, it is unclear whether it is its own phoneme or an allophone of /i/. Nonetheless, it is a quite prominent and noticeable characteristic of the languages in which it is present.

Common Slavic also had two nasal vowels: *ę and *ǫ . However, these are preserved only in modern Polish (along with a few lesser-known dialects and microlanguages; see Yus for more details).

Other phonemic vowels are found in certain languages (e.g. the schwa in Bulgarian and Slovenian, distinct high-mid and low-mid vowels in Slovenian, and the lax front vowel in Ukrainian).

An area of great difference among Slavic languages is that of prosody (i.e. syllabic distinctions such as vowel length, accent, and tone). Common Slavic had a complex system of prosody, inherited with little change from Proto-Indo-European. This consisted of phonemic vowel length and a free, mobile pitch accent:

The modern languages vary greatly in the extent to which they preserve this system. On one extreme, Serbo-Croatian preserves the system nearly unchanged (even more so in the conservative Chakavian dialect); on the other, Macedonian has basically lost the system in its entirety. Between them are found numerous variations:

Similarly, Slavic languages have extensive morphophonemic alternations in their derivational and inflectional morphology, including between velar and postalveolar consonants, front and back vowels, and a vowel and no vowel.

The following is a very brief selection of cognates in basic vocabulary across the Slavic language family, which may serve to give an idea of the sound changes involved. This is not a list of translations: cognates have a common origin, but their meaning may be shifted and loanwords may have replaced them.

Most languages of the former Soviet Union and of some neighbouring countries (for example, Mongolian) are significantly influenced by Russian, especially in vocabulary. The Romanian, Albanian, and Hungarian languages show the influence of the neighboring Slavic nations, especially in vocabulary pertaining to urban life, agriculture, and crafts and trade—the major cultural innovations at times of limited long-range cultural contact. In each one of these languages, Slavic lexical borrowings represent at least 15% of the total vocabulary. However, Romanian has much lower influence from Slavic than Albanian or Hungarian. This is potentially because Slavic tribes crossed and partially settled the territories inhabited by ancient Illyrians and Vlachs on their way to the Balkans.

The Slavic contributions to Germanic languages remains a moot question, though Max Vasmer, a specialist in Slavic etymology, has claimed that there were no Slavic loans into Proto-Germanic. Nevertheless, many linguists, including Andrzej Poppe, consider the Slavic contribution to the Germanic languages to be enormous. A large number of Slavic loanwords are found in the Gothic language: "hlaifs" (bread, from Proto-Slavonic "hleb"), "katils" (cauldron, from Proto-Slavonic "kotel"), "biuþs" (table, from Proto-Slavonic "bliudo"), "kaupjan" (buy, from Proto-Slavonic "kupit"), "skeinan" (shine, from Proto-Slavonic "sianye"), "boka" (letter, from Proto-Slavonic "bukva"), etc. On the other hand, scientists Rasmus Rask and August Schleicher argued that such a number of slovenisms in the German language is explained by the fact that the Slavic and Germanic languages have a common origin. However, there are isolated Slavic loans (mostly recent) into other Germanic languages. For example, the word for "border" (in modern German "Grenze", Dutch "grens") was borrowed from the Common Slavic "granica". There are, however, many cities and villages of Slavic origin in Eastern Germany, the largest of which are Berlin, Leipzig and Dresden. English derives "quark" (a kind of cheese, not the subatomic particle) from the German "Quark", which in turn is derived from the Slavic "tvarog", which means "curd". Many German surnames, particularly in Eastern Germany and Austria, are Slavic in origin. Swedish also has "torg" (market place) from Old Russian "tъrgъ" or Polish "targ", "tolk" (interpreter) from Old Slavic "tlŭkŭ", and "pråm" (barge) from West Slavonic "pramŭ".

The Czech word is now found in most languages worldwide, and the word , probably also from Czech, is found in many European languages, such as Greek "".

A well-known Slavic word in almost all European languages is vodka, a borrowing from Russian "водка" ("vodka") – which itself was borrowed from Polish "wódka" (lit. "little water"), from common Slavic "voda" ("water", cognate to the English word) with the diminutive ending "-ka". Owing to the medieval fur trade with Northern Russia, Pan-European loans from Russian include such familiar words as "sable". The English word "vampire" was borrowed (perhaps via French "vampire") from German "Vampir", in turn derived from Serbian "vampir", continuing Proto-Slavic "*ǫpyrь", although Polish scholar K. Stachowski has argued that the origin of the word is early Slavic "*vąpěrь", going back to Turkic "oobyr". Several European languages, including English, have borrowed the word "polje" (meaning "large, flat plain") directly from the former Yugoslav languages (i.e. Slovene, Croatian, and Serbian). During the heyday of the USSR in the 20th century, many more Russian words became known worldwide: "da", "Soviet", "sputnik", "perestroika", "glasnost", "kolkhoz", etc. Also in the English language borrowed from Russian is "samovar" (lit. "self-boiling") to refer to the specific Russian tea urn.

The following tree for the Slavic languages derives from the Ethnologue report for Slavic languages. It includes the ISO 639-1 and ISO 639-3 codes where available.
East Slavic languages:

West Slavic languages:

South Slavic languages:

Para- and supranational languages





</doc>
<doc id="26758" url="https://en.wikipedia.org/wiki?curid=26758" title="SGI">
SGI

SGI may refer to:





</doc>
<doc id="26764" url="https://en.wikipedia.org/wiki?curid=26764" title="International System of Units">
International System of Units

The International System of Units (SI, abbreviated from the French "") is the modern form of the metric system. It is the only system of measurement with an official status in nearly every country in the world. It comprises a coherent system of units of measurement starting with seven base units, which are the second (the unit of time with the symbol s), metre (length, m), kilogram (mass, kg), ampere (electric current, A), kelvin (thermodynamic temperature, K), mole (amount of substance, mol), and candela (luminous intensity, cd). The system allows for an unlimited number of additional units, called derived units, which can always be represented as products of powers of the base units. Twenty-two derived units have been provided with special names and symbols. The seven base units and the 22 derived units with special names and symbols may be used in combination to express other derived units, which are adopted to facilitate measurement of diverse quantities. The SI system also provides twenty prefixes to the unit names and unit symbols that may be used when specifying power-of-ten (i.e. decimal) multiples and sub-multiples of SI units. The SI is intended to be an evolving system; units and prefixes are created and unit definitions are modified through international agreement as the technology of measurement progresses and the precision of measurements improves.

Since 2019, the magnitudes of all SI units have been defined by declaring exact numerical values for seven "defining constants" when expressed in terms of their SI units. These defining constants are the speed of light in vacuum, , the hyperfine transition frequency of caesium , the Planck constant , the elementary charge , the Boltzmann constant , the Avogadro constant , and the luminous efficacy . The nature of the defining constants ranges from fundamental constants of nature such as to the purely technical constant . Prior to 2019, , , , and were not defined a priori but were rather very precisely measured quantities. In 2019, their values were fixed by definition to their best estimates at the time, ensuring continuity with previous definitions of the base units. One consequence of the redefinition of the SI is that the distinction between the base units and derived units is in principle not needed, since any unit can be constructed directly from the seven defining constants.

The current way of defining the SI system is a result of a decades-long move towards increasingly abstract and idealised formulation in which the realisations of the units are separated conceptually from the definitions. A consequence is that as science and technologies develop, new and superior realisations may be introduced without the need to redefine the unit. One problem with artefacts is that they can be lost, damaged, or changed; another is that they introduce uncertainties that cannot be reduced by advancements in science and technology. The last artefact used by the SI was the International Prototype of the Kilogram, a cylinder of platinum-iridium.
The original motivation for the development of the SI was the diversity of units that had sprung up within the centimetre–gram–second (CGS) systems (specifically the inconsistency between the systems of electrostatic units and electromagnetic units) and the lack of coordination between the various disciplines that used them. The General Conference on Weights and Measures (French: "" – CGPM), which was established by the Metre Convention of 1875, brought together many international organisations to establish the definitions and standards of a new system and to standardise the rules for writing and presenting measurements. The system was published in 1960 as a result of an initiative that began in 1948. It is based on the metre–kilogram–second system of units (MKS) rather than any variant of the CGS.

The International System of Units, the SI, is a decimal and metric system of units established in 1960 and periodically updated since then. The SI has an official status in most countries, including the United States and the United Kingdom, with these two countries being amongst a handful of nations that, to various degrees, continue to resist widespread internal adoption of the SI system. As a consequence, the SI system “has been used around the world as the preferred system of units, the basic language for science, technology, industry and trade.”

The only other types of measurement system that still have widespread use across the world are the Imperial and US customary measurement systems, and they are legally defined in terms of the SI system. There are other, less widespread systems of measurement that are occasionally used in particular regions of the world. In addition, there are many individual non-SI units that don't belong to any comprehensive system of units, but that are nevertheless still regularly used in particular fields and regions. Both of these categories of unit are also typically defined legally in terms of SI units.

The SI was established and is maintained by the General Conference on Weights and Measures (CGPM). In practice, the CGPM follows the recommendations of the Consultative Committee for Units (CCU), which is the actual body conducting technical deliberations concerning new scientific and technological developments related to the definition of units and the SI. The CCU reports to the International Committee for Weights and Measures (CIPM), which, in turn, reports to the CGPM. See below for more details.

All the decisions and recommendations concerning units are collected in a brochure called "The International System of Units (SI)", which is published by the International Bureau of Weights and Measures (BIPM) and periodically updated.

The SI selects seven units to serve as base units, corresponding to seven base physical quantities. They are the second, with the symbol , which is the SI unit of the physical quantity of time; the metre, symbol , the SI unit of length; kilogram (, the unit of mass); ampere (, electric current); kelvin (, thermodynamic temperature), mole (, amount of substance); and candela (, luminous intensity). Note that 'the choice of the base units was never unique, but grew historically and became familiar to users of the SI'. All units in the SI can be expressed in terms of the base units, and the base units serve as a preferred set for expressing or analysing the relationships between units.

The system allows for an unlimited number of additional units, called "derived units", which can always be represented as products of powers of the base units, possibly with a nontrivial numeric multiplier. When that multiplier is one, the unit is called a "coherent" derived unit. The base and coherent derived units of the SI together form a coherent system of units ("the set of coherent SI units"). Twenty-two coherent derived units have been provided with special names and symbols. The seven base units and the 22 derived units with special names and symbols may be used in combination to express other derived units, which are adopted to facilitate measurement of diverse quantities.

Like all metric systems, the SI uses metric prefixes to systematically construct, for the same physical quantity, a set of units that are decimal multiples of each other over a wide range.

For example, while the coherent unit of length is the metre, the SI provides a full range of smaller and larger units of length, any of which may be more convenient for any given application – for example, driving distances are normally given in kilometres (symbol ) rather than in metres. Here the metric prefix 'kilo-' (symbol 'k') stands for a factor of 1000; thus, = .

The current version of the SI provides twenty metric prefixes that signify decimal powers ranging from 10 to 10. Apart from the prefixes for 1/100, 1/10, 10, and 100, all the other ones are powers of 1000.

In general, given any coherent unit with a separate name and symbol, one forms a new unit by simply adding an appropriate metric prefix to the name of the coherent unit (and a corresponding prefix symbol to the unit's symbol). Since the metric prefix signifies a particular power of ten, the new unit is always a power-of-ten multiple or sub-multiple of the coherent unit. Thus, the conversion between units within the SI is always through a power of ten; this is why the SI system (and metric systems more generally) are called "decimal systems of measurement units".

The grouping formed by a prefix symbol attached to a unit symbol (e.g. ", ") constitutes a new inseparable unit symbol. This new symbol can be raised to a positive or negative power and can be combined with other unit symbols to form compound unit symbols. For example, is an SI unit of density, where is to be interpreted as ().

When prefixes are used with the coherent SI units, the resulting units are no longer coherent, because the prefix introduces a numerical factor other than one. The one exception is the kilogram, the only coherent SI unit whose name and symbol, for historical reasons, include a prefix.

The complete set of SI units consists of both the coherent set and the multiples and sub-multiples of coherent units formed by using the SI prefixes. For example, the metre, kilometre, centimetre, nanometre, etc. are all SI units of length, though only the metre is a "coherent" SI unit. A similar statement holds for derived units: for example, , , , /, etc. are all SI units of density, but of these, only is a "coherent" SI unit.

Moreover, the metre is the "only" coherent SI unit of length. Every physical quantity has exactly one coherent SI unit, although this unit may be expressible in different forms by using some of the special names and symbols. For example, the coherent SI unit of linear momentum may be written as either or as , and both forms are in use (e.g. compare respectively here and here).

On the other hand, several different quantities may share same coherent SI unit. For example, the joule per kelvin is the coherent SI unit for two distinct quantities: heat capacity and entropy. Furthermore, the same coherent SI unit may be a base unit in one context, but a coherent derived unit in another. For example, the ampere is the coherent SI unit for both electric current and magnetomotive force, but it is a base unit in the former case and a derived unit in the latter.

There is a special group of units that are called 'non-SI units that are accepted for use with the SI'. See Non-SI units mentioned in the SI for a full list. Most of these, in order to be converted to the corresponding SI unit, require conversion factors that are not powers of ten. Some common examples of such units are the customary units of time, namely the minute (conversion factor of 60 s/min, since 1 min ), the hour (), and the day (); the degree (for measuring plane angles, and the electronvolt (a unit of energy, 

The SI is intended to be an evolving system; units and prefixes are created and unit definitions are modified through international agreement as the technology of measurement progresses and the precision of measurements improves.

Since 2019, the magnitudes of all SI units have been defined in an abstract way, which is conceptually separated from any practical realisation of them. Namely, the SI units are defined by declaring that seven "defining constants" have certain exact numerical values when expressed in terms of their SI units. Probably the most widely known of these constants is the speed of light in vacuum, , which in the SI by definition has the exact value of = . The other six constants are formula_1, the hyperfine transition frequency of caesium; , the Planck constant; , the elementary charge; , the Boltzmann constant; , the Avogadro constant; and , the luminous efficacy of monochromatic radiation of frequency .<br>
</math> .}}}} The nature of the defining constants ranges from fundamental constants of nature such as to the purely technical constant . Prior to 2019, , , , and were not defined a priori but were rather very precisely measured quantities. In 2019, their values were fixed by definition to their best estimates at the time, ensuring continuity with previous definitions of the base units.

As far as realisations, what are believed to be the current best practical realisations of units are described in the so-called "mises en pratique", which are also published by the BIPM. The abstract nature of the definitions of units is what makes it possible to improve and change the "mises en pratique" as science and technology develop without having to change the actual definitions themselves.

In a sense, this way of defining the SI units is no more abstract than the way derived units are traditionally defined in terms of the base units. Consider a particular derived unit, for example, the joule, the unit of energy. Its definition in terms of the base units is ⋅/. Even if the practical realisations of the metre, kilogram, and second are available, a practical realisation of the joule would require some sort of reference to the underlying physical definition of work or energy—some actual physical procedure for realising the energy in the amount of one joule such that it can be compared to other instances of energy (such as the energy content of gasoline put into a car or of electricity delivered to a household).

The situation with the defining constants and all of the SI units is analogous. In fact, purely "mathematically" speaking, the SI units are defined "as if" we declared that it is the defining constant's units that are now the base units, with all other SI units being derived units. To make this clearer, first note that each defining constant can be taken as determining the magnitude of that defining constant's unit of measurement; for example, the definition of defines the unit as = ('the speed of one metre per second is equal to one th of the speed of light'). In this way, the defining constants directly define the following seven units: the hertz (), a unit of the physical quantity of frequency (note that problems can arise when dealing with frequency or the Planck constant because the units of angular measure (cycle or radian) are omitted in SI.); the metre per second (), a unit of speed; joule-second (), a unit of action; coulomb (), a unit of electric charge; joule per kelvin (), a unit of both entropy and heat capacity; the inverse mole (), a unit of a conversion constant between the amount of substance and the number of elementary entities (atoms, molecules, etc.); and lumen per watt (), a unit of a conversion constant between the physical power carried by electromagnetic radiation and the intrinsic ability of that same radiation to produce visual perception of brightness in humans. Further, one can show, using dimensional analysis, that every coherent SI unit (whether base or derived) can be written as a unique product of powers of the units of the SI defining constants (in complete analogy to the fact that every coherent derived SI unit can be written as a unique product of powers of the base SI units). For example, the kilogram can be written as Thus, the kilogram is defined in terms of the three defining constants , , and because, on the one hand, these three defining constants respectively define the units , , and , while, on the other hand, the kilogram can be written in terms of these three units, namely, True, the question of how to actually realise the kilogram in practice would, at this point, still be open, but that is not really different from the fact that the question of how to actually realise the joule in practice is still in principle open even once one has achieved the practical realisations of the metre, kilogram, and second.

One consequence of the redefinition of the SI is that the distinction between the base units and derived units is in principle not needed, since any unit can be constructed directly from the seven defining constants. Nevertheless, the distinction is retained because 'it is useful and historically well established', and also because the ISO/IEC 80000 series of standards specifies base and derived quantities that necessarily have the corresponding SI units.

The current way of defining the SI system is the result of a decades-long move towards increasingly abstract and idealised formulation in which the
realisations of the units are separated conceptually from the definitions.

The great advantage of doing it this way is that as science and technologies develop, new and superior realisations may be introduced without the need to redefine the units. Units can now be realised with ‘an accuracy that is ultimately limited only by the quantum structure of nature and our technical abilities but not by the definitions themselves. Any valid equation of physics relating the defining constants to a unit can be used to realise the unit, thus creating opportunities for innovation... with increasing accuracy as technology proceeds.’ In practice, the CIPM Consultative Committees provide so-called ""mises en pratique" (practical techniques), which are the descriptions of what are currently believed to be best experimental realisations of the units.

This system lacks the conceptual simplicity of using artefacts (referred to as "prototypes") as realisations of units to define those units: with prototypes, the definition and the realisation are one and the same. However, using artefacts has two major disadvantages that, as soon as it is technologically and scientifically feasible, result in abandoning them as means for defining units. One major disadvantage is that artefacts can be lost, damaged, or changed. The other is that they largely cannot benefit from advancements in science and technology. The last artefact used by the SI was the International Prototype Kilogram (IPK), a particular cylinder of platinum-iridium; from 1889 to 2019, the kilogram was by definition equal to the mass of the IPK. Concerns regarding its stability on the one hand, and progress in precise measurements of the Planck constant and the Avogadro constant on the other, led to a revision of the definition of the base units, put into effect on 20 May 2019. This was the biggest change in the SI system since it was first formally defined and established in 1960, and it resulted in the definitions described above.

In the past, there were also various other approaches to the definitions of some of the SI units. One made use of a specific physical state of a specific substance (the triple point of water, which was used in the definition of the kelvin); others referred to idealised experimental prescriptions (as in the case of the former SI definition of the ampere and the former SI definition (originally enacted in 1979) of the candela).

In the future, the set of defining constants used by the SI may be modified as more stable constants are found, or if it turns out that other constants can be more precisely measured.

The original motivation for the development of the SI was the diversity of units that had sprung up within the centimetre–gram–second (CGS) systems (specifically the inconsistency between the systems of electrostatic units and electromagnetic units) and the lack of coordination between the various disciplines that used them. The General Conference on Weights and Measures (French: "" – CGPM), which was established by the Metre Convention of 1875, brought together many international organisations to establish the definitions and standards of a new system and to standardise the rules for writing and presenting measurements. The system was published in 1960 as a result of an initiative that began in 1948. It is based on the metre–kilogram–second system of units (MKS) rather than any variant of the CGS.

The SI is regulated and continually developed by three international organisations that were established in 1875 under the terms of the Metre Convention. They are the General Conference on Weights and Measures (CGPM), the International Committee for Weights and Measures (CIPM), and the International Bureau of Weights and Measures (BIPM). The ultimate authority rests with the CGPM, which is a plenary body through which its Member States act together on matters related to measurement science and measurement standards; it usually convenes every four years. The CGPM elects the CIPM, which is an 18-person committee of eminent scientists. The CIPM operates based on the advice of a number of its Consultative Committees, which bring together the world's experts in their specified fields as advisers on scientific and technical matters. One of these committees is the Consultative Committee for Units (CCU), which is responsible for "matters related to the development of the International System of Units (SI), preparation of successive editions of the SI brochure, and advice to the CIPM on matters concerning units of measurement." It is the CCU which considers in detail all new scientific and technological developments related to the definition of units and the SI. In practice, when it comes to the definition of the SI, the CGPM simply formally approves the recommendations of the CIPM, which, in turn, follows the advice of the CCU.

The CCU has the following as members: national laboratories of the Member States of the CGPM charged with establishing national standards; relevant intergovernmental organisations and international bodies;
international commissions or committees;
scientific unions; personal members;
and, as an ex officio member of all Consultative Committees, the Director of the BIPM.

All the decisions and recommendations concerning units are collected in a brochure called "The International System of Units (SI)", which is published by the BIPM and periodically updated.

The International System of Units consists of a set of base units, derived units, and a set of decimal-based multipliers that are used as prefixes. The units, excluding prefixed units, form a coherent system of units, which is based on a system of quantities in such a way that the equations between the numerical values expressed in coherent units have exactly the same form, including numerical factors, as the corresponding equations between the quantities. For example, 1 N = 1 kg × 1 m/s says that "one" newton is the force required to accelerate a mass of "one" kilogram at "one" metre per second squared, as related through the principle of coherence to the equation relating the corresponding quantities: .

Derived units apply to derived quantities, which may by definition be expressed in terms of base quantities, and thus are not independent; for example, electrical conductance is the inverse of electrical resistance, with the consequence that the siemens is the inverse of the ohm, and similarly, the ohm and siemens can be replaced with a ratio of an ampere and a volt, because those quantities bear a defined relationship to each other. Other useful derived quantities can be specified in terms of the SI base and derived units that have no named units in the SI system, such as acceleration, which is defined in SI units as m/s.

The SI base units are the building blocks of the system and all the other units are derived from them.

The derived units in the SI are formed by powers, products, or quotients of the base units and are potentially unlimited in number. Derived units are associated with derived quantities; for example, velocity is a quantity that is derived from the base quantities of time and length, and thus the SI derived unit is metre per second (symbol m/s). The dimensions of derived units can be expressed in terms of the dimensions of the base units.

Combinations of base and derived units may be used to express other derived units. For example, the SI unit of force is the newton (N), the SI unit of pressure is the pascal (Pa)—and the pascal can be defined as one newton per square metre (N/m).

Prefixes are added to unit names to produce multiples and submultiples of the original unit. All of these are integer powers of ten, and above a hundred or below a hundredth all are integer powers of a thousand. For example, "kilo-" denotes a multiple of a thousand and "milli-" denotes a multiple of a thousandth, so there are one thousand millimetres to the metre and one thousand metres to the kilometre. The prefixes are never combined, so for example a millionth of a metre is a "micrometre", not a millimillimetre. Multiples of the kilogram are named as if the gram were the base unit, so a millionth of a kilogram is a "milligram", not a microkilogram. When prefixes are used to form multiples and submultiples of SI base and derived units, the resulting units are no longer coherent.

The BIPM specifies 20 prefixes for the International System of Units (SI):

Many non-SI units continue to be used in the scientific, technical, and commercial literature. Some units are deeply embedded in history and culture, and their use has not been entirely replaced by their SI alternatives. The CIPM recognised and acknowledged such traditions by compiling a list of non-SI units accepted for use with SI:

Some units of time, angle, and legacy non-SI units have a long history of use. Most societies have used the solar day and its non-decimal subdivisions as a basis of time and, unlike the foot or the pound, these were the same regardless of where they were being measured. The radian, being of a revolution, has mathematical advantages but is rarely used for navigation. Further, the units used in navigation around the world are similar. The tonne, litre, and hectare were adopted by the CGPM in 1879 and have been retained as units that may be used alongside SI units, having been given unique symbols. The catalogued units are given below:

These units are used in combination with SI units in common units such as the kilowatt-hour (1 kW⋅h = 3.6 MJ).

The basic units of the metric system, as originally defined, represented common quantities or relationships in nature. They still do – the modern precisely defined quantities are refinements of definition and methodology, but still with the same magnitudes. In cases where laboratory precision may not be required or available, or where approximations are good enough, the original definitions may suffice.


The symbols for the SI units are intended to be identical, regardless of the language used, but names are ordinary nouns and use the character set and follow the grammatical rules of the language concerned. Names of units follow the grammatical rules associated with common nouns: in English and in French they start with a lowercase letter (e.g., newton, hertz, pascal), even when the unit is named after a person and its symbol begins with a capital letter. This also applies to "degrees Celsius", since "degree" is the beginning of the unit. The only exceptions are in the beginning of sentences and in headings and publication titles. The English spelling for certain SI units differs: US English uses the spelling "deka-", "meter", and "liter", whilst International English uses "deca-", "metre", and "litre".

Although the writing of unit names is language-specific, the writing of unit symbols and the values of quantities is consistent across all languages and therefore the SI Brochure has specific rules in respect of writing them. The guideline produced by the National Institute of Standards and Technology (NIST) clarifies language-specific areas in respect of American English that were left open by the SI Brochure, but is otherwise identical to the SI Brochure.

General rules for writing SI units and quantities apply to text that is either handwritten or produced using an automated process:


The rules covering printing of quantities and units are part of ISO 80000-1:2009.

Further rules are specified in respect of production of text using printing presses, word processors, typewriters, and the like.

The quantities and equations that provide the context in which the SI units are defined are now referred to as the "International System of Quantities" (ISQ).
The ISQ is based on the quantities underlying each of the seven base units of the SI. Other quantities, such as area, pressure, and electrical resistance, are derived from these base quantities by clear non-contradictory equations. The ISQ defines the quantities that are measured with the SI units. The ISQ is formalised, in part, in the international standard ISO/IEC 80000, which was completed in 2009 with the publication of ISO 80000-1, and has largely been revised in 2019–2020 with the remainder being under review.

Metrologists carefully distinguish between the definition of a unit and its realisation. The definition of each base unit of the SI is drawn up so that it is unique and provides a sound theoretical basis on which the most accurate and reproducible measurements can be made. The realisation of the definition of a unit is the procedure by which the definition may be used to establish the value and associated uncertainty of a quantity of the same kind as the unit. A description of the "mise en pratique" of the base units is given in an electronic appendix to the SI Brochure.

The published "mise en pratique" is not the only way in which a base unit can be determined: the SI Brochure states that "any method consistent with the laws of physics could be used to realise any SI unit." In the current (2016) exercise to overhaul the definitions of the base units, various consultative committees of the CIPM have required that more than one "mise en pratique" shall be developed for determining the value of each unit. In particular:

The International Bureau of Weights and Measures (BIPM) has described SI as "the modern form of metric system". Changing technology has led to an evolution of the definitions and standards that has followed two principal strands – changes to SI itself, and clarification of how to use units of measure that are not part of SI but are still nevertheless used on a worldwide basis.

Since 1960 the CGPM has made a number of changes to the SI to meet the needs of specific fields, notably chemistry and radiometry. These are mostly additions to the list of named derived units, and include the "mole" (symbol mol) for an amount of substance, the "pascal" (symbol Pa) for pressure, the "siemens" (symbol S) for electrical conductance, the "becquerel" (symbol Bq) for "activity referred to a radionuclide", the "gray" (symbol Gy) for ionising radiation, the "sievert" (symbol Sv) as the unit of dose equivalent radiation, and the "katal" (symbol kat) for catalytic activity.

The range of defined prefixes pico- (10) to tera- (10) was extended to 10 to 10.

The 1960 definition of the standard metre in terms of wavelengths of a specific emission of the krypton 86 atom was replaced with the distance that light travels in vacuum in exactly second, so that the speed of light is now an exactly specified constant of nature.

A few changes to notation conventions have also been made to alleviate lexicographic ambiguities. An analysis under the aegis of CSIRO, published in 2009 by the Royal Society, has pointed out the opportunities to finish the realisation of that goal, to the point of universal zero-ambiguity machine readability.

After the metre was redefined in 1960, the International Prototype of the Kilogram (IPK) was the only physical artefact upon which base units (directly the kilogram and indirectly the ampere, mole and candela) depended for their definition, making these units subject to periodic comparisons of national standard kilograms with the IPK. During the 2nd and 3rd Periodic Verification of National Prototypes of the Kilogram, a significant divergence had occurred between the mass of the IPK and all of its official copies stored around the world: the copies had all noticeably increased in mass with respect to the IPK. During "extraordinary verifications" carried out in 2014 preparatory to redefinition of metric standards, continuing divergence was not confirmed. Nonetheless, the residual and irreducible instability of a physical IPK undermined the reliability of the entire metric system to precision measurement from small (atomic) to large (astrophysical) scales.

A proposal was made that:

The new definitions were adopted at the 26th CGPM on 16 November 2018, and came into effect on 20 May 2019. The change was adopted by the European Union through Directive (EU) 2019/1258.

The units and unit magnitudes of the metric system which became the SI were improvised piecemeal from everyday physical quantities starting in the mid-18th century. Only later were they moulded into an orthogonal coherent decimal system of measurement.

The degree centigrade as a unit of temperature resulted from the scale devised by Swedish astronomer Anders Celsius in 1742. His scale counter-intuitively designated 100 as the freezing point of water and 0 as the boiling point. Independently, in 1743, the French physicist Jean-Pierre Christin described a scale with 0 as the freezing point of water and 100 the boiling point. The scale became known as the centi-grade, or 100 gradations of temperature, scale.

The metric system was developed from 1791 onwards by a committee of the French Academy of Sciences, commissioned to create a unified and rational system of measures. The group, which included preeminent French men of science, used the same principles for relating length, volume, and mass that had been proposed by the English clergyman John Wilkins in 1668 and the concept of using the Earth's meridian as the basis of the definition of length, originally proposed in 1670 by the French abbot Mouton.

In March 1791, the Assembly adopted the committee's proposed principles for the new decimal system of measure including the metre defined to be 1/10,000,000 of the length of the quadrant of Earth's meridian passing through Paris, and authorised a survey to precisely establish the length of the meridian. In July 1792, the committee proposed the names "metre", "are", "litre" and "grave" for the units of length, area, capacity, and mass, respectively. The committee also proposed that multiples and submultiples of these units were to be denoted by decimal-based prefixes such as "centi" for a hundredth and "kilo" for a thousand.
Later, during the process of adoption of the metric system, the Latin "gramme" and "kilogramme", replaced the former provincial terms "gravet" (1/1000 "grave") and "grave". In June 1799, based on the results of the meridian survey, the standard "mètre des Archives" and "kilogramme des Archives" were deposited in the French National Archives. Subsequently, that year, the metric system was adopted by law in France. 

During the first half of the 19th century there was little consistency in the choice of preferred multiples of the base units: typically the myriametre ( metres) was in widespread use in both France and parts of Germany, while the kilogram ( grams) rather than the myriagram was used for mass.

In 1832, the German mathematician Carl Friedrich Gauss, assisted by Wilhelm Weber, implicitly defined the second as a base unit when he quoted the Earth's magnetic field in terms of millimetres, grams, and seconds. Prior to this, the strength of the Earth's magnetic field had only been described in relative terms. The technique used by Gauss was to equate the torque induced on a suspended magnet of known mass by the Earth's magnetic field with the torque induced on an equivalent system under gravity. The resultant calculations enabled him to assign dimensions based on mass, length and time to the magnetic field.

A candlepower as a unit of illuminance was originally defined by an 1860 English law as the light produced by a pure spermaceti candle weighing pound (76 grams) and burning at a specified rate. Spermaceti, a waxy substance found in the heads of sperm whales, was once used to make high-quality candles. At this time the French standard of light was based upon the illumination from a Carcel oil lamp. The unit was defined as that illumination emanating from a lamp burning pure rapeseed oil at a defined rate. It was accepted that ten standard candles were about equal to one Carcel lamp.

A French-inspired initiative for international cooperation in metrology led to the signing in 1875 of the Metre Convention, also called Treaty of the Metre, by 17 nations. Initially the convention only covered standards for the metre and the kilogram. In 1921, the Metre Convention was extended to include all physical units, including the ampere and others thereby enabling the CGPM to address inconsistencies in the way that the metric system had been used.

A set of 30 prototypes of the metre and 40 prototypes of the kilogram, in each case made of a 90% platinum-10% iridium alloy, were manufactured by "British metallurgy specialty firm" and accepted by the CGPM in 1889. One of each was selected at random to become the International prototype metre and International prototype kilogram that replaced the "mètre des Archives" and "kilogramme des Archives" respectively. Each member state was entitled to one of each of the remaining prototypes to serve as the national prototype for that country.

The treaty also established a number of international organisations to oversee the keeping of international standards of measurement:
In the 1860s, James Clerk Maxwell, William Thomson (later Lord Kelvin) and others working under the auspices of the British Association for the Advancement of Science, built on Gauss's work and formalised the concept of a coherent system of units with base units and derived units christened the centimetre–gram–second system of units in 1874. The principle of coherence was successfully used to define a number of units of measure based on the CGS, including the erg for energy, the dyne for force, the barye for pressure, the poise for dynamic viscosity and the stokes for kinematic viscosity.

In 1879, the CIPM published recommendations for writing the symbols for length, area, volume and mass, but it was outside its domain to publish recommendations for other quantities. Beginning in about 1900, physicists who had been using the symbol "μ" (mu) for "micrometre" or "micron", "λ" (lambda) for "microlitre", and "γ" (gamma) for "microgram" started to use the symbols "μm", "μL" and "μg".

At the close of the 19th century three different systems of units of measure existed for electrical measurements: a CGS-based system for electrostatic units, also known as the Gaussian or ESU system, a CGS-based system for electromechanical units (EMU) and an International system based on units defined by the Metre Convention. for electrical distribution systems. 
Attempts to resolve the electrical units in terms of length, mass, and time using dimensional analysis was beset with difficulties—the dimensions depended on whether one used the ESU or EMU systems. This anomaly was resolved in 1901 when Giovanni Giorgi published a paper in which he advocated using a fourth base unit alongside the existing three base units. The fourth unit could be chosen to be electric current, voltage, or electrical resistance. Electric current with named unit 'ampere' was chosen as the base unit, and the other electrical quantities derived from it according to the laws of physics. This became the foundation of the MKS system of units.

In the late 19th and early 20th centuries, a number of non-coherent units of measure based on the gram/kilogram, centimetre/metre, and second, such as the "Pferdestärke" (metric horsepower) for power, the darcy for permeability and "millimetres of mercury" for barometric and blood pressure were developed or propagated, some of which incorporated standard gravity in their definitions.

At the end of the Second World War, a number of different systems of measurement were in use throughout the world. Some of these systems were metric system variations; others were based on customary systems of measure, like the U.S customary system and Imperial system of the UK and British Empire.

In 1948, the 9th CGPM commissioned a study to assess the measurement needs of the scientific, technical, and educational communities and "to make recommendations for a single practical system of units of measurement, suitable for adoption by all countries adhering to the Metre Convention". This working document was "Practical system of units of measurement". Based on this study, the 10th CGPM in 1954 defined an international system derived from six base units including units of temperature and optical radiation in addition to those for the MKS system mass, length, and time units and Giorgi's current unit. Six base units were recommended: the metre, kilogram, second, ampere, degree Kelvin, and candela.

The 9th CGPM also approved the first formal recommendation for the writing of symbols in the metric system when the basis of the rules as they are now known was laid down. These rules were subsequently extended and now cover unit symbols and names, prefix symbols and names, how quantity symbols should be written and used, and how the values of quantities should be expressed.

In 1960, the 11th CGPM synthesised the results of the 12-year study into a set of 16 resolutions. The system was named the "International System of Units", abbreviated SI from the French name, .

When Maxwell first introduced the concept of a coherent system, he identified three quantities that could be used as base units: mass, length, and time. Giorgi later identified the need for an electrical base unit, for which the unit of electric current was chosen for SI. Another three base units (for temperature, amount of substance, and luminous intensity) were added later.

The early metric systems defined a unit of weight as a base unit, while the SI defines an analogous unit of mass. In everyday use, these are mostly interchangeable, but in scientific contexts the difference matters. Mass, strictly the inertial mass, represents a quantity of matter. It relates the acceleration of a body to the applied force via Newton's law, : force equals mass times acceleration. A force of 1 N (newton) applied to a mass of 1 kg will accelerate it at 1 m/s. This is true whether the object is floating in space or in a gravity field e.g. at the Earth's surface. Weight is the force exerted on a body by a gravitational field, and hence its weight depends on the strength of the gravitational field. Weight of a 1 kg mass at the Earth's surface is ; mass times the acceleration due to gravity, which is 9.81 newtons at the Earth's surface and is about 3.5 newtons at the surface of Mars. Since the acceleration due to gravity is local and varies by location and altitude on the Earth, weight is unsuitable for precision measurements of a property of a body, and this makes a unit of weight unsuitable as a base unit.

Although the term "metric system" is often used as an informal alternative name for the International System of Units, other metric systems exist, some of which were in widespread use in the past or are even still used in particular areas. There are also individual metric units such as the sverdrup that exist outside of any system of units. Most of the units of the other metric systems are not recognised by the SI. Here are some examples. The centimetre–gram–second (CGS) system was the dominant metric system in the physical sciences and electrical engineering from the 1860s until at least the 1960s, and is still in use in some fields. It includes such SI-unrecognised units as the gal, dyne, erg, barye, etc. in its mechanical sector, as well as the poise and stokes in fluid dynamics. When it comes to the units for quantities in electricity and magnetism, there are several versions of the CGS system. Two of these are obsolete: the CGS electrostatic ('CGS-ESU', with the SI-unrecognised units of statcoulomb, statvolt, statampere, etc.) and the CGS electromagnetic system ('CGS-EMU', with abampere, abcoulomb, oersted, maxwell, abhenry, gilbert, etc.). A 'blend' of these two systems is still popular and is known as the Gaussian system (which includes the gauss as a special name for the CGS-EMU unit maxwell per square centimetre). In engineering (other than electrical engineering), there was formerly a long tradition of using the gravitational metric system, whose SI-unrecognised units include the kilogram-force (kilopond), technical atmosphere, metric horsepower, etc. The metre–tonne–second (mts) system, used in the Soviet Union from 1933 to 1955, had such SI-unrecognised units as the sthène, pièze, etc. Other groups of SI-unrecognised metric units are the various legacy and CGS units related to ionising radiation (rutherford, curie, roentgen, rad, rem, etc.), radiometry (langley, jansky), photometry (phot, nox, stilb, nit, metre-candle, lambert, apostilb, skot, brill, troland, talbot, candlepower, candle), thermodynamics (calorie), and spectroscopy (reciprocal centimetre). The angstrom is still used in various fields. Some other SI-unrecognised metric units that don't fit into any of the already mentioned categories include the are, bar, barn, fermi,<ref name="IEEE/ASTM SI 10-2016></ref> gradian (gon, grad, or grade), metric carat, micron, millimetre of mercury, torr, millimetre (or centimetre, or metre) of water, millimicron, mho, stere, x unit, (unit of mass), (unit of magnetic flux density), and (unit of volume). In some cases, the SI-unrecognised metric units have equivalent SI units formed by combining a metric prefix with a coherent SI unit. For example, , , , etc. (a related group are the correspondences such as ≘ , ≘ , etc.). Sometimes it is not even a matter of a metric prefix: the SI-nonrecognised unit may be exactly the same as an SI coherent unit, except for the fact that the SI does not recognise the special name and symbol. For example, the nit is just an SI-unrecognised name for the SI unit candela per square metre and the talbot is an SI-unrecognised name for the SI unit lumen second. Frequently, a non-SI metric unit is related to an SI unit through a power of ten factor, but not one that has a metric prefix, e.g. = , = , etc. (and correspondences like ≘ ). Finally, there are metric units whose conversion factors to SI units are not powers of ten, e.g. and . Some SI-unrecognised metric units are still frequently used, e.g. the calorie (in nutrition), the rem (in the U.S.), the jansky (in radio astronomy), the reciprocal centimetre (in spectroscopy), the gauss (in industry) and the CGS-Gaussian units more generally (in some subfields of physics), the metric horsepower (for engine power, in Europe), the kilogram-force (for rocket engine thrust, in China and sometimes in Europe), etc. Others are now rarely used, such as the sthène and the rutherford.






</doc>
<doc id="26766" url="https://en.wikipedia.org/wiki?curid=26766" title="Sapiens">
Sapiens

Sapiens, a Latin word meaning wise, may refer to:




</doc>
<doc id="26768" url="https://en.wikipedia.org/wiki?curid=26768" title="Sirenia">
Sirenia

The Sirenia, commonly referred to as sea-cows or sirenians, are an order of fully aquatic, herbivorous mammals that inhabit swamps, rivers, estuaries, marine wetlands, and coastal marine waters. The Sirenia currently comprise the families Dugongidae (the dugong and, historically, Steller's sea cow) and Trichechidae (manatees) with a total of four species. The Protosirenidae (Eocene sirenians) and Prorastomidae (terrestrial sirenians) families are extinct. Sirenians are classified in the clade Paenungulata, alongside the elephants and the hyraxes, and evolved in the Eocene 50 million years ago (mya). The Dugongidae diverged from the Trichechidae in the late Eocene or early Oligocene (30-35 mya).

Sirenians grow to between in length and in weight. The now extinct Steller's sea cow was the largest sirenian to have lived, and could reach lengths of and weights of . Sirenians have a large, fusiform body to reduce drag through the water. They have heavy bones that act as ballast to counteract the buoyancy of their blubber. They have a thin layer of blubber and consequently are sensitive to temperature fluctuations, which cause migrations when water temperatures dip too low. Sirenians are slow-moving, typically coasting at , but they can reach in short bursts. They use their strong lips to pull out seagrasses, consuming 10–15% of their body weight per day.

While breathing, they hold just their nostrils above the surface, sometimes standing on their tails to do so. Sirenians typically inhabit warm, shallow, coastal waters, or rivers. They are mainly herbivorous, but have been known to consume animals such as birds and jellyfish. Males typically mate with more than one female (polygyny), and may participate in lek mating. Sirenians are K-selected, and display parental care.

The meat, oil, bones, and skins are valuable items sold in markets. Mortality is often caused by direct hunting by humans or other human-induced causes, such as habitat destruction, entanglement in fishing gear, and watercraft collisions. Steller's sea cow went extinct due to overhunting in 1768.

Sirenia, commonly sirenians, are also referred to by the common name sirens, deriving from the sirens of Greek mythology. This comes from a legend about their discovery, involving lonely sailors mistaking them for mermaids. "Seekoei" (sea cow) is also the name for a hippopotamus in Afrikaans.

Sirenians are classified within the cohort Afrotheria in the clade Paenungulata, alongside Proboscidea (elephants), Hyracoidea (hyraxes), Embrithopoda, Desmostylia, and Afroinsectiphilia. This clade was first established by George Gaylord Simpson in 1945 based on anatomical evidence, such as testicondy and similar fetal development. The Paenungulata, along with the Afrotheria, are one of the most well-supported mammalian clades in molecular phylogeny. Sirenia, Proboscidae, and Desmotylia are grouped together in the clade Tethytheria. Based on morphological similarities, Tethytheria, Perissodactyla, and Hyracoidea were considered to be grouped together as the Altungulata, but this has been invalidated by molecular data.

The evolution of sirenians is characterized by the appearance of several traits, which are found in all sirenians (monophyly). The nostrils are large and retracted, the upper-jaw bone contacts the frontal bone, the sagittal crest is missing, the mastoid fills the supratemporal fenestra (an opening on the top of the skull), a drop-like ectotympanic (a bony ring that holds the ear drum), and pachyosteosclerotic (dense and bulky) bones.

Sirenians first appeared in the fossil record in the Early Eocene and significantly diversified throughout the epoch. They inhabited rivers, estuaries, and nearshore marine waters. Sirenians, unlike other marine mammals such as cetaceans, lived in the New World. One of the earliest aquatic sirenians discovered is "Prorastomus" which dates back to 40 million years ago, and the first known sirenian, the quadruped "Pezosiren", lived 50 million years ago. An ancient sirenian fossil of a petrosal bone was found in Tunisia, dating back to approximately the same time as "Prorastomus". This is the oldest sirenian fossil to be found in Africa and supports molecular data suggesting that sirenians may have originated in Africa. Prorastomidae and Protosirenidae, the earliest sirenian families, consisted of pig-like amphibious creatures who died out at the end of the Eocene. When the Dugongidae appeared at this time, sirenians had evolved the characteristics of modern variety, including an aquatic streamlined body with flipper-like front legs with no hind limbs, and a powerful tail with horizontal caudal fins which uses an up-and-down motion to move them through the water.

The last of the sirenian families to appear, Trichechidae, apparently arose from early dugongids in the late Eocene or early Oligocene. It is a monophyletic taxon. In 1994, the family was expanded to include not only the subfamily Trichechinae ("Potamosiren", "Ribodon", and "Trichechus"), but also Miosireninae ("Anomotherium" and "Miosiren"). The African manatee and the West Indian manatee are more closely related to each other than to the Amazonian manatee.

Dugongidae comprises the subfamilies Dugonginae and Hydrodamalinae (which are both monophyletic) and the paraphyletic Halitheriinae. The tusks of modern-day dugongs may have originally been used for digging, but they are now used for social interaction. The genus "Dugong" probably originated in the Indo-Pacific area.

The tail fluke of a dugong is notched and similar to those of dolphins, whereas the tail fluke of manatees is paddle-shaped. The fluke is raised up and down in long strokes to move the animal forward, or twisted to turn. The forelimbs are paddle-like flippers which aid in turning and slowing. Unlike manatees, the dugong lacks nails on its flippers, which are only 15% of a dugong's body length. Manatees generally glide at speeds of , but can reach speeds of in short bursts. The body is fusiform to reduce drag in the water. Like cetaceans, the hind limbs are internal and vestigial. The snout is angled downwards to aid in bottom-feeding. Sirenians typically make two- to three-minute dives, but manatees can hold their breath for up to 15 minutes while resting and dugongs up to six minutes. They may stand on their tails to hold their heads above water.
Much like elephants, manatees are polyphyodonts, and continuously replace their teeth from the back of the jaw. Adults lack incisors, canines, and premolars, and instead have 8 to 10 cheek teeth in their mouth. Manatees have an infinite supply of teeth moving in from the back and shedding in the front, which are continuously formed by a dental capsule behind the tooth-row. These teeth are constantly worn down by the abrasive vascular plants they forage, particularly aquatic grasses. Unlike in manatees, the dugong's teeth do not continually grow back via horizontal tooth replacement. The dugong has two tusks which emerge in males during puberty, and sometime later in life for females after reaching the base of the premaxilla. The number of growth layer groups in a tusk indicates the age of a dugong.

Sirenians exhibit pachyostosis, a condition in which the ribs and other long bones are solid and contain little or no bone marrow. They have among the densest bones in the animal kingdom, which may be used as ballast, counteracting the buoyancy effect of their blubber and help keep sirenians suspended slightly below the water's surface. Manatees do not possess blubber, per se, but rather have thick skin, and, consequently, are sensitive to temperature changes. Likewise, they often migrate to warmer waters whenever the water temperature dips below . The lungs of sirenians are unlobed; they, along with the diaphragm, extend the entire length of the vertebral column, which help them control their buoyancy and reduce tipping in the water.

Extant sirenians grow to between in length and can weigh up to . Steller's sea cow was the largest sirenian to have lived, and could reach lengths of , and could weigh in at . A dugong's brain weighs a maximum of , about 0.1% of the animal's body weight. The body of sirenians is sparsely covered in short hair (vibrissae), except for on the muzzle, which may allow for tactile interpretation of their environment. Manatees are the only creatures to exhibit corneal avascularity, and lack blood vessels in the cornea, which prevents optical clarity and vision. This may be the result of irritations from or protection against their hypotonic freshwater environment.

Sirenians are referred to as "sea cows" because their diet consists mainly of seagrass. Dugongs sift through the seafloor in search of seagrasses. Dugongs use their sense of smell to find the seagrass because their eyesight is poor making it difficult to use their sense of sight to find food. They ingest the whole plant, including the roots, although they will feed on just the leaves if this is not possible. Manatees, in particular the West Indian manatee, are known to consume over 60 different freshwater and saltwater plants, such as shoalweed, water lettuce, muskgrass, manatee grass, and turtle grass. Using their divided upper lip, an adult manatee will commonly eat up to 10–15% of their body weight, or , per day, which requires the manatee to graze for several hours per day. However, 10% of the diet of the African manatee is fish and mollusks. Manatees have been known to eat small amounts of fish from nets. As opposed to bulk feeding, dugongs target high-nitrogen grasses to maximize nutrient intake, and, although almost completely herbivorous, dugongs will occasionally eat invertebrates such as jellyfish, sea squirts, and shellfish. Some populations of dugongs, such as the one in Moreton Bay, Australia, are omnivorous, feeding on invertebrates such as polychaetes or marine algae when their supply of seagrasses decrease. In other dugong populations in western and eastern Australia, there is evidence that dugongs actively seek out large invertebrates. Populations of Amazonian manatees become restricted to lakes during the July–August dry season when water levels begin to fall, and are thought to fast during this period. Their large fat reserves and low metabolic rates—only 36% of the usual placental mammal metabolic rate—allow them to survive for up to seven months with little or no food.

Despite being mostly solitary, sirenians congregate in groups while females are in estrus. These groups usually include one female with multiple males. Sirenians are K-selectors, so, despite the longevity, females give birth only a few times during their lives and invest considerable parental care in their young. Dugongs generally gather in groups of less than a dozen individuals for one to two days. Since they congregate in turbid waters, little is known about their reproductive behavior. The males are often seen with scars, and the tusks on dugongs grow in first for males, suggesting they are important in lekking. They have also been known to lunge at each other. The age when a female first gives birth is disputed, ranging anywhere from six to seventeen years. The time between births is unclear, with estimates ranging from 2 to 7 years. Though, in Sarasota, Florida 113 manatee were observed of known sex. Of these 113, there were 53 females that produced at least 55 calves during a five-year period of observation.

Manatees can reach sexual maturity as early as two to five years of age. Manatee gestation length is around one year, and then they lactate for one to two years. West Indian manatees and African manatees can breed year-round, and a female will mate with multiple male partners. Amazonian manatees have a breeding season, usually mating when the river levels begin to rise, which varies from place to place.

Manatees may be taken into captivity after being found stranded to facilitate their recovery, and there are many instances of manatees being successfully rehabilitated and released into the wild. As all extant sirenian species are considered to be 'vulnerable', these rehabilitation programs present a useful means to support these species. However, their vulnerability also means that the taking of manatees from the wild for commercial purposes is a conservation issue.

Diet in Captivity

Manatees, in comparison to other marine species, tend to do well in a captive environment, with incidences of manatees thriving in captivity. However, it can be difficult to replicate the conditions of their natural environment to the extent necessary to maintain a manatee at their healthiest, the typical diet fed to captive manatee populations may contain insufficient quantities of the nutrients they need. 

Manatee captive fed diets vary greatly from the manatee’s diet in the wild. In captivity manatees are fed 70-80% of leafy green vegetables, 10-20% of dried forage, and 5% vegetables and fruits. Dried forage are foods such as hay and timothy grass which are often the diets of different equine and cattle. The vegetables and fruits that are fed to manatees consist of romaine lettuce, carrots, apples, etc... Manatees in captivity have done well off of this varying diet, but the exact nutritional needs of a manatee’s diet are unknown.

When compared to the natural diet of manatees in the wild, there is a significant difference. In their natural habitat approximately ½ of the manatee’s diet is marine or estuarine plants; in comparison to romaine lettuce, their nutritional value is quite different. When compared to the captive diet, aquatic plants have an increase in dry matter, soluble neutral detergent fiber, and a decrease in digestible nutrients. Although it may seem to be a better diet for manatees to have more easily digestible nutrients, their gastrointestinal tract is adapted to the wild diet through microbial processes of fermentation. Although, we are still unsure if the nutrients given in the captive diet is sufficient since there is positive digestive feedback with the captive diet.

The three extant manatee species (family Trichechidae) and the dugong (family Dugongidae) are rated as vulnerable on the IUCN Red List of endangered species. All four are vulnerable to extinction from habitat loss and other negative impacts related to human population growth and coastal development. Steller's sea cow, extinct since 1786, was hunted to extinction by humans.

The meat, oil, bones, and skin of manatees are valuable items. In some countries, such as Nigeria and Cameroon, African manatees are sold to zoos, aquariums, and online as pets, sometimes being shipped internationally. Though illegal, lack of law enforcement in these areas induce poaching. Some residents of West African countries, such as Mali and Chad, believe that the oil of the African manatee can cure ailments such as ear infections, rheumatism, and skin conditions. Hunting is the largest source of mortality in Amazonian manatees, and there are no management plans except for in Colombia. Amazonian manatees, especially calves, are sometimes illegally sold as pets, but there are several institutions that care for and rescue these orphans, with the possibility of their releasing into the wild. The body parts of dugongs are used as medicinal remedies across the Indian Ocean.

Manatees have also faced threats in Cuba, an area that is not always known for its manatee population. Manatees in Cuba have faced poaching, entanglement and pollution. The area has some of the most extensive and best manatee habitat in the Caribbean, but the population has been unable to thrive there for numerous reasons. Existing information about manatees in Cuba is limited and scarce, this makes it difficult to spread awareness which therefore enhances the situation of illegal poaching and entanglement within fishing nets in a majority of the coastal communities. Poaching of the manatees has been a significant issue since the 1970s when it was initially reported that the hunting was taking its toll on the manatee population in Cuba. In 1975 it was recorded that the manatees' status in Cuba was rare and declining at an alarming rate due to pollution and hunting. In 1996 manatees were placed under protection through the Fishery Decree law 164. This law provided penalties against those that manipulated, harm, or injured manatees. However, it was seen that the hunting of manatees in Cuba in the 1990s may have been the result of economic hardships in this country and the manatees were an alternative source of protein. Although there have been efforts made to protect the population of manatees in Cuba, it has not proven to be helpful or as impactful as those working to protect the population had hoped. Many of these areas are seen as "paper parks" or parks or protected areas that only exist due to their name and nothing else, and they do not have a significant impact on conversation and protections.
Environmental hazards induced by humans also puts sirenians at risk. Sirenians, especially the West Indian manatee, face high mortality from watercraft collision, and about half of all West Indian manatee deaths are caused by watercraft collisions. An increased usage of hydroelectric power and subsequent damming of rivers increase waterway traffic, which can lead to vessel collisions, and manatees may become entangled in navigational locks. The urbanized coastline of areas such as the Caribbean and Australia can result in the decline of seagrass populations. Reliable areas of warm water in Florida are generally the result of discharge from power plants, but newer plants with more efficient cooling systems may disrupt the pattern of warm water refuges, and an increased demand for artesian springs for water, the natural source of warm water, decreases the number of warm water refuges. Sirenians can be caught as bycatch from fisheries, and they can be seen as pests with the interference of local fishermen and the destruction of their nets. African manatees have also been known to venture into rice paddies and destroy the crops during the rainy season, and these confrontations with locals may lead to intentional culling of the manatees.
Red tide, scientifically known as Karenia brevis is a harmful algae bloom that releases toxins into the water killing many marine species. In 1982, numerous sick manatees were accounted for and researchers believe this was due to the accumulation brevetoxins in filter-feeding organisms attached to seagrass blades, which are a popular diet for manatees. Manatee die-offs from exposure to red tide toxins were recorded by the Florida Fish and Wildlife Conservation Commission in southwest Florida in 2002, 2003, 2005, 2007, and most recently in 2013. As of June 20, 2018 the current red tide bloom spreads from Pasco county to Collier County off the West coast of Florida. As of January 2018 there have been a total of 472 manatee deaths caused by this red tide along with water crafts, cold stress, and other factors.

Weather disasters and other natural occurrences are also sources of mortality. The West Indian manatee and Dugong face risks from hurricanes and cyclones, which are predicted to increase in the future. These storms may also damage seagrass populations. Exposure to brevetoxin from "Karenia brevis" during a red tide event are also sources of mortality; they may be able to be exposed to brevetoxin after a red tide has subsided, as it could settle on seagrasses. African manatees can become stranded during the dry season when rivers and lakes become too small or dry up completely.

All sirenians are protected by the Marine Mammal Protection Act of 1972 (MMPA), the Endangered Species Act of 1973 (ESA), and the Convention on the International Trade in Endangered Species of Wild Fauna and Flora (CITES). In addition to this, the four species are further protected by various specialty organizations. The Dugong is listed in the Convention on Biological Diversity (CBD), the Convention on Migratory Species, and the Coral Triangle Initiative. In Florida, manatees are protected by the Florida Manatee Sanctuary Act of 1978, which implements actions such as the limitation or prohibition of watercraft speeds where manatees exist. Marine mammal rehabilitation programs have been underway and regulated in the United States for more than 40 years. In 1973 injured and distressed manatees were rescued or aided within Florida. Eventually, the program was formalized into the Manatee Rescue, Rehabilitation, and Release Program managed by the USFWS. In 2012 the program became the Manatee Rescue/Rehabilitation Partnership (MRP) with permitting and oversight by the USFWS. From 1973 through 2014, this program rescued 1,619 manatees and 526 Florida manatees have been released.





</doc>
<doc id="26769" url="https://en.wikipedia.org/wiki?curid=26769" title="South America">
South America

South America is a continent in the Western Hemisphere, mostly in the Southern Hemisphere, with a relatively small portion in the Northern Hemisphere. It may also be considered a subcontinent of the Americas, or America, which is how it is viewed in Spanish and Portuguese-speaking regions of the Americas. The reference to South America instead of other regions (like Latin America or the Southern Cone) has increased in the last decades due to changing geopolitical dynamics (in particular, the rise of Brazil).

It is bordered on the west by the Pacific Ocean and on the north and east by the Atlantic Ocean; North America and the Caribbean Sea lie to the northwest. It includes twelve sovereign states: Argentina, Bolivia, Brazil, Chile, Colombia, Ecuador, Guyana, Paraguay, Peru, Suriname, Uruguay, Venezuela, and a part of France : French Guiana. In addition, the ABC islands of the Kingdom of the Netherlands, the Falkland Islands (a British Overseas Territory), Trinidad and Tobago, and Panama may also be considered part of South America.

South America has an area of 17,840,000 square kilometers (6,890,000 sq mi). Its population has been estimated at more than floor(/1e6) million. South America ranks fourth in area (after Asia, Africa, and North America) and fifth in population (after Asia, Africa, Europe, and North America). Brazil is by far the most populous South American country, with more than half of the continent's population, followed by Colombia, Argentina, Venezuela and Peru. In recent decades Brazil has also concentrated half of the region's GDP and has become a first regional power.

Most of the population lives near the continent's western or eastern coasts while the interior and the far south are sparsely populated. The geography of western South America is dominated by the Andes mountains; in contrast, the eastern part contains both highland regions and vast lowlands where rivers such as the Amazon, Orinoco, and Paraná flow. Most of the continent lies in the tropics.

The continent's cultural and ethnic outlook has its origin with the interaction of indigenous peoples with European conquerors and immigrants and, more locally, with African slaves. Given a long history of colonialism, the overwhelming majority of South Americans speak Portuguese or Spanish, and societies and states reflect Western traditions.

South America occupies the southern portion of the Americas. The continent is generally delimited on the northwest by the Darién watershed along the Colombia–Panama border, although some may consider the border instead to be the Panama Canal. Geopolitically and geographically all of Panama – including the segment east of the Panama Canal in the isthmus – is typically included in North America alone and among the countries of Central America. Almost all of mainland South America sits on the South American Plate.

South America is home to the world's highest uninterrupted waterfall, Angel Falls in Venezuela; the highest single drop waterfall Kaieteur Falls in Guyana; the largest river by volume, the Amazon River; the longest mountain range, the Andes (whose highest mountain is Aconcagua at ); the driest non-polar place on earth, the Atacama Desert; the wettest place on earth, López de Micay in Colombia; the largest rainforest, the Amazon Rainforest; the highest capital city, La Paz, Bolivia; the highest commercially navigable lake in the world, Lake Titicaca; and, excluding research stations in Antarctica, the world's southernmost permanently inhabited community, Puerto Toro, Chile.

South America's major mineral resources are gold, silver, copper, iron ore, tin, and petroleum. These resources found in South America have brought high income to its countries especially in times of war or of rapid economic growth by industrialized countries elsewhere. However, the concentration in producing one major export commodity often has hindered the development of diversified economies. The fluctuation in the price of commodities in the international markets has led historically to major highs and lows in the economies of South American states, often causing extreme political instability. This is leading to efforts to diversify production to drive away from staying as economies dedicated to one major export.

South America is one of the most biodiverse continents on earth. South America is home to many interesting and unique species of animals including the llama, anaconda, piranha, jaguar, vicuña, and tapir. The Amazon rainforests possess high biodiversity, containing a major proportion of the Earth's species.

Brazil is the largest country in South America, encompassing around half of the continent's land area and population. The remaining countries and territories are divided among three regions: The Andean States, the Guianas and the Southern Cone.

Traditionally, South America also includes some of the nearby islands. Aruba, Bonaire, Curaçao, Trinidad, Tobago, and the federal dependencies of Venezuela sit on the northerly South American continental shelf and are often considered part of the continent. Geo-politically, the island states and overseas territories of the Caribbean are generally grouped as a part or subregion of North America, since they are more distant on the Caribbean Plate, even though San Andres and Providencia are politically part of Colombia and Aves Island is controlled by Venezuela.

Other islands that are included with South America are the Galápagos Islands that belong to Ecuador and Easter Island (in Oceania but belonging to Chile), Robinson Crusoe Island, Chiloé (both Chilean) and Tierra del Fuego (split in between Chile and Argentina). In the Atlantic, Brazil owns Fernando de Noronha, Trindade and Martim Vaz, and the Saint Peter and Saint Paul Archipelago, while the Falkland Islands are governed by the United Kingdom, whose sovereignty over the islands is disputed by Argentina. South Georgia and the South Sandwich Islands may be associated with either South America or Antarctica.
The distribution of the average temperatures in the region presents a constant regularity from the 30° of latitude south, when the isotherms tend, more and more, to be confused with the degrees of latitude.

In temperate latitudes, winters are milder and summers cooler than in North America. Because its most extensive part of the continent is in the equatorial zone, the region has more areas of equatorial plains than any other region.

The average annual temperatures in the Amazon basin oscillate around , with low thermal amplitudes and high rainfall indices. Between the Maracaibo Lake and the mouth of the Orinoco, predominates an equatorial climate of the type Congolese, that also includes parts of the Brazilian territory.

The east-central Brazilian plateau has a humid and warm tropical climate. The northern and eastern parts of the Argentine pampas have a humid subtropical climate with dry winters and humid summers of the Chinese type, while the western and eastern ranges have a subtropical climate of the dinaric type. At the highest points of the Andean region, climates are colder than the ones occurring at the highest point of the Norwegian fjords. In the Andean plateaus, the warm climate prevails, although it is tempered by the altitude, while in the coastal strip, there is an equatorial climate of the Guinean type. From this point until the north of the Chilean coast appear, successively, Mediterranean oceanic climate, temperate of the Breton type and, already in Tierra del Fuego, cold climate of the Siberian type.

The distribution of rainfall is related to the regime of winds and air masses. In most of the tropical region east of the Andes, winds blowing from the northeast, east and southeast carry moisture from the Atlantic, causing abundant rainfall. However, due to a consistently strong wind shear and a weak Intertropical Convergence Zone, South Atlantic tropical cyclones are rare. In the Orinoco Llanos and in the Guianas plateau, the precipitation levels go from moderate to high. The Pacific coast of Colombia and northern Ecuador are rainy regions, with Chocó in Colombia being the most rainy place in the world along with the northern slopes of Indian Himalayas. The Atacama Desert, along this stretch of coast, is one of the driest regions in the world. The central and southern parts of Chile are subject to extratropical cyclones, and most of the Argentine Patagonia is desert. In the pampas of Argentina, Uruguay and South of Brazil the rainfall is moderate, with rains well distributed during the year. The moderately dry conditions of the Chaco oppose the intense rainfall of the eastern region of Paraguay. In the semiarid coast of the Brazilian Northeast the rains are linked to a monsoon regime.

Important factors in the determination of climates are sea currents, such as the current Humboldt and Falklands. The equatorial current of the South Atlantic strikes the coast of the Northeast and there is divided into two others: the current of Brazil and a coastal current that flows to the northwest towards the Antilles, where there it moves towards northeast course thus forming the most Important and famous ocean current in the world, the Gulf Stream.

South America is believed to have been joined with Africa from the late Paleozoic Era to the early Mesozoic Era, until the supercontinent Pangaea began to rift and break apart about 225 million years ago. Therefore, South America and Africa share similar fossils and rock layers.

South America is thought to have been first inhabited by humans when people were crossing the Bering Land Bridge (now the Bering Strait) at least 15,000 years ago from the territory that is present-day Russia. They migrated south through North America, and eventually reached South America through the Isthmus of Panama.

The first evidence for the existence of the human race in South America dates back to about 9000 BC, when squashes, chili peppers and beans began to be cultivated for food in the highlands of the Amazon Basin. Pottery evidence further suggests that manioc, which remains a staple food today, was being cultivated as early as 2000 BC.

By 2000 BC, many agrarian communities had been settled throughout the Andes and the surrounding regions. Fishing became a widespread practice along the coast, helping establish fish as a primary source of food. Irrigation systems were also developed at this time, which aided in the rise of an agrarian society.

South American cultures began domesticating llamas, vicuñas, guanacos, and alpacas in the highlands of the Andes circa 3500 BC. Besides their use as sources of meat and wool, these animals were used for transportation of goods.

The rise of plant growing and the subsequent appearance of permanent human settlements allowed for the multiple and overlapping beginnings of civilizations in South America.

One of the earliest known South American civilizations was at Norte Chico, on the central Peruvian coast. Though a pre-ceramic culture, the monumental architecture of Norte Chico is contemporaneous with the pyramids of Ancient Egypt. Norte Chico governing class established a trade network and developed agriculture then followed by Chavín by 900 BC, according to some estimates and archaeological finds. Artifacts were found at a site called Chavín de Huantar in modern Peru at an elevation of . Chavín civilization spanned 900 BC to 300 BC.

In the central coast of Peru, around the beginning of the 1st millennium AD, Moche (100 BC – 700 AD, at the northern coast of Peru), Paracas and Nazca (400 BC – 800 AD, Peru) cultures flourished with centralized states with permanent militia improving agriculture through irrigation and new styles of ceramic art. At the Altiplano, Tiahuanaco or Tiwanaku (100 BC – 1200 AD, Bolivia) managed a large commercial network based on religion.

Around the 7th century, both Tiahuanaco and Wari or Huari Empire (600–1200, Central and northern Peru) expanded its influence to all the Andean region, imposing the Huari urbanism and Tiahuanaco religious iconography.

The Muisca were the main indigenous civilization in what is now Colombia. They established the Muisca Confederation of many clans, or "cacicazgos", that had a free trade network among themselves. They were goldsmiths and farmers.

Other important Pre-Columbian cultures include: the Cañaris (in south central Ecuador), Chimú Empire (1300–1470, Peruvian northern coast), Chachapoyas, and the Aymaran kingdoms (1000–1450, Western Bolivia and southern Peru).
Holding their capital at the great city of Cusco, the Inca civilization dominated the Andes region from 1438 to 1533. Known as "Tawantin suyu", and "the land of the four regions," in Quechua, the Inca Empire was highly distinct and developed. Inca rule extended to nearly a hundred linguistic or ethnic communities, some nine to fourteen million people connected by a 25,000 kilometer road system. Cities were built with precise, unmatched stonework, constructed over many levels of mountain terrain. Terrace farming was a useful form of agriculture.

The Mapuche in Central and Southern Chile resisted the European and Chilean settlers, waging the Arauco War for more than 300 years.

In 1494, Portugal and Spain, the two great maritime European powers of that time, on the expectation of new lands being discovered in the west, signed the Treaty of Tordesillas, by which they agreed, with the support of the Pope, that all the land outside Europe should be an exclusive duopoly between the two countries.
The treaty established an imaginary line along a north–south meridian 370 leagues west of the Cape Verde Islands, roughly 46° 37' W. In terms of the treaty, all land to the west of the line (known to comprise most of the South American soil) would belong to Spain, and all land to the east, to Portugal. As accurate measurements of longitude were impossible at that time, the line was not strictly enforced, resulting in a Portuguese expansion of Brazil across the meridian.

Beginning in the 1530s, the people and natural resources of South America were repeatedly exploited by foreign conquistadors, first from Spain and later from Portugal. These competing colonial nations claimed the land and resources as their own and divided it into colonies.

European infectious diseases (smallpox, influenza, measles, and typhus) – to which the native populations had no immune resistance – caused large-scale depopulation of the native population under Spanish control. Systems of forced labor, such as the haciendas and mining industry's mit'a also contributed to the depopulation. After this, African slaves, who had developed immunities to these diseases, were quickly brought in to replace them.

The Spaniards were committed to converting their native subjects to Christianity and were quick to purge any native cultural practices that hindered this end; however, many initial attempts at this were only partially successful, as native groups simply blended Catholicism with their established beliefs and practices. Furthermore, the Spaniards brought their language to the degree they did with their religion, although the Roman Catholic Church's evangelization in Quechua, Aymara, and Guaraní actually contributed to the continuous use of these native languages albeit only in the oral form.

Eventually, the natives and the Spaniards interbred, forming a mestizo class. At the beginning, many mestizos of the Andean region were offspring of Amerindian mothers and Spanish fathers. After independence, most mestizos had native fathers and European or mestizo mothers.

Many native artworks were considered pagan idols and destroyed by Spanish explorers; this included many gold and silver sculptures and other artifacts found in South America, which were melted down before their transport to Spain or Portugal. Spaniards and Portuguese brought the western European architectural style to the continent, and helped to improve infrastructures like bridges, roads, and the sewer system of the cities they discovered or conquered. They also significantly increased economic and trade relations, not just between the old and new world but between the different South American regions and peoples. Finally, with the expansion of the Portuguese and Spanish languages, many cultures that were previously separated became united through that of Latin American.

Guyana was first a Dutch, and then a British colony, though there was a brief period during the Napoleonic Wars when it was colonized by the French. The country was once partitioned into three parts, each being controlled by one of the colonial powers until the country was finally taken over fully by the British.

The indigenous peoples of the Americas in various European colonies were forced to work in European plantations and mines; along with African slaves who were also introduced in the proceeding centuries. The colonists were heavily dependent on indigenous labor during the initial phases of European settlement to maintain the subsistence economy, and natives were often captured by expeditions. The importation of African slaves began midway through the 16th century, but the enslavement of indigenous peoples continued well into the 17th and 18th centuries. The Atlantic slave trade brought African slaves primarily to South American colonies, beginning with the Portuguese since 1502. The main destinations of this phase were the Caribbean colonies and Brazil, as European nations built up economically slave-dependent colonies in the New World. Nearly 40% of all African slaves trafficked to the Americas went to Brazil. An estimated 4.9 million slaves from Africa came to Brazil during the period from 1501 to 1866.

While the Portuguese, English, French and Dutch settlers enslaved mainly African blacks, the Spaniards became very disposed of the natives. In 1750 Portugal abolished native slavery in the colonies because they considered them unfit for labour and began to import even more African slaves. Slaves were brought to the mainland on slave ships, under inhuman conditions and ill-treatment, and those who survived were sold into the slave markets.

After independence, all South American countries maintained slavery for some time. The first South American country to abolish slavery was Chile in 1823, Uruguay in 1830, Bolivia in 1831, Colombia and Ecuador in 1851, Argentina in 1853, Peru and Venezuela in 1854, Suriname in 1863, Paraguay in 1869, and in 1888 Brazil was the last South American nation and the last country in western world to abolish slavery.

The European Peninsular War (1807–1814), a theater of the Napoleonic Wars, changed the political situation of both the Spanish and Portuguese colonies. First, Napoleon invaded Portugal, but the House of Braganza avoided capture by escaping to Brazil. Napoleon also captured King Ferdinand VII of Spain, and appointed his own brother instead. This appointment provoked severe popular resistance, which created Juntas to rule in the name of the captured king.

Many cities in the Spanish colonies, however, considered themselves equally authorized to appoint local Juntas like those of Spain. This began the Spanish American wars of independence between the patriots, who promoted such autonomy, and the royalists, who supported Spanish authority over the Americas. The Juntas, in both Spain and the Americas, promoted the ideas of the Enlightenment. Five years after the beginning of the war, Ferdinand VII returned to the throne and began the Absolutist Restoration as the royalists got the upper hand in the conflict.

The independence of South America was secured by Simón Bolívar (Venezuela) and José de San Martín (Argentina), the two most important "Libertadores". Bolívar led a great uprising in the north, then led his army southward towards Lima, the capital of the Viceroyalty of Peru. Meanwhile, San Martín led an army across the Andes Mountains, along with Chilean expatriates, and liberated Chile. He organized a fleet to reach Peru by sea, and sought the military support of various rebels from the Viceroyalty of Peru. The two armies finally met in Guayaquil, Ecuador, where they cornered the Royal Army of the Spanish Crown and forced its surrender.

In the Portuguese Kingdom of Brazil, Dom Pedro I (also Pedro IV of Portugal), son of the Portuguese King Dom João VI, proclaimed the independent Kingdom of Brazil in 1822, which later became the Empire of Brazil. Despite the Portuguese loyalties of garrisons in Bahia, Cisplatina and Pará, independence was diplomatically accepted by the crown in Portugal in 1825, on condition of a high compensation paid by Brazil mediatized by the United Kingdom.

The newly independent nations began a process of fragmentation, with several civil and international wars. However, it was not as strong as in Central America. Some countries created from provinces of larger countries stayed as such up to modern times (such as Paraguay or Uruguay), while others were reconquered and reincorporated into their former countries (such as the Republic of Entre Ríos and the Riograndense Republic).

The first separatist attempt was in 1820 by the Argentine province of Entre Ríos, led by a caudillo. In spite of the "Republic" in its title, General Ramírez, its caudillo, never really intended to declare an independent Entre Rios. Rather, he was making a political statement in opposition to the monarchist and centralist ideas that back then permeated Buenos Aires politics. The "country" was reincorporated at the United Provinces in 1821.

In 1825 the Cisplatine Province declared its independence from the Empire of Brazil, which led to the Cisplatine War between the imperials and the Argentine from the United Provinces of the Río de la Plata to control the region. Three years later, the United Kingdom intervened in the question by proclaiming a tie and creating in the former Cisplatina a new independent country: The Oriental Republic of Uruguay.

Later in 1836, while Brazil was experiencing the chaos of the regency, Rio Grande do Sul proclaimed its independence motivated by a tax crisis. With the anticipation of the coronation of Pedro II to the throne of Brazil, the country could stabilize and fight the separatists, which the province of Santa Catarina had joined in 1839. The Conflict came to an end by a process of compromise by which both Riograndense Republic and Juliana Republic were reincorporated as provinces in 1845.

The Peru–Bolivian Confederation, a short-lived union of Peru and Bolivia, was blocked by Chile in the War of the Confederation (1836–1839) and again during the War of the Pacific (1879–1883). Paraguay was virtually destroyed by Argentina, Brazil and Uruguay in the Paraguayan War.

Despite the Spanish American wars of independence and the Brazilian War of Independence, the new nations quickly began to suffer with internal conflicts and wars among themselves.

In 1825 the proclamation of independence of Cisplatina led to the Cisplatine War between historical rivals the Empire of Brazil and the United Provinces of the Río de la Plata, Argentina's predecessor. The result was a stalemate, ending with the British arranging for the independence of Uruguay. Soon after, another Brazilian province proclaimed its independence leading to the Ragamuffin War which Brazil won.

Between 1836 and 1839 the War of the Confederation broke out between the short-lived Peru-Bolivian Confederation and Chile, with the support of the Argentine Confederation. The war was fought mostly in the actual territory of Peru and ended with a Confederate defeat and the dissolution of the Confederacy and annexation of many territories by Argentina.

Meanwhile, the Argentine Civil Wars plagued Argentina since its independence. The conflict was mainly between those who defended the centralization of power in Buenos Aires and those who defended a confederation. During this period it can be said that "there were two Argentines": the Argentine Confederation and the Argentine Republic. At the same time the political instability in Uruguay led to the Uruguayan Civil War among the main political factions of the country. All this instability in the platine region interfered with the goals of other countries such as Brazil, which was soon forced to take sides. In 1851 the Brazilian Empire, supporting the centralizing unitarians, and the Uruguayan government invaded Argentina and deposed the caudillo, Juan Manuel Rosas, who ruled the confederation with an iron hand. Although the Platine War did not put an end to the political chaos and civil war in Argentina, it brought temporary peace to Uruguay where the Colorados faction won, supported by the Brazilian Empire, British Empire, French Empire and the Unitarian Party of Argentina.

Peace lasted only a short time: in 1864 the Uruguayan factions faced each other again in the Uruguayan War. The Blancos supported by Paraguay started to attack Brazilian and Argentine farmers near the borders. The Empire made an initial attempt to settle the dispute between Blancos and Colorados without success. In 1864, after a Brazilian ultimatum was refused, the imperial government declared that Brazil's military would begin reprisals. Brazil declined to acknowledge a formal state of war, and, for most of its duration, the Uruguayan–Brazilian armed conflict was an undeclared war which led to the deposition of the "Blancos" and the rise of the pro-Brazilian "Colorados" to power again. This angered the Paraguayan government, which even before the end of the war invaded Brazil, beginning the biggest and deadliest war in both South American and Latin American histories: the Paraguayan War.

The Paraguayan War began when the Paraguayan dictator Francisco Solano López ordered the invasion of the Brazilian provinces of Mato Grosso and Rio Grande do Sul. His attempt to cross Argentinian territory without Argentinian approval led the pro-Brazilian Argentine government into the war. The pro-Brazilian Uruguayan government showed its support by sending troops. In 1865 the three countries signed the Treaty of the Triple Alliance against Paraguay. At the beginning of the war, the Paraguayans took the lead with several victories, until the Triple Alliance organized to repel the invaders and fight effectively. This was the second total war experience in the world after the American Civil War. It was deemed the greatest war effort in the history of all participating countries, taking almost 6 years and ending with the complete devastation of Paraguay. The country lost 40% of its territory to Brazil and Argentina and lost 60% of its population, including 90% of the men. The dictator Lopez was killed in battle and a new government was instituted in alliance with Brazil, which maintained occupation forces in the country until 1876.

The last South American war in the 19th century was the War of the Pacific with Bolivia and Peru on one side and Chile on the other. In 1879 the war began with Chilean troops occupying Bolivian ports, followed by Bolivia declaring war on Chile which activated an alliance treaty with Peru. The Bolivians were completely defeated in 1880 and Lima was occupied in 1881. The peace was signed with Peru in 1883 while a truce was signed with Bolivia in 1884. Chile annexed territories of both countries leaving Bolivia with no path to the sea.

In the new century, as wars became less violent and less frequent, Brazil entered into a small conflict with Bolivia for the possession of the Acre, which was acquired by Brazil in 1902. In 1917 Brazil declared war on the Central Powers, joined the allied side in World War I and sent a small fleet to the Mediterranean Sea and some troops to be integrated with the British and French forces. Brazil was the only South American country that fought in WWI. Later in 1932 Colombia and Peru entered a short armed conflict for territory in the Amazon. In the same year Paraguay declared war on Bolivia for possession of the Chaco, in a conflict that ended three years later with Paraguay's victory. Between 1941 and 1942 Peru and Ecuador fought decisively for territories claimed by both that were annexed by Peru, usurping Ecuador's frontier with Brazil.

Also in this period the first naval battle of World War II was fought on the continent, in the River Plate, between British forces and German submarines. The Germans still made numerous attacks on Brazilian ships on the coast, causing Brazil to declare war on the Axis powers in 1942, being the only South American country to fight in this war (and in both World Wars). Brazil sent naval and air forces to combat German and Italian submarines off the continent and throughout the South Atlantic, in addition to sending an expeditionary force to fight in the Italian Campaign.

A brief war was fought between Argentina and the UK in 1982, following an Argentine invasion of the Falkland Islands, which ended with an Argentine defeat. The last international war to be fought on South American soil was the 1995 Cenepa War between Ecuador and the Peru along their mutual border.

Wars became less frequent in the 20th century, with Bolivia-Paraguay and Peru-Ecuador fighting the last inter-state wars. Early in the 20th century, the three wealthiest South American countries engaged in a vastly expensive naval arms race which was catalyzed by the introduction of a new warship type, the "dreadnought". At one point, the Argentine government was spending a fifth of its entire yearly budget for just two dreadnoughts, a price that did not include later in-service costs, which for the Brazilian dreadnoughts was sixty percent of the initial purchase.
The continent became a battlefield of the Cold War in the late 20th century. Some democratically elected governments of Argentina, Brazil, Chile, Uruguay and Paraguay were overthrown or displaced by military dictatorships in the 1960s and 1970s. To curtail opposition, their governments detained tens of thousands of political prisoners, many of whom were tortured and/or killed on inter-state collaboration. Economically, they began a transition to neoliberal economic policies. They placed their own actions within the US Cold War doctrine of "National Security" against internal subversion. Throughout the 1980s and 1990s, Peru suffered from an internal conflict.

Argentina and Britain fought the Falklands War in 1982. The conflict lasted 74 days and ended with an Argentine surrender, returning the occupied Falkland islands to British control.

Colombia has had an ongoing, though diminished internal conflict, which started in 1964 with the creation of Marxist guerrillas (FARC-EP) and then involved several illegal armed groups of leftist-leaning ideology as well as the private armies of powerful drug lords. Many of these are now defunct, and only a small portion of the ELN remains, along with the stronger, though also greatly reduced, FARC.

Revolutionary movements and right-wing military dictatorships became common after World War II, but since the 1980s, a wave of democratization passed through the continent, and democratic rule is widespread now. Nonetheless, allegations of corruption are still very common, and several countries have developed crises which have forced the resignation of their governments, although, on most occasions, regular civilian succession has continued.
International indebtedness turned into a severe problem in the late 1980s, and some countries, despite having strong democracies, have not yet developed political institutions capable of handling such crises without resorting to unorthodox economic policies, as most recently illustrated by Argentina's default in the early 21st century. The last twenty years have seen an increased push towards regional integration, with the creation of uniquely South American institutions such as the Andean Community, Mercosur and Unasur. Notably, starting with the election of Hugo Chávez in Venezuela in 1998, the region experienced what has been termed a pink tide – the election of several leftist and center-left administrations to most countries of the area, except for the Guianas and Colombia.

Historically, the Hispanic countries were founded as Republican dictatorships led by caudillos. Brazil was the only exception, being a constitutional monarchy for its first 67 years of independence, until a coup d'état proclaimed a republic. In the late 19th century, the most democratic countries were Brazil, Chile, Argentina and Uruguay.

In the interwar period, nationalism grew stronger on the continent, influenced by countries like Nazi Germany and Fascist Italy. A series of authoritarian rules broke out in South American countries with views bringing them closer to the Axis Powers, like Vargas's Brazil. In the late 20th century, during the Cold War, many countries became military dictatorships under American tutelage in attempts to avoid the influence of the Soviet Union. After the fall of the authoritarian regimes, these countries became democratic republics.

During the first decade of the 21st century, South American governments have drifted to the political left, with leftist leaders being elected in Chile, Uruguay, Brazil, Argentina, Ecuador, Bolivia, Paraguay, Peru and Venezuela. The gross domestic product for each of those countries, however, dropped over that timeframe. Consequently, most South American countries are making increasing use of protectionist policies in order to help local economic development.

All South American countries are presidential republics with the exception of Suriname, a parliamentary republic. French Guiana is a French overseas department, while the Falkland Islands and South Georgia and the South Sandwich Islands are British overseas territories. It is currently the only inhabited continent in the world without monarchies; the Empire of Brazil existed during the 19th century and there was an unsuccessful attempt to establish a Kingdom of Araucanía and Patagonia in southern Argentina and Chile. Also in the twentieth century, Suriname was established as a constituent kingdom of the Kingdom of the Netherlands and Guyana retained the British monarch as head of state for 4 years after its independence.

Recently, an intergovernmental entity has been formed which aims to merge the two existing customs unions: Mercosur and the Andean Community, thus forming the third-largest trade bloc in the world.
This new political organization, known as Union of South American Nations, seeks to establish free movement of people, economic development, a common defense policy and the elimination of tariffs.

South America has over floor(/1e6) million inhabitants and a population growth rate of about 0.6% per year. There are several areas of sparse demographics such as tropical forests, the Atacama Desert and the icy portions of Patagonia. On the other hand, the continent presents regions of high population density, such as the great urban centers. The population is formed by descendants of Europeans (mainly Spaniards, Portuguese and Italians), Africans and Indigenous peoples. There is a high percentage of mestizos that vary greatly in composition by place. There is also a minor population of Asians, especially in Brazil. The two main languages are by far Spanish and Portuguese, followed by French, English and Dutch in smaller numbers.

Spanish and Portuguese are the most spoken languages in South America, with approximately 200 million speakers each. Spanish is the official language of most countries, along with other native languages in some countries. Portuguese is the official language of Brazil. Dutch is the official language of Suriname; English is the official language of Guyana, although there are at least twelve other languages spoken in the country, including Portuguese, Chinese, Hindustani and several native languages. English is also spoken in the Falkland Islands. French is the official language of French Guiana and the second language in Amapá, Brazil.

Indigenous languages of South America include Quechua in Peru, Bolivia, Ecuador, Chile and Colombia; Wayuunaiki in northern Colombia (La Guajira) and northwestern Venezuela (Zulia); Guaraní in Paraguay and, to a much lesser extent, in Bolivia; Aymara in Bolivia, Peru, and less often in Chile; and Mapudungun is spoken in certain pockets of southern Chile. At least three South American indigenous languages (Quechua, Aymara, and Guarani) are recognized along with Spanish as national languages.

Other languages found in South America include Hindustani and Javanese in Suriname; Italian in Argentina, Brazil, Uruguay and Venezuela; and German in certain pockets of Argentina and Brazil. German is also spoken in many regions of the southern states of Brazil, Riograndenser Hunsrückisch being the most widely spoken German dialect in the country; among other Germanic dialects, a Brazilian form of East Pomeranian is also well represented and is experiencing a revival. Welsh remains spoken and written in the historic towns of Trelew and Rawson in the Argentine Patagonia. There are also small clusters of Japanese-speakers in Brazil, Colombia and Peru. Arabic speakers, often of Lebanese, Syrian, or Palestinian descent, can be found in Arab communities in Argentina, Colombia, Brazil, Venezuela and in Paraguay.

An estimated 90% of South Americans are Christians (82% Roman Catholic, 8% other Christian denominations mainly traditional Protestants and Evangelicals but also Orthodox), accounting for c. 19% of Christians worldwide.

African descendent religions and Indigenous religions are also common throughout all South America, some examples of are Santo Daime, Candomblé, Umbanda and Encantados.

Crypto-Jews or Marranos, conversos, and Anusim were an important part of colonial life in Latin America.

Both Buenos Aires, Argentina and São Paulo, Brazil figure among the largest Jewish populations by urban area.

East Asian religions such as Japanese Buddhism, Shintoism, and Shinto-derived Japanese New Religions are common in Brazil and Peru. Korean Confucianism is especially found in Brazil while Chinese Buddhism and Chinese Confucianism have spread throughout the continent.

Kardecist Spiritism can be found in several countries.

Part of Religions in South America (2013):

Genetic admixture occurs at very high levels in South America. In Argentina, the European influence accounts for 65–79% of the genetic background, Amerindian for 17–31% and sub-Saharan African for 2–4%. In Colombia, the sub-Saharan African genetic background varied from 1% to 89%, while the European genetic background varied from 20% to 79%, depending on the region.
In Peru, European ancestries ranged from 1% to 31%, while the African contribution was only 1% to 3%. The Genographic Project determined the average Peruvian from Lima had about 28% European ancestry, 68% Native American, 2% Asian ancestry and 2% sub-Saharan African.

Descendants of indigenous peoples, such as the Quechua and Aymara, or the Urarina of Amazonia make up the majority of the population in Bolivia (56%) and, per some sources, in Peru (44%). In Ecuador, Amerindians are a large minority that comprises two-fifths of the population. The native European population is also a significant element in most other former Portuguese colonies.

People who identify as of primarily or totally European descent, or identify their phenotype as corresponding to such group, are more of a majority in Argentina, and Uruguay and more than half of the population of Chile (64.7%) and (48.4%) in Brazil. In Venezuela, according to the national census 42% of the population is primarily native Spanish, Italian and Portuguese descendants. In Colombia, people who identify as European descendant are about 37%. In Peru, European descendants are the third group in number (15%).

Mestizos (mixed European and Amerindian) are the largest ethnic group in Bolivia, Paraguay, Venezuela, Colombia and Ecuador and the second group in Peru and Chile.

South America is also home to one of the largest populations of Africans. This group is significantly present in Brazil, Colombia, Guyana, Suriname, French Guiana, Venezuela and Ecuador.

Brazil followed by Peru have the largest Japanese, Korean and Chinese communities in South America, Lima has the largest ethnic Chinese community in Latin America. Guyana and Suriname have the largest ethnic East Indian community.

In many places indigenous people still practice a traditional lifestyle based on subsistence agriculture or as hunter-gatherers. There are still some uncontacted tribes residing in the Amazon Rainforest.
The most populous country in South America is Brazil with /1e6 round 1 million people. The second largest country is Colombia with a population of . Argentina is the third most populous country with .

While Brazil, Argentina, and Colombia maintain the largest populations, large city populations are not restricted to those nations. The largest cities in South America, by far, are São Paulo, Rio de Janeiro, Buenos Aires, Santiago, Lima, and Bogotá. These cities are the only cities on the continent to exceed eight million, and three of five in the Americas. Next in size are Caracas, Belo Horizonte, Medellin and Salvador.

Five of the top ten metropolitan areas are in Brazil. These metropolitan areas all have a population of above 4 million and include the São Paulo metropolitan area, Rio de Janeiro metropolitan area, and Belo Horizonte metropolitan area. Whilst the majority of the largest metropolitan areas are within Brazil, Argentina is host to the second largest metropolitan area by population in South America: the Buenos Aires metropolitan region is above 13 million inhabitants.

South America has also been witness to the growth of megapolitan areas. In Brazil four megaregions exist including the Expanded Metropolitan Complex of São Paulo with more than 32 million inhabitants. The others are the Greater Rio, Greater Belo Horizonte and Greater Porto Alegre. Colombia also has four megaregions which comprise 72% of its population, followed by Venezuela, Argentina and Peru which are also homes of megaregions.

The top ten largest South American metropolitan areas by population as of 2015, based on national census numbers from each country:

South America relies less on the export of both manufactured goods and natural resources than the world average; merchandise exports from the continent were 16% of GDP on an exchange rate basis, compared to 25% for the world as a whole. Brazil (the seventh largest economy in the world and the largest in South America) leads in terms of merchandise exports at $251 billion, followed by Venezuela at $93 billion, Chile at $86 billion, and Argentina at $84 billion.

Since 1930, the continent has experienced remarkable growth and diversification in most economic sectors. Most agricultural and livestock products are destined for the domestic market and local consumption. However, the export of agricultural products is essential for the balance of trade in most countries.

The main agrarian crops are export crops, such as soy and wheat. The production of staple foods such as vegetables, corn or beans is large, but focused on domestic consumption. Livestock raising for meat exports is important in Argentina, Paraguay, Uruguay and Colombia. In tropical regions the most important crops are coffee, cocoa and bananas, mainly in Brazil, Colombia and Ecuador. Traditionally, the countries producing sugar for export are Peru, Guyana and Suriname, and in Brazil, sugar cane is also used to make ethanol. On the coast of Peru, northeast and south of Brazil, cotton is grown. Fifty percent of the South American surface is covered by forests, but timber industries are small and directed to domestic markets. In recent years, however, transnational companies have been settling in the Amazon to exploit noble timber destined for export. The Pacific coastal waters of South America are the most important for commercial fishing. The anchovy catch reaches thousands of tons, and tuna is also abundant (Peru is a major exporter). The capture of crustaceans is remarkable, particularly in northeastern Brazil and Chile.

Only Brazil and Argentina are part of the G20 (industrial countries), while only Brazil is part of the G8+5 (the most powerful and influential nations in the world). In the tourism sector, a series of negotiations began in 2005 to promote tourism and increase air connections within the region. Punta del Este, Florianópolis and Mar del Plata are among the most important resorts in South America.

The most industrialized countries in South America are Brazil, Argentina, Chile, Colombia, Venezuela and Uruguay respectively. These countries alone account for more than 75 percent of the region's economy and add up to a GDP of more than US$3.0 trillion. Industries in South America began to take on the economies of the region from the 1930s when the Great Depression in the United States and other countries of the world boosted industrial production in the continent. From that period the region left the agricultural side behind and began to achieve high rates of economic growth that remained until the early 1990s when they slowed due to political instabilities, economic crises and neoliberal policies.

Since the end of the economic crisis in Brazil and Argentina that occurred in the period from 1998 to 2002, which has led to economic recession, rising unemployment and falling population income, the industrial and service sectors have been recovering rapidly. Chile, Argentina and Brazil have recovered fastest, growing at an average of 5% per year. All of South America after this period has been recovering and showing good signs of economic stability, with controlled inflation and exchange rates, continuous growth, a decrease in social inequality and unemployment–factors that favor industry.

The main industries are: electronics, textiles, food, automotive, metallurgy, aviation, naval, clothing, beverage, steel, tobacco, timber, chemical, among others. Exports reach almost US$400 billion annually, with Brazil accounting for half of this.

The economic gap between the rich and poor in most South American nations is larger than on most other continents. The richest 10% receive over 40% of the nation's income in Bolivia, Brazil, Chile, Colombia, and Paraguay, while the poorest 20% receive 4% or less in Bolivia, Brazil, and Colombia. This wide gap can be seen in many large South American cities where makeshift shacks and slums lie in the vicinity of skyscrapers and upper-class luxury apartments; nearly one in nine South Americans live on less than $2 per day (on a purchasing power parity basis).

Tourism has increasingly become a significant source of income for many South American countries.

Historical relics, architectural and natural wonders, a diverse range of foods and culture, vibrant and colorful cities, and stunning landscapes attract millions of tourists every year to South America. Some of the most visited places in the region are Iguazu Falls, Recife, Olinda, Machu Picchu, Bariloche, the Amazon rainforest, Rio de Janeiro, São Luís, Salvador, Fortaleza, Maceió, Buenos Aires, Florianópolis, San Ignacio Miní, Isla Margarita, Natal, Lima, São Paulo, Angel Falls, Brasília, Nazca Lines, Cuzco, Belo Horizonte, Lake Titicaca, Salar de Uyuni, La Paz, Jesuit Missions of Chiquitos, Los Roques archipelago, Gran Sabana, Patagonia, Tayrona National Natural Park, Santa Marta, Bogotá, Cali, Medellín, Cartagena, Perito Moreno Glacier and the Galápagos Islands. In 2016 Brazil hosted the 2016 Summer Olympics.

South Americans are culturally influenced by their indigenous peoples, the historic connection with the Iberian Peninsula and Africa, and waves of immigrants from around the globe.

South American nations have a rich variety of music. Some of the most famous genres include vallenato and cumbia from Colombia, pasillo from Colombia and Ecuador, samba, bossa nova and música sertaneja from Brazil, and tango from Argentina and Uruguay. Also well known is the non-commercial folk genre Nueva Canción movement which was founded in Argentina and Chile and quickly spread to the rest of the Latin America. People on the Peruvian coast created the fine guitar and cajon duos or trios in the most mestizo (mixed) of South American rhythms such as the Marinera (from Lima), the Tondero (from Piura), the 19th century popular Creole Valse or Peruvian Valse, the soulful Arequipan Yaravi, and the early 20th century Paraguayan Guarania. In the late 20th century, Spanish rock emerged by young hipsters influenced by British pop and American rock. Brazil has a Portuguese-language pop rock industry as well a great variety of other music genres. In the central and western regions of Bolivia, Andean and folklore music like Diablada, Caporales and Morenada are the most representative of the country, which were originated by European, Aymara and Quechua influences.

The literature of South America has attracted considerable critical and popular acclaim, especially with the Latin American Boom of the 1960s and 1970s, and the rise of authors such as Mario Vargas Llosa, Gabriel García Márquez in novels and Jorge Luis Borges and Pablo Neruda in other genres. The Brazilians Machado de Assis and João Guimarães Rosa are widely regarded as the greatest Brazilian writers.

Because of South America's broad ethnic mix, South American cuisine has African, South American Indian, South Asian, East Asian, and European influences. Bahia, Brazil, is especially well known for its West African–influenced cuisine. Argentines, Chileans, Uruguayans, Brazilians, Bolivians, and Venezuelans regularly consume wine. People in Argentina, Paraguay, Uruguay, southern Chile, Bolivia and Brazil drink mate, an herb which is brewed. The Paraguayan version, terere, differs from other forms of mate in that it is served cold. Pisco is a liquor distilled from grapes in Peru and Chile. Peruvian cuisine mixes elements from Chinese, Japanese, Spanish, Italian, African, Arab, Andean, and Amazonic food.

The artist Oswaldo Guayasamín (1919–1999) from Ecuador, represented with his painting style the feeling of the peoples of Latin America highlighting social injustices in various parts of the world. The Colombian Fernando Botero (1932) is one of the greatest exponents of painting and sculpture that continues still active and has been able to develop a recognizable style of his own. For his part, the Venezuelan Carlos Cruz-Diez has contributed significantly to contemporary art, with the presence of works around the world.

Currently several emerging South American artists are recognized by international art critics: Guillermo Lorca – Chilean painter, Teddy Cobeña – Ecuadorian sculptor and recipient of international sculpture award in France) and Argentine artist Adrián Villar Rojas – winner of the Zurich Museum Art Award among many others.

A wide range of sports are played in the continent of South America, with football being the most popular overall, while baseball is the most popular in Venezuela.

Other sports include basketball, cycling, polo, volleyball, futsal, motorsports, rugby (mostly in Argentina and Uruguay), handball, tennis, golf, field hockey, boxing and cricket.

South America hosted its first Olympic Games in Rio de Janeiro, Brazil in 2016 and will host the Youth Olympic Games in Buenos Aires, Argentina in 2018.

South America shares with Europe supremacy over the sport of football as all winners in FIFA World Cup history and all winning teams in the FIFA Club World Cup have come from these two continents. Brazil holds the record at the FIFA World Cup with five titles in total. Argentina and Uruguay have two titles each. So far four South American nations have hosted the tournament including the first edition in Uruguay (1930). The other three were Brazil (1950, 2014), Chile (1962), and Argentina (1978).

South America is home to the longest running international football tournament; the Copa América, which has been regularly contested since 1916. Uruguay won the Copa América a record 15 times, surpassing hosts Argentina in 2011 to reach 15 titles (they were previously equal at 14 titles each during the 2011 Copa América).

Also, in South America, a multi-sport event, the South American Games, are held every four years. The first edition was held in La Paz in 1978 and the most recent took place in Santiago in 2014.

South American Cricket Championship is an international limited-overs cricket tournament played since 1995 featuring national teams from South America and certain other invited sides including teams from North America, currently played annually but until 2013 was usually played every two seasons.

Due to the diversity of topography and pluviometric precipitation conditions, the region's water resources vary enormously in different areas. In the Andes, navigation possibilities are limited, except for the Magdalena River, Lake Titicaca and the lakes of the southern regions of Chile and Argentina. Irrigation is an important factor for agriculture from northwestern Peru to Patagonia. Less than 10% of the known electrical potential of the Andes had been used until the mid-1960s.

The Brazilian Highlands has a much higher hydroelectric potential than the Andean region and its possibilities of exploitation are greater due to the existence of several large rivers with high margins and the occurrence of great differences forming huge cataracts, such as those of Paulo Afonso, Iguaçu and others. The Amazon River system has about 13,000 km of waterways, but its possibilities for hydroelectric use are still unknown.

Most of the continent's energy is generated through hydroelectric power plants, but there is also an important share of thermoelectric and wind energy. Brazil and Argentina are the only South American countries that generate nuclear power, each with two nuclear power plants. In 1991 these countries signed a peaceful nuclear cooperation agreement.

South American transportation systems are still deficient, with low kilometric densities. The region has about 1,700,000 km of highways and 100,000 km of railways, which are concentrated in the coastal strip, and the interior is still devoid of communication.

Only two railroads are continental: the Transandina, which connects Buenos Aires, in Argentina to Valparaíso, in Chile, and the Brazil–Bolivia Railroad, which makes it the connection between the port of Santos in Brazil and the city of Santa Cruz de la Sierra, in Bolivia. In addition, there is the Pan-American Highway, which crosses the Andean countries from north to south, although some stretches are unfinished.

Two areas of greater density occur in the railway sector: the platinum network, which develops around the Platine region, largely belonging to Argentina, with more than 45,000 km in length; And the Southeast Brazil network, which mainly serves the state of São Paulo, state of Rio de Janeiro and Minas Gerais. Brazil and Argentina also stand out in the road sector. In addition to the modern roads that extend through northern Argentina and south-east and south of Brazil, a vast road complex aims to link Brasília, the federal capital, to the South, Southeast, Northeast and Northern regions of Brazil.

The Port of Callao is the main port of Peru.

South America has one of the largest bays of navigable inland waterways in the world, represented mainly by the Amazon basin, the Platine basin, the São Francisco and the Orinoco basins, Brazil having about 54,000 km navigable, while Argentina has 6,500 km and Venezuela, 1,200 km.

The two main merchant fleets also belong to Brazil and Argentina. The following are those of Chile, Venezuela, Peru and Colombia. The largest ports in commercial movement are those of Buenos Aires, Santos, Rio de Janeiro, Bahía Blanca, Rosario, Valparaíso, Recife, Salvador, Montevideo, Paranaguá, Rio Grande, Fortaleza, Belém and Maracaibo.

In South America, commercial aviation has a magnificent expansion field, which has one of the largest traffic density lines in the world, Rio de Janeiro–São Paulo, and large airports, such as Congonhas, São Paulo–Guarulhos International and Viracopos (São Paulo), Rio de Janeiro International and Santos Dumont (Rio de Janeiro), El Dorado (Bogotá), Ezeiza (Buenos Aires), Tancredo Neves International Airport (Belo Horizonte), Curitiba International Airport (Curitiba), Brasilia, Caracas, Montevideo, Lima, Viru Viru International Airport (Santa Cruz de la Sierra), Recife, Salvador, Salgado Filho International Airport (Porto Alegre), Fortaleza, Manaus and Belém.

The main public transport in major cities is the bus. Many cities also have a diverse system of metro and subway trains, the first of which was the Buenos Aires subte, opened 1913. The Santiago subway is the largest network in South America, with 103 km, while the São Paulo subway is the largest in transportation, with more than 4.6 million passengers per day and was voted the best in the Americas. In Rio de Janeiro was installed the first railroad of the continent, in 1854. Today the city has a vast and diversified system of metropolitan trains, integrated with buses and subway. Recently it was also inaugurated in the city a Light Rail System called VLT, a small electrical trams at low speed, while São Paulo inaugurated its monorail, the first of South America. In Brazil, an express bus system called Bus Rapid Transit (BRT), which operates in several cities, has also been developed. Mi Teleférico, also known as Teleférico La Paz–El Alto (La Paz–El Alto Cable Car), is an aerial cable car urban transit system serving the La Paz–El Alto metropolitan area in Bolivia.




</doc>
<doc id="26771" url="https://en.wikipedia.org/wiki?curid=26771" title="Spindletop">
Spindletop

Spindletop is an oil field located in the southern portion of Beaumont, Texas in the United States. The Spindletop dome was derived from the Louann Salt evaporite layer of the Jurassic geologic period. On January 10, 1901, a well at Spindletop struck oil ("came in"). The Spindletop gusher blew for 9 days at a rate estimated at of oil per day. Gulf Oil and Texaco, now part of Chevron Corporation, were formed to develop production at Spindletop. According to Daniel Yergin, the Spindletop discovery led the United States into the oil age. Prior to Spindletop, oil was primarily used for lighting and as a lubricant. Because of the quantity of oil discovered, burning petroleum as a fuel for mass consumption suddenly became economically feasible.

The frenzy of oil exploration and the economic development it generated in the state became known as the Texas oil boom. The United States soon became the world's leading oil producer.

Higgins sought a source of natural gas for his brickyard and envisioned producing oil and gas from Sour Spring Mound, convinced it was an anticline. The eventual oil field would be called Spindletop, after a hill one mile to the east, and four miles south of Beaumont. The hill had the appearance of a spindle due to trees on its hilltop ("cimas de boneteros", "tops of spindle-trees"). The mound was famous for its gas seeps, which Higgins lit for his Baptist Sunday school class. In August 1892, George W. O'Brien, George W. Carroll, Pattillo Higgins, and J.F. Lanier formed the Gladys City Oil, Gas, and Manufacturing Company to do exploratory drilling. The company tried drilling two test wells, but ran into trouble trying to penetrate below , encountering a quicksand-like formation. Higgins quit the venture in 1896.

Pattillo Higgins then teamed with Captain Anthony F. Lucas, the leading expert in the U.S. on salt-dome formations. Lucas made a lease agreement in 1899 with the Gladys City Company and a subsequent agreement with Higgins. Lucas drilled to before running out of money. He secured additional funding from John H. Galey, James M. Guffey, and Andrew Mellon of Pittsburgh, and the Guffey Petroleum Company was formed. Yet, the deal left Lucas with only an eighth share, and Higgins with nothing.

Lucas and Galey employed the Hamill brothers for the drilling, while Galey picked the well site on land leased from the Beaumont Pasture Company. This tract of land was near the top of Sour Spring Mound. The well was spudded on 27 October 1900. At , they hit the quicksands that had stopped earlier efforts. A solution consisted of driving an eight-inch casing pipe through the sand over the next 20 days, with a four-inch "wash pipe" to flush out the sand from the bottom of the hole with water. At around , Lucas improvised a check valve to prevent the increased gas pressure from forcing sand into the casing, enabling them to reach a depth of , and past the thick quicksand formation. At a depth of , they adopted eighteen-hour shifts for continuous operations, drilling during the day, and keeping circulation going at night, to prevent a gas blowout. In early December, they hit a pocket of coarse water sand, when the adopted another innovation, mixing mud into the water which prevented the "heavier" water from dissipating into the sand. This drilling mud stabilized the hole, and soon they were drilling into a clay formation called gumbo. At they reached limestone, and on 9 December, oil started showing up in the slush pit. The oil was coming from a thick oil sand at a depth of . Yet, that oil sand was too soft and fine to develop at that time, and Caroline Lucas convinced Galey to continue drilling to , per contract. On Christmas Eve, they landed six-inch pipe below the sand at , then enjoyed the holiday, returning New Year's Day. The hit another gas pocket, which forced water and mud out of the hole for ten minutes. Then, at , they reached a sulphur layer, followed by more layers of limestone. On 10 January, they needed to replace the dull fishtail drill bit. While lowering the pipe down the hole, they only got to about 35 joints of pipe, or about , before a low rumble sent mud, and then drill stem out of the hole. This was followed by silence, an explosion of more mud and gas, more silence, a flow of oil, and then a loud roar. On January 10, 1901, at a depth of 1,139 ft (347 m), what is known as the Lucas Gusher or the Lucas Geyser blew oil over in the air at a rate of (4,200,000 gallons). Nine days passed before the well was brought under control using a Christmas Tree devised by the Hamills.

By late June, there were 13 gushers on Spindletop. These included those by David R. Beatty on 26 March, the Heywood Brothers Oil Company, two more from the J.M. Guffey Company, and the Higgins Oil and Fuel Company on 18 April. Then in July 1901, the Hogg-Swayne Syndicate leased 15 acres from J.M. Guffey.

Spindletop was the largest gusher the world had seen and catapulted Beaumont into an oil-fueled boomtown. Beaumont's population of 10,000 tripled in 3 months and eventually rose to 50,000. Speculation led land prices to increase rapidly. By the end of 1902, more than 500 companies had been formed and 285 wells were in operation.

Spindletop produced 17,420,949 barrels of oil in 1902, but only half that much in 1903 as production declined. Yet Spindletop inspired wildcatting along the Gulf Coastal Plain. Significant salt dome oil fields included Sour Lake and Saratoga in 1902, Batson Prairie in 1903, the Humble oil field in 1905, and the Goose Creek Oil Field in 1908.

Standard Oil, which then had a monopoly or near-monopoly on the petroleum industry in the eastern states, was prevented from moving aggressively into the new oilfield by state antitrust laws. Populist sentiment against Standard Oil was particularly strong at the time of the Spindletop discovery. In 1900, an oil-products marketing company affiliated with Standard Oil had been banned from the state for its cutthroat business practices. Although Standard built refineries in the area, it was unable to dominate the new Gulf Coast oil fields the way it had in the eastern states. As a result, a number of startup oil companies at Spindletop, such as Texaco and Gulf Oil, grew into formidable competitors to Standard Oil.

Among those drilling at Spindletop was W. Scott Heywood, a native of Cleveland, Ohio, who in 1901 made the first oil discovery in nearby Jeff Davis Parish in southwestern Louisiana. In 1932, Heywood was elected to a single term in the Louisiana State Senate.

Production at Spindletop began to decline rapidly after 1902, and the wells produced only by 1904. Unfortunately the developers had signed a 20-year contract to sell 25,000 barrels per day at $0.25 per barrel to Shell Oil. When the price climbed above $0.35 per barrel, the operation was stressed and Mellon who had lent money for Spindle Top’s development took control of the company, won a lawsuit allowing Mellon to renege on the contract, and created Gulf Oil. On November 14, 1925, the Yount-Lee Oil Company brought in its McFaddin No. 2 at a depth around , sparking a second boom, which culminated in the field's peak production year of 1927, during which 21 million barrels (3.3 GL) were produced. Spindletop continued as a productive source of oil until about 1936. It was then mined for sulfur from the 1950s to about 1975.

In 1976, Lamar University dedicated the Spindletop-Gladys City Boomtown Museum to preserve the history of the Spindletop oil gusher era in Beaumont. The museum features an oil derrick and many reconstructed Gladys City building interiors furnished with authentic artifacts from the Spindletop boomtown period.

The Lucas Gusher Monument is located at the museum. The monument, erected at the wellhead in July, 1941, was moved to the Spindletop-Gladys City Museum after it became unstable due to ground subsidence. According to an article by Nedra Foster, LS in the July/August, 2000 issue of the "Professional Surveyor Magazine," the monument was originally located within 4 ft of the site of the Spindletop well.

Today, the wellhead is marked at Spindletop Park by a flagpole flying the Texas flag. It is located about 1.5 miles south of the museum, off West Port Arthur Road/Spur 93. The site includes a viewing platform with information placards, about a quarter mile from the flagpole. The wellhead site is in the middle of swampland on private land and is not accessible. Directions to the park and viewing platform are available at the museum.

On December 4, 1955, the Spindletop story was dramatized in "Spindletop – The First Great Texas Oil Strike (January 10, 1901)" on the CBS history series, "You Are There". Robert Bray was cast as Pattillo Higgins, Mike Ragan as Marion Fletcher, Parley Baer as Captain Lucas, Jean Byron as Caroline Lucas, DeForest Kelley as Al Hammill, Tyler McVey as Mayor Wheat, and William Fawcett as a farmer.





</doc>
<doc id="26773" url="https://en.wikipedia.org/wiki?curid=26773" title="Stendhal">
Stendhal

Marie-Henri Beyle (; 23 January 1783 – 23 March 1842), better known by his pen name Stendhal (, ; ), was a 19th-century French writer. Best known for the novels "Le Rouge et le Noir" ("The Red and the Black", 1830) and "La Chartreuse de Parme" ("The Charterhouse of Parma", 1839), he is highly regarded for the acute analysis of his characters' psychology and considered one of the early and foremost practitioners of realism.

Born in Grenoble, Isère, he was an unhappy child, disliking his "unimaginative" father and mourning his mother, whom he passionately loved, and who died when he was seven. His closest friend was his younger sister, Pauline, with whom he maintained a steady correspondence throughout the first decade of the 19th century.
The military and theatrical worlds of the First French Empire were a revelation to Beyle. He was named an auditor with the Conseil d'État on 3 August 1810, and thereafter took part in the French administration and in the Napoleonic wars in Italy. He travelled extensively in Germany and was part of Napoleon's army in the 1812 invasion of Russia.

Stendhal witnessed the burning of Moscow from just outside the city. He was appointed Commissioner of War Supplies and sent to Smolensk to prepare provisions for the returning army. He crossed the Berezina River by finding a usable ford rather than the overwhelmed pontoon bridge, which probably saved his life and those of his companions. He arrived in Paris in 1813, largely unaware of the general fiasco that the retreat had become. Stendhal became known, during the Russian campaign, for keeping his wits about him, and maintaining his "sang-froid and clear-headedness." He also maintained his daily routine, shaving each day during the retreat from Moscow.

After the 1814 Treaty of Fontainebleau, he left for Italy, where he settled in Milan. He formed a particular attachment to Italy, where he spent much of the remainder of his career, serving as French consul at Trieste and Civitavecchia. His novel "The Charterhouse of Parma", written in 52 days, is set in Italy, which he considered a more sincere and passionate country than Restoration France. An aside in that novel, referring to a character who contemplates suicide after being jilted, speaks about his attitude towards his home country: "To make this course of action clear to my French readers, I must explain that in Italy, a country very far away from us, people are still driven to despair by love."

Stendhal identified with the nascent liberalism and his sojourn in Italy convinced him that Romanticism was essentially the literary counterpart of liberalism in politics. When Stendhal was appointed to a consular post in Trieste in 1830, Metternich refused his "exequatur" on account of Stendhal's liberalism and anti-clericalism.

Stendhal was a dandy and wit about town in Paris, as well as an obsessive womaniser. His genuine empathy towards women is evident in his books; Simone de Beauvoir spoke highly of him in "The Second Sex". One of his early works is "On Love," a rational analysis of romantic passion that was based on his unrequited love for Mathilde, Countess Dembowska, whom he met while living at Milan. This fusion of, and tension between, clear-headed analysis and romantic feeling is typical of Stendhal's great novels; he could be considered a Romantic realist.

Stendhal suffered miserable physical disabilities in his final years as he continued to produce some of his most famous work. As he noted in his journal, he was taking iodide of potassium and quicksilver to treat his syphilis, resulting in swollen armpits, difficulty swallowing, pains in his shrunken testicles, sleeplessness, giddiness, roaring in the ears, racing pulse and "tremors so bad he could scarcely hold a fork or a pen". Modern medicine has shown that his health problems were more attributable to his treatment than to his syphilis.

Stendhal died on 23 March 1842, a few hours after collapsing with a seizure on the streets of Paris. He is interred in the Cimetière de Montmartre.

Before settling on the pen name Stendhal, he published under many pen names, including "Louis Alexandre Bombet" and "Anastasius Serpière". The only book that Stendhal published under his own name was "The History of Painting" (1817). From the publication of "Rome, Naples, Florence" (September 1817) onwards, he published his works under the pseudonym "M. de Stendhal, officier de cavalerie". He borrowed this "nom de plume" from the German city of Stendal, birthplace of Johann Joachim Winckelmann, an art historian and archaeologist famous at the time.

In 1807 Stendhal stayed near Stendal, where he fell in love with a woman named Wilhelmine, whom he called Minette, and for whose sake he remained in the city. "I have no inclination, now, except for Minette, for this blonde and charming Minette, this soul of the north, such as I have never seen in France or Italy." Stendhal added an additional "H" to make more clear the Germanic pronunciation.

Stendhal used many aliases in his autobiographical writings and correspondence, and often assigned pseudonyms to friends, some of whom adopted the names for themselves. Stendhal used more than a hundred pseudonyms, which were astonishingly diverse. Some he used no more than once, while others he returned to throughout his life. "Dominique" and "Salviati" served as intimate pet names. He coins comic names "that make him even more bourgeois than he really is: Cotonnet, Bombet, Chamier." He uses many ridiculous names: "Don phlegm", "Giorgio Vasari", "William Crocodile", "Poverino", "Baron de Cutendre". One of his correspondents, Prosper Mérimée, said: "He never wrote a letter without signing a false name."

Stendhal's "Journal" and autobiographical writings include many comments on masks and the pleasures of "feeling alive in many versions." "Look upon life as a masked ball," is the advice that Stendhal gives himself in his diary for 1814. In "Memoirs of an Egotist" he writes: "Will I be believed if I say I'd wear a mask with pleasure and be delighted to change my name?...for me the supreme happiness would be to change into a lanky, blonde German and to walk about like that in Paris."

Contemporary readers did not fully appreciate Stendhal's realistic style during the Romantic period in which he lived. He was not fully appreciated until the beginning of the 20th century. He dedicated his writing to "the Happy Few" (in English in the original). This can be interpreted as a reference to Canto 11 of Lord Byron's "Don Juan", which refers to "the thousand happy few" who enjoy high society, or to the "we few, we happy few, we band of brothers" line of William Shakespeare's "Henry V", but Stendhal's use more likely refers to "The Vicar of Wakefield" by Oliver Goldsmith, parts of which he had memorized in the course of teaching himself English.

In "The Vicar of Wakefield", "the happy few" refers ironically to the small number of people who read the title character's obscure and pedantic treatise on monogamy. As a literary critic, such as in "Racine and Shakespeare", Stendhal championed the Romantic aesthetic by unfavorably comparing the rules and strictures of Jean Racine's classicism to the freer verse and settings of Shakespeare, and supporting the writing of plays in prose.

Today, Stendhal's works attract attention for their irony and psychological and historical dimensions. Stendhal was an avid fan of music, particularly the works of the composers Domenico Cimarosa, Wolfgang Amadeus Mozart and Gioacchino Rossini. He wrote a biography of Rossini, "Vie de Rossini" (1824), now more valued for its wide-ranging musical criticism than for its historical content.

In his works, Stendhal reprised excerpts appropriated from Giuseppe Carpani, Théophile Frédéric Winckler, Sismondi and others.




Stendhal's brief memoir, "Souvenirs d'Égotisme" ("Memoirs of an Egotist") was published posthumously in 1892. Also published was a more extended autobiographical work, thinly disguised as the "Life of Henry Brulard".


His other works include short stories, journalism, travel books ("A Roman Journal"), a famous collection of essays on Italian painting, and biographies of several prominent figures of his time, including Napoleon, Haydn, Mozart, Rossini and Metastasio.

In Stendhal's 1822 classic "On Love" he describes or compares the "birth of love", in which the love object is 'crystallized' in the mind, as being a process similar or analogous to a trip to Rome. In the analogy, the city of Bologna represents "indifference" and Rome represents "perfect love":

When we are in Bologna, we are entirely indifferent; we are not concerned to admire in any particular way the person with whom we shall perhaps one day be madly in love; even less is our imagination inclined to overrate their worth. In a word, in Bologna "crystallization" has not yet begun. When the journey begins, love departs. One leaves Bologna, climbs the Apennines, and takes the road to Rome. The departure, according to Stendhal, has nothing to do with one's will; it is an instinctive moment. This transformative process actuates in terms of four steps along a journey:


This journey or crystallization process (shown above) was detailed by Stendhal on the back of a playing card while speaking to Madame Gherardi, during his trip to the Salzburg salt mine.

Hippolyte Taine considered the psychological portraits of Stendhal's characters to be "real, because they are complex, many-sided, particular and original, like living human beings." Émile Zola concurred with Taine's assessment of Stendhal's skills as a "psychologist", and although emphatic in his praise of Stendhal's psychological accuracy and rejection of convention, he deplored the various implausibilities of the novels and Stendhal's clear authorial intervention.

The German philosopher Friedrich Nietzsche refers to Stendhal as "France's last great psychologist" in "Beyond Good and Evil" (1886). He also mentions Stendhal in the "Twilight of the Idols" (1889) during a discussion of Dostoevsky as a psychologist, saying that encountering Dostoevsky was "the most beautiful accident of my life, more so than even my discovery of Stendhal".

Ford Madox Ford, in "The English Novel", asserts that to Diderot and Stendhal "the Novel owes its next great step forward...At that point it became suddenly evident that the Novel as such was capable of being regarded as a means of profoundly serious and many-sided discussion and therefore as a medium of profoundly serious investigation into the human case."

Erich Auerbach considers modern "serious realism" to have begun with Stendhal and Balzac. In "", he remarks of a scene in "The Red and the Black" that "it would be almost incomprehensible without a most accurate and detailed knowledge of the political situation, the social stratification, and the economic circumstances of a perfectly definite historical moment, namely, that in which France found itself just before the July Revolution."

In Auerbach's view, in Stendhal's novels "characters, attitudes, and relationships of the "dramatis personæ", then, are very closely connected with contemporary historical circumstances; contemporary political and social conditions are woven into the action in a manner more detailed and more real than had been exhibited in any earlier novel, and indeed in any works of literary art except those expressly purporting to be politico-satirical tracts."

Simone de Beauvoir uses Stendhal as an example of a feminist author. In "The Second Sex" de Beauvoir writes “Stendhal never describes his heroines as a function of his heroes: he provides them with their own destinies.” She furthermore points out that it “is remarkable that Stendhal is both so profoundly romantic and so decidedly feminist; feminists are usually rational minds that adopt a universal point of view in all things; but it is not only in the name of freedom in general but also in the name of individual happiness that Stendhal calls for women’s emancipation.” Yet, Beauvoir criticises Stendhal for, although wanting a woman to be his equal, her only destiny he envisions for her remains a man.

Even Stendhal's autobiographical works, such as "The Life of Henry Brulard" or "Memoirs of an Egotist", are "far more closely, essentially, and concretely connected with the politics, sociology, and economics of the period than are, for example, the corresponding works of Rousseau or Goethe; one feels that the great events of contemporary history affected Stendhal much more directly than they did the other two; Rousseau did not live to see them, and Goethe had managed to keep aloof from them." Auerbach goes on to say: 

Vladimir Nabokov was dismissive of Stendhal, in "Strong Opinions" calling him "that pet of all those who like their French plain". In the notes to his translation of "Eugene Onegin", he asserts that "Le Rouge et le Noir" is "much overrated", and that Stendhal has a "paltry style". In "Pnin" Nabokov wrote satirically, "Literary departments still labored under the impression that Stendhal, Galsworthy, Dreiser, and Mann were great writers."

In 2019, rumours began amongst the undergraduate body at the University of Oxford that Stendhal was actually a woman posing as a man. Despite having been reported as such in several essays received by the French Department, this is most likely false.

Michael Dirda considers Stendhal "the greatest all round French writer – author of two of the top 20 French novels, author of a highly original autobiography ("Vie de Henry Brulard"), a superb travel writer, and as inimitable a presence on the page as any writer you'll ever meet."

In 1817 Stendhal was reportedly overcome by the cultural richness of Florence he encountered when he first visited the Tuscan city. As he described in his book "Naples and Florence: A Journey from Milan to Reggio":

As I emerged from the porch of Santa Croce, I was seized with a fierce palpitation of the heart (that same symptom which, in Berlin, is referred to as an attack of the nerves); the well-spring of life was dried up within me, and I walked in constant fear of falling to the ground.

The condition was diagnosed and named in 1979 by Italian psychiatrist Dr. Graziella Magherini, who had noticed similar psychosomatic conditions (racing heart beat, nausea and dizziness) amongst first-time visitors to the city.

In homage to Stendhal, Trenitalia named their overnight train service from Paris to Venice the Stendhal Express.






</doc>
<doc id="26775" url="https://en.wikipedia.org/wiki?curid=26775" title="Syndicalism">
Syndicalism

Syndicalism is a radical current in the labor movement that was most active in the early 20th century. Its main idea is the establishment of local worker-based organizations and the advancement of the demands and rights of workers through strikes. According to the Marxist historian Eric Hobsbawm, it was predominant in the revolutionary left in the decade which preceded the outbreak of World War I because Marxism was mostly reformist at that time.

Major syndicalist organizations included the General Confederation of Labor in France, the National Confederation of Labor in Spain, the Italian Syndicalist Union, the Free Workers' Union of Germany, and the Argentine Regional Workers' Federation. Although they did not regard themselves as syndicalists, the Industrial Workers of the World, the Irish Transport and General Workers' Union and the Canadian One Big Union are considered by most historians to belong to this current.

A number of syndicalist organizations were and still are to this day linked in the International Workers' Association, but some of its member organizations left for the International Confederation of Labor, formed in 2018.

The term "syndicalism" has French origins. In French, a "syndicat" is a trade union, usually a local union. The corresponding words in Spanish and Portuguese, "sindicato", and Italian, "sindacato", are similar. By extension, the French "syndicalisme" refers to trade unionism in general. The concept "syndicalisme révolutionnaire" or "revolutionary syndicalism" emerged in French socialist journals in 1903 and the French General Confederation of Labor ("Confédération générale du travail", CGT) came to use the term to describe its brand of unionism. "Revolutionary syndicalism", or more commonly "syndicalism" with the "revolutionary" implied, was then adapted to a number of languages by unionists following the French model.

Many scholars, including Ralph Darlington, Marcel van der Linden, and Wayne Thorpe, apply the term "syndicalism" to a number of organizations or currents within the labor movement that did not identify as "syndicalist". They apply the label to one big unionists or industrial unionists in North America and Australia, Larkinists in Ireland, and groups that identify as revolutionary industrialists, revolutionary unionists, anarcho-syndicalists, or councilists. This includes the Industrial Workers of the World (IWW) in the United States, for example, which claimed its industrial unionism was "a higher type of revolutionary labor organization than that proposed by the syndicalists". Van der Linden and Thorpe use "syndicalism" to refer to "all revolutionary, direct-actionist organizations". Darlington proposes that syndicalism be defined as "revolutionary trade unionism". He and van der Linden argue that it is justified to group together such a wide range of organizations because their similar modes of action or practice outweigh their ideological differences.

Others, like Larry Peterson and Erik Olssen, disagree with this broad definition. According to Olssen, this understanding has a "tendency to blur the distinctions between industrial unionism, syndicalism, and revolutionary socialism". Peterson gives a more restrictive definition of "syndicalism" based on five criteria: 

This definition excludes the IWW and the Canadian One Big Union (OBU). Peterson proposes the broader category "revolutionary industrial unionism" to encompass syndicalism, groups like the IWW and the OBU, and others. The defining commonality between these groups is that they sought to unite all workers in a general organization.

Syndicalism originated in France and spread from there. The French CGT was the model and inspiration for syndicalist groups throughout Europe and the world. Revolutionary industrial unionism, part of syndicalism in the broader sense, originated with the IWW in the United States and then caught on in other countries. In a number of countries, however, certain syndicalist practices and ideas predate the coining of the term in France or the founding of the IWW. In Bert Altena's view, a number of movements in Europe can be called syndicalist, even before 1900. According to the English social historian E.P. Thompson and the anarcho-syndicalist theorist Rudolf Rocker, there were syndicalist tendencies in Britain's labor movement as early as the 1830s. Syndicalists saw themselves as the heirs of the First International, the international socialist organization formed in 1864, particularly its anti-authoritarian wing led by Mikhail Bakunin. Bakunin and his followers advocated the general strike, rejected electoral politics, and anticipated workers' organizations replacing rule by the state. According to Lucien van der Walt, the Spanish section of the First International, formed in 1870, was in fact syndicalist. Kenyon Zimmer sees a "proto-syndicalism" in the influence the anarchist-led International Working People's Association (IWPA) and Central Labor Union, which originated in the American section of the First International, had in the Chicago labor movement of the 1880s. They were involved in the nationwide struggle for an eight-hour day. On May 3, 1886, the police killed three striking workers at a demonstration in Chicago. Seven policemen and four workers were killed the following day when someone, possibly a police member, threw a bomb into the crowd. Four anarchists were eventually executed for allegedly conspiring to the events. The Haymarket Affair, as these events become known, led anarchists and labor organizers, including syndicalists, in both the United States and Europe to re-evaluate the revolutionary meaning of the general strike.
According to Émile Pouget, a French anarchist and CGT leader, from "the United States, the idea of the general strike fertilized by the blood of anarchists hanged in Chicago [...] was imported to France". In the 1890s, French anarchists, conceding that individual action such as assassinations had failed, turned their focus to the labor movement. They were able to gain influence, particularly in the "bourses du travail", which served as labor exchanges, meeting places for unions, and trades councils and organized in a national federation in 1893. In 1895, the CGT was formed as a rival to the "bourses", but was at first much weaker. From the start, it advocated the general strike and aimed to unite all workers. Pouget, who was active in the CGT, supported the use of sabotage and direct action. In 1902, the "bourses" merged into the CGT. In 1906, the federation adopted the Charter of Amiens, which reaffirmed the CGT's independence from party politics and fixed the goal of uniting all French workers.

In 1905, the Industrial Workers of the World were formed in the United States by the Western Federation of Miners, the American Labor Union, and a broad coalition of socialists, anarchists, and labor unionists. Its base was mostly in the Western US where labor conflicts were most violent and workers therefore radicalized. Although Wobblies insisted their union was a distinctly American form of labor organization and not an import of European syndicalism, the IWW was syndicalist in the broader sense of the word. According to Melvyn Dubofsky and most other IWW historians, the IWW's industrial unionism was the specifically American form of syndicalism. Nevertheless, the IWW also had a presence in Canada and Mexico nearly from its inception, as the US economy and labor force was intertwined with those countries.
French syndicalism and American industrial unionism influenced the rise of syndicalism elsewhere. Syndicalist movements and organizations in a number of countries were established by activists who had spent time in France. Ervin Szabó visited Paris in 1904 and then established a Syndicalist Propaganda Group in his native Hungary in 1910. Several of the founders of the Spanish CNT had visited France. Alceste de Ambris and Armando Borghi, both leaders in Italy's USI, were in Paris for a few months from 1910 to 1911. French influence also spread through publications. Emile Pouget's pamphlets could be read in Italian, Spanish, Portuguese, English, German, and Swedish translations. Journals and newspapers in a number of countries advocated syndicalism. For example, " L'Action directe", a journal mainly for miners in Charleroi, Belgium, urged its readers to follow "the example of our confederated friends of France". The IWW's newspapers published articles on French syndicalism, particularly the tactic of sabotage and the CGT's "La Vie Ouvrière" carried articles about Britain's labor movement by the British syndicalist Tom Mann. Migration played a key role in spreading syndicalist ideas. The Argentine Regional Workers' Federation ("Federación Obrera Regional Argentina", FORA), openly anarchist by 1905, was formed by Italian and Spanish immigrants in 1901. Many IWW leaders were European immigrants, including Edmondo Rossoni who moved between the United States and Italy and was active in both the IWW and USI. International work processes also contributed to the diffusion of syndicalism. For example, sailors helped establish IWW presences in port cities around the world.

Syndicalists formed different kinds of organizations. Some, like the French radicals, worked within existing unions to infuse them with their revolutionary spirit. Some found existing unions entirely unsuitable and built federations of their own, a strategy known as "dual unionism". American syndicalists formed the IWW, though William Z. Foster later abandoned the IWW after a trip to France and set up the Syndicalist League of North America (SLNA), which sought to radicalize the established American Federation of Labor (AFL). In Ireland, the ITGWU broke away from a more moderate, and British-based, union. In Italy and Spain, syndicalists initially worked within the established union confederations before breaking away and forming USI and the CNT respectively. In Norway, there were both the Norwegian Trade Union Opposition ("Norske Fagopposition", NFO), syndicalists working within the mainstream Norwegian Confederation of Trade Unions ("Landsorganisasjonen i Norge" in Norwegian, LO), and the Norwegian Syndicalist Federation ("Norsk Syndikalistik Federation" in Norwegian, NSF), an independent syndicalist organization set up by the Swedish SAC. In Britain, there was a similar conflict between ISEL and the local IWW organization.

By 1914, there were syndicalist national labor confederations in Peru, Brazil, Argentina, Mexico, the Netherlands, Germany, Sweden, Spain, Italy, and France, while Belgian syndicalists were in the process of forming one. There were also groups advocating syndicalism in Russia, Japan, the United States, Portugal, Norway, Denmark, Hungary, and Great Britain. Outside of North America, the IWW also had organizations in Australia, New Zealand, where it was part of the Federation of Labour (FOL), Great Britain, though its membership had imploded by 1913, and South Africa. In Ireland, syndicalism took the form of the ITGWU, which espoused a mix of industrial unionism and socialist republicanism, and was labeled Larkinism.

Scholars have given several explanations for the emergence of syndicalism. Werner Sombart, a German economist and sociologist, commenting in 1905, ascribes the rise of syndicalism to the Italian and particularly the French mentality. He writes: "The only people who could possibly act up to such a system of teaching are Frenchmen and Italians. They are generally men who do things impulsively [...], who are seized upon by a sudden passionate enthusiasm [...], but they have little application, perseverance, calm or steadiness."

There was a significant uptick in workers' radicalism in most developed capitalist countries from 1911 to 1922, though it relented during World War I. Strikes increased in frequency, numbers of workers involved, and duration. According to van der Linden and Thorpe, syndicalism was only one way this radicalization expressed itself. In the United Kingdom, for example, the period from 1910 to 1914 became known as the Great Labour Unrest. Many historians see syndicalism as a consequence of this unrest, but Elie Halévy and the politician Lord Robert Cecil claim it was its cause. Employers in France likewise blamed an upsurge in workers' militancy in the same period on syndicalist leaders. Syndicalism was further encouraged by employers' hostility to workers' actions. The economist Ernesto Screpanti hypothesized that strike waves such as the one from 1911 to 1922 generally occur during the upper turning-points of the periodic global long cycles of boom and bust known as Kondratieff waves. Such waves of proletarian insurgency, claims Screpanti, were global in reach, saw workers breaking free of the dynamics of the capitalist system, and aimed to overthrow that system.

According to van der Linden and Thorpe, workers' radicalization manifested itself in their rejection of the dominant strategies in the, mostly socialist, labor movement, which was led by reformist trade unions and socialist parties. Lenin posited that "revolutionary syndicalism in many countries was a direct and inevitable result of opportunism, reformism and parliamentary cretinism." A feeling that ideological disputes were draining workers' power led Dutch, French, and American syndicalist organizations to declare themselves independent of any political groups. In countries like Italy, Spain, and Ireland, which was still under British rule, parliamentary politics were not seen as a serious means for workers to express their grievances. Most workers were disenfranchised. Yet even in France or Britain, where most male workers had the right to vote, many workers did not trust party politics. The enormous numerical growth of well-organized socialist parties, such as in Germany and Italy, did not, in the minds of many workers, correlate with any real advance in the class struggle as these parties were thought to be overly concerned with building the parties themselves and with electoral politics than with the class struggle and had therefore lost their original revolutionary edge. The socialists preached the inevitability of socialism, but were in practice bureaucratic and reformist. Similarly, the trade unions frequently allied with those parties, equally growing in numbers, were denounced for their expanding bureaucracies, their centralization, and for failing to represent workers' interests. For example, between 1902 and 1913 the German free trade unions' membership grew by 350% but its bureaucracy by more than 1900%.

Another common explanation for the rise of syndicalism is that it was a result of the economic backwardness of the countries in which it emerged, particularly France. Newer studies have questioned this account. According to van der Linden and Thorpe, changes in labor processes contributed to the radicalization of workers and thereby to the rise of syndicalism. This rise took place during the Second Industrial Revolution. Two groups of workers were most attracted to syndicalism: casual or seasonal laborers who frequently changed jobs, and workers whose occupations were becoming obsolete as a result of technological advances. The first group includes landless agricultural workers, construction workers, and dockers, all of whom were disproportionately represented in several countries' syndicalist movements. Because they frequently changed jobs, such workers did not have close relationships with their employers and the risk of losing one's job as a result of a strike was reduced. Moreover, because of the time constraints of their jobs they were forced to act immediately in order to achieve anything and could not plan for the long term by building up strike funds or powerful labor organizations or by engaging in mediation. Their working conditions gave them an inclination to engage in direct confrontation with employers and apply direct action. The second group includes miners, railway employees, and certain factory workers. Their occupations were deskilled by technological and organizational changes. These changes made workers from the second group similar in some respects to the first group. They did not entirely result from the introduction of new technology, but were also caused by changes in management methods. This included increased supervision of workers, piecework, internal promotions, all designed make workers docile and loyal and to transfer knowledge and control over the process of production from workers to employers. Frustration with this loss of power led to formal and informal resistance by workers. Altena disagrees with this explanation. According to him, it was workers with significant autonomy in their jobs and pride in their skills who were most attracted to syndicalism. Moreover, he argues, explanations based on workers' occupations cannot explain why only a minority of workers in those jobs became syndicalists or why in some professions workers in different locations had vastly different patterns of organization. The small size of many syndicalist unions also makes observations about which workers joined statistically irrelevant.
Syndicalism came to be seen as a viable strategy because the general strike became a practical possibility. Although it had been advocated before, there were not sufficient numbers of wage workers to bring society to a standstill and they had not achieved a sufficient degree of organization and solidarity until the 1890s, according van der Linden and Thorpe. Several general or political strikes then took place before World War: in 1893 and in 1902 in Belgium, in 1902 and in 1909 in Sweden, in 1903 in the Netherlands, in 1904 in Italy in addition to significant work stoppages during the Russian Revolution of 1905.

Darlington cites the significance of the conscious intervention by syndicalist militants. The industrial unrest of the period created conditions which made workers receptive to syndicalist leaders' agitation. They spread their ideas through pamphlets and newspapers and had considerable influence in a number of labor disputes. Finally, van der Linden and Thorpe point to spatial and geographical factors that shaped the rise of syndicalism. Workers who would otherwise not have had an inclination to syndicalism joined because syndicalism was dominant in their locales. Workers in the Canadian and American West for example, were generally more radical and drawn to the IWW and One Big Union than their counterparts in the East. Similarly, southern workers were more drawn to syndicalism in Italy. According to Altena, the emergence of syndicalism must be analyzed at the level of local communities. Only differences in local social and economic structures explain why some towns had a strong syndicalist presence, but others did not.
Syndicalism was not informed by theory or a systematically elaborated ideology the same way socialism was by Marxism. Émile Pouget, a CGT leader, maintained that: "What sets syndicalism apart from the various schools of socialism and makes it superior is its doctrinal sobriety. Inside the unions, there is little philosophising. They do better than that: they act!" Similarly, Andreu Nin of the Spanish CNT proclaimed in 1919: "I am a fanatic of action, of revolution. I believe in actions more than in remote ideologies and abstract questions." Though workers' education was important at least to committed activists, syndicalists distrusted bourgeois intellectuals, wanting to maintain workers' control over the movement. Syndicalist thinking was elaborated in pamphlets, leaflets, speeches, and articles and in the movement's own newspapers. These writings consisted mainly in calls to action and discussions of tactics in class struggle. The philosopher Georges Sorel's "Reflections on Violence" introduced syndicalist ideas to a broader audience. Sorel fancied himself the premier theorist of syndicalism and was frequently thought of as such, but he was not a part of the movement and his influence on syndicalism was insignificant, except in Italy and Poland.

The extent to which syndicalist positions reflected merely the views of leaders and to what extent those positions were shared by syndicalist organizations' rank-and-file is a matter of dispute. The historian Peter Stearns, commenting on French syndicalism, concludes that most workers did not identify with syndicalism's long-range goals and that syndicalist hegemony accounts for the relatively slow growth of the French labor movement as a whole. Workers who joined the syndicalist movement, he claims, were on the whole indifferent to doctrinal questions, their membership in syndicalist organizations was partly accidental and leaders were unable to convert workers to syndicalist ideas. Frederick Ridley, a political scientist, is more equivocal. According to him, leaders were very influential in the drafting of syndicalist ideas, but syndicalism was more than a mere tool of a few leaders, but a genuine product of the French labor movement. Darlington adds that in the Irish ITGWU most members were won over by the union's philosophy of direct action. Bert Altena argues that, though evidence of ordinary workers' convictions is scant, it indicates that they were aware of doctrinal differences between various currents in the labor movement and able to defend their own views. He points out that they likely understood syndicalist newspapers and debated political issues.

"Syndicalism" is used by some interchangeably with "anarcho-syndicalism". This term was first used in 1907, by socialists criticizing the political neutrality of the CGT, although it was rarely used until the early 1920s when communists used it disparagingly. Only from 1922 was it used by self-avowed anarcho-syndicalists. Syndicalism has traditionally been seen as a current within anarchism, but in some countries it was dominated by Marxists rather than anarchists. This was the case in Italy and much of the Anglophone world, including Ireland where anarchists had no significant influence on syndicalism. The extent to which syndicalist doctrine was a product of anarchism is debated. The anarchist Iain McKay argues that "syndicalism" is but a new name for ideas and tactics developed by Bakunin and the anarchist wing of the First International, while it is wholly inconsistent with positions Marx and Engels took. According to him, the fact that many Marxists embraced syndicalism merely indicates that they abandoned Marx's views and converted to Bakunin's. Altena too views syndicalism as part of the broader anarchist movement, but concedes there was a tension between this and the fact that it was also a labor movement. He also sees Marxist ideas reflected in the movement, as leading syndicalists such as F. Domela Nieuwenhuis and Christiaan Cornelissen as well as much of the Australian syndicalist movement were influenced by them, as well as older socialist notions. According to Darlington, anarchism, Marxism, and revolutionary trade unionism equally contributed to syndicalism, in addition to various influences in specific countries, including Blanquism, anti-clericalism, republicanism, and agrarian radicalism.

Bill Haywood, a leading figure in the IWW, defined the union's purpose at its founding congress as "the emancipation of the working class from the slave bondage of capitalism". Syndicalists held that society was divided into two great classes, the working class and the bourgeoisie. Their interests being irreconcilable, they must be in a constant state of class struggle. Tom Mann, a British syndicalist, declared that "the object of the unions is to wage the Class War". This war, according to syndicalist doctrine, was aimed not just at gaining concessions such as higher wages or a shorter working day, but at the revolutionary overthrow of capitalism.

Syndicalists agreed with Karl Marx's characterization of the state as the "executive committee of the ruling class". They held that a society's economic order determined its political order and concluded that the former could not be overthrown by changes to the latter. Nevertheless, a number of leading syndicalist figures worked in political parties and some ran for elected office. Jim Larkin, the leader of the Irish ITGWU, was active in the Labour Party, Haywood in the Socialist Party of America. Yet, they saw the economic sphere as the primary arena for revolutionary struggle, while involvement in politics could at best be an "echo" of industrial struggle. They were skeptical of parliamentary politics. According to Father Thomas Hagerty, a Catholic priest and IWW leader, "dropping pieces of paper into a hole in a box never did achieve emancipation for the working class, and to my thinking it will never achieve it". Syndicalist trade unions declared their political neutrality and autonomy from political parties. Political parties, syndicalists reasoned, grouped people according to their political views, uniting members of different classes. Unions, on the other hand, were to be purely working-class organizations, uniting the entire class, and could therefore not be divided on political grounds. The French syndicalist Pouget explained: "The CGT embraces outside of all the schools of politics all workers cognisant of the struggle to be waged for the elimination of wage-slavery and the employer class." In practice, however, this neutrality was more ambiguous. The CGT, for example, worked with the Socialist Party in the struggle against the Three-Year Law, which extended conscription. During the Spanish Civil War the CNT, whose policy barred anyone who had been a candidate for political office or had participated in political endeavors from representing it, was intimately connected with the Iberian Anarchist Federation ("Federación Anarquista Ibérica", FAI).

In the syndicalist conception, unions played a dual role. They were organs of struggle within capitalism for better working conditions, but they were also to play a key role in the revolution to overthrow capitalism. Victor Griffuelhes expressed this at the CGT's 1906 congress in the following manner: "In its day-to-day demands, syndicalism seeks the co-ordination of workers' efforts, the increase of workers' well-being by the achievement of immediate improvements, such as the reduction of working hours, the increase of wages, etc. But this task is only one aspect of the work of syndicalism; it prepares for complete emancipation, which can be realised only by expropriating the capitalist class". For unions to fulfill this role, it was necessary to prevent bureaucrats "whose sole purpose in life seems to be apologising for and defending the capitalist system of exploitation", according to Larkin from inhibiting workers' militant zeal. Battling bureaucracy and reformism within the labor movement was a major theme for syndicalists. One expression of this was many syndicalists' rejection of collective bargaining agreements, which were thought to force labor peace upon workers and break their solidarity. The Wobblie Vincent St. John declared: "There is but one bargain that the Industrial Workers of the World will make with the employing class complete surrender of the means of production." The Argentine Regional Workers' Federation ("Federación Obrera Regional Argentina", FORA) and the OBU did, however, accept such deals and others began accepting them eventually. Similarly, syndicalist unions did not work to build large strike funds, for fear that they would create bureaucracy separate from the rank-and-file and instill in workers the expectation that the union rather than they would wage the class struggle.
Syndicalists advocated direct action, including working to rule, passive resistance, sabotage, and strikes, particularly the general strike, as tactics in the class struggle, as opposed to indirect action such as electoral politics. The IWW engaged in around 30 mostly successful civil disobedience campaigns they deemed free speech fights. Wobblies would defy laws restricting public speeches, in order to clog up prisons and court systems as a result of hundreds of arrests, ultimately forcing public officials to rescind such laws. Sabotage ranged from slow or inefficient work to destruction of machinery and physical violence. French railway and postal workers cut telegraph and signal lines during strikes in 1909 and 1910.

The final step towards revolution, according to syndicalists, would be a general strike. It would be "the curtain drop on a tired old scene of several centuries, and the curtain raising on another", according to Griffuelhes.

Syndicalists remained vague about the society they envisioned to replace capitalism, claiming that it was impossible to foresee in detail. Labor unions were seen as being the embryo of a new society in addition to being the means of struggle within the old. Syndicalists generally agreed that in a free society production would be managed by workers. The state apparatus would be replaced by the rule of workers' organizations. In such a society individuals would be liberated, both in the economic sphere but also in their private and social lives.

Syndicalist policies on gender issues were mixed. The CNT did not admit women as members until 1918. The CGT dismissed feminism as a bourgeois movement. Syndicalists were mostly indifferent to the question of women's suffrage. Elizabeth Gurley Flynn, an IWW organizer, insisted that women "find their power at the point of production where they work", rather than at the ballot box. Of the 230 delegates present at the founding of Canada's One Big Union, a mere 3 were women. When a female radical criticized the masculinist atmosphere at the meeting, she was rebuffed by men who insisted that labor only concern itself with class rather than gender issues. The historian Todd McCallum concludes that syndicalists in the OBU advocated values of "radical manhood". Francis Shor argues that the "IWW promotion of sabotage represents a kind of masculine posturing which directly challenged the individualizing techniques of power mobilized by industrial capitalism". Thus, "the IWW's masculine identity incorporated features of working-class solidarity and protest [...] through 'virile' syndicalism." For example, while defending a black fellow worker against a racist insult, an IWW organizer in Louisiana insisted that "he is a man, a union man, an IWW—a MAN! ... and he has proven it by his action". During WWI, one of the IWW's anti-war slogans was "Don’t Be a Soldier! Be a Man!" In some case syndicalist attitudes towards women changed. In 1901, the CGT's agricultural union in southern France was hostile to women, but by 1909 this had changed. The CNT, initially hostile to independent women's organizations, worked closely with the libertarian feminist organization "Mujeres Libres" during the Civil War.
According to the historian Sharif Gemie, the male orientation of parts of the syndicalist labor movement reflected the ideas of the anarchist Pierre-Joseph Proudhon, who defended patriarchy because women, of their own accord, are "chained to nature".

Syndicalists were involved in a number strikes, labor disputes, and other struggles. In the United States, the IWW was involved in at least 150 strikes including miners' strikes in Goldfield, Nevada in 1906–1907, a steel workers' strike in McKees Rocks, Pennsylvania in 1909, a textile workers' strike in Lawrence, Massachusetts, timber workers' strikes in Louisiana and Arkansas in 1912–1913, and a silk workers' strike in Paterson, New Jersey. The most prominent was the struggle in Lawrence. Wobblie leaders brought together 23,000 mostly immigrant workers, many of whom did not speak English. They arranged for workers' children to be sent to live with sympathetic families outside of Lawrence for the duration of the strike so their parents could focus on the struggle. Unlike most IWW-led strikes, the struggle was successful. In Mexico, syndicalism first emerged in 1906 during a violent miners' strike in Cananea and an even more violent textile workers' strike in Río Blanco, Veracruz. In 1912, during the 1910–1920 Mexican Revolution, anarchists formed the syndicalist union House of the World Worker ("Casa del Obrero Mundial"). It led a series of successful strikes in 1913 in Mexico City and central Mexico. After the Constitutionalist Army occupied the capital in 1914, syndicalists allied with the government it established to defeat rural forces such as the Zapatistas and therefore received government support. Once those forces had been suppressed, this alliance broke apart and the "Casa" campaigned for workers' control of factories and the nationalization of foreign capital. It contributed to a rise in labor unrest that began in mid-1915. It led general strikes in May and in July–August 1916 in greater Mexico City. The latter was quelled by the army, marking the defeat of the "Casa", which was also suppressed.

In Portugal, the deposition of the King in 1910 was followed by a strike wave throughout the country. After the police occupied the offices of an agricultural union, syndicalists called for a general strike. During the strike, Lisbon was controlled by workers and there were armed uprisings in several other cities. In 1912, the strike wave ebbed off. Italian syndicalists successfully organized agricultural workers in the Po Valley by uniting different parts of agricultural working class. They were most successful in areas where the reformist union Federterra had been thwarted by employers. Syndicalists led large strikes by farm workers in Parma and Ferrara in 1907–1908, but these strikes failed as a result of employers' strikebreaking tactics and infighting among workers. In 1911–1913, syndicalists played an important role in a large strike wave in Italy's industrial centers. The syndicalist union confederation USI was formed in 1912 by veterans of both strike movements.

British Wobblies were involved in two major strikes in Scotland, one at Argyll Motor Works and the second at a Singer's sewing machine factory in Clydebank. In 1906, several industrial unionists began to spread their ideas and organize workers at Singer's. In 1911, they organized a strike after a woman was fired for not working hard enough. The strike was cleverly defeated by management and most activists lost their jobs. The ISEL leader Tom Mann was also at the center of several labor disputes during the Great Labour Unrest, including the 1911 Liverpool general transport strike where he chaired the strike committee. In Ireland, Jim Larkin and the ITGWU led 20,000 during the 1913 Dublin lockout. After the ITGWU attempted to unionize Dublin's trams and tram workers went on strike, the city's employers threatened to fire any workers who did not sign a pledge to not support the ITGWU, thereby turning the dispute into a city-wide conflict in late September. Workers' resistance crumbled in January 1914.
There was no international syndicalist organization prior to World War I. In 1907, CGT activists presented the Charter of Amiens and syndicalism to an international audience a higher form of anarchism at the International Anarchist Congress of Amsterdam in 1907. Discussions at the Congress led to the formation of the international syndicalist journal "Bulletin international du mouvement syndicaliste". The CGT was affiliated with the International Secretariat of National Trade Union Centers (ISNTUC), which brought together reformist socialist unions. Both the Dutch NAS and the British ISEL attempted to remedy the lack of a syndicalist counterpart to ISNTUC in 1913, simultaneously publishing calls for an international syndicalist congress in 1913. The CGT rejected the invitation. Its leaders feared that leaving ISNTUC, which it intended to revolutionize from within, would split the CGT and harm working-class unity. The IWW also did not participate, as it considered itself an international in its own right. The First International Syndicalist Congress was held in London from September 27 to October 2. It was attended by 38 delegates from 65 organizations in Argentina, Austria, Belgium, Brazil, Cuba, France, Germany, Italy, the Netherlands, Poland, Spain, Sweden, and the United Kingdom. Discussions were contentious and did not lead to the founding of a syndicalist international. Delegates did agree on a declaration of principles describing syndicalism's core tenets. They also decided to launch an International Syndicalist Information Bureau and to hold another congress in Amsterdam. This congress did not take place due to the outbreak of World War I.
Syndicalists had long opposed nationalism and militarism. Haywood held that "it is better to be a traitor to your country than to your class". French syndicalists viewed the Army as the primary defender of the capitalist order. In 1901, the CGT published a manual for soldiers encouraging desertion. Similarly, in 1911 British syndicalists distributed an "Open Letter to British Soldiers" imploring them not to shoot on striking workers, but to join the working class's struggle against capital. Patriotism, syndicalists argued, was a means of integrating workers into capitalist society by distracting them from their true class interest. In 1908, the CGT's congress invoked the slogan of the First International, proclaiming that the "workers have no fatherland".
When World War I broke out in July 1914, socialist parties and trade unions both in neutral and belligerent countries supported their respective nations' war efforts or national defense, despite previous pledges to do the opposite. Socialists agreed to put aside class conflict and vote for war credits. German socialists argued that war was necessary to defend against Russia's barbaric Tsarism, while their French counterparts pointed to the need to defend against Prussian militarism and the German "instinct of domination and of discipline". This collaboration between the socialist movement and the state was known as the "union sacrée" in France, the "Burgfrieden" in Germany, and "godsvrede" in the Netherlands. Moreover, a number of anarchists led by Peter Kropotkin, including the influential syndicalist Christiaan Cornelissen, issued the Manifesto of the Sixteen, supporting the Allied cause in the war. Most syndicalists, however, remained true to their internationalist and anti-militarist principles by opposing the war and their respective nation's participation in it.

The majority of the French CGT and a sizable minority in the Italian USI did not. The CGT had long had a moderate, reformist wing, which gained the upper hand. As a result, according to historians like Darlington or van der Linden and Thorpe, the CGT was no longer a revolutionary syndicalist organization after the start of World War I. It followed the French president's call for national unity by agreeing to a no-strike pledge and to resolve labor disputes through arbitration and by actively participating in the French war effort. Most of its members of military age were conscripted without resistance and its ranks shrank from 350,000 in 1913 to 49,000 dues-paying members in 1915. CGT leaders defended this course by arguing that France's war against Germany was a war between democracy and republicanism on the one side and barbaric militarism on the other. Italy did not initially participate in World War I, which was deeply unpopular in the country, when it broke out. The Socialist Party and the reformist General Confederation of Labor opposed Italian intervention in the Great War. Once Italy became a participant, the socialists refused to support the war effort, but also refrained from working against it. From the start of the war, even before Italy did so, a minority within USI, led by the most famous Italian syndicalist, Alceste De Ambris, called on the Italian state to take the Allies' side. The pro-war syndicalists saw Italian participation in the war as the completion of nationhood. They also felt compelled to oppose the socialists' neutrality and therefore support the war. Finally, they gave similar arguments as the French, warning of the dangers posed by the "suffocating imperialism of Germany", and felt obliged to follow the CGT's lead.
USI's pro-war wing had the support of less than a third of the organization's members and it was forced out in September 1914. Its anarchist wing, led by Armando Borghi, was firmly opposed to the war, deeming it incompatible with workers' internationalism and predicting that it would only serve elites and governments. Its opposition was met with government repression and Borghi and others were interned by the end of the war. The anti-war faction in the CGT, on the other hand, was a small minority. It was led by the likes of Pierre Monatte and Alphonse Merrheim. They would link up with anti-war socialists from around Europe at the 1915 Zimmerwald conference. They faced considerable difficulties putting up meaningful resistance against the war. The government called up militants to the Army, including Monatte. He considered refusing the order and being summarily executed, but decided this would be futile. Syndicalist organizations in other countries nearly unanimously opposed the war. "Let Germany win, let France win, it is all the same to the workers," José Negre of the CNT in neutral Spain declared. The CNT insisted that syndicalists could support neither side in an imperialist conflict. A wave of pro-British sentiment swept Ireland during the war, although the ITGWU and the rest of the Irish labor movement opposed it, and half of the ITGWU's membership enlisted in the British military. The ITGWU had also been significantly weakened in 1913 in the Dublin Lockout. After Jim Larkin left Ireland in 1914, James Connolly took over leadership of the union. Because of the organization's weakness, Connolly allied it along with its paramilitary force, the Irish Citizen Army, with the Irish Republican Brotherhood. Together, they instigated the Easter Rising, seeking to weaken the British Empire and hoping that the insurrection would spread throughout Europe. The uprising was quickly quelled by the British army and Connolly was executed. In Germany, the small FVdG opposed the socialists' "Burgfrieden" and Germany's involvement in the war, challenging the claim that the country was waging a defensive war. Its journals were suppressed and a number of its members were arrested. The United States did not enter the war until the spring of 1917. The start of the war had induced an economic boom in the US, tightening the labor market and thereby strengthening workers' bargaining position. The IWW profited from this, more than doubling its membership between 1916 and 1917. At the same time, the Wobblies fervently denounced the war and mulled calling an anti-war general strike. Once America became a combatant, the IWW maintained its anti-war stance, while its bitter rival, the AFL, supported the war. It did not, however, launch an anti-war campaign, as it feared the government would crush it if it did and wanted to focus on its economic struggles. The IWW's practical opposition to the war was limited, 95% of eligible IWW members registered for the draft, and most of those drafted served. Syndicalists in the Netherlands and Sweden, both neutral countries, criticized the truce socialists entered with their governments in order to shore up national defense. The Dutch NAS disowned Cornelissen, one of its founders, for his support for the war.

Syndicalists from Spain, Portugal, Great Britain, France, Brazil, Argentina, Italy, and Cuba met at an anti-war congress in El Ferrol, Spain, in April 1915. The congress was poorly planned and prohibited by the Spanish authorities, but delegates managed to discuss resistance to the war and extending international cooperation between syndicalist groups. Argentine, Brazilian, Spanish, and Portuguese delegates later met in October in Rio de Janeiro to continue discussions and resolved to deepen cooperation between South American syndicalists. While syndicalists were only able to put up a rather limited practical struggle against World War I, they also looked to challenge the war on an ideological or cultural level. They pointed to the horrors of war and spurned efforts to legitimate it as something noble. German syndicalists drew attention to the death, injury, destruction, and misery that the war wrought. German, Swedish, Dutch, and Spanish syndicalists denounced nationalism with "Tierra y Libertad", a syndicalist journal in Barcelona, calling it a "grotesque mentality". The Dutch newspaper "De Arbeid" criticized nationalism, because "it finds its embodiment in the state and is the denial of class antagonism between the haves and the have-nots". German and Spanish syndicalists went further still by putting into question the concept of nationhood itself and dismissing it as a mere social construct. The Germans pointed out that most inhabitants of the German Empire identified not as Germans, but in regional terms as Prussians or Bavarians and the like. Multilingual countries like Germany and Spain also could not claim a common language as a defining characteristic of the nation nor did members of the same nation share the same values or experiences, syndicalists in Spain and Germany argued. Syndicalists also argued against the notion that the war was a clash of different cultures or that it could be justified as a defense of civilization. Various cultures were not mutually hostile, they claimed, and the state should not be seen as the embodiment of culture, since culture was the product of the entire population, while the state acted in the interests of just a few. Moreover, they argued that if culture was to be understood as "high culture", the very workers dying in the war were denied access to that culture by capitalist conditions. Finally, syndicalists railed against religious justifications for war. Before the war, they had rejected religion as divisive at best, but support for the war by both Catholic and Protestant clergy revealed their hypocrisy and disgraced the principles Christianity claimed to uphold, they claimed.

As the war progressed, disaffection with worsening living conditions at home and a growing numbers of casualties at the front eroded the enthusiasm and patriotism the outbreak of war had aroused. Prices were on the rise, food was scarce, and it became increasingly clear that the war would not be short. In Germany, for example, food shortages led to demonstrations and riots in a number of cities in the summer of 1916. At the same time, anti-war demonstrations started. Strikes picked up from around 1916 or 1917 on across Europe and soldiers began to mutiny. Workers distrusted their socialist leaders who had joined the war effort. Thanks in part to their fidelity to internationalism, syndicalist organizations profited from this development and expanded as the war drew to an end.

Disaffection with the war condensed in the post-World War I revolutions that began with the 1917 Russian Revolution. In February 1917, strikes, riots, and troop mutinies broke out in Petrograd, forcing the Russian Tsar Nicholas II to abdicate on March 2 in favor of a provisional government. Immediately, anarchist groups emerged. Russian syndicalists organized around the journal "Golos Truda" ("The Voice of Labor"), which had a circulation of around 25,000, and the Union of Anarcho-Syndicalist Propaganda. Anarchists found themselves agreeing with the Bolsheviks led by Vladimir Lenin, who returned to Russia in April, as both sought to bring down the provisional government. Lenin abandoned the idea that capitalism is a necessary stage on Russia's path to communism; dismissed the establishment of a parliament, favoring that power be taken by soviets; and called for the abolition of the police, the army, the bureaucracy, and finally the state all sentiments syndicalists shared. Although the syndicalists also welcomed the soviets, they were most enthusiastic about the factory committees and workers' councils that had emerged in all industrial centers in the course of strikes and demonstrations in the February Revolution. The committees fought for higher wages and shorter hours, but above all for workers' control over production, which both the syndicalists and Bolsheviks supported. The syndicalists viewed the factory committees as the true form of syndicalist organization, not unions. Because they were better organized, the Bolsheviks were able to gain more traction in the committees with six times as many delegates in a typical factory. Despite the goals they had in common, syndicalists became anxious about the Bolsheviks' growing influence, especially after they won majorities in the Petrograd and Moscow soviets in September.

The Petrograd Soviet established the 66-member Military Revolutionary Committee, which included four anarchists, among them the syndicalist Shatov. On October 25, this committee led the October Revolution; after taking control of the Winter Palace and key points in the capital with little resistance, it proclaimed a Soviet government. Anarchists were jubilant at the toppling of the provisional government. They were concerned about the proclamation of a new government, fearing a dictatorship of the proletariat, even more so after the Bolsheviks created the central Soviet of People's Commissars composed only of members of their party. They called for decentralization of power, but agreed with Lenin's labor program, which endorsed workers' control in all enterprises of a certain minimum size. The introduction of workers' control led to economic chaos. Lenin turned to restoring discipline in the factories and order to the economy in December by putting the economy under state control. At a trade union congress in January, the syndicalists, who had paid little attention to the unions, only had 6 delegates, while the Bolsheviks had 273. No longer depending on their help in toppling the provisional government, the Bolsheviks were now in a position to ignore the syndicalists' opposition and outvoted them at this congress. They opted to disempower local committees by subordinating them to the trade unions, which in turn became organs of the state. The Bolsheviks argued that workers' control did not mean that workers controlled factories at the local level and that this control had to be centralized and put under a broader economic plan. The syndicalists criticized the Bolshevik regime bitterly, characterizing it as state capitalist. They denounced state control over the factories and agitated for decentralization of power in politics and the economy and "syndicalization" of industry. The Civil War against the White Army split anarchists. The syndicalists were criticized harshly, because most supported the Bolshevik regime in the war even as they excoriated Bolshevik policy. They reasoned that a White victory would be worse and that the Whites had to be defeated before a third revolution could topple the Bolsheviks. Yet, syndicalists were harassed and repeatedly arrested by the police, particularly the Cheka, from 1919 on. Their demands had some sway with workers and dissidents within the Bolshevik party and the Bolshevik leadership viewed them as the most dangerous part of the libertarian movement. After the Civil War ended, workers and sailors, including both anarchists and Bolsheviks, rose up in 1921 in Kronstadt, a bastion of radicalism since 1905, against what they saw as the rule of a small number of bureaucrats. Anarchists hailed the rebellion as the start of the third revolution. The government reacted by having anarchists throughout the country arrested, including a number of syndicalist leaders. The Russian syndicalist movement was thereby defeated.

Syndicalists in the West who had opposed World War I reacted gushingly to the Russian Revolution. Though they were still coming to grips with the evolving Bolshevik ideology and despite traditional anarchist suspicions of Marxism, they saw in Russia a revolution that had taken place against parliamentary politics and under the influence of workers' councils. They also, at this point, had only limited knowledge of the reality in Russia. Augustin Souchy, a German anarcho-syndicalist, hailed it "the great passion that swept us all along. In the East, so we believed, the sun of freedom rose." The Spanish CNT declared: "Bolshevism is the name, but the idea is that of all revolutions: economic freedom. [...] Bolshevism is the new life for which we struggle, it is freedom, harmony, justice, it is the life that we want and will enforce in the world." Borghi recalled: "We exulted in its victories. We trembled at its risks. [...] We made a symbol and an altar of its name, its dead, its living and its heroes." He called on Italians to "do as they did in Russia". Indeed, a revolutionary wave, inspired in part by Russia, swept Europe in the following years.

In Germany, strikes and protests against food shortage, mainly by women, escalated and by 1917 had eroded public confidence in the government. The German Emperor was forced to abdicate in November 1918 after sailors' mutinies sparked an insurrectionary movement throughout the country. The syndicalist FVdG, which had just 6,000 members before the war and was almost completely suppressed by the state during the war, regrouped at a conference in Berlin in December 1918. It was active in the revolutionary events of the following years, particularly in the Ruhr area. It supported spontaneous strikes and championed direct action and sabotage. The FVdG started to be held in high regard for its radicalism by workers, particularly miners, who appreciated the syndicalists' ability to theorize their struggles and their experience with direct action methods. Starting in the second half of 1919, workers disappointed by the socialist party's and unions' support for the war and previously non-unionized unskilled workers who were radicalized during the war flocked to the FVdG. The revolution also saw the introduction to Germany of industrial unionism along the lines of the IWW with some support from the American organization, but also with links to the left wing of the Communist Party. In December 1919, the Free Workers' Union of Germany (Syndicalists) ("Freie Arbeiter-Union Deutschlands (Syndikalisten)", FAUD) was formed, claiming to represent over 110,000 workers, more than eighteen times the FVdG's pre-war membership. Most of the new organization came from the FVdG, but industrial unionists, whose influence was dwindling, were also involved. Rudolf Rocker, an anarchist recently returned to Germany after spending several years in London, wrote the FAUD's program.
Class struggle peaked in Italy in the years 1919–1920, which became known as the "biennio rosso" or red biennium. Throughout this wave of labor radicalism, syndicalists, along with anarchists, formed the most consistently revolutionary faction on the left as socialists sought to rein in workers and prevent unrest. The Italian syndicalist movement had split during the war, as the syndicalist supporters of Italian intervention left USI. The interventionists, led by Alceste de Ambris and Edmondo Rossoni, formed the Italian Union of Labor ("Unione Italiana del Lavoro", UIL) in 1918. The UIL's national syndicalism emphasized workers' love of labor, self-sacrifice, and the nation rather than anti-capitalist class struggle. Both USI and the UIL grew significantly during the "biennio rosso". The first factory occupation of the "biennio" was carried out by the UIL at a steel plant in Dalmine in February 1919, before the military put an end to it. In July, a strike movement spread through Italy, culminating in a general strike on July 20. While USI supported it and was convinced by the workers' enthusiasm that revolution could be possible, the UIL and the socialists were opposed. The socialists succeeded in curtailing the general strike and it imploded with a day. The government, unsettled by the radicalism on display, reacted with repression against the far left and concessions to workers and peasants.

In Portugal, working class unrest picked up from the start of the war. In 1917, radicals began to dominate the labor movement as a result of the war, the dictatorship established that year, and the influence of the Russian Revolution. In November 1918, a general strike was called but failed and in 1919 the syndicalist General Confederation of Labour ("Confederação Geral do Trabalho", CGT) was formed as the country's first national union confederation.
In Brazil, in both Rio de Janeiro and São Paulo, syndicalists, along with anarchists and socialists, were leaders in a cycle of labor struggles from 1917 to 1919. It included a general strike in 1917, a failed uprising in 1918 inspired by the Russian Revolution, and a number of smaller strikes. The movement was put down by increased organization by employers to resist workers' demands and by government repression, including the closure of unions, arrests, deportations of foreign militants, and violence, with some 200 workers killed in São Paulo alone. In Argentina, FORA had split into the anarcho-communist FORA V and the syndicalist FORA IX. While FORA V called for a futile general strike in 1915, FORA IX was more careful. It called off general strikes it had planned in 1917 and 1918. In January 1919, five workers were by the authorities during a strike led by a union with tenuous links to FORA V. At the funeral, police killed another 39 workers. Both FORA organizations called for a general strike, which continued after FORA IX reached a settlement. Vigilantes, supported by business and the military, attacked unions and militants. In all, between 100 and 700 people died in what became known as the Tragic Week. Nevertheless, strikes continued to increase and both FORA V and IX grew.

The United States underwent an increase in labor militancy during the post-war period. 1919 saw a general strike in Seattle, large miners' strikes, a police strike in Boston, and a nationwide steel strike. The IWW, however, had been nearly destroyed in the previous two years by local criminal syndicalism laws, the federal government, and vigilante violence. It attempted to take credit for some of the strikes, but in reality was too weak to play a significant role. The post-war Red Scare intensified the attacks on the IWW and by the end of 1919 the IWW was practically powerless. In 1919 Canada was hit by a labor revolt, leading to the formation of One Big Union, which was only partly industrial unionist.
Though the Bolsheviks suppressed syndicalism in Russia, they courted syndicalists abroad as part of their international strategy. In March 1919, the Comintern or Third International was founded at a conference in Moscow. The Bolsheviks acknowledged syndicalism's opposition to socialist reformism and saw them as part of the revolutionary wing of the labor movement. No syndicalists attended the founding convention, mainly because the blockade of Russia by the Allies powers made travel to Moscow near impossible. After long discussions, the CNT opted to join the Comintern, though it classified its adherence as provisional as a concession to detractors of Bolshevism. USI also decided to join, though some like Borghi had reservations about the course of events in Russia. In France, the CGT's radical minority that had opposed the war enthusiastically supported Bolshevism. They formed the Revolutionary Syndicalist Committees and attempted to push the CGT as a whole to support the Comintern. The General Executive Board of the IWW decided join the Comintern, but this decision was never confirmed by a convention. German and Swedish syndicalists were more critical of Bolshevism from the start. Rocker declared already in August 1918 that the Bolshevik regime was "but a new system of tyranny".

Syndicalists became more estranged from the Comintern in 1920. The second congress of the Comintern in the summer of 1920 was attended by numerous syndicalists. The Italian USI, the Spanish CNT, the British shop stewards, and the revolutionary minority of the CGT had official representatives, but others like John Reed of the American IWW, Augustin Souchy of the German FAUD, and the Japanese Wobbly Taro Yoshiharo also attended in an unofficial capacity. This was the first major international gathering of syndicalists since the end of the war. Western syndicalists' knowledge of the facts on the ground in Russia was at this point rather limited. They thought of the soviets as organs of workers' control over production and the Bolsheviks depicted them as such. Syndicalists were not aware of the extent to which they were in reality subordinated to the communist party. The congress, however, revealed the irreconcilable differences between the syndicalist and the Bolshevik approach. Before the congress, the Comintern's Executive Committee arranged discussions with syndicalists to challenge the reformist International Federation of Trade Unions (IFTU). A document proposed by Alexander Lozovsky derided the apolitical unions as "lackeys of imperialist capitalism" for their betrayal during the war, to which syndicalists replied that of the syndicalist unions this only applied to the CGT. Throughout the preliminary meetings, syndicalists clashed with other delegates on the questions of the dictatorship of the proletariat and the conquest of state power as well as on relations with communists and the Comintern. Eventually all syndicalists agreed to the formation of a council tasked with spreading revolutionizing the trade union movement. Disagreements continued at the congress itself.

The International Workers' Association, formed in 1922, is an international syndicalist federation of various labour unions from different countries. At its peak, it represented millions of workers and competed directly for the hearts and minds of the working class with social democratic unions and parties.

From the early 1920s, syndicalist movements in most countries began to wane. State repression played a significant role, but movements that were not suppressed also declined. Faced with this decline, syndicalist organizations had three choices: They could stay true to their revolutionary principles and be marginalized. They could give up those principles in order to adapt to new conditions. Finally, they could disband or merge into non-syndicalist organizations. The IWW is an example of the first case. The French CGT, which according to van der Linden and Thorpe was no longer syndicalist after 1914, went the second route. By the end of the 1930s, meaningful legal syndicalist organizations existed only in Bolivia, Chile, Sweden and Uruguay.

Georges Sorel played a role in shaping the views of Benito Mussolini and by extension the wider Italian fascist movement. In March 1921, Sorel wrote that Mussolini was "a man no less extraordinary than Lenin". After Sorel's death in 1922, Agostino Lanzillo, a one-time syndicalist leader who had become a fascist, wrote in the Italian fascist review "Gerarchia", which was edited by Mussolini: "Perhaps fascism may have the good fortune to fulfill a mission that is the implicit aspiration of the whole oeuvre of the master of syndicalism: to tear away the proletariat from the domination of the Socialist party, to reconstitute it on the basis of spiritual liberty, and to animate it with the breath of creative violence. This would be the true revolution that would mold the forms of the Italy of tomorrow". This movement has been often called Fascist syndicalism.

Syndicalists were involved in the resistance against fascism in several countries. In Germany, the FAUD had already been reduced to a small organization, with a membership of just over 4000 in 1932. Augustin Souchy had urged his comrades to prepare for illegality and the FAUD congress in 1932 had made plans for this. When the Nazis took power in January 1933, most local groups preemptively dissolved and hid their money and other resources to use them in their illegal work. On March 9, shortly after the Reichstag fire, the FAUD's headquarters in Berlin were searched by the police and ten people were arrested. As the SS and SA rounded up opponents of Nazism, so too many anarcho-syndicalists were put in prisons, concentration camps, and torture chambers. Syndicalists distributed a number of newspapers, pamphlets, and leaflets, some smuggled from the Netherlands and Czechoslovakia, some printed in Germany. They passed information on the situation in Germany to their fellow syndicalists abroad. They organized clandestine meetings to coordinate their activities and build an underground resistance network. Illegal syndicalist activity peaked in 1934, but by late 1934 the Gestapo started to infiltrate the underground organization and another round of arrests began. Although the start of the Spanish Civil War in 1936 briefly revitalized syndicalist activity, the syndicalist network was ultimately crushed by the Gestapo by 1937 or 1938. Most syndicalists who had not been arrested gave up at this point. Several dozen German syndicalists went into exile and some ended up in Barcelona, working for the CNT and fighting in the Spanish Civil War. In France, too, many anarchists and syndicalists were involved in the Resistance. For instance, the anarchist Georges Gourdin, an activist in the CGT's Technicians' Federation, organized links between anarchists and aided them and other refugees in escaping the Gestapo. He was arrested by the Gestapo in 1944, tortured without giving up any information, and died at a camp near Nordhausen. The best known French anarchist resister was the pacifist Jean-René Saulière. He organized an anarchist resistance group which included the exiled Russian syndicalist Volin. The same day Toulouse was liberated in August 1944, a leaflet titled "Manifesto of the Anarcho-Syndicalist Libertarian Groups" was distributed by Saulière's network throughout the city.

In Poland, syndicalists were among the first to organize resistance against Nazism. In October 1939, they formed Union of Polish Syndicalists (ZSP) with 2,000–4,000 members. It published newspapers, but also had fighting units in the resistance. In 1942, it joined the Home Army (AK) led by the Polish government-in-exile. Syndicalists also formed the "Freedom" Syndicalist Organisation (SOW), which comprised several hundred activists and also had combatant units. The ZSP and the SOW were involved in the Warsaw Uprising of 1944. They formed a company consisting of several hundred soldiers who wore red and black bands and hung red and black flags on the building they captured.
The anarcho-syndicalist revolution during the Spanish Civil War resulted in the widespread implementation of anarchist and more broadly socialist organisational principles throughout various portions of the country for two to three years, primarily Catalonia, Aragon, Andalusia and parts of the Levante, with the main organisation being the Confederación Nacional del Trabajo. Much of Spain's economy was put under worker control—in anarchist strongholds like Catalonia, the figure was as high as 75%.
On the other side, there was a national syndicalist thread represented originally by the Juntas de Ofensiva Nacional-Sindicalista of Onésimo Redondo and Ramiro Ledesma, inspired by Georges Sorel and Action Française, and primarily based amongst students in Madrid and workers and peasants in and around Valladolid. Ledesma failed to win approval for his ideas from the CNT in 1931, and instead merged into the Falange, creating the "Central Obrera Nacional-Sindicalista" in 1934. After the nationalist victory in the civil war, a corporatist and vertical Spanish Labour Organization became the sole legal trade union in Francoist Spain.

Syndicalism's decline was the result of a number of factors. In Russia, Italy, Portugal, Germany, Spain, and the Netherlands, syndicalist movements were suppressed by authoritarian governments. The IWW in the United States and the Mexican House of the World Worker were weakened considerably by state repression. Syndicalist movements that were not suppressed also declined. According to van der Linden and Thorpe, this was primarily the result of the integration of the working class into capitalist relations. Proletarian families became units of individualized consumption as standards of living increased. This was partly the result of state intervention, particularly the emergence of the welfare state. Avenues for social reform opened up and the franchise was widened, giving parliamentary reformism legitimacy. Altena agrees that the state's growing influence in society was decisive for syndicalism's diminished influence. In addition to the welfare state, he refers to the increased significance of national policies, which eroded local autonomy. This made centralized unions able to negotiate national agreements more important and national and parliamentary politics more enticing for workers. They therefore turned to social democracy in larger numbers. Additionally, according to Altena syndicalism lost out to sports and entertainment in the cultural sphere.

Vadim Dam'e adds to this that the development of capitalist production and changes in the division of labor diminished syndicalism's recruitment base. According to authors like Stearns, Edward Shorter, Charles Tilly, and Bob Holton, who deem syndicalism a transitional form of workers' resistance between older craft-based artisanship and modern factory-based industry, syndicalism's decline was a product of that transition having been completed and workers being assimilated to capitalist factory discipline. Darlington counters that syndicalism attracted a variety of workers, not just artisans and skilled workers, but concedes that such changes did play a role in Spain, France, and some other countries.

Several authors claim that syndicalism's demise was the result of workers' inherent pragmatism or conservatism, causing them to only be interested in immediate material gains, rather than long-term goals like overthrowing capitalism. Robert Hoxie, Selig Perlman, and Patrick Renshaw invoke this argument to explain the IWW's decline and Stearns, Dermot Keogh, and G. D. H. Cole do so with respect to French, Irish, and British syndicalism, respectively. Darlington disputes the assumption that workers are incapable of developing a revolutionary consciousness. Seeking material gains is not incompatible, he claims, with developing class consciousness, which entails the awareness that workers' material interests conflict with capitalism, particularly in times of crisis.

According to many Marxists, syndicalism was a reaction to reformism in the labor movement and could not survive without it. The collapse of reformism after the war therefore automatically weakened syndicalism. According Eric Hobsbawm, the biggest reason for syndicalism's decline, however, was the rise of communism. Several communist parties drew their cadres from the syndicalists' ranks. To radical workers, the programmatic distinctions between syndicalism and communism were not all that relevant. The key is that after the war communism represented militancy or revolutionary attitude as such. Darlington, too, sees the effects of the Russian Revolution as an important reason for the decline of syndicalism. The emergence of communism highlighted syndicalism's inherent weaknesses: the contradiction of building organizations that sought to be both revolutionary cadre organizations and mass labor unions, the emphasis on economic struggle to the detriment of political action and the commitment to localism limiting its ability to provide an effective centralized organization and leadership. Bolshevism's overcoming of these limitations and its success in Russia drew syndicalist leaders and members. It also exacerbated splits within the syndicalist camp.

The Nationalist victory in the Spanish Civil War put an end to syndicalism as a mass movement. Immediately after World War II, there were attempts to rekindle anarcho-syndicalism in Germany, but they were thwarted by Cold War anti-communism, Stalinism, and a failure to attract newer younger activists. Syndicalists maintained some influence in Latin American labor movements into the 1970s. The protest movements of the late 1960s saw renewed interest in syndicalism by activists in Germany, the US, and Britain. During its Hot Autumn of 1969, Italy experienced labor actions reminiscent of syndicalism, but syndicalists did not actually exert any influence, according to Carl Levy. In the 1980s, in communist Poland, the trade union Solidarity ("Solidarność"), though not strictly syndicalist, attracted masses of dissident workers by reviving many syndicalist ideas and practices.

The IWA exists to this day, but with very little influence. At most, it is a "flicker of history, the custodian of doctrine" according to Wayne Thorpe. Among its member organizations is the British Solidarity Federation, which was formed in 1994, but has roots going back to 1950. The German Free Workers' Union ("Freie Arbeiterinnen- und Arbeiter-Union", FAU) was formed to carry on the FAUD's tradition in 1977, but has a membership of just 350 as of 2011. It left the IWA in 2018 to form the International Confederation of Labor (ICL). Spain has several syndicalist federations, including the CNT, which has around 50,000 members as of 2018. It, too, was a member of the IWA until 2018, when it joined the FAU in forming the ICT. After being defeated in the Civil War, tens of thousands of CNT militants went into exile, mostly in France. In exile, the organization atrophied, with just 5,000 mostly older members by 1960. During Spain's transition to democracy, the CNT was revived with a peak membership of over 300,000 in 1978. However, it was soon weakened, first by accusations of having been involved in the bombing of a nightclub, then by a schism. Members who favored participation in state-sponsored union elections left and formed an organization they would eventually name the General Confederation of Labor ("Confederación General del Trabajo", CGT). Despite these concessions, the CGT still views itself as an anarcho-syndicalist organization and has around 100,000 members as of 2018.

According to Darlington, syndicalism left a legacy that was widely admired by labor and political activists in a number of countries. For example, the IWW song "Solidarity Forever" became part of the American labor movement's canon. The strike wave, including the recruitment of unskilled and foreign-born workers by the Congress of Industrial Organizations, that swept the United States in the 1930s followed in the IWW's footsteps. The tactic of the sit-down strike, made famous by the United Auto Workers in the Flint sit-down strike, was pioneered by Wobblies in 1906.

In his study of French syndicalism, Stearns concludes that it was a dismal failure. The radicalism of syndicalist labor leaders, he claims, shocked French workers and the government and thereby weakened the labor movement as a whole. Syndicalism was most popular among workers not yet fully integrated into modern capitalist industry, but most French workers had adapted to this system and accepted it. Therefore, syndicalism was not able to seriously challenge prevailing conditions or even scare politicians and employers.



</doc>
<doc id="26779" url="https://en.wikipedia.org/wiki?curid=26779" title="Soviet Union">
Soviet Union

The Soviet Union, officially the Union of Soviet Socialist Republics (USSR), was a federal socialist state in Northern Eurasia that existed from 1922 to 1991. Nominally a union of multiple national Soviet republics, in practice its government and economy were highly centralized until its final years. It was a one-party state governed by the Communist Party, with Moscow as its capital in its largest republic, the Russian SFSR. Other major urban centers were Leningrad (Russian SFSR), Kiev (Ukraine SSR), Minsk (Belarus SSR), Tashkent (Uzbek SSR), Alma-Ata (Kazakh SSR) and Novosibirsk (Russian SFSR). It was the largest country in the world by surface area, spanning over 10,000 kilometers (6,200 mi) east to west across 11 time zones and over 7,200 kilometers (4,500 mi) north to south. Its territory included much of Eastern Europe as well as part of Northern Europe and all of Northern and Central Asia. It had five climate zones such as tundra, taiga, steppes, desert, and mountains. Its diverse population was collectively known as Soviet people.

The Soviet Union had its roots in the October Revolution of 1917 when the Bolsheviks, headed by Vladimir Lenin, overthrew the Provisional Government that had earlier replaced the monarchy. They established the Russian Soviet Republic, beginning a civil war between the Bolshevik Red Army and many anti-Bolshevik forces across the former Empire, among whom the largest faction was the White Guard. The disastrous distractive effect of the war and the Bolshevik policies led to 5 million deaths during the 1921–1922 famine in the region of Povolzhye. The Red Army expanded and helped local Communists take power, establishing soviets, repressing their political opponents and rebellious peasants through the policies of Red Terror and War Communism. In 1922, the Communists were victorious, forming the Soviet Union with the unification of the Russian, Transcaucasian, Ukrainian and Byelorussian republics. The New Economic Policy (NEP) which was introduced by Lenin led to a partial return of a free market and private property, resulting in a period of economic recovery.

Following Lenin's death in 1924, a troika and a brief power struggle, Joseph Stalin came to power in the mid-1920s. Stalin suppressed all political opposition to his rule inside the Communist Party, committed the state ideology to Marxism–Leninism and ended the NEP, initiating a centrally planned economy. As a result, the country underwent a period of rapid industrialization and forced collectivization, which led to a significant economic growth, but also led to a man-made famine in 1932–1933 and expanded the Gulag labour camp system founded back in 1918. Stalin also fomented political paranoia and conducted the Great Purge to remove opponents of his from the Party through the mass arbitrary arrest of many people (military leaders, Communist Party members and ordinary citizens alike) who were then sent to correctional labor camps or sentenced to death.

On 23 August 1939, after unsuccessful efforts to form an anti-fascist alliance with Western powers, the Soviets signed the non-aggression agreement with Nazi Germany. After the start of World War II, the formally neutral Soviets invaded and annexed territories of several Eastern European states, including eastern Poland and the Baltic states. In June 1941 the Germans invaded, opening the largest and bloodiest theater of war in history. Soviet war casualties accounted for the highest proportion of the conflict in the cost of acquiring the upper hand over Axis forces at intense battles such as Stalingrad. Soviet forces eventually captured Berlin and won World War II in Europe on 9 May 1945. The territory overtaken by the Red Army became satellite states of the Eastern Bloc. The Cold War emerged in 1947 as a result of a post-war Soviet dominance in Eastern Europe, where the Eastern Bloc confronted the Western Bloc that united in the North Atlantic Treaty Organization in 1949.

Following Stalin's death in 1953, a period known as de-Stalinization and Khrushchev Thaw occurred under the leadership of Nikita Khrushchev. The country developed rapidly, as millions of peasants were moved into industrialized cities. The USSR took an early lead in the Space Race with the first ever satellite and the first human spaceflight. In the 1970s, there was a brief "détente" of relations with the United States, but tensions resumed when the Soviet Union deployed troops in Afghanistan in 1979. The war drained economic resources and was matched by an escalation of American military aid to Mujahideen fighters.

In the mid-1980s, the last Soviet leader, Mikhail Gorbachev, sought to further reform and liberalize the economy through his policies of "glasnost" and "perestroika". The goal was to preserve the Communist Party while reversing economic stagnation. The Cold War ended during his tenure, and in 1989 Soviet satellite countries in Eastern Europe overthrew their respective communist regimes. This led to the rise of strong nationalist and separatist movements inside the USSR as well. Central authorities initiated a referendum—boycotted by the Baltic republics, Armenia, Georgia, and Moldova—which resulted in the majority of participating citizens voting in favor of preserving the Union as a renewed federation. In August 1991, a coup d'état was attempted by Communist Party hardliners. It failed, with Russian President Boris Yeltsin playing a high-profile role in facing down the coup, resulting in the banning of the Communist Party. On 25 December 1991, Gorbachev resigned and the remaining twelve constituent republics emerged from the dissolution of the Soviet Union as independent post-Soviet states. The Russian Federation (formerly the Russian SFSR) assumed the Soviet Union's rights and obligations and is recognized as its continued legal personality.

The USSR produced many significant social and technological achievements and innovations of the 20th century, including the world's first ministry of health, first human-made satellite, the first humans in space and the first probe to land on another planet, Venus. The country had the world's second-largest economy and the largest standing military in the world. The USSR was recognized as one of the five nuclear weapons states. It was a founding permanent member of the United Nations Security Council as well as a member of the Organization for Security and Co-operation in Europe, the World Federation of Trade Unions and the leading member of the Council for Mutual Economic Assistance and the Warsaw Pact.

The word "soviet" is derived from the Russian word "sovet" (), meaning "council", "assembly", "advice", "harmony", "concord", ultimately deriving from the proto-Slavic verbal stem of "vět-iti" ("to inform"), related to Slavic "věst" ("news"), English "wise", the root in "ad-vis-or" (which came to English through French), or the Dutch "weten" ("to know"; cf. "wetenschap" meaning "science"). The word "sovietnik" means "councillor".

Some organizations in Russian history were called "council" (). In the Russian Empire, the State Council which functioned from 1810 to 1917 was referred to as a Council of Ministers after the revolt of 1905.

During the Georgian Affair, Vladimir Lenin envisioned an expression of Great Russian ethnic chauvinism by Joseph Stalin and his supporters, calling for these nation-states to join Russia as semi-independent parts of a greater union which he initially named as the Union of Soviet Republics of Europe and Asia (). Stalin initially resisted the proposal but ultimately accepted it, although with Lenin's agreement changed the name to the Union of Soviet Socialist Republics (USSR), albeit all the republics began as "socialist soviet" and did not change to the other order until 1936. In addition, in the national languages of several republics, the word "council" or "conciliar" in the respective language was only quite late changed to an adaptation of the Russian "soviet" and never in others, e.g. Ukraine.

"СССР" (in Latin alphabet: "SSSR") is the abbreviation of USSR in Russian. It is written in Cyrillic alphabets. The Soviets used the Cyrillic abbreviation so frequently that audiences worldwide became familiar with its meaning. Notably, both Cyrillic letters used have orthographically-similar (but transliterally distinct) letters in Latin alphabets. Because of widespread familiarity with the Cyrillic abbreviation, Latin alphabet users in particular almost always use the orthographically-similar Latin letters "C" and "P" (as opposed to the transliteral Latin letters "S" and "R") when rendering the USSR's native abbreviation.

After "СССР", the most common short form names for the Soviet state in Russian were "Советский Союз" (transliteration: "Sovetskiy Soyuz") which literally means "Soviet Union", and also "Союз ССР" (transliteration: "Soyuz SSR") which, after compensating for grammatical differences, essentially translates to "Union of SSR's" in English.

In the English language media, the state was referred to as the Soviet Union or the USSR. In other European languages, the locally translated short forms and abbreviations are usually used such as "Union soviétique" and "URSS" in French, or "Sowjetunion" and "UdSSR" in German. In the English-speaking world, the Soviet Union was also informally called Russia and its citizens Russians, although that was technically incorrect since Russia was only one of the republics. Such misapplications of the linguistic equivalents to the term "Russia" and its derivatives were frequent in other languages as well.

With an area of , the Soviet Union was the world's largest country, a status that is retained by the Russian Federation. Covering a sixth of Earth's land surface, its size was comparable to that of North America. Two other successor states, Kazakhstan and Ukraine, rank among the top 10 countries by land area, and the largest country entirely in Europe, respectively. The European portion accounted for a quarter of the country's area and was the cultural and economic center. The eastern part in Asia extended to the Pacific Ocean to the east and Afghanistan to the south, and, except some areas in Central Asia, was much less populous. It spanned over east to west across 11 time zones, and over north to south. It had five climate zones: tundra, taiga, steppes, desert and mountains.

The USSR had the world's longest border, like Russia, measuring over , or circumferences of Earth. Two-thirds of it was a coastline. Across the Bering Strait was the United States. The country bordered Afghanistan, China, Czechoslovakia, Finland, Hungary, Iran, Mongolia, North Korea, Norway, Poland, Romania, and Turkey from 1945 to 1991.

The country's highest mountain was Communism Peak (now Ismoil Somoni Peak) in Tajikistan, at . The USSR also included most of the world's largest lakes; the Caspian Sea (shared with Iran), and Lake Baikal, the world's largest (by volume) and deepest freshwater lake that is also an internal body of water in Russia.

Modern revolutionary activity in the Russian Empire began with the 1825 Decembrist revolt. Although serfdom was abolished in 1861, it was done on terms unfavorable to the peasants and served to encourage revolutionaries. A parliament—the State Duma—was established in 1906 after the Russian Revolution of 1905, but Tsar Nicholas II resisted attempts to move from absolute to a constitutional monarchy. Social unrest continued and was aggravated during World War I by military defeat and food shortages in major cities.

A spontaneous popular uprising in Petrograd, in response to the wartime decay of Russia's economy and morale, culminated in the February Revolution and the toppling of Nicholas II and the imperial government in March 1917. The tsarist autocracy was replaced by the Russian Provisional Government, which intended to conduct elections to the Russian Constituent Assembly and to continue fighting on the side of the Entente in World War I.

At the same time, workers' councils, known in Russian as "Soviets", sprang up across the country. The Bolsheviks, led by Vladimir Lenin, pushed for socialist revolution in the Soviets and on the streets. On 7 November 1917, the Red Guards stormed the Winter Palace in Petrograd, ending the rule of the Provisional Government and leaving all political power to the Soviets. This event would later be officially known in Soviet bibliographies as the Great October Socialist Revolution. In December, the Bolsheviks signed an armistice with the Central Powers, though by February 1918, fighting had resumed. In March, the Soviets ended involvement in the war and signed the Treaty of Brest-Litovsk.

A long and bloody Civil War ensued between the Reds and the Whites, starting in 1917 and ending in 1923 with the Reds' victory. It included foreign intervention, the execution of the former tsar and his family, and the famine of 1921, which killed about five million people. In March 1921, during a related conflict with Poland, the Peace of Riga was signed, splitting disputed territories in Belarus and Ukraine between the Republic of Poland and Soviet Russia. Soviet Russia had to resolve similar conflicts with the newly established republics of Estonia, Finland, Latvia, and Lithuania.

On 28 December 1922, a conference of plenipotentiary delegations from the Russian SFSR, the Transcaucasian SFSR, the Ukrainian SSR and the Byelorussian SSR approved the Treaty on the Creation of the USSR and the Declaration of the Creation of the USSR, forming the Union of Soviet Socialist Republics. These two documents were confirmed by the first Congress of Soviets of the USSR and signed by the heads of the delegations, Mikhail Kalinin, Mikhail Tskhakaya, Mikhail Frunze, Grigory Petrovsky, and Alexander Chervyakov, on 30 December 1922. The formal proclamation was made from the stage of the Bolshoi Theatre.

An intensive restructuring of the economy, industry and politics of the country began in the early days of Soviet power in 1917. A large part of this was done according to the Bolshevik Initial Decrees, government documents signed by Vladimir Lenin. One of the most prominent breakthroughs was the GOELRO plan, which envisioned a major restructuring of the Soviet economy based on total electrification of the country. The plan became the prototype for subsequent Five-Year Plans and was fulfilled by 1931. After the economic policy of "War communism" during the Russian Civil War, as a prelude to fully developing socialism in the country, the Soviet government permitted some private enterprise to coexist alongside nationalized industry in the 1920s, and total food requisition in the countryside was replaced by a food tax.

From its creation, the government in the Soviet Union was based on the one-party rule of the Communist Party (Bolsheviks). The stated purpose was to prevent the return of capitalist exploitation, and that the principles of democratic centralism would be the most effective in representing the people's will in a practical manner. The debate over the future of the economy provided the background for a power struggle in the years after Lenin's death in 1924. Initially, Lenin was to be replaced by a "troika" consisting of Grigory Zinoviev of the Ukrainian SSR, Lev Kamenev of the Russian SFSR, and Joseph Stalin of the Transcaucasian SFSR.

On 1 February 1924, the USSR was recognized by the United Kingdom. The same year, a Soviet Constitution was approved, legitimizing the December 1922 union. Despite the foundation of the Soviet state as a federative entity of many constituent republics, each with its own political and administrative entities, the term "Soviet Russia"strictly applicable only to the Russian Federative Socialist Republicwas often applied to the entire country by non-Soviet writers and politicians.

On 3 April 1922, Stalin was named the General Secretary of the Communist Party of the Soviet Union. Lenin had appointed Stalin the head of the Workers' and Peasants' Inspectorate, which gave Stalin considerable power. By gradually consolidating his influence and isolating and outmanoeuvring his rivals within the party, Stalin became the undisputed leader of the country and, by the end of the 1920s, established a totalitarian rule. In October 1927, Zinoviev and Leon Trotsky were expelled from the Central Committee and forced into exile.

In 1928, Stalin introduced the first five-year plan for building a socialist economy. In place of the internationalism expressed by Lenin throughout the Revolution, it aimed to build Socialism in One Country. In industry, the state assumed control over all existing enterprises and undertook an intensive program of industrialization. In agriculture, rather than adhering to the "lead by example" policy advocated by Lenin, forced collectivization of farms was implemented all over the country.

Famines ensued as a result, causing deaths estimated at three to seven million; surviving kulaks were persecuted, and many were sent to Gulags to do forced labor. Social upheaval continued in the mid-1930s. Despite the turmoil of the mid-to-late 1930s, the country developed a robust industrial economy in the years preceding World War II.

Closer cooperation between the USSR and the West developed in the early 1930s. From 1932 to 1934, the country participated in the World Disarmament Conference. In 1933, diplomatic relations between the United States and the USSR were established when in November, the newly elected President of the United States, Franklin D. Roosevelt, chose to recognize Stalin's Communist government formally and negotiated a new trade agreement between the two countries. In September 1934, the country joined the League of Nations. After the Spanish Civil War broke out in 1936, the USSR actively supported the Republican forces against the Nationalists, who were supported by Fascist Italy and Nazi Germany.

In December 1936, Stalin unveiled a new constitution that was praised by supporters around the world as the most democratic constitution imaginable, though there was some skepticism. Stalin's Great Purge resulted in the detainment or execution of many "Old Bolsheviks" who had participated in the October Revolution with Lenin. According to declassified Soviet archives, the NKVD arrested more than one and a half million people in 1937 and 1938, of whom 681,692 were shot. Over those two years, there were an average of over one thousand executions a day.

In 1939, the Soviet Union made a dramatic shift toward Nazi Germany. Almost a year after Britain and France had concluded the Munich Agreement with Germany, the Soviet Union made agreements with Germany as well, both militarily and economically during extensive talks. The two countries concluded the Molotov–Ribbentrop Pact and the German–Soviet Commercial Agreement in August 1939. The former made possible the Soviet occupation of Lithuania, Latvia, Estonia, Bessarabia, northern Bukovina, and eastern Poland. In late November, unable to coerce the Republic of Finland by diplomatic means into moving its border back from Leningrad, Stalin ordered the invasion of Finland. In the east, the Soviet military won several decisive victories during border clashes with the Empire of Japan in 1938 and 1939. However, in April 1941, the USSR signed the Soviet–Japanese Neutrality Pact with Japan, recognizing the territorial integrity of Manchukuo, a Japanese puppet state.

Germany broke the Molotov–Ribbentrop Pact and invaded the Soviet Union on 22 June 1941 starting what was known in the USSR as the Great Patriotic War. The Red Army stopped the seemingly invincible German Army at the Battle of Moscow, aided by an unusually harsh winter. The Battle of Stalingrad, which lasted from late 1942 to early 1943, dealt a severe blow to Germany from which they never fully recovered and became a turning point in the war. After Stalingrad, Soviet forces drove through Eastern Europe to Berlin before Germany surrendered in 1945. The German Army suffered 80% of its military deaths in the Eastern Front. Harry Hopkins, a close foreign policy advisor to Franklin D. Roosevelt, spoke on 10 August 1943 of the USSR's decisive role in the war.
In the same year, the USSR, in fulfilment of its agreement with the Allies at the Yalta Conference, denounced the Soviet–Japanese Neutrality Pact in April 1945 and invaded Manchukuo and other Japan-controlled territories on 9 August 1945. This conflict ended with a decisive Soviet victory, contributing to the unconditional surrender of Japan and the end of World War II.

The USSR suffered greatly in the war, losing around 27 million people. Approximately 2.8 million Soviet POWs died of starvation, mistreatment, or executions in just eight months of 1941–42. During the war, the country together with the United States, the United Kingdom and China were considered the Big Four Allied powers, and later became the Four Policemen that formed the basis of the United Nations Security Council. It emerged as a superpower in the post-war period. Once denied diplomatic recognition by the Western world, the USSR had official relations with practically every country by the late 1940s. A member of the United Nations at its foundation in 1945, the country became one of the five permanent members of the United Nations Security Council, which gave it the right to veto any of its resolutions.

During the immediate post-war period, the Soviet Union rebuilt and expanded its economy, while maintaining its strictly centralized control. It took effective control over most of the countries of Eastern Europe (except Yugoslavia and later Albania), turning them into satellite states. The USSR bound its satellite states in a military alliance, the Warsaw Pact, in 1955, and an economic organization, Council for Mutual Economic Assistance or Comecon, a counterpart to the European Economic Community (EEC), from 1949 to 1991. The USSR concentrated on its own recovery, seizing and transferring most of Germany's industrial plants, and it exacted war reparations from East Germany, Hungary, Romania, and Bulgaria using Soviet-dominated joint enterprises. It also instituted trading arrangements deliberately designed to favor the country. Moscow controlled the Communist parties that ruled the satellite states, and they followed orders from the Kremlin. Later, the Comecon supplied aid to the eventually victorious Communist Party of China, and its influence grew elsewhere in the world. Fearing its ambitions, the Soviet Union's wartime allies, the United Kingdom and the United States, became its enemies. In the ensuing Cold War, the two sides clashed indirectly in proxy wars.

Stalin died on 5 March 1953. Without a mutually agreeable successor, the highest Communist Party officials initially opted to rule the Soviet Union jointly through a troika headed by Georgy Malenkov. This did not last, however, and Nikita Khrushchev eventually won the ensuing power struggle by the mid-1950s. In 1956, he denounced Stalin's use of repression and proceeded to ease controls over the party and society. This was known as de-Stalinization.

Moscow considered Eastern Europe to be a critically vital buffer zone for the forward defence of its western borders, in case of another major invasion such as the German invasion of 1941. For this reason, the USSR sought to cement its control of the region by transforming the Eastern European countries into satellite states, dependent upon and subservient to its leadership. Soviet military force was used to suppress anti-Stalinist uprisings in Hungary and Poland in 1956.

In the late 1950s, a confrontation with China regarding the Soviet rapprochement with the West, and what Mao Zedong perceived as Khrushchev's revisionism, led to the Sino–Soviet split. This resulted in a break throughout the global Marxist–Leninist movement, with the governments in Albania, Cambodia and Somalia choosing to ally with China.

During this period of the late 1950s and early 1960s, the USSR continued to realize scientific and technological exploits in the Space Race, rivaling the United States: launching the first artificial satellite, Sputnik 1 in 1957; a living dog named Laika in 1957; the first human being, Yuri Gagarin in 1961; the first woman in space, Valentina Tereshkova in 1963; Alexei Leonov, the first person to walk in space in 1965; the first soft landing on the Moon by spacecraft Luna 9 in 1966; and the first Moon rovers, Lunokhod 1 and Lunokhod 2.

Khrushchev initiated "The Thaw", a complex shift in political, cultural and economic life in the country. This included some openness and contact with other nations and new social and economic policies with more emphasis on commodity goods, allowing a dramatic rise in living standards while maintaining high levels of economic growth. Censorship was relaxed as well. Khrushchev's reforms in agriculture and administration, however, were generally unproductive. In 1962, he precipitated a crisis with the United States over the Soviet deployment of nuclear missiles in Cuba. An agreement was made with the United States to remove nuclear missiles from both Cuba and Turkey, concluding the crisis. This event caused Khrushchev much embarrassment and loss of prestige, resulting in his removal from power in 1964.

Following the ousting of Khrushchev, another period of collective leadership ensued, consisting of Leonid Brezhnev as General Secretary, Alexei Kosygin as Premier and Nikolai Podgorny as Chairman of the Presidium, lasting until Brezhnev established himself in the early 1970s as the preeminent Soviet leader.

In 1968, the Soviet Union and Warsaw Pact allies invaded Czechoslovakia to halt the Prague Spring reforms. In the aftermath, Brezhnev justified the invasion along with the earlier invasions of Eastern European states by introducing the Brezhnev Doctrine, which claimed the right of the Soviet Union to violate the sovereignty of any country that attempted to replace Marxism–Leninism with capitalism.

Brezhnev presided throughout "détente" with the West that resulted in treaties on armament control (SALT I, SALT II, Anti-Ballistic Missile Treaty) while at the same time building up Soviet military might.

In October 1977, the third Soviet Constitution was unanimously adopted. The prevailing mood of the Soviet leadership at the time of Brezhnev's death in 1982 was one of aversion to change. The long period of Brezhnev's rule had come to be dubbed one of "standstill", with an ageing and ossified top political leadership. This period is also known as the Era of Stagnation, a period of adverse economic, political, and social effects in the country, which began during the rule of Brezhnev and continued under his successors Yuri Andropov and Konstantin Chernenko.

In late 1979, the Soviet Union's military intervened in the ongoing civil war in neighboring Afghanistan, effectively ending a détente with the West.

Two developments dominated the decade that followed: the increasingly apparent crumbling of the Soviet Union's economic and political structures, and the patchwork attempts at reforms to reverse that process. Kenneth S. Deffeyes argued in "Beyond Oil" that the Reagan administration encouraged Saudi Arabia to lower the price of oil to the point where the Soviets could not make a profit selling their oil, and resulted in the depletion of the country's hard currency reserves.

Brezhnev's next two successors, transitional figures with deep roots in his tradition, did not last long. Yuri Andropov was 68 years old and Konstantin Chernenko 72 when they assumed power; both died in less than two years. In an attempt to avoid a third short-lived leader, in 1985, the Soviets turned to the next generation and selected Mikhail Gorbachev. He made significant changes in the economy and party leadership, called "perestroika". His policy of "glasnost" freed public access to information after decades of heavy government censorship. Gorbachev also moved to end the Cold War. In 1988, the USSR abandoned its war in Afghanistan and began to withdraw its forces. In the following year, Gorbachev refused to interfere in the internal affairs of the Soviet satellite states, which paved the way for the Revolutions of 1989. With the tearing down of the Berlin Wall and with East and West Germany pursuing unification, the Iron Curtain between the West and Soviet-controlled regions came down.

At the same time, the Soviet republics started legal moves towards potentially declaring sovereignty over their territories, citing the freedom to secede in Article 72 of the USSR constitution. On 7 April 1990, a law was passed allowing a republic to secede if more than two-thirds of its residents voted for it in a referendum. Many held their first free elections in the Soviet era for their own national legislatures in 1990. Many of these legislatures proceeded to produce legislation contradicting the Union laws in what was known as the "War of Laws". In 1989, the Russian SFSR convened a newly elected Congress of People's Deputies. Boris Yeltsin was elected its chairman. On 12 June 1990, the Congress declared Russia's sovereignty over its territory and proceeded to pass laws that attempted to supersede some of the Soviet laws. After a landslide victory of Sąjūdis in Lithuania, that country declared its independence restored on 11 March 1990.

A referendum for the preservation of the USSR was held on 17 March 1991 in nine republics (the remainder having boycotted the vote), with the majority of the population in those republics voting for preservation of the Union. The referendum gave Gorbachev a minor boost. In the summer of 1991, the New Union Treaty, which would have turned the country into a much looser Union, was agreed upon by eight republics. The signing of the treaty, however, was interrupted by the August Coup—an attempted coup d'état by hardline members of the government and the KGB who sought to reverse Gorbachev's reforms and reassert the central government's control over the republics. After the coup collapsed, Yeltsin was seen as a hero for his decisive actions, while Gorbachev's power was effectively ended. The balance of power tipped significantly towards the republics. In August 1991, Latvia and Estonia immediately declared the restoration of their full independence (following Lithuania's 1990 example). Gorbachev resigned as general secretary in late August, and soon afterwards, the party's activities were indefinitely suspended—effectively ending its rule. By the fall, Gorbachev could no longer influence events outside Moscow, and he was being challenged even there by Yeltsin, who had been elected President of Russia in July 1991.

The remaining 12 republics continued discussing new, increasingly looser, models of the Union. However, by December all except Russia and Kazakhstan had formally declared independence. During this time, Yeltsin took over what remained of the Soviet government, including the Moscow Kremlin. The final blow was struck on 1 December when Ukraine, the second-most powerful republic, voted overwhelmingly for independence. Ukraine's secession ended any realistic chance of the country staying together even on a limited scale.

On 8 December 1991, the presidents of Russia, Ukraine and Belarus (formerly Byelorussia), signed the Belavezha Accords, which declared the Soviet Union dissolved and established the Commonwealth of Independent States (CIS) in its place. While doubts remained over the authority of the accords to do this, on 21 December 1991, the representatives of all Soviet republics except Georgia signed the Alma-Ata Protocol, which confirmed the accords. On 25 December 1991, Gorbachev resigned as the President of the USSR, declaring the office extinct. He turned the powers that had been vested in the presidency over to Yeltsin. That night, the Soviet flag was lowered for the last time, and the Russian tricolor was raised in its place.

The following day, the Supreme Soviet, the highest governmental body, voted both itself and the country out of existence. This is generally recognized as marking the official, final dissolution of the Soviet Union as a functioning state, and the end of the Cold War. The Soviet Army initially remained under overall CIS command but was soon absorbed into the different military forces of the newly independent states. The few remaining Soviet institutions that had not been taken over by Russia ceased to function by the end of 1991.

Following the dissolution, Russia was internationally recognized as its legal successor on the international stage. To that end, Russia voluntarily accepted all Soviet foreign debt and claimed Soviet overseas properties as its own. Under the 1992 Lisbon Protocol, Russia also agreed to receive all nuclear weapons remaining in the territory of other former Soviet republics. Since then, the Russian Federation has assumed the Soviet Union's rights and obligations. Ukraine has refused to recognize exclusive Russian claims to succession of the USSR and claimed such status for Ukraine as well, which was codified in Articles 7 and 8 of its 1991 law On Legal Succession of Ukraine. Since its independence in 1991, Ukraine has continued to pursue claims against Russia in foreign courts, seeking to recover its share of the foreign property that was owned by the USSR.

The dissolution was followed by a severe drop in economic and social conditions in post-Soviet states, including a rapid increase in poverty, crime, corruption, unemployment, homelessness, rates of disease, infant mortality and domestic violence, as well as demographic losses and income inequality and the rise of an oligarchical class, along with decreases in calorie intake, life expectancy, adult literacy, and income. Between 1988/1989 and 1993/1995, the Gini ratio increased by an average of 9 points for all former socialist countries. The economic shocks that accompanied wholesale privatization were associated with sharp increases in mortality. Data shows Russia, Kazakhstan, Latvia, Lithuania and Estonia saw a tripling of unemployment and a 42% increase in male death rates between 1991 and 1994. In the following decades, only five or six of the post-communist states are on a path to joining the wealthy capitalist West while most are falling behind, some to such an extent that it will take over fifty years to catch up to where they were before the fall of the Soviet Bloc.

In summing up the international ramifications of these events, Vladislav Zubok stated: "The collapse of the Soviet empire was an event of epochal geopolitical, military, ideological, and economic significance." Before the dissolution, the country had maintained its status as one of the world's two superpowers for four decades after World War II through its hegemony in Eastern Europe, military strength, economic strength, aid to developing countries, and scientific research, especially in space technology and weaponry.

The analysis of the succession of states for the 15 post-Soviet states is complex. The Russian Federation is seen as the legal "continuator" state and is for most purposes the heir to the Soviet Union. It retained ownership of all former Soviet embassy properties, as well as the old Soviet UN membership and permanent membership on the Security Council. 

Of the two other co-founding states of the USSR at the time of the dissolution, Ukraine was the only one that had passed similar to Russia's laws that it is a state-successor of both the Ukrainian SSR and the USSR. Soviet treaties laid groundwork for Ukraine's future foreign agreements as well as they led to Ukraine agreeing to undertake 16.37% of debts of the Soviet Union for which it was going to receive its share of USSR's foreign property. Although it had a tough position at the time, due to Russia's position as a "single continuation of the USSR" that became widely accepted in the West as well as a constant pressure from the Western countries, allowed Russia to dispose state property of USSR abroad and conceal information about it. Due to that Ukraine never ratified "zero option" agreement that Russian Federation had signed with other former Soviet republics, as it denied disclosing of information about Soviet Gold Reserves and its Diamond Fund. The dispute over former Soviet property and assets between the two former republics is still ongoing:

Similar situation occurred with restitution of cultural property. Although on 14 February 1992 Russia and other former Soviet republics signed agreement "On the return of cultural and historic property to the origin states" in Minsk, it was halted by Russian State Duma that had eventually passed "Federal Law on Cultural Valuables Displaced to the USSR as a Result of the Second World War and Located on the Territory of the Russian Federation" which made restitution currently impossible. 

There are additionally four states that claim independence from the other internationally recognised post-Soviet states but possess limited international recognition: Abkhazia, Nagorno-Karabakh, South Ossetia and Transnistria. The Chechen separatist movement of the Chechen Republic of Ichkeria lacks any international recognition.

During his rule, Stalin always made the final policy decisions. Otherwise, Soviet foreign policy was set by the commission on the Foreign Policy of the Central Committee of the Communist Party of the Soviet Union, or by the party's highest body the Politburo. Operations were handled by the separate Ministry of Foreign Affairs. It was known as the People's Commissariat for Foreign Affairs (or Narkomindel), until 1946. The most influential spokesmen were Georgy Chicherin (1872–1936), Maxim Litvinov (1876–1951), Vyacheslav Molotov (1890–1986), Andrey Vyshinsky (1883–1954) and Andrei Gromyko (1909–1989). Intellectuals were based in the Moscow State Institute of International Relations.

The Communist leadership of the Soviet Union intensely debated foreign policy issues and change directions several times. Even after Stalin assumed dictatorial control in the late 1920s, there were debates, and he frequently changed positions.

During the country's early period, it was assumed that Communist revolutions would break out soon in every major industrial country, and it was the Soviet responsibility to assist them. The Comintern was the weapon of choice. A few revolutions did break out, but they were quickly suppressed (the longest lasting one was in Hungary)—the Hungarian Soviet Republic—lasted only from 21 March 1919 to 1 August 1919. The Russian Bolsheviks were in no position to give any help.

By 1921, Lenin, Trotsky, and Stalin realized that capitalism had stabilized itself in Europe and there would not be any widespread revolutions anytime soon. It became the duty of the Russian Bolsheviks to protect what they had in Russia, and avoid military confrontations that might destroy their bridgehead. Russia was now a pariah state, along with Germany. The two came to terms in 1922 with the Treaty of Rapallo that settled long-standing grievances. At the same time, the two countries secretly set up training programs for the illegal German army and air force operations at hidden camps in the USSR.

Moscow eventually stopped threatening other states, and instead worked to open peaceful relationships in terms of trade, and diplomatic recognition. The United Kingdom dismissed the warnings of Winston Churchill and a few others about a continuing communist threat, and opened trade relations and "de facto" diplomatic recognition in 1922. There was hope for a settlement of the pre-war tsarist debts, but it was repeatedly postponed. Formal recognition came when the new Labour Party came to power in 1924. All the other countries followed suit in opening trade relations. Henry Ford opened large-scale business relations with the Soviets in the late 1920s, hoping that it would lead to long-term peace. Finally, in 1933, the United States officially recognized the USSR, a decision backed by the public opinion and especially by US business interests that expected an opening of a new profitable market.

In the late 1920s and early 1930s, Stalin ordered Communist parties across the world to strongly oppose non-communist political parties, labor unions or other organizations on the left. Stalin reversed himself in 1934 with the Popular Front program that called on all Communist parties to join together with all anti-Fascist political, labor, and organizational forces that were opposed to fascism, especially of the Nazi variety.

In 1939, half a year after the Munich Agreement, the USSR attempted to form an anti-Nazi alliance with France and Britain. Adolf Hitler proposed a better deal, which would give the USSR control over much of Eastern Europe through the Molotov–Ribbentrop Pact. In September, Germany invaded Poland, and the USSR also invaded later that month, resulting in the partition of Poland. In response, Britain and France declared war on Germany, marking the beginning of World War II.

There were three power hierarchies in the Soviet Union: the legislature represented by the Supreme Soviet of the Soviet Union, the government represented by the Council of Ministers, and the Communist Party of the Soviet Union (CPSU), the only legal party and the final policymaker in the country.

At the top of the Communist Party was the Central Committee, elected at Party Congresses and Conferences. In turn, the Central Committee voted for a Politburo (called the Presidium between 1952–1966), Secretariat and the General Secretary (First Secretary from 1953 to 1966), the "de facto" highest office in the Soviet Union. Depending on the degree of power consolidation, it was either the Politburo as a collective body or the General Secretary, who always was one of the Politburo members, that effectively led the party and the country (except for the period of the highly personalized authority of Stalin, exercised directly through his position in the Council of Ministers rather than the Politburo after 1941). They were not controlled by the general party membership, as the key principle of the party organization was democratic centralism, demanding strict subordination to higher bodies, and elections went uncontested, endorsing the candidates proposed from above.

The Communist Party maintained its dominance over the state mainly through its control over the system of appointments. All senior government officials and most deputies of the Supreme Soviet were members of the CPSU. Of the party heads themselves, Stalin (1941–1953) and Khrushchev (1958–1964) were Premiers. Upon the forced retirement of Khrushchev, the party leader was prohibited from this kind of double membership, but the later General Secretaries for at least some part of their tenure occupied the mostly ceremonial position of Chairman of the Presidium of the Supreme Soviet, the nominal head of state. The institutions at lower levels were overseen and at times supplanted by primary party organizations.

However, in practice the degree of control the party was able to exercise over the state bureaucracy, particularly after the death of Stalin, was far from total, with the bureaucracy pursuing different interests that were at times in conflict with the party. Nor was the party itself monolithic from top to bottom, although factions were officially banned.

The Supreme Soviet (successor of the Congress of Soviets) was nominally the highest state body for most of the Soviet history, at first acting as a rubber stamp institution, approving and implementing all decisions made by the party. However, its powers and functions were extended in the late 1950s, 1960s and 1970s, including the creation of new state commissions and committees. It gained additional powers relating to the approval of the Five-Year Plans and the government budget. The Supreme Soviet elected a Presidium (successor of the Central Executive Committee) to wield its power between plenary sessions, ordinarily held twice a year, and appointed the Supreme Court, the Procurator General and the Council of Ministers (known before 1946 as the Council of People's Commissars), headed by the Chairman (Premier) and managing an enormous bureaucracy responsible for the administration of the economy and society. State and party structures of the constituent republics largely emulated the structure of the central institutions, although the Russian SFSR, unlike the other constituent republics, for most of its history had no republican branch of the CPSU, being ruled directly by the union-wide party until 1990. Local authorities were organized likewise into party committees, local Soviets and executive committees. While the state system was nominally federal, the party was unitary.

The state security police (the KGB and ) played an important role in Soviet politics. It was instrumental in the Great Purge, but was brought under strict party control after Stalin's death. Under Yuri Andropov, the KGB engaged in the suppression of political dissent and maintained an extensive network of informers, reasserting itself as a political actor to some extent independent of the party-state structure, culminating in the anti-corruption campaign targeting high-ranking party officials in the late 1970s and early 1980s.

The constitution, which was promulgated in 1918, 1924, 1936 and 1977, did not limit state power. No formal separation of powers existed between the Party, Supreme Soviet and Council of Ministers that represented executive and legislative branches of the government. The system was governed less by statute than by informal conventions, and no settled mechanism of leadership succession existed. Bitter and at times deadly power struggles took place in the Politburo after the deaths of Lenin and Stalin, as well as after Khrushchev's dismissal, itself due to a decision by both the Politburo and the Central Committee. All leaders of the Communist Party before Gorbachev died in office, except Georgy Malenkov and Khrushchev, both dismissed from the party leadership amid internal struggle within the party.

Between 1988 and 1990, facing considerable opposition, Mikhail Gorbachev enacted reforms shifting power away from the highest bodies of the party and making the Supreme Soviet less dependent on them. The Congress of People's Deputies was established, the majority of whose members were directly elected in competitive elections held in March 1989. The Congress now elected the Supreme Soviet, which became a full-time parliament, and much stronger than before. For the first time since the 1920s, it refused to rubber stamp proposals from the party and Council of Ministers. In 1990, Gorbachev introduced and assumed the position of the President of the Soviet Union, concentrated power in his executive office, independent of the party, and subordinated the government, now renamed the Cabinet of Ministers of the USSR, to himself.

Tensions grew between the Union-wide authorities under Gorbachev, reformists led in Russia by Boris Yeltsin and controlling the newly elected Supreme Soviet of the Russian SFSR, and communist hardliners. On 19–21 August 1991, a group of hardliners staged a coup attempt. The coup failed, and the State Council of the Soviet Union became the highest organ of state power "in the period of transition". Gorbachev resigned as General Secretary, only remaining President for the final months of the existence of the USSR.

The judiciary was not independent of the other branches of government. The Supreme Court supervised the lower courts (People's Court) and applied the law as established by the constitution or as interpreted by the Supreme Soviet. The Constitutional Oversight Committee reviewed the constitutionality of laws and acts. The Soviet Union used the inquisitorial system of Roman law, where the judge, procurator, and defence attorney collaborate to establish the truth.

Constitutionally, the USSR was a federation of constituent Union Republics, which were either unitary states, such as Ukraine or Byelorussia (SSRs), or federations, such as Russia or Transcaucasia (SFSRs), all four being the founding republics who signed the Treaty on the Creation of the USSR in December 1922. In 1924, during the national delimitation in Central Asia, Uzbekistan and Turkmenistan were formed from parts of Russia's Turkestan ASSR and two Soviet dependencies, the Khorezm and Bukharan SSRs. In 1929, Tajikistan was split off from the Uzbekistan SSR. With the constitution of 1936, the Transcaucasian SFSR was dissolved, resulting in its constituent republics of Armenia, Georgia and Azerbaijan being elevated to Union Republics, while Kazakhstan and Kirghizia were split off from Russian SFSR, resulting in the same status. In August 1940, Moldavia was formed from parts of Ukraine and Bessarabia and northern Bukovina. Estonia, Latvia and Lithuania (SSRs) were also admitted into the union which was not recognized by most of the international community and was considered an illegal occupation. Karelia was split off from Russia as a Union Republic in March 1940 and was reabsorbed in 1956. Between July 1956 and September 1991, there were 15 union republics (see map below).

While nominally a union of equals, in practice the Soviet Union was dominated by Russians. The domination was so absolute that for most of its existence, the country was commonly (but incorrectly) referred to as "Russia". While the RSFSR was technically only one republic within the larger union, it was by far the largest (both in terms of population and area), most powerful, most developed, and the industrial center of the Soviet Union. Historian Matthew White wrote that it was an open secret that the country's federal structure was "window dressing" for Russian dominance. For that reason, the people of the USSR were usually called "Russians", not "Soviets", since "everyone knew who really ran the show".

Under the Military Law of September 1925, the Soviet Armed Forces consisted of three components, namely the Land Forces, the Air Force, the Navy, Joint State Political Directorate (OGPU), and the Internal Troops. The OGPU later became independent and in 1934 joined the NKVD, and so its internal troops were under the joint leadership of the defense and internal commissariats. After World War II, Strategic Missile Forces (1959), Air Defense Forces (1948) and National Civil Defense Forces (1970) were formed, which ranked first, third, and sixth in the official Soviet system of importance (ground forces were second, Air Force Fourth, and Navy Fifth).

The army had the greatest political influence. In 1989, there served two million soldiers divided between 150 motorized and 52 armored divisions. Until the early 1960s, the Soviet navy was a rather small military branch, but after the Caribbean crisis, under the leadership of Sergei Gorshkov, it expanded significantly. It became known for battlecruisers and submarines. In 1989 there served 500 000 men. The Soviet Air Force focused on a fleet of strategic bombers and during war situation was to eradicate enemy infrastructure and nuclear capacity. The air force also had a number of fighters and tactical bombers to support the army in the war. Strategic missile forces had more than 1,400 intercontinental ballistic missiles (ICBMs), deployed between 28 bases and 300 command centers.

In the post-war period, the Soviet Army was directly involved in several military operations abroad. These included the suppression of the uprising in East Germany (1953), Hungarian revolution (1956) and the invasion of Czechoslovakia (1968). The Soviet Union also participated in the war in Afghanistan between 1979 and 1989.

In the Soviet Union, general conscription applied.

At the end of the 1950s, with the help of engineers and technologies captured and imported from defeated Nazi Germany, the Soviets constructed the first satellite - Sputnik 1 and thus overtook the United States. This was followed by other successful satellites and experimental dogs were sent. On April 12, 1961, the first cosmonaut, Yuri Gagarin, was sent to the space. He once flew around the Earth and successfully landed in the Kazakh steppe. At that time, the first plans for space shuttles and orbital stations were drawn up in Soviet design offices, but in the end personal disputes between designers and management prevented this.

The first big fiasco for the USSR was the landing on the moon by the Americans, when the Russians were not able to respond to the Americans in time with the same project. In the 1970s, more specific proposals for the design of the space shuttle began to emerge, but shortcomings, especially in the electronics industry (rapid overheating of electronics), postponed the program until the end of the 1980s. The first shuttle, the Buran, flew in 1988, but without a human crew. Another shuttle, Ptichka, eventually ended up under construction, as the shuttle project was canceled in 1991. For their launch into space, there is today an unused superpower rocket, Energia, which is the most powerful in the world.

In the late 1980s, the Soviet Union managed to build the Mir orbital station. It was built on the construction of Salyut stations and its tasks were purely civilian and research. In the 1990s, when the US Skylab was shut down due to lack of funds, it was the only orbital station in operation. Gradually, other modules were added to it, including American ones. However, the technical condition of the station deteriorated rapidly, especially after the fire, so in 2001 it was decided to bring it into the atmosphere where it burned down.

The Soviet Union adopted a command economy, whereby production and distribution of goods were centralized and directed by the government. The first Bolshevik experience with a command economy was the policy of War communism, which involved the nationalization of industry, centralized distribution of output, coercive requisition of agricultural production, and attempts to eliminate money circulation, private enterprises and free trade. After the severe economic collapse, Lenin replaced war communism by the New Economic Policy (NEP) in 1921, legalizing free trade and private ownership of small businesses. The economy quickly recovered as a result.

After a long debate among the members of Politburo about the course of economic development, by 1928–1929, upon gaining control of the country, Stalin abandoned the NEP and pushed for full central planning, starting forced collectivization of agriculture and enacting draconian labor legislation. Resources were mobilized for rapid industrialization, which significantly expanded Soviet capacity in heavy industry and capital goods during the 1930s. The primary motivation for industrialization was preparation for war, mostly due to distrust of the outside capitalist world. As a result, the USSR was transformed from a largely agrarian economy into a great industrial power, leading the way for its emergence as a superpower after World War II. The war caused extensive devastation of the Soviet economy and infrastructure, which required massive reconstruction.
By the early 1940s, the Soviet economy had become relatively self-sufficient; for most of the period until the creation of Comecon, only a tiny share of domestic products was traded internationally. After the creation of the Eastern Bloc, external trade rose rapidly. However, the influence of the world economy on the USSR was limited by fixed domestic prices and a state monopoly on foreign trade. Grain and sophisticated consumer manufactures became major import articles from around the 1960s. During the arms race of the Cold War, the Soviet economy was burdened by military expenditures, heavily lobbied for by a powerful bureaucracy dependent on the arms industry. At the same time, the USSR became the largest arms exporter to the Third World. Significant amounts of Soviet resources during the Cold War were allocated in aid to the other socialist states.
From the 1930s until its dissolution in late 1991, the way the Soviet economy operated remained essentially unchanged. The economy was formally directed by central planning, carried out by Gosplan and organized in five-year plans. However, in practice, the plans were highly aggregated and provisional, subject to "ad hoc" intervention by superiors. All critical economic decisions were taken by the political leadership. Allocated resources and plan targets were usually denominated in rubles rather than in physical goods. Credit was discouraged, but widespread. The final allocation of output was achieved through relatively decentralized, unplanned contracting. Although in theory prices were legally set from above, in practice they were often negotiated, and informal horizontal links (e.g. between producer factories) were widespread.

A number of basic services were state-funded, such as education and health care. In the manufacturing sector, heavy industry and defence were prioritized over consumer goods. Consumer goods, particularly outside large cities, were often scarce, of poor quality and limited variety. Under the command economy, consumers had almost no influence on production, and the changing demands of a population with growing incomes could not be satisfied by supplies at rigidly fixed prices. A massive unplanned second economy grew up at low levels alongside the planned one, providing some of the goods and services that the planners could not. The legalization of some elements of the decentralized economy was attempted with the reform of 1965.

Although statistics of the Soviet economy are notoriously unreliable and its economic growth difficult to estimate precisely, by most accounts, the economy continued to expand until the mid-1980s. During the 1950s and 1960s, it had comparatively high growth and was catching up to the West. However, after 1970, the growth, while still positive, steadily declined much more quickly and consistently than in other countries, despite a rapid increase in the capital stock (the rate of capital increase was only surpassed by Japan).

Overall, the growth rate of per capita income in the Soviet Union between 1960 and 1989 was slightly above the world average (based on 102 countries). According to Stanley Fischer and William Easterly, growth could have been faster. By their calculation, per capita income in 1989 should have been twice higher than it was, considering the amount of investment, education and population. The authors attribute this poor performance to the low productivity of capital. Steven Rosenfielde states that the standard of living declined due to Stalin's despotism. While there was a brief improvement after his death, it lapsed into stagnation.

In 1987, Mikhail Gorbachev attempted to reform and revitalize the economy with his program of "perestroika". His policies relaxed state control over enterprises but did not replace it by market incentives, resulting in a sharp decline in output. The economy, already suffering from reduced petroleum export revenues, started to collapse. Prices were still fixed, and the property was still largely state-owned until after the country's dissolution. For most of the period after World War II until its collapse, Soviet GDP (PPP) was the second-largest in the world, and third during the second half of the 1980s, although on a per-capita basis, it was behind that of First World countries. Compared to countries with similar per-capita GDP in 1928, the Soviet Union experienced significant growth.

In 1990, the country had a Human Development Index of 0.920, placing it in the "high" category of human development. It was the third-highest in the Eastern Bloc, behind Czechoslovakia and East Germany, and the 25th in the world of 130 countries.

The need for fuel declined in the Soviet Union from the 1970s to the 1980s, both per ruble of gross social product and per ruble of industrial product. At the start, this decline grew very rapidly but gradually slowed down between 1970 and 1975. From 1975 and 1980, it grew even slower, only 2.6%. David Wilson, a historian, believed that the gas industry would account for 40% of Soviet fuel production by the end of the century. His theory did not come to fruition because of the USSR's collapse. The USSR, in theory, would have continued to have an economic growth rate of 2–2.5% during the 1990s because of Soviet energy fields. However, the energy sector faced many difficulties, among them the country's high military expenditure and hostile relations with the First World.

In 1991, the Soviet Union had a pipeline network of for crude oil and another for natural gas. Petroleum and petroleum-based products, natural gas, metals, wood, agricultural products, and a variety of manufactured goods, primarily machinery, arms and military equipment, were exported. In the 1970s and 1980s, the USSR heavily relied on fossil fuel exports to earn hard currency. At its peak in 1988, it was the largest producer and second-largest exporter of crude oil, surpassed only by Saudi Arabia.

The Soviet Union placed great emphasis on science and technology within its economy, however, the most remarkable Soviet successes in technology, such as producing the world's first space satellite, typically were the responsibility of the military. Lenin believed that the USSR would never overtake the developed world if it remained as technologically backward as it was upon its founding. Soviet authorities proved their commitment to Lenin's belief by developing massive networks, research and development organizations. In the early 1960s, the Soviets awarded 40% of chemistry PhDs to women, compared to only 5% in the United States. By 1989, Soviet scientists were among the world's best-trained specialists in several areas, such as energy physics, selected areas of medicine, mathematics, welding and military technologies. Due to rigid state planning and bureaucracy, the Soviets remained far behind technologically in chemistry, biology, and computers when compared to the First World.

Under the Reagan administration, Project Socrates determined that the Soviet Union addressed the acquisition of science and technology in a manner that was radically different from what the US was using. In the case of the US, economic prioritization was being used for indigenous research and development as the means to acquire science and technology in both the private and public sectors. In contrast, the USSR was offensively and defensively maneuvering in the acquisition and utilization of the worldwide technology, to increase the competitive advantage that they acquired from the technology while preventing the US from acquiring a competitive advantage. However, technology-based planning was executed in a centralized, government-centric manner that greatly hindered its flexibility. This was exploited by the US to undermine the strength of the Soviet Union and thus foster its reform.

Transport was a vital component of the country's economy. The economic centralization of the late 1920s and 1930s led to the development of infrastructure on a massive scale, most notably the establishment of Aeroflot, an aviation enterprise. The country had a wide variety of modes of transport by land, water and air. However, due to inadequate maintenance, much of the road, water and Soviet civil aviation transport were outdated and technologically backward compared to the First World.

Soviet rail transport was the largest and most intensively used in the world; it was also better developed than most of its Western counterparts. By the late 1970s and early 1980s, Soviet economists were calling for the construction of more roads to alleviate some of the burdens from the railways and to improve the Soviet government budget. The street network and automotive industry remained underdeveloped, and dirt roads were common outside major cities. Soviet maintenance projects proved unable to take care of even the few roads the country had. By the early-to-mid-1980s, the Soviet authorities tried to solve the road problem by ordering the construction of new ones. Meanwhile, the automobile industry was growing at a faster rate than road construction. The underdeveloped road network led to a growing demand for public transport.

Despite improvements, several aspects of the transport sector were still riddled with problems due to outdated infrastructure, lack of investment, corruption and bad decision-making. Soviet authorities were unable to meet the growing demand for transport infrastructure and services.

The Soviet merchant navy was one of the largest in the world.

Excess deaths throughout World War I and the Russian Civil War (including the postwar famine) amounted to a combined total of 18 million, some 10 million in the 1930s, and more than 26 million in 1941–5. The postwar Soviet population was 45 to 50 million smaller than it would have been if pre-war demographic growth had continued. According to Catherine Merridale, "... reasonable estimate would place the total number of excess deaths for the whole period somewhere around 60 million."

The birth rate of the USSR decreased from 44.0 per thousand in 1926 to 18.0 in 1974, mainly due to increasing urbanization and the rising average age of marriages. The mortality rate demonstrated a gradual decrease as well – from 23.7 per thousand in 1926 to 8.7 in 1974. In general, the birth rates of the southern republics in Transcaucasia and Central Asia were considerably higher than those in the northern parts of the Soviet Union, and in some cases even increased in the post–World War II period, a phenomenon partly attributed to slower rates of urbanistion and traditionally earlier marriages in the southern republics. Soviet Europe moved towards sub-replacement fertility, while Soviet Central Asia continued to exhibit population growth well above replacement-level fertility.

The late 1960s and the 1970s witnessed a reversal of the declining trajectory of the rate of mortality in the USSR, and was especially notable among men of working age, but was also prevalent in Russia and other predominantly Slavic areas of the country. An analysis of the official data from the late 1980s showed that after worsening in the late-1970s and the early 1980s, adult mortality began to improve again. The infant mortality rate increased from 24.7 in 1970 to 27.9 in 1974. Some researchers regarded the rise as mostly real, a consequence of worsening health conditions and services. The rises in both adult and infant mortality were not explained or defended by Soviet officials, and the Soviet government stopped publishing all mortality statistics for ten years. Soviet demographers and health specialists remained silent about the mortality increases until the late-1980s, when the publication of mortality data resumed, and researchers could delve into the real causes.

Under Lenin, the state made explicit commitments to promote the equality of men and women. Many early Russian feminists and ordinary Russian working women actively participated in the Revolution, and many more were affected by the events of that period and the new policies. Beginning in October 1918, the Lenin's government liberalized divorce and abortion laws, decriminalized homosexuality (re-criminalized in the 1930s), permitted cohabitation, and ushered in a host of reforms. However, without birth control, the new system produced many broken marriages, as well as countless out-of-wedlock children. The epidemic of divorces and extramarital affairs created social hardships when Soviet leaders wanted people to concentrate their efforts on growing the economy. Giving women control over their fertility also led to a precipitous decline in the birth rate, perceived as a threat to their country's military power. By 1936, Stalin reversed most of the liberal laws, ushering in a pronatalist era that lasted for decades.

By 1917, Russia became the first great power to grant women the right to vote. After heavy casualties in World War I and II, women outnumbered men in Russia by a 4:3 ratio. This contributed to the larger role women played in Russian society compared to other great powers at the time.

Anatoly Lunacharsky became the first People's Commissar for Education of Soviet Russia. In the beginning, the Soviet authorities placed great emphasis on the elimination of illiteracy. All left-handed kids were forced to write with their right hand in the Soviet school system. Literate people were automatically hired as teachers. For a short period, quality was sacrificed for quantity. By 1940, Stalin could announce that illiteracy had been eliminated. Throughout the 1930s, social mobility rose sharply, which has been attributed to reforms in education. In the aftermath of World War II, the country's educational system expanded dramatically, which had a tremendous effect. In the 1960s, nearly all children had access to education, the only exception being those living in remote areas. Nikita Khrushchev tried to make education more accessible, making it clear to children that education was closely linked to the needs of society. Education also became important in giving rise to the New Man. Citizens directly entering the workforce had the constitutional right to a job and to free vocational training.

The education system was highly centralized and universally accessible to all citizens, with affirmative action for applicants from nations associated with cultural backwardness. However, as part of the general antisemitic policy, an unofficial Jewish quota was applied in the leading institutions of higher education by subjecting Jewish applicants to harsher entrance examinations. The Brezhnev era also introduced a rule that required all university applicants to present a reference from the local Komsomol party secretary. According to statistics from 1986, the number of higher education students per the population of 10,000 was 181 for the USSR, compared to 517 for the US.

The Soviet Union was an ethnically diverse country, with more than 100 distinct ethnic groups. The total population was estimated at 293 million in 1991. According to a 1990 estimate, the majority were Russians (50.78%), followed by Ukrainians (15.45%) and Uzbeks (5.84%).

All citizens of the USSR had their own ethnic affiliation. The ethnicity of a person was chosen at the age of sixteen by the child's parents. If the parents did not agree, the child was automatically assigned the ethnicity of the father. Partly due to Soviet policies, some of the smaller minority ethnic groups were considered part of larger ones, such as the Mingrelians of Georgia, who were classified with the linguistically related Georgians. Some ethnic groups voluntarily assimilated, while others were brought in by force. Russians, Belarusians, and Ukrainians shared close cultural ties, while other groups did not. With multiple nationalities living in the same territory, ethnic antagonisms developed over the years.

Members of various ethnicities participated in legislative bodies. Organs of power like the Politburo, the Secretariat of the Central Committee etc., were formally ethnically neutral, but in reality, ethnic Russians were overrepresented, although there were also non-Russian leaders in the Soviet leadership, such as Joseph Stalin, Grigory Zinoviev, Nikolai Podgorny or Andrei Gromyko. During the Soviet era, a significant number of ethnic Russians and Ukrainians migrated to other Soviet republics, and many of them settled there. According to the last census in 1989, the Russian "diaspora" in the Soviet republics had reached 25 million.

In 1917, before the revolution, health conditions were significantly behind those of developed countries. As Lenin later noted, "Either the lice will defeat socialism, or socialism will defeat the lice". The Soviet principle of health care was conceived by the People's Commissariat for Health in 1918. Health care was to be controlled by the state and would be provided to its citizens free of charge, a revolutionary concept at the time. Article 42 of the 1977 Soviet Constitution gave all citizens the right to health protection and free access to any health institutions in the USSR. Before Leonid Brezhnev became General Secretary, the Soviet healthcare system was held in high esteem by many foreign specialists. This changed, however, from Brezhnev's accession and Mikhail Gorbachev's tenure as leader, during which the health care system was heavily criticized for many basic faults, such as the quality of service and the unevenness in its provision. Minister of Health Yevgeniy Chazov, during the 19th Congress of the Communist Party of the Soviet Union, while highlighting such successes as having the most doctors and hospitals in the world, recognized the system's areas for improvement and felt that billions of Soviet rubles were squandered. 

After the revolution, life expectancy for all age groups went up. This statistic in itself was seen by some that the socialist system was superior to the capitalist system. These improvements continued into the 1960s when statistics indicated that the life expectancy briefly surpassed that of the United States. Life expectancy started to decline in the 1970s, possibly because of alcohol abuse. At the same time, infant mortality began to rise. After 1974, the government stopped publishing statistics on the matter. This trend can be partly explained by the number of pregnancies rising drastically in the Asian part of the country where infant mortality was the highest while declining markedly in the more developed European part of the Soviet Union.

Under Lenin, the government gave small language groups their own writing systems. The development of these writing systems was highly successful, even though some flaws were detected. During the later days of the USSR, countries with the same multilingual situation implemented similar policies. A serious problem when creating these writing systems was that the languages differed dialectally greatly from each other. When a language had been given a writing system and appeared in a notable publication, it would attain "official language" status. There were many minority languages which never received their own writing system; therefore, their speakers were forced to have a second language. There are examples where the government retreated from this policy, most notably under Stalin where education was discontinued in languages that were not widespread. These languages were then assimilated into another language, mostly Russian. During World War II, some minority languages were banned, and their speakers accused of collaborating with the enemy.

As the most widely spoken of the Soviet Union's many languages, Russian "de facto" functioned as an official language, as the "language of interethnic communication" (), but only assumed the "de jure" status as the official national language in 1990.

Christianity and Islam had the highest number of adherents among the religious citizens. Eastern Christianity predominated among Christians, with Russia's traditional Russian Orthodox Church being the largest Christian denomination. About 90% of the Soviet Union's Muslims were Sunnis, with Shias being concentrated in the Azerbaijan SSR. Smaller groups included Roman Catholics, Jews, Buddhists, and a variety of Protestant denominations (especially Baptists and Lutherans).

Religious influence had been strong in the Russian Empire. The Russian Orthodox Church enjoyed a privileged status as the church of the monarchy and took part in carrying out official state functions. The immediate period following the establishment of the Soviet state included a struggle against the Orthodox Church, which the revolutionaries considered an ally of the former ruling classes.

In Soviet law, the "freedom to hold religious services" was constitutionally guaranteed, although the ruling Communist Party regarded religion as incompatible with the Marxist spirit of scientific materialism. In practice, the Soviet system subscribed to a narrow interpretation of this right, and in fact utilized a range of official measures to discourage religion and curb the activities of religious groups.

The 1918 Council of People's Commissars decree establishing the Russian SFSR as a secular state also decreed that "the teaching of religion in all [places] where subjects of general instruction are taught, is forbidden. Citizens may teach and may be taught religion privately." Among further restrictions, those adopted in 1929 included express prohibitions on a range of church activities, including meetings for organized Bible study. Both Christian and non-Christian establishments were shut down by the thousands in the 1920s and 1930s. By 1940, as many as 90% of the churches, synagogues, and mosques that had been operating in 1917 were closed.
Under the doctrine of state atheism, there was a "government-sponsored program of forced conversion to atheism" conducted by the Communists. The regime targeted religions based on state interests, and while most organized religions were never outlawed, religious property was confiscated, believers were harassed, and religion was ridiculed while atheism was propagated in schools. In 1925, the government founded the League of Militant Atheists to intensify the propaganda campaign. Accordingly, although personal expressions of religious faith were not explicitly banned, a strong sense of social stigma was imposed on them by the formal structures and mass media, and it was generally considered unacceptable for members of certain professions (teachers, state bureaucrats, soldiers) to be openly religious. As for the Russian Orthodox Church, Soviet authorities sought to control it and, in times of national crisis, to exploit it for the regime's own purposes; but their ultimate goal was to eliminate it. During the first five years of Soviet power, the Bolsheviks executed 28 Russian Orthodox bishops and over 1,200 Russian Orthodox priests. Many others were imprisoned or exiled. Believers were harassed and persecuted. Most seminaries were closed, and the publication of most religious material was prohibited. By 1941, only 500 churches remained open out of about 54,000 in existence before World War I.

Convinced that religious anti-Sovietism had become a thing of the past, and with the looming threat of war, the Stalin regime began shifting to a more moderate religion policy in the late 1930s. Soviet religious establishments overwhelmingly rallied to support the war effort during World War II. Amid other accommodations to religious faith after the German invasion, churches were reopened. Radio Moscow began broadcasting a religious hour, and a historic meeting between Stalin and Orthodox Church leader Patriarch Sergius of Moscow was held in 1943. Stalin had the support of the majority of the religious people in the USSR even through the late 1980s. The general tendency of this period was an increase in religious activity among believers of all faiths.

Under Nikita Khrushchev, the state leadership clashed with the churches in 1958–1964, a period when atheism was emphasized in the educational curriculum, and numerous state publications promoted atheistic views. During this period, the number of churches fell from 20,000 to 10,000 from 1959 to 1965, and the number of synagogues dropped from 500 to 97. The number of working mosques also declined, falling from 1,500 to 500 within a decade.

Religious institutions remained monitored by the Soviet government, but churches, synagogues, temples, and mosques were all given more leeway in the Brezhnev era. Official relations between the Orthodox Church and the government again warmed to the point that the Brezhnev government twice honored Orthodox Patriarch Alexy I with the Order of the Red Banner of Labour. A poll conducted by Soviet authorities in 1982 recorded 20% of the Soviet population as "active religious believers."

The culture of the Soviet Union passed through several stages during the USSR's existence. During the first decade following the revolution, there was relative freedom and artists experimented with several different styles to find a distinctive Soviet style of art. Lenin wanted art to be accessible to the Russian people. On the other hand, hundreds of intellectuals, writers, and artists were exiled or executed, and their work banned, such as Nikolay Gumilyov who was shot for alleged conspiring against the Bolshevik regime, and Yevgeny Zamyatin.

The government encouraged a variety of trends. In art and literature, numerous schools, some traditional and others radically experimental, proliferated. Communist writers Maxim Gorky and Vladimir Mayakovsky were active during this time. As a means of influencing a largely illiterate society, films received encouragement from the state, and much of director Sergei Eisenstein's best work dates from this period.

During Stalin's rule, the Soviet culture was characterized by the rise and domination of the government-imposed style of socialist realism, with all other trends being severely repressed, with rare exceptions, such as Mikhail Bulgakov's works. Many writers were imprisoned and killed.

Following the Khrushchev Thaw, censorship was diminished. During this time, a distinctive period of Soviet culture developed, characterized by conformist public life and an intense focus on personal life. Greater experimentation in art forms was again permissible, resulting in the production of more sophisticated and subtly critical work. The regime loosened its emphasis on socialist realism; thus, for instance, many protagonists of the novels of author Yury Trifonov concerned themselves with problems of daily life rather than with building socialism. Underground dissident literature, known as "samizdat", developed during this late period. In architecture, the Khrushchev era mostly focused on functional design as opposed to the highly decorated style of Stalin's epoch.

In the second half of the 1980s, Gorbachev's policies of "perestroika" and "glasnost" significantly expanded freedom of expression throughout the country in the media and the press.

Founded on 20 July 1924 in Moscow, "Sovetsky Sport" was the first sports newspaper of the Soviet Union.

The Soviet Olympic Committee formed on 21 April 1951, and the IOC recognized the new body in its 45th session. In the same year, when the Soviet representative Konstantin Andrianov became an IOC member, the USSR officially joined the Olympic Movement. The 1952 Summer Olympics in Helsinki thus became first Olympic Games for Soviet athletes.

The Soviet Union national ice hockey team won nearly every world championship and Olympic tournament between 1954 and 1991 and never failed to medal in any International Ice Hockey Federation (IIHF) tournament in which they competed.

The advent of the state-sponsored "full-time amateur athlete" of the Eastern Bloc countries further eroded the ideology of the pure amateur, as it put the self-financed amateurs of the Western countries at a disadvantage. The Soviet Union entered teams of athletes who were all nominally students, soldiers, or working in a profession – in reality, the state paid many of these competitors to train on a full-time basis. Nevertheless, the IOC held to the traditional rules regarding amateurism.

A 1989 report by a committee of the Australian Senate claimed that "there is hardly a medal winner at the Moscow Games, certainly not a gold medal winner...who is not on one sort of drug or another: usually several kinds. The Moscow Games might well have been called the Chemists' Games".

A member of the IOC Medical Commission, Manfred Donike, privately ran additional tests with a new technique for identifying abnormal levels of testosterone by measuring its ratio to epitestosterone in urine. Twenty percent of the specimens he tested, including those from sixteen gold medalists, would have resulted in disciplinary proceedings had the tests been official. The results of Donike's unofficial tests later convinced the IOC to add his new technique to their testing protocols. The first documented case of "blood doping" occurred at the 1980 Summer Olympics when a runner was transfused with two pints of blood before winning medals in the 5000 m and 10,000 m.

Documentation obtained in 2016 revealed the Soviet Union's plans for a statewide doping system in track and field in preparation for the 1984 Summer Olympics in Los Angeles. Dated before the decision to boycott the 1984 Games, the document detailed the existing steroids operations of the program, along with suggestions for further enhancements. Dr. Sergei Portugalov of the Institute for Physical Culture prepared the communication, directed to the Soviet Union's head of track and field. Portugalov later became one of the leading figures involved in the implementation of Russian doping before the 2016 Summer Olympics.

Official Soviet environmental policy has always attached great importance to actions in which human beings actively improve nature. Lenin's quote "Communism is Soviet power and electrification of the country!" in many respects it summarizes the focus on modernization and industrial development. During the first five-year plan in 1928, Stalin proceeded to industrialize the country at all costs. Values such as environmental and nature protection have been completely ignored in the struggle to create a modern industrial society. After Stalin's death, they focused more on environmental issues, but the basic perception of the value of environmental protection remained the same.

The Soviet media has always focused on the vast expanse of land and the virtually indestructible natural resources. This made it feel that contamination and looting of nature were not a problem. The Soviet state also firmly believed that scientific and technological progress would solve all the problems. Official ideology said that under socialism environmental problems could easily be overcome, unlike capitalist countries, where they seemingly could not be solved. The Soviet authorities had an almost unwavering belief that man could transcend nature. However, when the authorities had to admit that there were environmental problems in the USSR in the 1980s, they explained the problems in such a way that socialism had not yet been fully developed; pollution in socialist society was only a temporary anomaly that would have been resolved if socialism had developed.

The Chernobyl disaster in 1986 was the first major accident at a civilian nuclear power plant, unparalleled in the world when a large number of radioactive isotopes were released into the atmosphere. Radioactive doses have scattered relatively far. The main health problem after the accident was 4,000 new cases of thyroid cancer, but this led to a relatively low number of deaths (WHO data, 2005). However, the long-term effects of the accident are unknown. Another major accident is the Kyshtym disaster.

After the fall of the USSR, it was discovered that the environmental problems were greater than what the Soviet authorities admitted. The Kola Peninsula was one of the places with clear problems. Around the industrial cities of Monchegorsk and Norilsk, where nickel, for example, is mined, all forests have been killed by contamination, while the northern and other parts of Russia have been affected by emissions. During the 1990s, people in the West were also interested in the radioactive hazards of nuclear facilities, decommissioned nuclear submarines, and the processing of nuclear waste or spent nuclear fuel. It was also known in the early 1990s that the USSR had transported radioactive material to the Barents Sea and Kara Sea, which was later confirmed by the Russian parliament. The crash of the K-141 Kursk submarine in 2000 in the west further raised concerns. In the past, there were accidents involving submarines K-19, K-8 or K-129.













</doc>
<doc id="26781" url="https://en.wikipedia.org/wiki?curid=26781" title="Social science">
Social science

Social science is the branch of science devoted to the study of societies and the relationships among individuals within those societies. The term was formerly used to refer to the field of sociology, the original "science of society", established in the 19th century. In addition to sociology, it now encompasses a wide array of academic disciplines, including anthropology, archaeology, economics, human geography, linguistics, management science, media studies, musicology, political science, psychology, welfare and nursing studies and social history. (For a more detailed list of sub-disciplines within the social sciences see: Outline of social science.)

Positivist social scientists use methods resembling those of the natural sciences as tools for understanding society, and so define science in its stricter modern sense. Interpretivist social scientists, by contrast, may use social critique or symbolic interpretation rather than constructing empirically falsifiable theories, and thus treat science in its broader sense. In modern academic practice, researchers are often eclectic, using multiple methodologies (for instance, by combining both quantitative and qualitative research). The term "social research" has also acquired a degree of autonomy as practitioners from various disciplines share in its aims and methods.

The history of the social sciences begins in the Age of Enlightenment after 1650, which saw a revolution within natural philosophy, changing the basic framework by which individuals understood what was "scientific". Social sciences came forth from the moral philosophy of the time and were influenced by the Age of Revolutions, such as the Industrial Revolution and the French Revolution. The "social sciences" developed from the sciences (experimental and applied), or the systematic knowledge-bases or prescriptive practices, relating to the social improvement of a group of interacting entities.

The beginnings of the social sciences in the 18th century are reflected in the grand encyclopedia of Diderot, with articles from Jean-Jacques Rousseau and other pioneers. The growth of the social sciences is also reflected in other specialized encyclopedias. The modern period saw ""social science"" first used as a distinct conceptual field. Social science was influenced by positivism, focusing on knowledge based on actual positive sense experience and avoiding the negative; metaphysical speculation was avoided. Auguste Comte used the term ""science sociale"" to describe the field, taken from the ideas of Charles Fourier; Comte also referred to the field as "social physics".

Following this period, five paths of development sprang forth in the social sciences, influenced by Comte in other fields. One route that was taken was the rise of social research. Large statistical surveys were undertaken in various parts of the United States and Europe. Another route undertaken was initiated by Émile Durkheim, studying "social facts", and Vilfredo Pareto, opening metatheoretical ideas and individual theories. A third means developed, arising from the methodological dichotomy present, in which social phenomena were identified with and understood; this was championed by figures such as Max Weber. The fourth route taken, based in economics, was developed and furthered economic knowledge as a hard science. The last path was the correlation of knowledge and social values; the antipositivism and verstehen sociology of Max Weber firmly demanded this distinction. In this route, theory (description) and prescription were non-overlapping formal discussions of a subject.

Around the start of the 20th century, Enlightenment philosophy was challenged in various quarters. After the use of classical theories since the end of the scientific revolution, various fields substituted mathematics studies for experimental studies and examining equations to build a theoretical structure. The development of social science subfields became very quantitative in methodology. The interdisciplinary and cross-disciplinary nature of scientific inquiry into human behaviour, social and environmental factors affecting it, made many of the natural sciences interested in some aspects of social science methodology. Examples of boundary blurring include emerging disciplines like social research of medicine, sociobiology, neuropsychology, bioeconomics and the history and sociology of science. Increasingly, quantitative research and qualitative methods are being integrated in the study of human action and its implications and consequences. In the first half of the 20th century, statistics became a free-standing discipline of applied mathematics. Statistical methods were used confidently.

In the contemporary period, Karl Popper and Talcott Parsons influenced the furtherance of the social sciences. Researchers continue to search for a unified consensus on what methodology might have the power and refinement to connect a proposed "grand theory" with the various midrange theories that, with considerable success, continue to provide usable frameworks for massive, growing data banks; for more, see consilience. The social sciences will for the foreseeable future be composed of different zones in the research of, and sometime distinct in approach toward, the field.

The term "social science" may refer either to the specific "sciences of society" established by thinkers such as Comte, Durkheim, Marx, and Weber, or more generally to all disciplines outside of "noble science" and arts. By the late 19th century, the academic social sciences were constituted of five fields: jurisprudence and amendment of the law, education, health, economy and trade, and art.

Around the start of the 21st century, the expanding domain of economics in the social sciences has been described as economic imperialism.

The social science disciplines are branches of knowledge taught and researched at the college or university level. Social science disciplines are defined and recognized by the academic journals in which research is published, and the learned social science societies and academic departments or faculties to which their practitioners belong. Social science fields of study usually have several sub-disciplines or branches, and the distinguishing lines between these are often both arbitrary and ambiguous.

Anthropology is the holistic "science of man", a science of the totality of human existence. The discipline deals with the integration of different aspects of the social sciences, humanities, and human biology. In the twentieth century, academic disciplines have often been institutionally divided into three broad domains. The "natural sciences" seek to derive general laws through reproducible and verifiable experiments. The "humanities" generally study local traditions, through their history, literature, music, and arts, with an emphasis on understanding particular individuals, events, or eras. The "social sciences" have generally attempted to develop scientific methods to understand social phenomena in a generalizable way, though usually with methods distinct from those of the natural sciences.

The anthropological social sciences often develop nuanced descriptions rather than the general laws derived in physics or chemistry, or they may explain individual cases through more general principles, as in many fields of psychology. Anthropology (like some fields of history) does not easily fit into one of these categories, and different branches of anthropology draw on one or more of these domains. Within the United States, anthropology is divided into four sub-fields: archaeology, physical or biological anthropology, anthropological linguistics, and cultural anthropology. It is an area that is offered at most undergraduate institutions. The word "anthropos" (ἄνθρωπος) in Ancient Greek means "human being" or "person". Eric Wolf described sociocultural anthropology as "the most scientific of the humanities, and the most humanistic of the sciences".

The goal of anthropology is to provide a holistic account of humans and human nature. This means that, though anthropologists generally specialize in only one sub-field, they always keep in mind the biological, linguistic, historic and cultural aspects of any problem. Since anthropology arose as a science in Western societies that were complex and industrial, a major trend within anthropology has been a methodological drive to study peoples in societies with more simple social organization, sometimes called "primitive" in anthropological literature, but without any connotation of "inferior". Today, anthropologists use terms such as "less complex" societies or refer to specific modes of subsistence or production, such as "pastoralist" or "forager" or "horticulturalist" to refer to humans living in non-industrial, non-Western cultures, such people or folk ("ethnos") remaining of great interest within anthropology.

The quest for holism leads most anthropologists to study a people in detail, using biogenetic, archaeological, and linguistic data alongside direct observation of contemporary customs. In the 1990s and 2000s, calls for clarification of what constitutes a culture, of how an observer knows where his or her own culture ends and another begins, and other crucial topics in writing anthropology were heard. It is possible to view all human cultures as part of one large, evolving global culture. These dynamic relationships, between what can be observed on the ground, as opposed to what can be observed by compiling many local observations remain fundamental in any kind of anthropology, whether cultural, biological, linguistic or archaeological.

Communication studies deals with processes of human communication, commonly defined as the sharing of symbols to create meaning. The discipline encompasses a range of topics, from face-to-face conversation to mass media outlets such as television broadcasting. Communication studies also examines how messages are interpreted through the political, cultural, economic, and social dimensions of their contexts. Communication is institutionalized under many different names at different universities, including "communication", "communication studies", "speech communication", "rhetorical studies", "communication science", "media studies", "communication arts", "mass communication", "media ecology", and "communication and media science".

Communication studies integrates aspects of both social sciences and the humanities. As a social science, the discipline often overlaps with sociology, psychology, anthropology, biology, political science, economics, and public policy, among others. From a humanities perspective, communication is concerned with rhetoric and persuasion (traditional graduate programs in communication studies trace their history to the rhetoricians of Ancient Greece). The field applies to outside disciplines as well, including engineering, architecture, mathematics, and information science.

Economics is a social science that seeks to analyze and describe the production, distribution, and consumption of wealth. The word "economics" is from the Ancient Greek "oikos", "family, household, estate", and νόμος "nomos", "custom, law", and hence means "household management" or "management of the state". An economist is a person using economic concepts and data in the course of employment, or someone who has earned a degree in the subject. The classic brief definition of economics, set out by Lionel Robbins in 1932, is "the science which studies human behavior as a relation between scarce means having alternative uses". Without scarcity and alternative uses, there is no economic problem. Briefer yet is "the study of how people seek to satisfy needs and wants" and "the study of the financial aspects of human behavior".

Economics has two broad branches: microeconomics, where the unit of analysis is the individual agent, such as a household or firm, and macroeconomics, where the unit of analysis is an economy as a whole. Another division of the subject distinguishes positive economics, which seeks to predict and explain economic phenomena, from normative economics, which orders choices and actions by some criterion; such orderings necessarily involve subjective value judgments. Since the early part of the 20th century, economics has focused largely on measurable quantities, employing both theoretical models and empirical analysis. Quantitative models, however, can be traced as far back as the physiocratic school. Economic reasoning has been increasingly applied in recent decades to other social situations such as politics, law, psychology, history, religion, marriage and family life, and other social interactions.

This paradigm crucially assumes (1) that resources are scarce because they are not sufficient to satisfy all wants, and (2) that "economic value" is willingness to pay as revealed for instance by market (arms' length) transactions. Rival heterodox schools of thought, such as institutional economics, green economics, Marxist economics, and economic sociology, make other grounding assumptions. For example, Marxist economics assumes that economics primarily deals with the investigation of exchange value, of which human labour is the source.

The expanding domain of economics in the social sciences has been described as economic imperialism.

Education encompasses teaching and learning specific skills, and also something less tangible but more profound: the imparting of knowledge, positive judgement and well-developed wisdom. Education has as one of its fundamental aspects the imparting of culture from generation to generation (see socialization). To educate means 'to draw out', from the Latin "educare", or to facilitate the realization of an individual's potential and talents. It is an application of pedagogy, a body of theoretical and applied research relating to teaching and learning and draws on many disciplines such as psychology, philosophy, computer science, linguistics, neuroscience, sociology and anthropology.

The education of an individual human begins at birth and continues throughout life. (Some believe that education begins even before birth, as evidenced by some parents' playing music or reading to the baby in the womb in the hope it will influence the child's development.) For some, the struggles and triumphs of daily life provide far more instruction than does formal schooling (thus Mark Twain's admonition to "never let school interfere with your education").

Geography as a discipline can be split broadly into two main sub fields: human geography and physical geography. The former focuses largely on the built environment and how space is created, viewed and managed by humans as well as the influence humans have on the space they occupy. This may involve cultural geography, transportation, health, military operations, and cities. The latter examines the natural environment and how the climate, vegetation and life, soil, oceans, water and landforms are produced and interact. Physical geography examines phenomena related to the measurement of earth. As a result of the two subfields using different approaches a third field has emerged, which is environmental geography. Environmental geography combines physical and human geography and looks at the interactions between the environment and humans. Other branches of geography include social geography, regional geography, and geomatics.

Geographers attempt to understand the Earth in terms of physical and spatial relationships. The first geographers focused on the science of mapmaking and finding ways to precisely project the surface of the earth. In this sense, geography bridges some gaps between the natural sciences and social sciences. Historical geography is often taught in a college in a unified Department of Geography.

Modern geography is an all-encompassing discipline, closely related to GISc, that seeks to understand humanity and its natural environment. The fields of urban planning, regional science, and planetology are closely related to geography. Practitioners of geography use many technologies and methods to collect data such as GIS, remote sensing, aerial photography, statistics, and global positioning systems (GPS).

History is the continuous, systematic narrative and research into past human events as interpreted through historiographical paradigms or theories.

History has a base in both the social sciences and the humanities. In the United States the National Endowment for the Humanities includes history in its definition of humanities (as it does for applied linguistics). However, the National Research Council classifies history as a social science. The historical method comprises the techniques and guidelines by which historians use primary sources and other evidence to research and then to write history. The Social Science History Association, formed in 1976, brings together scholars from numerous disciplines interested in social history.

The social science of law, jurisprudence, in common parlance, means a rule that (unlike a rule of ethics) is capable of enforcement through institutions. However, many laws are based on norms accepted by a community and thus have an ethical foundation. The study of law crosses the boundaries between the social sciences and humanities, depending on one's view of research into its objectives and effects. Law is not always enforceable, especially in the international relations context. It has been defined as a "system of rules", as an "interpretive concept" to achieve justice, as an "authority" to mediate people's interests, and even as "the command of a sovereign, backed by the threat of a sanction". However one likes to think of law, it is a completely central social institution. Legal policy incorporates the practical manifestation of thinking from almost every social science and the humanities. Laws are politics, because politicians create them. Law is philosophy, because moral and ethical persuasions shape their ideas. Law tells many of history's stories, because statutes, case law and codifications build up over time. And law is economics, because any rule about contract, tort, property law, labour law, company law and many more can have long-lasting effects on the distribution of wealth. The noun "law" derives from the late Old English "lagu", meaning something laid down or fixed and the adjective "legal" comes from the Latin word "lex".

Linguistics investigates the cognitive and social aspects of human language. The field is divided into areas that focus on aspects of the linguistic signal, such as syntax (the study of the rules that govern the structure of sentences), semantics (the study of meaning), morphology (the study of the structure of words), phonetics (the study of speech sounds) and phonology (the study of the abstract sound system of a particular language); however, work in areas like evolutionary linguistics (the study of the origins and evolution of language) and psycholinguistics (the study of psychological factors in human language) cut across these divisions.

The overwhelming majority of modern research in linguistics takes a predominantly synchronic perspective (focusing on language at a particular point in time), and a great deal of it—partly owing to the influence of Noam Chomsky—aims at formulating theories of the cognitive processing of language. However, language does not exist in a vacuum, or only in the brain, and approaches like contact linguistics, creole studies, discourse analysis, social interactional linguistics, and sociolinguistics explore language in its social context. Sociolinguistics often makes use of traditional quantitative analysis and statistics in investigating the frequency of features, while some disciplines, like contact linguistics, focus on qualitative analysis. While certain areas of linguistics can thus be understood as clearly falling within the social sciences, other areas, like acoustic phonetics and neurolinguistics, draw on the natural sciences. Linguistics draws only secondarily on the humanities, which played a rather greater role in linguistic inquiry in the 19th and early 20th centuries. Ferdinand Saussure is considered the father of modern linguistics.

Political science is an academic and research discipline that deals with the theory and practice of politics and the description and analysis of political systems and political behaviour. Fields and subfields of political science include political economy, political theory and philosophy, civics and comparative politics, theory of direct democracy, apolitical governance, participatory direct democracy, national systems, cross-national political analysis, political development, international relations, foreign policy, international law, politics, public administration, administrative behaviour, public law, judicial behaviour, and public policy. Political science also studies power in international relations and the theory of great powers and superpowers.

Political science is methodologically diverse, although recent years have witnessed an upsurge in the use of the scientific method, that is, the proliferation of formal-deductive model building and quantitative hypothesis testing. Approaches to the discipline include rational choice, classical political philosophy, interpretivism, structuralism, and behaviouralism, realism, pluralism, and institutionalism. Political science, as one of the social sciences, uses methods and techniques that relate to the kinds of inquiries sought: primary sources such as historical documents, interviews, and official records, as well as secondary sources such as scholarly articles are used in building and testing theories. Empirical methods include survey research, statistical analysis or econometrics, case studies, experiments, and model building. Herbert Baxter Adams is credited with coining the phrase "political science" while teaching history at Johns Hopkins University.

Psychology is an academic and applied field involving the study of behaviour and mental processes. Psychology also refers to the application of such knowledge to various spheres of human activity, including problems of individuals' daily lives and the treatment of mental illness. The word "psychology" comes from the Ancient Greek ψυχή "psyche" ("soul", "mind") and "logy" ("study").

Psychology differs from anthropology, economics, political science, and sociology in seeking to capture explanatory generalizations about the mental function and overt behaviour of individuals, while the other disciplines focus on creating descriptive generalizations about the functioning of social groups or situation-specific human behaviour. In practice, however, there is quite a lot of cross-fertilization that takes place among the various fields. Psychology differs from biology and neuroscience in that it is primarily concerned with the interaction of mental processes and behaviour, and of the overall processes of a system, and not simply the biological or neural processes themselves, though the subfield of neuropsychology combines the study of the actual neural processes with the study of the mental effects they have subjectively produced.
Many people associate psychology with clinical psychology, which focuses on assessment and treatment of problems in living and psychopathology. In reality, psychology has myriad specialties including social psychology, developmental psychology, cognitive psychology, educational psychology, industrial-organizational psychology, mathematical psychology, neuropsychology, and quantitative analysis of behaviour.

Psychology is a very broad science that is rarely tackled as a whole, major block. Although some subfields encompass a natural science base and a social science application, others can be clearly distinguished as having little to do with the social sciences or having a lot to do with the social sciences. For example, biological psychology is considered a natural science with a social scientific application (as is clinical medicine), social and occupational psychology are, generally speaking, purely social sciences, whereas neuropsychology is a natural science that lacks application out of the scientific tradition entirely. In British universities, emphasis on what tenet of psychology a student has studied and/or concentrated is communicated through the degree conferred: B.Psy. indicates a balance between natural and social sciences, B.Sc. indicates a strong (or entire) scientific concentration, whereas a B.A. underlines a majority of social science credits. This is not always necessarily the case however, and in many UK institutions students studying the B.Psy, B.Sc, and B.A. follow the same curriculum as outlined by The British Psychological Society and have the same options of specialism open to them regardless of whether they choose a balance, a heavy science basis, or heavy social science basis to their degree. If they applied to read the B.A. for example, but specialized in heavily science-based modules, then they will still generally be awarded the B.A.

Sociology is the systematic study of society, individuals' relationship to their societies, the consequences of difference, and other aspects of human social action. The meaning of the word comes from the suffix "-logy", which means "study of", derived from Ancient Greek, and the stem "soci-", which is from the Latin word "socius", meaning "companion", or society in general.

Auguste Comte (1798–1857) coined the term, Sociology, as a way to apply natural science principles and techniques to the social world in 1838. Comte endeavoured to unify history, psychology and economics through the descriptive understanding of the social realm. He proposed that social ills could be remedied through sociological positivism, an epistemological approach outlined in "The Course in Positive Philosophy" [1830–1842] and "A General View of Positivism" (1844). Though Comte is generally regarded as the "Father of Sociology", the discipline was formally established by another French thinker, Émile Durkheim (1858–1917), who developed positivism as a foundation to practical social research. Durkheim set up the first European department of sociology at the University of Bordeaux in 1895, publishing his "Rules of the Sociological Method". In 1896, he established the journal "L'Année Sociologique". Durkheim's seminal monograph, "Suicide" (1897), a case study of suicide rates among Catholic and Protestant populations, distinguished sociological analysis from psychology or philosophy.

Karl Marx rejected Comte's positivism but nevertheless aimed to establish a "science of society" based on historical materialism, becoming recognized as a founding figure of sociology posthumously as the term gained broader meaning. Around the start of the 20th century, the first wave of German sociologists, including Max Weber and Georg Simmel, developed sociological antipositivism. The field may be broadly recognized as an amalgam of three modes of social thought in particular: Durkheimian positivism and structural functionalism; Marxist historical materialism and conflict theory; and Weberian antipositivism and verstehen analysis. American sociology broadly arose on a separate trajectory, with little Marxist influence, an emphasis on rigorous experimental methodology, and a closer association with pragmatism and social psychology. In the 1920s, the Chicago school developed symbolic interactionism. Meanwhile, in the 1930s, the Frankfurt School pioneered the idea of critical theory, an interdisciplinary form of Marxist sociology drawing upon thinkers as diverse as Sigmund Freud and Friedrich Nietzsche. Critical theory would take on something of a life of its own after World War II, influencing literary criticism and the Birmingham School establishment of cultural studies.

Sociology evolved as an academic response to the challenges of modernity, such as industrialization, urbanization, secularization, and a perceived process of enveloping rationalization. The field generally concerns the social rules and processes that bind and separate people not only as individuals, but as members of associations, groups, communities and institutions, and includes the examination of the organization and development of human social life. The sociological field of interest ranges from the analysis of short contacts between anonymous individuals on the street to the study of global social processes. In the terms of sociologists Peter L. Berger and Thomas Luckmann, social scientists seek an understanding of the "Social Construction of Reality". Most sociologists work in one or more subfields. One useful way to describe the discipline is as a cluster of sub-fields that examine different dimensions of society. For example, social stratification studies inequality and class structure; demography studies changes in a population size or type; criminology examines criminal behaviour and deviance; and political sociology studies the interaction between society and state.

Since its inception, sociological epistemologies, methods, and frames of enquiry, have significantly expanded and diverged. Sociologists use a diversity of research methods, collect both quantitative and qualitative data, draw upon empirical techniques, and engage critical theory. Common modern methods include case studies, historical research, interviewing, participant observation, social network analysis, survey research, statistical analysis, and model building, among other approaches. Since the late 1970s, many sociologists have tried to make the discipline useful for purposes beyond the academy. The results of sociological research aid educators, lawmakers, administrators, developers, and others interested in resolving social problems and formulating public policy, through subdisciplinary areas such as evaluation research, methodological assessment, and public sociology.

In the early 1970s, women sociologists began to question sociological paradigms and the invisibility of women in sociological studies, analysis, and courses. In 1969, feminist sociologists challenged the discipline's androcentrism at the American Sociological Association's annual conference. This led to the founding of the organization Sociologists for Women in Society, and, eventually, a new sociology journal, Gender & Society. Today, the sociology of gender is considered to be one of the most prominent sub-fields in the discipline.

New sociological sub-fields continue to appear — such as community studies, computational sociology, environmental sociology, network analysis, actor-network theory, gender studies, and a growing list, many of which are cross-disciplinary in nature.

Additional applied or interdisciplinary fields related to the social sciences include:

The origin of the survey can be traced back at least early as the Domesday Book in 1086, while some scholars pinpoint the origin of demography to 1663 with the publication of John Graunt's "Natural and Political Observations upon the Bills of Mortality". Social research began most intentionally, however, with the positivist philosophy of science in the 19th century.

In contemporary usage, "social research" is a relatively autonomous term, encompassing the work of practitioners from various disciplines that share in its aims and methods. Social scientists employ a range of methods in order to analyse a vast breadth of social phenomena; from census survey data derived from millions of individuals, to the in-depth analysis of a single agent's social experiences; from monitoring what is happening on contemporary streets, to the investigation of ancient historical documents. The methods originally rooted in classical sociology and statistical mathematics have formed the basis for research in other disciplines, such as political science, media studies, and marketing and market research.

Social research methods may be divided into two broad schools:

Social scientists will commonly combine quantitative and qualitative approaches as part of a multi-strategy design. Questionnaires, field-based data collection, archival database information and laboratory-based data collections are some of the measurement techniques used. It is noted the importance of measurement and analysis, focusing on the (difficult to achieve) goal of objective research or statistical hypothesis testing. A mathematical model uses mathematical language to describe a system. The process of developing a mathematical model is termed 'mathematical modelling' (also modeling). Eykhoff (1974) defined a "mathematical model" as 'a representation of the essential aspects of an existing system (or a system to be constructed) that presents knowledge of that system in usable form'. Mathematical models can take many forms, including but not limited to dynamical systems, statistical models, differential equations, or game theoretic models.

These and other types of models can overlap, with a given model involving a variety of abstract structures. The system is a set of interacting or interdependent entities, real or abstract, forming an integrated whole. The concept of an "integrated whole" can also be stated in terms of a system embodying a set of relationships that are differentiated from relationships of the set to other elements, and from relationships between an element of the set and elements not a part of the relational regime. A dynamical system modeled as a mathematical formalization has a fixed "rule" that describes the time dependence of a point's position in its ambient space. Small changes in the state of the system correspond to small changes in the numbers. The "evolution rule" of the dynamical system is a fixed rule that describes what future states follow from the current state. The rule is deterministic: for a given time interval only one future state follows from the current state.

Social scientists often conduct program evaluation, which is a systematic method for collecting, analyzing, and using information to answer questions about projects, policies and programs, particularly about their effectiveness and efficiency. In both the public and private sectors, stakeholders often want to know whether the programs they are funding, implementing, voting for, receiving or objecting to are producing the intended effect. While program evaluation first focuses around this definition, important considerations often include how much the program costs per participant, how the program could be improved, whether the program is worthwhile, whether there are better alternatives, if there are unintended outcomes, and whether the program goals are appropriate and useful.

Some social theorists emphasize the subjective nature of research. These writers espouse social theory perspectives that include various types of the following:

Other fringe social theorists delve in alternative nature of research. These writers share social theory perspectives that include various types of the following:

Most universities offer degrees in social science fields. The Bachelor of Social Science is a degree targeted at the social sciences in particular, it is often more flexible and in-depth than other degrees that include social science subjects.

In the United States, a university may offer a student who studies a social sciences field a Bachelor of Arts degree, particularly if the field is within one of the traditional liberal arts such as history, or a BSc: Bachelor of Science degree such as those given by the London School of Economics, as the social sciences constitute one of the two main branches of science (the other being the natural sciences). In addition, some institutions have degrees for a particular social science, such as the Bachelor of Economics degree, though such specialized degrees are relatively rare in the United States.

Graduate students may get a Master's degree (Master of Arts, Master of Science or a field-specific degree such as Master of Public Administration) or Ph.D.

The social sciences receive less funding than the natural sciences. It has been estimated that only 0.12% of all funding for climate-related research is spent on the social science of climate change mitigation. Vastly more funding is spent on natural science studies of climate change and considerable sums are also spent on studies of impact of and adaptation to climate change. It has been argued that this is a misallocation of resources, as the most urgent puzzle at the current juncture is to work out how to change human behavior to mitigate climate change, whereas the natural science of climate change is already well established and there will be decades and centuries to handle adaptation.















</doc>
<doc id="26783" url="https://en.wikipedia.org/wiki?curid=26783" title="Statute">
Statute

A statute is a formal written enactment of a legislative authority that governs the legal entities of a city, state, or country by way of consent. Typically, statutes command or prohibit something, or declare policy. Statutes are rules made by legislative bodies; they are distinguished from case law or precedent, which is decided by courts, and regulations issued by government agencies.

In virtually all countries, newly enacted statutes are published and distributed so that everyone can look up the statutory law. This can be done in the form of a government gazette which may include other kinds of legal notices released by the government, or in the form of a series of books whose content is limited to legislative acts. In either form, statutes are traditionally published in chronological order based on date of enactment. 

A universal problem encountered by lawmakers throughout human history is how to organize published statutes. Such publications have a habit of starting small but growing rapidly over time, as new statutes are enacted in response to the exigencies of the moment. Eventually, persons trying to find the law are forced to sort through an enormous number of statutes enacted at various points in time to determine which portions are still in effect.
The solution adopted in many countries is to organize existing statutory law in topical arrangements (or "codified") within publications called codes, then ensure that new statutes are consistently drafted so that they add, amend, repeal or move various code sections. In turn, in theory, the code will thenceforth reflect the current cumulative state of the statutory law in that jurisdiction. In many nations statutory law is distinguished from and subordinate to constitutional law.

The term statute is also used to refer to an International treaty that establishes an institution, such as the Statute of the European Central Bank, a protocol to the international courts as well, such as the Statute of the International Court of Justice and the Rome Statute of the International Criminal Court. Statute is also another word for law. The term was adapted from England in about the 18th century.

In the autonomous communities of Spain, an autonomy statute is a legal document similar to the constitution of a federated state, save that it is enacted by the national legislature, rather than the autonomous community it governs. The autonomy statutes in Spain have the rank of "ley organica" (organic law), a category of special legislation reserved only for the main institutions and issues and mentioned in the constitution (the highest ranking legal instrument in Spain). "Leyes organicas" rank between the constitution and ordinary laws. The name was chosen, among others, to avoid confusion with the term "constitution" (i.e. the Spanish constitution of 1978).

In biblical terminology, statute (Hebrew "choq") refers to a law given without any reason or justification. The classic example is the statute regarding the Red Heifer.(Numbers 19:2)

The opposite of a chok is a "mishpat", a law given for a specified reason, e.g. the Sabbath laws, which were given because "God created the world in six days, but on the seventh day He rested" (Genesis 2:2-3).

"That which upholds, supports or maintains the regulatory order of the universe" meaning the "Law" or "Natural Law". This is a concept of central importance in Indian philosophy and religion.



</doc>
<doc id="26784" url="https://en.wikipedia.org/wiki?curid=26784" title="Statutory law">
Statutory law

Statutory law or statute law is written law passed by a body of legislature. This is as opposed to oral or customary law; or regulatory law promulgated by the executive or common law of the judiciary. 
Statutes may originate with national, state legislatures or local municipalities.

The term codified law refers to statutes that have been organized ("codified") by subject matter; in this narrower sense, some but not all statutes are considered "codified." The entire body of codified statute is referred to as a "code," such as the United States Code, the Ohio Revised Code or the Code of Canon Law. The substantive provisions of the Act could be codified (arranged by subject matter) in one or more titles of the United States Code while the provisions of the law that have not reached their "effective date" (remaining uncodified) would be available by reference to the United States Statutes at Large. Another meaning of "codified law" is a statute that takes the common law in a certain area of the law and puts it in statute or code form.

Another example of statutes that are not typically codified is a "private law" that may originate as a private bill, a law affecting only one person or a small group of persons. An example was divorce in Canada prior to the passage of the Divorce Act of 1968. If unavailable by administrative or judicial means, it was possible to obtain a legislative divorce by application to the Senate of Canada, which reviewed and investigated petitions for divorce, which would then be voted upon by the Senate and subsequently made into law. 

In the United Kingdom Parliament, private bills were used in the nineteenth century to create corporations, grant monopolies and give individuals attention to be more fully considered by the parliament. The government may also seek to have a bill introduced "unofficially" by a backbencher so as not to create a public scandal; such bills may also be introduced by the loyal opposition — members of the opposition party or parties. Sometimes a private member's bill may also have private bill aspects, in such case the proposed legislation is called a hybrid bill.




</doc>
<doc id="26785" url="https://en.wikipedia.org/wiki?curid=26785" title="Sanction">
Sanction

A sanction may be either a permission or a restriction, depending upon context, as the word is an auto-antonym.

Examples of sanctions include:





</doc>
<doc id="26786" url="https://en.wikipedia.org/wiki?curid=26786" title="Sarajevo">
Sarajevo

Sarajevo ( ; , ; "see ") is the capital and largest city of Bosnia and Herzegovina, with a population of 275,524 in its administrative limits. The Sarajevo metropolitan area, including Sarajevo Canton, East Sarajevo and nearby municipalities, is home to 555,210 inhabitants. Nestled within the greater Sarajevo valley of Bosnia, it is surrounded by the Dinaric Alps and situated along the Miljacka River in the heart of the Balkans.

Sarajevo is the political, financial, social and cultural center of Bosnia and Herzegovina and a prominent center of culture in the Balkans, with region-wide influence in entertainment, media, fashion and the arts.

Due to its long history of religious and cultural diversity, Sarajevo is sometimes called the "Jerusalem of Europe" or "Jerusalem of the Balkans". It is one of only a few major European cities to have a mosque, Catholic church, Orthodox church and synagogue within the same neighborhood. A regional center in education, the city is home to the Balkans' first institution of tertiary education in the form of an Islamic madrasa, today part of the University of Sarajevo.

Although settlement in the area stretches back to prehistoric times, the modern city arose as an Ottoman stronghold in the 15th century. Sarajevo has attracted international attention several times throughout its history. In 1885, Sarajevo was the first city in Europe and the second city in the world to have a full-time electric tram network running through the city, following San Francisco. In 1914, it was the site of the assassination of Archduke Franz Ferdinand of Austria by local Young Bosnia activist Gavrilo Princip that sparked World War I, which also ended Austro-Hungarian rule in Bosnia and resulted in the creation of the Kingdom of Yugoslavia. Later, after World War II, the establishment of the Socialist Republic of Bosnia and Herzegovina within the Second Yugoslavia led to a massive expansion of Sarajevo, then the constituent republic's capital, which culminated with the hosting of the 1984 Winter Olympics marking a prosperous era for the city. However, after the start of the Yugoslav Wars, for 1,425 days, from April 1992 to February 1996, the city suffered the longest siege of a capital city in the history of modern warfare, during the Bosnian War and the breakup of Yugoslavia.

Sarajevo has been undergoing post-war reconstruction, and is the fastest growing city in Bosnia and Herzegovina. The travel guide series "Lonely Planet" has named Sarajevo as the 43rd best city in the world, and in December 2009 listed Sarajevo as one of the top ten cities to visit in 2010. In 2011, Sarajevo was nominated to be the 2014 European Capital of Culture and in 2019, it hosted the European Youth Olympic Festival. In October 2019, Sarajevo was designated as a UNESCO Creative City for placing culture at the center of its development strategies, and is one of the world's eighteen Cities of Film. However, the UNESCO activity doesn't stop here; there is a UNESCO tentative monument, the Old Jewish Cemetery, almost 500 years old site that is the second-largest Jewish sepulchral complex in Europe, the one in Prague being the largest. It is also one of the most significant memorial complexes in the world. It represents the eternal proof of coexistence of two or more different confessions under different administrations and rules, and the proof of mutual respect and tolerance.

The earliest known name for the small central Bosnian region of today's Sarajevo is Vrhbosna.

The name "Sarajevo" derives from the Turkish noun "saray", meaning "palace" or "mansion" (from the Persian "sarāy", "house, palace"). The letter "j" in the Bosnian language is equivalent soundwise to the English letter "y" as in "boy" and "yet". The "evo" portion may come from the term "saray ovası" first recorded in 1455, meaning "the plains around the palace" or simply "palace plains".
However, in his Dictionary of Turkish loanwords, Abdulah Škaljić maintains that the ""evo"" ending is more likely to have come from the widespread Slavic suffix ""evo"" used to indicate place names, than from the Turkish ending ""ov"a", as proposed by some. The first mention of name Sarajevo was in 1507 letter written by Feriz Beg. The official name during the 400-year Ottoman period was "Saraybosna" (Palace of Bosnia), and it is still known by that name in modern Turkish.

Sarajevo has had many nicknames. The earliest is "Šeher", which is the term Isa-Beg Ishaković used to describe the town he was going to build. It is a Turkish word meaning an advanced city of key importance ("şehir") which in turn comes from "shahr" (city). As Sarajevo developed, numerous nicknames came from comparisons to other cities in the Islamic world, i.e. "Damascus of the North". The most popular of these was "European Jerusalem".

Sarajevo is near the geometric center of the triangular-shaped Bosnia-Herzegovina and within the historical region of Bosnia proper. It is situated above sea level and lies in the Sarajevo valley, in the middle of the Dinaric Alps. The valley itself once formed a vast expanse of greenery, but gave way to urban expansion and development in the post-World War II era. The city is surrounded by heavily forested hills and five major mountains. The highest of the surrounding peaks is Treskavica at , then Bjelašnica mountain at , Jahorina at , Trebević at , with Igman being the shortest. The last four are also known as the Olympic Mountains of Sarajevo (see also 1984 Winter Olympics). The city itself has its fair share of hilly terrain, as evidenced by the many steeply inclined streets and residences seemingly perched on the hillsides.

The Miljacka river is one of the city's chief geographic features. It flows through the city from east through the center of Sarajevo to west part of city where eventually meets up with the Bosna river. Miljacka river is "The Sarajevo River", with its source ("Vrelo Miljacke") south of the town of Pale at the foothills of Mount Jahorina, several kilometers to the east of Sarajevo center. The Bosna's source, Vrelo Bosne near Ilidža (west Sarajevo), is another notable natural landmark and a popular destination for Sarajevans and other tourists. Several smaller rivers and streams such as Koševski Potok also run through the city and its vicinity.

Sarajevo is close to the center of the triangular shape of Bosnia and Herzegovina in southeastern Europe. Sarajevo city proper consists of four municipalities (or "in Bosnian and Croatian: općina, in Serbian: opština"): Centar (Center), Novi Grad (New City), Novo Sarajevo (New Sarajevo), and Stari Grad (Old City), while Metropolitan area of Sarajevo (Greater Sarajevo area) includes these and the neighbouring municipalities of Ilidža, Hadžići, Vogošća and Ilijaš.

The Metropolitan area was reduced in the 1990s after the war and the Dayton-imposed administrative division of the country, with several municipalities partitioned along the border of the newly recognised Federation of Bosnia and Herzegovina (FBiH) and Republica Srpska (RS), creating several new municipalities which together form the city of Istočno Sarajevo in the Republica Srpska: Istočna Ilidza, Istočno Novo Sarajevo, Istočni Stari Grad, Lukavica, Pale (RS-section), and Trnovo (RS-section), along with the municipality of Sokolac (which was not traditionally part of the Sarajevo area and was not partitioned)

The city has an urban area of . Veliki Park (Great park) is the largest green area in the center of Sarajevo. It's nestled between Titova, Koševo, Džidžikovac, Tina Ujevića and Trampina Streets and in the lower part there is a monument dedicated to the Children of Sarajevo.

Sarajevo has either a humid continental climate (Köppen climate classification: Dfb), or an oceanic climate (Köppen climate classification: Cfb), depending on if either the 0 °C or the -3 °C isotherms are used. Sarajevo's climate exhibits four seasons and uniformly spread precipitation, typical of both Cfb and Dfb climates. The proximity of the Adriatic Sea moderates Sarajevo's climate somewhat, although the mountains to the south of the city greatly reduce this maritime influence. The average yearly temperature is , with January ( on average) being the coldest month of the year and July ( on average) the warmest.

The highest recorded temperature was on 19 August 1946, and on 23 August 2008 (41.0) while the lowest recorded temperature was on 25 January 1942. On average, Sarajevo has 7 days where the temperature exceeds and 4 days where the temperature drops below per year. The city typically experiences mildly cloudy skies, with an average yearly cloud cover of 45%.

The cloudiest month is December (75% average cloud cover) while the clearest is August (37%). Moderate precipitation occurs fairly consistently throughout the year, with an average 75 days of rainfall. Suitable climatic conditions have allowed winter sports to flourish in the region, as exemplified by the Winter Olympics in 1984 that were celebrated in Sarajevo. Average winds are and the city has 1,769 hours of sunshine.

Air pollution is a major issue in Sarajevo. According to the 2016 World Health Organization's Ambient Air Pollution Database, the annual average PM2.5 concentration in 2010 was estimated to be 30 μg/m based on PM10 measurement, which is 3 times higher than recommended by WHO Air Quality Guidelines for annual average PM2.5. There are no recent direct long-term PM2.5 measurements available in Sarajevo and only estimates can be made from PM10, which is the less health-relevant than PM2.5. Real-time air quality data in the form of PM10, ozone, NO, CO and SO by the Federal Hydrometeorological Institute.

One of the earliest findings of settlement in the Sarajevo area is that of the Neolithic Butmir culture. The discoveries at Butmir were made on the grounds of the modern-day Sarajevo suburb Ilidža in 1893 by Austro-Hungarian authorities during the construction of an agricultural school. The area's richness in flint was attractive to Neolithic humans, and the settlement flourished. The settlement developed unique ceramics and pottery designs, which characterize the Butmir people as a unique culture, as described at the International Congress of Archaeologists and Anthropologists meeting in Sarajevo in 1894.

The next prominent culture in Sarajevo were the Illyrians. The ancient people, who considered most of the West Balkans as their homeland, had several key settlements in the region, mostly around the river Miljacka and the Sarajevo valley. The Illyrians in the Sarajevo region belonged to the "Daesitiates", the last Illyrian people in Bosnia and Herzegovina to resist Roman occupation. Their defeat by the Roman emperor Tiberius in 9 AD marks the start of Roman rule in the region. The Romans never built up the region of modern-day Bosnia, but the Roman colony of Aquae Sulphurae was near the top of present-day Ilidža, and was the most important settlement of the time. After the Romans, the Goths settled the area, followed by the Slavs in the 7th century.

During the Middle Ages Sarajevo was part of the Bosnian province of Vrhbosna near the traditional center of the Kingdom of Bosnia. Though a city named "Vrhbosna" existed, the exact settlement in Sarajevo at this time is debated. Various documents note a place called "Tornik" in the region, most likely in the area of Marijin Dvor neighborhood. By all indications, Tornik was a very small marketplace surrounded by a proportionally small village, and was not considered very important by Ragusan merchants.

Other scholars say that "Vrhbosna" was a major town in the wider area of modern-day Sarajevo. Papal documents say that in 1238, a cathedral dedicated to Saint Paul was built in the area. Disciples of the notable saints Cyril and Methodius stopped in the region, founding a church near Vrelo Bosna. Whether or not the town was somewhere in the area of modern-day Sarajevo, the documents attest to its and the region's importance. There was also a citadel Hodidjed north-east to Old City, dating from around 1263 until it was occupied by the Ottoman Empire in 1429.

Sarajevo was founded by the Ottoman Empire in the 1450s upon its conquest of the region, with 1461 used as the city's founding date. The first Ottoman governor of Bosnia, Isa-Beg Ishaković, transformed the cluster of villages into a city and state capital by building a number of key structures, including a mosque, a closed marketplace, a public bath, a hostel, and of course the governor's castle ("Saray") which gave the city its present name. The mosque was named "Careva Džamija" (the Tsar's Mosque) in honor of the Sultan Mehmed II. With the improvements Sarajevo quickly grew into the largest city in the region. By the 15th Century the settlement was established as a city, named "Bosna-Saraj", around the citadel in 1461.

Following the expulsion of Jews from Spain at the end of the 15th century, and the invitation from the Ottoman Empire to resettle their population, Sephardic Jews arrived in Sarajevo, which over time would become a leading center of Sephardic culture and the Ladino language. Though relatively small in size, a Jewish quarter would develop over several blocks in Baščaršija.

Many local Christians converted to Islam at this time. To accommodate the new pilgrims on the road to Mecca, in 1541 Gazi Husrev-Bey’s quartermaster Vekil-Harrach built a Pilgrim's mosque for which it is still known to this day Hadžijska mosque.

Under leaders such as the second governor Gazi Husrev-beg, Sarajevo grew at a rapid rate. Husrev-beg greatly shaped the physical city, as most of what is now the Old Town was built during his reign. Sarajevo became known for its large marketplace and numerous mosques, which by the middle of the 16th century numbered more than 100. At the peak of the empire, Sarajevo was the biggest and most important Ottoman city in the Balkans after Istanbul. By 1660, the population of Sarajevo was estimated to be over 80,000. By contrast, Belgrade in 1683 had 100.000, and Zagreb as late as 1851 had 14,000 people. As political conditions changed, Sarajevo became the site of warfare.

In 1697, during the Great Turkish War, a raid was led by Prince Eugene of Savoy of the Habsburg Monarchy against the Ottoman Empire, which conquered Sarajevo and left it plague-infected and burned to the ground. After his men had looted thoroughly, they set the city on fire and destroyed nearly all of it in one day. Only a handful of neighborhoods, some mosques, and an Orthodox church, were left standing. Numerous other fires weakened the city, which was later rebuilt but never fully recovered from the destruction. By 1807, it had only some 60,000 residents.

In the 1830s, several battles of the Bosnian uprising had taken place around the city. These had been led by Husein Gradaščević. Today, a major city street is named "Zmaj od Bosne" (Dragon of Bosnia) in his honor. The rebellion failed and for several more decades the Ottoman state remained in control of Bosnia.

The Ottoman Empire made Sarajevo an important administrative centre by 1850. Baščaršija became the central commercial district and cultural center of the city in the 15th century when Isa-Beg Isaković founded the town. The toponym Baščaršija derives from the Turkish language.

Austria-Hungary's occupation of Bosnia and Herzegovina came in 1878 as part of the Treaty of Berlin, and complete annexation followed in 1908, angering the Serbs. Sarajevo was industrialized by Austria-Hungary, who used the city as a testing area for new inventions such as tramways, which were established in 1885 before they were later installed in Vienna. Architects and engineers wanting to help rebuild Sarajevo as a modern European capital rushed to the city. A fire that burned down a large part of the central city area ("čaršija") left more room for redevelopment. As a result, the city has a unique blend of the remaining Ottoman city market and contemporary western architecture. Sarajevo also has some examples of Secession- and Pseudo-Moorish styles that date from this period.

The Austro-Hungarian period was one of great development for the city, as the Western power brought its new acquisition up to the standards of the Victorian age. Various factories and other buildings were built at this time, and a large number of institutions were both Westernized and modernized. For the first time in history, Sarajevo's population began writing in Latin script.
For the first time in centuries, the city significantly expanded outside its traditional borders. Much of the city's contemporary central municipality (Centar) was constructed during this period.

Architecture in Sarajevo quickly developed into a wide range of styles and buildings. The Cathedral of Sacred Heart, for example, was constructed using elements of neo-gothic and Romanesque architecture. The National Museum, Sarajevo brewery, and City Hall were also constructed during this period. Additionally, Austrian officials made Sarajevo the first city in this part of Europe to have a tramway.

Although the Bosnia Vilayet "de jure" remained part of the Ottoman Empire, it was "de facto" governed as an integral part of Austria-Hungary with the Ottomans having no say in its day-to-day governance. This lasted until 1908 when the territory was formally annexed and turned into a condominium, jointly controlled by both Austrian Cisleithania and Hungarian Transleithania.

In the event that triggered World War I, the Archduke Franz Ferdinand of Austria was assassinated, along with his wife Sophie, Duchess of Hohenberg in Sarajevo on 28 June 1914 by Gavrilo Princip, a Bosnian and self-declared Yugoslav, and member of Young Bosnia. This was followed by the Anti-Serb riots in Sarajevo, which resulted in two deaths and destruction of property.

In the ensuing war, however, most of the Balkan offensives occurred near Belgrade, and Sarajevo largely escaped damage and destruction. Following the war, Bosnia was annexed into the Kingdom of Yugoslavia, and Sarajevo became the capital of the Drina Province.

After World War I and pressure from the Royal Serbian Army, alongside rebelling Slavic nations in Austria-Hungary, Sarajevo became part of the Kingdom of Yugoslavia. Though it held some political significance as the center of first the Bosnian region and then the Drinska Banovina, the city was no longer a national capital and saw a decline in global influence.

During World War II the Kingdom of Yugoslavia's army was overrun by German and Italian forces. Following a German bombing campaign, Sarajevo was captured on 15 April 1941 by the 16th Motorized infantry Division. The Axis powers created the Independent State of Croatia and included Sarajevo in its territory.

Immediately following the occupation, the main Sephardi Jewish synagogue, Il Kal Grande, was looted, burned, and destroyed by the Nazis. Within a matter of months, the centuries-old Sephardi and Ashkenazi Jewish communities of Sarajevo, comprising the vast majority of Bosnian Jewry, would be rounded up in the Old Synagogue (Stari hram) and deported to their deaths in Croatian concentration camps. Roughly 85% of Bosnia's Jewish population would perish at the hands of the Nazis and the Ustaše during the Holocaust. The Sarajevo Haggadah was the most important artifact which survived this period, smuggled out of Sarajevo and saved from the Nazis and Ustaše by the chief librarian of the National Museum, Derviš Korkut.

On 12 October 1941, a group of 108 notable Bosniak citizens of Sarajevo signed the Resolution of Sarajevo Muslims by which they condemned the Genocide of Serbs organized by the Ustaše, made a distinction between the Bosniaks who participated in such persecutions and the rest of the Bosniak population, presented information about the persecutions of Bosniaks by Serbs, and requested security for all citizens of the country, regardless of their identity. During the summer of 1941, Ustashe militia periodically interned and executed groups of Sarajevo Serbs. In August 1941, they arrested about one hundred Serbs suspected of ties to the resistance armies, mostly church officials and members of the intelligentsia, and executed them or deported the to concentration camps. By mid-summer 1942, around 20,000 Serbs found refuge in Sarajevo from Ustaše terror.

The city was bombed by the Allies from 1943 to 1944. The Yugoslav Partisan movement was represented in the city. In period February–May 1945 Maks Luburić set up Ustaše headquarters in a building known as Villa Luburić and used it as torture and execution place whose 323 victims were identified after the war. Resistance was led by Vladimir "Walter" Perić, who died while leading the liberation of the city on 6 April 1945.

After the war, Sarajevo was the capital of the Socialist Republic of Bosnia and Herzegovina within the Socialist Federal Republic of Yugoslavia. The Republic Government invested heavily in Sarajevo, building many new residential blocks in Novi Grad Municipality and Novo Sarajevo Municipality, while simultaneously developing the city's industry and transforming Sarajevo into a modern city. Sarajevo grew rapidly as it became an important regional industrial center in Yugoslavia. Between the end of the war and the end of Yugoslavia, the city grew from a population of 115,000 to more than 600,000 people. The Vraca Memorial Park, a monument for victims of World War II, was dedicated on 25 November, the "Day of Statehood of Bosnia and Herzegovina" when the ZAVNOBIH held their first meeting in 1943.

A crowning moment of Sarajevo's time in Socialist Yugoslavia was the 1984 Winter Olympics. Sarajevo beat out Sapporo, Japan; and Falun/Göteborg, Sweden to host the Olympic games. The games were followed by a tourism boom, making the 1980s one of the city's most prosperous decades.

The Bosnian War for independence resulted in large-scale destruction and dramatic population shifts during the Siege of Sarajevo between 1992 and 1996. Thousands of Sarajevans lost their lives under the constant bombardment and sniper shooting at civilians by the Serb forces during the siege, the longest siege of a capital city in the history of modern warfare. Bosnian Serb forces of the Republika Srpska and the Yugoslav People's Army besieged Sarajevo from 5 April 1992 to 29 February 1996.

When Bosnia and Herzegovina declared independence from Yugoslavia and achieved United Nations recognition, Serbian leaders declared a new Serbian national state Republika Srpska (RS) which was carved out from the territory of Bosnia and Herzegovina. The Army of Republika Srpska encircled Sarajevo with a siege force of 18,000 stationed in the surrounding hills, from which they assaulted the city with artillery, mortars, tanks, anti-aircraft guns, heavy machine-guns, multiple rocket launchers, rocket-launched aircraft bombs, and sniper rifles. From 2 May 1992, the Serbs blockaded the city. The Bosnian government defence forces inside the besieged city were poorly equipped and unable to break the siege.

During the siege, 11,541 people lost their lives, including over 1,500 children. An additional 56,000 people were wounded, including nearly 15,000 children. The 1991 census indicates that before the siege the city and its surrounding areas had a population of 525,980.

When the siege ended, the concrete scars caused by mortar shell explosions left marks that were filled with red resin. After the red resin was placed, it left floral patterns which led to them being dubbed Sarajevo Roses. Division of the territory according to the Dayton Agreement resulted in a mass exodus in early 1996 of some 62,000 Sarajevo Serbs from the city and its suburbs, creating today's more monoethnic postwar city.

Various modern buildings now occupy Sarajevo's skyline, most significantly the Bosmal City Center, BBI Centar, Sarajevo City Center and the Avaz Twist Tower, which at the time of its building was the tallest skyscraper in former Yugoslavia.

Recent years have seen population growth as well as increases in tourism. In 2014 the city saw anti-government protests and riots and record rainfall that caused historic flooding.

Sarajevo is the capital of the country of Bosnia and Herzegovina and its sub-entity, the Federation of Bosnia and Herzegovina, as well as of the Sarajevo Canton. It is also the "de jure" capital of another entity, Republika Srpska. Each of these levels of government has its parliament or council, as well as judicial courts, in the city. All national institutions and foreign embassies are in Sarajevo.

Sarajevo is home to the Council of Ministers of Bosnia and Herzegovina, Parliamentary Assembly of Bosnia and Herzegovina, Presidency of Bosnia and Herzegovina, the Constitutional Court of Bosnia and Herzegovina and the operational command of the Armed Forces of Bosnia and Herzegovina.

Bosnia and Herzegovina's Parliament office in Sarajevo was damaged heavily in the Bosnian War. Due to damage the staff and documents were moved to a nearby ground level office to resume the work. In late 2006, reconstruction work started on the Parliament and was finished in 2007. The cost of reconstruction is supported 80% by the Greek Government through the Hellenic Program of Balkans Reconstruction (ESOAV) and 20% by Bosnia-Herzegovina.

The city comprises four municipalities Centar, Novi Grad, Novo Sarajevo, and Stari Grad. Each operate their own municipal government, united they form one city government with its own constitution. The executive branch () consists of a mayor, with two deputies and a cabinet. The legislative branch consists of the City Council, or "Gradsko Vijeće". The council has 28 members, including a council speaker, two deputies, and a secretary. Councilors are elected by the municipality in numbers roughly proportional to their population. The city government also has a judicial branch based on the post-transitional judicial system as outlined by the High Representative's "High Judicial and Prosecutorial Councils".

Sarajevo's Municipalities are further split into "local communities" (Bosnian, "Mjesne zajednice"). Local communities have a small role in city government and are intended as a way for ordinary citizens to get involved in city government. They are based on key neighborhoods in the city.

Sarajevo's large manufacturing, administrative, and tourism sectors make it the strongest economic region of Bosnia and Herzegovina. Indeed, Sarajevo Canton generates almost 25% of the country's GDP. After years of war, Sarajevo's economy saw reconstruction and rehabilitation programs. The Central Bank of Bosnia and Herzegovina opened in Sarajevo in 1997 and the Sarajevo Stock Exchange began trading in 2002.

While Sarajevo had a large industrial base during its communist period, only a few pre-existing businesses have successfully adapted to the market economy. Sarajevo industries now include tobacco products, furniture, hosiery, automobiles, and communication equipment. Companies based in Sarajevo include BH Telecom, Bosnalijek, Energopetrol, Sarajevo Tobacco Factory, and Sarajevska Pivara (Sarajevo Brewery).

In 2019 the total export for the Sarajevo canton was worth about 1,427,496,000KM. Most of Sarajevo's exports (20.55%) head to Germany, with Serbia and Croatia following behind at 12% respectively. The largest amount of imported goods come from Croatia, at 20.95%. With a worth of total import at about 4,872,213,000KM, the total import is almost 3.4 times the total export.

In 1981 Sarajevo's GDP per capita was 133% of the Yugoslav average. Gross pay in Sarajevo in 2019 was or , while net salary was or .
Sarajevo has a wide tourist industry and a fast expanding service sector thanks to the strong annual growth in tourist arrivals. Sarajevo also benefits from being both a summer and winter destination with continuity in its tourism throughout the year. The travel guide series, "Lonely Planet" named Sarajevo as the 43rd best city in the world, and in December 2009 listed Sarajevo as one of the top ten cities to visit in 2010.

In 2019, 733.259 tourists visited Sarajevo, giving 1.667.545 overnight stays, which is more than 20% more than in 2018.

Sports-related tourism uses the legacy facilities of the 1984 Winter Olympics, especially the skiing facilities on the nearby mountains of Bjelašnica, Igman, Jahorina, Trebević, and Treskavica. Sarajevo's 600 years of history, influenced by both Western and Eastern empires, makes it a tourist attraction with splendid variations.
Sarajevo has hosted travellers for centuries, because it was an important trading center during the Ottoman and Austria-Hungarian empires and because was a natural stop for many routes between East and West. Examples of popular destinations in Sarajevo include the Vrelo Bosne park, the Sarajevo cathedral, and the Gazi Husrev-beg's Mosque. Tourism in Sarajevo is chiefly focused on historical, religious, cultural sites and winter sports.

Sarajevo is host to many parks throughout the city and on the outskirts of city. A popular activity among Sarajevo citizens is street chess, usually played at Trg oslobođenja Alija Izetbegović. Veliki Park is the largest green area in the center of Sarajevo. It's nestled between Titova, Koševo, Džidžikovac, Tina Ujevića and Trampina Streets and in the lower part there is a monument dedicated to the Children of Sarajevo. Hastahana is a popular place to relax in the Austro-Hungarian neighborhood of Marijin Dvor. Goat's Bridge, locally known as "Kozija Ćuprija", in the Miljacka Canyon is also a popular park destination along the Dariva walkway and river Miljacka. On December 24 of 2012, a park hosting two brass sculptures resembling two mourning mothers was dedicated as the Friendship Park, commemorating over 45 years of friendship between Sarajevo and Baku.

Sarajevo is also famous for its city lookouts; including an observation deck on Avaz Twist Tower, Park Prinčeva restaurant, Vidikovac lookout (Mt. Trebević), Zmajevac lookout and Yellow/White fortresses lookouts (in Vratnik) as well as numerous other rooftops throughout the city (i.e. Alta Shopping Center, BBI Center, Hotel Hecco Deluxe). A symbol of Sarajevo is the Trebevic cable car which was reconstructed in 2018, also it is one of the most popular tourist attractions in the city taking visitors from the city center to mount Trebevic.

There is also a UNESCO tentative monument, the Old Jewish Cemetery, almost 500 years old site that is the second-largest Jewish sepulchral complex in Europe, the one in Prague being the largest. It is also one of the most significant memorial complexes in the world. It represents the eternal proof of coexistence of two or more different confessions under different administrations and rules, and the proof of mutual respect and tolerance.

Thanks to steady but constant and stable growth after the war, today's built-up area that includes not only previously mentioned urban municipalities but the urban part of Hadžići that is uninterruptedly connected to Ilidža, the weternmost part of the Sarajevo urban settlement, is inhabited by more than 419,000 people, while the metro area including 8 additional municipalities, 14 in total goes up to 555,210 inhabitants. It is noticeable that the fastest-growing municipalities are Novi Grad, one of the main ones and the most inhabited one where the population has increased by almost 4,000 people or 2.95% since the 2013 Census, and Ilidža that has recorded an increase of almost 7% since 2013.

In June 2016, the final results of the 2013 census were published. According to the census, the population of the Sarajevo Canton was 413,593, with 55,181 residents in Centar Sarajevo, 118,553 in Novi Grad, 64,814 in Novo Sarajevo and 36,976 in Stari Grad.

The last official Yugoslav census took place 1991 and recorded 527,049 people living in the city of Sarajevo (ten municipalities). In the settlement of Sarajevo proper, there were 454,319 inhabitants. The war displaced hundreds of thousands of people, a large majority of whom have not returned.

The war changed the ethnic and religious profile of the city. It had long been a multicultural city, and often went by the nickname of "Europe's Jerusalem". At the time of the 1991 census, 49.2 per cent of the city's population of 527,049 were Bosniaks, 29.8 percent Serbs, 10.7 percent Yugoslavs, 6.6 percent Croats and 3.6 percent other ethnicities (Jews, Romas, etc.). 

According to academic Fran Markowitz there are a number of "administrative apparatuses and public pressures that push people who might prefer to identify as flexible, multiply constituted hybrids or with one of the now unnamed minority groups into one of the three Bosniac-Croat-Serb constituent nations". These include respondents being encouraged by census interviewers to identity as belonging to one of the three constituent peoples. Her analysis of marriage registration data shows, for instance, that 67 per cent of people marrying in 2003 identified as Bosniak or Muslim, which is significantly lower than the 79.6 per cent census figure from 2002 (unlike the census, where people respond to an interviewer, applicants to the marriage registry fill in the form themselves).

Sarajevo's location in a valley between mountains makes it a compact city. Narrow city streets and a lack of parking areas restrict automobile traffic but allow better pedestrian and cyclist mobility. The two main roads are Titova Ulica (Street of Marshal Tito) and the east-west Zmaj od Bosne (Dragon of Bosnia) highway (E761).
Located roughly at the center of the country, Sarajevo is Bosnia's main intersection. The city is connected to all the other major cities by highway or national road like Zenica, Banja Luka, Tuzla, Mostar, Goražde and Foča.

Tourists from Central Europe and elsewhere visiting Dalmatia driving via Budapest through Sarajevo also contribute to the traffic congestion in and around Sarajevo.
The trans-European highway, Corridor 5C, runs through Sarajevo connecting it to Budapest in the north, and Ploče at the Adriatic sea in the south. The highway is built by the government and should cost 3.5 billion Euro. Up until March 2012, the Federation of Bosnia and Herzegovina invested around 600 million Euro in the A1. In 2014 the sections Sarajevo-Zenica and Sarajevo-Tarcin were completed including the Sarajevo Beltway ring road.

Sarajevo's electric tramways, in operation since 1884 and electrified since 1895, are the oldest form of public transportation in the city.
Sarajevo had the first full-time (dawn to dusk) tram line in Europe, and the second in the world. Opened on New Year's Day in 1885, it was the testing line for the tram in Vienna and the Austro-Hungarian Empire, and operated by horses. Originally built to , the present system in 1960 was upgraded to . The trams played a pivotal role in the growth of the city in the 20th century.

There are seven tramway lines supplemented by five trolleybus lines and numerous bus routes. The main railroad station in Sarajevo is in the north-central area of the city. From there, the tracks head west before branching off in different directions, including to industrial zones in the city. Sarajevo is undergoing a major infrastructure renewal; many highways and streets are being repaved, the tram system is undergoing modernization, and new bridges and roads are under construction.

To solve traffic congestion in the city, Sarajevo-based architect Muzafer Osmanagić has proposed a study called "Eco Energy 2010–2015", idealizing a subway system underneath the bed of the river Miljacka. The first line of Metro Sarajevo would connect Baščaršija with Otoka. This line would cost some 150 million KM and be financed by the European Bank for Reconstruction and Development.

Sarajevo has daily international connections which twice a day connect the city with Zagreb and Ploče. There are also connections between Sarajevo and all major cities within Bosnia and Herzegovina. Once, the East Bosnian railway connected Sarajevo to Belgrade.

Trebević Cable Car, Sarajevo's key landmark during 1984 Winter Olympic Games, was rebuilt by JKP GRAS Sarajevo and Sarajevo Canton as one of the new transportation systems in 2017 and it reopened on 6 April 2018 at 11:00 AM. The cable car runs from Sarajevo at Bistrik station to the slopes of Trebević at Vidikovac station.

Sarajevo International Airport , also called Butmir, is just a few kilometers southwest of the city and was voted Best European Airport With Under 1,000,000 Passengers at the 15th Annual ACI-Europe in Munich in 2005.

First regular flights to Sarajevo using an airfield in the suburb of Butmir begin in 1930 when the domestic airliner Aeroput opened a regular route linking Belgrade to Podgorica through Sarajevo. Later, Aeroput opened routed which linked Sarajevo with Split, Rijeka and Dubrovnik, and in 1938 first international flights were introduced when Aeroput extended the route Dubrovnik – Sarajevo – Zagreb to Vienna, Brno and Prague. The airfield in Butmir remained in use all the way until 1969. The need for a new airport in Sarajevo, with an asphalt-concrete runway, was acknowledged in the mid-1960s when JAT, Yugoslav national carrier at that time, began acquiring jet planes. The construction of the airport began in 1966 at its present location, not far from the old one.

Sarajevo Airport opened on 2 June 1969 for domestic traffic. In 1970 Frankfurt became the first international destination served. Most of the time the airport was a 'feeder' airport where passengers embarked for flights to Zagreb and Belgrade on their way to international destinations. Over time the traffic volume steadily grew from 70,000 to 600,000 passengers a year. Later, during the Bosnian war, the airport was used for UN flights and humanitarian relief. Since the Dayton Accord in 1996, the airport retook its role as main air gate to Bosnia and Herzegovina.

In 2017, 957,971 passengers traveled through the airport, which was 61,4% of the total airport traffic in Bosnia-Herzegovina.

Plans for extension of the passenger terminal, together with upgrading and expanding the taxiway and apron, are planned to start in fall 2012. The existing terminal will be expanded by approximately . The upgraded airport will also be directly linked to the commercial retail center Sarajevo Airport Center, making it easier for tourists and travellers to spend their time before flight boarding shopping and enjoying the many amenities that will be offered.
Between 2015 and 2018 the airport will be upgraded for more than 25 million euros.

Sarajevo is twinned with:

Sarajevo is befriended with:

As the largest city of Bosnia and Herzegovina, Sarajevo is the main center of the country's media. Most of the communications and media infrastructure was destroyed during the war but reconstruction monitored by the Office of the High Representative has helped to modernize the industry as a whole. For example, internet was first made available to the city in 1995.

"Oslobođenje" (Liberation), founded in 1943, is Sarajevo's longest running continuously circulating newspaper and the only one to survive the war. However, this long running and trusted newspaper has fallen behind "Dnevni Avaz" (Daily Voice), founded in 1995, and "Jutarnje Novine" (Morning News) in circulation in Sarajevo. Other local periodicals include the Croatian newspaper Hrvatska riječ and the Bosnian magazine Start, as well as weekly newspapers "Slobodna Bosna" ("Free Bosnia") and "BH Dani" ("BH Days"). "Novi Plamen", a monthly magazine, is the most left-wing publication.

The Radiotelevision of Bosnia-Herzegovina is Sarajevo's public television station and was created in 1945 under the umbrella of the Yugoslav Radio Television. It had its first television program aired in 1961, while continuous programming started in 1969. It is one of three main TV stations in Bosnia and Herzegovina. Other stations based in the city include NRTV "Studio 99", NTV Hayat, TV 1, Open Broadcast Network, TV Kantona Sarajevo and Televizija Alfa.

The headquarters of Al Jazeera Balkans are also in Sarajevo, with a broadcasting studio at the top of the BBI Center. The news channel covers Bosnia and Herzegovina, Serbia, Croatia and Montenegro and the surrounding Balkan states.

Many small independent radio stations exist, including established stations such as Radio M, Radio Stari Grad (Radio Old Town), Studentski eFM Radio, Radio 202, Radio BIR, and RSG. Radio Free Europe, as well as several American and Western European stations are available.
Higher Education
Higher education has a long and rich tradition in Sarajevo. The first institution that can be classified as a tertiary educational institution was a school of Sufi philosophy established by Gazi Husrev-beg in 1537; numerous other religious schools have been established over time. In 1887, under the Austro-Hungarian Empire, a Sharia Law School began a five-year program. In the 1940s the University of Sarajevo became the city's first secular higher education institute, effectively building upon the foundations established by the Saraybosna Hanıka in 1537. In the 1950s, post-bachelor graduate degrees became available. Severely damaged during the war, it was recently rebuilt in partnership with more than 40 other universities.

There are also several universities in Sarajevo, including:

Primary and Secondary Education

, in Sarajevo there are 46 elementary schools (Grades 1–9) and 33 high schools (Grades 10–13), including three schools for children with special needs,

There are also several international schools in Sarajevo, catering to the expatriate community; some of which are Sarajevo International School and the French International School of Sarajevo, established in 1998.

Sarajevo has been home to many different religions for centuries, giving the city a range of diverse cultures. In the time of Ottoman occupation of Bosnia, Muslims, Bosnian Serbs, Roman Catholics, and Sephardi Jews all shared the city while maintaining distinctive identities. They were joined during the brief occupation by Austria-Hungary by a smaller number of Germans, Hungarians, Slovaks, Czechs and Ashkenazi Jews. By 1909, about 50% of the city's inhabitants were Muslim, 25% were Catholic, 15% were Orthodox, and 10% were Jewish.

Historically, Sarajevo has been home to several prominent Bosnian poets, scholars, philosophers and writers. To list only a very few; Nobel Prize-winner Vladimir Prelog is from the city, as are the writer Zlatko Topčić and the poet Abdulah Sidran. Nobel Prize-winner Ivo Andrić attended high school in Sarajevo for two years. Academy Award-winning director Danis Tanović live in the city.

The Sarajevo National Theatre is the oldest professional theater in Bosnia and Herzegovina, having been established in 1921.

The city is rich in museums, including the Museum of Sarajevo, the Ars Aevi Museum of Contemporary Art, Historical Museum of Bosnia and Herzegovina, The Museum of Literature and Theatre Arts of Bosnia and Herzegovina, and the National Museum of Bosnia and Herzegovina (established in 1888) home to the Sarajevo Haggadah, an illuminated manuscript and the oldest Sephardic Jewish document in the world issued in Barcelona around 1350, containing the traditional Jewish Haggadah, is on permanent display at the museum. It is the only remaining illustrated Sephardic Haggadah in the world. The National Museum also hosts year-round exhibitions pertaining to local, regional and international culture and history, and exhibits over 5,000 artefacts from Bosnia's history.

The Alija Izetbegović Museum was opened on 19 October 2007 and is in the old town fort, more specifically in the Vratnik Kapija towers Ploča and Širokac. The museum is a commemoration to the influence and body of work of Alija Izetbegović, the first president of the Republic of Bosnia and Herzegovina.

The city also hosts the Sarajevo National Theater, established in 1921, and the Sarajevo Youth Theatre. Some other cultural institutions include the Center for Sarajevo Culture, Sarajevo City Library, Art Gallery of Bosnia and Herzegovina, and the Bosniak Institute, a privately owned library and art collection focusing on Bosniak history.

Demolitions associated with the war, as well as reconstruction, destroyed several institutions and cultural or religious symbols including the Gazi Husrev-beg library, the national library, the Sarajevo Oriental Institute, and a museum dedicated to the 1984 Winter Olympics. Consequently, the different levels of government established strong cultural protection laws and institutions. Bodies charged with cultural preservation in Sarajevo include the Institute for the Protection of the Cultural, Historical and Natural Heritage of Bosnia and Herzegovina (and their Sarajevo Canton counterpart), and the Bosnia and Herzegovina Commission to Preserve National Monuments.

Sarajevo is and has historically been one of the most important musical enclaves in the region. The Sarajevo school of pop rock developed in the city between 1961 and 1991. This type of music began with bands like Indexi, Pro Arte, and singer-songwriter Kemal Monteno. It continued into the 1980s, with bands such as Plavi Orkestar, Crvena Jabuka, and Divlje Jagode, by most accounts, pioneering the regional rock and roll movement. Sarajevo was also the home and birthplace of arguably the most popular and influential Yugoslav rock band of all time, Bijelo Dugme, somewhat of a Bosnian parallel to the Rolling Stones, in both popularity and influence.

Sarajevo was also the home of a very notable post-punk urban subculture known as the New Primitives, which began during the early 1980s with the Baglama Band which was banned shortly after first LP and was brought into the mainstream through bands such as Zabranjeno Pušenje and Elvis J. Kurtović & His Meteors, as well as the Top Lista Nadrealista radio, and later television show. Other notable bands considered to be part of this subculture are Bombaj Štampa. Besides and separately from the "New Primitives", Sarajevo is the hometown to one of the most significant ex-Yugoslavian alternative industrial-noise bands, SCH (1983–current).

Perhaps more importantly, Sarajevo in the late 19th and throughout the 20th century was home to a burgeoning and large center of Sevdalinka record-making and contributed greatly to bringing this historical genre of music to the mainstream, which had for many centuries been a staple of Bosnian culture. Songwriters and musicians such as Himzo Polovina, Safet Isović, Zaim Imamović, Zehra Deović, Halid Bešlić, Hanka Paldum, Nada Mamula, Meho Puzić and many more composed and wrote some of their most important pieces in the city.

Sarajevo also greatly influenced the pop scene of Yugoslavia with musicians like Zdravko Čolić, Kemal Monteno, Dino Merlin, Seid Memić Vajta, Hari Mata Hari, Mladen Vojičić "Tifa", Željko Bebek, and many more.

Many newer Sarajevo-based bands have also found a name and established themselves in Sarajevo, such as Regina who also had two albums out in Yugoslavia and Letu Štuke, who actually formed their band in Yugoslavia with the famous Bosnian-American writer Aleksandar Hemon and got their real breakthrough later in the 2000s. Sarajevo is now home to an important and eclectic mix of new bands and independent musicians, which continue to thrive with the ever-increasing number of festivals, creative showcases and concerts around the country. The city is also home to the region's largest jazz festival, the Sarajevo Jazz Festival (see "Festival" section below this).

American heavy metal band Savatage, released a song entitled "Christmas Eve (Sarajevo 12/24)" on their 1995 album Dead Winter Dead, which was about a cello player playing a forgotten Christmas carol in war-torn Sarajevo. The song was later re-released by the same band under the name Trans-Siberian Orchestra on their 1996 debut album Christmas Eve and Other Stories, which the song gave them instant success.

Sarajevo is internationally renowned for its eclectic and diverse selection of over 50 annual festivals. The Sarajevo Film Festival was established in 1995 during the Bosnian War and has become the premier and largest film festival in South-East Europe. It has been hosted at the National Theater, with screenings at the Open-air theater Metalac and the Bosnian Cultural Center, all in downtown Sarajevo. The MESS International Festival is an experimental theatre festival and the oldest living theatre festival in the Balkans. The annual Sarajevo Youth Film Festival showcases feature, animated and short films from around the world and is the premier student film festival in the Balkans. The Sarajevo Winter Festival, Sarajevo Jazz Festival and Sarajevo International Music Festival are well-known, as is the Baščaršija Nights festival, a month-long showcase of local culture, music, and dance.

The first incarnation of the Sarajevo Film Festival was hosted in still-warring Sarajevo in 1995, and has now progressed into being the biggest and most significant festival in south-eastern Europe. A talent campus is also held during the duration of the festival, with lecturers speaking on behalf of world cinematography and holding workshops for film students from across South-Eastern Europe.

The Sarajevo Jazz Festival is the region's largest and most diverse of its kind. The festival takes place at the Bosnian Cultural Center (aka "Main Stage"), just down the street from the SFF, at the Sarajevo Youth Stage Theater (aka "Strange Fruits Stage"), at the Dom Vojske Federacije (aka "Solo Stage"), and at the CDA (aka "Groove Stage").

The city hosted the 1984 Winter Olympics. Yugoslavia won one medal, a silver in men's giant slalom awarded to Jure Franko. Many of the Olympic facilities survived the war or were reconstructed, including Olympic Hall Zetra and Asim Ferhatović Stadion. In an attempt to bring back some of Sarajevo's Olympic glory, the original Olympic luge and bobsled tracks are being repaired, due to the efforts of both the Olympic Committee of Bosnia and Herzegovina and local sports enthusiasts. After co-hosting the Southeast Europe Friendship games, Sarajevo was awarded the 2009 Special Olympic winter games, but cancelled these plans. The ice arena for the 1984 Olympics, Zetra Stadium, was used during the war as a temporary hospital and, later, for housing NATO troops of the IFOR.

In 2011 Sarajevo was the host city of the 51st World Military Skiing Championship with over 350 participants from 23 different nations. This was the first international event of such standing since the 1984 Olympics.
Football (soccer) is popular in Sarajevo; the city hosts "FK Sarajevo" and "FK Željezničar", which both compete in European and international cups and tournaments and have a very large trophy cabinet in the former Yugoslavia as well as independent Bosnia and Herzegovina. Other notable soccer clubs are "FK Olimpik", "SAŠK" and "Slavija".

One of only two stadiums in Bosnia and Herzegovina that has the UEFA category 3 is the Stadion Grbavica, the home stadium of FK Željezničar.

Another popular sport is basketball; the basketball club KK Bosna Sarajevo won the European Championship in 1979 as well as many Yugoslav and Bosnian national championships making it one of the greatest basketball clubs in the former Yugoslavia. The chess club, "Bosna" Sarajevo, has been a championship team since the 1980s and is the third ranked chess club in Europe, having won four consecutive European championships in the nineties. RK Bosna also competes in the European Champions League and is considered one of the most well organised handball clubs in South-Eastern Europe with a very large fan base and excellent national, as well as international results.
Sarajevo often holds international events and competitions in sports such as tennis and kickboxing.

The popularity of tennis has been picking up in recent years. Since 2003, BH Telecom Indoors is an annual tennis tournament in Sarajevo.

Since 2007, the Sarajevo Marathon is being organized in late September. Giro di Sarajevo is also run in the city with over 2,200 cyclists taking part in 2015.

In February 2019, Sarajevo and East Sarajevo hosted the European Youth Olympic Winter Festival (EYOWF).




</doc>
<doc id="26787" url="https://en.wikipedia.org/wiki?curid=26787" title="Science fiction">
Science fiction

Science fiction (sometimes shortened to sci-fi or SF) is a genre of speculative fiction that typically deals with imaginative and futuristic concepts such as advanced science and technology, space exploration, time travel, parallel universes, and extraterrestrial life. It has been called the "literature of ideas", and often explores the potential consequences of scientific, social, and technological innovations.

Science fiction, whose roots go back to ancient times, is related to fantasy, horror, and superhero fiction, and contains many subgenres. However its exact definition has long been disputed among authors, critics, and scholars.

Science fiction literature, film, television, and other media have become popular and influential over much of the world. Besides providing entertainment, it can also criticize present-day society, and is often said to inspire a "sense of wonder".

"Science fiction" is difficult to define precisely, as it includes a wide range of concepts and themes. American science fiction and fantasy writer James Blish wrote: "Wells used the term originally to cover what we would today call "hard" science fiction, in which a conscientious attempt to be faithful to already known facts (as of the date of writing) was the substrate on which the story was to be built, and if the story was also to contain a miracle, it ought at least not to contain a whole arsenal of them."

According to American writer and professor of biochemistry Isaac Asimov, "Science fiction can be defined as that branch of literature which deals with the reaction of human beings to changes in science and technology." American science-fiction author and engineer Robert A. Heinlein wrote that "A handy short definition of almost all science fiction might read: realistic speculation about possible future events, based solidly on adequate knowledge of the real world, past and present, and on a thorough understanding of the nature and significance of the scientific method."

"The Encyclopedia of Science Fiction", edited by John Clute and Peter Nicholls, contains an extensive discussion of the problem of definition, under the heading "Definitions of SF". The authors regard Yugoslav born academic, writer, and critic Darko Suvin's 1972 definition as having been most useful in catalysing academic debate. Suvin's definition is: "a literary genre whose necessary and sufficient conditions are the presence and interaction of estrangement and cognition, and whose main formal device is an imaginative framework alternative to the author's empirical environment".

American science fiction author and editor Lester del Rey wrote, "Even the devoted aficionado or fan—has a hard time trying to explain what science fiction is," and the lack of a "full satisfactory definition" is because "there are no easily delineated limits to science fiction." Author and editor Damon Knight summed up the difficulty, saying "science fiction is what we point to when we say it."

Science fiction had its beginnings in ancient times, when the line between myth and fact was blurred. Written in the 2nd century CE by the satirist Lucian, "A True Story" contains many themes and tropes characteristic of modern science fiction, including travel to other worlds, extraterrestrial lifeforms, interplanetary warfare, and artificial life. Some consider it the first science-fiction novel. Some of the stories from "The Arabian Nights", along with the 10th-century "The Tale of the Bamboo Cutter" and Ibn al-Nafis's 13th-century "Theologus Autodidactus," also contain elements of science fiction.

Written during the Scientific Revolution and the Age of Enlightenment, Johannes Kepler's "Somnium" (1634), Francis Bacon's "New Atlantis" (1627), Athanasius Kircher's "Itinerarium extaticum" (1656), Cyrano de Bergerac's "Comical History of the States and Empires of the Moon" (1657) and "The States and Empires of the Sun" (1662), Margaret Cavendish's "The Blazing World" (1666), Jonathan Swift's "Gulliver's Travels" (1726), Ludvig Holberg's "Nicolai Klimii Iter Subterraneum" (1741) and Voltaire's "Micromégas" (1752) are regarded as some of the first true science-fantasy works. Isaac Asimov and Carl Sagan considered "Somnium" the first science-fiction story; it depicts a journey to the Moon and how the Earth's motion is seen from there.

Following the 18th-century development of the novel as a literary form, Mary Shelley's "Frankenstein" (1818) and "The Last Man" (1826) helped define the form of the science-fiction novel. Brian Aldiss has argued that "Frankenstein" was the first work of science fiction. Edgar Allan Poe wrote several stories considered to be science fiction, including "The Unparalleled Adventure of One Hans Pfaall" (1835) which featured a trip to the Moon. Jules Verne was noted for his attention to detail and scientific accuracy, especially in "Twenty Thousand Leagues Under the Sea" (1870). In 1887, the novel "El anacronópete" by Spanish author Enrique Gaspar y Rimbau introduced the first time machine.

Many critics consider H. G. Wells one of science fiction's most important authors, or even "the Shakespeare of science fiction." His notable science-fiction works include "The Time Machine" (1895), "The Island of Doctor Moreau" (1896), "The Invisible Man" (1897), and "The War of the Worlds" (1898). His science fiction imagined alien invasion, biological engineering, invisibility, and time travel. In his non-fiction futurologist works he predicted the advent of airplanes, military tanks, nuclear weapons, satellite television, space travel, and something resembling the World Wide Web.

Edgar Rice Burroughs' "A Princess of Mars", published in 1912, was the first of his three-decade-long planetary romance series of Barsoom novels which were set on Mars and featured John Carter as the hero.

In 1926, Hugo Gernsback published the first American science-fiction magazine, "Amazing Stories". In its first issue he wrote:

In 1928, E. E. "Doc" Smith's first published work, "The Skylark of Space," written in collaboration with Lee Hawkins Garby, appeared in "Amazing Stories". It is often called the first great space opera. The same year, Philip Francis Nowlan's original Buck Rogers story, "Armageddon 2419", also appeared in "Amazing Stories". This was followed by a Buck Rogers comic strip, the first serious science-fiction comic.

In 1937, John W. Campbell became editor of "Astounding Science Fiction", an event which is sometimes considered the beginning of the Golden Age of Science Fiction, which is characterized by stories celebrating scientific achievement and progress. In 1942, Isaac Asimov started his Foundation series, which chronicles the rise and fall of galactic empires and introduced psychohistory. The series was later awarded a one-time Hugo Award for "Best All-Time Series." The "Golden Age" is often said to have ended in 1946, but sometimes the late 1940s and the 1950s are included.

Theodore Sturgeon's "More Than Human" (1953) explored possible future human evolution. In 1957, "" by the Russian writer and paleontologist Ivan Yefremov presented a view of a future interstellar communist civilization and is considered one of the most important Soviet science fiction novels. In 1959, Robert A. Heinlein's "Starship Troopers" marked a departure from his earlier juvenile stories and novels. It is one of the first and most influential examples of military science fiction, and introduced the concept of powered armor exoskeletons. The German space opera series "Perry Rhodan", written by various authors, started in 1961 with an account of the first Moon landing and has since expanded in space to multiple universes, and in time by billions of years. It has become the most popular science fiction book series of all time.

In the 1960s and 1970s, New Wave science fiction was known for its embrace of a high degree of experimentation, both in form and in content, and a highbrow and self-consciously "literary" or "artistic" sensibility. In 1961, "Solaris" by Stanisław Lem was published in Poland. The novel dealt with the theme of human limitations as its characters attempted to study a seemingly intelligent ocean on a newly discovered planet. 1965's "Dune" by Frank Herbert featured a much more complex and detailed imagined future society than had previous science fiction.

In 1968, Philip K. Dick's "Do Androids Dream of Electric Sheep?," was published. It is the literary source of the "Blade Runner" movie franchise. 1969's "The Left Hand of Darkness" by Ursula K. Le Guin was set on a planet in which the inhabitants have no fixed gender. It is one of the most influential examples of social science fiction, feminist science fiction, and anthropological science fiction.

In 1976, C. J. Cherryh published "Gate of Ivrel" and "Brothers of Earth", which began her Alliance-Union universe future history series. In 1979, "Science Fiction World" began publication in the People's Republic of China. It dominates the Chinese science fiction magazine market, at one time claiming a circulation of 300,000 copies per issue and an estimated 3-5 readers per copy (giving it a total estimated readership of at least 1 million), making it the world's most popular science fiction periodical.

In 1984, William Gibson's first novel, "Neuromancer," helped popularize cyberpunk and the word "cyberspace," a term he originally coined in his 1982 short story "Burning Chrome". In 1986, "Shards of Honor" by Lois McMaster Bujold began her Vorkosigan Saga. 1992's "Snow Crash" by Neal Stephenson predicted immense social upheaval due to the information revolution. In 2007, Liu Cixin's novel, "The Three-Body Problem", was published in China. It was translated into English by Ken Liu and published by Tor Books in 2014, and won the 2015 Hugo Award for Best Novel, making Liu the first Asian writer to win the award.

Emerging themes in late 20th and early 21st century science fiction include environmental issues, the implications of the Internet and the expanding information universe, questions about biotechnology, nanotechnology, and post-scarcity societies. Recent trends and subgenres include steampunk, biopunk, and mundane science fiction.

The first, or at least one of the first, recorded science fiction film is 1902's "A Trip to the Moon", directed by French filmmaker Georges Méliès. It was profoundly influential on later filmmakers, bringing a different kind of creativity and fantasy to the cinematic medium. In addition, Méliès's innovative editing and special effects techniques were widely imitated and became important elements of the medium.

1927's "Metropolis", directed by Fritz Lang, is the first feature-length science fiction film. Though not well received in its time, it is now considered a great and influential film. In 1954, "Godzilla", directed by Ishirō Honda, began the kaiju subgenre of science fiction film, which feature large creatures of any form, usually attacking a major city or engaging other monsters in battle.
1968's "", directed by Stanley Kubrick and based on the work of Arthur C. Clarke, rose above the mostly B-movie offerings up to that time both in scope and quality, and greatly influenced later science fiction films. That same year, "Planet of the Apes" (the original), directed by Franklin J. Schaffner and based on the 1963 French novel "La Planète des Singes" by Pierre Boulle, was released to popular and critical acclaim, due in large part to its vivid depiction of a post-apocalyptic world in which intelligent apes dominate humans.

In 1977, George Lucas began the "Star Wars" film series with the film now identified as ""Star Wars: Episode IV – A New Hope."" The series, often called a space opera, went on to become a worldwide popular culture phenomenon, and the second-highest-grossing film series of all time.

Since the 1980s, science fiction films, along with fantasy, horror, and superhero films, have dominated Hollywood's big-budget productions. Science fiction films often "cross-over" with other genres, including animation "(WALL-E" - 2008, "Big Hero 6" - 2014), gangster ("Sky Racket" - 1937), Western ("Serenity" - 2005), comedy ("Spaceballs" -1987, "Galaxy Quest" - 1999), war ("Enemy Mine" - 1985), action ("Edge of Tomorrow" - 2014, "The Matrix" - 1999), adventure ("Jupiter Ascending" - 2015, "Interstellar" - 2014), sports ("Rollerball" - 1975), mystery ("Minority Report" - 2002), thriller ("Ex Machina" - 2014), horror ("Alien" - 1979), film noir ("Blade Runner" - 1982), superhero ("Marvel Cinematic Universe" - 2008-), drama ("Melancholia" - 2011, "Predestination" -2014), and romance ("Her (film)" - 2013).

Science fiction and television have consistently been in a close relationship. Television or television-like technologies frequently appeared in science fiction long before television itself became widely available in the late 1940s and early 1950s.

The first known science fiction television program was a thirty-five-minute adapted excerpt of the play "RUR", written by the Czech playwright Karel Čapek, broadcast live from the BBC's Alexandra Palace studios on 11 February 1938. The first popular science fiction program on American television was the children's adventure serial "Captain Video and His Video Rangers", which ran from June 1949 to April 1955.

"The Twilight Zone" (the original series), produced and narrated by Rod Serling, who also wrote or co-wrote most of the episodes, ran from 1959 to 1964. It featured fantasy, suspense, and horror as well as science fiction, with each episode being a complete story. Critics have ranked it as one of the best TV programs of any genre.

The animated series "The Jetsons", while intended as comedy and only running for one season (1962–1963), predicted many inventions now in common use: flat-screen televisions, newspapers on a computer-like screen, computer viruses, video chat, tanning beds, home treadmills, and more. In 1963, the time travel-themed "Doctor Who" premiered on BBC Television. The original series ran until 1989 and was revived in 2005. It has been extremely popular worldwide and has greatly influenced later TV science fiction. Other programs in the 1960s included "The Outer Limits" (1963-1965), "Lost in Space" (1965-1968), and "The Prisoner" (1967).

"" (the original series), created by Gene Roddenberry, premiered in 1966 on NBC Television and ran for three seasons. It combined elements of space opera and Space Western. Only mildly successful at first, the series gained popularity through syndication and extraordinary fan interest. It became a very popular and influential franchise with many films and television shows, novels, and other works and products. "" (1987-1994) led to four additional "Star Trek" shows ("" (1993-1999), "" (1995-2001)"," "" (2001-2005), and "" (2017–present))--with more in some form of development.

The miniseries "V" premiered in 1983 on NBC. It depicted an attempted takeover of Earth by reptilian aliens. "Red Dwarf", a comic science fiction series aired on BBC Two between 1988 and 1999, and on Dave since 2009. "The X-Files", which featured UFOs and conspiracy theories, was created by Chris Carter and broadcast by Fox Broadcasting Company from 1993 to 2002, and again from 2016 to 2018. "Stargate", a film about ancient astronauts and interstellar teleportation, was released in 1994. "Stargate SG-1" premiered in 1997 and ran for 10 seasons (1997-2007). Spin-off series included "Stargate Infinity" (2002-2003), "Stargate Atlantis" (2004-2009), and "Stargate Universe" (2009-2011). Other 1990s series included "Quantum Leap" (1989-1993) and "Babylon 5" (1994-1999).

SyFy, launched in 1992 as The Sci-Fi Channel, specializes in science fiction, supernatural horror, and fantasy.

Science fiction's great rise in popularity during the first half of the 20th century was closely tied to the popular respect paid to science at that time, as well as the rapid pace of technological innovation and new inventions. Science fiction has often predicted scientific and technological progress. Some works predict that new inventions and progress will tend to improve life and society, for instance the stories of Arthur C. Clarke and "Star Trek". Others, such as H.G. Wells's "The Time Machine" and Aldous Huxley's "Brave New World", warn about possible negative consequences.

In 2001 the National Science Foundation conducted a survey on "Public Attitudes and Public Understanding: Science Fiction and Pseudoscience." It found that people who read or prefer science fiction may think about or relate to science differently than other people. They also tend to support the space program and the idea of contacting extraterrestrial civilizations. Carl Sagan wrote: "Many scientists deeply involved in the exploration of the solar system (myself among them) were first turned in that direction by science fiction."

Brian Aldiss described science fiction as "cultural wallpaper." Evidence for this widespread influence can be found in trends for writers to employ science fiction as a tool for advocacy and generating cultural insights, as well as for educators when teaching across a range of academic disciplines not limited to the natural sciences. Scholar and science fiction critic George Edgar Slusser said that science fiction "is the one real international literary form we have today, and as such has branched out to visual media, interactive media and on to whatever new media the world will invent in the 21st century. Crossover issues between the sciences and the humanities are crucial for the century to come."

Science fiction has sometimes been used as a means of social protest. George Orwell's "Nineteen Eighty-Four" (1949) is an important work of dystopian science fiction. It is often invoked in protests against governments and leaders who are seen as totalitarian. James Cameron's 2009 film "Avatar" was intended as a protest against imperialism, and specifically the European colonization of the Americas. Its images have been used by, among others, Palestinians in their protest against Israel.

Robots, artificial humans, human clones, intelligent computers, and their possible conflicts with human society have all been major themes of science fiction since, at least, the publication of Shelly's "Frankenstein". Some critics have seen this as reflecting authors’ concerns over the social alienation seen in modern society.

Feminist science fiction poses questions about social issues such as how society constructs gender roles, the role reproduction plays in defining gender, and the inequitable political or personal power of one gender over others. Some works have illustrated these themes using utopias to explore a society in which gender differences or gender power imbalances do not exist, or dystopias to explore worlds in which gender inequalities are intensified, thus asserting a need for feminist work to continue.

Climate fiction, or "cli-fi," deals with issues concerning climate change and global warming. University courses on literature and environmental issues may include climate change fiction in their syllabi, and it is often discussed by other media outside of science fiction fandom.

Libertarian science fiction focuses on the politics and social order implied by right libertarian philosophies with an emphasis on individualism and private property, and in some cases anti-statism.

Science fiction comedy often satirizes and criticizes present-day society, and sometimes makes fun of the conventions and clichés of more serious science fiction.

Science fiction is often said to inspire a "sense of wonder." Science fiction editor and critic David Hartwell wrote: "Science fiction’s appeal lies in combination of the rational, the believable, with the miraculous. It is an appeal to the sense of wonder." Carl Sagan said: "One of the great benefits of science fiction is that it can convey bits and pieces, hints and phrases, of knowledge unknown or inaccessible to the reader . . . works you ponder over as the water is running out of the bathtub or as you walk through the woods in an early winter snowfall."

In 1967, Isaac Asimov commented on the changes then occurring in the science fiction community: "And because today’s real life so resembles day-before-yesterday’s fantasy, the old-time fans are restless. Deep within, whether they admit it or not, is a feeling of disappointment and even outrage that the outer world has invaded their private domain. They feel the loss of a 'sense of wonder' because what was once truly confined to 'wonder' has now become prosaic and mundane."

The study of science fiction, or science fiction studies, is the critical assessment, interpretation, and discussion of science fiction literature, film, TV shows, new media, fandom, and fan fiction. Science fiction scholars study science fiction to better understand it and its relationship to science, technology, politics, other genres, and culture-at-large. Science fiction studies began around the turn of the 20th century, but it was not until later that science fiction studies solidified as a discipline with the publication of the academic journals "Extrapolation" (1959), "" (1972), and "Science Fiction Studies" (1973), and the establishment of the oldest organizations devoted to the study of science fiction in 1970, the Science Fiction Research Association and the Science Fiction Foundation. The field has grown considerably since the 1970s with the establishment of more journals, organizations, and conferences, as well as science fiction degree-granting programs such as those offered by the University of Liverpool and the University of Kansas.

Science fiction has historically been sub-divided between hard science fiction and soft science fiction–with the division centering on the feasibility of the science central to the story. However, this distinction has come under increasing scrutiny in the 21st century. Some authors, such as Tade Thompson and Jeff VanderMeer, have pointed out that stories that focus explicitly on physics, astronomy, mathematics, and engineering tend to be considered "hard" science fiction, while stories that focus on botany, mycology, zoology, and the social sciences tend to be categorized as "soft," regardless of the relative rigor of the science.

Max Gladstone defined "hard" science fiction as stories "where the math works," but pointed out that this ends up with stories that often seem "weirdly dated," as scientific paradigms shift over time. Michael Swanwick dismissed the traditional definition of "hard" SF altogether, instead saying that it was defined by characters striving to solve problems "in the right way–with determination, a touch of stoicism, and the consciousness that the universe is not on his or her side."

Ursula K. Le Guin also criticized the more traditional view on the difference between "hard" and "soft" SF: "The 'hard' science fiction writers dismiss everything except, well, physics, astronomy, and maybe chemistry. Biology, sociology, anthropology—that's not science to them, that's soft stuff. They're not that interested in what human beings do, really. But I am. I draw on the social sciences a great deal."

Respected authors of mainstream literature have written science fiction. Mary Shelley wrote a number of science fiction novels including "Frankenstein; or, The Modern Prometheus" (1818), and is considered a major writer of the Romantic Age. Aldous Huxley's "Brave New World" (1932) is often listed as one of England's most important novels, both for its criticism of modern culture and its prediction of future trends including reproductive technology and social engineering. Kurt Vonnegut was a highly respected American author whose works contain science fiction premises or themes. Other science fiction authors whose works are widely considered to be "serious" literature include Ray Bradbury (including, especially, "Fahrenheit 451" (1953) and "The Martian Chronicles" (1951)), Arthur C. Clarke (especially for "Childhood's End"), and Paul Myron Anthony Linebarger, writing under the name Cordwainer Smith. In his book "The Western Canon", literary critic Harold Bloom includes "Brave New World", "Solaris", "Cat's Cradle" (1963) by Vonnegut, and "The Left Hand of Darkness" as culturally and aesthetically significant works of western literature.

David Barnett has pointed out that there are books such as "The Road" (2006) by Cormac McCarthy, "Cloud Atlas" (2004) by David Mitchell, "The Gone-Away World" (2008) by Nick Harkaway, "The Stone Gods" (2007) by Jeanette Winterson, and "Oryx and Crake" (2003) by Margaret Atwood, which use recognizable science fiction tropes, but whose authors and publishers do not market them as science fiction. Doris Lessing, who was later awarded the Nobel Prize in literature, wrote a series of five SF novels, "Canopus in Argos: Archives" (1979-1983), which depict the efforts of more advanced species and civilizations to influence those less advanced, including humans on Earth.

In her much reprinted 1976 essay "Science Fiction and Mrs Brown," Le Guin asked: "Can a science fiction writer write a novel?"; and answered: "I believe that all novels, . . . deal with character, and that it is to express character–not to preach doctrines, sing songs, or celebrate the glories of the British Empire, that the form of the novel, so clumsy, verbose, and undramatic, so rich, elastic, and alive, has been evolved. . . . The great novelists have brought us to see whatever they wish us to see through some character. Otherwise they would not be novelists, but poets, historians, or pamphleteers." Orson Scott Card, best known for his 1985 science fiction novel "Ender's Game", has postulated that in science fiction the message and intellectual significance of the work is contained within the story itself and, therefore, does not need stylistic gimmicks or literary games.

Jonathan Lethem, in a 1998 essay in the "Village Voice" entitled "Close Encounters: The Squandered Promise of Science Fiction," suggested that the point in 1973 when Thomas Pynchon's "Gravity's Rainbow" was nominated for the Nebula Award and was passed over in favor of Clarke's "Rendezvous with Rama," stands as "a hidden tombstone marking the death of the hope that SF was about to merge with the mainstream." In the same year science fiction author and physicist Gregory Benford wrote: "SF is perhaps the defining genre of the twentieth century, although its conquering armies are still camped outside the Rome of the literary citadels."

Science fiction is being written, and has been written, by diverse authors from around the world. According to 2013 statistics by the science fiction publisher Tor Books, men outnumber women by 78% to 22% among submissions to the publisher. A controversy about voting slates in the 2015 Hugo Awards highlighted tensions in the science fiction community between a trend of increasingly diverse works and authors being honored by awards, and reaction by groups of authors and fans who preferred what they considered more "traditional" science fiction.

Among the most respected and well-known awards for science fiction are the Hugo Award for literature, presented by the World Science Fiction Society at Worldcon, and voted on by fans; the Nebula Award for literature, presented by the Science Fiction and Fantasy Writers of America, and voted on by the community of authors; the John W. Campbell Memorial Award for Best Science Fiction Novel, presented by a jury of writers; and the Theodore Sturgeon Memorial Award for short fiction, presented by a jury. One notable award for science fiction films and TV programs is the Saturn Award, which is presented annually by The Academy of Science Fiction, Fantasy, and Horror Films.

There are other national awards, like Canada's Prix Aurora Awards, regional awards, like the Endeavour Award presented at Orycon for works from the U.S. Pacific Northwest, and special interest or subgenre awards such as the Chesley Award for art, presented by the Association of Science Fiction & Fantasy Artists, or the World Fantasy Award for fantasy. Magazines may organize reader polls, notably the Locus Award.

Conventions (in fandom, often shortened as "cons," such as "comic-con") are held in cities around the world, catering to a local, regional, national, or international membership. General-interest conventions cover all aspects of science fiction, while others focus on a particular interest like media fandom, filking, and so on. Most science fiction conventions are organized by volunteers in non-profit groups, though most media-oriented events are organized by commercial promoters. The convention's activities are called "the program", which may include panel discussions, readings, autograph sessions, costume masquerades, and other events. Additional activities occur throughout the convention that are not part of the program. These commonly include a dealer's room, art show, and hospitality lounge (or "con suites").

Conventions may host award ceremonies. For instance, Worldcon presents the Hugo Awards each year. SF societies, referred to as "clubs" except in formal contexts, form a year-round base of activities for science fiction fans. They may be associated with an ongoing science fiction convention, or have regular club meetings, or both. Long-established groups like the New England Science Fiction Association and the Los Angeles Science Fantasy Society have clubhouses for meetings and storage of convention supplies and research materials. The Science Fiction and Fantasy Writers of America (SFWA) was founded by Damon Knight in 1965 as a non-profit organization to serve the community of professional science fiction authors.

Science fiction fandom is the "community of the literature of ideas[,] . . . the culture in which new ideas emerge and grow before being released into society at large." Members of this community ("fans"), as discussed above, are often in contact with each other at conventions or clubs, through print or online fanzines, or on the Internet using websites, mailing lists, and other resources. SF fandom emerged from the letters column in "Amazing Stories" magazine: soon fans began writing letters to each other, and then grouping their comments together in informal publications that became known as fanzines. Once they were in regular contact, fans wanted to meet each other, and they organized local clubs. In the 1930s, the first science fiction conventions gathered fans from a wider area.

The earliest organized online fandom was the SF Lovers Community, originally a mailing list in the late 1970s with a text archive file that was updated regularly. In the 1980s, Usenet groups greatly expanded the circle of fans online. In the 1990s, the development of the World-Wide Web exploded the community of online fandom by orders of magnitude, with thousands and then millions of websites devoted to science fiction and related genres for all media. Most such sites are relatively small, ephemeral, and/or narrowly focused, though sites like SF Site and SFcrowsnest offer a broad range of references and reviews.

The first science fiction fanzine, "The Comet", was published in 1930 by the Science Correspondence Club in Chicago, Illionois. Fanzine printing methods have changed over the decades, from the hectograph, the mimeograph, and the ditto machine, to modern photocopying. Distribution volumes rarely justify the cost of commercial printing. Contemporary fanzines are largely printed on computer printers or at local copy shops, or they may only be sent as email (termed "Ezines") or otherwise made available online (termed "webzines"). One of the best known fanzines today is "Ansible", edited by David Langford, winner of numerous Hugo awards. Other notable fanzines to win one or more Hugo awards include "File 770", "Mimosa", and "Plokta". Artists working for fanzines have frequently risen to prominence in the field, including Brad W. Foster, Teddy Harvia, and Joe Mayhew; the Hugos include a category for Best Fan Artists.

Forrest J Ackerman is credited with first using the term "sci-fi" (analogous to the then-trendy "hi-fi") in 1954. As science fiction entered popular culture, writers and fans active in the field came to associate the term with low-budget, low-tech "B-movies," and with low-quality pulp science fiction. By the 1970s, critics within the field, such as Damon Knight and Terry Carr, were using "sci fi" to distinguish hack-work from serious science fiction. Peter Nicholls writes that "SF" (or "sf") is "the preferred abbreviation within the community of sf writers and readers." Robert Heinlein found even "science fiction" insufficient for certain types of works in this genre, and suggested the term speculative fiction to be used instead for those that are more "serious" or "thoughtful."

Science fiction elements can include, among others:



</doc>
<doc id="26788" url="https://en.wikipedia.org/wiki?curid=26788" title="Spirotrich">
Spirotrich

The spirotrichs are a large and diverse group of ciliate protozoa. They typically have prominent oral cilia in the form of a series of polykinetids, called the adoral zone of membranelles, beginning anterior to the oral cavity and running down to the left side of the mouth. There may also be one or two paroral membranes on its right side. The body cilia are fused to form polykinetids called cirri in some, and are sparse to absent in others.

Forms with cirri are common throughout soil, freshwater, and marine environments. Individuals tend to be flattened, with cirri confined to the ventral surface. These are variously used for crawling over objects, acting as feet, swimming, or assisting in food capture. They are generally divided into hypotrichs and stichotrichs, but were originally all considered hypotrichs.

Forms with sparse or absent body cilia tend to be smaller and are mostly marine, but a few are common in freshwater. Again, they are generally divided into oligotrichs and choreotrichs, but were originally all considered oligotrichs. The latter group includes the tintinnids, which produce loricae or shells and are the predominant fossil ciliates.

As first defined by Bütschli in 1889 the spirotrichs were one of two orders, together with the now-abandoned holotrichs, and included all ciliates with prominent oral cilia: heterotrichs, hypotrichs, oligotrichs, and peritrichs, although the last were soon separated. The heterotrichs have an adoral zone of membranelles, but molecular and ultrastructure studies have shown they are a separate group that diverged from most other ciliates early on. A few of the smaller groups included with them may be genuine spirotrichs, however, such as the Protocruziida.

The remaining spirotrichs form a monophyletic group, but their relationships are uncertain. For the most part the oligotrichs and choreotrichs appear to form closely related, natural groups. However "Halteria" and its close relatives, originally considered oligotrichs, form a separate group and may even be modified stichotrichs. Studies also suggest the hypotrichs are paraphyletic to the stichotrichs, and possibly to the oligotrichs and choreotrichs as well. This stands in contrast to the earlier belief that they were the most advanced of all protozoa.



</doc>
<doc id="26789" url="https://en.wikipedia.org/wiki?curid=26789" title="Sexual selection">
Sexual selection

Sexual selection is a mode of natural selection in which members of one biological sex choose mates of the other sex to mate with (intersexual selection), and compete with members of the same sex for access to members of the opposite sex (intrasexual selection). These two forms of selection mean that some individuals have greater reproductive success than others within a population, for example because they are more attractive or prefer more attractive partners to produce offspring. For instance, in the breeding season, sexual selection in frogs occurs with the males first gathering at the water's edge and making their mating calls: croaking. The females then arrive and choose the males with the deepest croaks and best territories. In general, males benefit from frequent mating and monopolizing access to a group of fertile females. Females can have a limited number of offspring and maximize the return on the energy they invest in reproduction.

The concept was first articulated by Charles Darwin and Alfred Russel Wallace who described it as driving species adaptations and that many organisms had evolved features whose function was deleterious to their individual survival, and then developed by Ronald Fisher in the early 20th century. Sexual selection can lead males to extreme efforts to demonstrate their fitness to be chosen by females, producing sexual dimorphism in secondary sexual characteristics, such as the ornate plumage of birds such as birds of paradise and peafowl, or the antlers of deer, or the manes of lions, caused by a positive feedback mechanism known as a Fisherian runaway, where the passing-on of the desire for a trait in one sex is as important as having the trait in the other sex in producing the runaway effect. Although the sexy son hypothesis indicates that females would prefer male offspring, Fisher's principle explains why the sex ratio is most often 1:1. Sexual selection is also found in plants and fungi.

Sexual selection was first proposed by Charles Darwin in "The Origin of Species" (1859) and developed in "The Descent of Man and Selection in Relation to Sex" (1871), as he felt that natural selection alone was unable to account for certain types of non-survival adaptations. He once wrote to a colleague that "The sight of a feather in a peacock's tail, whenever I gaze at it, makes me sick!" His work divided sexual selection into male-male competition and female choice.

These views were to some extent opposed by Alfred Russel Wallace, mostly after Darwin's death. He accepted that sexual selection could occur, but argued that it was a relatively weak form of selection. He argued that male-male competitions were forms of natural selection, but that the "drab" peahen's coloration is itself adaptive as camouflage. In his opinion, ascribing mate choice to females was attributing the ability to judge standards of beauty to animals (such as beetles) far too cognitively undeveloped to be capable of aesthetic feeling.

Ronald Fisher, the English statistician and evolutionary biologist developed a number of ideas about sexual selection in his 1930 book "The Genetical Theory of Natural Selection" including the sexy son hypothesis and Fisher's principle. The Fisherian runaway describes how sexual selection accelerates the preference for a specific ornament, causing the preferred trait and female preference for it to increase together in a positive feedback runaway cycle. In a remark that was not widely understood for another 50 years he said:

This causes a dramatic increase in both the male's conspicuous feature and in female preference for it, resulting in marked sexual dimorphism, until practical physical constraints halt further exaggeration. A positive feedback loop is created, producing extravagant physical structures in the non-limiting sex. A classic example of female choice and potential runaway selection is the long-tailed widowbird. While males have long tails that are selected for by female choice, female tastes in tail length are still more extreme with females being attracted to tails longer than those that naturally occur. Fisher understood that female preference for long tails may be passed on genetically, in conjunction with genes for the long tail itself. Long-tailed widowbird offspring of both sexes inherit both sets of genes, with females expressing their genetic preference for long tails, and males showing off the coveted long tail itself.

Richard Dawkins presents a non-mathematical explanation of the runaway sexual selection process in his book "The Blind Watchmaker". Females that prefer long tailed males tend to have mothers that chose long-tailed fathers. As a result, they carry both sets of genes in their bodies. That is, genes for long tails and for preferring long tails become linked. The taste for long tails and tail length itself may therefore become correlated, tending to increase together. The more tails lengthen, the more long tails are desired. Any slight initial imbalance between taste and tails may set off an explosion in tail lengths. Fisher wrote that:

The female widowbird chooses to mate with the most attractive long-tailed male so that her progeny, if male, will themselves be attractive to females of the next generation—thereby fathering many offspring that carry the female's genes. Since the rate of change in preference is proportional to the average taste amongst females, and as females desire to secure the services of the most sexually attractive males, an additive effect is created that, if unchecked, can yield exponential increases in a given taste and in the corresponding desired sexual attribute.

Since Fisher's initial conceptual model of the 'runaway' process, Russell Lande and Peter O'Donald have provided detailed mathematical proofs that define the circumstances under which runaway sexual selection can take place.

The reproductive success of an organism is measured by the number of offspring left behind, and their quality or probable fitness.

Sexual preference creates a tendency towards assortative mating or homogamy. The general conditions of sexual discrimination appear to be (1) the acceptance of one mate precludes the effective acceptance of alternative mates, and (2) the rejection of an offer is followed by other offers, either certainly or at such high chance that the risk of non-occurrence is smaller than the chance advantage to be gained by selecting a mate. The conditions determining which sex becomes the more limited resource in intersexual selection have been hypothesized with Bateman's principle, which states that the sex which invests the most in producing offspring becomes a limiting resource for which the other sex competes, illustrated by the greater nutritional investment of an egg in a zygote, and the limited capacity of females to reproduce; for example, in humans, a woman can only give birth every ten months, whereas a male can become a father numerous times in the same period. More recently, researchers have doubted whether Bateman was correct. Hubbell and Johnson suggested that variance in reproductive success can be influenced by the time and allocations of mating. In 2005, Gowaty and Hubbell suggested that mating tendencies depend on the choice of strategy; in some cases, males can be more selective than females, whereas Bateman suggested that his paradigm would be "almost universal" among sexually reproducing species. Critics proposed that females might be more subject to sexual selection than males, but not in all circumstances.

Darwin's ideas on sexual selection were met with scepticism by his contemporaries and not considered of great importance until in the 1930s biologists decided to include sexual selection as a mode of natural selection. Only in the 21st century have they become more important in biology; the theory is now seen as generally applicable and analogous to natural selection.

A ten-year study, experimentally varying sexual selection on flour beetles with other factors held constant, showed that sexual selection protected even an inbred population against extinction.

The handicap principle of Amotz Zahavi, Russell Lande and W. D. Hamilton, holds that the fact that the male is able to survive until and through the age of reproduction with such a seemingly maladaptive trait is taken by the female to be a testament to his overall fitness. Such handicaps might prove he is either free of or resistant to disease, or that he possesses more speed or a greater physical strength that is used to combat the troubles brought on by the exaggerated trait. Zahavi's work spurred a re-examination of the field and several new theories. In 1984, Hamilton and Marlene Zuk introduced the "Bright Male" hypothesis, suggesting that male elaborations might serve as a marker of health, by exaggerating the effects of disease and deficiency. In 1990, Michael Ryan and A.S. Rand, working with the Túngara frog, proposed the hypothesis of "Sensory Exploitation", where exaggerated male traits may provide a sensory stimulation that females find hard to resist. Subsequently, the theories of the "Gravity Hypothesis" by Jordi Moya-Larano et al. (2002), invoking a simple biomechanical model to account for the adaptive value for smaller male spiders of speed in climbing vertical surfaces, and "Chase Away" by Brett Holland and William R. Rice have been added. In the late 1970s, Janzen and Mary Willson, noting that male flowers are often larger than female flowers, expanded the field of sexual selection into plants.

In the past few years, the field has exploded to include other areas of study, not all of which fit Darwin's definition of sexual selection. These include cuckoldry, nuptial gifts, sperm competition, infanticide (especially in primates), physical beauty, mating by subterfuge, species isolation mechanisms, male parental care, ambiparental care, mate location, polygamy, and homosexual rape in certain male animals.

Focusing on the effect of sexual conflict, as hypothesized by William Rice, Locke Rowe and Göran Arnvist, Thierry Lodé argues that divergence of interest constitutes a key for evolutionary process. Sexual conflict leads to an antagonistic co-evolution in which one sex tends to control the other, resulting in a tug of war. Besides, "the sexual propaganda theory" only argued that mates were opportunistically led, on the basis of various factors determining the choice such as phenotypic characteristics, apparent vigour of individuals, strength of mate signals, trophic resources, territoriality, etc., which could explain the maintenance of genetic diversity within populations.

Several workers have brought attention to the fact that elaborated characteristics that ought to be costly in one way or another for their bearers (e.g., the tail of the swordfish "Xiphophorus montezumae") do not always appear to have a cost in terms of energetics, performance or even survival. One possible explanation for the apparent lack of costs is that "compensatory traits" have evolved in concert with the sexually selected traits.

Sexual selection may explain how certain characteristics (such as feathers) had distinct survival value at an early stage in their evolution. Geoffrey Miller proposes that sexual selection might have contributed by creating evolutionary modules such as "Archaeopteryx" feathers as sexual ornaments, at first. The earliest proto-birds such as China's "Protarchaeopteryx", discovered in the early 1990s, had well-developed feathers but no sign of the top/bottom asymmetry that gives wings lift. Some have suggested that the feathers served as insulation, helping females incubate their eggs. But perhaps the feathers served as the kinds of sexual ornaments still common in most bird species, and especially in birds such as peacocks and birds-of-paradise today. If proto-bird courtship displays combined displays of forelimb feathers with energetic jumps, then the transition from display to aerodynamic functions could have been relatively smooth.

Sexual selection sometimes generates features that may help cause a species' extinction, as has been suggested for the giant antlers of the Irish elk ("Megaloceros giganteus") that became extinct in Pleistocene Europe. However, sexual selection can also do the opposite, driving species divergence—sometimes through elaborate changes in genitalia—such that new species emerge.

Sex differences directly related to reproduction and serving no direct purpose in courtship are called primary sexual characteristics. Traits amenable to sexual selection, which give an organism an advantage over its rivals (such as in courtship) without being directly involved in reproduction, are called secondary sex characteristics.

In most sexual species the males and females have different equilibrium strategies, due to a difference in relative investment in producing offspring. As formulated in Bateman's principle, females have a greater initial investment in producing offspring (pregnancy in mammals or the production of the egg in birds and reptiles), and this difference in initial investment creates differences in variance in expected reproductive success and bootstraps the sexual selection processes. Classic examples of reversed sex-role species include the pipefish, and Wilson's phalarope. Also, unlike a female, a male (except in monogamous species) has some uncertainty about whether or not he is the true parent of a child, and so is less interested in spending his energy helping to raise offspring that may or may not be related to him. As a result of these factors, males can be expected to be more willing to mate than females, while females are expected to be the ones doing the choosing (except in cases of forced copulations, which can occur for example in certain primates and ducks). The effects of sexual selection are thus often more pronounced in males than in females.

Differences in secondary sexual characteristics between males and females of a species are referred to as sexual dimorphisms. These can be as subtle as a size difference (sexual size dimorphism, often abbreviated as SSD) or as extreme as horns and colour patterns. Sexual dimorphisms abound in nature. Examples include the possession of antlers by only male deer, the brighter coloration of many male birds in comparison with females of the same species, or even more distinct differences in basic morphology, such as the drastically increased eye-span of the male stalk-eyed fly. The peacock, with its elaborate and colourful tail feathers, which the peahen lacks, is often referred to as perhaps the most extraordinary example of a dimorphism. Male and female black-throated blue warblers and Guianan cock-of-the-rocks also differ radically in their plumage. Early naturalists even believed the females to be a separate species. The largest sexual size dimorphism in vertebrates is the shell dwelling cichlid fish "Neolamprologus callipterus" in which males are up to 30 times the size of females. Many other fish such as guppies are sexually dimorphic. Extreme sexual size dimorphism, with females larger than males, is quite common in spiders and birds of prey.

The maintenance of sexual reproduction in a highly competitive world is one of the major puzzles in biology given that asexual reproduction can reproduce much more quickly as 50% of offspring are not males, unable to produce offspring themselves. Many non-exclusive hypotheses have been proposed, including the positive impact of an additional form of selection, sexual selection, on the probability of persistence of a species.

Male-male competition occurs when two males of the same species compete for the opportunity to mate with a female. Sexually dimorphic traits, size, sex ratio, and the social situation may all play a role in the effects male-male competition has on the reproductive success of a male and the mate choice of a female. Larger males tend to win male-male conflicts due to their sheer strength and ability to ward off other males from taking over their females. For instance, in the fly "Dryomyza anilis", size shows the strongest correlation to the outcome of male-male conflicts over resources like territory and females.

There are multiple types of male-male competition that may occur in a population at different times depending on the conditions. Competition variation occurs based on the frequency of various mating behaviours present in the population. One factor that can influence the type of competition observed is the population density of males. When there is a high density of males present in the population, competition tends to be less aggressive and therefore sneak tactics and disruptions techniques are more often employed. These techniques often indicate a type of competition referred to as scramble competition. In Japanese medaka, "Oryzias latipes", sneaking behaviours refer to when a male interrupts a mating pair during copulation by grasping on to either the male or the female and releasing their own sperm in the hopes of being the one to fertilize the female. Disruption is a technique which involves one male bumping the male that is copulating with the female away just before his sperm is released and the eggs are fertilized.

However, all techniques are not equally successful when in competition for reproductive success. Disruption results in a shorter copulation period and can therefore disrupt the fertilization of the eggs by the sperm, which frequently results in lower rates of fertilization and smaller clutch size.

Another factor that can influence male-male competition is the value of the resource to competitors. Male-male competition can pose many risks to a male's fitness, such as high energy expenditure, physical injury, lower sperm quality and lost paternity. The risk of competition must therefore be worth the value of the resource. A male is more likely to engage in competition for a resource that improves their reproductive success if the resource value is higher. While male-male competition can occur in the presence or absence of a female, competition occurs more frequently in the presence of a female. The presence of a female directly increases the resource value of a territory or shelter and so the males are more likely to accept the risk of competition when a female is present. The smaller males of a species are also more likely to engage in competition with larger males in the presence of a female. Due to the higher level of risk for subordinate males, they tend to engage in competition less frequently than larger, more dominant males and therefore breed less frequently than dominant males. This is seen in many species, such as the Omei treefrog, "Rhacophorus omeimontis," where larger males obtain more mating opportunities and mate with larger females.

A third factor that can impact the success of a male in competition is winner-loser effects. Burrowing crickets, "velarifictorous aspersus," compete for burrows to attract females using their large mandibles for fighting. Female burrowing crickets, are more likely to choose winner of a competition in the 2 hours after the fight. The presence winning male suppresses mating behaviours of the losing males because the winning male tends to produce more frequent and enhanced mating calls in this period of time.

Male-male competition can both positively and negatively affect female fitness. When there is a high density of males in a population and a large number of males attempting to mate with the female, she is more likely to resist mating attempts, resulting in lower fertilization rates. High levels of male-male competition can also result in a reduction in female investment in mating. Many forms of competition can also cause significant distress for the female negatively impacting her ability to reproduce. An increase in male-male competition can affect a females ability to select the best mates, and therefore decrease the likelihood of successful reproduction.

However, group mating in Japanese medaka has been shown to positively affect the fitness of females due to an increase in genetic variation, a higher likelihood of paternal care and a higher likelihood of successful fertilization.

Sexual selection has been observed to occur in plants, animals and fungi. In certain hermaphroditic snail and slug species of molluscs the throwing of love darts is a form of sexual selection. Certain male insects of the order Lepidoptera cement the vaginal pores of their females.

Today, biologists say that certain evolutionary traits can be explained by intraspecific competition—competition between members of the same species—distinguishing between competition before or after sexual intercourse.

Before copulation, "intrasexual selection"—usually between males—may take the form of "male-to-male combat". Also, "intersexual selection", or "mate choice", occurs when females choose between male mates. Traits selected by male combat are called secondary sexual characteristics (including horns, antlers, etc.), which Darwin described as "weapons", while traits selected by mate (usually female) choice are called "ornaments". Due to their sometimes greatly exaggerated nature, secondary sexual characteristics can prove to be a hindrance to an animal, thereby lowering its chances of survival. For example, the large antlers of a moose are bulky and heavy and slow the creature's flight from predators; they also can become entangled in low-hanging tree branches and shrubs, and undoubtedly have led to the demise of many individuals. Bright colourations and showy ornamenations, such as those seen in many male birds, in addition to capturing the eyes of females, also attract the attention of predators. Some of these traits also represent energetically costly investments for the animals that bear them. Because traits held to be due to sexual selection often conflict with the survival fitness of the individual, the question then arises as to why, in nature, in which survival of the fittest is considered the rule of thumb, such apparent liabilities are allowed to persist. However, one must also consider that intersexual selection can occur with an emphasis on resources that one sex possesses rather than morphological and physiological differences. For example, males of "Euglossa imperialis", a non-social bee species, form aggregations of territories considered to be leks, to defend fragrant-rich primary territories. The purpose of these aggregations is only facultative, since the more suitable fragrant-rich sites there are, the more habitable territories there are to inhabit, giving females of this species a large selection of males with whom to potentially mate.

After copulation, male–male competition distinct from conventional aggression may take the form of sperm competition, as described by Parker in 1970. More recently, interest has arisen in "cryptic" female choice, a phenomenon of internally fertilised animals such as mammals and birds, where a female can get rid of a male's sperm without his knowledge.

Finally, sexual conflict is said to occur between breeding partners, sometimes leading to an evolutionary arms race between males and females. Sexual selection can also occur as a product of pheromone release, such as with the stingless bee, "Trigona corvina".

Female mating preferences are widely recognized as being responsible for the rapid and divergent evolution of male secondary sexual traits. Females of many animal species prefer to mate with males with external ornaments – exaggerated features of morphology such as elaborate sex organs. These preferences may arise when an arbitrary female preference for some aspect of male morphology—initially, perhaps, a result of genetic drift—creates, in due course, selection for males with the appropriate ornament. One interpretation of this is known as the sexy son hypothesis. Alternatively, genes that enable males to develop impressive ornaments or fighting ability may simply show off greater disease resistance or a more efficient metabolism, features that also benefit females. This idea is known as the good genes hypothesis.

Bright colors that develop in animals during mating season function to attract partners. It has been suggested that there is a causal link between strength of display of ornaments involved in sexual selection and free radical biology. To test this idea, experiments were performed on male painted dragon lizards. Male lizards are brightly conspicuous in their breeding coloration, but their color declines with aging. Experiments involving administration of antioxidants to these males led to the conclusion that breeding coloration is a reflection of innate anti-oxidation capacity that protects against oxidative damage, including oxidative DNA damage. Thus color could act as a “health certificate” that allows females to visualize the underlying oxidative stress induced damage in potential mates.

Darwin conjectured that heritable traits such as beards and hairlessness in different human populations are results of sexual selection in humans. Geoffrey Miller has hypothesized that many human behaviours not clearly tied to survival benefits, such as humour, music, visual art, verbal creativity, and some forms of altruism, are courtship adaptations that have been favoured through sexual selection. In that view, many human artefacts could be considered subject to sexual selection as part of the extended phenotype, for instance clothing that enhances sexually selected traits . Some argue that the evolution of human intelligence is a sexually selected trait, as it would not confer enough fitness in itself relative to its high maintenance costs.




</doc>
<doc id="26790" url="https://en.wikipedia.org/wiki?curid=26790" title="Stanisław Lem">
Stanisław Lem

Stanisław Herman Lem (; 12 or 13 September 1921 – 27 March 2006) was a Polish writer of science fiction and essays on various subjects, including philosophy, futurology, and literary criticism. Many of his science fiction stories include satire and humor. Lem's books have been translated into 41 languages and have sold over 45 million copies. From the 1950s to 2000s, he published many books, both science fiction and philosophical/futurological. Worldwide, he is best known as the author of the 1961 novel "Solaris", which has been made into a feature film three times. In 1976, Theodore Sturgeon wrote that Lem was the most widely read science fiction writer in the world. The total print of Lem's books is over 30 million copies.

Lem's works explore philosophical themes through speculation on technology, the nature of intelligence, the impossibility of communication with and understanding of alien intelligence, despair about human limitations, and humanity's place in the universe. They are sometimes presented as fiction, but others are in the form of essays or philosophical books.

Translating his works is difficult due to passages with elaborate word formation, idiomatic wordplay, alien or robotic poetry, and puns.

Lem was born in 1921 in Lwów, interwar Poland (now Lviv, Ukraine) to a family of Jewish origin. According to his own account, he was actually born on 13 September, but the date was changed to the 12th on his birth certificate because of superstition. He was the son of Sabina née Woller (1892–1979) and Samuel Lem (1879–1954), a wealthy laryngologist and former physician in the Austro-Hungarian Army, and first cousin to Polish poet Marian Hemar (Lem's father's sister's son). In later years Lem sometimes claimed to have been raised Roman Catholic, but he went to Jewish religious lessons during his school years. He later became an atheist "for moral reasons ... the world appears to me to be put together in such a painful way that I prefer to believe that it was not created ... intentionally". In later years he would call himself both an agnostic and an atheist.

After the Soviet invasion and occupation of Eastern Poland, he was not allowed to study at Lwow Polytechnic as he wished because of his "bourgeois origin", and only due to his father's connections was accepted to study medicine at Lwów University in 1940. During the subsequent Nazi occupation (1941–1944), Lem's family, which had Jewish roots, avoided imprisonment in a ghetto, surviving with false papers. He would later recall:
During that time, Lem earned a living as a car mechanic and welder, and occasionally stole munitions from storehouses (to which he had access as an employee of a German company) to pass them on to the Polish resistance.

In 1945, the Polish Eastern Borderlands were annexed into Soviet Ukraine, and the family, along with many other Poles, was resettled to Kraków, where Lem, at his father's insistence, took up medical studies at the Jagiellonian University. He did not take his final examinations on purpose, to avoid the career of military doctor, which he suspected could have become lifelong. After receiving "absolutorium" (Polish term for the evidence of completion of the studies without diploma), he did an obligatory monthly work at a hospital, at a maternity ward, where he assisted at a number of childbirths and a caesarean section. Lem said that the sight of blood was one of the reasons he decided to drop medicine.

Lem made his literary debut in 1946 with a number of works of different genres, including poetry as well as a science fiction novel, "The Man from Mars" ("Człowiek z Marsa"), serialized in "" ("New World of Adventures"). Between 1948 and 1950 Lem was working as a scientific research assistant at the Jagiellonian University, and published a number of short stories, poems, reviews and similar works, particularly at "Tygodnik Powszechny". In 1951, he published his first book, "The Astronauts" ("Astronauci"). In 1953 he met and married (civil marriage) Barbara Leśniak, a medical student. 
Their church marriage ceremony was performed in February, 1954. In 1954, he published a short story anthology, "Sesame and Other Stories" (""). The following year, 1955, saw the publication of another science fiction novel, "The Magellanic Cloud" ("Obłok Magellana").

During the era of Stalinism, which had begun in Poland in the late 1940s, all published works had to be directly approved by the communist state. Thus "Astronauci" was not, in fact, the first novel Lem finished, just the first that made it past the censors. Going by the date of the finished manuscript, Lem's first book was a partly autobiographical novella "Hospital of the Transfiguration" ("Szpital Przemienienia"), finished in 1948. It would be published seven years later, in 1955, as a trilogy under the title "Czas nieutracony" ("Time Not Lost"). The experience of trying to push "Czas nieutracony" through the censors was one of the major reasons Lem decided to focus on the less-censored genre of science fiction. Nonetheless, most of Lem's works published in the 1950s also contain—forced upon him by the censors and editors—various references to socialist realism as well as the "glorious future of communism". Lem later criticized several of his early pieces as compromised by the ideological pressure.

Lem became truly productive after 1956, when the de-Stalinization period in the Soviet Union led to the "Polish October", when Poland experienced an increase in freedom of speech. Between 1956 and 1968, Lem authored seventeen books. His writing over the next three decades or so was split between science fiction (primarily prose) and essays about science and culture.

In 1957, he published his first non-fiction, philosophical book, "Dialogues", as well as a science fiction anthology, "The Star Diaries" ("Dzienniki gwiazdowe"), collecting short stories about one of his most popular characters, Ijon Tichy. 1959 saw the publication of three books: "Eden", "Śledztwo" and the short story anthology "Inwazja z Aldebarana". 1961 saw two more books, the first regarded as being among his top works: "Pamiętnik znaleziony w wannie", "Solaris", as well as "Powrót z gwiazd". This was followed by a collection of his essays and non-fiction prose, "Wejście na orbitę" (1962), and a short story anthology "Noc księżycowa" (1963). In 1964, Lem published a large work on the border of philosophy and sociology of science and futurology, "Summa Technologiae", as well as a novel, "The Invincible" ("Niezwyciężony").

1965 saw the publication of "The Cyberiad" ("Cyberiada") and of a short story anthology, "The Hunt" (). 1966 is the year of "Wysoki Zamek", followed in 1968 by "Głos Pana" and "Tales of Pirx the Pilot" ("Opowieści o pilocie Pirxie"). "Wysoki Zamek" was another of Lem's autobiographical works, and touched upon a theme that usually was not favored by the censors: Lem's youth in the pre-war, then-Polish, Lviv. 1967 and 1970 saw two more non-fiction treatises, "Filozofia przypadku" and "Fantastyka i futurologia". Ijon Tichy returned in 1971's "The Futurological Congress" "Kongres futurologiczny"; in the same year Lem released a genre-mixing experiment, "Doskonała próżnia", a collection of reviews of non-existent books. In 1973 a similar work, "Wielkość urojona", was published. In 1976, Lem published two novels: "Maska" and "Katar". In 1980, he published another set of reviews of non-existent works, "Prowokacja". The following year sees another Tichy novel, "Wizja lokalna", and "Golem XIV". Later in that decade, Lem published "Pokój na Ziemi" (1984) and "Fiasko" (1986), his final science fiction novel.

In the late 1970s and early 1980s, Lem cautiously supported the Polish dissident movement, and started publishing essays in Paris-based "Kultura". In 1982, with martial law in Poland declared, Lem moved to West Berlin, where he became a fellow of the Institute for Advanced Study, Berlin ("Wissenschaftskolleg zu Berlin"). After that, he settled in Vienna. He returned to Poland in 1988.

From the late 1980s onwards, he tended to concentrate on philosophical texts and essays, published in a number of Polish magazines ("Tygodnik Powszechny", "Odra", "Przegląd", and others). They were later collected in a number of anthologies.

In early 1980s literary critic and historian Stanisław Bereś conducted a lengthy interview with Lem, which got published in book format in 1987 as "Rozmowy ze Stanisławem Lemem" ("Conversations with Stanisław Lem)". That edition was subject to censorship. A revised, complete edition was published in 2002 as "Tako rzecze… Lem" ("Thus spoke... Lem").

In the early 1990s, Lem met with the literary scholar and critic Peter Swirski for a series of extensive interviews, published together with other critical materials and translations as "A Stanislaw Lem Reader" (1997); in the book, Lem speaks about a range of issues rarely touched on before in any interview. Moreover, the book includes Swirski's translation of Lem's retrospective essay "Thirty Years Later", devoted to Lem's nonfictional treatise "Summa Technologiae". During later interviews in 2005, Lem expressed his disappointment with the genre of science fiction, and his general pessimism regarding technical progress. He viewed the human body as unsuitable for space travel, held that information technology drowns people in a glut of low-quality information, and considered truly intelligent robots as both undesirable and impossible to construct. Subsequently, Peter Swirski has published a series of in-depth studies of Lem as a writer, philosopher, and futurologist; notable among them are the recent "From Literature to Biterature: Lem, Turing, Darwin" (2013), "Stanislaw Lem: Selected Letters to Michael Kandel" (2014), "Lemography" (2014), and "Stanislaw Lem: Philosopher of the Future" (2015).

Stanisław Lem works were influenced by such masters of Polish literature as Cyprian Norwid and Stanisław Ignacy Witkiewicz. His prose show a mastery of numerous genres and themes.

Lem's works include a great variety of thought.

One of Lem's major recurring themes, beginning from his very first novel, "The Man from Mars", was the impossibility of communication between profoundly alien beings, which may have no common ground with human intelligence, and humans. The best known example is the living planetary ocean in Lem's novel "Solaris". Other examples include swarms of mechanical insects (in "The Invincible"), and strangely ordered societies of more human-like beings in "Fiasco" and "Eden", describing the failure of the first contact.

Another key recurring theme is the shortcomings of humans. In "His Master's Voice", Lem describes the failure of humanity's intelligence to decipher and truly comprehend an apparent message from space. Two overlapping arcs of short stories, "Fables for Robots" ("Bajki Robotów"), translated in the collection "Mortal Engines"), and "The Cyberiad" ("Cyberiada") provide a commentary on humanity in the form of a series of grotesque, humorous, fairytale-like short stories about a mechanical universe inhabited by robots (who have occasional contact with biological "slimies" and human "palefaces"). Many of Lem's works include beings and/or machines who are more advanced than humans. Lem also underlines the uncertainties of evolution, including that it might not progress upwards in intelligence.

"Śledztwo" and "Katar" are crime novels (the latter without a murderer); "Pamiętnik..." is a psychological drama inspired by Kafka. "Doskonała próżnia" and "Wielkość urojona" are collections of reviews of non-existent books and introductions to them. Similarly, "Prowokacja" purports to review a Holocaust-themed work.

"Dialogues" and "Summa Technologiae" (1964) are Lem's two most famous philosophical texts. The "Summa" is notable for being a unique analysis of prospective social, cybernetic, and biological advances; in this work, Lem discusses philosophical implications of technologies that were completely in the realm of science fiction at the time, but are gaining importance today—for instance, virtual reality and nanotechnology.

Lem's criticism of most science fiction surfaced in literary and philosophical essays "Science Fiction and Futurology" and interviews. In the 1990s, Lem forswore science fiction and returned to futurological prognostications, most notably those expressed in "Blink of an Eye" ("").

Lem said that since the success of Solidarnosc, and the collapse of the Soviet empire, he felt his wild dreams about the future could no longer compare with the reality.

He became increasingly critical of modern technology in his later life, criticizing inventions such as the Internet, which he said "makes it easier to hurt our neighbors."

Lem was awarded an honorary membership in the Science Fiction Writers of America (SFWA) in 1973. SFWA Honorary membership is given to people who do not meet the publishing criteria for joining the regular membership, but who would be welcomed as members had their work appeared in the qualifying English-language publications. Lem never had a high opinion of American science fiction, describing it as ill-thought-out, poorly written, and interested more in making money than in ideas or new literary forms. After his eventual American publication, when he became eligible for regular membership, his honorary membership was rescinded. This formal action was interpreted by some of the SFWA members as a rebuke for his stance, and it seems that Lem interpreted it as such. Lem was invited to stay on with the organization with a regular membership, but declined. After many members (including Ursula K. Le Guin, who quit her membership and then refused the Nebula Award for Best Novelette for "The Diary of the Rose") protested against Lem's treatment by the SFWA, a member offered to pay his dues. Lem never accepted the offer.

Lem singled out only one American science fiction writer for praise, Philip K. Dick, in a 1984 English-language anthology of his critical essays, "". Lem had initially held a low opinion of Philip K. Dick (as he did for the bulk of American science fiction) and would later claim that this was due to a limited familiarity with Dick's work.

Dick, who had mental health problems, maintained that Stanisław Lem was probably a false name used by a composite committee operating on orders of the Communist party to gain control over public opinion, and wrote a letter to the FBI to that effect. Lem was also responsible for the Polish translation of Dick's work "Ubik" in 1972, and when Dick felt monetarily short-changed by the publisher, he held Lem personally responsible (see "").

Lem is one of the most highly acclaimed science fiction writers, hailed by critics as equal to such classic authors as H. G. Wells and Olaf Stapledon. In 1976, Theodore Sturgeon wrote that Lem was the most widely read science fiction writer in the world. In Poland, in the 1960s and 1970s, Lem remained under the radar of mainstream critics, who dismissed him as a "mass market", low-brow, youth-oriented writer; such dismissal might have given him a form of invisibility from censorship. His works were widely translated abroad, appearing in over 40 languages, though the bulk of them were in Eastern Bloc countries (Poland, Germany, Hungary, former Czechoslovakia and the former Soviet Union). Franz Rottensteiner, Lem's former agent abroad, had this to say about Lem's reception on international markets:
His best-known novels include "Solaris" (1961), "His Master's Voice" ("Głos pana", 1968), and the late "Fiasco" ("Fiasko", 1987).

"Solaris" was made into a film in 1968 by Russian director Boris Nirenburg, a film in 1972 by Russian director Andrei Tarkovsky—which won a Special Jury Prize at the Cannes Film Festival in 1972—and an American film in 2002 by Steven Soderbergh. "Solaris" is not the only work of Lem's to be filmed. Over ten film and television adaptations of his work exist, such as adaptations of "The Astronauts" ("First Spaceship on Venus", 1960) and "The Magellan Nebula" ("Ikarie XB-1", 1963). Lem himself was, however, critical of most of the screen adaptations, with the sole exception of "Przekładaniec" in 1968 by Andrzej Wajda. More recently, in 2013, the Israeli–Polish co-production "The Congress" was released, inspired by Lem's novel "The Futurological Congress".

Lem's works have been used in education, for example as teaching texts for philosophy students.

In 1981, the philosophers Douglas R. Hofstadter and Daniel C. Dennett included three extracts from Lem's fiction in their annotated anthology "The Mind's I", accompanied by Hofstadter's comment, which says in part that Lem's "literary and intuitive approach ... does a better job of convincing readers of his views than any hard-nosed scientific article ... might do".

Other influences exerted by Lem's works include Will Wright's popular city-planning game "SimCity", which was partly inspired by Lem's short story "The Seventh Sally".

A major character in the film "Planet 51", an alien Lem, was named by screenwriter Joe Stillman after Stanisław Lem. Since the film was intended to be a parody of American pulp science fiction shot in Eastern Europe, Stillman thought that it would be hilarious to hint at the writer whose works have nothing to do with little green men.



Lem's early works were socialist realist, possibly to satisfy state censors, and in his later years he was critical of this aspect of them. However he never showed any wish to relocate permanently in the west. By the standards of the Soviet bloc, Lem was financially well off for most of his life. He moved with his family to Vienna for a few years, during the time of the crackdown against the Polish trade union Solidarity.

Lem was a critic of capitalism, totalitarianism and of ideologies.

Lem believed there were no absolutes; ""I should wish, as do most men, that immutable truths existed, that not all would be eroded by the impact of historical time, that there were some essential propositions, be it only in the field of human values, the basic values, etc. In brief, I long for the absolute. But at the same time I am firmly convinced that there are no absolutes, that everything is historical, and that you cannot get away from history.""

Lem was concerned that if the human race attained prosperity and comfort this would lead it to passiveness and degeneration, and would prevent human's attainment of transcendence. He believed that transcendence (which in his conception appears to be different from present reality, includes happiness and may include eternal life) might be achievable through the construction of a great many artificial worlds, and "then examine in which one the "sum of happiness" will be the greatest."

Lem was a polyglot: he knew Polish, Latin (from medical school), German, French, English, Russian and Ukrainian. Lem stated that his IQ was tested at high school as 180.

Lem was married to Barbara Lem née Leśniak until his death. She died on 27 April 2016. Their only son, Tomasz, was born in 1968. He studied physics and mathematics at the University of Vienna, and graduated with a degree in physics from Princeton University. Tomasz wrote a memoir about his father, "Awantury na tle powszechnego ciążenia" ("Tantrums on the Background of the Universal Gravitation"), which contain numerous personal details about Stanisław Lem. The annotation of the book says Tomasz works as a translator and has a daughter, Anna.

As of 1984, Lem's writing pattern was to get up a short time before five in the morning and start writing soon after, for 5 or 6 hours before taking a break.

Lem was an aggressive driver. He loved sweets (especially halva and chocolate-covered marzipan), and did not give them up even when, toward the end of his life, he fell ill with diabetes. In the mid-80s due to health problems he stopped smoking.

Stanisław Lem died from heart disease in Kraków on 27 March 2006 at the age of 84.






</doc>
<doc id="26791" url="https://en.wikipedia.org/wiki?curid=26791" title="Satire">
Satire

Satire is a genre of literature and performing arts, usually fiction and less frequently in non-fiction, in which vices, follies, abuses and shortcomings are held up to ridicule, ideally with the intent of shaming individuals, corporations, government, or society itself into improvement. Although satire is usually meant to be humorous, its greater purpose is often constructive social criticism, using wit to draw attention to both particular and wider issues in society.

A feature of satire is strong irony or sarcasm —"in satire, irony is militant", according to literary critic Northrup Frye— but parody, burlesque, exaggeration, juxtaposition, comparison, analogy, and double entendre are all frequently used in satirical speech and writing. This "militant" irony or sarcasm often professes to approve of (or at least accept as natural) the very things the satirist wishes to question.

Satire is found in many artistic forms of expression, including internet memes, literature, plays, commentary, music, film and television shows, and media such as lyrics.

The word "satire" comes from the Latin word "satur" and the subsequent phrase "lanx satura." "Satur" meant "full" but the juxtaposition with "lanx" shifted the meaning to "miscellany or medley": the expression "lanx satura" literally means "a full dish of various kinds of fruits".

The word "satura" as used by Quintilian, however, was used to denote only Roman verse satire, a strict genre that imposed hexameter form, a narrower genre than what would be later intended as "satire". Quintilian famously said that "satura," that is a satire in hexameter verses, was a literary genre of wholly Roman origin ("satura tota nostra est"). He was aware of and commented on Greek satire, but at the time did not label it as such, although today the origin of satire is considered to be Aristophanes' Old Comedy. The first critic to use the term "satire" in the modern broader sense was Apuleius.

To Quintilian, the satire was a strict literary form, but the term soon escaped from the original narrow definition. Robert Elliott writes:
The word "satire" derives from "satura", and its origin was not influenced by the Greek mythological figure of the "satyr". In the 17th century, philologist Isaac Casaubon was the first to dispute the etymology of satire from satyr, contrary to the belief up to that time.

Laughter is not an essential component of satire; in fact there are types of satire that are not meant to be "funny" at all. Conversely, not all humour, even on such topics as politics, religion or art is necessarily "satirical", even when it uses the satirical tools of irony, parody, and burlesque.

Even light-hearted satire has a serious "after-taste": the organizers of the Ig Nobel Prize describe this as "first make people laugh, and then make them think".

Satire and irony in some cases have been regarded as the most effective source to understand a society, the oldest form of social study. They provide the keenest insights into a group's collective psyche, reveal its deepest values and tastes, and the society's structures of power. Some authors have regarded satire as superior to non-comic and non-artistic disciplines like history or anthropology. In a prominent example from ancient Greece, philosopher Plato, when asked by a friend for a book to understand Athenian society, referred him to the plays of Aristophanes.

Historically, satire has satisfied the popular need to debunk and ridicule the leading figures in politics, economy, religion and other prominent realms of power. Satire confronts public discourse and the collective imaginary, playing as a public opinion counterweight to power (be it political, economic, religious, symbolic, or otherwise), by challenging leaders and authorities. For instance, it forces administrations to clarify, amend or establish their policies. Satire's job is to expose problems and contradictions, and it's not obligated to solve them. Karl Kraus set in the history of satire a prominent example of a satirist role as confronting public discourse.

For its nature and social role, satire has enjoyed in many societies a special freedom license to mock prominent individuals and institutions. The satiric impulse, and its ritualized expressions, carry out the function of resolving social tension. Institutions like the ritual clowns, by giving expression to the antisocial tendencies, represent a safety valve which re-establishes equilibrium and health in the collective imaginary, which are jeopardized by the repressive aspects of society.

The state of political satire in a given society reflects the tolerance or intolerance that characterizes it, and the state of civil liberties and human rights. Under totalitarian regimes any criticism of a political system, and especially satire, is suppressed. A typical example is the Soviet Union where the dissidents, such as Aleksandr Solzhenitsyn and Andrei Sakharov were under strong pressure from the government. While satire of everyday life in the USSR was allowed, the most prominent satirist being Arkady Raikin, political satire existed in the form of anecdotes that made fun of Soviet political leaders, especially Brezhnev, famous for his narrow-mindedness and love for awards and decorations.

Satire is a diverse genre which is complex to classify and define, with a wide range of satiric "modes".

Satirical literature can commonly be categorized as either Horatian, Juvenalian, or Menippean.

Horatian satire, named for the Roman satirist Horace (65–8 BCE), playfully criticizes some social vice through gentle, mild, and light-hearted humour. Horace (Quintus Horatius Flaccus) wrote Satires to gently ridicule the dominant opinions and "philosophical beliefs of ancient Rome and Greece" (Rankin). Rather than writing in harsh or accusing tones, he addressed issues with humor and clever mockery. Horatian satire follows this same pattern of "gently [ridiculing] the absurdities and follies of human beings" (Drury).

It directs wit, exaggeration, and self-deprecating humour toward what it identifies as folly, rather than evil. Horatian satire's sympathetic tone is common in modern society.

A Horatian satirist's goal is to heal the situation with smiles, rather than by anger. Horatian satire is a gentle reminder to take life less seriously and evokes a wry smile. A Horatian satirist makes fun of general human folly rather than engaging in specific or personal attacks. Shamekia Thomas suggests, "In a work using Horatian satire, readers often laugh at the characters in the story who are the subject of mockery as well as themselves and society for behaving in those ways." Alexander Pope has been established as an author whose satire "heals with morals what it hurts with wit" (Green). Alexander Pope—and Horatian satire—attempt to teach.

Examples of Horatian satire:


Juvenalian satire, named for the writings of the Roman satirist Juvenal (late first century – early second century AD), is more contemptuous and abrasive than the Horatian. Juvenal disagreed with the opinions of the public figures and institutions of the Republic and actively attacked them through his literature. "He utilized the satirical tools of exaggeration and parody to make his targets appear monstrous and incompetent" (Podzemny). Juvenal's satire follows this same pattern of abrasively ridiculing societal structures. Juvenal also, unlike Horace, attacked public officials and governmental organizations through his satires, regarding their opinions as not just wrong, but evil.

Following in this tradition, Juvenalian satire addresses perceived social evil through scorn, outrage, and savage ridicule. This form is often pessimistic, characterized by the use of irony, sarcasm, moral indignation and personal invective, with less emphasis on humor. Strongly polarized political satire can often be classified as Juvenalian.

A Juvenal satirist's goal is generally to provoke some sort of political or societal change because he sees his opponent or object as evil or harmful. A Juvenal satirist mocks "societal structure, power, and civilization" (Thomas) by exaggerating the words or position of his opponent in order to jeopardize their opponent's reputation and/or power. Jonathan Swift has been established as an author who "borrowed heavily from Juvenal's techniques in [his critique] of contemporary English society" (Podzemny).

Examples of Juvenalian satire:

See Menippean satire.

In the history of theatre there has always been a conflict between engagement and disengagement on politics and relevant issue, between satire and grotesque on one side, and jest with teasing on the other. Max Eastman defined the spectrum of satire in terms of "degrees of biting", as ranging from satire proper at the hot-end, and "kidding" at the violet-end; Eastman adopted the term kidding to denote what is just satirical in form, but is not really firing at the target. Nobel laureate satirical playwright Dario Fo pointed out the difference between satire and teasing ("sfottò"). Teasing is the reactionary side of the comic; it limits itself to a shallow parody of physical appearance. The side-effect of teasing is that it humanizes and draws sympathy for the powerful individual towards which it is directed. Satire instead uses the comic to go against power and its oppressions, has a subversive character, and a moral dimension which draws judgement against its targets. Fo formulated an operational criterion to tell real satire from "sfottò", saying that real satire arouses an outraged and violent reaction, and that the more they try to stop you, the better is the job you are doing. Fo contends that, historically, people in positions of power have welcomed and encouraged good-humoured buffoonery, while modern day people in positions of power have tried to censor, ostracize and repress satire.

Teasing ("sfottò") is an ancient form of simple buffoonery, a form of comedy without satire's subversive edge. Teasing includes light and affectionate parody, good-humoured mockery, simple one-dimensional poking fun, and benign spoofs. Teasing typically consists of an impersonation of someone monkeying around with his exterior attributes, tics, physical blemishes, voice and mannerisms, quirks, way of dressing and walking, and/or the phrases he typically repeats. By contrast, teasing never touches on the core issue, never makes a serious criticism judging the target with irony; it never harms the target's conduct, ideology and position of power; it never undermines the perception of his morality and cultural dimension. "Sfottò" directed towards a powerful individual makes him appear more human and draws sympathy towards him. Hermann Göring propagated jests and jokes against himself, with the aim of humanizing his image.

Types of satire can also be classified according to the topics it deals with. From the earliest times, at least since the plays of Aristophanes, the primary topics of literary satire have been politics, religion and sex. This is partly because these are the most pressing problems that affect anybody living in a society, and partly because these topics are usually taboo. Among these, politics in the broader sense is considered the pre-eminent topic of satire. Satire which targets the clergy is a type of political satire, while religious satire is that which targets religious beliefs. Satire on sex may overlap with blue comedy, off-color humor and dick jokes.

Scatology has a long literary association with satire, as it is a classical mode of the grotesque, the grotesque body and the satiric grotesque. Shit plays a fundamental role in satire because it symbolizes death, the turd being "the ultimate dead object". The satirical comparison of individuals or institutions with human excrement, exposes their "inherent inertness, corruption and dead-likeness". The ritual clowns of clown societies, like among the Pueblo Indians, have ceremonies with filth-eating. In other cultures, sin-eating is an apotropaic rite in which the sin-eater (also called filth-eater), by ingesting the food provided, takes "upon himself the sins of the departed". Satire about death overlaps with black humor and gallows humor.

Another classification by topics is the distinction between political satire, religious satire and satire of manners. Political satire is sometimes called topical satire, satire of manners is sometimes called satire of everyday life, and religious satire is sometimes called philosophical satire. Comedy of manners, sometimes also called satire of manners, criticizes mode of life of common people; political satire aims at behavior, manners of politicians, and vices of political systems. Historically, comedy of manners, which first appeared in British theater in 1620, has uncritically accepted the social code of the upper classes. Comedy in general accepts the rules of the social game, while satire subverts them.

Another analysis of satire is the spectrum of his possible tones: wit, ridicule, irony, sarcasm, cynicism, the sardonic and invective.

The type of humour that deals with creating laughter at the expense of the person telling the joke is called reflexive humour. Reflexive humour can take place at dual levels of directing humour at self or at the larger community the self identifies with. The audience's understanding of the context of reflexive humour is important for its receptivity and success. Satire is found not only in written literary forms. In preliterate cultures it manifests itself in ritual and folk forms, as well as in trickster tales and oral poetry.

It appears also in graphic arts, music, sculpture, dance, cartoon strips, and graffiti. Examples are Dada sculptures, Pop Art works, music of Gilbert and Sullivan and Erik Satie, punk and rock music. In modern media culture, stand-up comedy is an enclave in which satire can be introduced into mass media, challenging mainstream discourse. Comedy roasts, mock festivals, and stand-up comedians in nightclubs and concerts are the modern forms of ancient satiric rituals.

One of the earliest examples of what we might call satire, The Satire of the Trades, is in Egyptian writing from the beginning of the 2nd millennium BC. The text's apparent readers are students, tired of studying. It argues that their lot as scribes is not only useful, but far superior to that of the ordinary man. Scholars such as Helck think that the context was meant to be serious.

The Papyrus Anastasi I (late 2nd millennium BC) contains a satirical letter which first praises the virtues of its recipient, but then mocks the reader's meagre knowledge and achievements.

The Greeks had no word for what later would be called "satire", although the terms cynicism and parody were used. Modern critics call the Greek playwright Aristophanes one of the best known early satirists: his plays are known for their critical political and societal commentary, particularly for the political satire by which he criticized the powerful Cleon (as in "The Knights"). He is also notable for the persecution he underwent. Aristophanes' plays turned upon images of filth and disease. His bawdy style was adopted by Greek dramatist-comedian Menander. His early play "Drunkenness" contains an attack on the politician Callimedon.

The oldest form of satire still in use is the Menippean satire by Menippus of Gadara. His own writings are lost. Examples from his admirers and imitators mix seriousness and mockery in dialogues and present parodies before a background of diatribe. As in the case of Aristophanes plays, menippean satire turned upon images of filth and disease.

The first Roman to discuss satire critically was Quintilian, who invented the term to describe the writings of Gaius Lucilius. The two most prominent and influential ancient Roman satirists are Horace and Juvenal, who wrote during the early days of the Roman Empire. Other important satirists in ancient Latin are Gaius Lucilius and Persius. "Satire" in their work is much wider than in the modern sense of the word, including fantastic and highly coloured humorous writing with little or no real mocking intent. When Horace criticized Augustus, he used veiled ironic terms. In contrast, Pliny reports that the 6th-century-BC poet Hipponax wrote "satirae" that were so cruel that the offended hanged themselves.

In the 2nd century AD, Lucian wrote "True History", a book satirizing the clearly unrealistic travelogues/adventures written by Ctesias, Iambulus, and Homer. He states that he was surprised they expected people to believe their lies, and stating that he, like them, has no actual knowledge or experience, but shall now tell lies as if he did. He goes on to describe a far more obviously extreme and unrealistic tale, involving interplanetary exploration, war among alien life forms, and life inside a 200 mile long whale back in the terrestrial ocean, all intended to make obvious the fallacies of books like "Indica" and "The Odyssey".

Medieval Arabic poetry included the satiric genre "hija". Satire was introduced into Arabic prose literature by the author Al-Jahiz in the 9th century. While dealing with serious topics in what are now known as anthropology, sociology and psychology, he introduced a satirical approach, "based on the premise that, however serious the subject under review, it could be made more interesting and thus achieve greater effect, if only one leavened the lump of solemnity by the insertion of a few amusing anecdotes or by the throwing out of some witty or paradoxical observations. He was well aware that, in treating of new themes in his prose works, he would have to employ a vocabulary of a nature more familiar in "hija", satirical poetry." For example, in one of his zoological works, he satirized the preference for longer human penis size, writing: "If the length of the penis were a sign of honor, then the mule would belong to the (honorable tribe of) Quraysh". Another satirical story based on this preference was an "Arabian Nights" tale called "Ali with the Large Member".

In the 10th century, the writer Tha'alibi recorded satirical poetry written by the Arabic poets As-Salami and Abu Dulaf, with As-Salami praising Abu Dulaf's wide breadth of knowledge and then mocking his ability in all these subjects, and with Abu Dulaf responding back and satirizing As-Salami in return. An example of Arabic political satire included another 10th-century poet Jarir satirizing Farazdaq as "a transgressor of the Sharia" and later Arabic poets in turn using the term "Farazdaq-like" as a form of political satire.

The terms "comedy" and "satire" became synonymous after Aristotle's "Poetics" was translated into Arabic in the medieval Islamic world, where it was elaborated upon by Islamic philosophers and writers, such as Abu Bischr, his pupil Al-Farabi, Avicenna, and Averroes. Due to cultural differences, they disassociated comedy from Greek dramatic representation and instead identified it with Arabic poetic themes and forms, such as "hija" (satirical poetry). They viewed comedy as simply the "art of reprehension", and made no reference to light and cheerful events, or troubled beginnings and happy endings, associated with classical Greek comedy. After the Latin translations of the 12th century, the term "comedy" thus gained a new semantic meaning in Medieval literature.

Ubayd Zakani introduced satire in Persian literature during the 14th century. His work is noted for its satire and obscene verses, often political or bawdy, and often cited in debates involving homosexual practices. He wrote the "Resaleh-ye Delgosha", as well as "Akhlaq al-Ashraf" ("Ethics of the Aristocracy") and the famous humorous fable "Masnavi Mush-O-Gorbeh" (Mouse and Cat), which was a political satire. His non-satirical serious classical verses have also been regarded as very well written, in league with the other great works of Persian literature. Between 1905 and 1911, Bibi Khatoon Astarabadi and other Iranian writers wrote notable satires.

In the Early Middle Ages, examples of satire were the songs by Goliards or vagants now best known as an anthology called Carmina Burana and made famous as texts of a composition by the 20th-century composer Carl Orff. Satirical poetry is believed to have been popular, although little has survived. With the advent of the High Middle Ages and the birth of modern vernacular literature in the 12th century, it began to be used again, most notably by Chaucer. The disrespectful manner was considered "unchristian" and ignored, except for the moral satire, which mocked misbehaviour in Christian terms. Examples are "Livre des Manières" by (~1178), and some of Chaucer's "Canterbury Tales". Sometimes epic poetry (epos) was mocked, and even feudal society, but there was hardly a general interest in the genre.

Direct social commentary via satire returned with a vengeance in the 16th century, when farcical texts such as the works of François Rabelais tackled more serious issues (and incurred the wrath of the crown as a result).

Two major satirists of Europe in the Renaissance were Giovanni Boccaccio and François Rabelais. Other examples of Renaissance satire include "Till Eulenspiegel", "Reynard the Fox", Sebastian Brant's "Narrenschiff" (1494), Erasmus's "Moriae Encomium" (1509), Thomas More's "Utopia" (1516), and "Carajicomedia" (1519).

The Elizabethan (i.e. 16th-century English) writers thought of satire as related to the notoriously rude, coarse and sharp satyr play. Elizabethan "satire" (typically in pamphlet form) therefore contains more straightforward abuse than subtle irony. The French Huguenot Isaac Casaubon pointed out in 1605 that satire in the Roman fashion was something altogether more civilised. Casaubon discovered and published Quintilian's writing and presented the original meaning of the term (satira, not satyr), and the sense of wittiness (reflecting the "dishfull of fruits") became more important again. Seventeenth-century English satire once again aimed at the "amendment of vices" (Dryden).

In the 1590s a new wave of verse satire broke with the publication of Hall's "Virgidemiarum", six books of verse satires targeting everything from literary fads to corrupt noblemen. Although Donne had already circulated satires in manuscript, Hall's was the first real attempt in English at verse satire on the Juvenalian model. The success of his work combined with a national mood of disillusion in the last years of Elizabeth's reign triggered an avalanche of satire—much of it less conscious of classical models than Hall's — until the fashion was brought to an abrupt stop by censorship.

Satire ("Kataksh" or "Vyang") has played a prominent role in Indian and Hindi literature, and is counted as one of the "ras" of literature in ancient books. With the commencement of printing of books in local language in the nineteenth century and especially after India's freedom, this grew. Many of the works of Tulsi Das, Kabir, Munshi Premchand, village ministrels, Hari katha singers, poets, Dalit singers and current day stand up Indian comedians incorporate satire, usually ridiculing authoritarians, fundamentalists and incompetent people in power. In India, it has usually been used as a means of expression and an outlet for common people to express their anger against authoritarian entities. A popular custom in Northern India of "Bura na mano Holi hai" continues, in which comedians on the stage roast local people of importance (who are usually brought in as special guests).

The Age of Enlightenment, an intellectual movement in the 17th and 18th centuries advocating rationality, produced a great revival of satire in Britain. This was fuelled by the rise of partisan politics, with the formalisation of the Tory and Whig parties—and also, in 1714, by the formation of the Scriblerus Club, which included Alexander Pope, Jonathan Swift, John Gay, John Arbuthnot, Robert Harley, Thomas Parnell, and Henry St John, 1st Viscount Bolingbroke. This club included several of the notable satirists of early-18th-century Britain. They focused their attention on Martinus Scriblerus, "an invented learned fool... whose work they attributed all that was tedious, narrow-minded, and pedantic in contemporary scholarship". In their hands astute and biting satire of institutions and individuals became a popular weapon. The turn to the 18th century was characterized by a switch from Horatian, soft, pseudo-satire, to biting "juvenal" satire.

Jonathan Swift was one of the greatest of Anglo-Irish satirists, and one of the first to practise modern journalistic satire. For instance, In his "A Modest Proposal" Swift suggests that Irish peasants be encouraged to sell their own children as food for the rich, as a solution to the "problem" of poverty. His purpose is of course to attack indifference to the plight of the desperately poor. In his book "Gulliver's Travels" he writes about the flaws in human society in general and English society in particular. John Dryden wrote an influential essay entitled "A Discourse Concerning the Original and Progress of Satire" that helped fix the definition of satire in the literary world. His satirical "Mac Flecknoe" was written in response to a rivalry with Thomas Shadwell and eventually inspired Alexander Pope to write his satirical "The Rape of the Lock". Other satirical works by Pope include the "Epistle to Dr Arbuthnot".

Alexander Pope (b. May 21, 1688) was a satirist known for his Horatian satirist style and translation of the "Iliad". Famous throughout and after the long 18th century, Pope died in 1744. Pope, in his "The Rape of the Lock", is delicately chiding society in a sly but polished voice by holding up a mirror to the follies and vanities of the upper class. Pope does not actively attack the self-important pomp of the British aristocracy, but rather presents it in such a way that gives the reader a new perspective from which to easily view the actions in the story as foolish and ridiculous. A mockery of the upper class, more delicate and lyrical than brutal, Pope nonetheless is able to effectively illuminate the moral degradation of society to the public. "The Rape of the Lock" assimilates the masterful qualities of a heroic epic, such as the "Iliad", which Pope was translating at the time of writing "The Rape of the Lock". However, Pope applied these qualities satirically to a seemingly petty egotistical elitist quarrel to prove his point wryly.

Daniel Defoe pursued a more journalistic type of satire, being famous for his "The True-Born Englishman" which mocks xenophobic patriotism, and "The Shortest-Way with the Dissenters"—advocating religious toleration by means of an ironical exaggeration of the highly intolerant attitudes of his time.

The pictorial satire of William Hogarth is a precursor to the development of political cartoons in 18th-century England. The medium developed under the direction of its greatest exponent, James Gillray from London. With his satirical works calling the king (George III), prime ministers and generals (especially Napoleon) to account, Gillray's wit and keen sense of the ridiculous made him the pre-eminent cartoonist of the era.

Ebenezer Cooke (1665–1732), author of "The Sot-Weed Factor" (1708), was among the first American colonialists to write literary satire. Benjamin Franklin (1706–1790) and others followed, using satire to shape an emerging nation's culture through its sense of the ridiculous.

Several satiric papers competed for the public's attention in the Victorian era (1837–1901) and Edwardian period, such as "Punch" (1841) and "Fun" (1861).

Perhaps the most enduring examples of Victorian satire, however, are to be found in the Savoy Operas of Gilbert and Sullivan. In fact, in "The Yeomen of the Guard", a jester is given lines that paint a very neat picture of the method and purpose of the satirist, and might almost be taken as a statement of Gilbert's own intent:

Novelists such as Charles Dickens (1812-1870) often used passages of satiric writing in their treatment of social issues.

Continuing the tradition of Swiftian journalistic satire, Sidney Godolphin Osborne (1808-1889) was the most prominent writer of scathing "Letters to the Editor" of the London Times. Famous in his day, he is now all but forgotten. His maternal grandfather William Eden, 1st Baron Auckland was considered to be a possible candidate for the authorship of the Junius letters. If this were true, we can read Osborne as following in his grandfather's satiric "Letters to the Editor" path. Osborne's satire was so bitter and biting that at one point he received a public censure from Parliament's then Home Secretary Sir James Graham. Osborne wrote mostly in the Juvenalian mode over a wide range of topics mostly centered on British government's and landlords' mistreatment of poor farm workers and field laborers. He bitterly opposed the New Poor Laws and was passionate on the subject of Great Britain's botched response to the Irish Famine and its mistreatment of soldiers during the Crimean War.

Later in the nineteenth century, in the United States, Mark Twain (1835–1910) grew to become American's greatest satirist: his novel "Huckleberry Finn" (1884) is set in the antebellum South, where the moral values Twain wishes to promote are completely turned on their heads. His hero, Huck, is a rather simple but goodhearted lad who is ashamed of the "sinful temptation" that leads him to help a runaway slave. In fact his conscience, warped by the distorted moral world he has grown up in, often bothers him most when he is at his best. He is prepared to do good, believing it to be wrong.

Twain's younger contemporary Ambrose Bierce (1842–1913) gained notoriety as a cynic, pessimist and black humorist with his dark, bitterly ironic stories, many set during the American Civil War, which satirized the limitations of human perception and reason. Bierce's most famous work of satire is probably "The Devil's Dictionary" (1906), in which the definitions mock cant, hypocrisy and received wisdom.

Karl Kraus is considered the first major European satirist since Jonathan Swift. In 20th-century literature, satire was used by English authors such as Aldous Huxley (1930s) and George Orwell (1940s), which under the inspiration of Zamyatin's Russian 1921 novel "We", made serious and even frightening commentaries on the dangers of the sweeping social changes taking place throughout Europe. Anatoly Lunacharsky wrote ‘Satire attains its greatest significance when a newly evolving class creates an ideology considerably more advanced than that of the ruling class, but has not yet developed to the point where it can conquer it. Herein lies its truly great ability to triumph, its scorn for its adversary and its hidden fear of it. Herein lies its venom, its amazing energy of hate, and quite frequently, its grief, like a black frame around glittering images. Herein lie its contradictions, and its power.’ Many social critics of this same time in the United States, such as Dorothy Parker and H. L. Mencken, used satire as their main weapon, and Mencken in particular is noted for having said that "one horse-laugh is worth ten thousand syllogisms" in the persuasion of the public to accept a criticism. Novelist Sinclair Lewis was known for his satirical stories such as "Main Street" (1920), "Babbitt" (1922), "Elmer Gantry" (1927; dedicated by Lewis to H. L. Menchen), and "It Can't Happen Here" (1935), and his books often explored and satirized contemporary American values. The film "The Great Dictator" (1940) by Charlie Chaplin is itself a parody of Adolf Hitler; Chaplin later declared that he would have not made the film if he had known about the concentration camps.
In the United States 1950s, satire was introduced into American stand-up comedy most prominently by Lenny Bruce and Mort Sahl. As they challenged the taboos and conventional wisdom of the time, were ostracized by the mass media establishment as "sick comedians". In the same period, Paul Krassner's magazine "The Realist" began publication, to become immensely popular during the 1960s and early 1970s among people in the counterculture; it had articles and cartoons that were savage, biting satires of politicians such as Lyndon Johnson and Richard Nixon, the Vietnam War, the Cold War and the War on Drugs. This baton was also carried by the original National Lampoon magazine, edited by Doug Kenney and Henry Beard and featuring blistering satire written by Michael O'Donoghue, P.J. O'Rourke, and Tony Hendra, among others. Prominent satiric stand-up comedian George Carlin acknowledged the influence "The Realist" had in his 1970s conversion to a satiric comedian.

A more humorous brand of satire enjoyed a renaissance in the UK in the early 1960s with the satire boom, led by such luminaries as Peter Cook, Alan Bennett, Jonathan Miller, and Dudley Moore, whose stage show "Beyond the Fringe" was a hit not only in Britain, but also in the United States. Other significant influences in 1960s British satire include David Frost, Eleanor Bron and the television program "That Was The Week That Was".

Joseph Heller's most famous work, "Catch-22" (1961), satirizes bureaucracy and the military, and is frequently cited as one of the greatest literary works of the twentieth century. Departing from traditional Hollywood farce and screwball, director and comedian Jerry Lewis used satire in his self-directed films "The Bellboy" (1960), "The Errand Boy" (1961) and "The Patsy" (1964) to comment on celebrity and the star-making machinery of Hollywood.
The film "Dr. Strangelove" (1964) starring Peter Sellers was a popular satire on the Cold War.

Contemporary popular usage of the term "satire" is often very imprecise. While satire often uses caricature and parody, by no means all uses of these or other humorous devices are satiric. Refer to the careful definition of satire that heads this article. "The Cambridge Companion to Roman Satire" also warns of the ambiguous nature of satire: 

Satire is used on many UK television programmes, particularly popular panel shows and quiz shows such as "Mock the Week" (2005–ongoing) and "Have I Got News for You" (1990–ongoing). It is found on radio quiz shows such as "The News Quiz" (1977–ongoing) and "The Now Show" (1998–ongoing). One of the most watched UK television shows of the 1980s and early 1990s, the puppet show "Spitting Image" was a satire of the royal family, politics, entertainment, sport and British culture of the era. Court Flunkey from "Spitting Image" is a caricature of James Gillray, intended as a homage to the father of political cartooning. 

Created by DMA Design in 1997, satire features prominently in the British video game series "Grand Theft Auto". Another example is the "Fallout" series, namely Interplay-developed "Fallout: A Post Nuclear Role Playing Game" (1995). Other games utilizing satire include "Postal" (1997), and "State of Emergency" (2002).

Trey Parker and Matt Stone's "South Park" (1997–ongoing) relies almost exclusively on satire to address issues in American culture, with episodes addressing racism, anti-Semitism, militant atheism, homophobia, sexism, environmentalism, corporate culture, political correctness and anti-Catholicism, among many other issues.

Satirical web series and sites include Emmy-nominated video game-themed "Honest Trailers" (2012–), Internet phenomena-themed Encyclopedia Dramatica (2004–), Uncyclopedia (2005–), and self-proclaimed "America's Finest News Source" "The Onion" (1988–).
In the United States, Stephen Colbert's television program, "The Colbert Report" (2005–14) is instructive in the methods of contemporary American satire; sketch comedy television show "Saturday Night Live" is also known for its satirical impressions and parodies of prominent persons and politicians, among some of the most notable, their parodies of U.S. political figures Hillary Clinton and of Sarah Palin. Colbert's character is an opinionated and self-righteous commentator who, in his TV interviews, interrupts people, points and wags his finger at them, and "unwittingly" uses a number of logical fallacies. In doing so, he demonstrates the principle of modern American political satire: the ridicule of the actions of politicians and other public figures by taking all their statements and purported beliefs to their furthest (supposedly) logical conclusion, thus revealing their perceived hypocrisy or absurdity.

In the United Kingdom, a popular modern satirist was the late Sir Terry Pratchett, author of the internationally best-selling "Discworld" book series. One of the most well-known and controversial British satirists is Chris Morris, co-writer and director of "Four Lions".

In Canada, satire has become an important part of the comedy scene. Stephen Leacock was one of the best known early Canadian satirists, and in the early 20th century, he achieved fame by targeting the attitudes of small town life. In more recent years, Canada has had several prominent satirical television series and radio shows. Some, including "CODCO", "The Royal Canadian Air Farce", "This Is That", and "This Hour Has 22 Minutes" deal directly with current news stories and political figures, while others, like "History Bites" present contemporary social satire in the context of events and figures in history. The Beaverton is a Canadian news satire site similar to The Onion. Canadian songwriter Nancy White uses music as the vehicle for her satire, and her comic folk songs are regularly played on CBC Radio.

Cartoonists often use satire as well as straight humour. Al Capp's satirical comic strip "Li'l Abner" was censored in September 1947. The controversy, as reported in "Time", centred on Capp's portrayal of the US Senate. Said Edward Leech of Scripps-Howard, "We don't think it is good editing or sound citizenship to picture the Senate as an assemblage of freaks and crooks... boobs and undesirables." Walt Kelly's "Pogo" was likewise censored in 1952 over his overt satire of Senator Joe McCarthy, caricatured in his comic strip as "Simple J. Malarky". Garry Trudeau, whose comic strip "Doonesbury" focuses on satire of the political system, and provides a trademark cynical view on national events. Trudeau exemplifies humour mixed with criticism. For example, the character Mark Slackmeyer lamented that because he was not legally married to his partner, he was deprived of the "exquisite agony" of experiencing a nasty and painful divorce like heterosexuals. This, of course, satirized the claim that gay unions would denigrate the sanctity of heterosexual marriage.
Like some literary predecessors, many recent television satires contain strong elements of parody and caricature; for instance, the popular animated series "The Simpsons" and "South Park" both parody modern family and social life by taking their assumptions to the extreme; both have led to the creation of similar series. As well as the purely humorous effect of this sort of thing, they often strongly criticise various phenomena in politics, economic life, religion and many other aspects of society, and thus qualify as satirical. Due to their animated nature, these shows can easily use images of public figures and generally have greater freedom to do so than conventional shows using live actors.

News satire is also a very popular form of contemporary satire, appearing in as wide an array of formats as the news media itself: print (e.g. "The Onion", "Waterford Whispers News", "Private Eye"), radio (e.g. "On the Hour"), television (e.g. "The Day Today", "The Daily Show", "Brass Eye") and the web (e.g. "Faking News", "El Koshary Today", "Babylon Bee", "The Beaverton", "The Daily Bonnet" and "The Onion"). Other satires are on the list of satirists and satires.

In an interview with "Wikinews", Sean Mills, President of "The Onion", said angry letters about their news parody always carried the same message. "It's whatever affects that person", said Mills. "So it's like, 'I love it when you make a joke about murder or rape, but if you talk about cancer, well my brother has cancer and that's not funny to me.' Or someone else can say, 'Cancer's "hilarious", but don't talk about rape because my cousin got raped.' Those are rather extreme examples, but if it affects somebody personally, they tend to be more sensitive about it."

Literary satire is usually written out of earlier satiric works, reprising previous conventions, commonplaces, stance, situations and tones of voice. Exaggeration is one of the most common satirical techniques. Contrarily diminution is also a satirical technique.

For its nature and social role, satire has enjoyed in many societies a special freedom license to mock prominent individuals and institutions. In Germany and Italy satire is protected by the constitution.

Since satire belongs to the realm of art and artistic expression, it benefits from broader lawfulness limits than mere freedom of information of journalistic kind. In some countries a specific "right to satire" is recognized and its limits go beyond the "right to report" of journalism and even the "right to criticize". Satire benefits not only of the protection to freedom of speech, but also to that to culture, and that to scientific and artistic production.

In September 2017 The Juice Media received an e-mail from the Australian National Symbols Officer requesting that the use of a satirical logo, called the "Coat of Harms" based on the Australian Coat of Arms, no longer be used as they had received complaints from the members of the public. Coincidentally 5 days later a Bill was proposed to Australian parliament to amend the Criminal Code Act 1995. If passed, those found to be in breach of the new amendment can face 2–5 years imprisonment.

As of June 2018, the Criminal Code Amendment (Impersonating a Commonwealth Body) Bill 2017 was before the Australian Senate with the third reading moved May 10, 2018.

Descriptions of satire's biting effect on its target include 'venomous', 'cutting', 'stinging', vitriol. Because satire often combines anger and humor, as well as the fact that it addresses and calls into question many controversial issues, it can be profoundly disturbing.

Because it is essentially ironic or sarcastic, satire is often misunderstood. A typical misunderstanding is to confuse the satirist with his persona.

Common uncomprehending responses to satire include revulsion (accusations of poor taste, or that "it's just not funny" for instance) and the idea that the satirist actually does support the ideas, policies, or people he is attacking. For instance, at the time of its publication, many people misunderstood Swift's purpose in "A Modest Proposal", assuming it to be a serious recommendation of economically motivated cannibalism.

Some critics of Mark Twain see "Huckleberry Finn" as racist and offensive, missing the point that its author clearly intended it to be satire (racism being in fact only one of a number of Mark Twain's known concerns attacked in "Huckleberry Finn"). This same misconception was suffered by the main character of the 1960s British television comedy satire "Till Death Us Do Part". The character of Alf Garnett (played by Warren Mitchell) was created to poke fun at the kind of narrow-minded, racist, little Englander that Garnett represented. Instead, his character became a sort of anti-hero to people who actually agreed with his views. (The same situation occurred with Archie Bunker in American TV show "All in the Family", a character derived directly from Garnett.)

The Australian satirical television comedy show "The Chaser's War on Everything" has suffered repeated attacks based on various perceived interpretations of the "target" of its attacks. The "Make a Realistic Wish Foundation" sketch (June 2009), which attacked in classical satiric fashion the heartlessness of people who are reluctant to donate to charities, was widely interpreted as an attack on the Make a Wish Foundation, or even the terminally ill children helped by that organisation. Prime Minister of the time Kevin Rudd stated that The Chaser team "should hang their heads in shame". He went on to say that "I didn't see that but it's been described to me. ...But having a go at kids with a terminal illness is really beyond the pale, absolutely beyond the pale." Television station management suspended the show for two weeks and reduced the third season to eight episodes.

The romantic prejudice against satire is the belief spread by the romantic movement that satire is something unworthy of serious attention; this prejudice has held considerable influence to this day. Such prejudice extends to humour and everything that arouses laughter, which are often underestimated as frivolous and unworthy of serious study. For instance, humor is generally neglected as a topic of anthropological research and teaching.

Because satire criticises in an ironic, essentially indirect way, it frequently escapes censorship in a way more direct criticism might not. Periodically, however, it runs into serious opposition, and people in power who perceive themselves as attacked attempt to censor it or prosecute its practitioners. In a classic example, Aristophanes was persecuted by the demagogue Cleon.

In 1599, the Archbishop of Canterbury John Whitgift and the Bishop of London Richard Bancroft, whose offices had the function of licensing books for publication in England, issued a decree banning verse satire. The decree, now known as the Bishops' Ban of 1599, ordered the burning of certain volumes of satire by John Marston, Thomas Middleton, Joseph Hall, and others; it also required histories and plays to be specially approved by a member of the Queen's Privy Council, and it prohibited the future printing of satire in verse.

The motives for the ban are obscure, particularly since some of the books banned had been licensed by the same authorities less than a year earlier. Various scholars have argued that the target was obscenity, libel, or sedition. It seems likely that lingering anxiety about the Martin Marprelate controversy, in which the bishops themselves had employed satirists, played a role; both Thomas Nashe and Gabriel Harvey, two of the key figures in that controversy, suffered a complete ban on all their works. In the event, though, the ban was little enforced, even by the licensing authority itself.

In 2005, the Jyllands-Posten Muhammad cartoons controversy caused global protests by offended Muslims and violent attacks with many fatalities in the Near East. It was not the first case of Muslim protests against criticism in the form of satire, but the Western world was surprised by the hostility of the reaction: Any country's flag in which a newspaper chose to publish the parodies was being burnt in a Near East country, then embassies were attacked, killing 139 people in mainly four countries; politicians throughout Europe agreed that satire was an aspect of the freedom of speech, and therefore to be a protected means of dialogue. Iran threatened to start an International Holocaust Cartoon Competition, which was immediately responded to by Jews with an Israeli Anti-Semitic Cartoons Contest.

In 2006 British comedian Sacha Baron Cohen released "Borat: Cultural Learnings of America for Make Benefit Glorious Nation of Kazakhstan", a "mockumentary" that satirized everyone, from high society to frat boys. The film was criticized by many. Although Baron Cohen is Jewish, some complained that it was antisemitic, and the government of Kazakhstan boycotted the film. The film itself had been a reaction to a longer quarrel between the government and the comedian.

In 2008, popular South African cartoonist and satirist Jonathan Shapiro (who is published under the pen name Zapiro) came under fire for depicting then-president of the ANC Jacob Zuma in the act of undressing in preparation for the implied rape of 'Lady Justice' which is held down by Zuma loyalists. The cartoon was drawn in response to Zuma's efforts to duck corruption charges, and the controversy was heightened by the fact that Zuma was himself acquitted of rape in May 2006. In February 2009, the South African Broadcasting Corporation, viewed by some opposition parties as the mouthpiece of the governing ANC, shelved a satirical TV show created by Shapiro, and in May 2009 the broadcaster pulled a documentary about political satire (featuring Shapiro among others) for the second time, hours before scheduled broadcast. Apartheid South Africa also had a long history of censorship.

On December 29, 2009, Samsung sued Mike Breen, and the "Korea Times" for $1 million, claiming criminal defamation over a satirical column published on Christmas Day, 2009.

On April 29, 2015, the UK Independence Party (UKIP) requested Kent Police investigate the BBC, claiming that comments made about Party leader Nigel Farage by a panelist on the comedy show "Have I Got News For You" might hinder his chances of success in the general election (which would take place a week later), and claimed the BBC breached the Representation of the People Act. Kent Police rebuffed the request to open an investigation, and the BBC released a statement, "Britain has a proud tradition of satire, and everyone knows that the contributors on "Have I Got News for You" regularly make jokes at the expense of politicians of all parties."

Satire is occasionally prophetic: the jokes precede actual events. Among the eminent examples are:

In June 2019, Punocracy, "Nigeria"'s foremost satire platform organised a nationwide writing competition for youth in the country with the objective to make satire a widely accepted and understood tool of socio-political commentary. Some of the entries addressed issues like "gender violence", "political corruption", religious hypocrisy, "internet fraud", educational decay and so on. The group also declared November 9 as World Satire Day with the idea of "trying to fight against the ills in the society not by ammunition but by humour, sarcasm etcetera".








</doc>
<doc id="26792" url="https://en.wikipedia.org/wiki?curid=26792" title="Samuel Butler (poet)">
Samuel Butler (poet)

Samuel Butler (baptized 14 February 1613 – 25 September 1680) was a poet and satirist. He is remembered now chiefly for a long satirical poem titled "Hudibras".

Samuel Butler was born in Strensham, Worcestershire, and was the son of a farmer and churchwarden, also named Samuel. His date of birth is unknown, but there is documentary evidence for the date of his baptism of 14 February. The date of Butler's baptism is given as 8 February by Treadway Russell Nash in his 1793 edition of "Hudibras". Nash had already mentioned Butler in his "Collections for a History of Worcestershire" (1781), and perhaps because the latter date seemed to be a revised account, it has been repeated by many writers and editors. However, The parish register of Strensham records under the year 1612: "Item was christened Samuell Butler the sonne of Samuell Butler the xiiijth of February anno ut supra". Lady Day, 25 March, was New Year's Day in England at the time, so the year of his baptism was 1613 according to the change of the start of the year with the Calendar Act of 1750 (see Old Style and New Style dates). Nash also claims in his 1793 edition of "Hudibras" that Butler's father entered his son's baptism into the register, an error that was also repeated in later publications; however, the entry was clearly written by a different hand.

Butler was brought up in the household of Sir William Russell of Strensham and became his clerk. "When just a Boy he would make observations and reflections on every Thing one sayd or did, and censure it to be either well or ill. He was never at the University for the reason alleged." He was educated at the King's School, Worcester, under Henry Bright whose teaching is recorded favourably by Thomas Fuller, a contemporary writer, in his "Worthies of England". In early youth he was a servant to the Countess of Kent. Through Lady Kent he met her steward, the jurist John Selden who influenced his later writings. He also tried his hand at painting but was reportedly not very good at it; one of his editors reporting that "his pictures served to stop windows and save the tax" (on window glass). Conversely, John Aubrey who knew Butler quite well enough to be one of his pallbearers, wrote that "He was thinking once to have made painting his Profession. His love to and skill in painting made a great friendship between him and Mr. Samuel Cowper (The Prince of Limners of this Age." He studied law but did not practice. 

After the Restoration he became secretary, or steward, to Richard Vaughan, 2nd Earl of Carbery, Lord President of Wales, which entailed living at least a year in Ludlow, Shropshire, until January 1662 while he was paying craftsmen working on repairing the castle there. In late 1662 the first part of "Hudibras", which he began writing when lodging at Holborn, London, in 1658 and continued to work on while in Ludlow, was published, and the other two in 1664 and 1678 respectively. One early purchaser of the first two parts was Samuel Pepys. While the diarist acknowledged that the book was the "greatest fashion" he could not see why it was found to be so witty.

Despite the popularity of "Hudibras", Butler was not offered a place at Court. "Satyrical Witts disoblige whom they converse with;and consequently make to themselves many Enemies and few Friends;and this was his manner and case." However, Butler is thought to have been in the employment of the Duke of Buckingham in the summer of 1670, and accompanied him on a diplomatic mission to France. Butler also received financial support in the form of a grant from King Charles II.

During the latter part of his life, Butler lived in a house in the now partially demolished Rose Street, to the west of Covent Garden.

Butler died of consumption on 25 September 1680, and was buried on 27 September in the Church-yard of St. Paul's, Covent Garden;in the north part next to the church at the east end. "His feet touch the wall. His grave 2 yards distant from the Pillaster of the Dore (by his desire) 6 feet deep"at the expense of a Mr. Longueville, although he was not in debt when he died. Aubrey in "Brief Lives" describes his grave as "being in the north part next to the church at the east end ... 2 yards distant from the pillaster of the dore". Also, a monument to him was placed in Westminster Abbey in 1732 by a printer, John Barber, and the Lord Mayor of London. There is also a memorial plaque to him in the small village church of Strensham, Worcestershire, near the town of Upton upon Severn, his birthplace.

"Hudibras" is directed against religious sectarianism. The poem was very popular in its time, and several of its phrases have passed into the dictionary. It was sufficiently popular to spawn imitators. "Hudibras" takes some of its characterization from "Don Quixote" but, unlike that work, it has many more references to personalities and events of the day. Butler was also influenced by satirists such as John Skelton and Paul Scarron's "Virgile travesti"; a satire on classical literature, particularly Virgil.

"Hudibras" was reprinted many times in the centuries following Butler's death. Two of the more noteworthy editions are those edited by Zachery Grey (1744) and Treadway Russell Nash (1793). The standard edition of the work was edited by John Wilders (1967).

Most of his other writings never saw print until they were collected and published by Robert Thyer in 1759. Butler wrote many short biographies, epigrams and verses the earliest surviving from 1644. Of his verses, the best known is "The Elephant on the Moon", about a mouse trapped in a telescope, a satire on Sir Paul Neale of the Royal Society. Butler's taste for the mock heroic is shown by another early poem "Cynarctomachy", or Battle between Bear and Dogs, which is both a homage to and a parody of a Greek poem ascribed to Homer, "Batrachomyomachia". 
He wrote the poem "Upon Philip Nye's Thanksgiving Beard" about the Puritan Philip Nye and later also mentioned him in "Hudibras".

His supposed lack of money later in life is strange as he had numerous unpublished works which could have offered him income including a set of Theophrastan character sketches which were not printed until 1759. Many other works are dubiously attributed to him.


Attribution:



</doc>
<doc id="26794" url="https://en.wikipedia.org/wiki?curid=26794" title="List of science fiction and fantasy artists">
List of science fiction and fantasy artists

This is a list of science fiction and fantasy artists, notable and well-known 20th- and 21st-century artists who have created book covers or interior illustrations for books, or who have had their own books or comic books of fantastic art with science fiction or fantasy themes published. Artists known exclusively for their work in comic books are not included. Many of the artists are known for their work in both the fantasy and sf fields. Artists who have won the Hugo Award, the World Fantasy Award, or the Chesley Award are noted, as are inductees into the Science Fiction Hall of Fame.

























 


</doc>
<doc id="26795" url="https://en.wikipedia.org/wiki?curid=26795" title="Saxophone">
Saxophone

The saxophone (referred to colloquially as the sax) is a family of woodwind instruments usually made of brass and played with a single-reed mouthpiece. Although most saxophones are made from brass, they are categorized as woodwind instruments, because sound is produced by an oscillating reed (traditionally made out of woody cane) rather than lips vibrating in a mouthpiece cup as with the brass instrument family. As with the other woodwind instruments, the pitch of the note being played is controlled by covering holes in the body tube to control the resonant frequency of the air column by changing the effective length of the tube.
The player covers or uncovers the holes by pressing keys.

The saxophone is used in classical music (such as concert bands, chamber music, solo repertoire, and, occasionally, orchestras), military bands, marching bands, jazz (such as big bands and jazz combos), and contemporary music. The saxophone is also used as a solo and melody instrument or as a member of a horn section in some styles of rock and roll and popular music. Saxophone players are called "saxophonists".

Since the first saxophone was invented by the Belgian instrument maker Adolphe Sax in the early 1840s, saxophones have been produced in a variety of series distinguished by transpositions within instrument sets and tuning standard. Sax patented the saxophone on 28 June 1846, in two groups of seven instruments each. Each series consisted of instruments ranked by pitch, in alternating transposition. The series pitched in B and E soon became dominant and most saxophones encountered today are from this series. Instruments from the series pitched in C and F never gained a foothold and constituted only a small percentage of instruments made by Sax. "High Pitch" (also marked "H" or "HP") saxophones tuned sharper than the (concert) A = 440 Hz standard were produced into the early twentieth century for sonic qualities suited for outdoor uses, but are not playable to modern tuning and are considered obsolete. "Low Pitch" (also marked "L" or "LP") saxophones are equivalent in tuning to modern instruments. C soprano and C melody saxophones were produced for the casual market as parlor instruments during the early twentieth century. Saxophones in F were introduced during the late 1920s but never gained acceptance. The modern saxophone family consists entirely of instruments in the B – E series, historical and experimental instruments notwithstanding. The saxophones with widest use and availability are the soprano, alto, tenor, and baritone saxophones.

In the "keyed" (below overtone-produced "altissimo") ranges of the various saxophones, the pitch is controlled by "keys" with shallow "cups" in which are fastened leather "pads" that seal "toneholes", controlling the resonant length, and thereby frequency, of the air column within the "bore". Small holes called "vents", located between the toneholes and the mouthpiece, are opened by an "octave key" to raise the pitch one octave by eliminating the fundamental frequency, leaving the first harmonic as the frequency defining the pitch. Most modern saxophones are keyed to produce a low B (relative to the instrument's transposition) with all keys closed; modern baritone saxophones commonly play a low A and altos keyed to low A have been produced in the past. The highest keyed note has traditionally been F two and a half octaves above low B, while the keyed range is extended to F on most recent performance-class instruments. A high G key is most common on modern soprano saxophones. Notes above F are considered part of the altissimo register of any saxophone, and can be produced using advanced embouchure techniques and fingering combinations. Keywork facilitating altissimo playing is a feature of modern saxophones. Modern saxophone players have extended the range to over four octaves on tenor and alto. Music for most saxophones is usually notated using treble clef.

Because all saxophones use the same key arrangement and fingering to produce a given notated pitch, it is not difficult for a competent player to switch among the various sizes when the music has been suitably transposed, and many players do so. Since the baritone and alto are pitched in E, players can read concert pitch music notated in the bass clef by reading it as if it were treble clef and adding three sharps to the key signature. This process, referred to as "clef substitution", makes it possible for the E instruments to play from parts written for baritone horn, bassoon, euphonium, string bass, trombone, or tuba. This can be useful if a band or orchestra lacks one of those instruments.

The "straight" soprano and sopranino saxophones consist of a straight conical tube with a flared "bell" at the end opposite the mouthpiece. Alto and larger saxophones include a detachable curved "neck" above the highest tone hole, directing the mouthpiece to the player's mouth and, with rare exceptions, a U-shaped "bow" that directs the bore upward and a curve in the throat of the bell directing it forward. The set of curves near the bell has become a distinctive feature of the saxophone family, to the extent that soprano and even sopranino saxes are sometimes made in the curved style. The baritone, bass, and contrabass saxophones accommodate the length of the bore with extra bows and right-angle bends between the main body and the mouthpiece.

The left hand operates keys from the upper part of the body tube while the right hand operates keys from the lower part. The right thumb sits under a "thumb hook" and left thumb is placed on a "thumb rest" to stabilize and balance the saxophone or operates the octave key. The weight of most saxophones is shared by the right thumb and a neckstrap attached to a "strap ring" on the rear of the body of the instrument. With the smaller instruments, relatively more of the weight is supported by the thumb. 

Keys consist of the cups, levers, and pivots that control the position of the pads over the toneholes. At rest, some keys are open and some are closed, held in position by springs that are overridden by finger or hand ("palm" keys) pressure. The keys are activated by pressure on "key touches", either directly on the pad cup or connected to it with levers, either directly or with joints called "linkages." The levers between the key cups and the pivots are called "key arms".

The fingering for the saxophone is a combination of that of the oboe with the Boehm system and is similar to the flute or upper register of the clarinet. The "stack" keys are operated by the first, second, and third fingers on each hand with slightly concave button-style key touches ("key buttons") operating with the same motion as the pad cups that they control. The stack keys are linked to higher stack keys with "regulation bar" and "bridge arm" linkages. Key buttons are advantageous for operating keys with direct downward finger pressure but provide disadvantages operating keys with other finger and hand motions, hence, their use on keys operated with such motions has diminished with the evolution of saxophone designs.

Palm keys and the "front F" key operated by the left hand, and the "high F", "high F" and "high G" keys operated by the right hand, control the upper end of the keyed range and are used to vent altissimo notes. "Chromatic" keys operated by the right hand provide alternate fingerings for F, B, and C within the stack range. The fourth fingers of the right and left hands open keys to raise pitch by a semitone as well as close keys towards the lower range of the instrument, with the lowest pitch "bell keys" operated by the left hand. The keys operated by the fourth fingers are referred to as "table" keys. Instruments that play to low A have a left thumb key for that note.

On saxophones produced since the early 1920s the G key operated from the left hand table is closed by closing keys on the lower stack regardless of pressure on the G actuating mechanism ("F-linked", or "stack-linked", G mechanism). That feature vastly increases the speed and playability of certain intervals to the point that saxophones with "direct G" action, in which the key stays open when the lower stack keys are depressed, are considered obsolete. Modern left hand tables also "articulate" the G key with the low C, B, and B keys to open it when any of those keys are depressed and the right hand stack keys are not. That also provides significant advantages for playing certain intervals near the lower range of the instrument. Some players willingly forego the benefits of the articulated G to play vintage instruments, but a front F key and a stack-linked G key are regarded as critical features by serious players.

From the earliest days of the saxophone the body and key cups have been made from sheet brass stock, owing to its workability in forming complex shapes. Mechanical keywork is assembled from components either hand-tooled or machined from other forms of brass stock. King introduced saxophones with necks and bells of sterling silver during the 1930s and continued that "silversonic" scheme into the early 1960s. Yanagisawa revived the scheme during the 1980s and later introduced entire instruments of sterling silver. 
Keilwerth and P. Mauriat have used nickel silver, a copper-nickel-zinc alloy more commonly used for flutes, for the bodies of some saxophone models. 
For visual and tonal effect, higher copper variants of brass are sometimes substituted for the more common "yellow brass" and "cartridge brass." Yanagisawa made its 902 and 992 series saxophones with the high copper alloy phosphor bronze to achieve a darker, more "vintage" tone than the brass 901 and 991 models. Other saxophones made of high copper alloys are sold under various brands.

Other materials are used for some mechanical parts and keywork. Since 1920, most saxophones have replaceable key buttons operating the stack keys, usually made from either plastic or mother of pearl. Some saxophones are made with abalone, stone, or wood key buttons. On some premium models, the key button material is used to form the convex key touches for other keys. The rods and screw pins that the keywork's hinges pivot on, and the needle and leaf springs that hold keys in their rest position, are usually made of blued or stainless steel. Mechanical buffers of felt, cork, leather, and various synthetic materials are used to reduce friction, to minimize mechanical noise from movement of keys, and to optimize the action of the keywork for positive pad sealing, intonation, speed, and "feel." Nickel silver is sometimes used for hinges for its advantages of mechanical durability, although the most common material for such applications has remained brass. Saxophones with high copper bodies still have brass keywork owing to its more suitable mechanical properties relative to those alloys.

Before final assembly, manufacturers usually apply a finish to the surface of the horn. The most common finish is a thin coating of clear or colored acrylic lacquer. The lacquer serves to protect the brass from oxidation and maintains its shiny appearance. Silver or gold plating are offered as premium options on some models. Some silver plated saxophones are also lacquered. Plating saxophones with gold is an expensive process because an underplating of silver is required for the gold to adhere to. Nickel plating has been used on the bodies of early budget model saxophones and is commonly used on keywork when a more durable finish than lacquer is desired, mostly with student model saxophones. Chemical surface treatment of the base metal has come into use as an alternative to the lacquer and plating finishes in recent years. Some saxophonists, retailers, and repair technicians argue that the type of lacquer or plating (or absence of lacquer) may be a factor affecting the instrument's tone quality.

The saxophone uses a single-reed mouthpiece similar to that of the clarinet. Each size of saxophone (alto, tenor, etc.) uses a different size of reed and mouthpiece.

Most saxophonists use reeds made from "Arundo donax" cane, but since middle of the twentieth century some have also been made of fiberglass and other composite materials. Saxophone reeds are proportioned slightly differently from clarinet reeds, being wider for the same length. Reeds are commercially available in a vast array of brands, styles, and strengths. Saxophonists experiment with reeds of different strength (hardnesses) and material to find which strength and cut suits their mouthpiece, embouchure, physiology, and playing style.

Mouthpiece design has a profound impact on tone. Different mouthpiece design characteristics and features tend to be favored for different styles. Early mouthpieces were designed to produce a "warm" and "round" sound for classical playing. Among classical mouthpieces, those with a concave ("excavated") "chamber" are more true to Adolphe Sax's original design; these provide a softer or less piercing tone favored by the Raschèr school of classical playing. Saxophonists who follow the French school of classical playing, influenced by Marcel Mule, generally use mouthpieces with smaller chambers for a somewhat "brighter" sound with relatively more upper harmonics. The use of the saxophone in dance orchestras and jazz ensembles from the 1920s onward placed emphasis on "dynamic range" and projection, leading to innovation in mouthpiece chamber shapes and "tip" designs, as well as metal construction. At the opposite extreme from the classical mouthpieces are those with a small chamber and a low clearance above the reed between the tip and the chamber, called high "baffle". These produce a bright sound with maximum projection, suitable for having a sound stand out among amplified instruments and are commonly used in modern pop and smooth jazz.

Mouthpieces come in a wide variety of materials, including vulcanized rubber (sometimes called hard rubber or ebonite), plastic, and metals such as bronze or surgical steel. Less common materials that have been used include wood, glass, crystal, porcelain, and bone. Recently, Delrin has been added to the stock of mouthpiece materials.

The effect of mouthpiece materials on tone of the saxophone has been the subject of much debate. According to Larry Teal, the mouthpiece material has little, if any, effect on the sound, and the physical dimensions give a mouthpiece its tone color. There are examples of "dark" sounding metal pieces and "bright" sounding hard rubber pieces. The lower rigidity of hard rubber relative to metal restricts some design characteristics affecting tone and response more than with metal. The extra bulk required near the tip with hard rubber affects mouth position and airflow characteristics. Recently, increased mass of the mouthpiece over the "shank", which fits over the neck cork, has become a design feature to enhance the integrity of the harmonic series by stabilizing the mouthpiece/neck connection. Shank weights (large rings of brass over the shank) are used with some Delrin mouthpieces to increase "resonance and projection." Other "hybrid" designs with a hard rubber body and a substantial metal shank have a similar mass distribution, although its contribution to sound characteristics is not highlighted in product descriptions.

The saxophone was designed around 1840 by Adolphe Sax, a Belgian instrument maker, flautist, and clarinetist. Born in Dinant and originally based in Brussels, he moved to Paris in 1842 to establish his musical instrument business. Before working on the saxophone, he made several improvements to the bass clarinet by improving its keywork and acoustics and extending its lower range. Sax was also a maker of the ophicleide, a large conical brass instrument in the bass register with keys similar to a woodwind instrument. His experience with these two instruments allowed him to develop the skills and technologies needed to make the first saxophones.

As an outgrowth of his work improving the bass clarinet, Sax began developing an instrument with the projection of a brass instrument and the agility of a woodwind. He wanted it to overblow at the octave, unlike the clarinet, which rises in pitch by a twelfth when overblown. An instrument that overblows at the octave has identical fingering for both registers.

Sax created an instrument with a single-reed mouthpiece and conical brass body. Having constructed saxophones in several sizes in the early 1840s, Sax applied for, and received, a 15-year patent for the instrument on 28 June 1846. The patent encompassed 14 versions of the fundamental design, split into two categories of seven instruments each, and ranging from sopranino to contrabass. A limited number of instruments in the series pitched in F and C were produced by Sax, but the series pitched in E and B quickly became the standard. All the instruments were given an initial written range from the B below the treble staff to the E one half-step below the third ledger line above staff, giving each saxophone a range of two and a half octaves. Sax's patent expired in 1866. Thereafter, numerous other instrument manufacturers implemented their own improvements to the design and keywork.

Sax's original keywork, which was based on the "Triebert system 3" oboe for the left hand and the "Boehm" clarinet for the right, was simplistic and made certain legato passages and wide intervals extremely difficult to finger; that system would later evolve with extra keys, linkage mechanisms, and alternate fingerings to make some intervals less difficult.

Early in the development of the saxophone the upper keyed range was extended to E, then F above the staff; 1880s era sheet music for saxophone was written for the range of low B to F. In 1887 the Buffet-Crampon company obtained a patent for extending the bell and adding an extra key to extend the range downwards by one semitone to B. This extension is currently standard in most modern designs, with the notable exception of baritone saxophones further extended and keyed to low A. The upper range to F would remain the standard for nearly a century until the altissimo F key became common on modern saxophones.

In the 1840s and 1850s, Sax's invention gained use in small classical ensembles (both all-saxophone and mixed), as a solo instrument, and in French and British military bands. Saxophone method books were published and saxophone instruction was offered at conservatories in France, Switzerland, Belgium, Spain, and Italy. By 1856 the French "Garde Republicaine" band included eight saxophones, making it the large ensemble that featured the instrument most prominently. The saxophone was used experimentally in orchestral scores, but never came into widespread use as an orchestral instrument. In 1853-54 the orchestra of Louis Antoine Jullien featured a soprano saxophone on a concert tour of the United States.

After an early period of interest and support from classical music communities in Europe, their interest in the instrument waned in the late nineteenth century. Saxophone teaching at the Paris Conservatory was suspended from 1870 to 1900 and classical saxophone repertoire stagnated during that period. But it was during this same period that the saxophone began to be promoted in the United States, largely through the efforts of Patrick Gilmore, leader of the "22nd Regiment band", and Edward A. Lefebre, a Dutch emigre and saxophonist with family business associations with Sax. Lefebre settled in New York in early 1872 after he arrived as a clarinetist with a British opera company. Gilmore organized the World Peace Jubilee and International Music Festival taking place in Boston that summer. The Garde Republicaine band performed and Lefebre was a clarinetist with the Great Festival Orchestra for that event. In the fall of 1873 Gilmore was reorganizing the 22nd Regiment band under the influence of the Garde Republicaine band and recruited Lefebre, who had established a reputation in New York as a saxophonist over the previous year. Gilmore's band soon featured a soprano-alto-tenor-baritone saxophone section, which also performed as a quartet. The Gilmore-Lefebre association lasted until Gilmore's death in 1892, during which time Lefebre also performed in smaller ensembles of various sizes and instrumentation, and worked with composers to increase light classical and popular repertoire for saxophone.

Lefebre's later promotional efforts were extremely significant in broadening adoption of the saxophone. Starting towards the end of the 1880s he consulted with the brass instrument manufacturer C.G. Conn to develop and start production of improved saxophones to replace the costly, scantly available, and mechanically unreliable European instruments in the American market. The early 1890s saw regular production of saxophones commence at Conn and its offshoot Buescher Manufacturing Company, which dramatically increased availability of saxophones in the US. Lefebre worked with the music publisher Carl Fischer to distribute his transcriptions, arrangements, and original works for saxophone, and worked with the Conn Conservatory to further saxophone pedagogy in the US. Lefebre's associations with Conn and Fischer lasted into the first decade of the twentieth century and Fischer continued to publish new arrangements of Lefebre's works posthumously.

While the saxophone remained marginal and regarded mainly as a novelty instrument in the classical music world, many new musical niches were established for it during the early decades of the twentieth century. Its early use in Vaudeville and ragtime bands around the turn of the century laid the groundwork for its use in dance orchestras and eventually jazz. As the market for saxophones grew in the US, the manufacturing industry grew; the Martin Band Instrument Company started producing saxophones between 1905 and 1912, and the Cleveland Band Instrument Company started producing saxophones under contract to the H. N. White Company in 1916. The saxophone was promoted for the casual market with introduction of the C-soprano and C-melody (between alto and tenor) saxophones to play in key with pianos from the same sheet music. Production of such instruments stopped during the Great Depression. During the 1920s the saxophone came into use as a jazz instrument, fostered by the influences of the Fletcher Henderson Orchestra and the Duke Ellington Orchestra. Starting in the late 1920s and early 1930s, the modern era of classical saxophone was launched largely through the efforts of Marcel Mule and Sigurd Raschèr, and classical repertoire for the instrument expanded rapidly.

The use of the saxophone for more dynamic and more technically demanding styles of playing added incentive for improvements in keywork and acoustic design. Early saxophones had two separate octave keys operated by the left thumb to control the two octave vents required on alto and larger saxophones. A substantial advance in keywork around the turn of the century was the development of mechanisms by which the left thumb operates the two octave vents with a single octave key. Ergonomic design of keywork evolved rapidly during the 1920s and 1930s. The front F mechanism supporting alternate fingerings for high E and F, and stack-linked G key action, became standard during the 1920s, followed by improvements to the left hand table key mechanisms controlling the G and bell keys. New bore designs during the 1920s and 1930s resulted from the quest for improved intonation, dynamic response, and tonal qualities. The 1920s were also the era of design experiments such as the Buescher straight altos and tenors, the King "Saxello" soprano, the C.G. Conn "mezzo-soprano" saxophone keyed in F, and the "Conn-O-Sax" saxophone – English horn hybrid.

The modern layout of the saxophone emerged during the 1930s and 1940s, first with right-side bell keys introduced by C. G. Conn on baritones, then by King on altos and tenors. The mechanics of the left hand table were revolutionized by Selmer with their Balanced Action instruments in 1936, capitalizing on the right-side bell key layout. In 1948 Selmer introduced their Super Action saxophones with offset left and right hand stack keys. Between 30 and 40 years after Selmer devised their final layout it had been adopted for virtually every saxophone being produced, from student to professional models.

The high F key was also first introduced as an option on the Balanced Action model, although it took several decades for it to gain acceptance because of perceived deleterious effects on intonation in its early implementations.

The saxophone first gained popularity in military bands. Although the instrument was initially ignored in Germany, French and Belgian military bands were quick to include the instrument in their ensembles. Most French and Belgian military bands incorporate at least a quartet of saxophones, comprising an E baritone, B tenor, E alto and B soprano. These four instruments have proved the most popular of all of Sax's creations, with the E contrabass and B bass usually considered impractically large and the E sopranino insufficiently powerful. British military bands tend to include at minimum two saxophonists, on the alto and tenor.

The saxophone was introduced into the concert band, which usually calls for an E alto saxophone, a B tenor saxophone, and an E baritone saxophone. A concert band may include two altos, one tenor, and one baritone. A B soprano saxophone is also used, in which case it is played by the first alto saxophonist. A bass saxophone in B is used in some concert band music (especially music by Percy Grainger).

Saxophones are used in chamber music, such as saxophone quartets and other chamber combinations of instruments. The classical saxophone quartet consists of a B soprano saxophone, E alto saxophone, B tenor saxophone, and E baritone saxophone (SATB). On occasion, the soprano is replaced with a second alto sax (AATB); a few professional saxophone quartets have featured non-standard instrumentation, such as James Fei's Alto Quartet (four altos).

There is a repertoire of classical compositions and arrangements for the SATB instrumentation dating back to the nineteenth century, particularly by French composers who knew Sax. However, the largest body of chamber works for saxophone are from the modern era of classical saxophone initiated by Marcel Mule in 1928. Sigurd Raschèr followed as a soloist in orchestral works, starting in 1931, and also figured prominently in development of modern classical saxophone repertoire. The Mule quartet is often considered the prototype for quartets due the level of virtuosity demonstrated by its members and its central role in the development of modern quartet repertoire. However, organized quartets existed before Mule's ensemble, the prime example being the quartet headed by Edward A. Lefebre (1834–1911), which was a subset of Patrick Gilmore's 22nd Regiment band between 1873 and 1893.

In the 20th and 21st centuries, the saxophone found increased popularity in symphony orchestras. The instrument has also been used in genres such as opera and choral music. Many musical theatre scores include parts for saxophone, sometimes doubling another woodwind or brass instrument.

Coincident with the more widespread availability of saxophones in the US around the turn of the century was the rise of ragtime music. The bands featuring the syncopated Latin- and African-American rhythmic influences of ragtime were an exciting new feature of the American cultural landscape and provided the groundwork for new styles of dancing. Two of the best known ragtime-playing brass bands with saxophones were those led by W. C. Handy and James R. Europe. Europe's 369th Infantry Regiment Band popularized ragtime in France during its 1918 tour. The rise of dance bands into the 1920s followed from the popularity of ragtime. The saxophone was also used in Vaudeville entertainment during the same period. Ragtime, Vaudeville, and dance bands introduced much of the American public to the saxophone. Rudy Wiedoeft became the best known individual saxophone stylist and virtuoso during this period leading into the "saxophone craze" of the 1920s. Following it, the saxophone became featured in music as diverse as the "sweet" music of Paul Whiteman and Guy Lombardo, jazz, swing, and large stage show bands.

The rise of the saxophone as a jazz instrument followed its widespread adoption in dance bands during the early 1920s. The Fletcher Henderson Orchestra, formed in 1923, featured arrangements to back up improvisation, bringing the first elements of jazz to the large dance band format. Following the innovations of the Fletcher Henderson Orchestra, the Duke Ellington Orchestra and Jean Goldkette's Victor Recording Orchestra featured jazz solos with saxophones and other instruments. The association of dance bands with jazz would reach its peak with the swing music of the 1930s. The large show band format, influenced by the 1930s swing bands, would be used as backing for popular vocalists and stage shows in the post World War II era, and provided a foundation for big band jazz. Show bands with saxophone sections became a staple of television talk shows (such as the Tonight Show that featured bands led by Doc Severinsen and Branford Marsalis) and Las Vegas stage shows. The swing era fostered the later saxophone styles that permeated bebop and rhythm and blues in the early postwar era.

Coleman Hawkins established the tenor saxophone as a jazz solo instrument during his stint with Fletcher Henderson from 1923 to 1934. Hawkins' arpeggiated, rich-toned, vibrato-laden style was the main influence on swing era tenor players before Lester Young, and his influence continued with other big-toned tenor players into the era of modern jazz. Among the tenor players directly influenced by him were Chu Berry, Charlie Barnet, Tex Beneke, Ben Webster, Vido Musso, Herschel Evans, Buddy Tate, and Don Byas. Hawkins' bandmate Benny Carter and Duke Ellington's alto saxophonist Johnny Hodges became influential on swing era alto styles, while Harry Carney brought the baritone saxophone to prominence with the Duke Ellington Orchestra. The New Orleans player Sidney Bechet gained recognition for playing the soprano saxophone during the 1920s, but the instrument did not come into wide use until the modern era of jazz.

As Chicago style jazz evolved from New Orleans jazz in the 1920s, one of its defining features was the addition of saxophones to the ensemble. The small Chicago ensembles offered more improvisational freedom than did the New Orleans or large band formats, fostering the innovations of saxophonists Jimmy Dorsey (alto), Frankie Trumbauer (c-melody), Bud Freeman (tenor) and Stump Evans (baritone). Dorsey and Trumbauer became important influences on tenor saxophonist Lester Young.

Lester Young's approach on tenor saxophone differed from Hawkins', emphasizing more melodic "linear" playing that wove in and out of the chordal structure and longer phrases that differed from those suggested by the tune. He used vibrato less, fitting it to the passage he was playing. His tone was smoother and darker than that of his 1930s contemporaries. Young's playing was a major influence on the modern jazz saxophonists Al Cohn, Stan Getz, Zoot Sims, Dexter Gordon, Wardell Gray, Lee Konitz, Warne Marsh, Charlie Parker, and Art Pepper.

The influence of Lester Young with the Count Basie Orchestra in the late 1930s and the popularity of Hawkins' 1939 recording of "Body and Soul" marked the saxophone as an influence on jazz equal to the trumpet, which had been the defining instrument of jazz since its beginnings in New Orleans. But the greatest influence of the saxophone on jazz was to occur a few years later when alto saxophonist Charlie Parker became an icon of the bebop revolution that influenced generations of jazz musicians. The small-group format of bebop and post-bebop jazz ensembles gained ascendancy in the 1940s as musicians used the harmonic and melodic freedom pioneered by Parker, Dizzy Gillespie, Thelonious Monk, and Bud Powell in extended jazz solos.

During the 1950s, prominent alto players included Sonny Stitt, Cannonball Adderley, Jackie McLean, Lou Donaldson, Sonny Criss and Paul Desmond, while prominent tenor players included Lester Young, Coleman Hawkins, Dexter Gordon, John Coltrane, Sonny Rollins, Stan Getz, Zoot Sims, Lucky Thompson, Eddie "Lockjaw" Davis, and Paul Gonsalves. Serge Chaloff, Gerry Mulligan, Pepper Adams and Leo Parker brought the baritone saxophone to prominence as a solo instrument. Steve Lacy renewed attention to the soprano saxophone in the context of modern jazz and John Coltrane boosted the instrument's popularity during the 1960s. Smooth jazz musician Kenny G also uses the soprano sax as his principal instrument.

Saxophonists such as John Coltrane, Ornette Coleman, Sam Rivers, and Pharoah Sanders defined the forefront of creative exploration with the avant-garde movement of the 1960s. The new realms offered with Modal, harmolodic, and free jazz were explored with every device that saxophonists could conceive of. Sheets of sound, tonal exploration, upper harmonics, and multiphonics were hallmarks of the creative possibilities that saxophones offered. One lasting influence of the avant-garde movement is the exploration of non-Western ethnic sounds on the saxophone, for example, the African-influenced sounds used by Sanders and the Indian-influenced sounds used by Coltrane. The devices of the avant-garde movement have continued to be influential in music that challenges the boundaries between avant-garde and other categories of jazz, such as that of alto saxophonists Steve Coleman and Greg Osby.

Some ensembles such as the World Saxophone Quartet use the soprano-alto-tenor-baritone (SATB) format of the classical saxophone quartet for jazz. In the 1990s, World Saxophone Quartet founder Hamiet Bluiett formed the quartet Baritone Nation (four baritones).

The "jump swing" bands of the 1940s gave rise to rhythm and blues, featuring horn sections and exuberant, strong-toned, heavily rhythmic styles of saxophone playing with a melodic sense based on blues tonalities. Illinois Jacquet, Sam Butera, Arnett Cobb, and Jimmy Forrest were major influences on R&B tenor styles and Louis Jordan, Eddie "Cleanhead" Vinson, Earl Bostic, and Bull Moose Jackson were major influences on alto. The R&B saxophone players influenced later genres including rock and roll, ska, soul, and funk. Horn section work continued with Johnny Otis and Ray Charles featuring horn sections and the Memphis Horns, the Phenix Horns, and Tower of Power achieving distinction for their section playing. Horn sections were added to the Chicago and West Coast blues bands of Lowell Fulson, T-Bone Walker, B.B. King, and Guitar Slim. Rock and soul fusion bands such as Chicago, The Electric Flag, and Blood, Sweat, and Tears featured horn sections. Bobby Keys and Clarence Clemons became influential rock and roll saxophone stylists. Junior Walker, King Curtis and Maceo Parker became influential soul and funk saxophone stylists, influencing the more technical jazz-fusion sounds of Michael Brecker and Bob Mintzer and pop-jazz players such as Candy Dulfer.

A number of experimental saxophones and saxophone-related instruments have appeared since Sax's original work, most with no lasting impact. During the early 1920s Reiffel & Husted of Chicago produced a slide soprano saxophone.
During the 1920s some straight alto and tenor saxophones were produced by Buescher, which proved cumbersome to handle and difficult to transport. Buescher custom produced one straight baritone saxophone as novelty instrument for a vaudeville performer.
C.G. Conn introduced two new variants in 1928–1929, the "Conn-O-Sax" and the mezzo-soprano saxophone keyed in F. The Conn-O-Sax is a straight-conical bore instrument in F (one step above the E alto) with a slightly curved neck and spherical bell. This instrument, which combines a saxophone bore and keys with a bell shaped similar to that of a heckelphone, was intended to imitate the timbre of the English horn and was produced only in 1929 and 1930. The instrument has a key range from low A to high G. Fewer than 100 Conn-O-Saxes are in existence and they are highly sought by collectors. The Conn mezzo-soprano experienced a similarly short production run as the economics of the Great Depression curtailed the market for what were regarded as novelty instruments. Most were expended by Conn as objects of repair training exercises.

The most successful of the unusual 1920s designs was the King "Saxello", essentially a straight B soprano, but with a slightly curved neck and tipped bell, made by the H. N. White Company. Such instruments now command prices up to US$4,000. Its lasting influence is shown in the number of companies, including Keilwerth, Rampone & Cazzani ("altello" model), L.A. Sax and Sax Dakota USA, marketing straight-bore, tipped-bell soprano saxophones as saxellos (or "saxello sopranos").

Interest in two 1920s variants was revived by jazz musician Rahsaan Roland Kirk, who called his straight Buescher alto a "stritch" and his Saxello a "manzello.". The Buescher straight alto was a production instrument while the manzello was in fact a Saxello with a custom-made large bell and modified keywork. More recently, the mezzo-soprano, or a modern variant of it, came into use by jazz musicians Anthony Braxton, James Carter, Vinny Golia, and Joe Lovano.

Some of the 1920s experimental designs, in addition to the Saxello, provide the basis for similar instruments produced during the modern era. Straight altos and tenors have been revived by Keilwerth, L.A. Sax and Sax Dakota USA. A mezzo-soprano in the key of G has been produced by Danish woodwind technician Peter Jessen, most notably played by Joe Lovano. This instrument is more in the timbral quality of Bb soprano saxophone.

The "contralto" saxophone, similar in size to the orchestral c-soprano, was developed in the late 20th century by California instrument maker Jim Schmidt. This instrument has a larger bore and a new fingering system, and does not resemble the orchestral instrument except for its key and register.
Benedikt Eppelsheim, of Munich, Germany has introduced recent innovations at the upper and lower ends of the saxophone range. The soprillo sax is a piccolo-sized straight instrument with the upper speaker hole built into the mouthpiece. The instrument, which extends Sax's original family, is pitched a full octave higher than the B soprano sax. 
The tubax, developed in 1999 by Eppelsheim, plays the same range and with the same fingering as the E contrabass saxophone; its bore, however, is narrower than that of a contrabass saxophone, resulting in a more compact instrument with a "reedier" tone (akin to the double-reed contrabass sarrusophone). It can be played with the smaller (and more commonly available) baritone saxophone mouthpiece and reeds. Eppelsheim has also produced subcontrabass tubaxes in C and B, the latter being the lowest saxophone ever made.

Among the 2000s developments is the aulochrome, a double soprano saxophone invented by Belgian instrument maker François Louis in 2001.

Since the 1950s, saxophones with non-metallic bodies have occasionally been in production. Such instruments have failed to gain acceptance over a number of issues including durability, repairability, and deficiencies in key action and tone. The best known of these efforts is the 1950s Grafton acrylic alto saxophone used briefly by Charlie Parker and Ornette Coleman. It had a production run of over 10 years as a budget model saxophone. The polycarbonate Vibratosax is in production as a low cost alternative to metal saxophones. Wooden Sawat saxophones are made in Thailand on a small scale. Opinions vary on the significance of body materials to sound.

The fingering scheme of the saxophone, which has had only minor changes since the instrument's original invention, has presented inherent acoustic problems related to closed keys below the first open tonehole that affect response of, and slightly muffle, some notes. There is also a lack of tactile consistency between key centers, requiring extra effort from the player to adjust modes of muscle memory when moving between key centers. Two efforts to remedy the acoustic problems and awkward aspects of the original fingering system are noteworthy.

The Leblanc Rationale and System saxophones have key mechanics designed to remedy the acoustic problems associated with closed keys below the first open tonehole. They also enable players to make half-step shifts of scales by depressing one key while keeping the rest of the fingering consistent with that of the fingering a half step away. Some Leblanc System features were built into the Vito Model 35 saxophones of the 1950s and 1960s. Despite the advantages of that system, acceptance was impaired by the expense and mechanical reliability issues related to the complexity of certain key mechanisms.

The chromatic, or linear fingering, saxophone is a project of instrument designer and builder Jim Schmidt, developing a horn maximizing tactile and logical consistency between every interval regardless of the key, and avoiding the acoustic problems associated closed keys below the first open tone hole. Several working prototypes have been built and presented at trade shows. Production of this original and expensive saxophone is on an individual order basis.

Inexpensive keyless folk versions of the saxophone made of bamboo (recalling a chalumeau) were developed in the 20th century by instrument makers in Hawaii, Jamaica, Thailand, Indonesia, Ethiopia, and Argentina. The Hawaiian instrument, called a xaphoon, was invented during the 1970s and is also marketed as a "bamboo sax", although its cylindrical bore more closely resembles that of a clarinet, and its lack of any keywork makes it more akin to a recorder. Jamaica's best known exponent of a similar type of homemade bamboo "saxophone" was the mento musician and instrument maker 'Sugar Belly' (William Walker). In the Minahasa region of the Indonesian island of Sulawesi, there exist entire bands made up of bamboo "saxophones" and "brass" instruments of various sizes. These instruments are imitations of European instruments, made using local materials. Similar instruments are produced in Thailand.

In Argentina, Ángel Sampedro del Río and Mariana García have produced bamboo saxophones of various sizes since 1985, the larger of which have bamboo keys to allow for the playing of lower notes.

Many synthesizer wind controllers are played and fingered like a saxophone.





</doc>
<doc id="26797" url="https://en.wikipedia.org/wiki?curid=26797" title="Sackbut">
Sackbut

A sackbut is a type of trombone from the Renaissance and Baroque eras, characterised by a telescopic slide that is used to vary the length of the tube to change pitch. Unlike the earlier slide trumpet from which it evolved, the sackbut possesses a U-shaped slide, with two parallel sliding tubes, which allows for playing scales in a lower range.

Records of the term "trombone" predates the term "sackbut" by two decades, and evidence for the German term "Posaune" is even older. "Sackbut", originally a French term, was used in England until the instrument fell into disuse in the eighteenth century; when it returned, the Italian term "trombone" became dominant. In modern English, an older trombone or its replica is called a sackbut.

An older instrument generally differs from modern trombones by its smaller, more cylindrically-proportioned bore, and its less-flared bell. The bell section was more resonant (since it did not contain the tuning slide and was loosely stayed rather than firmly braced to itself). These traits produce a "covered, blended sound which was a timbre particularly effective for working with voices... zincks and crumhorns", as in an alta capella.

The revived instrument had changed in specific ways. In the mid-18th century, the bell flare increased, crooks fell out of use, and flat, removable stays were replaced by tubular braces. The new shape produced a stronger sound, suitable to open-air performance in the marching bands where trombones became popular again in the 19th century. Before the early 19th century, most trombones adjusted tuning with a crook on the joint between the bell and slide or, more rarely, between the mouthpiece and the slide, rather than the modern tuning slide on the bell curve, whose cylindrical sections prevent the instrument from flaring smoothly through this section. Older trombones also generally don't have water keys, stockings, a leadpipe, or a slide lock, but as these parts are not critical to sound, replicas may include them. Bore size remained variable, as it still is today.

The first reference to a slide instrument was probably "trompette des ménestrels", first found in Burgundy in the 1420s and later in other regions of Europe. The name distinguished the instrument from the "trompettes de guerre" (war trumpets), which were of fixed length.

The next word to appear in the 15th century that implied a slide was the "sackbutt" group of words. There are two theories for the sources: it is either derived from the Middle French "sacquer" (to pull) and "bouter" (to push) or from the Spanish "sacar" (to draw or pull) and "bucha" (a tube or pipe). The term survives in numerous English spelling variations including sacbutt, sackbutte, sagbut, shagbolt, sacabushe, shakbusse and shakbusshe.

Closely related to "sackbutt" was the name used in France: "sacqueboute" and in Spain, where it was "sacabuche". These terms were used in England and France until the 18th century.

In Scotland in 1538 the slide instrument is referred to as "draucht trumpet" (drawn trumpet) as opposed to a "weir trumpet" (war trumpet), which had a fixed length.

In Germany, the original word was "Posaune", appearing about 1450 and is still used today. This (as well as "bason") derives from "busine," which is Latinate and meant straight trumpet.

In Italy it was (and remains) "trombone", which derived from trumpet in the Latin "tromba" or "drompten", used in the Low Countries. The first records of it being used are around 1440, but it is not clear whether this was just a nickname for a trumpet player. In 1487 a writer links the words "trompone" and "sacqueboute" and mentions the instrument as playing the contratenor part in a danceband.

The trombone developed from the trumpet. Up until 1375 trumpets were simply a long straight tube with a bell flare.

There are various uses of "sackbut"-like words in the Bible, which has led to a faulty translation from the Latin bible that suggested the trombones date back as far as 600 BC, but there is no evidence of slides at this time.

From 1375 the iconography sees trumpets being made with bends, and some in 'S' shapes. Around 1400 we see the "loop"-shaped trumpet appear in paintings and at some point in the 15th century, a single slide was added. This slide trumpet was known as a "trompette des ménestrels" in the alta capella bands.

The earliest clear evidence of a double slide instrument is in a fresco painting by Filippino Lippi in Rome, "The Assumption of the Virgin", dating from 1488–93.

From the 15th to the 19th centuries, the instrument designs changed very little overall, apart from a slight widening of the bell in classical era. Since the 19th century, trombone bore sizes and bells have increased significantly.

It was one of the most important instruments in Baroque polychoral works, along with the cornett and organ.

Sackbuts come in several sizes. According to Michael Praetorius, these were:

The pitch of the trombones has (notionally) moved up a semi-tone since the 17th century, and this is explained in the section on pitch.

Because the tenor instrument is described as "Gemeine" (common or ordinary), this is probably the most widely used trombone.

The basses, due to their longer slides, have a hinged handle on the slide stay, which is used to reach the long positions.

The giant Octav-Posaun / double bass trombone / contra-bass trombone in the style of those made in 16th/17th centuries is represented by only a few existing instruments. There is an original instrument made by Georg Nicolaus Öller built in Stockholm in 1639 and housed in the Scenkonstmuseet. In addition, Ewald Meinl has made a modern copy of this instrument, and it is currently owned and played by Wim Becu.

The bore size of renaissance/baroque trombones is approximately and the bell rarely more than in diameter. This compares with modern tenor trombones, which commonly have bores to and bells to .

Modern reproductions of sackbuts sacrifice some authenticity to harness manufacturing techniques and inventions that make them more comfortable for modern players, while retaining much of the original character of the old instruments.

Some original instruments could be disassembled into the constituent straight tubes, bowed tubes, bell flare, and stays, with ferrules at the joints. Mersenne has a diagram. (Little imagination is needed to see how it could be reassembled—with an extra tube—into something approaching a natural trumpet.) There is a debate as to whether they used tight fittings, wax or another joining substance. Modern sackbut reproductions are usually soldered together. Some modern sackbut reproductions use glue as a compromise to give a loose fitting for high resonance without risk of falling apart.

Tuning slides came in during the very late 18th century. Early trombonists adjusted pitch with the slide, and by adding variously shaped and sized crooks. Modern reproductions often have a bell bow tuning slide or telescopic slide between the slide and bell sections. Crooks are still used, as are variously sized bell bow sections for larger changes.

The stays on period sackbuts are flat. While the bell stay remained flat, from about 1660 the slide stays became tubular. On many modern reproductions round slide stays are much more comfortable to play and easier to make.

A loose connection between the bell stay and the bell is thought key to a resonant bell, and thus a better sackbut sound. Original instruments have a hinge joint. Modern copies with a tuning slide in the bell can need more support for operation of the slide, so either an extra stay by the tuning slide is provided or a joint without play in only one axis is employed.

The original way to make the slide tubes was to roll a flat piece of metal around a solid cylinder mandrel, and the joining edges soldered together. Modern manufacturers now draw the tubes. They also tend to have stockings, which were only invented around 1850. In addition, modern made slides are usually made of nickel silver with chrome plating, giving a smoother finish and quieter action than simply the brass that would have originally been used.

The water key was added in the 19th century, but modern reproductions often have them.

Until some time in the 18th century, the trombone was in A and the pitch of that A was about a half-step higher than it is today—460–480 Hz. There was a transition around the 18th century when trombones started to be thought of in B at around 440 Hz. This change did not require a change in the instrument, merely a new set of slide positions for each note. But it does mean that the baroque and renaissance repertoire was intended to be played at the higher pitch. There are many examples of evidence for this:



The tenor trombones that survive are pitched closest to B at A=440 Hz, which is the same as A at A=466 Hz. So what we now think of as a tenor trombone with B in first position, pitched at A=440 was actually thought of as a trombone in A (in first position), pitched at A=466. Surviving basses in D at A=466 (E at 440)—for example: Ehe, 1612 (Leipzig) and Hainlein, c.1630 (Nuremberg) confirm Praetorius' description. It is also worth noting that Rognoni's "Suzanne ung jour" setting descends repeatedly to BB, which is a tone lower than the lowest note playable on a bass in F; on a bass in D, it falls in (modern) fifth position.

Many groups now perform at A=466 Hz for the sake of greater historical accuracy.

The sackbut was described as suitable for playing with the 'loud' ensembles in the outdoors, as well as the 'soft' ensembles inside.

The alta capella bands are seen in drawings as entertaining outside with ensembles including shawms, trumpets and trombones. When pushed, sackbuts can easily make a loud and brassy sound.

The sackbut also responds very well to rather soft playing—more so than a modern trombone. The sound is characterized by a more delicate, vocal timbre. The flat rims and shallow cups of the older mouthpieces are instrumental in providing the player with a much wider palette of articulations and tonal colours. This flexibility lends itself to a vocal style of playing and facilitates very characterful phrasing.

Mersenne wrote in 1636, "It should be blown by a skillful musician so that it may not imitate the sounds of the trumpet, but rather assimilate itself to the sweetness of the human voice, lest it should emit a warlike rather than a peaceful sound."

The Lorenzo da Lucca was said to have had "in his playing a certain grace and lightness with a manner so pleasing".

Musicians of the 16th and 17th centuries benefited from a broader base of skills than the average performer today.

These traditions continued into the baroque with musicians expected to give expression to the written music by ornamenting with a mixture of one-note “graces” and whole passage “divisions” (also known as “diminutions”). The suggestions for producing effective ornaments without disrupting the line and harmony are discussed alongside countless examples in the 16th and early 17th century Italian division tutors. Graces such as the accento, portar della voce, tremolo, groppo, trillo, esclamationo and intonatio are all to be considered by performers of any music in this period.

“Cornetts and trombones...play divisions that are neither scrappy, nor so wild and involved that they spoil the underlying melody and the composer's design: but are introduced at such moments and with such vivacity and charm that they give the music the greatest beauty and spirit”
Bottrigari, Venice 1594

Along with the improvisation, many of these tutors discuss articulation. Francesco Rognoni in 1620 describes the tonguing as the most important part of producing “a good and beautiful effect in playing wind instruments, and principally the cornett” (which of course had a very similar role to the trombone). The treatises discuss the various strengths of consonants from “le” through “de” to “te”. But the focus of the text is for playing rapid notes “similar to the gorgia of the human voice” with “soft and smooth” double tonguing (“lingua riversa”) using “le re le re”. This is opposed to using “te che te che,” which is described as “harsh, barbarous and displeasing”. The natural ‘pairing’ of notes these articulations provide is similar to the instructions for string players who are instructed to slur (“lireggiar”) pairs of eighth notes with one bow stroke per quarter beat.

Another integral part of the early music sound-world is the musical temperament. Music in the middle-ages favours intervals of the fourth and fifth, which is why Pythagorean tuning was used. The interval of a third was used as a clash until the Renaissance, when it became consonant in compositions, which went hand-in-hand with the widespread use of meantone temperament. During the 17th century, Well temperament began to become more and more popular as the range of keys increased. Temperament affects the colour of a composition, and therefore modern performances, typically employing equal temperament, may not be true representations of the composers' intentions.
These old tunings are the result of the natural harmonic series of a brass instrument such as the sackbut. 
As the bell is smaller than a modern trombone, the harmonic series is closer to a perfect harmonic series, which is the basis for just tuning. Without adjusting the slide, the first to second harmonic is a perfect octave, second to third harmonic is a fifth slightly wider than equal temperament and fourth to fifth harmonic is a major third slightly narrower than in equal temperament. These adjusted intervals make chords ring and are the basis of meantone. In fact, Daniel Speer says “Once you have found a good C (third position), this is also the place you will find your F.” Playing a sounding C and F in exactly the same position on a modern orchestra sounds out of tune, but it tunes perfectly well on in a sackbut choir if everyone plays natural harmonics.

Plenty of musical understanding can be gathered from reading the original music print. Publishers such as SPES and Arnaldo Forni Edition provide facsimile copies of plenty of music for trombone from this era. To read these it one needs to become familiar with the old clefs, time signatures, ligatures and notational conventions of the era.

The sound of sackbuts (and trombones) has long been thought especially solemn and noble, had an association with death and the afterlife. The instrument was a symbol of divine presence, the voice of the angels and instrument of judgment. This symbolism can be seen, for instance, in "L'Orfeo", "Alceste", "The Magic Flute", the "Death March" from "Saul", and funeral aequales.

This association was probably encouraged by the lack of distinction made between natural horns, slide trumpets, and trombones in this Renaissance; they were used and often named interchangeably. Martin Luther's 1534 translation of the Bible into German renders the Greek "shophar" and "salpigx" to "Posaune". "Posaune" at the time could refer to a natural horn or other brass instrument, but it later came to mean exclusively "trombone" (similarly, English translations generally have "trumpet", and only occasionally "horn" or "shofar"). This gives the later reader of the Luther Bible texts such as: “…we shall all be changed, in a moment, in the twinkling of an eye, at the last trombone; for the trombone shall sound and the dead shall be raised incorruptible” (1 Corinthians 15:52).

The sackbut replaced the slide trumpet in the 15th century alta capella wind bands that were common in towns throughout Europe playing courtly dance music. See Waits.

Another key use of the trombone was in ceremonies, in conjunction with the trumpet. In many towns in Germany and Northern Italy, 'piffari' bands were employed by local governments throughout the 16th century to give regular concerts in public squares and would lead processions for festivals. Piffari usually contained a mix of wind, brass and percussion instruments and sometimes viols.

Venice's doge had his own piffari company and they gave an hour-long concert in the Piazza each day, as well as sometimes performing for services in St. Mark's. Each of the six confraternities in Venice also had their own independent piffari groups too, which would all play at a lavish procession on the feast of Corpus Domini. These groups are in addition to the musicians employed by St. Mark's to play in the balconies with the choir (the piffari would play on the main level).

It also was used in church music both for instrumental service music and as a doubling instrument for choral music. The treble and high alto parts were most often played by cornetts or shawms, with the violin sometimes replacing the cornett in 17th century Italian music.

The first record of trombones being used in churches was in Innsbruck 1503. Seville Cathedral's records show employment of trombonists in 1526, followed by several other Spanish cathedrals during the 16th century, used not only for ceremonial music and processionals, but also for accompaniment of the liturgical texts as well, doubling voices.

The sacred use of trombones was brought to a fine art by the Andrea Gabrieli, Giovanni Gabrieli and their contemporaries c.1570-1620 Venice and there is also evidence of trombonists being employed in churches and cathedrals in Italy at times during the second half of the 16th century in Bologna, Rome, Padua, Mantua and Modena.

Since ensembles had flexible instrumentation at this time, there is relatively little music before Giovanni Gabrieli's publication "Symphoniae sacrae" (1597) that specifically mentions trombones. The only example currently known is the music by Francesco Corteccia for the Medici wedding 1539.

The 17th century brings two pieces of real solo trombone repertoire.

Giovanni Martino Cesare wrote "La Hieronyma," (Musikverlag Max Hieber, MH6012) the earliest known piece for accompanied solo trombone. It comes from Cesare's collection "Musicali Melodie per voci et instrumenti a una, due, tre, quattro, cinque, e sei" published in Munich 1621 of 28 pieces for a mixture of violins, cornetts, trombone, vocal soloists and organ continuo. The collection also contains "La Bavara" for four trombones.

The other solo trombone piece of the 17th century, "Sonata trombone & basso" (modern edition by H Weiner, Ensemble Publications), was written around 1665. This anonymous piece is also known as the 'St. Thomas Sonata' because it was kept in the library of the Saint Thomas Augustinian Monastery in Brno, Czech Republic.

Francesco Rognoni was another composer who specified the trombone in a set of divisions (variations) on the well-known song "Suzanne ung jour" (London Pro Musica, REP15). Rognoni was a master violin and gamba player whose treatise "Selva di Varie passaggi secondo l'uso moderno" (Milan 1620 and facsimile reprint by Arnaldo Forni Editore 2001) details improvisation of diminutions and Suzanne is given as one example. Although most diminutions are written for organ, string instruments or cornett, Suzanne is "per violone over Trombone alla bastarda". With virtuosic semiquaver passages across the range of the instrument, it reflects Praetorius' comments about the large range of the tenor and bass trombones, and good players of the Quartposaune (bass trombone in F) could play fast runs and leaps like a viola bastarda or cornetto. The term "bastarda" describes a technique that made variations on all the different voices of a part song, rather than just the melody or the bass: "considered illegitimate because it was not polyphonic".

In the 17th century, a considerable repertoire of chamber music using sackbut with various combinations of violins, cornetts and dulcians, often with continuo, appeared. Composers included Dario Castello, Giovanni Battista Fontana, Giovanni Paolo Cima, Andrea Cima, Johann Heinrich Schmelzer and Matthias Weckmann.
Antonio Bertali wrote several trio sonatas for 2 violins, trombone and bass continuo in the mid-17th century. One such "Sonata a 3" is freely available in facsimile form from the Düben Collection website hosted by Uppsala universitet. A "Sonata a3 in C" is published by Musica Rara and attributed to Biber, although the authorship is unclear and it is more likely to have been written by Bertali.

Dario Castello, a wind player at St. Mark's Venice in the early 17th century had two books of "Sonate Concertate" published in 1621 and 1629. The sonatas of 1-4 parts with bass continuo often specify trombones, as well as cornett, violin and bassoon. The numerous reprints during the 17th century affirm his popularity then, as perhaps now.

Giuseppe Scarani joined St. Mark's Venice in 1629 as a singer and in the following year published "Sonate concertate", a volume of works for 2 or 3 (unspecified) instruments (and b.c.). The title has been suggested was chosen to try and capture some of Castello's success.

Tiburtio Massaino wrote a Canzona for eight trombones, published in Raverio's 1608 collection.

Johann Heinrich Schmelzer wrote several sonatas that included trombones—such as his "Sonata à 7" for two cornetts, two trumpets, three trombones, and basso continuo.

Daniel Speer published a four-part sonata in "Neu-gebachene Taffel-Schnitz" (1685). In 1687, Speer published the first written instruction in sackbut (and several other instruments) playing: "Grund-richtiger/kurtz/leicht und noethiger Unterricht der Musicalischen Kunst". The second edition in 1697 provides two three part sonatas for trombones.

An English work of note from this period is Matthew Locke's "Music for His Majestys Sagbutts and Cornetts", a suite for Charles II's coronation 1661.

Non-serious music, often based on dances for festive occasions, rarely had specified instrumentation. Often you find something like "per diversi musici". Indeed, the groups that would perform them would often be full of multi-instrumentalists.

Johann Pezel wrote for Stadtpfeifer with his "Hora decima musicorum" (1670), containing sonatas, as well as "Fünff-stimmigte blasende Music" (1685) with five-part intradas and dance pieces.

Well known pieces from Germany includes Samuel Scheidt's "Ludi Musici" (1621) and Johann Hermann Schein's "Banchetto musicale" (1617).

The first English piece scored for trombone is John Adson's "Courtly Masquing Ayres" (1611). Another light collection suitable for including trombones is Anthony Holborne's "Pavans, Galliards, Allmains, and other short Aeirs both Grave and Light in Five Parts for Viols, Violins or Other Musicall Winde Instruments" (1599).

Trombonists were in the regular ensemble at St. Mark's Venice from its formation in 1568 until they left the payroll in 1732. The first two ensemble directors—"maestro di concerti"—Girolamo Dalla Casa (1568–1601) and Giovanni Bassano (1601–1617)—were cornett players and the nucleus of the group was two cornetts and two trombones, although for the larger ceremonies many extra players were hired. During a mass attended by the Doge, evidence suggests they would have played a canzona in the Gradual after the Epistle and the Agnus Dei, a sonata in the Offertory as well as reinforcing vocal parts or substituting for absent singers.

This ensemble was used extensively by Giovanni Gabrieli in pieces substantially for brass, voices and organ in Venice up until his death in 1612. He was greatly influential in Venetian composers in other churches and confraternities, and his early baroque and cori spezzati style is seen in contemporaries like Giovanni Picchi and Giovanni Battista Grillo.

It is suggested that Monteverdi wrote his "Vespro della Beata Vergine" (1610) as a pitch for employment at St. Mark's as successor to Giovanni Gabrieli. In addition to the Magnificat, two movements specify trombones: the opening "Deus in adiutorium" is for six voices, two violins, two cornetts, three trombones, five viole da braccio and basso continuo; Sonata sopra "Sancta Maria, ora pro nobis" is for soprano, two violins, two cornetts, three trombones (one of which can be a viola da braccio) and basso continuo. Monteverdi also leaves the option to use trombones as part of the "sex instrumentis" of the "Dixit Dominus" and in the instrumental "Ritornello a 5" between verses of "Ave maris stella".

From around 1617, when the "maestro de' concerti" at St. Marks changed to violinist Francesco Bonfante and correspondingly the ensemble changed from basically a brass ensemble to being more evenly mixed with brass, wind and string instruments.

Monteverdi arrived at St. Mark's in 1613 and it is unsurprising that he includes trombones and strings for several more sacred works during his time here, published in his "Selva morale e spirituale" 1641. Of the c.40 items in this collection, six specify three or four trombones (or viola da braccio, ad lib): SV268 Beatus vir I, SV263 Dixit Dominus I, SV263 Dixit Dominus II, SV261 Et iterum venturus est, SV258 Gloria in excelsis Deo, SV281 Magnificat I. Each is for 3-8 voices with 3 violins (apart from SV261), the trombones/violas and basso continuo. Monteverdi also specified trombones in two more sacred works: SV198 Laetatus sum (i) (1650) for 6 voices, 2 violins, 2 trombones and bassoon and SV272 Laudate Dominum omnes gentes I (1641) for 5 voices ‘concertato’, 4 voice chorus ad lib, 4 viola da braccio or trombones and basso continuo.

A prolific composer for trombones in Germany in the 17th century was Heinrich Schütz. His "Fili me, Absalon" (SWV 269) and "Attendite, popule meus" (SWV 270), are both scored for bass voice, four trombones (of which two are optionally violins) and basso continuo, are well known. They are part of his first "Symphoniae Sacrae" collection dating from 1629 and commentators have noted that the style reflects his studies in Venice with Giovanni Gabrieli 1609-1612. Other pieces that specify trombones (according to Grove) are (grouped by the collection they were published in): Concert mit 11 Stimmen (1618): SWV 21, in "Psalmen Davids" (Psalms of David) Op. 2 (1619): SWV 38, 40-46, Symphoniae sacrae I Op.6 (1629): SWV 259, 269-271, 274, Symphoniae sacrae II Op.10 (1647): SWV 344, Symphoniae sacrae III Op. 12 (1650): SWV 398a, Historia (1664): SWV 435, 448, 449, 453, 461, 452, 466-470, 473, 474-476, Schwanengesang Psalm 119 (1671): SWV 500, although many others are suitable for trombones too.

Johann Hermann Schein specified trombones in some of his sacred vocal works in the "Opella nova, ander Theil, geistlicher Concerten" collection (Leipzig, 1626). For example, "Uns ist ein Kind geboren" is scored for violino, traversa, alto trombone, tenor voice, fagotto and basso continuo. "Mach dich auf, werde licht, Zion" uses Canto 1: violino, cornetto, flauto picciolo e voce, Canto 2: voce e traversa, Alto: Trombone e Voce, Tenore: Voce e Trombone, Basso: Fagotto Trombone e Voce and Basso Continuo, during which solos for each of the trombonists are specified. Of particular interest is "Maria, gegrüsset seist du, Holdselige," which uses soprano and tenor voices, alto trombone, 2 tenor trombones and on the bass line "trombone grosso," which goes down to pedal A, and a couple of diatonic scale passages from bottom C.

German composer Johann Rudolf Ahle wrote some notable sacred pieces for voices and trombones. "Höre, Gott" uses five favoriti singers, two ripieno choirs (which double other parts at intense moments) and seven trombones, with basso continuo. And his most famous "Neu-gepflanzte Thüringische Lust-Garten.." (1657–65) contains several sacred works with 3 or 4 trombones, including "Magnificat a 8" for SATB soloists, cornett, 3 trombones and continuo and "Herr nun lässestu deinen Diener a 5" for bass, 4 trombones and continuo.

Dieterich Buxtehude specifies trombones in a few sacred concertos using style derived from polychoral Venetian works and one secular piece. For example, "Gott fähret auf mit Jauchzen" (BuxWV33 from CW v, 44) is scored for SSB voices, 2 violins, 2 violas, trombones, 2 cornetts, 2 trumpets, bassoon and basso continuo.

There are a few vocal works involving trombones in works by Andreas Hammerschmidt. These include "Lob- und Danck Lied aus dem 84. Psalm" for 9 voices, 5 trumpets, 3 trombones, 5 violas and basso continuo (Freiberg, 1652). There is also "Hochzeitsgesang für Daniel Sartorius: Es ist nicht gut, dass der Mensch allein sei" for 5 voices, 2 violins, 2 trombones, bassoon and basso continuo.

Johann Schelle has numerous sacred vocal works that use trombones. For instance "Vom Himmel kam der Engel Schar" is scored for soprano, tenor, SSATB choir, 2 violins, 2 violas, 2 cornetts, 3 trombones, 2 trumpets, timpani, basso continuo, and "Lobe den Herrn, meine Seele" is for two choirs of SSATB and similar instruments to the previous work.

The lesser known Austrian composer Christoph Strauss, Kapellmeister to the Habsburg Emperor Mathias 1616-1620, wrote two important collections for trombones, cornetts and voices. His motets published in Nova ac diversimoda sacrarum cantionum composition, seu motettae (Vienna, 1613) are in a similar tradition to Gabrieli's music. Of the sixteen motets in the collection, all are titled "concerto" apart from the "sonata" "Expectans Expectavi Dominum" for 6 trombones, cantus voice and tenor voice. In 1631 he published a number of masses, which were much more baroque, with basso continuo, rhetorical word painting and obligato usage of instruments.

Later in the 17th century, Heinrich Ignaz Franz Biber composed sacred works for voices and orchestra featuring trombones. His "Requiem" mass (1692) uses an orchestra of strings, 3 trombones and basso continuo. A similar ensemble accompanies 8 vocal lines in his "Lux perpetua" (c1673), and three more similar works in the 1690s.

Monteverdi ushers sackbuts into the first great opera, 'L'Orfeo' 1607. The orchestra at the first performance, as shown in the first publication, the list of "stromenti" at the front of the score specifies four trombones, but at one point in Act 3, however, the score calls for five trombones.

There is relatively little repertoire for the trombone in the late baroque.

Johann Sebastian Bach uses trombones in fourteen of his church cantatas—BWV 2, 3, 4, 21, 23, 25, 28, 38, 64, 68, 96, 101, 121, 135—as well as motet BWV 118. He uses the trombone sound to reflect the (by now) archaic sounds of the Renaissance trombones doubling voices (with cornett playing the soprano line), yet he also uses them independently, which John Eliot Gardiner says prepares the way for their use in Beethoven's Symphony No. 5. The cantatas were either composed in Leipzig during 1723-1725, or (for BWV 4, 21 & 23) the trombone parts were added to the existing cantata during the same period. The cornett and trombone parts would have been played by the Stadtpfeifer.

In England, George Frideric Handel includes trombones in three of his oratorios: "Saul" (1738), "Israel in Egypt" (1738) and "Samson" (1741). There are no other documented groups or performances with trombone players in England at this time, and it has been suggested that the premiers took place with a visiting group from Germany, as was the custom in Paris at this time.

Vienna's Imperial court used trombones in church music:

Johann Joseph Fux was Hofkapellmeister in Vienna from 1715 until 1741. Many of his masses use the choir strengthened by strings, cornetts and trombones, often with independent moments for the instrumentalists and sometimes. "Missa SS Trinitatis" uses two choirs, which again points to the traditions going back to Gabrieli. His highly successful Requiem is for five vocal parts, two cornetts, two trombones, strings and continuo. He also uses the trombone in smaller motets and antiphons, such as his setting of "Alma Redemptoris mater" for soprano, alto trombone, strings and continuo. Some of his chamber music involves trombones, as do many of his operas, used as an obbligato instrument.

Also in the Vienna court was Antonio Caldara, vice-kapellmeister 1717–1736. Among his output are two Holy Week settings as Da Capo arias: "Deh sciogliete, o mesti lumi" for soprano, unison violins, bassoon, two trombones and organ and "Dio, qual sia" for soprano, trombone, bassoon and basso continuo.

Again this period suffers from a lack of trombone players. Most of these works derive from Vienna and Salzburg.

Joseph Haydn uses trombones in "Il rotorno di Tobia", "Die sieben letzten Worte", "The Creation", "Die Jahreszeiten", "Der Sturm", "Orfeo ed Euridice" and secular cantata choruses.

Wolfgang Amadeus Mozart uses trombones in connection with death or the supernatural. This includes the Requiem (K626, 1791), Great Mass in C minor (K423, 1783), "Coronation Mass (C major)" (K317, 1779), several other masses, "Vesperae Solennes de Confessore" (K339, 1780), "Vesperae de Dominica", his arrangement of Handel's "Messiah" plus two of his three great operas: "Don Giovanni" (K527, 1787) and "Die Zauberflöte" (K620, 1791). Mozart's first use of the trombone was an obligato line in the oratorio "Die Schuldigkeit des ersten Gebots" (K35, 1767)

Christoph Willibald Gluck includes trombones in five of his operas: "Iphigénie en Aulide" (1774), Orfeo ed Euridice (1774), "Alceste" (1776), "Iphigénie en Tauride" (1779) and "Echo et Narcisse" (1779), as well as ballet "Don Juan" (1761).

Some chamber music in this period includes trombone in an obligato role with voice, and also as a concerto instrument with string orchestra. Composers include the likes of Leopold Mozart, Georg Christoph Wagenseil, Johann Albrechtsberger, Michael Haydn and Johann Ernst Eberlin.

For works for trombone post-1800, please see trombone.

Many groups specializing in period music make frequent and prominent use of the sackbut.

External links:



Plenty of recordings of the authentic sackbut are now available from the groups such as Concerto Palatino, HMSC, Gabrieli Consort and the Toulouse Sacqueboutiers. For a closer examination of the instrument, here are some recommended recordings where the sackbut is heavily featured in a "solo" capacity.

The earliest instruments:
Other notable sackbuts:
For more information, see Herbert (2006).







</doc>
