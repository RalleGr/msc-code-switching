<doc id="14912" url="https://en.wikipedia.org/wiki?curid=14912" title="Incest">
Incest

Incest is human sexual activity between family members or close relatives. This typically includes sexual activity between people in consanguinity (blood relations), and sometimes those related by affinity (marriage or stepfamily), adoption, clan, or lineage.

The incest taboo is one of the most widespread of all cultural taboos, both in present and in past societies. Most modern societies have laws regarding incest or social restrictions on closely consanguineous marriages. In societies where it is illegal, consensual adult incest is seen by some as a victimless crime. Some cultures extend the incest taboo to relatives with no consanguinity such as milk-siblings, step-siblings, and adoptive siblings, albeit sometimes with less intensity. Third-degree relatives (such as half-aunt, half-nephew, first cousin) on average have 12.5% common genetic heritage, and sexual relations between them are viewed differently in various cultures, from being discouraged to being socially acceptable. Children of incestuous relationships have been regarded as illegitimate, and are still so regarded in some societies today. In most cases, the parents did not have the option to marry to remove that status, as incestuous marriages were, and are, normally also prohibited.

A common justification for prohibiting incest is avoiding inbreeding: a collection of genetic disorders suffered by the children of parents with a close genetic relationship. Such children are at greater risk for congenital disorders, death, and developmental and physical disability, and that risk is proportional to their parents' coefficient of relationship—a measure of how closely the parents are related genetically. But cultural anthropologists have noted that inbreeding avoidance cannot form the sole basis for the incest taboo because the boundaries of the incest prohibition vary widely between cultures, and not necessarily in ways that maximize the avoidance of inbreeding.

In some societies, such as those of Ancient Egypt, brother–sister, father–daughter, mother–son, cousin–cousin, aunt–nephew, uncle–niece, and other combinations of relations within a royal family were married as a means of perpetuating the royal lineage. Some societies, such as the Balinese and some Inuit tribes, have different views about what constitutes illegal or immoral incest. However, sexual relations with a first-degree relative (meaning a parent or sibling) are almost universally forbidden.

The English word "incest" is derived from the Latin "incestus", which has a general meaning of "impure, unchaste".
It was introduced into Middle English, both in the generic Latin sense (preserved throughout the Middle English period) and in the narrow modern sense.
The derived adjective "incestuous" appears in the 16th century.
Before the Latin term came in, incest was known in Old English as "sib-leger" (from "sibb" 'kinship' + "leger" 'to lie') or "mǣġhǣmed" (from "mǣġ" 'kin, parent' + "hǣmed" 'sexual intercourse') but in time, both words fell out of use. Terms like "incester" and "incestual" have been used to describe those interested or involved in sexual relations with relatives among humans, while "inbreeder" has been used in relation to similar behavior among non-human animals or organisms.

Other words that describe sexual attraction to relatives include consanguinophilia, consanguinamory, synegenesophilia, incestuality and incestophilia.

In ancient China, first cousins with the same surnames (i.e., those born to the father's brothers) were not permitted to marry, while those with different surnames could marry (i.e., maternal cousins and paternal cousins born to the father's sisters).

Several of the Egyptian Pharaohs married their siblings and had several children with them. For example, Tutankhamun married his half-sister Ankhesenamun, and was himself the child of an incestuous union between Akhenaten and an unidentified sister-wife. Several scholars, such as Frier et al., state that sibling marriages were widespread among all classes in Egypt during the Graeco-Roman period. Numerous papyri and the Roman census declarations attest to many husbands and wives being brother and sister, of the same father and mother. However, it has also been argued that available evidence does not support the view such relations were common.
The most famous of these relationships were in the Ptolemaic royal family; Cleopatra VII was married to her younger brother, Ptolemy XIII, while her mother and father, Cleopatra V and Ptolemy XII, had also been brother and sister. Before the Ptolemies' rule, only circumstances of half-sibling incest could be observed within the royal family in Egypt. Arsinoe II and her younger brother, Ptolemy II Philadelphus, were the first ones to break tradition and participate in a full-sibling marriage. 

The fable of "Oedipus", with a theme of inadvertent incest between a mother and son, ends in disaster and shows ancient taboos against incest as Oedipus blinds himself in disgust and shame after his incestuous actions. In the "sequel" to Oedipus, "Antigone", his four children are also punished for their parents' incestuousness. Incest appears in the commonly accepted version of the birth of Adonis, when his mother, Myrrha has sex with her father Cinyras during a festival, disguised as a prostitute.

In Ancient Greece, Spartan King Leonidas I, hero of the legendary Battle of Thermopylae, was married to his niece Gorgo, daughter of his half-brother Cleomenes I. Greek law allowed marriage between a brother and sister if they had different mothers. For example, some accounts say that Elpinice was for a time married to her half-brother Cimon.

Incest was sometimes acknowledged as a positive sign of tyranny in Ancient Greece. Herodotus recounts a dream of Hippias, son of Pesistratus, in which he "slept with his own mother," and this dream gave him assurance that he would regain power over Athens. Suetonius attributes this omen to a dream of Julius Caesar, explaining the symbolism of dreaming of sexual intercourse with one's own mother.

Incest is mentioned and condemned in Virgil's "Aeneid" Book VI: "hic thalamum invasit natae vetitosque hymenaeos;" "This one invaded a daughter's room and a forbidden sex act".

Roman civil law prohibited marriages within four degrees of consanguinity but had no degrees of affinity with regards to marriage. Roman civil laws prohibited any marriage between parents and children, either in the ascending or descending line "ad infinitum". Adoption was considered the same as affinity in that an adoptive father could not marry an unemancipated daughter or granddaughter even if the adoption had been dissolved. Incestuous unions were discouraged and considered "nefas" (against the laws of gods and man) in ancient Rome. In AD 295 incest was explicitly forbidden by an imperial edict, which divided the concept of "incestus" into two categories of unequal gravity: the "incestus iuris gentium," which was applied to both Romans and non-Romans in the Empire, and the "incestus iuris civilis," which concerned only Roman citizens. Therefore, for example, an Egyptian could marry an aunt, but a Roman could not. Despite the act of incest being unacceptable within the Roman Empire, Roman Emperor Caligula is rumored to have had sexual relationships with all three of his sisters (Julia Livilla, Drusilla, and Agrippina the Younger). Emperor Claudius, after executing his previous wife, married his brother's daughter Agrippina the Younger, and changed the law to allow an otherwise illegal union. The law prohibiting marrying a sister's daughter remained. The taboo against incest in Ancient Rome is demonstrated by the fact that politicians would use charges of incest (often false charges) as insults and means of political disenfranchisement.

However, scholars agree that during the first two centuries A.D., in Roman Egypt, full sibling marriage occurred with some frequency among commoners as both Egyptians and Romans announced weddings that have been between full-siblings. This is the only evidence for brother-sister marriage among commoners in any society.

In Norse mythology, there are themes of brother-sister marriage, a prominent example being between Njörðr and his unnamed sister (perhaps Nerthus), parents of Freyja and Freyr. Loki in turn also accuses Freyja and Freyr of having a sexual relationship.

The earliest Biblical reference to incest involved Cain. It was cited that he knew his wife and she conceived and bore Enoch. During this period, there was no other woman except Eve or there was an unnamed sister and so this meant Cain had incestuous relationship with his mother or his sister. According to the Book of Jubilees, Cain married his sister Awan. Later, in of the Hebrew Bible, the Patriarch Abraham married his half-sister Sarah. Other references include the passage in Samuel where Amnon, King David's son, raped his half-sister, Tamar. According to Michael D. Coogan, it would have been perfectly all right for Amnon to have married her, the Bible being inconsistent about prohibiting incest.

In Genesis 19:30-38, living in an isolated area after the destruction of Sodom and Gomorrah, Lot's two daughters conspired to inebriate and seduce their father due to the lack of available partners to continue his line of descent. Because of intoxication, Lot "perceived not" when his firstborn, and the following night his younger daughter, lay with him (Genesis 19:32–35). 

Moses was also born to an incestuous marriage. detailed how his father Amram was the nephew of his mother Jochebed. An account noted that the incestuous relations did not suffer the fate of childlessness, which was the punishment for such couples in levitical law. It stated, however, that the incest exposed Moses "to the peril of wild beasts, of the weather, of the water, and more."

Many European monarchs were related due to political marriages, sometimes resulting in distant cousins – and even first cousins – being married. This was especially true in the Habsburg, Hohenzollern, Savoy and Bourbon royal houses. However, relations between siblings, which may have been tolerated in other cultures, were considered abhorrent. For example, the accusation that Anne Boleyn and her brother George Boleyn had committed incest was one of the reasons that both siblings were executed in May 1536.

Incestuous marriages were also seen in the royal houses of ancient Japan and Korea, Inca Peru, Ancient Hawaii, and, at times, Central Africa, Mexico, and Thailand. Like the pharaohs of ancient Egypt, the Inca rulers married their sisters. Huayna Capac, for instance, was the son of Topa Inca Yupanqui and the Inca's sister and wife.

The ruling Inca king was expected to marry his full sister. If he had no children by his eldest sister, he married the second and third until they had children. Preservation of the purity of the Sun's blood was one of the reasons for the brother-sister marriage of the Inca king. The Inca kings claimed divine descent from celestial bodies, and emulated the behavior of their celestial ancestor, the Sun, who married his sister, the Moon. Another reason the princes and kings married their sisters was so the heir might inherit the kingdom as much as through his mother as through his father. Therefore, the prince could invoke both principles of inheritance. 

Half-sibling marriages were found in ancient Japan such as the marriage of Emperor Bidatsu and his half-sister Empress Suiko. Japanese Prince Kinashi no Karu had sexual relationships with his full sister Princess Karu no Ōiratsume, although the action was regarded as foolish. In order to prevent the influence of the other families, a half-sister of Korean Goryeo Dynasty monarch Gwangjong became his wife in the 10th century. Her name was Daemok. Marriage with a family member not related by blood was also regarded as contravening morality and was therefore incest. One example of this is the 14th century Chunghye of Goryeo, who raped one of his deceased father's concubines, who was thus regarded to be his mother.

In India, the largest proportion of women aged 13 to 49 who marry their close relative are in Tamil Nadu, then Andhra Pradesh, Karnataka, and Maharashtra. While it is rare for uncle-niece marriages, it is more common in Andhra Pradesh and Tamil Nadu.

In some Southeast Asian cultures, stories of incest being common among certain ethnicities are sometimes told as expressions of contempt for those ethnicities.

Marriages between younger brothers and their older sisters were common among the early Udegei people.

In the Hawaiian Islands, high "ali'i" chiefs were obligated to marry their older sisters in order increase their "mana". These copulations were thought to maintain the purity of the royal blood. Another reason for these familial unions was to maintain a limited size of the ruling "ali'i" group. As per the priestly regulations of Kanalu, put in place after multiple disasters, "chiefs must increase their numbers and this can be done if a brother marries his older sister."

Incest between an adult and a person under the age of consent is considered a form of child sexual abuse that has been shown to be one of the most extreme forms of childhood abuse; it often results in serious and long-term psychological trauma, especially in the case of parental incest. Its prevalence is difficult to generalize, but research has estimated 10–15% of the general population as having at least one such sexual contact, with less than 2% involving intercourse or attempted intercourse. Among women, research has yielded estimates as high as 20%.

Father–daughter incest was for many years the most commonly reported and studied form of incest. More recently, studies have suggested that sibling incest, particularly older brothers having sexual relations with younger siblings, is the most common form of incest, with some studies finding sibling incest occurring more frequently than other forms of incest. Some studies suggest that adolescent perpetrators of sibling abuse choose younger victims, abuse victims over a lengthier period, use violence more frequently and severely than adult perpetrators, and that sibling abuse has a higher rate of penetrative acts than father or stepfather incest, with father and older brother incest resulting in greater reported distress than stepfather incest.

Sex between an adult family member and a child is usually considered a form of child sexual abuse, also known as child incestuous abuse, and for many years has been the most reported form of incest. Father–daughter and stepfather–stepdaughter sex is the most commonly reported form of adult–child incest, with most of the remaining involving a mother or stepmother. Many studies found that stepfathers tend to be far more likely than biological fathers to engage in this form of incest. One study of adult women in San Francisco estimated that 17% of women were abused by stepfathers and 2% were abused by biological fathers. Father–son incest is reported less often, but it is not known how close the frequency is to heterosexual incest because it is likely more under-reported. Prevalence of incest between parents and their children is difficult to estimate due to secrecy and privacy.

In a 1999 news story, "BBC" reported, "Close-knit family life in India masks an alarming amount of sexual abuse of children and teenage girls by family members, a new report suggests. Delhi organisation RAHI said 76% of respondents to its survey had been abused when they were children—40% of those by a family member."

According to the National Center for Victims of Crime a large proportion of rape committed in the United States is perpetrated by a family member:
A study of victims of father–daughter incest in the 1970s showed that there were "common features" within families before the occurrence of incest: estrangement between the mother and the daughter, extreme paternal dominance, and reassignment of some of the mother's traditional major family responsibility to the daughter. Oldest and only daughters were more likely to be the victims of incest. It was also stated that the incest experience was psychologically harmful to the woman in later life, frequently leading to feelings of low self-esteem, very unhealthy sexual activity, contempt for other women, and other emotional problems.

Adults who as children were incestuously victimized by adults often suffer from low self-esteem, difficulties in interpersonal relationships, and sexual dysfunction, and are at an extremely high risk of many mental disorders, including depression, anxiety disorders, phobic avoidance reactions, somatoform disorder, substance abuse, borderline personality disorder, and complex post-traumatic stress disorder. 

The Goler clan in Nova Scotia is a specific instance in which child sexual abuse in the form of forced adult/child and sibling/sibling incest took place over at least three generations. A number of Goler children were victims of sexual abuse at the hands of fathers, mothers, uncles, aunts, sisters, brothers, cousins, and each other. During interrogation by police, several of the adults openly admitted to engaging in many forms of sexual activity, up to and including full intercourse, multiple times with the children. Sixteen adults (both men and women) were charged with hundreds of allegations of incest and sexual abuse of children as young as five. In July 2012, twelve children were removed from the 'Colt' family (a pseudonym) in New South Wales, Australia, after the discovery of four generations of incest. Child protection workers and psychologists said interviews with the children indicated "a virtual sexual free-for-all".

In Japan, there is a popular misconception that mother-son incestuous contact is common, due to the manner in which it is depicted in the press and popular media. According to Hideo Tokuoka, "When Americans think of incest, they think of fathers and daughters; in Japan one thinks of mothers and sons" due to the extensive media coverage of mother-son incest there. Some western researchers assumed that mother-son incest is common in Japan, but research into victimization statistics from police and health-care systems discredits this; it shows that the vast majority of sexual abuse, including incest, in Japan is perpetrated by men against young girls.

While incest between adults and children generally involves the adult as the perpetrator of abuse, there are rare instances of sons sexually assaulting their mothers. These sons are typically mid adolescent to young adult, and, unlike parent-initiated incest, the incidents involve some kind of physical force. Although the mothers may be accused of being seductive with their sons and inviting the sexual contact, this is contrary to evidence. Such accusations can parallel other forms of rape, where, due to victim blaming, a woman is accused of somehow being at fault for the rape. In some cases, mother-son incest is best classified as acquaintance rape of the mother by the adolescent son.

Childhood sibling–sibling incest is considered to be widespread but rarely reported. Sibling–sibling incest becomes child-on-child sexual abuse when it occurs without consent, without equality, or as a result of coercion. In this form, it is believed to be the most common form of intrafamilial abuse. The most commonly reported form of abusive sibling incest is abuse of a younger sibling by an older sibling. A 2006 study showed a large portion of adults who experienced sibling incest abuse have "distorted" or "disturbed" beliefs (such as that the act was "normal") both about their own experience and the subject of sexual abuse in general.

Sibling abusive incest is most prevalent in families where one or both parents are often absent or emotionally unavailable, with the abusive siblings using incest as a way to assert their power over a weaker sibling. Absence of the father in particular has been found to be a significant element of most cases of sexual abuse of female children by a brother. The damaging effects on both childhood development and adult symptoms resulting from brother–sister sexual abuse are similar to the effects of father–daughter, including substance abuse, depression, suicidality, and eating disorders.

Sexual activity between adult close relatives is sometimes ascribed to genetic sexual attraction. This form of incest has not been widely reported, but evidence has indicated that this behavior does take place, possibly more often than many people realize. Internet chatrooms and topical websites exist that provide support for incestuous couples.

Proponents of incest between consenting adults draw clear boundaries between the behavior of consenting adults and rape, child molestation, and abusive incest. However, even consensual relationships such as these are still legally classified as incest, and criminalized in many jurisdictions (although there are certain exceptions). James Roffee, a senior lecturer in criminology at Monash University and former worker on legal responses to familial sexual activity in England and Wales, and Scotland, discussed how the European Convention on Human Rights deems all familial sexual acts to be criminal, even if all parties give their full consent and are knowledgeable to all possible consequences. He also argues that the use of particular language tools in the legislation manipulates the reader to deem all familial sexual activities as immoral and criminal, even if all parties are consenting adults.

According to one incest participant who was quoted for an article in "The Guardian":

In "Slate", William Saletan drew a legal connection between gay sex and incest between consenting adults. As he described in his article, in 2003, U.S. Senator Rick Santorum commented on a pending U.S. Supreme Court case involving sodomy laws (primarily as a matter of constitutional rights to privacy and equal protection under the law):

Saletan argued that, legally and morally, there is essentially no difference between the two, and went on to support incest between consenting adults being covered by a legal right to privacy. UCLA law professor Eugene Volokh has made similar arguments. In a more recent article, Saletan said that incest is wrong because it introduces the possibility of irreparably damaging family units by introducing "a notoriously incendiary dynamic—sexual tension—into the mix".

In the Netherlands, marrying one's nephew or niece is legal, but only with the explicit permission of the Dutch Government, due to the possible risk of genetic defects among the offspring. Nephew-niece marriages predominantly occur among foreign immigrants. In November 2008, the Christian Democratic (CDA) party's Scientific Institute announced that it wanted a ban on marriages to nephews and nieces.

Consensual sex between adults (persons of 18 years and older) is always lawful in the Netherlands and Belgium, even among closely related family members. Sexual acts between an adult family member and a minor are illegal, though they are not classified as incest, but as abuse of the authority such an adult has over a minor, comparable to that of a teacher, coach or priest.

In Florida, consensual adult sexual intercourse with someone known to be your aunt, uncle, niece or nephew constitutes a felony of the third degree. Other states also commonly prohibit marriages between such kin. The legality of sex with a half-aunt or half-uncle varies state by state.

In the United Kingdom, incest includes only sexual intercourse with a parent, grandparent, child or sibling, but the more recently introduced offence of "sex with an adult relative" extends also as far as half-siblings, uncles, aunts, nephews and nieces. However, the term 'incest' remains widely used in popular culture to describe any form of sexual activity with a relative. In Canada, marriage between uncles and nieces and between aunts and nephews is legal.

The most public case of consensual adult sibling incest in recent years is the case of a brother-sister couple from Germany, Patrick Stübing and Susan Karolewski. Because of violent behavior on the part of his father, Patrick was taken in at the age of 3 by foster parents, who adopted him later. At the age of 23 he learned about his biological parents, contacted his mother, and met her and his then 16-year-old sister Susan for the first time. The now-adult Patrick moved in with his birth family shortly thereafter. After their mother died suddenly six months later, the siblings became intimately close, and had their first child together in 2001. By 2004, they had four children together: Eric, Sarah, Nancy, and Sofia. The public nature of their relationship, and the repeated prosecutions and even jail time they have served as a result, has caused some in Germany to question whether incest between consenting adults should be punished at all. An article about them in "Der Spiegel" states that the couple are happy together. According to court records, the first three children have mental and physical disabilities, and have been placed in foster care. In April 2012, at the European Court of Human Rights, Patrick Stübing lost his case that the conviction violated his right to a private and family life. On September 24, 2014, the German Ethics Council has recommended that the government abolish laws criminalizing incest between siblings, arguing that such bans impinge upon citizens.

Some societies differentiate between full sibling and half sibling relations. In ancient societies, full sibling and half sibling marriages occurred.

Marriages and sexual relationships between first cousins are stigmatized as incest in some cultures, but tolerated in much of the world. Currently, 24 US states prohibit marriages between first cousins, and another seven permit them only under special circumstances.
The United Kingdom permits both marriage and sexual relations between first cousins.

In some non-Western societies, marriages between close biological relatives account for 20% to 60% of all marriages.

First- and second-cousin marriages are rare, accounting for less than 1% of marriages in Western Europe, North America and Oceania, while reaching 9% in South America, East Asia and South Europe and about 50% in regions of the Middle East, North Africa and South Asia. Communities such as the Dhond and the Bhittani of Pakistan clearly prefer marriages between cousins as belief they ensure purity of the descent line, provide intimate knowledge of the spouses, and ensure that patrimony will not pass into the hands of "outsiders". Cross-cousin marriages are preferred among the Yanomami of Brazilian Amazonia, among many other tribal societies identified by anthropologists.

There are some cultures in Asia which stigmatize cousin marriage, in some instances even marriages between second cousins or more remotely related people. This is notably true in the culture of Korea. In South Korea, before 1997, anyone with the same last name and clan were prohibited from marriage. In light of this law being held unconstitutional, South Korea now only prohibits up to third cousins (see Article 809 of the Korean Civil Code). Hmong culture prohibits the marriage of anyone with the same last name – to do so would result in being shunned by the entire community, and they are usually stripped of their last name. Some Hindu communities in India prohibit cousin marriages.

In a review of 48 studies on the children parented by cousins, the rate of birth defects was twice that of non-related couples: 4% for cousin couples as opposed to 2% for the general population.

Some cultures include relatives by marriage in incest prohibitions; these relationships are called affinity rather than consanguinity. For example, the question of the legality and morality of a widower who wished to marry his deceased wife's sister was the subject of long and fierce debate in the United Kingdom in the 19th century, involving, among others, Matthew Boulton and Charles La Trobe. The marriages were entered into in Scotland and Switzerland respectively, where they were legal. In medieval Europe, standing as a godparent to a child also created a bond of affinity. But in other societies, a deceased spouse's sibling was considered the ideal person to marry. The Hebrew Bible forbids a man from marrying his brother's widow with the exception that, if his brother died childless, the man is instead required to marry his brother's widow so as to "raise up seed to him" (per ). Some societies have long practiced sororal polygyny, a form of polygamy in which a man marries multiple wives who are sisters to each other (though not closely related to him).

In Islamic law, marriage among close blood relations like parents, stepparent, parents in-law, siblings, stepsiblings, the children of siblings, aunts and uncles is forbidden, while first or second cousins may marry. Marrying the widow of a brother, or the sister of deceased or divorced wife is also allowed.

Offspring of biologically related parents are subject to the possible impact of inbreeding. Such offspring have a higher possibility of congenital birth defects (see Coefficient of relationship) because it increases the proportion of zygotes that are homozygous for deleterious recessive alleles that produce such disorders (see Inbreeding depression). Because most such alleles are rare in populations, it is unlikely that two unrelated marriage partners will both be heterozygous carriers. However, because close relatives share a large fraction of their alleles, the probability that any such rare deleterious allele present in the common ancestor will be inherited from both related parents is increased dramatically with respect to non-inbred couples. Contrary to common belief, inbreeding does not in itself alter allele frequencies, but rather increases the relative proportion of homozygotes to heterozygotes. This has two contrary effects.
The closer two persons are related, the higher the zygosity, and thus the more severe the biological costs of inbreeding. This fact likely explains why inbreeding between close relatives, such as siblings, is less common than inbreeding between cousins.

There may also be other deleterious effects besides those caused by recessive diseases. Thus, similar immune systems may be more vulnerable to infectious diseases (see Major histocompatibility complex and sexual selection).

A 1994 study found a mean excess mortality with inbreeding among first cousins of 4.4%. Children of parent-child or sibling-sibling unions are at increased risk compared to cousin-cousin unions. Studies suggest that 20-36% of these children will die or have major disability due to the inbreeding. A study of 29 offspring resulting from brother-sister or father-daughter incest found that 20 had congenital abnormalities, including four directly attributable to autosomal recessive alleles.

Laws regarding sexual activity between close relatives vary considerably between jurisdictions, and depend on the type of sexual activity and the nature of the family relationship of the parties involved, as well as the age and sex of the parties. Prohibition of incest laws may extend to restrictions on marriage rights, which also vary between jurisdictions. Most jurisdictions prohibit parent-child and sibling marriages, while others also prohibit first-cousin and uncle-niece and aunt-nephew marriages. In most places, incest is illegal, regardless of the ages of the two partners. In other countries, incestuous relationships between consenting adults (with the age varying by location) are permitted, including in the Netherlands, France, Slovenia and Spain. Sweden is the only country that allows marriage between half-siblings and they must seek government counseling before marriage.

While the legality of consensual incest varies by country, sexual assault committed against a relative is usually seen as a very serious crime. In some legal systems, the fact of a perpetrator being a close relative to the victim constitutes an aggravating circumstance in the case of sexual crimes such as rape and sexual conduct with a minor – this is the case in Romania.

According to the Torah, per , "the children of Israel"—Israelite men and women alike—are forbidden from sexual relations between people who are "near of kin" (verse 6), who are defined as:


And Moses commanded the children of Israel according to the word of the LORD, saying: 'The tribe of the sons of Joseph speaketh right. This is the thing which the LORD hath commanded concerning the daughters of Zelophehad, saying: Let them be married to whom they think best; only into the family of the tribe of their father shall they be married. So shall no inheritance of the children of Israel remove from tribe to tribe; for the children of Israel shall cleave every one to the inheritance of the tribe of his fathers. And every daughter, that possesseth an inheritance in any tribe of the children of Israel, shall be wife unto one of the family of the tribe of her father, that the children of Israel may possess every man the inheritance of his fathers. So shall no inheritance remove from one tribe to another tribe; for the tribes of the children of Israel shall cleave each one to its own inheritance.' Even as the LORD commanded Moses, so did the daughters of Zelophehad. For Mahlah, Tirzah, and Hoglah, and Milcah, and Noah, the daughters of Zelophehad, were married unto their father's brothers' sons. ()

Incestuous relationships are considered so severe among "chillulim HaShem", acts which bring shame to the name of God, as to be, along with the other forbidden relationships that are mentioned in Leviticus 18, punishable by death as specified in Leviticus 20.

In the 4th century BCE, the Soferim ("scribes") declared that there were relationships within which marriage constituted incest, in addition to those mentioned by the Torah. These additional relationships were termed "seconds" (Hebrew: "sheniyyot"), and included the wives of a man's grandfather and grandson. The classical rabbis prohibited marriage between a man and any of these "seconds" of his, on the basis that doing so would act as a "safeguard" against infringing the biblical incest rules, although there was inconclusive debate about exactly what the limits should be for the definition of "seconds".

Marriages that are forbidden in the Torah (with the exception of uncle-niece marriages) were regarded by the rabbis of the Middle Ages as invalid – as if they had never occurred; any children born to such a couple were regarded as bastards under Jewish law, and the relatives of the spouse were not regarded as forbidden relations for a further marriage. On the other hand, those relationships which were prohibited due to qualifying as "seconds", and so forth, were regarded as wicked, but still valid; while they might have pressured such a couple to divorce, any children of the union were still seen as legitimate.

The Catholic Church regards incest as a sin against the Sacrament of Matrimony. For the Catholic Church, at the heart of the immorality of incest is the corruption and disordering of proper family relations. These disordered relationships take on a particularly grave and immoral character when it becomes child sexual abuse. 

As the "Catechism of the Catholic Church" says: 2388 "Incest" designates intimate relations between relatives or in-laws within a degree that prohibits marriage between them. St. Paul stigmatizes this especially grave offense: 'It is actually reported that there is immorality among you...for a man is living with his father's wife...In the name of the Lord Jesus...you are to deliver this man to Satan for the destruction of the flesh...' Incest corrupts family relationships and marks a regression toward animality.

2389 Connected to incest is any sexual abuse perpetrated by adults on children or adolescents entrusted to their care. The offense is compounded by the scandalous harm done to the physical and moral integrity of the young, who will remain scarred by it all their lives; and the violation of responsibility for their upbringing. 

The Book of Common Prayer of the Anglican Communion allows marriages up to and including first cousins.

The Quran gives specific rules regarding incest, which prohibit a man from marrying or having sexual relationships with:


Cousin marriage finds support in Islamic scriptures and is widespread in the Middle East.

Although Islam allows cousin marriage, there are Hadiths attributed to Muhammad calling for distance from the marriage of relatives.

In Ancient Persia, incest between cousins is a blessed virtue although in some sources incest is believed to be related to that of parent-child or brothers-sisters. Under Zoroastrianism royalty, clergy, and commoners practiced incest, though the extent in the latter class was unknown. This tradition was called Xwedodah (). The tradition was considered so sacred, that the bodily fluids produced by an incestuous couple were thought to have curative powers. For instance, the Vendidad advised corpse-bearers to purify themselves with a mixture of urine of a married incestuous couple. Friedrich Nietzsche, in his book "The Birth of Tragedy", cited that among Zoroastrians a wise priest is born only by Xvaetvadatha.

To what extent Xvaetvadatha was practiced in Sasanian Iran and before, especially outside the royal and noble families (“dynastic incest”) and, perhaps, the clergy, and whether practices ascribed to them can be assumed to be characteristic of the general population is not clear. There is a lack of genealogies and census material on the frequency of Xvaetvadatha. Evidence from Dura-Europos, however, combined with that of the Jewish and Christian sources citing actual cases under the Sasanians, strengthen the evidence of the Zoroastrian texts. In the post-Sasanian Zoroastrian literature, Xvaetvadatha is said to refer to marriages between cousins instead, which have always been relatively common. It has been observed that such incestuous acts received a great deal of glorification as a religious practice and, in addition to being condemned by foreigners (though the reliability of these accusations is questionable since accusations of incest were a common way of denigrating other groups), were considered a great challenge by its own proponents, with accounts suggesting that four copulations was deemed a rare achievement worthy of eternal salvation. It has been suggested that because taking up incestuous relations was a great personal challenge, seemingly repugnant even to Zoroastrians of the time, that it served as an honest signal of commitment and devotion to religious ideals.

Rigveda regard incest to be "evil". Hinduism speaks of incest in abhorrent terms. Hindus believe there are both karmic and practical bad effects of incest and thus practice strict rules of both endogamy and exogamy, in relation to the family tree ("gotra") or bloodline ("Pravara").
Marriage within the "gotra" ("swagotra" marriages) are banned under the rule of exogamy in the traditional matrimonial system. People within the "gotra" are regarded as kin and marrying such a person would be thought of as incest. Marriage with paternal cousins (a form of parallel-cousin relationship) is strictly prohibited.

Although generally marriages between persons having the same "gotra" are prohibited, how this is defined may vary regionally.
Depending on culture and caste of the population in the region, marriage may be restricted up to seven generations of "gotra" of father, mother, and grandmother. In a few rural areas, marriage is banned within same local community is only allowed with those from outside of the community, as they consider a small village to be like brothers and sisters of one large family. These rules are strictly enforced and a couple breaking them is violently punished sometimes.

Many species of mammals, including humanity's closest primate relatives, tend to avoid mating with close relatives, especially if there are alternative partners available. However, some chimpanzees have been recorded attempting to mate with their mothers. Male rats have been recorded engaging in mating with their sisters, but they tend to prefer non-related females over their sisters.

Livestock breeders often practice controlled breeding to eliminate undesirable characteristics within a population, which is also coupled with culling of what is considered unfit offspring, especially when trying to establish a new and desirable trait in the stock.

North Carolina State University found that bed bugs, in contrast to most other insects, tolerate incest and are able to genetically withstand the effects of inbreeding quite well.




</doc>
<doc id="14914" url="https://en.wikipedia.org/wiki?curid=14914" title="Industrial Revolution">
Industrial Revolution

The Industrial Revolution, now also known as the First Industrial Revolution, was the transition to new manufacturing processes in Europe and the United States, in the period from about 1760 to sometime between 1820 and 1840. This transition included going from hand production methods to machines, new chemical manufacturing and iron production processes, the increasing use of steam power and water power, the development of machine tools and the rise of the mechanized factory system. The Industrial Revolution also led to an unprecedented rise in the rate of population growth.

Textiles were the dominant industry of the Industrial Revolution in terms of employment, value of output and capital invested. The textile industry was also the first to use modern production methods.

The Industrial Revolution began in Great Britain, and many of the technological innovations were of British origin. By the mid-18th century Britain was the world's leading commercial nation, controlling a global trading empire with colonies in North America and the Caribbean, and with major military and political hegemony on the Indian subcontinent, particularly with the proto-industrialised Mughal Bengal, through the activities of the East India Company. The development of trade and the rise of business were among the major causes of the Industrial Revolution.

The Industrial Revolution marks a major turning point in history; almost every aspect of daily life was influenced in some way. In particular, average income and population began to exhibit unprecedented sustained growth. Some economists say that the major effect of the Industrial Revolution was that the standard of living for the general population in the western world began to increase consistently for the first time in history, although others have said that it did not begin to meaningfully improve until the late 19th and 20th centuries.

GDP per capita was broadly stable before the Industrial Revolution and the emergence of the modern capitalist economy, while the Industrial Revolution began an era of per-capita economic growth in capitalist economies. Economic historians are in agreement that the onset of the Industrial Revolution is the most important event in the history of humanity since the domestication of animals and plants. Although the structural change from agriculture to industry is widely associated with the Industrial Revolution, in the United Kingdom it was already almost complete by 1760.

The precise start and end of the Industrial Revolution is still debated among historians, as is the pace of economic and social changes. Eric Hobsbawm held that the Industrial Revolution began in Britain in the 1780s and was not fully felt until the 1830s or 1840s, while T. S. Ashton held that it occurred roughly between 1760 and 1830. Rapid industrialization first began in Britain, starting with mechanized spinning in the 1780s, with high rates of growth in steam power and iron production occurring after 1800. Mechanized textile production spread from Great Britain to continental Europe and the United States in the early 19th century, with important centres of textiles, iron and coal emerging in Belgium and the United States and later textiles in France.

An economic recession occurred from the late 1830s to the early 1840s when the adoption of the original innovations of the Industrial Revolution, such as mechanized spinning and weaving, slowed and their markets matured. Innovations developed late in the period, such as the increasing adoption of locomotives, steamboats and steamships, hot blast iron smelting and new technologies, such as the electrical telegraph, widely introduced in the 1840s and 1850s, were not powerful enough to drive high rates of growth. Rapid economic growth began to occur after 1870, springing from a new group of innovations in what has been called the Second Industrial Revolution. These innovations included new steel making processes, mass-production, assembly lines, electrical grid systems, the large-scale manufacture of machine tools and the use of increasingly advanced machinery in steam-powered factories.

The earliest recorded use of the term "Industrial Revolution" seems to have been in a letter from 6 July 1799 written by French envoy Louis-Guillaume Otto, announcing that France had entered the race to industrialise. In his 1976 book "", Raymond Williams states in the entry for "Industry": "The idea of a new social order based on major industrial change was clear in Southey and Owen, between 1811 and 1818, and was implicit as early as Blake in the early 1790s and Wordsworth at the turn of the [19th] century." The term "Industrial Revolution" applied to technological change was becoming more common by the late 1830s, as in Jérôme-Adolphe Blanqui's description in 1837 of "la révolution industrielle". Friedrich Engels in "The Condition of the Working Class in England" in 1844 spoke of "an industrial revolution, a revolution which at the same time changed the whole of civil society". However, although Engels wrote in the 1840s, his book was not translated into English until the late 1800s, and his expression did not enter everyday language until then. Credit for popularising the term may be given to Arnold Toynbee, whose 1881 lectures gave a detailed account of the term.

Economic historians and authors such as Mendels, Pomeranz and Kridte argue that the proto-industrialization in parts of Europe, Islamic world, Mughal India, and China created the social and economic conditions that led to the Industrial Revolution, thus causing the Great Divergence.

Some historians, such as John Clapham and Nicholas Crafts, have argued that the economic and social changes occurred gradually and the term "revolution" is a misnomer. This is still a subject of debate among some historians.

The commencement of the Industrial Revolution is closely linked to a small number of innovations, beginning in the second half of the 18th century. By the 1830s the following gains had been made in important technologies:

In 1750 Britain imported 2.5 million pounds of raw cotton, most of which was spun and woven by cottage industry in Lancashire. The work was done by hand in workers' homes or occasionally in shops of master weavers. In 1787 raw cotton consumption was 22 million pounds, most of which was cleaned, carded and spun on machines. The British textile industry used 52 million pounds of cotton in 1800, which increased to 588 million pounds in 1850.

The share of value added by the cotton textile industry in Britain was 2.6% in 1760, 17% in 1801 and 22.4% in 1831. Value added by the British woollen industry was 14.1% in 1801. Cotton factories in Britain numbered approximately 900 in 1797. In 1760 approximately one-third of cotton cloth manufactured in Britain was exported, rising to two-thirds by 1800. In 1781 cotton spun amounted to 5.1 million pounds, which increased to 56 million pounds by 1800. In 1800 less than 0.1% of world cotton cloth was produced on machinery invented in Britain. In 1788 there were 50,000 spindles in Britain, rising to 7 million over the next 30 years.

Wages in Lancashire, a core region for cottage industry and later factory spinning and weaving, were about six times those in India in 1770, when overall productivity in Britain was about three times higher than in India.

Parts of India, China, Central America, South America and the Middle-East have a long history of hand manufacturing cotton textiles, which became a major industry sometime after 1000 AD. In tropical and subtropical regions where it was grown, most was grown by small farmers alongside their food crops and was spun and woven in households, largely for domestic consumption. In the 15th century China began to require households to pay part of their taxes in cotton cloth. By the 17th century almost all Chinese wore cotton clothing. Almost everywhere cotton cloth could be used as a medium of exchange. In India a significant amount of cotton textiles were manufactured for distant markets, often produced by professional weavers. Some merchants also owned small weaving workshops. India produced a variety of cotton cloth, some of exceptionally fine quality.

Cotton was a difficult raw material for Europe to obtain before it was grown on colonial plantations in the Americas. The early Spanish explorers found Native Americans growing unknown species of excellent quality cotton: sea island cotton ("Gossypium barbadense") and upland green seeded cotton "Gossypium hirsutum". Sea island cotton grew in tropical areas and on barrier islands of Georgia and South Carolina, but did poorly inland. Sea island cotton began being exported from Barbados in the 1650s. Upland green seeded cotton grew well on inland areas of the southern U.S., but was not economical because of the difficulty of removing seed, a problem solved by the cotton gin. A strain of cotton seed brought from Mexico to Natchez, Mississippi in 1806 became the parent genetic material for over 90% of world cotton production today; it produced bolls that were three to four times faster to pick.

The Age of Discovery was followed by a period of colonialism beginning around the 16th century. Following the discovery of a trade route to India around southern Africa by the Portuguese, the Dutch established the Verenigde Oostindische Compagnie (abbr. VOC) or Dutch East India Company, the world's first transnational corporation and the first multinational enterprise to issue shares of stock to the public. The British later founded the East India Company, along with smaller companies of different nationalities which established trading posts and employed agents to engage in trade throughout the Indian Ocean region and between the Indian Ocean region and North Atlantic Europe. One of the largest segments of this trade was in cotton textiles, which were purchased in India and sold in Southeast Asia, including the Indonesian archipelago, where spices were purchased for sale to Southeast Asia and Europe. By the mid-1760s cloth was over three-quarters of the East India Company's exports. Indian textiles were in demand in North Atlantic region of Europe where previously only wool and linen were available; however, the amount of cotton goods consumed in Western Europe was minor until the early 19th century.

By 1600 Flemish refugees began weaving cotton cloth in English towns where cottage spinning and weaving of wool and linen was well established; however, they were left alone by the guilds who did not consider cotton a threat. Earlier European attempts at cotton spinning and weaving were in 12th-century Italy and 15th-century southern Germany, but these industries eventually ended when the supply of cotton was cut off. The Moors in Spain grew, spun and wove cotton beginning around the 10th century.

British cloth could not compete with Indian cloth because India's labour cost was approximately one-fifth to one-sixth that of Britain's. In 1700 and 1721 the British government passed Calico Acts in order to protect the domestic woollen and linen industries from the increasing amounts of cotton fabric imported from India.

The demand for heavier fabric was met by a domestic industry based around Lancashire that produced fustian, a cloth with flax warp and cotton weft. Flax was used for the warp because wheel-spun cotton did not have sufficient strength, but the resulting blend was not as soft as 100% cotton and was more difficult to sew.

On the eve of the Industrial Revolution, spinning and weaving were done in households, for domestic consumption and as a cottage industry under the putting-out system. Occasionally the work was done in the workshop of a master weaver. Under the putting-out system, home-based workers produced under contract to merchant sellers, who often supplied the raw materials. In the off season the women, typically farmers' wives, did the spinning and the men did the weaving. Using the spinning wheel, it took anywhere from four to eight spinners to supply one hand loom weaver.

The flying shuttle, patented in 1733 by John Kay, with a number of subsequent improvements including an important one in 1747, doubled the output of a weaver, worsening the imbalance between spinning and weaving. It became widely used around Lancashire after 1760 when John's son, Robert, invented the drop box, which facilitated changing thread colors.

Lewis Paul patented the roller spinning frame and the flyer-and-bobbin system for drawing wool to a more even thickness. The technology was developed with the help of John Wyatt of Birmingham. Paul and Wyatt opened a mill in Birmingham which used their new rolling machine powered by a donkey. In 1743 a factory opened in Northampton with 50 spindles on each of five of Paul and Wyatt's machines. This operated until about 1764. A similar mill was built by Daniel Bourn in Leominster, but this burnt down. Both Lewis Paul and Daniel Bourn patented carding machines in 1748. Based on two sets of rollers that travelled at different speeds, it was later used in the first cotton spinning mill. Lewis's invention was later developed and improved by Richard Arkwright in his water frame and Samuel Crompton in his spinning mule.

In 1764 in the village of Stanhill, Lancashire, James Hargreaves invented the spinning jenny, which he patented in 1770. It was the first practical spinning frame with multiple spindles. The jenny worked in a similar manner to the spinning wheel, by first clamping down on the fibres, then by drawing them out, followed by twisting. It was a simple, wooden framed machine that only cost about £6 for a 40-spindle model in 1792, and was used mainly by home spinners. The jenny produced a lightly twisted yarn only suitable for weft, not warp.

The spinning frame or water frame was developed by Richard Arkwright who, along with two partners, patented it in 1769. The design was partly based on a spinning machine built for Thomas High by clockmaker John Kay, who was hired by Arkwright. For each spindle the water frame used a series of four pairs of rollers, each operating at a successively higher rotating speed, to draw out the fibre, which was then twisted by the spindle. The roller spacing was slightly longer than the fibre length. Too close a spacing caused the fibres to break while too distant a spacing caused uneven thread. The top rollers were leather-covered and loading on the rollers was applied by a weight. The weights kept the twist from backing up before the rollers. The bottom rollers were wood and metal, with fluting along the length. The water frame was able to produce a hard, medium count thread suitable for warp, finally allowing 100% cotton cloth to be made in Britain. A horse powered the first factory to use the spinning frame. Arkwright and his partners used water power at a factory in Cromford, Derbyshire in 1771, giving the invention its name.

Samuel Crompton's Spinning Mule was introduced in 1779. Mule implies a hybrid because it was a combination of the spinning jenny and the water frame, in which the spindles were placed on a carriage, which went through an operational sequence during which the rollers stopped while the carriage moved away from the drawing roller to finish drawing out the fibres as the spindles started rotating. Crompton's mule was able to produce finer thread than hand spinning and at a lower cost. Mule spun thread was of suitable strength to be used as warp, and finally allowed Britain to produce highly competitive yarn in large quantities.

Realising that the expiration of the Arkwright patent would greatly increase the supply of spun cotton and led to a shortage of weavers, Edmund Cartwright developed a vertical power loom which he patented in 1785. In 1776 he patented a two-man operated loom which was more conventional. Cartwright built two factories; the first burned down and the second was sabotaged by his workers. Cartwright's loom design had several flaws, the most serious being thread breakage. Samuel Horrocks patented a fairly successful loom in 1813. Horock's loom was improved by Richard Roberts in 1822 and these were produced in large numbers by Roberts, Hill & Co.

The demand for cotton presented an opportunity to planters in the Southern United States, who thought upland cotton would be a profitable crop if a better way could be found to remove the seed. Eli Whitney responded to the challenge by inventing the inexpensive cotton gin. A man using a cotton gin could remove seed from as much upland cotton in one day as would previously, working at the rate of one pound of cotton per day, have taken a woman two months to process.

These advances were capitalised on by entrepreneurs, of whom the best known is Richard Arkwright. He is credited with a list of inventions, but these were actually developed by such people as Thomas Highs and John Kay; Arkwright nurtured the inventors, patented the ideas, financed the initiatives, and protected the machines. He created the cotton mill which brought the production processes together in a factory, and he developed the use of powerfirst horse power and then water powerwhich made cotton manufacture a mechanised industry. Other inventors increased the efficiency of the individual steps of spinning (carding, twisting and spinning, and rolling) so that the supply of yarn increased greatly. Before long steam power was applied to drive textile machinery. Manchester acquired the nickname Cottonopolis during the early 19th century owing to its sprawl of textile factories.

Although mechanization dramatically decreased the cost of cotton cloth, by the mid-19th century machine-woven cloth still could not equal the quality of hand-woven Indian cloth, in part due to the fineness of thread made possible by the type of cotton used in India, which allowed high thread counts. However, the high productivity of British textile manufacturing allowed coarser grades of British cloth to undersell hand-spun and woven fabric in low-wage India, eventually destroying the industry.

The earliest European attempts at mechanized spinning were with wool; however, wool spinning proved more difficult to mechanize than cotton. Productivity improvement in wool spinning during the Industrial Revolution was significant but was far less than that of cotton.

Arguably the first highly mechanised factory was John Lombe's water-powered silk mill at Derby, operational by 1721. Lombe learned silk thread manufacturing by taking a job in Italy and acting as an industrial spy; however, because the Italian silk industry guarded its secrets closely, the state of the industry at that time is unknown. Although Lombe's factory was technically successful, the supply of raw silk from Italy was cut off to eliminate competition. In order to promote manufacturing the Crown paid for models of Lombe's machinery which were exhibited in the Tower of London.

Bar iron was the commodity form of iron used as the raw material for making hardware goods such as nails, wire, hinges, horse shoes, wagon tires, chains, etc. and for structural shapes. A small amount of bar iron was converted into steel. Cast iron was used for pots, stoves and other items where its brittleness was tolerable. Most cast iron was refined and converted to bar iron, with substantial losses. Bar iron was also made by the bloomery process, which was the predominant iron smelting process until the late 18th century.

In the UK in 1720 there were 20,500 tons of cast iron produced with charcoal and 400 tons with coke. In 1750 charcoal iron production was 24,500 and coke iron was 2,500 tons. In 1788 the production of charcoal cast iron was 14,000 tons while coke iron production was 54,000 tons. In 1806 charcoal cast iron production was 7,800 tons and coke cast iron was 250,000 tons.

In 1750 the UK imported 31,200 tons of bar iron and either refined from cast iron or directly produced 18,800 tons of bar iron using charcoal and 100 tons using coke. In 1796 the UK was making 125,000 tons of bar iron with coke and 6,400 tons with charcoal; imports were 38,000 tons and exports were 24,600 tons. In 1806 the UK did not import bar iron but exported 31,500 tons.

A major change in the iron industries during the era of the Industrial Revolution was the replacement of wood and other bio-fuels with coal. For a given amount of heat, coal required much less labour to mine than cutting wood and converting it to charcoal, and coal was much more abundant than wood, supplies of which were becoming scarce before the enormous increase in iron production that took place in the late 18th century. By 1750 coke had generally replaced charcoal in smelting of copper and lead, and was in widespread use in making glass. In the smelting and refining of iron, coal and coke produced inferior iron to that made with charcoal because of the coal's sulfur content. Low sulfur coals were known, but they still contained harmful amounts. Conversion of coal to coke only slightly reduces the sulfur content. A minority of coals are coking.

Another factor limiting the iron industry before the Industrial Revolution was the scarcity of water power to power blast bellows. This limitation was overcome by the steam engine.

Use of coal in iron smelting started somewhat before the Industrial Revolution, based on innovations by Sir Clement Clerke and others from 1678, using coal reverberatory furnaces known as cupolas. These were operated by the flames playing on the ore and charcoal or coke mixture, reducing the oxide to metal. This has the advantage that impurities (such as sulphur ash) in the coal do not migrate into the metal. This technology was applied to lead from 1678 and to copper from 1687. It was also applied to iron foundry work in the 1690s, but in this case the reverberatory furnace was known as an air furnace. (The foundry cupola is a different, and later, innovation.)

By 1709 Abraham Darby made progress using coke to fuel his blast furnaces at Coalbrookdale. However, the coke pig iron he made was not suitable for making wrought iron and was used mostly for the production of cast iron goods, such as pots and kettles. He had the advantage over his rivals in that his pots, cast by his patented process, were thinner and cheaper than theirs.

Coke pig iron was hardly used to produce wrought iron until 1755–56, when Darby's son Abraham Darby II built furnaces at Horsehay and Ketley where low sulfur coal was available (and not far from Coalbrookdale). These new furnaces were equipped with water-powered bellows, the water being pumped by Newcomen steam engines. The Newcomen engines were not attached directly to the blowing cylinders because the engines alone could not produce a steady air blast. Abraham Darby III installed similar steam-pumped, water-powered blowing cylinders at the Dale Company when he took control in 1768. The Dale Company used several Newcomen engines to drain its mines and made parts for engines which it sold throughout the country.

Steam engines made the use of higher-pressure and volume blast practical; however, the leather used in bellows was expensive to replace. In 1757, iron master John Wilkinson patented a hydraulic powered blowing engine for blast furnaces. The blowing cylinder for blast furnaces was introduced in 1760 and the first blowing cylinder made of cast iron is believed to be the one used at Carrington in 1768 that was designed by John Smeaton. Cast iron cylinders for use with a piston were difficult to manufacture; the cylinders had to be free of holes and had to be machined smooth and straight to remove any warping. James Watt had great difficulty trying to have a cylinder made for his first steam engine. In 1774 John Wilkinson, who built a cast iron blowing cylinder for his iron works, invented a precision boring machine for boring cylinders. After Wilkinson bored the first successful cylinder for a Boulton and Watt steam engine in 1776, he was given an exclusive contract for providing cylinders. After Watt developed a rotary steam engine in 1782, they were widely applied to blowing, hammering, rolling and slitting.

The solutions to the sulfur problem were the addition of sufficient limestone to the furnace to force sulfur into the slag and the use of low sulfur coal. Use of lime or limestone required higher furnace temperatures to form a free-flowing slag. The increased furnace temperature made possible by improved blowing also increased the capacity of blast furnaces and allowed for increased furnace height. In addition to lower cost and greater availability, coke had other important advantages over charcoal in that it was harder and made the column of materials (iron ore, fuel, slag) flowing down the blast furnace more porous and did not crush in the much taller furnaces of the late 19th century.

As cast iron became cheaper and widely available, it began being a structural material for bridges and buildings. A famous early example was the Iron Bridge built in 1778 with cast iron produced by Abraham Darby III. However, most cast iron was converted to wrought iron.

Europe relied on the bloomery for most of its wrought iron until the large scale production of cast iron. Conversion of cast iron was done in a finery forge, as it long had been. An improved refining process known as potting and stamping was developed, but this was superseded by Henry Cort's puddling process. Cort developed two significant iron manufacturing processes: rolling in 1783 and puddling in 1784. Puddling produced a structural grade iron at a relatively low cost.

Puddling was a means of decarburizing molten pig iron by slow oxidation in a reverberatory furnace by manually stirring it with a long rod. The decarburized iron, having a higher melting point than cast iron, was raked into globs by the puddler. When the glob was large enough, the puddler would remove it. Puddling was backbreaking and extremely hot work. Few puddlers lived to be 40. Because puddling was done in a reverberatory furnace, coal or coke could be used as fuel. The puddling process continued to be used until the late 19th century when iron was being displaced by steel. Because puddling required human skill in sensing the iron globs, it was never successfully mechanised. Rolling was an important part of the puddling process because the grooved rollers expelled most of the molten slag and consolidated the mass of hot wrought iron. Rolling was 15 times faster at this than a trip hammer. A different use of rolling, which was done at lower temperatures than that for expelling slag, was in the production of iron sheets, and later structural shapes such as beams, angles and rails.

The puddling process was improved in 1818 by Baldwyn Rogers, who replaced some of the sand lining on the reverberatory furnace bottom with iron oxide. In 1838 John Hall patented the use of roasted tap cinder (iron silicate) for the furnace bottom, greatly reducing the loss of iron through increased slag caused by a sand lined bottom. The tap cinder also tied up some phosphorus, but this was not understood at the time. Hall's process also used iron scale or rust, which reacted with carbon in the molten iron. Hall's process, called "wet puddling", reduced losses of iron with the slag from almost 50% to around 8%.

Puddling became widely used after 1800. Up to that time British iron manufacturers had used considerable amounts of iron imported from Sweden and Russia to supplement domestic supplies. Because of the increased British production, imports began to decline in 1785 and by the 1790s Britain eliminated imports and became a net exporter of bar iron.

Hot blast, patented by James Beaumont Neilson in 1828, was the most important development of the 19th century for saving energy in making pig iron. By using preheated combustion air, the amount of fuel to make a unit of pig iron was reduced at first by between one-third using coke or two-thirds using coal; however, the efficiency gains continued as the technology improved. Hot blast also raised the operating temperature of furnaces, increasing their capacity. Using less coal or coke meant introducing fewer impurities into the pig iron. This meant that lower quality coal or anthracite could be used in areas where coking coal was unavailable or too expensive; however, by the end of the 19th century transportation costs fell considerably.

Shortly before the Industrial Revolution an improvement was made in the production of steel, which was an expensive commodity and used only where iron would not do, such as for cutting edge tools and for springs. Benjamin Huntsman developed his crucible steel technique in the 1740s. The raw material for this was blister steel, made by the cementation process.

The supply of cheaper iron and steel aided a number of industries, such as those making nails, hinges, wire and other hardware items. The development of machine tools allowed better working of iron, causing it to be increasingly used in the rapidly growing machinery and engine industries.

The development of the stationary steam engine was an important element of the Industrial Revolution; however, during the early period of the Industrial Revolution, most industrial power was supplied by water and wind. In Britain by 1800 an estimated 10,000 horsepower was being supplied by steam. By 1815 steam power had grown to 210,000 hp.

The first commercially successful industrial use of steam power was due to Thomas Savery in 1698. He constructed and patented in London a low-lift combined vacuum and pressure water pump, that generated about one horsepower (hp) and was used in numerous water works and in a few mines (hence its "brand name", "The Miner's Friend"). Savery's pump was economical in small horsepower ranges, but was prone to boiler explosions in larger sizes. Savery pumps continued to be produced until the late 18th century.

The first successful piston steam engine was introduced by Thomas Newcomen before 1712. A number of Newcomen engines were installed in Britain for draining hitherto unworkable deep mines, with the engine on the surface; these were large machines, requiring a significant amount of capital to build, and produced upwards of . They were also used to power municipal water supply pumps. They were extremely inefficient by modern standards, but when located where coal was cheap at pit heads, opened up a great expansion in coal mining by allowing mines to go deeper. Despite their disadvantages, Newcomen engines were reliable and easy to maintain and continued to be used in the coalfields until the early decades of the 19th century. By 1729, when Newcomen died, his engines had spread (first) to Hungary in 1722, Germany, Austria, and Sweden. A total of 110 are known to have been built by 1733 when the joint patent expired, of which 14 were abroad. In the 1770s the engineer John Smeaton built some very large examples and introduced a number of improvements. A total of 1,454 engines had been built by 1800.
A fundamental change in working principles was brought about by Scotsman James Watt. With financial support from his business partner Englishman Matthew Boulton, he had succeeded by 1778 in perfecting his steam engine, which incorporated a series of radical improvements, notably the closing off of the upper part of the cylinder, thereby making the low-pressure steam drive the top of the piston instead of the atmosphere, use of a steam jacket and the celebrated separate steam condenser chamber. The separate condenser did away with the cooling water that had been injected directly into the cylinder, which cooled the cylinder and wasted steam. Likewise, the steam jacket kept steam from condensing in the cylinder, also improving efficiency. These improvements increased engine efficiency so that Boulton and Watt's engines used only 20–25% as much coal per horsepower-hour as Newcomen's. Boulton and Watt opened the Soho Foundry for the manufacture of such engines in 1795.

By 1783 the Watt steam engine had been fully developed into a double-acting rotative type, which meant that it could be used to directly drive the rotary machinery of a factory or mill. Both of Watt's basic engine types were commercially very successful, and by 1800, the firm Boulton & Watt had constructed 496 engines, with 164 driving reciprocating pumps, 24 serving blast furnaces, and 308 powering mill machinery; most of the engines generated from .

Until about 1800 the most common pattern of steam engine was the beam engine, built as an integral part of a stone or brick engine-house, but soon various patterns of self-contained rotative engines (readily removable, but not on wheels) were developed, such as the table engine. Around the start of the 19th century, at which time the Boulton and Watt patent expired, the Cornish engineer Richard Trevithick and the American Oliver Evans began to construct higher-pressure non-condensing steam engines, exhausting against the atmosphere. High pressure yielded an engine and boiler compact enough to be used on mobile road and rail locomotives and steam boats.

The development of machine tools, such as the engine lathe, planing, milling and shaping machines powered by these engines, enabled all the metal parts of the engines to be easily and accurately cut and in turn made it possible to build larger and more powerful engines.

Small industrial power requirements continued to be provided by animal and human muscle until widespread electrification in the early 20th century. These included crank-powered, treadle-powered and horse-powered workshop and light industrial machinery.

Pre-industrial machinery was built by various craftsmenmillwrights built water and windmills, carpenters made wooden framing, and smiths and turners made metal parts. Wooden components had the disadvantage of changing dimensions with temperature and humidity, and the various joints tended to rack (work loose) over time. As the Industrial Revolution progressed, machines with metal parts and frames became more common. Other important uses of metal parts were in firearms and threaded fasteners, such as machine screws, bolts and nuts. There was also the need for precision in making parts. Precision would allow better working machinery, interchangeability of parts and standardization of threaded fasteners.

The demand for metal parts led to the development of several machine tools. They have their origins in the tools developed in the 18th century by makers of clocks and watches and scientific instrument makers to enable them to batch-produce small mechanisms.

Before the advent of machine tools, metal was worked manually using the basic hand tools of hammers, files, scrapers, saws and chisels. Consequently, the use of metal machine parts was kept to a minimum. Hand methods of production were very laborious and costly and precision was difficult to achieve.

The first large precision machine tool was the cylinder boring machine invented by John Wilkinson in 1774. It was used for boring the large-diameter cylinders on early steam engines. Wilkinson's boring machine differed from earlier cantilevered machines used for boring cannon in that the cutting tool was mounted on a beam that ran through the cylinder being bored and was supported outside on both ends.

The planing machine, the milling machine and the shaping machine were developed in the early decades of the 19th century. Although the milling machine was invented at this time, it was not developed as a serious workshop tool until somewhat later in the 19th century.

Henry Maudslay, who trained a school of machine tool makers early in the 19th century, was a mechanic with superior ability who had been employed at the Royal Arsenal, Woolwich. He worked as an apprentice in the Royal Gun Foundry of Jan Verbruggen. In 1774 Jan Verbruggen had installed a horizontal boring machine in Woolwich which was the first industrial size lathe in the UK. Maudslay was hired away by Joseph Bramah for the production of high-security metal locks that required precision craftsmanship. Bramah patented a lathe that had similarities to the slide rest lathe. Maudslay perfected the slide rest lathe, which could cut machine screws of different thread pitches by using changeable gears between the spindle and the lead screw. Before its invention screws could not be cut to any precision using various earlier lathe designs, some of which copied from a template. The slide rest lathe was called one of history's most important inventions. Although it was not entirely Maudslay's idea, he was the first person to build a functional lathe using a combination of known innovations of the lead screw, slide rest and change gears.

Maudslay left Bramah's employment and set up his own shop. He was engaged to build the machinery for making ships' pulley blocks for the Royal Navy in the Portsmouth Block Mills. These machines were all-metal and were the first machines for mass production and making components with a degree of interchangeability. The lessons Maudslay learned about the need for stability and precision he adapted to the development of machine tools, and in his workshops he trained a generation of men to build on his work, such as Richard Roberts, Joseph Clement and Joseph Whitworth.

James Fox of Derby had a healthy export trade in machine tools for the first third of the century, as did Matthew Murray of Leeds. Roberts was a maker of high-quality machine tools and a pioneer of the use of jigs and gauges for precision workshop measurement.

The effect of machine tools during the Industrial Revolution was not that great because other than firearms, threaded fasteners and a few other industries there were few mass-produced metal parts. The techniques to make mass-produced metal parts made with sufficient precision to be interchangeable is largely attributed to a program of the U.S. Department of War which perfected interchangeable parts for firearms in the early 19th century.

In the half century following the invention of the fundamental machine tools the machine industry became the largest industrial sector of the U.S. economy, by value added.

The large-scale production of chemicals was an important development during the Industrial Revolution. The first of these was the production of sulphuric acid by the lead chamber process invented by the Englishman John Roebuck (James Watt's first partner) in 1746. He was able to greatly increase the scale of the manufacture by replacing the relatively expensive glass vessels formerly used with larger, less expensive chambers made of riveted sheets of lead. Instead of making a small amount each time, he was able to make around in each of the chambers, at least a tenfold increase.

The production of an alkali on a large scale became an important goal as well, and Nicolas Leblanc succeeded in 1791 in introducing a method for the production of sodium carbonate. The Leblanc process was a reaction of sulfuric acid with sodium chloride to give sodium sulfate and hydrochloric acid. The sodium sulfate was heated with limestone (calcium carbonate) and coal to give a mixture of sodium carbonate and calcium sulfide. Adding water separated the soluble sodium carbonate from the calcium sulfide. The process produced a large amount of pollution (the hydrochloric acid was initially vented to the air, and calcium sulfide was a useless waste product). Nonetheless, this synthetic soda ash proved economical compared to that from burning specific plants (barilla) or from kelp, which were the previously dominant sources of soda ash, and also to potash (potassium carbonate) produced from hardwood ashes.

These two chemicals were very important because they enabled the introduction of a host of other inventions, replacing many small-scale operations with more cost-effective and controllable processes. Sodium carbonate had many uses in the glass, textile, soap, and paper industries. Early uses for sulfuric acid included pickling (removing rust) iron and steel, and for bleaching cloth.

The development of bleaching powder (calcium hypochlorite) by Scottish chemist Charles Tennant in about 1800, based on the discoveries of French chemist Claude Louis Berthollet, revolutionised the bleaching processes in the textile industry by dramatically reducing the time required (from months to days) for the traditional process then in use, which required repeated exposure to the sun in bleach fields after soaking the textiles with alkali or sour milk. Tennant's factory at St Rollox, North Glasgow, became the largest chemical plant in the world.

After 1860 the focus on chemical innovation was in dyestuffs, and Germany took world leadership, building a strong chemical industry. Aspiring chemists flocked to German universities in the 1860–1914 era to learn the latest techniques. British scientists by contrast, lacked research universities and did not train advanced students; instead, the practice was to hire German-trained chemists.

In 1824 Joseph Aspdin, a British bricklayer turned builder, patented a chemical process for making portland cement which was an important advance in the building trades. This process involves sintering a mixture of clay and limestone to about , then grinding it into a fine powder which is then mixed with water, sand and gravel to produce concrete. Portland cement was used by the famous English engineer Marc Isambard Brunel several years later when constructing the Thames Tunnel. Cement was used on a large scale in the construction of the London sewerage system a generation later.

Another major industry of the later Industrial Revolution was gas lighting. Though others made a similar innovation elsewhere, the large-scale introduction of this was the work of William Murdoch, an employee of Boulton & Watt, the Birmingham steam engine pioneers. The process consisted of the large-scale gasification of coal in furnaces, the purification of the gas (removal of sulphur, ammonia, and heavy hydrocarbons), and its storage and distribution. The first gas lighting utilities were established in London between 1812 and 1820. They soon became one of the major consumers of coal in the UK. Gas lighting affected social and industrial organisation because it allowed factories and stores to remain open longer than with tallow candles or oil. Its introduction allowed nightlife to flourish in cities and towns as interiors and streets could be lighted on a larger scale than before.

Glass was made in ancient Greece and Rome. A new method of producing glass, known as the cylinder process, was developed in Europe during the early 19th century. In 1832 this process was used by the Chance Brothers to create sheet glass. They became the leading producers of window and plate glass. This advancement allowed for larger panes of glass to be created without interruption, thus freeing up the space planning in interiors as well as the fenestration of buildings. The Crystal Palace is the supreme example of the use of sheet glass in a new and innovative structure.

A machine for making a continuous sheet of paper on a loop of wire fabric was patented in 1798 by Nicholas Louis Robert who worked for Saint-Léger Didot family in France. The paper machine is known as a Fourdrinier after the financiers, brothers Sealy and Henry Fourdrinier, who were stationers in London. Although greatly improved and with many variations, the Fourdriner machine is the predominant means of paper production today.

The method of continuous production demonstrated by the paper machine influenced the development of continuous rolling of iron and later steel and other continuous production processes.

The British Agricultural Revolution is considered one of the causes of the Industrial Revolution because improved agricultural productivity freed up workers to work in other sectors of the economy. However, per-capita food supply in Europe was stagnant or declining and did not improve in some parts of Europe until the late 18th century.

Industrial technologies that affected farming included the seed drill, the Dutch plough, which contained iron parts, and the threshing machine.

The English lawyer Jethro Tull invented an improved seed drill in 1701. It was a mechanical seeder which distributed seeds evenly across a plot of land and planted them at the correct depth. This was important because the yield of seeds harvested to seeds planted at that time was around four or five. Tull's seed drill was very expensive and not very reliable and therefore did not have much of an effect. Good quality seed drills were not produced until the mid 18th century.

Joseph Foljambe's "Rotherham plough" of 1730 was the first commercially successful iron plough. The threshing machine, invented by the Scottish engineer Andrew Meikle in 1784, displaced hand threshing with a flail, a laborious job that took about one-quarter of agricultural labour. It took several decades to diffuse and was the final straw for many farm labourers, who faced near starvation, leading to the 1830 agricultural rebellion of the Swing Riots.

Machine tools and metalworking techniques developed during the Industrial Revolution eventually resulted in precision manufacturing techniques in the late 19th century for mass-producing agricultural equipment, such as reapers, binders and combine harvesters.

Coal mining in Britain, particularly in South Wales, started early. Before the steam engine, pits were often shallow bell pits following a seam of coal along the surface, which were abandoned as the coal was extracted. In other cases, if the geology was favourable, the coal was mined by means of an adit or drift mine driven into the side of a hill. Shaft mining was done in some areas, but the limiting factor was the problem of removing water. It could be done by hauling buckets of water up the shaft or to a sough (a tunnel driven into a hill to drain a mine). In either case, the water had to be discharged into a stream or ditch at a level where it could flow away by gravity. The introduction of the steam pump by Thomas Savery in 1698 and the Newcomen steam engine in 1712 greatly facilitated the removal of water and enabled shafts to be made deeper, enabling more coal to be extracted. These were developments that had begun before the Industrial Revolution, but the adoption of John Smeaton's improvements to the Newcomen engine followed by James Watt's more efficient steam engines from the 1770s reduced the fuel costs of engines, making mines more profitable. The Cornish engine, developed in the 1810s, was much more efficient than the Watt steam engine.

Coal mining was very dangerous owing to the presence of firedamp in many coal seams. Some degree of safety was provided by the safety lamp which was invented in 1816 by Sir Humphry Davy and independently by George Stephenson. However, the lamps proved a false dawn because they became unsafe very quickly and provided a weak light. Firedamp explosions continued, often setting off coal dust explosions, so casualties grew during the entire 19th century. Conditions of work were very poor, with a high casualty rate from rock falls.

At the beginning of the Industrial Revolution, inland transport was by navigable rivers and roads, with coastal vessels employed to move heavy goods by sea. Wagonways were used for conveying coal to rivers for further shipment, but canals had not yet been widely constructed. Animals supplied all of the motive power on land, with sails providing the motive power on the sea. The first horse railways were introduced toward the end of the 18th century, with steam locomotives being introduced in the early decades of the 19th century. Improving sailing technologies boosted average sailing speed 50% between 1750 and 1830.

The Industrial Revolution improved Britain's transport infrastructure with a turnpike road network, a canal and waterway network, and a railway network. Raw materials and finished products could be moved more quickly and cheaply than before. Improved transportation also allowed new ideas to spread quickly.

Before and during the Industrial Revolution navigation on several British rivers was improved by removing obstructions, straightening curves, widening and deepening and building navigation locks. Britain had over 1,000 miles of navigable rivers and streams by 1750.

Canals and waterways allowed bulk materials to be economically transported long distances inland. This was because a horse could pull a barge with a load dozens of times larger than the load that could be drawn in a cart.

In the UK, canals began to be built in the late 18th century to link the major manufacturing centres across the country. Known for its huge commercial success, the Bridgewater Canal in North West England, which opened in 1761 and was mostly funded by The 3rd Duke of Bridgewater. From Worsley to the rapidly growing town of Manchester its construction cost £168,000 (£ ), but its advantages over land and river transport meant that within a year of its opening in 1761, the price of coal in Manchester fell by about half. This success helped inspire a period of intense canal building, known as Canal Mania. New canals were hastily built in the aim of replicating the commercial success of the Bridgewater Canal, the most notable being the Leeds and Liverpool Canal and the Thames and Severn Canal which opened in 1774 and 1789 respectively.

By the 1820s a national network was in existence. Canal construction served as a model for the organisation and methods later used to construct the railways. They were eventually largely superseded as profitable commercial enterprises by the spread of the railways from the 1840s on. The last major canal to be built in the United Kingdom was the Manchester Ship Canal, which upon opening in 1894 was the largest ship canal in the world, and opened Manchester as a port. However it never achieved the commercial success its sponsors had hoped for and signalled canals as a dying mode of transport in an age dominated by railways, which were quicker and often cheaper.

Britain's canal network, together with its surviving mill buildings, is one of the most enduring features of the early Industrial Revolution to be seen in Britain.

France was known for having an excellent system of roads at the time of the Industrial Revolution; however, most of the roads on the European Continent and in the U.K. were in bad condition and dangerously rutted.

Much of the original British road system was poorly maintained by thousands of local parishes, but from the 1720s (and occasionally earlier) turnpike trusts were set up to charge tolls and maintain some roads. Increasing numbers of main roads were turnpiked from the 1750s to the extent that almost every main road in England and Wales was the responsibility of a turnpike trust. New engineered roads were built by John Metcalf, Thomas Telford and most notably John McAdam, with the first 'macadamised' stretch of road being Marsh Road at Ashton Gate, Bristol in 1816. The major turnpikes radiated from London and were the means by which the Royal Mail was able to reach the rest of the country. Heavy goods transport on these roads was by means of slow, broad wheeled, carts hauled by teams of horses. Lighter goods were conveyed by smaller carts or by teams of pack horse. Stagecoaches carried the rich, and the less wealthy could pay to ride on carriers carts.

Reducing friction was one of the major reasons for the success of railroads compared to wagons. This was demonstrated on an iron plate covered wooden tramway in 1805 at Croydon, England.
"A good horse on an ordinary turnpike road can draw two thousand pounds, or one ton. A party of gentlemen were invited to witness the experiment, that the superiority of the new road might be established by ocular demonstration. Twelve wagons were loaded with stones, till each wagon weighed three tons, and the wagons were fastened together. A horse was then attached, which drew the wagons with ease, six miles in two hours, having stopped four times, in order to show he had the power of starting, as well as drawing his great load."

Railways were made practical by the widespread introduction of inexpensive puddled iron after 1800, the rolling mill for making rails, and the development of the high-pressure steam engine also around 1800.

Wagonways for moving coal in the mining areas had started in the 17th century and were often associated with canal or river systems for the further movement of coal. These were all horse drawn or relied on gravity, with a stationary steam engine to haul the wagons back to the top of the incline. The first applications of the steam locomotive were on wagon or plate ways (as they were then often called from the cast-iron plates used). Horse-drawn public railways did not begin until the early years of the 19th century when improvements to pig and wrought iron production were lowering costs.

Steam locomotives began being built after the introduction of high-pressure steam engines after the expiration of the Boulton and Watt patent in 1800. High-pressure engines exhausted used steam to the atmosphere, doing away with the condenser and cooling water. They were also much lighter weight and smaller in size for a given horsepower than the stationary condensing engines. A few of these early locomotives were used in mines. Steam-hauled public railways began with the Stockton and Darlington Railway in 1825.

The rapid introduction of railways followed the 1829 Rainhill Trials, which demonstrated Robert Stephenson's successful locomotive design and the 1828 development of hot blast, which dramatically reduced the fuel consumption of making iron and increased the capacity of the blast furnace.

On 15 September 1830, the Liverpool and Manchester Railway was opened, the first inter-city railway in the world and was attended by Prime Minister, the Duke of Wellington. The railway was engineered by Joseph Locke and George Stephenson, linked the rapidly expanding industrial town of Manchester with the port town of Liverpool. The opening was marred by problems, due to the primitive nature of the technology being employed, however problems were gradually ironed out and the railway became highly successful, transporting passengers and freight. The success of the inter-city railway, particularly in the transport of freight and commodities, led to Railway Mania.

Construction of major railways connecting the larger cities and towns began in the 1830s but only gained momentum at the very end of the first Industrial Revolution. After many of the workers had completed the railways, they did not return to their rural lifestyles but instead remained in the cities, providing additional workers for the factories.

Other developments included more efficient water wheels, based on experiments conducted by the British engineer John Smeaton, the beginnings of a machine industry and the rediscovery of concrete (based on hydraulic lime mortar) by John Smeaton, which had been lost for 1,300 years.

Prior to the Industrial Revolution, most of the workforce was employed in agriculture, either as self-employed farmers as landowners or tenants, or as landless agricultural labourers. It was common for families in various parts of the world to spin yarn, weave cloth and make their own clothing. Households also spun and wove for market production. At the beginning of the Industrial Revolution India, China and regions of Iraq and elsewhere in Asia and the Middle East produced most of the world's cotton cloth while Europeans produced wool and linen goods.

In Britain by the 16th century the putting-out system, by which farmers and townspeople produced goods for market in their homes, often described as "cottage industry", was being practiced. Typical putting out system goods included spinning and weaving. Merchant capitalists typically provided the raw materials, paid workers by the piece, and were responsible for the sale of the goods. Embezzlement of supplies by workers and poor quality were common problems. The logistical effort in procuring and distributing raw materials and picking up finished goods were also limitations of the putting out system.

Some early spinning and weaving machinery, such as a 40 spindle jenny for about six pounds in 1792, was affordable for cottagers. Later machinery such as spinning frames, spinning mules and power looms were expensive (especially if water powered), giving rise to capitalist ownership of factories.

The majority of textile factory workers during the Industrial Revolution were unmarried women and children, including many orphans. They typically worked for 12 to 14 hours per day with only Sundays off. It was common for women take factory jobs seasonally during slack periods of farm work. Lack of adequate transportation, long hours and poor pay made it difficult to recruit and maintain workers. Many workers, such as displaced farmers and agricultural workers, who had nothing but their labour to sell, became factory workers out of necessity. (See: British Agricultural Revolution, Threshing machine)

The change in the social relationship of the factory worker compared to farmers and cottagers was viewed unfavourably by Karl Marx; however, he recognized the increase in productivity made possible by technology.

Some economists, such as Robert E. Lucas, Jr., say that the real effect of the Industrial Revolution was that "for the first time in history, the living standards of the masses of ordinary people have begun to undergo sustained growth ... Nothing remotely like this economic behaviour is mentioned by the classical economists, even as a theoretical possibility." Others, however, argue that while growth of the economy's overall productive powers was unprecedented during the Industrial Revolution, living standards for the majority of the population did not grow meaningfully until the late 19th and 20th centuries, and that in many ways workers' living standards declined under early capitalism: for instance, studies have shown that real wages in Britain only increased 15% between the 1780s and 1850s, and that life expectancy in Britain did not begin to dramatically increase until the 1870s. Similarly, the average height of the population declined during the Industrial Revolution, implying that their nutritional status was also decreasing. Real wages were not keeping up with the price of food.

During the Industrial Revolution, the life expectancy of children increased dramatically. The percentage of the children born in London who died before the age of five decreased from 74.5% in 1730–1749 to 31.8% in 1810–1829.

The effects on living conditions of the industrial revolution have been very controversial, and were hotly debated by economic and social historians from the 1950s to the 1980s. A series of 1950s essays by Henry Phelps Brown and Sheila V. Hopkins later set the academic consensus that the bulk of the population, that was at the bottom of the social ladder, suffered severe reductions in their living standards. During 1813–1913, there was a significant increase in worker wages.

Chronic hunger and malnutrition were the norm for the majority of the population of the world including Britain and France, until the late 19th century. Until about 1750, in large part due to malnutrition, life expectancy in France was about 35 years and about 40 years in Britain. The United States population of the time was adequately fed, much taller on average and had life expectancy of 45–50 years although U.S. life expectancy declined by a few years by the mid 19th century. Food consumption per capita also declined during an episode known as the Antebellum Puzzle.

Food supply in Great Britain was adversely affected by the Corn Laws (1815–1846). The Corn Laws, which imposed tariffs on imported grain, were enacted to keep prices high in order to benefit domestic producers. The Corn Laws were repealed in the early years of the Great Irish Famine.

The initial technologies of the Industrial Revolution, such as mechanized textiles, iron and coal, did little, if anything, to lower food prices. In Britain and the Netherlands, food supply increased before the Industrial Revolution due to better agricultural practices; however, population grew too, as noted by Thomas Malthus. This condition is called the Malthusian trap, and it finally started to be overcome by transportation improvements, such as canals, improved roads and steamships. Railroads and steamships were introduced near the end of the Industrial Revolution.

The rapid population growth in the 19th century included the new industrial and manufacturing cities, as well as service centers such as Edinburgh and London. The critical factor was financing, which was handled by building societies that dealt directly with large contracting firms. Private renting from housing landlords was the dominant tenure. P. Kemp says this was usually of advantage to tenants. People moved in so rapidly there was not enough capital to build adequate housing for everyone, so low-income newcomers squeezed into increasingly overcrowded slums. Clean water, sanitation, and public health facilities were inadequate; the death rate was high, especially infant mortality, and tuberculosis among young adults. Cholera from polluted water and typhoid were endemic. Unlike rural areas, there were no famines such as the one that devastated Ireland in the 1840s.

A large exposé literature grew up condemning the unhealthy conditions. By far the most famous publication was by one of the founders of the Socialist movement, "The Condition of the Working Class in England" in 1844 Friedrich Engels described backstreet sections of Manchester and other mill towns, where people lived in crude shanties and shacks, some not completely enclosed, some with dirt floors. These shanty towns had narrow walkways between irregularly shaped lots and dwellings. There were no sanitary facilities. Population density was extremely high. However, not everyone lived in such poor conditions. The Industrial Revolution also created a middle class of businessmen, clerks, foremen and engineers who lived in much better conditions.

Conditions improved over the course of the 19th century due to new public health acts regulating things such as sewage, hygiene and home construction. In the introduction of his 1892 edition, Engels notes that most of the conditions he wrote about in 1844 had been greatly improved. For example, the Public Health Act 1875 led to the more sanitary byelaw terraced house.

In "The Condition of the Working Class in England" in 1844 Friedrich Engels described how untreated sewage created awful odours and turned the rivers green in industrial cities.

In 1854 John Snow traced a cholera outbreak in Soho in London to faecal contamination of a public water well by a home cesspit. Snow's findings that cholera could be spread by contaminated water took some years to be accepted, but his work led to fundamental changes in the design of public water and waste systems.

Pre-industrial water supply relied on gravity systems and pumping of water was done by water wheels. Pipes were typically made of wood. Steam powered pumps and iron pipes allowed the widespread piping of water to horse watering troughs and households.

Modern industrialization began in England and Scotland in the 18th century, where there were relatively high levels of literacy among farmers, especially in Scotland. This permitted the recruitment of literate craftsman, skilled workers, foremen and managers who supervised the emerging textile factories and coal mines. Much of a labor was unskilled, and especially in textile mills children as young as eight proved useful in handling chores and adding to the family income. Indeed, children were taken out of school to work alongside their parents in the factories. However by the mid-nineteenth century, unskilled labor forces were common in Western Europe, and British industry moved upscale, needing many more engineers and skilled workers who could handle technical instructions and handle complex situations. Literacy was essential to be hired. A senior government official told Parliament in 1870:

The invention of the paper machine and the application of steam power to the industrial processes of printing supported a massive expansion of newspaper and pamphlet publishing, which contributed to rising literacy and demands for mass political participation.

Consumers benefited from falling prices for clothing and household articles such as cast iron cooking utensils, and in the following decades, stoves for cooking and space heating. Coffee, tea, sugar, tobacco and chocolate became affordable to many in Europe. Watches and household clocks became popular consumer items.

Meeting the demands of the consumer revolution and growth in wealth of the middle classes in Britain, potter and entrepreneur Josiah Wedgwood, founder of Wedgwood fine china and porcelain, created goods such as tableware, which was starting to become a common feature on dining tables.

A growing consumer culture also saw people start to spend more money on entertainment. Increased literacy rates, industrialisation, and the invention of railway, created a new market for cheap popular literature for the masses and the ability for it to be circulated on a large scale. Penny dreadfuls were created in the 1830s to meet this demand. "The Guardian" described penny dreadfuls as "Britain's first taste of mass-produced popular culture for the young", and "the Victorian equivalent of video games". More than one million boys' periodicals were sold per week.

In 1861, Welsh entrepreneur Pryce Pryce-Jones formed the first mail order business, an idea which would change the nature of retail. Selling Welsh flannel, he created mail order catalogues, with customers able to order by mail for the first timethis following the Uniform Penny Post in 1840 and the invention of the postage stamp (Penny Black) where there was a charge of one penny for carriage and delivery between any two places in the United Kingdom irrespective of distanceand the goods were delivered throughout the UK via the newly created railway system. As the railway network expanded overseas, so did his business.

The Industrial Revolution was the first period in history during which there was a simultaneous increase in both population and per capita income.

According to Robert Hughes in "The Fatal Shore", the population of England and Wales, which had remained steady at six million from 1700 to 1740, rose dramatically after 1740. The population of England had more than doubled from 8.3 million in 1801 to 16.8 million in 1850 and, by 1901, had nearly doubled again to 30.5 million. Improved conditions led to the population of Britain increasing from 10 million to 40 million in the 1800s. Europe's population increased from about 100 million in 1700 to 400 million by 1900.

The growth of modern industry since the late 18th century led to massive urbanisation and the rise of new great cities, first in Europe and then in other regions, as new opportunities brought huge numbers of migrants from rural communities into urban areas. In 1800, only 3% of the world's population lived in cities, compared to nearly 50% today (the beginning of the 21st century). Manchester had a population of 10,000 in 1717, but by 1911 it had burgeoned to 2.3 million.

Women's historians have debated the effect of the Industrial Revolution and capitalism generally on the status of women. Taking a pessimistic side, Alice Clark argued that when capitalism arrived in 17th-century England, it lowered the status of women as they lost much of their economic importance. Clark argues that in 16th-century England, women were engaged in many aspects of industry and agriculture. The home was a central unit of production and women played a vital role in running farms, and in some trades and landed estates. Their useful economic roles gave them a sort of equality with their husbands. However, Clark argues, as capitalism expanded in the 17th century, there was more and more division of labour with the husband taking paid labour jobs outside the home, and the wife reduced to unpaid household work. Middle- and upper-class women were confined to an idle domestic existence, supervising servants; lower-class women were forced to take poorly paid jobs. Capitalism, therefore, had a negative effect on powerful women.

In a more positive interpretation, Ivy Pinchbeck argues that capitalism created the conditions for women's emancipation. Tilly and Scott have emphasised the continuity in the status of women, finding three stages in English history. In the pre-industrial era, production was mostly for home use and women produce much of the needs of the households. The second stage was the "family wage economy" of early industrialisation; the entire family depended on the collective wages of its members, including husband, wife and older children. The third or modern stage is the "family consumer economy," in which the family is the site of consumption, and women are employed in large numbers in retail and clerical jobs to support rising standards of consumption.

Ideas of thrift and hard work characterized middle-class families as the Industrial Revolution swept Europe. These values were displayed in Samuel Smiles' book "Self-Help", in which he states that the misery of the poorer classes was "voluntary and self-imposed – the results of idleness, thriftlessness, intemperance, and misconduct."

In terms of social structure, the Industrial Revolution witnessed the triumph of a middle class of industrialists and businessmen over a landed class of nobility and gentry. Ordinary working people found increased opportunities for employment in the new mills and factories, but these were often under strict working conditions with long hours of labour dominated by a pace set by machines. As late as the year 1900, most industrial workers in the United States still worked a 10-hour day (12 hours in the steel industry), yet earned from 20% to 40% less than the minimum deemed necessary for a decent life; however, most workers in textiles, which was by far the leading industry in terms of employment, were women and children. For workers of the labouring classes, industrial life "was a stony desert, which they had to make habitable by their own efforts." Also, harsh working conditions were prevalent long before the Industrial Revolution took place. Pre-industrial society was very static and often cruel – child labour, dirty living conditions, and long working hours were just as prevalent before the Industrial Revolution.

Industrialisation led to the creation of the factory. The factory system contributed to the growth of urban areas, as large numbers of workers migrated into the cities in search of work in the factories. Nowhere was this better illustrated than the mills and associated industries of Manchester, nicknamed "Cottonopolis", and the world's first industrial city. Manchester experienced a six-times increase in its population between 1771 and 1831. Bradford grew by 50% every ten years between 1811 and 1851 and by 1851 only 50% of the population of Bradford was actually born there.

In addition, between 1815 and 1939, 20 percent of Europe's population left home, pushed by poverty, a rapidly growing population, and the displacement of peasant farming and artisan manufacturing. They were pulled abroad by the enormous demand for labour overseas, the ready availability of land, and cheap transportation. Still, many did not find a satisfactory life in their new homes, leading 7 million of them to return to Europe. This mass migration had large demographic effects: in 1800, less than one percent of the world population consisted of overseas Europeans and their descendants; by 1930, they represented 11 percent. The Americas felt the brunt of this huge emigration, largely concentrated in the United States.

For much of the 19th century, production was done in small mills, which were typically water-powered and built to serve local needs. Later, each factory would have its own steam engine and a chimney to give an efficient draft through its boiler.

In other industries, the transition to factory production was not so divisive. Some industrialists themselves tried to improve factory and living conditions for their workers. One of the earliest such reformers was Robert Owen, known for his pioneering efforts in improving conditions for workers at the New Lanark mills, and often regarded as one of the key thinkers of the early socialist movement.

By 1746 an integrated brass mill was working at Warmley near Bristol. Raw material went in at one end, was smelted into brass and was turned into pans, pins, wire, and other goods. Housing was provided for workers on site. Josiah Wedgwood and Matthew Boulton (whose Soho Manufactory was completed in 1766) were other prominent early industrialists, who employed the factory system.

The Industrial Revolution led to a population increase but the chances of surviving childhood did not improve throughout the Industrial Revolution, although "infant" mortality rates were reduced markedly. There was still limited opportunity for education and children were expected to work. Employers could pay a child less than an adult even though their productivity was comparable; there was no need for strength to operate an industrial machine, and since the industrial system was completely new, there were no experienced adult labourers. This made child labour the labour of choice for manufacturing in the early phases of the Industrial Revolution between the 18th and 19th centuries. In England and Scotland in 1788, two-thirds of the workers in 143 water-powered cotton mills were described as children.

Child labour existed before the Industrial Revolution but with the increase in population and education it became more visible. Many children were forced to work in relatively bad conditions for much lower pay than their elders, 10–20% of an adult male's wage.

Reports were written detailing some of the abuses, particularly in the coal mines and textile factories, and these helped to popularise the children's plight. The public outcry, especially among the upper and middle classes, helped stir change in the young workers' welfare.

Politicians and the government tried to limit child labour by law but factory owners resisted; some felt that they were aiding the poor by giving their children money to buy food to avoid starvation, and others simply welcomed the cheap labour. In 1833 and 1844, the first general laws against child labour, the Factory Acts, were passed in Britain: Children younger than nine were not allowed to work, children were not permitted to work at night, and the work day of youth under the age of 18 was limited to twelve hours. Factory inspectors supervised the execution of the law, however, their scarcity made enforcement difficult. About ten years later, the employment of children and women in mining was forbidden. Although laws such as these decreased the number of child labourers, child labour remained significantly present in Europe and the United States until the 20th century.

The Industrial Revolution concentrated labour into mills, factories and mines, thus facilitating the organisation of "combinations" or trade unions to help advance the interests of working people. The power of a union could demand better terms by withdrawing all labour and causing a consequent cessation of production. Employers had to decide between giving in to the union demands at a cost to themselves or suffering the cost of the lost production. Skilled workers were hard to replace, and these were the first groups to successfully advance their conditions through this kind of bargaining.

The main method the unions used to effect change was strike action. Many strikes were painful events for both sides, the unions and the management. In Britain, the Combination Act 1799 forbade workers to form any kind of trade union until its repeal in 1824. Even after this, unions were still severely restricted. One British newspaper in 1834 described unions as "the most dangerous institutions that were ever permitted to take root, under shelter of law, in any country..."

In 1832, the Reform Act extended the vote in Britain but did not grant universal suffrage. That year six men from Tolpuddle in Dorset founded the Friendly Society of Agricultural Labourers to protest against the gradual lowering of wages in the 1830s. They refused to work for less than ten shillings a week, although by this time wages had been reduced to seven shillings a week and were due to be further reduced to six. In 1834 James Frampton, a local landowner, wrote to the Prime Minister, Lord Melbourne, to complain about the union, invoking an obscure law from 1797 prohibiting people from swearing oaths to each other, which the members of the Friendly Society had done. James Brine, James Hammett, George Loveless, George's brother James Loveless, George's brother in-law Thomas Standfield, and Thomas's son John Standfield were arrested, found guilty, and transported to Australia. They became known as the Tolpuddle Martyrs. In the 1830s and 1840s, the Chartist movement was the first large-scale organised working class political movement which campaigned for political equality and social justice. Its "Charter" of reforms received over three million signatures but was rejected by Parliament without consideration.

Working people also formed friendly societies and co-operative societies as mutual support groups against times of economic hardship. Enlightened industrialists, such as Robert Owen also supported these organisations to improve the conditions of the working class.

Unions slowly overcame the legal restrictions on the right to strike. In 1842, a general strike involving cotton workers and colliers was organised through the Chartist movement which stopped production across Great Britain.

Eventually, effective political organisation for working people was achieved through the trades unions who, after the extensions of the franchise in 1867 and 1885, began to support socialist political parties that later merged to become the British Labour Party.

The rapid industrialisation of the English economy cost many craft workers their jobs. The movement started first with lace and hosiery workers near Nottingham and spread to other areas of the textile industry owing to early industrialisation. Many weavers also found themselves suddenly unemployed since they could no longer compete with machines which only required relatively limited (and unskilled) labour to produce more cloth than a single weaver. Many such unemployed workers, weavers, and others, turned their animosity towards the machines that had taken their jobs and began destroying factories and machinery. These attackers became known as Luddites, supposedly followers of Ned Ludd, a folklore figure. The first attacks of the Luddite movement began in 1811. The Luddites rapidly gained popularity, and the British government took drastic measures, using the militia or army to protect industry. Those rioters who were caught were tried and hanged, or transported for life.

Unrest continued in other sectors as they industrialised, such as with agricultural labourers in the 1830s when large parts of southern Britain were affected by the Captain Swing disturbances. Threshing machines were a particular target, and hayrick burning was a popular activity. However, the riots led to the first formation of trade unions, and further pressure for reform.

The traditional centers of hand textile production such as India, parts of the Middle East and later China could not withstand the competition from machine-made textiles, which over a period of decades destroyed the hand made textile industries and left millions of people without work, many of whom starved.

The Industrial Revolution also generated an enormous and unprecedented economic division in the world, as measured by the share of manufacturing output.
Cheap cotton textiles increased the demand for raw cotton; previously, it had primarily been consumed in subtropical regions where it was grown, with little raw cotton available for export. Consequently, prices of raw cotton rose. Some cotton had been grown in the West Indies, particularly in Hispaniola, but Haitian cotton production was halted by the Haitian Revolution in 1791. The invention of the cotton gin in 1792 allowed Georgia green seeded cotton to be profitable, leading to the widespread growth of cotton plantations in the United States and Brazil. In 1791 world cotton production was estimated to be 490,000,000 pounds with U.S. production accounting to 2,000,000 pounds. By 1800, U.S. production was 35,000,000 pounds, of which 17,790,000 were exported. In 1945 the U.S. produced seven-eights of the 1,169,600,000 pounds of world production.

The Americas, particularly the U.S., had labour shortages and high priced labour, which made slavery attractive. America's cotton plantations were highly efficient and profitable, and able to keep up with demand. The U.S. Civil War created a "cotton famine" that led to increased production in other areas of the world, including new colonies in Africa.

The origins of the environmental movement lay in the response to increasing levels of smoke pollution in the atmosphere during the Industrial Revolution. The emergence of great factories and the concomitant immense growth in coal consumption gave rise to an unprecedented level of air pollution in industrial centers; after 1900 the large volume of industrial chemical discharges added to the growing load of untreated human waste. The first large-scale, modern environmental laws came in the form of Britain's Alkali Acts, passed in 1863, to regulate the deleterious air pollution (gaseous hydrochloric acid) given off by the Leblanc process, used to produce soda ash. An Alkali inspector and four sub-inspectors were appointed to curb this pollution. The responsibilities of the inspectorate were gradually expanded, culminating in the Alkali Order 1958 which placed all major heavy industries that emitted smoke, grit, dust and fumes under supervision.

The manufactured gas industry began in British cities in 1812–1820. The technique used produced highly toxic effluent that was dumped into sewers and rivers. The gas companies were repeatedly sued in nuisance lawsuits. They usually lost and modified the worst practices. The City of London repeatedly indicted gas companies in the 1820s for polluting the Thames and poisoning its fish. Finally, Parliament wrote company charters to regulate toxicity. The industry reached the US around 1850 causing pollution and lawsuits.

In industrial cities local experts and reformers, especially after 1890, took the lead in identifying environmental degradation and pollution, and initiating grass-roots movements to demand and achieve reforms. Typically the highest priority went to water and air pollution. The Coal Smoke Abatement Society was formed in Britain in 1898 making it one of the oldest environmental NGOs. It was founded by artist Sir William Blake Richmond, frustrated with the pall cast by coal smoke. Although there were earlier pieces of legislation, the Public Health Act 1875 required all furnaces and fireplaces to consume their own smoke. It also provided for sanctions against factories that emitted large amounts of black smoke. The provisions of this law were extended in 1926 with the Smoke Abatement Act to include other emissions, such as soot, ash, and gritty particles and to empower local authorities to impose their own regulations.

In his 1983 book "Nations and Nationalism", philosopher Ernest Gellner argues that the industrial revolution and economic modernization spurred the creation of nations.

The Industrial Revolution in Continental Europe came a little later than in Great Britain. In many industries, this involved the application of technology developed in Britain in new places, starting with Belgium. Often the technology was purchased from Britain or British engineers and entrepreneurs moved abroad in search of new opportunities. By 1809, part of the Ruhr Valley in Westphalia was called 'Miniature England' because of its similarities to the industrial areas of England. The German, Belgian and many other European governments all provided state funding to the new industries. In some cases (such as iron), the different availability of resources locally meant that only some aspects of the British technology were adopted.

Belgium was the second country, after Britain, in which the Industrial Revolution took place and the first in continental Europe: Wallonia (French-speaking southern Belgium) was the first region to follow the British model successfully. Starting in the middle of the 1820s, and especially after Belgium became an independent nation in 1830, numerous works comprising coke blast furnaces as well as puddling and rolling mills were built in the coal mining areas around Liège and Charleroi. The leader was a transplanted Englishman John Cockerill. His factories at Seraing integrated all stages of production, from engineering to the supply of raw materials, as early as 1825.

Wallonia exemplified the radical evolution of industrial expansion. Thanks to coal (the French word "houille" was coined in Wallonia), the region geared up to become the 2nd industrial power in the world after Britain. But it is also pointed out by many researchers, with its "Sillon industriel", 'Especially in the Haine, Sambre and Meuse valleys, between the Borinage and Liège, [...] there was a huge industrial development based on coal-mining and iron-making...'. Philippe Raxhon wrote about the period after 1830: "It was not propaganda but a reality the Walloon regions were becoming the second industrial power all over the world after Britain." "The sole industrial centre outside the collieries and blast furnaces of Walloon was the old cloth-making town of Ghent." Michel De Coster, Professor at the Université de Liège wrote also: "The historians and the economists say that Belgium was the second industrial power of the world, in proportion to its population and its territory [...] But this rank is the one of Wallonia where the coal-mines, the blast furnaces, the iron and zinc factories, the wool industry, the glass industry, the weapons industry... were concentrated."

Wallonia was also the birthplace of a strong Socialist party and strong trade-unions in a particular sociological landscape. At the left, the "Sillon industriel", which runs from Mons in the west, to Verviers in the east (except part of North Flanders, in another period of the industrial revolution, after 1920). Even if Belgium is the second industrial country after Britain, the effect of the industrial revolution there was very different. In 'Breaking stereotypes', Muriel Neven and Isabelle Devious say:

The industrial revolution changed a mainly rural society into an urban one, but with a strong contrast between northern and southern Belgium. During the Middle Ages and the Early Modern Period, Flanders was characterised by the presence of large urban centres [...] at the beginning of the nineteenth century this region (Flanders), with an urbanisation degree of more than 30 per cent, remained one of the most urbanised in the world. By comparison, this proportion reached only 17 per cent in Wallonia, barely 10 per cent in most West European countries, 16 per cent in France and 25 per cent in Britain. Nineteenth century industrialisation did not affect the traditional urban infrastructure, except in Ghent [...] Also, in Wallonia the traditional urban network was largely unaffected by the industrialisation process, even though the proportion of city-dwellers rose from 17 to 45 per cent between 1831 and 1910. Especially in the Haine, Sambre and Meuse valleys, between the Borinage and Liège, where there was a huge industrial development based on coal-mining and iron-making, urbanisation was fast. During these eighty years the number of municipalities with more than 5,000 inhabitants increased from only 21 to more than one hundred, concentrating nearly half of the Walloon population in this region. Nevertheless, industrialisation remained quite traditional in the sense that it did not lead to the growth of modern and large urban centres, but to a conurbation of industrial villages and towns developed around a coal-mine or a factory. Communication routes between these small centres only became populated later and created a much less dense urban morphology than, for instance, the area around Liège where the old town was there to direct migratory flows.

The industrial revolution in France followed a particular course as it did not correspond to the main model followed by other countries. Notably, most French historians argue France did not go through a clear "take-off". Instead, France's economic growth and industrialisation process was slow and steady through the 18th and 19th centuries. However, some stages were identified by Maurice Lévy-Leboyer:

Based on its leadership in chemical research in the universities and industrial laboratories, Germany, which was unified in 1871, became dominant in the world's chemical industry in the late 19th century. At first the production of dyes based on aniline was critical.

Germany's political disunitywith three dozen statesand a pervasive conservatism made it difficult to build railways in the 1830s. However, by the 1840s, trunk lines linked the major cities; each German state was responsible for the lines within its own borders. Lacking a technological base at first, the Germans imported their engineering and hardware from Britain, but quickly learned the skills needed to operate and expand the railways. In many cities, the new railway shops were the centres of technological awareness and training, so that by 1850, Germany was self-sufficient in meeting the demands of railroad construction, and the railways were a major impetus for the growth of the new steel industry. Observers found that even as late as 1890, their engineering was inferior to Britain's. However, German unification in 1870 stimulated consolidation, nationalisation into state-owned companies, and further rapid growth. Unlike the situation in France, the goal was support of industrialisation, and so heavy lines crisscrossed the Ruhr and other industrial districts, and provided good connections to the major ports of Hamburg and Bremen. By 1880, Germany had 9,400 locomotives pulling 43,000 passengers and 30,000 tons of freight, and pulled ahead of France.

During the period 1790–1815 Sweden experienced two parallel economic movements: an "agricultural revolution" with larger agricultural estates, new crops and farming tools and a commercialisation of farming, and a "protoindustrialisation", with small industries being established in the countryside and with workers switching between agricultural work in summer and industrial production in winter. This led to economic growth benefiting large sections of the population and leading up to a "consumption revolution" starting in the 1820s. Between 1815 and 1850, the protoindustries developed into more specialised and larger industries. This period witnessed increasing regional specialisation with mining in Bergslagen, textile mills in Sjuhäradsbygden and forestry in Norrland. Several important institutional changes took place in this period, such as free and mandatory schooling introduced in 1842 (as the first country in the world), the abolition of the national monopoly on trade in handicrafts in 1846, and a stock company law in 1848.

From 1850 to 1890, Sweden experienced its "first" Industrial Revolution with a veritable explosion in export, dominated by crops, wood and steel. Sweden abolished most tariffs and other barriers to free trade in the 1850s and joined the gold standard in 1873. Large infrastructural investments were made during this period, mainly in the expanding rail road network, which was financed in part by the government and in part by private enterprises. From 1890 to 1930, new industries developed with their focus on the domestic market: mechanical engineering, power utilities, papermaking and textile.

The industrial revolution began about 1870 as Meiji period leaders decided to catch up with the West. The government built railroads, improved roads, and inaugurated a land reform programme to prepare the country for further development. It inaugurated a new Western-based education system for all young people, sent thousands of students to the United States and Europe, and hired more than 3,000 Westerners to teach modern science, mathematics, technology, and foreign languages in Japan (Foreign government advisors in Meiji Japan).

In 1871, a group of Japanese politicians known as the Iwakura Mission toured Europe and the United States to learn western ways. The result was a deliberate state-led industrialisation policy to enable Japan to quickly catch up. The Bank of Japan, founded in 1882, used taxes to fund model steel and textile factories. Education was expanded and Japanese students were sent to study in the west.

Modern industry first appeared in textiles, including cotton and especially silk, which was based in home workshops in rural areas.

During the late 18th an early 19th centuries when the UK and parts of Western Europe began to industrialise, the US was primarily an agricultural and natural resource producing and processing economy. The building of roads and canals, the introduction of steamboats and the building of railroads were important for handling agricultural and natural resource products in the large and sparsely populated country of the period.

Important American technological contributions during the period of the Industrial Revolution were the cotton gin and the development of a system for making interchangeable parts, the latter aided by the development of the milling machine in the US. The development of machine tools and the system of interchangeable parts were the basis for the rise of the US as the world's leading industrial nation in the late 19th century.

Oliver Evans invented an automated flour mill in the mid-1780s that used control mechanisms and conveyors so that no labour was needed from the time grain was loaded into the elevator buckets until flour was discharged into a wagon. This is considered to be the first modern materials handling system an important advance in the progress toward mass production.

The United States originally used horse-powered machinery for small scale applications such as grain milling, but eventually switched to water power after textile factories began being built in the 1790s. As a result, industrialisation was concentrated in New England and the Northeastern United States, which has fast-moving rivers. The newer water-powered production lines proved more economical than horse-drawn production. In the late 19th century steam-powered manufacturing overtook water-powered manufacturing, allowing the industry to spread to the Midwest.

Thomas Somers and the Cabot Brothers founded the Beverly Cotton Manufactory in 1787, the first cotton mill in America, the largest cotton mill of its era, and a significant milestone in the research and development of cotton mills in the future. This mill was designed to use horse power, but the operators quickly learned that the horse-drawn platform was economically unstable, and had economic losses for years. Despite the losses, the Manufactory served as a playground of innovation, both in turning a large amount of cotton, but also developing the water-powered milling structure used in Slater's Mill.

In 1793, Samuel Slater (1768–1835) founded the Slater Mill at Pawtucket, Rhode Island. He had learned of the new textile technologies as a boy apprentice in Derbyshire, England, and defied laws against the emigration of skilled workers by leaving for New York in 1789, hoping to make money with his knowledge. After founding Slater's Mill, he went on to own 13 textile mills. Daniel Day established a wool carding mill in the Blackstone Valley at Uxbridge, Massachusetts in 1809, the third woollen mill established in the US (The first was in Hartford, Connecticut, and the second at Watertown, Massachusetts.) The John H. Chafee Blackstone River Valley National Heritage Corridor retraces the history of "America's Hardest-Working River', the Blackstone. The Blackstone River and its tributaries, which cover more than from Worcester, Massachusetts to Providence, Rhode Island, was the birthplace of America's Industrial Revolution. At its peak over 1,100 mills operated in this valley, including Slater's mill, and with it the earliest beginnings of America's Industrial and Technological Development.

Merchant Francis Cabot Lowell from Newburyport, Massachusetts memorised the design of textile machines on his tour of British factories in 1810. Realising that the War of 1812 had ruined his import business but that a demand for domestic finished cloth was emerging in America, on his return to the United States, he set up the Boston Manufacturing Company. Lowell and his partners built America's second cotton-to-cloth textile mill at Waltham, Massachusetts, second to the Beverly Cotton Manufactory. After his death in 1817, his associates built America's first planned factory town, which they named after him. This enterprise was capitalised in a public stock offering, one of the first uses of it in the United States. Lowell, Massachusetts, using of canals and 10,000 horsepower delivered by the Merrimack River, is considered by some as a major contributor to the success of the American Industrial Revolution. The short-lived utopia-like Waltham-Lowell system was formed, as a direct response to the poor working conditions in Britain. However, by 1850, especially following the Irish Potato Famine, the system had been replaced by poor immigrant labour.

A major U.S. contribution to industrialisation was the development of techniques to make interchangeable parts from metal. Precision metal machining techniques were developed by the U.S. Department of War to make interchangeable parts for small firearms. The development work took place at the Federal Arsenals at Springfield Armory and Harpers Ferry Armory. Techniques for precision machining using machine tools included using fixtures to hold the parts in proper position, jigs to guide the cutting tools and precision blocks and gauges to measure the accuracy. The milling machine, a fundamental machine tool, is believed to have been invented by Eli Whitney, who was a government contractor who built firearms as part of this program. Another important invention was the Blanchard lathe, invented by Thomas Blanchard. The Blanchard lathe, or pattern tracing lathe, was actually a shaper that could produce copies of wooden gun stocks. The use of machinery and the techniques for producing standardised and interchangeable parts became known as the American system of manufacturing.

Precision manufacturing techniques made it possible to build machines that mechanised the shoe industry. and the watch industry. The industrialisation of the watch industry started 1854 also in Waltham, Massachusetts, at the Waltham Watch Company, with the development of machine tools, gauges and assembling methods adapted to the micro precision required for watches.

 
Steel is often cited as the first of several new areas for industrial mass-production, which are said to characterise a "Second Industrial Revolution", beginning around 1850, although a method for mass manufacture of steel was not invented until the 1860s, when Sir Henry Bessemer invented a new furnace which could convert molten pig iron into steel in large quantities. However, it only became widely available in the 1870s after the process was modified to produce more uniform quality. Bessemer steel was being displaced by the open hearth furnace near the end of the 19th century.
This Second Industrial Revolution gradually grew to include chemicals, mainly the chemical industries, petroleum (refining and distribution), and, in the 20th century, the automotive industry, and was marked by a transition of technological leadership from Britain to the United States and Germany.

The increasing availability of economical petroleum products also reduced the importance of coal and further widened the potential for industrialisation.

A new revolution began with electricity and electrification in the electrical industries. The introduction of hydroelectric power generation in the Alps enabled the rapid industrialisation of coal-deprived northern Italy, beginning in the 1890s.

By the 1890s, industrialisation in these areas had created the first giant industrial corporations with burgeoning global interests, as companies like U.S. Steel, General Electric, Standard Oil and Bayer AG joined the railroad and ship companies on the world's stock markets.

The causes of the Industrial Revolution were complicated and remain a topic for debate. Geographic factors include Britain's vast mineral resources. In addition to metal ores, Britain had the highest quality coal reserves known at the time. Britain also had abundant water power and highly productive agriculture. Britain also had numerous seaports and navigable waterways.

Some historians believe the Industrial Revolution was an outgrowth of social and institutional changes brought by the end of feudalism in Britain after the English Civil War in the 17th century, although feudalism began to break down after the Black Death of the mid 14th century, followed by other epidemics, until the population reached a low in the 14th century. This created labour shortages and led to falling food prices and a peak in real wages around 1500, after which population growth began reducing wages. Inflation caused by coinage debasement after 1540 followed by precious metals supply increasing from the Americas caused land rents (often long-term leases that transferred to heirs on death) to fall in real terms.

The Enclosure movement and the British Agricultural Revolution made food production more efficient and less labour-intensive, forcing the farmers who could no longer be self-sufficient in agriculture into cottage industry, for example weaving, and in the longer term into the cities and the newly developed factories. The colonial expansion of the 17th century with the accompanying development of international trade, creation of financial markets and accumulation of capital are also cited as factors, as is the scientific revolution of the 17th century. A change in marrying patterns to getting married later made people able to accumulate more human capital during their youth, thereby encouraging economic development.

Until the 1980s, it was universally believed by academic historians that technological innovation was the heart of the Industrial Revolution and the key enabling technology was the invention and improvement of the steam engine. However, recent research into the Marketing Era has challenged the traditional, supply-oriented interpretation of the Industrial Revolution.

Lewis Mumford has proposed that the Industrial Revolution had its origins in the Early Middle Ages, much earlier than most estimates. He explains that the model for standardised mass production was the printing press and that "the archetypal model for the industrial era was the clock". He also cites the monastic emphasis on order and time-keeping, as well as the fact that medieval cities had at their centre a church with bell ringing at regular intervals as being necessary precursors to a greater synchronisation necessary for later, more physical, manifestations such as the steam engine.

The presence of a large domestic market should also be considered an important driver of the Industrial Revolution, particularly explaining why it occurred in Britain. In other nations, such as France, markets were split up by local regions, which often imposed tolls and tariffs on goods traded among them. Internal tariffs were abolished by Henry VIII of England, they survived in Russia until 1753, 1789 in France and 1839 in Spain.

Governments' grant of limited monopolies to inventors under a developing patent system (the Statute of Monopolies in 1623) is considered an influential factor. The effects of patents, both good and ill, on the development of industrialisation are clearly illustrated in the history of the steam engine, the key enabling technology. In return for publicly revealing the workings of an invention the patent system rewarded inventors such as James Watt by allowing them to monopolise the production of the first steam engines, thereby rewarding inventors and increasing the pace of technological development. However, monopolies bring with them their own inefficiencies which may counterbalance, or even overbalance, the beneficial effects of publicising ingenuity and rewarding inventors. Watt's monopoly prevented other inventors, such as Richard Trevithick, William Murdoch, or Jonathan Hornblower, whom Boulton and Watt sued, from introducing improved steam engines, thereby retarding the spread of steam power.

One question of active interest to historians is why the Industrial Revolution occurred in Europe and not in other parts of the world in the 18th century, particularly China, India, and the Middle East (which pioneered in shipbuilding, textile production, water mills, and much more in the period between 750 and 1100), or at other times like in Classical Antiquity or the Middle Ages. A recent account argued that Europeans have been characterized for thousands of years by a freedom-loving culture originating from the aristocratic societies of early Indo-European invaders. Many historians, however, have challenged this explanation as being not only Eurocentric, but also ignoring historical context. In fact, before the Industrial Revolution, "there existed something of a global economic parity between the most advanced regions in the world economy." These historians have suggested a number of other factors, including education, technological changes (see Scientific Revolution in Europe), "modern" government, "modern" work attitudes, ecology, and culture.

China was the world's most technologically advanced country for many centuries; however, China stagnated economically and technologically and was surpassed by Western Europe before the Age of Discovery, by which time China banned imports and denied entry to foreigners. China was also a totalitarian society. China also heavily taxed transported goods. Modern estimates of per capita income in Western Europe in the late 18th century are of roughly 1,500 dollars in purchasing power parity (and Britain had a per capita income of nearly 2,000 dollars) whereas China, by comparison, had only 450 dollars. India was essentially feudal, politically fragmented and not as economically advanced as Western Europe.

Historians such as David Landes and sociologists Max Weber and Rodney Stark credit the different belief systems in Asia and Europe with dictating where the revolution occurred. The religion and beliefs of Europe were largely products of Judaeo-Christianity and Greek thought. Conversely, Chinese society was founded on men like Confucius, Mencius, Han Feizi (Legalism), Lao Tzu (Taoism), and Buddha (Buddhism), resulting in very different worldviews. Other factors include the considerable distance of China's coal deposits, though large, from its cities as well as the then unnavigable Yellow River that connects these deposits to the sea.

Regarding India, the Marxist historian Rajani Palme Dutt said: "The capital to finance the Industrial Revolution in India instead went into financing the Industrial Revolution in Britain." In contrast to China, India was split up into many competing kingdoms after the decline of the Mughal Empire, with the major ones in its aftermath including the Marathas, Sikhs, Bengal Subah, and Kingdom of Mysore. In addition, the economy was highly dependent on two sectorsagriculture of subsistence and cotton, and there appears to have been little technical innovation. It is believed that the vast amounts of wealth were largely stored away in palace treasuries by monarchs prior to the British take over.

Economic historian Joel Mokyr argued that political fragmentation (the presence of a large number of European states) made it possible for heterodox ideas to thrive, as entrepreneurs, innovators, ideologues and heretics could easily flee to a neighboring state in the event that the one state would try to suppress their ideas and activities. This is what set Europe apart from the technologically advanced, large unitary empires such as China and India by providing "an insurance against economic and technological stagnation". China had both a printing press and movable type, and India had similar levels scientific and technological achievement as Europe in 1700, yet the Industrial Revolution would occur in Europe, not China or India. In Europe, political fragmentation was coupled with an "integrated market for ideas" where Europe's intellectuals used the of Latin, had a shared intellectual basis in Europe's classical heritage and the pan-European institution of the Republic of Letters.

In addition, Europe's monarchs desperately needed revenue, pushing them into alliances with their merchant classes. Small groups of merchants were granted monopolies and tax-collecting responsibilities in exchange for payments to the state. Located in a region "at the hub of the largest and most varied network of exchange in history," Europe advanced as the leader of the Industrial Revolution. In the Americas, Europeans found a windfall of silver, timber, fish, and maize, leading historian Peter Stearns to conclude that "Europe's Industrial Revolution stemmed in great part from Europe's ability to draw disproportionately on world resources."

Modern capitalism originated in the Italian city-states around the end of the first millennium. The city-states were prosperous cities that were independent from feudal lords. They were largely republics whose governments were typically composed of merchants, manufacturers, members of guilds, bankers and financiers. The Italian city-states built a network of branch banks in leading western European cities and introduced double entry bookkeeping. Italian commerce was supported by schools that taught numeracy in financial calculations through abacus schools.

Great Britain provided the legal and cultural foundations that enabled entrepreneurs to pioneer the Industrial Revolution. Key factors fostering this environment were:
There were two main values that really drove the Industrial Revolution in Britain. These values were self-interest and an entrepreneurial spirit. Because of these interests, many industrial advances were made that resulted in a huge increase in personal wealth and a consumer revolution. These advancements also greatly benefitted the British society as a whole. Countries around the world started to recognise the changes and advancements in Britain and use them as an example to begin their own Industrial Revolutions.

The debate about the start of the Industrial Revolution also concerns the massive lead that Great Britain had over other countries. Some have stressed the importance of natural or financial resources that Britain received from its many overseas colonies or that profits from the British slave trade between Africa and the Caribbean helped fuel industrial investment. However, it has been pointed out that slave trade and West Indian plantations provided only 5% of the British national income during the years of the Industrial Revolution. Even though slavery accounted for so little, Caribbean-based demand accounted for 12% of Britain's industrial output.
Instead, the greater liberalisation of trade from a large merchant base may have allowed Britain to produce and use emerging scientific and technological developments more effectively than countries with stronger monarchies, particularly China and Russia. Britain emerged from the Napoleonic Wars as the only European nation not ravaged by financial plunder and economic collapse, and having the only merchant fleet of any useful size (European merchant fleets were destroyed during the war by the Royal Navy). Britain's extensive exporting cottage industries also ensured markets were already available for many early forms of manufactured goods. The conflict resulted in most British warfare being conducted overseas, reducing the devastating effects of territorial conquest that affected much of Europe. This was further aided by Britain's geographical positionan island separated from the rest of mainland Europe.
Another theory is that Britain was able to succeed in the Industrial Revolution due to the availability of key resources it possessed. It had a dense population for its small geographical size. Enclosure of common land and the related agricultural revolution made a supply of this labour readily available. There was also a local coincidence of natural resources in the North of England, the English Midlands, South Wales and the Scottish Lowlands. Local supplies of coal, iron, lead, copper, tin, limestone and water power resulted in excellent conditions for the development and expansion of industry. Also, the damp, mild weather conditions of the North West of England provided ideal conditions for the spinning of cotton, providing a natural starting point for the birth of the textiles industry.

The stable political situation in Britain from around 1688 following the Glorious Revolution, and British society's greater receptiveness to change (compared with other European countries) can also be said to be factors favouring the Industrial Revolution. Peasant resistance to industrialisation was largely eliminated by the Enclosure movement, and the landed upper classes developed commercial interests that made them pioneers in removing obstacles to the growth of capitalism. (This point is also made in Hilaire Belloc's "The Servile State".)

The French philosopher Voltaire wrote about capitalism and religious tolerance in his book on English society, "Letters on the English" (1733), noting why England at that time was more prosperous in comparison to the country's less religiously tolerant European neighbours. "Take a view of the Royal Exchange in London, a place more venerable than many courts of justice, where the representatives of all nations meet for the benefit of mankind. There the Jew, the Mahometan [Muslim], and the Christian transact together, as though they all professed the same religion, and give the name of infidel to none but bankrupts. There the Presbyterian confides in the Anabaptist, and the Churchman depends on the Quaker's word. If one religion only were allowed in England, the Government would very possibly become arbitrary; if there were but two, the people would cut one another's throats; but as there are such a multitude, they all live happy and in peace."

Britain's population grew 280% 1550–1820, while the rest of Western Europe grew 50–80%. Seventy percent of European urbanisation happened in Britain 1750–1800. By 1800, only the Netherlands was more urbanised than Britain. This was only possible because coal, coke, imported cotton, brick and slate had replaced wood, charcoal, flax, peat and thatch. The latter compete with land grown to feed people while mined materials do not. Yet more land would be freed when chemical fertilisers replaced manure and horse's work was mechanised. A workhorse needs for fodder while even early steam engines produced four times more mechanical energy.

In 1700, 5/6 of coal mined worldwide was in Britain, while the Netherlands had none; so despite having Europe's best transport, most urbanised, well paid, literate people and lowest taxes, it failed to industrialise. In the 18th century, it was the only European country whose cities and population shrank. Without coal, Britain would have run out of suitable river sites for mills by the 1830s. Based on science and experimentation from the continent, the steam engine was developed specifically for pumping water out of mines, many of which in Britain had been mined to below the water table. Although extremely inefficient they were economical because they used unsaleable coal. Iron rails were developed to transport coal, which was a major economic sector in Britain.

Economic historian Robert Allen has argued that high wages, cheap capital and very cheap energy in Britain made it the ideal place for the industrial revolution to occur. These factors made it vastly more profitable to invest in research and development, and to put technology to use in Britain than other societies. However, two 2018 studies in "The Economic History Review" showed that wages were not particularly high in the British spinning sector or the construction sector, casting doubt on Allen's explanation.

Knowledge of innovation was spread by several means. Workers who were trained in the technique might move to another employer or might be poached. A common method was for someone to make a study tour, gathering information where he could. During the whole of the Industrial Revolution and for the century before, all European countries and America engaged in study-touring; some nations, like Sweden and France, even trained civil servants or technicians to undertake it as a matter of state policy. In other countries, notably Britain and America, this practice was carried out by individual manufacturers eager to improve their own methods. Study tours were common then, as now, as was the keeping of travel diaries. Records made by industrialists and technicians of the period are an incomparable source of information about their methods.

Another means for the spread of innovation was by the network of informal philosophical societies, like the Lunar Society of Birmingham, in which members met to discuss 'natural philosophy' ("i.e." science) and often its application to manufacturing. The Lunar Society flourished from 1765 to 1809, and it has been said of them, "They were, if you like, the revolutionary committee of that most far reaching of all the eighteenth century revolutions, the Industrial Revolution". Other such societies published volumes of proceedings and transactions. For example, the London-based Royal Society of Arts published an illustrated volume of new inventions, as well as papers about them in its annual "Transactions".

There were publications describing technology. Encyclopaedias such as Harris's "Lexicon Technicum" (1704) and Abraham Rees's "Cyclopaedia" (1802–1819) contain much of value. "Cyclopaedia" contains an enormous amount of information about the science and technology of the first half of the Industrial Revolution, very well illustrated by fine engravings. Foreign printed sources such as the "Descriptions des Arts et Métiers" and Diderot's "Encyclopédie" explained foreign methods with fine engraved plates.

Periodical publications about manufacturing and technology began to appear in the last decade of the 18th century, and many regularly included notice of the latest patents. Foreign periodicals, such as the "Annales des Mines", published accounts of travels made by French engineers who observed British methods on study tours.

Another theory is that the British advance was due to the presence of an entrepreneurial class which believed in progress, technology and hard work. The existence of this class is often linked to the Protestant work ethic (see Max Weber) and the particular status of the Baptists and the dissenting Protestant sects, such as the Quakers and Presbyterians that had flourished with the English Civil War. Reinforcement of confidence in the rule of law, which followed establishment of the prototype of constitutional monarchy in Britain in the Glorious Revolution of 1688, and the emergence of a stable financial market there based on the management of the national debt by the Bank of England, contributed to the capacity for, and interest in, private financial investment in industrial ventures.

Dissenters found themselves barred or discouraged from almost all public offices, as well as education at England's only two universities at the time (although dissenters were still free to study at Scotland's four universities). When the restoration of the monarchy took place and membership in the official Anglican Church became mandatory due to the Test Act, they thereupon became active in banking, manufacturing and education. The Unitarians, in particular, were very involved in education, by running Dissenting Academies, where, in contrast to the universities of Oxford and Cambridge and schools such as Eton and Harrow, much attention was given to mathematics and the sciences – areas of scholarship vital to the development of manufacturing technologies.

Historians sometimes consider this social factor to be extremely important, along with the nature of the national economies involved. While members of these sects were excluded from certain circles of the government, they were considered fellow Protestants, to a limited extent, by many in the middle class, such as traditional financiers or other businessmen. Given this relative tolerance and the supply of capital, the natural outlet for the more enterprising members of these sects would be to seek new opportunities in the technologies created in the wake of the scientific revolution of the 17th century.

During the Industrial Revolution, an intellectual and artistic hostility towards the new industrialisation developed, associated with the Romantic movement. Romanticism revered the traditionalism of rural life and recoiled against the upheavals caused by industrialization, urbanization and the wretchedness of the working classes. Its major exponents in English included the artist and poet William Blake and poets William Wordsworth, Samuel Taylor Coleridge, John Keats, Lord Byron and Percy Bysshe Shelley. The movement stressed the importance of "nature" in art and language, in contrast to "monstrous" machines and factories; the "Dark satanic mills" of Blake's poem "And did those feet in ancient time". Mary Shelley's novel "Frankenstein" reflected concerns that scientific progress might be two-edged. French Romanticism likewise was highly critical of industry.





</doc>
<doc id="14918" url="https://en.wikipedia.org/wiki?curid=14918" title="International Court of Justice">
International Court of Justice

The International Court of Justice (ICJ), sometimes known as the World Court, is one of the six principal organs of the United Nations (UN). It settles disputes between states in accordance with international law and gives advisory opinions on international legal issues. The ICJ is the only international court that adjudicates general disputes between countries, with its rulings and opinions serving as primary sources of international law.

The ICJ is the successor of the Permanent Court of International Justice (PCIJ), which was established in 1920 by the League of Nations. Following the Second World War, both the League and the PCIJ were replaced by the United Nations and ICJ, respectively. The Statute of the ICJ, which sets forth its purposes draws heavily from that of its predecessor, whose decisions remain valid. All member states of the UN are party to the ICJ Statute and may initiate contentious cases; however, advisory proceedings may only be submitted by certain UN organs and agencies.

The ICJ is a panel of 15 judges elected by the General Assembly and Security Council for nine-year terms. No more than one nationality may serve on the court at the same time, and judges as a whole must represent the principal civilizations and legal systems of the world. Seated in the Peace Palace in The Hague, Netherlands, the ICJ is the only principal UN organ not located in New York City. Its official working languages are English and French.

Since the entry of its first case on 22 May 1947, the ICJ has entertained 178 cases through November 2019.

The first permanent institution established for the purpose of settling international disputes was the Permanent Court of Arbitration (PCA), which was created by the Hague Peace Conference of 1899. Initiated by Russian Czar Nicholas II, the conference involved all the world's major powers, as well as several smaller states, resulted in the first multilateral treaties concerned with the conduct of warfare. Among these was the "Convention for the Pacific Settlement of International Disputes", which set forth the institutional and procedural framework for arbitral proceedings, which would take place in The Hague, Netherlands. Although the proceedings would be supported by a permanent bureau—whose functions would be equivalent to that of a secretariat or court registry—the arbitrators would be appointed by the disputing states from a larger pool provided by each member of the Convention. The PCA was established in 1900 and began proceedings in 1902.

A second Hague Peace Conference in 1907, which involved most of the world's sovereign states, revised the Convention and enhanced the rules governing arbitral proceedings before the PCA. During this conference, the United States, Great Britain and Germany submitted a joint proposal for a permanent court whose judges would serve full-time. As the delegates could not agree as to how the judges would be selected, the matter was temporarily shelved pending an agreement to be adopted at a later convention.

The Hague Peace Conferences, and the ideas that emerged therefrom, influenced the creation of the Central American Court of Justice, which was established in 1908 as one of the earliest regional judicial bodies. Various plans and proposals were made between 1911 and 1919 for the establishment of an international judicial tribunal, which would not be realized into the formation of a new international system following the First World War.

The unprecedented bloodshed of the First World War led to the creation of the League of Nations, established by the Paris Peace Conference of 1919 as the first worldwide intergovernmental organization aimed at maintaining peace and collective security. Article 14 League's Covenant called for the establishment of a Permanent Court of International Justice (PCIJ), which would be responsible for adjudicating any international dispute submitted to it by the contesting parties, as well as to provide an advisory opinion upon any dispute or question referred to it by the League of Nations.

In December 1920, following several drafts and debates, the Assembly of the League unanimously adopted the Statute of the PCIJ, which was signed and ratified the following year by a majority of members. Among other things, the new Statute resolved the contentious issues of selecting judges by providing that the judges be elected by both the Council and the Assembly of the League concurrently but independently. The makeup of the PCIJ would reflect the "main forms of civilization and the principal legal systems of the world”. The PCIJ would be permanently placed at the Peace Palace in The Hague, alongside Permanent Court of Arbitration.

The PCIJ represented a major innovation in international jurisprudence in several ways:


Unlike the ICJ, the PCIJ was not part of the League, nor were members of the League automatically a party to its Statute. The United States, which played a key role in both the second Hague Peace Conference and the Paris Peace Conference, was notably not a member of the League, although several of its nationals served as judges of the Court.

From its first session in 1922 until 1940, the PCIJ dealt with 29 interstate disputes and issued 27 advisory opinions. The Court's widespread acceptance was reflected by the fact that several hundred international treaties and agreements conferred jurisdiction upon it over specified categories of disputes. In addition to helping resolve several serious international disputes, the PCIJ helped clarify several ambiguities in international law that contributed to its development.

The United States played a major role in setting up the World Court but never joined. Presidents Wilson, Harding, Coolidge, Hoover and Roosevelt all supported membership, but it was impossible to get a 2/3 majority in the Senate for a treaty.

Following a peak of activity in 1933, the PCIJ began to decline in its activities due to the growing international tension and isolationism that characterized the era. The Second World War effectively put an end to the Court, which held its last public session in December 1939 and issued its last orders in February 1940. In 1942 the United States and United Kingdom jointly declared support for establishing or re-establishing an international court after the war, and in 1943, the U.K. chaired a panel of jurists from around the world, the "Inter-Allied Committee", to discuss the matter. Its 1944 report recommended that:


Several months later, a conference of the major Allied Powers—China, the USSR, the U.K., and the U.S.—issued a joint declaration recognizing the necessity “of establishing at the earliest practicable date a general international organization, based on the principle of the sovereign equality of all peace-loving States, and open to membership by all such States, large and small, for the maintenance of international peace and security”.

The following Allied conference at Dumbarton Oaks, in the United States, published a proposal in October 1944 that called for the establishment of an intergovernmental organization that would include an international court. A meeting was subsequently convened in Washington, D.C. in April 1945, involving 44 jurists from around the world to draft a statute for the proposed court. The draft statute was substantially similar to that of the PCIJ, and it was questioned whether a new court should even be created. During the San Francisco Conference, which took place from 25 April to 26 June 1945 and involved 50 countries, it was decided that an entirely new court should be established as a principal organ of the new United Nations. The statute of this court would form an integral part of the United Nations Charter, which, to maintain continuity, expressly held that the Statute of the International Court of Justice (ICJ) was based upon that of the PCIJ.

Consequently, the PCIJ convened for the last time in October 1945 and resolved to transfer its archives to its successor, which would take its place at the Peace Palace. The judges of the PCIJ all resigned on 31 January 1946, with the election of the first members of the ICJ taking place the following February at the First Session of the United Nations General Assembly and Security Council. In April 1946, the PCIJ was formally dissolved, and the ICJ, in its first meeting, elected as President José Gustavo Guerrero of El Salvador, who had served as the last President of the PCIJ. The Court also appointed members of its Registry, drawn largely from that of the PCIJ, and held an inaugural public sitting later that month.

The first case was submitted in May 1947 by the United Kingdom against Albania concerning incidents in the Corfu Channel.

Established in 1945 by the UN Charter, the court began work in 1946 as the successor to the Permanent Court of International Justice. The Statute of the International Court of Justice, similar to that of its predecessor, is the main constitutional document constituting and regulating the court.

The court's workload covers a wide range of judicial activity. After the court ruled that the United States's covert war against Nicaragua was in violation of international law ("Nicaragua v. United States"), the United States withdrew from compulsory jurisdiction in 1986 to accept the court's jurisdiction only on a discretionary basis. Chapter XIV of the United Nations Charter authorizes the UN Security Council to enforce Court rulings. However, such enforcement is subject to the veto power of the five permanent members of the Council, which the United States used in the "Nicaragua" case.

The ICJ is composed of fifteen judges elected to nine-year terms by the UN General Assembly and the UN Security Council from a list of people nominated by the national groups in the Permanent Court of Arbitration. The election process is set out in Articles 4–19 of the ICJ Statute. Elections are staggered, with five judges elected every three years to ensure continuity within the court. Should a judge die in office, the practice has generally been to elect a judge in a special election to complete the term. Judges of the International Court of Justice are entitled to the style of His/Her Excellency. 

No two judges may be nationals of the same country. According to Article 9, the membership of the court is supposed to represent the "main forms of civilization and of the principal legal systems of the world". Essentially, that has meant common law, civil law and socialist law (now post-communist law).

There is an informal understanding that the seats will be distributed by geographic regions so that there are five seats for Western countries, three for African states (including one judge of francophone civil law, one of Anglophone common law and one Arab), two for Eastern European states, three for Asian states and two for Latin American and Caribbean states. For most of the court's history, the five permanent members of the United Nations Security Council (France, USSR, China, the United Kingdom, and the United States) have always had a judge serving, thereby occupying three of the Western seats, one of the Asian seats and one of the Eastern European seats. Exceptions have been China not having a judge on the court from 1967 to 1985, during which time it did not put forward a candidate, and British judge Sir Christopher Greenwood being withdrawn as a candidate for election for a second nine-year term on the bench in 2017, leaving no judges from the United Kingdom on the court. Greenwood had been supported by the UN Security Council but failed to get a majority in the UN General Assembly. Indian judge Dalveer Bhandari instead took the seat.

Article 6 of the Statute provides that all judges should be "elected regardless of their nationality among persons of high moral character" who are either qualified for the highest judicial office in their home states or known as lawyers with sufficient competence in international law. Judicial independence is dealt with specifically in Articles 16–18. Judges of the ICJ are not able to hold any other post or act as counsel. In practice, members of the court have their own interpretation of these rules and allow them to be involved in outside arbitration and hold professional posts as long as there is no conflict of interest. A judge can be dismissed only by a unanimous vote of the other members of the court. Despite these provisions, the independence of ICJ judges has been questioned. For example, during the "Nicaragua" case, the United States issued a communiqué suggesting that it could not present sensitive material to the court because of the presence of judges from the Soviet bloc.

Judges may deliver joint judgments or give their own separate opinions. Decisions and advisory opinions are by majority, and, in the event of an equal division, the President's vote becomes decisive, which occurred in the "Legality of the Use by a State of Nuclear Weapons in Armed Conflict" (Opinion requested by WHO), [1996] ICJ Reports 66. Judges may also deliver separate dissenting opinions.

Article 31 of the statute sets out a procedure whereby "ad hoc" judges sit on contentious cases before the court. The system allows any party to a contentious case (if it otherwise does not have one of that party's nationals sitting on the court) to select one additional person to sit as a judge on that case only. It is thus possible that as many as seventeen judges may sit on one case.

The system may seem strange when compared with domestic court processes, but its purpose is to encourage states to submit cases. For example, if a state knows that it will have a judicial officer who can participate in deliberation and offer other judges local knowledge and an understanding of the state's perspective, it may be more willing to submit to the jurisdiction of the court. Although this system does not sit well with the judicial nature of the body, it is usually of little practical consequence. "Ad hoc" judges usually (but not always) vote in favour of the state that appointed them and thus cancel each other out.

Generally, the court sits as full bench, but in the last fifteen years, it has on occasion sat as a chamber. Articles 26–29 of the statute allow the court to form smaller chambers, usually 3 or 5 judges, to hear cases. Two types of chambers are contemplated by Article 26: firstly, chambers for special categories of cases, and second, the formation of "ad hoc" chambers to hear particular disputes. In 1993, a special chamber was established, under Article 26(1) of the ICJ statute, to deal specifically with environmental matters (although it has never been used).

"Ad hoc" chambers are more frequently convened. For example, chambers were used to hear the "Gulf of Maine Case" (Canada/US). In that case, the parties made clear they would withdraw the case unless the court appointed judges to the chamber acceptable to the parties. Judgments of chambers may have either less authority than full Court judgments or diminish the proper interpretation of universal international law informed by a variety of cultural and legal perspectives. On the other hand, the use of chambers might encourage greater recourse to the court and thus enhance international dispute resolution.

, the composition of the court is as follows:

As stated in Article 93 of the UN Charter, all UN members are automatically parties to the court's statute. Non-UN members may also become parties to the court's statute under the Article 93(2) procedure. For example, before becoming a UN member state, Switzerland used this procedure in 1948 to become a party, and Nauru became a party in 1988. Once a state is a party to the court's statute, it is entitled to participate in cases before the court. However, being a party to the statute does not automatically give the court jurisdiction over disputes involving those parties. The issue of jurisdiction is considered in the three types of ICJ cases: contentious issues, incidental jurisdiction, and advisory opinions.

In contentious cases (adversarial proceedings seeking to settle a dispute), the ICJ produces a binding ruling between states that agree to submit to the ruling of the court. Only states may be parties in contentious cases. Individuals, corporations, component parts of a federal state, NGOs, UN organs and self-determination groups are excluded from direct participation in cases although the court may receive information from public international organizations. That does not preclude non-state interests from being the subject of proceedings if a state brings the case against another. For example, a state may, in cases of "diplomatic protection", bring a case on behalf of one of its nationals or corporations.

Jurisdiction is often a crucial question for the court in contentious cases. (See Procedure below.) The key principle is that the ICJ has jurisdiction only on the basis of consent. Article 36 outlines four bases on which the court's jurisdiction may be founded:

Until rendering a final judgment, the court has competence to order interim measures for the protection of the rights of a party to a dispute. One or both parties to a dispute may apply the ICJ for issuing interim measures. In the "Frontier Dispute" Case, both parties to the dispute, Burkina Faso and Mali, submitted an application to the court to indicate interim measures. Incidental jurisdiction of the court derives from the Article 41 of the Statute of it. Such as the final judgment, the order for interim measures of the court are binding on state parties to the dispute. The ICJ has competence to indicate interim measures only if the "prima facie" jurisdiction is satisfied.

 An advisory opinion is a function of the court open only to specified United Nations bodies and agencies. The UN Charter grants the General Assembly or the Security Council a power to request the court to issue an advisory opinion on any legal question. Other organs of the UN rather than GA and SC may not request an advisory opinion of the ICJ unless the General Assembly authorizes them. Other organs of the UN only request an advisory opinion of the court regarding the matters falling into the scope of their activities. On receiving a request, the court decides which states and organizations might provide useful information and gives them an opportunity to present written or oral statements. Advisory opinions were intended as a means by which UN agencies could seek the court's help in deciding complex legal issues that might fall under their respective mandates.

In principle, the court's advisory opinions are only consultative in character but they are influential and widely respected. Certain instruments or regulations can provide in advance that the advisory opinion shall be specifically binding on particular agencies or states, but inherently, they are non-binding under the Statute of the Court. This non-binding character does not mean that advisory opinions are without legal effect, because the legal reasoning embodied in them reflects the court's authoritative views on important issues of international law. In arriving at them, the court follows essentially the same rules and procedures that govern its binding judgments delivered in contentious cases submitted to it by sovereign states.

An advisory opinion derives its status and authority from the fact that it is the official pronouncement of the principal judicial organ of the United Nations.

Advisory opinions have often been controversial because the questions asked are controversial or the case was pursued as an indirect way of bringing what is really a contentious case before the court. Examples of advisory opinions can be found in the section advisory opinions in the List of International Court of Justice cases article. One such well-known advisory opinion is the "Nuclear Weapons Case".


Article 94 establishes the duty of all UN members to comply with decisions of the court involving them. If parties do not comply, the issue may be taken before the Security Council for enforcement action. There are obvious problems with such a method of enforcement. If the judgment is against one of the permanent five members of the Security Council or its allies, any resolution on enforcement would then be vetoed. That occurred, for example, after the "Nicaragua" case, when Nicaragua brought the issue of the United States' noncompliance with the court's decision before the Security Council. Furthermore, if the Security Council refuses to enforce a judgment against any other state, there is no method of forcing the state to comply. Furthermore, the most effective form to take action for the Security Council, coercive action under Chapter VII of the United Nations Charter, can be justified only if international peace and security are at stake. The Security Council has never done that so far.

The relationship between the ICJ and the Security Council, and the separation of their powers, was considered by the court in 1992 in the "Pan Am" case. The court had to consider an application from Libya for the order of provisional measures of protection to safeguard its rights, which, it alleged, were being infringed by the threat of economic sanctions by the United Kingdom and United States. The problem was that these sanctions had been authorized by the Security Council, which resulted in a potential conflict between the Chapter VII functions of the Security Council and the judicial function of the court. The court decided, by eleven votes to five, that it could not order the requested provisional measures because the rights claimed by Libya, even if legitimate under the Montreal Convention, could not be "prima facie" regarded as appropriate since the action was ordered by the Security Council. In accordance with Article 103 of the UN Charter, obligations under the Charter took precedence over other treaty obligations. Nevertheless, the court declared the application admissible in 1998. A decision on the merits has not been given since the parties (United Kingdom, United States, and Libya) settled the case out of court in 2003.

There was a marked reluctance on the part of a majority of the court to become involved in a dispute in such a way as to bring it potentially into conflict with the Council. The court stated in the "Nicaragua" case that there is no necessary inconsistency between action by the Security Council and adjudication by the ICJ. However, when there is room for conflict, the balance appears to be in favour of the Security Council.

Should either party fail "to perform the obligations incumbent upon it under a judgment rendered by the Court", the Security Council may be called upon to "make recommendations or decide upon measures" if the Security Council deems such actions necessary. In practice, the court's powers have been limited by the unwillingness of the losing party to abide by the court's ruling and by the Security Council's unwillingness to impose consequences. However, in theory, "so far as the parties to the case are concerned, a judgment of the Court is binding, final and without appeal", and "by signing the Charter, a State Member of the United Nations undertakes to comply with any decision of the International Court of Justice in a case to which it is a party."

For example, the United States had previously accepted the court's compulsory jurisdiction upon its creation in 1946 but in 1984, after "Nicaragua v. United States", withdrew its acceptance following the court's judgment that called on the US to "cease and to refrain" from the "unlawful use of force" against the government of Nicaragua. The court ruled (with only the American judge dissenting) that the United States was "in breach of its obligation under the Treaty of Friendship with Nicaragua not to use force against Nicaragua" and ordered the United States to pay war reparations.

When deciding cases, the court applies international law as summarized in of the ICJ Statute, which provides that in arriving at its decisions the court shall apply international conventions, international custom and the "general principles of law recognized by civilized nations." It may also refer to academic writing ("the teachings of the most highly qualified publicists of the various nations") and previous judicial decisions to help interpret the law although the court is not formally bound by its previous decisions under the doctrine of stare decisis. makes clear that the common law notion of precedent or "stare decisis" does not apply to the decisions of the ICJ. The court's decision binds only the parties to that particular controversy. Under 38(1)(d), however, the court may consider its own previous decisions.

If the parties agree, they may also grant the court the liberty to decide "ex aequo et bono" ("in justice and fairness"), granting the ICJ the freedom to make an equitable decision based on what is fair under the circumstances. That provision has not been used in the court's history. So far, the International Court of Justice has dealt with about 130 cases.

The ICJ is vested with the power to make its own rules. Court procedure is set out in the "Rules of Court of the International Court of Justice 1978" (as amended on 29 September 2005).

Cases before the ICJ will follow a standard pattern. The case is lodged by the applicant, which files a written memorial setting out the basis of the court's jurisdiction and the merits of its claim. The respondent may accept the court's jurisdiction and file its own memorial on the merits of the case.

A respondent that does not wish to submit to the jurisdiction of the court may raise preliminary objections. Any such objections must be ruled upon before the court can address the merits of the applicant's claim. Often, a separate public hearing is held on the preliminary objections and the court will render a judgment. Respondents normally file preliminary objections to the jurisdiction of the court and/or the admissibility of the case. Inadmissibility refers to a range of arguments about factors the court should take into account in deciding jurisdiction, such as the fact that the issue is not justiciable or that it is not a "legal dispute".

In addition, objections may be made because all necessary parties are not before the court. If the case necessarily requires the court to rule on the rights and obligations of a state that has not consented to the court's jurisdiction, the court does not proceed to issue a judgment on the merits.

If the court decides it has jurisdiction and the case is admissible, the respondent then is required to file a Memorial addressing the merits of the applicant's claim. Once all written arguments are filed, the court holds a public hearing on the merits.

Once a case has been filed, any party (usually the applicant) may seek an order from the court to protect the "status quo" pending the hearing of the case. Such orders are known as Provisional (or Interim) Measures and are analogous to interlocutory injunctions in United States law. Article 41 of the statute allows the court to make such orders. The court must be satisfied to have "prima facie" jurisdiction to hear the merits of the case before it grants provisional measures.

In cases in which a third state's interests are affected, that state may be permitted to intervene in the case and participate as a full party. Under Article 62, a state "with an interest of a legal nature" may apply; however, it is within the court's discretion whether or not to allow the intervention. Intervention applications are rare, and the first successful application occurred only in 1991.

Once deliberation has taken place, the court issues a majority opinion. Individual judges may issue concurring opinions (if they agree with the outcome reached in the judgment of the court but differ in their reasoning) or dissenting opinions (if they disagree with the majority). No appeal is possible, but any party may ask for the court to clarify if there is a dispute as to the meaning or scope of the court's judgment.

The International Court has been criticized with respect to its rulings, its procedures, and its authority. As with criticisms of the United Nations, many of these criticisms refer more to the general authority assigned to the body by member states through its charter than to specific problems with the composition of judges or their rulings. Major criticisms include the following:






</doc>
<doc id="14919" url="https://en.wikipedia.org/wiki?curid=14919" title="International Standard Book Number">
International Standard Book Number

The International Standard Book Number (ISBN) is a numeric commercial book identifier which is intended to be unique. Publishers purchase ISBNs from an affiliate of the International ISBN Agency.

An ISBN is assigned to each separate edition and variation (except reprintings) of a publication. For example, an e-book, a paperback and a hardcover edition of the same book will each have a different ISBN. The ISBN is ten digits long if assigned before 2007, and thirteen digits long if assigned on or after 1 January 2007. The method of assigning an ISBN is nation-specific and varies between countries, often depending on how large the publishing industry is within a country.

The initial ISBN identification format was devised in 1967, based upon the 9-digit Standard Book Numbering (SBN) created in 1966. The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108 (the 9-digit SBN code can be converted to a 10-digit ISBN by prefixing it with a zero digit '0').

Privately published books sometimes appear without an ISBN. The International ISBN Agency sometimes assigns such books ISBNs on its own initiative.

Another identifier, the International Standard Serial Number (ISSN), identifies periodical publications such as magazines and newspapers. The International Standard Music Number (ISMN) covers musical scores.

The Standard Book Number (SBN) is a commercial system using nine-digit code numbers to identify books. It was created by Gordon Foster, Emeritus Professor of Statistics at Trinity College, Dublin, for the booksellers and stationers WHSmith and others in 1965. The ISBN identification format was conceived in 1967 in the United Kingdom by David Whitaker (regarded as the "Father of the ISBN") and in 1968 in the United States by Emery Koltay (who later became director of the U.S. ISBN agency R. R. Bowker).

The 10-digit ISBN format was developed by the International Organization for Standardization (ISO) and was published in 1970 as international standard ISO 2108. The United Kingdom continued to use the nine-digit SBN code until 1974. ISO has appointed the International ISBN Agency as the registration authority for ISBN worldwide and the ISBN Standard is developed under the control of ISO Technical Committee 46/Subcommittee 9 TC 46/SC 9. The ISO on-line facility only refers back to 1978.

An SBN may be converted to an ISBN by prefixing the digit "0". For example, the second edition of "Mr. J. G. Reeder Returns", published by Hodder in 1965, has , where "340" indicates the publisher, "01381" is the serial number assigned by the publisher, and "8" is the check digit. By prefixing a zero, this can be converted to ; the check digit does not need to be re-calculated. Some publishers, such as Ballantine Books, would sometimes use 12-digit SBNs where the last three digits indicated the price of the book; for example, "Woodstock Handmade Houses" had a 12-digit Standard Book Number of 345-24223-8-595 (valid SBN: 345-24223-8, : 0-345-24223-8), and it cost .

Since 1 January 2007, ISBNs have contained thirteen digits, a format that is compatible with "Bookland" European Article Numbers, which have 13 digits.

A separate ISBN is assigned to each edition and variation (except reprintings) of a publication. For example, an ebook, audiobook, paperback, and hardcover edition of the same book will each have a different ISBN assigned to it. The ISBN is thirteen digits long if assigned on or after 1 January 2007, and ten digits long if assigned before 2007. An International Standard Book Number consists of four parts (if it is a 10-digit ISBN) or five parts (for a 13-digit ISBN).

Section 5 of the International ISBN Agency's official user manual describes the structure of the 13-digit ISBN, as follows:

A 13-digit ISBN can be separated into its parts ("prefix element", "registration group", "registrant", "publication" and "check digit"), and when this is done it is customary to separate the parts with hyphens or spaces. Separating the parts ("registration group", "registrant", "publication" and "check digit") of a 10-digit ISBN is also done with either hyphens or spaces. Figuring out how to correctly separate a given ISBN is complicated, because most of the parts do not use a fixed number of digits.

ISBN issuance is country-specific, in that ISBNs are issued by the ISBN registration agency that is responsible for that country or territory regardless of the publication language. The ranges of ISBNs assigned to any particular country are based on the publishing profile of the country concerned, and so the ranges will vary depending on the number of books and the number, type, and size of publishers that are active. Some ISBN registration agencies are based in national libraries or within ministries of culture and thus may receive direct funding from government to support their services. In other cases, the ISBN registration service is provided by organisations such as bibliographic data providers that are not government funded.

A full directory of ISBN agencies is available on the International ISBN Agency website. List for a few countries is given below:


The ISBN registration group identifier is a 1- to 5-digit number that is valid within a single prefix element (i.e. one of 978 or 979), and can be separated between hyphens, such as . Registration group identifiers have primarily been allocated within the 978 prefix element. The single-digit group identifiers within the 978-prefix element are: 0 or 1 for English-speaking countries; 2 for French-speaking countries; 3 for German-speaking countries; 4 for Japan; 5 for Russian-speaking countries; and 7 for People's Republic of China. An example 5-digit group identifier is 99936, for Bhutan. The allocated group IDs are: 0–5, 600–625, 65, 7, 80–94, 950–989, 9917–9989, and 99901–99983. Books published in rare languages typically have longer group identifiers.

Within the 979 prefix element, the registration group identifier 0 is reserved for compatibility with International Standard Music Numbers (ISMNs), but such material is not actually assigned an ISBN. The registration group identifiers within prefix element 979 that have been assigned are 8 for the United States of America, 10 for France, 11 for the Republic of Korea, and 12 for Italy.

The original 9-digit standard book number (SBN) had no registration group identifier, but prefixing a zero (0) to a 9-digit SBN creates a valid 10-digit ISBN.

The national ISBN agency assigns the registrant element (cf. ) and an accompanying series of ISBNs within that registrant element to the publisher; the publisher then allocates one of the ISBNs to each of its books. In most countries, a book publisher is not legally required to assign an ISBN, although most large bookstores only handle publications that have ISBNs assigned to them.

A listing of more than 900,000 assigned publisher codes is published, and can be ordered in book form. The web site of the ISBN agency does not offer any free method of looking up publisher codes. Partial lists have been compiled (from library catalogs) for the English-language groups: identifier 0 and identifier 1.

Publishers receive blocks of ISBNs, with larger blocks allotted to publishers expecting to need them; a small publisher may receive ISBNs of one or more digits for the registration group identifier, several digits for the registrant, and a single digit for the publication element. Once that block of ISBNs is used, the publisher may receive another block of ISBNs, with a different registrant element. Consequently, a publisher may have different allotted registrant elements. There also may be more than one registration group identifier used in a country. This might occur once all the registrant elements from a particular registration group have been allocated to publishers.

By using variable block lengths, registration agencies are able to customise the allocations of ISBNs that they make to publishers. For example, a large publisher may be given a block of ISBNs where fewer digits are allocated for the registrant element and many digits are allocated for the publication element; likewise, countries publishing many titles have few allocated digits for the registration group identifier and many for the registrant and publication elements. Here are some sample <nowiki>ISBN-10</nowiki> codes, illustrating block length variations.
English-language registration group elements are 0 and 1 (2 of more than 220 registration group elements). These two registration group elements are divided into registrant elements in a systematic pattern, which allows their length to be determined, as follows:

A check digit is a form of redundancy check used for error detection, the decimal equivalent of a binary check bit. It consists of a single digit computed from the other digits in the number. The method for the 10-digit ISBN is an extension of that for SBNs, so the two systems are compatible; an SBN prefixed with a zero (the 10-digit ISBN) will give the same check digit as the SBN without the zero. The check digit is base eleven, and can be an integer between 0 and 9, or an 'X'. The system for 13-digit ISBNs is not compatible with SBNs and will, in general, give a different check digit from the corresponding 10-digit ISBN, so does not provide the same protection against transposition. This is because the 13-digit code was required to be compatible with the EAN format, and hence could not contain an 'X'.

According to the 2001 edition of the International ISBN Agency's official user manual, the <nowiki>ISBN-10</nowiki> check digit (which is the last digit of the 10-digit ISBN) must range from 0 to 10 (the symbol 'X' is used for 10), and must be such that the sum of the ten digits, each multiplied by its (integer) weight, descending from 10 to 1, is a multiple of 11. That is, if is the th digit, then must be chosen such that:

For example, for an <nowiki>ISBN-10</nowiki> of 0-306-40615-2:
Formally, using modular arithmetic, this is rendered:
It is also true for <nowiki>ISBN-10</nowiki>s that the sum of all ten digits, each multiplied by its weight in "ascending" order from 1 to 10, is a multiple of 11. For this example:
Formally, this is rendered:
The two most common errors in handling an ISBN (e.g. when typing it or writing it down) are a single altered digit or the transposition of adjacent digits. It can be proven mathematically that all pairs of valid <nowiki>ISBN-10</nowiki>s differ in at least two digits. It can also be proven that there are no pairs of valid <nowiki>ISBN-10</nowiki>s with eight identical digits and two transposed digits. (These proofs are true because the ISBN is less than eleven digits long and because 11 is a prime number.) The ISBN check digit method therefore ensures that it will always be possible to detect these two most common types of error, i.e., if either of these types of error has occurred, the result will never be a valid ISBN – the sum of the digits multiplied by their weights will never be a multiple of 11. However, if the error were to occur in the publishing house and remain undetected, the book would be issued with an invalid ISBN.

In contrast, it is possible for other types of error, such as two altered non-transposed digits, or three altered digits, to result in a valid ISBN (although it is still unlikely).

Each of the first nine digits of the 10-digit ISBN—excluding the check digit itself—is multiplied by its (integer) weight, descending from 10 to 2, and the sum of these nine products found. The value of the check digit is simply the one number between 0 and 10 which, when added to this sum, means the total is a multiple of 11.

For example, the check digit for an <nowiki>ISBN-10</nowiki> of 0-306-40615-"?" is calculated as follows:

Adding 2 to 130 gives a multiple of 11 (because 132 = 12×11) – this is the only number between 0 and 10 which does so. Therefore, the check digit has to be 2, and the complete sequence is <nowiki>ISBN 0-306-40615-2</nowiki>. If the value of formula_1 required to satisfy this condition is 10, then an 'X' should be used.

Alternatively, modular arithmetic is convenient for calculating the check digit using modulus 11. The remainder of this sum when it is divided by 11 (i.e. its value modulo 11), is computed. This remainder plus the check digit must equal either 0 or 11. Therefore, the check digit is (11 minus the remainder of the sum of the products modulo 11) modulo 11. Taking the remainder modulo 11 a second time accounts for the possibility that the first remainder is 0. Without the second modulo operation, the calculation could result in a check digit value of = 11, which is invalid. (Strictly speaking, the "first" "modulo 11" is not needed, but it may be considered to simplify the calculation.)

For example, the check digit for the <nowiki>ISBN-10</nowiki> of 0-306-40615-"?" is calculated as follows:
Thus the check digit is 2.

It is possible to avoid the multiplications in a software implementation by using two accumulators. Repeatedly adding codice_1 into codice_2 computes the necessary multiples:

// Returns ISBN error syndrome, zero for a valid ISBN, non-zero for an invalid one.
// digits[i] must be between 0 and 10.
int CheckISBN(int const digits[10])

The modular reduction can be done once at the end, as shown above (in which case codice_2 could hold a value as large as 496, for the invalid <nowiki>ISBN 99999-999-9-X</nowiki>), or codice_2 and codice_1 could be reduced by a conditional subtract after each addition.

Appendix 1 of the International ISBN Agency's official user manual describes how the 13-digit ISBN check digit is calculated. The <nowiki>ISBN-13</nowiki> check digit, which is the last digit of the ISBN, must range from 0 to 9 and must be such that the sum of all the thirteen digits, each multiplied by its (integer) weight, alternating between 1 and 3, is a multiple of 10.

Formally, using modular arithmetic, this is rendered:
The calculation of an <nowiki>ISBN-13</nowiki> check digit begins with the first twelve digits of the 13-digit ISBN (thus excluding the check digit itself). Each digit, from left to right, is alternately multiplied by 1 or 3, then those products are summed modulo 10 to give a value ranging from 0 to 9. Subtracted from 10, that leaves a result from 1 to 10. A zero (0) replaces a ten (10), so, in all cases, a single check digit results.

For example, the <nowiki>ISBN-13</nowiki> check digit of 978-0-306-40615-"?" is calculated as follows:

Thus, the check digit is 7, and the complete sequence is <nowiki>ISBN 978-0-306-40615-7</nowiki>.

In general, the <nowiki>ISBN-13</nowiki> check digit is calculated as follows.

Let
Then
This check system – similar to the UPC check digit formula – does not catch all errors of adjacent digit transposition. Specifically, if the difference between two adjacent digits is 5, the check digit will not catch their transposition. For instance, the above example allows this situation with the 6 followed by a 1. The correct order contributes 3×6+1×1 = 19 to the sum; while, if the digits are transposed (1 followed by a 6), the contribution of those two digits will be 3×1+1×6 = 9. However, 19 and 9 are congruent modulo 10, and so produce the same, final result: both ISBNs will have a check digit of 7. The <nowiki>ISBN-10</nowiki> formula uses the prime modulus 11 which avoids this blind spot, but requires more than the digits 0–9 to express the check digit.

Additionally, if the sum of the 2nd, 4th, 6th, 8th, 10th, and 12th digits is tripled then added to the remaining digits (1st, 3rd, 5th, 7th, 9th, 11th, and 13th), the total will always be divisible by 10 (i.e., end in 0).

An <nowiki>ISBN-10</nowiki> is converted to <nowiki>ISBN-13</nowiki> by prepending "978" to the <nowiki>ISBN-10</nowiki> and recalculating the final checksum digit using the <nowiki>ISBN-13</nowiki> algorithm. The reverse process can also be performed, but not for numbers commencing with a prefix other than 978, which have no 10-digit equivalent.

Publishers and libraries have varied policies about the use of the ISBN check digit. Publishers sometimes fail to check the correspondence of a book title and its ISBN before publishing it; that failure causes book identification problems for libraries, booksellers, and readers. For example, is shared by two books – "Ninja gaiden®: a novel based on the best-selling game by Tecmo" (1990) and "Wacky laws" (1997), both published by Scholastic.

Most libraries and booksellers display the book record for an invalid ISBN issued by the publisher. The Library of Congress catalogue contains books published with invalid ISBNs, which it usually tags with the phrase "Cancelled ISBN". However, book-ordering systems such as Amazon.com will not search for a book if an invalid ISBN is entered to its search engine. OCLC often indexes by invalid ISBNs, if the book is indexed in that way by a member library.

Only the term "ISBN" should be used; the terms "eISBN" and "e-ISBN" have historically been sources of confusion and should be avoided. If a book exists in one or more digital (e-book) formats, each of those formats must have its own ISBN. In other words, each of the three separate EPUB, Amazon Kindle, and PDF formats of a particular book will have its own specific ISBN. They should not share the ISBN of the paper version, and there is no generic "eISBN" which encompasses all the e-book formats for a title.

Currently the barcodes on a book's back cover (or inside a mass-market paperback book's front cover) are EAN-13; they may have a separate barcode encoding five digits called an EAN-5 for the currency and the recommended retail price. For 10-digit ISBNs, the number "978", the Bookland "country code", is prefixed to the ISBN in the barcode data, and the check digit is recalculated according to the EAN-13 formula (modulo 10, 1x and 3x weighting on alternating digits).

Partly because of an expected shortage in certain ISBN categories, the International Organization for Standardization (ISO) decided to migrate to a 13-digit ISBN (<nowiki>ISBN-13</nowiki>). The process began on 1 January 2005 and was planned to conclude on 1 January 2007. , all the 13-digit ISBNs began with 978. As the 978 ISBN supply is exhausted, the 979 prefix was introduced. Part of the 979 prefix is reserved for use with the Musicland code for musical scores with an ISMN. The 10-digit ISMN codes differed visually as they began with an "M" letter; the bar code represents the "M" as a zero (0), and for checksum purposes it counted as a 3. All ISMNs are now thirteen digits commencing ; to will be used by ISBN.

Publisher identification code numbers are unlikely to be the same in the 978 and 979 ISBNs, likewise, there is no guarantee that language area code numbers will be the same. Moreover, the 10-digit ISBN check digit generally is not the same as the 13-digit ISBN check digit. Because the GTIN-13 is part of the Global Trade Item Number (GTIN) system (that includes the GTIN-14, the GTIN-12, and the GTIN-8), the 13-digit ISBN falls within the 14-digit data field range.

Barcode format compatibility is maintained, because (aside from the group breaks) the <nowiki>ISBN-13</nowiki> barcode format is identical to the EAN barcode format of existing 10-digit ISBNs. So, migration to an EAN-based system allows booksellers the use of a single numbering system for both books and non-book products that is compatible with existing ISBN based data, with only minimal changes to information technology systems. Hence, many booksellers (e.g., Barnes & Noble) migrated to EAN barcodes as early as March 2005. Although many American and Canadian booksellers were able to read EAN-13 barcodes before 2005, most general retailers could not read them. The upgrading of the UPC barcode system to full EAN-13, in 2005, eased migration to the <nowiki>ISBN-13</nowiki> in North America.




</doc>
<doc id="14921" url="https://en.wikipedia.org/wiki?curid=14921" title="IP address">
IP address

An Internet Protocol address (IP address) is a numerical label assigned to each device connected to a computer network that uses the Internet Protocol for communication.
An IP address serves two main functions: host or network interface identification and location addressing.

Internet Protocol version 4 (IPv4) defines an IP address as a 32-bit number. However, because of the growth of the Internet and the depletion of available IPv4 addresses, a new version of IP (IPv6), using 128 bits for the IP address, was standardized in 1998. IPv6 deployment has been ongoing since the mid-2000s.

IP addresses are written and displayed in human-readable notations, such as in IPv4, and in IPv6. The size of the routing prefix of the address is designated in CIDR notation by suffixing the address with the number of significant bits, e.g., , which is equivalent to the historically used subnet mask .

The IP address space is managed globally by the Internet Assigned Numbers Authority (IANA), and by five regional Internet registries (RIRs) responsible in their designated territories for assignment to local Internet registries, such as Internet service providers (ISPs), and other end users. IPv4 addresses were distributed by IANA to the RIRs in blocks of approximately 16.8 million addresses each, but have been exhausted at the IANA level since 2011. Only one of the RIRs still has a supply for local assignments in Africa. Some IPv4 addresses are reserved for private networks and are not globally unique.

Network administrators assign an IP address to each device connected to a network. Such assignments may be on a "static" (fixed or permanent) or "dynamic" basis, depending on network practices and software features.

An IP address serves two principal functions. It identifies the host, or more specifically its network interface, and it provides the location of the host in the network, and thus the capability of establishing a path to that host. Its role has been characterized as follows: "A name indicates what we seek. An address indicates where it is. A route indicates how to get there."
The header of each IP packet contains the IP address of the sending host, and that of the destination host.

Two versions of the Internet Protocol are in common use on the Internet today. The original version of the Internet Protocol that was first deployed in 1983 in the ARPANET, the predecessor of the Internet, is Internet Protocol version 4 (IPv4).

The rapid exhaustion of IPv4 address space available for assignment to Internet service providers and end-user organizations by the early 1990s, prompted the Internet Engineering Task Force (IETF) to explore new technologies to expand the addressing capability on the Internet. The result was a redesign of the Internet Protocol which became eventually known as Internet Protocol Version 6 (IPv6) in 1995.
IPv6 technology was in various testing stages until the mid-2000s when commercial production deployment commenced.

Today, these two versions of the Internet Protocol are in simultaneous use. Among other technical changes, each version defines the format of addresses differently. Because of the historical prevalence of IPv4, the generic term "IP address" typically still refers to the addresses defined by IPv4. The gap in version sequence between IPv4 and IPv6 resulted from the assignment of version 5 to the experimental Internet Stream Protocol in 1979, which however was never referred to as IPv5.

Other versions v1 to v9 were defined, but only v4 and v6 ever gained widespread use. v1 and v2 were names for TCP protocols in 1974 and 1977, as there was to separate IP specification at the time. v3 was defined in 1978, and v3.1 is the first version where TCP is separated from IP. v6 is a synthesis of several suggested versions, v6 "Simple Internet Protocol", v7 "TP/IX: The Next Internet", v8 "PIP — The P Internet Protocol", and v9 "TUBA — Tcp & Udp with Big Addresses".

IP networks may be divided into subnetworks in both IPv4 and IPv6. For this purpose, an IP address is recognized as consisting of two parts: the "network prefix" in the high-order bits and the remaining bits called the "rest field", "host identifier", or "interface identifier" (IPv6), used for host numbering within a network. The subnet mask or CIDR notation determines how the IP address is divided into network and host parts.

The term "subnet mask" is only used within IPv4. Both IP versions however use the CIDR concept and notation. In this, the IP address is followed by a slash and the number (in decimal) of bits used for the network part, also called the "routing prefix". For example, an IPv4 address and its subnet mask may be and , respectively. The CIDR notation for the same IP address and subnet is , because the first 24 bits of the IP address indicate the network and subnet.

An IPv4 address has a size of 32 bits, which limits the address space to (2) addresses. Of this number, some addresses are reserved for special purposes such as private networks (~18 million addresses) and multicast addressing (~270 million addresses).

IPv4 addresses are usually represented in dot-decimal notation, consisting of four decimal numbers, each ranging from 0 to 255, separated by dots, e.g., . Each part represents a group of 8 bits (an octet) of the address. In some cases of technical writing, IPv4 addresses may be presented in various hexadecimal, octal, or binary representations.

In the early stages of development of the Internet Protocol, the network number was always the highest order octet (most significant eight bits). Because this method allowed for only 256 networks, it soon proved inadequate as additional networks developed that were independent of the existing networks already designated by a network number. In 1981, the addressing specification was revised with the introduction of classful network architecture.

Classful network design allowed for a larger number of individual network assignments and fine-grained subnetwork design. The first three bits of the most significant octet of an IP address were defined as the "class" of the address. Three classes ("A", "B", and "C") were defined for universal unicast addressing. Depending on the class derived, the network identification was based on octet boundary segments of the entire address. Each class used successively additional octets in the network identifier, thus reducing the possible number of hosts in the higher order classes ("B" and "C"). The following table gives an overview of this now obsolete system.

Classful network design served its purpose in the startup stage of the Internet, but it lacked scalability in the face of the rapid expansion of networking in the 1990s. The class system of the address space was replaced with Classless Inter-Domain Routing (CIDR) in 1993. CIDR is based on variable-length subnet masking (VLSM) to allow allocation and routing based on arbitrary-length prefixes. Today, remnants of classful network concepts function only in a limited scope as the default configuration parameters of some network software and hardware components (e.g. netmask), and in the technical jargon used in network administrators' discussions.

Early network design, when global end-to-end connectivity was envisioned for communications with all Internet hosts, intended that IP addresses be globally unique. However, it was found that this was not always necessary as private networks developed and public address space needed to be conserved.

Computers not connected to the Internet, such as factory machines that communicate only with each other via TCP/IP, need not have globally unique IP addresses. Today, such private networks are widely used and typically connect to the Internet with network address translation (NAT), when needed.

Three non-overlapping ranges of IPv4 addresses for private networks are reserved. These addresses are not routed on the Internet and thus their use need not be coordinated with an IP address registry. Any user may use any of the reserved blocks. Typically, a network administrator will divide a block into subnets; for example, many home routers automatically use a default address range of through ().

In IPv6, the address size was increased from 32 bits in IPv4 to 128 bits, thus providing up to 2 (approximately ) addresses. This is deemed sufficient for the foreseeable future.

The intent of the new design was not to provide just a sufficient quantity of addresses, but also redesign routing in the Internet by allowing more efficient aggregation of subnetwork routing prefixes. This resulted in slower growth of routing tables in routers. The smallest possible individual allocation is a subnet for 2 hosts, which is the square of the size of the entire IPv4 Internet. At these levels, actual address utilization ratios will be small on any IPv6 network segment. The new design also provides the opportunity to separate the addressing infrastructure of a network segment, i.e. the local administration of the segment's available space, from the addressing prefix used to route traffic to and from external networks. IPv6 has facilities that automatically change the routing prefix of entire networks, should the global connectivity or the routing policy change, without requiring internal redesign or manual renumbering.

The large number of IPv6 addresses allows large blocks to be assigned for specific purposes and, where appropriate, to be aggregated for efficient routing. With a large address space, there is no need to have complex address conservation methods as used in CIDR.

All modern desktop and enterprise server operating systems include native support for the IPv6 protocol, but it is not yet widely deployed in other devices, such as residential networking routers, voice over IP (VoIP) and multimedia equipment, and some networking hardware.

Just as IPv4 reserves addresses for private networks, blocks of addresses are set aside in IPv6. In IPv6, these are referred to as unique local addresses (ULAs). The routing prefix is reserved for this block, which is divided into two blocks with different implied policies. The addresses include a 40-bit pseudorandom number that minimizes the risk of address collisions if sites merge or packets are misrouted.

Early practices used a different block for this purpose (), dubbed site-local addresses. However, the definition of what constituted a "site" remained unclear and the poorly defined addressing policy created ambiguities for routing. This address type was abandoned and must not be used in new systems.

Addresses starting with , called link-local addresses, are assigned to interfaces for communication on the attached link. The addresses are automatically generated by the operating system for each network interface. This provides instant and automatic communication between all IPv6 hosts on a link. This feature is used in the lower layers of IPv6 network administration, such as for the Neighbor Discovery Protocol.

Private and link-local address prefixes may not be routed on the public Internet.

IP addresses are assigned to a host either dynamically as they join the network, or persistently by configuration of the host hardware or software. Persistent configuration is also known as using a static IP address. In contrast, when a computer's IP address is assigned each time it restarts, this is known as using a dynamic IP address.

Dynamic IP addresses are assigned by network using Dynamic Host Configuration Protocol (DHCP). DHCP is the most frequently used technology for assigning addresses. It avoids the administrative burden of assigning specific static addresses to each device on a network. It also allows devices to share the limited address space on a network if only some of them are online at a particular time. Typically, dynamic IP configuration is enabled by default in modern desktop operating systems.

The address assigned with DHCP is associated with a "lease" and usually has an expiration period. If the lease is not renewed by the host before expiry, the address may be assigned to another device. Some DHCP implementations attempt to reassign the same IP address to a host, based on its MAC address, each time it joins the network. A network administrator may configure DHCP by allocating specific IP addresses based on MAC address.

DHCP is not the only technology used to assign IP addresses dynamically. Bootstrap Protocol is a similar protocol and predecessor to DHCP. Dialup and some broadband networks use dynamic address features of the Point-to-Point Protocol.

Computers and equipment used for the network infrastructure, such as routers and mail servers, are typically configured with static addressing.

In the absence or failure of static or dynamic address configurations, an operating system may assign a link-local address to a host using stateless address autoconfiguration.

A "sticky dynamic IP address" is an informal term used by cable and DSL Internet access subscribers to describe a dynamically assigned IP address that seldom changes. The addresses are usually assigned with DHCP. Since the modems are usually powered on for extended periods of time, the address leases are usually set to long periods and simply renewed. If a modem is turned off and powered up again before the next expiration of the address lease, it often receives the same IP address.

Address block is defined for the special use in link-local addressing for IPv4 networks. In IPv6, every interface, whether using static or dynamic address assignments, also receives a link-local address automatically in the block . These addresses are only valid on the link, such as a local network segment or point-to-point connection, to which a host is connected. These addresses are not routable and, like private addresses, cannot be the source or destination of packets traversing the Internet.

When the link-local IPv4 address block was reserved, no standards existed for mechanisms of address autoconfiguration. Filling the void, Microsoft developed a protocol called Automatic Private IP Addressing (APIPA), whose first public implementation appeared in Windows 98. APIPA has been deployed on millions of machines and became a de facto standard in the industry. In May 2005, the IETF defined a formal standard for it.

An IP address conflict occurs when two devices on the same local physical or wireless network claim to have the same IP address. A second assignment of an address generally stops the IP functionality of one or both of the devices. Many modern operating systems notify the administrator of IP address conflicts. When IP addresses are assigned by multiple people and systems with differing methods, any of them may be at fault. If one of the devices involved in the conflict is the default gateway access beyond the LAN for all devices on the LAN, all devices may be impaired.

IP addresses are classified into several classes of operational characteristics: unicast, multicast, anycast and broadcast addressing.

The most common concept of an IP address is in unicast addressing, available in both IPv4 and IPv6. It normally refers to a single sender or a single receiver, and can be used for both sending and receiving. Usually, a unicast address is associated with a single device or host, but a device or host may have more than one unicast address. Sending the same data to multiple unicast addresses requires the sender to send all the data many times over, once for each recipient.

Broadcasting is an addressing technique available in IPv4 to address data to all possible destinations on a network in one transmission operation as an "all-hosts broadcast". All receivers capture the network packet. The address is used for network broadcast. In addition, a more limited directed broadcast uses the all-ones host address with the network prefix. For example, the destination address used for directed broadcast to devices on the network is .

IPv6 does not implement broadcast addressing and replaces it with multicast to the specially defined all-nodes multicast address.

A multicast address is associated with a group of interested receivers. In IPv4, addresses through (the former Class D addresses) are designated as multicast addresses. IPv6 uses the address block with the prefix for multicast. In either case, the sender sends a single datagram from its unicast address to the multicast group address and the intermediary routers take care of making copies and sending them to all interested receivers (those that have joined the corresponding multicast group).

Like broadcast and multicast, anycast is a one-to-many routing topology. However, the data stream is not transmitted to all receivers, just the one which the router decides is closest in the network. Anycast addressing is a built-in feature of IPv6. In IPv4, anycast addressing is implemented with Border Gateway Protocol using the shortest-path metric to choose destinations. Anycast methods are useful for global load balancing and are commonly used in distributed DNS systems.

A host may use geolocation software to deduce the location of its communicating peer.
A public IP address, in common parlance, is a globally routable unicast IP address, meaning that the address is not an address reserved for use in private networks, such as those reserved by , or the various IPv6 address formats of local scope or site-local scope, for example for link-local addressing. Public IP addresses may be used for communication between hosts on the global Internet.

For security and privacy considerations, network administrators often desire to restrict public Internet traffic within their private networks. The source and destination IP addresses contained in the headers of each IP packet are a convenient means to discriminate traffic by IP address blocking or by selectively tailoring responses to external requests to internal servers. This is achieved with firewall software running on the network's gateway router. A database of IP addresses of restricted and permissible traffic may be maintained in blacklists and whitelists, respectively.

Multiple client devices can appear to share an IP address, either because they are part of a shared web hosting service environment or because an IPv4 network address translator (NAT) or proxy server acts as an intermediary agent on behalf of the client, in which case the real originating IP address is masked from the server receiving a request. A common practice is to have a NAT mask many devices in a private network. Only the public interface(s) of the NAT needs to have an Internet-routable address.

The NAT device maps different IP addresses on the private network to different TCP or UDP port numbers on the public network. In residential networks, NAT functions are usually implemented in a residential gateway. In this scenario, the computers connected to the router have private IP addresses and the router has a public address on its external interface to communicate on the Internet. The internal computers appear to share one public IP address.

Computer operating systems provide various diagnostic tools to examine network interfaces and address configuration. Microsoft Windows provides the command-line interface tools ipconfig and netsh and users of Unix-like systems may use ifconfig, netstat, route, lanstat, fstat, and iproute2 utilities to accomplish the task.


</doc>
<doc id="14922" url="https://en.wikipedia.org/wiki?curid=14922" title="If and only if">
If and only if

In logic and related fields such as mathematics and philosophy, if and only if (shortened as iff) is a biconditional logical connective between statements, where either both statements are true or both are false.

The connective is biconditional (a statement of material equivalence), and can be likened to the standard material conditional ("only if", equal to "if ... then") combined with its reverse ("if"); hence the name. The result is that the truth of either one of the connected statements requires the truth of the other (i.e. either both statements are true, or both are false), though it is controversial whether the connective thus defined is properly rendered by the English "if and only if" — with its pre-existing meaning. For example, "P if and only if Q" means that the only case in which "P" is true is if "Q" is also true, whereas in the case of "P if Q" there could be other scenarios where "P" is true when "Q" is false.

In writing, phrases commonly used as alternatives to P "if and only if" Q include: "Q is necessary and sufficient for P", "P is equivalent (or materially equivalent) to Q" (compare material implication), "P precisely if Q", "P precisely (or exactly) when Q", "P exactly in case Q", and "P just in case Q". Some authors regard "iff" as unsuitable in formal writing; others consider it a "borderline case" and tolerate its use.

In logical formulae, logical symbols are used instead of these phrases; see the discussion of notation.

The truth table of "P" formula_1 "Q" is as follows:
It is equivalent to that produced by the XNOR gate, and opposite to that produced by the XOR gate.

The corresponding logical symbols are "↔", "formula_1", and "≡", and sometimes "iff". These are usually treated as equivalent. However, some texts of mathematical logic (particularly those on first-order logic, rather than propositional logic) make a distinction between these, in which the first, ↔, is used as a symbol in logic formulas, while ⇔ is used in reasoning about those logic formulas (e.g., in metalogic). In Łukasiewicz's Polish notation, it is the prefix symbol 'E'.

Another term for this logical connective is exclusive nor.

In TeX "if and only if" is shown as a long double arrow: formula_3 via command \iff.

In most logical systems, one proves a statement of the form "P iff Q" by proving either "if P, then Q" and "if Q, then P", or "if P, then Q" and "if not-P, then not-Q". Proving these pair of statements sometimes leads to a more natural proof, since there are not obvious conditions in which one would infer a biconditional directly. An alternative is to prove the disjunction "(P and Q) or (not-P and not-Q)", which itself can be inferred directly from either of its disjuncts—that is, because "iff" is truth-functional, "P iff Q" follows if P and Q have been shown to be both true, or both false.

Usage of the abbreviation "iff" first appeared in print in John L. Kelley's 1955 book "General Topology".
Its invention is often credited to Paul Halmos, who wrote "I invented 'iff,' for 'if and only if'—but I could never believe I was really its first inventor."

It is somewhat unclear how "iff" was meant to be pronounced. In current practice, the single 'word' "iff" is almost always read as the four words "if and only if". However, in the preface of "General Topology", Kelley suggests that it should be read differently: "In some cases where mathematical content requires 'if and only if' and euphony demands something less I use Halmos' 'iff'". The authors of one discrete mathematics textbook suggest: "Should you need to pronounce iff, really hang on to the 'ff' so that people hear the difference from 'if'", implying that "iff" could be pronounced as .

Technically, definitions are always "if and only if" statements; some texts — such as Kelley's "General Topology" — follow the strict demands of logic, and use "if and only if" or "iff" in definitions of new terms. However, this logically correct usage of "if and only if" is relatively uncommon, as the majority of textbooks, research papers and articles (including English Wikipedia articles) follow the special convention to interpret "if" as "if and only if" whenever a mathematical definition is involved (as in "a topological space is compact if every open cover has a finite subcover").


Sufficiency is the converse of necessity. That is to say, given "P"→"Q" (i.e. if "P" then "Q"), "P" would be a sufficient condition for "Q", and "Q" would be a necessary condition for "P". Also, given "P"→"Q", it is true that "¬Q"→"¬P" (where ¬ is the negation operator, i.e. "not"). This means that the relationship between "P" and "Q", established by "P"→"Q", can be expressed in the following, all equivalent, ways:
As an example, take the first example, above, which states "P"→"Q", where "P" is "the fruit in question is an apple" and "Q" is "Madison will eat the fruit in question". The following are four equivalent ways of expressing this very relationship:
We see that, in the second example, above, can be restated in the form of "if...then" as "If Madison will eat the fruit in question, then it is an apple"; taking this in conjunction with the first example, we find that the third example can be stated as "If the fruit in question is an apple, then Madison will eat it; "and" if Madison will eat the fruit, then it is an apple".

Euler diagrams show logical relationships among events, properties, and so forth. "P only if Q", "if P then Q", and "P→Q" all mean that P is a subset, either proper or improper, of Q. "P if Q", "if Q then P", and Q→P all mean that Q is a proper or improper subset of P. "P if and only if Q" and "Q if and only if P" both mean that the sets P and Q are identical to each other.

Iff is used outside the field of logic as well. Wherever logic is applied, especially in mathematical discussions, it has the same meaning as above: it is an abbreviation for "if and only if", indicating that one statement is both necessary and sufficient for the other. This is an example of mathematical jargon (although, as noted above, "if" is more often used than "iff" in statements of definition).

The elements of "X" are "all and only" the elements of "Y" is used to mean: "for any "z" in the domain of discourse, "z" is in "X" if and only if "z" is in "Y"."




</doc>
<doc id="14923" url="https://en.wikipedia.org/wiki?curid=14923" title="IP">
IP

IP or Ip may refer to:










</doc>
<doc id="14926" url="https://en.wikipedia.org/wiki?curid=14926" title="List of Italian dishes">
List of Italian dishes

This is a list of Italian dishes and foods. Italian cuisine has developed through centuries of social and political changes, with roots as far back as the 4th century BC. Italian cuisine has its origins in Etruscan, ancient Greek, and ancient Roman cuisines.
Significant changes occurred with the discovery of the New World and the introduction of potatoes, tomatoes, bell peppers and maize, now central to the cuisine but not introduced in quantity until the 18th century. The cuisine of Italy is noted for its regional diversity, abundance of difference in taste, and is known to be one of the most popular in the world, with influences abroad.

Pizza and spaghetti, both associated with the Neapolitan traditions of cookery, are especially popular abroad, but the varying geographical conditions of the twenty regions of Italy, together with the strength of local traditions, afford a wide range of dishes.

The cuisine of Italy has many unique dishes and foods.



Rice ("riso") dishes are very common in Northern Italy, especially in the Lombardia and Veneto regions, though rice dishes are found throughout the country.






Feast of the Seven Fishes







Tuscan bread specialties


Specialties of the Norcineria (Umbrian Butcher)

Unique ham and sausage specialties





Apulian bread specialties




Most important ingredients (see also Italian Herbs and Spices):

Other common ingredients:



</doc>
<doc id="14928" url="https://en.wikipedia.org/wiki?curid=14928" title="Isaac Ambrose">
Isaac Ambrose

Isaac Ambrose (1604 – 20 January 1664) was an English Puritan divine. He graduated with a BA. from Brasenose College, Oxford, on 1624. He obtained the curacy of St Edmund’s Church, Castleton, Derbyshire, in 1627. He was one of king's four preachers in Lancashire in 1631. He was twice imprisoned by commissioners of array. He worked for establishment of Presbyterianism; successively at Leeds, Preston, and Garstang, whence he was ejected for nonconformity in 1662. He also published religious works.

Ambrose was born in 1604. He was the son of Richard Ambrose, vicar of Ormskirk, and was probably descended from the Ambroses of Lowick in Furness, a well-known Roman Catholic family. He entered Brasenose College, Oxford, in 1621, in his seventeenth year.

Having graduated B.A. in 1624 and been ordained, Ambroses received in 1627 the little cure of Castleton in Derbyshire. By the influence of William Russell, earl of Bedford, he was appointed one of the king's itinerant preachers in Lancashire, and after living for a time in Garstang, he was selected by the Lady Margaret Hoghton as vicar of Preston. He associated himself with Presbyterianism, and was on the celebrated committee for the ejection of "scandalous and ignorant ministers and schoolmasters" during the Commonwealth.

So long as Ambrose continued at Preston he was favoured with the warm friendship of the Hoghton family, their ancestral woods and the tower near Blackburn affording him sequestered places for those devout meditations and "experiences" that give such a charm to his diary, portions of which are quoted in his "Prima Media" and "Ultima" (1650, 1659). The immense auditory of his sermon ("Redeeming the Time") at the funeral of Lady Hoghton was long a living tradition all over the county. On account of the feeling engendered by the civil war Ambrose left his great church of Preston in 1654, and became minister of Garstang, whence, however, in 1662 he was ejected along two thousand ministers who refused to conform (see Great Ejection). His after years were passed among old friends and in quiet meditation at Preston. He died of apoplexy about 20 January 1664.

As a religious writer Ambrose has a vividness and freshness of imagination possessed by scarcely any of the Puritan Nonconformists. Many who have no love for Puritan doctrine, nor sympathy with Puritan experience, have appreciated the pathos and beauty of his writings, and his "Looking unto Jesus" long held its own in popular appreciation with the writings of John Bunyan.

Dr Edmund Calamy the Elder (1600–1666) wrote about him:
In the opinion of John Eglington Bailey (his biographer in the DNB), his character has been misrepresented by Wood. He was of a peaceful disposition; and though he put his name to the fierce "Harmonious Consent", he was not naturally a partisan. He evaded the political controversies of the time. His gentleness of character and earnest presentation of the gospel attached him to his people. He was much given to secluding himself, retiring every May into the woods of Hoghton Tower and remaining there a month.

Bailey continues that Dr. Halley justly characterises him as the most meditative puritan of Lancashire. This quality pervades his writings, which abound, besides, in deep feeling and earnest piety. Mr. Hunter has called attention to his recommendation of diaries as a means of advancing personal piety, and has remarked, in reference to the fragments from Ambrose's diary quoted in the "Media", that "with such passages before us we cannot but lament that the carelessness of later times should have suffered such a curious and valuable document to perish; for perished it is to be feared it has".



</doc>
<doc id="14933" url="https://en.wikipedia.org/wiki?curid=14933" title="International Convention for the Regulation of Whaling">
International Convention for the Regulation of Whaling

The International Convention for the Regulation of Whaling is an international environmental agreement signed in 1946 in order to "provide for the proper conservation of whale stocks and thus make possible the orderly development of the whaling industry". It governs the commercial, scientific, and aboriginal subsistence whaling practices of eighty-nine member nations.

It was signed by 15 nations in Washington, D.C. on 2 December 1946 and took effect on 10 November 1948. Its protocol (which represented the first substantial revision of the convention and extended the definition of a "whale-catcher" to include helicopters as well as ships) was signed in Washington on 19 November 1956. The convention is a successor to the International Agreement for the Regulation of Whaling, signed in London on 8 June 1937, and the protocols for that agreement signed in London on 24 June 1938, and 26 November 1945.

The objectives of the agreement are the protection of all whale species from overhunting, the establishment of a system of international regulation for the whale fisheries to ensure proper conservation and development of whale stocks, and safeguarding for future generations the great natural resources represented by whale stocks. The primary instrument for the realization of these aims is the International Whaling Commission which was established pursuant to this convention. The commission has made many revisions to the schedule that makes up the bulk of the convention. The Commission process has also reserved for governments the right to carry out scientific research which involves killing of whales.

There have been consistent disagreement over the scope of the convention. According to the IWC:
The 1946 Convention does not define a 'whale', although a list of names in a number of languages was annexed to the Final Act of the Convention. Some Governments take the view that the IWC has the legal competence to regulate catches of only these named great whales (the baleen whales and the sperm whale). Others believe that all cetaceans, including the smaller dolphins and porpoises, also fall within IWC jurisdiction.
As of January 2014, membership consists of 89 states of the world. The initial signatory states were Argentina, Australia, Brazil, Canada, Chile, Denmark, France, the Netherlands, New Zealand, Norway, Peru, South Africa, the Soviet Union, the United Kingdom and the United States.

Although Norway is a party to the convention, it maintains an objection to paragraph 10(e) of the convention (the section referring to the 1986 moratorium). Therefore, that provision is not binding upon Norway and the 1986 IWC global moratorium does not apply to it.

As of January 2014, eight states that were formerly parties to the convention have withdrawn by denouncing it. These states are Canada (which withdrew on 30 June 1982), Egypt, Greece, Jamaica, Mauritius, Philippines, the Seychelles and Venezuela. Belize, Brazil, Dominica, Ecuador, Iceland, Japan, New Zealand, and Panama have all withdrawn from the convention for a period of time after ratification but subsequently have ratified it a second time. The Netherlands, Norway, and Sweden have all withdrawn from the convention twice, only to have accepted it a third time.

Japan withdrew from the convention in January 2019.



</doc>
<doc id="14934" url="https://en.wikipedia.org/wiki?curid=14934" title="International Organization for Standardization">
International Organization for Standardization

The International Organization for Standardization (ISO; ) is an international standard-setting body composed of representatives from various national standards organizations. In contrast to many international organizations, which utilize the British English form of spelling, the ISO uses English with Oxford spelling as one of its official languages along with French and Russian. 

Founded on 23 February 1947, the organization promotes worldwide proprietary, industrial, and commercial standards. It is headquartered in Geneva, Switzerland, and works in 164 countries. 

It was one of the first organizations granted general consultative status with the United Nations Economic and Social Council.

The International Organization for Standardization is an independent, non-governmental organization, the members of which are the standards organizations of the 164 member countries. It is the world's largest developer of voluntary international standards and it facilitates world trade by providing common standards among nations. More than twenty thousand standards have been set, covering everything from manufactured products and technology to food safety, agriculture, and healthcare.

Use of the standards aids in the creation of products and services that are safe, reliable, and of good quality. The standards help businesses increase productivity while minimizing errors and waste. By enabling products from different markets to be directly compared, they facilitate companies in entering new markets and assist in the development of global trade on a fair basis. The standards also serve to safeguard consumers and the end-users of products and services, ensuring that certified products conform to the minimum standards set internationally.

The organization today known as ISO, began in the 1920s as the International Federation of the National Standardizing Associations (ISA). It was suspended in 1942 during World War II, but after the war ISA was approached by the recently-formed United Nations Standards Coordinating Committee (UNSCC) with a proposal to form a new global standards body. In October 1946, ISA and UNSCC delegates from 25 countries met in London and agreed to join forces to create the new International Organization for Standardization. The new organization officially began operations in February 1947.

The three official languages of the ISO are English (with Oxford spelling), French, and Russian.

The name of the organization in French is ', and in Russian, ('). "ISO" is not an acronym. ISO gives this explanation of the name: "Because 'International Organization for Standardization' would have different acronyms in different languages (IOS in English, OIN in French), our founders decided to give it the short form "ISO". "ISO" is derived from the Greek word "" (, meaning "equal"). Whatever the country, whatever the language, the short form of our name is always "ISO"." During the founding meetings of the new organization, the Greek word explanation was not invoked, so this meaning may have been made public later, making it a backronym.

Both the name "ISO" and the ISO logo are registered trademarks and their use is restricted.

ISO is a voluntary organization whose members are recognized authorities on standards, each one representing one country. Members meet annually at a General Assembly to discuss the strategic objectives of ISO. The organization is coordinated by a central secretariat based in Geneva.

A council with a rotating membership of 20 member bodies provides guidance and governance, including setting the annual budget of the central secretariat.

The technical management board is responsible for more than 250 technical committees, who develop the ISO standards.

ISO has formed two joint committees with the International Electrotechnical Commission (IEC) to develop standards and terminology in the areas of electrical and electronic related technologies.

ISO/IEC Joint Technical Committee 1 (JTC 1) was created in 1987 to "[d]evelop, maintain, promote and facilitate IT standards", where IT refers to information technology.

ISO/IEC Joint Technical Committee 2 (JTC 2) was created in 2009 for the purpose of "[s]tandardization in the field of energy efficiency and renewable energy sources".

ISO has 164 national members.

ISO has three membership categories,

Participating members are called "P" members, as opposed to observing members, who are called "O" members.

ISO is funded by a combination of:

International standards are the main products of ISO. It also publishes technical reports, technical specifications, publicly available specifications, technical corrigenda, and guides.

International standards

Technical reports

For example:

Technical and publicly available specifications

For example:

Technical corrigenda

ISO guides

For example:

ISO documents are copyrighted and ISO charges for most copies. It does not, however, charge for most draft copies of documents in electronic format. Although they are useful, care must be taken using these drafts as there is the possibility of substantial change before they become finalized as standards. Some standards by ISO and its official U.S. representative (and, via the U.S. National Committee, the International Electrotechnical Commission) are made freely available.

A standard published by ISO/IEC is the last stage of a long process that commonly starts with the proposal of new work within a committee. Some abbreviations used for marking a standard with its status are:

Abbreviations used for amendments are:

Other abbreviations are:

International Standards are developed by ISO technical committees (TC) and subcommittees (SC) by a process with six steps:

The TC/SC may set up working groups (WG) of experts for the preparation of a working drafts. Subcommittees may have several working groups, which may have several Sub Groups (SG).

It is possible to omit certain stages, if there is a document with a certain degree of maturity at the start of a standardization project, for example, a standard developed by another organization. ISO/IEC directives also allow the so-called "Fast-track procedure". In this procedure a document is submitted directly for approval as a draft International Standard (DIS) to the ISO member bodies or as a final draft International Standard (FDIS), if the document was developed by an international standardizing body recognized by the ISO Council.

The first step—a proposal of work (New Proposal) is approved at the relevant subcommittee or technical committee (e.g., SC29 and JTC1 respectively in the case of Moving Picture Experts Group – ISO/IEC JTC1/SC29/WG11). A working group (WG) of experts is set up by the TC/SC for the preparation of a working draft. When the scope of a new work is sufficiently clarified, some of the working groups (e.g., MPEG) usually make open request for proposals—known as a "call for proposals". The first document that is produced, for example, for audio and video coding standards is called a verification model (VM) (previously also called a "simulation and test model"). When a sufficient confidence in the stability of the standard under development is reached, a working draft (WD) is produced. This is in the form of a standard, but is kept internal to working group for revision. When a working draft is sufficiently solid and the working group is satisfied that it has developed the best technical solution to the problem being addressed, it becomes a committee draft (CD). If it is required, it is then sent to the P-members of the TC/SC (national bodies) for ballot.

The committee draft becomes final committee draft (FCD) if the number of positive votes exceeds the quorum. Successive committee drafts may be considered until consensus is reached on the technical content. When consensus is reached, the text is finalized for submission as a draft International Standard (DIS). Then the text is submitted to national bodies for voting and comment within a period of five months. It is approved for submission as a final draft International Standard (FDIS) if a two-thirds majority of the P-members of the TC/SC are in favour and if not more than one-quarter of the total number of votes cast are negative. ISO will then hold a ballot with National Bodies where no technical changes are allowed (yes/no ballot), within a period of two months. It is approved as an International Standard (IS) if a two-thirds majority of the P-members of the TC/SC is in favour and not more than one-quarter of the total number of votes cast are negative. After approval, only minor editorial changes are introduced into the final text. The final text is sent to the ISO central secretariat, which publishes it as the International Standard.

International Workshop Agreements (IWAs) follow a slightly different process outside the usual committee system but overseen by the ISO, allowing "key industry players to negotiate in an open workshop environment" in order to shape the IWA standard.

On occasion, the fact that many of the ISO-created standards are ubiquitous has led to common use of "ISO" to describe the product that conforms to a standard. Some examples of this are:

With the exception of a small number of isolated standards, normally ISO standards are not available free of charge, but for a purchase fee, which has been seen by some as unaffordable by small open source projects.

The ISO/IEC JTC1 fast-track procedures ("Fast-track" as used by OOXML and "PAS" as used by OpenDocument) have garnered criticism in relation to the standardization of Office Open XML (ISO/IEC 29500). Martin Bryan, outgoing convenor of ISO/IEC JTC1/SC34 WG1, is quoted as saying:

I would recommend my successor that it is perhaps time to pass WG1’s outstanding standards over to OASIS, where they can get approval in less than a year and then do a PAS submission to ISO, which will get a lot more attention and be approved much faster than standards currently can be within WG1.

The disparity of rules for PAS, Fast-Track and ISO committee generated standards is fast making ISO a laughing stock in IT circles. The days of open standards development are fast disappearing. Instead we are getting "standardization by corporation".
The computer security entrepreneur and Ubuntu founder, Mark Shuttleworth, commented on the Standardization of Office Open XML process by saying: "I think it de-values the confidence people have in the standards setting process", and alleged that ISO did not carry out its responsibility. He also noted that Microsoft had intensely lobbied many countries that traditionally had not participated in ISO and stacked technical committees with Microsoft employees, solution providers, and resellers sympathetic to Office Open XML:

When you have a process built on trust and when that trust is abused, ISO should halt the process... ISO is an engineering old boys club and these things are boring so you have to have a lot of passion … then suddenly you have an investment of a lot of money and lobbying and you get artificial results. The process is not set up to deal with intensive corporate lobbying and so you end up with something being a standard that is not clear.





</doc>
<doc id="14936" url="https://en.wikipedia.org/wiki?curid=14936" title="Individualist anarchism">
Individualist anarchism

Individualist anarchism is the branch of anarchism that emphasizes the individual and their will over external determinants such as groups, society, traditions and ideological systems. Although usually contrasted to social anarchism, both individualist and social anarchism have influenced each other. Mutualism, an economic theory particularly influential within individualist anarchism whose pursued liberty has been called the synthesis of communism and property, has been considered sometimes part of individualist anarchism and other times part of social anarchism. Many anarcho-communists regard themselves as radical individualists, seeing anarcho-communism as the best social system for the realization of individual freedom.

As a term, "individualist anarchism" is not a single philosophy, but it refers to a group of individualist philosophies that sometimes are in conflict. Among the early influences on individualist anarchism were William Godwin (philosophical anarchism), Josiah Warren (sovereignty of the individual), Max Stirner (egoism), Lysander Spooner (natural law), Pierre-Joseph Proudhon (mutualism), Henry David Thoreau (transcendentalism), Herbert Spencer (law of equal liberty) and Anselme Bellegarrigue (civil disobedience). From there, individualist anarchism expanded through Europe and the United States, where prominent 19th-century individualist anarchist Benjamin Tucker held that "if the individual has the right to govern himself, all external government is tyranny".

Individualist anarchism has been described as the anarchist school most influenced by and tied to liberalism (especially classical liberalism) as well as the liberal wing—contra the collectivist or communist wing—of anarchism and libertarian socialism. Nonetheless, the very idea of an individualist–socialist divide is contested as individualist anarchism is largely socialistic and can be considered a form of individualist socialism, with non-Lockean individualism encompassing socialism.
The term "individualist anarchism" is often used as a classificatory term, but in very different ways. Some such as the authors of "An Anarchist FAQ" use the classification individualist anarchism/social anarchism. Others such as Geoffrey Ostergaard, who see individualist anarchism as distinctly non-socialist, recognizing anarcho-capitalist as part of the individualist anarchist tradition, use the classification individualist anarchism/socialist anarchism accordingly. Other classifications include communal/mutualist anarchism. Michael Freeden identifies four broad types of individualist anarchism. Freeden says the first is the type associated with William Godwin that advocates self-government with a "progressive rationalism that included benevolence to others". The second type is the amoral self-serving rationality of egoism as most associated with Max Stirner. The third type is "found in Herbert Spencer's early predictions, and in that of some of his disciples such as Wordsworth Donisthorpe, foreseeing the redundancy of the state in the source of social evolution". The fourth type retains a moderated form of egoism and accounts for social cooperation through the advocacy of market relationships. Individualist anarchism of different kinds have the following things in common:

The egoist form of individualist anarchism, derived from the philosophy of Max Stirner, supports the individual doing exactly what he pleases—taking no notice of God, state, or moral rules. To Stirner, rights were "spooks" in the mind, and he held that society does not exist but "the individuals are its reality"—he supported property by force of might rather than moral right. Stirner advocated self-assertion and foresaw "associations of egoists" drawn together by respect for each other's ruthlessness.

Individualist anarchists such as Benjamin Tucker argued that it was "not Socialist Anarchism against Individualist Anarchism, but of Communist Socialism against Individualist Socialism". Tucker further noted that "the fact that State Socialism has overshadowed other forms of Socialism gives it no right to a monopoly of the Socialistic idea". In 1888, Tucker, who proclaimed himself to be an anarchistic socialist in opposition to state socialism, included the full text of a "Socialistic Letter" by Ernest Lesigne in his essay "State Socialism and Anarchism". According to Lesigne, there are two socialisms: "One is dictatorial, the other libertarian". Tucker's two socialisms were the state socialism which he associated to the Marxist school and the libertarian socialism that he advocated. According to Tucker, what those two schools of socialism had in common was the labor theory of value and the ends, by which anarchism pursued different means.
For historian Eunice Minette Schuster, American individualist anarchism "stresses the isolation of the individual – his right to his own tools, his mind, his body, and to the products of his labor. To the artist who embraces this philosophy it is "aesthetic" anarchism, to the reformer, ethical anarchism, to the independent mechanic, economic anarchism. The former is concerned with philosophy, the latter with practical demonstration. The economic anarchist is concerned with constructing a society on the basis of anarchism. Economically he sees no harm whatever in the private possession of what the individual produces by his own labor, but only so much and no more. The aesthetic and ethical type found expression in the transcendentalism, humanitarianism, and romanticism of the first part of the nineteenth century, the economic type in the pioneer life of the West during the same period, but more favorably after the Civil War". It is for this reason that it has been suggested that in order to understand individualist anarchism one must take into account "the social context of their ideas, namely the transformation of America from a pre-capitalist to a capitalist society [...] the non-capitalist nature of the early U.S. can be seen from the early dominance of self-employment (artisan and peasant production). At the beginning of the 19th century, around 80% of the working (non-slave) male population were self-employed. The great majority of Americans during this time were farmers working their own land, primarily for their own needs" and "[i]ndividualist anarchism is clearly a form of artisanal socialism [...] while communist anarchism and anarcho-syndicalism are forms of industrial (or proletarian) socialism".

Contemporary individualist anarchist Kevin Carson characterizes American individualist anarchism by saying that "[u]nlike the rest of the socialist movement, the individualist anarchists believed that the natural wage of labor in a free market was its product, and that economic exploitation could only take place when capitalists and landlords harnessed the power of the state in their interests. Thus, individualist anarchism was an alternative both to the increasing statism of the mainstream socialist movement, and to a classical liberal movement that was moving toward a mere apologetic for the power of big business".
In European individualist anarchism, a different social context helped the rise of European individualist illegalism and as such "[t]he illegalists were proletarians who had nothing to sell but their labour power, and nothing to discard but their dignity; if they disdained waged-work, it was because of its compulsive nature. If they turned to illegality it was due to the fact that honest toil only benefited the employers and often entailed a complete loss of dignity, while any complaints resulted in the sack; to avoid starvation through lack of work it was necessary to beg or steal, and to avoid conscription into the army many of them had to go on the run". A European tendency of individualist anarchism advocated violent individual acts of individual reclamation, propaganda by the deed and criticism of organization. Such individualist anarchist tendencies include French illegalism and Italian anti-organizational insurrectionarism. Bookchin reports that at the end of the 19th century and the beginning of the 20th "it was in times of severe social repression and deadening social quiescence that individualist anarchists came to the foreground of libertarian activity – and then primarily as terrorists. In France, Spain, and the United States, individualistic anarchists committed acts of terrorism that gave anarchism its reputation as a violently sinister conspiracy".

Another important tendency within individualist anarchist currents emphasizes individual subjective exploration and defiance of social conventions. Individualist anarchist philosophy attracted "amongst artists, intellectuals and the well-read, urban middle classes in general". Murray Bookchin describes a lot of individualist anarchism as people who "expressed their opposition in uniquely personal forms, especially in fiery tracts, outrageous behavior and aberrant lifestyles in the cultural ghettos of fin de siecle New York, Paris and London. As a credo, individualist anarchism remained largely a bohemian lifestyle, most conspicuous in its demands for sexual freedom ('free love') and enamored of innovations in art, behavior, and clothing". In this way, free love currents and other radical lifestyles such as naturism had popularity among individualist anarchists.

For Catalan historian Xavier Diez, "under its iconoclastic, antiintelectual, antitheist run, which goes against all sacralized ideas or values it entailed, a philosophy of life which could be considered a reaction against the sacred gods of capitalist society. Against the idea of nation, it opposed its internationalism. Against the exaltation of authority embodied in the military institution, it opposed its antimilitarism. Against the concept of industrial civilization, it opposed its naturist vision". In regards to economic questions, there are diverse positions. There are adherents to mutualism (Proudhon, Émile Armand and the early Tucker), egoistic disrespect for "ghosts" such as private property and markets (Stirner, John Henry Mackay, Lev Chernyi and the later Tucker) and adherents to anarcho-communism (Albert Libertad, illegalism and Renzo Novatore). Anarchist historian George Woodcock finds a tendency in individualist anarchism of a "distrust (of) all co-operation beyond the barest minimum for an ascetic life". On the issue of violence opinions have gone from a violentist point of view mainly exemplified by illegalism and insurrectionary anarchism to one that can be called anarcho-pacifist. In the particular case of Spanish individualist anarchist Miguel Gimenez Igualada, he went from illegalist practice in his youth towards a pacifist position later in his life.

William Godwin can be considered an individualist anarchist and philosophical anarchist who was influenced by the ideas of the Age of Enlightenment, and developed what many consider the first expression of modern anarchist thought. According to Peter Kropotkin, Godwin was "the first to formulate the political and economical conceptions of anarchism, even though he did not give that name to the ideas developed in his work". Godwin himself attributed the first anarchist writing to Edmund Burke's "A Vindication of Natural Society". Godwin advocated extreme individualism, proposing that all cooperation in labor be eliminated. Godwin was a utilitarian who believed that all individuals are not of equal value, with some of us "of more worth and importance" than others depending on our utility in bringing about social good. Therefore, he does not believe in equal rights, but the person's life that should be favored that is most conducive to the general good. Godwin opposed government because it infringes on the individual's right to "private judgement" to determine which actions most maximize utility, but also makes a critique of all authority over the individual's judgement. This aspect of Godwin's philosophy, minus the utilitarianism, was developed into a more extreme form later by Stirner.

Godwin took individualism to the radical extent of opposing individuals performing together in orchestras, writing in "Political Justice" that "everything understood by the term co-operation is in some sense an evil". The only apparent exception to this opposition to cooperation is the spontaneous association that may arise when a society is threatened by violent force. One reason he opposed cooperation is he believed it to interfere with an individual's ability to be benevolent for the greater good. Godwin opposes the idea of government, but wrote that a minimal state as a present "necessary evil" that would become increasingly irrelevant and powerless by the gradual spread of knowledge. He expressly opposed democracy, fearing oppression of the individual by the majority, though he believed it to be preferable to dictatorship.
Godwin supported individual ownership of property, defining it as "the empire to which every man is entitled over the produce of his own industry". However, he also advocated that individuals give to each other their surplus property on the occasion that others have a need for it, without involving trade (e.g. gift economy). Thus while people have the right to private property, they should give it away as enlightened altruists. This was to be based on utilitarian principles and he said: "Every man has a right to that, the exclusive possession of which being awarded to him, a greater sum of benefit or pleasure will result than could have arisen from its being otherwise appropriated". However, benevolence was not to be enforced, being a matter of free individual "private judgement". He did not advocate a community of goods or assert collective ownership as is embraced in communism, but his belief that individuals ought to share with those in need was influential on the later development of anarcho-communism.

Godwin's political views were diverse and do not perfectly agree with any of the ideologies that claim his influence as writers of the "Socialist Standard", organ of the Socialist Party of Great Britain, consider Godwin both an individualist and a communist; Murray Rothbard did not regard Godwin as being in the individualist camp at all, referring to him as the "founder of communist anarchism";<ref name="rothbard/burke">Rothbard, Murray. "Edmund Burke, Anarchist."</ref> and historian Albert Weisbord considers him an individualist anarchist without reservation. Some writers see a conflict between Godwin's advocacy of "private judgement" and utilitarianism as he says that ethics requires that individuals give their surplus property to each other resulting in an egalitarian society, but at the same time he insists that all things be left to individual choice. As noted by Kropotkin, many of Godwin's views changed over time.

William Godwin's influenced "the socialism of Robert Owen and Charles Fourier. After success of his British venture, Owen himself established a cooperative community within the United States at New Harmony, Indiana during 1825. One member of this commune was Josiah Warren (1798–1874), considered to be the first individualist anarchist. After New Harmony failed Warren shifted his ideological loyalties from socialism to anarchism (which was no great leap, given that Owen's socialism had been predicated on Godwin's anarchism)".

Pierre-Joseph Proudhon (1809–1865) was the first philosopher to label himself an "anarchist". Some consider Proudhon to be an individualist anarchist while others regard him to be a social anarchist. Some commentators do not identify Proudhon as an individualist anarchist due to his preference for association in large industries, rather than individual control. Nevertheless, he was influential among some of the American individualists—in the 1840s and 1850s, Charles A. Dana and William B. Greene introduced Proudhon's works to the United States. Greene adapted Proudhon's mutualism to American conditions and introduced it to Benjamin Tucker.

Proudhon opposed government privilege that protects capitalist, banking and land interests and the accumulation or acquisition of property (and any form of coercion that led to it) which he believed hampers competition and keeps wealth in the hands of the few. Proudhon favoured a right of individuals to retain the product of their labour as their own property, but he believed that any property beyond that which an individual produced and could possess was illegitimate. Thus he saw private property as both essential to liberty and a road to tyranny, the former when it resulted from labour and was required for labour and the latter when it resulted in exploitation (profit, interest, rent and tax). He generally called the former "possession" and the latter "property". For large-scale industry, he supported workers associations to replace wage labour and opposed the ownership of land.

Proudhon maintained that those who labour should retain the entirety of what they produce and that monopolies on credit and land are the forces that prohibit such. He advocated an economic system that included private property as possession and exchange market, but without profit, which he called mutualism. It is Proudhon's philosophy that was explicitly rejected by Joseph Dejacque in the inception of anarcho-communism, with the latter asserting directly to Proudhon in a letter that "it is not the product of his or her labour that the worker has a right to, but to the satisfaction of his or her needs, whatever may be their nature". An individualist rather than anarcho-communist, Proudhon said that "communism [...] is the very denial of society in its foundation" and famously declared that "property is theft" in reference to his rejection of ownership rights to land being granted to a person who is not using that land.

After Dejacque and others split from Proudhon due to the latter's support of individual property and an exchange economy, the relationship between the individualists (who continued in relative alignment with the philosophy of Proudhon) and the anarcho-communists was characterised by various degrees of antagonism and harmony. For example, individualists like Tucker on the one hand translated and reprinted the works of collectivists like Mikhail Bakunin while on the other hand rejected the economic aspects of collectivism and communism as incompatible with anarchist ideals.

Mutualism is an anarchist school of thought which can be traced to the writings of Pierre-Joseph Proudhon, who envisioned a society where each person might possess a means of production, either individually or collectively, with trade representing equivalent amounts of labor in the free market. Integral to the scheme was the establishment of a mutual-credit bank which would lend to producers at a minimal interest rate only high enough to cover the costs of administration. Mutualism is based on a labor theory of value which holds that when labour or its product is sold, in exchange it ought to receive goods or services embodying "the amount of labor necessary to produce an article of exactly similar and equal utility". Some mutualists believe that if the state did not intervene, as a result of increased competition in the marketplace, individuals would receive no more income than that in proportion to the amount of labor they exert. Mutualists oppose the idea of individuals receiving an income through loans, investments and rent as they believe these individuals are not labouring. Some of them argue that if state intervention ceased, these types of incomes would disappear due to increased competition in capital. Although Proudhon opposed this type of income, he expressed that he "never meant to [...] forbid or suppress, by sovereign decree, ground rent and interest on capital. I believe that all these forms of human activity should remain free and optional for all".
Insofar as they ensure the workers right to the full product of their labor, mutualists support markets and private property in the product of labor. However, they argue for conditional titles to land, whose private ownership is legitimate only so long as it remains in use or occupation (which Proudhon called "possession"). Proudhon's Mutualism supports labor-owned cooperative firms and associations for "we need not hesitate, for we have no choice [...] it is necessary to form an ASSOCIATION among workers [...] because without that, they would remain related as subordinates and superiors, and there would ensue two [...] castes of masters and wage-workers, which is repugnant to a free and democratic society" and so "it becomes necessary for the workers to form themselves into democratic societies, with equal conditions for all members, on pain of a relapse into feudalism". As for capital goods (man-made, non-land, means of production), mutualist opinion differs on whether these should be commonly managed public assets or private property.

Following Proudhon, mutualists originally considered themselves to be libertarian socialists. However, "some mutualists have abandoned the labor theory of value, and prefer to avoid the term 'socialist.' But they still retain some cultural attitudes, for the most part, that set them off from the libertarian right". Mutualists have distinguished themselves from state socialism and do not advocate social control over the means of production. Benjamin Tucker said of Proudhon that "though opposed to socializing the ownership of capital, Proudhon aimed nevertheless to socialize its effects by making its use beneficial to all instead of a means of impoverishing the many to enrich the few [...] by subjecting capital to the natural law of competition, thus bringing the price of its own use down to cost".

Johann Kaspar Schmidt (October 25, 1806 – June 26, 1856), better known as Max Stirner (the "nom de plume" he adopted from a schoolyard nickname he had acquired as a child because of his high brow, in German "Stirn"), was a German philosopher, who ranks as one of the literary fathers of nihilism, existentialism, post-modernism and anarchism, especially of individualist anarchism. Stirner's main work is "The Ego and Its Own", also known as "The Ego and His Own" ("Der Einzige und sein Eigentum" in German which translates literally as "The Only One [individual] and his Property" or "The Unique Individual and His Property"). This work was first published in 1844 in Leipzig and has since appeared in numerous editions and translations.

Max Stirner's philosophy, sometimes called egoism, is a form of individualist anarchism. Stirner was a Hegelian philosopher whose "name appears with familiar regularity in historically oriented surveys of anarchist thought as one of the earliest and best-known exponents of individualist anarchism". In 1844, Stirner's work "The Ego and Its Own" was published and is considered to be "a founding text in the tradition of individualist anarchism". Stirner does not recommend that the individual try to eliminate the state, but simply that they disregard the state when it conflicts with one's autonomous choices and go along with it when doing so is conducive to one's interests. Stirner says that the egoist rejects pursuit of devotion to "a great idea, a good cause, a doctrine, a system, a lofty calling", arguing that the egoist has no political calling, but rather "lives themselves out" without regard to "how well or ill humanity may fare thereby". Stirner held that the only limitation on the rights of the individual is that individual's power to obtain what he desires. Stirner proposes that most commonly accepted social institutions, including the notion of state, property as a right, natural rights in general and the very notion of "society" as a legal and ideal abstractness, were mere spooks in the mind. Stirner wants to "abolish not only the state but also society as an institution responsible for its members". Stirner advocated self-assertion and foresaw Union of egoists, non-systematic associations which he proposed in as a form of organization in place of the state. A Union is understood as a relation between egoists which is continually renewed by all parties' support through an act of will. Even murder is permissible "if it is right for me", although it is claimed by egoist anarchists that egoism will foster genuine and spontaneous unions between individuals.
For Stirner, property simply comes about through might, arguing that "[w]hoever knows how to take, to defend, the thing, to him belongs property". He further says that "[w]hat I have in my power, that is my own. So long as I assert myself as holder, I am the proprietor of the thing" and that "I do not step shyly back from your property, but look upon it always as my property, in which I respect nothing. Pray do the like with what you call my property!" His concept of "egoistic property" not only a lack of moral restraint on how one obtains and uses "things", but includes other people as well. His embrace of egotism is in stark contrast to Godwin's altruism. Although Stirner was opposed to communism, for the same reasons he opposed capitalism, humanism, liberalism, property rights and nationalism, seeing them as forms of authority over the individual and as spooks in the mind, he has influenced many anarcho-communists and post-left anarchists. The writers of "An Anarchist FAQ" report that "many in the anarchist movement in Glasgow, Scotland, took Stirner's 'Union of egoists' literally as the basis for their anarcho-syndicalist organising in the 1940s and beyond". Similarly, the noted anarchist historian Max Nettlau states that "[o]n reading Stirner, I maintain that he cannot be interpreted except in a socialist sense". Stirner does not personally oppose the struggles carried out by certain ideologies such as socialism, humanism or the advocacy of human rights. Rather, he opposes their legal and ideal abstractness, a fact that makes him different from the liberal individualists, including the anarcho-capitalists and right-libertarians, but also from the "Übermensch" theories of fascism as he places the individual at the center and not the sacred collective. About socialism, Stirner wrote in a letter to Moses Hess that "I am not at all against socialism, but against consecrated socialism; my selfishness is not opposed to love [...] nor is it an enemy of sacrifice, nor of self-denial [...] and least of all of socialism [...] — in short, it is not an enemy of true interests; it rebels not against love, but against sacred love, not against thought, but against sacred thought, not against socialists, but against sacred socialism".

This position on property is much different from the Native American, natural law, form of individualist anarchism which defends the inviolability of the private property that has been earned through labor. However, Benjamin Tucker rejected the natural rights philosophy and adopted Stirner's egoism in 1886, with several others joining with him. This split the American individualists into fierce debate, "with the natural rights proponents accusing the egoists of destroying libertarianism itself". Other egoists include James L. Walker, Sidney Parker, Dora Marsden and John Beverly Robinson. In Russia, individualist anarchism inspired by Stirner combined with an appreciation for Friedrich Nietzsche attracted a small following of bohemian artists and intellectuals such as Lev Chernyi as well as a few lone wolves who found self-expression in crime and violence. They rejected organizing, believing that only unorganized individuals were safe from coercion and domination, believing this kept them true to the ideals of anarchism. This type of individualist anarchism inspired anarcha-feminist Emma Goldman.

Although Stirner's philosophy is individualist, it has influenced some libertarian communists and anarcho-communists. "For Ourselves Council for Generalized Self-Management" discusses Stirner and speaks of a "communist egoism" which is said to be a "synthesis of individualism and collectivism" and says that "greed in its fullest sense is the only possible basis of communist society". Forms of libertarian communism such as Situationism are influenced by Stirner. Anarcho-communist Emma Goldman was influenced by both Stirner and Peter Kropotkin and blended their philosophies together in her own as shown in books of hers such as "Anarchism And Other Essays".

Josiah Warren is widely regarded as the first American anarchist and the four-page weekly paper he edited during 1833, "The Peaceful Revolutionist", was the first anarchist periodical published, an enterprise for which he built his own printing press, cast his own type and made his own printing plates. Warren was a follower of Robert Owen and joined Owen's community at New Harmony, Indiana. Warren termed the phrase "Cost the limit of price", with "cost" here referring not to monetary price paid but the labor one exerted to produce an item. Therefore, "[h]e proposed a system to pay people with certificates indicating how many hours of work they did. They could exchange the notes at local time stores for goods that took the same amount of time to produce". He put his theories to the test by establishing an experimental "labor for labor store" called the Cincinnati Time Store where trade was facilitated by notes backed by a promise to perform labor. The store proved successful and operated for three years after which it was closed so that Warren could pursue establishing colonies based on mutualism. These included Utopia and Modern Times. Warren said that Stephen Pearl Andrews' "The Science of Society" (published in 1852) was the most lucid and complete exposition of Warren's own theories. Catalan historian Xavier Diez report that the intentional communal experiments pioneered by Warren were influential in European individualist anarchists of the late 19th and early 20th centuries such as Émile Armand and the intentional communities started by them.

Henry David Thoreau (1817–1862) was an important early influence in individualist anarchist thought in the United States and Europe. Thoreau was an American author, poet, naturalist, tax resister, development critic, surveyor, historian, philosopher and leading transcendentalist. He is best known for his book "Walden", a reflection upon simple living in natural surroundings; and his essay, "Civil Disobedience", an argument for individual resistance to civil government in moral opposition to an unjust state. His thought is an early influence on green anarchism, but with an emphasis on the individual experience of the natural world influencing later naturist currents, simple living as a rejection of a materialist lifestyle and self-sufficiency were Thoreau's goals and the whole project was inspired by transcendentalist philosophy. Many have seen in Thoreau one of the precursors of ecologism and anarcho-primitivism represented today in John Zerzan. For George Woodcock, this attitude can be also motivated by certain idea of resistance to progress and of rejection of the growing materialism which is the nature of American society in the mid 19th century.

The essay Civil Disobedience ("Resistance to Civil Government") was first published in 1849. It argues that people should not permit governments to overrule or atrophy their consciences and that people have a duty to avoid allowing such acquiescence to enable the government to make them the agents of injustice. Thoreau was motivated in part by his disgust with slavery and the Mexican–American War. The essay later influenced Mohandas Gandhi, Martin Luther King Jr., Martin Buber and Leo Tolstoy through its advocacy of nonviolent resistance. It is also the main precedent for anarcho-pacifism. The American version of individualist anarchism has a strong emphasis on the non-aggression principle and individual sovereignty. Some individualist anarchists such as Thoreau do not speak of economics, but simply of the right of "disunion" from the state and foresee the gradual elimination of the state through social evolution.

An important current within individualist anarchism is free love. Free love advocates sometimes traced their roots back to Josiah Warren and to experimental communities, and viewed sexual freedom as a clear, direct expression of an individual's self-ownership. Free love particularly stressed women's rights since most sexual laws, such as those governing marriage and use of birth control, discriminated against women. The most important American free love journal was "Lucifer the Lightbearer" (1883–1907) edited by Moses Harman and Lois Waisbrooker but also there existed Ezra Heywood and Angela Heywood's "The Word" (1872–1890, 1892–1893). M. E. Lazarus was also an important American individualist anarchist who promoted free love. John William Lloyd, a collaborator of Benjamin Tucker's periodical "Liberty", published in 1931 a sex manual that he called "The Karezza Method or Magnetation: The Art of Connubial Love".

In Europe, the main propagandist of free love within individualist anarchism was Émile Armand. He proposed the concept of "la camaraderie amoureuse" to speak of free love as the possibility of voluntary sexual encounter between consenting adults. He was also a consistent proponent of polyamory. In France, there was also feminist activity inside individualist anarchism as promoted by individualist feminists Marie Küge, Anna Mahé, Rirette Maitrejean and Sophia Zaïkovska.

The Brazilian individualist anarchist Maria Lacerda de Moura lectured on topics such as education, women's rights, free love and antimilitarism. Her writings and essays garnered her attention not only in Brazil, but also in Argentina and Uruguay. She also wrote for the Spanish individualist anarchist magazine "Al Margen" alongside Miguel Gimenez Igualada. In Germany, the Stirnerists Adolf Brand and John Henry Mackay were pioneering campaigners for the acceptance of male bisexuality and homosexuality.

Freethought as a philosophical position and as activism was important in both North American and European individualist anarchism, but in the United States freethought was basically an anti-Christian, anti-clerical movement whose purpose was to make the individual politically and spiritually free to decide for himself on religious matters. A number of contributors to "Liberty" were prominent figures in both freethought and anarchism. The individualist anarchist George MacDonald was a co-editor of "Freethought" and for a time "The Truth Seeker". E.C. Walker was co-editor of "Lucifer, the Light-Bearer". Many of the anarchists were ardent freethinkers; reprints from freethought papers such as "Lucifer, the Light-Bearer", "Freethought" and "The Truth Seeker" appeared in "Liberty". The church was viewed as a common ally of the state and as a repressive force in and of itself.

In Europe, a similar development occurred in French and Spanish individualist anarchist circles: "Anticlericalism, just as in the rest of the libertarian movement, is another of the frequent elements which will gain relevance related to the measure in which the (French) Republic begins to have conflicts with the church [...] Anti-clerical discourse, frequently called for by the french individualist André Lorulot, will have its impacts in "Estudios" (a Spanish individualist anarchist publication). There will be an attack on institutionalized religion for the responsibility that it had in the past on negative developments, for its irrationality which makes it a counterpoint of philosophical and scientific progress. There will be a criticism of proselitism and ideological manipulation which happens on both believers and agnostics". This tendencies will continue in French individualist anarchism in the work and activism of Charles-Auguste Bontemps and others. In the Spanish individualist anarchist magazine "Ética" and "Iniciales", "there is a strong interest in publishing scientific news, usually linked to a certain atheist and anti-theist obsession, philosophy which will also work for pointing out the incompatibility between science and religion, faith and reason. In this way there will be a lot of talk on Darwin's theories or on the negation of the existence of the soul".

Another important current, especially within French and Spanish individualist anarchist groups was naturism. Naturism promoted an ecological worldview, small ecovillages and most prominently nudism as a way to avoid the artificiality of the industrial mass society of modernity. Naturist individualist anarchists saw the individual in his biological, physical and psychological aspects and avoided and tried to eliminate social determinations. An early influence in this vein was Henry David Thoreau and his famous book "Walden". Important promoters of this were Henri Zisly and Émile Gravelle who collaborated in "La Nouvelle Humanité" followed by "Le Naturien", "Le Sauvage", "L'Ordre Naturel" and "La Vie Naturelle".

This relationship between anarchism and naturism was quite important at the end of the 1920s in Spain, when "[t]he linking role played by the 'Sol y Vida' group was very important. The goal of this group was to take trips and enjoy the open air. The Naturist athenaeum, 'Ecléctico', in Barcelona, was the base from which the activities of the group were launched. First "Etica" and then "Iniciales", which began in 1929, were the publications of the group, which lasted until the Spanish Civil War. We must be aware that the naturist ideas expressed in them matched the desires that the libertarian youth had of breaking up with the conventions of the bourgeoisie of the time. That is what a young worker explained in a letter to 'Iniciales' He writes it under the odd pseudonym of 'silvestre del campo', (wild man in the country). "I find great pleasure in being naked in the woods, bathed in light and air, two natural elements we cannot do without. By shunning the humble garment of an exploited person, (garments which, in my opinion, are the result of all the laws devised to make our lives bitter), we feel there no others left but just the natural laws. Clothes mean slavery for some and tyranny for others. Only the naked man who rebels against all norms, stands for anarchism, devoid of the prejudices of outfit imposed by our money-oriented society". The relation between anarchism and naturism "gives way to the Naturist Federation, in July 1928, and to the lV Spanish Naturist Congress, in September 1929, both supported by the Libertarian Movement. However, in the short term, the Naturist and Libertarian movements grew apart in their conceptions of everyday life. The Naturist movement felt closer to the Libertarian individualism of some French theoreticians such as Henri Ner (real name of Han Ryner) than to the revolutionary goals proposed by some Anarchist organisations such as the FAI, (Federación Anarquista Ibérica)".

The thought of German philosopher Friedrich Nietzsche has been influential in individualist anarchism, specifically in thinkers such as France's Émile Armand, the Italian Renzo Novatore and the Colombian Biofilo Panclasta. Robert C. Holub, author of "Nietzsche: Socialist, Anarchist, Feminist" posits that "translations of Nietzsche's writings in the United States very likely appeared first in "Liberty", the anarchist journal edited by Benjamin Tucker".

For American anarchist historian Eunice Minette Schuster, "[i]t is apparent [...] that Proudhonian Anarchism was to be found in the United States at least as early as 1848 and that it was not conscious of its affinity to the Individualist Anarchism of Josiah Warren and Stephen Pearl Andrews [...] William B. Greene presented this Proudhonian Mutualism in its purest and most systematic form". William Batchelder Greene (1819–1878) is best known for the works "Mutual Banking" (1850), which proposed an interest-free banking system; and "Transcendentalism", a critique of the New England philosophical school. He saw mutualism as the synthesis of "liberty and order". His "associationism [...] is checked by individualism. [...] 'Mind your own business,' 'Judge not that ye be not judged.' Over matters which are purely personal, as for example, moral conduct, the individual is sovereign, as well as over that which he himself produces. For this reason he demands 'mutuality' in marriage – the equal right of a woman to her own personal freedom and property and feminist and spiritualist tendencies". Within some individualist anarchist circles, "mutualism" came to mean non-communist anarchism.

Contemporary American anarchist Hakim Bey reports that "Steven Pearl Andrews [...] was not a fourierist (see Charles Fourier), but he lived through the brief craze for phalansteries in America & adopted a lot of fourierist principles & practices, [...] a maker of worlds out of words. He syncretized Abolitionism, Free Love, spiritual universalism, (Josiah) Warren, & (Charles) Fourier into a grand utopian scheme he called the Universal Pantarchy. [...] He was instrumental in founding several 'intentional communities,' including the 'Brownstone Utopia' on 14th St. in New York, & 'Modern Times' in Brentwood, Long Island. The latter became as famous as the best-known fourierist communes (Brook Farm in Massachusetts & the North American Phalanx in New Jersey) – in fact, Modern Times became downright notorious (for 'Free Love') & finally foundered under a wave of scandalous publicity. Andrews (& Victoria Woodhull) were members of the infamous Section 12 of the 1st International, expelled by Marx for its anarchist, feminist, & spiritualist tendencies".

Another form of individualist anarchism was found in the United States as advocated by the so-called Boston anarchists. By default, American individualists had no difficulty accepting the concepts that "one man employ another" or that "he direct him", in his labor but rather demanded that "all natural opportunities requisite to the production of wealth be accessible to all on equal terms and that monopolies arising from special privileges created by law be abolished".

They believed state monopoly capitalism (defined as a state-sponsored monopoly) prevented labor from being fully rewarded. Voltairine de Cleyre summed up the philosophy by saying that the anarchist individualists "are firm in the idea that the system of employer and employed, buying and selling, banking, and all the other essential institutions of Commercialism, centred upon private property, are in themselves good, and are rendered vicious merely by the interference of the State".

Even among the 19th-century American individualists, there was not a monolithic doctrine as they disagreed amongst each other on various issues including intellectual property rights and possession versus property in land. A major schism occurred later in the 19th century when Tucker and some others abandoned their traditional support of natural rights as espoused by Lysander Spooner and converted to an "egoism" modeled upon Max Stirner's philosophy. Lysander Spooner besides his individualist anarchist activism was also an important anti-slavery activist and became a member of the First International.

Some Boston anarchists, including Benjamin Tucker, identified themselves as socialists, which in the 19th century was often used in the sense of a commitment to improving conditions of the working class (i.e. "the labor problem"). The Boston anarchists such as Tucker and his followers continue to be considered socialists due to their opposition to usury. They do so because as the modern economist Jim Stanford points out there are many different kinds of competitive markets such as market socialism and capitalism is only one type of a market economy. By around the start of the 20th century, the heyday of individualist anarchism had passed.

George Woodcock reports that the American individualist anarchists Lysander Spooner and William B. Greene had been members of the socialist First International.

Two individualist anarchists who wrote in Benjamin Tucker's "Liberty" were also important labor organizers of the time. Joseph Labadie (April 18, 1850 – October 7, 1933) was an American labor organizer, individualist anarchist, social activist, printer, publisher, essayist and poet. In 1883, Labadie embraced a non-violent version of individualist anarchism. Without the oppression of the state, Labadie believed, humans would choose to harmonize with "the great natural laws [...] without robbing [their] fellows through interest, profit, rent and taxes". However, he supported community cooperation as he supported community control of water utilities, streets and railroads. Although he did not support the militant anarchism of the Haymarket anarchists, he fought for the clemency of the accused because he did not believe they were the perpetrators. In 1888, Labadie organized the Michigan Federation of Labor, became its first president and forged an alliance with Samuel Gompers. A colleague of Labadie's at "Liberty", Dyer Lum was another important individualist anarchist labor activist and poet of the era. A leading anarcho-syndicalist and a prominent left-wing intellectual of the 1880s, he is remembered as the lover and mentor of early anarcha-feminist Voltairine de Cleyre.

Lum was a prolific writer who wrote a number of key anarchist texts and contributed to publications including "Mother Earth", "Twentieth Century", "The Alarm" (the journal of the International Working People's Association) and "The Open Court" among others. Lum's political philosophy was a fusion of individualist anarchist economics—"a radicalized form of "laissez-faire" economics" inspired by the Boston anarchists—with radical labor organization similar to that of the Chicago anarchists of the time. Herbert Spencer and Pierre-Joseph Proudhon influenced Lum strongly in his individualist tendency. He developed a "mutualist" theory of unions and as such was active within the Knights of Labor and later promoted anti-political strategies in the American Federation of Labor. Frustration with abolitionism, spiritualism and labor reform caused Lum to embrace anarchism and radicalize workers. Convinced of the necessity of violence to enact social change he volunteered to fight in the American Civil War, hoping thereby to bring about the end of slavery. Kevin Carson has praised Lum's fusion of individualist "laissez-faire" economics with radical labor activism as "creative" and described him as "more significant than any in the Boston group".

Some of the American individualist anarchists later in this era such as Benjamin Tucker abandoned natural rights positions and converted to Max Stirner's egoist anarchism. Rejecting the idea of moral rights, Tucker said that there were only two rights, "the right of might" and "the right of contract". He also said after converting to Egoist individualism that "[i]n times past [...] it was my habit to talk glibly of the right of man to land. It was a bad habit, and I long ago sloughed it off [...] Man's only right to land is his might over it". In adopting Stirnerite egoism in 1886, Tucker rejected natural rights which had long been considered the foundation of libertarianism in the United States. This rejection galvanized the movement into fierce debates, with the natural rights proponents accusing the egoists of destroying libertarianism itself. So bitter was the conflict that a number of natural rights proponents withdrew from the pages of "Liberty" in protest even though they had hitherto been among its frequent contributors. Thereafter, "Liberty" championed egoism although its general content did not change significantly.

Several periodicals were undoubtedly influenced by "Liberty"'s presentation of egoism. They included "I" published by Clarence Lee Swartz, edited by William Walstein Gordak and J. William Lloyd (all associates of "Liberty"); and "The Ego" and "The Egoist", both of which were edited by Edward H. Fulton. Among the egoist papers that Tucker followed were the German "Der Eigene", edited by Adolf Brand; and "The Eagle" and "The Serpent", issued from London. The latter, the most prominent English-language egoist journal, was published from 1898 to 1900 with the subtitle "A Journal of Egoistic Philosophy and Sociology".

American anarchists who adhered to egoism include Benjamin Tucker, John Beverley Robinson, Steven T. Byington, Hutchins Hapgood, James L. Walker, Victor Yarros and Edward H. Fulton. Robinson wrote an essay called "Egoism" in which he states that "[m]odern egoism, as propounded by Stirner and Nietzsche, and expounded by Ibsen, Shaw and others, is all these; but it is more. It is the realization by the individual that they are an individual; that, as far as they are concerned, they are the only individual". Walker published the work "The Philosophy of Egoism" in which he argued that egosim "implies a rethinking of the self-other relationship, nothing less than 'a complete revolution in the relations of mankind' that avoids both the 'archist' principle that legitimates domination and the 'moralist' notion that elevates self-renunciation to a virtue. Walker describes himself as an 'egoistic anarchist' who believed in both contract and cooperation as practical principles to guide everyday interactions". For Walker, "what really defines egoism is not mere self-interest, pleasure, or greed; it is the sovereignty of the individual, the full expression of the subjectivity of the individual ego".

Italian anti-organizationalist individualist anarchism was brought to the United States by Italian born individualists such as Giuseppe Ciancabilla and others who advocated for violent propaganda by the deed there. Anarchist historian George Woodcock reports the incident in which the important Italian social anarchist Errico Malatesta became involved "in a dispute with the individualist anarchists of Paterson, who insisted that anarchism implied no organization at all, and that every man must act solely on his impulses. At last, in one noisy debate, the individual impulse of a certain Ciancabilla directed him to shoot Malatesta, who was badly wounded but obstinately refused to name his assailant".

Enrico Arrigoni (pseudonym Frank Brand) was an Italian American individualist anarchist Lathe operator, house painter, bricklayer, dramatist and political activist influenced by the work of Max Stirner. He took the pseudonym Brand from a fictional character in one of Henrik Ibsen's plays. In the 1910s, he started becoming involved in anarchist and anti-war activism around Milan. From the 1910s until the 1920s, he participated in anarchist activities and popular uprisings in various countries including Switzerland, Germany, Hungary, Argentina and Cuba. He lived from the 1920s onwards in New York City, where he edited the individualist anarchist eclectic journal "Eresia" in 1928. He also wrote for other American anarchist publications such as "L' Adunata dei refrattari", "Cultura Obrera", "Controcorrente" and "Intesa Libertaria". During the Spanish Civil War, he went to fight with the anarchists, but he was imprisoned and was helped on his release by Emma Goldman. Afterwards, Arrigoni became a longtime member of the Libertarian Book Club in New York City. His written works include "The Totalitarian Nightmare" (1975), "The Lunacy of the Superman" (1977), "Adventures in the Country of the Monoliths" (1981) and "Freedom: My Dream" (1986).

Murray Bookchin has identified post-left anarchy as a form of individualist anarchism in "" where he identifies "a shift among Euro-American anarchists away from social anarchism and toward individualist or lifestyle anarchism. Indeed, lifestyle anarchism today is finding its principal expression in spray-can graffiti, post-modernist nihilism, antirationalism, neo-primitivism, anti-technologism, neo-Situationist 'cultural terrorism', mysticism, and a 'practice' of staging Foucauldian 'personal insurrections'". Post-left anarchist Bob Black in his long critique of Bookchin's philosophy called "Anarchy After Leftism" said about post-left anarchy that "[i]t is, unlike Bookchinism, "individualistic" in the sense that if the freedom and happiness of the individual – i.e., each and every really existing person, every Tom, Dick and Murray – is not the measure of the good society, what is?"

A strong relationship does exist between post-left anarchism and the work of individualist anarchist Max Stirner. Jason McQuinn says that "when I (and other anti-ideological anarchists) criticize ideology, it is always from a specifically critical, anarchist perspective rooted in both the skeptical, individualist-anarchist philosophy of Max Stirner. Bob Black and Feral Faun/Wolfi Landstreicher also strongly adhere to stirnerist egoist anarchism. Bob Black has humorously suggested the idea of "marxist stirnerism".

Hakim Bey has said that "[f]rom Stirner's 'Union of Self-Owning Ones' we proceed to Nietzsche's circle of 'Free Spirits' and thence to Charles Fourier's 'Passional Series', doubling and redoubling ourselves even as the Other multiplies itself in the eros of the group". Bey also wrote that "[t]he Mackay Society, of which Mark & I are active members, is devoted to the anarchism of Max Stirner, Benj. Tucker & John Henry Mackay [...] The Mackay Society, incidentally, represents a little-known current of individualist thought which never cut its ties with revolutionary labor. Dyer Lum, Ezra & Angela Haywood represent this school of thought; Jo Labadie, who wrote for Tucker's "Liberty", made himself a link between the American 'plumb-line' anarchists, the 'philosophical' individualists, & the syndicalist or communist branch of the movement; his influence reached the Mackay Society through his son, Laurance. Like the Italian Stirnerites (who influenced us through our late friend Enrico Arrigoni) we support all anti-authoritarian currents, despite their apparent contradictions".

As far as posterior individualist anarchists, Jason McQuinn for some time used the pseudonym Lev Chernyi in honor of the Russian individualist anarchist of the same name while Feral Faun has quoted Italian individualist anarchist Renzo Novatore and has translated both Novatore and the young Italian individualist anarchist Bruno Filippi

Egoism has had a strong influence on insurrectionary anarchism as can be seen in the work of Wolfi Landstreicher. Feral Faun wrote in 1995:
In the game of insurgence – a lived guerilla war game – it is strategically necessary to use identities and roles. Unfortunately, the context of social relationships gives these roles and identities the power to define the individual who attempts to use them. So I, Feral Faun, became [...] an anarchist [...] a writer [...] a Stirner-influenced, post-situationist, anti-civilization theorist [...] if not in my own eyes, at least in the eyes of most people who've read my writings.

European individualist anarchism proceeded from the roots laid by William Godwin, Pierre-Joseph Proudhon and Max Stirner. Proudhon was an early pioneer of anarchism as well as of the important individualist anarchist current of mutualism. Stirner became a central figure of individualist anarchism through the publication of his seminal work "The Ego and Its Own" which is considered to be "a founding text in the tradition of individualist anarchism". Another early figure was Anselme Bellegarrigue. Individualist anarchism expanded and diversified through Europe, incorporating influences from North American individualist anarchism.

European individualist anarchists include Albert Libertad, Bellegarrigue, Oscar Wilde, Émile Armand, Lev Chernyi, John Henry Mackay, Han Ryner, Adolf Brand, Miguel Gimenez Igualada, Renzo Novatore and currently Michel Onfray. Important currents within it include free love, anarcho-naturism and illegalism.

From the legacy of Proudhon and Stirner there emerged a strong tradition of French individualist anarchism. An early important individualist anarchist was Anselme Bellegarrigue. He participated in the French Revolution of 1848, was author and editor of "Anarchie, Journal de l'Ordre and Au fait ! Au fait ! Interprétation de l'idée démocratique" and wrote the important early Anarchist Manifesto in 1850. Catalan historian of individualist anarchism Xavier Diez reports that during his travels in the United States "he at least contacted (Henry David) Thoreau and, probably (Josiah) Warren". "Autonomie Individuelle" was an individualist anarchist publication that ran from 1887 to 1888. It was edited by Jean-Baptiste Louiche, Charles Schæffer and Georges Deherme.

Later, this tradition continued with such intellectuals as Albert Libertad, André Lorulot, Émile Armand, Victor Serge, Zo d'Axa and Rirette Maitrejean, who in 1905 developed theory in the main individualist anarchist journal in France, "L'Anarchie". Outside this journal, Han Ryner wrote "Petit Manuel individualiste" (1903). In 1891, Zo d'Axa created the journal L'En-Dehors.
Anarcho-naturism was promoted by Henri Zisly, Emile Gravelle and Georges Butaud. Butaud was an individualist "partisan of the "milieux libres", publisher of 'Flambeau' ('an enemy of authority') in 1901 in Vienna" and most of his energies were devoted to creating anarchist colonies (communautés expérimentales) in which he participated in several.

In this sense, "the theoretical positions and the vital experiences of [F]rench individualism are deeply iconoclastic and scandalous, even within libertarian circles. The call of nudist naturism, the strong defence of birth control methods, the idea of "unions of egoists" with the sole justification of sexual practices, that will try to put in practice, not without difficulties, will establish a way of thought and action, and will result in sympathy within some, and a strong rejection within others".
French individualist anarchists grouped behind Émile Armand, published "L'Unique" after World War II. "L'Unique" went from 1945 to 1956 with a total of 110 numbers. Gérard de Lacaze-Duthiers was a French writer, art critic, pacifist and anarchist. Lacaze-Duthiers, an art critic for the Symbolist review journal "La Plume", was influenced by Oscar Wilde, Friedrich Nietzsche and Max Stirner. His (1906) "L'Ideal Humain de l'Art" helped found the "artistocracy movement"—a movement advocating life in the service of art. His ideal was an anti-elitist aestheticism: "All men should be artists". Together with André Colomer and Manuel Devaldes, in 1913 he founded "L'Action d'Art", an anarchist literary journal. After World War II, he contributed to the journal "L'Unique". Within the synthesist anarchist organization, the Fédération Anarchiste, there existed an individualist anarchist tendency alongside anarcho-communist and anarchosyndicalist currents. Individualist anarchists participating inside the Fédération Anarchiste included Charles-Auguste Bontemps, Georges Vincey and André Arru. The new base principles of the francophone Anarchist Federation were written by the individualist anarchist Charles-Auguste Bontemps and the anarcho-communist Maurice Joyeux which established an organization with a plurality of tendencies and autonomy of federated groups organized around synthesist principles. Charles-Auguste Bontemps was a prolific author mainly in the anarchist, freethinking, pacifist and naturist press of the time. His view on anarchism was based around his concept of "Social Individualism" on which he wrote extensively. He defended an anarchist perspective which consisted on "a collectivism of things and an individualism of persons".

In 2002, Libertad organized a new version of the "L'EnDehors", collaborating with "Green Anarchy" and including several contributors, such as Lawrence Jarach, Patrick Mignard, Thierry Lodé, Ron Sakolsky and Thomas Slut. Numerous articles about capitalism, human rights, free love and social fights were published. "The EnDehors" continues now as a website, EnDehors.org.

The prolific contemporary French philosopher Michel Onfray has been writing from an individualist anarchist perspective influenced by Nietzsche, French post-structuralists thinkers such as Michel Foucault and Gilles Deleuze; and Greek classical schools of philosophy such as the Cynics and Cyrenaics. Among the books which best expose Onfray's individualist anarchist perspective include "La sculpture de soi : la morale esthétique" ("The Sculpture of Oneself: Aesthetic Morality"), "La philosophie féroce : exercices anarchistes", "La puissance d'exister" and "Physiologie de Georges Palante, portrait d'un nietzchéen de gauche" which focuses on French individualist philosopher Georges Palante.

Illegalism is an anarchist philosophy that developed primarily in France, Italy, Belgium and Switzerland during the early 1900s as an outgrowth of Stirner's individualist anarchism. Illegalists usually did not seek moral basis for their actions, recognizing only the reality of "might" rather than "right"; and for the most part, illegal acts were done simply to satisfy personal desires, not for some greater ideal, although some committed crimes as a form of propaganda of the deed. The illegalists embraced direct action and propaganda of the deed.

Influenced by theorist Max Stirner's egoism as well as Pierre-Joseph Proudhon (his view that "Property is theft!"), Clément Duval and Marius Jacob proposed the theory of la "reprise individuelle" (individual reclamation) which justified robbery on the rich and personal direct action against exploiters and the system.

Illegalism first rose to prominence among a generation of Europeans inspired by the unrest of the 1890s, during which Ravachol, Émile Henry, Auguste Vaillant and Sante Geronimo Caserio committed daring crimes in the name of anarchism in what is known as propaganda of the deed. France's Bonnot Gang was the most famous group to embrace illegalism.

In Germany, the Scottish-German John Henry McKay became the most important propagandist for individualist anarchist ideas. He fused Stirnerist egoism with the positions of Benjamin Tucker and actually translated Tucker into German. Two semi-fictional writings of his own, "Die Anarchisten" and "Der Freiheitsucher", contributed to individualist theory through an updating of egoist themes within a consideration of the anarchist movement. English translations of these works arrived in the United Kingdom and in individualist American circles led by Tucker. McKay is also known as an important European early activist for gay rights.

Using the pseudonym Sagitta, Mackay wrote a series of works for pederastic emancipation, titled "Die Buecher der namenlosen Liebe" ("Books of the Nameless Love"). This series was conceived in 1905 and completed in 1913 and included the "Fenny Skaller", a story of a pederast. Under the same pseudonym, he also published fiction, such as "Holland" (1924) and a pederastic novel of the Berlin boy-bars, "Der Puppenjunge" ("The Hustler") (1926).
Adolf Brand (1874–1945) was a German writer, Stirnerist anarchist and pioneering campaigner for the acceptance of male bisexuality and homosexuality. In 1896, Brand published a German homosexual periodical, "Der Eigene". This was the first ongoing homosexual publication in the world. The name was taken from writings of egoist philosopher Max Stirner (who had greatly influenced the young Brand) and refers to Stirner's concept of "self-ownership" of the individual. "Der Eigene" concentrated on cultural and scholarly material and may have had an average of around 1,500 subscribers per issue during its lifetime, although the exact numbers are uncertain. Contributors included Erich Mühsam, Kurt Hiller, John Henry Mackay (under the pseudonym Sagitta) and artists Wilhelm von Gloeden, Fidus and Sascha Schneider. Brand contributed many poems and articles himself. Benjamin Tucker followed this journal from the United States.

"Der Einzige" was a German individualist anarchist magazine. It appeared in 1919 as a weekly, then sporadically until 1925 and was edited by cousins Anselm Ruest (pseudonym for Ernst Samuel) and Mynona (pseudonym for Salomo Friedlaender). Its title was adopted from the book "Der Einzige und sein Eigentum" ("The Ego and Its Own") by Max Stirner. Another influence was the thought of German philosopher Friedrich Nietzsche. The publication was connected to the local expressionist artistic current and the transition from it towards Dada.

In Italy, individualist anarchism had a strong tendency towards illegalism and violent propaganda by the deed similar to French individualist anarchism, but perhaps more extreme and which emphazised criticism of organization be it anarchist or of other type. In this respect, we can consider notorious magnicides carried out or attempted by individualists Giovanni Passannante, Sante Caserio, Michele Angiolillo, Luigi Luccheni and Gaetano Bresci who murdered King Umberto I. Caserio lived in France and coexisted within French illegalism and later assassinated French President Sadi Carnot. The theoretical seeds of current insurrectionary anarchism were already laid out at the end of 19th century Italy in a combination of individualist anarchism criticism of permanent groups and organization with a socialist class struggle worldview. During the rise of fascism, this thought also motivated Gino Lucetti, Michele Schirru and Angelo Sbardellotto in attempting the assassination of Benito Mussolini.

During the early 20th century, the intellectual work of individualist anarchist Renzo Novatore came to importance and he was influenced by Max Stirner, Friedrich Nietzsche, Georges Palante, Oscar Wilde, Henrik Ibsen, Arthur Schopenhauer and Charles Baudelaire. He collaborated in numerous anarchist journals and participated in futurism avant-garde currents. In his thought, he adhered to Stirnerist disrespect for private property, only recognizing property of one's own spirit. Novatore collaborated in the individualist anarchist journal "Iconoclasta!" alongside the young Stirnerist illegalist Bruno Filippi.

The individualist philosopher and poet Renzo Novatore belonged to the leftist section of the avant-garde movement of futurism alongside other individualist anarcho-futurists such as Dante Carnesecchi, Leda Rafanelli, Auro d'Arcola and Giovanni Governato. There was also Pietro Bruzzi who published the journal "L'Individualista" in the 1920s alongside Ugo Fedeli and Francesco Ghezzi, but who fell to fascist forces later. Bruzzi also collaborated with the Italian American individualist anarchist publication "Eresia" of New York City edited by Enrico Arrigoni.

During the Founding Congress of the Italian Anarchist Federation in 1945, there was a group of individualist anarchists led by Cesare Zaccaria who was an important anarchist of the time. Later during the IX Congress of the Italian Anarchist Federation in Carrara in 1965, a group decided to split off from this organization and created the "Gruppi di Iniziativa Anarchica". In the 1970s, it was mostly composed of "veteran individualist anarchists with an of pacifism orientation, naturism".

In the famous Italian insurrectionary anarchist essay written by an anonymous writer, "At Daggers Drawn with the Existent, its Defenders and its False Critics", there reads how "[t]he workers who, during a wildcat strike, carried a banner saying, 'We are not asking for anything' understood that the defeat is in the claim itself ('the claim against the enemy is eternal'). There is no alternative but to take everything. As Stirner said: 'No matter how much you give them, they will always ask for more, because what they want is no less than the end of every concession'". The contemporary imprisoned Italian insurrectionary anarchist philosopher writes from an explicit individualist anarchist perspective in such essays as "Critica individualista anarchica alla modernità" ("Individualist Anarchist Critique of Modernity"). Horst Fantazzini (March 4, 1939 – December 24, 2001) was an Italian-German individualist anarchist who pursued an illegalist lifestyle and practice until his death in 2001. He gained media notoriety mainly due to his many bank robberies through Italy and other countries. In 1999, the film "Ormai è fatta!" appeared based on his life.

Individualist anarchism was one of the three categories of anarchism in Russia, along with the more prominent anarcho-communism and anarcho-syndicalism. The ranks of the Russian individualist anarchists were predominantly drawn from the intelligentsia and the working class. For anarchist historian Paul Avrich, "[t]he two leading exponents of individualist anarchism, both based in Moscow, were Aleksei Alekseevich Borovoi and Lev Chernyi (Pavel Dmitrievich Turchaninov). From Nietzsche, they inherited the desire for a complete overturn of all values accepted by bourgeois society political, moral, and cultural. Furthermore, strongly influenced by Max Stirner and Benjamin Tucker, the German and American theorists of individualist anarchism, they demanded the total liberation of the human personality from the fetters of organized society".

Some Russian individualists anarchists "found the ultimate expression of their social alienation in violence and crime, others attached themselves to avant-garde literary and artistic circles, but the majority remained "philosophical" anarchists who conducted animated parlor discussions and elaborated their individualist theories in ponderous journals and books".

Lev Chernyi was an important individualist anarchist involved in resistance against the rise to power of the Bolshevik Party as he adhered mainly to Stirner and the ideas of Tucker. In 1907, he published a book entitled "Associational Anarchism" in which he advocated the "free association of independent individuals". On his return from Siberia in 1917, he enjoyed great popularity among Moscow workers as a lecturer. Chernyi was also Secretary of the Moscow Federation of Anarchist Groups, which was formed in March 1917. He was an advocate "for the seizure of private homes", which was an activity seen by the anarchists after the October Revolution as direct expropriation on the bourgoise. He died after being accused of participation in an episode in which this group bombed the headquarters of the Moscow Committee of the Communist Party. Although most likely not being really involved in the bombing, he might have died of torture.

Chernyi advocated a Nietzschean overthrow of the values of bourgeois Russian society, and rejected the voluntary communes of anarcho-communist Peter Kropotkin as a threat to the freedom of the individual. Scholars including Avrich and Allan Antliff have interpreted this vision of society to have been greatly influenced by the individualist anarchists Max Stirner and Benjamin Tucker. Subsequent to the book's publication, Chernyi was imprisoned in Siberia under the Russian Czarist regime for his revolutionary activities.

On the other hand, Alexei Borovoi (1876?–1936) was a professor of philosophy at Moscow University, "a gifted orator and the author of numerous books, pamphlets, and articles which attempted to reconcile individualist anarchism with the doctrines of syndicallism". He wrote among other theoretical works "Anarkhizm" in 1918, just after the October Revolution; and "Anarchism and Law". For him, "the chief importance is given not to Anarchism as the aim but to Anarchy as the continuous quest for the aim". He manifests there that "[n]o social ideal, from the point of view of anarchism, could be referred to as absolute in a sense that supposes it's the crown of human wisdom, the end of social and ethical quest of man".

While Spain was influenced by American individualist anarchism, it was more closely related to the French currents. Around the start of the 20th century, individualism in Spain gathered force through the efforts of people such as Dorado Montero, Ricardo Mella, Federico Urales, Miguel Gimenez Igualada, Mariano Gallardo and J. Elizalde who translated French and American individualists. Important in this respect were also magazines such as "La Idea Libre", "La revista blanca", "Etica", "Iniciales", "Al margen", "Estudios" and "Nosotros". The most influential thinkers there were Max Stirner, Émile Armand and Han Ryner. Just as in France, the spread of Esperanto and anationalism had importance just as naturism and free love currents. Later, Armand and Ryner themselves started writing in the Spanish individualist press. Armand's concept of amorous camaraderie had an important role in motivating polyamory as realization of the individual.

Catalan historian Xavier Diez reports that the Spanish individualist anarchist press was widely read by members of anarcho-communist groups and by members of the anarcho-syndicalist trade union CNT. There were also the cases of prominent individualist anarchists such as Federico Urales and Miguel Gimenez Igualada who were members of the CNT and J. Elizalde who was a founding member and first secretary of the Iberian Anarchist Federation (IAF).

Spanish individualist anarchist Miguel Giménez Igualada wrote the lengthy theory book called "Anarchism" espousing his individualist anarchism. Between October 1937 and February 1938, he was editor of the individualist anarchist magazine "Nosotros" in which many works of Armand and Ryner appeared. He also participated in the publishing of another individualist anarchist maganize "Al Margen: Publicación quincenal individualista". In his youth, he engaged in illegalist activities. His thought was deeply influenced by Max Stirner, of which he was the main popularizer in Spain through his own writings. He published and wrote the preface to the fourth edition in Spanish of "The Ego and Its Own" from 1900. He proposed the creation of a "Union of egoists" to be a federation of individualist anarchists in Spain, but it did not succeed. In 1956, he published an extensive treatise on Stirner, dedicated to fellow individualist anarchist Émile Armand. Afterwards, he traveled and lived in Argentina, Uruguay and Mexico.

Federico Urales was an important individualist anarchist who edited "La Revista Blanca". The individualist anarchism of Urales was influenced by Auguste Comte and Charles Darwin. He saw science and reason as a defense against blind servitude to authority. He was critical of influential individualist thinkers such as Nietzsche and Stirner for promoting an asocial egoist individualism and instead promoted an individualism with solidarity seen as a way to guarantee social equality and harmony. He was highly critical of anarcho-syndicalism, which he viewed as plagued by excessive bureaucracy; and he thought that it tended towards reformism. Instead, he favored small groups based on ideological alignment. He supported and participated in the establishment of the IAF in 1927.

In 1956, Miguel Giménez Igualada—on exile escaping from Franco's dictatorship—published an extensive treatise on Stirner which he dedicated to fellow individualist anarchist Émile Armand. On the subject of individualist anarchist theory, he publisheds "Anarchism" in 1968 during his exile in Mexico from Franco's dictatorship in Spain. He was present in the First Congress of the Mexican Anarchist Federation in 1945.

In 2000, Ateneo Libertario Ricardo Mella, Ateneo Libertario Al Margen, Ateneu Enciclopèdic Popular, Ateneo Libertario de Sant Boi and Ateneu Llibertari Poble Sec y Fundació D'Estudis Llibertaris i Anarcosindicalistes republished Émile Armand's writings on free love and individualist anarchism in a compilation titled "Individualist anarchism and Amorous camaraderie". Recently, Spanish historian Xavier Diez has dedicated extensive research on Spanish individualist anarchism as can be seen in his books "El anarquismo individualista en España: 1923–1938" and "Utopia sexual a la premsa anarquista de Catalunya. La revista Ética-Iniciales(1927–1937)" which deals with free love thought as present in the Spanish individualist anarchist magazine "Iniciales".

The English Enlightenment political theorist William Godwin was an important influence as mentioned before. The Irish anarchist writer of the Decadent Movement Oscar Wilde influenced individualist anarchists such as Renzo Novatore and gained the admiration of Benjamin Tucker. In his important essay "The Soul of Man under Socialism" from 1891, Wilde defended socialism as the way to guarantee individualism and so he saw that "[w]ith the abolition of private property, then, we shall have true, beautiful, healthy Individualism. Nobody will waste his life in accumulating things, and the symbols for things. One will live. To live is the rarest thing in the world. Most people exist, that is all". For anarchist historian George Woodcock, "Wilde's aim in "The Soul of Man under Socialism" is to seek the society most favorable to the artist [...] for Wilde art is the supreme end, containing within itself enlightenment and regeneration, to which all else in society must be subordinated [...] Wilde represents the anarchist as aesthete". Woodcock finds that "[t]he most ambitious contribution to literary anarchism during the 1890s was undoubtedly Oscar Wilde "The Soul of Man under Socialism"" and finds that it is influenced mainly by the thought of William Godwin.

In the late 19th century in the United Kingdom, there existed individualist anarchists such as Wordsworth Donisthorpe, Joseph Hiam Levy, Joseph Greevz Fisher, John Badcock Jr., Albert Tarn and Henry Albert Seymour who were close to the United States group around Benjamin Tucker's magazine "Liberty". In the mid-1880s, Seymour published a journal called "The Anarchist" and also later took a special interest in free love as he participated in the journal "The Adult: A Journal for the Advancement of Freedom in Sexual Relationships". "The Serpent", issued from London, was the most prominent English-language egoist journal and published from 1898 to 1900 with the subtitle "A Journal of Egoistic Philosophy and Sociology". Henry Meulen was another British anarchist who was notable for his support of free banking.

In the United Kingdom, Herbert Read was influenced highly by egoism as he later approached existentialism (see existentialist anarchism). Albert Camus devoted a section of "The Rebel" to Stirner. Although throughout his book Camus is concerned to present "the rebel" as a preferred alternative to "the revolutionary", he nowhere acknowledges that this distinction is taken from the one that Stirner makes between "the revolutionary" and "the insurrectionist". Sidney Parker is a British egoist individualist anarchist who wrote articles and edited anarchist journals from 1963 to 1993 such as "Minus One", "Egoist", and "Ego". Donald Rooum is an English anarchist cartoonist and writer with a long association with Freedom Press. Rooum stated that for his thought, "[t]he most influential source is Max Stirner. I am happy to be called a Stirnerite anarchist, provided 'Stirnerite' means one who agrees with Stirner's general drift, not one who agrees with Stirner's every word". "An Anarchist FAQ" reports: "From meeting anarchists in Glasgow during the Second World War, long-time anarchist activist and artist Donald Rooum likewise combined Stirner and anarcho-communism".

In the hybrid of post-structuralism and anarchism called post-anarchism, Saul Newman has written a lot on Stirner and his similarities to post-structuralism. He writes:
Max Stirner's impact on contemporary political theory is often neglected. However in Stirner's political thinking there can be found a surprising convergence with poststructuralist theory, particularly with regard to the function of power. Andrew Koch, for instance, sees Stirner as a thinker who transcends the Hegelian tradition he is usually placed in, arguing that his work is a precursor poststructuralist ideas about the foundations of knowledge and truth.

Newman has published several essays on Stirner. "War on the State: Stirner and Deleuze's Anarchism" and "Empiricism, Pluralism, and Politics in Deleuze and Stirner" discusses what he sees are similarities between Stirner's thought and that of Gilles Deleuze. In "Spectres of Stirner: A Contemporary Critique of Ideology", he discusses the conception of ideology in Stirner. In "Stirner and Foucault: Toward a Post-Kantian Freedom", similarities between Stirner and Michel Foucault. He also wrote "Politics of the Ego: Stirner's Critique of Liberalism".

Argentine anarchist historian Angel Cappelletti reports that in Argentina "[a]mong the workers that came from Europe in the 2 first decades of the century, there was curiously some stirnerian individualists influenced by the philosophy of Nietzsche, that saw syndicalism as a potential enemy of anarchist ideology. They established [...] affinity groups that in 1912 came to, according to Max Nettlau, to the number of 20. In 1911 there appeared, in Colón, the periodical "El Único", that defined itself as 'Publicación individualista'".

Vicente Rojas Lizcano, whose pseudonym was Biófilo Panclasta, was a Colombian individualist anarchist writer and activist. In 1904, he began using the name Biofilo Panclasta. "Biofilo" in Spanish stands for "lover of life" and "Panclasta" for "enemy of all". He visited more than fifty countries propagandizing for anarchism which in his case was highly influenced by the thought of Stirner and Nietszche. Among his written works there are "Siete años enterrado vivo en una de las mazmorras de Gomezuela: Horripilante relato de un resucitado"(1932) and "Mis prisiones, mis destierros y mi vida" (1929) which talk about his many adventures while living his life as an adventurer, activist and vagabond as well as his thought and the many times he was imprisoned in different countries.
Maria Lacerda de Moura was a Brazilian teacher, journalist, anarcha-feminist and individualist anarchist. Her ideas regarding education were largely influenced by Francisco Ferrer. She later moved to São Paulo and became involved in journalism for the anarchist and labor press. There she also lectured on topics including education, women's rights, free love and antimilitarism. Her writings and essays garnered her attention not only in Brazil, but also in Argentina and Uruguay. In February 1923, she launched "Renascença", a periodical linked with the anarchist, progressive and freethinking circles of the period. Her thought was mainly influenced by individualist anarchists such as Han Ryner and Émile Armand. She maintained contact with Spanish individualist anarchist circles.

Horst Matthai Quelle was a Spanish language German anarchist philosopher influenced by Max Stirner. In 1938, at the beginning of the German economic crisis and the rise of Nazism and fascism in Europe, Quelle moved to Mexico. Quelle earned his undergraduate degree, master's and doctorate in philosophy at the National Autonomous University of Mexico, where he returned as a professor of philosophy in the 1980s. He argued that since the individual gives form to the world, he is those objects, the others and the whole universe. One of his main views was a "theory of infinite worlds" which for him was developed by pre-Socratic philosophers.

During the 1990s in Argentina, there appeared a Stirnerist publication called "El Único: publicacion periódica de pensamiento individualista".

Philosopher Murray Bookchin criticized individualist anarchism for its opposition to democracy and its embrace of "lifestylism" at the expense of anti-capitalism. Bookchin claimed that individualist anarchism supports only negative liberty and rejects the idea of positive liberty. Philosopher Albert Meltzer proposed that individualist anarchism differs radically from revolutionary anarchism and that it "is sometimes too readily conceded 'that this is, after all, anarchism'". Meltzer claimed that Benjamin Tucker's acceptance of the use of a private police force (including to break up violent strikes to protect the "employer's 'freedom'") is contradictory to the definition of anarchism as "no government". Meltzer opposed anarcho-capitalism for similar reasons, arguing that it actually supports a "limited State" and that "it is only possible to conceive of Anarchism which is free, communistic and offering no economic necessity for repression of countering it". Tucker's views of strikes and trade unions evolved from skepticism, believing that strikes should be organized by free workers rather than by bureaucratic union officials and organizations, to sympathize with those involved in the Haymarket massacre.

Philosopher George Bernard Shaw initially had flirtations with individualist anarchism before coming to the conclusion that it was "the negation of socialism, and is, in fact, unsocialism carried as near to its logical conclusion as any sane man dare carry it". Shaw's argument was that even if wealth was initially distributed equally, the degree of "laissez-faire" advocated by Tucker would result in the distribution of wealth becoming unequal because it would permit private appropriation and accumulation. According to academic Carlotta Anderson, American individualist anarchists accept that free competition results in unequal wealth distribution, but they "do not see that as an injustice". Tucker explained that "[i]f I go through life free and rich, I shall not cry because my neighbor, equally free, is richer. Liberty will ultimately make all men rich; it will not make all men equally rich. Authority may (and may not) make all men equally rich in purse; it certainly will make them equally poor in all that makes life best worth living". Nonetheless, Peter Marshall states that "the egalitarian implications of traditional individualist anarchists like Spooner and Tucker" have been overlooked.

Without the labor theory of value, some argue that 19th-century individualist anarchists approximate the modern movement of anarcho-capitalism, although this has been contested or rejected. As economic theory changed, the popularity of the labor theory of classical economics was superseded by the subjective theory of value of neoclassical economics and Murray Rothbard, a student of Ludwig von Mises, combined Mises' Austrian School of economics with the absolutist views of human rights and rejection of the state he had absorbed from studying the individualist American anarchists of the 19th century such as Tucker and Lysander Spooner. In the mid-1950s, Rothbard argued that "we are not anarchists [...] but not archists either [...]. Perhaps, then, we could call ourselves by a new name: nonarchist", concerned with differentiating himself from communist and socialistic economic views of other anarchists, including the individualist anarchists of the 19th century. American mutualist Joe Peacott has criticized anarcho-capitalists for trying to hegemonize the individualist anarchism label and make appear as if all individualist anarchists are in favor of capitalism. Peacott has stated that "individualists, both past and present, agree with the communist anarchists that present-day capitalism is based on economic coercion, not on voluntary contract. Rent and interest are mainstays of modern capitalism, and are protected and enforced by the state. Without these two unjust institutions, capitalism could not exist".

There is a strong current within anarchism which does not consider that anarcho-capitalism can be considered a part of the anarchist movement due to the fact that anarchism has historically been an anti-capitalist movement and because anarchists see it as incompatible with capitalist forms. Although anarcho-capitalism has been regarded by some as a form of individualist anarchism, others disagree or contest such divide because individualist anarchism is largely libertarian socialist. Rothbard argued that individualist anarchism is different from anarcho-capitalism and other capitalist theories due to the individualist anarchists retaining the labor theory of value and socialist doctrines; and many writers deny that anarcho-capitalism is a form of anarchism at all or that capitalism itself is compatible with anarchism.



</doc>
<doc id="14937" url="https://en.wikipedia.org/wiki?curid=14937" title="Italo Calvino">
Italo Calvino

Italo Calvino (, , ; 15 October 1923 – 19 September 1985) was an Italian journalist and writer of short stories and novels. His best known works include the "Our Ancestors" trilogy (1952–1959), the "Cosmicomics" collection of short stories (1965), and the novels "Invisible Cities" (1972) and "If on a winter's night a traveler" (1979).

Admired in Britain, Australia and the United States, he was the most translated contemporary Italian writer at the time of his death.

Italo Calvino is buried in the cemetery of Castiglione della Pescaia, in Tuscany.

Italo Calvino was born in Santiago de las Vegas, a suburb of Havana, Cuba, in 1923. His father, Mario, was a tropical agronomist and botanist who also taught agriculture and floriculture. Born 47 years earlier in Sanremo, Italy, Mario Calvino had emigrated to Mexico in 1909 where he took up an important position with the Ministry of Agriculture. In an autobiographical essay, Italo Calvino explained that his father "had been in his youth an anarchist, a follower of Kropotkin and then a Socialist Reformist". In 1917, Mario left for Cuba to conduct scientific experiments, after living through the Mexican Revolution.

Calvino's mother, Giuliana Luigia Evelina "Eva" Mameli, was a botanist and university professor. A native of Sassari in Sardinia and 11 years younger than her husband, she married while still a junior lecturer at Pavia University. Born into a secular family, Eva was a pacifist educated in the "religion of civic duty and science". Eva gave Calvino his unusual first name to remind him of his Italian heritage, although since he wound up growing up in Italy after all, Calvino thought his name sounded "belligerently nationalist". Calvino described his parents as being "very different in personality from one another", suggesting perhaps deeper tensions behind a comfortable, albeit strict, middle-class upbringing devoid of conflict. As an adolescent, he found it hard relating to poverty and the working-class, and was "ill at ease" with his parents' openness to the laborers who filed into his father's study on Saturdays to receive their weekly paycheck.

In 1925, less than two years after Calvino's birth, the family returned to Italy and settled permanently in Sanremo on the Ligurian coast. Calvino's brother Floriano, who became a distinguished geologist, was born in 1927.

The family divided their time between the Villa Meridiana, an experimental floriculture station which also served as their home, and Mario's ancestral land at San Giovanni Battista. On this small working farm set in the hills behind Sanremo, Mario pioneered in the cultivation of then exotic fruits such as avocado and grapefruit, eventually obtaining an entry in the "Dizionario biografico degli italiani" for his achievements. The vast forests and luxuriant fauna omnipresent in Calvino's early fiction such as "The Baron in the Trees" derives from this "legacy". In an interview, Calvino stated that "San Remo continues to pop out in my books, in the most diverse pieces of writing." He and Floriano would climb the tree-rich estate and perch for hours on the branches reading their favorite adventure stories. Less salubrious aspects of this "paternal legacy" are described in "The Road to San Giovanni", Calvino's memoir of his father in which he exposes their inability to communicate: "Talking to each other was difficult. Both verbose by nature, possessed of an ocean of words, in each other's presence we became mute, would walk in silence side by side along the road to San Giovanni." A fan of Rudyard Kipling's "The Jungle Book" as a child, Calvino felt that his early interest in stories made him the "black sheep" of a family that held literature in less esteem than the sciences. Fascinated by American movies and cartoons, he was equally attracted to drawing, poetry, and theatre. On a darker note, Calvino recalled that his earliest memory was of a Marxist professor who had been brutally assaulted by Benito Mussolini's Blackshirts: "I remember clearly that we were at dinner when the old professor came in with his face beaten up and bleeding, his bowtie all torn up over it, asking for help."

Other legacies include the parents' beliefs in Freemasonry, Republicanism with elements of Anarchism and Marxism. Austere freethinkers with an intense hatred of the ruling National Fascist Party, Eva and Mario also refused to give their sons any education in the Catholic Faith or any other religion. Italo attended the English nursery school St George's College, followed by a Protestant elementary private school run by Waldensians. His secondary schooling, with a classical lyceum curriculum, was completed at the state-run Liceo Gian Domenico Cassini where, at his parents' request, he was exempted from religion classes but frequently asked to justify his anti-conformism to teachers, janitors, and fellow pupils. In his mature years, Calvino described the experience as having made him "tolerant of others' opinions, particularly in the field of religion, remembering how irksome it was to hear myself mocked because I did not follow the majority's beliefs". In 1938, Eugenio Scalfari, who went on to found the weekly magazine "L'Espresso" and "La Repubblica", a major Italian newspaper, came from Civitavecchia to join the same class though a year younger, and they shared the same desk. The two teenagers formed a lasting friendship, Calvino attributing his political awakening to their university discussions. Seated together "on a huge flat stone in the middle of a stream near our land", he and Scalfari founded the MUL (University Liberal Movement).

Eva managed to delay her son's enrolment in the Party's armed scouts, the "Balilla Moschettieri", and then arranged that he be excused, as a non-Catholic, from performing devotional acts in Church. But later on, as a compulsory member, he could not avoid the assemblies and parades of the "Avanguardisti", and was forced to participate in the Italian invasion of the French Riviera in June 1940.

In 1941, Calvino enrolled at the University of Turin, choosing the Agriculture Faculty where his father had previously taught courses in agronomy. Concealing his literary ambitions to please his family, he passed four exams in his first year while reading anti-Fascist works by Elio Vittorini, Eugenio Montale, Cesare Pavese, Johan Huizinga, and Pisacane, and works by Max Planck, Werner Heisenberg, and Albert Einstein on physics. Disdainful of Turin students, Calvino saw himself as enclosed in a "provincial shell" that offered the illusion of immunity from the Fascist nightmare: "We were ‘hard guys’ from the provinces, hunters, snooker-players, show-offs, proud of our lack of intellectual sophistication, contemptuous of any patriotic or military rhetoric, coarse in our speech, regulars in the brothels, dismissive of any romantic sentiment and desperately devoid of women."

Calvino transferred to the University of Florence in 1943 and reluctantly passed three more exams in agriculture. By the end of the year, the Germans had succeeded in occupying Liguria and setting up Benito Mussolini's puppet Republic of Salò in northern Italy. Now twenty years old, Calvino refused military service and went into hiding. Reading intensely in a wide array of subjects, he also reasoned politically that, of all the partisan groupings, the communists were the best organized with "the most convincing political line".

In spring 1944, Eva encouraged her sons to enter the Italian Resistance in the name of "natural justice and family virtues". Using the battlename of "Santiago", Calvino joined the "Garibaldi Brigades", a clandestine Communist group and, for twenty months, endured the fighting in the Maritime Alps until 1945 and the Liberation. As a result of his refusal to be a conscript, his parents were held hostage by the Nazis for an extended period at the Villa Meridiana. Calvino wrote of his mother's ordeal that "she was an example of tenacity and courage… behaving with dignity and firmness before the SS and the Fascist militia, and in her long detention as a hostage, not least when the blackshirts three times pretended to shoot my father in front of her eyes. The historical events which mothers take part in acquire the greatness and invincibility of natural phenomena".

Calvino settled in Turin in 1945, after a long hesitation over living there or in Milan. He often humorously belittled this choice, describing Turin as a "city that is serious but sad". Returning to university, he abandoned Agriculture for the Arts Faculty. A year later, he was initiated into the literary world by Elio Vittorini, who published his short story "Andato al comando" (1945; "Gone to Headquarters") in "Il Politecnico", a Turin-based weekly magazine associated with the university. The horror of the war had not only provided the raw material for his literary ambitions but deepened his commitment to the Communist cause. Viewing civilian life as a continuation of the partisan struggle, he confirmed his membership of the Italian Communist Party. On reading Vladimir Lenin's "State and Revolution", he plunged into post-war political life, associating himself chiefly with the worker's movement in Turin.

In 1947, he graduated with a Master's thesis on Joseph Conrad, wrote short stories in his spare time, and landed a job in the publicity department at the Einaudi publishing house run by Giulio Einaudi. Although brief, his stint put him in regular contact with Cesare Pavese, Natalia Ginzburg, Norberto Bobbio, and many other left-wing intellectuals and writers. He then left Einaudi to work as a journalist for the official Communist daily, "L'Unità", and the newborn Communist political magazine, "Rinascita". During this period, Pavese and poet Alfonso Gatto were Calvino's closest friends and mentors.

His first novel, "Il sentiero dei nidi di ragno" ("The Path to the Nest of Spiders") written with valuable editorial advice from Pavese, won the Premio Riccione on publication in 1947. With sales topping 5000 copies, a surprise success in postwar Italy, the novel inaugurated Calvino's neorealist period. In a clairvoyant essay, Pavese praised the young writer as a "squirrel of the pen" who "climbed into the trees, more for fun than fear, to observe partisan life as a fable of the forest". In 1948, he interviewed one of his literary idols, Ernest Hemingway, travelling with Natalia Ginzburg to his home in Stresa.

"Ultimo viene il corvo" ("The Crow Comes Last"), a collection of stories based on his wartime experiences, was published to acclaim in 1949. Despite the triumph, Calvino grew increasingly worried by his inability to compose a worthy second novel. He returned to Einaudi in 1950, responsible this time for the literary volumes. He eventually became a consulting editor, a position that allowed him to hone his writing talent, discover new writers, and develop into "a reader of texts". In late 1951, presumably to advance in the Communist Party, he spent two months in the Soviet Union as correspondent for "l'Unità". While in Moscow, he learned of his father's death on 25 October. The articles and correspondence he produced from this visit were published in 1952, winning the Saint-Vincent Prize for journalism.

Over a seven-year period, Calvino wrote three realist novels, "The White Schooner" (1947–1949), "Youth in Turin" (1950–1951), and "The Queen's Necklace" (1952–54), but all were deemed defective. Calvino’s first efforts as a fictionist were marked with his experience in the Italian resistance during the Second World War, however his acclamation as a writer of fantastic stories came in the 1950s. During the eighteen months it took to complete "I giovani del Po" ("Youth in Turin"), he made an important self-discovery: "I began doing what came most naturally to me – that is, following the memory of the things I had loved best since boyhood. Instead of making myself write the book I "ought" to write, the novel that was expected of me, I conjured up the book I myself would have liked to read, the sort by an unknown writer, from another age and another country, discovered in an attic." The result was "Il visconte dimezzato" (1952; "The Cloven Viscount") composed in 30 days between July and September 1951. The protagonist, a seventeenth century viscount sundered in two by a cannonball, incarnated Calvino's growing political doubts and the divisive turbulence of the Cold War. Skillfully interweaving elements of the fable and the fantasy genres, the allegorical novel launched him as a modern "fabulist". In 1954, Giulio Einaudi commissioned his "Fiabe Italiane" (1956; "Italian Folktales") on the basis of the question, "Is there an Italian equivalent of the Brothers Grimm?" For two years, Calvino collated tales found in 19th century collections across Italy then translated 200 of the finest from various dialects into Italian. Key works he read at this time were Vladimir Propp's "Morphology of the Folktale" and "Historical Roots of Russian Fairy Tales", stimulating his own ideas on the origin, shape and function of the story.

In 1952 Calvino wrote with Giorgio Bassani for "Botteghe Oscure", a magazine named after the popular name of the party's head-offices in Rome. He also worked for "Il Contemporaneo", a Marxist weekly.

From 1955 to 1958 Calvino had an affair with Italian actress Elsa De Giorgi, a married, older woman. Excerpts of the hundreds of love letters Calvino wrote to her were published in the "Corriere della Sera" in 2004, causing some controversy.

In 1957, disillusioned by the 1956 Soviet invasion of Hungary, Calvino left the Italian Communist Party. In his letter of resignation published in "L'Unità" on 7 August, he explained the reason of his dissent (the violent suppression of the Hungarian uprising and the revelation of Joseph Stalin's crimes) while confirming his "confidence in the democratic perspectives" of world Communism. He withdrew from taking an active role in politics and never joined another party. Ostracized by the PCI party leader Palmiro Togliatti and his supporters on publication of "Becalmed in the Antilles" ("La gran bonaccia delle Antille"), a satirical allegory of the party's immobilism, Calvino began writing "The Baron in the Trees". Completed in three months and published in 1957, the fantasy is based on the "problem of the intellectual's political commitment at a time of shattered illusions". He found new outlets for his periodic writings in the journals "Città aperta" and "Tempo presente", the magazine "Passato e presente", and the weekly "Italia Domani". With Vittorini in 1959, he became co-editor of "'Il Menabò", a cultural journal devoted to literature in the modern industrial age, a position he held until 1966.

Despite severe restrictions in the US against foreigners holding communist views, Calvino was allowed to visit the United States, where he stayed six months from 1959 to 1960 (four of which he spent in New York), after an invitation by the Ford Foundation. Calvino was particularly impressed by the "New World": "Naturally I visited the South and also California, but I always felt a New Yorker. My city is New York." The letters he wrote to Einaudi describing this visit to the United States were first published as "American Diary 1959–1960" in "Hermit in Paris" in 2003.

In 1962 Calvino met Argentinian translator Esther Judith Singer ("Chichita") and married her in 1964 in Havana, during a trip in which he visited his birthplace and was introduced to Ernesto "Che" Guevara. On 15 October 1967, a few days after Guevara's death, Calvino wrote a tribute to him that was published in Cuba in 1968, and in Italy thirty years later. He and his wife settled in Rome in the via Monte Brianzo where their daughter, Giovanna, was born in 1965. Once again working for Einaudi, Calvino began publishing some of his "Cosmicomics" in "Il Caffè", a literary magazine.

Vittorini's death in 1966 greatly affected Calvino. He went through what he called an "intellectual depression", which the writer himself described as an important passage in his life: "...I ceased to be young. Perhaps it's a metabolic process, something that comes with age, I'd been young for a long time, perhaps too long, suddenly I felt that I had to begin my old age, yes, old age, perhaps with the hope of prolonging it by beginning it early."

In the fermenting atmosphere that evolved into 1968's cultural revolution (the French May), he moved with his family to Paris in 1967, setting up home in a villa in the Square de Châtillon. Nicknamed "L'ironique amusé", he was invited by Raymond Queneau in 1968 to join the Oulipo ("Ouvroir de littérature potentielle") group of experimental writers where he met Roland Barthes, and Georges Perec, all of whom influenced his later production. That same year, he turned down the Viareggio Prize for "Ti con zero" ("Time and the Hunter") on the grounds that it was an award given by "institutions emptied of meaning". He accepted, however, both the Asti Prize and the Feltrinelli Prize for his writing in 1970 and 1972, respectively. In two autobiographical essays published in 1962 and 1970, Calvino described himself as "atheist" and his outlook as "non-religious".

Calvino had more intense contacts with the academic world, with notable experiences at the Sorbonne (with Barthes) and the University of Urbino. His interests included classical studies: Honoré de Balzac, Ludovico Ariosto, Dante, Ignacio de Loyola, Cervantes, Shakespeare, Cyrano de Bergerac, and Giacomo Leopardi. Between 1972–1973 Calvino published two short stories, "The Name, the Nose" and the Oulipo-inspired "The Burning of the Abominable House" in the Italian edition of "Playboy". He became a regular contributor to the Italian newspaper "Corriere della Sera", spending his summer vacations in a house constructed in Roccamare near Castiglione della Pescaia, Tuscany.

In 1975 Calvino was made Honorary Member of the American Academy. Awarded the Austrian State Prize for European Literature in 1976, he visited Mexico, Japan, and the United States where he gave a series of lectures in several American towns. After his mother died in 1978 at the age of 92, Calvino sold Villa Meridiana, the family home in San Remo. Two years later, he moved to Rome in Piazza Campo Marzio near the Pantheon and began editing the work of Tommaso Landolfi for Rizzoli. Awarded the French Légion d'honneur in 1981, he also accepted to be jury president of the 29th Venice Film Festival.

During the summer of 1985, Calvino prepared a series of texts on literature for the Charles Eliot Norton Lectures to be delivered at Harvard University in the fall. On 6 September, he was admitted to the ancient hospital of Santa Maria della Scala in Siena where he died during the night between 18 and 19 September of a cerebral hemorrhage. His lecture notes were published posthumously in Italian in 1988 and in English as "Six Memos for the Next Millennium" in 1993.


A selected bibliography of Calvino's writings follows, listing the works that have been translated into and published in English, along with a few major untranslated works. More exhaustive bibliographies can be found in Martin McLaughlin's "Italo Calvino", and Beno Weiss's "Understanding Italo Calvino".




The "Scuola Italiana Italo Calvino", an Italian curriculum school in Moscow, Russia, is named after him. A crater on the planet Mercury, Calvino, and a main belt asteroid, "22370 Italocalvino", are also named after him. "Salt Hill Journal" and University of Louisville award annually the Italo Calvino Prize "for a work of fiction written in the fabulist experimental style of Italo Calvino".




General




</doc>
<doc id="14939" url="https://en.wikipedia.org/wiki?curid=14939" title="Intercontinental ballistic missile">
Intercontinental ballistic missile

An intercontinental ballistic missile (ICBM) is a guided ballistic missile with a minimum range of primarily designed for nuclear weapons delivery (delivering one or more thermonuclear warheads). Similarly, conventional, chemical, and biological weapons can also be delivered with varying effectiveness, but have never been deployed on ICBMs. Most modern designs support multiple independently targetable reentry vehicles (MIRVs), allowing a single missile to carry several warheads, each of which can strike a different target. Russia, United States, China, France, India, United Kingdom, and North Korea are the only countries that have operational ICBMs.

Early ICBMs had limited precision, which made them suitable for use only against the largest targets, such as cities. They were seen as a "safe" basing option, one that would keep the deterrent force close to home where it would be difficult to attack. Attacks against military targets (especially hardened ones) still demanded the use of a more precise, manned bomber. Second- and third-generation designs (such as the LGM-118 Peacekeeper) dramatically improved accuracy to the point where even the smallest point targets can be successfully attacked.

ICBMs are differentiated by having greater range and speed than other ballistic missiles: intermediate-range ballistic missiles (IRBMs), medium-range ballistic missiles (MRBMs), short-range ballistic missiles (SRBMs) and tactical ballistic missiles 
(TBMs). Short and medium-range ballistic missiles are known collectively as theatre ballistic missiles.

The first practical design for an ICBM grew out of Nazi Germany's V-2 rocket program. The liquid-fueled V-2, designed by Wernher von Braun and his team, was widely used at the end of World War II to bomb British and Belgian cities.

Under "Projekt Amerika," von Braun's team developed the A9/10 ICBM, intended for use in bombing New York and other American cities. Initially intended to be guided by radio, it was changed to be a piloted craft after the failure of Operation Elster. The second stage of the A9/A10 rocket was tested a few times in January and February 1945.

After the war, the U.S. executed Operation Paperclip, which brought von Braun and hundreds of other leading German scientists to the United States to develop IRBMs, ICBMs, and launchers for the U.S. Army.

This technology was predicted by U.S. Army General Hap Arnold, who wrote in 1943:

After World War II, the U.S. and USSR started rocket research programs based on the V-2 and other German wartime designs. Each branch of the U.S. military started its own programs, leading to considerable duplication of effort. In the USSR, rocket research was centrally organized, although several teams worked on different designs. 

In the USSR, early development was focused on missiles able to attack European targets. This changed in 1953 when Sergei Korolyov was directed to start development of a true ICBM able to deliver newly developed hydrogen bombs. Given steady funding throughout, the R-7 developed with some speed. The first launch took place on 15 May 1957 and led to an unintended crash from the site. The first successful test followed on 21 August 1957; the R-7 flew over and became the world's first ICBM. The first strategic-missile unit became operational on 9 February 1959 at Plesetsk in north-west Russia.

It was the same R-7 launch vehicle that placed the first artificial satellite in space, Sputnik, on 4 October 1957. The first human spaceflight in history was accomplished on a derivative of R-7, Vostok, on 12 April 1961, by Soviet cosmonaut Yuri Gagarin. A heavily modernized version of the R-7 is still used as the launch vehicle for the Soviet/Russian Soyuz spacecraft, marking more than 60 years of operational history of Sergei Korolyov's original rocket design.
The US initiated ICBM research in 1946 with the RTV-A-2 Hiroc project. This was a three-stage effort with the ICBM development not starting until the third stage. However, funding was cut after only three partially successful launches in 1948 of the second stage design, used to test variations on the V-2 design. With overwhelming air superiority and truly intercontinental bombers, the newly forming US Air Force did not take the problem of ICBM development seriously. Things changed in 1953 with the Soviet testing of their first thermonuclear weapon, but it was not until 1954 that the Atlas missile program was given the highest national priority. The Atlas A first flew on 11 June 1957; the flight lasted only about 24 seconds before the rocket blew up. The first successful flight of an Atlas missile to full range occurred 28 November 1958. The first armed version of the Atlas, the Atlas D, was declared operational in January 1959 at Vandenberg, although it had not yet flown. The first test flight was carried out on 9 July 1959, and the missile was accepted for service on 1 September.

The R-7 and Atlas each required a large launch facility, making them vulnerable to attack, and could not be kept in a ready state. Failure rates were very high throughout the early years of ICBM technology. Human spaceflight programs (Vostok, Mercury, Voskhod, Gemini, etc.) served as a highly visible means of demonstrating confidence in reliability, with successes translating directly to national defense implications. The US was well behind the Soviet Union in the Space Race, so US President John F. Kennedy increased the stakes with the Apollo program, which used Saturn rocket technology that had been funded by President Dwight D. Eisenhower.
These early ICBMs also formed the basis of many space launch systems. Examples include R-7, Atlas, Redstone, Titan, and Proton, which was derived from the earlier ICBMs but never deployed as an ICBM. The Eisenhower administration supported the development of solid-fueled missiles such as the LGM-30 Minuteman, Polaris and Skybolt. Modern ICBMs tend to be smaller than their ancestors, due to increased accuracy and smaller and lighter warheads, and use solid fuels, making them less useful as orbital launch vehicles.

The Western view of the deployment of these systems was governed by the strategic theory of mutual assured destruction. In the 1950s and 1960s, development began on anti-ballistic missile systems by both the US and USSR; these systems were restricted by the 1972 ABM treaty. The first successful ABM test were conducted by the USSR in 1961, which later deployed a fully operational system defending Moscow in the 1970s (see Moscow ABM system).

The 1972 SALT treaty froze the number of ICBM launchers of both the US and the USSR at existing levels and allowed new submarine-based SLBM launchers only if an equal number of land-based ICBM launchers were dismantled. Subsequent talks, called SALT II, were held from 1972 to 1979 and actually reduced the number of nuclear warheads held by the US and USSR. SALT II was never ratified by the United States Senate, but its terms were nevertheless honored by both sides until 1986, when the Reagan administration "withdrew" after accusing the USSR of violating the pact.

In the 1980s, President Ronald Reagan launched the Strategic Defense Initiative as well as the MX and Midgetman ICBM programs.

China developed a minimal independent nuclear deterrent entering its own cold war after an ideological split with the Soviet Union beginning in the early 1960s. After first testing a domestic built nuclear weapon in 1964, it went on to develop various warheads and missiles. Beginning in the early 1970s, the liquid fuelled DF-5 ICBM was developed and used as a satellite launch vehicle in 1975. The DF-5, with a range of --long enough to strike the western US and the USSR--was silo deployed, with the first pair in service by 1981 and possibly twenty missiles in service by the late 1990s. China also deployed the JL-1 Medium-range ballistic missile with a reach of aboard the ultimately unsuccessful type 92 submarine.

In 1991, the United States and the Soviet Union agreed in the START I treaty to reduce their deployed ICBMs and attributed warheads.

, all five of the nations with permanent seats on the United Nations Security Council have operational long-range ballistic missile systems; Russia, the United States, and China also have land-based ICBMs (the US missiles are silo-based, while China and Russia have both silo and road-mobile (DF-31, RT-2PM2 Topol-M missiles).

Israel is believed to have deployed a road mobile nuclear ICBM, the Jericho III, which entered service in 2008; an upgraded version is in development.

India successfully test fired Agni V, with a strike range of more than on 19 April 2012, claiming entry into the ICBM club. The missile's actual range is speculated by foreign researchers to be up to with India having downplayed its capabilities to avoid causing concern to other countries.

By 2012 there was speculation by some intelligence agencies that North Korea is developing an ICBM. North Korea successfully put a satellite into space on 12 December 2012 using the Unha-3 rocket. The United States claimed that the launch was in fact a way to test an ICBM. (See Timeline of first orbital launches by country.) In early July 2017, North Korea claimed for the first time to have tested successfully an ICBM capable of carrying a large thermonuclear warhead.

In July 2014, China announced the development of its newest generation of ICBM, the Dongfeng-41 (DF-41), which has a range of 12,000 kilometres (7,500 miles), capable of reaching the United States, and which analysts believe is capable of being outfitted with MIRV technology.

Most countries in the early stages of developing ICBMs have used liquid propellants, with the known exceptions being the Indian Agni-V, the planned but cancelled South African RSA-4 ICBM, and the now in service Israeli Jericho III.

The RS-28 Sarmat (Russian: РС-28 Сармат; NATO reporting name: SATAN 2), is a Russian liquid-fueled, MIRV-equipped, super-heavy thermonuclear armed intercontinental ballistic missile in development by the Makeyev Rocket Design Bureau from 2009, intended to replace the previous R-36 missile. Its large payload would allow for up to 10 heavy warheads or 15 lighter ones or up to 24 hypersonic glide vehicles Yu-74, or a combination of warheads and massive amounts of countermeasures designed to defeat anti-missile systems; it was heralded by the Russian military as a response to the US Prompt Global Strike.

The following flight phases can be distinguished:

ICBMs usually use the trajectory which optimizes range for a given amount of payload (the "minimum-energy trajectory"); an alternative is a depressed trajectory, which allows less payload, shorter flight time, and has a much lower apogee.

Modern ICBMs typically carry multiple independently targetable reentry vehicles ("MIRVs"), each of which carries a separate nuclear warhead, allowing a single missile to hit multiple targets. MIRV was an outgrowth of the rapidly shrinking size and weight of modern warheads and the Strategic Arms Limitation Treaties (SALT I and SALT II), which imposed limitations on the number of launch vehicles. It has also proved to be an "easy answer" to proposed deployments of anti-ballistic missile (ABM) systems: It is far less expensive to add more warheads to an existing missile system than to build an ABM system capable of shooting down the additional warheads; hence, most ABM system proposals have been judged to be impractical. The first operational ABM systems were deployed in the United States during the 1970s. The Safeguard ABM facility, located in North Dakota, was operational from 1975 to 1976. The USSR deployed its ABM-1 Galosh system around Moscow in the 1970s, which remains in service. Israel deployed a national ABM system based on the Arrow missile in 1998, but it is mainly designed to intercept shorter-ranged theater ballistic missiles, not ICBMs. The Alaska-based United States national missile defense system attained initial operational capability in 2004.
ICBMs can be deployed from multiple platforms:
The last three kinds are mobile and therefore hard to find.
During storage, one of the most important features of the missile is its serviceability. One of the key features of the first computer-controlled ICBM, the Minuteman missile, was that it could quickly and easily use its computer to test itself.
After launch, a booster pushes the missile and then falls away. Most modern boosters are solid-fueled rocket motors, which can be stored easily for long periods of time. Early missiles used liquid-fueled rocket motors. Many liquid-fueled ICBMs could not be kept fueled all the time as the cryogenic fuel liquid oxygen boiled off and caused ice formation, and therefore fueling the rocket was necessary before launch. This procedure was a source of significant operational delay, and might allow the missiles to be destroyed by enemy counterparts before they could be used. To resolve this problem the United Kingdom invented the missile silo that protected the missile from a first strike and also hid fuelling operations underground.

Once the booster falls away, the remaining "bus" releases several warheads, each of which continues on its own unpowered ballistic trajectory, much like an artillery shell or cannonball. The warhead is encased in a cone-shaped reentry vehicle and is difficult to detect in this phase of flight as there is no rocket exhaust or other emissions to mark its position to defenders. The high speeds of the warheads make them difficult to intercept and allow for little warning, striking targets many thousands of kilometers away from the launch site (and due to the possible locations of the submarines: anywhere in the world) within approximately 30 minutes.

Many authorities say that missiles also release aluminized balloons, electronic noise-makers, and other items intended to confuse interception devices and radars.

As the nuclear warhead reenters the Earth's atmosphere its high speed causes compression of the air, leading to a dramatic rise in temperature which would destroy it if it were not shielded in some way. As a result, warhead components are contained within an aluminium honeycomb substructure, sheathed in a pyrolytic carbon-epoxy synthetic resin composite material heat shield. Warheads are also often radiation-hardened (to protect against nuclear-tipped ABMs or the nearby detonation of friendly warheads), one neutron-resistant material developed for this purpose in the UK is three-dimensional quartz phenolic.

Circular error probable is crucial, because halving the circular error probable decreases the needed warhead energy by a factor of four. Accuracy is limited by the accuracy of the navigation system and the available geodetic information.

Strategic missile systems are thought to use custom integrated circuits designed to calculate navigational differential equations thousands to millions of FLOPS in order to reduce navigational errors caused by calculation alone. These circuits are usually a network of binary addition circuits that continually recalculate the missile's position. The inputs to the navigation circuit are set by a general purpose computer according to a navigational input schedule loaded into the missile before launch.

One particular weapon developed by the Soviet Unionthe Fractional Orbital Bombardment Systemhad a partial orbital trajectory, and unlike most ICBMs its target could not be deduced from its orbital flight path. It was decommissioned in compliance with arms control agreements, which address the maximum range of ICBMs and prohibit orbital or fractional-orbital weapons. However, according to reports, Russia is working on the new Sarmat ICBM which leverages Fractional Orbital Bombardment concepts to use a Southern polar approach instead of flying over the Northern polar regions. Using this approach, it is theorized, avoids the US missile defense batteries in California and Alaska.

New development of ICBM technology are ICBMs able to carry hypersonic glide vehicles as a payload such as RS-28 Sarmat.

Specific types of ICBMs (current, past and under development) include:

Russia, the United States, China, North Korea and India are the only countries currently known to possess land-based ICBMs, Israel has also tested ICBMs but is not open about actual deployment.
The United States currently operates 405 ICBMs in three USAF bases. The only model deployed is LGM-30G Minuteman-III. All previous USAF Minuteman II missiles were destroyed in accordance with START II, and their launch silos have been sealed or sold to the public. The powerful MIRV-capable Peacekeeper missiles were phased out in 2005.
The Russian Strategic Rocket Forces have 286 ICBMs able to deliver 958 nuclear warheads: 46 silo-based R-36M2 (SS-18), 30 silo-based UR-100N (SS-19), 36 mobile RT-2PM "Topol" (SS-25), 60 silo-based RT-2UTTH "Topol M" (SS-27), 18 mobile RT-2UTTH "Topol M" (SS-27), 84 mobile RS-24 "Yars" (SS-29), and 12 silo-based RS-24 "Yars" (SS-29).

China has developed several long range ICBMs, like the DF-31. The Dongfeng 5 or DF-5 is a 3-stage liquid fuel ICBM and has an estimated range of 13,000 kilometers. The DF-5 had its first flight in 1971 and was in operational service 10 years later. One of the downsides of the missile was that it took between 30 and 60 minutes to fuel. The Dong Feng 31 (a.k.a. CSS-10) is a medium-range, three-stage, solid-propellant intercontinental ballistic missile, and is a land-based variant of the submarine-launched JL-2.

The DF-41 or CSS-X-10 can carry up to 10 nuclear warheads, which are MIRVs and has a range of approximately . The DF-41 deployed in underground Xinjiang, Qinghai, Gansu and Inner Mongolia area. The mysterious underground subway ICBM carrier systems they called "Underground Great Wall Project".

Israel is believed to have deployed a road mobile nuclear ICBM, the Jericho III, which entered service in 2008. It is possible for the missile to be equipped with a single nuclear warhead or up to three MIRV warheads. It is believed to be based on the Shavit space launch vehicle and is estimated to have a range of . In November 2011 Israel tested an ICBM believed to be an upgraded version of the Jericho III.

India has a series of ballistic missiles called Agni. On 19 April 2012, India successfully test fired its first Agni-V, a three-stage solid fueled missile, with a strike range of more than . The missile was test-fired for the second time on 15 September 2013. On 31 January 2015, India conducted a third successful test flight of the Agni-V from the Abdul Kalam Island facility. The test used a canisterised version of the missile, mounted over a Tatra truck.

An anti-ballistic missile is a missile which can be deployed to counter an incoming nuclear or non-nuclear ICBM. ICBMs can be intercepted in three regions of their trajectory: boost phase, mid-course phase or terminal phase. Currently China, the US, Russia, France, India and Israel have developed anti-ballistic missile systems, of which the Russian A-135 anti-ballistic missile system and the US Ground-Based Midcourse Defense systems have the capability to intercept ICBMs carrying nuclear, chemical, biological, or conventional warheads.





</doc>
<doc id="14943" url="https://en.wikipedia.org/wiki?curid=14943" title="Irish traditional music session">
Irish traditional music session

Irish traditional music sessions are mostly informal gatherings at which people play Irish traditional music. The Irish language word for "session" is "seisiún". This article discusses tune-playing, although "session" can also refer to a singing session or a mixed session (tunes and songs).

Barry Foy's "Field Guide to the Irish Music Session" defines a session as:

"...a gathering of Irish traditional musicians for the purpose of celebrating their common interest in the music by playing it together in a relaxed, informal setting, while in the process generally beefing up the mystical cultural mantra that hums along uninterruptedly beneath all manifestations of Irishness worldwide."

The general session scheme is that someone starts a tune, and those who know it join in. Good session etiquette requires not playing if one does not know the tune (or at least quietly playing an accompaniment part) and waiting until a tune one knows comes along. In an "open" session, anyone who is able to play Irish music is welcome. Most often there are more-or-less recognized session leaders; sometimes there are no leaders. At times a song will be sung or a slow air played by a single musician between sets.

Sessions are usually held in public houses or taverns. A pub owner might have one or two musicians paid to come regularly in order for the session to have a base. These musicians can perform during any gaps during the day or evening when no other performers are there and wish to play. Sunday afternoons and weekday nights (especially Tuesday and Wednesday) are common times for sessions to be scheduled, on the theory that these are the least likely times for dances and concerts to be held, and therefore the times that professional musicians will be most able to show up.

Sessions can be held in homes or at various public places in addition to pubs; often at a festival sessions will be got together in the beer tent or in the vendor's booth of a music-loving craftsperson or dealer. When a particularly large musical event "takes over" an entire village, spontaneous sessions may erupt on the street corners. Sessions may also take place occasionally at wakes. House sessions are not as common now as they were in the past. This can be seen in the book "Peig" by Peig Sayers. In the early stages of the book when Peig was young they often went to sessions at peoples houses in a practice called 'bothántiocht'.



</doc>
<doc id="14946" url="https://en.wikipedia.org/wiki?curid=14946" title="Ice">
Ice

Ice is water frozen into a solid state. Depending on the presence of impurities such as particles of soil or bubbles of air, it can appear transparent or a more or less opaque bluish-white color.

In the Solar System, ice is abundant and occurs naturally from as close to the Sun as Mercury to as far away as the Oort cloud objects. Beyond the Solar System, it occurs as interstellar ice. It is abundant on Earth's surfaceparticularly in the polar regions and above the snow lineand, as a common form of precipitation and deposition, plays a key role in Earth's water cycle and climate. It falls as snowflakes and hail or occurs as frost, icicles or ice spikes and aggregates from snow as glaciers, ice sheets.

Ice molecules can exhibit eighteen or more different phases (packing geometries) that depend on temperature and pressure. When water is cooled rapidly (quenching), up to three different types of amorphous ice can form depending on the history of its pressure and temperature. When cooled slowly correlated proton tunneling occurs below (, ) giving rise to macroscopic quantum phenomena. Virtually all the ice on Earth's surface and in its atmosphere is of a hexagonal crystalline structure denoted as ice I (spoken as "ice one h") with minute traces of cubic ice denoted as ice I. The most common phase transition to ice I occurs when liquid water is cooled below (, ) at standard atmospheric pressure. It may also be deposited directly by water vapor, as happens in the formation of frost. The transition from ice to water is melting and from ice directly to water vapor is sublimation.

Ice is used in a variety of ways, including cooling, winter sports and ice sculpture.

As a naturally occurring crystalline inorganic solid with an ordered structure, ice is considered to be a mineral. It possesses a regular crystalline structure based on the molecule of water, which consists of a single oxygen atom covalently bonded to two hydrogen atoms, or H–O–H. However, many of the physical properties of water and ice are controlled by the formation of hydrogen bonds between adjacent oxygen and hydrogen atoms; while it is a weak bond, it is nonetheless critical in controlling the structure of both water and ice.

An unusual property of water is that its solid form—ice frozen at atmospheric pressure—is approximately 8.3% less dense than its liquid form; this is equivalent to a volumetric expansion of 9%. The density of ice is 0.9167–0.9168 g/cm at 0 °C and standard atmospheric pressure (101,325 Pa), whereas water has a density of 0.9998–0.999863 g/cm at the same temperature and pressure. Liquid water is densest, essentially 1.00 g/cm, at 4 °C and begins to lose its density as the water molecules begin to form the hexagonal crystals of ice as the freezing point is reached. This is due to hydrogen bonding dominating the intermolecular forces, which results in a packing of molecules less compact in the solid. Density of ice increases slightly with decreasing temperature and has a value of 0.9340 g/cm at −180 °C (93 K).

When water freezes, it increases in volume (about 9% for fresh water). The effect of expansion during freezing can be dramatic, and ice expansion is a basic cause of freeze-thaw weathering of rock in nature and damage to building foundations and roadways from frost heaving. It is also a common cause of the flooding of houses when water pipes burst due to the pressure of expanding water when it freezes.

The result of this process is that ice (in its most common form) floats on liquid water, which is an important feature in Earth's biosphere. It has been argued that without this property, natural bodies of water would freeze, in some cases permanently, from the bottom up, resulting in a loss of bottom-dependent animal and plant life in fresh and sea water. Sufficiently thin ice sheets allow light to pass through while protecting the underside from short-term weather extremes such as wind chill. This creates a sheltered environment for bacterial and algal colonies. When sea water freezes, the ice is riddled with brine-filled channels which sustain sympagic organisms such as bacteria, algae, copepods and annelids, which in turn provide food for animals such as krill and specialised fish like the bald notothen, fed upon in turn by larger animals such as emperor penguins and minke whales.

When ice melts, it absorbs as much energy as it would take to heat an equivalent mass of water by 80 °C. During the melting process, the temperature remains constant at 0 °C. While melting, any energy added breaks the hydrogen bonds between ice (water) molecules. Energy becomes available to increase the thermal energy (temperature) only after enough hydrogen bonds are broken that the ice can be considered liquid water. The amount of energy consumed in breaking hydrogen bonds in the transition from ice to water is known as the "heat of fusion".

As with water, ice absorbs light at the red end of the spectrum preferentially as the result of an overtone of an oxygen–hydrogen (O–H) bond stretch. Compared with water, this absorption is shifted toward slightly lower energies. Thus, ice appears blue, with a slightly greener tint than liquid water. Since absorption is cumulative, the color effect intensifies with increasing thickness or if internal reflections cause the light to take a longer path through the ice.

Other colors can appear in the presence of light absorbing impurities, where the impurity is dictating the color rather than the ice itself. For instance, icebergs containing impurities (e.g., sediments, algae, air bubbles) can appear brown, grey or green.

Ice may be any one of the 18 known solid crystalline phases of water, or in an amorphous solid state at various densities.

Most liquids under increased pressure freeze at "higher" temperatures because the pressure helps to hold the molecules together. However, the strong hydrogen bonds in water make it different: for some pressures higher than , water freezes at a temperature "below" 0 °C, as shown in the phase diagram below. The melting of ice under high pressures is thought to contribute to the movement of glaciers.

Ice, water, and water vapour can coexist at the triple point, which is exactly 273.16 K (0.01 °C) at a pressure of 611.657 Pa. The kelvin was in fact defined as of the difference between this triple point and absolute zero, though this definition changed in May 2019. Unlike most other solids, ice is difficult to superheat. In an experiment, ice at −3 °C was superheated to about 17 °C for about 250 picoseconds.

Subjected to higher pressures and varying temperatures, ice can form in 18 separate known crystalline phases. With care, at least 15 of these phases (one of the known exceptions being ice X) can be recovered at ambient pressure and low temperature in metastable form. The types are differentiated by their crystalline structure, proton ordering, and density. There are also two metastable phases of ice under pressure, both fully hydrogen-disordered; these are IV and XII. Ice XII was discovered in 1996. In 2006, XIII and XIV were discovered. Ices XI, XIII, and XIV are hydrogen-ordered forms of ices I, V, and XII respectively. In 2009, ice XV was found at extremely high pressures and −143 °C. At even higher pressures, ice is predicted to become a metal; this has been variously estimated to occur at 1.55 TPa or 5.62 TPa.

As well as crystalline forms, solid water can exist in amorphous states as amorphous ice (ASW) of varying densities. Water in the interstellar medium is dominated by amorphous ice, making it likely the most common form of water in the universe. Low-density ASW (LDA), also known as hyperquenched glassy water, may be responsible for noctilucent clouds on Earth and is usually formed by deposition of water vapor in cold or vacuum conditions. High-density ASW (HDA) is formed by compression of ordinary ice I or LDA at GPa pressures. Very-high-density ASW (VHDA) is HDA slightly warmed to 160K under 1–2 GPa pressures.

In outer space, hexagonal crystalline ice (the predominant form found on Earth) is extremely rare. Amorphous ice is more common; however, hexagonal crystalline ice can be formed by volcanic action.
Ice from a theorized superionic water may possess two crystalline structures. At pressures in excess of such "superionic ice" would take on a body-centered cubic structure. However, at pressures in excess of the structure may shift to a more stable face-centered cubic lattice.

The low coefficient of friction ("slipperiness") of ice has been attributed to the pressure of an object coming into contact with the ice, melting a thin layer of the ice and allowing the object to glide across the surface. For example, the blade of an ice skate, upon exerting pressure on the ice, would melt a thin layer, providing lubrication between the ice and the blade. This explanation, called "pressure melting", originated in the 19th century. It, however, did not account for skating on ice temperatures lower than , which is often skated upon.

A second theory describing the coefficient of friction of ice suggested that ice molecules at the interface cannot properly bond with the molecules of the mass of ice beneath (and thus are free to move like molecules of liquid water). These molecules remain in a semi-liquid state, providing lubrication regardless of pressure against the ice exerted by any object. However, the significance of this hypothesis is disputed by experiments showing a high coefficient of friction for ice using atomic force microscopy.

A third theory is "friction heating", which suggests that friction of the material is the cause of the ice layer melting. However, this theory does not sufficiently explain why ice is slippery when standing still even at below-zero temperatures.

A comprehensive theory of ice friction takes into account all the above-mentioned friction mechanisms. This model allows quantitative estimation of the friction coefficient of ice against various materials as a function of temperature and sliding speed. In typical conditions related to winter sports and tires of a vehicle on ice, melting of a thin ice layer due to the frictional heating is the primary reason for the slipperiness. The mechanism controlling the frictional properties of ice is still an active area of scientific study.

The term that collectively describes all of the parts of the Earth's surface where water is in frozen form is the "cryosphere." Ice is an important component of the global climate, particularly in regard to the water cycle. Glaciers and snowpacks are an important storage mechanism for fresh water; over time, they may sublimate or melt. Snowmelt is an important source of seasonal fresh water. The World Meteorological Organization defines several kinds of ice depending on origin, size, shape, influence and so on. Clathrate hydrates are forms of ice that contain gas molecules trapped within its crystal lattice.

Ice that is found at sea may be in the form of drift ice floating in the water, fast ice fixed to a shoreline or anchor ice if attached to the sea bottom. Ice which calves (breaks off) from an ice shelf or glacier may become an iceberg. Sea ice can be forced together by currents and winds to form pressure ridges up to tall. Navigation through areas of sea ice occurs in openings called "polynyas" or "leads" or requires the use of a special ship called an "icebreaker".

Ice on land ranges from the largest type called an "ice sheet" to smaller ice caps and ice fields to glaciers and ice streams to the snow line and snow fields.

Aufeis is layered ice that forms in Arctic and subarctic stream valleys. Ice, frozen in the stream bed, blocks normal groundwater discharge, and causes the local water table to rise, resulting in water discharge on top of the frozen layer. This water then freezes, causing the water table to rise further and repeat the cycle. The result is a stratified ice deposit, often several meters thick.

Freezing rain is a type of winter storm called an ice storm where rain falls and then freezes producing a glaze of ice. Ice can also form icicles, similar to stalactites in appearance, or stalagmite-like forms as water drips and re-freezes.

The term "ice dam" has three meanings (others discussed below). On structures, an ice dam is the buildup of ice on a sloped roof which stops melt water from draining properly and can cause damage from water leaks in buildings.

Ice which forms on moving water tends to be less uniform and stable than ice which forms on calm water. Ice jams (sometimes called "ice dams"), when broken chunks of ice pile up, are the greatest ice hazard on rivers. Ice jams can cause flooding, damage structures in or near the river, and damage vessels on the river. Ice jams can cause some hydropower industrial facilities to completely shut down. An ice dam is a blockage from the movement of a glacier which may produce a proglacial lake. Heavy ice flows in rivers can also damage vessels and require the use of an icebreaker to keep navigation possible.

Ice discs are circular formations of ice surrounded by water in a river.

Pancake ice is a formation of ice generally created in areas with less calm conditions.

Ice forms on calm water from the shores, a thin layer spreading across the surface, and then downward. Ice on lakes is generally four types: primary, secondary, superimposed and agglomerate. Primary ice forms first. Secondary ice forms below the primary ice in a direction parallel to the direction of the heat flow. Superimposed ice forms on top of the ice surface from rain or water which seeps up through cracks in the ice which often settles when loaded with snow.

Shelf ice occurs when floating pieces of ice are driven by the wind piling up on the windward shore.

Candle ice is a form of rotten ice that develops in columns perpendicular to the surface of a lake.

Rime is a type of ice formed on cold objects when drops of water crystallize on them. This can be observed in foggy weather, when the temperature drops during the night. Soft rime contains a high proportion of trapped air, making it appear white rather than transparent, and giving it a density about one quarter of that of pure ice. Hard rime is comparatively dense.

Ice pellets are a form of precipitation consisting of small, translucent balls of ice. This form of precipitation is also referred to as "sleet" by the United States National Weather Service. (In British English "sleet" refers to a mixture of rain and snow.) Ice pellets are usually smaller than hailstones. They often bounce when they hit the ground, and generally do not freeze into a solid mass unless mixed with freezing rain. The METAR code for ice pellets is "PL".

Ice pellets form when a layer of above-freezing air is located between above the ground, with sub-freezing air both above and below it. This causes the partial or complete melting of any snowflakes falling through the warm layer. As they fall back into the sub-freezing layer closer to the surface, they re-freeze into ice pellets. However, if the sub-freezing layer beneath the warm layer is too small, the precipitation will not have time to re-freeze, and freezing rain will be the result at the surface. A temperature profile showing a warm layer above the ground is most likely to be found in advance of a warm front during the cold season, but can occasionally be found behind a passing cold front.

Like other precipitation, hail forms in storm clouds when supercooled water droplets freeze on contact with condensation nuclei, such as dust or dirt. The storm's updraft blows the hailstones to the upper part of the cloud. The updraft dissipates and the hailstones fall down, back into the updraft, and are lifted up again. Hail has a diameter of or more. Within METAR code, GR is used to indicate larger hail, of a diameter of at least and GS for smaller. Stones just larger than golf ball-sized are one of the most frequently reported hail sizes. Hailstones can grow to and weigh more than . In large hailstones, latent heat released by further freezing may melt the outer shell of the hailstone. The hailstone then may undergo 'wet growth', where the liquid outer shell collects other smaller hailstones. The hailstone gains an ice layer and grows increasingly larger with each ascent. Once a hailstone becomes too heavy to be supported by the storm's updraft, it falls from the cloud.

Hail forms in strong thunderstorm clouds, particularly those with intense updrafts, high liquid water content, great vertical extent, large water droplets, and where a good portion of the cloud layer is below freezing . Hail-producing clouds are often identifiable by their green coloration. The growth rate is maximized at about , and becomes vanishingly small much below as supercooled water droplets become rare. For this reason, hail is most common within continental interiors of the mid-latitudes, as hail formation is considerably more likely when the freezing level is below the altitude of . Entrainment of dry air into strong thunderstorms over continents can increase the frequency of hail by promoting evaporational cooling which lowers the freezing level of thunderstorm clouds giving hail a larger volume to grow in. Accordingly, hail is actually less common in the tropics despite a much higher frequency of thunderstorms than in the mid-latitudes because the atmosphere over the tropics tends to be warmer over a much greater depth. Hail in the tropics occurs mainly at higher elevations.

Snow crystals form when tiny supercooled cloud droplets (about 10 μm in diameter) freeze. These droplets are able to remain liquid at temperatures lower than , because to freeze, a few molecules in the droplet need to get together by chance to form an arrangement similar to that in an ice lattice; then the droplet freezes around this "nucleus." Experiments show that this "homogeneous" nucleation of cloud droplets only occurs at temperatures lower than . In warmer clouds an aerosol particle or "ice nucleus" must be present in (or in contact with) the droplet to act as a nucleus. Our understanding of what particles make efficient ice nuclei is poor – what we do know is they are very rare compared to that cloud condensation nuclei on which liquid droplets form. Clays, desert dust and biological particles may be effective, although to what extent is unclear. Artificial nuclei are used in cloud seeding. The droplet then grows by condensation of water vapor onto the ice surfaces.

So-called "diamond dust", also known as ice needles or ice crystals, forms at temperatures approaching due to air with slightly higher moisture from aloft mixing with colder, surface-based air. The METAR identifier for diamond dust within international hourly weather reports is "IC".

Ablation of ice refers to both its melting and its dissolution.

In fresh ambient melting describes a phase transition from solid to liquid.

To melt ice means breaking the hydrogen bonds between the water molecules. The ordering of the molecules in the solid breaks down to a less ordered state and the solid melts to become a liquid. This is achieved by increasing the internal energy of the ice beyond the melting point. When ice melts it absorbs as much energy as would be required to heat an equivalent amount of water by 80 °C. While melting, the temperature of the ice surface remains constant at 0 °C. The velocity of the melting process depends on the efficiency of the energy exchange process. An ice surface in fresh water melts solely by free convection with a velocity that depends linearly on the water temperature, "T", when "T" is less than 3.98 °C, and superlinearly when "T" is equal to or greater than 3.98 °C, with the rate being proportional to (T − 3.98 °C), with "α" =  for "T" much greater than 8 °C, and α =  for in between temperatures "T".

In salty ambient conditions, dissolution rather than melting often causes the ablation of ice. For example, the temperature of the Arctic Ocean is generally below the melting point of ablating sea ice. The phase transition from solid to liquid is achieved by mixing salt and water molecules, similar to the dissolution of sugar in water, even though the water temperature is far below the melting point of the sugar. Hence dissolution is rate limited by salt transport whereas melting can occur at much higher rates that are characteristic for heat transport.

Humans have used ice for cooling and food preservation for centuries, relying on harvesting natural ice in various forms and then transitioning to the mechanical production of the material. Ice also presents a challenge to transportation in various forms and a setting for winter sports.

Ice has long been valued as a means of cooling. In 400 BC Iran, Persian engineers had already mastered the technique of storing ice in the middle of summer in the desert. The ice was brought in during the winters from nearby mountains in bulk amounts, and stored in specially designed, naturally cooled "refrigerators", called yakhchal (meaning "ice storage"). This was a large underground space (up to 5000 m) that had thick walls (at least two meters at the base) made of a special mortar called "sarooj", composed of sand, clay, egg whites, lime, goat hair, and ash in specific proportions, and which was known to be resistant to heat transfer. This mixture was thought to be completely water impenetrable. The space often had access to a qanat, and often contained a system of windcatchers which could easily bring temperatures inside the space down to frigid levels on summer days. The ice was used to chill treats for royalty.

There were thriving industries in 16th–17th century England whereby low-lying areas along the Thames Estuary were flooded during the winter, and ice harvested in carts and stored inter-seasonally in insulated wooden houses as a provision to an icehouse often located in large country houses, and widely used to keep fish fresh when caught in distant waters. This was allegedly copied by an Englishman who had seen the same activity in China. Ice was imported into England from Norway on a considerable scale as early as 1823.

In the United States, the first cargo of ice was sent from New York City to Charleston, South Carolina, in 1799, and by the first half of the 19th century, ice harvesting had become big business. Frederic Tudor, who became known as the "Ice King", worked on developing better insulation products for the long distance shipment of ice, especially to the tropics; this became known as the ice trade.

Trieste sent ice to Egypt, Corfu, and Zante; Switzerland sent it to France; and Germany sometimes was supplied from Bavarian lakes. The Hungarian Parliament building used ice harvested in the winter from Lake Balaton for air conditioning.

Ice houses were used to store ice formed in the winter, to make ice available all year long, and early refrigerators were known as iceboxes, because they had a block of ice in them. In many cities, it was not unusual to have a regular ice delivery service during the summer. The advent of artificial refrigeration technology has since made delivery of ice obsolete.

Ice is still harvested for ice and snow sculpture events. For example, a swing saw is used to get ice for the Harbin International Ice and Snow Sculpture Festival each year from the frozen surface of the Songhua River.

Ice is now produced on an industrial scale, for uses including food storage and processing, chemical manufacturing, concrete mixing and curing, and consumer or packaged ice. Most commercial icemakers produce three basic types of fragmentary ice: flake, tubular and plate, using a variety of techniques. Large batch ice makers can produce up to 75 tons of ice per day. In 2002, there were 426 commercial ice-making companies in the United States, with a combined value of shipments of $595,487,000. Home refrigerators can also make ice with a built in icemaker, which will typically make ice cubes or crushed ice. Stand-alone icemaker units that make ice cubes are often called ice machines.

Ice can present challenges to safe transportation on land, sea and in the air.

Ice forming on roads is a dangerous winter hazard. Black ice is very difficult to see, because it lacks the expected frosty surface. Whenever there is freezing rain or snow which occurs at a temperature near the melting point, it is common for ice to build up on the windows of vehicles. Driving safely requires the removal of the ice build-up. Ice scrapers are tools designed to break the ice free and clear the windows, though removing the ice can be a long and laborious process.

Far enough below the freezing point, a thin layer of ice crystals can form on the inside surface of windows. This usually happens when a vehicle has been left alone after being driven for a while, but can happen while driving, if the outside temperature is low enough. Moisture from the driver's breath is the source of water for the crystals. It is troublesome to remove this form of ice, so people often open their windows slightly when the vehicle is parked in order to let the moisture dissipate, and it is now common for cars to have rear-window defrosters to solve the problem. A similar problem can happen in homes, which is one reason why many colder regions require double-pane windows for insulation.

When the outdoor temperature stays below freezing for extended periods, very thick layers of ice can form on lakes and other bodies of water, although places with flowing water require much colder temperatures. The ice can become thick enough to drive onto with automobiles and trucks. Doing this safely requires a thickness of at least 30 cm (one foot).

For ships, ice presents two distinct hazards. Spray and freezing rain can produce an ice build-up on the superstructure of a vessel sufficient to make it unstable, and to require it to be hacked off or melted with steam hoses. And icebergs – large masses of ice floating in water (typically created when glaciers reach the sea) – can be dangerous if struck by a ship when underway. Icebergs have been responsible for the sinking of many ships, the most famous being the "Titanic". For harbors near the poles, being ice-free is an important advantage. Ideally, all year long. Examples are Murmansk (Russia), Petsamo (Russia, formerly Finland) and Vardø (Norway). Harbors which are not ice-free are opened up using icebreakers.

For aircraft, ice can cause a number of dangers. As an aircraft climbs, it passes through air layers of different temperature and humidity, some of which may be conducive to ice formation. If ice forms on the wings or control surfaces, this may adversely affect the flying qualities of the aircraft. During the first non-stop flight across the Atlantic, the British aviators Captain John Alcock and Lieutenant Arthur Whitten Brown encountered such icing conditions – Brown left the cockpit and climbed onto the wing several times to remove ice which was covering the engine air intakes of the Vickers Vimy aircraft they were flying.

One vulnerability effected by icing that is associated with reciprocating internal combustion engines is the carburetor. As air is sucked through the carburetor into the engine, the local air pressure is lowered, which causes adiabatic cooling. Thus, in humid near-freezing conditions, the carburetor will be colder, and tend to ice up. This will block the supply of air to the engine, and cause it to fail. For this reason, aircraft reciprocating engines with carburetors are provided with carburetor air intake heaters. The increasing use of fuel injection—which does not require carburetors—has made "carb icing" less of an issue for reciprocating engines.

Jet engines do not experience carb icing, but recent evidence indicates that they can be slowed, stopped, or damaged by internal icing in certain types of atmospheric conditions much more easily than previously believed. In most cases, the engines can be quickly restarted and flights are not endangered, but research continues to determine the exact conditions which produce this type of icing, and find the best methods to prevent, or reverse it, in flight.

Ice also plays a central role in winter recreation and in many sports such as ice skating, tour skating, ice hockey, bandy, ice fishing, ice climbing, curling, broomball and sled racing on bobsled, luge and skeleton. Many of the different sports played on ice get international attention every four years during the Winter Olympic Games.

A sort of sailboat on blades gives rise to ice yachting. Another sport is ice racing, where drivers must speed on lake ice, while also controlling the skid of their vehicle (similar in some ways to dirt track racing). The sport has even been modified for ice rinks.



The solid phases of several other volatile substances are also referred to as "ices"; generally a volatile is classed as an ice if its melting point lies above or around 100 K. The best known example is dry ice, the solid form of carbon dioxide.

A "magnetic analogue" of ice is also realized in some insulating magnetic materials in which the magnetic moments mimic the position of protons in water ice and obey energetic constraints similar to the Bernal-Fowler ice rules arising from the geometrical frustration of the proton configuration in water ice. These materials are called spin ice.




</doc>
<doc id="14951" url="https://en.wikipedia.org/wiki?curid=14951" title="Ionic bonding">
Ionic bonding

Ionic bonding is a type of chemical bonding that involves the electrostatic attraction between oppositely charged ions, and is the primary interaction occurring in ionic compounds. It is one of the main types of bonding along with covalent bonding and metallic bonding. Ions are atoms (or groups of atoms) with an electrostatic charge. Atoms that gain electrons make negatively charged ions (called anions). Atoms that lose electrons make positively charged ions (called cations). This transfer of electrons is known as electrovalence in contrast to covalence. In the simplest case, the cation is a metal atom and the anion is a nonmetal atom, but these ions can be of a more complex nature, e.g. molecular ions like or . In simpler words, an ionic bond results from the transfer of electrons from a metal to a non-metal in order to obtain a full valence shell for both atoms.

It is important to recognize that "clean" ionic bonding — in which one atom or molecule completely transfers an electron to another — cannot exist: all ionic compounds have some degree of covalent bonding, or electron sharing. Thus, the term "ionic bonding" is given when the ionic character is greater than the covalent character – that is, a bond in which a large electronegativity difference exists between the two atoms, causing the bonding to be more polar (ionic) than in covalent bonding where electrons are shared more equally. Bonds with partially ionic and partially covalent character are called polar covalent bonds. 

Ionic compounds conduct electricity when molten or in solution, typically not when solid. Ionic compounds generally have a high melting point, depending on the charge of the ions they consist of. The higher the charges the stronger the cohesive forces and the higher the melting point. They also tend to be soluble in water; the stronger the cohesive forces, the lower the solubility.

Atoms that have an almost full or almost empty valence shell tend to be very reactive. Atoms that are strongly electronegative (as is the case with halogens) often have only one or two empty orbitals in their valence shell, and frequently bond with other molecules or gain electrons to form anions. Atoms that are weakly electronegative (such as alkali metals) have relatively few valence electrons, which can easily be shared with atoms that are strongly electronegative. As a result, weakly electronegative atoms tend to distort their electron cloud and form cations.

Ionic bonding can result from a redox reaction when atoms of an element (usually metal), whose ionization energy is low, give some of their electrons to achieve a stable electron configuration. In doing so, cations are formed. An atom of another element (usually nonmetal) with greater electron affinity accepts the electron(s) to attain a stable electron configuration, and after accepting electron(s) an atom becomes an anion. Typically, the stable electron configuration is one of the noble gases for elements in the s-block and the p-block, and particular stable electron configurations for d-block and f-block elements. The electrostatic attraction between the anions and cations leads to the formation of a solid with a crystallographic lattice in which the ions are stacked in an alternating fashion. In such a lattice, it is usually not possible to distinguish discrete molecular units, so that the compounds formed are not molecular in nature. However, the ions themselves can be complex and form molecular ions like the acetate anion or the ammonium cation.

For example, common table salt is sodium chloride. When sodium (Na) and chlorine (Cl) are combined, the sodium atoms each lose an electron, forming cations (Na), and the chlorine atoms each gain an electron to form anions (Cl). These ions are then attracted to each other in a 1:1 ratio to form sodium chloride (NaCl).

However, to maintain charge neutrality, strict ratios between anions and cations are observed so that ionic compounds, in general, obey the rules of stoichiometry despite not being molecular compounds. For compounds that are transitional to the alloys and possess mixed ionic and metallic bonding, this may not be the case anymore. Many sulfides, e.g., do form non-stoichiometric compounds.

Many ionic compounds are referred to as salts as they can also be formed by the neutralization reaction of an Arrhenius base like NaOH with an Arrhenius acid like HCl

The salt NaCl is then said to consist of the acid rest Cl and the base rest Na.

The removal of electrons from the cation is endothermic, raising the system's overall energy. There may also be energy changes associated with breaking of existing bonds or the addition of more than one electron to form anions. However, the action of the anion's accepting the cation's valence electrons and the subsequent attraction of the ions to each other releases (lattice) energy and, thus, lowers the overall energy of the system.

Ionic bonding will occur only if the overall energy change for the reaction is favorable. In general, the reaction is exothermic, but, e.g., the formation of mercuric oxide (HgO) is endothermic. The charge of the resulting ions is a major factor in the strength of ionic bonding, e.g. a salt CA is held together by electrostatic forces roughly four times weaker than CA according to Coulombs law, where C and A represent a generic cation and anion respectively. The sizes of the ions and the particular packing of the lattice are ignored in this rather simplistic argument.

Ionic compounds in the solid state form lattice structures. The two principal factors in determining the form of the lattice are the relative charges of the ions and their relative sizes. Some structures are adopted by a number of compounds; for example, the structure of the rock salt sodium chloride is also adopted by many alkali halides, and binary oxides such as magnesium oxide. Pauling's rules provide guidelines for predicting and rationalizing the crystal structures of ionic crystals

For a solid crystalline ionic compound the enthalpy change in forming the solid from gaseous ions is termed the lattice energy.
The experimental value for the lattice energy can be determined using the Born–Haber cycle. It can also be calculated (predicted) using the Born–Landé equation as the sum of the electrostatic potential energy, calculated by summing interactions between cations and anions, and a short-range repulsive potential energy term. The electrostatic potential can be expressed in terms of the interionic separation and a constant (Madelung constant) that takes account of the geometry of the crystal. The further away from the nucleus the weaker the shield. The Born-Landé equation gives a reasonable fit to the lattice energy of, e.g., sodium chloride, where the calculated (predicted) value is −756 kJ/mol, which compares to −787 kJ/mol using the Born–Haber cycle. In aqueous solution the binding strength can be described by the Bjerrum or Fuoss equation as function of the ion charges, rather independent of the nature of the ions such as polaribility or size The strength of salt bridges is most often evaluated by measurements of equilibria between molecules containing cationic and anionioc sites, most often in solution. Equilibrium constants in water indicate additive free energy contributions for each salt bridge. Another method for the identification of hydrogen bonds also in complicated molecules is crystallography, sometimes also NMR-spectroscopy.

The attractive forces defining the strength of ionic bonding can be modelled by Coulomb's Law. Ionic bond strengths are typically (cited ranges vary) between 170 and 1500 kJ/mol.

Ions in crystal lattices of purely ionic compounds are spherical; however, if the positive ion is small and/or highly charged, it will distort the electron cloud of the negative ion, an effect summarised in Fajans' rules. This polarization of the negative ion leads to a build-up of extra charge density between the two nuclei, that is, to partial covalency. Larger negative ions are more easily polarized, but the effect is usually important only when positive ions with charges of 3+ (e.g., Al) are involved. However, 2+ ions (Be) or even 1+ (Li) show some polarizing power because their sizes are so small (e.g., LiI is ionic but has some covalent bonding present). Note that this is not the ionic polarization effect that refers to displacement of ions in the lattice due to the application of an electric field.

In ionic bonding, the atoms are bound by attraction of oppositely charged ions, whereas, in covalent bonding, atoms are bound by sharing electrons to attain stable electron configurations. In covalent bonding, the molecular geometry around each atom is determined by valence shell electron pair repulsion VSEPR rules, whereas, in ionic materials, the geometry follows maximum packing rules. One could say that covalent bonding is more "directional" in the sense that the energy penalty for not adhering to the optimum bond angles is large, whereas ionic bonding has no such penalty. There are no shared electron pairs to repel each other, the ions should simply be packed as efficiently as possible. This often leads to much higher coordination numbers. In NaCl, each ion has 6 bonds and all bond angles are 90°. In CsCl the coordination number is 8. By comparison carbon typically has a maximum of four bonds.

Purely ionic bonding cannot exist, as the proximity of the entities involved in the bonding allows some degree of sharing electron density between them. Therefore, all ionic bonding has some covalent character. Thus, bonding is considered ionic where the ionic character is greater than the covalent character. The larger the difference in electronegativity between the two types of atoms involved in the bonding, the more ionic (polar) it is. Bonds with partially ionic and partially covalent character are called polar covalent bonds. For example, Na–Cl and Mg–O interactions have a few percent covalency, while Si–O bonds are usually ~50% ionic and ~50% covalent. Pauling estimated that an electronegativity difference of 1.7 (on the Pauling scale) corresponds to 50% ionic character, so that a difference greater than 1.7 corresponds to a bond which is predominantly ionic.
Ionic character in covalent bonds can be directly measured for atoms having quadrupolar nuclei (H, N, Br, Cl or I). These nuclei are generally objects of NQR nuclear quadrupole resonance and NMR nuclear magnetic resonance studies. Interactions between the nuclear quadrupole moments "Q" and the electric field gradients (EFG) are characterized via the nuclear quadrupole coupling constants
where the "eq" term corresponds to the principal component of the EFG tensor and "e" is the elementary charge. In turn, the electric field gradient opens the way to description of bonding modes in molecules when the QCC values are accurately determined by NMR or NQR methods. 

In general, when ionic bonding occurs in the solid (or liquid) state, it is not possible to talk about a single "ionic bond" between two individual atoms, because the cohesive forces that keep the lattice together are of a more collective nature. This is quite different in the case of covalent bonding, where we can often speak of a distinct bond localized between two particular atoms. However, even if ionic bonding is combined with some covalency, the result is "not" necessarily discrete bonds of a localized character. In such cases, the resulting bonding often requires description in terms of a band structure consisting of gigantic molecular orbitals spanning the entire crystal. Thus, the bonding in the solid often retains its collective rather than localized nature. When the difference in electronegativity is decreased, the bonding may then lead to a semiconductor, a semimetal or eventually a metallic conductor with metallic bonding.




</doc>
<doc id="14952" url="https://en.wikipedia.org/wiki?curid=14952" title="IBF (disambiguation)">
IBF (disambiguation)


IBF may also refer to:



</doc>
<doc id="14958" url="https://en.wikipedia.org/wiki?curid=14958" title="Immune system">
Immune system

The immune system is a host defense system comprising many biological structures and processes within an organism that protects against disease. To function properly, an immune system must detect a wide variety of agents, known as pathogens, from viruses to parasitic worms, and distinguish them from the organism's own healthy tissue. In many species, there are two major subsystems of the immune system: the innate immune system and the adaptive immune system. Both subsystems use humoral immunity and cell-mediated immunity to perform their functions. In humans, the blood–brain barrier, blood–cerebrospinal fluid barrier, and similar fluid–brain barriers separate the peripheral immune system from the neuroimmune system, which protects the brain.

Pathogens can rapidly evolve and adapt, and thereby avoid detection and neutralization by the immune system; however, multiple defense mechanisms have also evolved to recognize and neutralize pathogens. Even simple unicellular organisms such as bacteria possess a rudimentary immune system in the form of enzymes that protect against bacteriophage infections. Other basic immune mechanisms evolved in ancient eukaryotes and remain in their modern descendants, such as plants and invertebrates. These mechanisms include phagocytosis, antimicrobial peptides called defensins, and the complement system. Jawed vertebrates, including humans, have even more sophisticated defense mechanisms, including the ability to adapt over time to recognize specific pathogens more efficiently. Adaptive (or acquired) immunity creates immunological memory after an initial response to a specific pathogen, leading to an enhanced response to subsequent encounters with that same pathogen. This process of acquired immunity is the basis of vaccination.

Disorders of the immune system can result in autoimmune diseases, inflammatory diseases and cancer. Immunodeficiency occurs when the immune system is less active than normal, resulting in recurring and life-threatening infections. In humans, immunodeficiency can either be the result of a genetic disease such as severe combined immunodeficiency, acquired conditions such as HIV/AIDS, or the use of immunosuppressive medication. In contrast, autoimmunity results from a hyperactive immune system attacking normal tissues as if they were foreign organisms. Common autoimmune diseases include Hashimoto's thyroiditis, rheumatoid arthritis, diabetes mellitus type 1, and systemic lupus erythematosus. Immunology covers the study of all aspects of the immune system.

The immune system protects its host from infection with layered defenses of increasing specificity. In simple terms, physical barriers prevent pathogens such as bacteria and viruses from entering the organism. If a pathogen breaches these barriers, the innate immune system provides an immediate, but non-specific response. Innate immune systems are found in all plants and animals. If pathogens successfully evade the innate response, vertebrates possess a second layer of protection, the adaptive immune system, which is activated by the innate response. Here, the immune system adapts its response during an infection to improve its recognition of the pathogen. This improved response is then retained after the pathogen has been eliminated, in the form of an immunological memory, and allows the adaptive immune system to mount faster and stronger attacks each time this pathogen is encountered.

Both innate and adaptive immunity depend on the ability of the immune system to distinguish between self and non-self molecules. In immunology, "self" molecules are those components of an organism's body that can be distinguished from foreign substances by the immune system. Conversely, "non-self" molecules are those recognized as foreign molecules. One class of non-self molecules are called antigens (short for "anti"body "gen"erators) and are defined as substances that bind to specific immune receptors and elicit an immune response.

Newborn infants have no prior exposure to microbes and are particularly vulnerable to infection. Several layers of passive protection are provided by the mother. During pregnancy, a particular type of antibody, called IgG, is transported from mother to baby directly through the placenta, so human babies have high levels of antibodies even at birth, with the same range of antigen specificities as their mother. Breast milk or colostrum also contains antibodies that are transferred to the gut of the infant and protect against bacterial infections until the newborn can synthesize its own antibodies. This is passive immunity because the fetus does not actually make any memory cells or antibodies—it only borrows them. This passive immunity is usually short-term, lasting from a few days up to several months. In medicine, protective passive immunity can also be transferred artificially from one individual to another via antibody-rich serum.

Microorganisms or toxins that successfully enter an organism encounter the cells and mechanisms of the innate immune system. The innate response is usually triggered when microbes are identified by pattern recognition receptors, which recognize components that are conserved among broad groups of microorganisms, or when damaged, injured or stressed cells send out alarm signals, many of which (but not all) are recognized by the same receptors as those that recognize pathogens. Innate immune defenses are non-specific, meaning these systems respond to pathogens in a generic way. This system does not confer long-lasting immunity against a pathogen. The innate immune system is the dominant system of host defense in most organisms.

Cells in the innate immune system use pattern recognition receptors (PRRs) to recognize molecular structures that are produced by microbial pathogens. PRRs are germline-encoded host sensors, which detect molecules typical for the pathogens. They are proteins expressed, mainly, by cells of the innate immune system, such as dendritic cells, macrophages, monocytes, neutrophils and epithelial cells, to identify two classes of molecules: pathogen-associated molecular patterns (PAMPs), which are associated with microbial pathogens, and damage-associated molecular patterns (DAMPs), which are associated with components of host's cells that are released during cell damage or death.

Recognition of extracellular or endosomal pathogen-associated molecular patterns (PAMPs) is mediated by transmembrane proteins known as toll-like receptors (TLRs). TLRs share a typical structural motif, the Leucine rich repeats (LRR), which give them their specific appearance and are also responsible for TLR functionality. Toll-like receptors were first discovered in "Drosophila" and trigger the synthesis and secretion of cytokines and activation of other host defense programs that are necessary for both innate or adaptive immune responses. To date, ten functional members of the TLR family have been described in humans.

Cells in the innate immune system have pattern recognition receptors that detect infection or cell damage in the cytosol. Three major classes of these cytosolic receptors are NOD–like receptors, RIG (retinoic acid-inducible gene)-like receptors, and cytosolic DNA sensors.

Inflammasomes are multiprotein complexes (consist of an NLR, the adaptor protein ASC, and the effector molecule pro-caspase-1) that form in response to cytosolic PAMPs and DAMPs, whose function is to generate active forms of the inflammatory cytokines IL-1β and IL-18.

Several barriers protect organisms from infection, including mechanical, chemical, and biological barriers. The waxy cuticle of most leaves, the exoskeleton of insects, the shells and membranes of externally deposited eggs, and skin are examples of mechanical barriers that are the first line of defense against infection. However, as organisms cannot be completely sealed from their environments, other systems act to protect body openings such as the lungs, intestines, and the genitourinary tract. In the lungs, coughing and sneezing mechanically eject pathogens and other irritants from the respiratory tract. The flushing action of tears and urine also mechanically expels pathogens, while mucus secreted by the respiratory and gastrointestinal tract serves to trap and entangle microorganisms.

Chemical barriers also protect against infection. The skin and respiratory tract secrete antimicrobial peptides such as the β-defensins. Enzymes such as lysozyme and phospholipase A2 in saliva, tears, and breast milk are also antibacterials. Vaginal secretions serve as a chemical barrier following menarche, when they become slightly acidic, while semen contains defensins and zinc to kill pathogens. In the stomach, gastric acid serves as a powerful chemical defense against ingested pathogens.

Within the genitourinary and gastrointestinal tracts, commensal flora serve as biological barriers by competing with pathogenic bacteria for food and space and, in some cases, by changing the conditions in their environment, such as pH or available iron. As a result of the symbiotic relationship between commensals and the immune system, the probability that pathogens will reach sufficient numbers to cause illness is reduced. However, since most antibiotics non-specifically target bacteria and do not affect fungi, oral antibiotics can lead to an "overgrowth" of fungi and cause conditions such as a vaginal candidiasis (a yeast infection). There is good evidence that re-introduction of probiotic flora, such as pure cultures of the lactobacilli normally found in unpasteurized yogurt, helps restore a healthy balance of microbial populations in intestinal infections in children and encouraging preliminary data in studies on bacterial gastroenteritis, inflammatory bowel diseases, urinary tract infection and post-surgical infections.

Leukocytes (white blood cells) act like independent, single-celled organisms and are the second arm of the innate immune system. The innate leukocytes include the phagocytes (macrophages, neutrophils, and dendritic cells), innate lymphoid cells, mast cells, eosinophils, basophils, and natural killer cells. These cells identify and eliminate pathogens, either by attacking larger pathogens through contact or by engulfing and then killing microorganisms. Innate cells are also important mediators in lymphoid organ development and the activation of the adaptive immune system.

Phagocytosis is an important feature of cellular innate immunity performed by cells called phagocytes that engulf, or eat, pathogens or particles. Phagocytes generally patrol the body searching for pathogens, but can be called to specific locations by cytokines. Once a pathogen has been engulfed by a phagocyte, it becomes trapped in an intracellular vesicle called a phagosome, which subsequently fuses with another vesicle called a lysosome to form a phagolysosome. The pathogen is killed by the activity of digestive enzymes or following a respiratory burst that releases free radicals into the phagolysosome. Phagocytosis evolved as a means of acquiring nutrients, but this role was extended in phagocytes to include engulfment of pathogens as a defense mechanism. Phagocytosis probably represents the oldest form of host defense, as phagocytes have been identified in both vertebrate and invertebrate animals.

Neutrophils and macrophages are phagocytes that travel throughout the body in pursuit of invading pathogens. Neutrophils are normally found in the bloodstream and are the most abundant type of phagocyte, normally representing 50% to 60% of the total circulating leukocytes, and consisting of neutrophil-killer and neutrophil-cager subpopulations. During the acute phase of inflammation, particularly as a result of bacterial infection, neutrophils migrate toward the site of inflammation in a process called chemotaxis, and are usually the first cells to arrive at the scene of infection. Macrophages are versatile cells that reside within tissues and produce a wide array of chemicals including enzymes, complement proteins, and cytokines, while they can also act as scavengers that rid the body of worn-out cells and other debris, and as antigen-presenting cells that activate the adaptive immune system.

Dendritic cells (DC) are phagocytes in tissues that are in contact with the external environment; therefore, they are located mainly in the skin, nose, lungs, stomach, and intestines. They are named for their resemblance to neuronal dendrites, as both have many spine-like projections, but dendritic cells are in no way connected to the nervous system. Dendritic cells serve as a link between the bodily tissues and the innate and adaptive immune systems, as they present antigens to T cells, one of the key cell types of the adaptive immune system.

Granulocytes are leukocytes that have granules in their cytoplasm. In this category are neutrophils, mast cells, basophils, and eosinophils. Mast cells reside in connective tissues and mucous membranes, and regulate the inflammatory response. They are most often associated with allergy and anaphylaxis. Basophils and eosinophils are related to neutrophils. They secrete chemical mediators that are involved in defending against parasites and play a role in allergic reactions, such as asthma.

Innate lymphoid cells (ILCs) are a group of innate immune cells that are derived from common lymphoid progenitor (CLP) and belong to the lymphoid lineage. These cells are defined by absence of antigen specific B or T cell receptor because of the lack of recombination activating gene (RAG). ILCs do not express myeloid or dendritic cell markers.

Natural killer cells, one of member ILCs, are lymphocytes and a component of the innate immune system which does not directly attack invading microbes. Rather, NK cells destroy compromised host cells, such as tumor cells or virus-infected cells, recognizing such cells by a condition known as "missing self." This term describes cells with low levels of a cell-surface marker called MHC I (major histocompatibility complex)—a situation that can arise in viral infections of host cells. They were named "natural killer" because of the initial notion that they do not require activation in order to kill cells that are "missing self." For many years it was unclear how NK cells recognize tumor cells and infected cells. It is now known that the MHC makeup on the surface of those cells is altered and the NK cells become activated through recognition of "missing self". Normal body cells are not recognized and attacked by NK cells because they express intact self MHC antigens. Those MHC antigens are recognized by killer cell immunoglobulin receptors (KIR) which essentially put the brakes on NK cells.

Inflammation is one of the first responses of the immune system to infection. The symptoms of inflammation are redness, swelling, heat, and pain, which are caused by increased blood flow into tissue. Inflammation is produced by eicosanoids and cytokines, which are released by injured or infected cells. Eicosanoids include prostaglandins that produce fever and the dilation of blood vessels associated with inflammation, and leukotrienes that attract certain white blood cells (leukocytes). Common cytokines include interleukins that are responsible for communication between white blood cells; chemokines that promote chemotaxis; and interferons that have anti-viral effects, such as shutting down protein synthesis in the host cell. Growth factors and cytotoxic factors may also be released. These cytokines and other chemicals recruit immune cells to the site of infection and promote healing of any damaged tissue following the removal of pathogens.

The complement system is a biochemical cascade that attacks the surfaces of foreign cells. It contains over 20 different proteins and is named for its ability to "complement" the killing of pathogens by antibodies. Complement is the major humoral component of the innate immune response. Many species have complement systems, including non-mammals like plants, fish, and some invertebrates.

In humans, this response is activated by complement binding to antibodies that have attached to these microbes or the binding of complement proteins to carbohydrates on the surfaces of microbes. This recognition signal triggers a rapid killing response. The speed of the response is a result of signal amplification that occurs after sequential proteolytic activation of complement molecules, which are also proteases. After complement proteins initially bind to the microbe, they activate their protease activity, which in turn activates other complement proteases, and so on. This produces a catalytic cascade that amplifies the initial signal by controlled positive feedback. The cascade results in the production of peptides that attract immune cells, increase vascular permeability, and opsonize (coat) the surface of a pathogen, marking it for destruction. This deposition of complement can also kill cells directly by disrupting their plasma membrane.

The adaptive immune system evolved in early vertebrates and allows for a stronger immune response as well as immunological memory, where each pathogen is "remembered" by a signature antigen. The adaptive immune response is antigen-specific and requires the recognition of specific "non-self" antigens during a process called antigen presentation. Antigen specificity allows for the generation of responses that are tailored to specific pathogens or pathogen-infected cells. The ability to mount these tailored responses is maintained in the body by "memory cells". Should a pathogen infect the body more than once, these specific memory cells are used to quickly eliminate it.

The cells of the adaptive immune system are special types of leukocytes, called lymphocytes. B cells and T cells are the major types of lymphocytes and are derived from hematopoietic stem cells in the bone marrow. B cells are involved in the humoral immune response, whereas T cells are involved in cell-mediated immune response.

Killer T cells only recognize antigens coupled to Class I MHC molecules, while helper T cells and regulatory T cells only recognize antigens coupled to Class II MHC molecules. These two mechanisms of antigen presentation reflect the different roles of the two types of T cell. A third, minor subtype are the γδ T cells that recognize intact antigens that are not bound to MHC receptors. The double-positive T cells are exposed to a wide variety of self-antigens in the thymus, in which iodine is necessary for its thymus development and activity.

In contrast, the B cell antigen-specific receptor is an antibody molecule on the B cell surface and recognizes whole pathogens without any need for antigen processing. Each lineage of B cell expresses a different antibody, so the complete set of B cell antigen receptors represent all the antibodies that the body can manufacture.

Both B cells and T cells carry receptor molecules that recognize specific targets. T cells recognize a "non-self" target, such as a pathogen, only after antigens (small fragments of the pathogen) have been processed and presented in combination with a "self" receptor called a major histocompatibility complex (MHC) molecule.

There are two major subtypes of T cells: the killer T cell and the helper T cell. In addition there are regulatory T cells which have a role in modulating immune response.

Killer T cells are a sub-group of T cells that kill cells that are infected with viruses (and other pathogens), or are otherwise damaged or dysfunctional. As with B cells, each type of T cell recognizes a different antigen. Killer T cells are activated when their T-cell receptor (TCR) binds to this specific antigen in a complex with the MHC Class I receptor of another cell. Recognition of this MHC:antigen complex is aided by a co-receptor on the T cell, called CD8. The T cell then travels throughout the body in search of cells where the MHC I receptors bear this antigen. When an activated T cell contacts such cells, it releases cytotoxins, such as perforin, which form pores in the target cell's plasma membrane, allowing ions, water and toxins to enter. The entry of another toxin called granulysin (a protease) induces the target cell to undergo apoptosis. T cell killing of host cells is particularly important in preventing the replication of viruses. T cell activation is tightly controlled and generally requires a very strong MHC/antigen activation signal, or additional activation signals provided by "helper" T cells (see below).

Helper T cells regulate both the innate and adaptive immune responses and help determine which immune responses the body makes to a particular pathogen. These cells have no cytotoxic activity and do not kill infected cells or clear pathogens directly. They instead control the immune response by directing other cells to perform these tasks.

Helper T cells express T cell receptors (TCR) that recognize antigen bound to Class II MHC molecules. The MHC:antigen complex is also recognized by the helper cell's CD4 co-receptor, which recruits molecules inside the T cell (e.g., Lck) that are responsible for the T cell's activation. Helper T cells have a weaker association with the MHC:antigen complex than observed for killer T cells, meaning many receptors (around 200–300) on the helper T cell must be bound by an MHC:antigen in order to activate the helper cell, while killer T cells can be activated by engagement of a single MHC:antigen molecule. Helper T cell activation also requires longer duration of engagement with an antigen-presenting cell. The activation of a resting helper T cell causes it to release cytokines that influence the activity of many cell types. Cytokine signals produced by helper T cells enhance the microbicidal function of macrophages and the activity of killer T cells. In addition, helper T cell activation causes an upregulation of molecules expressed on the T cell's surface, such as CD40 ligand (also called CD154), which provide extra stimulatory signals typically required to activate antibody-producing B cells.

Gamma delta T cells (γδ T cells) possess an alternative T-cell receptor (TCR) as opposed to CD4+ and CD8+ (αβ) T cells and share the characteristics of helper T cells, cytotoxic T cells and NK cells. The conditions that produce responses from γδ T cells are not fully understood. Like other 'unconventional' T cell subsets bearing invariant TCRs, such as CD1d-restricted natural killer T cells, γδ T cells straddle the border between innate and adaptive immunity. On one hand, γδ T cells are a component of adaptive immunity as they rearrange TCR genes to produce receptor diversity and can also develop a memory phenotype. On the other hand, the various subsets are also part of the innate immune system, as restricted TCR or NK receptors may be used as pattern recognition receptors. For example, large numbers of human Vγ9/Vδ2 T cells respond within hours to common molecules produced by microbes, and highly restricted Vδ1+ T cells in epithelia respond to stressed epithelial cells.
A B cell identifies pathogens when antibodies on its surface bind to a specific foreign antigen. This antigen/antibody complex is taken up by the B cell and processed by proteolysis into peptides. The B cell then displays these antigenic peptides on its surface MHC class II molecules. This combination of MHC and antigen attracts a matching helper T cell, which releases lymphokines and activates the B cell. As the activated B cell then begins to divide, its offspring (plasma cells) secrete millions of copies of the antibody that recognizes this antigen. These antibodies circulate in blood plasma and lymph, bind to pathogens expressing the antigen and mark them for destruction by complement activation or for uptake and destruction by phagocytes. Antibodies can also neutralize challenges directly, by binding to bacterial toxins or by interfering with the receptors that viruses and bacteria use to infect cells.

When B cells and T cells are activated and begin to replicate, some of their offspring become long-lived memory cells. Throughout the lifetime of an animal, these memory cells remember each specific pathogen encountered and can mount a strong response if the pathogen is detected again. This is "adaptive" because it occurs during the lifetime of an individual as an adaptation to infection with that pathogen and prepares the immune system for future challenges. Immunological memory can be in the form of either passive short-term memory or active long-term memory.
The immune system is involved in many aspects of physiological regulation in the body. The immune system interacts intimately with other systems, such as the endocrine and the nervous systems. The immune system also plays a crucial role in embryogenesis (development of the embryo), as well as in tissue repair and regeneration.

Hormones can act as immunomodulators, altering the sensitivity of the immune system. For example, female sex hormones are known immunostimulators of both adaptive and innate immune responses. Some autoimmune diseases such as lupus erythematosus strike women preferentially, and their onset often coincides with puberty. By contrast, male sex hormones such as testosterone seem to be immunosuppressive. Other hormones appear to regulate the immune system as well, most notably prolactin, growth hormone and vitamin D.

When a T-cell encounters a foreign pathogen, it extends a vitamin D receptor. This is essentially a signaling device that allows the T-cell to bind to the active form of vitamin D, the steroid hormone calcitriol. T-cells have a symbiotic relationship with vitamin D. Not only does the T-cell extend a vitamin D receptor, in essence asking to bind to the steroid hormone version of vitamin D, calcitriol, but the T-cell expresses the gene CYP27B1, which is the gene responsible for converting the pre-hormone version of vitamin D, calcidiol into the steroid hormone version, calcitriol. Only after binding to calcitriol can T-cells perform their intended function. Other immune system cells that are known to express CYP27B1 and thus activate vitamin D calcidiol, are dendritic cells, keratinocytes and macrophages.

It is conjectured that a progressive decline in hormone levels with age is partially responsible for weakened immune responses in aging individuals. Conversely, some hormones are regulated by the immune system, notably thyroid hormone activity. The age-related decline in immune function is also related to decreasing vitamin D levels in the elderly. As people age, two things happen that negatively affect their vitamin D levels. First, they stay indoors more due to decreased activity levels. This means that they get less sun and therefore produce less cholecalciferol via UVB radiation. Second, as a person ages the skin becomes less adept at producing vitamin D.

The immune system is affected by sleep and rest, and sleep deprivation is detrimental to immune function. Complex feedback loops involving cytokines, such as interleukin-1 and tumor necrosis factor-α produced in response to infection, appear to also play a role in the regulation of non-rapid eye movement (REM) sleep. Thus the immune response to infection may result in changes to the sleep cycle, including an increase in slow-wave sleep relative to REM sleep.

When suffering from sleep deprivation, active immunizations may have a diminished effect and may result in lower antibody production, and a lower immune response, than would be noted in a well-rested individual. Additionally, proteins such as NFIL3, which have been shown to be closely intertwined with both T-cell differentiation and our circadian rhythms, can be affected through the disturbance of natural light and dark cycles through instances of sleep deprivation, shift work, etc. As a result, these disruptions can lead to an increase in chronic conditions such as heart disease, chronic pain, and asthma.

In addition to the negative consequences of sleep deprivation, sleep and the intertwined circadian system have been shown to have strong regulatory effects on immunological functions affecting both the innate and the adaptive immunity. First, during the early slow-wave-sleep stage, a sudden drop in blood levels of cortisol, epinephrine, and norepinephrine induce increased blood levels of the hormones leptin, pituitary growth hormone, and prolactin. These signals induce a pro-inflammatory state through the production of the pro-inflammatory cytokines interleukin-1, interleukin-12, TNF-alpha and IFN-gamma. These cytokines then stimulate immune functions such as immune cells activation, proliferation, and differentiation. It is during this time that undifferentiated, or less differentiated, like naïve and central memory T cells, peak (i.e. during a time of a slowly evolving adaptive immune response). In addition to these effects, the milieu of hormones produced at this time (leptin, pituitary growth hormone, and prolactin) support the interactions between APCs and T-cells, a shift of the T1/T2 cytokine balance towards one that supports T1, an increase in overall T cell proliferation, and naïve T cell migration to lymph nodes. This milieu is also thought to support the formation of long-lasting immune memory through the initiation of Th1 immune responses.

In contrast, during wake periods differentiated effector cells, such as cytotoxic natural killer cells and CTLs (cytotoxic T lymphocytes), peak in order to elicit an effective response against any intruding pathogens. As well during awake active times, anti-inflammatory molecules, such as cortisol and catecholamines, peak. There are two theories as to why the pro-inflammatory state is reserved for sleep time. First, inflammation would cause serious cognitive and physical impairments if it were to occur during wake times. Second, inflammation may occur during sleep times due to the presence of melatonin. Inflammation causes a great deal of oxidative stress and the presence of melatonin during sleep times could actively counteract free radical production during this time.

Overnutrition is associated with diseases such as diabetes and obesity, which are known to affect immune function. More moderate malnutrition, as well as certain specific trace mineral and nutrient deficiencies, can also compromise the immune response.

Foods rich in certain fatty acids may foster a healthy immune system. Likewise, fetal undernourishment can cause a lifelong impairment of the immune system.

The immune system, particularly the innate component, plays a decisive role in tissue repair after an insult. Key actors include macrophages and neutrophils, but other cellular actors, including γδ T cells, innate lymphoid cells (ILCs), and regulatory T cells (Tregs), are also important. The plasticity of immune cells and the balance between pro-inflammatory and anti-inflammatory signals are crucial aspects of efficient tissue repair. Immune components and pathways are involved in regeneration as well, for example in amphibians. According to one hypothesis, organisms that can regenerate could be less immunocompetent than organisms that cannot regenerate.

The immune system is a remarkably effective structure that incorporates specificity, inducibility and adaptation. Failures of host defense do occur, however, and fall into three broad categories: immunodeficiencies, autoimmunity, and hypersensitivities.

Immunodeficiencies occur when one or more of the components of the immune system are inactive. The ability of the immune system to respond to pathogens is diminished in both the young and the elderly, with immune responses beginning to decline at around 50 years of age due to immunosenescence. In developed countries, obesity, alcoholism, and drug use are common causes of poor immune function, while malnutrition is the most common cause of immunodeficiency in developing countries. Diets lacking sufficient protein are associated with impaired cell-mediated immunity, complement activity, phagocyte function, IgA antibody concentrations, and cytokine production. Additionally, the loss of the thymus at an early age through genetic mutation or surgical removal results in severe immunodeficiency and a high susceptibility to infection.

Immunodeficiencies can also be inherited or 'acquired'. Chronic granulomatous disease, where phagocytes have a reduced ability to destroy pathogens, is an example of an inherited, or congenital, immunodeficiency. AIDS and some types of cancer cause acquired immunodeficiency.

Overactive immune responses form the other end of immune dysfunction, particularly the autoimmune disorders. Here, the immune system fails to properly distinguish between self and non-self, and attacks part of the body. Under normal circumstances, many T cells and antibodies react with "self" peptides. One of the functions of specialized cells (located in the thymus and bone marrow) is to present young lymphocytes with self antigens produced throughout the body and to eliminate those cells that recognize self-antigens, preventing autoimmunity.

Hypersensitivity is an immune response that damages the body's own tissues. They are divided into four classes (Type I – IV) based on the mechanisms involved and the time course of the hypersensitive reaction. Type I hypersensitivity is an immediate or anaphylactic reaction, often associated with allergy. Symptoms can range from mild discomfort to death. Type I hypersensitivity is mediated by IgE, which triggers degranulation of mast cells and basophils when cross-linked by antigen.
Type II hypersensitivity occurs when antibodies bind to antigens on the patient's own cells, marking them for destruction. This is also called antibody-dependent (or cytotoxic) hypersensitivity, and is mediated by IgG and IgM antibodies.
Immune complexes (aggregations of antigens, complement proteins, and IgG and IgM antibodies) deposited in various tissues trigger Type III hypersensitivity reactions. Type IV hypersensitivity (also known as cell-mediated or "delayed type hypersensitivity") usually takes between two and three days to develop. Type IV reactions are involved in many autoimmune and infectious diseases, but may also involve "contact dermatitis" (poison ivy). These reactions are mediated by T cells, monocytes, and macrophages.

Inflammation is one of the first responses of the immune system to infection, but it can appear without known cause.

Inflammation is produced by eicosanoids and cytokines, which are released by injured or infected cells. Eicosanoids include prostaglandins that produce fever and the dilation of blood vessels associated with inflammation, and leukotrienes that attract certain white blood cells (leukocytes). Common cytokines include interleukins that are responsible for communication between white blood cells; chemokines that promote chemotaxis; and interferons that have anti-viral effects, such as shutting down protein synthesis in the host cell. Growth factors and cytotoxic factors may also be released. These cytokines and other chemicals recruit immune cells to the site of infection and promote healing of any damaged tissue following the removal of pathogens.

The immune response can be manipulated to suppress unwanted responses resulting from autoimmunity, allergy, and transplant rejection, and to stimulate protective responses against pathogens that largely elude the immune system (see immunization) or cancer.

Immunosuppressive drugs are used to control autoimmune disorders or inflammation when excessive tissue damage occurs, and to prevent transplant rejection after an organ transplant.

Anti-inflammatory drugs are often used to control the effects of inflammation. Glucocorticoids are the most powerful of these drugs; however, these drugs can have many undesirable side effects, such as central obesity, hyperglycemia, osteoporosis, and their use must be tightly controlled. Lower doses of anti-inflammatory drugs are often used in conjunction with cytotoxic or immunosuppressive drugs such as methotrexate or azathioprine.
Cytotoxic drugs inhibit the immune response by killing dividing cells such as activated T cells. However, the killing is indiscriminate and other constantly dividing cells and their organs are affected, which causes toxic side effects. Immunosuppressive drugs such as cyclosporin prevent T cells from responding to signals correctly by inhibiting signal transduction pathways.

Cancer immunotherapy covers the medical ways to stimulate the immune system to attack cancer tumours.

Long-term "active" memory is acquired following infection by activation of B and T cells. Active immunity can also be generated artificially, through vaccination. The principle behind vaccination (also called immunization) is to introduce an antigen from a pathogen in order to stimulate the immune system and develop specific immunity against that particular pathogen without causing disease associated with that organism. This deliberate induction of an immune response is successful because it exploits the natural specificity of the immune system, as well as its inducibility. With infectious disease remaining one of the leading causes of death in the human population, vaccination represents the most effective manipulation of the immune system mankind has developed.

Most viral vaccines are based on live attenuated viruses, while many bacterial vaccines are based on acellular components of micro-organisms, including harmless toxin components. Since many antigens derived from acellular vaccines do not strongly induce the adaptive response, most bacterial vaccines are provided with additional adjuvants that activate the antigen-presenting cells of the innate immune system and maximize immunogenicity.

Another important role of the immune system is to identify and eliminate tumors. This is called immune surveillance. The "transformed cells" of tumors express antigens that are not found on normal cells. To the immune system, these antigens appear foreign, and their presence causes immune cells to attack the transformed tumor cells. The antigens expressed by tumors have several sources; some are derived from oncogenic viruses like human papillomavirus, which causes cancer of the cervix, vulva, vagina, penis, anus, mouth, and throat, while others are the organism's own proteins that occur at low levels in normal cells but reach high levels in tumor cells. One example is an enzyme called tyrosinase that, when expressed at high levels, transforms certain skin cells (e.g. melanocytes) into tumors called melanomas. A third possible source of tumor antigens are proteins normally important for regulating cell growth and survival, that commonly mutate into cancer inducing molecules called oncogenes.

The main response of the immune system to tumors is to destroy the abnormal cells using killer T cells, sometimes with the assistance of helper T cells. Tumor antigens are presented on MHC class I molecules in a similar way to viral antigens. This allows killer T cells to recognize the tumor cell as abnormal. NK cells also kill tumorous cells in a similar way, especially if the tumor cells have fewer MHC class I molecules on their surface than normal; this is a common phenomenon with tumors. Sometimes antibodies are generated against tumor cells allowing for their destruction by the complement system.

Clearly, some tumors evade the immune system and go on to become cancers. Tumor cells often have a reduced number of MHC class I molecules on their surface, thus avoiding detection by killer T cells. Some tumor cells also release products that inhibit the immune response; for example by secreting the cytokine TGF-β, which suppresses the activity of macrophages and lymphocytes. In addition, immunological tolerance may develop against tumor antigens, so the immune system no longer attacks the tumor cells.

Paradoxically, macrophages can promote tumor growth when tumor cells send out cytokines that attract macrophages, which then generate cytokines and growth factors such as tumor-necrosis factor alpha that nurture tumor development or promote stem-cell-like plasticity. In addition, a combination of hypoxia in the tumor and a cytokine produced by macrophages induces tumor cells to decrease production of a protein that blocks metastasis and thereby assists spread of cancer cells. Anti-tumor M1 macrophages are recruited in early phases to tumor development but are progressively differentiated to M2 with pro-tumor effect, an immunosuppressor switch. The hypoxia reduces the cytokine production for the anti-tumor response and progressively macrophages acquire pro-tumor M2 functions driven by the tumor microenvironment, including IL-4 and IL-10. 

Larger drugs (>500 Da) can provoke a neutralizing immune response, meaning that the immune system produces neutralizing antibodies that counteract the action of the drugs, particularly if the drugs are administered repeatedly, or in larger doses. This limits the effectiveness of drugs based on larger peptides and proteins (which are typically larger than 6000 Da). In some cases, the drug itself is not immunogenic, but may be co-administered with an immunogenic compound, as is sometimes the case for Taxol. Computational methods have been developed to predict the immunogenicity of peptides and proteins, which are particularly useful in designing therapeutic antibodies, assessing likely virulence of mutations in viral coat particles, and validation of proposed peptide-based drug treatments. Early techniques relied mainly on the observation that hydrophilic amino acids are overrepresented in epitope regions than hydrophobic amino acids; however, more recent developments rely on machine learning techniques using databases of existing known epitopes, usually on well-studied virus proteins, as a training set. A publicly accessible database has been established for the cataloguing of epitopes from pathogens known to be recognizable by B cells. The emerging field of bioinformatics-based studies of immunogenicity is referred to as "immunoinformatics". Immunoproteomics is the study of large sets of proteins (proteomics) involved in the immune response.

It is likely that a multicomponent, adaptive immune system arose with the first vertebrates, as invertebrates do not generate lymphocytes or an antibody-based humoral response. Many species, however, utilize mechanisms that appear to be precursors of these aspects of vertebrate immunity. Immune systems appear even in the structurally most simple forms of life, with bacteria using a unique defense mechanism, called the restriction modification system to protect themselves from viral pathogens, called bacteriophages. Prokaryotes also possess acquired immunity, through a system that uses CRISPR sequences to retain fragments of the genomes of phage that they have come into contact with in the past, which allows them to block virus replication through a form of RNA interference. Prokaryotes also possess other defense mechanisms. Offensive elements of the immune systems are also present in unicellular eukaryotes, but studies of their roles in defense are few.

Pattern recognition receptors are proteins used by nearly all organisms to identify molecules associated with pathogens. Antimicrobial peptides called defensins are an evolutionarily conserved component of the innate immune response found in all animals and plants, and represent the main form of invertebrate systemic immunity. The complement system and phagocytic cells are also used by most forms of invertebrate life. Ribonucleases and the RNA interference pathway are conserved across all eukaryotes, and are thought to play a role in the immune response to viruses.

Unlike animals, plants lack phagocytic cells, but many plant immune responses involve systemic chemical signals that are sent through a plant. Individual plant cells respond to molecules associated with pathogens known as Pathogen-associated molecular patterns or PAMPs. When a part of a plant becomes infected, the plant produces a localized hypersensitive response, whereby cells at the site of infection undergo rapid apoptosis to prevent the spread of the disease to other parts of the plant. Systemic acquired resistance (SAR) is a type of defensive response used by plants that renders the entire plant resistant to a particular infectious agent. RNA silencing mechanisms are particularly important in this systemic response as they can block virus replication.

Evolution of the adaptive immune system occurred in an ancestor of the jawed vertebrates. Many of the classical molecules of the adaptive immune system (e.g., immunoglobulins and T-cell receptors) exist only in jawed vertebrates. However, a distinct lymphocyte-derived molecule has been discovered in primitive jawless vertebrates, such as the lamprey and hagfish. These animals possess a large array of molecules called Variable lymphocyte receptors (VLRs) that, like the antigen receptors of jawed vertebrates, are produced from only a small number (one or two) of genes. These molecules are believed to bind pathogenic antigens in a similar way to antibodies, and with the same degree of specificity.

The success of any pathogen depends on its ability to elude host immune responses. Therefore, pathogens evolved several methods that allow them to successfully infect a host, while evading detection or destruction by the immune system. Bacteria often overcome physical barriers by secreting enzymes that digest the barrier, for example, by using a type II secretion system. Alternatively, using a type III secretion system, they may insert a hollow tube into the host cell, providing a direct route for proteins to move from the pathogen to the host. These proteins are often used to shut down host defenses.

An evasion strategy used by several pathogens to avoid the innate immune system is to hide within the cells of their host (also called intracellular pathogenesis). Here, a pathogen spends most of its life-cycle inside host cells, where it is shielded from direct contact with immune cells, antibodies and complement. Some examples of intracellular pathogens include viruses, the food poisoning bacterium "Salmonella" and the eukaryotic parasites that cause malaria ("Plasmodium falciparum") and leishmaniasis ("Leishmania spp."). Other bacteria, such as "Mycobacterium tuberculosis", live inside a protective capsule that prevents lysis by complement. Many pathogens secrete compounds that diminish or misdirect the host's immune response. Some bacteria form biofilms to protect themselves from the cells and proteins of the immune system. Such biofilms are present in many successful infections, e.g., the chronic "Pseudomonas aeruginosa" and "Burkholderia cenocepacia" infections characteristic of cystic fibrosis. Other bacteria generate surface proteins that bind to antibodies, rendering them ineffective; examples include "Streptococcus" (protein G), "Staphylococcus aureus" (protein A), and "Peptostreptococcus magnus" (protein L).

The mechanisms used to evade the adaptive immune system are more complicated. The simplest approach is to rapidly change non-essential epitopes (amino acids and/or sugars) on the surface of the pathogen, while keeping essential epitopes concealed. This is called antigenic variation. An example is HIV, which mutates rapidly, so the proteins on its viral envelope that are essential for entry into its host target cell are constantly changing. These frequent changes in antigens may explain the failures of vaccines directed at this virus. The parasite "Trypanosoma brucei" uses a similar strategy, constantly switching one type of surface protein for another, allowing it to stay one step ahead of the antibody response. Masking antigens with host molecules is another common strategy for avoiding detection by the immune system. In HIV, the envelope that covers the virion is formed from the outermost membrane of the host cell; such "self-cloaked" viruses make it difficult for the immune system to identify them as "non-self" structures.

Immunology is a science that examines the structure and function of the immune system. It originates from medicine and early studies on the causes of immunity to disease. The earliest known reference to immunity was during the plague of Athens in 430 BC. Thucydides noted that people who had recovered from a previous bout of the disease could nurse the sick without contracting the illness a second time. In the 18th century, Pierre-Louis Moreau de Maupertuis made experiments with scorpion venom and observed that certain dogs and mice were immune to this venom. In the 10th century, Persian physician al-Razi (also known as Rhazes) wrote the first recorded theory of acquired immunity, noting that a smallpox bout protected its survivors from future infections. Although he explained the immunity in terms of "excess moisture" getting expelled from the blood—therefore preventing the disease to occur for a second time—this theory explained many observations about smallpox known during this time.

These and other observations of acquired immunity were later exploited by Louis Pasteur in his development of vaccination and his proposed germ theory of disease. Pasteur's theory was in direct opposition to contemporary theories of disease, such as the miasma theory. It was not until Robert Koch's 1891 proofs, for which he was awarded a Nobel Prize in 1905, that microorganisms were confirmed as the cause of infectious disease. Viruses were confirmed as human pathogens in 1901, with the discovery of the yellow fever virus by Walter Reed.

Immunology made a great advance towards the end of the 19th century, through rapid developments, in the study of humoral immunity and cellular immunity. Particularly important was the work of Paul Ehrlich, who proposed the side-chain theory to explain the specificity of the antigen-antibody reaction; his contributions to the understanding of humoral immunity were recognized by the award of a Nobel Prize in 1908, which was jointly awarded to the founder of cellular immunology, Elie Metchnikoff.

Immunology is strongly experimental in everyday practice but is also characterized by an ongoing theoretical attitude. Many theories have been suggested in immunology from the end of the nineteenth century up to the present time. The end of the 19th century and the beginning of the 20th century saw a battle between "cellular" and "humoral" theories of immunity. According to the cellular theory of immunity, represented in particular by Elie Metchnikoff, it was cells—more precisely, phagocytes—that were responsible for immune responses. In contrast, the humoral theory of immunity, held, among others, by Robert Koch and Emil von Behring, stated that the active immune agents were soluble components (molecules) found in the organism's “humors” rather than its cells.

In the mid-1950s, Frank Burnet, inspired by a suggestion made by Niels Jerne, formulated the clonal selection theory (CST) of immunity. On the basis of CST, Burnet developed a theory of how an immune response is triggered according to the self/nonself distinction: "self" constituents (constituents of the body) do not trigger destructive immune responses, while "nonself" entities (pathogens, an allograft) trigger a destructive immune response. The theory was later modified to reflect new discoveries regarding histocompatibility or the complex "two-signal" activation of T cells. The self/nonself theory of immunity and the self/nonself vocabulary have been criticized, but remain very influential.

More recently, several theoretical frameworks have been suggested in immunology, including "autopoietic" views, "cognitive immune" views, the "danger model" (or "danger theory"), and the "discontinuity" theory. The danger model, suggested by Polly Matzinger and colleagues, has been very influential, arousing many comments and discussions.

The following organs and body parts play a role in the immune system.




</doc>
<doc id="14959" url="https://en.wikipedia.org/wiki?curid=14959" title="Immunology">
Immunology

Immunology is a branch of biology that covers the study of immune systems in all organisms. Immunology charts, measures, and contextualizes the physiological functioning of the immune system in states of both health and diseases; malfunctions of the immune system in immunological disorders (such as autoimmune diseases, hypersensitivities, immune deficiency, and transplant rejection); and the physical, chemical, and physiological characteristics of the components of the immune system "in vitro", "in situ", and "in vivo". Immunology has applications in numerous disciplines of medicine, particularly in the fields of organ transplantation, oncology, rheumatology, virology, bacteriology, parasitology, psychiatry, and dermatology.

The term was coined by Russian biologist Ilya Ilyich Mechnikov, who advanced studies on immunology and received the Nobel Prize for his work in 1908. He pinned small thorns into starfish larvae and noticed unusual cells surrounding the thorns. This was the active response of the body trying to maintain its integrity. It was Mechnikov who first observed the phenomenon of phagocytosis, in which the body defends itself against a foreign body.

Prior to the designation of immunity, from the etymological root "immunis", which is Latin for "exempt", early physicians characterized organs that would later be proven as essential components of the immune system. The important lymphoid organs of the immune system are the thymus, bone marrow, and chief lymphatic tissues such as spleen, tonsils, lymph vessels, lymph nodes, adenoids, and liver. When health conditions worsen to emergency status, portions of immune system organs, including the thymus, spleen, bone marrow, lymph nodes, and other lymphatic tissues, can be surgically excised for examination while patients are still alive.

Many components of the immune system are typically cellular in nature and not associated with any specific organ, but rather are embedded or circulating in various tissues located throughout the body.

Classical immunology ties in with the fields of epidemiology and medicine. It studies the relationship between the body systems, pathogens, and immunity. The earliest written mention of immunity can be traced back to the plague of Athens in 430 BCE. Thucydides noted that people who had recovered from a previous bout of the disease could nurse the sick without contracting the illness a second time. Many other ancient societies have references to this phenomenon, but it was not until the 19th and 20th centuries before the concept developed into scientific theory.

The study of the molecular and cellular components that comprise the immune system, including their function and interaction, is the central science of immunology. The immune system has been divided into a more primitive innate immune system and, in vertebrates, an acquired or adaptive immune system. The latter is further divided into humoral (or antibody) and cell-mediated components.

The immune system has the capability of self and non-self-recognition. An antigen is a substance that ignites the immune response. The cells involved in recognizing the antigen are Lymphocytes. Once they recognize, they secrete antibodies. Antibodies are proteins that neutralize the disease-causing microorganisms. Antibodies don't directly kill pathogens, but instead, identify antigens as targets for destruction by other immune cells such as phagocytes or NK cells.

The humoral (antibody) response is defined as the interaction between antibodies and antigens. Antibodies are specific proteins released from a certain class of immune cells known as B lymphocytes, while antigens are defined as anything that elicits the generation of antibodies ("anti"body "gen"erators). Immunology rests on an understanding of the properties of these two biological entities and the cellular response to both.

It's now getting clear that the immune responses contribute to the development of many common disorders not traditionally viewed as immunologic, including metabolic, cardiovascular, cancer, and neurodegenerative conditions like Alzheimer’s disease. Besides, there are direct implications of the immune system in the infectious diseases (tuberculosis, malaria, hepatitis, pneumonia, dysentery, and helminth infestations) as well. Hence, research in the field of immunology is of prime importance for the advancements in the fields of modern medicine, biomedical research, and biotechnology.

Immunological research continues to become more specialized, pursuing non-classical models of immunity and functions of cells, organs and systems not previously associated with the immune system (Yemeserach 2010).

Clinical immunology is the study of diseases caused by disorders of the immune system (failure, aberrant action, and malignant growth of the cellular elements of the system). It also involves diseases of other systems, where immune reactions play a part in the pathology and clinical features.

The diseases caused by disorders of the immune system fall into two broad categories:
Other immune system disorders include various hypersensitivities (such as in asthma and other allergies) that respond inappropriately to otherwise harmless compounds.

The most well-known disease that affects the immune system itself is AIDS, an immunodeficiency characterized by the suppression of CD4+ ("helper") T cells, dendritic cells and macrophages by the Human Immunodeficiency Virus (HIV).

Clinical immunologists also study ways to prevent the immune system's attempts to destroy allografts (transplant rejection).

The body’s capability to react to antigens depends on a person's age, antigen type, maternal factors and the area where the antigen is presented. Neonates are said to be in a state of physiological immunodeficiency, because both their innate and adaptive immunological responses are greatly suppressed. Once born, a child’s immune system responds favorably to protein antigens while not as well to glycoproteins and polysaccharides. In fact, many of the infections acquired by neonates are caused by low virulence organisms like "Staphylococcus" and "Pseudomonas". In neonates, opsonic activity and the ability to activate the complement cascade is very limited. For example, the mean level of C3 in a newborn is approximately 65% of that found in the adult. Phagocytic activity is also greatly impaired in newborns. This is due to lower opsonic activity, as well as diminished up-regulation of integrin and selectin receptors, which limit the ability of neutrophils to interact with adhesion molecules in the endothelium. Their monocytes are slow and have a reduced ATP production, which also limits the newborn's phagocytic activity. Although, the number of total lymphocytes is significantly higher than in adults, the cellular and humoral immunity is also impaired. Antigen-presenting cells in newborns have a reduced capability to activate T cells. Also, T cells of a newborn proliferate poorly and produce very small amounts of cytokines like IL-2, IL-4, IL-5, IL-12, and IFN-g which limits their capacity to activate the humoral response as well as the phagocitic activity of macrophage. B cells develop early during gestation but are not fully active.

Maternal factors also play a role in the body’s immune response. At birth, most of the immunoglobulin present is maternal IgG. Because IgM, IgD, IgE and IgA don't cross the placenta, they are almost undetectable at birth. Some IgA is provided by breast milk. These passively-acquired antibodies can protect the newborn for up to 18 months, but their response is usually short-lived and of low affinity. These antibodies can also produce a negative response. If a child is exposed to the antibody for a particular antigen before being exposed to the antigen itself then the child will produce a dampened response. Passively acquired maternal antibodies can suppress the antibody response to active immunization. Similarly, the response of T-cells to vaccination differs in children compared to adults, and vaccines that induce Th1 responses in adults do not readily elicit these same responses in neonates. Between six and nine months after birth, a child’s immune system begins to respond more strongly to glycoproteins, but there is usually no marked improvement in their response to polysaccharides until they are at least one year old. This can be the reason for distinct time frames found in vaccination schedules.

During adolescence, the human body undergoes various physical, physiological and immunological changes triggered and mediated by hormones, of which the most significant in females is 17-β-estradiol (an estrogen) and, in males, is testosterone. Estradiol usually begins to act around the age of 10 and testosterone some months later. There is evidence that these steroids not only act directly on the primary and secondary sexual characteristics but also have an effect on the development and regulation of the immune system, including an increased risk in developing pubescent and post-pubescent autoimmunity. There is also some evidence that cell surface receptors on B cells and macrophages may detect sex hormones in the system.

The female sex hormone 17-β-estradiol has been shown to regulate the level of immunological response, while some male androgens such as testosterone seem to suppress the stress response to infection. Other androgens, however, such as DHEA, increase immune response. As in females, the male sex hormones seem to have more control of the immune system during puberty and post-puberty than during the rest of a male's adult life.

Physical changes during puberty such as thymic involution also affect immunological response.

Ecoimmunology, or ecological immunology, explores the relationship between the immune system of an organism and its social, biotic and abiotic environment.

More recent ecoimmunological research has focused on host pathogen defences traditionally considered "non-immunological", such as pathogen avoidance, self-medication, symbiont-mediated defenses, and fecundity trade-offs. Behavioural immunity, a phrase coined by Mark Schaller, specifically refers to psychological pathogen avoidance drivers, such as disgust aroused by stimuli encountered around pathogen-infected individuals, such as the smell of vomit. More broadly, "behavioural" ecological immunity has been demonstrated in multiple species. For example, the Monarch butterfly often lays its eggs on certain toxic milkweed species when infected with parasites. These toxins reduce parasite growth in the offspring of the infected Monarch. However, when uninfected Monarch butterflies are forced to feed only on these toxic plants, they suffer a fitness cost as reduced lifespan relative to other uninfected Monarch butterflies. This indicates that laying eggs on toxic plants is a costly behaviour in Monarchs which has probably evolved to reduce the severity of parasite infection.

Symbiont-mediated defenses are also heritable across host generations, despite a non-genetic direct basis for the transmission. Aphids, for example, rely on several different symbionts for defense from key parasites, and can vertically transmit their symbionts from parent to offspring. Therefore, a symbiont that successfully confers protection from a parasite is more likely to be passed to the host offspring, allowing coevolution with parasites attacking the host in a way similar to traditional immunity.

The use of immune system components or antigens to treat a disease or disorder is known as immunotherapy. Immunotherapy is most commonly used to treat allergies, autoimmune disorders such as Crohn’s disease and rheumatoid arthritis, and certain cancers. Immunotherapy is also often used in the immunosuppressed (such as HIV patients) and people suffering from other immune deficiencies.
This includes regulating factors such as IL-2, IL-10, GM-CSF B, IFN-α.

The specificity of the bond between antibody and antigen has made the antibody an excellent tool for the detection of substances by a variety of diagnostic techniques. Antibodies specific for a desired antigen can be conjugated with an isotopic (radio) or fluorescent label or with a color-forming enzyme in order to detect it. However, the similarity between some antigens can lead to false positives and other errors in such tests by antibodies cross-reacting with antigens that aren't exact matches.

The study of the interaction of the immune system with cancer cells can lead to diagnostic tests and therapies with which to find and fight cancer. The immunology concerned with physiological reaction characteristic of the immune state.

This area of the immunology is devoted to the study of immunological aspects of the reproductive process including fetus acceptance. The term has also been used by fertility clinics to address fertility problems, recurrent miscarriages, premature deliveries and dangerous complications such as pre-eclampsia.

Immunology is strongly experimental in everyday practice but is also characterized by an ongoing theoretical attitude. Many theories have been suggested in immunology from the end of the nineteenth century up to the present time. The end of the 19th century and the beginning of the 20th century saw a battle between "cellular" and "humoral" theories of immunity. According to the cellular theory of immunity, represented in particular by Elie Metchnikoff, it was cells – more precisely, phagocytes – that were responsible for immune responses. In contrast, the humoral theory of immunity, held by Robert Koch and Emil von Behring, among others, stated that the active immune agents were soluble components (molecules) found in the organism's "humors" rather than its cells.

In the mid-1950s, Macfarlane Burnet, inspired by a suggestion made by Niels Jerne, formulated the clonal selection theory (CST) of immunity. On the basis of CST, Burnet developed a theory of how an immune response is triggered according to the self/nonself distinction: "self" constituents (constituents of the body) do not trigger destructive immune responses, while "nonself" entities (e.g., pathogens, an allograft) trigger a destructive immune response. The theory was later modified to reflect new discoveries regarding histocompatibility or the complex "two-signal" activation of T cells. The self/nonself theory of immunity and the self/nonself vocabulary have been criticized, but remain very influential.

More recently, several theoretical frameworks have been suggested in immunology, including "autopoietic" views, "cognitive immune" views, the "danger model" (or "danger theory"), and the "discontinuity" theory. The danger model, suggested by Polly Matzinger and colleagues, has been very influential, arousing many comments and discussions.



</doc>
<doc id="14960" url="https://en.wikipedia.org/wiki?curid=14960" title="IPA">
IPA

IPA commonly refers to:


IPA may also refer to:











</doc>
<doc id="14961" url="https://en.wikipedia.org/wiki?curid=14961" title="Ice beer">
Ice beer

Ice beer is a beer style, in which beer has undergone some degree of fractional freezing during production. These brands generally have higher alcohol content than typical beer and generally have a low price relative to their alcohol content.

The process of "icing" beer involves lowering the temperature of a batch of beer until ice crystals form. Since ethanol has a much lower freezing point (-114 °C; -173.2 °F) than water (0 °C; 32 °F), when the ice is removed, the alcohol concentration of the beer increases. The process is known as fractional freezing or freeze distillation.

Ice beer was developed by brewing a strong, dark lager, then freezing the beer and removing some of the ice. This concentrates the aroma and taste of the beer, and also raises the alcoholic strength of the finished beer. This produces a beer with 12 to 15 per cent alcohol. In North America, water would be added to lower the alcohol level.

Eisbock was introduced to Canada in 1989 by the microbrewery Niagara Falls Brewing Company. The brewers started with a strong dark lager (15.3 degrees Plato/1.061 original gravity, 6% alcohol by volume), then used the traditional method of freezing and removing ice to concentrate aroma and flavours while increasing the alcoholic strength to 8% ABV. Niagara Falls Eisbock was released annually as a seasonal winter beer; each year the label would feature a different historic view of nearby Niagara Falls in winter. This continued each year until the company was sold in 1994.

Despite this precedent, the large Canadian brewer Molson (now part of Molson Coors) claimed to have made the first ice beer in North America when it introduced "Canadian Ice" in April 1993. However, Molson's main competitor in Canada, Labatt (now part of Anheuser-Busch InBev), claimed to have patented the ice beer process earlier. When Labatt introduced an ice beer in August 1993, capturing a 10% market share in Canada, this instigated the so-called "Ice Beer Wars" of the 1990s.

Labatt had patented a specific method for making ice beer in 1997, 1998 and 2000, "A process for chill-treating, which is exemplified by a process for preparing a fermented malt beverage wherein brewing materials are mashed with water and the resulting mash is heated and wort separated therefrom. The wort is boiled cooled and fermented, and the beer is subjected to a finishing stage, which includes aging, to produce the final beverage. The improvement comprises subjecting the beer to a cold stage comprising rapidly cooling the beer to a temperature of about its freezing point in such a manner that ice crystals are formed therein in only minimal amounts. The resulting cooled beer is then mixed for a short period of time with a beer slurry containing ice crystals, without any appreciable collateral increase in the amount of ice crystals in the resulting mixture. Finally, the so-treated beer is extracted from the mixture." The company provides the following explanation for the layman: "During this unique process, the temperature is reduced until fine ice crystals form in the beer. Then using an exclusive process, the crystals are removed. The result is a full-flavoured balanced beer."

Miller acquired the U.S. marketing and distribution rights to Molson's products, and first introduced the Molson product in the United States in August 1993 as "Molson Ice". Miller also introduced the "Icehouse" brand under the "Plank Road Brewery" brand name shortly thereafter, and it is still sold nationwide.

Anheuser-Busch introduced "Bud Ice" (5.5% ABV) in 1994, and it remains one of the country's top selling ice beers. "Bud Ice" has a somewhat lower alcohol content than most other ice beer brands. In 1995, Anheuser-Busch also introduced two other major brands: "Busch Ice" (5.9% ABV, introduced 1995) and "Natural Ice" (also 5.9% ABV, also introduced in 1995). "Natural Ice" is the No. 1 selling ice beer brand in the United States; its low price makes it very popular on college campuses all over the country. Keystone Ice, a value-based subdivision of Coors, also produces a 5.9% ABV brew labeled "Keystone Ice".

Common ice beer brands in Canada in 2017, with approximately 5.5 to 6 per cent alcohol content, include Carling Ice, Molson Keystone Ice, Molson's Black Ice, Busch Ice, Old Milwaukee Ice, Brick's Laker Ice and Labatt Ice. There is a Labatt Maximum Ice too, with 7.1 per cent alcohol.

The ice beers are typically known for their high alcohol-to-dollar ratio. In some areas, a substantial number of ice beer products are considered to often be bought by "street drunks," and are prohibited for sale. For example, most of the products that are explicitly listed as prohibited in the beer and malt liquor category in the Seattle area are ice beers.



</doc>
<doc id="14962" url="https://en.wikipedia.org/wiki?curid=14962" title="Identity element">
Identity element

In mathematics, an identity element, or neutral element, is a special type of element of a set with respect to a binary operation on that set, which leaves any element of the set unchanged when combined with it. This concept is used in algebraic structures such as groups and rings. The term "identity element" is often shortened to "identity" (as in the case of additive identity and multiplicative identity), when there is no possibility of confusion, but the identity implicitly depends on the binary operation it is associated with.

Let be a set  equipped with a binary operation ∗. Then an element  of  is called a left identity if for all  in , and a right identity if for all  in . If is both a left identity and a right identity, then it is called a two-sided identity, or simply an identity.

An identity with respect to addition is called an additive identity (often denoted as 0) and an identity with respect to multiplication is called a multiplicative identity (often denoted as 1). These need not be ordinary addition and multiplication—as the underlying operation could be rather arbitrary. In the case of a group for example, the identity element is sometimes simply denoted by the symbol formula_1. The distinction between additive and multiplicative identity is used most often for sets that support both binary operations, such as rings, integral domains, and fields. The multiplicative identity is often called unity in the latter context (a ring with unity). This should not be confused with a unit in ring theory, which is any element having a multiplicative inverse. By its own definition, unity itself is necessarily a unit.

As the last example (a semigroup) shows, it is possible for to have several left identities. In fact, every element can be a left identity. In a similar manner, there can be several right identities. But if there is both a right identity and a left identity, then they must be equal, resulting in a single two-sided identity. 

To see this, note that if is a left identity and is a right identity, then . In particular, there can never be more than one two-sided identity: if there were two, say and , then would have to be equal to both and .

It is also quite possible for to have "no" identity element, such as the case of even integers under the multiplication operation. Another common example is the cross product of vectors, where the absence of an identity element is related to the fact that the direction of any nonzero cross product is always orthogonal to any element multiplied. That is, it is not possible to obtain a non-zero vector in the same direction as the original. Yet another example of group without identity element involves the additive semigroup of positive natural numbers.





</doc>
<doc id="14967" url="https://en.wikipedia.org/wiki?curid=14967" title="Instrumental">
Instrumental

An instrumental is a recording without any vocals, although it might include some inarticulate vocals, such as shouted backup vocals in a Big Band setting. Through semantic widening, a broader sense of the word song may refer to instrumentals. The music is primarily or exclusively produced using musical instruments. An instrumental can exist in music notation, after it is written by a composer; in the mind of the composer (especially in cases where the composer themselves will perform the piece, as in the case of a blues solo guitarist or a folk music fiddle player); as a piece that is performed live by a single instrumentalist or a musical ensemble, which could range in components from a duo or trio to a large Big Band, concert band or orchestra.

In a song that is otherwise sung, a section that is not sung but which is played by instruments can be called an instrumental interlude, or, if it occurs at the beginning of the song, before the singer starts to sing, an instrumental introduction. If the instrumental section highlights the skill, musicality, and often the virtuosity of a particular performer (or group of performers), the section may be called a "solo" (e.g., the guitar solo that is a key section of heavy metal music and hard rock songs). If the instruments are percussion instruments, the interlude can be called a percussion interlude or "percussion break". These interludes are a form of break in the song.

In commercial popular music, instrumental tracks are sometimes renderings, remixes of a corresponding release that features vocals, but they may also be compositions originally conceived without vocals. One example of a genre in which both vocal/instrumental and solely instrumental songs are produced is blues. A blues band often uses mostly songs that have lyrics that are sung, but during the band's show, they may also perform instrumental songs which only include electric guitar, harmonica, upright bass/electric bass and drum kit.

The opposite of instrumental music, that is, music for voices alone, without any accompaniment instruments, is a cappella, an Italian phrase that means "in the chapel". In early music, instruments such as trumpet and drums were considered outdoor instruments, and music for inside a chapel typically used quieter instruments, voices, or just voices alone. A capella music exists in both Classical music choir pieces (for choir without any accompanist piano or pipe organ) and in popular music styles such as doo wop groups and Barbershop quartets. For genres in which a non-vocal song or interlude is conceived using computers and software, rather than with acoustic musical instruments or electronic musical instruments, the term instrumental is still used for it.

Some recordings which include brief or non-musical use of the human voice are typically considered instrumentals. Examples include songs with the following:

Songs including actual musical—rhythmic, melodic, and lyrical—vocals might still be categorized as instrumentals if the vocals appear only as a short part of an extended piece (e.g., "Unchained Melody" (Les Baxter), "Batman Theme", "TSOP (The Sound of Philadelphia)", "Pick Up the Pieces", "The Hustle", "Fly, Robin, Fly", "Get Up and Boogie", "Do It Any Way You Wanna", and "Gonna Fly Now"), though this definition is loose and subjective.

Falling just outside of that definition is "Theme From Shaft" by Isaac Hayes.

"Better Off Alone", which began as an instrumental by DJ Jurgen, had vocals by Judith Pronk, who would become a seminal part of Alice Deejay, added in later releases of the track.



</doc>
<doc id="14968" url="https://en.wikipedia.org/wiki?curid=14968" title="Regular icosahedron">
Regular icosahedron

In geometry, a regular icosahedron ( or ) is a convex polyhedron with 20 faces, 30 edges and 12 vertices. It is one of the five Platonic solids, and the one with the most sides.

It has five equilateral triangular faces meeting at each vertex. It is represented by its Schläfli symbol {3,5}, or sometimes by its vertex figure as 3.3.3.3.3 or 3. It is the dual of the dodecahedron, which is represented by {5,3}, having three pentagonal faces around each vertex.

A regular icosahedron is a strictly convex deltahedron and a gyroelongated pentagonal bipyramid and a biaugmented pentagonal antiprism in any of six orientations.

The name comes . The plural can be either "icosahedrons" or "icosahedra" ().

If the edge length of a regular icosahedron is "a", the radius of a circumscribed sphere (one that touches the icosahedron at all vertices) is

and the radius of an inscribed sphere (tangent to each of the icosahedron's faces) is

while the midradius, which touches the middle of each edge, is

where "ϕ" is the golden ratio.

The surface area "A" and the volume "V" of a regular icosahedron of edge length "a" are:

The latter is "F" = "20" times the volume of a general tetrahedron with apex at the center of the
inscribed sphere, where the volume of the tetrahedron is one third times the base area times its height "r".

The volume filling factor of the circumscribed sphere is:

A sphere inscribed in an icosahedron will enclose 89.635% of its volume, compared to only 75.47% for a dodecahedron.

The midsphere of an icosahedron will have a volume 1.01664 times the volume of the icosahedron, which is by far the closest similarity in volume of any platonic solid with its midsphere. This arguably makes the icosahedron the "roundest" of the platonic solids.

The vertices of an icosahedron centered at the origin with an edge-length of 2 and a circumradius of formula_7 are described by circular permutations of:
where "ϕ" =  is the golden ratio.

Taking all permutations (not just cyclic ones) results in the Compound of two icosahedra.

Note that these vertices form five sets of three concentric, mutually orthogonal golden rectangles, whose edges form Borromean rings.

If the original icosahedron has edge length 1, its dual dodecahedron has edge length = = "ϕ" − 1.
The 12 edges of a regular octahedron can be subdivided in the golden ratio so that the resulting vertices define a regular icosahedron. This is done by first placing vectors along the octahedron's edges such that each face is bounded by a cycle, then similarly subdividing each edge into the golden mean along the direction of its vector. The five octahedra defining any given icosahedron form a regular polyhedral compound, while the two icosahedra that can be defined in this way from any given octahedron form a uniform polyhedron compound.

The locations of the vertices of a regular icosahedron can be described using spherical coordinates, for instance as latitude and longitude. If two vertices are taken to be at the north and south poles (latitude ±90°), then the other ten vertices are at latitude ±arctan() ≈ ±26.57°. These ten vertices are at evenly spaced longitudes (36° apart), alternating between north and south latitudes.

This scheme takes advantage of the fact that the regular icosahedron is a pentagonal gyroelongated bipyramid, with D dihedral symmetry—that is, it is formed of two congruent pentagonal pyramids joined by a pentagonal antiprism.

The icosahedron has three special orthogonal projections, centered on a face, an edge and a vertex:
The icosahedron can also be represented as a spherical tiling, and projected onto the plane via a stereographic projection. This projection is conformal, preserving angles but not areas or lengths. Straight lines on the sphere are projected as circular arcs on the plane.

The following construction of the icosahedron avoids tedious computations in the number field [] necessary in more elementary approaches.

The existence of the icosahedron amounts to the existence of six equiangular lines in . Indeed, intersecting such a system of equiangular lines with a Euclidean sphere centered at their common intersection yields the twelve vertices of a regular icosahedron as can easily be checked. Conversely, supposing the existence of a regular icosahedron, lines defined by its six pairs of opposite vertices form an equiangular system.

In order to construct such an equiangular system, we start with this 6 × 6 square matrix:

A straightforward computation yields (where "" is the 6 × 6 identity matrix). This implies that "A" has eigenvalues – and , both with multiplicity 3 since "A" is symmetric and of trace zero.

The matrix induces thus a Euclidean structure on the quotient space , which is isomorphic to since the kernel of has dimension 3. The image under the projection of the six coordinate axes "v", …, "v" in forms thus a system of six equiangular lines in intersecting pairwise at a common acute angle of arccos . Orthogonal projection of ±"v", …, ±"v" onto the -eigenspace of "A" yields thus the twelve vertices of the icosahedron.

A second straightforward construction of the icosahedron uses representation theory of the alternating group "A" acting by direct isometries on the icosahedron.

The rotational symmetry group of the regular icosahedron is isomorphic to the alternating group on five letters. This non-abelian simple group is the only non-trivial normal subgroup of the symmetric group on five letters. Since the Galois group of the general quintic equation is isomorphic to the symmetric group on five letters, and this normal subgroup is simple and non-abelian, the general quintic equation does not have a solution in radicals. The proof of the Abel–Ruffini theorem uses this simple fact, and Felix Klein wrote a book that made use of the theory of icosahedral symmetries to derive an analytical solution to the general quintic equation, . See icosahedral symmetry: related geometries for further history, and related symmetries on seven and eleven letters.

The full symmetry group of the icosahedron (including reflections) is known as the full icosahedral group, and is isomorphic to the product of the rotational symmetry group and the group "C" of size two, which is generated by the reflection through the center of the icosahedron.

The icosahedron has a large number of stellations. According to specific rules defined in the book "The Fifty-Nine Icosahedra", 59 stellations were identified for the regular icosahedron. The first form is the icosahedron itself. One is a regular Kepler–Poinsot polyhedron. Three are regular compound polyhedra.

The small stellated dodecahedron, great dodecahedron, and great icosahedron are three facetings of the regular icosahedron. They share the same vertex arrangement. They all have 30 edges. The regular icosahedron and great dodecahedron share the same edge arrangement but differ in faces (triangles vs pentagons), as do the small stellated dodecahedron and great icosahedron (pentagrams vs triangles).
There are distortions of the icosahedron that, while no longer regular, are nevertheless vertex-uniform. These are invariant under the same rotations as the tetrahedron, and are somewhat analogous to the snub cube and snub dodecahedron, including some forms which are chiral and some with T-symmetry, i.e. have different planes of symmetry from the tetrahedron.

The icosahedron is unique among the Platonic solids in possessing a dihedral angle not less than 120°. Its dihedral angle is approximately 138.19°. Thus, just as hexagons have angles not less than 120° and cannot be used as the faces of a convex regular polyhedron because such a construction would not meet the requirement that at least three faces meet at a vertex and leave a positive defect for folding in three dimensions, icosahedra cannot be used as the cells of a convex regular polychoron because, similarly, at least three cells must meet at an edge and leave a positive defect for folding in four dimensions (in general for a convex polytope in "n" dimensions, at least three facets must meet at a peak and leave a positive defect for folding in "n"-space). However, when combined with suitable cells having smaller dihedral angles, icosahedra can be used as cells in semi-regular polychora (for example the snub 24-cell), just as hexagons can be used as faces in semi-regular polyhedra (for example the truncated icosahedron). Finally, non-convex polytopes do not carry the same strict requirements as convex polytopes, and icosahedra are indeed the cells of the icosahedral 120-cell, one of the ten non-convex regular polychora.

An icosahedron can also be called a gyroelongated pentagonal bipyramid. It can be decomposed into a gyroelongated pentagonal pyramid and a pentagonal pyramid or into a pentagonal antiprism and two equal pentagonal pyramids.

It can be projected to 3D from the 6D 6-demicube using the same basis vectors that form the hull of the Rhombic triacontahedron from the 6-cube. Shown here including the inner 20 vertices which are not connected by the 30 outer hull edges of 6D norm length . The inner vertices form a dodecahedron.

<br>The 3D projection basis vectors [u,v,w] used are:

There are 3 uniform colorings of the icosahedron. These colorings can be represented as 11213, 11212, 11111, naming the 5 triangular faces around each vertex by their color.

The icosahedron can be considered a snub tetrahedron, as snubification of a regular tetrahedron gives a regular icosahedron having chiral tetrahedral symmetry. It can also be constructed as an alternated truncated octahedron, having pyritohedral symmetry. The pyritohedral symmetry version is sometimes called a pseudoicosahedron, and is dual to the pyritohedron.

Many viruses, e.g. herpes virus, have icosahedral shells. Viral structures are built of repeated identical protein subunits known as capsomeres, and the icosahedron is the easiest shape to assemble using these subunits. A "regular" polyhedron is used because it can be built from a single basic unit protein used over and over again; this saves space in the viral genome.

Various bacterial organelles with an icosahedral shape were also found. The icosahedral shell encapsulating enzymes and labile intermediates are built of different types of proteins with BMC domains.

In 1904, Ernst Haeckel described a number of species of Radiolaria, including "Circogonia icosahedra", whose skeleton is shaped like a regular icosahedron. A copy of Haeckel's illustration for this radiolarian appears in the article on regular polyhedra.

The closo-carboranes are chemical compounds with shape very close to icosahedron. Icosahedral twinning also occurs in crystals, especially nanoparticles.

Many borides and allotropes of boron contain boron B icosahedron as a basic structure unit.

Icosahedral dice with twenty sides have been used since ancient times.

In several roleplaying games, such as "Dungeons & Dragons", the twenty-sided die (d20 for short) is commonly used in determining success or failure of an action. This die is in the form of a regular icosahedron. It may be numbered from "0" to "9" twice (in which form it usually serves as a ten-sided die, or d10), but most modern versions are labeled from "1" to "20".

An icosahedron is the three-dimensional game board for Icosagame, formerly known as the Ico Crystal Game.

An icosahedron is used in the board game "Scattergories" to choose a letter of the alphabet. Six letters are omitted (Q, U, V, X, Y, and Z).

In the "Nintendo 64" game "", the boss Miracle Matter is a regular icosahedron.

Inside a Magic 8-Ball, various answers to yes–no questions are inscribed on a regular icosahedron.

R. Buckminster Fuller and Japanese cartographer Shoji Sadao designed a world map in the form of an unfolded icosahedron, called the Fuller projection, whose maximum distortion is only 2%. The American electronic music duo ODESZA use a regular icosahedron as their logo.

The skeleton of the icosahedron (the vertices and edges) forms a graph. It is one of 5 Platonic graphs, each a skeleton of its Platonic solid.

The high degree of symmetry of the polygon is replicated in the properties of this graph, which is distance-transitive and symmetric. The automorphism group has order 120. The vertices can be colored with 4 colors, the edges with 5 colors, and the diameter is 3.

The icosahedral graph is Hamiltonian: there is a cycle containing all the vertices. It is also a planar graph.

There are 4 related Johnson solids, including pentagonal faces with a subset of the 12 vertices. The similar dissected regular icosahedron has 2 adjacent vertices diminished, leaving two trapezoidal faces, and a bifastigium has 2 opposite sets of vertices removed and 4 trapezoidal faces. The pentagonal antiprism is formed by removing two opposite vertices.

The icosahedron can be transformed by a truncation sequence into its dual, the dodecahedron:
As a snub tetrahedron, and alternation of a truncated octahedron it also exists in the tetrahedral and octahedral symmetry families:

This polyhedron is topologically related as a part of sequence of regular polyhedra with Schläfli symbols {3,"n"}, continuing into the hyperbolic plane.
The regular icosahedron, seen as a "snub tetrahedron", is a member of a sequence of snubbed polyhedra and tilings with vertex figure (3.3.3.3."n") and Coxeter–Dynkin diagram . These figures and their duals have ("n"32) rotational symmetry, being in the Euclidean plane for "n" = 6, and hyperbolic plane for any higher "n". The series can be considered to begin with "n" = 2, with one set of faces degenerated into digons.

The icosahedron can tessellate hyperbolic space in the order-3 icosahedral honeycomb, with 3 icosahedra around each edge, 12 icosahedra around each vertex, with Schläfli symbol {3,5,3}. It is one of four regular tessellations in the hyperbolic 3-space.




</doc>
<doc id="14971" url="https://en.wikipedia.org/wiki?curid=14971" title="Industrial archaeology of Dartmoor">
Industrial archaeology of Dartmoor

The industrial archaeology of Dartmoor covers a number of the industries which have, over the ages, taken place on Dartmoor, and the remaining evidence surrounding them. Currently only three industries are economically significant, yet all three will inevitably leave their own traces on the moor: china clay mining, farming and tourism.

A good general guide to the commercial activities on Dartmoor at the end of the 19th century is William Crossing's "The Dartmoor Worker".

In former times, lead, silver, tin and copper were mined extensively on Dartmoor. The most obvious evidence of mining to the casual visitor to Dartmoor are the remains of the old engine-house at Wheal Betsy which is alongside the A386 road between Tavistock and Okehampton. The word "Wheal" has a particular meaning in Devon and Cornwall being either a tin or a copper mine, however in the case of Wheal Betsy it was principally lead and silver which were mined.

Once widely practised by many miners across the moor, by the early 1900s only a few tinners remained, and mining had almost completely ceased twenty years later. Some of the more significant mines were Eylesbarrow, Knock Mine, Vitifer Mine and Hexworthy Mine. The last active mine in the Dartmoor area was Great Rock Mine, which shut down in 1969.

Dartmoor granite has been used in many Devon and Cornish buildings. The prison at Princetown was built from granite taken from Walkhampton Common. When the horse tramroad from Plymouth to Princetown was completed in 1823, large quantities of granite were more easily transported.

There were three major granite quarries on the moor: Haytor, Foggintor and Merrivale. The granite quarries around Haytor were the source of the stone used in several famous structures, including the New London Bridge, completed in 1831. This granite was transported from the moor via the Haytor Granite Tramway, stretches of which are still visible.

The extensive quarries at Foggintor provided granite for the construction of London's Nelson's Column in the early 1840s, and New Scotland Yard was faced with granite from the quarry at Merrivale. Merrivale Quarry continued excavating and working its own granite until the 1970s, producing gravestones and agricultural rollers. Work at Merrivale continued until the 1990s, for the last 20 years imported stone such as gabbro from Norway and Italian marble was dressed and polished. The unusual pink granite at Great Trowlesworthy Tor was also quarried, and there were many other small granite quarries dotted around the moor. Various metamorphic rocks were also quarried in the metamorphic aureole around the edge of the moor, most notably at Meldon.

In 1844 a factory for making gunpowder was built on the open moor, not far from Postbridge. Gunpowder was needed for the tin mines and granite quarries then in operation on the moor. The buildings were widely spaced from one another for safety and the mechanical power for grinding ("incorporating") the powder was derived from waterwheels driven by a leat.

Now known as "Powdermills" or "Powder Mills", there are extensive remains of this factory still visible. Two chimneys still stand and the walls of the two sturdily-built incorporating mills with central waterwheels survive well: they were built with substantial walls but flimsy roofs so that in the event of an explosion, the force of the blast would be directed safely upwards. The ruins of a number of ancillary buildings also survive. A proving mortar—a type of small cannon used to gauge the strength of the gunpowder—used by the factory still lies by the side of the road to the nearby pottery.

Peat-cutting for fuel occurred at some locations on Dartmoor until certainly the 1970s, usually for personal use. The right of Dartmoor commoners to cut peat for fuel is known as "turbary". These rights were conferred a long time ago, pre-dating most written records. The area once known as the "Turbary of Alberysheved" between the River Teign and the headwaters of the River Bovey is mentioned in the Perambulation of the Forest of Dartmoor of 1240 (by 1609 the name of the area had changed to Turf Hill).

An attempt was made to commercialise the cutting of peat in 1901 at Rattle Brook Head, however this quickly failed.

From at least the 13th century until early in the 20th, rabbits were kept on a commercial scale, both for their flesh and their fur. Documentary evidence for this exists in place names such as Trowlesworthy Warren (mentioned in a document dated 1272) and Warren House Inn. The physical evidence, in the form of pillow mounds is also plentiful, for example there are 50 pillow mounds at Legis Tor Warren. The sophistication of the warreners is shown by the existence of vermin traps that were placed near the warrens to capture weasels and stoats attempting to get at the rabbits.

The significance of the term "warren" nowadays is not what it once was. In the Middle Ages it was a privileged place, and the creatures of the warren were protected by the king 'for his princely delight and pleasure'.

The subject of warrening on Dartmoor was addressed in Eden Phillpotts' story "The River".

Farming has been practised on Dartmoor since time immemorial. The dry-stone walls which separate fields and mark boundaries give an idea of the extent to which the landscape has been shaped by farming. There is little or no arable farming within the moor, mostly being given over to livestock farming on account of the thin and rocky soil. Some Dartmoor farms are remote in the extreme.




</doc>
<doc id="14972" url="https://en.wikipedia.org/wiki?curid=14972" title="Idempotence">
Idempotence

Idempotence (, ) is the property of certain operations in mathematics and computer science whereby they can be applied multiple times without changing the result beyond the initial application. The concept of idempotence arises in a number of places in abstract algebra (in particular, in the theory of projectors and closure operators) and functional programming (in which it is connected to the property of referential transparency).

The term was introduced by Benjamin Peirce in the context of elements of algebras that remain invariant when raised to a positive integer power, and literally means "(the quality of having) the same power", from + "potence" (same + power).

An element "x" of a magma ("M", •) is said to be "idempotent" if:
If all elements are idempotent with respect to •, then • is called idempotent.
The formula ∀"x", is called the idempotency law for •.


In the monoid ("E", ∘) of the functions from a set "E" to itself with the function composition ∘, idempotent elements are the functions such that , in other words such that for all "x" in "E", (the image of each element in "E" is a fixed point of "f"). For example:
Other examples include:

If the set "E" has "n" elements, we can partition it into "k" chosen fixed points and non-fixed points under "f", and then "k" is the number of different idempotent functions. Hence, taking into account all possible partitions,
is the total number of possible idempotent functions on the set. The integer sequence of the number of idempotent functions as given by the sum above for "n" = 0, 1, 2, 3, 4, 5, 6, 7, 8, … starts with 1, 1, 3, 10, 41, 196, 1057, 6322, 41393, … .

Neither the property of being idempotent nor that of being not is preserved under function composition. As an example for the former, mod 3 and "g"("x") = max("x", 5) are both idempotent, but is not, although happens to be. As an example for the latter, the negation function ¬ on the Boolean domain is not idempotent, but is. Similarly, unary negation of real numbers is not idempotent, but is.

In computer science, the term "idempotence" may have a different meaning depending on the context in which it is applied:

This is a very useful property in many situations, as it means that an operation can be repeated or retried as often as necessary without causing unintended effects. With non-idempotent operations, the algorithm may have to keep track of whether the operation was already performed or not.

A function looking up a customer's name and address in a database is typically idempotent, since this will not cause the database to change. Similarly, changing a customer's address to XYZ is typically idempotent, because the final address will be the same no matter how many times XYZ is submitted. However, placing an order for a cart for the customer is typically not idempotent, since running the call several times will lead to several orders being placed. Canceling an order is idempotent, because the order remains canceled no matter how many requests are made.

A composition of idempotent methods or subroutines, however, is not necessarily idempotent if a later method in the sequence changes a value that an earlier method depends on – "idempotence is not closed under composition". For example, suppose the initial value of a variable is 3 and there is a sequence that reads the variable, then changes it to 5, and then reads it again. Each step in the sequence is idempotent: both steps reading the variable have no side effects and changing a variable to 5 will always have the same effect no matter how many times it is executed. Nonetheless, executing the entire sequence once produces the output (3, 5), but executing it a second time produces the output (5, 5), so the sequence is not idempotent.

In the Hypertext Transfer Protocol (HTTP), idempotence and safety are the major attributes that separate HTTP verbs. Of the major HTTP verbs, GET, PUT, and DELETE should be implemented in an idempotent manner according to the standard, but POST need not be. GET retrieves a resource; PUT stores content at a resource; and DELETE eliminates a resource. As in the example above, reading data usually has no side effects, so it is idempotent (in fact "nullipotent"). Storing and deleting a given set of content are each usually idempotent as long as the request specifies a location or identifier that uniquely identifies that resource and only that resource again in the future. The PUT and DELETE operations with unique identifiers reduce to the simple case of assignment to an immutable variable of either a value or the null-value, respectively, and are idempotent for the same reason; the end result is always the same as the result of the initial execution, even if the response differs.

Violation of the unique identification requirement in storage or deletion typically causes violation of idempotence. For example, storing or deleting a given set of content without specifying a unique identifier: POST requests, which do not need to be idempotent, often do not contain unique identifiers, so the creation of the identifier is delegated to the receiving system which then creates a corresponding new record. Similarly, PUT and DELETE requests with nonspecific criteria may result in different outcomes depending on the state of the system - for example, a request to delete the most recent record. In each case, subsequent executions will further modify the state of the system, so they are not idempotent.

In Event stream processing, idempotence refers to the ability of a system to produce the same outcome, even if the same file, event or message is received more than once.

In a load-store architecture, instructions that might possibly cause a page fault are idempotent. So if a page fault occurs, the OS can load the page from disk and then simply re-execute the faulted instruction. In a processor where such instructions are not idempotent, dealing with page faults is much more complex.

When reformatting output, pretty-printing is expected to be idempotent. In other words, if the output is already "pretty", there should be nothing to do for the pretty-printer.

In service-oriented architecture (SOA), a multiple-step orchestration process composed entirely of idempotent steps can be replayed without side-effects if any part of that process fails.

Many operations that are idempotent often have ways to "resume"
a process if it is interrupted --
ways that finish much faster than starting all over from the beginning.
For example, resuming a file transfer, 
synchronizing files, 
creating a software build,
installing an application and all of its dependencies with a package manager,
etc.

Applied examples that many people could encounter in their day-to-day lives include elevator call buttons and crosswalk buttons. The initial activation of the button moves the system into a requesting state, until the request is satisfied. Subsequent activations of the button between the initial activation and the request being satisfied have no effect, unless the system is designed to adjust the time for satisfying the request based on the number of activations.




</doc>
<doc id="14973" url="https://en.wikipedia.org/wiki?curid=14973" title="Ithaca, New York">
Ithaca, New York

Ithaca is a city in the Finger Lakes region of New York. It is the seat of Tompkins County, as well as the largest community in the Ithaca–Tompkins County metropolitan area. This area contains the municipalities of the Town of Ithaca, the village of Cayuga Heights, and other towns and villages in Tompkins County. The city of Ithaca is located on the southern shore of Cayuga Lake, in Central New York, about south-west of Syracuse. It is named after the Greek island of Ithaca. Additionally, Ithaca is located southeast of Toronto, and northwest of New York City.

Ithaca is home to Cornell University, an Ivy League school of over 20,000 students, most of whom study at its local campus. In addition, Ithaca College is a private, nonsectarian, liberal arts college of over 7,000 students, located just south of the city in the Town of Ithaca, adding to the area's "college town" atmosphere. Nearby is Tompkins Cortland Community College (TC3). These three colleges bring tens of thousands of students, who increase Ithaca's seasonal population during the school year. The city's voters are notably more liberal than those in the remainder of Tompkins County or in upstate New York, generally voting for Democratic Party candidates.

As of 2010, the city's population was 30,014. A 2019 census estimate stated the population was 30,837.

Namgyal Monastery in Ithaca is the North American seat of Tenzin Gyatso, the 14th Dalai Lama.

Native Americans lived in this area for thousands of years. When discovered by Europeans, this area was controlled by the Cayuga tribe of Indians, one of the Five Nations of the "Haudenosaunee" or Iroquois League. Jesuit missionaries from New France (Quebec) are said to have had a mission to convert the Cayuga as early as 1657.

Saponi and Tutelo peoples, Siouan-speaking tribes, later occupied lands at the south end of Cayuga Lake. Dependent tributaries of the Cayuga, they had been permitted to settle on the tribe's hunting lands at the south end of Cayuga Lake, as well as in Pony (originally Sapony) Hollow of what is known as present-day Newfield, New York. Remnants of these tribes had been forced from Virginia and North Carolina by tribal conflicts and European colonial settlement. Similarly, the Tuscarora people, an Iroquoian-speaking tribe from the Carolinas, migrated after defeat in the Yamasee War; they settled with the Oneida people and became the sixth nation of the Haudenosaunee, with chiefs stating the migration was complete in 1722.
During the Revolutionary War, four of the then six Iroquois nations helped the British attempt to crush the revolution, although bands made decisions on fighting in a highly decentralized way. Conflict with the rebel colonists was fierce throughout the Mohawk Valley and western New York. In retaliation for conflicts to the east and resentment at the savage way in which the Iroquois made war, the 1779 Sullivan Expedition was conducted against the Iroquois in the west of the state, destroying more than 40 villages and stored winter crops and forcing their retreat from the area. It destroyed the Tutelo village of Coregonal, located near what is now the junction of state routes 13 and 13A just south of the Ithaca city limits. Most Iroquois were forced from the state after the Revolutionary War, but some remnants remained. The state sold off the former Iroquois lands to stimulate development and settlement by Americans; lands were also granted as payment to veterans of the war.

Within the current boundaries of the City of Ithaca, Native Americans maintained only a temporary hunting camp at the base of Cascadilla Gorge. In 1788, eleven men from Kingston, New York came to the area with two Delaware people (Lenape) guides, to explore what they considered wilderness. The following year Jacob Yaple, Isaac Dumond, and Peter Hinepaw returned with their families and constructed log cabins. That same year Abraham Bloodgood of Albany obtained a patent from the state for 1,400 acres, which included all of the present downtown west of Tioga Street. In 1790, the federal government and state began an official program to grant land in the area, known as the Central New York Military Tract, as payment for service to the American soldiers of the Revolutionary War, as the government was cash poor. Most local land titles trace back to these Revolutionary war grants.

As part of this process, the Central New York Military Tract, which included northern Tompkins County, was surveyed by Simeon De Witt, Bloodgood's son-in-law. De Witt was also the nephew of Governor George Clinton. The Commissioners of Lands of New York State (chairman Gov. George Clinton) met in 1790. The Military Tract township in which proto-Ithaca was located was named the Town of Ulysses. A few years later De Witt moved to Ithaca, then called variously "The Flats," "The City," or "Sodom"; he renamed it for the Greek island home of Ulysses in the spirit of the multitude of settlement names in the region derived from classical literature, such as Aurelius, Ovid, and especially of Ulysses, New York, the town that contained Ithaca at the time.

Around 1791 De Witt surveyed what is now the downtown area into lots and sold them at modest prices. That same year John Yaple built a grist mill on Cascadilla Creek. The first frame house was erected in 1800 by Abram Markle. In 1804 the village had a postmaster and in 1805 a tavern.

Ithaca became a transshipping point for salt from curing beds near Salina, New York to buyers south and east. This prompted construction in 1810 of the Owego Turnpike. When the War of 1812 cut off access to Nova Scotia gypsum, used for fertilizer, Ithaca became the center of trade in Cayuga gypsum. The Cayuga Steamboat Company was organized in 1819 and in 1820 launched the first steamboat on Cayuga Lake, the "Enterprise." In 1821, the village was incorporated at the same time the Town of Ithaca was organized and separated from the parent Town of Ulysses. In 1834, the Ithaca and Owego Railroad's first horse-drawn train began service, connecting traffic on the east–west Erie Canal (completed in 1825) with the Susquehanna River to the south to expand the trade network.

With the depression of 1837, the railroad was re-organized as the Cayuga & Susquehanna. It was re-engineered with switchbacks in the late 1840s; in the late 20th century a short section of this route in the city and town of Ithaca was used for the South Hill Recreation Way.

However, easier railroad routes were constructed, such as that of the Syracuse, Binghamton & New York (1854). In the decade following the Civil War, railroads were built from Ithaca to surrounding points (Geneva; Cayuga; Cortland; and Elmira, New York; and Athens, Pennsylvania), mainly with financing from Ezra Cornell. The geography of the city, on a steep hill by the lake, has prevented it from being directly connected to a major transportation artery. When the Lehigh Valley Railroad built its main line from Pennsylvania to Buffalo, New York in 1890, it bypassed Ithaca (running via eastern Schuyler County on easier grades), as the Delaware, Lackawanna and Western Railroad had done in the 1850s.
In the late 19th century, more industry developed in Ithaca. In 1883 William Henry Baker and his partners started the Ithaca Gun Company, making shotguns. The original factory was located in the Fall Creek neighborhood of the city, on a slope later known as Gun Hill, where the nearby waterfall supplied the main source of energy for the plant. The company became an icon in the hunting and shooting world, its shotguns famous for their fine decorative work. Wooden gunstocks with knots or other imperfections were donated to the high school woodworking shop to be made into lamps. John Philip Sousa and trick-shooter Annie Oakley favored Ithaca guns. In 1937 the company began producing the Ithaca 37, based on a 1915 patent by noted firearms designer John Browning. Its 12-gauge shotguns were the standard used for decades by the New York City Police Department and Los Angeles Police Department.

In 1885, Ithaca Children's Home was established on West Seneca Street. The orphanage had two programs at the time: a residential home for both orphaned and destitute children, and a day nursery. The village established its first trolley in 1887. Ithaca developed as a small manufacturing and retail center and was incorporated as a city in 1888. The largest industrial company in the area was Morse Chain, elements of which were absorbed into Emerson Power Transmission on South Hill and Borg Warner Automotive in Lansing, New York.

Ithaca claims to be the birthplace of the ice cream sundae, created in 1892 when fountain shop owner Chester Platt "served his local priest vanilla ice cream covered in cherry syrup with a dark candied cherry on top. The priest suggested the dessert be named after the day, Sunday—although the spelling was later changed out of fear some would find it offensive." The local Unitarian church, where the priest, Rev. John Scott, preached, has an annual "Sundae Sunday" every September in commemoration. Ithaca's claim has long been disputed by Two Rivers, Wisconsin. Also in 1892, the Ithaca Kitty became one of the first mass-produced stuffed animal toys in the United States.
In 1903 a typhoid epidemic, resulting from poor sanitation infrastructure, devastated the city. One out of ten citizens fell ill or died.

In 1900 Cornell anatomy professor G.S. Moler made an early movie using frame-by-frame technology. For "The Skeleton Dance," he took single-frame photos of a human skeleton in varying positions, giving the illusion of a dancing skeleton. During the early 20th century, Ithaca was an important center in the silent film industry. These films often featured the local natural scenery. Many of these films were the work of Leopold Wharton and his brother Theodore; their studio was on the site of what is now Stewart Park.

The Star Theatre on East Seneca Street was built in 1911 and became the most popular vaudeville venue in the region. Wharton movies were also filmed and shown there. After the film industry centralized in Hollywood, production in Ithaca effectively ceased. Few of the silent films made in Ithaca have been preserved.

After World War II, the Langmuir Research Labs of General Electric developed as a major employer; the defense industry continued to expand. GE's headquarters were in Schenectady, New York, to the east in the Mohawk Valley.

For decades, the Ithaca Gun Company tested their shotguns behind the plant on Lake Street; the shot fell into the Fall Creek gorge at the base of Ithaca Falls. Lead accumulated in the soil in and around the factory and gorge. A major lead clean-up effort sponsored by the United States Superfund took place from 2002 to 2004, managed through the Environmental Protection Agency. The old Ithaca Gun building has been dismantled. It was scheduled to be replaced by development of an apartment complex on the cleaned land.

The former Morse Chain company factory on South Hill, now owned by Emerson Power Transmission, was the site of extensive groundwater and soil contamination from its industrial operations. Emerson Power Transmission has been working with the state and South Hill residents to determine the extent and danger of the contamination and aid in cleanup.

In 2004, Gayraud Townsend, a 20-year-old senior in Cornell's School of Industrial and Labor Relations, was sworn in as alderman of the city council, the first black male to be elected to the council and the youngest African American to be elected to office in the United States. He served his full term and has mentored other student politicians. In 2011 Cornell Class of 2009 graduate Svante Myrick was elected as the youngest mayor of the city of Ithaca.

The valley in which Cayuga Lake is located is long and narrow with a north–south orientation. Ithaca is located at the southern end (the "head") of the lake, but the valley continues to the southwest behind the city. Originally a river valley, it was deepened and widened by the action of Pleistocene ice sheets over the last several hundred thousand years. The lake, which drains to the north, formed behind a dam of glacial moraine. The rock is predominantly Devonian and, north of Ithaca, is relatively fossil rich. Glacial erratics can be found in the area. The world-renowned fossils found in this area can be examined at the Museum of the Earth.

Ithaca was founded on flat land just south of the lake—land that formed in fairly recent geological times when silt filled the southern end of the lake. The city ultimately spread to the adjacent hillsides, which rise several hundred feet above the central flats: East Hill, West Hill, and South Hill. Its sides are fairly steep, and a number of the streams that flow into the valley from east or west have cut deep canyons, usually with several waterfalls.

The natural vegetation of the Ithaca area, seen in areas unbuilt and not been farmed on, is northern temperate broadleaf forest, dominated by deciduous trees.

According to the Köppen climate classification method, Ithaca experiences a warm-summer humid continental climate, also known as a hemiboreal climate ("Dfb"). Summers are warm but brief, and it is cool-to-cold the rest of the year, with long, snowy winters; an average of of snow falls per year. In addition, frost may occur any time of year except mid-summer.

Winter is typically characterized by freezing temperatures, cloudy skies, and light-to-moderate snows, with some heavier falls; the largest snowfall in one day was on February 14, 1914. But the season is also variable; there can be short mild periods with some rain, but also outbreaks of frigid air with night temperatures down to or lower. Summers usually bring sunshine, along with moderate heat and humidity, but also frequent afternoon thunderstorms. Nights are pleasant and sometimes cool. Occasionally, there can be heatwaves, with temperatures rising into the to range, but they tend to be brief.

The average date of the first freeze is October 5, and the average date of the last freeze is May 15, giving Ithaca a growing season of 141 days. The average date of the first and last snowfalls are November 12 and April 7, respectively. Extreme temperatures range from as recently as February 2, 1961 up to on July 9, 1936.

The valley flatland has slightly cooler weather in winter, and occasionally Ithaca residents experience simultaneous snow on the hills and rain in the valley. The phenomenon of mixed precipitation (rain, wind, and snow), common in the late fall and early spring, is known tongue-in-cheek as "ithacation" to many of the local residents.

Due to the microclimates created by the impact of the lakes, the region surrounding Ithaca (Finger Lakes American Viticultural Area) experiences a short but adequate growing season for winemaking similar to the Rhine Valley wine district of Germany. As such, the region is home to many wineries.
Ithaca is the larger principal city of the Ithaca-Cortland CSA, a Combined Statistical Area that includes the Ithaca metropolitan area (Tompkins County) and the Cortland micropolitan area (Cortland County), which had a combined population of 145,100 at the 2000 census.

As of the census of 2000, there were 29,287 people, 10,287 households, and 2,962 families residing in the city. The population density was 5,360.9 people per square mile (2,071.0/km). There were 10,736 housing units at an average density of 1,965.2 per square mile (759.2/km). The racial makeup of the city was 73.97% White, 13.65% Asian, 6.71% Black or African American, 0.39% Native American, 0.05% Pacific Islander, 1.86% from other races, and 3.36% from two or more races. Hispanic or Latino of any race were 5.31% of the population.

There were 10,287 households, out of which 14.2% had children under the age of 18 living with them, 19.0% were married couples living together, 7.8% had a female householder with no husband present, and 71.2% were non-families. 43.3% of all households were made up of individuals, and 7.4% had someone living alone who was 65 years of age or older. The average household size was 2.13 and the average family size was 2.81.

In the city, the population was spread out, with 9.2% under the age of 18, 53.8% from 18 to 24, 20.1% from 25 to 44, 10.6% from 45 to 64, and 6.3% who were 65 years of age or older. The median age was 22 years. For every 100 females, there were 102.6 males. For every 100 females age 18 and over, there were 102.2 males.

The median income for a household in the city was $21,441, and the median income for a family was $42,304. Males had a median income of $29,562 versus $27,828 for females. The per capita income for the city was $13,408. About 13.2% of individuals and 4.2% of families were below the poverty line.

The term "Greater Ithaca" encompasses both the City and Town of Ithaca, as well as several smaller settled places within or adjacent to the Town:

Municipalities


Census-designated places

There are two governmental entities in the area: the Town of Ithaca and the City of Ithaca. The Town of Ithaca is one of the nine towns comprising Tompkins County. The City of Ithaca is surrounded by, but legally independent of, the Town.

The City of Ithaca has a mayor–council government. The charter of the City of Ithaca provides for a full-time mayor and city judge, each independent and elected at-large. Since 1995, the mayor has been elected to a four-year term, and since 1989, the city judge has been elected to a six-year term.

Since 1983, the city has been divided into five wards. Each elects two representatives to the city council, known as the Common Council, for staggered four-year terms. In March 2015, the Common Council unanimously adopted a resolution recognizing freedom from domestic violence as a fundamental human right.

Since students won the right to vote where they attend colleges, some have become more active in local politics. In 2004, Gayraud Townsend, a 20-year-old senior in Cornell's School of Industrial and Labor Relations, was sworn in as alderman of the city council, representing the 4th Ward. He is the first black male to be elected to the council and was then the youngest African American to be elected to office in the United States. He served his full term and has mentored other young student politicians. In 2011, Cornell graduate Svante Myrick was elected Mayor of the City of Ithaca, becoming the youngest mayor in the city's history.

In December 2005, the City and Town governments began discussing opportunities for increased government consolidation, including the possibility of joining the two into a single entity. This topic had been previously discussed in 1963 and 1969. Cayuga Heights, a village adjacent to the city on its northeast, voted against annexation into the city of Ithaca in 1954.

Politically, the majority of city's voters (many of them students) have supported liberalism and the Democratic Party. A November 2004 study by ePodunk lists it as New York's most liberal city. This contrasts with the more conservative leanings of the generally rural Upstate New York region; the city's voters are also more liberal than those in the rest of Tompkins County. In 2008, Barack Obama, running against New York State's US Senator Hillary Clinton, won Tompkins County in the Democratic Presidential Primary, the only county that he won in New York State. Obama won Tompkins County (including Ithaca) by a wide margin of 41% over his opponent John McCain in the November 2008 election.

Ithaca is a major educational center in Central New York. The two major post-secondary educational institutions located in Ithaca were each founded in the late nineteenth century. In 1865, Ezra Cornell founded Cornell University, which overlooks the town from East Hill. It was opened as a coeducational institution. Women first enrolled in 1870. Ezra Cornell also established a public library for the city. Ithaca College was founded as the Ithaca Conservatory of Music in 1892. Ithaca College was originally located in the downtown area, but relocated to South Hill in the 1960s. In 2018 there were 23,600 students enrolled at Cornell and 6,700 at Ithaca College. Tompkins Cortland Community College is located in the neighboring Town of Dryden, and has an extension center in downtown Ithaca. Empire State College offers non-traditional college courses to adults in downtown Ithaca.

The Ithaca City School District, based in Ithaca, encompasses the city and its surrounding area and enrolls about 5,500 K-12 students in eight elementary schools (roughly one for every neighborhood, i.e., Cayuga Heights, Fall Creek, etc.), two middle schools (Boynton and Dewitt), Ithaca High School, and the Lehman Alternative Community School, a combined middle and high school. Several private elementary and secondary schools are located in the Ithaca area, including the Roman Catholic Immaculate Conception School, the Cascadilla School, the New Roots Charter School, the Elizabeth Ann Clune Montessori School, the Namaste Montessori School (in the Trumansburg area), and the Ithaca Waldorf School. Ithaca has two networks for supporting its home-schooling families: Loving Education At Home (LEAH) and the Northern Light Learning Center (NLLC). TST BOCES is located in Tompkins County.

The Tompkins County Public Library, located at 101 East Green Street, serves as the public library for Tompkins County and is the Central Library for the Finger Lakes Library System. The library serves over 37,000 registered borrowers and contains 250,000 items in its circulating collection.

The economy of Ithaca is based on education, with agriculture, technology and tourism in supporting roles. As of 2006, Ithaca has continued to have one of the few expanding economies in New York State outside New York City. It draws commuters for work from the neighboring rural counties of Cortland, Tioga, and Schuyler, as well as from the more urbanized Chemung County.

Ithaca has tried to maintain its traditional downtown shopping area with its pedestrian orientation; this includes the Ithaca Commons pedestrian mall and Center Ithaca, a small mixed-use complex built at the end of the urban renewal era. Another commercial center, Collegetown, is located next to the Cornell campus. It features a number of restaurants, shops, and bars, and an increasing number of high-rise apartments. It is primarily frequented by Cornell University students.

Ithaca has many of the businesses characteristic of small American university towns: bookstores, art house cinemas, craft stores, and vegetarian-friendly restaurants. The collective Moosewood Restaurant, founded in 1973, published a number of vegetarian cookbooks. "Bon Appetit" magazine ranked it among the thirteen most influential restaurants of the 20th century. Ithaca has many local restaurants and chains both in the city and town with a range of ethnic foods and has been regarded as having more restaurants per capita than New York City. It has become a destination and residence for retirees.

The Ithaca Farmers Market, a cooperative with 150 vendors who live within 30 miles of Ithaca, first opened for business on Saturdays in 1973. It is located at Steamboat Landing, where steamboats from Cayuga Lake used to dock.

The South Hills Business Campus originally opened in 1957 as the regional headquarters of the National Cash Register Company. Running three full factory shifts, NCR was a major employer. Although it was sold in 1991 to American Telephone and Telegraph and later acquired by Cognitive TPG, TPG remains a major tenant of the South Hill Business Campus, which is now owned by a group of private investors.

Ithaca, home to Cornell University College of Agriculture and Life Sciences, has a deep connection to Central New York's farming and dairy industries. About 60 small farms are located in the greater Ithaca/Trumansburg area, including a number of research farms managed by the Cornell University Agricultural Experiment Station. Cornell's Dairy Research Facility is a center of research and support for New York's large and growing milk and yogurt industries.

The "Ithaca Journal" was founded in 1815 and is a morning daily newspaper which has been owned by Gannett since 1912. The "Ithaca Times" is a free alternative weekly newspaper that's published every Wednesday. The "Cornell Daily Sun" is also published in Ithaca, operating since 1880. Other media outlets include the online independent news outlet, The Ithaca Voice, and the online magazine 14850.com.

Ithaca is home to several radio stations:

Public radio:
Other FM stations include: Saga's "98.7 The Vine", a low-powered translator station; WFIZ "Z95.5", airing a top-40 (CHR) format; contemporary Christian music station WCII 88.9; and classic rock "The Wall" WLLW 99.3 and 96.3, based in Seneca Falls with a transmitter in Ithaca.

Founded in 1983, the Sciencenter, is a non-profit hands-on science museum, accredited by the American Alliance of Museums (AAM) and is a member of the Association of Science-Technology Centers (ASTC) and Association of Children's Museums (ACM).

The Museum of the Earth is a natural history museum created in 2003 by the Paleontological Research Institution (PRI). The PRI was founded in Ithaca in 1932 and is the publisher of the oldest journal of paleontology in the western hemisphere. Exhibits cover the 4.5 billion year history of the earth in an accessible manner, including interactive displays. As of 2004, the PRI is now formally affiliated with Cornell.

The Cayuga Nature Center occupies the site of the 1914 Cayuga Preventorium, a facility for children with tuberculosis; treatment of what was then considered an incurable disease was based on rest and good nutrition. In 1981, the Cayuga Nature Center was incorporated as an independent, private, non-profit educational organization, offering environmental education to local school districts. In 2011, the PRI merged with the Cayuga Nature Center, making it a sister organization to the Museum of the Earth.

The Cornell Lab of Ornithology is located in the Imogene Powers Johnson Center for Birds and Biodiversity. The Lab's Visitors' Center and observation areas are open to the public. Displays include a surround sound theater, object-theater presentation, sound studio, and informational kiosks featuring bird sounds and information.

The Herbert F. Johnson Museum of Art houses one of the finest collections of art in upstate New York. Special exhibitions are mounted each year, plus selections from a global permanent collection, which is displayed on six public floors. The collection includes art from throughout Asia, Africa, Europe, the Americas, graphic arts, medallic art, and Tiffany glass, ranging from the ancient to the contemporary.

The Center for the Arts at Ithaca, Inc., operates the "Hangar Theatre". Opened in 1975 in a renovated municipal airport hangar, the Hangar hosts a summer season and brings a range of theatre to regional audiences including students, producing a school tour and Artists-in-the-Schools programs.
Ithaca is also the home to Kitchen Theatre Company, a non-profit professional company with a theatre on West State Street; and Civic Ensemble, a creative collaborative ensemble staging emerging playwrights' work and community-based original productions.

Ithaca is noted for its annual community celebration, The Ithaca Festival. The Constance Saltonstall Foundation for the Arts provides grants and summer fellowships at the Saltonstall Arts Colony for New York State artists and writers. Ithaca also hosts one of the largest used-book sales in the United States.

The city and town also sponsor The Apple Festival in the fall, the Chili Fest in February, the Finger Lakes International Dragon Boat Festival in July; Porchfest in late September, and the Ithaca Brew Fest in Stewart Park in September.

Ithaca has also pioneered the Ithaca Health Fund, a popular cooperative health insurance. Ithaca is home to one of the United States' first local currency systems, Ithaca Hours, developed by Paul Glover.

Ithaca is the home of the Cayuga Chamber Orchestra.

The Cornell Concert Series has been hosting musicians and ensembles of international stature since 1903. For its initial 84 years, the series featured Western classical artists exclusively. In 1987, however, the series broke with tradition to present Ravi Shankar and has since grown to encompass a broader spectrum of the world's great musics. Now, it balances of a mix of Western classical music, traditions from around the world, jazz, and new musics in these genres. In a single season, Cornell Concert Series presents performers ranging from the Leipzig Tomanerchor and Danish Quartet to Simon Shaheen, Vida Guitar Quartet, and Eighth Blackbird.

The School of Music at Ithaca College was founded in 1892 by William Egbert as a music conservatory on Buffalo Street. Among the degree programs offered are those in Performance, Theory, Music Education, and Composition. Since 1941, the School of Music has been accredited by the National Association of Schools of Music.

Ithaca's Suzuki school, Ithaca Talent Education, provides musical training for children of all ages and also teacher training for undergraduate and graduate-level students. The Community School of Music and Art uses an extensive scholarship system to offer classes and lessons to any student, regardless of age, background, economic status, or artistic ability.

A number of musicians call Ithaca home, most notably Samite of Uganda, The Burns Sisters, The Horse Flies, Johnny Dowd, Mary Lorson, cellist Hank Roberts, reggae band John Brown's Body, Kurt Riley, X Ambassadors, and Alex Kresovich. Old-time music is a staple and folk music is featured weekly on WVBR-FM's "Bound for Glory", North America's longest-running live folk concert broadcast. The Finger Lakes GrassRoots Festival of Music and Dance, hosted by local band Donna the Buffalo, is held annually during the third week in July in the nearby village of Trumansburg, with more than 60 local, national and international acts.

Ithaca is the center of a thriving live music scene, featuring over 200 groups playing most genres of American popular music, the predominant genres being Folk, Rock, Blues, Jazz, and Country. There are over 80 live music venues within a 40-mile radius of the city, including cafes, pubs, clubs, and concert halls.

In 2009, the Ithaca metropolitan statistical area (MSA) ranked as the highest in the United States for percentage of commuters who walked to work (15.1 percent). In 2013, the Ithaca MSA ranked as the second lowest in the United States for percentage of commuters who traveled by private vehicle (68.7 percent). During the same year, 17.5 percent of commuters in the Ithaca MSA walked to work.

Ithaca is in the rural Finger Lakes region about northwest of New York City; the nearest larger cities, Binghamton and Syracuse, are an hour's drive away by car, Rochester and Scranton are two hours, Buffalo and Albany are three. New York City, Philadelphia, Toronto, and Ottawa are about four hours away.

Ithaca lies at over a half hour's drive from any interstate highway, and all car trips to Ithaca involve some driving on two-lane state rural highways. The city is at the convergence of many regional two-lane state highways: Routes 13, 13A, 34, 79, 89, 96, 96B, and 366. These are usually not congested except in Ithaca proper. However, Route 79 between the I-81 access at Whitney Point and Ithaca receives a significant amount of Ithaca-bound congestion right before Ithaca's colleges reopen after breaks.

In July 2008, a non-profit called Ithaca Carshare began a carsharing service in Ithaca. Ithaca Carshare has a fleet of vehicles shared by over 1500 members as of July 2015 and has become a popular service among both city residents and the college communities. Vehicles are located throughout Ithaca downtown and the two major institutions. With Ithaca Carshare as the first locally run carsharing organization in New York State, others have since launched in Buffalo, Albany, NY, and Syracuse.

Rideshare services to promote carpooling and vanpooling are operated by ZIMRIDE and VRIDE. A community mobility education program, Way2Go is operated by Cornell Cooperative Extension of Tompkins County. Way2Go's website provides consumer information and videos. Way2Go works collaboratively to help people save money, stress less, go green and improve mobility options. The 2-1-1 Tompkins/Cortland Help line connects people with services, including transportation, in the community, by telephone and web on a 24/7 basis. The information and referral service is operated by the Human Services Coalition of Tompkins County, Inc. Together, 2-1-1 Information and Referral and Way2Go are a one-call, one-click resource designed to mobility services information for Ithaca and throughout Tompkins County.

As a growing urban area, Ithaca is facing steady increases in levels of vehicular traffic on the city grid and on the state highways. Outlying areas have limited bus service, and many people consider a car essential. However, many consider Ithaca a walkable and bikeable community. One positive trend for the health of downtown Ithaca is the new wave of increasing urban density in and around the Ithaca Commons. Because the downtown area is the region's central business district, dense mixed-use development that includes housing may increase the proportion of people who can walk to work and recreation, and mitigate the likely increased pressure on already busy roads as Ithaca grows. The downtown area is also the area best served by frequent public transportation. Still, traffic congestion around the Commons is likely to progressively increase.

There is frequent intercity bus service by Greyhound Lines, New York Trailways, OurBus, and Shortline (Coach USA), particularly to Binghamton and New York City, with limited service to Rochester, Buffalo and Syracuse, and (via connections in Binghamton) to Utica and Albany. OurBus also provides limited holiday services to Allentown, Pennsylvania, Philadelphia, and Washington, DC. Cornell University runs a premium campus to campus bus between its Ithaca campus and its medical school in Manhattan, New York City which is open to the public. Starting September 2019, intercity buses serving Ithaca operated from the downtown bus stop at 131 East Green Street, as the former Greyhound bus station on West State Street closed due to staff retirement and building maintenance issues.

Ithaca is the center of an extensive bus public transportation network. Tompkins Consolidated Area Transit, Inc. (TCAT, Inc.) is a not-for-profit corporation that provides public transportation for Tompkins County, New York. TCAT was reorganized as a non-profit corporation in 2004 and is primarily supported locally by Cornell University, the City of Ithaca and Tompkins County. TCAT's ridership increased from 2.7 million in 2004 to 4.4 million in 2013. TCAT operates 34 routes, many running seven days a week. It has frequent service to downtown, Cornell University, Ithaca College, and the Shops at Ithaca Mall in the Town of Lansing, but less frequent service to many residential and rural areas, including Trumansburg and Newfield. Chemung County Transit (C-TRAN) runs weekday commuter service from Chemung County to Ithaca. Cortland Transit runs commuter service to Cornell University. Tioga County Public Transit operates three routes to Ithaca and Cornell, but will cease operating on November 30, 2014.

GADABOUT Transportation Services, Inc. provides demand-response paratransit service for seniors over 60 and people with disabilities. Ithaca Dispatch provides local and regional taxi service. In addition, Ithaca Airline Limousine and IthaCar Service connect to the local airports.

Ithaca is served by Ithaca Tompkins International Airport, located about three miles to the northeast of the city center. In late 2019 the airport completed a major $34.8 million renovation which included a larger terminal with additional passenger gates and jet bridges, expanded passenger amenities, and a customs facility that enables it to receive international charter and private flights.

American Eagle offers daily flights to its hub at Philadelphia, and weekly service to its Charlotte hub, both operated by Piedmont Airlines using Embraer ERJ-145 airliners. Delta Connection provides service to its hub at Detroit Metro airport, operated by SkyWest Airlines using Bombardier CRJ-200 airliners. United Express offers two daily flights to Washington Dulles International Airport, operated by CommutAir using the Embraer ERJ-145.

Into the mid-twentieth century, it was possible to reach Ithaca by passenger rail. Two trains per day serviced Ithaca along either the Delaware, Lackawanna and Western Railroad or the Lehigh Valley Railroad. The trip took "about seven hours" from New York City, "about eight hours" from Philadelphia, and "about three hours" from Buffalo. There has been no passenger rail service since 1961. From the 1870s through 1961 there were trains to Buffalo via Geneva, New York; to New York City via Wilkes-Barre, Pennsylvania (Lehigh Valley Railroad) and (until the 1940s) Scranton, Pennsylvania (DL&W); to Auburn, New York via Cayuga Junction; and to the US northeast via Cortland, New York The Lehigh Valley's top New York City-Ithaca-Buffalo passenger train, the "Black Diamond," was optimistically publicized as 'The Handsomest Train in the World', perhaps to compensate for its roundabout route to Buffalo. It was named after the railroad's largest commodity, anthracite coal.

Into the early 1940s, the Lackawanna Railroad operated two shuttle trains a day to Owego, where passengers could transfer to trains to Buffalo and Chicago to the west and eastbound to Hoboken across from New York City. Until 1958, the Lackawanna maintained service through nearby Cortland. Until 1958, two Lehigh Valley trains a day made west-bound stops in Ithaca, and three trains a day made the station stops east-bound for New York City. The last train passenger train making stops in Ithaca was the Lehigh's "Maple Leaf," discontinued on February 1, 1961.

Within Ithaca, electric railways ran along Stewart Avenue and Eddy Street. In fact, Ithaca was the fourth community in New York state with a street railway; streetcars ran from 1887 to summer 1935.
In 2019 the Ithaca Central Railroad, a Watco subsidiary, took over the operation of the Norfolk Southern Ithaca Secondary line from Sayre, Pennsylvania to the Lake Ridge site of the AES Cayuga, a coal power plant (known as Milliken Station during NYSEG ownership). Unit trains of coal, delivered at Sayre, Pennsylvania from the Norfolk Southern are now rare. The main traffic is salt from the Cargill salt mine on the east shore of Cayuga Lake near Myers Point.


In addition to its liberal politics, Ithaca is commonly listed among the most culturally liberal of American small cities. The "Utne Reader" named Ithaca "America's most enlightened town" in 1997. According to ePodunk's Gay Index, Ithaca has a score of 231, versus a national average score of 100.

Like many small college towns, Ithaca has also received accolades for having a high overall quality of life. In 2004, "Cities Ranked and Rated" named Ithaca the best "emerging city" to live in the United States. In 2006, the Internet realty website "Relocate America" named Ithaca the fourth best city in the country to relocate to. In July 2006, Ithaca was listed as one of the "12 Hippest Hometowns for Vegetarians" by "VegNews Magazine" and chosen by "Mother Earth News" as one of the "12 Great Places You've Never Heard Of."

In 2012, the city was listed among the 10 best places to retire in the U.S. by U.S. News.

Ithaca was also ranked 13th among America's Best College Towns by "Travel + Leisure" in 2013 and ranked as the #1 Best College Town in America in the American Institute for Economic Research's 2013–2014 College Destination Index.

In its earliest years, during the frontier days, what is now Ithaca was briefly known by the names "The Flats" and "Sodom," the name of the Biblical city of sin, due to its reputation as a town of "notorious immorality", a place of horse racing, gambling, profanity, Sabbath breaking, and readily available liquor. These names did not last long; Simeon De Witt renamed the town Ithaca in the early 19th century, though nearby Robert H. Treman State Park still contains Lucifer Falls. Today, Ithaca is primarily known for its growing wineries and microbreweries, live music, colleges, and small dairy farms.



</doc>
<doc id="14975" url="https://en.wikipedia.org/wiki?curid=14975" title="Ivy League">
Ivy League

The Ivy League is an American collegiate athletic conference comprising eight private universities in the Northeastern United States. The term "Ivy League" is typically used beyond the sports context to refer to the eight schools as a group of elite colleges with connotations of academic excellence, selectivity in admissions, and social elitism. Its members (in alphabetical order) are Brown University, Columbia University, Cornell University, Dartmouth College, Harvard University, the University of Pennsylvania, Princeton University, and Yale University.

While the term was in use as early as 1933, it became official only after the formation of the NCAA Division I athletic conference in 1954. All of the Ivies except Cornell were founded during the colonial period; they thus account for seven of the nine Colonial Colleges chartered before the American Revolution. The other two colonial colleges, Rutgers University and the College of William & Mary, became public institutions instead.

Ivy League schools are viewed as some of the most prestigious universities in the world. All eight universities place in the top 17 of the 2020 "U.S. News & World Report" National Universities ranking, including four Ivies in the top five. "U.S. News" has named a member of the Ivy League as the best national university every year since 2001: , Princeton eleven times, Harvard twice, and the two schools tied for first five times. In the 2020 "U.S. News & World Report" Best Global University Ranking, three Ivies rank in the top 10 internationally (Harvard first, Columbia seventh, and Princeton eighth). 

Undergraduate enrollments range from about 4,500 to about 15,000, larger than most liberal arts colleges and smaller than most state universities. Total enrollment, which includes graduate students, ranges from approximately 6,600 at Dartmouth to over 20,000 at Columbia, Cornell, Harvard, and Penn. Ivy League financial endowments range from Brown's $4.2 billion to Harvard's $40.9 billion, the largest financial endowment of any academic institution in the world.

The Ivy League has drawn many comparisons to other elite grouping of universities in other nations such as Oxbridge in the United Kingdom, the C9 League in China, and the Imperial Universities in Japan. These counterparts are often referred to in the media as the "Ivy League" of their respective nations.

Ivy League universities have some of the largest university financial endowments in the world, allowing the universities to provide abundant resources for their academic programs, financial aid, and research endeavors. , Harvard University had an endowment of $38.3 billion, the largest of any educational institution. Each university attracts millions of dollars in annual research funding from both the federal government and private sources.

"Planting the ivy" was a customary class day ceremony at many colleges in the 1800s. In 1893, an alumnus told "The Harvard Crimson", "In 1850, class day was placed upon the University Calendar. ... the custom of planting the ivy, while the ivy oration was delivered, arose about this time." At Penn, graduating seniors started the custom of planting ivy at a university building each spring in 1873 and that practice was formally designated as "Ivy Day" in 1874. Ivy planting ceremonies are reported for Yale, Simmons, Bryn Mawr and many others. Princeton's "Ivy Club" was founded in 1879.

The first usage of "Ivy" in reference to a group of colleges is from sportswriter Stanley Woodward (1895–1965).

The first known instance of the term "Ivy League" being used appeared in "The Christian Science Monitor" on February 7, 1935. Several sportswriters and other journalists used the term shortly later to refer to the older colleges, those along the northeastern seaboard of the United States, chiefly the nine institutions with origins dating from the colonial era, together with the United States Military Academy (West Point), the United States Naval Academy, and a few others. These schools were known for their long-standing traditions in intercollegiate athletics, often being the first schools to participate in such activities. However, at this time, none of these institutions made efforts to form an athletic league.

A common folk etymology attributes the name to the Roman numeral for four (IV), asserting that there was such a sports league originally with four members. The "Morris Dictionary of Word and Phrase Origins" helped to perpetuate this belief. The supposed "IV League" was formed over a century ago and consisted of Harvard, Yale, Princeton, and a fourth school that varies depending on who is telling the story. However, it is clear that Harvard, Princeton, Yale and Columbia met on November 23, 1876, at the so-called Massasoit Convention to decide on uniform rules for the emerging game of American football, which rapidly spread.

Seven out of the eight Ivy League schools were founded before the American Revolution; Cornell was founded just after the American Civil War. These seven were the primary colleges in the Northern and Middle Colonies, and their early faculties and founding boards were largely drawn from other Ivy League institutions. There were also some British graduates from the University of Cambridge, the University of Oxford, the University of St. Andrews, the University of Edinburgh, and elsewhere on their boards. Similarly, the founder of the College of William & Mary, in 1693, was a British graduate of the University of Aberdeen and the University of Edinburgh. Cornell provided Stanford University with its first president.

The influence of these institutions on the founding of other colleges and universities is notable. This included the Southern public college movement which blossomed in the decades surrounding the turn of the 19th century when Georgia, South Carolina, North Carolina and Virginia established what became the flagship universities for each of these states. In 1801, a majority of the first board of trustees for what became the University of South Carolina were Princeton alumni. They appointed Jonathan Maxcy, a Brown graduate, as the university's first president. Thomas Cooper, an Oxford alumnus and University of Pennsylvania faculty member, became the second president of the South Carolina college. The founders of the University of California, Berkeley came from Yale, hence the school colors of University of California at Berkeley are Yale Blue and California Gold.

Some of the Ivy League schools have identifiable Protestant roots, while others were founded as non-sectarian schools. "King's College" was founded at the behest of King George II of Great Britain and the Church of England, but renamed Columbia College following the American revolution. In the early nineteenth century, the specific purpose of training Calvinist ministers was handed off to theological seminaries, but a denominational tone and such relics as compulsory chapel often lasted well into the twentieth century. Penn and Brown were officially founded as nonsectarian schools. Brown's charter promised no religious tests and "full liberty of conscience", but placed control in the hands of a board of twenty-two Baptists, five Quakers, four Congregationalists, and five Episcopalians. Cornell has been strongly nonsectarian from its founding.

"Ivy League" is sometimes used as a way of referring to an elite class, even though institutions such as Cornell University were among the first in the United States to reject racial and gender discrimination in their admissions policies. This dates back to at least 1935. Novels and memoirs attest this sense, as a social elite; to some degree independent of the actual schools.

After the Second World War, the present Ivy League institutions slowly widened their selection of their students. They had always had distinguished faculties; some of the first Americans with doctorates had taught for them; but they now decided that they could not both be world-class research institutions and be competitive in the highest ranks of American college sport; in addition, the schools experienced the scandals of any other big-time football programs, although more quietly.

The first formal athletic league involving eventual Ivy League schools (or any US colleges, for that matter) was created in 1870 with the formation of the Rowing Association of American Colleges. The RAAC hosted a de facto national championship in rowing during the period 1870–1894. In 1895, Cornell, Columbia, and Penn founded the Intercollegiate Rowing Association, which remains the oldest collegiate athletic organizing body in the US. To this day, the IRA Championship Regatta determines the national champion in rowing and all of the Ivies are regularly invited to compete.

A basketball league was later created in 1902, when Columbia, Cornell, Harvard, Yale, and Princeton formed the Eastern Intercollegiate Basketball League; they were later joined by Penn and Dartmouth.

In 1906, the organization that eventually became the National Collegiate Athletic Association was formed, primarily to formalize rules for the emerging sport of football. But of the 39 original member colleges in the NCAA, only two of them (Dartmouth and Penn) later became Ivies.

In February 1903, intercollegiate wrestling began when Yale accepted a challenge from Columbia, published in the Yale News. The dual meet took place prior to a basketball game hosted by Columbia and resulted in a tie. Two years later, Penn and Princeton also added wrestling teams, leading to the formation of the student-run Intercollegiate Wrestling Association, now the Eastern Intercollegiate Wrestling Association (EIWA), the first and oldest collegiate wrestling league in the US.

In 1930, Columbia, Cornell, Dartmouth, Penn, Princeton and Yale formed the Eastern Intercollegiate Baseball League; they were later joined by Harvard, Brown, Army and Navy.

Before the formal establishment of the Ivy League, there was an "unwritten and unspoken agreement among certain Eastern colleges on athletic relations". The earliest reference to the "Ivy colleges" came in 1933, when Stanley Woodward of the New York Herald Tribune used it to refer to the eight current members plus Army. In 1935, the Associated Press reported on an example of collaboration between the schools:

Despite such collaboration, the universities did not seem to consider the formation of the league as imminent. Romeyn Berry, Cornell's manager of athletics, reported the situation in January 1936 as follows:

Within a year of this statement and having held month-long discussions about the proposal, on December 3, 1936, the idea of "the formation of an Ivy League" gained enough traction among the undergraduate bodies of the universities that the "Columbia Daily Spectator", "The Cornell Daily Sun", "The Dartmouth", "The Harvard Crimson", "The Daily Pennsylvanian", "The Daily Princetonian" and the "Yale Daily News" would simultaneously run an editorial entitled "Now Is the Time", encouraging the seven universities to form the league in an effort to preserve the ideals of athletics. Part of the editorial read as follows:

The Ivies have been competing in sports as long as intercollegiate sports have existed in the United States. Rowing teams from Harvard and Yale met in the first sporting event held between students of two U.S. colleges on Lake Winnipesaukee, New Hampshire, on August 3, 1852. Harvard's team, "The Oneida", won the race and was presented with trophy black walnut oars from then-presidential nominee General Franklin Pierce.
The proposal did not succeed—on January 11, 1937, the athletic authorities at the schools rejected the "possibility of a heptagonal league in football such as these institutions maintain in basketball, baseball and track." However, they noted that the league "has such promising possibilities that it may not be dismissed and must be the subject of further consideration."

In 1945 the presidents of the eight schools signed the first "Ivy Group Agreement", which set academic, financial, and athletic standards for the football teams. The principles established reiterated those put forward in the Harvard-Yale-Princeton presidents' Agreement of 1916. The Ivy Group Agreement established the core tenet that an applicant's ability to play on a team would not influence admissions decisions:

In 1954, the presidents extended the Ivy Group Agreement to all intercollegiate sports, effective with the 1955–56 basketball season. This is generally reckoned as the formal formation of the Ivy League. As part of the transition, Brown, the only Ivy that hadn't joined the EIBL, did so for the 1954–55 season. A year later, the Ivy League absorbed the EIBL. The Ivy League claims the EIBL's history as its own. Through the EIBL, it is the oldest basketball conference in Division I.

As late as the 1960s many of the Ivy League universities' undergraduate programs remained open only to men, with Cornell the only one to have been coeducational from its founding (1865) and Columbia being the last (1983) to become coeducational. Before they became coeducational, many of the Ivy schools maintained extensive social ties with nearby Seven Sisters women's colleges, including weekend visits, dances and parties inviting Ivy and Seven Sisters students to mingle. This was the case not only at Barnard College and Radcliffe College, which are adjacent to Columbia and Harvard, but at more distant institutions as well. The movie "Animal House" includes a satiric version of the formerly common visits by Dartmouth men to Massachusetts to meet Smith and Mount Holyoke women, a drive of more than two hours. As noted by Irene Harwarth, Mindi Maline, and Elizabeth DeBra, "The 'Seven Sisters' was the name given to Barnard, Smith, Mount Holyoke, Vassar, Bryn Mawr, Wellesley, and Radcliffe, because of their parallel to the Ivy League men's colleges."

In 1982 the Ivy League considered adding two members, with Army, Navy, and Northwestern as the most likely candidates; if it had done so, the league could probably have avoided being moved into the recently created Division I-AA (now Division I FCS) for football. In 1983, following the admission of women to Columbia College, Columbia University and Barnard College entered into an athletic consortium agreement by which students from both schools compete together on Columbia University women's athletic teams, which replaced the women's teams previously sponsored by Barnard.

When Army and Navy departed the Eastern Intercollegiate Baseball League in 1992, nearly all intercollegiate competition involving the eight schools became united under the Ivy League banner. The two major exceptions are wrestling, with the Ivies that sponsor wrestling—all except Dartmouth and Yale—members of the EIWA and hockey, with the Ivies that sponsor hockey—all except Penn and Columbia—members of ECAC Hockey.

The Ivy League schools are highly selective, with their acceptance rates being approximately 10% or less at all of the universities. Admitted students come from around the world, although students from the Northeastern United States make up a significant proportion of students. In 2018, seven of the eight Ivy League schools reported record-high application numbers; seven also reported record-low acceptance rates. There have been arguments that Ivy League schools discriminate against Asian candidates. For example, in August 2020, the US Justice Department argued that Yale University discriminated against Asian candidates on the basis of their race, a charge the university denied. Harvard was subject to a similar challenge in 2019 from an Asian American student group, with regard to which a federal judge found Harvard to be in compliance with constitutional requirements. The student group has since appealed that decision, and the appeal is still pending as of August 2020.

Members of the League have been highly ranked by various university rankings. All of the Ivy League schools are consistently ranked within the top 20 national universities by the "U.S. News & World Report" Best Colleges Ranking. "The Wall Street Journal" rankings place all eight of the universities within the top 15 in the country.

Further, Ivy League members have produced many Nobel laureates, winners of the Nobel Prize and the Nobel Memorial Prize in Economic Sciences.

Collaboration between the member schools is illustrated by the student-led Ivy Council that meets in the fall and spring of each year, with representatives from every Ivy League school. The governing body of the Ivy League is the Council of Ivy Group presidents, composed of each university president. During meetings, the presidents discuss common procedures and initiatives for their universities.

The universities collaborate academically through the IvyPlus Exchange Scholar Program, which allows students to cross-register at one of the Ivies or another eligible school such as the University of California at Berkeley, the University of Chicago, the Massachusetts Institute of Technology, and Stanford University.

Different fashion trends and styles have emerged from Ivy League campuses over time, and fashion trends such as Ivy League and preppy are styles often associated with the Ivy League and its culture.

Ivy League style is a style of men's dress, popular during the late 1950s, believed to have originated on Ivy League campuses. The clothing stores J. Press and Brooks Brothers represent perhaps the quintessential Ivy League dress manner. The Ivy League style is said to be the predecessor to the preppy style of dress.

Preppy fashion started around 1912 to the late 1940s and 1950s as the Ivy League style of dress. J. Press represents the quintessential preppy clothing brand, stemming from the collegiate traditions that shaped the preppy subculture. In the mid-twentieth century J. Press and Brooks Brothers, both being pioneers in preppy fashion, had stores on Ivy League school campuses, including Harvard, Princeton, and Yale.

Some typical preppy styles also reflect traditional upper class New England leisure activities, such as equestrian, sailing or yachting, hunting, fencing, rowing, lacrosse, tennis, golf, and rugby. Longtime New England outdoor outfitters, such as L.L. Bean, became part of conventional preppy style. This can be seen in sport stripes and colours, equestrian clothing, plaid shirts, field jackets and nautical-themed accessories. Vacationing in Palm Beach, Florida, long popular with the East Coast upper class, led to the emergence of bright colour combinations in leisure wear seen in some brands such as Lilly Pulitzer. By the 1980s, other brands such as Lacoste, Izod and Dooney & Bourke became associated with preppy style.

Today, these styles continue to be popular on Ivy League campuses, throughout the U.S., and abroad, and are oftentimes labeled as "Classic American style" or "Traditional American style".

The Ivy League is often associated with the upper class White Anglo-Saxon Protestant community of the Northeast, Old money, or more generally, the American upper middle and upper classes. Although most Ivy League students come from upper middle- and upper-class families, the student body has become increasingly more economically and ethnically diverse. The universities provide significant financial aid to help increase the enrollment of lower income and middle class students. Several reports suggest, however, that the proportion of students from less-affluent families remains low.

Phrases such as "Ivy League snobbery" are ubiquitous in nonfiction and fiction writing of the early and mid-twentieth century. A Louis Auchincloss character dreads "the aridity of snobbery which he knew infected the Ivy League colleges". A business writer, warning in 2001 against discriminatory hiring, presented a cautionary example of an attitude to avoid (the bracketed phrase is his):

The phrase "Ivy League" historically has been perceived as connected not only with academic excellence but also with social elitism. In 1936, sportswriter John Kieran noted that student editors at Harvard, Yale, Princeton, Cornell, Columbia, Dartmouth, and Penn were advocating the formation of an athletic association. In urging them to consider "Army and Navy and Georgetown and Fordham and Syracuse and Brown and Pitt" as candidates for membership, he exhorted:

Aspects of Ivy stereotyping were illustrated during the 1988 presidential election, when George H. W. Bush (Yale '48) derided Michael Dukakis (graduate of Harvard Law School) for having "foreign-policy views born in Harvard Yard's boutique." "New York Times" columnist Maureen Dowd asked "Wasn't this a case of the pot calling the kettle elite?" Bush explained, however, that, unlike Harvard, Yale's reputation was "so diffuse, there isn't a symbol, I don't think, in the Yale situation, any symbolism in it. ... Harvard boutique to me has the connotation of liberalism and elitism" and said "Harvard" in his remark was intended to represent "a philosophical enclave" and not a statement about class. Columnist Russell Baker opined that "Voters inclined to loathe and fear elite Ivy League schools rarely make fine distinctions between Yale and Harvard. All they know is that both are full of rich, fancy, stuck-up and possibly dangerous intellectuals who never sit down to supper in their undershirt no matter how hot the weather gets." Still, the last five presidents have all attended Ivy League schools for at least part of their education— George H. W. Bush (Yale undergrad), Bill Clinton (Yale Law School), George W. Bush (Yale undergrad, Harvard Business School), Barack Obama (Columbia undergrad, Harvard Law School), and Donald Trump (Penn undergrad).

Of the 44 men who have served as President of the United States, 16 have graduated from an Ivy League university. Of them, eight have degrees from Harvard, five from Yale, three from Columbia, two from Princeton and one from Penn. Twelve presidents have earned Ivy undergraduate degrees. Three of these were transfer students: Donald Trump transferred from Fordham University, Barack Obama transferred from Occidental College, and John F. Kennedy transferred from Princeton to Harvard. John Adams was the first president to graduate from college, graduating from Harvard in 1755.

Students of the Ivy League largely hail from the Northeast, largely from the New York City, Boston, and Philadelphia areas. As all eight Ivy League universities are within the Northeast, it is no surprise that most graduates end up working and residing in the Northeast after graduation. An unscientific survey of Harvard seniors from the Class of 2013 found that 42% hailed from the Northeast and 55% overall were planning on working and residing in the Northeast. Boston and New York City are traditionally where many Ivy League graduates end up living.

Students of the Ivy League, both graduate and undergraduate, come primarily from upper middle and upper class families. In recent years, however, the universities have looked towards increasing socioeconomic and class diversity, by providing greater financial aid packages to applicants from lower, working, and lower middle class American families.

In 2013, 46% of Harvard undergraduate students came from families in the top 3.8% of all American households (i.e., over $200,000 annual income). In 2012, the bottom 25% of the American income distribution accounted for only 3–4% of students at Brown, a figure that had remained unchanged since 1992. In 2014, 69% of incoming freshmen students at Yale College came from families with annual incomes of over $120,000, putting most Yale College students in the upper middle and/or upper class. (The median household income in the U.S. in 2013 was $52,700.)

In the 2011–2012 academic year, students qualifying for Pell Grants (federally funded scholarships on the basis of need) comprised 20% at Harvard, 18% at Cornell, 17% at Penn, 16% at Columbia, 15% at Dartmouth and Brown, 14% at Yale, and 12% at Princeton. Nationally, 35% of American university students qualify for a Pell Grant.

Ivy champions are recognized in sixteen men's and sixteen women's sports. In some sports, Ivy teams actually compete as members of another league, the Ivy championship being decided by isolating the members' records in play against each other; for example, the six league members who participate in ice hockey do so as members of ECAC Hockey, but an Ivy champion is extrapolated each year. In one sport, rowing, the Ivies recognize team champions for each sex in both heavyweight and lightweight divisions. While the Intercollegiate Rowing Association governs all four sex- and bodyweight-based divisions of rowing, the only one that is sanctioned by the NCAA is women's heavyweight. The Ivy League was the last Division I basketball conference to institute a conference postseason tournament; the first tournaments for men and women were held at the end of the 2016–17 season. The tournaments only award the Ivy League automatic bids for the NCAA Division I Men's and Women's Basketball Tournaments; the official conference championships continue to be awarded based solely on regular-season results. Before the 2016–17 season, the automatic bids were based solely on regular-season record, with a one-game playoff (or series of one-game playoffs if more than two teams were tied) held to determine the automatic bid. The Ivy League is one of only two Division I conferences which award their official basketball championships solely on regular-season results; the other is the Southeastern Conference. Since its inception, an Ivy League school has yet to win either the men's or women's Division I NCAA Basketball Tournament.

On average, each Ivy school has more than 35 varsity teams. All eight are in the top 20 for number of sports offered for both men and women among Division I schools. Unlike most Division I athletic conferences, the Ivy League prohibits the granting of athletic scholarships; all scholarships awarded are need-based (financial aid). In addition, the Ivies have a rigid policy against redshirting, even for medical reasons; an athlete loses a year of eligibility for every year enrolled at an Ivy institution. Additionally, the Ivies prohibit graduate students from participating in intercollegiate athletics, even if they have remaining athletic eligibility. Ivy League teams' non-league games are often against the members of the Patriot League, which have similar academic standards and athletic scholarship policies (although unlike the Ivies, the Patriot League allows both redshirting and play by eligible graduate students).

In the time before recruiting for college sports became dominated by those offering athletic scholarships and lowered academic standards for athletes, the Ivy League was successful in many sports relative to other universities in the country. In particular, Princeton won 26 recognized national championships in college football (last in 1935), and Yale won 18 (last in 1927). Both of these totals are considerably higher than those of other historically strong programs such as Alabama, which has won 15, Notre Dame, which claims 11 but is credited by many sources with 13, and USC, which has won 11. Yale, whose coach Walter Camp was the "Father of American Football," held on to its place as the all-time wins leader in college football throughout the entire 20th century, but was finally passed by Michigan on November 10, 2001. Harvard, Yale, Princeton and Penn each have over a dozen former scholar-athletes enshrined in the College Football Hall of Fame. Currently Dartmouth holds the record for most Ivy League football titles, with 18, followed closely by Harvard and Penn, each with 17 titles. In addition, the Ivy League has produced Super Bowl winners Kevin Boothe (Cornell), two-time Pro Bowler Zak DeOssie (Brown), Sean Morey (Brown), All-Pro selection Matt Birk (Harvard), Calvin Hill (Yale), Derrick Harmon (Cornell) and 1999 "Mr. Irrelevant" Jim Finn (Penn).

Beginning with the 1982 football season, the Ivy League has competed in Division I-AA (renamed FCS The Ivy League teams are eligible for the FCS tournament held to determine the national champion, and the league champion is eligible for an automatic bid (and any other team may qualify for an at-large selection) from the NCAA. However, since its inception in 1956, the Ivy League has not played any postseason games due to concerns about the extended December schedule's effects on academics. (The last postseason game for a member was , the 1934 Rose Bowl, won by For this reason, any Ivy League team invited to the FCS playoffs turns down the bid. The Ivy League plays a strict 10-game schedule, compared to other FCS members' schedules of 11 (or, in some seasons, 12) regular season games, plus post-season, which expanded in 2013 to five rounds with 24 teams, with a bye week for the top eight teams. Football is the only sport in which the Ivy League declines to compete for a national title.

In addition to varsity football, Penn, Princeton and Cornell also field teams in the 10-team Collegiate Sprint Football League, in which all players must weigh 178 pounds or less. Penn and Princeton are the last remaining founding members of the league from its 1934 debut, and Cornell is the next-oldest, joining in 1937. Yale and Columbia previously fielded teams in the league but no longer do so.

The Ivy League is home to some of the oldest college rugby teams in the United States. Although these teams are not "varsity" sports, they compete annually in the Ivy Rugby Conference.

The table above includes the number of team championships won from the beginning of official Ivy League competition (1956–57 academic year) through 2016–17. Princeton and Harvard have on occasion won ten or more Ivy League titles in a year, an achievement accomplished 10 times by Harvard and 24 times by Princeton, including a conference-record 15 championships in 2010–11. Only once has one of the other six schools earned more than eight titles in a single academic year (Cornell with nine in 2005–06). In the 38 academic years beginning 1979–80, Princeton has averaged 10 championships per year, one-third of the conference total of 33 sponsored sports.
In the 12 academic years beginning 2005–06 Princeton has won championships in 31 different sports, all except wrestling and men's tennis.

Rivalries run deep in the Ivy League. For instance, Princeton and Penn are longstanding men's basketball rivals; "Puck Frinceton" T-shirts are worn by Quaker fans at games. In only 11 instances in the history of Ivy League basketball, and in only seven seasons since Yale's 1962 title, has neither Penn nor Princeton won at least a share of the Ivy League title in basketball, with Princeton champion or co-champion 26 times and Penn 25 times. Penn has won 21 outright, Princeton 19 outright. Princeton has been a co-champion 7 times, sharing 4 of those titles with Penn (these 4 seasons represent the only times Penn has been co-champion). Harvard won its first title of either variety in 2011, losing a dramatic play-off game to Princeton for the NCAA tournament bid, then rebounded to win outright championships in 2012, 2013, and 2014. Harvard also won the 2013 Great Alaska Shootout, defeating TCU to become the only Ivy League school to win the now-defunct tournament.

Rivalries exist between other Ivy league teams in other sports, including Cornell and Harvard in hockey, Harvard and Princeton in swimming, and Harvard and Penn in football (Penn and Harvard have won 28 Ivy League Football Championships since 1982, Penn-16; Harvard-12). During that time Penn has had 8 undefeated Ivy League Football Championships and Harvard has had 6 undefeated Ivy League Football Championships. In men's lacrosse, Cornell and Princeton are perennial rivals, and they are two of three Ivy League teams to have won the NCAA tournament. In 2009, the Big Red and Tigers met for their 70th game in the NCAA tournament. No team other than Harvard or Princeton has won the men's swimming conference title outright since 1972, although Yale, Columbia, and Cornell have shared the title with Harvard and Princeton during this time. Similarly, no program other than Princeton and Harvard has won the women's swimming championship since Brown's 1999 title. Princeton or Cornell has won every indoor and outdoor track and field championship, both men's and women's, every year since 2002–03, with one exception (Columbia women won the indoor championship in 2012). Harvard and Yale are football and crew rivals although the competition has become unbalanced; Harvard has won all but one of the last 15 football games and all but one of the last 13 crew races.

The Yale–Princeton series is the nation's second-longest by games played, exceeded only by "The Rivalry" between Lehigh and Lafayette, which began later in 1884 but included two or three games in each of 17 early seasons. For the first three decades of the Yale-Princeton rivalry, the two played their season-ending game at a neutral site, usually New York City, and with one exception (1890: Harvard), the winner of the game also won at least a share of the national championship that year, covering the period 1869 through 1903. This phenomenon of a finale contest at a neutral site for the national title created a social occasion for the society elite of the metropolitan area akin to a Super Bowl in the era prior to the establishment of the NFL in 1920. These football games were also financially profitable for the two universities, so much that they began to play baseball games in New York City as well, drawing record crowds for that sport also, largely from the same social demographic. In a period when the only professional team sports were fledgling baseball leagues, these high-profile early contests between Princeton and Yale played a role in popularizing spectator sports, demonstrating their financial potential and raising public awareness of Ivy universities at a time when few people attended college.

This list, which is current through July 1, 2015, includes NCAA championships and women's AIAW championships (one each for Yale and Dartmouth). Excluded from this list are all other national championships earned outside the scope of NCAA competition, including football titles and retroactive Helms Foundation titles.

The term "Ivy" is sometimes used to connote a positive comparison to or an association with the Ivy League, often along academic lines. The term has been used to describe the Little Ivies, a grouping of small liberal arts colleges in the Northeastern United States. Other common uses include the Public Ivies, the Hidden Ivies, and the Southern Ivies.

The term "Ivy Plus" is sometimes used to refer to the Ancient Eight plus several other schools for purposes of alumni associations, university consortia, or endowment comparisons. In his book "Untangling the Ivy League", Zawel writes, "The inclusion of non-Ivy League schools under this term is commonplace for some schools and extremely rare for others. Among these other schools, Massachusetts Institute of Technology and Stanford University are almost always included. The University of Chicago and Duke University are often included as well."



</doc>
<doc id="14976" url="https://en.wikipedia.org/wiki?curid=14976" title="Ithaca Hours">
Ithaca Hours

The Ithaca HOUR is a local currency formerly used in Ithaca, New York and was one of the longest-running local currency systems, though it is now no longer in circulation . It has inspired other similar systems in Madison, Wisconsin; Santa Barbara, California,
Corvallis, Oregon; and a proposed system in the Lehigh Valley, Pennsylvania. One Ithaca HOUR is valued at US$10 and is generally recommended to be used as payment for one hour's work, although the rate is negotiable.

Ithaca HOURS are not backed by national currency and cannot be freely converted to national currency, although some businesses may agree to buy them.
HOURS are printed on high-quality paper and use faint graphics that would be difficult to reproduce, and each bill is stamped with a serial number, in order to discourage counterfeiting.

In 2002, a one-tenth hour bill was introduced, partly due to the encouragement and funding from Alternatives Federal Credit Union and feedback from retailers who complained about the awkwardness of only having larger denominations to work with; the bills bear the signatures of both HOURS president Steve Burke and the president of AFCU.

Although Ithaca HOUR notes can be found, in recent years it has fallen into disuse, which can be attributed to several reasons. First was that the founder of the system, Paul Glover, moved out of the area. While in Ithaca, Glover had acted as an evangelist and networker for HOURS, helping spread their use and helping businesses find ways to spend HOURS they had received. Secondly, the usage of HOURS declined as a result of the general shift away from cash transactions towards electronic transfers with debit or credit cards. Glover has emphasized that every local currency needs at least one full-time networker to "promote, facilitate and troubleshoot" currency circulation.

Ithaca HOURS were started by Paul Glover in November 1991. The system has historical roots in scrip and alternative and local currencies that proliferated in America during the Great Depression.

While doing research into local economics during 1989, Glover had seen an "Hour" note 19th century British industrialist Robert Owen issued to his workers for spending at his company store. After Ithaca HOURS began, he discovered that Owen's Hours were based on Josiah Warren's "Time Store" notes of 1827.

In May 1991, local student Patrice Jennings interviewed Glover about the Ithaca LETS enterprise. This conversation strongly reinforced his interest in trade systems. Jennings's research on the Ithaca LETS and its failure was integral to the development of the HOUR currency; conversations between Jennings and Glover helped ensure that HOURS used knowledge of what had not worked with the LETS system.

Within a few days, he had designs for the HOUR and Half HOUR notes. He established that each HOUR would be worth the equivalent of $10, which was about the average hourly amount that workers earned in surrounding Tompkins County, although the exact rate of exchange for any given transaction was to be decided by the parties themselves. At GreenStar Cooperative Market, a local food co-op, Glover approached Gary Fine, a local massage therapist, with photocopied samples. Fine became the first person to sign a list formally agreeing to accept HOURS in exchange for services. Soon after, Jim Rohrrsen, the proprietor of a local toy store, became the first retailer to sign-up to accept Ithaca HOURS in exchange for merchandise.

When the system was first started, 90 people agreed to accept HOURS as pay for their services. They all agreed to accept HOURS despite the lack of a business plan or guarantee. Glover then began to ask for small donations to help pay for printing HOURS.

Fine Line Printing completed the first run of 1,500 HOURS and 1,500 Half HOURS in October 1991. These notes, the first modern local currency, were nearly twice as large as the current Ithaca HOURS. Because they didn't fit well in people's wallets, almost all of the original notes have been removed from circulation.

The first issue of Ithaca Money was printed at Our Press, a printing shop in Chenango Bridge, New York, on October 16, 1991. The next day Glover issued 10 HOURS to Ithaca Hours, the organization he founded to run the system, as the first of four reimbursements for the cost of printing HOURS. The day after that, October 18, 1991, 382 HOURS were disbursed and prepared for mailing to the first 93 pioneers.

On October 19, 1991, Glover bought a samosa from Catherine Martinez at the Farmers' Market with Half HOUR #751—the first use of an HOUR. Several other Market vendors enrolled that day. During the next years more than a thousand individuals enrolled to accept HOURS, plus 500 businesses.

Stacks of the Ithaca Money newspaper were distributed all over town with an invitation to "join the fun."

A Barter Potluck was held at GIAC on November 12, 1991, the first of many monthly gatherings where food and skills were exchanged, acquaintances made, and friendships renewed.

In 1996, Glover was running the Ithaca Hours system from his home, and the system had an advisory board and a governing board called the "Barter Potluck". The board and Glover put forth the idea that economic interactions should be based on harmony rather than on more Hobbesian forms of competition. In one interview, Glover stated that "There's a growing movement called "ecological economics" and Ithaca HOURS is part of that cosmos. Last year I wrote an article which discusses moving us toward the provision of food, fuel, clothing, housing, transportation, [and other] necessities in ways which are healing of nature, or which are less depleting at least and which bring people together on the basis of their shared pride, not arrogance." Thus one underlying principle of the local currency movement is to create "fair trade" with a minimum of conflict or exploitation of either people or natural resources.

The Advisory Board incorporated the Ithaca HOUR system as Ithaca Hours, Inc. in October 1998, and hosted the first elections for Board of Directors in March 1999. The first Board of Directors included Monica Hargraves, Dan Cogan, Margaret McCasland, Erica Van Etten, Greg Spence Wolf, Bob LeRoy, LeGrace Benson, Wally Woods, Jennifer Elges, and Donald Stephenson. In May 1999 Glover turned the administration of Ithaca HOURS over to the newly elected Board of Directors. Glover has continued to support Ithaca Hours through community outreach to present, most notably through the Ithaca Health Fund (now incorporated as part
of the Ithaca Health Alliance) and Ithaca Community News.

The current Board of Directors, 2014-2015, includes Erik Lehmann (Chair), Danielle Klock, and Bob LeRoy.

Several million dollars value of HOURS have been traded since 1991 among thousands of residents and over 500 area businesses, including the Cayuga Medical Center, Alternatives Federal Credit Union, the public library, many local farmers, movie theatres, restaurants, healers, plumbers, carpenters, electricians, and landlords.

One of the primary functions of the Ithaca Hours system is to promote local economic development. Businesses who receive Hours must spend them on local goods and services, thus building a network of inter-supporting local businesses. While non-local businesses are welcome to accept Hours, those businesses need to spend them on local goods and services to be economically sustainable.

In their mission to promote local economic development, the Board of Directors also makes interest-free loans of Ithaca HOURS to local businesses and grants to local non-profit organizations.




</doc>
<doc id="14979" url="https://en.wikipedia.org/wiki?curid=14979" title="Interstellar cloud">
Interstellar cloud

An interstellar cloud is generally an accumulation of gas, plasma, and dust in our and other galaxies. Put differently, an interstellar cloud is a denser-than-average region of the interstellar medium, (ISM), the matter and radiation that exists in the space between the star systems in a galaxy. Depending on the density, size, and temperature of a given cloud, its hydrogen can be neutral, making an H I region; ionized, or plasma making it an H II region; or molecular, which are referred to simply as molecular clouds, or sometime dense clouds. Neutral and ionized clouds are sometimes also called "diffuse clouds". An interstellar cloud is formed by the gas and dust particles from a red giant in its later life.

The chemical composition of interstellar clouds is determined by studying electromagnetic radiation or EM radiation that they emanate, and we receive – from radio waves through visible light, to gamma rays on the electromagnetic spectrum – that we receive from them. Large radio telescopes scan the intensity in the sky of particular frequencies of electromagnetic radiation which are characteristic of certain molecules' spectra. Some interstellar clouds are cold and tend to give out EM radiation of large wavelengths. A map of the abundance of these molecules can be made, enabling an understanding of the varying composition of the clouds. In hot clouds, there are often ions of many elements, whose spectra can be seen in visible and ultraviolet light.

Radio telescopes can also scan over the frequencies from one point in the map, recording the intensities of each type of molecule. Peaks of frequencies mean that an abundance of that molecule or atom is present in the cloud. The height of the peak is proportional to the relative percentage that it makes up.

Until recently the rates of reactions in interstellar clouds were expected to be very slow, with minimal products being produced due to the low temperature and density of the clouds. However, organic molecules were observed in the spectra that scientists would not have expected to find under these conditions, such as formaldehyde, methanol, and vinyl alcohol. The reactions needed to create such substances are familiar to scientists only at the much higher temperatures and pressures of earth and earth-based laboratories. The fact that they were found indicates that these chemical reactions in interstellar clouds take place faster than suspected, likely in gas-phase reactions unfamiliar to organic chemistry as observed on earth. These reactions are studied in the CRESU experiment.

Interstellar clouds also provide a medium to study the presence and proportions of metals in space. The presence and ratios of these elements may help develop theories on the means of their production, especially when their proportions are inconsistent with those expected to arise from stars as a result of fusion and thereby suggest alternate means, such as cosmic ray spallation.

These interstellar clouds possess a velocity higher than can be explained by the rotation of the Milky Way. By definition, these clouds must have a v greater than 90 km s, where v is the local standard rest velocity. They are detected primarily in the 21 cm line of neutral hydrogen, and typically have a lower portion of heavy elements than is normal for interstellar clouds in the Milky Way.

Theories intended to explain these unusual clouds include materials left over from the formation of the galaxy, or tidally-displaced matter drawn away from other galaxies or members of the Local Group. An example of the latter is the Magellanic Stream. To narrow down the origin of these clouds, a better understanding of their distances and metallicity is needed.

High-velocity clouds are identified with an HVC prefix, as with HVC 127-41-330.




</doc>
<doc id="14980" url="https://en.wikipedia.org/wiki?curid=14980" title="Imhotep">
Imhotep

Imhotep (; "the one who comes in peace"; fl. late 27th century BC) was an Egyptian chancellor to the Pharaoh Djoser, probable architect of the Djoser's step pyramid, and high priest of the sun god Ra at Heliopolis. Very little is known of Imhotep as a historical figure, but in the 3,000 years following his death, he was gradually glorified and deified.

Traditions from long after Imhotep's death treated him as a great author of wisdom texts and especially as a physician. No text from his lifetime mentions these capacities and no text mentions his name in the first 1,200 years following his death. Apart from the three short contemporary inscriptions that establish him as chancellor to the Pharaoh, the first text to reference Imhotep dates to the time of Amenhotep III (c. 1391–1353 BC). It is addressed to the owner of a tomb, and reads:

It appears that this libation to Imhotep was done regularly, as they are attested on papyri associated with statues of Imhotep until the Late Period (c. 664–332 BC). To Wildung, this cult holds its origin in the slow evolution of the memory of Imhotep among intellectuals from his death onward. To Alan Gardiner, this cult is so distinct from the offerings usually made to commoners that the epithet of "demi-god" is likely justified to describe the way Imhotep was venerated in the New Kingdom (c. 1550–1077 BC).

The first references to the healing abilities of Imhotep occur from the Thirtieth Dynasty (c. 380–343 BC) onward, some 2,200 years after his death.

Imhotep is among the few non-royal Egyptians who were deified after their death, and until the 21st century, he was thought to be only one of two commonersalong with Amenhotep, son of Haputo achieve this status. The center of his cult was in Memphis. The location of his tomb remains unknown, despite efforts to find it. The consensus is that it is hidden somewhere at Saqqara.

Imhotep's historicity is confirmed by two contemporary inscriptions made during his lifetime on the base or pedestal of one of Djoser's statues (Cairo JE 49889) and also by a graffito on the enclosure wall surrounding Sekhemkhet's unfinished step pyramid. The latter inscription suggests that Imhotep outlived Djoser by a few years and went on to serve in the construction of Pharaoh Sekhemkhet's pyramid, which was abandoned due to this ruler's brief reign.

Imhotep was one of the chief officials of the Pharaoh Djoser. Egyptologists ascribe to him the design of the Pyramid of Djoser, a step pyramid at Saqqara built during the 3rd Dynasty. He may also have been responsible for the first known use of stone columns to support a building. Despite these later attestations, the pharaonic Egyptians themselves never credited Imhotep as the designer of the stepped pyramid nor with the invention of stone architecture.

Two thousand years after his death, Imhotep's status had risen to that of a god of medicine and healing. He was eventually equated with Thoth, the god of architecture, mathematics, and medicine, and patron of scribes: Imhotep's cult had merged with that of his former tutelary god.

He was revered in the region of Thebes as the "brother" of Amenhotep, son of Hapu, another deified architect, in the temples dedicated to Thoth. Imhotep was also linked to Asklepios by the Greeks.

According to myth, Imhotep's mother was a mortal named Kheredu-ankh, she too being eventually revered as a demi-goddess as the daughter of Banebdjedet. Alternatively, since Imhotep was known as the "Son of Ptah", his mother was sometimes claimed to be Sekhmet, the patron of Upper Egypt whose consort was Ptah.

The Upper Egyptian Famine Stela, which dates from the Ptolemaic period (305–30 BC), bears an inscription containing a legend about a famine lasting seven years during the reign of Djoser. Imhotep is credited with having been instrumental in ending it. One of his priests explained the connection between the god Khnum and the rise of the Nile to the Pharaoh, who then had a dream in which the Nile god spoke to him, promising to end the drought.

A demotic papyrus from the temple of Tebtunis, dating to the 2nd century AD, preserves a long story about Imhotep.The Pharaoh Djoser plays a prominent role in the story, which also mentions Imhotep's family; his father the god Ptah, his mother Khereduankh, and his younger sister Renpetneferet. At one point Djoser desires Renpetneferet, and Imhotep disguises himself and tries to rescue her. The text also refers to the royal tomb of Djoser. Part of the legend includes an anachronistic battle between the Old Kingdom and the Assyrian armies where Imhotep fights an Assyrian sorceress in a duel of magic.

As an instigator of Egyptian culture, Imhotep's idealized image lasted well into the Roman period. In the Ptolemaic period, the Egyptian priest and historian Manetho credited him with inventing the method of a stone-dressed building during Djoser's reign, though he was not the first to actually build with stone. Stone walling, flooring, lintels, and jambs had appeared sporadically during the Archaic Period, though it is true that a building of the size of the step pyramid made entirely out of stone had never before been constructed. Before Djoser, Pharaohs were buried in mastaba tombs.

Egyptologist James Peter Allen states that "The Greeks equated him with their own god of medicine, Asklepios, although ironically there is no evidence that Imhotep himself was a physician."






</doc>
<doc id="14981" url="https://en.wikipedia.org/wiki?curid=14981" title="Ictinus">
Ictinus

Ictinus (; , "Iktinos") was an architect active in the mid 5th century BC. Ancient sources identify Ictinus and Callicrates as co-architects of the Parthenon. He co-wrote a book on the project – which is now lost – in collaboration with Carpion.

Pausanias identifies Ictinus as architect of the Temple of Apollo at Bassae. That temple was Doric on the exterior, Ionic on the interior, and incorporated a Corinthian column, the earliest known, at the center rear of the cella. Sources also identify Ictinus as architect of the Telesterion at Eleusis, a gigantic hall used in the Eleusinian Mysteries.

Pericles also commissioned Ictinus to design the Telesterion (Hall of mystery
) at Eleusis, but his involvement was terminated when Pericles fell from power. Three other architects took over instead. It seems likely that Ictinus's reputation was harmed by his links with the fallen ruler, as he is singled out for condemnation by Aristophanes in his play "The Birds", dated to around 414 BC. It depicts the royal kite or "ictinus" – a play on the architect's name – not as a noble bird of prey but as a scavenger stealing sacrifices from the gods and money from men. As no other classical author describes the bird in this fashion, Aristophanes likely intended it to be a dig at the architect.

The artist Jean Auguste Dominique Ingres painted a scene showing Ictinus together with the lyric poet Pindar. The painting is known as "Pindar and Ictinus" and is exhibited at the National Gallery, London.



</doc>
<doc id="14982" url="https://en.wikipedia.org/wiki?curid=14982" title="Isidore of Miletus">
Isidore of Miletus

Isidore of Miletus (; ; ) was one of the two main Byzantine Greek architects (Anthemius of Tralles was the other) that Emperor Justinian I commissioned to design the cathedral Hagia Sophia in Constantinople from 532 to 537. The creation of an important compilation of Archimedes' works has been attributed to him. The spurious Book XV from Euclid's Elements has been partly attributed to Isidore of Miletus.

Isidore of Miletus was a renowned scientist and mathematician before Emperor Justinian I hired him. Isidorus taught stereometry and physics at the universities, first of Alexandria then of Constantinople, and wrote a commentary on an older treatise on vaulting. Eutocius together with Isidore studied Archimedes' work. Isidore is also renowned for producing the first comprehensive compilation of Archimedes' work, the Archimedes palimpsest survived to the present.

Emperor Justinian I appointed his architects to rebuild the Hagia Sophia following his victory over protesters within the capital city of his Roman Empire, Constantinople. The first basilica was completed in 360 and remodelled from 404 to 415, but had been damaged in 532 in the course of the Nika Riot, “The temple of Sophia, the baths of Zeuxippus, and the imperial courtyard from the Propylaia all the way to the so-called House of Ares were burned up and destroyed, as were both of the great porticoes that lead to the forum that is named after Constantine, houses of prosperous people, and a great deal of other properties.”

The rival factions of Byzantine society, the Blues and the Greens, opposed each other in the chariot races at the Hippodrome and often resorted to violence. During the Nika Riot, more than thirty thousand people died. Emperor Justinian I ensured that his new structure would not be burned down, like its predecessors, by commissioning architects that would build the church mainly out of stone, rather than wood, “He compacted it of baked brick and mortar, and in many places bound it together with iron, but made no use of wood, so that the church should no longer prove combustible.”

Isidore of Miletus and Anthemius of Tralles originally planned on a main hall of the Hagia Sophia that measured 70 by 75 metres (230 x 250 ft), making it the largest church in Constantinople, but the original dome was nearly 6 metres (20 ft) lower than it was constructed, “Justinian suppressed these riots and took the opportunity of marking his victory by erecting in 532-7 the new Hagia Sophia, one of the largest, most lavish, and most expensive buildings of all time.”

Although Isidore of Miletus and Anthemius of Tralles were not formally educated in architecture, they were scientists who could organize the logistics of drawing thousands of labourers and unprecedented loads of rare raw materials from around the Roman Empire to construct the Hagia Sophia for Emperor Justinian I. The finished product was built in admirable form for the Roman Emperor, “All of these elements marvellously fitted together in mid-air, suspended from one another and reposing only on the parts adjacent to them, produce a unified and most remarkable harmony in the work, and yet do not allow the spectators to rest their gaze upon any one of them for a length of time.”

The Hagia Sophia architects innovatively combined the longitudinal structure of a Roman basilica and the central plan of a drum-supported dome, in order to withstand the high magnitude earthquakes of the Marmara Region, “However, in May 558, little more than 20 years after the Church’s dedication, following the earthquakes of August 553 and December 557, parts of the central dome and its supporting structure system collapsed.” The Hagia Sophia was repeatedly cracked by earthquakes and was quickly repaired. Isidore of Miletus’ nephew, Isidore the Younger, introduced the new dome design that can be viewed in the Hagia Sophia in present-day Istanbul, Turkey.

After a great earthquake in 989 ruined the dome of Hagia Sophia, the Byzantine officials summoned Trdat the Architect to Byzantium to organize repairs. The restored dome was completed by 994.



</doc>
<doc id="14984" url="https://en.wikipedia.org/wiki?curid=14984" title="International Atomic Energy Agency">
International Atomic Energy Agency

The International Atomic Energy Agency (IAEA) is an international organization that seeks to promote the peaceful use of nuclear energy, and to inhibit its use for any military purpose, including nuclear weapons. The IAEA was established as an autonomous organisation on 29 July 1957. Though established independently of the United Nations through its own international treaty, the IAEA Statute, the IAEA reports to both the United Nations General Assembly and Security Council.

The IAEA has its headquarters in Vienna, Austria. The IAEA has two "Regional Safeguards Offices" which are located in Toronto, Canada, and in Tokyo, Japan. The IAEA also has two liaison offices which are located in New York City, United States, and in Geneva, Switzerland. In addition, the IAEA has laboratories and research centers located in Seibersdorf, Austria, in Monaco and in Trieste, Italy.

The IAEA serves as an intergovernmental forum for scientific and technical co-operation in the peaceful use of nuclear technology and nuclear power worldwide. The programs of the IAEA encourage the development of the peaceful applications of nuclear energy, science and technology, provide international safeguards against misuse of nuclear technology and nuclear materials, and promote nuclear safety (including radiation protection) and nuclear security standards and their implementation.

The IAEA and its former Director General, Mohamed ElBaradei, were jointly awarded the Nobel Peace Prize on 7 October 2005. The current Director General is Rafael Grossi, an Argentinian diplomat who previously served as an IAEA's chief of cabinet, whose appointment was approved at the special session of the IAEA's General Conference on 2 December 2019, as the successor of Yukiya Amano, who died in July 2019.

In 1953, the President of the United States, Dwight D. Eisenhower, proposed the creation of an international body to both regulate and promote the peaceful use of atomic power (nuclear power), in his Atoms for Peace address to the UN General Assembly. In September 1954, the United States proposed to the General Assembly the creation of an international agency to take control of fissile material, which could be used either for nuclear power or for nuclear weapons. This agency would establish a kind of "nuclear bank."

The United States also called for an international scientific conference on all of the peaceful aspects of nuclear power. By November 1954, it had become clear that the Soviet Union would reject any international custody of fissile material if the United States did not agree to a disarmament first, but that a "clearing house" for nuclear transactions might be possible. From 8 to 20 August 1955, the United Nations held the International Conference on the Peaceful Uses of Atomic Energy in Geneva, Switzerland. In October 1957, a Conference on the IAEA Statute was held at the Headquarters of the United Nations to approve the founding document for the IAEA, which was negotiated in 1955–1957 by a group of twelve countries. The Statute of the IAEA was approved on 23 October 1956 and came into force on 29 July 1957.

Former US Congressman W. Sterling Cole served as the IAEA's first Director General from 1957 to 1961. Cole served only one term, after which the IAEA was headed by two Swedes for nearly four decades: the scientist Sigvard Eklund held the job from 1961 to 1981, followed by former Swedish Foreign Minister Hans Blix, who served from 1981 to 1997. Blix was succeeded as Director General by Mohamed ElBaradei of Egypt, who served until November 2009.

Beginning in 1986, in response to the nuclear reactor explosion and disaster near Chernobyl, Ukraine, the IAEA increased its efforts in the field of nuclear safety. The same happened after the 2011 Fukushima disaster in Fukushima, Japan.

Both the IAEA and its then Director General, ElBaradei, were awarded the Nobel Peace Prize in 2005. In ElBaradei's acceptance speech in Oslo, he stated that only one percent of the money spent on developing new weapons would be enough to feed the entire world, and that, if we hope to escape self-destruction, then nuclear weapons should have no place in our collective conscience, and no role in our security.

On 2 July 2009, Yukiya Amano of Japan was elected as the Director General for the IAEA, defeating Abdul Samad Minty of South Africa and Luis E. Echávarri of Spain. On 3 July 2009, the Board of Governors voted to appoint Yukiya Amano "by acclamation," and IAEA General Conference in September 2009 approved. He took office on 1 December 2009. After Amano's death, his Chief of Coordination Cornel Feruta of Romania was named Acting Director General.

On August 2, 2019, Rafael Grossi was presented as the Argentine candidate to become the Director General of IAEA. On 28 October, 2019, the IAEA Board of Governors held its first vote to elect the new Director General, but none of the candidates secured the two-thirds majority in the 35-member IAEA Board of Governors needed to be elected. The next day, 29 October, the second voting round was held, and Grossi won 24 of the 23 needed votes required for Director General Appointment. He assumed office on 3 December 2019. Following a special meeting of the IAEA General Conference to approve his appointment, on December 3 Grossi became the first Latin American to head the Agency.

The IAEA's mission is guided by the interests and needs of Member States, strategic plans and the vision embodied in the IAEA Statute (see below). Three main pillars – or areas of work – underpin the IAEA's mission: Safety and Security; Science and Technology; and Safeguards and Verification.

The IAEA as an autonomous organisation is not under direct control of the UN, but the IAEA does report to both the UN General Assembly and Security Council. Unlike most other specialised international agencies, the IAEA does much of its work with the Security Council, and not with the United Nations Economic and Social Council. The structure and functions of the IAEA are defined by its founding document, the IAEA Statute (see below). The IAEA has three main bodies: the Board of Governors, the General Conference, and the Secretariat.

The IAEA exists to pursue the "safe, secure and peaceful uses of nuclear sciences and technology" (Pillars 2005). The IAEA executes this mission with three main functions: the inspection of existing nuclear facilities to ensure their peaceful use, providing information and developing standards to ensure the safety and security of nuclear facilities, and as a hub for the various fields of science involved in the peaceful applications of nuclear technology.

The IAEA recognises knowledge as the nuclear energy industry's most valuable asset and resource, without which the industry cannot operate safely and economically. Following the IAEA General Conference since 2002 resolutions the Nuclear Knowledge Management, a formal programme was established to address Member States' priorities in the 21st century.

In 2004, the IAEA developed a Programme of Action for Cancer Therapy (PACT). PACT responds to the needs of developing countries to establish, to improve, or to expand radiotherapy treatment programs. The IAEA is raising money to help efforts by its Member States to save lives and to reduce suffering of cancer victims.

The IAEA has established programs to help developing countries in planning to build systematically the capability to manage a nuclear power program, including the Integrated Nuclear Infrastructure Group, which has carried out Integrated Nuclear Infrastructure Review missions in Indonesia, Jordan, Thailand and Vietnam. The IAEA reports that roughly 60 countries are considering how to include nuclear power in their energy plans.

To enhance the sharing of information and experience among IAEA Member States concerning the seismic safety of nuclear facilities, in 2008 the IAEA established the International Seismic Safety Centre. This centre is establishing safety standards and providing for their application in relation to site selection, site evaluation and seismic design.

The Board of Governors is one of two policy making bodies of the IAEA. The Board consists of 22 member states elected by the General Conference, and at least 10 member states nominated by the outgoing Board. The outgoing Board designates the ten members who are the most advanced in atomic energy technology, plus the most advanced members from any of the following areas that are not represented by the first ten: North America, Latin America, Western Europe, Eastern Europe, Africa, Middle East and South Asia, South East Asia, the Pacific, and the Far East. These members are designated for one year terms. The General Conference elects 22 members from the remaining nations to two-year terms. Eleven are elected each year. The 22 elected members must also represent a stipulated geographic diversity. The 35 Board members for the 2018–2019 period are: Argentina, Armenia, Australia, Azerbaijan, Belgium, Brazil, Canada, Chile, China, Ecuador, Egypt, France, Germany, India, Indonesia, Italy, Japan, Jordan, Kenya, the Republic of Korea, Morocco, the Netherlands, Niger, Pakistan, Portugal, the Russian Federation, Serbia, South Africa, the Sudan, Sweden, Thailand, the United Kingdom of Great Britain and Northern Ireland, the United States of America, Uruguay and the Bolivarian Republic of Venezuela.

The Board, in its five yearly meetings, is responsible for making most of the policy of the IAEA. The Board makes recommendations to the General Conference on IAEA activities and budget, is responsible for publishing IAEA standards and appoints the Director General subject to General Conference approval. Board members each receive one vote. Budget matters require a two-thirds majority. All other matters require only a simple majority. The simple majority also has the power to stipulate issues that will thereafter require a two-thirds majority. Two-thirds of all Board members must be present to call a vote. The Board elects its own chairman.

The General Conference is made up of all 171 member states. It meets once a year, typically in September, to approve the actions and budgets passed on from the Board of Governors. The General Conference also approves the nominee for Director General and requests reports from the Board on issues in question (Statute). Each member receives one vote. Issues of budget, Statute amendment and suspension of a member's privileges require a two- thirds majority and all other issues require a simple majority. Similar to the Board, the General Conference can, by simple majority, designate issues to require a two- thirds majority. The General Conference elects a President at each annual meeting to facilitate an effective meeting. The President only serves for the duration of the session (Statute).

The main function of the General Conference is to serve as a forum for debate on current issues and policies. Any of the other IAEA organs, the Director General, the Board and member states can table issues to be discussed by the General Conference (IAEA Primer). This function of the General Conference is almost identical to the General Assembly of the United Nations.

The Secretariat is the professional and general service staff of the IAEA. The Secretariat is headed by the Director General. The Director General is responsible for enforcement of the actions passed by the Board of Governors and the General Conference. The Director General is selected by the Board and approved by the General Conference for renewable four-year terms. The Director General oversees six departments that do the actual work in carrying out the policies of the IAEA: Nuclear Energy, Nuclear Safety and Security, Nuclear Sciences and Applications, Safeguards, Technical Cooperation, and Management.

The IAEA budget is in two parts. The regular budget funds most activities of the IAEA and is assessed to each member nation (€344 million in 2014). The Technical Cooperation Fund is funded by voluntary contributions with a general target in the US$90 million range.

The IAEA is generally described as having three main missions:

According to Article II of the IAEA Statute, the objective of the IAEA is "to accelerate and enlarge the contribution of atomic energy to peace, health and prosperity throughout the world." Its primary functions in this area, according to Article III, are to encourage research and development, to secure or provide materials, services, equipment and facilities for Member States, to foster exchange of scientific and technical information and training.

Three of the IAEA's six Departments are principally charged with promoting the peaceful uses of nuclear energy. The Department of Nuclear Energy focuses on providing advice and services to Member States on nuclear power and the nuclear fuel cycle. The Department of Nuclear Sciences and Applications focuses on the use of non-power nuclear and isotope techniques to help IAEA Member States in the areas of water, energy, health, biodiversity, and agriculture. The Department of Technical Cooperation provides direct assistance to IAEA Member States, through national, regional, and inter-regional projects through training, expert missions, scientific exchanges, and provision of equipment.

Article II of the IAEA Statute defines the Agency's twin objectives as promoting peaceful uses of atomic energy and "<nowiki>ensur[ing]</nowiki>, so far as it is able, that assistance provided by it or at its request or under its supervision or control is not used in such a way as to further any military purpose." To do this, the IAEA is authorised in Article III.A.5 of the Statute "to establish and administer safeguards designed to ensure that special fissionable and other materials, services, equipment, facilities, and information made available by the Agency or at its request or under its supervision or control are not used in such a way as to further any military purpose; and to apply safeguards, at the request of the parties, to any bilateral or multilateral arrangement, or at the request of a State, to any of that State's activities in the field of atomic energy."

The Department of Safeguards is responsible for carrying out this mission, through technical measures designed to verify the correctness and completeness of states' nuclear declarations.

The IAEA classifies safety as one of its top three priorities. It spends 8.9 percent of its 352 million-euro ($469 million) regular budget in 2011 on making plants secure from accidents. Its resources are used on the other two priorities: technical co-operation and preventing
nuclear weapons proliferation.

The IAEA itself says that, beginning in 1986, in response to the nuclear reactor explosion and disaster near Chernobyl, Ukraine, the IAEA redoubled its efforts in the field of nuclear safety. The IAEA says that the same happened after the Fukushima disaster in Fukushima, Japan.

In June 2011, the IAEA chief said he had "broad support for his plan to strengthen international safety checks on nuclear power plants to help avoid any repeat of Japan's Fukushima crisis". Peer-reviewed safety checks on reactors worldwide, organised by the IAEA, have been proposed.

In 2011, Russian nuclear accident specialist Iouli Andreev was critical of the response to Fukushima, and says that the IAEA did not learn from the 1986 Chernobyl disaster. He has accused the IAEA and corporations of "wilfully ignoring lessons from the world's worst nuclear accident 25 years ago to protect the industry's expansion". The IAEA's role "as an advocate for nuclear power has made it a target for protests".

The journal "Nature" has reported that the IAEA response to the 2011 Fukushima Daiichi nuclear disaster in Japan was "sluggish and sometimes confusing", drawing calls for the agency to "take a more proactive role in nuclear safety". But nuclear experts say that the agency's complicated mandate and the constraints imposed by its member states mean that reforms will not happen quickly or easily, although its INES "emergency scale is very likely to be revisited" given the confusing way in which it was used in Japan.

Some scientists say that the Fukushima nuclear accidents have revealed that the nuclear industry lacks sufficient oversight, leading to renewed calls to redefine the mandate of the IAEA so that it can better police nuclear power plants worldwide. There are several problems with the IAEA says Najmedin Meshkati of University of Southern California:

It recommends safety standards, but member states are not required to comply; it promotes nuclear energy, but it also monitors nuclear use; it is the sole global organisation overseeing the nuclear energy industry, yet it is also weighed down by checking compliance with the Nuclear Non-Proliferation Treaty (NPT).
In 2011, the journal "Nature" reported that the International Atomic Energy Agency should be strengthened to make independent assessments of nuclear safety and that "the public would be better served by an IAEA more able to deliver frank and independent assessments of nuclear crises as they unfold".

The process of joining the IAEA is fairly simple. Normally, a State would notify the Director General of its desire to join, and the Director would submit the application to the Board for consideration. If the Board recommends approval, and the General Conference approves the application for membership, the State must then submit its instrument of acceptance of the IAEA Statute to the United States, which functions as the depositary Government for the IAEA Statute. The State is considered a member when its acceptance letter is deposited. The United States then informs the IAEA, which notifies other IAEA Member States. Signature and ratification of the Nuclear Non-Proliferation Treaty (NPT) are not preconditions for membership in the IAEA.

The IAEA has 171 member states. Most UN members and the Holy See are Member States of the IAEA. Non-member states Cape Verde (2007), Tonga (2011), Comoros (2014) and Gambia (2016) have been approved for membership and will become a Member State if they deposit the necessary legal instruments.

Four states have withdrawn from the IAEA. North Korea was a Member State from 1974 to 1994, but withdrew after the Board of Governors found it in non-compliance with its safeguards agreement and suspended most technical co-operation. Nicaragua became a member in 1957, withdrew its membership in 1970, and rejoined in 1977, Honduras joined in 1957, withdrew in 1967, and rejoined in 2003, while Cambodia joined in 1958, withdrew in 2003, and rejoined in 2009.

There are four regional cooperative areas within IAEA, that share information, and organize conferences within their regions:

The African Regional Cooperative Agreement for Research, Development and Training Related to Nuclear Science and Technology (AFRA):
Cooperative Agreement for Arab States in Asia for Research, Development and Training related to Nuclear Science and Technology (ARASIA):
Regional Cooperative Agreement for Research, Development and Training Related to Nuclear Science and Technology for Asia and the Pacific (RCA):
Cooperation Agreement for the Promotion of Nuclear Science and Technology in Latin America and the Caribbean (ARCAL):
Typically issued in July each year, the IAEA Annual Report summarizes and highlights developments over the past year in major areas of the Agency's work. It includes a summary of major issues, activities, and achievements, and status tables and graphs related to safeguards, safety, and science and technology.



 


</doc>
<doc id="14985" url="https://en.wikipedia.org/wiki?curid=14985" title="International Civil Aviation Organization">
International Civil Aviation Organization

The International Civil Aviation Organization (ICAO; ; ) is a specialized agency of the United Nations. It changes the principles and techniques of international air navigation and fosters the planning and development of international air transport to ensure safe and orderly growth. Its headquarters is located in the "Quartier International" of Montreal, Quebec, Canada.

The ICAO Council adopts standards and recommended practices concerning air navigation, its infrastructure, flight inspection, prevention of unlawful interference, and facilitation of border-crossing procedures for international civil aviation. ICAO defines the protocols for air accident investigation that are followed by in countries signatory to the Chicago Convention on International Civil Aviation.

The Air Navigation Commission (ANC) is the technical body within ICAO. The Commission is composed of 19 Commissioners, nominated by the ICAO's contracting states and appointed by the ICAO Council. Commissioners serve as independent experts, who although nominated by their states, do not serve as state or political representatives. International Standards And Recommended Practices are developed under the direction of the ANC through the formal process of ICAO Panels. Once approved by the Commission, standards are sent to the Council, the political body of ICAO, for consultation and coordination with the Member States before final adoption.

ICAO is distinct from other international air transport organizations, particularly because it alone is vested with international authority (among signatory states): other organizations include the International Air Transport Association (IATA), a trade association representing airlines; the Civil Air Navigation Services Organization (CANSO), an organization for Air navigation service providers (ANSPs); and the Airports Council International, a trade association of airport authorities.

The forerunner to ICAO was the International Commission for Air Navigation (ICAN). It held its first convention in 1903 in Berlin, Germany, but no agreements were reached among the eight countries that attended. At the second convention in 1906, also held in Berlin, twenty-seven countries attended. The third convention, held in London in 1912 allocated the first radio callsigns for use by aircraft. ICAN continued to operate until 1945.

Fifty-two countries signed the Chicago Convention on International Civil Aviation, also known as the Chicago Convention, in Chicago, Illinois, on 7 December 1944. Under its terms, a Provisional International Civil Aviation Organization was to be established, to be replaced in turn by a permanent organization when twenty-six countries ratified the convention. Accordingly, PICAO began operating on 6 June 1945, replacing ICAN. The twenty-sixth country ratified the Convention on 5 March 1947 and, consequently, PICAO was disestablished on 4 April 1947 and replaced by ICAO, which began operations the same day. In October 1947, ICAO became an agency of the United Nations under its Economic and Social Council (ECOSOC).
In April 2013, Qatar offered to serve as the new permanent seat of the Organization. Qatar promised to construct a massive new headquarters for ICAO and to cover all moving expenses, stating that Montreal "was too far from Europe and Asia", "had cold winters", was hard to attend due to the Canadian government's slow issuance of visas, and that the taxes imposed on ICAO by Canada were too high. According to "The Globe and Mail", Qatar's invitation was at least partly motivated by the pro-Israel foreign policy of Canadian Prime Minister Stephen Harper. Approximately one month later, Qatar withdrew its bid after a separate proposal to the ICAO's governing council to move the ICAO triennial conference to Doha was defeated by a vote of 22–14.

In January 2020, ICAO blocked a number of Twitter users—among them think-tank analysts, employees of the United States Congress, and journalists—who mentioned Taiwan in tweets related to ICAO. Many of the tweets concerned the COVID-19 pandemic and Taiwan's exclusion from ICAO safety and health bulletins due to Chinese pressure.

In response to questions from reporters, ICAO issued a tweet stating that publishers of "irrelevant, compromising and offensive material" would be "precluded". Since that action the organization has followed a policy of blocking anyone asking about it. The United States House Committee on Foreign Affairs harshly criticized ICAO's perceived failure to uphold principles of fairness, inclusion, and transparency by silencing non-disruptive opposing voices. Senator Marco Rubio also criticized the move. The Ministry of Foreign Affairs (Taiwan) and Taiwanese legislators criticized the move with MOFA head Jaushieh Joseph Wu tweeting in support of those blocked.

Anthony Philbin, chief of communications of the ICAO Secretary General, rejected criticism of ICAO's handling of the situation: "We felt we were completely warranted in taking the steps we did to defend the integrity of the information and discussions our followers should reasonably expect from our feeds." In exchanges with International Flight Network, Philbin refused to acknowledge the existence of Taiwan.

On February 1, the US State Department issued a press release which heavily criticized ICAO's actions, characterizing them as "outrageous, unacceptable, and not befitting of a UN organization."

The 9th edition of the Convention on International Civil Aviation includes modifications from years 1948 up to 2006. ICAO refers to its current edition of the Convention as the "Statute" and designates it as ICAO Document 7300/9. The Convention has 19 Annexes that are listed by title in the article Convention on International Civil Aviation.

, there are 193 ICAO members, consisting of 192 of the 193 UN members (all but Liechtenstein, which lacks an international airport), plus the Cook Islands.

Despite Liechtenstein not being a direct party to ICAO, its government has delegated Switzerland to enter into the treaty on its behalf, and the treaty applies in the territory of Liechtenstein.

The Republic of China (Taiwan) was a founding member of ICAO but was replaced by the People's Republic of China as the legal representative of China in 1971 and as such, did not take part in the organization. In 2013, Taiwan was for the first time invited to attend the ICAO Assembly, at its 38th session, as a guest under the name of Chinese Taipei. , it has not been invited to participate again, due to renewed PRC pressure. Host government, Canada, supports Taiwan's inclusion in ICAO. Support also comes from Canada's commercial sector with the president of the Air Transport Association of Canada saying in 2019 that "It's about safety in aviation so from a strictly operational and non-political point of view, I believe Taiwan should be there."

The Council of ICAO is elected by the Assembly every 3 years and consists of 36 members elected in 3 groups. The present Council was elected in October 2019. 
The structure of the present Council is as follows:

ICAO also standardizes certain functions for use in the airline industry, such as the Aeronautical Message Handling System (AMHS). This makes it a standards organization.

Each country should have an accessible Aeronautical Information Publication (AIP), based on standards defined by ICAO, containing information essential to air navigation. Countries are required to update their AIP manuals every 28 days and so provide definitive regulations, procedures and information for each country about airspace and airports. ICAO's standards also dictate that temporary hazards to aircraft must be regularly published using NOTAMs.

ICAO defines an International Standard Atmosphere (also known as ICAO Standard Atmosphere), a model of the standard variation of pressure, temperature, density, and viscosity with altitude in the Earth's atmosphere. This is useful in calibrating instruments and designing aircraft. The standardized pressure is also used in calibrating instruments in-flight, particularly above the transition altitude.

ICAO is active in infrastructure management, including communication, navigation and surveillance / air traffic management (CNS/ATM) systems, which employ digital technologies (like satellite systems with various levels of automation) in order to maintain a seamless global air traffic management system.

ICAO has published standards for machine-readable passports. Machine-readable passports have an area where some of the information otherwise written in textual form is also written as strings of alphanumeric characters, printed in a manner suitable for optical character recognition. This enables border controllers and other law enforcement agents to process such passports more quickly, without having to enter the information manually into a computer. ICAO's technical standard for machine-readable passports is contained in Document 9303 "Machine Readable Travel Documents".

A more recent standard covers biometric passports. These contain biometrics to authenticate the identity of travellers. The passport's critical information is stored on a tiny RFID computer chip, much like information stored on smart cards. Like some smart cards, the passport book design calls for an embedded contactless chip that is able to hold digital signature data to ensure the integrity of the passport and the biometric data.

Both ICAO and IATA have their own airport and airline code systems.

ICAO uses 4-letter airport codes (vs. IATA's 3-letter codes). The ICAO code is based on the region and country of the airport—for example, Charles de Gaulle Airport has an ICAO code of LFPG, where "L" indicates Southern Europe, "F", France, "PG", Paris de Gaulle, while Orly Airport has the code LFPO (the 3rd letter sometimes refers to the particular flight information region (FIR) or the last two may be arbitrary). In most parts of the world, ICAO and IATA codes are unrelated; for example, Charles de Gaulle Airport has an IATA code of CDG. However, the location prefix for continental United States is "K", and ICAO codes are usually the IATA code with this prefix. For example, the ICAO code for Los Angeles International Airport is KLAX. Canada follows a similar pattern, where a prefix of "C" is usually added to an IATA code to create the ICAO code. For example, Calgary International Airport is YYC or CYYC. (In contrast, airports in Hawaii are in the Pacific region and so have ICAO codes that start with "PH"; Kona International Airport's code is PHKO. Similarly, airports in Alaska have ICAO codes that start with "PA". Merrill Field, for instance is PAMR.) Note that not all airports are assigned codes in both systems; for example, airports that do not have airline service do not need an IATA code.

ICAO also assigns 3-letter airline codes (versus the more-familiar 2-letter IATA codes—for example, "UAL" vs. "UA" for United Airlines). ICAO also provides telephony designators to aircraft operators worldwide, a one- or two-word designator used on the radio, usually, but not always, similar to the aircraft operator name. For example, the identifier for Japan Airlines International is "JAL" and the designator is "Japan Air", but Aer Lingus is "EIN" and "Shamrock". Thus, a Japan Airlines flight numbered 111 would be written as "JAL111" and pronounced "Japan Air One One One" on the radio, while a similarly numbered Aer Lingus would be written as "EIN111" and pronounced "Shamrock One One One". In the US, FAA practices require the digits of the flight number to be spoken in group format ("Japan Air One Eleven" in the above example) while individual digits are used for the aircraft tail number used for unscheduled civil flights.

ICAO maintains the standards for aircraft registration ("tail numbers"), including the alphanumeric codes that identify the country of registration. For example, airplanes registered in the United States have tail numbers starting with "N".

ICAO is also responsible for issuing 2-4 character alphanumeric "aircraft type designators" for those aircraft types which are most commonly provided with air traffic service. These codes provide an abbreviated aircraft type identification, typically used in flight plans. For example, the Boeing 747-100, -200 and -300 are given the type designators "B741", "B742" and "B743" respectively.

ICAO recommends a unification of units of measurement within aviation based on the International System of Units (SI). Technically this makes SI units preferred, but in practice the following non-SI units are still in widespread use within commercial aviation:


Knots, nautical miles and feet have been permitted for temporary use since 1979, but a termination date has not yet been established, which would complete metrication of worldwide aviation. Since 2010, ICAO recommends using:


Notably, aviation in Russia, Sweden and China currently use km/h for reporting airspeed, and many present-day European glider planes also indicate airspeed in kilometres per hour. Sweden, China and North Korea use metres for reporting altitude when communicating with pilots. Russia also formerly used metres exclusively for reporting altitude, but in 2011 changed to feet for high altitude flight. From February 2017, Russian airspace started transitioning to reporting altitude in feet only. Runway lengths are now commonly given in metres worldwide, except in North America where feet are commonly used.

The table below summarizes some of the units commonly used in flight and ground operations, as well as their recommended replacement. A full list of recommended units can be found in annex 5 to the Convention on International Civil Aviation.

 Altitude, elevation, height.

ICAO has a headquarters, seven regional offices, and one regional sub-office:

Emissions from international aviation are specifically excluded from the targets agreed under the Kyoto Protocol. Instead, the Protocol invites developed countries to pursue the limitation or reduction of emissions through the International Civil Aviation Organization. ICAO's environmental committee continues to consider the potential for using market-based measures such as trading and charging, but this work is unlikely to lead to global action. It is currently developing guidance for states who wish to include aviation in an emissions trading scheme (ETS) to meet their Kyoto commitments, and for airlines who wish to participate voluntarily in a trading scheme.

Emissions from domestic aviation are included within the Kyoto targets agreed by countries. This has led to some national policies such as fuel and emission taxes for domestic air travel in the Netherlands and Norway, respectively. Although some countries tax the fuel used by domestic aviation, there is no duty on kerosene used on international flights.

ICAO is currently opposed to the inclusion of aviation in the European Union Emission Trading Scheme (EU ETS). The EU, however, is pressing ahead with its plans to include aviation.

On 6 October 2016, the ICAO finalized an agreement among its 191 member nations to address the more than of carbon dioxide emitted annually by international passenger and cargo flights. The agreement will use an offsetting scheme called CORSIA (the Carbon Offsetting and Reduction Scheme for International Aviation) under which forestry and other carbon-reducing activities are directly funded, amounting to about 2% of annual revenues for the sector. Rules against 'double counting' should ensure that existing forest protection efforts are not recycled. The scheme does not take effect until 2021 and will be voluntary until 2027, but many countries, including the US and China, have promised to begin at its 2020 inception date. Under the agreement, the global aviation emissions target is a 50% reduction by 2050 relative to 2005. NGO reaction to the deal was mixed.

The agreement has critics. It is not aligned with the 2015 Paris climate agreement, which set the objective of restricting global warming to 1.5 to 2 °C. A late draft of the agreement would have required the air transport industry to assess its share of global carbon budgeting to meet that objective, but the text was removed in the agreed version. CORSIA will regulate only about 25 percent of aviation's international emissions, since it grandfathers all emissions below the 2020 level, allowing unregulated growth until then. Only 65 nations will participate in the initial voluntary period, not including significant emitters Russia, India and perhaps Brazil. The agreement does not cover domestic emissions, which are 40% of the global industry's overall emissions. One observer of the ICAO convention made this summary: although another critic called it "a timid step in the right direction."

Most air accident investigations are carried out by an agency of a country that is associated in some way with the accident. For example, the Air Accidents Investigation Branch conducts accident investigations on behalf of the British Government. ICAO has conducted four investigations involving air disasters, of which two were passenger airliners shot down while in international flight over hostile territory.

ICAO is looking at having a singular ledger for drone registration to help law enforcement globally. Currently, ICAO is responsible for creating drone regulations across the globe, and it is expected that it will only maintain the registry. This activity is seen as a forerunner to global regulations on flying drones under the auspices of the ICAO.

ICAO currently maintains the 'UAS Regulation Portal' for various countries to list their country's UAS regulations and also review the best practices from across the globe.




</doc>
<doc id="14986" url="https://en.wikipedia.org/wiki?curid=14986" title="International Maritime Organization">
International Maritime Organization

The International Maritime Organization (IMO) (French: "Organisation Maritime Internationale" (OMI), known as the Inter-Governmental Maritime Consultative Organization (IMCO) until 1982, is a specialised agency of the United Nations responsible for regulating shipping. The IMO was established following agreement at a UN conference held in Geneva in 1948 and the IMO came into existence ten years later, meeting for the first time in 1959. Headquartered in London, United Kingdom, the IMO currently has 174 member states and three associate members.

The IMO's primary purpose is to develop and maintain a comprehensive regulatory framework for shipping and its remit today includes safety, environmental concerns, legal matters, technical co-operation, maritime security and the efficiency of shipping. IMO is governed by an assembly of members and is financially administered by a council of members elected from the assembly. The work of IMO is conducted through five committees and these are supported by technical subcommittees. Other UN organisations may observe the proceedings of the IMO. Observer status is granted to qualified non-governmental organisations.

IMO is supported by a permanent secretariat of employees who are representative of the organisation's members. The secretariat is composed of a Secretary-General who is periodically elected by the assembly, and various divisions such as those for marine safety, environmental protection and a conference section.

Inter-Governmental Maritime Consultative Organization (IMCO) was formed in order to bring the regulation of the safety of shipping into an international framework, for which the creation of the United Nations provided an opportunity. Hitherto such international conventions had been initiated piecemeal, notably the Safety of Life at Sea Convention (SOLAS), first adopted in 1914 following the "Titanic" disaster. IMCO's first task was to update that convention; the resulting 1960 convention was subsequently recast and updated in 1974 and it is that convention that has been subsequently modified and updated to adapt to changes in safety requirements and technology.

When IMCO began its operations in 1959 certain other pre-existing conventions were brought under its aegis, most notable the International Convention for the Prevention of Pollution of the Sea by Oil (OILPOL) 1954. The first meetings of the newly formed IMCO were held in London in 1959. Throughout its existence IMCO, later renamed the IMO in 1982, has continued to produce new and updated conventions across a wide range of maritime issues covering not only safety of life and marine pollution but also encompassing safe navigation, search and rescue, wreck removal, tonnage measurement, liability and compensation, ship recycling, the training and certification of seafarers, and piracy. More recently SOLAS has been amended to bring an increased focus on maritime security through the International Ship and Port Facility Security (ISPS) Code. The IMO has also increased its focus on smoke emissions from ships.

In January 1959, IMO began to maintain and promote the 1954 OILPOL Convention. Under the guidance of IMO, the convention was amended in 1962, 1969, and 1971.

As oil trade and industry developed, many people in the industry began to recognise a need for further improvements in regards to oil pollution prevention at sea. This became increasingly apparent in 1967, when the tanker "Torrey Canyon" spilled 120,000 tons of crude oil when it ran aground entering the English Channel

The "Torrey Canyon" grounding was the largest oil pollution incident recorded up to that time. This incident prompted a series of new conventions.

IMO held an emergency session of its Council to deal with the need to readdress regulations pertaining to maritime pollution. In 1969, the IMO Assembly decided to host an international gathering in 1973 dedicated to this issue. The goal at hand was to develop an international agreement for controlling general environmental contamination by ships when out at sea.

During the next few years IMO brought to the forefront a series of measures designed to prevent large ship accidents and to minimise their effects. It also detailed how to deal with the environmental threat caused by routine ship duties such as the cleaning of oil cargo tanks or the disposal of engine room wastes. By tonnage, the aforementioned was a bigger problem than accidental pollution.

The most significant thing to come out of this conference was the International Convention for the Prevention of Pollution from Ships, 1973. It covers not only accidental and operational oil pollution but also different types of pollution by chemicals, goods in packaged form, sewage, garbage and air pollution.

The original MARPOL was signed on 17 February 1973, but did not come into force due to lack of ratifications. The current convention is a combination of 1973 Convention and the 1978 Protocol. It entered into force on 2 October 1983. As of May 2013, 152 states, representing 99.2 per cent of the world's shipping tonnage, are involved in the convention.

In 1983 the IMO established the World Maritime University in Malmö, Sweden.

The IMO headquarters are located in a large purpose-built building facing the River Thames on the Albert Embankment, in Lambeth, London. The organisation moved into its new headquarters in late 1982, with the building being officially opened by Queen Elizabeth II on 17 May 1983. The architects of the building were Douglass Marriott, Worby & Robinson. The front of the building is dominated by a seven-metre high, ten-tonne bronze sculpture of the bow of a ship, with a lone seafarer maintaining a look-out. The previous headquarters of IMO were at 101 Piccadilly (now the home of the Embassy of Japan), prior to that at 22 Berners Street in Fitzrovia and originally in Chancery Lane.

To become a member of the IMO, a state ratifies a multilateral treaty known as the Convention on the International Maritime Organization. As of 2020, there are 174 member states of the IMO, which includes 173 of the UN member states plus the Cook Islands. The first state to ratify the convention was the United Kingdom in 1949. The most recent members to join were Armenia and Nauru, which became IMO members in January and May 2018, respectively.

These are the current members with the year they joined:
Albania (1993)<br>
Algeria (1963)<br>
Angola (1977)<br>
Antigua and Barbuda (1986)<br>
Argentina (1953)<br>
Armenia (2018)<br>
Australia (1952)<br>
Austria (1975)<br>
Azerbaijan (1995)<br>
Bahamas (1976)<br>
Bahrain (1976)<br>
Bangladesh (1976)<br>
Barbados (1970)<br>
Belarus (2016)<br>
Belgium (1951)<br>
Belize (1990)<br>
Benin (1980)<br>
Bolivia (1987)<br>
Bosnia and Herzegovina (1993)<br>
Brazil (1963)<br>
Brunei Darussalam (1984)<br>
Bulgaria (1960)<br>
Cabo Verde (1976)<br>
Cambodia (1961)<br>
Cameroon (1961)<br>
Canada (1948)<br>
Chile (1972)<br>
China (1973)<br>
Colombia (1974)<br>
Comoros (2001)<br>
Congo (1975)<br>
Cook Islands (2008)<br>
Costa Rica (1981)<br>
Côte d'Ivoire (1960)<br>
Croatia (1992)<br>
Cuba (1966)<br>
Cyprus (1973)<br>
Czechia (1993)<br>
Democratic People's Republic of Korea (1986)<br>
Democratic Republic of the Congo (1973)<br>
Denmark (1959)<br>
Djibouti (1979)<br>
Dominica (1979)<br>
Dominican Republic (1953)<br>
Ecuador (1956)<br>
Egypt (1958)<br>
El Salvador (1981)<br>
Equatorial Guinea (1972)<br>
Eritrea (1993)<br>
Estonia (1992)<br>
Ethiopia (1975)<br>
Fiji (1983)<br>
Finland (1959)<br>
France (1952)<br>
Gabon (1976)<br>
Gambia (1979)<br>
Georgia (1993)<br>
Germany (1959)<br>
Ghana (1959)<br>
Greece (1958)<br>
Grenada (1998)<br>
Guatemala (1983)<br>
Guinea (1975)<br>
Guinea-Bissau (1977)<br>
Guyana (1980)<br>
Haiti (1953)<br>
Honduras (1954)<br>
Hungary (1970)<br>
Iceland (1960)<br>
India (1959)<br>
Indonesia (1961)<br>
Iran (1958)<br>
Iraq (1973)<br>
Ireland (1951)<br>
Israel (1952)<br>
Italy (1957)<br>
Jamaica (1976)<br>
Japan (1958)<br>
Jordan (1973)<br>
Kazakhstan (1994)<br>
Kenya (1973)<br>
Kiribati (2003)<br>
Kuwait (1960)<br>
Latvia (1993)<br>
Lebanon (1966)<br>
Liberia (1959)<br>
Libya (1970)<br>
Lithuania (1995)<br>
Luxembourg (1991)<br>
Madagascar (1961)<br>
Malawi (1989)<br>
Malaysia (1971)<br>
Maldives (1967)<br>
Malta (1966)<br>
Marshall Islands (1998)<br>
Mauritania (1961)<br>
Mauritius (1978)<br>
Mexico (1954)<br>
Monaco (1989)<br>
Mongolia (1996)<br>
Montenegro (2006)<br>
Morocco (1962)<br>
Mozambique (1979)<br>
Myanmar (1951)<br>
Namibia (1994)<br>
Nauru (2018)<br>
Nepal (1979)<br>
Netherlands (1949)<br>
New Zealand (1960)<br>
Nicaragua (1982)<br>
Nigeria (1962)<br>
North Macedonia (1993)<br>
Norway (1958)<br>
Oman (1974)<br>
Pakistan (1958)<br>
Palau (2011)<br>
Panama (1958)<br>
Papua New Guinea (1976)<br>
Paraguay (1993)<br>
Peru (1968)<br>
Philippines (1964)<br>
Poland (1960)<br>
Portugal (1976)<br>
Qatar (1977)<br>
Republic of Korea (1962)<br>
Republic of Moldova (2001)<br>
Romania (1965)<br>
Russian Federation (1958)<br>
Saint Kitts and Nevis (2001)<br>
Saint Lucia (1980)<br>
Saint Vincent and the Grenadines (1981)<br>
Samoa (1996)<br>
San Marino (2002)<br>
São Tomé and Príncipe (1990)<br>
Saudi Arabia (1969)<br>
Senegal (1960)<br>
Serbia (2000)<br>
Seychelles (1978)<br>
Sierra Leone (1973)<br>
Singapore (1966)<br>
Slovakia (1993)<br>
Slovenia (1993)<br>
Solomon Islands (1988)<br>
Somalia (1978)<br>
South Africa (1995)<br>
Spain (1962)<br>
Sri Lanka (1972)<br>
Sudan (1974)<br>
Suriname (1976)<br>
Sweden (1959)<br>
Switzerland (1955)<br>
Syria (1963)<br>
Tanzania (1974)<br>
Thailand (1973)<br>
Timor-Leste (2005)<br>
Togo (1983)<br>
Tonga (2000)<br>
Trinidad and Tobago (1965)<br>
Tunisia (1963)<br>
Turkey (1958)<br>
Turkmenistan (1993)<br>
Tuvalu (2004)<br>
Uganda (2009)<br>
Ukraine (1994)<br>
United Arab Emirates (1980)<br>
United Kingdom (1949)<br>
United States of America (1950)<br>
Uruguay (1968)<br>
Vanuatu (1986)<br>
Venezuela (1975)<br>
Viet Nam (1984)<br>
Yemen (1979)<br>
Zambia (2014)<br>
Zimbabwe (2005)
The three associate members of the IMO are the Faroe Islands, Hong Kong and Macao.

In 1961, the territories of Sabah and Sarawak, which had been included through the participation of United Kingdom, became joint associate members. In 1963 they became part of Malaysia.

Most UN member states that are not members of IMO are landlocked countries. These include Afghanistan, Andorra, Bhutan, Botswana, Burkina Faso, Burundi, Central African Republic, Chad, Kyrgyzstan, Laos, Lesotho, Liechtenstein, Mali, Niger, Rwanda, South Sudan, Swaziland, Tajikistan and Uzbekistan. However, the Federated States of Micronesia, an island-nation in the Pacific Ocean, is also a non-member, as is the same for similar Taiwan, itself a non-member of the UN.

The IMO consists of an Assembly, a Council and five main Committees: the Maritime Safety Committee; the Marine Environment Protection Committee; the Legal Committee; the Technical Co-operation Committee and the Facilitation Committee. A number of Sub-Committees support the work of the main technical committees.

IMO is the source of approximately 60 legal instruments that guide the regulatory development of its member states to improve safety at sea, facilitate trade among seafaring states and protect the maritime environment. The most well known is the International Convention for the Safety of Life at Sea (SOLAS), as well as International Convention on Oil Pollution Preparedness, Response and Co-operation (OPRC). Others include the International Oil Pollution Compensation Funds (IOPC). It also functions as a depository of yet to be ratified treaties, such as the International Convention on Liability and Compensation for Damage in Connection with the Carriage of Hazardous and Noxious Substances by Sea, 1996 (HNS Convention) and Nairobi International Convention of Removal of Wrecks (2007).

IMO regularly enacts regulations, which are broadly enforced by national and local maritime authorities in member countries, such as the International Regulations for Preventing Collisions at Sea (COLREG). The IMO has also enacted a Port State Control (PSC) authority, allowing domestic maritime authorities such as coast guards to inspect foreign-flag ships calling at ports of the many port states. Memoranda of Understanding (protocols) were signed by some countries unifying Port State Control procedures among the signatories.

Conventions, Codes and Regulations:

Recent initiatives at the IMO have included amendments to SOLAS, which upgraded fire protection standards on passenger ships, the International Convention on Standards of Training, Certification and Watchkeeping for Seafarers (STCW) which establishes basic requirements on training, certification and watchkeeping for seafarers and to the Convention on the Prevention of Maritime Pollution (MARPOL 73/78), which required double hulls on all tankers.

In December 2002, new amendments to the 1974 SOLAS Convention were enacted. These amendments gave rise to the International Ship and Port Facility Security (ISPS) Code, which went into effect on 1 July 2004. The concept of the code is to provide layered and redundant defences against smuggling, terrorism, piracy, stowaways, etc. The ISPS Code required most ships and port facilities engaged in international trade to establish and maintain strict security procedures as specified in ship and port specific Ship Security Plans and Port Facility Security Plans.

The IMO has a role in tackling international climate change. The First Intersessional Meeting of IMO's Working Group on Greenhouse Gas Emissions from Ships took place in Oslo, Norway (23–27 June 2008), tasked with developing the technical basis for the reduction mechanisms that may form part of a future IMO regime to control greenhouse gas emissions from international shipping, and a draft of the actual reduction mechanisms themselves, for further consideration by IMO's Marine Environment Protection Committee (MEPC). The IMO participated in the 2015 United Nations Climate Change Conference in Paris seeking to establish itself as the "appropriate international body to address greenhouse gas emissions from ships engaged in international trade". Nonetheless, there has been widespread criticism of the IMO's relative inaction since the conclusion of the Paris conference, with the initial data-gathering step of a three-stage process to reduce maritime greenhouse emissions expected to last until 2020. The IMO has also taken action to mitigate the global effects of ballast water and sediment discharge, through the 2004 Ballast Water Management Convention, which entered into force in September 2017.

The IMO is also responsible for publishing the International Code of Signals for use between merchant and naval vessels. IMO has harmonised information available to seafarers and shore-side traffic services called e-Navigation. An e-Navigation strategy was ratified in 2005, and an implementation plan was developed through three IMO sub-committees. The plan was completed by 2014 and implemented in November of that year. IMO has also served as a key partner and enabler of US international and interagency efforts to establish Maritime Domain Awareness.

The governing body of the International Maritime Organization is the Assembly which meets every two years. In between Assembly sessions a Council, consisting of 40 Member States elected by the Assembly, acts as the governing body. The technical work of the International Maritime Organization is carried out by a series of Committees. The Secretariat consists of some 300 international civil servants headed by a Secretary-General.

The current Secretary-General is Kitack Lim (South Korea), elected for a four-year term at the 114th session of the IMO Council in June 2015 and at the 29th session of the IMO's Assembly in November 2015. His mandate started on 1 January 2016. At the 31st session of the Assembly in 2019 he was re-appointed for a second term, ending on 31 December 2023.

The technical work of the International Maritime Organisation is carried out by a series of Committees. These include:

It is regulated in the Article 28(a) of the Convention on the IMO:

The Maritime Safety Committee is the most senior of these and is the main Technical Committee; it oversees the work of its nine sub-committees and initiates new topics. One broad topic it deals with is the effect of the human element on casualties; this work has been put to all of the sub-committees, but meanwhile, the Maritime Safety Committee has developed a code for the management of ships which will ensure that agreed operational procedures are in place and followed by the ship and shore-side staff.

The MSC and MEPC are assisted in their work by a number of sub-committees which are open to all Member States. The committees are:


The names of the IMO sub-committees were changed in 2013. Prior to 2013 there were nine Sub-Committees as follows:


Resolution MSC.255(84), of 16 May 2008, adopts the "Code of the International Standards and Recommended Practices for a Safety Investigation into a Marine casualty or Marine Incident". It is also known as the Casualty Investigation Code.

Sea transportation is one of few industrial areas that still commonly uses non-metric units such as the nautical mile (nmi) for distance and knots (kn) for speed or velocity. One nautical mile is approximately one minute of arc of latitude along any meridian arc, and is today precisely defined as 1852 meters (about 1.151 statute miles).

In 1975, the assembly of the IMO decided that future conventions of the International Convention for the Safety of Life at Sea (SOLAS) and other IMO instruments should use SI units only.





</doc>
<doc id="14987" url="https://en.wikipedia.org/wiki?curid=14987" title="International Labour Organization">
International Labour Organization

The International Labour Organization (ILO) is a United Nations agency whose mandate is to advance social and economic justice through setting international labour standards. Founded in 1919 under the League of Nations, it is the first and oldest specialised agency of the UN. The ILO has 187 member states: 186 out of 193 UN member states plus the Cook Islands. It is headquartered in Geneva, Switzerland, with around 40 field offices around the world, and employs some 2,700 staff from over 150 nations, of whom 900 work in technical cooperation programmes and projects.

The ILO's international labour standards are broadly aimed at ensuring accessible, productive, and sustainable work worldwide in conditions of freedom, equity, security and dignity. They are set forth in 189 conventions and treaties, of which eight are classified as fundamental according to the 1998 Declaration on Fundamental Principles and Rights at Work; together they protect freedom of association and the effective recognition of the right to collective bargaining, the elimination of forced or compulsory labour, the abolition of child labour, and the elimination of discrimination in respect of employment and occupation. The ILO is subsequently a major contributor to international labour law.

Within the UN system the organization has a unique tripartite structure: all standards, policies, and programmes require discussion and approval from the representatives of governments, employers, and workers. This framework is maintained in the ILO's three main bodies: The International Labour Conference, which meets annually to formulate international labour standards; the Governing Body, which serves as the executive council and decides the agency's policy and budget; and the International Labour Office, the permanent secretariat that administers the organization and implements activities. The secretariat is led by the Director-General, currently Guy Ryder of the United Kingdom, who was elected by the Governing Body in 2012.

In 1969, the ILO received the Nobel Peace Prize for improving fraternity and peace among nations, pursuing decent work and justice for workers, and providing technical assistance to other developing nations. In 2019, the organization convened the Global Commission on the Future of Work, whose report made ten recommendations for governments to meet the challenges of the 21st century labor environment; these include a universal labour guarantee, social protection from birth to old age and an entitlement to lifelong learning. With its focus on international development, it is a member of the United Nations Development Group, a coalition of UN organization aimed at helping meet the Sustainable Development Goals.

Unlike other United Nations specialized agencies, the International Labour Organization has a tripartite governing structure that brings together governments, employers, and workers of 187 member States, to set labour standards, develop policies and devise programmes promoting decent work for all women and men. The structure is intended to ensure the views of all three groups are reflected in ILO labour standards, policies, and programmes, though governments have twice as many representatives as the other two groups.

The Governing Body is the executive body of the International Labour Organization. It meets three times a year, in March, June and November. It takes decisions on ILO policy, decides the agenda of the International Labour Conference, adopts the draft Programme and Budget of the Organization for submission to the Conference, elects the Director-General, requests information from the member states concerning labour matters, appoints commissions of inquiry and supervises the work of the International Labour Office.

Juan Somavía was the ILO's Director-General from 1999 until October 2012 when Guy Ryder was elected. The ILO Governing Body re-elected Guy Rider as Director-General for a second five-year-term in November 2016.

This governing body is composed of 56 titular members (28 governments, 14 employers and 14 workers) and 66 deputy members (28 governments, 19 employers and 19 workers).

Ten of the titular government seats are permanently held by States of chief industrial importance: Brazil, China, France, Germany, India, Italy, Japan, the Russian Federation, the United Kingdom and the United States. The other Government members are elected by the Conference every three years (the last elections were held in June 2017). The Employer and Worker members are elected in their individual capacity.

The ILO organises once a year the International Labour Conference in Geneva to set the broad policies of the ILO, including conventions and recommendations. Also known as the "international parliament of labour", the conference makes decisions about the ILO's general policy, work programme and budget and also elects the Governing Body.

Each member state is represented by a delegation: two government delegates, an employer delegate, a worker delegate and their respective advisers. All of them have individual voting rights and all votes are equal, regardless the population of the delegate's member State. The employer and worker delegates are normally chosen in agreement with the most representative national organizations of employers and workers. Usually, the workers and employers' delegates coordinate their voting. All delegates have the same rights and are not required to vote in blocs.

Delegate have the same rights, they can express themselves freely and vote as they wish. This diversity of viewpoints does not prevent decisions being adopted by very large majorities or unanimously.

Heads of State and prime ministers also participate in the Conference. International organizations, both governmental and others, also attend but as observers.

The ILO has 187 state members. 186 of the 193 member states of the United Nations plus the Cook Islands are members of the ILO. The UN member states which are not members of the ILO are Andorra, Bhutan, Liechtenstein, Micronesia, Monaco, Nauru, and North Korea.

The ILO constitution permits any member of the UN to become a member of the ILO. To gain membership, a nation must inform the director-general that it accepts all the obligations of the ILO constitution. Other states can be admitted by a two-thirds vote of all delegates, including a two-thirds vote of government delegates, at any ILO General Conference. The Cook Islands, a non-UN state, joined in June 2015.

Members of the ILO under the League of Nations automatically became members when the organization's new constitution came into effect after World War II.

The ILO is a specialized agency of the United Nations (UN). As with other UN specialized agencies (or programmes) working on international development, the ILO is also a member of the United Nations Development Group.

Through July 2018, the ILO had adopted 189 conventions. If these conventions are ratified by enough governments, they come in force. However, ILO conventions are considered international labour standards regardless of ratification. When a convention comes into force, it creates a legal obligation for ratifying nations to apply its provisions.

Every year the International Labour Conference's Committee on the Application of Standards examines a number of alleged breaches of international labour standards. Governments are required to submit reports detailing their compliance with the obligations of the conventions they have ratified. Conventions that have not been ratified by member states have the same legal force as recommendations.

In 1998, the 86th International Labour Conference adopted the "Declaration on Fundamental Principles and Rights at Work". This declaration contains four fundamental policies:

The ILO asserts that its members have an obligation to work towards fully respecting these principles, embodied in relevant ILO conventions. The ILO conventions which embody the fundamental principles have now been ratified by most member states.

This device is employed for making conventions more flexible or for amplifying obligations by amending or adding provisions on different points.
Protocols are always linked to Convention, even though they are international treaties they do not exist on their own. As with Conventions, Protocols can be ratified.

Recommendations do not have the binding force of conventions and are not subject to ratification. Recommendations may be adopted at the same time as conventions to supplement the latter with additional or more detailed provisions. In other cases recommendations may be adopted separately and may address issues separate from particular conventions.

While the ILO was established as an agency of the League of Nations following World War I, its founders had made great strides in social thought and action before 1919. The core members all knew one another from earlier private professional and ideological networks, in which they exchanged knowledge, experiences, and ideas on social policy. Prewar "epistemic communities", such as the International Association for Labour Legislation (IALL), founded in 1900, and political networks, such as the socialist Second International, were a decisive factor in the institutionalization of international labour politics.

In the post–World War I euphoria, the idea of a "makeable society" was an important catalyst behind the social engineering of the ILO architects. As a new discipline, international labour law became a useful instrument for putting social reforms into practice. The utopian ideals of the founding members—social justice and the right to decent work—were changed by diplomatic and political compromises made at the Paris Peace Conference of 1919, showing the ILO's balance between idealism and pragmatism.

Over the course of the First World War, the international labour movement proposed a comprehensive programme of protection for the working classes, conceived as compensation for labour's support during the war. Post-war reconstruction and the protection of labour unions occupied the attention of many nations during and immediately after World War I. In Great Britain, the Whitley Commission, a subcommittee of the Reconstruction Commission, recommended in its July 1918 Final Report that "industrial councils" be established throughout the world. The British Labour Party had issued its own reconstruction programme in the document titled "Labour and the New Social Order". In February 1918, the third Inter-Allied Labour and Socialist Conference (representing delegates from Great Britain, France, Belgium and Italy) issued its report, advocating an international labour rights body, an end to secret diplomacy, and other goals. And in December 1918, the American Federation of Labor (AFL) issued its own distinctively apolitical report, which called for the achievement of numerous incremental improvements via the collective bargaining process.

As the war drew to a close, two competing visions for the post-war world emerged. The first was offered by the International Federation of Trade Unions (IFTU), which called for a meeting in Bern, Switzerland, in July 1919. The Bern meeting would consider both the future of the IFTU and the various proposals which had been made in the previous few years. The IFTU also proposed including delegates from the Central Powers as equals. Samuel Gompers, president of the AFL, boycotted the meeting, wanting the Central Powers delegates in a subservient role as an admission of guilt for their countries' role in the bringing about war. Instead, Gompers favoured a meeting in Paris which would only consider President Woodrow Wilson's Fourteen Points as a platform. Despite the American boycott, the Bern meeting went ahead as scheduled. In its final report, the Bern Conference demanded an end to wage labour and the establishment of socialism. If these ends could not be immediately achieved, then an international body attached to the League of Nations should enact and enforce legislation to protect workers and trade unions.

Meanwhile, the Paris Peace Conference sought to dampen public support for communism. Subsequently, the Allied Powers agreed that clauses should be inserted into the emerging peace treaty protecting labour unions and workers' rights, and that an international labour body be established to help guide international labour relations in the future. The advisory Commission on International Labour Legislation was established by the Peace Conference to draft these proposals. The Commission met for the first time on 1 February 1919, and Gompers was elected as the chairman.
Two competing proposals for an international body emerged during the Commission's meetings. The British proposed establishing an international parliament to enact labour laws which each member of the League would be required to implement. Each nation would have two delegates to the parliament, one each from labour and management. An international labour office would collect statistics on labour issues and enforce the new international laws. Philosophically opposed to the concept of an international parliament and convinced that international standards would lower the few protections achieved in the United States, Gompers proposed that the international labour body be authorized only to make recommendations, and that enforcement be left up to the League of Nations. Despite vigorous opposition from the British, the American proposal was adopted.

Gompers also set the agenda for the draft charter protecting workers' rights. The Americans made 10 proposals. Three were adopted without change: That labour should not be treated as a commodity; that all workers had the right to a wage sufficient to live on; and that women should receive equal pay for equal work. A proposal protecting the freedom of speech, press, assembly, and association was amended to include only freedom of association. A proposed ban on the international shipment of goods made by children under the age of 16 was amended to ban goods made by children under the age of 14. A proposal to require an eight-hour work day was amended to require the eight-hour work day "or" the 40-hour work week (an exception was made for countries where productivity was low). Four other American proposals were rejected. Meanwhile, international delegates proposed three additional clauses, which were adopted: One or more days for weekly rest; equality of laws for foreign workers; and regular and frequent inspection of factory conditions.

The Commission issued its final report on 4 March 1919, and the Peace Conference adopted it without amendment on 11 April. The report became Part XIII of the Treaty of Versailles.

The first annual conference, referred to as the International Labour Conference (ILC), began on 29 October 1919 at the Pan American Union Building in Washington, D.C. and adopted the first six International Labour Conventions, which dealt with hours of work in industry, unemployment, maternity protection, night work for women, minimum age, and night work for young persons in industry. The prominent French socialist Albert Thomas became its first director-general.

Despite open disappointment and sharp critique, the revived International Federation of Trade Unions (IFTU) quickly adapted itself to this mechanism. The IFTU increasingly oriented its international activities around the lobby work of the ILO.

At the time of establishment, the U.S. government was not a member of ILO, as the US Senate rejected the covenant of the League of Nations, and the United States could not join any of its agencies. Following the election of Franklin Delano Roosevelt to the U.S. presidency, the new administration made renewed efforts to join the ILO without league membership. On 19 June 1934, the U.S. Congress passed a joint resolution authorizing the president to join ILO without joining the League of Nations as a whole. On 22 June 1934, the ILO adopted a resolution inviting the U.S. government to join the organization. On 20 August 1934, the U.S. government responded positively and took its seat at the ILO.

During the Second World War, when Switzerland was surrounded by German troops, ILO director John G. Winant made the decision to leave Geneva. In August 1940, the government of Canada officially invited the ILO to be housed at McGill University in Montreal. Forty staff members were transferred to the temporary offices and continued to work from McGill until 1948.

The ILO became the first specialized agency of the United Nations system after the demise of the league in 1946. Its constitution, as amended, includes the Declaration of Philadelphia (1944) on the aims and purposes of the organization.

Beginning in the late 1950s the organization was under pressure to make provisions for the potential membership of ex-colonies which had become independent; in the Director General’s report of 1963 the needs of the potential new members were first recognized. The tensions produced by these changes in the world environment negatively affected the established politics within the organization and they were the precursor to the eventual problems of the organization with the USA

In July, 1970, the United States withdrew 50% of its financial support to the ILO following the appointment of an assistant director-general from the Soviet Union. This appointment (by the ILO's British director-general, C. Wilfred Jenks) drew particular criticism from AFL–CIO president George Meany and from Congressman John E. Rooney. However, the funds were eventually paid.

On 12 June 1975, the ILO voted to grant the Palestinian Liberation Organization observer status at its meetings. Representatives of the United States and Israel walked out of the meeting. The U.S. House of Representatives subsequently decided to withhold funds. The United States gave notice of full withdrawal on 6 November 1975, stating that the organization had become politicized. The United States also suggested that representation from communist countries was not truly "tripartite"—including government, workers, and employers—because of the structure of these economies. The withdrawal became effective on 1 November 1977.

The United States returned to the organization in 1980 after extracting some concession from the organization. It was partly responsible for the ILO's shift away from a human rights approach and towards support for the Washington Consensus. Economist Guy Standing wrote "the ILO quietly ceased to be an international body attempting to redress structural inequality and became one promoting employment equity".

In 1981, the government of Poland declared martial law. It interrupted the activities of Solidarnosc detained many of its leaders and members. The ILO Committee on Freedom of Association filed a complaint against Poland at the 1982 International Labour Conference. A Commission of Inquiry established to investigate found Poland had violated ILO Conventions No. 87 on freedom of association and No. 98 on trade union rights, which the country had ratified in 1957. The ILO and many other countries and organizations put pressure on the Polish government, which finally gave legal status to Solidarnosc in 1989. During that same year, there was a roundtable discussion between the government and Solidarnoc which agreed on terms of relegalization of the organization under ILO principles. The government also agreed to hold the first free elections in Poland since the Second World War.

The ILO is a major provider of labour statistics. Labour statistics are an important tool for its member states to monitor their progress toward improving labour standards. As part of their statistical work, ILO maintains several databases. This database covers 11 major data series for over 200 countries. In addition, ILO publishes a number of compilations of labour statistics, such as the Key Indicators of Labour Markets (KILM). KILM covers 20 main indicators on labour participation rates, employment, unemployment, educational attainment, labour cost, and economic performance. Many of these indicators have been prepared by other organizations. For example, the Division of International Labour Comparisons of the U.S. Bureau of Labor Statistics prepares the hourly compensation in manufacturing indicator.
The U.S. Department of Labor also publishes a yearly report containing a "List of Goods Produced by Child Labor or Forced Labor" issued by the Bureau of International Labor Affairs. The December 2014 updated edition of the report listed a total of 74 countries and 136 goods.

The International Training Centre of the International Labour Organization (ITCILO) is based in Turin, Italy. Together with the University of Turin Department of Law, the ITC offers training for ILO officers and secretariat members, as well as offering educational programmes. The ITC offers more than 450 training and educational programmes and projects every year for some 11,000 people around the world.

For instance, the ITCILO offers a Master of Laws programme in management of development, which aims specialize professionals in the field of cooperation and development.

The term "child labour" is often defined as work that deprives children of their childhood, potential, dignity, and is harmful to their physical and mental development.

"Child labour" refers to work that is mentally, physically, socially or morally dangerous and harmful to children. Further, it can involve interfering with their schooling by depriving them of the opportunity to attend school, obliging them to leave school prematurely, or requiring them to attempt to combine school attendance with excessively long and heavy work.

In its most extreme forms, child labour involves children being enslaved, separated from their families, exposed to serious hazards and illnesses and left to fend for themselves on the streets of large cities – often at a very early age. Whether or not particular forms of "work" can be called "child labour" depends on the child's age, the type and hours of work performed, the conditions under which it is performed and the objectives pursued by individual countries. The answer varies from country to country, as well as among sectors within countries.

The ILO's International Programme on the Elimination of Child Labour (IPEC) was created in 1992 with the overall goal of the progressive elimination of child labour, which was to be achieved through strengthening the capacity of countries to deal with the problem and promoting a worldwide movement to combat child labour. The IPEC currently has operations in 88 countries, with an annual expenditure on technical cooperation projects that reached over US$61 million in 2008. It is the largest programme of its kind globally and the biggest single operational programme of the ILO.

The number and range of the IPEC's partners have expanded over the years and now include employers' and workers' organizations, other international and government agencies, private businesses, community-based organizations, NGOs, the media, parliamentarians, the judiciary, universities, religious groups and children and their families.

The IPEC's work to eliminate child labour is an important facet of the ILO's Decent Work Agenda. Child labour prevents children from acquiring the skills and education they need for a better future,

Because of different cultural views involving labour, the ILO developed a series of culturally sensitive mandates, including convention Nos. 169, 107, 138, and 182, to protect indigenous culture, traditions, and identities. Convention Nos. 138 and 182 lead in the fight against child labour, while Nos. 107 and 169 promote the rights of indigenous and tribal peoples and protect their right to define their own developmental priorities.

In many indigenous communities, parents believe children learn important life lessons through the act of work and through the participation in daily life. Working is seen as a learning process preparing children of the future tasks they will eventually have to do as an adult. It is a belief that the family's and child well-being and survival is a shared responsibility between members of the whole family. They also see work as an intrinsic part of their child's developmental process. While these attitudes toward child work remain, many children and parents from indigenous communities still highly value education.

The ILO has considered the fight against forced labour to be one of its main priorities. During the interwar years, the issue was mainly considered a colonial phenomenon, and the ILO's concern was to establish minimum standards protecting the inhabitants of colonies from the worst abuses committed by economic interests. After 1945, the goal became to set a uniform and universal standard, determined by the higher awareness gained during World War II of politically and economically motivated systems of forced labour, but debates were hampered by the Cold War and by exemptions claimed by colonial powers. Since the 1960s, declarations of labour standards as a component of human rights have been weakened by government of postcolonial countries claiming a need to exercise extraordinary powers over labour in their role as emergency regimes promoting rapid economic development.

In June 1998 the International Labour Conference adopted a Declaration on Fundamental Principles and Rights at Work and its follow-up that obligates member states to respect, promote and realize freedom of association and the right to collective bargaining, the elimination of all forms of forced or compulsory labour, the effective abolition of child labour, and the elimination of discrimination in respect of employment and occupation.

With the adoption of the declaration, the ILO created the InFocus Programme on Promoting the Declaration which is responsible for the reporting processes and technical cooperation activities associated with the declaration; and it carries out awareness raising, advocacy and knowledge functions.

In November 2001, following the publication of the InFocus Programme's first global report on forced labour, the ILO's governing body created a special action programme to combat forced labour (SAP-FL), as part of broader efforts to promote the 1998 Declaration on Fundamental Principles and Rights at Work and its follow-up.
Since its inception, the SAP-FL has focused on raising global awareness of forced labour in its different forms, and mobilizing action against its manifestation. Several thematic and country-specific studies and surveys have since been undertaken, on such diverse aspects of forced labour as bonded labour, human trafficking, forced domestic work, rural servitude, and forced prisoner labour.

In 2013, the SAP-FL was integrated into the ILO's Fundamental Principles and Rights at Work Branch (FUNDAMENTALS) bringing together the fight against forced and child labour and working in the context of Alliance 8.7.

One major tool to fight forced labour was the adoption of the ILO Forced Labour Protocol by the International Labour Conference in 2014. It was ratified for the second time in 2015 and in 9 November 2016 it entered into force. The new protocol brings the existing ILO Convention 29 on Forced Labour, adopted in 1930, into the modern era to address practices such as human trafficking. The accompanying Recommendation 203 provides technical guidance on its implementation.

In 2015, the ILO launched a global campaign to end modern slavery, in partnership with the International Organization of Employers (IOE) and the International Trade Union Confederation (ITUC). The 50 for Freedom campaign aims to mobilize public support and encourage countries to ratify the ILO’s Forced Labour Protocol.

To protect the right of labours for fixing minimum wage, ILO has created Minimum Wage-Fixing Machinery Convention, 1928, Minimum Wage Fixing Machinery (Agriculture) Convention, 1951 and Minimum Wage Fixing Convention, 1970 as minimum wage law.

The International Labour Organization (ILO) is the lead UN-agency on HIV workplace policies and programmes and private sector mobilization. ILOAIDS is the branch of the ILO dedicated to this issue.

The ILO has been involved with the HIV response since 1998, attempting to prevent potentially devastating impact on labour and productivity and that it says can be an enormous burden for working people, their families and communities. In June 2001, the ILO's governing body adopted a pioneering code of practice on HIV/AIDS and the world of work, which was launched during a special session of the UN General Assembly.

The same year, ILO became a cosponsor of the Joint United Nations Programme on HIV/AIDS (UNAIDS).

In 2010, the 99th International Labour Conference adopted the ILO's recommendation concerning HIV and AIDS and the world of work, 2010 (No. 200), the first international labour standard on HIV and AIDS. The recommendation lays out a comprehensive set of principles to protect the rights of HIV-positive workers and their families, while scaling up prevention in the workplace. Working under the theme of "Preventing HIV, Protecting Human Rights at Work", ILOAIDS undertakes a range of policy advisory, research and technical support functions in the area of HIV and AIDS and the world of work. The ILO also works on promoting social protection as a means of reducing vulnerability to HIV and mitigating its impact on those living with or affected by HIV.

ILOAIDS ran a "Getting to Zero" campaign to arrive at zero new infections, zero AIDS-related deaths and zero-discrimination by 2015. Building on this campaign, ILOAIDS is executing a programme of voluntary and confidential counselling and testing at work, known as VCT@WORK.

As the word "migrant" suggests, migrant workers refer to those who moves from one country to another to do their job. For the rights of migrant workers, ILO has adopted conventions, including Migrant Workers (Supplementary Provisions) Convention, 1975 and United Nations Convention on the Protection of the Rights of All Migrant Workers and Members of Their Families in 1990.

Domestic workers are those who perform a variety of tasks for and in other peoples' homes. For example, they may cook, clean the house, and look after children. Yet they are often the ones with the least consideration, excluded from labour and social protection. This is mainly due to the fact that women have traditionally carried out the tasks without pay. For the rights and decent work of domestic workers including migrant domestic workers, ILO has adopted the Convention on Domestic Workers on 16 June 2011.

Seeking a process of globalization that is inclusive, democratically governed and provides opportunities and tangible benefits for all countries and people. The World Commission on the Social Dimension of Globalization was established by the ILO's governing body in February 2002 at the initiative of the director-general in response to the fact that there did not appear to be a space within the multilateral system that would cover adequately and comprehensively the social dimension of the various aspects of globalization. The World Commission Report, A Fair Globalization: Creating Opportunities for All, is the first attempt at structured dialogue among representatives of constituencies with different interests and opinions on the social dimension of globalization.

The ILO launched the Future of Work Initiative in order to gain understanding on the transformations that occur in the world of work and thus be able to develop ways of responding to these challenges. The initiative begun in 2016 by gathering the views of government representatives, workers, employers, academics and other relevant figures around the world. About 110 countries participated in dialogues at the regional and national level. These dialogues were structured around "four centenary conversations: work and society, decent jobs for all, the organization of work and production, and the governance of work." The second step took place in 2017 with the establishment of the Global Commission on the Future of Work dealing with the same "four centenary conversations". A report is expected to be published prior to the 2019 Centenary International Labour Conference. ILO is also assessing the impact of technological disruptions on employments worldwide. The agency is worried about the global economic and health impact of technology, like industrial and process automation, artificial intelligence (AI), Robots and robotic process of automation on human labor and is increasingly being considered by commentators, but in widely divergent ways. Among the salient views technology will bring less work, make workers redundant or end work by replacing the human labor. The other fold of view is technological creativity and abundant opportunities for economy boosts. In the modern era, technology has changed the way we think, design, and deploy the system solutions but no doubt there are threats to human jobs. Paul Schulte (Director of the Education and Information Division, and Co-Manager of the Nanotechnology Research Center, National Institute for Occupational Safety and Health, Centers for Disease Control) and DP Sharma, (International Consultant, Information Technology and Scientist) clearly articulated such disruptions and warned that it will be worse than ever before if appropriate actions will not be taken timely.He said that human generation needs to reinvent in terms of competitive accuracy, speed, capacity and honesty. Machines are more honest than human labours and its a crystal clear threat to this generation. The science and technology have no reverse gear and accepting the challenge " Human vs. Machine" is the only remedy for survival.

The ILO has also looked at the transition to a green economy, and the impact thereof on employment. It came to the conclusion a shift to a greener economy could create 24 million new jobs globally by 2030, if the right policies are put in place. Also, if a transition to a green economy were not to take place, 72 million full-time jobs may be lost by 2030 due to heat stress, and temperature increases will lead to shorter available work hours, particularly in agriculture




</doc>
<doc id="14990" url="https://en.wikipedia.org/wiki?curid=14990" title="IMO">
IMO

IMO or 
Imo may refer to:



</doc>
<doc id="14996" url="https://en.wikipedia.org/wiki?curid=14996" title="International English">
International English

International English is the concept of the English language as a global means of communication in numerous dialects, and the movement towards an international standard for the language. It is also referred to as Global English, World English, Common English, Continental English, General English, Engas (English as associate language), or Globish. Sometimes, these terms refer simply to the array of varieties of English spoken throughout the world.

Sometimes, "international English" and the related terms above refer to a desired standardisation, i.e., Standard English; however, there is no consensus on the path to this goal. There have been many proposals for making International English more accessible to people from different nationalities. Basic English is an example, but it failed to make progress. More recently, there have been proposals for English as a lingua franca (ELF) in which non-native speakers take a highly active role in the development of the language. It has also been argued that International English is held back by its traditional spelling. There has been slow progress in adopting alternate spellings.

The modern concept of International English does not exist in isolation, but is the product of centuries of development of the English language.

The English language evolved in England, from a set of West Germanic dialects spoken by the Angles and Saxons, who arrived from continental Europe in the 5th century. Those dialects became known as "Englisc" (literally "Anglish"), the language today referred to as Anglo-Saxon or Old English (the language of the poem "Beowulf"). However, less than a quarter of the vocabulary of Modern English is derived from the shared ancestry with other West Germanic languages because of extensive borrowings from Norse, Norman, Latin, and other languages. It was during the Viking invasions of the Anglo-Saxon period that Old English was influenced by contact with Norse, a group of North Germanic dialects spoken by the Vikings, who came to control a large region in the North of England known as the Danelaw. Vocabulary items entering English from Norse (including the pronouns "they", and "them") are thus attributable to the on-again-off-again Viking occupation of Northern England during the centuries prior to the Norman Conquest (see, e.g., Canute the Great). Soon after the Norman Conquest of 1066, the "Englisc" language ceased being a literary language (see, e.g., Ormulum) and was replaced by Anglo-Norman as the written language of England. During the Norman Period, English absorbed a significant component of French vocabulary (approximately one-third of the vocabulary of Modern English). With this new vocabulary, additional vocabulary borrowed from Latin (with Greek, another approximately one-third of Modern English vocabulary, though some borrowings from Latin and Greek date from later periods), a simplified grammar, and use of the orthographic conventions of French instead of Old English orthography, the language became Middle English (the language of Chaucer). The "difficulty" of English as a written language thus began in the High Middle Ages, when French orthographic conventions were used to spell a language whose original, more suitable orthography had been forgotten after centuries of nonuse. During the late medieval period, King Henry V of England (lived 1387–1422) ordered the use of the English of his day in proceedings before him and before the government bureaucracies. That led to the development of Chancery English, a standardised form used in the government bureaucracy. (The use of so-called Law French in English courts continued through the Renaissance, however.)

The emergence of English as a language of Wales results from the incorporation of Wales into England and also dates from approximately this time period. Soon afterward, the development of printing by Caxton and others accelerated the development of a standardised form of English. Following a change in vowel pronunciation that marks the transition of English from the medieval to the Renaissance period, the language of the Chancery and Caxton became Early Modern English (the language of Shakespeare's day) and with relatively moderate changes eventually developed into the English language of today. Scots, as spoken in the lowlands and along the east coast of Scotland, developed largely independent of Modern English, and is based on the Northern dialects of Anglo-Saxon, particularly Northumbrian, which also serve as the basis of Northern English dialects such as those of Yorkshire and Newcastle upon Tyne. Northumbria was within the Danelaw and therefore experienced greater influence from Norse than did the Southern dialects. As the political influence of London grew, the Chancery version of the language developed into a written standard across Great Britain, further progressing in the modern period as Scotland became united with England as a result of the Acts of Union of 1707.

English was introduced to Ireland twice—a medieval introduction that led to the development of the now-extinct Yola dialect, and a modern introduction in which Hibernian English largely replaced Irish as the most widely spoken language during the 19th century, following the Act of Union of 1800. Received Pronunciation (RP) is generally viewed as a 19th-century development and is not reflected in North American English dialects (except the affected Transatlantic accent), which are based on 18th-century English.

The establishment of the first permanent English-speaking colony in North America in 1607 was a major step towards the globalisation of the language. British English was only partially standardised when the American colonies were established. Isolated from each other by the Atlantic Ocean, the dialects in England and the colonies began evolving independently.

The British colonisation of Australia starting in 1788 brought the English language to Oceania. By the 19th century, the standardisation of British English was more settled than it had been in the previous century, and this relatively well-established English was brought to Africa, Asia and New Zealand. It developed both as the language of English-speaking settlers from Britain and Ireland, and as the administrative language imposed on speakers of other languages in the various parts of the British Empire. The first form can be seen in New Zealand English, and the latter in Indian English. In Europe, English received a more central role particularly since 1919, when the Treaty of Versailles was composed not only in French, the common language of diplomacy at the time, but, under special request from American president Woodrow Wilson, also in English – a major milestone in the globalisation of English.

The English-speaking regions of Canada and the Caribbean are caught between historical connections with the UK and the Commonwealth and geographical and economic connections with the U.S. In some things they tend to follow British standards, whereas in others, especially commercial, they follow the U.S. standard.

Braj Kachru divides the use of English into three concentric circles.

The "inner circle" is the traditional base of English and includes countries such as the United Kingdom and Ireland and the anglophone populations of the former British colonies of the United States, Australia, New Zealand, South Africa, Canada, and various islands of the Caribbean, Indian Ocean, and Pacific Ocean.

In the "outer circle" are those countries where English has official or historical importance ("special significance"). This includes most of the countries of the Commonwealth of Nations (the former British Empire), including populous countries such as India, Pakistan, and Nigeria; and others, such as the Philippines, under the sphere of influence of English-speaking countries. Here English may serve as a useful lingua franca between ethnic and language groups. Higher education, the legislature and judiciary, national commerce, and so on, may all be carried out predominantly in English.

The "expanding circle" refers to those countries where English has no official role, but is nonetheless important for certain functions, e.g., international business and tourism. By the twenty-first century, non-native English speakers have come to outnumber native speakers by a factor of three, according to the British Council. Darius Degher, a professor at Malmö University in Sweden, uses the term "decentered English" to describe this shift, along with attendant changes in what is considered important to English users and learners. The Scandinavian language area as well as the Netherlands have a near complete bilingualism between their native languages and English as a foreign second language. Elsewhere in Europe, although not universally, English knowledge is still rather common among non-native speakers. In many cases this leads to accents derived from the native languages altering pronunciations of the spoken English in these countries.

Research on English as a lingua franca in the sense of "English in the Expanding Circle" is comparatively recent. Linguists who have been active in this field are Jennifer Jenkins, Barbara Seidlhofer, Christiane Meierkord and Joachim Grzega.

English as an additional language (EAL) is usually based on the standards of either American English or British English as well as incorporating foreign terms. English as an international language (EIL) is EAL with emphasis on learning different major dialect forms; in particular, it aims to equip students with the linguistic tools to communicate internationally. Roger Nunn considers different types of competence in relation to the teaching of English as an International Language, arguing that linguistic competence has yet to be adequately addressed in recent considerations of EIL.

Several models of "simplified English" have been suggested for teaching English as a foreign language:
Furthermore, Randolph Quirk and Gabriele Stein thought about a Nuclear English, which, however, has never been fully developed.

With reference to the term "Globish", Robert McCrum has used this to mean "English as global language". Jean-Paul Nerriere uses it for a constructed language.

Basic Global English, or BGE, is a concept of global English initiated by German linguist Joachim Grzega. It evolved from the idea of creating a type of English that can be learned more easily than regular British or American English and that serves as a tool for successful global communication. BGE is guided by creating "empathy and tolerance" between speakers in a global context. This applies to the context of global communication, where different speakers with different mother tongues come together. BGE aims to develop this competence as quickly as possible.

English language teaching is almost always related to a corresponding culture, e. g., learners either deal with American English and therefore with American culture, or British English and therefore with British culture. Basic Global English seeks to solve this problem by creating one collective version of English. Additionally, its advocates promote it as a system suited for self-teaching as well as classroom teaching.

BGE is based on 20 elementary grammar rules that provide a certain degree of variation. For example, regular as well as irregular formed verbs are accepted. Pronunciation rules are not as strict as in British or American English, so there is a certain degree of variation for the learners. Exceptions that cannot be used are pronunciations that would be harmful to mutual understanding and therefore minimize the success of communication.

Basic Global English is based on a 750-word vocabulary. Additionally, every learner has to acquire the knowledge of 250 additional words. These words can be chosen freely, according to the specific needs and interests of the learner.

BGE provides not only basic language skills, but also so called "Basic Politeness Strategies". These include creating a positive atmosphere, accepting an offer with "Yes, please" or refusing with "No, thank you", and small talk topics to choose and to avoid.

Basic Global English has been tested in two elementary schools in Germany. For the practical test of BGE, 12 lessons covered half of a school year. After the BGE teaching, students could answer questions about themselves, their family, their hobbies etc. Additionally they could form questions themselves about the same topics. Besides that, they also learned the numbers from 1 to 31 and vocabulary including things in their school bag and in their classroom. The students as well as the parents had a positive impression of the project.

International English sometimes refers to English as it is actually being used and developed in the world; as a language owned not just by native speakers, but by all those who come to use it.
Basically, it covers the English language at large, often (but not always or necessarily) implicitly seen as standard. It is certainly also commonly used in connection with the acquisition, use, and study of English as the world's lingua franca ('TEIL: Teaching English as an International Language'), and especially when the language is considered as a whole in contrast with "British English", "American English", "South African English", and the like. — McArthur (2002, p. 444–445)
It especially means English words and phrases generally understood throughout the English-speaking world as opposed to localisms. The importance of non-native English language skills can be recognised behind the long-standing joke that the international language of science and technology is broken English.

International English reaches toward cultural neutrality. This has a practical use:
What could be better than a type of English that saves you from having to re-edit publications for individual regional markets! Teachers and learners of English as a second language also find it an attractive idea — both often concerned that their English should be neutral, without American or British or Canadian or Australian coloring. Any regional variety of English has a set of political, social and cultural connotations attached to it, even the so-called 'standard' forms.
According to this viewpoint, International English is a concept of English that minimises the aspects defined by either the colonial imperialism of Victorian Britain or the cultural imperialism of the 20th century United States. While British colonialism laid the foundation for English over much of the world, International English is a product of an emerging world culture, very much attributable to the influence of the United States as well, but conceptually based on a far greater degree of cross-talk and linguistic transculturation, which tends to mitigate both U.S. influence and British colonial influence.

The development of International English often centres on academic and scientific communities, where formal English usage is prevalent, and creative use of the language is at a minimum. This formal International English allows entry into Western culture as a whole and Western cultural values in general.

The continued growth of the English language itself is seen by authors such as Alistair Pennycook as a kind of cultural imperialism, whether it is English in one form or English in two slightly different forms.

Robert Phillipson argues against the possibility of such neutrality in his "Linguistic Imperialism" (1992). Learners who wish to use purportedly correct English are in fact faced with the dual standard of American English and British English, and other less known standard Englishes (including Australian, Scottish and Canadian).

Edward Trimnell, author of "Why You Need a Foreign Language & How to Learn One" (2005) argues that the international version of English is only adequate for communicating basic ideas. For complex discussions and business/technical situations, English is not an adequate communication tool for non-native speakers of the language. Trimnell also asserts that native English-speakers have become "dependent on the language skills of others" by placing their faith in international English.

Some reject both what they call "linguistic imperialism" and David Crystal's theory of the neutrality of English. They argue that the phenomenon of the global spread of English is better understood in the framework of appropriation (e.g., Spichtinger 2000), that is, English used for local purposes around the world. Demonstrators in non-English speaking countries often use signs in English to convey their demands to TV-audiences around the globe, for example.

In English-language teaching, Bobda shows how Cameroon has moved away from a mono-cultural, Anglo-centered way of teaching English and has gradually appropriated teaching material to a Cameroonian context. This includes non-Western topics, such as the rule of Emirs, traditional medicine, and polygamy (1997:225). Kramsch and Sullivan (1996) describe how Western methodology and textbooks have been appropriated to suit local Vietnamese culture. The Pakistani textbook "Primary Stage English" includes lessons such as "Pakistan My Country", "Our Flag", and "Our Great Leader" (Malik 1993: 5,6,7), which might sound jingoistic to Western ears. Within the native culture, however, establishing a connection between English Language Teaching (ELT), patriotism, and Muslim faith is seen as one of the aims of ELT. The Punjab Textbook Board openly states: "The board ... takes care, through these books to inoculate in the students a love of the Islamic values and awareness to guard the ideological frontiers of your [the students] home lands." (Punjab Text Book Board 1997).

Many difficult choices must be made if further standardisation of English is pursued. These include whether to adopt a current standard, or move towards a more neutral, but artificial one. A true International English might supplant both current American and British English as a variety of English for international communication, leaving these as local dialects, or would rise from a merger of General American and standard British English with admixture of other varieties of English and would generally replace all these varieties of English.
We may, in due course, all need to be in control of two standard Englishes—the one which gives us our national and local identity, and the other which puts us in touch with the rest of the human race. In effect, we may all need to become bilingual in our own language. — David Crystal (1988: p. 265)
This is the situation long faced by many users of English who possess a "non-standard" dialect of English as their birth tongue but have also learned to write (and perhaps also speak) a more standard dialect. (This phenomenon is known in linguistics as "diglossia".) Many academics often publish material in journals requiring different varieties of English and change style and spellings as necessary without great difficulty.

As far as spelling is concerned, the differences between American and British usage became noticeable due to the first influential lexicographers (dictionary writers) on each side of the Atlantic. Samuel Johnson's dictionary of 1755 greatly favoured Norman-influenced spellings such as "centre" and "colour"; on the other hand, Noah Webster's first guide to American spelling, published in 1783, preferred spellings like "center" and the Latinate "color". The difference in strategy and philosophy of Johnson and Webster are largely responsible for the main division in English spelling that exists today. However, these differences are extremely minor. Spelling is but a small part of the differences between dialects of English, and may not even reflect dialect differences at all (except in phonetically spelled dialogue). International English refers to much more than an agreed spelling pattern.

Two approaches to International English are the individualistic and inclusive approach and the new dialect approach.

The individualistic approach gives control to individual authors to write and spell as they wish (within purported standard conventions) and to accept the validity of differences. The "Longman Grammar of Spoken and Written English", published in 1999, is a descriptive study of both American and British English in which each chapter follows individual spelling conventions according to the preference of the main editor of that chapter.

The new dialect approach appears in "The Cambridge Guide to English Usage" (Peters, 2004), which attempts to avoid any language bias and accordingly uses an idiosyncratic international spelling system of mixed American and British forms (but tending to prefer the American English spellings).



</doc>
<doc id="14997" url="https://en.wikipedia.org/wiki?curid=14997" title="International African Institute">
International African Institute

The International African Institute (IAI) was founded (as the International Institute of African Languages and Cultures - IIALC) in 1926 in London for the study of African languages. Frederick Lugard was the first chairman (1926 to his death in 1945); Diedrich Hermann Westermann (1926 to 1939) and Maurice Delafosse (1926) were the initial co-directors.

Since 1928, the IAI has published a quarterly journal, "Africa". For some years during the 1950s and 1960s, the assistant editor was the novelist Barbara Pym.

The IAI's mission is "to promote the education of the public in the study of Africa and its languages and cultures". Its operations includes seminars, journals, monographs, edited volumes and stimulating scholarship within Africa.

The IAI has been involved in scholarly publishing since 1927. Scholars whose work has been published by the institute include Emmanuel Akeampong, Samir Amin, Karin Barber, Alex de Waal, Patrick Chabal, Mary Douglas, E.E. Evans Pritchard, Jack Goody, Jane Guyer, Monica Hunter, Bronislaw Malinowski, Z.K. Matthews, D.A. Masolo, Achille Mbembe, Thomas Mofolo, John Middleton, Simon Ottenburg, J.D.Y. Peel, Mamphela Ramphele, Isaac Schapera, Monica Wilson and V.Y. Mudimbe.

IAI publications fall into a number of series, notably International African Library and International African Seminars. The International African Library is published from volume 41 (2011) by Cambridge University Press; Volumes 7-40 are available from Edinburgh University Press. there are 49 volumes.

The archives of the International African Institute are held at the Archives Division of the Library of the London School of Economics. An online catalogue of these papers is available.

In 1928, the IAI (then IIALC) published an "Africa Alphabet" to facilitate standardization of Latin-based writing systems for African languages.

From April 1929 to 1950, the IAI offered prizes for works of literature in African languages.




</doc>
<doc id="14998" url="https://en.wikipedia.org/wiki?curid=14998" title="IAI">
IAI

IAI is an acronym for:



</doc>
<doc id="15000" url="https://en.wikipedia.org/wiki?curid=15000" title="Insulin-like growth factor">
Insulin-like growth factor

The insulin-like growth factors (IGFs) are proteins with high sequence similarity to insulin. IGFs are part of a complex system that cells use to communicate with their physiologic environment. This complex system (often referred to as the IGF "axis") consists of two cell-surface receptors (IGF1R and IGF2R), two ligands (Insulin-like growth factor 1 (IGF-1) and Insulin-like growth factor 2 (IGF-2)), a family of seven high-affinity IGF-binding proteins (IGFBP1 to IGFBP7), as well as associated IGFBP degrading enzymes, referred to collectively as proteases.

-The IGF "axis" is also commonly referred to as the Growth Hormone/IGF-1 Axis. Insulin-like growth factor 1 (IGF-1, or sometimes with a Roman numeral as IGF-I) is mainly secreted by the liver as a result of stimulation by growth hormone (GH). IGF-1 is important for both the regulation of normal physiology, as well as a number of pathological states, including cancer. The IGF axis has been shown to play roles in the promotion of cell proliferation and the inhibition of cell death (apoptosis). Insulin-like growth factor 2 (IGF-2, or sometimes as IGF-II) is thought to be a primary growth factor required for early development while IGF-1 expression is required for achieving maximal growth. Gene knockout studies in mice have confirmed this, though other animals are likely to regulate the expression of these genes in distinct ways. While IGF-2 may be primarily fetal in action it is also essential for development and function of organs such as the brain, liver, and kidney.

Factors that are thought to cause variation in the levels of GH and IGF-1 in the circulation include an individual's genetic make-up, the time of day, age, sex, exercise status, stress levels, nutrition level, body mass index (BMI), disease state, race, estrogen status, and xenobiotic intake.

IGF-1 has an involvement in regulating neural development including neurogenesis, myelination, synaptogenesis, and dendritic branching and neuroprotection after neuronal damage. Increased serum levels of IGF-I in children have been associated with higher IQ.

IGF-1 shapes the development of the cochlea through controlling apoptosis. Its deficit can cause hearing loss. Serum level of it also underlies a correlation between short height and reduced hearing abilities particularly around 3–5 years of age, and at age 18 (late puberty).

The IGFs are known to bind the IGF-1 receptor, the insulin receptor, the IGF-2 receptor, the insulin-related receptor and possibly other receptors. The IGF-1 receptor is the "physiological" receptor—IGF-1 binds to it at significantly higher affinity than it binds the insulin receptor. Like the insulin receptor, the IGF-1 receptor is a receptor tyrosine kinase—meaning the receptor signals by causing the addition of a phosphate molecule on particular tyrosines. The IGF-2 receptor only binds IGF-2 and acts as a "clearance receptor"—it activates no intracellular signaling pathways, functioning only as an IGF-2 sequestering agent and preventing IGF-2 signaling.

Since many distinct tissue types express the IGF-1 receptor, IGF-1's effects are diverse. It acts as a neurotrophic factor, inducing the survival of neurons. It may catalyse skeletal muscle hypertrophy, by inducing protein synthesis, and by blocking muscle atrophy. It is protective for cartilage cells, and is associated with activation of osteocytes, and thus may be an anabolic factor for bone. Since at high concentrations it is capable of activating the insulin receptor, it can also complement for the effects of insulin. Receptors for IGF-1 are found in vascular smooth muscle, while typical receptors for insulin are not found in vascular smooth muscle.

IGF-1 and IGF-2 are regulated by a family of proteins known as the IGF-Binding Proteins. These proteins help to modulate IGF action in complex ways that involve both inhibiting IGF action by preventing binding to the IGF-1 receptor as well as promoting IGF action possibly through aiding in delivery to the receptor and increasing IGF half-life. Currently, there are seven characterized IGF Binding Proteins (IGFBP1 to IGFBP7). There is currently significant data suggesting that IGFBPs play important roles in addition to their ability to regulate IGFs. 
IGF-1 and IGFBP-3 are GH dependent, whereas IGFBP-1 is insulin regulated.
IGFBP-1 production from the liver is significantly elevated during insulinopenia while serum levels of bioactive IGF-1 is increased by insulin.

Studies of recent interest show that the Insulin/IGF axis play an important role in aging. Nematodes, fruit-flies, and other organisms have an increased life span when the gene equivalent to the mammalian insulin is knocked out. It is somewhat difficult to relate this finding to the mammals, however, because in the smaller organism there are many genes (at least 37 in the nematode "Caenorhabditis elegans") that are "insulin-like" or "IGF-1-like", whereas in the mammals insulin-like proteins comprise only seven members (insulin, IGFs, relaxins, EPIL, and relaxin-like factor). The human insulin-like genes have apparently distinct roles with some but less crosstalk presumably because there are multiple insulin-receptor-like proteins in humans. Simpler organisms typically have fewer receptors; for example, only one insulin-like receptor exists in the nematode "C. elegans". Additionally, "C. elegans" do not have specialized organs such as the (Islets of Langerhans), which sense insulin in response to glucose homeostasis. Moreover, IGF1 affects lifespan in nematodes by causing dauer formation, a developmental stage of C. elegans larva. There is no mammalian correlate. Therefore, it is an open question as to whether either IGF-1 or insulin in the mammal may perturb aging, although there is the suggestion that dietary restriction phenomena may be related.

Other studies are beginning to uncover the important role the IGFs play in diseases such as cancer and diabetes, showing for instance that IGF-1 stimulates growth of both prostate and breast cancer cells. Researchers are not in complete agreement about the degree of cancer risk that IGF-1 poses.



</doc>
<doc id="15001" url="https://en.wikipedia.org/wiki?curid=15001" title="IGF">
IGF

IGF may stand for:


</doc>
<doc id="15004" url="https://en.wikipedia.org/wiki?curid=15004" title="Idiot">
Idiot

An idiot, in modern use, is a stupid or foolish person.

It was formerly a technical term in legal and psychiatric contexts for some kinds of profound intellectual disability where the mental age is two years or less, and the person cannot guard themself against common physical dangers. The term was gradually replaced by the term profound mental retardation (which has itself since been replaced by other terms). Along with terms like moron, imbecile, and cretin, it is archaic and offensive in those uses.

The word "idiot" comes from the Greek noun "idiōtēs" 'a private person, individual', 'a private citizen' (as opposed to an official), 'a common man', 'a person lacking professional skill, layman', later 'unskilled', 'ignorant', derived from the adjective "idios" 'private', 'one's own'. In Latin, "idiota" was borrowed in the meaning 'uneducated', 'ignorant', 'common', and in Late Latin came to mean 'crude, illiterate, ignorant'. In French, it kept the meaning of 'illiterate', 'ignorant', and added the meaning 'stupid' in the 13th century. In English, it added the meaning 'mentally deficient' in the 14th century.

Many political commentators, starting as early as 1856, have interpreted the word "idiot" as reflecting the Ancient Greeks' attitudes to civic participation and private life, combining the ancient meaning of 'private citizen' with the modern meaning 'fool' to conclude that the Greeks used the word to say that it is selfish and foolish not to participate in public life. But this is not how the Greeks used the word.

It is certainly true that the Greeks valued civic participation and criticized non-participation. Thucydides quotes Pericles' Funeral Oration as saying: "[we] regard... him who takes no part in these [public] duties not as unambitious but as useless" (τόν τε μηδὲν τῶνδε μετέχοντα οὐκ ἀπράγμονα, ἀλλ᾽ ἀχρεῖον νομίζομεν). However, neither he nor any other ancient author uses the word "idiot" to describe non-participants, or in a derogatory sense; its most common use was simply a private citizen or amateur as opposed to a government official, professional, or expert. The derogatory sense came centuries later, and was unrelated to the political meaning.

In 19th- and early 20th-century medicine and psychology, an "idiot" was a person with a very profound intellectual disability. In the early 1900s, Dr. Henry H. Goddard proposed a classification system for intellectual disability based on the Binet-Simon concept of mental age. Individuals with the lowest mental age level (less than three years) were identified as "idiots"; "imbeciles" had a mental age of three to seven years, and "morons" had a mental age of seven to ten years. The term "idiot" was used to refer to people having an IQ below 30 IQ, or intelligence quotient, was originally determined by dividing a person's mental age, as determined by standardized tests, by their actual age. The concept of mental age has fallen into disfavor, though, and IQ is now determined on the basis of statistical distributions.

In the obsolete medical classification (ICD-9, 1977), these people were said to have "profound mental retardation" or "profound mental subnormality" with IQ under 20. This term is not in use in the United Kingdom.

Until 2007, the California Penal Code Section 26 stated that "Idiots" were one of six types of people who are not capable of committing crimes. In 2007 the code was amended to read "persons who are mentally incapacitated." In 2008, Iowa voters passed a measure replacing "idiot, or insane person" in the State's constitution with "person adjudged mentally incompetent."

In several U.S. states, "idiots" do not have the right to vote:


The constitution of the state of Arkansas was amended in the general election of 2008 to, among other things, repeal a provision (Article 3, Section 5) which had until its repeal prohibited "idiots or insane persons" from voting.

A few authors have used "idiot" characters in novels, plays and poetry. Often these characters are used to highlight or indicate something else (allegory). Examples of such usage are William Faulkner's "The Sound and the Fury", Daphne du Maurier's "Rebecca" and William Wordsworth's "The Idiot Boy". Idiot characters in literature are often confused with or subsumed within mad or lunatic characters. The most common intersection between these two categories of mental impairment occurs in the polemic surrounding Edmund from William Shakespeare's "King Lear".

In Fyodor Dostoevsky's novel "The Idiot" the title refers to the central character Prince Myshkin, a man whose innocence, kindness and humility, combined with his occasional epileptic symptoms, cause many in the corrupt, egoistic culture around him to mistakenly assume that he lacks intelligence. In "The Antichrist", Nietzsche applies the word 'idiot' to Jesus in a comparable fashion, almost certainly in an allusion to Dostoevsky's use of the word: "One has to regret that no Dostoevsky lived in the neighbourhood of this most interesting "décadent"; I mean someone who could feel the thrilling fascination of such a combination of the sublime, the sick and the childish."



</doc>
<doc id="15012" url="https://en.wikipedia.org/wiki?curid=15012" title="Islamism">
Islamism

Islamism is a concept whose meaning has been debated in both public and academic contexts. The term can refer to diverse forms of social and political activism advocating that public and political life should be guided by Islamic principles or more specifically to movements which call for full implementation of "sharia" (Islamic order or law). It is commonly used interchangeably with the terms political Islam or Islamic fundamentalism. In academic usage, the term "Islamism" does not specify what vision of "Islamic order" or sharia are being advocated, or how their advocates intend to bring them about. In Western mass media it tends to refer to groups whose aim is to establish a sharia-based Islamic state, often with implication of violent tactics and human rights violations, and has acquired connotations of political extremism. In the Muslim world, the term has positive connotations among its proponents.

Different currents of Islamist thought include advocating a "revolutionary" strategy of Islamizing society through exercise of state power, and alternately a "reformist" strategy to re-Islamizing society through grass-roots social and political activism. Islamists may emphasize the implementation of sharia; pan-Islamic political unity, including an Islamic state; or selective removal of non-Muslim, particularly Western military, economic, political, social, or cultural influences in the Muslim world that they believe to be incompatible with Islam.

Graham Fuller has argued for a broader notion of Islamism as a form of identity politics, involving "support for [Muslim] identity, authenticity, broader regionalism, revivalism, [and] revitalization of the community." Some authors hold the term "Islamic activism" to be synonymous and preferable to "Islamism", and Rached Ghannouchi writes that Islamists prefer to use the term "Islamic movement" themselves.

Central and prominent figures in twentieth-century Islamism include Hasan al-Banna, Sayyid Qutb, Abul Ala Maududi, and Ruhollah Khomeini. Most Islamist thinkers emphasize peaceful political processes, which are supported by the majority of contemporary Islamists. Others, Sayyid Qutb in particular, called for violence, and his followers are generally considered Islamic extremists, although Qutb denounced the killing of innocents.
According to Robin Wright, Islamist movements have "arguably altered the Middle East more than any trend since the modern states gained independence", redefining "politics and even borders". Following the Arab Spring, some Islamist currents became heavily involved in democratic politics, while others spawned "the most aggressive and ambitious Islamist militia" to date, ISIS.

The term "Islamism", which originally denoted the religion of Islam, first appeared in the English language as "Islamismus" in 1696, and as "Islamism" in 1712. The term appears in the U.S. Supreme Court decision in "In Re Ross" (1891). By the turn of the twentieth century the shorter and purely Arabic term "Islam" had begun to displace it, and by 1938, when Orientalist scholars completed "The Encyclopaedia of Islam", "Islamism" seems to have virtually disappeared from English usage.

The term "Islamism" acquired its contemporary connotations in French academia in the late 1970s and early 1980s. From French, it began to migrate to the English language in the mid-1980s, and in recent years has largely displaced the term Islamic fundamentalism in academic circles.

The new use of the term "Islamism" at first functioned as "a marker for scholars more likely to sympathize" with new Islamic movements; however, as the term gained popularity it became more specifically associated with political groups such as the Taliban or the Algerian Armed Islamic Group, as well as with highly publicized acts of violence.

"Islamists" who have spoken out against the use of the term, insisting they are merely "Muslims", include Ayatollah Mohammad Hussein Fadlallah (1935-2010), the spiritual mentor of Hezbollah, and Abbassi Madani (1931- ), leader of the Algerian Islamic Salvation Front.

A 2003 article in the "Middle East Quarterly" states:
In summation, the term Islamism enjoyed its first run, lasting from Voltaire to the First World War, as a synonym for Islam. Enlightened scholars and writers generally preferred it to Mohammedanism. Eventually both terms yielded to Islam, the Arabic name of the faith, and a word free of either pejorative or comparative associations. There was no need for any other term, until the rise of an ideological and political interpretation of Islam challenged scholars and commentators to come up with an alternative, to distinguish Islam as modern ideology from Islam as a faith... To all intents and purposes, Islamic fundamentalism and Islamism have become synonyms in contemporary American usage.

The Council on American–Islamic Relations complained in 2013 that the Associated Press's definition of "Islamist"—a "supporter of government in accord with the laws of Islam [and] who view the Quran as a political model"—had become a pejorative shorthand for "Muslims we don't like". Mansoor Moaddel, a sociologist at Eastern Michigan University, criticized it as "not a good term" because "the use of the term Islamist does not capture the phenomena that is quite heterogeneous."

The AP Stylebook entry for "Islamist" reads as follows: "An advocate or supporter of a political movement that favors reordering government and society in accordance with laws prescribed by Islam. Do not use as a synonym for Islamic fighters, militants, extremists or radicals, who may or may not be Islamists. Where possible, be specific and use the name of militant affiliations: al-Qaida-linked, Hezbollah, Taliban, etc. Those who view the Quran as a political model encompass a wide range of Muslims, from mainstream politicians to militants known as jihadi."

Islamism has been defined as:

Islamism takes different forms and spans a wide range of strategies and tactics towards the powers in place—"destruction, opposition, collaboration, indifference" that have varied as "circumstances have changed"—and thus is not a united movement.

Moderate and reformist Islamists who accept and work within the democratic process include parties like the Tunisian Ennahda Movement. Jamaat-e-Islami of Pakistan is basically a socio-political and democratic Vanguard party but has also gained political influence through military coup d'états in the past. Other Islamist groups like Hezbollah in Lebanon and Hamas in Palestine participate in the democratic and political process as well as armed attacks. Jihadist organizations like al-Qaeda and the Egyptian Islamic Jihad, and groups such as the Taliban, entirely reject democracy, often declaring as "kuffar" those Muslims who support it (see "takfirism"), as well as calling for violent/offensive jihad or urging and conducting attacks on a religious basis.

Another major division within Islamism is between what Graham E. Fuller has described as the fundamentalist "guardians of the tradition" (Salafis, such as those in the Wahhabi movement) and the "vanguard of change and Islamic reform" centered around the Muslim Brotherhood. Olivier Roy argues that "Sunni pan-Islamism underwent a remarkable shift in the second half of the 20th century" when the Muslim Brotherhood movement and its focus on Islamisation of pan-Arabism was eclipsed by the Salafi movement with its emphasis on "sharia rather than the building of Islamic institutions," and rejection of Shia Islam. Following the Arab Spring, Roy has described Islamism as "increasingly interdependent" with democracy in much of the Arab Muslim world, such that "neither can now survive without the other." While Islamist political culture itself may not be democratic, Islamists need democratic elections to maintain their legitimacy. At the same time, their popularity is such that no government can call itself democratic that excludes mainstream Islamist groups.

The relationship between the notions of Islam and Islamism has been subject to disagreement.

Hayri Abaza argues that the failure to distinguish between Islam and Islamism leads many in the West to support illiberal Islamic regimes, to the detriment of progressive moderates who seek to separate religion from politics.
In contrast, Abid Ullah Jan, writes "If Islam is a way of life, how can we say that those who want to live by its principles in legal, social, political, economic, and political spheres of life are not Muslims, but Islamists and believe in Islamism, not [just] Islam." A writer for the International Crisis Group maintains that "the conception of 'political Islam'" is a creation of Americans to explain the Iranian Islamic Revolution and apolitical Islam was a historical fluke of the "short-lived era of the heyday of secular Arab nationalism between 1945 and 1970", and it is quietist/non-political Islam, not Islamism, that requires explanation.

Another source distinguishes Islamist from Islamic "by the fact that the latter refers to a religion and culture in existence over a millennium, whereas the first is a political/religious phenomenon linked to the great events of the 20th century". Islamists have, at least at times, defined themselves as "Islamiyyoun/Islamists" to differentiate themselves from "Muslimun/Muslims". Daniel Pipes describes Islamism as a modern ideology that owes more to European utopian political ideologies and "isms" than to the traditional Islamic religion.

Few observers contest the influence of Islamism within the Muslim world. Following the collapse of the Soviet Union, political movements based on the liberal ideology of free expression and democratic rule have led the opposition in other parts of the world such as Latin America, Eastern Europe and many parts of Asia; however "the simple fact is that political Islam currently reigns as the most powerful ideological force across the Muslim world today".

People see the unchanging socioeconomic condition in the Muslim world as a major factor. Olivier Roy believes "the socioeconomic realities that sustained the Islamist wave are still here and are not going to change: poverty, uprootedness, crises in values and identities, the decay of the educational systems, the North-South opposition, and the problem of immigrant integration into the host societies".

The strength of Islamism also draws from the strength of religiosity in general in the Muslim world. Compared to Western societies, "[w]hat is striking about the Islamic world is that ... it seems to have been the least penetrated by irreligion". Where other peoples may look to the physical or social sciences for answers in areas which their ancestors regarded as best left to scripture, in the Muslim world, religion has become more encompassing, not less, as "in the last few decades, it has been the fundamentalists who have increasingly represented the cutting edge" of Muslim culture.

Even before the Arab Spring, Islamists in Egypt and other Muslim countries had been described as "extremely influential. ... They determine how one dresses, what one eats. In these areas, they are incredibly successful. ... Even if the Islamists never come to power, they have transformed their countries." Democratic, peaceful and political Islamists are now dominating the spectrum of Islamist ideology as well as the political system of the Muslim world. Moderate strains of Islamism have been described as "competing in the democratic public square in places like Turkey, Tunisia, Malaysia and Indonesia".

Moderate Islamism is the emerging Islamist discourses and movements which considered deviated from the traditional Islamist discourses of the mid-20th century. Moderate Islamism is characterized by pragmatic participation within the existing constitutional and political framework, in the most cases democratic institution. Moderate Islamists make up the majority of the contemporary Islamist movements. From the philosophical perspective, their discourses are represented by reformation or reinterpretation of modern socio-political institutions and values imported from the West including democracy. This had led to the conception of Islamic form of such institutions, and Islamic interpretations are often attempted within this conception. In the example of democracy, Islamic democracy as an Islamized form of the system has been intellectually developed. In Islamic democracy, the concept of "shura", the tradition of consultation which considered as Sunnah of the prophet Muhammad, is invoked to Islamically reinterpret and legitimatize the institution of democracy.

Performance, goal, strategy, and outcome of moderate Islamist movements vary considerably depending on the country and its socio-political and historical context. In terms of performance, most of the Islamist political parties are oppositions. However, there are few examples they govern or obtain the substantial amount of the popular votes. This includes National Congress of Sudan, National Iraqi Alliance of Iraq and Justice and Development Party (PJD) of Morocco. Their goal also ranges widely. The Ennahda Movement of Tunisia and Prosperous Justice Party (PKS) of Indonesia formally resigned their vision of implementing sharia. In Morocco, PJD supported King Muhammad VI's "Mudawana", a "startlingly progressive family law" which grants women the right to a divorce, raises the minimum age for marriage to 18, and, in the event of separation, stipulates equal distribution of property. To the contrary, National Congress of Sudan has implemented the strict interpretation of sharia with the foreign support from the conservative states. Movements of the former category are also termed as Post-Islamism (see below). Their political outcome is interdependent with their goal and strategy, in which what analysts call "inclusion-moderation theory" is in effect. Inclusion-moderation theory assumes that the more lenient the Islamists become, the less likely their survival will be threatened. Similarly, the more accommodating the government be, the less extreme Islamists become.

Moderate Islamism within the democratic institution is a relatively recent phenomenon. Throughout the 80s and 90s, major moderate Islamist movements such as the Muslim Brotherhood and the Ennahda were excluded from democratic political participation. Islamist movements operated within the state framework were markedly scrutinized during the Algerian Civil War (1991-2002) and after the increase of terrorism in Egypt in the 90s. Reflecting on these failures, Islamists turned increasingly into revisionist and receptive to democratic procedures in the 21st century. The possibility of accommodating this new wave of modernist Islamism has been explored among the Western intellectuals, with the concept such as Turkish model was proposed. The concept was inspired by the perceived success of Turkish Justice and Development Party (AKP) led by Recep Tayyip Erdoğan in harmonizing the Islamist principles within the secular state framework. Turkish model, however, has been considered came "unstuck" after recent purge and violations of democratic principles by the Erdoğan regime. Critics of the concept hold that Islamist aspirations are fundamentally incompatible with the democratic principles, thus even moderate Islamists are totalitarian in nature. As such, it requires strong constitutional checks and the effort of the mainstream Islam to detach political Islam from the public discourses.

Post-Islamism is a term proposed by Iranian political sociologist Asef Bayat, referring to the Islamist movements which marked by the critical departure from the traditional Islamist discourses of the mid-20th century. Bayat explained it as "a condition where, following a phase of experimentation, the appeal, energy, symbols and sources of legitimacy of Islamism get exhausted, even among its once-ardent supporters. As such, post-Islamism is not anti-Islamic, but rather reflects a tendency to resecularize religion." It originally pertained only to Iran, where "post-Islamism is expressed in the idea of fusion between Islam (as a personalized faith) and individual freedom and choice; and post-Islamism is associated with the values of democracy and aspects of modernity". A 2008 Lowy Institute for International Policy paper suggests that PKS of Indonesia and AKP of Turkey are post-Islamist. The characterization can be applied to Malaysian Islamic Party (PAS), and used to describe the "ideological evolution" within the Ennahda of Tunisia.

The contemporary Salafi movement encompasses a broad range of ultraconservative Islamist doctrines which share the reformist mission of Ibn Taymiyyah. From the perspective of political Islam, the Salafi movement can be broadly categorized into three groups; the quietist (or the purist), the activist (or "haraki") and the jihadist (Salafi jihadism, see below). The quietist school advocates for societal reform through religious education and proselytizing rather than political activism. The activist school, to the contrary, encourages political participation within the constitutional and political framework. The jihadist school is inspired by the ideology of Sayyid Qutb (Qutbism, see below), and rejects the legitimacy of secular institutions and promotes the revolution in order to pave the way for the establishment of a new Caliphate.

The quietist Salafi movement is stemming from the teaching of Nasiruddin Albani, who challenged the notion of "taqlid" (imitation, conformity to the legal precedent) as a blind adherence. As such, they alarm the political participation as potentially leading to the division of the Muslim community. This school is exemplified by Madkhalism which based on the writings of Rabee al-Madkhali. Madkhalism was originated in the 90s Saudi Arabia, as a reaction against the rise of the Salafi activism and the threat of Salafi Jihadism. It rejects any kind of opposition against the secular governance, thus endorsed by the authoritarian governments of Egypt and Saudi Arabia during the 90s. The influence of the quietist school has waned significantly in the Middle East recently, as the governments began incorporating Islamist factions emanating from the popular demand.

The politically active Salafi movement, Salafi activism or "harakis", is based on the religious belief that endorses non-violent political activism in order to protect God's Divine governance. This means that politics is a field which requires Salafi principles to be applied as well, in the same manner with other aspects of society and life. Salafi activism was originated in the 50s to 60s Saudi Arabia, where many Muslim Brothers had taken refuge from the prosecution by the Nasser regime. There, Muslim Brothers' Islamism had synthesized with Salafism, and led to the creation of the Salafi activist trend exemplified by the Sahwa movement in the 80s, promulgated by Safar Al-Hawali and Salman al-Ouda. Today, the school makes up the majority of Salafism. There are many active Salafist political parties throughout the Muslim world, including Al Nour Party of Egypt, Al Islah of Yemen and Al Asalah of Bahrain.

The antecedent of the contemporary Salafi movement is Wahhabism, which traces back to the 18th-century reform movement in Najd by Muhammad ibn Abd al-Wahhab. Although having different roots, Wahhabism and Salafism are considered more or less merged in the 60s Saudi Arabia. In the process, Salafism had been greatly influenced by Wahhabism, and today they share the similar religious outlook. Wahhabism is also described as a Saudi brand of Salafism. From the political perspective, Wahhabism is marked in its teaching of "bay'ah" (oath to allegiance), which requires Muslims to present an allegiance to the ruler of the society. Wahhabis have traditionally given their allegiance to the House of Saud, and this has made them apolitical in Saudi Arabia. However, there are small numbers of other strains including Salafi Jihadist offshoot which decline to present an allegiance to the House of Saud. Wahhabism is also characterized by its disinterest in social justice, anticolonialism, or economic equality, expounded upon by the mainstream Islamists. Historically, Wahhabism was state-sponsored and internationally propagated by Saudi Arabia with the help of funding from mainly Saudi petroleum exports, leading to the "explosive growth" of its influence (and subsequently, the influence of Salafism) from the 70s (a phenomenon often dubbed as Petro-Islam). Today, both Wahhabism and Salafism exert their influence worldwide, and they have been indirectly contributing to the upsurge of Salafi Jihadism as well.

Qutbism is an ideology formulated by Sayyid Qutb, an influential figure of the Muslim Brotherhood during the 50s and 60s, which justifies the use of violence in order to push the Islamist goals. Qutbism is marked by the two distinct methodological concepts; one is "takfirism", which in the context of Qutbism, indicates the excommunication of fellow Muslims who are deemed equivalent to apostate, and another is "offensive Jihad", a concept which promotes violence in the name of Islam against the perceived "kuffar" (infidels). Based on the two concepts, Qutbism promotes engagement against the state apparatus in order to topple down its regime. Fusion of Qutbism and Salafi Movement had resulted in the development of Salafi jihadism (see below).

Qutbism is considered a product of the extreme repression experienced by Qutb and his fellow Muslim Brothers under the Nasser regime, which was resulted from the 1954 Muslim Brothers plot to assassinate Nasser. During the repression, thousands of Muslim Brothers were imprisoned, many of them, including Qutb, tortured and held in concentration camps. Under this condition, Qutb had cultivated his Islamist ideology in his seminal work "Ma'alim fi-l-Tariq (Milestones)", in which he equated the Muslims within the Nasser regime with secularism and the West, and described them as regression back to "jahiliyyah" (period of time before the advent of Islam). In this context, he allowed the "tafkir" (which was an unusual practice before the rejuvenation by Qutb) of said Muslims. Although Qutb was executed before the completion of his ideology, his idea was disseminated and continuously expanded by the later generations, among them Abdullah Yusuf Azzam and Ayman Al-Zawahiri, who was a student of Qutb's brother Muhammad Qutb and later became a mentor of Osama bin Laden. Al-Zawahiri was considered "the purity of Qutb's character and the torment he had endured in prison," and had played an extensive role in the normalization of offensive Jihad within the Qutbist discourse. Both al-Zawahiri and bin Laden had become the core of Jihadist movements which exponentially developed in the backdrop of the late 20th-century geopolitical crisis throughout the Muslim world.

Salafi jihadism is a term coined by Gilles Kepel in 2002, referring to the ideology which actively promotes and conducts violence and terrorism in order to pursue the establishment of an Islamic state or a new Caliphate. Today, the term is often simplified to "Jihadism" or "Jihadist movement" in popular usage according to Martin Kramer. It is a hybrid ideology between Qutbism, Salafism, Wahhabism and other minor Islamist strains. Qutbism taught by scholars like Abdullah Azzam provided the political intellectual underpinnings with the concepts like takfirism, and Salafism and Wahhabism provided the religious intellectual input. Salafi Jihadism makes up a tiny minority of the contemporary Islamist movements.

Distinct characteristics of Salafi Jihadism noted by Robin Wright include the formal process of taking "bay'ah" (oath of allegiance) to the leader, which is inspired by the Wahhabi teaching. Another characteristic is its flexibility to cut ties with the less-popular movements when its strategically or financially convenient, exemplified by the relations between al-Qaeda and al-Nusra Front. Other marked developments of Salafi Jihadism include the concepts of "near enemy" and "far enemy". "Near enemy" connotes the despotic regime occupying the Muslim society, and the term was coined by Mohammed Abdul-Salam Farag in order to justify the assassination of Anwar al-Sadat by the Salafi Jihadi organization Egyptian Islamic Jihad (EIJ) in 1981. Later, the concept of "far enemy" which connotes the West was introduced and formally declared by al-Qaeda in 1996.

Salafi Jihadism emerged out during the 80s when the Soviet invaded Afghanistan. Local mujahideen had extracted financial, logistical and military support from Saudi Arabia, Pakistan and the United States. Later, Osama bin Laden established al-Qaeda as a transnational Salafi Jihadi organization in 1988 to capitalize this financial, logistical and military network and to expand their operation. The ideology had seen its rise during the 90s when the Muslim world experienced numerous geopolitical crisis, notably the Algerian Civil War (1991–2002), Bosnian War (1992–1995), and the First Chechen War (1994–1996). Within these conflicts, political Islam often acted as a mobilizing factor for the local belligerents, who demanded financial, logistical and military support from al-Qaeda, in the exchange for active proliferation of the ideology. After the 1998 bombings of US embassies, September 11 attacks (2001), the US-led invasion of Afghanistan (2001) and Iraq (2003), Salafi Jihadism had seen its momentum. However, it got devastated by the US counterterrorism operations, culminated in bin Laden's death in 2011. After the Arab Spring (2011) and subsequent Syrian Civil War (2011–present), the remnants of al-Qaeda franchise in Iraq had restored their capacity, which rapidly developed into the Islamic State of Iraq and the Levant, spreading its influence throughout the conflict zones of MENA region and the globe.

Some Islamic revivalist movements and leaders pre-dating Islamism include:

The end of the 19th century saw the dismemberment of most of the Muslim Ottoman Empire by non-Muslim European colonial powers. The empire spent massive sums on Western civilian and military technology to try to modernize and compete with the encroaching European powers, and in the process went deep into debt to these powers.

In this context, the publications of Jamal ad-din al-Afghani (1837–97), Muhammad Abduh (1849–1905) and Rashid Rida (1865–1935) preached Islamic alternatives to the political, economic, and cultural decline of the empire. Muhammad Abduh and Rashid Rida formed the beginning of the Islamist movement, as well as the reformist Islamist movement.

Their ideas included the creation of a truly Islamic society under sharia law, and the rejection of taqlid, the blind imitation of earlier authorities, which they believed deviated from the true messages of Islam. Unlike some later Islamists, Early Salafiyya strongly emphasized the restoration of the Caliphate.

Muhammad Iqbal was a philosopher, poet and politician in British India who is widely regarded as having inspired the Islamic Nationalism and Pakistan Movement in British India. Iqbal is admired as a prominent classical poet by Pakistani, Iranian, Indian and other international scholars of literature. Though Iqbal is best known as an eminent poet, he is also a highly acclaimed "Islamic philosophical thinker of modern times".

While studying law and philosophy in England and Germany, Iqbal became a member of the London branch of the All India Muslim League. He came back to Lahore in 1908. While dividing his time between law practice and philosophical poetry, Iqbal had remained active in the Muslim League. He did not support Indian involvement in World War I and remained in close touch with Muslim political leaders such as Muhammad Ali Johar and Muhammad Ali Jinnah. He was a critic of the mainstream Indian nationalist and secularist Indian National Congress. Iqbal's seven English lectures were published by Oxford University press in 1934 in a book titled The Reconstruction of Religious Thought in Islam. These lectures dwell on the role of Islam as a religion as well as a political and legal philosophy in the modern age.

Iqbal expressed fears that not only would secularism and secular nationalism weaken the spiritual foundations of Islam and Muslim society, but that India's Hindu-majority population would crowd out Muslim heritage, culture and political influence. In his travels to Egypt, Afghanistan, Palestine and Syria, he promoted ideas of greater Islamic political co-operation and unity, calling for the shedding of nationalist differences. Sir Mummad Iqbal was elected president of the Muslim League in 1930 at its session in Allahabad as well as for the session in Lahore in 1932. In his Allahabad Address on 29 December 1930, Iqbal outlined a vision of an independent state for Muslim-majority provinces in northwestern India. This address later inspired the Pakistan movement.

The thoughts and vision of Iqbal later influenced many reformist Islamists, e.g., Muhammad Asad, Sayyid Abul Ala Maududi and Ali Shariati.

Sayyid Abul Ala Maududi was an important early twentieth-century figure in the Islamic revival in India, and then after independence from Britain, in Pakistan. Trained as a lawyer he chose the profession of journalism, and wrote about contemporary issues and most importantly about Islam and Islamic law. Maududi founded the Jamaat-e-Islami party in 1941 and remained its leader until 1972. However, Maududi had much more impact through his writing than through his political organising. His extremely influential books (translated into many languages) placed Islam in a modern context, and influenced not only conservative ulema but liberal modernizer Islamists such as al-Faruqi, whose "Islamization of Knowledge" carried forward some of Maududi's key principles.

Maududi believed that Islam was all-encompassing: "Everything in the universe is 'Muslim' for it obeys God by submission to His laws... The man who denies God is called Kafir (concealer) because he conceals by his disbelief what is inherent in his nature and embalmed in his own soul."

Maududi also believed that Muslim society could not be Islamic without Sharia, and Islam required the establishment of an Islamic state. This state should be a "theo-democracy," based on the principles of: "tawhid" (unity of God), "risala" (prophethood) and "khilafa" (caliphate).
Although Maududi talked about Islamic revolution, by "revolution" he meant not the violence or populist policies of the Iranian Revolution, but the gradual changing the hearts and minds of individuals from the top of society downward through an educational process or "da'wah".

Roughly contemporaneous with Maududi was the founding of the Muslim Brotherhood in Ismailiyah, Egypt in 1928 by Hassan al Banna. His was arguably the first, largest and most influential modern Islamic political/religious organization. Under the motto "the Qur'an is our constitution,"
it sought Islamic revival through preaching and also by providing basic community services including schools, mosques, and workshops. Like Maududi, Al Banna believed in the necessity of government rule based on Shariah law implemented gradually and by persuasion, and of eliminating all imperialist influence in the Muslim world.

Some elements of the Brotherhood, though perhaps against orders, did engage in violence against the government, and its founder Al-Banna was assassinated in 1949 in retaliation for the assassination of Egypt's premier Mahmud Fami Naqrashi three months earlier. The Brotherhood has suffered periodic repression in Egypt and has been banned several times, in 1948 and several years later following confrontations with Egyptian president Gamal Abdul Nasser, who jailed thousands of members for several years.

Despite periodic repression, the Brotherhood has become one of the most influential movements in the Islamic world, particularly in the Arab world. For many years it was
described as "semi-legal" and was the only opposition group in Egypt able to field candidates during elections. In the 2011–12 Egyptian parliamentary election, the political parties identified as "Islamist" (the Brotherhood's Freedom and Justice Party, Salafi Al-Nour Party and liberal Islamist Al-Wasat Party) won 75% of the total seats. Mohamed Morsi, an Islamist of Muslim Brotherhood, was the first democratically elected president of Egypt. He was deposed during the 2013 Egyptian coup d'état.

Maududi's political ideas influenced Sayyid Qutb a leading member of the Muslim Brotherhood movement, and one of the key philosophers of Islamism and highly influential thinkers of Islamic universalism. Qutb believed things had reached such a state that the Muslim community had literally ceased to exist. It "has been extinct for a few centuries," having reverted to Godless ignorance (Jahiliyya).

To eliminate jahiliyya, Qutb argued Sharia, or Islamic law, must be established. Sharia law was not only accessible to humans and essential to the existence of Islam, but also all-encompassing, precluding "evil and corrupt" non-Islamic ideologies like communism, nationalism, or secular democracy.

Qutb preached that Muslims must engage in a two-pronged attack of converting individuals through preaching Islam peacefully and also waging what he called militant jihad so as to forcibly eliminate the "power structures" of Jahiliyya—not only from the Islamic homeland but from the face of the earth.

Qutb was both a member of the brotherhood and enormously influential in the Muslim world at large. Qutb is considered by some (Fawaz A. Gerges) to be "the founding father and leading theoretician" of modern jihadists, such as Osama bin Laden. However, the Muslim Brotherhood in Egypt and in Europe has not embraced his vision of undemocratic Islamic state and armed jihad, something for which they have been denounced by radical Islamists.

Islamic fervor was understood as a weapon that the United States could use as a weapon in its Cold War against the Soviet Union and its communist allies because communism professes atheism. In a September 1957 White House meeting between U.S. President Eisenhower and senior U.S. foreign policy officials, it was agreed to use the communists' lack of religion against them by setting up a secret task force to deliver weapons to Middle East despots, including the Saudi Arabian rulers. "We should do everything possible to stress the 'holy war' aspect" that has currency in the Middle East, President Eisenhower stated in agreement. 

The quick and decisive defeat of the Arab troops during the Six-Day War by Israeli troops constituted a pivotal event in the Arab Muslim world. The defeat along with economic stagnation in the defeated countries, was blamed on the secular Arab nationalism of the ruling regimes. A steep and steady decline in the popularity and credibility of secular, socialist and nationalist politics ensued. Ba'athism, Arab socialism, and Arab nationalism suffered, and different democratic and anti-democratic Islamist movements inspired by Maududi and Sayyid Qutb gained ground.

The first modern "Islamist state" (with the possible exception of Zia's Pakistan) was established among the Shia of Iran. In a major shock to the rest of the world, Ayatollah Ruhollah Khomeini led the Iranian Revolution of 1979 in order to overthrow the oil-rich, well-armed, Westernized and pro-American secular monarchy ruled by Shah Muhammad Reza Pahlavi.

The views of Ali Shariati, the ideologue of the Iranian Revolution, resembled those of Mohammad Iqbal, the ideological father of the State of Pakistan, but Khomeini's beliefs are perceived to be placed somewhere between the beliefs of Shia Islam and the beliefs of Sunni Islamic thinkers like Mawdudi and Qutb. He believed that complete imitation of the Prophet Mohammad and his successors such as Ali for the restoration of Sharia law was essential to Islam, that many secular, Westernizing Muslims were actually agents of the West and therefore serving Western interests, and that acts such as the "plundering" of Muslim lands was part of a long-term conspiracy against Islam by Western governments.

His views differed from those of Sunni scholars in:

The revolution was influenced by Marxism through Islamist thought and also by writings that sought either to counter Marxism (Muhammad Baqir al-Sadr's work) or to integrate socialism and Islamism (Ali Shariati's work). A strong wing of the revolutionary leadership was made up of leftists or "radical populists", such as Ali Akbar Mohtashami-Pur.

While initial enthusiasm for the Iranian revolution in the Muslim world was intense, it has waned as critics hold and campaign that "purges, executions, and atrocities tarnished its image".

The Islamic Republic has also maintained its hold on power in Iran in spite of US economic sanctions, and has created or assisted like-minded Shia terrorist groups in Iraq, Egypt, Syria, Jordan (SCIRI) and Lebanon (Hezbollah) (two Muslim countries that also have large Shiite populations).
During the 2006 Israel-Lebanon conflict, the Iranian government enjoyed something of a resurgence in popularity amongst the predominantly Sunni "Arab street," due to its support for Hezbollah and to President Mahmoud Ahmadinejad's vehement opposition to the United States and his call that Israel shall vanish.

The strength of the Islamist movement was manifest in an event which might have seemed sure to turn Muslim public opinion against fundamentalism, but did just the opposite. In 1979 the Grand Mosque in Mecca Saudi Arabia was seized by an armed fundamentalist group and held for over a week. Scores were killed, including many pilgrim bystanders in a gross violation of one of the most holy sites in Islam (and one where arms and violence are strictly forbidden).

Instead of prompting a backlash against the movement from which the attackers originated, however, Saudi Arabia, already very conservative, responded by shoring up its fundamentalist credentials with even more Islamic restrictions. Crackdowns followed on everything from shopkeepers who did not close for prayer and newspapers that published pictures of women, to the selling of dolls, teddy bears (images of animate objects are considered haraam), and dog food (dogs are considered unclean).

In other Muslim countries, blame for and wrath against the seizure was directed not against fundamentalists, but against Islamic fundamentalism's foremost geopolitical enemy—the United States. Ayatollah Khomeini sparked attacks on American embassies when he announced:
It is not beyond guessing that this is the work of criminal American imperialism and international Zionism despite the fact that the object of the fundamentalists' revolt was the Kingdom of Saudi Arabia, America's major ally in the region. Anti-American demonstrations followed in the Philippines, Turkey, Bangladesh, India, the UAE, Pakistan, and Kuwait. The US Embassy in Libya was burned by protesters chanting pro-Khomeini slogans and the embassy in Islamabad, Pakistan was burned to the ground.

In 1979, the Soviet Union deployed its 40th Army into Afghanistan, attempting to suppress an Islamic rebellion against an allied Marxist regime in the Afghan Civil War. The conflict, pitting indigenous impoverished Muslims (mujahideen) against an anti-religious superpower, galvanized thousands of Muslims around the world to send aid and sometimes to go themselves to fight for their faith. Leading this pan-Islamic effort was Palestinian sheikh Abdullah Yusuf Azzam. While the military effectiveness of these "Afghan Arabs" was marginal, an estimated 16,000 to 35,000 Muslim volunteers came from around the world to fight in Afghanistan.

When the Soviet Union abandoned the Marxist Najibullah regime and withdrew from Afghanistan in 1989 (the regime finally fell in 1992), the victory was seen by many Muslims as the triumph of Islamic faith over superior military power and technology that could be duplicated elsewhere.

The jihadists gained legitimacy and prestige from their triumph both within the militant community and among ordinary Muslims, as well as the confidence to carry their jihad to other countries where they believed Muslims required assistance.|

The "veterans of the guerrilla campaign" returning home to Algeria, Egypt, and other countries "with their experience, ideology, and weapons," were often eager to continue armed jihad.

The collapse of the Soviet Union itself, in 1991, was seen by many Islamists, including Bin Laden, as the defeat of a superpower at the hands of Islam. Concerning the $6 billion in aid given by the US and Pakistan's military training and intelligence support to the mujahideen, bin Laden wrote: "[T]he US has no mentionable role" in "the collapse of the Soviet Union ... rather the credit goes to God and the mujahidin" of Afghanistan.

Another factor in the early 1990s that worked to radicalize the Islamist movement was the Gulf War, which brought several hundred thousand US and allied non-Muslim military personnel to Saudi Arabian soil to put an end to Saddam Hussein's occupation of Kuwait. Prior to 1990 Saudi Arabia played an important role in restraining the many Islamist groups that received its aid. But when Saddam, secularist and Ba'athist dictator of neighboring Iraq, attacked Kuwait (his enemy in the war), western troops came to protect the Saudi monarchy. Islamists accused the Saudi regime of being a puppet of the west.

These attacks resonated with conservative Muslims and the problem did not go away with Saddam's defeat either, since American troops remained stationed in the kingdom, and a de facto cooperation with the Palestinian-Israeli peace process developed. Saudi Arabia attempted to compensate for its loss of prestige among these groups by repressing those domestic Islamists who attacked it (bin Laden being a prime example), and increasing aid to Islamic groups (Islamist madrassas around the world and even aiding some violent Islamist groups) that did not, but its pre-war influence on behalf of moderation was greatly reduced. One result of this was a campaign of attacks on government officials and tourists in Egypt, a bloody civil war in Algeria and Osama bin Laden's terror attacks climaxing in the 9/11 attack.

In Afghanistan, the mujahideen's victory against the Soviet Union in the 1980s did not lead to justice and prosperity, due to a vicious and destructive civil war between political and tribal warlords, making Afghanistan one of the poorest countries on earth. In 1992, the Democratic Republic of Afghanistan ruled by communist forces collapsed, and democratic Islamist elements of mujahdeen founded the Islamic State of Afghanistan. In 1996, a more conservative and anti-democratic Islamist movement known as the Taliban rose to power, defeated most of the warlords and took over roughly 80% of Afghanistan.

The Taliban were spawned by the thousands of madrasahs the Deobandi movement established for impoverished Afghan refugees and supported by governmental and religious groups in neighboring Pakistan. The Taliban differed from other Islamist movements to the point where they might be more properly described as Islamic fundamentalist or neofundamentalist, interested in spreading "an idealized and systematized version of conservative tribal village customs" under the label of Sharia to an entire country. Their ideology was also described as being influenced by Wahhabism, and the extremist jihadism of their guest Osama bin Laden.

The Taliban considered "politics" to be against Sharia and thus did not hold elections. They were led by Mullah Mohammed Omar who was given the title "Amir al-Mu'minin" or Commander of the Faithful, and a pledge of loyalty by several hundred Taliban-selected Pashtun clergy in April 1996. Taliban were overwhelmingly Pashtun and were accused of not sharing power with the approximately 60% of Afghans who belonged to other ethnic groups. (see: Taliban#Ideology)

The Taliban's hosting of Osama bin Laden led to an American-organized attack which drove them from power following the 9/11 attacks.
Taliban are still very much alive and fighting a vigorous insurgency with suicide bombings and armed attacks being launched against NATO and Afghan government targets.

An Islamist movement influenced by Salafism and the jihad in Afghanistan, as well as the Muslim Brotherhood, was the FIS or Front Islamique de Salut (the Islamic Salvation Front) in Algeria. Founded as a broad Islamist coalition in 1989 it was led by Abbassi Madani, and a charismatic Islamist young preacher, Ali Belhadj. Taking advantage of economic failure and unpopular social liberalization and secularization by the ruling leftist-nationalist FLN government, it used its preaching to advocate the establishment of a legal system following Sharia law, economic liberalization and development program, education in Arabic rather than French, and gender segregation, with women staying home to alleviate the high rate of unemployment among young Algerian men. The FIS won sweeping victories in local elections and it was going to win national elections in 1991 when voting was canceled by a military coup d'état.

As Islamists took up arms to overthrow the government, the FIS's leaders were arrested and it became overshadowed by Islamist guerrilla groups, particularly the Islamic Salvation Army, MIA and Armed Islamic Group (or GIA). A bloody and devastating civil war ensued in which between 150,000 and 200,000 people were killed over the next decade.

The civil war was not a victory for Islamists. By 2002 the main guerrilla groups had either been destroyed or had surrendered. The popularity of Islamist parties has declined to the point that "the Islamist candidate, Abdallah Jaballah, came a distant third with 5% of the vote" in the 2004 presidential election.

Jamaat-e-Islami Bangladesh is the largest Islamist party in the country and supports the implementation of Sharia law and promotes the country's main right-wing politics. Since 2000, the main political opposition Bangladesh Nationalist Party (BNP) has been allied with it and another Islamic party, Islami Oikya Jote. Some of their leaders and supporters, including former ministers and MPs, have been hanged for alleged war crimes during Bangladesh's struggle for independence and speaking against the ruling Bangladesh Awami League.

In the 2012, the party named "Islam" had four candidates and they were elected in Molenbeek and Anderlecht. In 2018, they ran candidates in 28 municipalities. Its policies include schools must offer halal food and women must be able to wear a headscarf anywhere. Another of the Islam Party's goals is to separate men and women on public transportation. The party's president argues this policy will help protect women from sexual harassment.

The Islamist movements gradually grew since the 1990s. The first Islamist groups and networks were predominantly influenced by the countries they immigrated from. Those involved had close contact with militant islamists in the Middle East, South Asia and North Africa. Their operations had supporting militant groups financially as their first priority. Since the 1990s, people from the Islamist movements joined several conflicts to train with or participate in fighting with Islamist militants.

In the 2000s the Islamist movements grew and by 2014 there were militants among the Islamist movements in Copenhagen, Aarhus and Odense. Several people from crime gangs join Islamist movements that sympathise with militant Islamism. The militant Islamist movement were estimated to encompass some hundreds in 2014.

The Danish National Centre for Social Research released a report commissioned by the Ministry of Children, Integration and Social Affairs documenting 15 extremist groups operating in Denmark. The majority of these organizations were non-Muslim far-right or far-left groups, but five were Sunni Islamist groups. These Sunni Islamist groups include Hizb ut-Tahrir Denmark, "Dawah-bærere" (Dawah Carriers), "Kaldet til Islam" (The Call to Islam), "Dawah-centret" (The Dawah Centre), and the "Muslimsk Ungdomscenter" (The Muslim Youth Centre). All of these Sunni Islamist groups operate in Greater Copenhagen with the exception of "Muslimsk Ungdomscenter", which operates in Aarhus. Altogether, roughly 195 to 415 Muslims belong to one of these organizations and most are young men.

While Qutb's ideas became increasingly radical during his imprisonment prior to his execution in 1966, the leadership of the Brotherhood, led by Hasan al-Hudaybi, remained moderate and interested in political negotiation and activism. Fringe or splinter movements inspired by the final writings of Qutb in the mid-1960s (particularly the manifesto "Milestones", a.k.a. "Ma'alim fi-l-Tariq") did, however, develop and they pursued a more radical direction. By the 1970s, the Brotherhood had renounced violence as a means of achieving its goals.

The path of violence and military struggle was then taken up by the Egyptian Islamic Jihad organization responsible for the assassination of Anwar Sadat in 1981. Unlike earlier anti-colonial movements the extremist group directed its attacks against what it believed were "apostate" leaders of Muslim states, leaders who held secular leanings or who had introduced or promoted Western/foreign ideas and practices into Islamic societies. Its views were outlined in a pamphlet written by Muhammad Abd al-Salaam Farag, in which he states:
...there is no doubt that the first battlefield for jihad is the extermination of these infidel leaders and to replace them by a complete Islamic Order...

Another of the Egyptian groups which employed violence in their struggle for Islamic order was al-Gama'a al-Islamiyya (Islamic Group). Victims of their campaign against the Egyptian state in the 1990s included the head of the counter-terrorism police (Major General Raouf Khayrat), a parliamentary speaker (Rifaat al-Mahgoub), dozens of European tourists and Egyptian bystanders, and over 100 Egyptian police. Ultimately the campaign to overthrow the government was unsuccessful, and the major jihadi group, Jamaa Islamiya (or al-Gama'a al-Islamiyya), renounced violence in 2003. Other lesser known groups include the Islamic Liberation Party, Salvation from Hell and Takfir wal-Hijra, and these groups have variously been involved in activities such as attempted assassinations of political figures, arson of video shops and attempted takeovers of government buildings.

The Democratic Union of Muslims, a party founded in 2012, planned to take part in 2019 municipal elections. They presented candidate lists for 50 different cities. The Democratic Union of Muslims also fielded candidates for European Parliament elections. The rise of the party can be attributed to French Muslim dissatisfaction with mainstream political parties. Ultimately, it represents an alternative to the Islamophobia in France.

Hamas is a Palestinian Sunni Islamist organization that governs the Gaza Strip where it has moved to establish sharia law in matters such as separation of the genders, using the lash for punishment, and Islamic dress code.<ref name="islamist/islamic">* "This is particularly the case in view of the scholarly debate on the compatibility of Islam and democracy but even more so in view of Hamas's self-definition as an Islamic national liberation movement." "The Palestinian Hamas: vision, violence, and coexistence", by Shaul Mishal & Avraham Sela, 2006, p. xxviii ;
Hamas also has a military resistance wing, the Izz ad-Din al-Qassam Brigades.

For some decades prior to the First Palestine Intifada in 1987, the Muslim Brotherhood in Palestine took a "quiescent" stance towards Israel, focusing on preaching, education and social services, and benefiting from Israel's "indulgence" to build up a network of mosques and charitable organizations. As the First Intifada gathered momentum and Palestinian shopkeepers closed their shops in support of the uprising, the Brotherhood announced the formation of HAMAS ("zeal"), devoted to Jihad against Israel. Rather than being more moderate than the PLO, the 1988 Hamas charter took a more uncompromising stand, calling for the destruction of Israel and the establishment of an Islamic state in Palestine. It was soon competing with and then overtaking the PLO for control of the intifada. The Brotherhood's base of devout middle class found common cause with the impoverished youth of the intifada in their cultural conservatism and antipathy for activities of the secular middle class such as drinking alcohol and going about without hijab.

Hamas has continued to be a major player in Palestine. From 2000 to 2007 it killed 542 people in 140 suicide bombing or "martyrdom operations". In the January 2006 legislative election—its first foray into the political process—it won the majority of the seats, and in 2007 it drove the PLO out of Gaza. Hamas has been praised by Muslims for driving Israel out of the Gaza Strip, but criticized for failure to achieve its demands in the 2008–09 and 2014 Gaza Wars despite heavy destruction and significant loss of life.

Early in the history of the state of Pakistan (12 March 1949), a parliamentary resolution (the Objectives Resolution) was adopted in accordance with the vision of founding fathers of Pakistan (Muhammad Iqbal, Muhammad Ali Jinnah, Liaquat Ali Khan). proclaiming:

This resolution later became a key source of inspiration for writers of the Constitution of Pakistan, and is included in the constitution as preamble.

In July 1977, General Zia-ul-Haq overthrew Prime Minister Zulfiqar Ali Bhutto's regime in Pakistan. Ali Bhutto, a leftist in democratic competition with Islamists, had announced banning alcohol and nightclubs within six months, shortly before he was overthrown. Zia-ul-Haq was much more committed to Islamism, and "Islamization" or implementation of Islamic law, became a cornerstone of his eleven-year military dictatorship and Islamism became his "official state ideology". Zia ul Haq was an admirer of Mawdudi and Mawdudi's party Jamaat-e-Islami became the "regime's ideological and political arm". In Pakistan this Islamization from above was "probably" more complete "than under any other regime except those in Iran and Sudan," but Zia-ul-Haq was also criticized by many Islamists for imposing "symbols" rather than substance, and using Islamization to legitimize his means of seizing power. Unlike neighboring Iran, Zia-ul-Haq's policies were intended to "avoid revolutionary excess", and not to strain relations with his American and Persian Gulf state allies. Zia-ul-Haq was killed in 1988 but Islamization remains an important element in Pakistani society.

For many years, Sudan had an Islamist regime under the leadership of Hassan al-Turabi. His National Islamic Front first gained influence when strongman General Gaafar al-Nimeiry invited members to serve in his government in 1979. Turabi built a powerful economic base with money from foreign Islamist banking systems, especially those linked with Saudi Arabia. He also recruited and built a cadre of influential loyalists by placing sympathetic students in the university and military academy while serving as minister of education.

After al-Nimeiry was overthrown in 1985 the party did poorly in national elections, but in 1989 it was able to overthrow the elected post-al-Nimeiry government with the help of the military. Turabi was noted for proclaiming his support for the democratic process and a liberal government before coming to power, but strict application of sharia law, torture and mass imprisonment of the opposition, and an intensification of the long-running war in southern Sudan, once in power. The NIF regime also harbored Osama bin Laden for a time (before 9/11), and worked to unify Islamist opposition to the American attack on Iraq in the 1991 Gulf War.

After Sudanese intelligence services were implicated in an assassination attempt on the President of Egypt, UN economic sanctions were imposed on Sudan, a poor country, and Turabi fell from favor. He was imprisoned for a time in 2004–05. Some of the NIF policies, such as the war with the non-Muslim south, have been reversed, though the National Islamic Front still holds considerable power in the government of Omar al-Bashir and National Congress Party, another Islamist party in country.

Switzerland is not normally seen as a center of Islamism, especially when compared to countries such as Belgium or France. However, from 2012 to 2018, the majority of the country's jihadist and would-be jihadist population were radicalized in Switzerland.

Turkey had a number of Islamist parties, often changing names as they were banned by the constitutional court for anti-secular activities. Necmettin Erbakan (1926–2011) was the leader of several of the parties, the National Order Party ("Milli Nizam Partisi", 1970–1971), the National Salvation Party ("Milli Selamet Partisi", 1972–1981), and the Welfare Party ("Refah Partisi", 1983-1998); he also became a member of the Felicity Party ("Saadet Partisi", 2003–2011). Current Turkish President Recep Tayyip Erdoğan has long been considered a champion of political Islam.

The Justice and Development Party (AKP), which has dominated Turkish politics since 2002, is sometimes described as Islamist, but rejects such classification.


Hizb ut-Tahrir is an influential international Islamist movement, founded in 1953 by an Islamic Qadi "(judge)" Taqiuddin al-Nabhani. HT is unique from most other Islamist movements in that the party focuses not on implementation of Sharia on local level or on providing social services, but on unifying the Muslim world under its vision of a new Islamic caliphate spanning from North Africa and the Middle East to much of central and South Asia.

To this end it has drawn up and published a 186-article constitution for its proposed caliphate-state specifying specific policies such as sharia law, a "unitary ruling system" headed by a caliph elected by Muslims, an economy based on the gold standard, public ownership of utilities, public transport, and energy resources, death for apostates and Arabic as the "sole language of the State."

In its focus on the Caliphate, the party takes a different view of Muslim history than some other Islamists such as Muhammad Qutb. HT sees Islam's pivotal turning point as occurring not with the death of Ali, or one of the other four rightly guided Caliphs in the 7th century, but with the abolition of the Ottoman Caliphate in 1924. This is believed to have ended the true Islamic system, something for which it blames "the disbelieving (Kafir) colonial powers" working through Turkish modernist Mustafa Kemal Atatürk.

HT does not engage in armed jihad or work for a democratic system, but works to take power through "ideological struggle" to change Muslim public opinion, and in particular through elites who will "facilitate" a "change of the government," i.e., launch a "bloodless" coup. It allegedly attempted and failed such coups in 1968 and 1969 in Jordan, and in 1974 in Egypt, and is now banned in both countries.

The party is sometimes described as "Leninist" and "rigidly controlled by its central leadership," with its estimated one million members required to spend "at least two years studying party literature under the guidance of mentors "(Murshid)"" before taking "the party oath." HT is particularly active in the ex-soviet republics of Central Asia and in Europe.

In the UK its rallies have drawn thousands of Muslims, and the party has been described by two observers (Robert S. Leiken and Steven Brooke) to have outpaced the Muslim Brotherhood in both membership and radicalism.

One observer (Quinn Mecham) notes four trends in Islamism rising from the Arab Spring of 2010-11:
Another observer (Tarek Osman) notes with concern that

"The Islamic State", formerly known as the "Islamic State of Iraq and the Levant" and before that as the "Islamic State of Iraq", (also called by the Arabic acronym "Daesh"), is a Wahhabi/Salafi jihadist extremist militant group which is led by and mainly composed of Sunni Arabs from Syria and Iraq. In 2014, the group proclaimed itself a caliphate, with religious, political and military authority over all Muslims worldwide.
, it had control over territory occupied by ten million people in Syria and Iraq, and has nominal control over small areas of Libya, Nigeria, and Afghanistan. (While a self-described state, it lacks international recognition.) ISIL also operates or has affiliates in other parts of the world, including North Africa and South Asia

Originating as the "Jama'at al-Tawhid wal-Jihad" in 1999, ISIL pledged allegiance to al-Qaeda in 2004, participated in the Iraqi insurgency that followed the invasion of Iraq by Western coalition forces in 2003, joined the fight in the Syrian Civil War beginning in 2011, and was expelled from al-Qaeda in early 2014, (which complained of its failure to consult and "notorious intransigence"). ISIL gained prominence after it drove Iraqi government forces out of key cities in western Iraq in an offensive in June that same year. The group is adept at social media, posting Internet videos of beheadings of soldiers, civilians, journalists and aid workers, and is known for its destruction of cultural heritage sites.
The United Nations (UN) has held ISIL responsible for human rights abuses and war crimes, and Amnesty International has reported ethnic cleansing by the group on a "historic scale". The group has been designated a terrorist organisation by the UN, the European Union (EU) and member states, the United States, India, Indonesia, Turkey, Saudi Arabia, Syria and other countries.

Islamist movements such as the Muslim Brotherhood, "are well known for providing shelters, educational assistance, free or low cost medical clinics, housing assistance to students from out of town, student advisory groups, facilitation of inexpensive mass marriage ceremonies to avoid prohibitively costly dowry demands, legal assistance, sports facilities, and women's groups." All this compares very favourably against incompetent, inefficient, or neglectful governments whose commitment to social justice is limited to rhetoric.

The Arab world—the original heart of the Muslim world—has been afflicted with economic stagnation. For example, it has been estimated that in the mid 1990s the exports of Finland, a country of five million, exceeded those of the entire Arab world of 260 million, excluding oil revenue. This economic stagnation is argued to have commenced with the demise of the Ottoman Caliphate in 1924, with trade networks being disrupted and societies torn apart with the creation of new nation states; prior to this, the Middle East had a diverse and growing economy and more general prosperity.
Strong population growth combined with economic stagnation has created urban agglomerations in Cairo, Istanbul, Tehran, Karachi, Dhaka, and Jakarta each with well over 12 million citizens, millions of them young and unemployed or underemployed. Such a demographic, alienated from the westernized ways of the urban elite, but uprooted from the comforts and more passive traditions of the villages they came from, is understandably favourably disposed to an Islamic system promising a better world—an ideology providing an "emotionally familiar basis for group identity, solidarity, and exclusion; an acceptable basis for legitimacy and authority; an immediately intelligible formulation of principles for both a critique of the present and a program for the future."

Islamism can also be described as part of identity politics, specifically the religiously-oriented nationalism that emerged in the Third World in the 1970s: "resurgent Hinduism in India, Religious Zionism in Israel, militant Buddhism in Sri Lanka, resurgent Sikh nationalism in the Punjab, 'Liberation Theology' of Catholicism in Latin America, and Islamism in the Muslim world." These all challenged Westernized ruling elites on behalf of 'authenticity' and tradition.

The modern revival of Islamic devotion and the attraction to things Islamic can be traced to several events.

By the end of World War I, most Muslim states were seen to be dominated by the Christian-leaning Western states. It is argued that either the claims of Islam were false and the Christian or post-Christian West had finally come up with another system that was superior, or Islam had failed through not being true to itself. Thus, a redoubling of faith and devotion by Muslims was called for to reverse this tide.

The connection between the lack of an Islamic spirit and the lack of victory was underscored by the disastrous defeat of Arab nationalist-led armies fighting under the slogan "Land, Sea and Air" in the 1967 Six-Day War, compared to the (perceived) near-victory of the Yom Kippur War six years later. In that war the military's slogan was "God is Great".

Along with the Yom Kippur War came the Arab oil embargo where the (Muslim) Persian Gulf oil-producing states' dramatic decision to cut back on production and quadruple the price of oil, made the terms oil, Arabs and Islam synonymous—with power—in the world, and especially in the Muslim world's public imagination. Many Muslims believe as Saudi Prince Saud al Faisal did that the hundreds of billions of dollars in wealth obtained from the Persian Gulf's huge oil deposits were nothing less than a gift from God to the Islamic faithful.

As the Islamic revival gained momentum, governments such as Egypt's, which had previously repressed (and was still continuing to repress) Islamists, joined the bandwagon. They banned alcohol and flooded the airwaves with religious programming, giving the movement even more exposure.

Starting in the mid-1970s the Islamic resurgence was funded by an abundance of money from Saudi Arabian oil exports. The tens of billions of dollars in "petro-Islam" largesse obtained from the recently heightened price of oil funded an estimated "90% of the expenses of the entire faith."

Throughout the Muslim world, religious institutions for people both young and old, from children's maddrassas to high-level scholarships received Saudi funding,
"books, scholarships, fellowships, and mosques" (for example, "more than 1500 mosques were built and paid for with money obtained from public Saudi funds over the last 50 years"), along with training in the Kingdom for the preachers and teachers who went on to teach and work at these universities, schools, mosques, etc.

The funding was also used to reward journalists and academics who followed the Saudis' strict interpretation of Islam; and satellite campuses were built around Egypt for Al-Azhar University, the world's oldest and most influential Islamic university.

The interpretation of Islam promoted by this funding was the strict, conservative Saudi-based Wahhabism or Salafism. In its harshest form it preached that Muslims should not only "always oppose" infidels "in every way," but "hate them for their religion ... for Allah's sake," that democracy "is responsible for all the horrible wars of the 20th century," that Shia and other non-Wahhabi Muslims were infidels, etc. While this effort has by no means converted all, or even most Muslims to the Wahhabist interpretation of Islam, it has done much to overwhelm more moderate local interpretations, and has set the Saudi-interpretation of Islam as the "gold standard" of religion in minds of some or many Muslims.

Qatar stands out among state sponsors of Islamism as well. Over the past two decades, the country has exerted a semi-formal patronage for the international movement of the Muslim Brotherhood. Former Qatari Sheikh Hamad bin Khalifa al-Thani in particular has distinguished himself as one of the most dedicated supporter of the Muslim Brotherhood and of Islamist movements in general both in the Middle Eastern region and across the globe.

In 1999 the Muslim Brotherhood was disbanded in Qatar. The country's longstanding support for the group has been often explained as determined by a strategic calculus that limited the role played by religion in Qatar. As the director of the Center for International and Regional Studies at the Doha-based branch of Georgetown University, Mehran Kamrava, posited, Qatar presenting itself as the state patron of the Muslim Brotherhood has caused religion in Qatar to not "play any role in articulating or forming oppositional sentiments."

Qatar's patronage has been primarily expressed through the ruling family's endorsement of Muslim Brotherhood's most representative figures, especially Yusuf al-Qaradawi. Qaradawi is a prominent, yet controversial Sunni preacher and theologian who continues to serve as the spiritual leader of the Muslim Brotherhood. An Egyptian citizen, Qaradawi fled Egypt for Qatar in 1961 after being imprisoned under President Gamal Abdul Nasser. In 1962 he chaired the Qatari Secondary Institute of Religious Studies, and in 1977 he founded and directed the Shariah and Islamic Studies department at the University of Qatar. He left Qatar to return to Egypt shortly before the 2011 Egyptian Revolution.

For twenty years, Qaradawi has hosted a popular show titled Shariah and Life on the Qatari-based media channel Al-Jazeera, a government sponsored channel notoriously supportive of the Muslim Brotherhood and Islamism and often designated as a propaganda outlet for the Qatari government. From that platform, he has promoted his Islamist—and often radical views—on life, politics, and culture.

His positions, as well as his controversial ties to extremist and terrorist individuals and organizations, made him persona non grata to the U.S., UK and French governments respectively in 1999, 2008, and 2012.

Beyond the visibility and political protection granted to Yussuf al-Qaradawi, Qatar has historically hosted several Muslim Brothers especially after Egyptian President Mohammed Morsi, a Muslim Brotherhood representative, was overthrown in July 2013. Before 2013, however, Qatar had made a substantial investment on Morsi's leadership and had devolved about $10 million to Egypt since Morsi was elected, allegedly also to "buy political advantage" in the country.

Qatar's political and financial support for Islamist movements and factions was not limited to the Egyptian case. Qatar is known to have backed Islamist factions in Libya, Syria and Yemen.

In Libya in particular, Qatar has supported the Islamist government established in Tripoli. During the 2011 revolution that ousted President Muammar Gaddafi, Qatar provided "tens of millions of dollars in aid, military training and more than 20,000 tons of weapons" to anti-Gaddafi rebels and Islamist militias in particular. The flow of weapons was not suspended after Gaddafi's government was removed. Qatar maintained its influence through key facilitators on the field, including cleric Ali al-Sallabi, the leader of the Islamist militia "February 17 Katiba" Ismail al-Sallabi, and the Tripoli Military Council leader Abdel Hakim Belhaj.

Hamas, as well, has been among the primary beneficiaries of Qatar's financial support. Not only does the Gulf emirate host Hamas' politburo continuously since 2012; Hamas leader Khaled Meshaal has often met with international delegations on Qatari territory.

More recently, Qatar has channeled material support to Hamas' terrorist operations by exploiting its official commitment to finance Gaza reconstruction. Mostly through "truckloads of construction material being shipped into Gaza", Qatar has funneled dual-use substances that could be employed to produce explosives into Gaza.

In a 2003 interview with Al-Hayat Hamas politburo declared that most of Qatar's support was collected through charities and popular committees. Qatar's largest NGO, Qatar Charity, in particular has played a great role in Qatar's mission to support Islamist worldwide.

Officially through its "Ghaith" initiative but also through conspicuous donations that preceded the "Ghaith" program, Qatar Charity has financed the building or reconstruction of mosques and cultural institutes across the globe. Just like Saudi Arabia, Qatar has devolved considerable energies to spreading Salafism and to "win areas of influence" in the countries that beneficiated from its support. In France in particular Qatar has heavily invested in the Union des Organisations Islamiques des France (UOIF), an umbrella organization informally acting as the representative of the Muslim Brotherhood in the country through which Qatar Charity has channeled funds for the Assalam mosque in Nantes (€4.4 million) and the mosque in Mulhouse (€2 million).

During the 1970s and sometimes later, Western and pro-Western governments often supported sometimes fledgling Islamists and Islamist groups that later came to be seen as dangerous enemies. Islamists were considered by Western governments bulwarks against—what were thought to be at the time—more dangerous leftist/communist/nationalist insurgents/opposition, which Islamists were correctly seen as opposing. The US spent billions of dollars to aid the mujahideen Muslim Afghanistan enemies of the Soviet Union, and non-Afghan veterans of the war returned home with their prestige, "experience, ideology, and weapons", and had considerable impact.

Although it is a strong opponent of Israel's existence, Hamas, officially created in 1987, traces back its origins to institutions and clerics supported by Israel in the 1970s and 1980s. Israel tolerated and supported Islamist movements in Gaza, with figures like Ahmed Yassin, as Israel perceived them preferable to the secular and then more powerful al-Fatah with the PLO.

Egyptian President Anwar Sadatwhose policies included opening Egypt to Western investment ("infitah"); transferring Egypt's allegiance from the Soviet Union to the United States; and making peace with Israel—released Islamists from prison and welcomed home exiles in tacit exchange for political support in his struggle against leftists. His "encouraging of the emergence of the Islamist movement" was said to have been "imitated by many other Muslim leaders in the years that followed." This "gentlemen's agreement" between Sadat and Islamists broke down in 1975 but not before Islamists came to completely dominate university student unions. Sadat was later assassinated and a formidable insurgency was formed in Egypt in the 1990s. The French government has also been reported to have promoted Islamist preachers "in the hope of channeling Muslim energies into zones of piety and charity."

Muslim alienation from Western ways, including its political ways.


Islamism, or elements of Islamism, have been criticized for: repression of free expression and individual rights, rigidity, hypocrisy, lack of true understanding of Islam, misinterpreting the Quran and Sunnah, antisemitism, and for innovations to Islam (bid'ah), notwithstanding proclaimed opposition to any such innovation by Islamists.

The U.S. government has engaged in efforts to counter militant Islamism (Jihadism), since 2001. These efforts were centred in the U.S. around public diplomacy programmes conducted by the State Department. There have been calls to create an independent agency in the U.S. with a specific mission of undermining Jihadism. Christian Whiton, an official in the George W. Bush administration, called for a new agency focused on the nonviolent practice of "political warfare" aimed at undermining the ideology. U.S. Defense Secretary Robert Gates called for establishing something similar to the defunct U.S. Information Agency, which was charged with undermining the communist ideology during the Cold War.





</doc>
<doc id="15014" url="https://en.wikipedia.org/wiki?curid=15014" title="Instructional theory">
Instructional theory

An instructional theory is "a theory that offers explicit guidance on how to better help people learn and develop." It provides insights about what is likely to happen and why with respect to different kinds of teaching and learning activities while helping indicate approaches for their evaluation. Instructional designers focus on how to best structure material and instructional behavior to facilitate learning. 

Originating in the United States in the late 1970s, "instructional theory" is influenced by three basic theories in educational thought: behaviorism, the theory that helps us understand how people conform to predetermined standards; cognitivism, the theory that learning occurs through mental associations; and constructivism, the theory explores the value of human activity as a critical function of gaining knowledge. Instructional theory is heavily influenced by the 1956 work of Benjamin Bloom, a University of Chicago professor, and the results of his Taxonomy of Education Objectives—one of the first modern codifications of the learning process. One of the first instructional theorists was Robert M. Gagne, who in 1965 published "Conditions of Learning" for the Florida State University's Department of Educational Research.

Instructional theory is different than learning theory. A learning theory "describes" how learning takes place, and an instructional theory "prescribes" how to better help people learn. Learning theories often inform instructional theory, and three general theoretical stances take part in this influence: behaviorism (learning as response acquisition), cognitivism (learning as knowledge acquisition), and constructivism (learning as knowledge construction). Instructional theory helps us create conditions that increases the probability of learning. Its goal is understanding the instructional system and to improve the process of instruction.

Instructional theories identify what instruction or teaching should be like. It outlines strategies that an educator may adopt to achieve the learning objectives. Instructional theories are adapted based on the educational content and more importantly the learning style of the students. They are used as teaching guidelines/tools by teachers/trainers to facilitate learning. Instructional theories encompass different instructional methods, models and strategies.

David Merrill's First Principles of Instruction discusses universal methods of instruction, situational methods and core ideas of the post-industrial paradigm of instruction.

Universal Methods of Instruction:

Situational Methods:

based on different approaches to instruction
based on different learning outcomes:
Core ideas for the Post-industrial Paradigm of Instruction:
Four tasks of Instructional theory:

Paulo Freire's work appears to critique instructional approaches that adhere to the knowledge acquisition stance, and his work "Pedagogy of the Oppressed" has had a broad influence over a generation of American educators with his critique of various "banking" models of education and analysis of the teacher-student relationship.

Freire explains, "Narration (with the teacher as narrator) leads the students to memorize mechanically the narrated content. Worse yet, it turns them into "containers", into "receptacles" to be "filled" by the teacher. The more completely she fills the receptacles, the better a teacher she is. The more meekly the receptacles permit themselves to be filled, the better students they are." In this way he explains educator creates an act of depositing knowledge in a student. The student thus becomes a repository of knowledge. Freire explains that this system that diminishes creativity and knowledge suffers. Knowledge, according to Freire, comes about only through the learner by inquiry and pursuing the subjects in the world and through interpersonal interaction.

Freire further states, "In the banking concept of education, knowledge is a gift bestowed by those who consider themselves knowledgeable upon those whom they consider to know nothing. Projecting an absolute ignorance onto others, a characteristic of the ideology of oppression, negates education and knowledge as processes of inquiry. The teacher presents himself to his students as their necessary opposite; by considering their ignorance absolute, he justifies his own existence. The students, alienated like the slave in the Hegelian dialectic, accept their ignorance as justifying the teacher's existence—but, unlike the slave, they never discover that they educate the teacher."
Freire then offered an alternative stance and wrote, "The raison d'etre of libertarian education, on the other hand, lies in its drive towards reconciliation. Education must begin with the solution of the teacher-student contradiction, by reconciling the poles of the contradiction so that both are simultaneously teachers and students."

In the article, "A process for the critical analysis of instructional theory", the authors use an ontology-building process to review and analyze concepts across different instructional theories. Here are their findings:


Linking Premise to Practice: An Instructional Theory-Strategy Model Approach By: Bowden, Randall. Journal of College Teaching & Learning, v5 n3 p69-76 Mar 2008


</doc>
<doc id="15018" url="https://en.wikipedia.org/wiki?curid=15018" title="Infusoria">
Infusoria

Infusoria is a collective term for minute aquatic creatures such as ciliates, euglenoids, protozoa, unicellular algae and small invertebrates that exist in freshwater ponds. Some authors (e.g., Bütschli) used the term as a synonym for Ciliophora. In modern formal classifications, the term is considered obsolete; the microorganisms previously included in the Infusoria are mostly assigned to the kingdom Protista. 

Infusoria are used by owners of aquariums to feed fish fry; newly hatched fry of many common aquarium species can be successfully raised on this food during early development due to its size and nutritional content. Many home aquaria are unable to naturally supply sufficient infusoria for fish-rearing, so hobbyists may create and maintain their own supply cultures or use one of the many commercial cultures available. Infusoria can be cultured by soaking any decomposing matter of organic or vegetative origin, such as papaya skin, in a jar of aged water. The culture starts to proliferate in two to three days, depending on temperature and light received. The water first turns cloudy, but clears up once the infusoria eat the bacteria that caused the cloudiness. At this point, the infusoria are ready, and usually are visible to the naked eye as small, white specks swimming in the container.





</doc>
<doc id="15019" url="https://en.wikipedia.org/wiki?curid=15019" title="ISO/IEC 8859-1">
ISO/IEC 8859-1

ISO/IEC 8859-1:1998, "Information technology — 8-bit single-byte coded graphic character sets — Part 1: Latin alphabet No. 1", is part of the ISO/IEC 8859 series of ASCII-based standard character encodings, first edition published in 1987. ISO 8859-1 encodes what it refers to as "Latin alphabet no. 1", consisting of 191 characters from the Latin script. This character-encoding scheme is used throughout the Americas, Western Europe, Oceania, and much of Africa. It is also commonly used in most standard romanizations of East-Asian languages. It is the basis for most popular 8-bit character sets and the first block of characters in Unicode.

ISO-8859-1 was (according to the standards at least) the default encoding of documents delivered via HTTP with a MIME type beginning with "text/" (HTML5 changed this to Windows-1252). , 1.9% of all (while only 0.7% of the top-1000) Web sites claim to use . However, this includes an unknown number of pages actually using Windows-1252 and/or UTF-8, both of which are commonly recognized by browsers, despite the character set tag.

It is the default encoding of the values of certain descriptive HTTP headers, and defines the repertoire of characters allowed in HTML 3.2 documents (HTML 4.0 uses Unicode, "i.e.", UTF-8), and is specified by many other standards. This and similar sets are often assumed to be the encoding of 8-bit text on Unix and Microsoft Windows if there is no byte order mark (BOM); this is only gradually being changed to UTF-8.

ISO-8859-1 is the IANA preferred name for this standard when supplemented with the C0 and C1 control codes from ISO/IEC 6429. The following other aliases are registered: iso-ir-100, csISOLatin1, latin1, l1, IBM819. Code page 28591 a.k.a. Windows-28591 is used for it in Windows. IBM calls it code page 819 or CP819 (CCSID 819). Oracle calls it WE8ISO8859P1.

Each character is encoded as a single eight-bit code value. These code values can be used in almost any data interchange system to communicate in the following languages:

ISO-8859-1 was commonly used for certain languages, even though it lacks characters used by these languages. In most cases, only a few letters are missing or they are rarely used, and they can be replaced with characters that are in ISO-8859-1 using some form of typographic approximation. The following table lists such languages.

The letter "ÿ", which appears in French only very rarely, mainly in city names such as L'Haÿ-les-Roses and never at the beginning of words, is included only in lowercase form. The slot corresponding to its uppercase form is occupied by the lowercase letter "ß" from the German language, which did not have an uppercase form at the time when the standard was created.

For some languages listed above, the correct typographical quotation marks are missing, as only , , and are included. Also, this scheme does not provide for oriented (6- or 9-shaped) single or double quotation marks. Some fonts will display the spacing grave accent (0x60) and the apostrophe (0x27) as a matching pair of oriented single quotation marks, but this is not considered part of the modern standard.

ISO 8859-1 was based on the Multinational Character Set used by Digital Equipment Corporation (DEC) in the popular VT220 terminal in 1983. It was developed within the European Computer Manufacturers Association (ECMA), and published in March 1985 as ECMA-94, by which name it is still sometimes known. The second edition of ECMA-94 (June 1986) also included ISO 8859-2, ISO 8859-3, and ISO 8859-4 as part of the specification.

The original draft of ISO 8859-1 placed French "Œ" and "œ" at code points 215 (0xD7) and 247 (0xF7), as in the ECMA-94. However, the delegate from France, being neither a linguist nor a typographer, falsely stated that these are not independent French letters on their own, but mere ligatures (like "ﬁ" or "ﬂ"), supported by the delegate team from Bull Publishing Company, who regularly did not print French with "Œ/œ" in their house style at the time. An anglophone delegate from Canada insisted in retaining "Œ/œ" but was rebuffed by the French delegate and the team from Bull. These code points were soon filled with × and ÷ under the suggestion of the German delegation. Then things went even worse for the French language, when it was again falsely stated that the letter "ÿ" is "not French", resulting in the absence of the capital "Ÿ". In fact, the letter "ÿ" is found in a number of French proper names, and the capital letter has been used in dictionaries and encyclopedias. These characters were added to ISO/IEC 8859-15:1999. BraSCII matches the original draft.

In 1985, Commodore adopted ECMA-94 for its new AmigaOS operating system. The Seikosha MP-1300AI impact dot-matrix printer, used with the Amiga 1000, included this encoding.

In 1990, the very first version of Unicode used the code points of ISO-8859-1 as the first 256 Unicode code points.

In 1992, the IANA registered the character map ISO_8859-1:1987, more commonly known by its preferred MIME name of ISO-8859-1 (note the extra hyphen over ISO 8859-1), a superset of ISO 8859-1, for use on the Internet. This map assigns the C0 and C1 control codes to the unassigned code values thus provides for 256 characters via every possible 8-bit value.

 In the original draft, however, Œ was at codepoint 215 (0xD7) and œ was at codepoint 247 (0xF7).

ISO/IEC 8859-15 was developed in 1999, as an update of ISO/IEC 8859-1. It provides some characters for French and Finnish text and the euro sign, which are missing from ISO/IEC 8859-1. This required the removal of some infrequently used characters from ISO/IEC 8859-1, including fraction symbols and letter-free diacritics: , , , , , , , and . Ironically, three of the newly added characters (, , and ) had already been present in DEC's 1983 Multinational Character Set (MCS), the predecessor to ISO/IEC 8859-1 (1987). Since their original code points were now reused for other purposes, the characters had to be reintroduced under different, less logical code points.

ISO-IR-204, a more minor modification, had been registered in 1998, altering ISO-8859-1 by replacing the universal currency sign (¤) with the euro sign (the same substitution made by ISO-8859-15).

The popular Windows-1252 character set adds all the missing characters provided by ISO/IEC 8859-15, plus a number of typographic symbols, by replacing the rarely used C1 controls in the range 128 to 159 (hex 80 to 9F). It is very common to mislabel Windows-1252 text as being in ISO-8859-1. A common result was that all the quotes and apostrophes (produced by "smart quotes" in word-processing software) were replaced with question marks or boxes on non-Windows operating systems, making text difficult to read. Many web browsers and e-mail clients will interpret ISO-8859-1 control codes as Windows-1252 characters, and that behavior was later standardized in HTML5.

The Apple Macintosh computer introduced a character encoding called Mac Roman in 1984. It was meant to be suitable for Western European desktop publishing. It is a superset of ASCII, and has most of the characters that are in ISO-8859-1 and all the extra characters from Windows-1252 but in a totally different arrangement. The few printable characters that are in ISO 8859-1, but not in this set, are often a source of trouble when editing text on Web sites using older Macintosh browsers, including the last version of Internet Explorer for Mac.

DOS had code page 850, which had all printable characters that ISO-8859-1 had (albeit in a totally different arrangement) plus the most widely used graphic characters from code page 437.

Between 1989 and 2015, Hewlett-Packard used another superset of ISO-8859-1 on many of their calculators. This proprietary character set was sometimes referred to simply as "ECMA-94" as well.




</doc>
<doc id="15020" url="https://en.wikipedia.org/wiki?curid=15020" title="ISO/IEC 8859">
ISO/IEC 8859

ISO/IEC 8859 is a joint ISO and IEC series of standards for 8-bit character encodings. The series of standards consists of numbered parts, such as ISO/IEC 8859-1, ISO/IEC 8859-2, etc. There are 15 parts, excluding the abandoned ISO/IEC 8859-12. The ISO working group maintaining this series of standards has been disbanded.

ISO/IEC 8859 parts 1, 2, 3, and 4 were originally Ecma International standard ECMA-94.

While the bit patterns of the 95 printable ASCII characters are sufficient to exchange information in modern English, most other languages that use Latin alphabets need additional symbols not covered by ASCII. ISO/IEC 8859 sought to remedy this problem by utilizing the eighth bit in an 8-bit byte to allow positions for another 96 printable characters. Early encodings were limited to 7 bits because of restrictions of some data transmission protocols, and partially for historical reasons. However, more characters were needed than could fit in a single 8-bit character encoding, so several mappings were developed, including at least ten suitable for various Latin alphabets.

The ISO/IEC 8859-"n" encodings only contain printable characters, and were designed to be used in conjunction with control characters mapped to the unassigned bytes. To this end a series of encodings registered with the IANA add the C0 control set (control characters mapped to bytes 0 to 31) from ISO 646 and the C1 control set (control characters mapped to bytes 128 to 159) from ISO 6429, resulting in full 8-bit character maps with most, if not all, bytes assigned. These sets have ISO-8859-"n" as their preferred MIME name or, in cases where a preferred MIME name is not specified, their canonical name. Many people use the terms ISO/IEC 8859-"n" and ISO-8859-"n" interchangeably. ISO/IEC 8859-11 did not get such a charset assigned, presumably because it was almost identical to TIS 620.

The ISO/IEC 8859 standard is designed for reliable information exchange, not typography; the standard omits symbols needed for high-quality typography, such as optional ligatures, curly quotation marks, dashes, etc. As a result, high-quality typesetting systems often use proprietary or idiosyncratic extensions on top of the ASCII and ISO/IEC 8859 standards, or use Unicode instead.

As a rule of thumb, if a character or symbol was not already part of a widely used data-processing character set and was also not usually provided on typewriter keyboards for a national language, it did not get in. Hence the directional double quotation marks "«" and "»" used for some European languages were included, but not the directional double quotation marks "“" and "”" used for English and some other languages.

French did not get its "œ" and "Œ" ligatures because they could be typed as 'oe'. Likewise, "Ÿ", needed for all-caps text, was dropped as well. Albeit under different codepoints, these three characters were later reintroduced with ISO/IEC 8859-15 in 1999, which also introduced the new euro sign character €. Likewise Dutch did not get the "ĳ" and "Ĳ" letters, because Dutch speakers had become used to typing these as two letters instead.

Romanian did not initially get its "Ș"/"ș" and "Ț"/"ț" (with comma) letters, because these letters were initially unified with "Ş"/"ş" and "Ţ"/"ţ" (with cedilla) by the Unicode Consortium, considering the shapes with comma beneath to be glyph variants of the shapes with cedilla. However, the letters with explicit comma below were later added to the Unicode standard and are also in ISO/IEC 8859-16.

Most of the ISO/IEC 8859 encodings provide diacritic marks required for various European languages using the Latin script. Others provide non-Latin alphabets: Greek, Cyrillic, Hebrew, Arabic and Thai. Most of the encodings contain only spacing characters, although the Thai, Hebrew, and Arabic ones do also contain combining characters.

The standard makes no provision for the scripts of East Asian languages ("CJK"), as their ideographic writing systems require many thousands of code points. Although it uses Latin based characters, Vietnamese does not fit into 96 positions (without using combining diacritics such as in Windows-1258) either. Each Japanese syllabic alphabet (hiragana or katakana, see Kana) would fit, as in JIS X 0201, but like several other alphabets of the world they are not encoded in the ISO/IEC 8859 system.

ISO/IEC 8859 is divided into the following parts:

Each part of ISO/IEC 8859 is designed to support languages that often borrow from each other, so the characters needed by each language are usually accommodated by a single part. However, there are some characters and language combinations that are not accommodated without transcriptions. Efforts were made to make conversions as smooth as possible. For example, German has all of its seven special characters at the same positions in all Latin variants (1–4, 9, 10, 13–16), and in many positions the characters only differ in the diacritics between the sets. In particular, variants 1–4 were designed jointly, and have the property that every encoded character appears either at a given position or not at all.

At position 0xA0 there's always the non breaking space and 0xAD is mostly the soft hyphen, which only shows at line breaks. Other empty fields are either unassigned or the system used is not able to display them.

There are new additions as and versions. LRM stands for left-to-right mark (U+200E) and RLM stands for right-to-left mark (U+200F).

Since 1991, the Unicode Consortium has been working with ISO and IEC to develop the Unicode Standard and ISO/IEC 10646: the Universal Character Set (UCS) in tandem. Newer editions of ISO/IEC 8859 express characters in terms of their Unicode/UCS names and the "U+nnnn" notation, effectively causing each part of ISO/IEC 8859 to be a Unicode/UCS character encoding scheme that maps a very small subset of the UCS to single 8-bit bytes. The first 256 characters in Unicode and the UCS are identical to those in ISO/IEC-8859-1 (Latin-1).

Single-byte character sets including the parts of ISO/IEC 8859 and derivatives of them were favoured throughout the 1990s, having the advantages of being well-established and more easily implemented in software: the equation of one byte to one character is simple and adequate for most single-language applications, and there are no combining characters or variant forms. As Unicode-enabled operating systems became more widespread, ISO/IEC 8859 and other legacy encodings became less popular. While remnants of ISO 8859 and single-byte character models remain entrenched in many operating systems, programming languages, data storage systems, networking applications, display hardware, and end-user application software, most modern computing applications use Unicode internally, and rely on conversion tables to map to and from other encodings, when necessary.

The ISO/IEC 8859 standard was maintained by ISO/IEC Joint Technical Committee 1, Subcommittee 2, Working Group 3 (ISO/IEC JTC 1/SC 2/WG 3). In June 2004, WG 3 disbanded, and maintenance duties were transferred to SC 2. The standard is not currently being updated, as the Subcommittee's only remaining working group, WG 2, is concentrating on development of Unicode's Universal Coded Character Set.

The WHATWG Encoding Standard, which specifies the character encodings permitted in HTML5 which compliant browsers must support, includes most parts of ISO/IEC 8859, except for parts 1, 9 and 11, which are instead interpreted as Windows-1252, Windows-1254 and Windows-874 respectively. Authors of new pages and the designers of new protocols are instructed to use UTF-8 instead.




</doc>
<doc id="15022" url="https://en.wikipedia.org/wiki?curid=15022" title="Infrared">
Infrared

Infrared (IR), sometimes called infrared light, is electromagnetic radiation (EMR) with wavelengths longer than those of visible light. It is therefore generally invisible to the human eye, although IR at wavelengths up to 1050 nanometers (nm)s from specially pulsed lasers can be seen by humans under certain conditions. IR wavelengths extend from the nominal red edge of the visible spectrum at 700 nanometers (frequency 430 THz), to 1 millimeter (300 GHz). Most of the thermal radiation emitted by objects near room temperature is infrared. As with all EMR, IR carries radiant energy and behaves both like a wave and like its quantum particle, the photon.

Infrared radiation was discovered in 1800 by astronomer Sir William Herschel, who discovered a type of invisible radiation in the spectrum lower in energy than red light, by means of its effect on a thermometer. Slightly more than half of the total energy from the Sun was eventually found to arrive on Earth in the form of infrared. The balance between absorbed and emitted infrared radiation has a critical effect on Earth's climate.

Infrared radiation is emitted or absorbed by molecules when they change their rotational-vibrational movements. It excites vibrational modes in a molecule through a change in the dipole moment, making it a useful frequency range for study of these energy states for molecules of the proper symmetry. Infrared spectroscopy examines absorption and transmission of photons in the infrared range.

Infrared radiation is used in industrial, scientific, military, 
law enforcement, and medical applications. Night-vision devices using active near-infrared illumination allow people or animals to be observed without the observer being detected. Infrared astronomy uses sensor-equipped telescopes to penetrate dusty regions of space such as molecular clouds, detect objects such as planets, and to view highly red-shifted objects from the early days of the universe. Infrared thermal-imaging cameras are used to detect heat loss in insulated systems, to observe changing blood flow in the skin, and to detect overheating of electrical apparatus.

Extensive uses for military and civilian applications include target acquisition, surveillance, night vision, homing, and tracking. Humans at normal body temperature radiate chiefly at wavelengths around 10 μm (micrometers). Non-military uses include thermal efficiency analysis, environmental monitoring, industrial facility inspections, detection of grow-ops, remote temperature sensing, short-range wireless communication, spectroscopy, and weather forecasting.

Infrared radiation extends from the nominal red edge of the visible spectrum at 700 nanometers (nm) to 1 millimeter (mm). This range of wavelengths corresponds to a frequency range of approximately 430 THz down to 300 GHz. Below infrared is the microwave portion of the electromagnetic spectrum.

Sunlight, at an effective temperature of 5780 kelvins (5510 °C, 9940 °F), is composed of near-thermal-spectrum radiation that is slightly more than half infrared. At zenith, sunlight provides an irradiance of just over 1 kilowatt per square meter at sea level. Of this energy, 527 watts is infrared radiation, 445 watts is visible light, and 32 watts is ultraviolet radiation. Nearly all the infrared radiation in sunlight is near infrared, shorter than 4 micrometers.

On the surface of Earth, at far lower temperatures than the surface of the Sun, some thermal radiation consists of infrared in the mid-infrared region, much longer than in sunlight. However, black-body, or thermal, radiation is continuous: it gives off radiation at all wavelengths. Of these natural thermal radiation processes, only lightning and natural fires are hot enough to produce much visible energy, and fires produce far more infrared than visible-light energy.

In general, objects emit infrared radiation across a spectrum of wavelengths, but sometimes only a limited region of the spectrum is of interest because sensors usually collect radiation only within a specific bandwidth. Thermal infrared radiation also has a maximum emission wavelength, which is inversely proportional to the absolute temperature of object, in accordance with Wien's displacement law.

Therefore, the infrared band is often subdivided into smaller sections.

A commonly used sub-division scheme is:

NIR and SWIR is sometimes called "reflected infrared", whereas MWIR and LWIR is sometimes referred to as "thermal infrared". Due to the nature of the blackbody radiation curves, typical "hot" objects, such as exhaust pipes, often appear brighter in the MW compared to the same object viewed in the LW.

The International Commission on Illumination (CIE) recommended the division of infrared radiation into the following three bands:

ISO 20473 specifies the following scheme:

Astronomers typically divide the infrared spectrum as follows:

These divisions are not precise and can vary depending on the publication. The three regions are used for observation of different temperature ranges, and hence different environments in space.

The most common photometric system used in astronomy allocates capital letters to different spectral regions according to filters used; I, J, H, and K cover the near-infrared wavelengths; L, M, N, and Q refer to the mid-infrared region. These letters are commonly understood in reference to atmospheric windows and appear, for instance, in the titles of many papers.

A third scheme divides up the band based on the response of various detectors:


Near-infrared is the region closest in wavelength to the radiation detectable by the human eye. mid- and far-infrared are progressively further from the visible spectrum. Other definitions follow different physical mechanisms (emission peaks, vs. bands, water absorption) and the newest follow technical reasons (the common silicon detectors are sensitive to about 1,050 nm, while InGaAs's sensitivity starts around 950 nm and ends between 1,700 and 2,600 nm, depending on the specific configuration). No international standards for these specifications are currently available.

The onset of infrared is defined (according to different standards) at various values typically between 700 nm and 800 nm, but the boundary between visible and infrared light is not precisely defined. The human eye is markedly less sensitive to light above 700 nm wavelength, so longer wavelengths make insignificant contributions to scenes illuminated by common light sources. However, particularly intense near-IR light (e.g., from IR lasers, IR LED sources, or from bright daylight with the visible light removed by colored gels) can be detected up to approximately 780 nm, and will be perceived as red light. Intense light sources providing wavelengths as long as 1050 nm can be seen as a dull red glow, causing some difficulty in near-IR illumination of scenes in the dark (usually this practical problem is solved by indirect illumination). Leaves are particularly bright in the near IR, and if all visible light leaks from around an IR-filter are blocked, and the eye is given a moment to adjust to the extremely dim image coming through a visually opaque IR-passing photographic filter, it is possible to see the Wood effect that consists of IR-glowing foliage.

In optical communications, the part of the infrared spectrum that is used is divided into seven bands based on availability of light sources transmitting/absorbing materials (fibers) and detectors:

The C-band is the dominant band for long-distance telecommunication networks. The S and L bands are based on less well established technology, and are not as widely deployed.

Infrared radiation is popularly known as "heat radiation", but light and electromagnetic waves of any frequency will heat surfaces that absorb them. Infrared light from the Sun accounts for 49% of the heating of Earth, with the rest being caused by visible light that is absorbed then re-radiated at longer wavelengths. Visible light or ultraviolet-emitting lasers can char paper and incandescently hot objects emit visible radiation. Objects at room temperature will emit radiation concentrated mostly in the 8 to 25 μm band, but this is not distinct from the emission of visible light by incandescent objects and ultraviolet by even hotter objects (see black body and Wien's displacement law).

Heat is energy in transit that flows due to a temperature difference. Unlike heat transmitted by thermal conduction or thermal convection, thermal radiation can propagate through a vacuum. Thermal radiation is characterized by a particular spectrum of many wavelengths that are associated with emission from an object, due to the vibration of its molecules at a given temperature. Thermal radiation can be emitted from objects at any wavelength, and at very high temperatures such radiation is associated with spectra far above the infrared, extending into visible, ultraviolet, and even X-ray regions (e.g. the solar corona). Thus, the popular association of infrared radiation with thermal radiation is only a coincidence based on typical (comparatively low) temperatures often found near the surface of planet Earth.

The concept of emissivity is important in understanding the infrared emissions of objects. This is a property of a surface that describes how its thermal emissions deviate from the idea of a black body. To further explain, two objects at the same physical temperature may not show the same infrared image if they have differing emissivity. For example, for any pre-set emissivity value, objects with higher emissivity will appear hotter, and those with a lower emissivity will appear cooler (assuming, as is often the case, that the surrounding environment is cooler than the objects being viewed). When an object has less than perfect emissivity, it obtains properties of reflectivity and/or transparency, and so the temperature of the surrounding environment is partially reflected by and/or transmitted through the object. If the object were in a hotter environment, then a lower emissivity object at the same temperature would likely appear to be hotter than a more emissive one. For that reason, incorrect selection of emissivity and not accounting for environmental temperatures will give inaccurate results when using infrared cameras and pyrometers.

 Infrared is used in night vision equipment when there is insufficient visible light to see. Night vision devices operate through a process involving the conversion of ambient light photons into electrons that are then amplified by a chemical and electrical process and then converted back into visible light. Infrared light sources can be used to augment the available ambient light for conversion by night vision devices, increasing in-the-dark visibility without actually using a visible light source.

The use of infrared light and night vision devices should not be confused with thermal imaging, which creates images based on differences in surface temperature by detecting infrared radiation (heat) that emanates from objects and their surrounding environment.

Infrared radiation can be used to remotely determine the temperature of objects (if the emissivity is known). This is termed thermography, or in the case of very hot objects in the NIR or visible it is termed pyrometry. Thermography (thermal imaging) is mainly used in military and industrial applications but the technology is reaching the public market in the form of infrared cameras on cars due to greatly reduced production costs.

Thermographic cameras detect radiation in the infrared range of the electromagnetic spectrum (roughly 900–14,000 nanometers or 0.9–14 μm) and produce images of that radiation. Since infrared radiation is emitted by all objects based on their temperatures, according to the black-body radiation law, thermography makes it possible to "see" one's environment with or without visible illumination. The amount of radiation emitted by an object increases with temperature, therefore thermography allows one to see variations in temperature (hence the name).

A hyperspectral image is a "picture" containing continuous spectrum through a wide spectral range at each pixel. Hyperspectral imaging is gaining importance in the field of applied spectroscopy particularly with NIR, SWIR, MWIR, and LWIR spectral regions. Typical applications include biological, mineralogical, defence, and industrial measurements.

Thermal infrared hyperspectral imaging can be similarly performed using a thermographic camera, with the fundamental difference that each pixel contains a full LWIR spectrum. Consequently, chemical identification of the object can be performed without a need for an external light source such as the Sun or the Moon. Such cameras are typically applied for geological measurements, outdoor surveillance and UAV applications.

In infrared photography, infrared filters are used to capture the near-infrared spectrum. Digital cameras often use infrared blockers. Cheaper digital cameras and camera phones have less effective filters and can "see" intense near-infrared, appearing as a bright purple-white color. This is especially pronounced when taking pictures of subjects near IR-bright areas (such as near a lamp), where the resulting infrared interference can wash out the image. There is also a technique called 'T-ray' imaging, which is imaging using far-infrared or terahertz radiation. Lack of bright sources can make terahertz photography more challenging than most other infrared imaging techniques. Recently T-ray imaging has been of considerable interest due to a number of new developments such as terahertz time-domain spectroscopy.

Infrared tracking, also known as infrared homing, refers to a passive missile guidance system, which uses the emission from a target of electromagnetic radiation in the infrared part of the spectrum to track it. Missiles that use infrared seeking are often referred to as "heat-seekers" since infrared (IR) is just below the visible spectrum of light in frequency and is radiated strongly by hot bodies. Many objects such as people, vehicle engines, and aircraft generate and retain heat, and as such, are especially visible in the infrared wavelengths of light compared to objects in the background.

Infrared radiation can be used as a deliberate heating source. For example, it is used in infrared saunas to heat the occupants. It may also be used in other heating applications, such as to remove ice from the wings of aircraft (de-icing). Infrared can be used in cooking and heating food as it predominantly heats the opaque, absorbent objects, rather than the air around them.

Infrared heating is also becoming more popular in industrial manufacturing processes, e.g. curing of coatings, forming of plastics, annealing, plastic welding, and print drying. In these applications, infrared heaters replace convection ovens and contact heating.

Efficiency is achieved by matching the wavelength of the infrared heater to the absorption characteristics of the material.

A variety of technologies or proposed technologies take advantage of infrared emissions to cool buildings or other systems. The LWIR (8–15 μm) region is especially useful since some radiation at these wavelengths can escape into space through the atmosphere.

IR data transmission is also employed in short-range communication among computer peripherals and personal digital assistants. These devices usually conform to standards published by IrDA, the Infrared Data Association. Remote controls and IrDA devices use infrared light-emitting diodes (LEDs) to emit infrared radiation that is focused by a plastic lens into a narrow beam. The beam is modulated, i.e. switched on and off, to prevent interference from other sources of infrared (like sunlight or artificial lighting). The receiver uses a silicon photodiode to convert the infrared radiation to an electric current. It responds only to the rapidly pulsing signal created by the transmitter, and filters out slowly changing infrared radiation from ambient light. Infrared communications are useful for indoor use in areas of high population density. IR does not penetrate walls and so does not interfere with other devices in adjoining rooms. Infrared is the most common way for remote controls to command appliances.
Infrared remote control protocols like RC-5, SIRC, are used to communicate with infrared.

Free space optical communication using infrared lasers can be a relatively inexpensive way to install a communications link in an urban area operating at up to 4 gigabit/s, compared to the cost of burying fiber optic cable, except for the radiation damage. "Since the eye cannot detect IR, blinking or closing the eyes to help prevent or reduce damage may not happen."

Infrared lasers are used to provide the light for optical fiber communications systems. Infrared light with a wavelength around 1,330 nm (least dispersion) or 1,550 nm (best transmission) are the best choices for standard silica fibers.

IR data transmission of encoded audio versions of printed signs is being researched as an aid for visually impaired people through the RIAS (Remote Infrared Audible Signage) project.
Transmitting IR data from one device to another is sometimes referred to as beaming.

Infrared vibrational spectroscopy (see also near-infrared spectroscopy) is a technique that can be used to identify molecules by analysis of their constituent bonds. Each chemical bond in a molecule vibrates at a frequency characteristic of that bond. A group of atoms in a molecule (e.g., CH) may have multiple modes of oscillation caused by the stretching and bending motions of the group as a whole. If an oscillation leads to a change in dipole in the molecule then it will absorb a photon that has the same frequency. The vibrational frequencies of most molecules correspond to the frequencies of infrared light. Typically, the technique is used to study organic compounds using light radiation from 4000–400 cm, the mid-infrared. A spectrum of all the frequencies of absorption in a sample is recorded. This can be used to gain information about the sample composition in terms of chemical groups present and also its purity (for example, a wet sample will show a broad O-H absorption around 3200 cm). The unit for expressing radiation in this application, cm, is the spectroscopic wavenumber. It is the frequency divided by the speed of light in vacuum.

In the semiconductor industry, infrared light can be used to characterize materials such as thin films and periodic trench structures. By measuring the reflectance of light from the surface of a semiconductor wafer, the index of refraction (n) and the extinction Coefficient (k) can be determined via the Forouhi-Bloomer dispersion equations. The reflectance from the infrared light can also be used to determine the critical dimension, depth, and sidewall angle of high aspect ratio trench structures.

Weather satellites equipped with scanning radiometers produce thermal or infrared images, which can then enable a trained analyst to determine cloud heights and types, to calculate land and surface water temperatures, and to locate ocean surface features. The scanning is typically in the range 10.3–12.5 μm (IR4 and IR5 channels).

Clouds with high and cold tops, such as cyclones or cumulonimbus clouds, appear red or black, lower warmer clouds such as stratus or stratocumulus show up as blue or grey, with intermediate clouds shaded accordingly. Hot land surfaces will show up as dark-grey or black. One disadvantage of infrared imagery is that low cloud such as stratus or fog can have a temperature similar to the surrounding land or sea surface and does not show up. However, using the difference in brightness of the IR4 channel (10.3–11.5 μm) and the near-infrared channel (1.58–1.64 μm), low cloud can be distinguished, producing a "fog" satellite picture. The main advantage of infrared is that images can be produced at night, allowing a continuous sequence of weather to be studied.

These infrared pictures can depict ocean eddies or vortices and map currents such as the Gulf Stream, which are valuable to the shipping industry. Fishermen and farmers are interested in knowing land and water temperatures to protect their crops against frost or increase their catch from the sea. Even El Niño phenomena can be spotted. Using color-digitized techniques, the gray-shaded thermal images can be converted to color for easier identification of desired information.

The main water vapour channel at 6.40 to 7.08 μm can be imaged by some weather satellites and shows the amount of moisture in the atmosphere.
In the field of climatology, atmospheric infrared radiation is monitored to detect trends in the energy exchange between the earth and the atmosphere. These trends provide information on long-term changes in Earth's climate. It is one of the primary parameters studied in research into global warming, together with solar radiation.
A pyrgeometer is utilized in this field of research to perform continuous outdoor measurements. This is a broadband infrared radiometer with sensitivity for infrared radiation between approximately 4.5 μm and 50 μm.

Astronomers observe objects in the infrared portion of the electromagnetic spectrum using optical components, including mirrors, lenses and solid state digital detectors. For this reason it is classified as part of optical astronomy. To form an image, the components of an infrared telescope need to be carefully shielded from heat sources, and the detectors are chilled using liquid helium.

The sensitivity of Earth-based infrared telescopes is significantly limited by water vapor in the atmosphere, which absorbs a portion of the infrared radiation arriving from space outside of selected atmospheric windows. This limitation can be partially alleviated by placing the telescope observatory at a high altitude, or by carrying the telescope aloft with a balloon or an aircraft. Space telescopes do not suffer from this handicap, and so outer space is considered the ideal location for infrared astronomy.

The infrared portion of the spectrum has several useful benefits for astronomers. Cold, dark molecular clouds of gas and dust in our galaxy will glow with radiated heat as they are irradiated by imbedded stars. Infrared can also be used to detect protostars before they begin to emit visible light. Stars emit a smaller portion of their energy in the infrared spectrum, so nearby cool objects such as planets can be more readily detected. (In the visible light spectrum, the glare from the star will drown out the reflected light from a planet.)

Infrared light is also useful for observing the cores of active galaxies, which are often cloaked in gas and dust. Distant galaxies with a high redshift will have the peak portion of their spectrum shifted toward longer wavelengths, so they are more readily observed in the infrared.

Infrared cleaning is a technique used by some motion picture film scanners, film scanners and flatbed scanners to reduce or remove the effect of dust and scratches upon the finished scan. It works by collecting an additional infrared channel from the scan at the same position and resolution as the three visible color channels (red, green, and blue). The infrared channel, in combination with the other channels, is used to detect the location of scratches and dust. Once located, those defects can be corrected by scaling or replaced by inpainting.

Infrared reflectography can be applied to paintings to reveal underlying layers in a non-destructive manner, in particular the artist's underdrawing or outline drawn as a guide. Art conservators use the technique to examine how the visible layers of paint differ from the underdrawing or layers in between (such alterations are called pentimenti when made by the original artist). This is very useful information in deciding whether a painting is the prime version by the original artist or a copy, and whether it has been altered by over-enthusiastic restoration work. In general, the more pentimenti, the more likely a painting is to be the prime version. It also gives useful insights into working practices. Reflectography often reveals the artist's use of carbon black, which shows up well in reflectograms, as long as it has not also been used in the ground underlying the whole painting.
Recent progress in the design of infrared-sensitive cameras makes it possible to discover and depict not only underpaintings and pentimenti, but entire paintings that were later overpainted by the artist. Notable examples are Picasso's "Woman Ironing" and "Blue Room", where in both cases a portrait of a man has been made visible under the painting as it is known today.

Similar uses of infrared are made by conservators and scientists on various types of objects, especially very old written documents such as the Dead Sea Scrolls, the Roman works in the Villa of the Papyri, and the Silk Road texts found in the Dunhuang Caves. Carbon black used in ink can show up extremely well.

The pit viper has a pair of infrared sensory pits on its head. There is uncertainty regarding the exact thermal sensitivity of this biological infrared detection system.

Other organisms that have thermoreceptive organs are pythons (family Pythonidae), some boas (family Boidae), the Common Vampire Bat ("Desmodus rotundus"), a variety of jewel beetles ("Melanophila acuminata"), darkly pigmented butterflies ("Pachliopta aristolochiae" and "Troides rhadamantus plateni"), and possibly blood-sucking bugs ("Triatoma infestans").

Some fungi like "Venturia inaequalis" require near-infrared light for ejection

Although near-infrared vision (780–1000 nm) has long been deemed impossible due to noise in visual pigments, sensation of near-infrared light was reported in the common carp and in three cichlid species. Fish use NIR to capture prey and for phototactic swimming orientation. NIR sensation in fish may be relevant under poor lighting conditions during twilight and in turbid surface waters.

Near-infrared light, or photobiomodulation, is used for treatment of chemotherapy-induced oral ulceration as well as wound healing. There is some work relating to anti-herpes virus treatment. Research projects include work on central nervous system healing effects via cytochrome c oxidase upregulation and other possible mechanisms.

Strong infrared radiation in certain industry high-heat settings may be hazardous to the eyes, resulting in damage or blindness to the user. Since the radiation is invisible, special IR-proof goggles must be worn in such places.

The discovery of infrared radiation is ascribed to William Herschel, the astronomer, in the early 19th century. Herschel published his results in 1800 before the Royal Society of London. Herschel used a prism to refract light from the sun and detected the infrared, beyond the red part of the spectrum, through an increase in the temperature recorded on a thermometer. He was surprised at the result and called them "Calorific Rays". The term "infrared" did not appear until late 19th century.

Other important dates include:




</doc>
<doc id="15023" url="https://en.wikipedia.org/wiki?curid=15023" title="Icosidodecahedron">
Icosidodecahedron

In geometry, an icosidodecahedron is a polyhedron with twenty (icosi) triangular faces and twelve (dodeca) pentagonal faces. An icosidodecahedron has 30 identical vertices, with two triangles and two pentagons meeting at each, and 60 identical edges, each separating a triangle from a pentagon. As such it is one of the Archimedean solids and more particularly, a quasiregular polyhedron.

An icosidodecahedron has icosahedral symmetry, and its first stellation is the compound of a dodecahedron and its dual icosahedron, with the vertices of the icosidodecahedron located at the midpoints of the edges of either.

Its dual polyhedron is the rhombic triacontahedron. An icosidodecahedron can be split along any of six planes to form a pair of pentagonal rotundae, which belong among the Johnson solids.

The icosidodecahedron can be considered a "pentagonal gyrobirotunda", as a combination of two rotundae (compare pentagonal orthobirotunda, one of the Johnson solids). In this form its symmetry is D, [10,2], (2*5), order 20.

The wire-frame figure of the icosidodecahedron consists of six flat regular decagons, meeting in pairs at each of the 30 vertices.

The icosidodecahedron has 6 central decagons. Projected into a sphere, they define 6 great circles. Buckminster Fuller used these 6 great circles, along with 15 and 10 others in two other polyhedra to define his 31 great circles of the spherical icosahedron.

Convenient Cartesian coordinates for the vertices of an icosidodecahedron with unit edges are given by the even permutations of:
where "φ" is the golden ratio, .

The icosidodecahedron has four special orthogonal projections, centered on a vertex, an edge, a triangular face, and a pentagonal face. The last two correspond to the A and H Coxeter planes.
The surface area "A" and the volume "V" of the icosidodecahedron of edge length "a" are:

The icosidodecahedron can also be represented as a spherical tiling, and projected onto the plane via a stereographic projection. This projection is conformal, preserving angles but not areas or lengths. Straight lines on the sphere are projected as circular arcs on the plane.

The icosidodecahedron is a rectified dodecahedron and also a rectified icosahedron, existing as the full-edge truncation between these regular solids.

The icosidodecahedron contains 12 pentagons of the dodecahedron and 20 triangles of the icosahedron:
The icosidodecahedron exists in a sequence of symmetries of quasiregular polyhedra and tilings with vertex configurations (3."n"), progressing from tilings of the sphere to the Euclidean plane and into the hyperbolic plane. With orbifold notation symmetry of *"n"32 all of these tilings are wythoff construction within a fundamental domain of symmetry, with generator points at the right angle corner of the domain.

The icosidodecahedron is related to the Johnson solid called a pentagonal orthobirotunda created by two pentagonal rotunda connected as mirror images. The "icosidodecahedron" can therefore be called a "pentagonal gyrobirotunda" with the gyration between top and bottom halves.

The truncated cube can be turned into an icosidodecahedron by dividing the octagons into two pentagons and two triangles. It has pyritohedral symmetry.

Eight uniform star polyhedra share the same vertex arrangement. Of these, two also share the same edge arrangement: the small icosihemidodecahedron (having the triangular faces in common), and the small dodecahemidodecahedron (having the pentagonal faces in common). The vertex arrangement is also shared with the compounds of five octahedra and of five tetrahemihexahedra.

In four-dimensional geometry the icosidodecahedron appears in the regular 600-cell as the equatorial slice that belongs to the vertex-first passage of the 600-cell through 3D space. In other words: the 30 vertices of the 600-cell which lie at arc distances of 90 degrees on its circumscribed hypersphere from a pair of opposite vertices, are the vertices of an icosidodecahedron. The wire frame figure of the 600-cell consists of 72 flat regular decagons. Six of these are the equatorial decagons to a pair of opposite vertices. They are precisely the six decagons which form the wire frame figure of the icosidodecahedron.

In the mathematical field of graph theory, a icosidodecahedral graph is the graph of vertices and edges of the icosidodecahedron, one of the Archimedean solids. It has 30 vertices and 60 edges, and is a quartic graph Archimedean graph.

In Star Trek Universe, the Vulcan game of logic Kal-Toh has the goal to create a holographic icosidodecahedron.

In "The Wrong Stars", book one of the Axiom series, by Tim Pratt, Elena has a icosidodecahedron machine on either side of her. [Paperback p 336]

The Hoberman sphere is an icosadodecahedron.





</doc>
<doc id="15024" url="https://en.wikipedia.org/wiki?curid=15024" title="ISO 8601">
ISO 8601

ISO 8601 "Data elements and interchange formats – Information interchange – Representation of dates and times" is an international standard covering the exchange of date- and time-related data. It was issued by the International Organization for Standardization (ISO) and was first published in 1988. The purpose of this standard is to provide an unambiguous and well-defined method of representing dates and times, so as to avoid misinterpretation of numeric representations of dates and times, particularly when data is transferred between countries with different conventions for writing numeric dates and times.

In general, ISO 8601 applies to representations and formats of dates in the Gregorian (and potentially proleptic Gregorian) calendar, of times based on the 24-hour timekeeping system (with optional UTC offset), of , and combinations thereof. The standard does not assign any specific meaning to elements of the date/time to be represented; the meaning will depend on the context of its use. In addition, dates and times to be represented cannot include words with no specified numerical meaning in the standard (e.g., names of years in the Chinese calendar) or that do not use characters (e.g., images, sounds).

In representations for interchange, dates and times are arranged so the largest temporal term (the year) is placed to the left and each successively smaller term is placed to the right of the previous term. Representations must be written in a combination of Arabic numerals and certain characters (such as "-", ":", "T", "W", and "Z") that are given specific meanings within the standard; the implication is that some commonplace ways of writing parts of dates, such as "January" or "Thursday", are not allowed in interchange representations.

The first edition of the ISO 8601 standard was published as "ISO 8601:1988" in 1988. It unified and replaced a number of older ISO standards on various aspects of date and time notation: ISO 2014, ISO 2015, ISO 2711, ISO 3307, and ISO 4031. It has been superseded by a second edition "ISO 8601:2000" in 2000, by a third edition "ISO 8601:2004" published on 1 December 2004, and withdrawn and revised by "ISO 8601-1:2019" and "ISO 8601-2:2019" on 25 February 2019. ISO 8601 was prepared by, and is under the direct responsibility of, ISO Technical Committee TC 154.

ISO 2014, though superseded, is the standard that originally introduced the all-numeric date notation in most-to-least-significant order . The ISO week numbering system was introduced in ISO 2015, and the identification of days by ordinal dates was originally defined in ISO 2711.

Issued in February 2019, the fourth revision of the standard ISO 8601-1:2019 represents slightly updated contents of the previous ISO 8601:2004 standard, whereas the new ISO 8601-2:2019 defines various extensions such as uncertainties or parts of the Extended Date/Time Format (EDTF).


The standard uses the Gregorian calendar, which "serves as an international standard for civil use."

ISO 8601 fixes a reference calendar date to the Gregorian calendar of 20 May 1875 as the date the (Metre Convention) was signed in Paris. However, ISO calendar dates before the convention are still compatible with the Gregorian calendar all the way back to the official introduction of the Gregorian calendar on 15 October 1582. 

Earlier dates, in the proleptic Gregorian calendar, may be used by mutual agreement of the partners exchanging information. The standard states that every date must be consecutive, so usage of the Julian calendar would be contrary to the standard (because at the switchover date, the dates would not be consecutive).

ISO 8601 prescribes, as a minimum, a four-digit year [YYYY] to avoid the year 2000 problem. It therefore represents years from 0000 to 9999, year 0000 being equal to 1 BC and all others AD. However, years prior to 1583 are not automatically allowed by the standard. Instead "values in the range [0000] through [1582] shall only be used by mutual agreement of the partners in information interchange."

To represent years before 0000 or after 9999, the standard also permits the expansion of the year representation but only by prior agreement between the sender and the receiver. An expanded year representation [±YYYYY] must have an agreed-upon number of extra year digits beyond the four-digit minimum, and it must be prefixed with a + or − sign instead of the more common AD/BC (or CE/BCE) notation; by convention 1 BC is labelled +0000, 2 BC is labeled −0001, and so on.

Calendar date representations are in the form shown in the adjacent box. [YYYY] indicates a four-digit year, 0000 through 9999. [MM] indicates a two-digit month of the year, 01 through 12. [DD] indicates a two-digit day of that month, 01 through 31. For example, "5 April 1981" may be represented as either in the "extended format" or "19810405" in the "basic format".

The standard also allows for calendar dates to be written with reduced accuracy. For example, one may write to mean "1981 April". The 2000 version allowed writing to mean "April 5" but the 2004 version does not allow omitting the year when a month is present. One may simply write "1981" to refer to that year or "19" to refer to the century from 1900 to 1999 inclusive. Although the standard allows both the and YYYYMMDD formats for complete calendar date representations, if the day [DD] is omitted then only the format is allowed. By disallowing dates of the form YYYYMM, the standard avoids confusion with the truncated representation YYMMDD (still often used).

Week date representations are in the formats as shown in the adjacent box. [YYYY] indicates the "ISO week-numbering year" which is slightly different from the traditional Gregorian calendar year (see below). [Www] is the "week number" prefixed by the letter "W", from W01 through W53. [D] is the "weekday number", from 1 through 7, beginning with Monday and ending with Sunday.

There are several mutually equivalent and compatible descriptions of week 01:

As a consequence, if 1 January is on a Monday, Tuesday, Wednesday or Thursday, it is in week 01. If 1 January is on a Friday, Saturday or Sunday, it is in week 52 or 53 of the previous year (there is no week 00). 28 December is always in the last week of its year.

The week number can be described by counting the Thursdays: week 12 contains the 12th Thursday of the year.

The "ISO week-numbering year" starts at the first day (Monday) of week 01 and ends at the Sunday before the new ISO year (hence without overlap or gap). It consists of 52 or 53 full weeks. The first ISO week of a year may have up to three days that are actually in the Gregorian calendar year that is ending; if three, they are Monday, Tuesday and Wednesday. Similarly, the last ISO week of a year may have up to three days that are actually in the Gregorian calendar year that is starting; if three, they are Friday, Saturday, and Sunday. The Thursday of each ISO week is always in the Gregorian calendar year denoted by the ISO week-numbering year.

Examples:

An ordinal date is a simple form for occasions when the arbitrary nature of week and month definitions are more of an impediment than an aid, for instance, when comparing dates from different calendars. As represented above, [YYYY] indicates a year. [DDD] is the day of that year, from 001 through 365 (366 in leap years). For example, is also .

This format is used with simple hardware systems that have a need for a date system, but where including full calendar calculation software may be a significant nuisance. This system is sometimes referred to as "Julian Date", but this can cause confusion with the astronomical Julian day, a sequential count of the number of days since day 0 beginning Greenwich noon, Julian proleptic calendar (or noon on ISO date which uses the Gregorian proleptic calendar with a year 0000).

ISO 8601 uses the 24-hour clock system. The "basic format" is [hh][mm][ss] and the "extended format" is [hh]:[mm]:[ss].
So a time might appear as either "134730" in the "basic format" or "13:47:30" in the "extended format".

Either the seconds, or the minutes and seconds, may be omitted from the basic or extended time formats for greater brevity but decreased accuracy; the resulting reduced accuracy time formats are:

"Midnight" is a special case and may be referred to as either "00:00" or "24:00", except in ISO 8601-1:2019 where "24:00" is no longer permitted. The notation "00:00" is used at the beginning of a calendar day and is the more frequently used. At the end of a day use "24:00". "2007-04-05T24:00" is the same instant as "2007-04-06T00:00" (see "Combined date and time representations" below).

A decimal fraction may be added to the lowest order time element present, in any of these representations. A decimal mark, either a comma or a dot (without any preference as stated in resolution 10 of the 22nd General Conference CGPM in 2003, but with a preference for a comma according to ISO 8601:2004) is used as a separator between the time element and its fraction. To denote "14 hours, 30 and one half minutes", do not include a seconds figure. Represent it as "14:30,5", "1430,5", "14:30.5", or "1430.5". There is no limit on the number of decimal places for the decimal fraction. However, the number of decimal places needs to be agreed to by the communicating parties. For example, in Microsoft SQL Server, the precision of a decimal fraction is 3, i.e., "yyyy-mm-ddThh:mm:ss[.mmm]".

Time zones in ISO 8601 are represented as local time (with the location unspecified), as UTC, or as an offset from UTC.

If no UTC relation information is given with a time representation, the time is assumed to be in local time. While it "may" be safe to assume local time when communicating in the same time zone, it is ambiguous when used in communicating across different time zones. Even within a single geographic time zone, some local times will be ambiguous if the region observes daylight saving time. It is usually preferable to indicate a time zone (zone designator) using the standard's notation.

If the time is in UTC, add a "Z" directly after the time without a space. "Z" is the zone designator for the zero UTC offset. "09:30 UTC" is therefore represented as "09:30Z" or "0930Z". "14:45:15 UTC" would be "14:45:15Z" or "144515Z".

The "Z" suffix in the ISO 8601 time representation is sometimes referred to as "Zulu time" because the same letter is used to designate the Zulu time zone. However the ACP 121 standard that defines the list of military time zones makes no mention of UTC and derives the "Zulu time" from the Greenwich Mean Time which was formerly used as the international civil time standard. GMT is no longer precisely defined by the scientific community and can refer to either UTC or UT1 depending on context.

The UTC offset is appended to the time in the same way that 'Z' was above, in the form ±[hh]:[mm], ±[hh][mm], or ±[hh].

Negative UTC offsets describe a time zone west of , where the civil time is behind (or earlier) than UTC so the zone designator will look like "−03:00","−0300", or "−03".

Positive UTC offsets describe a time zone east of , where the civil time is ahead (or later) than UTC so the zone designator will look like "+02:00","+0200", or "+02".

Examples

See List of UTC time offsets for other UTC offsets.

To represent a negative offset, ISO 8601 specifies using either a hyphen–minus or a minus sign character. If the interchange character set is limited and does not have a minus sign character, then the hyphen–minus should be used. ASCII does not have a minus sign, so its hyphen–minus character (code is 45 decimal or 2D hexadecimal) would be used. If the character set has a minus sign, then that character should be used. Unicode has a minus sign, and its character code is U+2212 (2212 hexadecimal); the HTML character entity invocation is codice_1.

The following times all refer to the same moment: "18:30Z", "22:30+04", "1130−0700", and "15:00−03:30". Nautical time zone letters are not used with the exception of Z. To calculate UTC time one has to subtract the offset from the local time, e.g. for "15:00−03:30" do 15:00 − (−03:30) to get 18:30 UTC.

An offset of zero, in addition to having the special representation "Z", can also be stated numerically as "+00:00", "+0000", or "+00". However, it is not permitted to state it numerically with a negative sign, as "−00:00", "−0000", or "−00". The section dictating sign usage (section 3.4.2 in the 2004 edition of the standard) states that a plus sign must be used for a positive or zero value, and a minus sign for a negative value. Contrary to this rule, RFC 3339, which is otherwise a profile of ISO 8601, permits the use of "-00", with the same denotation as "+00" but a differing connotation.

A single point in time can be represented by concatenating a complete date expression, the letter ""T"" as a delimiter, and a valid time expression. For example, . It is permitted to omit the ""T"" character by mutual agreement as in .
Separating date and time parts with other characters such as space is not allowed in ISO 8601, but allowed in its profile RFC 3339.

If a time zone designator is required, it follows the combined date and time. For example, or .

Either basic or extended formats may be used, but both date and time must use the same format. The date expression may be calendar, week, or ordinal, and must use a complete representation. The time may be represented using a specified reduced accuracy format.

Durations define the amount of intervening time in a time interval and are represented by the format P[n]Y[n]M[n]DT[n]H[n]M[n]S or P[n]W as shown to the right. In these representations, the [n] is replaced by the value for each of the date and time elements that follow the [n]. Leading zeros are not required, but the maximum number of digits for each element should be agreed to by the communicating parties. The capital letters "P", "Y", "M", "W", "D", "T", "H", "M", and "S" are designators for each of the date and time elements and are not replaced.


For example, "P3Y6M4DT12H30M5S" represents a duration of "three years, six months, four days, twelve hours, thirty minutes, and five seconds".

Date and time elements including their designator may be omitted if their value is zero, and lower-order elements may also be omitted for reduced precision. For example, "P23DT23H" and "P4Y" are both acceptable duration representations. However, at least one element must be present, thus "P" is not a valid representation for a duration of 0 seconds. "PT0S" or "P0D", however, are both valid and represent the same duration.

To resolve ambiguity, "P1M" is a one-month duration and "PT1M" is a one-minute duration (note the time designator, T, that precedes the time value). The smallest value used may also have a decimal fraction, as in "P0.5Y" to indicate half a year. This decimal fraction may be specified with either a comma or a full stop, as in "P0,5Y" or "P0.5Y". The standard does not prohibit date and time values in a duration representation from exceeding their "carry over points" except as noted below. Thus, "PT36H" could be used as well as "P1DT12H" for representing the same duration. But keep in mind that "PT36H" is not the same as "P1DT12H" when switching from or to Daylight saving time.

Alternatively, a format for duration based on combined date and time representations may be used by agreement between the communicating parties either in the basic format PYYYYMMDDThhmmss or in the extended format . For example, the first duration shown above would be . However, individual date and time values cannot exceed their moduli (e.g. a value of 13 for the month or 25 for the hour would not be permissible).

Although the standard describes a duration as part of time intervals, which are discussed in the next section, the duration format (or a subset thereof) is widely used independent of time intervals, as with the Java 8 Duration class.

A time interval is the intervening time between two time points. The amount of intervening time is expressed by a duration (as described in the previous section). The two time points (start and end) are expressed by either a combined date and time representation or just a date representation.

There are four ways to express a time interval:


Of these, the first three require two values separated by an "interval designator" which is usually a solidus (more commonly referred to as a forward slash "/"). Section 4.4.2 of the standard notes that: "In certain application areas a double hyphen is used as a separator instead of a solidus." The standard does not define the term "double hyphen", but previous versions used notations like "2000--2002". Use of a double hyphen instead of a solidus allows inclusion in computer filenames. A solidus is a reserved character and not allowed in a filename in common operating systems.

For <start>/<end> expressions, if any elements are missing from the end value, they are assumed to be the same as for the start value including the time zone. This feature of the standard allows for concise representations of time intervals. For example, the date of a two-hour meeting including the start and finish times could be simply shown as "2007-12-14T13:30/15:30", where "/15:30" implies "/2007-12-14T15:30" (the same date as the start), or the beginning and end dates of a monthly billing period as "2008-02-15/03-14", where "/03-14" implies "/2008-03-14" (the same year as the start).

If greater precision is desirable to represent the time interval, then more time elements can be added to the representation. An interval denoted can start at any time on and end at any time on , whereas includes the start and end times.
To explicitly include all of the start and end dates, the interval would be represented as .

Repeating intervals are specified in clause "4.5 Recurring time interval". They are formed by adding "R[n]/" to the beginning of an interval expression, where "R" is used as the letter itself and [n] is replaced by the number of repetitions. Leaving out the value for [n] means an unbounded number of repetitions. If the interval specifies the start (forms 1 and 2 above), then this is the start of the repeating interval. If the interval specifies the end but not the start (form 3 above), then this is the end of the repeating interval. For example, to repeat the interval of "P1Y2M10DT2H30M" five times starting at , use .

ISO 8601:2000 allowed truncation (by agreement), where leading components of a date or time are omitted. Notably, this allowed two-digit years to be used and the ambiguous formats YY-MM-DD and YYMMDD. This provision was removed in ISO 8601:2004.

On the Internet, the World Wide Web Consortium (W3C) uses ISO 8601 in defining a profile of the standard that restricts the supported date and time formats to reduce the chance of error and the complexity of software.

ISO 8601 is referenced by several specifications, but the full range of options of ISO 8601 is not always used. For example, the various electronic program guide standards for TV, digital radio, etc. use several forms to describe points in time and durations. The ID3 audio meta-data specification also makes use of a subset of ISO 8601.
The X.690 encoding standard's GeneralizedTime makes use of another subset of ISO 8601.

The ISO 8601 week date, as of 2006, appeared in its basic form on major brand commercial packaging in the United States. Its appearance depended on the particular packaging, canning, or bottling plant more than any particular brand. The format is particularly useful for quality assurance, so that production errors can be readily traced to work weeks, and products can be correctly targeted for recall.

RFC 3339 defines a profile of ISO 8601 for use in Internet protocols and standards. It explicitly excludes durations and dates before the common era. The more complex formats such as week numbers and ordinal days are not permitted.

RFC 3339 deviates from ISO 8601 in allowing a zero time zone offset to be specified as "-00:00", which ISO 8601 forbids. RFC 3339 intends "-00:00" to carry the connotation that it is not stating a preferred time zone, whereas the conforming "+00:00" or any non-zero offset connotes that the offset being used is preferred. This convention regarding "-00:00" is derived from earlier RFCs, such as RFC 2822 which uses it for timestamps in email headers. RFC 2822 made no claim that any part of its timestamp format conforms to ISO 8601, and so was free to use this convention without conflict.



Implementation overview


</doc>
<doc id="15027" url="https://en.wikipedia.org/wiki?curid=15027" title="Isa">
Isa

Isa or ISA may refer to:











In alphabetical order:





</doc>
<doc id="15028" url="https://en.wikipedia.org/wiki?curid=15028" title="International Seabed Authority">
International Seabed Authority

The International Seabed Authority (ISA) () is an intergovernmental body based in Kingston, Jamaica, that was established to organize, regulate and control all mineral-related activities in the international seabed area beyond the limits of national jurisdiction, an area underlying most of the world's oceans. It is an organization established by the United Nations Convention on the Law of the Sea.

Following at least ten preparatory meetings over the years, the Authority held its first inaugural meeting in its host country, Jamaica, on 16 November 1994, the day the Convention came into force. The articles governing the Authority have been made "noting the political and economic changes, including market-oriented approaches, affecting the implementation" of the Convention. The Authority obtained its observer status to the United Nations in October 1996.

Currently, the Authority has 167 members and the European Union, composed of all parties to the United Nations Convention on the Law of the Sea.

Two principal organs establish the policies and govern the work of the Authority: the Assembly, in which all members are represented, and a 36-member Council elected by the Assembly. Council members are chosen according to a formula designed to ensure equitable representation of countries from various groups, including those engaged in seabed mineral exploration and the land-based producers of minerals found on the seabed. The Authority holds one annual session, usually of two weeks' duration.

Also established is a 30-member Legal and Technical Commission which advises the Council and a 15-member Finance Committee that deals with budgetary and related matters. All members are experts nominated by governments and elected to serve in their individual capacity.

The Authority operates by contracting with private and public corporations and other entities authorizing them to explore, and eventually exploit, specified areas on the deep seabed for mineral resources essential for building most technological products. The Convention also established a body called the Enterprise which is to serve as the Authority's own mining operator, but no concrete steps have been taken to bring this into being.

The Authority currently has a Secretariat of 37 authorized posts and a biennial budget of $9.1 million for 2017 and $8.9 million for 2018. In July 2016, the Assembly of the Authority elected Michael Lodge of the United Kingdom, for a four-year term as Secretary-General beginning 1 January 2017. He succeeds Nii Allotey Odunton of Ghana, who had served two consecutive four-year terms since 2008.

The exploitation system envisaged in the UN Convention on the Law of the Sea, overseen by the Authority, came to life with the signature in 2001/02 of 15-year contracts with seven organizations that had applied for specific seabed areas in which they were authorized to explore for polymetallic nodules. In 2006, a German entity was added to the list.

These contractors are: Yuzhmorgeologya (Russian Federation); Interoceanmetal Joint Organization (IOM) (Bulgaria, Cuba, Slovakia, Czech Republic, Poland and Russian Federation); the Government of the Republic of Korea; China Ocean Minerals Research and Development Association (COMRA) (China); Deep Ocean Resources Development Company (DORD) (Japan); Institut français de recherche pour l’exploitation de la mer (IFREMER) (France); the Government of India, the Federal Institute for Geosciences and Natural Resources of Germany.

All but one of the current areas of exploration are in the Clarion-Clipperton Zone, in the Equatorial North Pacific Ocean south and southeast of Hawaii. The remaining area, being explored by India, is in the Central Indian Basin of the Indian Ocean.

Each area is limited to , of which half is to be relinquished to the Authority after eight years. Each contractor is required to report once a year on its activities in its assigned area. So far, none of them has indicated any serious move to begin commercial exploitation.

In 2008, the Authority received two new applications for authorization to explore for polymetallic nodules, coming for the first time from private firms in developing island nations of the Pacific. Sponsored by their respective governments, they were submitted by Nauru Ocean Resources Inc. and Tonga Offshore Mining Limited. A 15-year exploration contract was granted by the Authority to Nauru Ocean Resources Inc. on 22 July 2011 and to Tonga Offshore Mining Limited on 12 January 2012.

Fifteen-year exploration contracts for polymetallic nodules were also granted to G-TECH Sea Mineral Resources NV (Belgium) on 14 January 2013; Marawa Research and Exploration Ltd (Kiribati) on 19 January 2015; Ocean Mineral Singapore Pte Ltd on 22 January 2015; UK Seabed Resources Ltd (two contracts on 8 February 2013 and 29 March 2016 respectively); Cook Islands Investment Corporation on 15 July 2016 and more recently China Minmetals Corporation on 12 May 2017. 

The Authority has signed seven contracts for the exploration for polymetallic sulphides in the South West Indian Ridge, Central Indian Ridge and Mid-Atlantic Ridge with China Ocean Mineral Resources Research and Development Association (18 November 2011); the Government of Russia (29 October 2012); Government of the Republic of Korea (24 June 2014); Institut français de recherche pour l’exploitation de la mer (Ifremer,France, 18 November 2014); the Federal Institute for Geosciences and Natural Resources of Germany (6 May 2015); and the Government of India (26 September 2016) and the Government of the Republic of Poland (12 February 2018). 

The Authority also holds five contracts for the exploration of cobalt-rich ferromanganese crusts in the Western Pacific Ocean with China Ocean Mineral Resources Research and Development Association (29 April 2014); Japan Oil Gas and Metals National Corporation (JOGMEC, 27 January 2014); Ministry of Natural Resources and Environment of the Russian Federation (10 March 2015), Companhia De Pesquisa de Recursos Minerais (9 November 2015) and the Government of the Republic of Korea (27 March 2018). 

The Authority's main legislative accomplishment to date has been the adoption, in the year 2000, of regulations governing exploration for polymetallic nodules. These resources, also called manganese nodules, contain varying amounts of manganese, cobalt, copper and nickel. They occur as potato-sized lumps scattered about on the surface of the ocean floor, mainly in the central Pacific Ocean but with some deposits in the Indian Ocean.

The Council of the Authority began work, in August 2002, on another set of regulations, covering polymetallic sulfides and cobalt-rich ferromanganese crusts, which are rich sources of such minerals as copper, iron, zinc, silver and gold, as well as cobalt. The sulphides are found around volcanic hot springs, especially in the western Pacific Ocean, while the crusts occur on oceanic ridges and elsewhere at several locations around the world. The Council decided in 2006 to prepare separate sets of regulations for sulphides and for crusts, with priority given to sulphides. It devoted most of its sessions in 2007 and 2008 to this task, but several issues remained unresolved. Chief among these were the definition and configuration of the area to be allocated to contractors for exploration, the fees to be paid to the Authority and the question of how to deal with any overlapping claims that might arise. Meanwhile, the Legal and Technical Commission reported progress on ferromanganese crusts.

In addition to its legislative work, the Authority organizes annual workshops on various aspects of seabed exploration, with emphasis on measures to protect the marine environment from any harmful consequences. It disseminates the results of these meetings through publications. Studies over several years covering the key mineral area of the Central Pacific resulted in a technical study on biodiversity, species ranges and gene flow in the abyssal Pacific nodule province, with emphasis on predicting and managing the impacts of deep seabed mining A workshop at Manoa, Hawaii, in October 2007 produced a rationale and recommendations for the establishment of "preservation reference areas" in the Clarion-Clipperton Zone, where nodule mining would be prohibited in order to leave the natural environment intact. The most recent workshop, held at Chennai, India, in February 2008, concerned polymetallic nodule mining technology, with special reference to its current status and challenges ahead

Contrary to early hopes that seabed mining would generate extensive revenues for both the exploiting countries and the Authority, no technology has yet been developed for gathering deep-sea minerals at costs that can compete with land-based mines. Until recently, the consensus has been that economic mining of the ocean depths might be decades away. Moreover, the United States, with some of the most advanced ocean technology in the world, has not yet ratified the Law of the Sea Convention and is thus not a member of the Authority.

In recent years, however, interest in deep-sea mining, especially with regard to ferromanganese crusts and polymetallic sulphides, has picked up among several firms now operating in waters within the national zones of Papua New Guinea, Fiji and Tonga. Papua New Guinea was the first country in the world to grant commercial exploration licenses for seafloor massive sulphide deposits when it granted the initial license to Nautilus Minerals in 1997. Japan's new ocean policy emphasizes the need to develop methane hydrate and hydrothermal deposits within Japan's exclusive economic zone and calls for the commercialization of these resources within the next 10 years. Reporting on these developments in his annual report to the Authority in April 2008, Secretary-General Nandan referred also to the upward trend in demand and prices for cobalt, copper, nickel and manganese, the main metals that would be derived from seabed mining, and he noted that technologies being developed for offshore extraction could be adapted for deep sea mining.

In its preamble, UNCLOS defines the international seabed area—the part under ISA jurisdiction—as "the seabed and ocean floor and the subsoil thereof, beyond the limits of national jurisdiction". There are no maps annexed to the Convention to delineate this area. Rather, UNCLOS outlines the areas of national jurisdiction, leaving the rest for the international portion. National jurisdiction over the seabed normally leaves off at seaward from baselines running along the shore, unless a nation can demonstrate that its continental shelf is naturally prolonged beyond that limit, in which case it may claim up to . ISA has no role in determining this boundary. Rather, this task is left to another body established by UNCLOS, the Commission on the Limits of the Continental Shelf, which examines scientific data submitted by coastal states that claim a broader reach. Maritime boundaries between states are generally decided by bilateral negotiation (sometimes with the aid of judicial bodies), not by ISA.

Recently, there has been much interest in the possibility of exploiting seabed resources in the Arctic Ocean, bordered by Canada, Denmark, Iceland, Norway, Russia and the United States (see Territorial claims in the Arctic). Mineral exploration and exploitation activities in any seabed area not belonging to these states would fall under ISA jurisdiction.

In 2006 the Authority established an Endowment Fund to Support Collaborative Marine Scientific Research on the International Seabed Area. The Fund will aid experienced scientists and technicians from developing countries to participate in deep-sea research organized by international and national institutions. A campaign was launched in February 2008 to identify participants, establish a network of cooperating bodies and seek outside funds to augment the initial $3 million endowment from the Authority.

The International Seabed Authority Endowment Fund promotes and encourages the conduct of collaborative marine scientific research in the international seabed area through two main activities:

The Secretariat of the International Seabed Authority is facilitating these activities by creating and maintaining an ongoing list of opportunities for scientific collaboration, including research cruises, deep-sea sample analysis, and training and internship programmes. This entails building a network of co-operating groups interested in (or presently undertaking) these types of activities and programmes, such as universities, institutions, contractors with the Authority and other entities.

The Secretariat is also actively seeking applications from scientists and other technical personnel from developing nations to be considered for assistance under the Fund. Application guidelines have been prepared for potential recipients to participate in marine scientific research programmes or other scientific co-operation activity, to enroll in training programmes, and to qualify for technical assistance. An advisory panel will evaluate all incoming applications and make recommendations to the Secretary-General of the International Seabed Authority so successful applicants may be awarded with Fund assistance.

To maximize opportunities for and participation in the Fund, the Secretariat is also seeking donations and in-kind contributions to build on the initial investment of US$3 million. This entails raising awareness of the Fund, reporting on its successes and encouraging new activities and participants.

In 2017, the Authority registered seven voluntary commitments with the UN Oceans Conference for Sustainable Development Goal 14. These were:

The exact nature of the ISA's mission and authority has been questioned by opponents of the Law of the Sea Treaty who are generally skeptical of multilateral engagement by the United States. The United States is the only major maritime power that has not ratified the Convention (see United States non-ratification of the UNCLOS), with one of the main anti-ratification arguments being a charge that the ISA is flawed or unnecessary. In its original form, the Convention included certain provisions that some found objectionable, such as:

Because of these concerns, the United States pushed for modification of the Convention, obtaining a 1994 Agreement on Implementation that somewhat mitigates them and thus modifies the ISA's authority. Despite this change the United States has not ratified the Convention and so is not a member of ISA, although it sends sizable delegations to participate in meetings as an observer.




</doc>
<doc id="15029" url="https://en.wikipedia.org/wiki?curid=15029" title="Industry Standard Architecture">
Industry Standard Architecture

Industry Standard Architecture (ISA) is the 16-bit internal bus of IBM PC/AT and similar computers based on the Intel 80286 and its immediate successors during the 1980s. The bus was (largely) backward compatible with the 8-bit bus of the 8088-based IBM PC, including the IBM PC/XT as well as IBM PC compatibles.

Originally referred to as the PC/AT-bus, it was also termed "I/O Channel" by IBM. The ISA term was coined as a retronym by competing PC-clone manufacturers in the late 1980s or early 1990s as a reaction to IBM attempts to replace the AT-bus with its new and incompatible Micro Channel architecture.

The 16-bit ISA bus was also used with 32-bit processors for several years. An attempt to extend it to 32 bits, called Extended Industry Standard Architecture (EISA), was not very successful, however. Later buses such as VESA Local Bus and PCI were used instead, often along with ISA slots on the same mainboard. Derivatives of the AT bus structure were and still are used in ATA/IDE, the PCMCIA standard, Compact Flash, the PC/104 bus, and internally within Super I/O chips.

Even when ISA disappeared from consumer desktops many years ago, it is still used in industrial PCs, where certain specialized expansion cards, never transitioned to PCI and PCI Express, are used.

The ISA bus was developed by a team led by Mark Dean at IBM as part of the IBM PC project in 1981 Compaq created the term "Industry Standard Architecture" (ISA) to replace "PC compatible". ISA originated as an 8-bit system. A 16-bit version, the IBM AT bus, was introduced with the release of the IBM PC/AT in 1984. In 1988, the 32-bit Extended Industry Standard Architecture (EISA) standard was proposed by the "Gang of Nine" group of PC-compatible manufacturers that included Compaq. In the process, they retroactively renamed the AT bus to "ISA" to avoid infringing IBM's trademark on its PC/AT computer.

IBM designed the 8-bit version as a buffered interface to the motherboard buses of the Intel 8088 (16/8 bit) CPU in the IBM PC and PC/XT. The 16-bit version was an upgrade for the motherboard buses of the Intel 80286 CPU used in the IBM AT. The ISA bus was therefore synchronous with the CPU clock, until sophisticated buffering methods were implemented by chipsets to interface ISA to much faster CPUs.

ISA was designed to connect peripheral cards to the motherboard and allows for bus mastering. Only the first 16 MB of main memory is addressable. The original 8-bit bus ran from the 4.77 MHz clock of the 8088 CPU in the IBM PC and PC/XT. The original 16-bit bus ran from the CPU clock of the 80286 in IBM PC/AT computers, which was 6 MHz in the first models and 8 MHz in later models. The IBM RT PC also used the 16-bit bus. ISA was also used in some non-IBM compatible machines such as Motorola 68k-based Apollo (68020) and Amiga 3000 (68030) workstations, the short-lived AT&T Hobbit and the later PowerPC-based BeBox.

Companies like Dell improved the AT bus's performance but in 1987, IBM replaced the AT bus with its proprietary Micro Channel Architecture (MCA). MCA overcame many of the limitations then apparent in ISA but was also an effort by IBM to regain control of the PC architecture and the PC market. MCA was far more advanced than ISA and had many features that would later appear in PCI. However, MCA was also a closed standard whereas IBM had released full specifications and circuit schematics for ISA. Computer manufacturers responded to MCA by developing the Extended Industry Standard Architecture (EISA) and the later VESA Local Bus (VLB). VLB used some electronic parts originally intended for MCA because component manufacturers already were equipped to manufacture them. Both EISA and VLB were backwards-compatible expansions of the AT (ISA) bus.

Users of ISA-based machines had to know special information about the hardware they were adding to the system. While a handful of devices were essentially "plug-n-play", this was rare. Users frequently had to configure parameters when adding a new device, such as the IRQ line, I/O address, or DMA channel. MCA had done away with this complication and PCI actually incorporated many of the ideas first explored with MCA, though it was more directly descended from EISA.

This trouble with configuration eventually led to the creation of ISA PnP, a plug-n-play system that used a combination of modifications to hardware, the system BIOS, and operating system software to automatically manage resource allocations. In reality, ISA PnP could be troublesome and did not become well-supported until the architecture was in its final days.

PCI slots were the first physically-incompatible expansion ports to directly squeeze ISA off the motherboard. At first, motherboards were largely ISA, including a few PCI slots. By the mid-1990s, the two slot types were roughly balanced, and ISA slots soon were in the minority of consumer systems. Microsoft's PC 99 specification recommended that ISA slots be removed entirely, though the system architecture still required ISA to be present in some vestigial way internally to handle the floppy drive, serial ports, etc., which was why the software compatible LPC bus was created. ISA slots remained for a few more years, and towards the turn of the century it was common to see systems with an Accelerated Graphics Port (AGP) sitting near the central processing unit, an array of PCI slots, and one or two ISA slots near the end. In late 2008, even floppy disk drives and serial ports were disappearing, and the extinction of vestigial ISA (by then the LPC bus) from chipsets was on the horizon.

PCI slots are "rotated" compared to their ISA counterparts—PCI cards were essentially inserted "upside-down," allowing ISA and PCI connectors to squeeze together on the motherboard. Only one of the two connectors can be used in each slot at a time, but this allowed for greater flexibility.

The AT Attachment (ATA) hard disk interface is directly descended from the 16-bit ISA of the PC/AT. ATA has its origins in hardcards that integrated a hard disk drive (HDD) and a hard disk controller (HDC) onto one card. This was at best awkward and at worst damaging to the motherboard, as ISA slots were not designed to support such heavy devices as HDDs. The next generation of Integrated Drive Electronics drives moved both the drive and controller to a drive bay and used a ribbon cable and a very simple interface board to connect it to an ISA slot. ATA is basically a standardization of this arrangement plus a uniform command structure for software to interface with the HDC within the drive. ATA has since been separated from the ISA bus and connected directly to the local bus, usually by integration into the chipset, for much higher clock rates and data throughput than ISA could support. ATA has clear characteristics of 16-bit ISA, such as a 16-bit transfer size, signal timing in the PIO modes and the interrupt and DMA mechanisms.

The PC/XT-bus is an eight-bit ISA bus used by Intel 8086 and Intel 8088 systems in the IBM PC and IBM PC XT in the 1980s. Among its 62 pins were demultiplexed and electrically buffered versions of the 8 data and 20 address lines of the 8088 processor, along with power lines, clocks, read/write strobes, interrupt lines, etc. Power lines included −5 V and ±12 V in order to directly support pMOS and enhancement mode nMOS circuits such as dynamic RAMs among other things. The XT bus architecture uses a single Intel 8259 PIC, giving eight vectorized and prioritized interrupt lines. It has four DMA channels originally provided by the Intel 8237, 3 of the DMA channels are brought out to the XT bus expansion slots; of these, 2 are normally already allocated to machine functions (diskette drive and hard disk controller):
The PC/AT-bus, a 16-bit (or 80286-) version of the PC/XT bus, was introduced with the IBM PC/AT. This bus was officially termed "I/O Channel" by IBM. It extends the XT-bus by adding a second shorter edge connector in-line with the eight-bit XT-bus connector, which is unchanged, retaining compatibility with most 8-bit cards. The second connector adds four additional address lines for a total of 24, and 8 additional data lines for a total of 16. It also adds new interrupt lines connected to a second 8259 PIC (connected to one of the lines of the first) and 4 × 16-bit DMA channels, as well as control lines to select 8- or 16-bit transfers.

The 16-bit AT bus slot originally used two standard edge connector sockets in early IBM PC/AT machines. However, with the popularity of the AT-architecture and the 16-bit ISA bus, manufacturers introduced specialized 98-pin connectors that integrated the two sockets into one unit. These can be found in almost every AT-class PC manufactured after the mid-1980s. The ISA slot connector is typically black (distinguishing it from the brown EISA connectors and white PCI connectors).

Motherboard devices have dedicated IRQs (not present in the slots). 16-bit devices can use either PC-bus or PC/AT-bus IRQs. It is therefore possible to connect up to 6 devices that use one 8-bit IRQ each, or up to 5 devices that use one 16-bit IRQ each. At the same time, up to 4 devices may use one 8-bit DMA channel each, while up to 3 devices can use one 16-bit DMA channel each.

Originally, the bus clock was synchronous with the CPU clock, resulting in varying bus clock frequencies among the many different IBM "clones" on the market (sometimes as high as 16 or 20 MHz), leading to software or electrical timing problems for certain ISA cards at bus speeds they were not designed for. Later motherboards or integrated chipsets used a separate clock generator, or a clock divider which either fixed the ISA bus frequency at 4, 6, or 8 MHz or allowed the user to adjust the frequency via the BIOS setup. When used at a higher bus frequency, some ISA cards (certain Hercules-compatible video cards, for instance), could show significant performance improvements.

Memory address decoding for the selection of 8 or 16-bit transfer mode was limited to 128 KiB sections, leading to problems when mixing 8- and 16-bit cards as they could not co-exist in the same 128 KiB area. This is because the MEMCS16 line is required to be set based on the value of LA17-23 only.

ISA is still used today for specialized industrial purposes. In 2008 IEI Technologies released a modern motherboard for Intel Core 2 Duo processors which, in addition to other special I/O features, is equipped with two ISA slots. It is marketed to industrial and military users who have invested in expensive specialized ISA bus adaptors, which are not available in PCI bus versions.

Similarly, ADEK Industrial Computers is releasing a motherboard in early 2013 for Intel Core i3/i5/i7 processors, which contains one (non-DMA) ISA slot.

The PC/104 bus, used in industrial and embedded applications, is a derivative of the ISA bus, utilizing the same signal lines with different connectors. The LPC bus has replaced the ISA bus as the connection to the legacy I/O devices on recent motherboards; while physically quite different, LPC looks just like ISA to software, so that the peculiarities of ISA such as the 16 MiB DMA limit (which corresponds to the full address space of the Intel 80286 CPU used in the original IBM AT) are likely to stick around for a while.

As explained in the "History" section, ISA was the basis for development of the ATA interface, used for ATA (a.k.a. IDE) and more recently Serial ATA (SATA) hard disks. Physically, ATA is essentially a simple subset of ISA, with 16 data bits, support for exactly one IRQ and one DMA channel, and 3 address bits. To this ISA subset, ATA adds two IDE address select ("chip select") lines and a few unique signal lines specific to ATA/IDE hard disks (such as the Cable Select/Spindle Sync. line.) In addition to the physical interface channel, ATA goes beyond and far outside the scope of ISA by also specifying a set of physical device registers to be implemented on every ATA (IDE) drive and a full set of protocols and device commands for controlling fixed disk drives using these registers. The ATA device registers are accessed using the address bits and address select signals in the ATA physical interface channel, and all operations of ATA hard disks are performed using the ATA-specified protocols through the ATA command set. The earliest versions of the ATA standard featured a few simple protocols and a basic command set comparable to the command sets of MFM and RLL controllers (which preceded ATA controllers), but the latest ATA standards have much more complex protocols and instruction sets that include optional commands and protocols providing such advanced optional-use features as sizable hidden system storage areas, password security locking, and programmable geometry translation.

A further deviation between ISA and ATA is that while the ISA bus remained locked into a single standard clock rate (for backward hardware compatibility), the ATA interface offered many different speed modes, could select among them to match the maximum speed supported by the attached drives, and kept adding faster speeds with later versions of the ATA standard (up to 133 MB/s for ATA-6, the latest.) In most forms, ATA ran much faster than ISA, provided it was connected directly to a local bus faster than the ISA bus.

Before the 16-bit ATA/IDE interface, there was an 8-bit XT-IDE (also known as XTA) interface for hard disks. It was not nearly as popular as ATA has become, and XT-IDE hardware is now fairly hard to find. Some XT-IDE adapters were available as 8-bit ISA cards, and XTA sockets were also present on the motherboards of Amstrad's later XT clones as well as a short-lived line of Philips units. The XTA pinout was very similar to ATA, but only eight data lines and two address lines were used, and the physical device registers had completely different meanings. A few hard drives (such as the Seagate ST351A/X) could support either type of interface, selected with a jumper.

Many later AT (and AT successor) motherboards had no integrated hard drive interface but relied on a separate hard drive interface plugged into an ISA/EISA/VLB slot. There were even a few 80486 based units shipped with MFM/RLL interfaces and drives instead of the increasingly common AT-IDE.

Commodore built the XT-IDE based peripheral hard drive / memory expansion unit A590 for their Amiga 500 and 500+ computers that also supported a SCSI drive. Later models – the A600, A1200, and the Amiga 4000 series – use AT-IDE drives.

The PCMCIA specification can be seen as a superset of ATA. The standard for PCMCIA hard disk interfaces, which included PCMCIA flash drives, allows for the mutual configuration of the port and the drive in an ATA mode. As a de facto extension, most PCMCIA flash drives additionally allow for a simple ATA mode that is enabled by pulling a single pin low, so that PCMCIA hardware and firmware are unnecessary to use them as an ATA drive connected to an ATA port. PCMCIA flash drive to ATA adapters are thus simple and inexpensive, but are not guaranteed to work with any and every standard PCMCIA flash drive. Further, such adapters cannot be used as generic PCMCIA ports, as the PCMCIA interface is much more complex than ATA.

Although most modern computers do not have physical ISA buses, all IBM compatible computers — x86, and x86-64 (most non-mainframe, non-embedded) — have ISA buses allocated in physical address space. Embedded controller chips (southbridge) and CPUs themselves provide services such as temperature monitoring and voltage readings through these buses as ISA devices.

IEEE started a standardization of the ISA bus in 1985, called the P996 specification. However, despite there even having been books published on the P996 specification, it never officially progressed past draft status.
There still is an existing user base with old computers, so some ISA cards are still manufactured, e.g. with USB ports or complete single board computers based on modern processors, USB 3.0, and SATA.





</doc>
<doc id="15030" url="https://en.wikipedia.org/wiki?curid=15030" title="Intergovernmental Panel on Climate Change">
Intergovernmental Panel on Climate Change

The Intergovernmental Panel on Climate Change (IPCC) is an intergovernmental body of the United Nations that is dedicated to providing the world with objective, scientific information relevant to understanding the scientific basis of the
risk of human-induced climate change, its natural, political, and economic impacts and risks, and possible response options.

The IPCC was established in 1988 by the World Meteorological Organization (WMO) and the United Nations Environment Programme (UNEP) and was later endorsed by the United Nations General Assembly. Membership is open to all members of the WMO and UN.
The IPCC produces reports that contribute to the work of the United Nations Framework Convention on Climate Change (UNFCCC), the main international treaty on climate change. The objective of the UNFCCC is to "stabilize greenhouse gas concentrations in the atmosphere at a level that would prevent dangerous anthropogenic (human-induced) interference with the climate system". The IPCC's Fifth Assessment Report was a critical scientific input into the UNFCCC's Paris Agreement in 2015.

IPCC reports cover the "scientific, technical and socio-economic information relevant to understanding the scientific basis of risk of human-induced climate change, its potential impacts and options for adaptation and mitigation." The IPCC does not carry out original research, nor does it monitor climate or related phenomena itself. Rather, it assesses published literature, including peer-reviewed and non-peer-reviewed sources. However, the IPCC can be said to stimulate research in climate science. Chapters of IPCC reports often close with sections on limitations and knowledge or research gaps, and the announcement of an IPCC special report can catalyse research activity in that area.

Thousands of scientists and other experts contribute on a voluntary basis to writing and reviewing reports, which are then reviewed by governments. IPCC reports contain a "Summary for Policymakers", which is subject to line-by-line approval by delegates from all participating governments. Typically, this involves the governments of more than 120 countries.

The IPCC provides an internationally accepted authority on climate change, producing reports that have the agreement of leading climate scientists and consensus from participating governments. The 2007 Nobel Peace Prize was shared between the IPCC and Al Gore.

Following the election of a new Bureau in 2015, the IPCC embarked on its sixth assessment cycle. Besides the Sixth Assessment Report, to be completed in 2022, the IPCC released the Special Report on Global Warming of 1.5 °C in October 2018, released an update to its 2006 Guidelines for National Greenhouse Gas Inventories—the 2019 Refinement—in May 2019, and delivered two further special reports in 2019: the Special Report on Climate Change and Land (SRCCL), published online on 7 August, and the Special Report on the Ocean and Cryosphere in a Changing Climate (SROCC), released on 25 September 2019. This makes the sixth assessment cycle the most ambitious in the IPCC's 30-year history. The IPCC also decided to prepare a special report on cities and climate change in the seventh assessment cycle and held a conference in March 2018 to stimulate research in this area.

The IPCC developed from an international scientific body, the Advisory Group on Greenhouse Gases set up in 1985 by the International Council of Scientific Unions, the United Nations Environment Programme (UNEP), and the World Meteorological Organization (WMO) to provide recommendations based on current research. This small group of scientists lacked the resources to cover the increasingly complex interdisciplinary nature of climate science. The United States Environmental Protection Agency and State Department wanted an international convention to agree restrictions on greenhouse gases, and the conservative Reagan Administration was concerned about unrestrained influence from independent scientists or from United Nations bodies including UNEP and the WMO. The U.S. government was the main force in forming the IPCC as an autonomous intergovernmental body in which scientists took part both as experts on the science and as official representatives of their governments, to produce reports which had the firm backing of all the leading scientists worldwide researching the topic, and which then had to gain consensus agreement from every one of the participating governments. In this way, it was formed as a hybrid between a scientific body and an intergovernmental political organisation.

The United Nations formally endorsed the creation of the IPCC in 1988. Some of the reasons the UN stated in its resolution include

The IPCC was tasked with reviewing peer-reviewed scientific literature and other relevant publications to provide information on the state of knowledge about climate change.

The IPCC does not conduct its own original research. It produces comprehensive assessments, reports on special topics, and methodologies. The assessments build on previous reports, highlighting the latest knowledge. For example, the wording of the reports from the first to the fifth assessment reflects the growing evidence for a changing climate caused by human activity.

The IPCC has adopted and published "Principles Governing IPCC Work", which states that the IPCC will assess:
This document also states that IPCC will do this work by assessing "on a comprehensive, objective, open and transparent basis the scientific, technical and socio-economic information relevant to understanding the scientific basis" of these topics. The Principles also state that "IPCC reports should be neutral with respect to policy, although they may need to deal objectively with scientific, technical and socio-economic factors relevant to the application of particular policies."

Korean economist Hoesung Lee has been the chair of the IPCC since 8 October 2015, with the election of the new IPCC Bureau.
Before this election, the IPCC was led by Vice-Chair Ismail El Gizouli, who was designated acting Chair after the resignation of Rajendra K. Pachauri in February 2015. The previous chairs were Rajendra K. Pachauri, elected in May 2002; Robert Watson in 1997; and Bert Bolin in 1988. The chair is assisted by an elected bureau including vice-chairs and working group co-chairs, and by a secretariat.

The Panel itself is composed of representatives appointed by governments. Participation of delegates with appropriate expertise is encouraged. Plenary sessions of the IPCC and IPCC Working Groups are held at the level of government representatives. Non-Governmental and Intergovernmental Organizations admitted as observer organizations may also attend. Sessions of the Panel, IPCC Bureau, workshops, expert and lead authors meetings are by invitation only. About 500 people from 130 countries attended the 48th Session of the Panel in Incheon, Republic of Korea, in October 2018, including 290 government officials and 60 representatives of observer organizations. The opening ceremonies of sessions of the Panel and of Lead Author Meetings are open to media, but otherwise IPCC meetings are closed.

There are several major groups:

The IPCC receives funding through the IPCC Trust Fund, established in 1989 by the United Nations Environment Programme (UNEP) and the World Meteorological Organization (WMO), Costs of the Secretary and of housing the secretariat are provided by the WMO, while UNEP meets the cost of the Depute Secretary. Annual cash contributions to the Trust Fund are made by the WMO, by UNEP, and by IPCC Members. Payments and their size are voluntary. The Panel is responsible for considering and adopting by consensus the annual budget. The organization is required to comply with the Financial Regulations and Rules of the WMO.

The IPCC has published five comprehensive assessment reports reviewing the latest climate science, as well as a number of special reports on particular topics. These reports are prepared by teams of relevant researchers selected by the Bureau from government nominations. Expert reviewers from a wide range of governments, IPCC observer organizations and other organizations are invited at different
stages to comment on various aspects of the drafts.

The IPCC published its First Assessment Report (FAR) in 1990, a supplementary report in 1992, a Second Assessment Report (SAR) in 1995, a Third Assessment Report (TAR) in 2001, a Fourth Assessment Report (AR4) in 2007 and a Fifth Assessment Report (AR5) in 2014. The IPCC is currently preparing its Sixth Assessment Report (AR6), which will be completed in 2022.

Each assessment report is in three volumes, corresponding to Working Groups I, II, and III. It is completed by a synthesis report that integrates the working group contributions and any special reports produced in that assessment cycle.

The IPCC does not carry out research nor does it monitor climate related data. Lead authors of IPCC reports assess the available information about climate change based on published sources. According to IPCC guidelines, authors should give priority to peer-reviewed sources. Authors may refer to non-peer-reviewed sources (the "grey literature"), provided that they are of sufficient quality. Examples of non-peer-reviewed sources include model results, reports from government agencies and non-governmental organizations, and industry journals. Each subsequent IPCC report notes areas where the science has improved since the previous report and also notes areas where further research is required.

There are generally three stages in the review process:
Review comments are in an open archive for at least five years.

There are several types of endorsement which documents receive:

The Panel is responsible for the IPCC and its endorsement of Reports allows it to ensure they meet IPCC standards.

There have been a range of commentaries on the IPCC's procedures, examples of which are discussed later in the article (see also IPCC Summary for Policymakers). Some of these comments have been supportive, while others have been critical. Some commentators have suggested changes to the IPCC's procedures.

Each chapter has a number of authors who are responsible for writing and editing the material. A chapter typically has two "coordinating lead authors", ten to fifteen "lead authors", and a somewhat larger number of "contributing authors". The coordinating lead authors are responsible for assembling the contributions of the other authors, ensuring that they meet stylistic and formatting requirements, and reporting to the Working Group chairs. Lead authors are responsible for writing sections of chapters. Contributing authors prepare text, graphs or data for inclusion by the lead authors.

Authors for the IPCC reports are chosen from a list of researchers prepared by governments and participating organisations, and by the Working Group/Task Force Bureaux, as well as other experts known through their published work. The choice of authors aims for a range of views, expertise and geographical representation, ensuring representation of experts from developing and developed countries and countries with economies in transition.

The IPCC First Assessment Report (FAR) was completed in 1990, and served as the basis of the UNFCCC.

The executive summary of the WG I Summary for Policymakers report says they are certain that emissions resulting from human activities are substantially increasing the atmospheric concentrations of the greenhouse gases, resulting on average in an additional warming of the Earth's surface. They calculate with confidence that CO has been responsible for over half the enhanced greenhouse effect. They predict that under a "business as usual" (BAU) scenario, global mean temperature will increase by about 0.3 °C per decade during the [21st] century. They judge that global mean surface air temperature has increased by 0.3 to 0.6 °C over the last 100 years, broadly consistent with prediction of climate models, but also of the same magnitude as natural climate variability. The unequivocal detection of the enhanced greenhouse effect is not likely for a decade or more.

The 1992 supplementary report was an update, requested in the context of the negotiations on the UNFCCC at the Earth Summit (United Nations Conference on Environment and Development) in Rio de Janeiro in 1992.

The major conclusion was that research since 1990 did "not affect our fundamental understanding of the science of the greenhouse effect and either confirm or do not justify alteration of the major conclusions of the first IPCC scientific assessment". It noted that transient (time-dependent) simulations, which had been very preliminary in the FAR, were now improved, but did not include aerosol or ozone changes.

"Climate Change 1995", the IPCC Second Assessment Report (SAR), was finished in 1996. It is split into four parts:

Each of the last three parts was completed by a separate Working Group (WG), and each has a Summary for Policymakers (SPM) that represents a consensus of national representatives. The SPM of the WG I report contains headings:


The Third Assessment Report (TAR) was completed in 2001 and consists of four reports, three of them from its Working Groups:

A number of the TAR's conclusions are given quantitative estimates of how probable it is that they are correct, e.g., greater than 66% probability of being correct. These are "Bayesian" probabilities, which are based on an expert assessment of all the available evidence.

"Robust findings" of the TAR Synthesis Report include:

In 2001, 16 national science academies issued a joint statement on climate change.
The joint statement was made by the Australian Academy of Science, the Royal Flemish Academy of Belgium for Science and the Arts, the Brazilian Academy of Sciences, the Royal Society of Canada, the Caribbean Academy of Sciences, the Chinese Academy of Sciences, the French Academy of Sciences, the German Academy of Natural Scientists Leopoldina, the Indian National Science Academy, the Indonesian Academy of Sciences, the Royal Irish Academy, Accademia Nazionale dei Lincei (Italy), the Academy of Sciences Malaysia, the Academy Council of the Royal Society of New Zealand, the Royal Swedish Academy of Sciences, and the Royal Society (UK).
The statement, also published as an editorial in the journal Science, stated "we support the [TAR's] conclusion that it is at least 90% certain that temperatures will continue to rise, with average global surface temperature projected to increase by between 1.4 and 5.8 °C above 1990 levels by 2100".
The TAR has also been endorsed by the Canadian Foundation for Climate and Atmospheric Sciences, Canadian Meteorological and Oceanographic Society, and European Geosciences Union (refer to "Endorsements of the IPCC").

In 2001, the US National Research Council (US NRC) produced a report that assessed Working Group I's (WGI) contribution to the TAR. US NRC (2001) "generally agrees" with the WGI assessment, and describes the full WGI report as an "admirable summary of research activities in climate science".

IPCC author Richard Lindzen has made a number of criticisms of the TAR. Among his criticisms, Lindzen has stated that the WGI Summary for Policymakers (SPM) does not faithfully summarize the full WGI report. For example, Lindzen states that the SPM understates the uncertainty associated with climate models. John Houghton, who was a co-chair of TAR WGI, has responded to Lindzen's criticisms of the SPM. Houghton has stressed that the SPM is agreed upon by delegates from many of the world's governments, and that any changes to the SPM must be supported by scientific evidence.

IPCC author Kevin Trenberth has also commented on the WGI SPM. Trenberth has stated that during the drafting of the WGI SPM, some government delegations attempted to "blunt, and perhaps obfuscate, the messages in the report". However, Trenberth concludes that the SPM is a "reasonably balanced summary".

US NRC (2001) concluded that the WGI SPM and Technical Summary are "consistent" with the full WGI report. US NRC (2001) stated:
[...] the full [WGI] report is adequately summarized in the Technical Summary. The full WGI report and its Technical Summary are not specifically directed at policy. The Summary for Policymakers reflects less emphasis on communicating the basis for uncertainty and a stronger emphasis on areas of major concern associated with human-induced climate change. This change in emphasis appears to be the result of a summary process in which scientists work with policy makers on the document. Written responses from U.S. coordinating and lead scientific authors to the committee indicate, however, that (a) no changes were made without the consent of the convening lead authors (this group represents a fraction of the lead and contributing authors) and (b) most changes that did occur lacked significant impact.

The Fourth Assessment Report (AR4) was published in 2007. Like previous assessment reports, it consists of four reports:
People from over 130 countries contributed to the IPCC Fourth Assessment Report, which took 6 years to produce. Contributors to AR4 included more than 2500 scientific expert reviewers, more than 800 contributing authors, and more than 450 lead authors.

"Robust findings" of the Synthesis report include:



Global warming projections from AR4 are shown below. The projections apply to the end of the 21st century (2090–99), relative to temperatures at the end of the 20th century (1980–99). Add 0.7 °C to projections to make them relative to pre-industrial levels instead of 1980–99. (UK Royal Society, 2010, p=10. Descriptions of the greenhouse gas emissions scenarios can be found in Special Report on Emissions Scenarios.

"Likely" means greater than 66% probability of being correct, based on expert judgement.

Several science academies have referred to and/or reiterated some of the conclusions of AR4. These include:

The Netherlands Environmental Assessment Agency (PBL, "et al.", 2009; 2010) has carried out two reviews of AR4. These reviews are generally supportive of AR4's conclusions. PBL (2010) make some recommendations to improve the IPCC process. A literature assessment by the US National Research Council (US NRC, 2010) concludes:Climate change is occurring, is caused largely by human activities, and poses significant risks for—and in many cases is already affecting—a broad range of human and natural systems ["emphasis in original text"]. [...] This conclusion is based on a substantial array of scientific evidence, including recent work, and is consistent with the conclusions of recent assessments by the U.S. Global Change Research Program [...], the Intergovernmental Panel on Climate Change’s Fourth Assessment Report [...], and other assessments of the state of scientific knowledge on climate change.

Some errors have been found in the IPCC AR4 Working Group II report. Two errors include the melting of Himalayan glaciers (see later section), and Dutch land area that is below sea level.

The IPCC's Fifth Assessment Report (AR5) was completed in 2014. AR5 followed the same general format as of AR4, with three Working Group reports and a Synthesis report. The Working Group I report (WG1) was published in September 2013.

Conclusions of AR5 are summarized below:

Projections in AR5 are based on "Representative Concentration Pathways" (RCPs). The RCPs are consistent with a wide range of possible changes in future anthropogenic greenhouse gas emissions. Projected changes in global mean surface temperature and sea level are given in the main RCP article.

The sixth assessment report is scheduled to be released in the first half of 2022. Its three working groups are titled The Physical Science Basis; Impacts, Adaptation and Vulnerability and Mitigation of Climate Change. They are all currently scheduled for publication in the second half of 2021.

In addition to climate assessment reports, the IPCC publishes Special Reports on specific topics. The preparation and approval process for all IPCC Special Reports follows the same procedures as for IPCC Assessment Reports. In the year 2011 two IPCC Special Report were finalized, the Special Report on Renewable Energy Sources and Climate Change Mitigation (SRREN) and the Special Report on Managing Risks of Extreme Events and Disasters to Advance Climate Change Adaptation (SREX). Both Special Reports were requested by governments.

The Special Report on Emissions Scenarios (SRES) is a report by the IPCC which was published in 2000. The SRES contains "scenarios" of future changes in emissions of greenhouse gases and sulfur dioxide. One of the uses of the SRES scenarios is to project future changes in climate, e.g., changes in global mean temperature. The SRES scenarios were used in the IPCC's Third and Fourth Assessment Reports.

The SRES scenarios are "baseline" (or "reference") scenarios, which means that they do not take into account any current or future measures to limit greenhouse gas (GHG) emissions (e.g., the Kyoto Protocol to the United Nations Framework Convention on Climate Change). SRES emissions projections are broadly comparable in range to the baseline projections that have been developed by the scientific community.

There have been a number of comments on the SRES. Parson "et al." (2007) stated that the SRES represented "a substantial advance from prior scenarios". At the same time, there have been criticisms of the SRES.

The most prominently publicized criticism of SRES focused on the fact that all but one of the participating models compared gross domestic product (GDP) across regions using market exchange rates (MER), instead of the more correct purchasing-power parity (PPP) approach. This criticism is discussed in the main SRES article.

This report assesses existing literature on renewable energy commercialisation for the mitigation of climate change. It was published in 2012 and covers the six most important renewable energy technologies in a transition, as well as their integration into present and future energy systems. It also takes into consideration the environmental and social consequences associated with these technologies, the cost and strategies to overcome technical as well as non-technical obstacles to their application and diffusion. The full report in PDF form is found here

More than 130 authors from all over the world contributed to the preparation of IPCC Special Report on Renewable Energy Sources and Climate Change Mitigation (SRREN) on a voluntary basis – not to mention more than 100 scientists, who served as contributing authors.

The report was published in 2012. It assesses the effect that climate change has on the threat of natural disasters and how nations can better manage an expected change in the frequency of occurrence and intensity of severe weather patterns. It aims to become a resource for decision-makers to prepare more effectively for managing the risks of these events. A potentially important area for consideration is also the detection of trends in extreme events and the attribution of these trends to human influence. The full report, 594 pages in length, may be found here in PDF form.

More than 80 authors, 19 review editors, and more than 100 contributing authors from all over the world contributed to the preparation of SREX.

When the Paris Agreement was adopted, the UNFCCC invited the Intergovernmental Panel on Climate Change to write a special report on "How can humanity prevent the global temperature rise more than 1.5 degrees above pre-industrial level". The completed report, Special Report on Global Warming of 1.5 °C (SR15), was released on 8 October 2018. Its full title is "Global Warming of 1.5 °C, an IPCC special report on the impacts of global warming of 1.5 °C above pre-industrial levels and related global greenhouse gas emission pathways, in the context of strengthening the global response to the threat of climate change, sustainable development, and efforts to eradicate poverty".

The finished report summarizes the findings of scientists, showing that maintaining a temperature rise to below 1.5 °C remains possible, but only through "rapid and far-reaching transitions in energy, land, urban and infrastructure..., and industrial systems". Meeting the Paris target of is possible but would require "deep emissions reductions", "rapid", "far-reaching and unprecedented changes in all aspects of society". In order to achieve the 1.5 °C target, CO2 emissions must decline by 45% (relative to 2010 levels) by 2030, reaching net zero by around 2050. Deep reductions in non-CO2 emissions (such as nitrous oxide and methane) will also be required to limit warming to 1.5 °C. Under the pledges of the countries entering the Paris Accord, a sharp rise of 3.1 to 3.7 °C is still expected to occur by 2100. Holding this rise to 1.5 °C avoids the worst effects of a rise by even 2 °C. However, a warming of even 1.5 degrees will still result in large-scale drought, famine, heat stress, species die-off, loss of entire ecosystems, and loss of habitable land, throwing more than 100 Million into poverty. Effects will be most drastic in arid regions including the Middle East and the Sahel in Africa, where fresh water will remain in some areas following a 1.5 °C rise in temperatures but are expected to dry up completely if the rise reaches 2 °C.

The final draft of the "Special Report on climate change and land" (SRCCL)—with the full title, "Special Report on climate change, desertification, land degradation, sustainable land management, food security, and greenhouse gas fluxes in terrestrial ecosystems" was published online on 7 August 2019. The SRCCL consists of seven chapters, Chapter 1: Framing and Context, Chapter 2: Land-Climate Interactions, Chapter 3: Desertification, Chapter 4: Land Degradation, Chapter 5: Food Security, Chapter 5 Supplementary Material, Chapter 6: Interlinkages between desertification, land degradation, food security and GHG fluxes: Synergies, trade-offs and Integrated Response Options, and Chapter 7: Risk management and decision making in relation to sustainable development.

The "Special Report on the Ocean and Cryosphere in a Changing Climate" (SROCC) was approved on 25 September 2019 in Monaco. Among other findings, the report concluded that sea level rises could be up to two feet higher by the year 2100, even if efforts to reduce greenhouse gas emissions and to limit global warming are successful; coastal cities across the world could see so-called "storm[s] of the century" at least once a year.

Within IPCC the National Greenhouse Gas Inventory Program develops methodologies to estimate emissions of greenhouse gases. This has been undertaken since 1991 by the IPCC WGI in close collaboration with the Organisation for Economic Co-operation and Development and the International Energy Agency.
The objectives of the National Greenhouse Gas Inventory Program are:

The 1996 Guidelines for National Greenhouse Gas Investories provide the methodological basis for the estimation of national greenhouse gas emissions inventories. Over time these guidelines have been completed with good practice reports: "Good Practice Guidance and Uncertainty Management in National Greenhouse Gas Inventories" and "Good Practice Guidance for Land Use, Land-Use Change and Forestry".

The 1996 guidelines and the two good practice reports are to be used by parties to the UNFCCC and to the Kyoto Protocol in their annual submissions of national greenhouse gas inventories.

The 2006 "IPCC Guidelines for National Greenhouse Gas Inventories" is the latest version of these emission estimation methodologies, including a large number of default emission factors. Although the IPCC prepared this new version of the guidelines on request of the parties to the UNFCCC, the methods have not yet been officially accepted for use in national greenhouse gas emissions reporting under the UNFCCC and the Kyoto Protocol.

The IPCC concentrates its activities on the tasks allotted to it by the relevant WMO Executive Council and UNEP Governing Council resolutions and decisions as well as on actions in support of the UNFCCC process. While the preparation of the assessment reports is a major IPCC function, it also supports other activities, such as the Data Distribution Centre and the National Greenhouse Gas Inventories Programme, required under the UNFCCC. This involves publishing default emission factors, which are factors used to derive emissions estimates based on the levels of fuel consumption, industrial production and so on.

The IPCC also often answers inquiries from the UNFCCC Subsidiary Body for Scientific and Technological Advice (SBSTA).

In December 2007, the IPCC was awarded the Nobel Peace Prize "for their efforts to build up and disseminate greater knowledge about man-made climate change, and to lay the foundations for the measures that are needed to counteract such change". The award is shared with Former U.S. Vice-President Al Gore for his work on climate change and the documentary "An Inconvenient Truth".

There is widespread support for the IPCC in the scientific community, which is reflected in publications by other scientific bodies and experts. However, criticisms of the IPCC have been made.

Since 2010 the IPCC has come under yet unparalleled public and political scrutiny. The global IPCC consensus approach has been challenged internally and externally, for example, during the 2009 Climatic Research Unit email controversy ("Climategate"). It is contested by some as an information monopoly with results for both the quality and the impact of the IPCC work as such.

A paragraph in the 2007 Working Group II report ("Impacts, Adaptation and Vulnerability"), chapter 10 included a projection that Himalayan glaciers could disappear by 2035

This projection was not included in the final summary for policymakers. The IPCC has since acknowledged that the date is incorrect, while reaffirming that the conclusion in the final summary was robust. They expressed regret for "the poor application of well-established IPCC procedures in this instance". The date of 2035 has been correctly quoted by the IPCC from the WWF report, which has misquoted its own source, an ICSI report "Variations of Snow and Ice in the past and at present on a Global and Regional Scale".

Rajendra K. Pachauri responded in an interview with "Science".

Former IPCC chairman Robert Watson said, regarding the Himalayan glaciers estimation, "The mistakes all appear to have gone in the direction of making it seem like climate change is more serious by overstating the impact. That is worrying. The IPCC needs to look at this trend in the errors and ask why it happened". Martin Parry, a climate expert who had been co-chair of the IPCC working group II, said that "What began with a single unfortunate error over Himalayan glaciers has become a clamour without substance" and the IPCC had investigated the other alleged mistakes, which were "generally unfounded and also marginal to the assessment".

The third assessment report (TAR) prominently featured a graph labeled "Millennial Northern Hemisphere temperature reconstruction" based on a 1999 paper by Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes (MBH99), which has been referred to as the "hockey stick graph". This graph extended the similar graph in from the IPCC Second Assessment Report of 1995, and differed from a schematic in the first assessment report that lacked temperature units, but appeared to depict larger global temperature variations over the past 1000 years, and higher temperatures during the Medieval Warm Period than the mid 20th century. The schematic was not an actual plot of data, and was based on a diagram of temperatures in central England, with temperatures increased on the basis of documentary evidence of Medieval vineyards in England. Even with this increase, the maximum it showed for the Medieval Warm Period did not reach temperatures recorded in central England in 2007. The MBH99 finding was supported by cited reconstructions by , , and , using differing data and methods. The Jones et al. and Briffa reconstructions were overlaid with the MBH99 reconstruction in Figure 2.21 of the IPCC report.

These studies were widely presented as demonstrating that the current warming period is exceptional in comparison to temperatures between 1000 and 1900, and the MBH99 based graph featured in publicity. Even at the draft stage, this finding was disputed by contrarians: in May 2000 Fred Singer's Science and Environmental Policy Project held a press event on Capitol Hill, Washington, D.C., featuring comments on the graph Wibjörn Karlén and Singer argued against the graph at a United States Senate Committee on Commerce, Science and Transportation hearing on 18 July 2000. Contrarian John Lawrence Daly featured a modified version of the IPCC 1990 schematic, which he mis-identified as appearing in the IPCC 1995 report, and argued that "Overturning its own previous view in the 1995 report, the IPCC presented the 'Hockey Stick' as the new orthodoxy with hardly an apology or explanation for the abrupt U-turn since its 1995 report". Criticism of the MBH99 reconstruction in a review paper, which was quickly discredited in the Soon and Baliunas controversy, was picked up by the Bush administration, and a Senate speech by US Republican senator James Inhofe alleged that "manmade global warming is the greatest hoax ever perpetrated on the American people". The data and methodology used to produce the "hockey stick graph" was criticized in papers by Stephen McIntyre and Ross McKitrick, and in turn the criticisms in these papers were examined by other studies and comprehensively refuted by , which showed errors in the methods used by McIntyre and McKitrick.

On 23 June 2005, Rep. Joe Barton, chairman of the House Committee on Energy and Commerce wrote joint letters with Ed Whitfield, chairman of the Subcommittee on Oversight and Investigations demanding full records on climate research, as well as personal information about their finances and careers, from Mann, Bradley and Hughes. Sherwood Boehlert, chairman of the House Science Committee, said this was a "misguided and illegitimate investigation" apparently aimed at intimidating scientists, and at his request the U.S. National Academy of Sciences arranged for its National Research Council to set up a special investigation. The National Research Council's report agreed that there were some statistical failings, but these had little effect on the graph, which was generally correct. In a 2006 letter to "Nature", Mann, Bradley, and Hughes pointed out that their original article had said that "more widespread high-resolution data are needed before more confident conclusions can be reached" and that the uncertainties were "the point of the article".

The IPCC Fourth Assessment Report (AR4) published in 2007 featured a graph showing 12 proxy based temperature reconstructions, including the three highlighted in the 2001 Third Assessment Report (TAR); as before, and had both been calibrated by newer studies. In addition, analysis of the Medieval Warm Period cited reconstructions by (as cited in the TAR) and . Ten of these 14 reconstructions covered 1,000 years or longer. Most reconstructions shared some data series, particularly tree ring data, but newer reconstructions used additional data and covered a wider area, using a variety of statistical methods. The section discussed the divergence problem affecting certain tree ring data.

Some critics have contended that the IPCC reports tend to be conservative by consistently underestimating the pace and impacts of global warming, and report only the "lowest common denominator" findings.

On the eve of the publication of IPCC's Fourth Assessment Report in 2007 another study was published suggesting that temperatures and sea levels have been rising at or above the maximum rates proposed during IPCC's 2001 Third Assessment Report. The study compared IPCC 2001 projections on temperature and sea level change with observations. Over the six years studied, the actual temperature rise was near the top end of the range given by IPCC's 2001 projection, and the actual sea level rise was above the top of the range of the IPCC projection.

Another example of scientific research which suggests that previous estimates by the IPCC, far from overstating dangers and risks, have actually understated them is a study on projected rises in sea levels. When the researchers' analysis was "applied to the possible scenarios outlined by the Intergovernmental Panel on Climate Change (IPCC), the researchers found that in 2100 sea levels would be 0.5–1.4 m [50–140 cm] above 1990 levels. These values are much greater than the 9–88 cm as projected by the IPCC itself in its Third Assessment Report, published in 2001". This may have been due, in part, to the expanding human understanding of climate.

Greg Holland from the National Center for Atmospheric Research, who reviewed a multi-meter sea level rise study by Jim Hansen, noted “"There is no doubt that the sea level rise, within the IPCC, is a very conservative number, so the truth lies somewhere between IPCC and Jim."”

In reporting criticism by some scientists that IPCC's then-impending January 2007 report understates certain risks, particularly sea level rises, an AP story quoted Stefan Rahmstorf, professor of physics and oceanography at Potsdam University as saying "In a way, it is one of the strengths of the IPCC to be very conservative and cautious and not overstate any climate change risk".

In his December 2006 book, "", and in an interview on Fox News on 31 January 2007, energy expert Joseph Romm noted that the IPCC Fourth Assessment Report is already out of date and omits recent observations and factors contributing to global warming, such as the release of greenhouse gases from thawing tundra.

Political influence on the IPCC has been documented by the release of a memo by ExxonMobil to the Bush administration, and its effects on the IPCC's leadership. The memo led to strong Bush administration lobbying, evidently at the behest of ExxonMobil, to oust Robert Watson, a climate scientist, from the IPCC chairmanship, and to have him replaced by Pachauri, who was seen at the time as more mild-mannered and industry-friendly.

Michael Oppenheimer, a long-time participant in the IPCC and coordinating lead author of the Fifth Assessment Report conceded in "Science Magazine's State of the Planet 2008–2009" some limitations of the IPCC consensus approach and asks for concurring, smaller assessments of special problems instead of the large scale approach as in the previous IPCC assessment reports. It has become more important to provide a broader exploration of uncertainties. Others see as well mixed blessings of the drive for consensus within the IPCC process and ask to include dissenting or minority positions or to improve statements about uncertainties.

The IPCC process on climate change and its efficiency and success has been compared with dealings with other environmental challenges (compare Ozone depletion and global warming). In case of the Ozone depletion, global regulation based on the Montreal Protocol has been successful. In case of Climate Change, the Kyoto Protocol failed. The Ozone case was used to assess the efficiency of the IPCC process.
The lockstep situation of the IPCC is having built a broad science consensus while states and governments still follow different, if not opposing goals. The underlying linear model of policy-making of "the more knowledge we have, the better the political response will be" is being doubted.

According to Sheldon Ungar's comparison with global warming, the actors in the ozone depletion case had a better understanding of scientific ignorance and uncertainties. The ozone case communicated to lay persons "with easy-to-understand bridging metaphors derived from the popular culture" and related to "immediate risks with everyday relevance", while the public opinion on climate change sees no imminent danger. The stepwise mitigation of the ozone layer challenge was based as well on successfully reducing regional burden sharing conflicts. In case of the IPCC conclusions and the failure of the Kyoto Protocol, varying regional cost-benefit analysis and burden-sharing conflicts with regard to the distribution of emission reductions remain an unsolved problem. In the UK, a report for a House of Lords committee asked to urge the IPCC to involve better assessments of costs and benefits of climate change, but the Stern Review, ordered by the UK government, made a stronger argument in favor to combat human-made climate change.

Since the IPCC does not carry out its own research, it operates on the basis of scientific papers and independently documented results from other scientific bodies, and its schedule for producing reports requires a deadline for submissions prior to the report's final release. In principle, this means that any significant new evidence or events that change our understanding of climate science between this deadline and publication of an IPCC report cannot be included. In an area of science where our scientific understanding is rapidly changing, this has been raised as a serious shortcoming in a body which is widely regarded as the ultimate authority on the science. However, there has generally been a steady evolution of key findings and levels of scientific confidence from one assessment report to the next.

The submission deadlines for the Fourth Assessment Report (AR4) differed for the reports of each Working Group. Deadlines for the Working Group I report were adjusted during the drafting and review process in order to ensure that reviewers had access to unpublished material being cited by the authors. The final deadline for cited publications was 24 July 2006. The final WG I report was released on 30 April 2007 and the final AR4 Synthesis Report was released on 17 November 2007.Rajendra Pachauri, the IPCC chair, admitted at the launch of this report that since the IPCC began work on it, scientists have recorded "much stronger trends in climate change", like the unforeseen dramatic melting of polar ice in the summer of 2007, and added, "that means you better start with intervention much earlier".

Scientists who participate in the IPCC assessment process do so without any compensation other than the normal salaries they receive from their home institutions. The process is labor-intensive, diverting time and resources from participating scientists' research programs. Concerns have been raised that the large uncompensated time commitment and disruption to their own research may discourage qualified scientists from participating.

In May 2010, Pachauri noted that the IPCC currently had no process for responding to errors or flaws once it issued a report. The problem, according to Pachauri, was that once a report was issued the panels of scientists producing the reports were disbanded.

In February 2010, in response to controversies regarding claims in the Fourth Assessment Report, five climate scientists – all contributing or lead IPCC report authors – wrote in the journal "Nature" calling for changes to the IPCC. They suggested a range of new organizational options, from tightening the selection of lead authors and contributors, to dumping it in favor of a small permanent body, or even turning the whole climate science assessment process into a moderated "living" Wikipedia-IPCC. Other recommendations included that the panel employ a full-time staff and remove government oversight from its processes to avoid political interference.

The 2018 report "What Lies Beneath" by the Breakthrough - National Centre for Climate Restoration, with contributions from Kevin Anderson, James Hansen, Michael E. Mann, Michael Oppenheimer, Naomi Oreskes, Stefan Rahmstorf, Eric Rignot, Hans Joachim Schellnhuber, Kevin Trenberth, and others, urges the IPCC, the wider UNFCCC negotiations, and national policy makers to change their approach. The authors note, "We urgently require a reframing of scientific research within an existential risk-management framework."

In March 2010, at the invitation of the United Nations secretary-general and the chair of the IPCC, the InterAcademy Council (IAC) was asked to review the IPCC's processes for developing its reports. The IAC panel, chaired by Harold Tafler Shapiro, convened on 14 May 2010 and released its report on 1 September 2010.

The IAC found that, "The IPCC assessment process has been successful overall". The panel, however, made seven formal recommendations for improving the IPCC's assessment process, including:
The panel also advised that the IPCC avoid appearing to advocate specific policies in response to its scientific conclusions. Commenting on the IAC report, "Nature News" noted that "The proposals were met with a largely favourable response from climate researchers who are eager to move on after the media scandals and credibility challenges that have rocked the United Nations body during the past nine months".

Papers and electronic files of certain working groups of the IPCC, including reviews and comments on drafts of their Assessment Reports, are archived at the Environmental Science and Public Policy Archives in the Harvard Library.

Various scientific bodies have issued official statements endorsing and concurring with the findings of the IPCC.




























</doc>
<doc id="15031" url="https://en.wikipedia.org/wiki?curid=15031" title="IPCC (disambiguation)">
IPCC (disambiguation)

IPCC, or Intergovernmental Panel on Climate Change, is a scientific body under the auspices of the United Nations.

IPCC may also refer to:


</doc>
