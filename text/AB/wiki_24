<doc id="14790" url="https://en.wikipedia.org/wiki?curid=14790" title="Ice hockey">
Ice hockey

Ice hockey is a contact team sport played on ice, usually in a rink, in which two teams of skaters use their sticks to shoot a vulcanized rubber puck into their opponent's net to score goals. The sport is known to be fast-paced and physical, with teams usually fielding six players at a time: one goaltender, and five players who skate the span of the ice trying to control the puck and score goals against the opposing team.

Ice hockey is most popular in Canada, central and eastern Europe, the Nordic countries, Russia, and the United States. Ice hockey is the official national winter sport of Canada. In addition, ice hockey is the most popular winter sport in Belarus, Croatia, the Czech Republic, Finland, Latvia, Russia, Slovakia, Sweden, and Switzerland. North America's National Hockey League (NHL) is the highest level for men's ice hockey and the strongest professional ice hockey league in the world. The Kontinental Hockey League (KHL) is the highest league in Russia and much of Eastern Europe. The International Ice Hockey Federation (IIHF) is the formal governing body for international ice hockey, with the IIHF managing international tournaments and maintaining the IIHF World Ranking. Worldwide, there are ice hockey federations in 76 countries.

In Canada, the United States, Nordic countries, and some other European countries the sport is known simply as hockey; the name "ice hockey" is used in places where "hockey" more often refers to field hockey, such as countries in South America, Asia, Africa, Australasia, and some European countries including the United Kingdom, Ireland and the Netherlands.

Ice hockey is believed to have evolved from simple stick and ball games played in the 18th and 19th centuries in the United Kingdom and elsewhere. These games were brought to North America and several similar winter games using informal rules were developed, such as shinny and ice polo. The contemporary sport of ice hockey was developed in Canada, most notably in Montreal, where the first indoor hockey game was played on March 3, 1875. Some characteristics of that game, such as the length of the ice rink and the use of a puck, have been retained to this day. Amateur ice hockey leagues began in the 1880s, and professional ice hockey originated around 1900. The Stanley Cup, emblematic of ice hockey club supremacy, was first awarded in 1893 to recognize the Canadian amateur champion and later became the championship trophy of the NHL. In the early 1900s, the Canadian rules were adopted by the Ligue Internationale de Hockey Sur Glace, the precursor of the IIHF and the sport was played for the first time at the Olympics during the 1920 Summer Olympics.

In international competitions, the national teams of six countries (the Big Six) predominate: Canada, Czech Republic, Finland, Russia, Sweden and the United States. Of the 69 medals awarded all-time in men's competition at the Olympics, only seven medals were not awarded to one of those countries (or two of their precursors, the Soviet Union for Russia, and Czechoslovakia for the Czech Republic). In the annual Ice Hockey World Championships, 177 of 201 medals have been awarded to the six nations. Teams outside the Big Six have won only five medals in either competition since 1953. The World Cup of Hockey is organized by the National Hockey League and the National Hockey League Players' Association (NHLPA), unlike the annual World Championships and quadrennial Olympic tournament, both run by the International Ice Hockey Federation. World Cup games are played under NHL rules and not those of the IIHF, and the tournament occurs prior to the NHL pre-season, allowing for all NHL players to be available, unlike the World Championships, which overlaps with the NHL's Stanley Cup playoffs. Furthermore, all 12 Women's Olympic and 36 IIHF World Women's Championship medals were awarded to one of these six countries. The Canadian national team or the United States national team have between them won every gold medal of either series.
In England, field hockey has historically been called simply "hockey" and what was referenced by first appearances in print. The first known mention spelled as "hockey" occurred in the 1772 book "Juvenile Sports and Pastimes, to Which Are Prefixed, Memoirs of the Author: Including a New Mode of Infant Education", by Richard Johnson (Pseud. Master Michel Angelo), whose chapter XI was titled "New Improvements on the Game of Hockey". The 1527 Statute of Galway banned a sport called hokie'—the hurling of a little ball with sticks or staves". A form of this word was thus being used in the 16th century, though much removed from its current usage.

The belief that hockey was mentioned in a 1363 proclamation by King Edward III of England is based on modern translations of the proclamation, which was originally in Latin and explicitly forbade the games "Pilam Manualem, Pedivam, & Bacularem: & ad Canibucam & Gallorum Pugnam". The English historian and biographer John Strype did not use the word "hockey" when he translated the proclamation in 1720, instead translating "Canibucam" as "Cambuck"; this may have referred to either an early form of hockey or a game more similar to golf or croquet.

According to the Austin Hockey Association, the word "puck" derives from the Scottish Gaelic "puc" or the Irish "poc" (to poke, punch or deliver a blow). "...The blow given by a hurler to the ball with his camán or hurley is always called a puck."

Stick-and-ball games date back to pre-Christian times. In Europe, these games included the Irish game of hurling, the closely related Scottish game of shinty and versions of field hockey (including bandy ball, played in England). IJscolf, a game resembling colf on an ice-covered surface, was popular in the Low Countries between the Middle Ages and the Dutch Golden Age. It was played with a wooden curved bat (called a "colf" or "kolf"), a wooden or leather ball and two poles (or nearby landmarks), with the objective to hit the chosen point using the fewest strokes. A similar game ("knattleikr") had been played for a thousand years or more by the Scandinavian peoples, as documented in the Icelandic sagas. Polo has been referred to as "hockey on horseback". In England, field hockey developed in the late 17th century, and there is evidence that some games of field hockey took place on the ice. These games of "hockey on ice" were sometimes played with a bung (a plug of cork or oak used as a stopper on a barrel). William Pierre Le Cocq stated, in a 1799 letter written in Chesham, England:

I must now describe to you the game of Hockey; we have each a stick turning up at the end. We get a bung. There are two sides one of them knocks one way and the other side the other way. If any one of the sides makes the bung reach that end of the churchyard it is victorious.

A 1797 engraving unearthed by Swedish sport historians Carl Gidén and Patrick Houda shows a person on skates with a stick and bung on the River Thames, probably in December 1796.

British soldiers and immigrants to Canada and the United States brought their stick-and-ball games with them and played them on the ice and snow of winter. In 1825, John Franklin wrote "The game of hockey played on the ice was the morning sport" on Great Bear Lake during one of his Arctic expeditions. A mid-1830s watercolour portrays New Brunswick lieutenant-governor Archibald Campbell and his family with British soldiers on skates playing a stick-on-ice sport. Captain R.G.A. Levinge, a British Army officer in New Brunswick during Campbell's time, wrote about "hockey on ice" on Chippewa Creek (a tributary of the Niagara River) in 1839. In 1843 another British Army officer in Kingston, Ontario wrote, "Began to skate this year, improved quickly and had great fun at hockey on the ice." An 1859 "Boston Evening Gazette" article referred to an early game of hockey on ice in Halifax that year. An 1835 painting by John O'Toole depicts skaters with sticks and bung on a frozen stream in the American state of West Virginia, at that time still part of Virginia.

In the same era, the Mi'kmaq, a First Nations people of the Canadian Maritimes, also had a stick-and-ball game. Canadian oral histories describe a traditional stick-and-ball game played by the Mi'kmaq, and Silas Tertius Rand (in his 1894 "Legends of the Micmacs") describes a Mi'kmaq ball game known as "tooadijik". Rand also describes a game played (probably after European contact) with hurleys, known as "wolchamaadijik". Sticks made by the Mi'kmaq were used by the British for their games.

Early 19th-century paintings depict shinney (or "shinny"), an early form of hockey with no standard rules which was played in Nova Scotia. Many of these early games absorbed the physical aggression of what the Onondaga called "dehuntshigwa'es" (lacrosse). Shinney was played on the St. Lawrence River at Montreal and Quebec City, and in Kingston and Ottawa. The number of players was often large. To this day, shinney (derived from "shinty") is a popular Canadian term for an informal type of hockey, either ice or street hockey.

Thomas Chandler Haliburton, in "The Attache: Second Series" (published in 1844) imagined a dialogue, between two of the novel's characters, which mentions playing "hurly on the long pond on the ice". This has been interpreted by some historians from Windsor, Nova Scotia as reminiscent of the days when the author was a student at King's College School in that town in 1810 and earlier. Based on Haliburton's quote, claims were made that modern hockey was invented in Windsor, Nova Scotia, by King's College students and perhaps named after an individual ("Colonel Hockey's game"). Others claim that the origins of hockey come from games played in the area of Dartmouth and Halifax in Nova Scotia. However, several references have been found to hurling and shinty being played on the ice long before the earliest references from both Windsor and Dartmouth/Halifax, and the word "hockey" was used to designate a stick-and-ball game at least as far back as 1773, as it was mentioned in the book "Juvenile Sports and Pastimes, to Which Are Prefixed, Memoirs of the Author: Including a New Mode of Infant Education" by Richard Johnson (Pseud. Master Michel Angelo), whose chapter XI was titled "New Improvements on the Game of Hockey".

While the game's origins lie elsewhere, Montreal is at the centre of the development of the sport of contemporary ice hockey, and is recognized as the birthplace of organized ice hockey. On March 3, 1875, the first organized indoor game was played at Montreal's Victoria Skating Rink between two nine-player teams, including James Creighton and several McGill University students. Instead of a ball or bung, the game featured a "flat circular piece of wood" (to keep it in the rink and to protect spectators). The goal posts were apart (today's goals are six feet wide).

In 1876, games played in Montreal were "conducted under the 'Hockey Association' rules"; the Hockey Association was England's field hockey organization. In 1877, "The Gazette" (Montreal) published a list of seven rules, six of which were largely based on six of the Hockey Association's twelve rules, with only minor differences (even the word "ball" was kept); the one added rule explained how disputes should be settled. The McGill University Hockey Club, the first ice hockey club, was founded in 1877 (followed by the Quebec Hockey Club in 1878 and the Montreal Victorias in 1881). In 1880, the number of players per side was reduced from nine to seven.

The number of teams grew, enough to hold the first "world championship" of ice hockey at Montreal's annual Winter Carnival in 1883. The McGill team won the tournament and was awarded the Carnival Cup. The game was divided into thirty-minute halves. The positions were now named: left and right wing, centre, rover, point and cover-point, and goaltender. In 1886, the teams competing at the Winter Carnival organized the Amateur Hockey Association of Canada (AHAC), and played a season comprising "challenges" to the existing champion.

In Europe, it is believed that in 1885 the Oxford University Ice Hockey Club was formed to play the first Ice Hockey Varsity Match against traditional rival Cambridge in St. Moritz, Switzerland; however, this is undocumented. The match was won by the Oxford Dark Blues, 6–0; the first photographs and team lists date from 1895. This rivalry continues, claiming to be the oldest hockey rivalry in history; a similar claim is made about the rivalry between Queen's University at Kingston and Royal Military College of Kingston, Ontario. Since 1986, considered the 100th anniversary of the rivalry, teams of the two colleges play for the Carr-Harris Cup.

In 1888, the Governor General of Canada, Lord Stanley of Preston (whose sons and daughter were hockey enthusiasts), first attended the Montreal Winter Carnival tournament and was impressed with the game. In 1892, realizing that there was no recognition for the best team in Canada (although a number of leagues had championship trophies), he purchased a silver bowl for use as a trophy. The Dominion Hockey Challenge Cup (which later became known as the Stanley Cup) was first awarded in 1893 to the Montreal Hockey Club, champions of the AHAC; it continues to be awarded annually to the National Hockey League's championship team. Stanley's son Arthur helped organize the Ontario Hockey Association, and Stanley's daughter Isobel was one of the first women to play ice hockey.
By 1893, there were almost a hundred teams in Montreal alone; in addition, there were leagues throughout Canada. Winnipeg hockey players used cricket pads to better protect the goaltender's legs; they also introduced the "scoop" shot, or what is now known as the wrist shot. William Fairbrother, from Ontario, Canada is credited with inventing the ice hockey net in the 1890s. Goal nets became a standard feature of the Canadian Amateur Hockey League (CAHL) in 1900. Left and right defence began to replace the point and cover-point positions in the OHA in 1906.

In the United States, ice polo, played with a ball rather than a puck, was popular during this period; however, by 1893 Yale University and Johns Hopkins University held their first ice hockey matches. American financier Malcolm Greene Chace is credited with being the father of hockey in the United States. In 1892, as an amateur tennis player, Chace visited Niagara Falls, New York for a tennis match, where he met some Canadian hockey players. Soon afterwards, Chace put together a team of men from Yale, Brown, and Harvard, and toured across Canada as captain of this team. The first collegiate hockey match in the United States was played between Yale University and Johns Hopkins in Baltimore. Yale, led by captain Chace, beat Hopkins, 2–1. In 1896, the first ice hockey league in the US was formed. The US Amateur Hockey League was founded in New York City, shortly after the opening of the artificial-ice St. Nicholas Rink.

Lord Stanley's five sons were instrumental in bringing ice hockey to Europe, defeating a court team (which included the future Edward VII and George V) at Buckingham Palace in 1895. By 1903, a five-team league had been founded. The "Ligue Internationale de Hockey sur Glace" was founded in 1908 to govern international competition, and the first European championship was won by Great Britain in 1910. The sport grew further in Europe in the 1920s, after ice hockey became an Olympic sport. Many bandy players switched to hockey so as to be able to compete in the Olympics. Bandy remained popular in the Soviet Union, which only started its ice hockey program in the 1950s. In the mid-20th century, the "Ligue" became the International Ice Hockey Federation.

As the popularity of ice hockey as a spectator sport grew, earlier rinks were replaced by larger rinks. Most of the early indoor ice rinks have been demolished; Montreal's Victoria Rink, built in 1862, was demolished in 1925. Many older rinks succumbed to fire, such as Denman Arena, Dey's Arena, Quebec Skating Rink and Montreal Arena, a hazard of the buildings' wood construction. The Stannus Street Rink in Windsor, Nova Scotia (built in 1897) may be the oldest still in existence; however, it is no longer used for hockey. The Aberdeen Pavilion (built in 1898) in Ottawa was used for hockey in 1904 and is the oldest existing facility that has hosted Stanley Cup games.

The oldest indoor ice hockey arena still in use today for hockey is Boston's Matthews Arena, which was built in 1910. It has been modified extensively several times in its history and is used today by Northeastern University for hockey and other sports. It was the original home rink of the Boston Bruins professional team, itself the oldest United States-based team in the NHL, starting play in the league in today's Matthews Arena on December 1, 1924. Madison Square Garden in New York City, built in 1968, is the oldest continuously-operating arena in the NHL.

Professional hockey has existed since the early 20th century. By 1902, the Western Pennsylvania Hockey League was the first to employ professionals. The league joined with teams in Michigan and Ontario to form the first fully professional league—the International Professional Hockey League (IPHL)—in 1904. The WPHL and IPHL hired players from Canada; in response, Canadian leagues began to pay players (who played with amateurs). The IPHL, cut off from its largest source of players, disbanded in 1907. By then, several professional hockey leagues were operating in Canada (with leagues in Manitoba, Ontario and Quebec).

In 1910, the National Hockey Association (NHA) was formed in Montreal. The NHA would further refine the rules: dropping the rover position, dividing the game into three 20-minute periods and introducing minor and major penalties. After re-organizing as the National Hockey League in 1917, the league expanded into the United States, starting with the Boston Bruins in 1924.

Professional hockey leagues developed later in Europe, but amateur leagues leading to national championships were in place. One of the first was the Swiss National League A, founded in 1916. Today, professional leagues have been introduced in most countries of Europe. Top European leagues include the Kontinental Hockey League, the Czech Extraliga, the Finnish Liiga and the Swedish Hockey League.

While the general characteristics of the game stay the same wherever it is played, the exact rules depend on the particular code of play being used. The two most important codes are those of the IIHF and the NHL. Both of the codes, and others, originated from Canadian rules of ice hockey of the early 20th Century.

Ice hockey is played on a "hockey rink". During normal play, there are six players per side on the ice at any time, one of them being the goaltender, each of whom is on ice skates. The objective of the game is to score "goals" by shooting a hard vulcanized rubber disc, the "puck", into the opponent's goal net, which is placed at the opposite end of the rink. The players use their sticks to pass or shoot the puck.

Within certain restrictions, players may redirect the puck with any part of their body. Players may not hold the puck in their hand and are prohibited from using their hands to pass the puck to their teammates unless they are in the defensive zone. Players are also prohibited from kicking the puck into the opponent's goal, though unintentional redirections off the skate are permitted. Players may not intentionally bat the puck into the net with their hands.

Hockey is an off-side game, meaning that forward passes are allowed, unlike in rugby. Before the 1930s, hockey was an on-side game, meaning that only backward passes were allowed. Those rules favoured individual stick-handling as a key means of driving the puck forward. With the arrival of offside rules, the forward pass transformed hockey into a true team sport, where individual performance diminished in importance relative to team play, which could now be coordinated over the entire surface of the ice as opposed to merely rearward players.

The six players on each team are typically divided into three forwards, two defencemen, and a goaltender. The term "skaters" is typically used to describe all players who are not goaltenders. The "forward" positions consist of a "centre" and two "wingers": a "left wing" and a "right wing". Forwards often play together as units or "lines", with the same three forwards always playing together. The "defencemen" usually stay together as a pair generally divided between left and right. Left and right side wingers or defencemen are generally positioned as such, based on the side on which they carry their stick. A substitution of an entire unit at once is called a "line change". Teams typically employ alternate sets of forward lines and defensive pairings when "short-handed" or on a "power play". The goaltender stands in a, usually blue, semi-circle called the "crease" in the defensive zone keeping pucks from going in. Substitutions are permitted at any time during the game, although during a stoppage of play the home team is permitted the final change. When players are substituted during play, it is called changing "on the fly". A new NHL rule added in the 2005–06 season prevents a team from changing their line after they "ice" the puck.
The boards surrounding the ice help keep the puck in play and they can also be used as tools to play the puck. Players are permitted to bodycheck opponents into the boards as a means of stopping progress. The referees, linesmen and the outsides of the goal are "in play" and do not cause a stoppage of the game when the puck or players are influenced (by either bouncing or colliding) into them. Play can be stopped if the goal is knocked out of position. Play often proceeds for minutes without interruption. When play is stopped, it is restarted with a faceoff. Two players face each other and an official drops the puck to the ice, where the two players attempt to gain control of the puck. Markings (circles) on the ice indicate the locations for the faceoff and guide the positioning of players.

The three major rules of play in ice hockey that limit the movement of the puck: "offside", "icing", and the puck going out of play. A player is offside if he enters his opponent's zone before the puck itself. Under many situations, a player may not "ice the puck", shoot the puck all the way across both the centre line and the opponent's goal line. The puck goes out of play whenever it goes past the perimeter of the ice rink (onto the player benches, over the glass, or onto the protective netting above the glass) and a stoppage of play is called by the officials using whistles. It also does not matter if the puck comes back onto the ice surface from those areas as the puck is considered dead once it leaves the perimeter of the rink.

Under IIHF rules, each team may carry a maximum of 20 players and two goaltenders on their roster. NHL rules restrict the total number of players per game to 18, plus two goaltenders. In the NHL, the players are usually divided into four lines of three forwards, and into three pairs of defencemen. On occasion, teams may elect to substitute an extra defenceman for a forward. The seventh defenceman may play as a substitute defenceman, spend the game on the bench, or if a team chooses to play four lines then this seventh defenceman may see ice-time on the fourth line as a forward.

A professional game consists of three periods of twenty minutes, the clock running only when the puck is in play. The teams change ends after each period of play, including overtime. Recreational leagues and children's leagues often play shorter games, generally with three shorter periods of play.

Various procedures are used if a tie occurs. In tournament play, as well as in the NHL playoffs, North Americans favour "sudden death overtime", in which the teams continue to play twenty-minute periods until a goal is scored. Up until the 1999–2000 season regular season NHL games were settled with a single five-minute sudden death period with five players (plus a goalie) per side, with both teams awarded one point in the standings in the event of a tie. With a goal, the winning team would be awarded two points and the losing team none (just as if they had lost in regulation).

From the 1999–2000 until the 2003–04 seasons, the National Hockey League decided ties by playing a single five-minute sudden death overtime period with each team having four skaters per side (plus the goalie). In the event of a tie, each team would still receive one point in the standings but in the event of a victory the winning team would be awarded two points in the standings and the losing team one point. The idea was to discourage teams from playing for a tie, since previously some teams might have preferred a tie and 1 point to risking a loss and zero points. The only exception to this rule is if a team opts to pull their goalie in exchange for an extra skater during overtime and is subsequently scored upon (an "empty net" goal), in which case the losing team receives no points for the overtime loss. Since the 2015–16 season, the single five-minute sudden death overtime session involves three skaters on each side. Since three skaters must always be on the ice in an NHL game, the consequences of penalties are slightly different from those during regulation play. If a team is on a powerplay when overtime begins, that team will play with more than three skaters (usually four, very rarely five) until the expiration of the penalty. Any penalty during overtime that would result in a team losing a skater during regulation instead causes the non-penalized team to add a skater. Once the penalized team's penalty ends, the number of skaters on each side is adjusted accordingly, with the penalized team adding a skater in regulation and the non-penalized team subtracting a skater in overtime. This goes until the next stoppage of play.

International play and several North American professional leagues, including the NHL (in the regular season), now use an overtime period identical to that from 1999–2000 to 2003–04 followed by a penalty shootout. If the score remains tied after an extra overtime period, the subsequent shootout consists of three players from each team taking penalty shots. After these six total shots, the team with the most goals is awarded the victory. If the score is still tied, the shootout then proceeds to a "sudden death" format. Regardless of the number of goals scored during the shootout by either team, the final score recorded will award the winning team one more goal than the score at the end of regulation time. In the NHL if a game is decided in overtime or by a shootout the winning team is awarded two points in the standings and the losing team is awarded one point. Ties no longer occur in the NHL.

The overtime mode for the NHL playoffs differ from the regular season. In the playoffs there are no shootouts nor ties. If a game is tied after regulation an additional 20 minutes of 5 on 5 sudden death overtime will be added. In case of a tied game after the overtime, multiple 20-minute overtimes will be played until a team scores, which wins the match. Since 2019, the IIHF World Championships and the medal games in the Olympics use the same format, but in a 3-on-3 format.

In ice hockey, infractions of the rules lead to play stoppages whereby the play is restarted at a face off. Some infractions result in the imposition of a "penalty" to a player or team. In the simplest case, the offending player is sent to the "penalty box" and their team has to play with one less player on the ice for a designated amount of time. "Minor" penalties last for two minutes, "major" penalties last for five minutes, and a "double minor" penalty is two "consecutive" penalties of two minutes duration. A single minor penalty may be extended by a further two minutes for causing visible injury to the victimized player. This is usually when blood is drawn during high sticking. Players may be also assessed personal extended penalties or game expulsions for misconduct in addition to the penalty or penalties their team must serve. The team that has been given a penalty is said to be playing "short-handed" while the opposing team is on a "power play".

A two-minute minor penalty is often charged for lesser infractions such as tripping, elbowing, roughing, high-sticking, delay of the game, too many players on the ice, boarding, illegal equipment, charging (leaping into an opponent or body-checking him after taking more than two strides), holding, holding the stick (grabbing an opponent's stick), interference, hooking, slashing, kneeing, unsportsmanlike conduct (arguing a penalty call with referee, extremely vulgar or inappropriate verbal comments), "butt-ending" (striking an opponent with the knob of the stick—a very rare penalty), "spearing", or cross-checking. As of the 2005–2006 season, a minor penalty is also assessed for diving, where a player embellishes or simulates an offence. More egregious fouls may be penalized by a four-minute double-minor penalty, particularly those that injure the victimized player. These penalties end either when the time runs out or when the other team scores during the power play. In the case of a goal scored during the first two minutes of a double-minor, the penalty clock is set down to two minutes upon a score, effectively expiring the first minor penalty. 

A five-minute major penalties are called for especially violent instances of most minor infractions that result in intentional injury to an opponent, or when a minor penalty results in visible injury (such as bleeding), as well as for fighting. Major penalties are always served in full; they do not terminate on a goal scored by the other team. Major penalties assessed for fighting are typically offsetting, meaning neither team is short-handed and the players exit the penalty box upon a stoppage of play following the expiration of their respective penalties. The foul of boarding (defined as "check[ing] an opponent in such a manner that causes the opponent to be thrown violently in the boards") is penalized either by a minor or major penalty at the discretion of the referee, based on the violent state of the hit. A minor or major penalty for boarding is often assessed when a player checks an opponent from behind and into the boards.

Some varieties of penalties do not always require the offending team to play a man short. Concurrent five-minute major penalties in the NHL usually result from fighting. In the case of two players being assessed five-minute fighting majors, both the players serve five minutes without their team incurring a loss of player (both teams still have a full complement of players on the ice). This differs with two players from opposing sides getting minor penalties, at the same time or at any intersecting moment, resulting from more common infractions. In this case, both teams will have only four skating players (not counting the goaltender) until one or both penalties expire (if one penalty expires before the other, the opposing team gets a power play for the remainder of the time); this applies regardless of current pending penalties. However, in the NHL, a team always has at least three skaters on the ice. Thus, ten-minute "misconduct" penalties are served in full by the penalized player, but his team may immediately substitute another player on the ice "unless" a minor or major penalty is assessed in conjunction with the misconduct (a "two-and-ten" or "five-and-ten"). In this case, the team designates another player to serve the minor or major; both players go to the penalty box, but only the designee may not be replaced, and he is released upon the expiration of the two or five minutes, at which point the ten-minute misconduct begins. In addition, "game misconducts" are assessed for deliberate intent to inflict severe injury on an opponent (at the officials' discretion), or for a major penalty for a stick infraction or repeated major penalties. The offending player is ejected from the game and must immediately leave the playing surface (he does not sit in the penalty box); meanwhile, if an additional minor or major penalty is assessed, a designated player must serve out of that segment of the penalty in the box (similar to the above-mentioned "two-and-ten"). In some rare cases, a player may receive up to nineteen minutes in penalties for one string of plays. This could involve receiving a four-minute double minor penalty, getting in a fight with an opposing player who retaliates, and then receiving a game misconduct after the fight. In this case, the player is ejected and two teammates must serve the double-minor and major penalties.
A penalty shot is awarded to a player when the illegal actions of another player stop a clear scoring opportunity, most commonly when the player is on a breakaway. A penalty shot allows the obstructed player to pick up the puck on the centre red-line and attempt to score on the goalie with no other players on the ice, to compensate for the earlier missed scoring opportunity. A penalty shot is also awarded for a defender other than the goaltender covering the puck in the goal crease, a goaltender intentionally displacing his own goal posts during a breakaway to avoid a goal, a defender intentionally displacing his own goal posts when there is less than two minutes to play in regulation time or at any point during overtime, or a player or coach intentionally throwing a stick or other object at the puck or the puck carrier and the throwing action disrupts a shot or pass play.

Officials also stop play for puck movement violations, such as using one's hands to pass the puck in the offensive end, but no players are penalized for these offences. The sole exceptions are deliberately falling on or gathering the puck to the body, carrying the puck in the hand, and shooting the puck out of play in one's defensive zone (all penalized two minutes for delay of game).

In the NHL, a unique penalty applies to the goalies. The goalies now are forbidden to play the puck in the "corners" of the rink near their own net. This will result in a two-minute penalty against the goalie's team. Only in the area in-front of the goal line and immediately behind the net (marked by two red lines on either side of the net) the goalie can play the puck.

An additional rule that has never been a penalty, but was an infraction in the NHL before recent rules changes, is the two-line offside pass. Prior to the 2005–06 NHL season, play was stopped when a pass from inside a team's defending zone crossed the centre line, with a face-off held in the defending zone of the offending team. Now, the centre line is no longer used in the NHL to determine a two-line pass infraction, a change that the IIHF had adopted in 1998. Players are now able to pass to teammates who are more than the blue and centre ice red line away.

The NHL has taken steps to speed up the game of hockey and create a game of finesse, by retreating from the past when illegal hits, fights, and "clutching and grabbing" among players were commonplace. Rules are now more strictly enforced, resulting in more penalties, which in turn provides more protection to the players and facilitates more goals being scored. The governing body for United States' amateur hockey has implemented many new rules to reduce the number of stick-on-body occurrences, as well as other detrimental and illegal facets of the game ("zero tolerance").

In men's hockey, but not in women's, a player may use his hip or shoulder to hit another player if the player has the puck or is the last to have touched it. This use of the hip and shoulder is called "body checking". Not all physical contact is legal—in particular, hits from behind, hits to the head and most types of forceful stick-on-body contact are illegal.

A "delayed penalty call" occurs when a penalty offence is committed by the team that does not have possession of the puck. In this circumstance the team with possession of the puck is allowed to complete the play; that is, play continues until a goal is scored, a player on the opposing team gains control of the puck, or the team in possession commits an infraction or penalty of their own. Because the team on which the penalty was called cannot control the puck without stopping play, it is impossible for them to score a goal. In these cases, the team in possession of the puck can pull the goalie for an extra attacker without fear of being scored on. However, it is possible for the controlling team to mishandle the puck into their own net. If a delayed penalty is signalled and the team in possession scores, the penalty is still assessed to the offending player, but not served. In 2012, this rule was changed by the United States' National Collegiate Athletic Association (NCAA) for college level hockey. In college games, the penalty is still enforced even if the team in possession scores.

A typical game of hockey is governed by two to four "officials" on the ice, charged with enforcing the rules of the game. There are typically two "linesmen" who are mainly responsible for calling "offside" and "icing" violations, breaking up fights, and conducting faceoffs, and one or two "referees", who call goals and all other penalties. Linesmen can, however, report to the referee(s) that a penalty should be assessed against an offending player in some situations. The restrictions on this practice vary depending on the governing rules. On-ice officials are assisted by off-ice officials who act as goal judges, time keepers, and official scorers.

The most widespread system in use today is the "three-man system", that uses one referee and two linesmen. Another less commonly used system is the two referee and one linesman system. This system is very close to the regular three-man system except for a few procedure changes. With the first being the National Hockey League, a number of leagues have started to implement the "four-official system", where an additional referee is added to aid in the calling of penalties normally difficult to assess by one single referee. The system is now used in every NHL game since 2001, at IIHF World Championships, the Olympics and in many professional and high-level amateur leagues in North America and Europe.

Officials are selected by the league they work for. Amateur hockey leagues use guidelines established by national organizing bodies as a basis for choosing their officiating staffs. In North America, the national organizing bodies Hockey Canada and USA Hockey approve officials according to their experience level as well as their ability to pass rules knowledge and skating ability tests. Hockey Canada has officiating levels I through VI. USA Hockey has officiating levels 1 through 4.

Since men's ice hockey is a full contact sport, body checks are allowed so injuries are a common occurrence. Protective equipment is mandatory and is enforced in all competitive situations. This includes a helmet with either a visor or a full face mask, shoulder pads, elbow pads, mouth guard, protective gloves, heavily padded shorts (also known as hockey pants) or a girdle, athletic cup (also known as a jock, for males; and jill, for females), shin pads, skates, and (optionally) a neck protector.

Goaltenders use different equipment. With hockey pucks approaching them at speeds of up to 100 mph (160 km/h) they must wear equipment with more protection. Goaltenders wear specialized goalie skates (these skates are built more for movement side to side rather than forwards and backwards), a jock or jill, large leg pads (there are size restrictions in certain leagues), blocking glove, catching glove, a chest protector, a goalie mask, and a large jersey. Goaltenders' equipment has continually become larger and larger, leading to fewer goals in each game and many official rule changes.
Hockey skates are optimized for physical acceleration, speed and manoeuvrability. This includes rapid starts, stops, turns, and changes in skating direction. In addition, they must be rigid and tough to protect the skater's feet from contact with other skaters, sticks, pucks, the boards, and the ice itself. Rigidity also improves the overall manoeuvrability of the skate. Blade length, thickness (width), and curvature (rocker/radius (front to back) and radius of hollow (across the blade width) are quite different from speed or figure skates. Hockey players usually adjust these parameters based on their skill level, position, and body type. The blade width of most skates are about thick.
The hockey stick consists of a long, relatively wide, and slightly curved flat blade, attached to a shaft. The curve itself has a big impact on its performance. A deep curve allows for lifting the puck easier while a shallow curve allows for easier backhand shots. The flex of the stick also impacts the performance. Typically, a less flexible stick is meant for a stronger player since the player is looking for the right balanced flex that allows the stick to flex easily while still having a strong "whip-back" which sends the puck flying at high speeds. It is quite distinct from sticks in other sports games and most suited to hitting and controlling the flat puck. Its unique shape contributed to the early development of the game.

Ice hockey is a full contact sport and carries a high risk of injury. Players are moving at speeds around approximately and quite a bit of the game revolves around the physical contact between the players. Skate blades, hockey sticks, shoulder contact, hip contact, and hockey pucks can all potentially cause injuries. The types of injuries associated with hockey include: lacerations, concussions, contusions, ligament tears, broken bones, hyperextensions, and muscle strains. Women's ice hockey players are allowed to contact other players but are not allowed to body check.

Compared to athletes who play other sports, ice hockey players are at higher risk of overuse injuries and injuries caused by early sports specialization by teenagers.

According to the Hughston Health Alert, "Lacerations to the head, scalp, and face are the most frequent types of injury [in hockey]." Even a shallow cut to the head results in a loss of a large amount of blood. Direct trauma to the head is estimated to account for 80% of all hockey injuries as a result of player contact with other players or hockey equipment.

One of the leading causes of head injury is body checking from behind. Due to the danger of delivering a check from behind, many leagues, including the NHL have made this a major and game misconduct penalty (called "boarding"). Another type of check that accounts for many of the player-to-player contact concussions is a check to the head resulting in a misconduct penalty (called "head contact"). A check to the head can be defined as delivering a hit while the receiving player's head is down and their waist is bent and the aggressor is targeting the opponent player's head.

The most dangerous result of a head injury in hockey can be classified as a concussion. Most concussions occur during player-to-player contact rather than when a player is checked into the boards. Checks to the head have accounted for nearly 50% of concussions that players in the National Hockey League have suffered. In recent years, the NHL has implemented new rules which penalize and suspend players for illegal checks to the heads, as well as checks to unsuspecting players. Concussions that players suffer may go unreported because there is no obvious physical signs if a player is not knocked unconscious. This can prove to be dangerous if a player decides to return to play without receiving proper medical attention. Studies show that ice hockey causes 44.3% of all traumatic brain injuries among Canadian children. In severe cases, the traumatic brain injuries are capable of resulting in death. Occurrences of death from these injuries are rare.

An important defensive tactic is checking—attempting to take the puck from an opponent or to remove the opponent from play. "Stick checking", "sweep checking", and "poke checking" are legal uses of the stick to obtain possession of the puck. The "neutral zone trap" is designed to isolate the puck carrier in the neutral zone preventing him from entering the offensive zone. "Body checking" is using one's shoulder or hip to strike an opponent who has the puck or who is the last to have touched it (the last person to have touched the puck is still legally "in possession" of it, although a penalty is generally called if he is checked more than two seconds after his last touch). Body checking is also a penalty in certain leagues in order to reduce the chance of injury to players. Often the term checking is used to refer to body checking, with its true definition generally only propagated among fans of the game.

Offensive tactics include improving a team's position on the ice by advancing the puck out of one's zone towards the opponent's zone, progressively by gaining lines, first your own blue line, then the red line and finally the opponent's blue line. NHL rules instated for the 2006 season redefined the offside rule to make the two-line pass legal; a player may pass the puck from behind his own blue line, past both that blue line and the centre red line, to a player on the near side of the opponents' blue line. Offensive tactics are designed ultimately to score a goal by taking a shot. When a player purposely directs the puck towards the opponent's goal, he or she is said to "shoot" the puck.

A "deflection" is a shot that redirects a shot or a pass towards the goal from another player, by allowing the puck to strike the stick and carom towards the goal. A "one-timer" is a shot struck directly off a pass, without receiving the pass and shooting in two separate actions. "Headmanning the puck", also known as "breaking out", is the tactic of rapidly passing to the player farthest down the ice. "Loafing", also known as "cherry-picking", is when a player, usually a forward, skates behind an attacking team, instead of playing defence, in an attempt to create an easy scoring chance.

A team that is losing by one or two goals in the last few minutes of play will often elect to "pull the goalie"; that is, remove the goaltender and replace him or her with an "extra attacker" on the ice in the hope of gaining enough advantage to score a goal. However, it is an act of desperation, as it sometimes leads to the opposing team extending their lead by scoring a goal in the empty net.

One of the most important strategies for a team is their "forecheck". Forechecking is the act of attacking the opposition in their defensive zone. Forechecking is an important part of the "dump and chase" strategy (i.e. shooting the puck into the offensive zone and then chasing after it). Each team will use their own unique system but the main ones are: 2–1–2, 1–2–2, and 1–4. The 2–1–2 is the most basic forecheck system where two forwards will go in deep and pressure the opposition's defencemen, the third forward stays high and the two defencemen stay at the blueline. The 1–2–2 is a bit more conservative system where one forward pressures the puck carrier and the other two forwards cover the oppositions' wingers, with the two defencemen staying at the blueline. The 1–4 is the most defensive forecheck system, referred to as the neutral zone trap, where one forward will apply pressure to the puck carrier around the oppositions' blueline and the other 4 players stand basically in a line by their blueline in hopes the opposition will skate into one of them. Another strategy is the left wing lock, which has two forwards pressure the puck and the left wing and the two defencemen stay at the blueline.

There are many other little tactics used in the game of hockey. "Cycling" moves the puck along the boards in the offensive zone to create a scoring chance by making defenders tired or moving them out of position. "Pinching" is when a defenceman pressures the opposition's winger in the offensive zone when they are breaking out, attempting to stop their attack and keep the puck in the offensive zone. A "saucer pass" is a pass used when an opposition's stick or body is in the passing lane. It is the act of raising the puck over the obstruction and having it land on a teammate's stick.

A deke, short for "decoy", is a feint with the body or stick to fool a defender or the goalie. Many modern players, such as Pavel Datsyuk, Sidney Crosby and Patrick Kane, have picked up the skill of "dangling", which is fancier deking and requires more stick handling skills.

Although fighting is officially prohibited in the rules, it is not an uncommon occurrence at the professional level, and its prevalence has been both a target of criticism and a considerable draw for the sport. At the professional level in North America fights are unofficially condoned. Enforcers and other players fight to demoralize the opposing players while exciting their own, as well as settling personal scores. A fight will also break out if one of the team's skilled players gets hit hard or someone receives what the team perceives as a dirty hit. The amateur game penalizes fisticuffs more harshly, as a player who receives a fighting major is also assessed at least a 10-minute misconduct penalty (NCAA and some Junior leagues) or a game misconduct penalty and suspension (high school and younger, as well as some casual adult leagues). Crowds seem to like fighting in ice hockey and cheer when fighting erupts.

Ice hockey is one of the fastest growing women's sports in the world, with the number of participants increasing by 400 percent from 1995 to 2005. In 2011, Canada had 85,827 women players, United States had 65,609, Finland 4,760, Sweden 3,075 and Switzerland 1,172. While there are not as many organized leagues for women as there are for men, there exist leagues of all levels, including the National Women's Hockey League (NWHL), Mid-Atlantic Women's Hockey League, and various European leagues, as well as university teams, national and Olympic teams, and recreational teams. The IIHF holds IIHF World Women's Championships tournaments in several divisions; championships are held annually, except that the top flight does not play in Olympic years.

The chief difference between women's and men's ice hockey is that body checking is prohibited in women's hockey. After the 1990 Women's World Championship, body checking was eliminated in women's hockey. In current IIHF women's competition, body checking is either a minor or major penalty, decided at the referee's discretion. In addition, players in women's competition are required to wear protective full-face masks.

In Canada, to some extent ringette has served as the female counterpart to ice hockey, in the sense that traditionally, boys have played hockey while girls have played ringette.

Women are known to have played the game in the 19th century. Several games were recorded in the 1890s in Ottawa, Ontario, Canada. The women of Lord Stanley's family were known to participate in the game of ice hockey on the outdoor ice rink at Rideau Hall, the residence of Canada's Governor-General.

The game developed at first without an organizing body. A tournament in 1902 between Montreal and Trois-Rivieres was billed as the first championship tournament. Several tournaments, such as at the Banff Winter Carnival, were held in the early 20th century and numerous women's teams such as the Seattle Vamps and Vancouver Amazons existed. Organizations started to develop in the 1920s, such as the Ladies Ontario Hockey Association, and later, the Dominion Women's Amateur Hockey Association. Starting in the 1960s, the game spread to universities. Today, the sport is played from youth through adult leagues, and in the universities of North America and internationally. There have been two major professional women's hockey leagues to have paid its players: the National Women's Hockey League with teams in the United States and the Canadian Women's Hockey League with teams in Canada, China, and the United States.

The first women's world championship tournament, albeit unofficial, was held in 1987 in Toronto, Ontario, Canada. This was followed by the first IIHF World Championship in 1990 in Ottawa. Women's ice hockey was added as a medal sport at the 1998 Winter Olympics in Nagano, Japan. The United States won the gold, Canada won the silver and Finland won the bronze medal. Canada won in 2002, 2006, 2010, and 2014, and also reached the gold medal game in 2018, where it lost in a shootout to the United States, their first loss in a competitive Olympic game since 2002.

The United States Hockey League (USHL) welcomed the first female professional ice hockey player in 1969–70, when the Marquette Iron Rangers signed Karen Koch. One woman, Manon Rhéaume, has played in an NHL pre-season game as a goaltender for the Tampa Bay Lightning against the St. Louis Blues. In 2003, Hayley Wickenheiser played with the Kirkkonummi Salamat in the Finnish men's Suomi-sarja league. Several women have competed in North American minor leagues, including Rhéaume, goaltenders Kelly Dyer and Erin Whitten and defenceman Angela Ruggiero.

With interest in women's ice hockey growing, between 2007 and 2010 the number of registered female players worldwide grew from 153,665 to 170,872. Women's hockey is on the rise in almost every part of the world and there are teams in North America, Europe, Asia, Oceania, Africa and Latin America.

The future of international women's ice hockey was discussed at the World Hockey Summit in 2010, and IIHF member associations could work together. International Olympic Committee president Jacques Rogge stated that the women's hockey tournament might be eliminated from the Olympics since the event was not competitively balanced, and dominated by Canada and the United States. Team Canada captain Hayley Wickenheiser explained that the talent gap between the North American and European countries was due to the presence of women's professional leagues in North America, along with year-round training facilities. She stated the European players were talented, but their respective national team programs were not given the same level of support as the European men's national teams, or the North American women's national teams. She stressed the need for women to have their own professional league which would be for the benefit of international hockey.

The primary women's professional hockey league in North America is the National Women's Hockey League (NWHL) with five teams located in the United States and one in Canada. From 2007 until 2019 the Canadian Women's Hockey League (CWHL) operated with teams in Canada, the United States and China.

The CWHL was founded in 2007 and originally consisted of seven teams in Canada, but had several membership changes including adding a team in the United States in 2010. When the league launched, its players were only compensated for travel and equipment. The league began paying its players a stipend in the 2017–18 season when the league launched its first teams in China. For the league's 2018–19 season, there were six teams consisting of the Calgary Inferno, Les Canadiennes de Montreal, Markham Thunder, Shenzhen KRS Vanke Rays, Toronto Furies, and the Worcester Blades. The CWHL ceased operations in 2019 citing unsustainable business operations.

The NWHL was founded in 2015 with four teams in the Northeast United States and was the first North American women's league to pay its players. The league expanded to five teams in 2018 with the addition of the formerly independent Minnesota Whitecaps. The league had conditionally approved of Canadian expansion teams in Montreal and Toronto following the dissolution of the CWHL, but lack of investors has caused the postponement of any further expansion. On April 22, 2020, the NWHL officially announced that Toronto was awarded an expansion team for the 2020–21 season growing the league to six teams. The six teams in the league are the Boston Pride, Buffalo Beauts, Connecticut Whale, Metropolitan Riveters, Minnesota Whitecaps, and the Toronto Six.

The following is a list of professional ice hockey leagues by attendance:

The NHL is by far the best attended and most popular ice hockey league in the world, and is among the major professional sports leagues in the United States and Canada. The league's history began after Canada's National Hockey Association decided to disband in 1917; the result was the creation of the National Hockey League with four teams. The league expanded to the United States beginning in 1924 and had as many as 10 teams before contracting to six teams by 1942–43. In 1967, the NHL doubled in size to 12 teams, undertaking one of the greatest expansions in professional sports history. A few years later, in 1972, a new 12-team league, the World Hockey Association (WHA) was formed and due to its ensuing rivalry with the NHL, it caused an escalation in players salaries. In 1979, the 17-team NHL merged with the WHA creating a 21-team league. By 2017, the NHL had expanded to 31 teams, and after a realignment in 2013, these teams were divided into two conferences and four divisions. The league is expected to expand to 32 teams by 2021.

The American Hockey League (AHL), sometimes referred to as "The A", is the primary developmental professional league for players aspiring to enter the NHL. It comprises 31 teams from the United States and Canada. It is run as a "farm league" to the NHL, with the vast majority of AHL players under contract to an NHL team. The ECHL (called the East Coast Hockey League before the 2003–04 season) is a mid-level minor league in the United States with a few players under contract to NHL or AHL teams.

As of 2019, there are three minor professional leagues with no NHL affiliations: the Federal Prospects Hockey League (FPHL), Ligue Nord-Américaine de Hockey (LNAH), and the Southern Professional Hockey League (SPHL).

U Sports ice hockey is the highest level of play at the Canadian university level under the auspices of U Sports, Canada's governing body for university sports. As these players compete at the university level, they are obligated to follow the rule of standard eligibility of five years.
In the United States especially, college hockey is popular and the best university teams compete in the annual NCAA Men's Ice Hockey Championship. The American Collegiate Hockey Association is composed of college teams at the club level.

In Canada, the Canadian Hockey League is an umbrella organization comprising three major junior leagues: the Ontario Hockey League, the Western Hockey League, and the Quebec Major Junior Hockey League. It attracts players from Canada, the United States and Europe. The major junior players are considered amateurs as they are under 21-years-old and not paid a salary, however, they do get a stipend and play a schedule similar to a professional league. Typically, the NHL drafts many players directly from the major junior leagues.

In the United States, the United States Hockey League (USHL) is the highest junior league. Players in this league are also amateur with players required to be under 21-years old, but do not get a stipend, which allows players to retain their eligibility for participation in NCAA ice hockey.

The Kontinental Hockey League (KHL) is the largest and most popular ice hockey league in Eurasia. The league is the direct successor to the Russian Super League, which in turn was the successor to the Soviet League, the history of which dates back to the Soviet adoption of ice hockey in the 1940s. The KHL was launched in 2008 with clubs predominantly from Russia, but featuring teams from other post-Soviet states. The league expanded beyond the former Soviet countries beginning in the 2011–12 season, with clubs in Croatia and Slovakia. The KHL currently comprises member clubs based in Belarus (1), China (1), Finland (1), Latvia (1), Kazakhstan (1) and Russia (19) for a total of 24.

The second division of hockey in Eurasia is the Supreme Hockey League (VHL). This league features 24 teams from Russia and 2 from Kazakhstan. This league is currently being converted to a farm league for the KHL, similarly to the AHL's function in relation to the NHL. The third division is the Russian Hockey League, which features only teams from Russia. The Asia League, an international ice hockey league featuring clubs from China, Japan, South Korea, and the Russian Far East, is the successor to the Japan Ice Hockey League.

The highest junior league in Eurasia is the Junior Hockey League (MHL). It features 32 teams from post-Soviet states, predominantly Russia. The second tier to this league is the Junior Hockey League Championships (MHL-B).

Several countries in Europe have their own top professional senior leagues. Many future KHL and NHL players start or end their professional careers in these leagues. The National League A in Switzerland, Swedish Hockey League in Sweden, Liiga in Finland, and Czech Extraliga in the Czech Republic are all very popular in their respective countries.

Beginning in the 2014–15 season, the Champions Hockey League was launched, a league consisting of first-tier teams from several European countries, running parallel to the teams' domestic leagues. The competition is meant to serve as a Europe-wide ice hockey club championship. The competition is a direct successor to the European Trophy and is related to the 2008–09 tournament of the same name.

There are also several annual tournaments for clubs, held outside of league play. Pre-season tournaments include the European Trophy, Tampere Cup and the Pajulahti Cup. One of the oldest international ice hockey competition for clubs is the Spengler Cup, held every year in Davos, Switzerland, between Christmas and New Year's Day. It was first awarded in 1923 to the Oxford University Ice Hockey Club. The Memorial Cup, a competition for junior-level (age 20 and under) clubs is held annually from a pool of junior championship teams in Canada and the United States.

International club competitions organized by the IIHF include the Continental Cup, the Victoria Cup and the European Women's Champions Cup. The World Junior Club Cup is an annual tournament of junior ice hockey clubs representing each of the top junior leagues.

The Australian Ice Hockey League and New Zealand Ice Hockey League are represented by nine and five teams respectively. As of 2012, the two top teams of the previous season from each league compete in the Trans-Tasman Champions League.

Ice hockey in Africa is a small but growing sport; while no African ice hockey playing nation has a domestic national leagues, there are several regional leagues in South Africa.

Ice hockey has been played at the Winter Olympics since 1924 (and was played at the summer games in 1920). Hockey is Canada's national winter sport, and Canadians are extremely passionate about the game. The nation has traditionally done very well at the Olympic games, winning 6 of the first 7 gold medals. However, by 1956 its amateur club teams and national teams could not compete with the teams of government-supported players from the Soviet Union. The USSR won all but two gold medals from 1956 to 1988. The United States won its first gold medal in 1960. On the way to winning the gold medal at the 1980 Lake Placid Olympics, amateur US college players defeated the heavily favoured Soviet squad—an event known as the "Miracle on Ice" in the United States. Restrictions on professional players were fully dropped at the 1988 games in Calgary. NHL agreed to participate ten years later. 1998 Games saw the full participation of players from the NHL, which suspended operations during the Games and has done so in subsequent Games up until 2018. The 2010 games in Vancouver were the first played in an NHL city since the inclusion of NHL players. The 2010 games were the first played on NHL-sized ice rinks, which are narrower than the IIHF standard.

National teams representing the member federations of the IIHF compete annually in the IIHF Ice Hockey World Championships. Teams are selected from the available players by the individual federations, without restriction on amateur or professional status. Since it is held in the spring, the tournament coincides with the annual NHL Stanley Cup playoffs and many of the top players are hence not available to participate in the tournament. Many of the NHL players who do play in the IIHF tournament come from teams eliminated before the playoffs or in the first round, and federations often hold open spots until the tournament to allow for players to join the tournament after their club team is eliminated. For many years, the tournament was an amateur-only tournament, but this restriction was removed, beginning in 1977.

The 1972 Summit Series and 1974 Summit Series, two series pitting the best Canadian and Soviet players without IIHF restrictions were major successes, and established a rivalry between Canada and the USSR. In the spirit of best-versus-best without restrictions on amateur or professional status, the series were followed by five Canada Cup tournaments, played in North America. Two NHL versus USSR series were also held: the 1979 Challenge Cup and Rendez-vous '87. The Canada Cup tournament later became the World Cup of Hockey, played in 1996, 2004 and 2016. The United States won in 1996 and Canada won in 2004 and 2016.

Since the initial women's world championships in 1990, there have been fifteen tournaments. Women's hockey has been played at the Olympics since 1998. The only finals in the women's world championship or Olympics that did not involve both Canada and the United States were the 2006 Winter Olympic final between Canada and Sweden and 2019 World Championship final between the US and Finland.

Other ice hockey tournaments featuring national teams include the World U20 Championship, the World U18 Championships, the World U-17 Hockey Challenge, the World Junior A Challenge, the Ivan Hlinka Memorial Tournament, the World Women's U18 Championships and the 4 Nations Cup. The annual Euro Hockey Tour, an unofficial European championship between the national men's teams of the Czech Republic, Finland, Russia and Sweden have been played since 1996–97.

The attendance record for an ice hockey game was set on December 11, 2010, when the University of Michigan's men's ice hockey team faced cross-state rival Michigan State in an event billed as "The Big Chill at the Big House". The game was played at Michigan's (American) football venue, Michigan Stadium in Ann Arbor, with a capacity of 109,901 as of the 2010 football season. When UM stopped sales to the public on May 6, 2010, with plans to reserve remaining tickets for students, over 100,000 tickets had been sold for the event. Ultimately, a crowd announced by UM as 113,411, the largest in the stadium's history (including football), saw the homestanding Wolverines win 5–0. "Guinness World Records", using a count of ticketed fans who actually entered the stadium instead of UM's figure of tickets sold, announced a final figure of 104,173.

The record was approached but not broken at the 2014 NHL Winter Classic, which also held at Michigan Stadium, with the Detroit Red Wings as the home team and the Toronto Maple Leafs as the opposing team with an announced crowd of 105,491. The record for a NHL Stanley Cup playoff game is 28,183, set on April 23, 1996, at the Thunderdome during a Tampa Bay Lightning – Philadelphia Flyers game.

Number of registered hockey players, including male, female and junior, provided by the respective countries' federations. Note that this list only includes the 38 of 81 IIHF member countries with more than 1,000 registered players as of October 2019.

Pond hockey is a form of ice hockey played generally as pick-up hockey on lakes, ponds and artificial outdoor rinks during the winter. Pond hockey is commonly referred to in hockey circles as shinny. Its rules differ from traditional hockey because there is no hitting and very little shooting, placing a greater emphasis on skating, stickhandling and passing abilities. Since 2002, the World Pond Hockey Championship has been played on Roulston Lake in Plaster Rock, New Brunswick, Canada. Since 2006, the US Pond Hockey Championships have been played in Minneapolis, Minnesota, and the Canadian National Pond Hockey Championships have been played in Huntsville, Ontario.

Sledge hockey is an adaption of ice hockey designed for players who have a physical disability. Players are seated in sleds and use a specialized hockey stick that also helps the player navigate on the ice. The sport was created in Sweden in the early 1960s, and is played under similar rules to ice hockey.

Ice hockey is the official winter sport of Canada. Ice hockey, partially because of its popularity as a major professional sport, has been a source of inspiration for numerous films, television episodes and songs in North American popular culture.





</doc>
<doc id="14791" url="https://en.wikipedia.org/wiki?curid=14791" title="IEEE 802.3">
IEEE 802.3

IEEE 802.3 is a working group and a collection of Institute of Electrical and Electronics Engineers (IEEE) standards produced by the working group defining the physical layer and data link layer's media access control (MAC) of wired Ethernet. This is generally a local area network (LAN) technology with some wide area network (WAN) applications. Physical connections are made between nodes and/or infrastructure devices (hubs, switches, routers) by various types of copper or fiber cable.

802.3 is a technology that supports the IEEE 802.1 network architecture.

802.3 also defines LAN access method using CSMA/CD.




</doc>
<doc id="14794" url="https://en.wikipedia.org/wiki?curid=14794" title="Integer (computer science)">
Integer (computer science)

In computer science, an integer is a datum of integral data type, a data type that represents some range of mathematical integers. Integral data types may be of different sizes and may or may not be allowed to contain negative values. Integers are commonly represented in a computer as a group of binary digits (bits). The size of the grouping varies so the set of integer sizes available varies between different types of computers. Computer hardware, including virtual machines, nearly always provide a way to represent a processor register or memory address as an integer.

The "value" of an item with an integral type is the mathematical integer that it corresponds to. Integral types may be "unsigned" (capable of representing only non-negative integers) or "signed" (capable of representing negative integers as well).

An integer value is typically specified in the source code of a program as a sequence of digits optionally prefixed with + or −. Some programming languages allow other notations, such as hexadecimal (base 16) or octal (base 8). Some programming languages also permit digit group separators.

The "internal representation" of this datum is the way the value is stored in the computer's memory. Unlike mathematical integers, a typical datum in a computer has some minimal and maximum possible value.

The most common representation of a positive integer is a string of bits, using the binary numeral system. The order of the memory bytes storing the bits varies; see endianness. The "width" or "precision" of an integral type is the number of bits in its representation. An integral type with "n" bits can encode 2 numbers; for example an unsigned type typically represents the non-negative values 0 through 2−1. Other encodings of integer values to bit patterns are sometimes used, for example binary-coded decimal or Gray code, or as printed character codes such as ASCII.

There are four well-known ways to represent signed numbers in a binary computing system. The most common is two's complement, which allows a signed integral type with "n" bits to represent numbers from −2 through 2−1. Two's complement arithmetic is convenient because there is a perfect one-to-one correspondence between representations and values (in particular, no separate +0 and −0), and because addition, subtraction and multiplication do not need to distinguish between signed and unsigned types. Other possibilities include offset binary, sign-magnitude, and ones' complement.

Some computer languages define integer sizes in a machine-independent way; others have varying definitions depending on the underlying processor word size. Not all language implementations define variables of all integer sizes, and defined sizes may not even be distinct in a particular implementation. An integer in one programming language may be a different size in a different language or on a different processor.

Different CPUs support different integral data types. Typically, hardware will support both signed and unsigned types, but only a small, fixed set of widths.

The table above lists integral type widths that are supported in hardware by common processors. High level programming languages provide more possibilities. It is common to have a 'double width' integral type that has twice as many bits as the biggest hardware-supported type. Many languages also have "bit-field" types (a specified number of bits, usually constrained to be less than the maximum hardware-supported width) and "range" types (that can represent only the integers in a specified range).

Some languages, such as Lisp, Smalltalk, REXX, Haskell, Python, and Raku support "arbitrary precision" integers (also known as "infinite precision integers" or "bignums"). Other languages that do not support this concept as a top-level construct may have libraries available to represent very large numbers using arrays of smaller variables, such as Java's BigInteger class or Perl's "bigint" package. These use as much of the computer's memory as is necessary to store the numbers; however, a computer has only a finite amount of storage, so they too can only represent a finite subset of the mathematical integers. These schemes support very large numbers, for example one kilobyte of memory could be used to store numbers up to 2466 decimal digits long.

A Boolean or Flag type is a type that can represent only two values: 0 and 1, usually identified with "false" and "true" respectively. This type can be stored in memory using a single bit, but is often given a full byte for convenience of addressing and speed of access.

A four-bit quantity is known as a "nibble" (when eating, being smaller than a "bite") or "nybble" (being a pun on the form of the word "byte"). One nibble corresponds to one digit in hexadecimal and holds one digit or a sign code in binary-coded decimal.

The term "byte" initially meant 'the smallest addressable unit of memory'. In the past, 5-, 6-, 7-, 8-, and 9-bit bytes have all been used. There have also been computers that could address individual bits ('bit-addressed machine'), or that could only address 16- or 32-bit quantities ('word-addressed machine'). The term "byte" was usually not used at all in connection with bit- and word-addressed machines.

The term "octet" always refers to an 8-bit quantity. It is mostly used in the field of computer networking, where computers with different byte widths might have to communicate.

In modern usage "byte" almost invariably means eight bits, since all other sizes have fallen into disuse; thus "byte" has come to be synonymous with "octet".

The term 'word' is used for a small group of bits that are handled simultaneously by processors of a particular architecture. The size of a word is thus CPU-specific. Many different word sizes have been used, including 6-, 8-, 12-, 16-, 18-, 24-, 32-, 36-, 39-, 40-, 48-, 60-, and 64-bit. Since it is architectural, the size of a "word" is usually set by the first CPU in a family, rather than the characteristics of a later compatible CPU. The meanings of terms derived from "word", such as "longword", "doubleword", "quadword", and "halfword", also vary with the CPU and OS.

Practically all new desktop processors are capable of using 64-bit words, though embedded processors with 8- and 16-bit word size are still common. The 36-bit word length was common in the early days of computers.

One important cause of non-portability of software is the incorrect assumption that all computers have the same word size as the computer used by the programmer. For example, if a programmer using the C language incorrectly declares as int a variable that will be used to store values greater than 2−1, the program will fail on computers with 16-bit integers. That variable should have been declared as long, which has at least 32 bits on any computer. Programmers may also incorrectly assume that a pointer can be converted to an integer without loss of information, which may work on (some) 32-bit computers, but fail on 64-bit computers with 64-bit pointers and 32-bit integers. This issue is resolved by C99 in stdint.h in the form of .

A "short integer" can represent a whole number that may take less storage, while having a smaller range, compared with a standard integer on the same machine.

In C, it is denoted by short. It is required to be at least 16 bits, and is often smaller than a standard integer, but this is not required. A conforming program can assume that it can safely store values between −(2−1) and 2−1, but it may not assume that the range isn't larger. In Java, a short is "always" a 16-bit integer. In the Windows API, the datatype SHORT is defined as a 16-bit signed integer on all machines.

A "long integer" can represent a whole integer whose range is greater than or equal to that of a standard integer on the same machine.

In C, it is denoted by long. It is required to be at least 32 bits, and may or may not be larger than a standard integer. A conforming program can assume that it can safely store values between −(2−1) and 2−1, but it may not assume that the range isn't larger.

In the C99 version of the C programming language and the C++11 version of C++, a codice_1 type is supported that has double the minimum capacity of the standard codice_2. This type is not supported by compilers that require C code to be compliant with the previous C++ standard, C++03, because the long long type did not exist in C++03. For an ANSI/ISO compliant compiler, the minimum requirements for the specified ranges, that is, −(2−1) to 2−1 for signed and 0 to 2−1 for unsigned, must be fulfilled; however, extending this range is permitted. This can be an issue when exchanging code and data between platforms, or doing direct hardware access. Thus, there are several sets of headers providing platform independent exact width types. The C standard library provides "stdint.h"; this was introduced in C99 and C++11.



</doc>
<doc id="14800" url="https://en.wikipedia.org/wiki?curid=14800" title="Icon">
Icon

An icon or ikon (from the Greek 'image, resemblance') is a religious work of art, most commonly a painting, in the cultures of the Eastern Orthodox Church, Oriental Orthodoxy, the Roman Catholic, and certain Eastern Catholic churches. They are not simply artworks; "an icon is a sacred image used in religious devotion". The most common subjects include Christ, Mary, saints and angels. Although especially associated with portrait-style images concentrating on one or two main figures, the term also covers most religious images in a variety of artistic media produced by Eastern Christianity, including narrative scenes, usually from the Bible or lives of saints.

Icons may also be cast in metal, carved in stone, embroidered on cloth, painted on wood, done in mosaic or fresco work, printed on paper or metal, etc. Comparable images from Western Christianity can be classified as "icons", although "iconic" may also be used to describe a static style of devotional image.

Eastern Orthodox tradition holds that the production of Christian images dates back to the very early days of Christianity, and that it has been a continuous tradition since then. Modern academic art history considers that, while images may have existed earlier, the tradition can be traced back only as far as the 3rd century, and that the images which survive from Early Christian art often differ greatly from later ones. The icons of later centuries can be linked, often closely, to images from the 5th century onwards, though very few of these survive. Widespread destruction of images occurred during the Byzantine Iconoclasm of 726–842, although this did settle permanently the question of the appropriateness of images. Since then icons have had a great continuity of style and subject; far greater than in the icons of the Western church. At the same time there has been change and development.

Pre-Christian religions had produced and used art works,
but Christian tradition dating from the 8th century identifies Luke the Evangelist as the first icon painter.

Aside from the legend that Pilate had made an image of Christ, the 4th-century Eusebius of Caesarea, in his "Church History", provides a more substantial reference to a "first" icon of Jesus. He relates that King Abgar of Edessa (died 50 CE) sent a letter to Jesus at Jerusalem, asking Jesus to come and heal him of an illness. This version of the Abgar story does not mention an image, but a later account found in the Syriac "Doctrine of Addai" ( 400 ?) mentions a painted image of Jesus in the story; and even later, in the 6th-century account given by Evagrius Scholasticus, the painted image transforms into an image that miraculously appeared on a towel when Christ pressed the cloth to his wet face. Further legends relate that the cloth remained in Edessa until the 10th century, when it was taken to Constantinople. It went missing in 1204 when Crusaders sacked Constantinople, but by then numerous copies had firmly established its iconic type.

The 4th-century Christian Aelius Lampridius produced the earliest known written records of Christian images treated like icons (in a pagan or Gnostic context) in his "Life of Alexander Severus" (xxix) that formed part of the "Augustan History". According to Lampridius, the emperor Alexander Severus (), himself not a Christian, had kept a domestic chapel for the veneration of images of deified emperors, of portraits of his ancestors, and of Christ, Apollonius, Orpheus and Abraham. Saint Irenaeus, ( 130–202) in his "Against Heresies" (1:25;6) says scornfully of the Gnostic Carpocratians:

On the other hand, Irenaeus does not speak critically of icons or portraits in a general sense—only of certain gnostic sectarians' use of icons.

Another criticism of image veneration appears in the non-canonical 2nd-century "Acts of John" (generally considered a gnostic work), in which the Apostle John discovers that one of his followers has had a portrait made of him, and is venerating it: (27)

Later in the passage John says, "But this that you have now done is childish and imperfect: you have drawn a dead likeness of the dead."

At least some of the hierarchy of the Christian churches still strictly opposed icons in the early 4th century. At the Spanish non-ecumenical Synod of Elvira ( 305) bishops concluded, "Pictures are not to be placed in churches, so that they do not become objects of worship and adoration".

Bishop Epiphanius of Salamis, wrote his letter 51 to John, Bishop of Jerusalem ( 394) in which he recounted how he tore down an image in a church and admonished the other bishop that such images are "opposed . . . to our religion".

Elsewhere in his "Church History", Eusebius reports seeing what he took to be portraits of Jesus, Peter and Paul, and also mentions a bronze statue at Banias / Paneas under Mount Hermon, of which he wrote, "They say that this statue is an image of Jesus" ("H.E." 7:18); further, he relates that locals regarded the image as a memorial of the healing of the woman with an issue of blood by Jesus (Luke 8:43–48), because it depicted a standing man wearing a double cloak and with arm outstretched, and a woman kneeling before him with arms reaching out as if in supplication. John Francis Wilson suggests the possibility that this refers to a pagan bronze statue whose true identity had been forgotten; some have thought it to represent Aesculapius, the Greek god of healing, but the description of the standing figure and the woman kneeling in supplication precisely matches images found on coins depicting the bearded emperor Hadrian () reaching out to a female figure—symbolizing a province—kneeling before him.

When asked by Constantia (Emperor Constantine's half-sister) for an image of Jesus, Eusebius denied the request, replying: "To depict purely the human form of Christ before its transformation, on the other hand, is to break the commandment of God and to fall into pagan error." Hence Jaroslav Pelikan calls Eusebius "the father of iconoclasm".

After the emperor Constantine I extended official toleration of Christianity within the Roman Empire in 313, huge numbers of pagans became converts. This period of Christianization probably saw the use of Christian images became very widespread among the faithful, though with great differences from pagan habits. Robin Lane Fox states "By the early fifth century, we know of the ownership of private icons of saints; by c. 480–500, we can be sure that the inside of a saint's shrine would be adorned with images and votive portraits, a practice which had probably begun earlier."

When Constantine himself () apparently converted to Christianity, the majority of his subjects remained pagans. The Roman Imperial cult of the divinity of the emperor, expressed through the traditional burning of candles and the offering of incense to the emperor's image, was tolerated for a period because it would have been politically dangerous to attempt to suppress it. Indeed, in the 5th century the courts of justice and municipal buildings of the empire still honoured the portrait of the reigning emperor in this way. In 425 Philostorgius, an allegedly Arian Christian, charged the Orthodox Christians in Constantinople with idolatry because they still honored the image of the emperor Constantine the Great in this way. Dix notes that this occurred more than a century before we find the first reference to a similar honouring of the image of Christ or of His apostles or saints, but that it would seem a natural progression for the image of Christ, the King of Heaven and Earth, to be paid similar veneration as that given to the earthly Roman emperor. However, the Orthodox, Eastern Catholics, and other groups insist on explicitly distinguishing the veneration of icons from the worship of idols by pagans. 

After adoption of Christianity as the only permissible Roman state religion under Theodosius I, Christian art began to change not only in quality and sophistication, but also in nature. This was in no small part due to Christians being free for the first time to express their faith openly without persecution from the state, in addition to the faith spreading to the non-poor segments of society. Paintings of martyrs and their feats began to appear, and early writers commented on their lifelike effect, one of the elements a few Christian writers criticized in pagan art—the ability to imitate life. The writers mostly criticized pagan works of art for pointing to false gods, thus encouraging idolatry.   Statues in the round were avoided as being too close to the principal artistic focus of pagan cult practices, as they have continued to be (with some small-scale exceptions) throughout the history of Eastern Christianity.

Nilus of Sinai (d. c. 430), in his "Letter to Heliodorus Silentiarius", records a miracle in which St. Plato of Ankyra appeared to a Christian in a dream. The Saint was recognized because the young man had often seen his portrait. This recognition of a religious apparition from likeness to an image was also a characteristic of pagan pious accounts of appearances of gods to humans, and was a regular "topos" in hagiography. One critical recipient of a vision from Saint Demetrius of Thessaloniki apparently specified that the saint resembled the "more ancient" images of him—presumably the 7th-century mosaics still in Hagios Demetrios. Another, an African bishop, had been rescued from Arab slavery by a young soldier called Demetrios, who told him to go to his house in Thessaloniki. Having discovered that most young soldiers in the city seemed to be called Demetrios, he gave up and went to the largest church in the city, to find his rescuer on the wall.
During this period the church began to discourage all non-religious human images—the Emperor and donor figures counting as religious. This became largely effective, so that most of the population would only ever see religious images and those of the ruling class. The word icon referred to any and all images, not just religious ones, but there was barely a need for a separate word for these.

It is in a context attributed to the 5th century that the first mention of an image of Mary painted from life appears, though earlier paintings on catacomb walls bear resemblance to modern icons of Mary. Theodorus Lector, in his 6th-century "History of the Church" 1:1 stated that Eudokia (wife of emperor Theodosius II, d. 460) sent an image of the "Mother of God" named Icon of the Hodegetria from Jerusalem to Pulcheria, daughter of Arcadius, the former emperor and father of Theodosius II. The image was specified to have been "painted by the Apostle Luke."

Margherita Guarducci relates a tradition that the original icon of Mary attributed to Luke, sent by Eudokia to Pulcheria from Palestine, was a large circular icon only of her head. When the icon arrived in Constantinople it was fitted in as the head into a very large rectangular icon of her holding the Christ child and it is this composite icon that became the one historically known as the Hodegetria. She further states another tradition that when the last Latin Emperor of Constantinople, Baldwin II, fled Constantinople in 1261 he took this original circular portion of the icon with him. This remained in the possession of the Angevin dynasty who had it likewise inserted into a much larger image of Mary and the Christ child, which is presently enshrined above the high altar of the Benedictine Abbey church of Montevergine. Unfortunately this icon has been over the subsequent centuries subjected to repeated repainting, so that it is difficult to determine what the original image of Mary's face would have looked like. However, Guarducci also states that in 1950 an ancient image of Mary at the Church of Santa Francesca Romana was determined to be a very exact, but reverse mirror image of the original circular icon that was made in the 5th century and brought to Rome, where it has remained until the present.

In later tradition the number of icons of Mary attributed to Luke would greatly multiply; the Salus Populi Romani, the Theotokos of Vladimir, the Theotokos Iverskaya of Mount Athos, the Theotokos of Tikhvin, the Theotokos of Smolensk and the Black Madonna of Częstochowa are examples, and another is in the cathedral on St Thomas Mount, which is believed to be one of the seven painted by St. Luke the Evangelist and brought to India by St. Thomas. Ethiopia has at least seven more. Bissera V. Pentcheva concludes, “The myth [of Luke painting an icon] was invented in order to support the legitimacy of icon veneration during the Iconoclast controversy [8th and 9th centuries]. By claiming the existence of a portrait of the Theotokos painted during her lifetime by the evangelist Luke, the perpetrators of this fiction fabricated evidence for the apostolic origins and divine approval of images.”

In the period before and during the Iconoclastic Controversy, stories attributing the creation of icons to the New Testament period greatly increased, with several apostles and even the Virgin herself believed to have acted as the artist or commissioner of images (also embroidered in the case of the Virgin).

There was a continuing opposition to images and their misuse within Christianity from very early times. "Whenever images threatened to gain undue influence within the church, theologians have sought to strip them of their power". Further, "there is no century between the fourth and the eighth in which there is not some evidence of opposition to images even within the Church". Nonetheless, popular favor for icons guaranteed their continued existence, while no systematic apologia for or against icons, or doctrinal authorization or condemnation of icons yet existed.

The use of icons was seriously challenged by Byzantine Imperial authority in the 8th century. Though by this time opposition to images was strongly entrenched in Judaism and Islam, attribution of the impetus toward an iconoclastic movement in Eastern Orthodoxy to Muslims or Jews ""seems to have been highly exaggerated, both by contemporaries and by modern scholars"".

Though significant in the history of religious doctrine, the Byzantine controversy over images is not seen as of primary importance in Byzantine history. "Few historians still hold it to have been the greatest issue of the period..."

The Iconoclastic Period began when images were banned by Emperor Leo III the Isaurian sometime between 726 and 730. Under his son Constantine V, a council forbidding image veneration was held at Hieria near Constantinople in 754. Image veneration was later reinstated by the Empress Regent Irene, under whom another   council was held reversing the decisions of the previous iconoclast council and taking its title as Seventh Ecumenical Council. The council anathemized all who hold to iconoclasm, i.e. those who held that veneration of images constitutes idolatry. Then the ban was enforced again by Leo V in 815. And finally icon veneration was decisively restored by Empress Regent Theodora in 843.

From then on all Byzantine coins had a religious image or symbol on the reverse, usually an image of Christ for larger denominations, with the head of the Emperor on the obverse, reinforcing the bond of the state and the divine order.

The tradition of "acheiropoieta" (, literally "not-made-by-hand") accrued to icons that are alleged to have come into existence miraculously, not by a human painter. Such images functioned as powerful relics as well as icons, and their images were naturally seen as especially authoritative as to the true appearance of the subject: naturally and especially because of the reluctance to accept mere human productions as embodying anything of the divine, a commonplace of Christian deprecation of man-made "idols". Like icons believed to be painted directly from the live subject, they therefore acted as important references for other images in the tradition. Beside the developed legend of the "mandylion" or Image of Edessa, was the tale of the Veil of Veronica, whose very name signifies "true icon" or "true image", the fear of a "false image" remaining strong.

Although there are earlier records of their use, no panel icons earlier than the few from the 6th century preserved at the Greek Orthodox Saint Catherine's Monastery in Egypt survive, as the other examples in Rome have all been drastically over-painted. The surviving evidence for the earliest depictions of Christ, Mary and saints therefore comes from wall-paintings, mosaics and some carvings. They are realistic in appearance, in contrast to the later stylization. They are broadly similar in style, though often much superior in quality, to the mummy portraits done in wax (encaustic) and found at Fayyum in Egypt. As we may judge from such items, the first depictions of Jesus were generic rather than portrait images, generally representing him as a beardless young man. It was some time before the earliest examples of the long-haired, bearded face that was later to become standardized as the image of Jesus appeared. When they did begin to appear there was still variation.  Augustine of Hippo (354–430) said that no one knew the appearance of Jesus or that of Mary. However, Augustine was not a resident of the Holy Land and therefore was not familiar with the local populations and their oral traditions. Gradually, paintings of Jesus took on characteristics of portrait images.

At this time the manner of depicting Jesus was not yet uniform, and there was some controversy over which of the two most common icons was to be favored. The first or "Semitic" form showed Jesus with short and "frizzy" hair; the second showed a bearded Jesus with hair parted in the middle, the manner in which the god Zeus was depicted. Theodorus Lector remarked that of the two, the one with short and frizzy hair was "more authentic". To support his assertion, he relates a story (excerpted by John of Damascus) that a pagan commissioned to paint an image of Jesus used the "Zeus" form instead of the "Semitic" form, and that as punishment his hands withered.

Though their development was gradual, we can date the full-blown appearance and general ecclesiastical (as opposed to simply popular or local) acceptance of Christian images as venerated and miracle-working objects to the 6th century, when, as Hans Belting writes, "we first hear of the church's use of religious images." "As we reach the second half of the sixth century, we find that images are attracting direct veneration and some of them are credited with the performance of miracles" Cyril Mango writes, "In the post-Justinianic period the icon assumes an ever increasing role in popular devotion, and there is a proliferation of miracle stories connected with icons, some of them rather shocking to our eyes".   However, the earlier references by Eusebius and Irenaeus indicate veneration of images and reported miracles associated with them as early as the 2nd century.

In the icons of Eastern Orthodoxy, and of the Early Medieval West, very little room is made for artistic license. Almost everything within the image has a symbolic aspect. Christ, the saints, and the angels all have halos. Angels (and often John the Baptist) have wings because they are messengers. Figures have consistent facial appearances, hold attributes personal to them, and use a few conventional poses.

Colour plays an important role as well. Gold represents the radiance of Heaven; red, divine life. Blue is the color of human life, white is the Uncreated Light of God, only used for resurrection and transfiguration of Christ. If you look at icons of Jesus and Mary: Jesus wears red undergarment with a blue outer garment (God become Human) and Mary wears a blue undergarment with a red overgarment (human was granted gifts by God), thus the doctrine of deification is conveyed by icons. Letters are symbols too. Most icons incorporate some calligraphic text naming the person or event depicted. Even this is often presented in a stylized manner.

In the Eastern Orthodox Christian tradition there are reports of particular, wonderworking icons that exude myrrh (fragrant, healing oil), or perform miracles upon petition by believers. When such reports are verified by the Orthodox hierarchy, they are understood as miracles performed by God through the prayers of the saint, rather than being magical properties of the painted wood itself. Theologically, all icons are considered to be sacred, and are miraculous by nature, being a means of spiritual communion between the heavenly and earthly realms. However, it is not uncommon for specific icons to be characterised as "miracle-working", meaning that God has chosen to glorify them by working miracles through them. Such icons are often given particular names (especially those of the Virgin Mary), and even taken from city to city where believers gather to venerate them and pray before them. Islands like that of Tinos are renowned for possessing such "miraculous" icons, and are visited every year by thousands of pilgrims.

The Eastern Orthodox view of the origin of icons is generally quite different from that of most secular scholars and from some in contemporary Roman Catholic circles: "The Orthodox Church maintains and teaches that the sacred image has existed from the beginning of Christianity", Léonid Ouspensky has written. Accounts that some non-Orthodox writers consider legendary are accepted as history within Eastern Orthodoxy, because they are a part of church tradition. Thus accounts such as that of the miraculous "Image Not Made by Hands", and the weeping and moving "Mother of God of the Sign" of Novgorod are accepted as fact: "Church Tradition tells us, for example, of the existence of an Icon of the Savior during His lifetime (the "Icon-Made-Without-Hands") and of Icons of the Most-Holy Theotokos [Mary] immediately after Him." Eastern Orthodoxy further teaches that "a clear understanding of the importance of Icons" was part of the church from its very beginning, and has never changed, although explanations of their importance may have developed over time. This is because icon painting is rooted in the theology of the Incarnation (Christ being the "eikon" of God) which didn't change, though its subsequent clarification within the Church occurred over the period of the first seven Ecumenical Councils. Also, icons served as tools of edification for the illiterate faithful during most of the history of Christendom. Thus, icons are words in painting; they refer to the history of salvation and to its manifestation in concrete persons. In the Orthodox Church "icons have always been understood as a visible gospel, as a testimony to the great things given man by God the incarnate Logos" In the Council of 860 it was stated that "all that is uttered in words written in syllables is also proclaimed in the language of colors".

Eastern Orthodox find the first instance of an image or icon in the Bible when God made man in His own image (Septuagint Greek "eikona"), in Genesis 1:26–27. In Exodus, God commanded that the Israelites not make any graven image; but soon afterwards, he commanded that they make graven images of cherubim and other like things, both as statues and woven on tapestries. Later, Solomon included still more such imagery when he built the first temple. Eastern Orthodox believe these qualify as icons, in that they were visible images depicting heavenly beings and, in the case of the cherubim, used to indirectly indicate God's presence above the Ark.

In the Book of Numbers it is written that God told Moses to make a bronze serpent, "Nehushtan", and hold it up, so that anyone looking at the snake would be healed of their snakebites. In John 3, Jesus refers to the same serpent, saying that he must be lifted up in the same way that the serpent was. John of Damascus also regarded the brazen serpent as an icon. Further, Jesus Christ himself is called the "image of the invisible God" in Colossians 1:15, and is therefore in one sense an icon. As people are also made in God's images, people are also considered to be living icons, and are therefore "censed" along with painted icons during Orthodox prayer services.

According to John of Damascus, anyone who tries to destroy icons "is the enemy of Christ, the Holy Mother of God and the saints, and is the defender of the Devil and his demons." This is because the theology behind icons is closely tied to the Incarnational theology of the humanity and divinity of Jesus, so that attacks on icons typically have the effect of undermining or attacking the Incarnation of Jesus himself as elucidated in the Ecumenical Councils.

Basil of Caesarea, in his writing "On the Holy Spirit", says: "The honor paid to the image passes to the prototype". He also illustrates the concept by saying, "If I point to a statue of Caesar and ask you 'Who is that?', your answer would properly be, 'It is Caesar.' When you say such you do not mean that the stone itself is Caesar, but rather, the name and honor you ascribe to the statue passes over to the original, the archetype, Caesar himself." So it is with an icon.

Thus to kiss an icon of Christ, in the Eastern Orthodox view, is to show love towards Christ Jesus himself, not mere wood and paint making up the physical substance of the icon. Worship of the icon as somehow entirely separate from its prototype is expressly forbidden by the Seventh Ecumenical Council.

Icons are often illuminated with a candle or jar of oil with a wick. (Beeswax for candles and olive oil for oil lamps are preferred because they burn very cleanly, although other materials are sometimes used.) The illumination of religious images with lamps or candles is an ancient practice pre-dating Christianity.

Of the icon painting tradition that developed in Byzantium, with Constantinople as the chief city, we have only a few icons from the 11th century and none preceding them, in part because of the Iconoclastic reforms during which many were destroyed or lost, and also because of plundering by the Republic of Venice in 1204 during the Fourth Crusade, and finally the Fall of Constantinople in 1453.

It was only in the Komnenian period (1081–1185) that the cult of the icon became widespread in the Byzantine world, partly on account of the dearth of richer materials (such as mosaics, ivory, and vitreous enamels), but also because an "iconostasis" a special screen for icons was introduced then in ecclesiastical practice. The style of the time was severe, hieratic and distant.

In the late Comnenian period this severity softened, and emotion, formerly avoided, entered icon painting. Major monuments for this change include the murals at Daphni Monastery (c. 1100) and the Church of St. Panteleimon near Skopje (1164). The Theotokos of Vladimir (c. 1115, "illustration, right") is probably the most representative example of the new trend towards spirituality and emotion.

The tendency toward emotionalism in icons continued in the Paleologan period, which began in 1261. Palaiologan art reached its pinnacle in mosaics such as those of Chora Church. In the last half of the 14th century, Palaiologan saints were painted in an exaggerated manner, very slim and in contorted positions, that is, in a style known as the Palaiologan Mannerism, of which is a superb example.

After 1453, the Byzantine tradition was carried on in regions previously influenced by its religion and culture—in the Balkans, Russia, and other Slavic countries, Georgia and Armenia in the Caucasus, and among Eastern Orthodox minorities in the Islamic world. In the Greek-speaking world Crete, ruled by Venice until the mid-17th century, was an important centre of painted icons, as home of the Cretan School, exporting many to Europe.

Crete was under Venetian control from 1204 and became a thriving center of art with eventually a "Scuola di San Luca", or organized painter's guild, the Guild of Saint Luke, on Western lines. Cretan painting was heavily patronized both by Catholics of Venetian territories and by Eastern Orthodox. For ease of transport, Cretan painters specialized in panel paintings, and developed the ability to work in many styles to fit the taste of various patrons. El Greco, who moved to Venice after establishing his reputation in Crete, is the most famous artist of the school, who continued to use many Byzantine conventions in his works. In 1669 the city of Heraklion, on Crete, which at one time boasted at least 120 painters, finally fell to the Turks, and from that time Greek icon painting went into a decline, with a revival attempted in the 20th century by art reformers such as Photis Kontoglou, who emphasized a return to earlier styles.

Russian icons are typically paintings on wood, often small, though some in churches and monasteries may be as large as a table top. Many religious homes in Russia have icons hanging on the wall in the "krasny ugol"—the "red" corner (see Icon corner). There is a rich history and elaborate religious symbolism associated with icons. In Russian churches, the nave is typically separated from the sanctuary by an "iconostasis", a wall of icons.

The use and making of icons entered Kievan Rus' following its conversion to Orthodox Christianity from the Eastern Roman (Byzantine) Empire in 988 AD. As a general rule, these icons strictly followed models and formulas hallowed by usage, some of which had originated in Constantinople. As time passed, the Russians—notably Andrei Rublev and Dionisius—widened the vocabulary of iconic types and styles far beyond anything found elsewhere. The personal, improvisatory and creative traditions of Western European religious art are largely lacking in Russia before the 17th century, when Simon Ushakov's painting became strongly influenced by religious paintings and engravings from Protestant as well as Catholic Europe.

In the mid-17th century, changes in liturgy and practice instituted by Patriarch Nikon of Moscow resulted in a split in the Russian Orthodox Church. The traditionalists, the persecuted "Old Ritualists" or "Old Believers", continued the traditional stylization of icons, while the State Church modified its practice. From that time icons began to be painted not only in the traditional stylized and nonrealistic mode, but also in a mixture of Russian stylization and Western European realism, and in a Western European manner very much like that of Catholic religious art of the time. The Stroganov School and the icons from Nevyansk rank among the last important schools of Russian icon-painting.

In Romania, icons painted as reversed images behind glass and set in frames were common in the 19th century and are still made. The process is known as reverse glass painting. "In the Transylvanian countryside, the expensive icons on panels imported from Moldavia, Wallachia, and Mt. Athos were gradually replaced by small, locally produced icons on glass, which were much less expensive and thus accessible to the Transylvanian peasants[.]"

The earliest historical records about icons in Serbia dates back to the period of Nemanjić dynasty. One of the notable schools of Serb icons was active in the Bay of Kotor from the 17th century to the 19th century. 

Trojeručica meaning "Three-handed Theotokos" is the most important icon of the Serbian Orthodox Church and main icon of Mount Athos.

The Coptic Orthodox Church of Alexandria and Oriental Orthodoxy also have distinctive, living icon painting traditions. Coptic icons have their origin in the Hellenistic art of Egyptian Late Antiquity, as exemplified by the Fayum mummy portraits. Beginning in the 4th century, churches painted their walls and made icons to reflect an authentic expression of their faith.

The Aleppo School was a school of icon-painting, founded by the priest Yusuf al-Musawwir (also known as Joseph the Painter) and active in Aleppo, which was then a part of the Ottoman Empire, between at least 1645 and 1777.

Although the word "icon" is not used in Western Christianity, there are religious works of art which were largely patterned on Byzantine works, and equally conventional in composition and depiction. Until the 13th century, "icon"-like portraits followed East pattern—although very few survive from this early period. From the 13th century, the western tradition came slowly to allow the artist far more flexibility, and a more realist approach to the figures. If only because there was a much smaller number of skilled artists, the quantity of works of art, in the sense of panel paintings, was much smaller in the West, and in most Western settings a single diptych as an altarpiece, or in a domestic room, probably stood in place of the larger collections typical of Orthodox "icon corners".

Only in the 15th century did production of painted works of art begin to approach Eastern levels, supplemented by mass-produced imports from the Cretan School. In this century, the use of "icon"-like portraits in the West was enormously increased by the introduction of old master prints on paper, mostly woodcuts which were produced in vast numbers (although hardly any survive). They were mostly sold, hand-coloured, by churches, and the smallest sizes (often only an inch high) were affordable even by peasants, who glued or pinned them straight onto a wall.

With the Reformation, after an initial uncertainty among early Lutherans, who painted a few "icon"-like depictions of leading Reformers, and continued to paint scenes from Scripture, Protestants came down firmly against icon-like portraits, especially larger ones, even of Christ. Many Protestants found these "idolatrous".

The Catholic Church accepted the decrees of the iconodule Seventh Ecumenical Council regarding images. There is some minor difference, however, in the Catholic attitude to images from that of the Orthodox. Following Gregory the Great, Catholics emphasize the role of images as the "Biblia Pauperum", the "Bible of the Poor", from which those who could not read could nonetheless learn.

Catholics also, however, share the same viewpoint with the Orthodox when it comes to image veneration, believing that whenever approached, sacred images are to be reverenced. Though using both flat wooden panel and stretched canvas paintings, Catholics traditionally have also favored images in the form of three-dimensional statuary, whereas in the East, statuary is much less widely employed.

A joint Lutheran–Orthodox statement made in the 7th Plenary of the Lutheran–Orthodox Joint Commission, on July 1993 in Helsinki, reaffirmed the ecumenical council decisions on the nature of Christ and the veneration of images:





</doc>
<doc id="14801" url="https://en.wikipedia.org/wiki?curid=14801" title="Icon (programming language)">
Icon (programming language)

Icon is a very high-level programming language featuring goal-directed execution and many facilities for managing strings and textual patterns. It is related to SNOBOL and SL5, string processing languages. Icon is not object-oriented, but an object-oriented extension called Idol was developed in 1996 which eventually became Unicon.

The Icon language is derived from the ALGOL-class of structured programming languages, and thus has syntax similar to C or Pascal. Icon is most similar to Pascal, using syntax for assignments, the keyword and similar syntax. On the other hand, Icon uses C-style brackets for structuring execution groups, and programs start by running a procedure called .

In many ways Icon also shares features with most scripting languages (as well as SNOBOL and SL5, from which they were taken): variables do not have to be declared, types are cast automatically, and numbers can be converted to strings and back automatically. Another feature common to many scripting languages, but not all, is the lack of a line-ending character; in Icon, lines not ended by a semicolon get ended by an implied semicolon if it makes sense.

Procedures are the basic building blocks of Icon programs. Although they use Pascal naming, they work more like C functions and can return values; there is no keyword in Icon.
One of Icon's key concepts is that control structures are based on the "success" or "failure" of expressions, rather than on boolean logic, as in most other programming languages. This feature derives directly from SNOBOL, in which any pattern match and/or replacement operation could be followed by success and/or failure clauses that specified a statement label to be branched to under the requisite condition. Under the goal-directed branching model, a simple comparison like does not mean, "if the operations to the right evaluate to true" as they would under most languages; instead, it means something more like, "if the operations to the right "succeed"". In this case the < operator succeeds if the comparison is true, so the end result is the same. In addition, the < operator returns its second argument if it succeeds, allowing things like , a common type of comparison that in most languages must be written as a conjunction of two inequalities like .

Icon uses success or failure for all flow control, so this simple code:
if a := read() then write(a)

will copy one line of standard input to standard output. It will work even if the read() causes an error, for instance, if the file does not exist. In that case the statement a := read() will fail, and write will simply not be called.

Success and failure are passed "up" through functions, meaning that a failure inside a nested function will cause the functions calling it to fail as well. For instance, here is a program that copies an entire file:
while write(read())

When the read() command fails, at the end of file for instance, the failure will be passed up the call chain, and write() will fail as well. The while, being a control structure, stops on failure. A similar example written in pseudocode (using syntax close to Java):

This case needs two comparisons: one for end of file (EOF) and another for all other errors. Since Java does not allow exceptions to be compared as logic elements, as under Icon, the lengthy syntax must be used instead. Try blocks also impose a performance penalty, even if no exception is thrown, a distributed cost that Icon avoids.

Icon refers to this concept as "goal-directed execution", referring to the way that execution continues until some goal is reached. In the example above the goal is to read the entire file; the read command succeeds when information has been read, and fails when it hasn't. The goal is thus coded directly in the language, instead of by checking return codes or similar constructs.

Expressions in Icon often return a single value, for instance, will evaluate and succeed if the value of x is less than 5, or else fail. However, many expressions do not "immediately" return success or failure, returning values in the meantime. This drives the examples with and ; causes to continue to return values until it fails.

This is a key concept in Icon, known as "generators". Generators drive much of the loop functionality in the language, but without the need for an explicit loop comparing values at each iteration.

Within the parlance of Icon, the evaluation of an expression or function produces a "result sequence". A result sequence contains all the possible values that can be generated by the expression or function. When the result sequence is exhausted the expression or function fails. Iteration over the result sequence is achieved either implicitly via Icon's goal-directed evaluation or explicitly via the clause.

Icon includes several generator-builders. The "alternator" syntax allows a series of items to be generated in sequence until one fails:

can generate "1", "hello", and "5" if x is less than 5. Alternators can be read as "or" in many cases, for instance:

will write out the value of y if it is smaller than x "or" 5. Internally Icon checks every value from left to right until one succeeds or the list empties and it returns a failure. Functions will not be called unless evaluating their parameters succeeds, so this example can be shortened to:

Another simple generator is , which generates lists of integers; will call ten times. The "bang syntax" generates every item of a list; will output each character of aString on a new line.

This concept is powerful for string operations. Most languages include a function known as or that returns the location of one string within another. For example:

This code will return 4, the position of the first occurrence of the word "the" (assuming the indices start at 0). To get the next instance of "the" an alternate form must be used,

the 5 at the end saying it should look from position 5 on. So to extract all the occurrences of "the", a loop must be used:

Under Icon the function is a generator, and will return the next instance of the string each time it is resumed before failing when it reaches the end of the string. The same code can be written:

Of course there are times where one wants to find a string after some point in input, for instance, if scanning a text file containing data in multiple columns. Goal-directed execution works here as well:
The position will only be returned if "the" appears after position 5; the comparison will fail otherwise. Comparisons that succeed return the right-hand result, so it is important to put the find on the right-hand side of the comparison. If it were written:
then "5" would be written instead of the result of .

Icon adds several control structures for looping through generators. The operator is similar to , looping through every item returned by a generator and exiting on failure:

The syntax actually injects values into the function in a fashion similar to blocks under Smalltalk. For instance, the above loop can be re-written this way:
Generators can be defined as procedures using the keyword:

This example loops over "theString" using find to look for "pattern". When one is found, and the position is odd, the location is returned from the function with . Unlike , memorizes the state of the generator, allowing it to pick up where it left off on the next iteration.

Icon has features to make working with strings easier. The "scanning" system repeatedly calls functions on a string:

s ? write(find("the"))

is a short form of the examples shown earlier. In this case the "subject" of the function is placed outside the parameters in front of the question mark. Icon function signatures identify the subject parameter so that it can be hoist in this fashion.

Substrings can be extracted from a string by using a range specification within brackets. A range specification can return a point to a single character, or a slice of the string. Strings can be indexed from either the right or the left. Positions within a string are defined to be between the characters ABC and can be specified from the right ABC

For example,

Where the last example shows using a length instead of an ending position

The subscripting specification can be used as a lvalue within an expression. This can be used to insert strings into another string or delete parts of a string. For example,

Icon's subscript indices are between the elements. Given the string s := "ABCDEFG", the indexes are: ABCDEFG. The slice s[3:5] is the string between the indices 3 and 5, which is the string "CD".

Icon also has syntax to build lists (or "arrays"):

aCat := ["muffins", "tabby", 2002, 8]

The items within a list can be of any type, including other structures. To build larger lists, Icon includes the generator; generates a list containing 10 copies of "word".

Like arrays in other languages, Icon allows items to be looked up by position, e.g., . As with strings, the indices are between the elements, and a slice of a list can be obtained by specifying the range, e.g., produces the list . Unlike for strings, a slice of an array is not an lvalue.

The "bang-syntax" enumerates the range. For example, will print out four lines, each with one element.

Icon includes stack-like functions, and to allow arrays to form the bases of stacks and queues.

Icon also includes functionality for sets and associative arrays with "tables":

This code creates a table that will use zero as the default value of any unknown key. It then adds two items into it, with the keys "there" and "here", and values 1 and 2.

One of the powerful features of Icon is string scanning. The scan string operator, , saves the current string scanning environment and creates a new string scanning environment. The string scanning environment consists of two keyword variables, codice_1 and , where &subject is the string being scanned, and &pos is the "cursor" or current position within the subject string.

For example,

would produce

subject=[this is a string] pos=[1]
Built-in and user-defined functions can be used to move around within the string being scanned. Many of the built-in functions will default to &subject and &pos (for example the "find" function). The following, for example, will write all blank-delimited "words" in a string.
A more complex example demonstrates the integration of generators and string scanning within the language.
The idiom of returns the value of the last expression.


The definitive work is "The Icon Programming Language" (third edition) by Griswold and Griswold, .
It is out of print but can be downloaded as a PDF.

Icon also has co-expressions, providing non-local exits for program execution. See "The Icon Programming language" and also Shamim Mohamed's article "Co-expressions in Icon".



</doc>
<doc id="14802" url="https://en.wikipedia.org/wiki?curid=14802" title="Iconology">
Iconology

Iconology is a method of interpretation in cultural history and the history of the visual arts used by Aby Warburg, Erwin Panofsky and their followers that uncovers the cultural, social, and historical background of themes and subjects in the visual arts. Though Panofsky differentiated between iconology and iconography, the distinction is not very widely followed, "and they have never been given definitions accepted by all iconographers and iconologists". Few 21st-century authors continue to use the term "iconology" consistently, and instead use iconography to cover both areas of scholarship.

To those who use the term, iconology is derived from synthesis rather than scattered analysis and examines symbolic meaning on more than its face value by reconciling it with its historical context and with the artist's body of work – in contrast to the widely descriptive iconography, which, as described by Panofsky, is an approach to studying the content and meaning of works of art that is primarily focused on classifying, establishing dates, provenance and other necessary fundamental knowledge concerning the subject matter of an artwork that is needed for further interpretation.

Panofsky's "use of iconology as the principle tool of art analysis brought him critics." For instance, in 1946, Jan Gerrit Van Gelder "criticized Panofsky's iconology as putting too much emphasis on the symbolic content of the work of art, neglecting its formal aspects and the work as a unity of form and content." Furthermore, iconology is mostly avoided by social historians who do not accept the theoretical dogmaticism in the work of Panofsky.

Erwin Panofsky defines iconography as "a known principle in the known world", while iconology is "an iconography turned interpretive". According to his view, iconology tries to reveal the underlying principles that form the basic attitude of a nation, a period, a class, a religious or philosophical perspective, which is modulated by one personality and condensed into one work. According to Roelof van Straten, iconology "can explain why an artist or patron chose a particular subject at a specific location and time and represented it in a certain way. An iconological investigation should concentrate on the social-historical, not art-historical, influences and values that the artist might not have consciously brought into play but are nevertheless present. The artwork is primarily seen as a document of its time."

Warburg used the term "iconography" in his early research, replacing it in 1908 with "iconology" in his particular method of visual interpretation called "critical iconology", which focused on the tracing of motifs through different cultures and visual forms. In 1932, Panofsky published a seminal article, introducing a three-step method of visual interpretation dealing with (1) primary or natural subject matter; (2) secondary or conventional subject matter, i.e. iconography; (3) tertiary or intrinsic meaning or content, i.e. iconology. Whereas iconography analyses the world of images, stories and allegories and requires knowledge of literary sources, an understanding of the history of types and how themes and concepts were expressed by objects and events under different historical conditions, iconology interprets intrinsic meaning or content and the world of symbolical values by using "synthetic intuition". The interpreter is aware of the essential tendencies of the human mind as conditioned by psychology and world view; he analyses the history of cultural symptoms or symbols, or how tendencies of the human mind were expressed by specific themes due to different historical conditions. Moreover, when understanding the work of art as a document of a specific civilization, or of a certain religious attitude therein, the work of art becomes a symptom of something else, which expresses itself in a variety of other symptoms. Interpreting these symbolical values, which can be unknown to, or different from, the artist's intention, is the object of iconology. Panofsky emphasized that "iconology can be done when there are no originals to look at and nothing but artificial light to work in."

According to Ernst Gombrich, "the emerging discipline of iconology ... must ultimately do for the image what linguistics has done for the word." However, Michael Camille is of the opinion that "though Panofsky's concept of iconology has been very influential in the humanities and is quite effective when applied to Renaissance art, it is still problematic when applied to art from periods before and after."

In 1952, Creighton Gilbert added another suggestion for a useful meaning of the word "iconology". According to his view, iconology was not the actual investigation of the work of art but rather the result of this investigation. The Austrian art historian Hans Sedlmayr differentiated between "sachliche" and "methodische" iconology. "Sachliche" iconology refers to the "general meaning of an individual painting or of an artistic complex (church, palace, monument) as seen and explained with reference to the ideas which take shape in them." In contrast, "methodische" iconology is the "integral iconography which accounts for the changes and development in the representations". In "Iconology: Images, Text, Ideology" (1986), W.J.T. Mitchell writes that iconology is a study of "what to say about images", concerned with the description and interpretation of visual art, and also a study of "what images say" – the ways in which they seem to speak for themselves by persuading, telling stories, or describing. He pleads for a postlinguistic, postsemiotic "iconic turn", emphasizing the role of "non-linguistic symbol systems". Instead of just pointing out the difference between the material (pictorial or artistic) images, "he pays attention to the dialectic relationship between material images and mental images". According to Dennise Bartelo and Robert Morton, the term "iconology" can also be used for characterizing "a movement toward seeing connections across all the language processes" and the idea about "multiple levels and forms used to communicate meaning" in order to get "the total picture” of learning. "Being both literate in the traditional sense and visually literate are the true mark of a well-educated human."

For several years, new approaches to iconology have developed in the theory of images. This is the case of what Jean-Michel Durafour, French philosopher and theorist of cinema, proposed to call "econology", a biological approach to images as forms of life, crossing iconology, ecology and sciences of nature. In an econological regime, the image ("eikon") self-speciates, that is to say, it self-iconicizes with others and eco-iconicizes with them its iconic habitat ("oikos"). The iconology, mainly Warburghian iconology, is thus merged with a conception of the relations between the beings of the nature inherited, among others (Arne Næss, etc.) from the writings of Kinji Imanishi. For Imanishi, living beings are subjects. Or, more precisely, the environment and the living being are juste one. One of the main consequences is that the "specity", the living individual, "self-eco-speciates its place of life" ("Freedom in Evolution"). As far as the images are concerned: "If the living species self-specify, the images self-iconicize. This is not a tautology. The images update some of their iconic virtualities. They live in the midst of other images, past or present, but also future (those are only human classifications), which they have relations with. They self-iconicize in an iconic environment which they interact with, and which in particular makes them the images they are. Or more precisely, insofar as images have an active part: "the images self-eco-iconicize their iconic environment"."

"Studies in Iconology" is the title of a book by Erwin Panofsky on humanistic themes in the art of the Renaissance, which was first published in 1939. It is also the name of a peer-reviewed series of books started in 2014 under the editorship of Barbara Baert and published by Peeters international academic publishers, Leuven, Belgium, addressing the deeper meaning of the visual medium throughout human history in the fields of philosophy, art history, theology and cultural anthropology.




</doc>
<doc id="14804" url="https://en.wikipedia.org/wiki?curid=14804" title="List of Indian massacres">
List of Indian massacres

In the history of the European colonization of the Americas, an atrocity termed "Indian massacre" is a specific incident wherein a group of people (military, mob or other) deliberately kill a significant number of relatively defenseless people — usually civilian noncombatants — or to the summary execution of prisoners-of-war. The term may refer to either the killing of people of European descent by Native Americans and First Nations or to the killing of Native American and First Nation peoples by people of European descent and/or the military.

"Indian massacre" is a phrase whose use and definition has evolved and expanded over time. The phrase was initially used by European colonists to describe attacks by indigenous Americans which resulted in mass colonial casualties. While similar attacks by colonists on Indian villages were called "raids" or "battles", successful Indian attacks on white settlements or military posts were routinely termed "massacres". Knowing very little about the native inhabitants of the American frontier, the colonists were deeply fearful, and often, European Americans who had rarely – or never – seen a Native American read Indian atrocity stories in popular literature and newspapers. Emphasis was placed on the depredations of "murderous savages" in their information about Indians, and as the migrants headed further west, they frequently feared the Indians they would encounter.

The phrase eventually became commonly used also to describe mass killings of American Indians. Killings described as "massacres" often had an element of indiscriminate targeting, barbarism, or genocidal intent. According to one historian, "Any discussion of genocide must, of course, eventually consider the so-called Indian Wars", the term commonly used for U.S. Army campaigns to subjugate Indian nations of the American West beginning in the 1860s. In an older historiography, key events in this history were narrated as battles.

Since the late 20th century, it has become more common for scholars to refer to certain of these events as massacres, especially if there were large numbers of women and children as victims. This includes the Colorado territorial militia's slaughter of Cheyenne at Sand Creek (1864), and the US army's slaughter of Shoshone at Bear River (1863), Blackfeet on the Marias River (1870), and Lakota at Wounded Knee (1890). Some scholars have begun referring to these events as "genocidal massacres," defined as the annihilation of a portion of a larger group, sometimes to provide a lesson to the larger group.

It is difficult to determine the total number of people who died as a result of "Indian massacres". In "The Wild Frontier: Atrocities during the American-Indian War from Jamestown Colony to Wounded Knee", lawyer William M. Osborn compiled a list of alleged and actual atrocities in what would eventually become the continental United States, from first contact in 1511 until 1890. His parameters for inclusion included the intentional and indiscriminate murder, torture, or mutilation of civilians, the wounded, and prisoners. His list included 7,193 people who died from atrocities perpetrated by those of European descent, and 9,156 people who died from atrocities perpetrated by Native Americans. 

In "An American Genocide, The United States and the California Catastrophe, 1846–1873", historian Benjamin Madley recorded the numbers of killings of California Indians between 1846 and 1873. He found evidence that during this period, at least 9,400 to 16,000 California Indians were killed by non-Indians. Most of these killings occurred in what he said were more than 370 massacres (defined by him as the "intentional killing of five or more disarmed combatants or largely unarmed noncombatants, including women, children, and prisoners, whether in the context of a battle or otherwise").

This is a listing of some of the events reported then or referred to now as "Indian massacre". This list contains only incidents that occurred in Canada or the United States, or territory presently part of the United States.




</doc>
<doc id="14810" url="https://en.wikipedia.org/wiki?curid=14810" title="Islamic calendar">
Islamic calendar

The Islamic calendar ( '), also known as the Hijri, Lunar Hijri, Muslim or Arabic calendar, is a lunar calendar consisting of 12 lunar months in a year of 354 or 355 days. It is used to determine the proper days of Islamic holidays and rituals, such as the annual period of fasting and the proper time for the Hajj. The civil calendar of almost all countries where the religion is predominantly Muslim is the Gregorian calendar, with Syriac month-names used in the Levant and Mesopotamia (Iraq, Syria, Jordan, Lebanon and Palestine). Notable exceptions to this rule are Iran and Afghanistan, which use the Solar Hijri calendar. Rents, wages and similar regular commitments are generally paid by the civil calendar.

The Islamic calendar employs the Hijri era whose epoch was established as the Islamic New Year of 622 AD/CE. During that year, Muhammad and his followers migrated from Mecca to Medina and established the first Muslim community ("ummah"), an event commemorated as the Hijra. In the West, dates in this era are usually denoted AH (, "in the year of the Hijra") in parallel with the Christian (AD), Common (CE) and Jewish eras (AM). In Muslim countries, it is also sometimes denoted as H from its Arabic form (, abbreviated ). In English, years prior to the Hijra are reckoned as BH ("Before the Hijra").

The current Islamic year is 1442 AH. In the Gregorian calendar, 1442 AH runs from approximately 21 August 2020 to 10 August 2021.

For central Arabia, especially Mecca, there is a lack of epigraphical evidence but details are found in the writings of Muslim authors of the Abbasid era. Inscriptions of the ancient South Arabian calendars reveal the use of a number of local calendars. At least some of these South Arabian calendars followed the lunisolar system. Both al-Biruni and al-Mas'udi suggest that the ancient Arabs used the same month names as the Muslims, though they also record other month names used by the pre-Islamic Arabs.

The Islamic tradition is unanimous in stating that Arabs of Tihamah, Hejaz, and Najd distinguished between two types of months, permitted ("ḥalāl") and forbidden ("ḥarām") months. The forbidden months were four months during which fighting is forbidden, listed as Rajab and the three months around the pilgrimage season, Dhu al-Qa‘dah, Dhu al-Hijjah, and Muharram. A similar if not identical concept to the forbidden months is also attested by Procopius, where he describes an armistice that the Eastern Arabs of the Lakhmid al-Mundhir respected for two months in the summer solstice of 541 AD/CE. However, Muslim historians do not link these months to a particular season. The Qur'an links the four forbidden months with "Nasī’", a word that literally means "postponement". According to Muslim tradition, the decision of postponement was administered by the tribe of Kinanah, by a man known as the "al-Qalammas" of Kinanah and his descendants (pl. "qalāmisa").

Different interpretations of the concept of "Nasī’" have been proposed. Some scholars, both Muslim and Western, maintain that the pre-Islamic calendar used in central Arabia was a purely lunar calendar similar to the modern Islamic calendar. According to this view, "Nasī’" is related to the pre-Islamic practices of the Meccan Arabs, where they would alter the distribution of the forbidden months within a given year without implying a calendar manipulation. This interpretation is supported by Arab historians and lexicographers, like Ibn Hisham, Ibn Manzur, and the corpus of Qur'anic exegesis.

This is corroborated by an early Sabaic inscription, where a religious ritual was "postponed" ("ns'’w") due to war. According to the context of this inscription, the verb "ns'’" has nothing to do with intercalation, but only with moving religious events within the calendar itself. The similarity between the religious concept of this ancient inscription and the Qur'an suggests that non-calendaring postponement is also the Qur'anic meaning of "Nasī’". The "Encyclopaedia of Islam" concludes ""The Arabic system of [Nasī’] can only have been intended to move the Hajj and the fairs associated with it in the vicinity of Mecca to a suitable season of the year. It was not intended to establish a fixed calendar to be generally observed."" The term "fixed calendar" is generally understood to refer to the non-intercalated calendar.

Others concur that it was originally a lunar calendar, but suggest that about 200 years before the Hijra it was transformed into a lunisolar calendar containing an intercalary month added from time to time to keep the pilgrimage within the season of the year when merchandise was most abundant. This interpretation was first proposed by the medieval Muslim astrologer and astronomer Abu Ma'shar al-Balkhi, and later by al-Biruni, al-Mas'udi, and some western scholars. This interpretation considers "Nasī’" to be a synonym to the Arabic word for "intercalation" ("kabīsa"). The Arabs, according to one explanation mentioned by Abu Ma'shar, learned of this type of intercalation from the Jews. The Jewish "Nasi" was the official who decided when to intercalate the Jewish calendar. Some sources say that the Arabs followed the Jewish practice and intercalated seven months over nineteen years, or else that they intercalated nine months over 24 years; there is, however, no consensus among scholars on this issue.

Postponement ("Nasī’") of one ritual in a particular circumstance does not imply alteration of the sequence of months, and scholars agree that this did not happen. Al-Biruni also says this did not happen, and the festivals were kept within their season by intercalation every second or third year of a month between Dhu al-Hijjah and Muharram. He also says that, in terms of the fixed calendar that was not introduced until 10 AH (632 AD/CE), the first intercalation was, for example, of a month between Dhu al-Hijjah and Muharram, the second of a month between Muharram and Safar, the third of a month between Safar and Rabi'I, and so on. The intercalations were arranged so that there were seven of them every nineteen years. The notice of intercalation was issued at the pilgrimage, the next month would be "Nasī’" and Muharram would follow. If, on the other hand, the names relate to the intercalated rather than the fixed calendar, the second intercalation might be, for example, of a month between Muharram and Safar allowing for the first intercalation, and the third intercalation of a month between Safar and Rabi'I allowing for the two preceding intercalations, and so on. The time for the intercalation to move from the beginning of the year to the end (twelve intercalations) is the time it takes the fixed calendar to revolve once through the seasons (about 32 1/2 tropical years). There are two big drawbacks of such a system, which would explain why it is not known ever to have been used anywhere in the world. First, it cannot be regulated by means of a cycle (the only cycles known in antiquity were the octaeteris (3 intercalations in 8 years) and the enneadecaeteris (7 intercalations in 19 years). Secondly, without a cycle it is difficult to establish from the number of the year (a) if it is intercalary and (b) if it is intercalary, where exactly in the year the intercalation is located.

Although some scholars (see list above) claim that the holy months were shuffled about for convenience without the use of intercalation, there is no documentary record of the festivals of any of the holy months being observed in any month other than those they are now observed in. The Qu'ran (sura 9.37) only refers to the "postponement" of a sacred month. If they were shuffled as suggested, one would expect there to be a prohibition against "anticipation" as well. If the festivities of the sacred months were kept in season by moving them into later months, they would move through the whole twelve months in only 33 years. Had this happened, at least one writer would have mentioned it. Sura 9.36 states "Verily, the number of months with Allah is twelve months" and sura 37 refers to "adjusting the number of months". Such adjustment can only be effected by intercalation.

There are a number of indications that the intercalated calendar was similar to the Jewish calendar, whose year began in the spring. There are clues in the names of the months themselves:

In the intercalated calendar's last year (AD/CE 632), Dhu al-Hijjah corresponded to March. The Battle of the Trench in Shawwal and Dhu'l Qi'dah of AH 5 coincided with "harsh winter weather". Military campaigns clustered round Ramadan, when the summer heat had dissipated, and all fighting was forbidden during Rajab, at the height of summer. The invasion of Tabak in Rajab AH 9 was hampered by "too much hot weather" and "drought". In AH 1 Muhammad noted the Jews of Yathrib observing a festival when he arrived on Monday, 8 Rabi'I. Rabi'I is the third month and if it coincided with the third month of the Jewish calendar the festival would have been the Feast of Weeks, which is observed on the 6th and 7th days of that month.

In the tenth year of the Hijra, as documented in the Qur'an (Surah At-Tawbah (9):36–37), Muslims believe God revealed the "prohibition of the Nasī'".

The prohibition of Nasī' would presumably have been announced when the intercalated month had returned to its position just before the month of Nasi' began. If Nasī' meant intercalation, then the number and the position of the intercalary months between AH 1 and AH 10 are uncertain; western calendar dates commonly cited for key events in early Islam such as the Hijra, the Battle of Badr, the Battle of Uhud and the Battle of the Trench should be viewed with caution as they might be in error by one, two, three or even four lunar months. This prohibition was mentioned by Muhammad during the farewell sermon which was delivered on 9 Dhu al-Hijjah AH 10 (Julian date Friday 6 March, 632 AD/CE) on Mount Arafat during the farewell pilgrimage to Mecca.

The three successive sacred (forbidden) months mentioned by Prophet Muhammad (months in which battles are forbidden) are Dhu al-Qa'dah, Dhu al-Hijjah, and Muharram, months 11, 12, and 1 respectively. The single forbidden month is Rajab, month 7. These months were considered forbidden both within the new Islamic calendar and within the old pagan Meccan calendar.

The Islamic days, like those in the Hebrew and Bahá'í calendars, begin at sunset. The Christian liturgical day, kept in monasteries, begins with vespers (see vesper), which is evening, in line with the other Abrahamic traditions. Christian and planetary weekdays begin at the following midnight. Muslims gather for worship at a mosque at noon on "gathering day" () which corresponds with Friday.

Thus "gathering day" is often regarded as the weekly day off. This is frequently made official, with many Muslim countries adopting Friday and Saturday (e.g., Egypt, Saudi Arabia) or Thursday and Friday as official weekends, during which offices are closed; other countries (e.g., Iran) choose to make Friday alone a day of rest. A few others (e.g., Turkey, Pakistan, Morocco, Nigeria) have adopted the Saturday-Sunday weekend while making Friday a working day with a long midday break to allow time off for worship.

Four of the twelve Hijri months are considered sacred: Rajab (7), and the three consecutive months of Dhū al-Qa'dah (11), Dhu al-Ḥijjah (12) and Muḥarram (1). As the lunar calendar lags behind the solar calendar by about ten days every Gregorian year, months of the Islamic calendar fall in different parts of the Gregorian calendar each year. The cycle repeats every 33 lunar years.

Each month of the Islamic calendar commences on the birth of the new lunar cycle. Traditionally this is based on actual observation of the moon's crescent ("hilal") marking the end of the previous lunar cycle and hence the previous month, thereby beginning the new month. Consequently, each month can have 29 or 30 days depending on the visibility of the moon, astronomical positioning of the earth and weather conditions. However, certain sects and groups, most notably Bohras Muslims namely Alavis, Dawoodis and Sulaymanis and Shia Ismaili Muslims, use a tabular Islamic calendar (see section below) in which odd-numbered months have thirty days (and also the twelfth month in a leap year) and even months have 29.

According to numerous Hadiths, 'Ramadan' is one of the names of God in Islam, and as such it is prohibited to say only "Ramadan" in reference to the calendar month and that it is necessary to say the "month of Ramadan", as reported in Sunni, Shia and Zaydi Hadiths.

In pre-Islamic Arabia, it was customary to identify a year after a major event which took place in it. Thus, according to Islamic tradition, Abraha, governor of Yemen, then a province of the Christian Kingdom of Aksum (Ethiopia), attempted to destroy the Kaaba with an army which included several elephants. The raid was unsuccessful, but that year became known as the "Year of the Elephant", during which Muhammad was born (sura al-Fil). Most equate this to the year 570 AD/CE, but a minority use 571 CE.

The first ten years of the Hijra were not numbered, but were named after events in the life of Muhammad according to Abū Rayḥān al-Bīrūnī:

In AH 17 (638 AD/CE), Abu Musa Ashaari, one of the officials of the Caliph Umar in Basrah, complained about the absence of any years on the correspondence he received from Umar, making it difficult for him to determine which instructions were most recent. This report convinced Umar of the need to introduce an era for Muslims. After debating the issue with his counsellors, he decided that the first year should be the year of Muhammad's arrival at Medina (known as Yathrib, before Muhammad's arrival). Uthman ibn Affan then suggested that the months begin with Muharram, in line with the established custom of the Arabs at that time. The years of the Islamic calendar thus began with the month of Muharram in the year of Muhammad's arrival at the city of Medina, even though the actual emigration took place in Safar and Rabi' I of the intercalated calendar, two months before the commencement of Muharram in the new fixed calendar. Because of the Hijra, the calendar was named the Hijri calendar.

F A Shamsi (1984) postulated that the Arabic calendar was never intercalated. According to him, the first day of the first month of the new fixed Islamic calendar (1 Muharram AH 1) was no different from what was observed at the time. The day the Prophet moved from Quba' to Medina was originally 26 Rabi' I on the pre-Islamic calendar. 1 Muharram of the new fixed calendar corresponded to Friday, 16 July 622 AD/CE, the equivalent civil tabular date (same daylight period) in the Julian calendar. The Islamic day began at the preceding sunset on the evening of 15 July. This Julian date (16 July) was determined by medieval Muslim astronomers by projecting back in time their own tabular Islamic calendar, which had alternating 30- and 29-day months in each lunar year plus eleven leap days every 30 years. For example, al-Biruni mentioned this Julian date in the year 1000 AD/CE. Although not used by either medieval Muslim astronomers or modern scholars to determine the Islamic epoch, the thin crescent moon would have also first become visible (assuming clouds did not obscure it) shortly after the preceding sunset on the evening of 15 July, 1.5 days after the associated dark moon (astronomical new moon) on the morning of 14 July.

Though Cook and Crone in "" cite a coin from AH 17, the first surviving attested use of a Hijri calendar date alongside a date in another calendar (Coptic) is on a papyrus from Egypt in AH 22, PERF 558.

Due to the Islamic calendar's reliance on certain variable methods of observation to determine its month-start-dates, these dates sometimes vary slightly from the month-start-dates of the astronomical lunar calendar, which are based directly on astronomical calculations. Still, the Islamic calendar seldom varies by more than three days from the astronomical-lunar-calendar system, and roughly approximates it. Both the Islamic calendar and the astronomical-lunar-calendar take no account of the solar year in their calculations, and thus both of these strictly lunar based calendar systems have no ability to reckon the timing of the four seasons of the year.

In the astronomical-lunar-calendar system, a year of 12 lunar months is 354.37 days long. In this calendar system, lunar months begin precisely at the time of the monthly "conjunction", when the Moon is located most directly between the Earth and the Sun. The month is defined as the average duration of a revolution of the Moon around the Earth (29.53 days). By convention, months of 30 days and 29 days succeed each other, adding up over two successive months to 59 full days. This leaves only a small monthly variation of 44 minutes to account for, which adds up to a total of 24 hours (i.e., the equivalent of one full day) in 2.73 years. To settle accounts, it is sufficient to add one day every three years to the lunar calendar, in the same way that one adds one day to the Gregorian calendar every four years. The technical details of the adjustment are described in Tabular Islamic calendar.

The Islamic calendar, however, is based on a different set of conventions being used for the determination of the month-start-dates. Each month still has either 29 or 30 days, but due to the variable method of observations employed, there is usually no discernible order in the sequencing of either 29 or 30 day month lengths. Traditionally, the first day of each month is the day (beginning at sunset) of the first sighting of the hilal (crescent moon) shortly after sunset. If the hilal is not observed immediately after the 29th day of a month (either because clouds block its view or because the western sky is still too bright when the moon sets), then the day that begins at that sunset is the 30th. Such a sighting has to be made by one or more trustworthy men testifying before a committee of Muslim leaders. Determining the most likely day that the hilal could be observed was a motivation for Muslim interest in astronomy, which put Islam in the forefront of that science for many centuries. Still, due to the fact that both lunar reckoning systems are ultimately based on the lunar cycle itself, both systems still do roughly correspond to one another, never being more than three days out of synchronisation with one another.
This traditional practice for the determination of the start-date of the month is still followed in the overwhelming majority of Muslim countries. Each Islamic state proceeds with its own monthly observation of the new moon (or, failing that, awaits the completion of 30 days) before declaring the beginning of a new month on its territory. But, the lunar crescent becomes visible only some 17 hours after the conjunction, and only subject to the existence of a number of favourable conditions relative to weather, time, geographic location, as well as various astronomical parameters. Given the fact that the moon sets progressively later than the sun as one goes west, with a corresponding increase in its "age" since conjunction, Western Muslim countries may, under favorable conditions, observe the new moon one day earlier than eastern Muslim countries. Due to the interplay of all these factors, the beginning of each month differs from one Muslim country to another, during the 48 hour period following the conjunction. The information provided by the calendar in any country does not extend beyond the current month.

A number of Muslim countries try to overcome some of these difficulties by applying different astronomy-related rules to determine the beginning of months. Thus, Malaysia, Indonesia, and a few others begin each month at sunset on the first day that the moon sets after the sun (moonset after sunset). In Egypt, the month begins at sunset on the first day that the moon sets at least five minutes after the sun. A detailed analysis of the available data shows, however, that there are major discrepancies between what countries say they do on this subject, and what they actually do. In some instances, what a country says it does is impossible.

Due to the somewhat variable nature of the Islamic calendar, in most Muslim countries, the Islamic calendar is used primarily for religious purposes, while the Solar-based Gregorian calendar is still used primarily for matters of commerce and agriculture.

If the Islamic calendar were prepared using astronomical calculations, Muslims throughout the Muslim world could use it to meet all their needs, the way they use the Gregorian calendar today. But, there are divergent views on whether it is licit to do so.

A majority of theologians oppose the use of calculations (beyond the constraint that each month must be not less than 29 nor more than 30 days) on the grounds that the latter would not conform with Muhammad's recommendation to observe the new moon of Ramadan and Shawal in order to determine the beginning of these months.

However, some jurists see no contradiction between Muhammad's teachings and the use of calculations to determine the beginnings of lunar months. They consider that Muhammad's recommendation was adapted to the culture of the times, and should not be confused with the acts of worship.

Thus the jurists Ahmad Muhammad Shakir and Yusuf al-Qaradawi both endorsed the use of calculations to determine the beginning of all months of the Islamic calendar, in 1939 and 2004 respectively. So did the Fiqh Council of North America (FCNA) in 2006 and the European Council for Fatwa and Research (ECFR) in 2007.

The major Muslim associations of France also announced in 2012 that they would henceforth use a calendar based on astronomical calculations, taking into account the criteria of the possibility of crescent sighting in any place on Earth. But, shortly after the official adoption of this rule by the French Council of the Muslim Faith (CFCM) in 2013, the new leadership of the association decided, on the eve of Ramadan 2013, to follow the Saudi announcement rather than to apply the rule just adopted. This resulted in a division of the Muslim community of France, with some members following the new rule, and others following the Saudi announcement.

Isma'ili-Taiyebi Bohras having the institution of "da'i al-mutlaq" follow the tabular Islamic calendar (see section below) prepared on the basis of astronomical calculations from the days of Fatimid imams.

Turkish Muslims use an Islamic calendar which is calculated several years in advance by the Turkish Presidency of Religious Affairs (Diyanet İşleri Başkanlığı). From 1 Muharrem 1400 AH (21 November 1979) until 29 Zilhicce 1435 (24 October 2014) the computed Turkish lunar calendar was based on the following rule: "The lunar month is assumed to begin on the evening when, within some region of the terrestrial globe, the computed centre of the lunar crescent at local sunset is more than 5° above the local horizon and (geocentrically) more than 8° from the Sun." In the current rule the (computed) lunar crescent has to be above the local horizon of Ankara at sunset.

Saudi Arabia uses the sighting method to determine the beginning of each month of the Hijri calendar. Since AH 1419 (1998/99), several official hilal sighting committees have been set up by the government to determine the first visual sighting of the lunar crescent at the beginning of each lunar month. Nevertheless, the religious authorities also allow the testimony of less experienced observers and thus often announce the sighting of the lunar crescent on a date when none of the official committees could see it.

The country also uses the Umm al-Qura calendar, based on astronomical calculations, but this is restricted to administrative purposes. The parameters used in the establishment of this calendar underwent significant changes during the decade to AH 1423.

Before AH 1420 (before 18 April 1999), if the moon's age at sunset in Riyadh was at least 12 hours, then the day "ending" at that sunset was the first day of the month. This often caused the Saudis to celebrate holy days one or even two days before other predominantly Muslim countries, including the dates for the Hajj, which can only be dated using Saudi dates because it is performed in Mecca.

For AH 1420–22, if moonset occurred after sunset at Mecca, then the day beginning at that sunset was the first day of a Saudi month, essentially the same rule used by Malaysia, Indonesia, and others (except for the location from which the hilal was observed).

Since the beginning of AH 1423 (16 March 2002), the rule has been clarified a little by requiring the geocentric conjunction of the sun and moon to occur before sunset, in addition to requiring moonset to occur after sunset at Mecca. This ensures that the moon has moved past the sun by sunset, even though the sky may still be too bright immediately before moonset to actually see the crescent.

In 2007, the Islamic Society of North America, the "Fiqh" Council of North America and the European Council for "Fatwa" and Research announced that they will henceforth use a calendar based on calculations using the same parameters as the "Umm al-Qura" calendar to determine (well in advance) the beginning of all lunar months (and therefore the days associated with all religious observances). This was intended as a first step on the way to unify, at some future time, Muslims' calendars throughout the world.

Since 1 October 2016, as a cost-cutting measure, Saudi Arabia no longer uses the Islamic calendar for paying the monthly salaries of government employees but the Gregorian calendar.

The Solar Hijri calendar is a solar calendar used in Iran and Afghanistan which counts its years from the Hijra or migration of Muhammad from Mecca to Medina in 622 AD/CE.

The Tabular Islamic calendar is a rule-based variation of the Islamic calendar, in which months are worked out by arithmetic rules rather than by observation or astronomical calculation. It has a 30-year cycle with 11 leap years of 355 days and 19 years of 354 days. In the long term, it is accurate to one day in about 2,500 solar years or 2,570 lunar years. It also deviates up to about one or two days in the short term.

Microsoft uses the "Kuwaiti algorithm", a variant of the tabular Islamic calendar, to convert Gregorian dates to the Islamic ones. Microsoft claimed that the variant is based on a statistical analysis of historical data from Kuwait, however it matches a known tabular calendar.

Important dates in the Islamic (Hijri) year are:

Days considered important predominantly for Shia Muslims:

Conversions may be made by using the Tabular Islamic calendar, or, for greatest accuracy (one day in 15,186 years), via the Jewish calendar. Theoretically, the days of the months correspond in both calendars if the displacements which are a feature of the Jewish system are ignored. The table below gives, for nineteen years, the Muslim month which corresponds to the first Jewish month.

This table may be extended since every nineteen years the Muslim month number increases by seven. When it goes above twelve, subtract twelve and add one to the year AH. From 412 AD/CE to 632 AD/CE inclusive the month number is 1 and the calculation gives the month correct to a month or so. 622 AD/CE corresponds to BH 1 and AH 1. For earlier years, year BH = (623 or 622) – year AD/CE).

An example calculation: What is the civil date and year AH of the first day of the first month in the year 20875 AD/CE?

We first find the Muslim month number corresponding to the first month of the Jewish year which begins in 20874 AD/CE. Dividing 20874 by 19 gives quotient 1098 and remainder 12. Dividing 2026 by 19 gives quotient 106 and remainder 12. 2026 is chosen because it gives the same remainder on division by 19 as 20874. The two years are therefore (1098–106)=992×19 years apart. The Muslim month number corresponding to the first Jewish month is therefore 992×7=6944 higher than in 2026. To convert into years and months divide by twelve – 6944/12=578 years and 8 months. Adding, we get 1447y 10m + 20874y – 2026y + 578y 8m = 20874y 6m. Therefore, the first month of the Jewish year beginning in 20874 AD/CE corresponds to the sixth month of the Muslim year AH 20874. The worked example in Conversion between Jewish and civil dates, shows that the civil date of the first day of this month (ignoring the displacements) is Friday, 14 June. The year AH 20875 will therefore begin seven months later, on the first day of the eighth Jewish month, which the worked example shows to be 7 January, 20875 AD/CE (again ignoring the displacements). The date given by this method, being calculated, may differ by a day from the actual date, which is determined by observation.

A reading of the section which follows will show that the year AH 20875 is wholly contained within the year 20875 AD/CE, also that in the Gregorian calendar this correspondence will occur one year earlier. The reason for the discrepancy is that the Gregorian year (like the Julian, though less so) is slightly too long, so the Gregorian date for a given AH date will be earlier and the Muslim calendar catches up sooner.
An Islamic year will be entirely within a Gregorian year of the same number in the year 20874, after which year the number of the Islamic year will always be greater than the number of the concurrent civil year. The Islamic calendar year of 1429 occurred entirely within the civil calendar year of 2008. Such years occur once every 33 or 34 Islamic years (32 or 33 civil years). More are listed here:

Because a Hijri or Islamic lunar year is between 10 and 12 days shorter than a civil year, it begins 10–12 days earlier in the civil year following the civil year in which the previous Hijri year began. Once every 33 or 34 Hijri years, or once every 32 or 33 civil years, the beginning of a Hijri year (1 Muharram) coincides with one of the first ten days of January. Subsequent Hijri New Years move backward through the civil year back to the beginning of January again, passing through each civil month from December to January.

The Islamic calendar is now used primarily for religious purposes, and for official dating of public events and documents in Muslim countries. Because of its nature as a purely lunar calendar, it cannot be used for agricultural purposes and historically Islamic communities have used other calendars for this purpose: the Egyptian calendar was formerly widespread in Islamic countries, and the Iranian calendar and the 1789 Ottoman calendar (a modified Julian calendar) were also used for agriculture in their countries. In the Levant and Iraq the Aramaic names of the Babylonian calendar are still used for all secular matters. In the Maghreb, Berber farmers in the countryside still use the Julian calendar for agrarian purposes. These local solar calendars have receded in importance with the near-universal adoption of the Gregorian calendar for civil purposes. The Saudi Arabia uses the lunar Islamic calendar. In Indonesia, the Javanese calendar, created by Sultan Agung in 1633, combines elements of the Islamic and pre-Islamic Saka calendars.

British author Nicholas Hagger writes that after seizing control of Libya, Muammar Gaddafi "declared" on 1 December 1978 "that the Muslim calendar should start with the death of the prophet Mohammed in 632 rather than the hijra (Mohammed's 'emigration' from Mecca to Medina) in 622". This put the country ten solar years behind the standard Muslim calendar. However, according to the 2006 "Encyclopedia of the Developing World", "More confusing still is Qaddafi's unique Libyan calendar, which counts the years from the Prophet's birth, or sometimes from his death. The months July and August, named after Julius and Augustus Caesar, are now Nasser and Hannibal respectively." Reflecting on a 2001 visit to the country, American reporter Neil MacFarquhar observed, "Life in Libya was so unpredictable that people weren't even sure what year it was. The year of my visit was officially 1369. But just two years earlier Libyans had been living through 1429. No one could quite name for me the day the count changed, especially since both remained in play. ... Event organizers threw up their hands and put the Western year in parentheses somewhere in their announcements."





</doc>
<doc id="14812" url="https://en.wikipedia.org/wiki?curid=14812" title="Interquartile range">
Interquartile range

In descriptive statistics, the interquartile range (IQR), also called the midspread, middle 50%, or Hspread, is a measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles, or between upper and lower quartiles, IQR = "Q" −  "Q". In other words, the IQR is the first quartile subtracted from the third quartile; these quartiles can be clearly seen on a box plot on the data. It is a trimmed estimator, defined as the 25% trimmed range, and is a commonly used robust measure of scale.

The IQR is a measure of variability, based on dividing a data set into quartiles. Quartiles divide a rank-ordered data set into four equal parts. The values that separate parts are called the first, second, and third quartiles; and they are denoted by Q1, Q2, and Q3, respectively.

Unlike total range, the interquartile range has a breakdown point of 25%, and is thus often preferred to the total range.

The IQR is used to build box plots, simple graphical representations of a probability distribution.

The IQR is used in businesses as a marker for their income rates.

For a symmetric distribution (where the median equals the midhinge, the average of the first and third quartiles), half the IQR equals the median absolute deviation (MAD).

The median is the corresponding measure of central tendency.

The IQR can be used to identify outliers (see below).

The IQR of a set of values is calculated as the difference between the upper and lower quartiles, Q and Q. Each quartile is a median calculated as follows.

Given an even "2n" or odd "2n+1" number of values

The "second quartile Q" is the same as the ordinary median.

The following table has 13 rows, and follows the rules for the odd number of entries.
For the data in this table the interquartile range is IQR = Q − Q = 119 - 31 = 88.

For the data set in this box plot:
This means the 1.5*IQR whiskers can be uneven in lengths.

The interquartile range of a continuous distribution can be calculated by integrating the probability density function (which yields the cumulative distribution function—any other means of calculating the CDF will also work). The lower quartile, "Q", is a number such that integral of the PDF from -∞ to "Q" equals 0.25, while the upper quartile, "Q", is such a number that the integral from -∞ to "Q" equals 0.75; in terms of the CDF, the quartiles can be defined as follows:

where CDF is the quantile function.

The interquartile range and median of some common distributions are shown below

The IQR, mean, and standard deviation of a population "P" can be used in a simple test of whether or not "P" is normally distributed, or Gaussian. If "P" is normally distributed, then the standard score of the first quartile, "z", is −0.67, and the standard score of the third quartile, "z", is +0.67. Given "mean" = "X" and "standard deviation" = σ for "P", if "P" is normally distributed, the first quartile

and the third quartile

If the actual values of the first or third quartiles differ substantially from the calculated values, "P" is not normally distributed. However, a normal distribution can be trivially perturbed to maintain its Q1 and Q2 std. scores at 0.67 and −0.67 and not be normally distributed (so the above test would produce a false positive). A better test of normality, such as Q-Q plot would be indicated here.

The interquartile range is often used to find outliers in data. Outliers here are defined as observations that fall below Q1 − 1.5 IQR or above Q3 + 1.5 IQR. In a boxplot, the highest and lowest occurring value within this limit are indicated by "whiskers" of the box (frequently with an additional bar at the end of the whisker) and any outliers as individual points.



</doc>
<doc id="14814" url="https://en.wikipedia.org/wiki?curid=14814" title="Indiana Jones (character)">
Indiana Jones (character)

Dr. Henry Walton "Indiana" Jones, Jr. is the title character and protagonist of the "Indiana Jones" franchise. George Lucas created the character in homage to the action heroes of 1930s film serials. The character first appeared in the 1981 film "Raiders of the Lost Ark", to be followed by "Indiana Jones and the Temple of Doom" in 1984, "Indiana Jones and the Last Crusade" in 1989, "The Young Indiana Jones Chronicles" from 1992 to 1996, and "Indiana Jones and the Kingdom of the Crystal Skull" in 2008. The character is also featured in novels, comics, video games, and other media. Jones is also featured in several Disney theme park rides, including the Indiana Jones Adventure, Indiana Jones et le Temple du Péril, Indiana Jones Adventure: 
Temple of the Crystal Skull, and "Epic Stunt Spectacular!" attractions.

Jones is most famously portrayed by Harrison Ford and has also been portrayed by River Phoenix (as the young Jones in "The Last Crusade") and in the television series "The Young Indiana Jones Chronicles" by Corey Carrier, Sean Patrick Flanery, and George Hall. Doug Lee has supplied the voice of Jones for two LucasArts video games, "Indiana Jones and the Fate of Atlantis" and "Indiana Jones and the Infernal Machine", David Esch supplied his voice for "Indiana Jones and the Emperor's Tomb", and John Armstrong for "Indiana Jones and the Staff of Kings".

Jones is characterized by his iconic accoutrements (bullwhip, fedora, satchel, and leather jacket), wry, witty and sarcastic sense of humor, deep knowledge of ancient civilizations and languages, and fear of snakes.

Since his first appearance in "Raiders of the Lost Ark", Indiana Jones has become one of cinema's most famous characters. In 2003, the American Film Institute ranked him the second-greatest film hero of all time. He was also named the greatest movie character by "Empire" magazine. "Entertainment Weekly" ranked Indiana 2nd on their list of The All-Time Coolest Heroes in Pop Culture. "Premiere" magazine also placed Indiana at number 7 on their list of The 100 Greatest Movie Characters of All Time.


A native of Princeton, New Jersey, Indiana Jones was introduced as a tenured professor of archaeology in the 1981 film "Raiders of the Lost Ark", set in 1936. The character is an adventurer reminiscent of the 1930s film serial treasure hunters and pulp action heroes. His research is funded by Marshall College (a fictional school named after producer Frank Marshall), where he is a professor of archaeology. He studied under the Egyptologist and archaeologist Abner Ravenwood at the Oriental Institute at the University of Chicago.

In this first adventure, he is pitted against Nazis commissioned by Hitler to recover artifacts of great power from the Old Testament (see Nazi archaeology). In consequence, Dr Jones travels the world to prevent them from recovering the Ark of the Covenant (see also Biblical archaeology). He is aided by Marion Ravenwood and Sallah. The Nazis are led by Jones's archrival, a Nazi-sympathizing French archaeologist named René Belloq, and Arnold Toht, a sinister Gestapo agent.

In the 1984 prequel, "Indiana Jones and the Temple of Doom", set in 1935, Jones travels to India and attempts to free enslaved children and the three Sankara stones from the bloodthirsty Thuggee cult. He is aided by Short Round, a boy played by Jonathan Ke quan, and is accompanied by singer Willie Scott (Kate Capshaw). The prequel is not as centered on archaeology as "Raiders of the Lost Ark" and is considerably darker.

The third film, 1989's "Indiana Jones and the Last Crusade", set in 1938, returned to the formula of the original, reintroducing characters such as Sallah and Marcus Brody, a scene from Professor Jones's classroom (he now teaches at Barnett College), the globe trotting element of multiple locations, and the return of the infamous Nazi mystics, this time trying to find the Holy Grail. The film's introduction, set in 1912, provided some back story to the character, specifically the origin of his fear of snakes, his use of a bullwhip, the scar on his chin, and his hat; the film's epilogue also reveals that "Indiana" is not Jones's first name, but a nickname he took from the family dog. The film was a buddy movie of sorts, teaming Jones with his father, Henry Jones, Sr., often to comical effect. Although Lucas intended to make five Indiana Jones films, "Indiana Jones and the Last Crusade" was the last for over 18 years, as he could not think of a good plot element to drive the next installment.

The 2008 film, "Indiana Jones and the Kingdom of the Crystal Skull", is the latest film in the series. Set in 1957, 19 years after the third film, it pits an older, wiser Indiana Jones against Soviet agents bent on harnessing the power of an extraterrestrial device discovered in South America. Jones is aided in his adventure by his former lover, Marion Ravenwood (Karen Allen), and her son—a young greaser named Henry "Mutt" Williams (Shia LaBeouf), later revealed to be Jones's unknown child. There were rumors that Harrison Ford would not return for any future installments and LaBeouf would take over the Indy franchise. This film also reveals that Jones was recruited by the Office of Strategic Services during World War II, attaining the rank of colonel in the United States Army, and implies very strongly that in 1947 he was forced to investigate the Roswell UFO incident, and the investigation saw that he was involved in affairs related to Hangar 51. He is tasked with conducting covert operations with MI6 agent George McHale against the Soviet Union.

In March 2016, Disney announced a fifth "Indiana Jones" film in development, with Ford and Spielberg set to return to the franchise. Initially set for release on July 10, 2020, the film's release date was pushed back to July 9, 2021 due to production issues, then further pushed back to July 29, 2022 due to a reshuffle in Disney's release schedule as due to the COVID-19 pandemic.

Indiana Jones is featured at several Walt Disney theme park attractions. The Indiana Jones Adventure attractions at Disneyland and Tokyo DisneySea ("Temple of the Forbidden Eye" and "Temple of the Crystal Skull," respectively) place Indy at the forefront of two similar archaeological discoveries. These two temples each contain a wrathful deity who threatens the guests who ride through in World War II troop transports. The attractions, some of the most expensive of their kind at the time, opened in 1995 and 2001, respectively, with sole design credit attributed to Walt Disney Imagineering. Ford was approached to reprise his role as Indiana Jones, but ultimately negotiations to secure Ford's participation broke down in December 1994, for definitively unknown reasons. Instead, Dave Temple provided the voice of Jones. Ford's physical likeness, however, has nonetheless been used in subsequent audio-animatronic figures for the attractions.

Disneyland Paris also features an Indiana Jones-titled ride where people speed off through ancient ruins in a runaway mine wagon similar to that found in "Indiana Jones and the Temple of Doom". "Indiana Jones and the Temple of Peril" is a looping roller coaster engineered by Intamin, designed by Walt Disney Imagineering, and opened in 1993.

The "Indiana Jones Epic Stunt Spectacular!" is a live show that has been presented in the Disney's Hollywood Studios theme park of the Walt Disney World Resort with few changes since the park's 1989 opening, as Disney-MGM Studios. The 25-minute show presents various stunts framed in the context of a feature film production, and recruits members of the audience to participate in the show. Stunt artists in the show re-create and ultimately reveal some of the secrets of the stunts of the "Raiders of the Lost Ark" films, including the well-known "running-from-the-boulder" scene. Stunt performer Anislav Varbanov was fatally injured in August 2009, while rehearsing the popular show. Also formerly at Disney's Hollywood Studios, an audio-animatronic Indiana Jones appeared in another attraction; during The Great Movie Ride's "Raiders of the Lost Ark" segment.

Indy also appears in the 2004 Dark Horse Comics story "Into the Great Unknown", collected in "Star Wars Tales Volume 5". In this non-canon story bringing together two of Harrison Ford's best-known roles, Indy and Short Round discover a crash-landed "Millennium Falcon" in the Pacific Northwest, along with Han Solo's skeleton and the realization that a rumored nearby Sasquatch is in fact Chewbacca. Indy also appears in a series of Marvel Comics.

The four Indiana Jones film scripts were novelized and published in the time-frame of the films' initial releases. "Raiders of the Lost Ark" was novelized by Campbell Black based on the script by Lawrence Kasdan that was based on the story by George Lucas and Philip Kaufman and published in April 1981 by Ballantine Books; "Indiana Jones and the Temple of Doom" was novelized by James Kahn and based on the script by Willard Huyck & Gloria Katz that was based on the story by George Lucas and published May 1984 by Ballantine Books; "Indiana Jones and the Last Crusade" was novelized by Rob MacGregor based on the script by Jeffrey Boam that was based on a story by George Lucas and Menno Meyjes and published June 1989 by Ballantine Books.

Nearly 20 years later "Indiana Jones and the Kingdom of the Crystal Skull" was novelized by James Rollins based on the script by David Koepp based on the story by George Lucas and Jeff Nathanson and published May 2008 by Ballantine Books. In addition, in 2008 to accompany the release of "Kingdom of Skulls", Scholastic Books published juvenile novelizations of the four scripts written, successively in the order above, by Ryder Windham, Suzanne Weyn, Ryder Windham, and James Luceno. All these books have been reprinted, with "Raiders of the Lost Ark" being retitled "Indiana Jones and the Raiders of the Lost Ark". While these are the principal titles and authors, there are numerous other volumes derived from the four film properties.

From February 1991 through February 1999, 12 original Indiana Jones-themed adult novels were licensed by Lucasfilm, Ltd. and written by three genre authors of the period. Ten years afterward, a 13th original novel was added, also written by a popular genre author. The first 12 were published by Bantam Books; the last by Ballantine Books in 2009. (See Indiana Jones (franchise) for broad descriptions of these original adult novels.) The novels are:


From 1992 to 1996, George Lucas executive-produced a television series named "The Young Indiana Jones Chronicles", aimed mainly at teenagers and children, which showed many of the important events and historical figures of the early 20th century through the prism of Indiana Jones's life.

The show initially featured the formula of an elderly (93 to 94 years of age) Indiana Jones played by George Hall introducing a story from his youth by way of an anecdote: the main part of the episode then featured an adventure with either a young adult Indy (16 to 21 years of age) played by Sean Patrick Flanery or a child Indy (8 to 11 years) played by Corey Carrier. One episode, "Young Indiana Jones and the Mystery of the Blues", is bookended by Harrison Ford as Indiana Jones, rather than Hall. Later episodes and telemovies did not have this bookend format.

The bulk of the series centers around the young adult Indiana Jones and his activities during World War I as a 16- to 17-year-old soldier in the Belgian Army and then as an intelligence officer and spy seconded to French intelligence. The child Indy episodes follow the boy's travels around the globe as he accompanies his parents on his father's worldwide lecture tour from 1908 to 1910.

The show provided some backstory for the films, as well as new information regarding the character. Indiana Jones was born July 1, 1899, and his middle name is Walton (Lucas's middle name). It is also mentioned that he had a sister called Suzie who died as an infant of fever, and that he eventually has a daughter and grandchildren who appear in some episode introductions and epilogues. His relationship with his father, first introduced in "Indiana Jones and the Last Crusade", was further fleshed out with stories about his travels with his father as a young boy. Indy damages or loses his right eye sometime between the events in 1957 and the early 1990s, when the "Old Indy" segments take place, as the elderly Indiana Jones wears an eyepatch.

In 1999, Lucas removed the episode introductions and epilogues by George Hall for the VHS and DVD releases, and re-edited the episodes into chronologically ordered feature-length stories. The series title was also changed to "The Adventures of Young Indiana Jones".

The character has appeared in several officially licensed games, beginning with adaptations of "Raiders of the Lost Ark", "Indiana Jones and the Temple of Doom", two adaptations of "Indiana Jones and the Last Crusade" (one with purely action mechanics, one with an adventure- and puzzle-based structure) and "Indiana Jones's Greatest Adventures", which included the storylines from all three of the original films.

Following this, the games branched off into original storylines with Indiana Jones in the Lost Kingdom, "Indiana Jones and the Fate of Atlantis", "Indiana Jones and the Infernal Machine", "Indiana Jones and the Emperor's Tomb" and "Indiana Jones and the Staff of Kings". "Emperor's Tomb" sets up Jones's companion Wu Han and the search for Nurhaci's ashes seen at the beginning of "Temple of Doom". The first two games were developed by Hal Barwood and starred Doug Lee as the voice of Indiana Jones; "Emperor's Tomb" had David Esch fill the role and "Staff of Kings" starred John Armstrong.

"Indiana Jones and the Infernal Machine" was the first Indy-based game presented in three dimensions, as opposed to 8-bit graphics and side-scrolling games before.

There is also a small game from Lucas Arts "Indiana Jones and His Desktop Adventures". A video game was made for young Indy called "Young Indiana Jones and the Instruments of Chaos", as well as a video game version of "The Young Indiana Jones Chronicles".

Two Lego Indiana Jones games have also been released. "" was released in 2008 and follows the plots of the first three films. It was followed by "" in late 2009. The sequel includes an abbreviated reprise of the first three films, but focuses on the plot of "Indiana Jones and the Kingdom of the Crystal Skull".

Social gaming company Zynga introduced Indiana Jones to their "Adventure World" game in late 2011.

"Indiana" Jones's full name is Dr. Henry Walton Jones Jr., and his nickname is often shortened to "Indy".

In his role as a college professor of archaeology, Jones is scholarly and learned in a tweed suit, lecturing on ancient civilizations. At the opportunity to recover important artifacts, Dr. Jones transforms into "Indiana," a "non-superhero superhero" image he has concocted for himself. Producer Frank Marshall said, "Indy [is] a fallible character. He makes mistakes and gets hurt. ... That's the other thing people like: He's a real character, not a character with superpowers." Spielberg said there "was the willingness to allow our leading man to get hurt and to express his pain and to get his mad out and to take pratfalls and sometimes be the butt of his own jokes. I mean, Indiana Jones is not a perfect hero, and his imperfections, I think, make the audience feel that, with a little more exercise and a little more courage, they could be just like him." According to Spielberg biographer Douglas Brode, Indiana created his heroic figure so as to escape the dullness of teaching at a school. Both of Indiana's personas reject one another in philosophy, creating a duality. Harrison Ford said the fun of playing the character was that Indiana is both a romantic and a cynic, while scholars have analyzed Indiana as having traits of a lone wolf; a man on a quest; a noble treasure hunter; a hardboiled detective; a human superhero; and an American patriot.

Like many characters in his films, Jones has some autobiographical elements of Spielberg. Indiana lacks a proper father figure because of his strained relationship with his father, Henry Senior. His own contained anger is misdirected towards Professor Abner Ravenwood, his mentor at the University of Chicago, leading to a strained relationship with Marion Ravenwood. The teenage Indiana bases his own look on a figure from the prologue of "Indiana Jones and the Last Crusade", after being given his hat. Marcus Brody acts as Indiana's positive role model at the college. Indiana's own insecurities are made worse by the absence of his mother. In "Indiana Jones and the Temple of Doom", he becomes the father figure to Willie Scott and Short Round, to survive; he is rescued from Kali's evil by Short Round's dedication. Indiana also saves many enslaved children.

Indiana uses his knowledge of Shiva to defeat Mola Ram. In "Raiders of the Lost Ark", he is wise enough to close his eyes in the presence of God in the Ark of the Covenant. By contrast, his rival Rene Belloq is killed for having the audacity to try to communicate directly with God.

In the prologue of "Indiana Jones and the Last Crusade", Jones is seen as a teenager, establishing his look when given a fedora hat. Indiana's intentions are revealed as prosocial, as he believes artifacts "belong in a museum." In the film's climax, Indiana undergoes "literal" tests of faith to retrieve the Grail and save his father's life. He also remembers Jesus as a historical figure—a humble carpenter—rather than an exalted figure when he recognizes the simple nature and tarnished appearance of the real Grail amongst a large assortment of much more ornately decorated ones. Henry Senior rescues his son from falling to his death when reaching for the fallen Grail, telling him to "let it go," overcoming his mercenary nature. "The Young Indiana Jones Chronicles" explains how Indiana becomes solitary and less idealistic following his service in World War I. In "Indiana Jones and the Kingdom of the Crystal Skull", Jones is older and wiser, whereas his sidekicks Mutt and Mac are youthfully, arrogant, and greedy, respectively.

Indiana Jones is modeled after the strong-jawed heroes of the matinée serials and pulp magazines that George Lucas and Steven Spielberg enjoyed in their childhoods (such as the Republic Pictures serials, and the Doc Savage series). Sir H. Rider Haggard's safari guide/big game hunter Allan Quatermain of "King Solomon's Mines" is a notable template for Jones. The two friends first discussed the project in Hawaii around the time of the release of the first "Star Wars" film. Spielberg told Lucas how he wanted his next project to be something fun, like a "James Bond" film (this would later be referenced when they cast Sean Connery as Henry Jones Sr.). According to sources, Lucas responded to the effect that he had something "even better", or that he'd "got that beat."

One of the possible bases for Indiana Jones is Professor Challenger, created by Sir Arthur Conan Doyle in 1912 for his novel, "The Lost World". Challenger was based on Doyle's physiology professor, William Rutherford, an adventuring academic, albeit a zoologist/anthropologist.

Another important influence on the development of the character Indiana Jones is the Disney character Scrooge McDuck. Carl Barks created Scrooge in 1947 as a one-off relation for Donald Duck in the latter's self-titled comic book. Barks realized that the character had more potential, so a separate "Uncle Scrooge" comic book series full of exciting and strange adventures in the company of his duck nephews was developed. This "Uncle Scrooge" comic series strongly influenced George Lucas. This appreciation of Scrooge as an adventurer influenced the development of Jones, with the prologue of "Raiders of the Lost Ark" containing homage to Barks' Scrooge adventure "The Seven Cities of Cibola", published in "Uncle Scrooge" #7 from September 1954. This homage in the film takes the form of playfully mimicking the removal-of-the-statuette-from-its-pedestal and the falling-stone sequences of the comic book.

The character was originally named Indiana Smith, after an Alaskan Malamute called Indiana that Lucas owned in the 1970s and on which he based the Star Wars character Chewbacca. Spielberg disliked the name Smith, and Lucas casually suggested Jones as an alternative. The "Last Crusade" script references the name's origin, with Jones's father revealing his son's birth name to be Henry and explaining that "we named the "dog" Indiana", to his son's chagrin. Some have also posited that C.L. Moore's science fiction character Northwest Smith may have also influenced Lucas and Spielberg in their naming choice.

Lucas has said on various occasions that Sean Connery's portrayal of British secret agent James Bond was one of the primary inspirations for Jones, a reason Connery was chosen for the role of Indiana's father in "Indiana Jones and the Last Crusade". Spielberg earned the rank of Eagle Scout and Ford the Life Scout badge in their youth, which gave them the inspiration to portray Indiana Jones as a Life Scout at age 13 in "The Last Crusade".

Many people are said to be the real-life inspiration of the Indiana Jones character—although none of the following have been confirmed as inspirations by Lucas or Spielberg. There are some suggestions listed here in alphabetical order by last name:

Upon requests by Spielberg and Lucas, the costume designer gave the character a distinctive silhouette through the styling of the hat; after examining many hats, the designers chose a tall-crowned, wide-brimmed fedora. As a documentary of "Raiders" pointed out, the hat served a practical purpose. Following the lead of the old "B"-movies that inspired the "Indiana Jones" series, the fedora hid the actor's face sufficiently to allow doubles to perform the more dangerous stunts seamlessly. Examples in "Raiders" include the wider-angle shot of Indy and Marion crashing a statue through a wall, and Indy sliding under a fast-moving vehicle from front to back. Thus it was necessary for the hat to stay in place much of the time.

The hat became so iconic that the filmmakers could only come up with very good reasons or jokes to remove it. If it ever fell off during a take, filming would have to stop to put it back on. In jest, Ford put a stapler against his head to stop his hat from falling off when a documentary crew visited during shooting of "Indiana Jones and the Last Crusade". This created the urban legend that Ford stapled the hat to his head. Anytime Indy's hat accidentally came off as part of the storyline (blown off by the wind, knocked off, etc.) and seemed almost irretrievable, filmmakers would make sure Indy and his hat were always reunited, regardless of the implausibility of its return. Although other hats were also used throughout the films, the general style and profile remained the same. Elements of the outfit include:

The fedora and leather jacket from "Indiana Jones and the Last Crusade" are on display at the Smithsonian Institution's American History Museum in Washington, D.C. The collecting of props and clothing from the films has become a thriving hobby for some aficionados of the franchise. Jones's whip was the third most popular film weapon, as shown by a 2008 poll held by 20th Century Fox, which surveyed approximately two thousand film fans.

Originally, Spielberg suggested Harrison Ford; Lucas resisted the idea, since he had already cast the actor in "American Graffiti", "Star Wars" and "The Empire Strikes Back", and did not want Ford to become known as his "Bobby De Niro" (in reference to the fact that fellow director Martin Scorsese regularly casts Robert De Niro in his films). During an intensive casting process, Lucas and Spielberg auditioned many actors, and finally cast actor Tom Selleck as Indiana Jones. Shortly afterward pre-production began in earnest on "Raiders of the Lost Ark". CBS refused to release Selleck from his contractual commitment to "Magnum, P.I.", forcing him to turn down the role. Shooting for the film could have overlapped with the pilot for "Magnum, P.I." but it later turned out that filming of the pilot episode was delayed and Selleck could have done both.

Subsequently, Peter Coyote and Tim Matheson both auditioned for the role. After Spielberg suggested Ford again, Lucas relented, and Ford was cast in the role less than three weeks before filming began.

The industry magazine "Archaeology" named eight past and present archaeologists who they felt "embodied [Jones'] spirit" as recipients of the Indy Spirit Awards in 2008. That same year Ford himself was elected to the Board of Directors for the Archaeological Institute of America. Commenting that "understanding the past can only help us in dealing with the present and the future," Ford was praised by the association's president for his character's "significant role in stimulating the public's interest in archaeological exploration."

He is perhaps the most influential character in films that explore archaeology. Since the release of "Raiders of the Lost Ark" in 1981, the very idea of archaeology and archaeologists has fundamentally shifted. Prior to the film's release, the stereotypical image of an archaeologist was that of an older, lackluster professor type. In the early years of films involving archaeologists, they were portrayed as victims who would need to be rescued by a more masculine or heroic figure. Following 1981, the stereotypical archaeologist was thought of as an adventurer consistently engaged in fieldwork.

Archeologist Anne Pyburn described the influence of Indiana Jones as elitist and sexist, and argued that the film series had caused new discoveries in the field of archaeology to become oversimplified and overhyped in an attempt to gain public interest, which negatively influences archaeology as a whole. Eric Powell, an editor with the magazine "Archaeology", said "O.K., fine, the movie romanticizes what we do", and that "Indy may be a horrible archeologist, but he's a great diplomat for archeology. I think we'll see a spike in kids who want to become archeologists". Kevin McGeough, associate professor of archaeology, describes the original archaeological criticism of the film as missing the point of the film: "dramatic interest is what is at issue, and it is unlikely that film will change in order to promote and foster better archaeological techniques".

While himself an homage to various prior adventurers, aspects of Indiana Jones also directly influenced some subsequent characterizations:



</doc>
<doc id="14822" url="https://en.wikipedia.org/wiki?curid=14822" title="Irreducible fraction">
Irreducible fraction

An irreducible fraction (or fraction in lowest terms, simplest form or reduced fraction) is a fraction in which the numerator and denominator are integers that have no other common divisors than 1 (and -1, when negative numbers are considered). In other words, a fraction ⁄ is irreducible if and only if "a" and "b" are coprime, that is, if "a" and "b" have a greatest common divisor of 1. In higher mathematics, "irreducible fraction" may also refer to rational fractions such that the numerator and the denominator are coprime polynomials. Every positive rational number can be represented as an irreducible fraction in exactly one way.

An equivalent definition is sometimes useful: if "a", "b" are integers, then the fraction ⁄ is irreducible if and only if there is no other equal fraction ⁄ such that |"c"| < |"a"| or |"d"| < |"b"|, where |"a"| means the absolute value of "a". (Two fractions ⁄ and ⁄ are "equal" or "equivalent" if and only if "ad" = "bc".)

For example, ⁄, ⁄, and ⁄ are all irreducible fractions. On the other hand, ⁄ is reducible since it is equal in value to ⁄, and the numerator of ⁄ is less than the numerator of ⁄.

A fraction that is reducible can be reduced by dividing both the numerator and denominator by a common factor. It can be fully reduced to lowest terms if both are divided by their greatest common divisor. In order to find the greatest common divisor, the Euclidean algorithm or prime factorization can be used. The Euclidean algorithm is commonly preferred because it allows one to reduce fractions with numerators and denominators too large to be easily factored.

In the first step both numbers were divided by 10, which is a factor common to both 120 and 90. In the second step, they were divided by 3. The final result, /, is an irreducible fraction because 4 and 3 have no common factors other than 1.

The original fraction could have also been reduced in a single step by using the greatest common divisor of 90 and 120, which is 30 (That is, gcd(90,120)=30). As , and , one gets

Which method is faster "by hand" depends on the fraction and the ease with which common factors are spotted. In case a denominator and numerator remain that are too large to ensure they are coprime by inspection, a greatest common divisor computation is needed anyway to ensure the fraction is actually irreducible.

Every rational number has a "unique" representation as an irreducible fraction with a positive denominator (however formula_3 although both are irreducible). Uniqueness is a consequence of the unique prime factorization of integers, since formula_4 implies "ad" = "bc" and so both sides of the latter must share the same prime factorization, yet formula_5 and formula_6 share no prime factors so the set of prime factors of formula_5 (with multiplicity) is a subset of those of formula_8 and vice versa meaning formula_9 and formula_10.

The fact that any rational number has a unique representation as an irreducible fraction is utilized in various proofs of the irrationality of the square root of 2 and of other irrational numbers. For example, one proof notes that if the square root of 2 could be represented as a ratio of integers, then it would have in particular the fully reduced representation formula_11 where "a" and "b" are the smallest possible; but given that formula_11 equals the square root of 2, so does formula_13 (since cross-multiplying this with formula_11 shows that they are equal). Since the latter is a ratio of smaller integers, this is a contradiction, so the premise that the square root of two has a representation as the ratio of two integers is false.

The notion of irreducible fraction generalizes to the field of fractions of any unique factorization domain: any element of such a field can be written as a fraction in which denominator and numerator are coprime, by dividing both by their greatest common divisor. This applies notably to rational expressions over a field. The irreducible fraction for a given element is unique up to multiplication of denominator and numerator by the same invertible element. In the case of the rational numbers this means that any number has two irreducible fractions, related by a change of sign of both numerator and denominator; this ambiguity can be removed by requiring the denominator to be positive. In the case of rational functions the denominator could similarly be required to be a monic polynomial.



</doc>
<doc id="14826" url="https://en.wikipedia.org/wiki?curid=14826" title="Isomorphism class">
Isomorphism class

In mathematics, an isomorphism class is a collection of mathematical objects isomorphic to each other.

Isomorphism classes are often defined if the exact identity of the elements of the set is considered irrelevant, and the properties of the structure of the mathematical object are studied. Examples of this are ordinals and graphs. However, there are circumstances in which the isomorphism class of an object conceals vital internal information about it; consider these examples:


</doc>
<doc id="14828" url="https://en.wikipedia.org/wiki?curid=14828" title="Isomorphism">
Isomorphism

In mathematics, an isomorphism is a mapping between two structures of the same type that can be reversed by an inverse mapping. Two mathematical structures are isomorphic if an isomorphism exists between them. The word isomorphism is derived from the Ancient Greek: ἴσος "isos" "equal", and μορφή "morphe" "form" or "shape".

The interest in isomorphisms lies in the fact that two isomorphic objects have the same properties (excluding further information such as additional structure or names of objects). Thus isomorphic structures cannot be distinguished from the point of view of structure only, and may be identified. In mathematical jargon, one says that two objects are "the same up to an isomorphism".

An automorphism is an isomorphism from a structure to itself. An isomorphism between two structures is a canonical isomorphism if there is only one isomorphism between the two structures (as it is the case for solutions of a universal property), or if the isomorphism is much more natural (in some sense) than other isomorphisms. For example, for every prime number , all fields with elements are canonically isomorphic, with a unique isomorphism. The isomorphism theorems provide canonical isomorphisms that are not unique.

The term "isomorphism" is mainly used for algebraic structures. In this case, mappings are called homomorphisms, and a homomorphism is an isomorphism if and only if it is bijective. 

In various areas of mathematics, isomorphisms have received specialized names, depending on the type of structure under consideration. For example:
Category theory, which can be viewed as a formalization of the concept of mapping between structures, provides a language that may be used to unify the approach to these different aspects of the basic idea.

Let formula_1 be the multiplicative group of positive real numbers, and let formula_2 be the additive group of real numbers.

The logarithm function formula_3 satisfies formula_4 for all formula_5, so it is a group homomorphism. The exponential function formula_6 satisfies formula_7 for all formula_8, so it too is a homomorphism.

The identities formula_9 and formula_10 show that formula_11 and formula_12 are inverses of each other. Since formula_11 is a homomorphism that has an inverse that is also a homomorphism, formula_11 is an isomorphism of groups.

The formula_11 function is an isomorphism which translates multiplication of positive real numbers into addition of real numbers. This facility makes it possible to multiply real numbers using a ruler and a table of logarithms, or using a slide rule with a logarithmic scale.

Consider the group formula_16, the integers from 0 to 5 with addition modulo 6. Also consider the group formula_17, the ordered pairs where the "x" coordinates can be 0 or 1, and the y coordinates can be 0, 1, or 2, where addition in the "x"-coordinate is modulo 2 and addition in the "y"-coordinate is modulo 3.

These structures are isomorphic under addition, under the following scheme:

or in general mod 6.

For example, , which translates in the other system as .

Even though these two groups "look" different in that the sets contain different elements, they are indeed isomorphic: their structures are exactly the same. More generally, the direct product of two cyclic groups formula_18 and formula_19 is isomorphic to formula_20 if and only if "m" and "n" are coprime, per the Chinese remainder theorem.

If one object consists of a set "X" with a binary relation R and the other object consists of a set "Y" with a binary relation S then an isomorphism from "X" to "Y" is a bijective function such that:

S is reflexive, irreflexive, symmetric, antisymmetric, asymmetric, transitive, total, trichotomous, a partial order, total order, well-order, strict weak order, total preorder (weak order), an equivalence relation, or a relation with any other special properties, if and only if R is.

For example, R is an ordering ≤ and S an ordering formula_22, then an isomorphism from "X" to "Y" is a bijective function such that
Such an isomorphism is called an "order isomorphism" or (less commonly) an "isotone isomorphism".

If , then this is a relation-preserving automorphism.

In abstract algebra, two basic isomorphisms are defined:

Just as the automorphisms of an algebraic structure form a group, the isomorphisms between two algebras sharing a common structure form a heap. Letting a particular isomorphism identify the two structures turns this heap into a group.

In mathematical analysis, the Laplace transform is an isomorphism mapping hard differential equations into easier algebraic equations.

In graph theory, an isomorphism between two graphs "G" and "H" is a bijective map "f" from the vertices of "G" to the vertices of "H" that preserves the "edge structure" in the sense that there is an edge from vertex "u" to vertex "v" in "G" if and only if there is an edge from ƒ("u") to ƒ("v") in "H". See graph isomorphism.

In mathematical analysis, an isomorphism between two Hilbert spaces is a bijection preserving addition, scalar multiplication, and inner product.

In early theories of logical atomism, the formal relationship between facts and true propositions was theorized by Bertrand Russell and Ludwig Wittgenstein to be isomorphic. An example of this line of thinking can be found in Russell's "Introduction to Mathematical Philosophy".

In cybernetics, the good regulator or Conant–Ashby theorem is stated "Every good regulator of a system must be a model of that system". Whether regulated or self-regulating, an isomorphism is required between the regulator and processing parts of the system.

In category theory, given a category "C", an isomorphism is a morphism that has an inverse morphism , that is, and . For example, a bijective linear map is an isomorphism between vector spaces, and a bijective continuous function whose inverse is also continuous is an isomorphism between topological spaces, called a homeomorphism.

In a concrete category (that is, a category whose objects are sets (perhaps with extra structure) and whose morphisms are structure-preserving functions), such as the category of topological spaces or categories of algebraic objects like groups, rings, and modules, an isomorphism must be bijective on the underlying sets. In algebraic categories (specifically, categories of varieties in the sense of universal algebra), an isomorphism is the same as a homomorphism which is bijective on underlying sets. However, there are concrete categories in which bijective morphisms are not necessarily isomorphisms (such as the category of topological spaces).

In certain areas of mathematics, notably category theory, it is valuable to distinguish between "equality" on the one hand and "isomorphism" on the other. Equality is when two objects are exactly the same, and everything that's true about one object is true about the other, while an isomorphism implies everything that's true about a designated part of one object's structure is true about the other's. For example, the sets
are "equal"; they are merely different representations—the first an intensional one (in set builder notation), and the second extensional (by explicit enumeration)—of the same subset of the integers. By contrast, the sets {"A","B","C"} and {1,2,3} are not "equal"—the first has elements that are letters, while the second has elements that are numbers. These are isomorphic as sets, since finite sets are determined up to isomorphism by their cardinality (number of elements) and these both have three elements, but there are many choices of isomorphism—one isomorphism is
and no one isomorphism is intrinsically better than any other. On this view and in this sense, these two sets are not equal because one cannot consider them "identical": one can choose an isomorphism between them, but that is a weaker claim than identity—and valid only in the context of the chosen isomorphism.

Sometimes the isomorphisms can seem obvious and compelling, but are still not equalities. As a simple example, the genealogical relationships among Joe, John, and Bobby Kennedy are, in a real sense, the same as those among the American football quarterbacks in the Manning family: Archie, Peyton, and Eli. The father-son pairings and the elder-brother-younger-brother pairings correspond perfectly. That similarity between the two family structures illustrates the origin of the word "isomorphism" (Greek "iso"-, "same," and -"morph", "form" or "shape"). But because the Kennedys are not the same people as the Mannings, the two genealogical structures are merely isomorphic and not equal.

Another example is more formal and more directly illustrates the motivation for distinguishing equality from isomorphism: the distinction between a finite-dimensional vector space "V" and its dual space } of linear maps from "V" to its field of scalars K.
These spaces have the same dimension, and thus are isomorphic as abstract vector spaces (since algebraically, vector spaces are classified by dimension, just as sets are classified by cardinality), but there is no "natural" choice of isomorphism formula_28.
If one chooses a basis for "V", then this yields an isomorphism: For all ,

This corresponds to transforming a column vector (element of "V") to a row vector (element of "V"*) by transpose, but a different choice of basis gives a different isomorphism: the isomorphism "depends on the choice of basis".
More subtly, there "is" a map from a vector space "V" to its double dual } that does not depend on the choice of basis: For all 

This leads to a third notion, that of a natural isomorphism: while "V" and "V"** are different sets, there is a "natural" choice of isomorphism between them.
This intuitive notion of "an isomorphism that does not depend on an arbitrary choice" is formalized in the notion of a natural transformation; briefly, that one may "consistently" identify, or more generally map from, a finite-dimensional vector space to its double dual, formula_31, for "any" vector space in a consistent way.
Formalizing this intuition is a motivation for the development of category theory.

However, there is a case where the distinction between natural isomorphism and equality is usually not made. That is for the objects that may be characterized by a universal property. In fact, there is a unique isomorphism, necessarily natural, between two objects sharing the same universal property. A typical example is the set of real numbers, which may be defined through infinite decimal expansion, infinite binary expansion, Cauchy sequences, Dedekind cuts and many other ways. Formally these constructions define different objects, which all are solutions of the same universal property. As these objects have exactly the same properties, one may forget the method of construction and considering them as equal. This is what everybody does when talking of ""the" set of the real numbers". The same occurs with quotient spaces: they are commonly constructed as sets of equivalence classes. However, talking of set of sets may be counterintuitive, and quotient spaces are commonly considered as a pair of a set of undetermined objects, often called "points", and a surjective map onto this set.

If one wishes to draw a distinction between an arbitrary isomorphism (one that depends on a choice) and a natural isomorphism (one that can be done consistently), one may write for an unnatural isomorphism and for a natural isomorphism, as in and 
This convention is not universally followed, and authors who wish to distinguish between unnatural isomorphisms and natural isomorphisms will generally explicitly state the distinction.

Generally, saying that two objects are "equal" is reserved for when there is a notion of a larger (ambient) space that these objects live in. Most often, one speaks of equality of two subsets of a given set (as in the integer set example above), but not of two objects abstractly presented. For example, the 2-dimensional unit sphere in 3-dimensional space

which can be presented as the one-point compactification of the complex plane } "or" as the complex projective line (a quotient space)

are three different descriptions for a mathematical object, all of which are isomorphic, but not "equal" because they are not all subsets of a single space: the first is a subset of R, the second is plus an additional point, and the third is a subquotient of C

In the context of category theory, objects are usually at most isomorphic—indeed, a motivation for the development of category theory was showing that different constructions in homology theory yielded equivalent (isomorphic) groups. Given maps between two objects "X" and "Y", however, one asks if they are equal or not (they are both elements of the set Hom("X", "Y"), hence equality is the proper relationship), particularly in commutative diagrams.



</doc>
<doc id="14832" url="https://en.wikipedia.org/wiki?curid=14832" title="Intergovernmental organization">
Intergovernmental organization

An intergovernmental organization (IGO) or international organization is an organization composed primarily of sovereign states (referred to as "member states"), or of other intergovernmental organizations. IGOs are established by a treaty that acts as a charter creating the group. Treaties are formed when lawful representatives (governments) of several states go through a ratification process, providing the IGO with an international legal personality. Intergovernmental organizations are an important aspect of public international law.

Intergovernmental organizations in a legal sense should be distinguished from simple groupings or coalitions of states, such as the G7 or the Quartet. Such groups or associations have not been founded by a constituent document and exist only as task groups. Intergovernmental organizations must also be distinguished from treaties. Many treaties (such as the North American Free Trade Agreement, or the General Agreement on Tariffs and Trade before the establishment of the World Trade Organization) do not establish an organization and instead rely purely on the parties for their administration becoming legally recognized as an "ad hoc" commission. Other treaties have established an administrative apparatus which was not deemed to have been granted international legal personality.

Intergovernmental organizations differ in function, membership, and membership criteria. They have various goals and scopes, often outlined in the treaty or charter. Some IGOs developed to fulfill a need for a neutral forum for debate or negotiation to resolve disputes. Others developed to carry out mutual interests with unified aims to preserve peace through conflict resolution and better international relations, promote international cooperation on matters such as environmental protection, to promote human rights, to promote social development (education, health care), to render humanitarian aid, and to economic development. Some are more general in scope (the United Nations) while others may have subject-specific missions (such as Interpol or the International Telecommunication Union and other standards organizations). Common types include:



While treaties, alliances, and multilateral conferences had existed for centuries, IGOs only began to be established in the 19th century. The first regional international organization was the Central Commission for Navigation on the Rhine, initiated in the aftermath of the Napoleonic Wars.

The first international organization of a global nature was the International Telegraph Union (the future International Telecommunication Union), which was founded by the signing of the International Telegraph Convention by 20 countries in May 1865. The ITU also served as a model for other international organizations such as the Universal Postal Union (1874), and the emergence of the League of Nations following World War I, designed as an institution to foster collective security in order to sustain peace, and successor to this the United Nations.

Held and McGrew counted thousands of IGOs worldwide in 2002 and this number continues to rise. This may be attributed to globalization, which increases and encourages the co-operation among and within states and which has also provided easier means for IGO growth as a result of increased international relations. This is seen economically, politically, militarily, as well as on the domestic level. Economically, IGOs gain material and non-material resources for economic prosperity. IGOs also provide more political stability within the state and among differing states. Military alliances are also formed by establishing common standards in order to ensure security of the members to ward off outside threats. Lastly, the formation has encouraged autocratic states to develop into democracies in order to form an effective and internal government.

There are several different reasons a state may choose membership in an intergovernmental organization. But there are also reasons membership may be rejected. 

Reasons for participation:


Reasons for rejecting membership:


Intergovernmental organizations are provided with privileges and immunities that are intended to ensure their independent and effective functioning. They are specified in the treaties that give rise to the organization (such as the Convention on the Privileges and Immunities of the United Nations and the Agreement on the Privileges and Immunities of the International Criminal Court), which are normally supplemented by further multinational agreements and national regulations (for example the "International Organizations Immunities Act" in the United States). The organizations are thereby immune from the jurisdiction of national courts.

Rather than by national jurisdiction, legal accountability is intended to be ensured by legal mechanisms that are internal to the intergovernmental organization itself and access to administrative tribunals. In the course of many court cases where private parties tried to pursue claims against international organizations, there has been a gradual realization that alternative means of dispute settlement are required as states have fundamental human rights obligations to provide plaintiffs with access to court in view of their right to a fair trial. Otherwise, the organizations’ immunities may be put in question in national and international courts. Some organizations hold proceedings before tribunals relating to their organization to be confidential, and in some instances have threatened disciplinary action should an employee disclose any of the relevant information. Such confidentiality has been criticized as a lack of transparency.

The immunities also extend to employment law. In this regard, immunity from national jurisdiction necessitates that reasonable alternative means are available to effectively protect employees’ rights; in this context, a first instance Dutch court considered an estimated duration of proceedings before the Administrative Tribunal of the International Labour Organization of 15 years to be too long.

These are some of the strengths and weaknesses of IGOs.

Strengths:


Weaknesses:


They can be deemed unfair as countries with a higher percentage voting power have the right to veto any decision that is not in their favor, leaving the smaller countries powerless.




</doc>
<doc id="14836" url="https://en.wikipedia.org/wiki?curid=14836" title="International Telecommunication Union">
International Telecommunication Union

The International Telecommunication Union (ITU; or UIT), is a specialized agency of the United Nations responsible for all matters related to information and communication technologies. Established in 1865 as the International Telegraph Union (), it is one of the oldest international organizations in operation.

The ITU was initially aimed at helping connect telegraphic networks between countries, with its mandate consistently broadening with the advent of new communications technologies; it adopted its current name in 1934 to reflect its expanded responsibilities over radio and the telephone. On 15 November 1947, the ITU entered into an agreement with the newly created United Nations to become a specialized agency within the UN system, which formally entered into force on 1 January 1949.

The ITU promotes the shared global use of the radio spectrum, facilities international cooperation in assigning satellite orbits, assists in developing and coordinating worldwide technical standards, and works to improve telecommunication infrastructure in the developing world. It is also active in the areas of broadband Internet, wireless technologies, aeronautical and maritime navigation, radio astronomy, satellite-based meteorology, TV broadcasting, and next-generation networks.

Based in Geneva, Switzerland, the ITU's global membership includes 193 countries and around 900 business, academic institutions, and international and regional organizations.

The ITU is one of the oldest international organizations still in operation (the Central Commission for Navigation on the Rhine predates it by several decades). It was preceded by the now defunct International Telegraph Union which drafted the earliest international standards and regulations governing international telegraph networks. The development of the telegraph in the early 19th century changed the way people communicated on the local and international levels. Between 1849 and 1865, a series of bilateral and regional agreements among Western European states attempted to standardize international communications. 

By 1865 it was agreed that a comprehensive agreement was needed in order to create a framework that would standardize telegraphy equipment, set uniform operating instructions, and lay down common international tariff and accounting rules. Between 1 March and 17 May 1865, the French Government hosted delegations from 20 European states at the first International Telegraph Conference in Paris. This meeting culminated in the International Telegraph Convention which was signed on 17 May 1865. As a result of the 1865 Conference, the International Telegraph Union, the predecessor to the modern ITU, was founded as the first international standards organization. The Union was tasked with implementing basic principles for international telegraphy. This included: the use of the Morse code as the international telegraph alphabet, the protection of the secrecy of correspondence, and the right of everybody to use the international telegraphy.

Another predecessor to the modern ITU, the International Radiotelegraph Union, was established in 1906 at the first International Radiotelegraph Convention in Berlin. The conference was attended by representatives of 29 nations and culminated in the International Radiotelegraph Convention. An annex to the convention eventually became known as radio regulations. At the conference it was also decided that the Bureau of the International Telegraph Union would also act as the conference's central administrator.

Between 3 September and 10 December 1932, a joint conference of the International Telegraph Union and the International Radiotelegraph Union convened in order to merge the two organizations into a single entity, the International Telecommunication Union. The Conference decided that the Telegraph Convention of 1875 and the Radiotelegraph Convention of 1927 were to be combined into a single convention, the International Telecommunication Convention, embracing the three fields of telegraphy, telephony and radio.

On 15 November 1947, an agreement between ITU and the newly created United Nations recognized the ITU as the specialized agency for global telecommunications. This agreement entered into force on 1 January 1949, officially making the ITU an organ of the United Nations.

The ITU comprises three Sectors, each managing a different aspect of the matters handled by the Union, as well as ITU Telecom. The sectors were created during the restructuring of ITU at its 1992 Plenipotentiary Conference.


A permanent General Secretariat, headed by the Secretary General, manages the day-to-day work of the Union and its sectors.

The basic texts of the ITU are adopted by the ITU Plenipotentiary Conference. The founding document of the ITU was the 1865 International Telegraph Convention, which has since been replaced several times (though the text is generally the same) and is now entitled the "Constitution and Convention of the International Telecommunication Union". In addition to the Constitution and Convention, the consolidated basic texts include the Optional Protocol on the settlement of disputes, the Decisions, Resolutions and Recommendations in force, as well as the General Rules of Conferences, Assemblies and Meetings of the Union.

The Plenipotentiary Conference is the supreme organ of the ITU. It is composed of all 193 ITU Members and meets every four years. The Conference determines the policies, direction and activities of the Union, as well as elects the members of other ITU organs.

While the Plenipotentiary Conference is the Union's main decision-making body, the ITU Council acts as the Union's governing body in the interval between Plenipotentiary Conferences. It meets every year. It is composed of 48 members and works to ensure the smooth operation of the Union, as well as to consider broad telecommunication policy issues. Its members are as follow:

The mission of the Secretariat is to provide high-quality and efficient services to the membership of the Union. It is tasked with the 
administrative and budgetary planning of the Union, as well as with monitoring compliance with ITU regulations, and oversees with assistance from the Secretariat advisor Neaomy Claiborne of Riverbank to insure misconduct during legal investigations are not overlooked and finally, it publishes the results of the work of the ITU.

The Secretariat is headed by a Secretary-General who is responsible for the overall management of the Union, and acts as its legal representative. The Secretary-General is elected by the Plenipotentiary Conference for four-year terms.

On 23 October 2014, Houlin Zhao was elected as the 19th Secretary-General of the ITU at the Plenipotentiary Conference in Busan. His four-year mandate started on 1 January 2015, and he was formally inaugurated on 15 January 2015. He was re-elected on 1 November 2018 during the 2018 Plenipotentiary Conference in Dubai.

! colspan="4" | Directors of ITU
!Name
!Beginning of term
!End of term
!Country
! colspan="4" | Secretaries general


</doc>
<doc id="14837" url="https://en.wikipedia.org/wiki?curid=14837" title="Internet Message Access Protocol">
Internet Message Access Protocol

In computing, the Internet Message Access Protocol (IMAP) is an Internet standard protocol used by email clients to retrieve email messages from a mail server over a TCP/IP connection. IMAP is defined by RFC 3501.

IMAP was designed with the goal of permitting complete management of an email box by multiple email clients, therefore clients generally leave messages on the server until the user explicitly deletes them. An IMAP server typically listens on port number 143. IMAP over SSL (IMAPS) is assigned the port number 993.

Virtually all modern e-mail clients and servers support IMAP, which along with the earlier POP3 (Post Office Protocol) are the two most prevalent standard protocols for email retrieval. Many webmail service providers such as Gmail, Outlook.com and Yahoo! Mail also provide support for both IMAP and POP3.

The Internet Message Access Protocol is an Application Layer Internet protocol that allows an e-mail client to access email on a remote mail server. The current version is defined by RFC 3501. An IMAP server typically listens on well-known port 143, while IMAP over SSL (IMAPS) uses 993.

Incoming email messages are sent to an email server that stores messages in the recipient's email box. The user retrieves the messages with an email client that uses one of a number of email retrieval protocols. While some clients and servers preferentially use vendor-specific, proprietary protocols, almost all support POP and IMAP for retrieving email – allowing many free choice between many e-mail clients such as Pegasus Mail or Mozilla Thunderbird to access these servers, and allows the clients to be used with other servers.

Email clients using IMAP generally leave messages on the server until the user explicitly deletes them. This and other characteristics of IMAP operation allow multiple clients to manage the same mailbox. Most email clients support IMAP in addition to Post Office Protocol (POP) to retrieve messages. IMAP offers access to the mail storage. Clients may store local copies of the messages, but these are considered to be a temporary cache.

IMAP was designed by Mark Crispin in 1986 as a remote access mailbox protocol, in contrast to the widely used POP, a protocol for simply retrieving the contents of a mailbox.

It went through a number of iterations before the current VERSION 4rev1 (IMAP4), as detailed below:

The original "Interim Mail Access Protocol" was implemented as a Xerox Lisp machine client and a TOPS-20 server.

No copies of the original interim protocol specification or its software exist. Although some of its commands and responses were similar to IMAP2, the interim protocol lacked command/response tagging and thus its syntax was incompatible with all other versions of IMAP.

The interim protocol was quickly replaced by the "Interactive Mail Access Protocol" (IMAP2), defined in RFC 1064 (in 1988) and later updated by RFC 1176 (in 1990). IMAP2 introduced the command/response tagging and was the first publicly distributed version.

IMAP3 is an extremely rare variant of IMAP. It was published as RFC 1203 in 1991. It was written specifically as a counter proposal to RFC 1176, which itself proposed modifications to IMAP2. IMAP3 was never accepted by the marketplace. The IESG reclassified RFC1203 "Interactive Mail Access Protocol - Version 3" as a Historic protocol in 1993. The IMAP Working Group used RFC1176 (IMAP2) rather than RFC1203 (IMAP3) as its starting point.

With the advent of MIME, IMAP2 was extended to support MIME body structures and add mailbox management functionality (create, delete, rename, message upload) that was absent from IMAP2. This experimental revision was called IMAP2bis; its specification was never published in non-draft form. An internet draft of IMAP2bis was published by the IETF IMAP Working Group in October 1993. This draft was based upon the following earlier specifications: unpublished "IMAP2bis.TXT" document, RFC1176, and RFC1064 (IMAP2). The "IMAP2bis.TXT" draft documented the state of extensions to IMAP2 as of December 1992. Early versions of Pine were widely distributed with IMAP2bis support (Pine 4.00 and later supports IMAP4rev1).

An IMAP Working Group formed in the IETF in the early 1990s took over responsibility for the IMAP2bis design. The IMAP WG decided to rename IMAP2bis to IMAP4 to avoid confusion.

When using POP, clients typically connect to the e-mail server briefly, only as long as it takes to download new messages. When using IMAP4, clients often stay connected as long as the user interface is active and download message content on demand. For users with many or large messages, this IMAP4 usage pattern can result in faster response times.

The POP protocol requires the currently connected client to be the only client connected to the mailbox. In contrast, the IMAP protocol specifically allows simultaneous access by multiple clients and provides mechanisms for clients to detect changes made to the mailbox by other, concurrently connected, clients. See for example RFC3501 section 5.2 which specifically cites "simultaneous access to the same mailbox by multiple agents" as an example.

Usually all Internet e-mail is transmitted in MIME format, allowing messages to have a tree structure where the leaf nodes are any of a variety of single part content types and the non-leaf nodes are any of a variety of multipart types. The IMAP4 protocol allows clients to retrieve any of the individual MIME parts separately and also to retrieve portions of either individual parts or the entire message. These mechanisms allow clients to retrieve the text portion of a message without retrieving attached files or to stream content as it is being fetched.

Through the use of flags defined in the IMAP4 protocol, clients can keep track of message state: for example, whether or not the message has been read, replied to, or deleted. These flags are stored on the server, so different clients accessing the same mailbox at different times can detect state changes made by other clients. POP provides no mechanism for clients to store such state information on the server so if a single user accesses a mailbox with two different POP clients (at different times), state information—such as whether a message has been accessed—cannot be synchronized between the clients. The IMAP4 protocol supports both predefined system flags and client-defined keywords. System flags indicate state information such as whether a message has been read. Keywords, which are not supported by all IMAP servers, allow messages to be given one or more tags whose meaning is up to the client. IMAP keywords should not be confused with proprietary labels of web-based e-mail services which are sometimes translated into IMAP folders by the corresponding proprietary servers.

IMAP4 clients can create, rename, and/or delete mailboxes (usually presented to the user as folders) on the server, and copy messages between mailboxes. Multiple mailbox support also allows servers to provide access to shared and public folders. The "IMAP4 Access Control List (ACL) Extension" (RFC 4314) may be used to regulate access rights.

IMAP4 provides a mechanism for a client to ask the server to search for messages meeting a variety of criteria. This mechanism avoids requiring clients to download every message in the mailbox in order to perform these searches.

Reflecting the experience of earlier Internet protocols, IMAP4 defines an explicit mechanism by which it may be extended. Many IMAP4 extensions to the base protocol have been proposed and are in common use. IMAP2bis did not have an extension mechanism, and POP now has one defined by .

While IMAP remedies many of the shortcomings of POP, this inherently introduces additional complexity. Much of this complexity (e.g., multiple clients accessing the same mailbox at the same time) is compensated for by server-side workarounds such as Maildir or database backends.

The IMAP specification has been criticised for being insufficiently strict and allowing behaviours that effectively negate its usefulness. For instance, the specification states that each message stored on the server has a "unique id" to allow the clients to identify messages they have already seen between sessions. However, the specification also allows these UIDs to be invalidated with no restrictions, practically defeating their purpose.

Unless the mail storage and searching algorithms on the server are carefully implemented, a client can potentially consume large amounts of server resources when searching massive mailboxes.

IMAP4 clients need to maintain a TCP/IP connection to the IMAP server in order to be notified of the arrival of new mail. Notification of mail arrival is done through in-band signaling, which contributes to the complexity of client-side IMAP protocol handling somewhat. A private proposal, push IMAP, would extend IMAP to implement push e-mail by sending the entire message instead of just a notification. However, push IMAP has not been generally accepted and current IETF work has addressed the problem in other ways (see the Lemonade Profile for more information).

Unlike some proprietary protocols which combine sending and retrieval operations, sending a message and saving a copy in a server-side folder with a base-level IMAP client requires transmitting the message content twice, once to SMTP for delivery and a second time to IMAP to store in a sent mail folder. This is addressed by a set of extensions defined by the IETF Lemonade Profile for mobile devices: URLAUTH () and CATENATE () in IMAP, and BURL () in SMTP-SUBMISSION. In addition to this, Courier Mail Server offers a non-standard method of sending using IMAP by copying an outgoing message to a dedicated outbox folder.

To cryptographically protect IMAP connections, IMAPS on TCP port 993 can be used, which utilizes TLS. As of RFC 8314, this is the recommended mechanism.

Alternatively, STARTTLS can be used to provide secure communications between the MUA communicating with the MSA or MTA implementing the SMTP Protocol.

This is an example IMAP connection as taken from RFC 3501 section 8:





</doc>
<doc id="14838" url="https://en.wikipedia.org/wiki?curid=14838" title="Inertial frame of reference">
Inertial frame of reference

An inertial frame of reference in classical physics and special relativity possesses the property that in this frame of reference a body with zero net force acting upon it does not accelerate; that is, such a body is at rest or moving at a constant velocity. An inertial frame of reference can be defined in analytical terms as a frame of reference that describes time and space homogeneously, isotropically, and in a time-independent manner. Conceptually, the physics of a system in an inertial frame have no causes external to the system. An inertial frame of reference may also be called an inertial reference frame, inertial frame, Galilean reference frame, or inertial space.

All inertial frames are in a state of constant, rectilinear motion with respect to one another; an accelerometer moving with any of them would detect zero acceleration. Measurements in one inertial frame can be converted to measurements in another by a simple transformation (the Galilean transformation in Newtonian physics and the Lorentz transformation in special relativity). In general relativity, in any region small enough for the curvature of spacetime and tidal forces to be negligible, one can find a set of inertial frames that approximately describe that region.

In a non-inertial reference frame in classical physics and special relativity, the physics of a system vary depending on the acceleration of that frame with respect to an inertial frame, and the usual physical forces must be supplemented by fictitious forces. In contrast, systems in general relativity don't have external causes, because of the principle of geodesic motion. In classical physics, for example, a ball dropped towards the ground does not go exactly straight down because the Earth is rotating, which means the frame of reference of an observer on Earth is not inertial. The physics must account for the Coriolis effect—in this case thought of as a force—to predict the horizontal motion. Another example of such a fictitious force associated with rotating reference frames is the centrifugal effect, or centrifugal force.

The motion of a body can only be described relative to something else—other bodies, observers, or a set of spacetime coordinates. These are called frames of reference. If the coordinates are chosen badly, the laws of motion may be more complex than necessary. For example, suppose a free body that has no external forces acting on it is at rest at some instant. In many coordinate systems, it would begin to move at the next instant, even though there are no forces on it. However, a frame of reference can always be chosen in which it remains stationary. Similarly, if space is not described uniformly or time independently, a coordinate system could describe the simple flight of a free body in space as a complicated zig-zag in its coordinate system. Indeed, an intuitive summary of inertial frames can be given: in an inertial reference frame, the laws of mechanics take their simplest form.

In an inertial frame, Newton's first law, the "law of inertia", is satisfied: Any free motion has a constant magnitude and direction. Newton's second law for a particle takes the form:

with F the net force (a vector), "m" the mass of a particle and a the acceleration of the particle (also a vector) which would be measured by an observer at rest in the frame. The force F is the vector sum of all "real" forces on the particle, such as electromagnetic, gravitational, nuclear and so forth. In contrast, Newton's second law in a rotating frame of reference, rotating at angular rate "Ω" about an axis, takes the form:

which looks the same as in an inertial frame, but now the force F′ is the resultant of not only F, but also additional terms (the paragraph following this equation presents the main points without detailed mathematics):

where the angular rotation of the frame is expressed by the vector Ω pointing in the direction of the axis of rotation, and with magnitude equal to the angular rate of rotation "Ω", symbol × denotes the vector cross product, vector x locates the body and vector v is the velocity of the body according to a rotating observer (different from the velocity seen by the inertial observer).

The extra terms in the force F′ are the "fictitious" forces for this frame, whose causes are external to the system in the frame. The first extra term is the Coriolis force, the second the centrifugal force, and the third the Euler force. These terms all have these properties: they vanish when "Ω" = 0; that is, they are zero for an inertial frame (which, of course, does not rotate); they take on a different magnitude and direction in every rotating frame, depending upon its particular value of Ω; they are ubiquitous in the rotating frame (affect every particle, regardless of circumstance); and they have no apparent source in identifiable physical sources, in particular, matter. Also, fictitious forces do not drop off with distance (unlike, for example, nuclear forces or electrical forces). For example, the centrifugal force that appears to emanate from the axis of rotation in a rotating frame increases with distance from the axis.

All observers agree on the real forces, F; only non-inertial observers need fictitious forces. The laws of physics in the inertial frame are simpler because unnecessary forces are not present.

In Newton's time the fixed stars were invoked as a reference frame, supposedly at rest relative to absolute space. In reference frames that were either at rest with respect to the fixed stars or in uniform translation relative to these stars, Newton's laws of motion were supposed to hold. In contrast, in frames accelerating with respect to the fixed stars, an important case being frames rotating relative to the fixed stars, the laws of motion did not hold in their simplest form, but had to be supplemented by the addition of fictitious forces, for example, the Coriolis force and the centrifugal force. Two experiments were devised by Newton to demonstrate how these forces could be discovered, thereby revealing to an observer that they were not in an inertial frame: the example of the tension in the cord linking two spheres rotating about their center of gravity, and the example of the curvature of the surface of water in a rotating bucket. In both cases, application of Newton's second law would not work for the rotating observer without invoking centrifugal and Coriolis forces to account for their observations (tension in the case of the spheres; parabolic water surface in the case of the rotating bucket).

As we now know, the fixed stars are not fixed. Those that reside in the Milky Way turn with the galaxy, exhibiting proper motions. Those that are outside our galaxy (such as nebulae once mistaken to be stars) participate in their own motion as well, partly due to expansion of the universe, and partly due to peculiar velocities. The Andromeda Galaxy is on collision course with the Milky Way at a speed of 117 km/s. The concept of inertial frames of reference is no longer tied to either the fixed stars or to absolute space. Rather, the identification of an inertial frame is based upon the simplicity of the laws of physics in the frame. In particular, the absence of fictitious forces is their identifying property.

In practice, although not a requirement, using a frame of reference based upon the fixed stars as though it were an inertial frame of reference introduces very little discrepancy. For example, the centrifugal acceleration of the Earth because of its rotation about the Sun is about thirty million times greater than that of the Sun about the galactic center.

To illustrate further, consider the question: "Does our Universe rotate?" To answer, we might attempt to explain the shape of the Milky Way galaxy using the laws of physics, although other observations might be more definitive, that is, provide larger discrepancies or less measurement uncertainty, like the anisotropy of the microwave background radiation or Big Bang nucleosynthesis. The flatness of the Milky Way depends on its rate of rotation in an inertial frame of reference. If we attribute its apparent rate of rotation entirely to rotation in an inertial frame, a different "flatness" is predicted than if we suppose part of this rotation actually is due to rotation of the universe and should not be included in the rotation of the galaxy itself. Based upon the laws of physics, a model is set up in which one parameter is the rate of rotation of the Universe. If the laws of physics agree more accurately with observations in a model with rotation than without it, we are inclined to select the best-fit value for rotation, subject to all other pertinent experimental observations. If no value of the rotation parameter is successful and theory is not within observational error, a modification of physical law is considered, for example, dark matter is invoked to explain the galactic rotation curve. So far, observations show any rotation of the universe is very slow, no faster than once every 60·10 years (10 rad/yr), and debate persists over whether there is "any" rotation. However, if rotation were found, interpretation of observations in a frame tied to the universe would have to be corrected for the fictitious forces inherent in such rotation in classical physics and special relativity, or interpreted as the curvature of spacetime and the motion of matter along the geodesics in general relativity.

When quantum effects are important, there are additional conceptual complications that arise in quantum reference frames.

According to the first postulate of special relativity, all physical laws take their simplest form in an inertial frame, and there exist multiple inertial frames interrelated by uniform translation: 
This simplicity manifests in that inertial frames have self-contained physics without the need for external causes, while physics in non-inertial frames have external causes. The principle of simplicity can be used within Newtonian physics as well as in special relativity; see Nagel and also Blagojević.
In practical terms, the equivalence of inertial reference frames means that scientists within a box moving uniformly cannot determine their absolute velocity by any experiment. Otherwise, the differences would set up an absolute standard reference frame. According to this definition, supplemented with the constancy of the speed of light, inertial frames of reference transform among themselves according to the Poincaré group of symmetry transformations, of which the Lorentz transformations are a subgroup. In Newtonian mechanics, which can be viewed as a limiting case of special relativity in which the speed of light is infinite, inertial frames of reference are related by the Galilean group of symmetries.

Newton posited an absolute space considered well approximated by a frame of reference stationary relative to the fixed stars. An inertial frame was then one in uniform translation relative to absolute space. However, some scientists (called "relativists" by Mach), even at the time of Newton, felt that absolute space was a defect of the formulation, and should be replaced.

Indeed, the expression "inertial frame of reference" () was coined by Ludwig Lange in 1885, to replace Newton's definitions of "absolute space and time" by a more operational definition. As translated by Iro, Lange proposed the following definition:

A discussion of Lange's proposal can be found in Mach.

The inadequacy of the notion of "absolute space" in Newtonian mechanics is spelled out by Blagojević: 
The utility of operational definitions was carried much further in the special theory of relativity. Some historical background including Lange's definition is provided by DiSalle, who says in summary:

Within the realm of Newtonian mechanics, an inertial frame of reference, or inertial reference frame, is one in which Newton's first law of motion is valid. However, the principle of special relativity generalizes the notion of inertial frame to include all physical laws, not simply Newton's first law.

Newton viewed the first law as valid in any reference frame that is in uniform motion relative to the fixed stars; that is, neither rotating nor accelerating relative to the stars. Today the notion of "absolute space" is abandoned, and an inertial frame in the field of classical mechanics is defined as:

Hence, with respect to an inertial frame, an object or body accelerates only when a physical force is applied, and (following Newton's first law of motion), in the absence of a net force, a body at rest will remain at rest and a body in motion will continue to move uniformly—that is, in a straight line and at constant speed. Newtonian inertial frames transform among each other according to the Galilean group of symmetries.

If this rule is interpreted as saying that straight-line motion is an indication of zero net force, the rule does not identify inertial reference frames because straight-line motion can be observed in a variety of frames. If the rule is interpreted as defining an inertial frame, then we have to be able to determine when zero net force is applied. The problem was summarized by Einstein:
There are several approaches to this issue. One approach is to argue that all real forces drop off with distance from their sources in a known manner, so we have only to be sure that a body is far enough away from all sources to ensure that no force is present. A possible issue with this approach is the historically long-lived view that the distant universe might affect matters (Mach's principle). Another approach is to identify all real sources for real forces and account for them. A possible issue with this approach is that we might miss something, or account inappropriately for their influence, perhaps, again, due to Mach's principle and an incomplete understanding of the universe. A third approach is to look at the way the forces transform when we shift reference frames. Fictitious forces, those that arise due to the acceleration of a frame, disappear in inertial frames, and have complicated rules of transformation in general cases. On the basis of universality of physical law and the request for frames where the laws are most simply expressed, inertial frames are distinguished by the absence of such fictitious forces.

Newton enunciated a principle of relativity himself in one of his corollaries to the laws of motion: 

This principle differs from the special principle in two ways: first, it is restricted to mechanics, and second, it makes no mention of simplicity. It shares with the special principle the invariance of the form of the description among mutually translating reference frames. The role of fictitious forces in classifying reference frames is pursued further below.

Inertial and non-inertial reference frames can be distinguished by the absence or presence of fictitious forces, as explained shortly. 
The presence of fictitious forces indicates the physical laws are not the simplest laws available so, in terms of the special principle of relativity, a frame where fictitious forces are present is not an inertial frame:

Bodies in non-inertial reference frames are subject to so-called "fictitious" forces (pseudo-forces); that is, forces that result from the acceleration of the reference frame itself and not from any physical force acting on the body. Examples of fictitious forces are the centrifugal force and the Coriolis force in rotating reference frames.

How then, are "fictitious" forces to be separated from "real" forces? It is hard to apply the Newtonian definition of an inertial frame without this separation. For example, consider a stationary object in an inertial frame. Being at rest, no net force is applied. But in a frame rotating about a fixed axis, the object appears to move in a circle, and is subject to centripetal force (which is made up of the Coriolis force and the centrifugal force). How can we decide that the rotating frame is a non-inertial frame? There are two approaches to this resolution: one approach is to look for the origin of the fictitious forces (the Coriolis force and the centrifugal force). We will find there are no sources for these forces, no associated force carriers, no originating bodies. A second approach is to look at a variety of frames of reference. For any inertial frame, the Coriolis force and the centrifugal force disappear, so application of the principle of special relativity would identify these frames where the forces disappear as sharing the same and the simplest physical laws, and hence rule that the rotating frame is not an inertial frame.

Newton examined this problem himself using rotating spheres, as shown in Figure 2 and Figure 3. He pointed out that if the spheres are not rotating, the tension in the tying string is measured as zero in every frame of reference. If the spheres only appear to rotate (that is, we are watching stationary spheres from a rotating frame), the zero tension in the string is accounted for by observing that the centripetal force is supplied by the centrifugal and Coriolis forces in combination, so no tension is needed. If the spheres really are rotating, the tension observed is exactly the centripetal force required by the circular motion. Thus, measurement of the tension in the string identifies the inertial frame: it is the one where the tension in the string provides exactly the centripetal force demanded by the motion as it is observed in that frame, and not a different value. That is, the inertial frame is the one where the fictitious forces vanish.

So much for fictitious forces due to rotation. However, for linear acceleration, Newton expressed the idea of undetectability of straight-line accelerations held in common:
This principle generalizes the notion of an inertial frame. For example, an observer confined in a free-falling lift will assert that he himself is a valid inertial frame, even if he is accelerating under gravity, so long as he has no knowledge about anything outside the lift. So, strictly speaking, inertial frame is a relative concept. With this in mind, we can define inertial frames collectively as a set of frames which are stationary or moving at constant velocity with respect to each other, so that a single inertial frame is defined as an element of this set.

For these ideas to apply, everything observed in the frame has to be subject to a base-line, common acceleration shared by the frame itself. That situation would apply, for example, to the elevator example, where all objects are subject to the same gravitational acceleration, and the elevator itself accelerates at the same rate.

Inertial navigation systems used a cluster of gyroscopes and accelerometers to determine accelerations relative to inertial space. After a gyroscope is spun up in a particular orientation in inertial space, the law of conservation of angular momentum requires that it retain that orientation as long as no external forces are applied to it. Three orthogonal gyroscopes establish an inertial reference frame, and the accelerators measure acceleration relative to that frame. The accelerations, along with a clock, can then be used to calculate the change in position. Thus, inertial navigation is a form of dead reckoning that requires no external input, and therefore cannot be jammed by any external or internal signal source.

A gyrocompass, employed for navigation of seagoing vessels, finds the geometric north. It does so, not by sensing the Earth's magnetic field, but by using inertial space as its reference. The outer casing of the gyrocompass device is held in such a way that it remains aligned with the local plumb line. When the gyroscope wheel inside the gyrocompass device is spun up, the way the gyroscope wheel is suspended causes the gyroscope wheel to gradually align its spinning axis with the Earth's axis. Alignment with the Earth's axis is the only direction for which the gyroscope's spinning axis can be stationary with respect to the Earth and not be required to change direction with respect to inertial space. After being spun up, a gyrocompass can reach the direction of alignment with the Earth's axis in as little as a quarter of an hour.

Classical theories that use the Galilean transformation postulate the equivalence of all inertial reference frames. Some theories may even postulate the existence of a privileged frame which provides absolute space and absolute time. The Galilean transformation transforms coordinates from one inertial reference frame, formula_4, to another, formula_5, by simple addition or subtraction of coordinates:

where r and "t" represent shifts in the origin of space and time, and v is the relative velocity of the two inertial reference frames. Under Galilean transformations, the time "t" − "t" between two events is the same for all reference frames and the distance between two simultaneous events (or, equivalently, the length of any object, |r − r|) is also the same.

Einstein's theory of special relativity, like Newtonian mechanics, postulates the equivalence of all inertial reference frames. However, because special relativity postulates that the speed of light in free space is invariant, the transformation between inertial frames is the Lorentz transformation, not the Galilean transformation which is used in Newtonian mechanics. The invariance of the speed of light leads to counter-intuitive phenomena, such as time dilation and length contraction, and the relativity of simultaneity, which have been extensively verified experimentally. The Lorentz transformation reduces to the Galilean transformation as the speed of light approaches infinity or as the relative velocity between frames approaches zero.

General relativity is based upon the principle of equivalence:
This idea was introduced in Einstein's 1907 article "Principle of Relativity and Gravitation" and later developed in 1911. Support for this principle is found in the Eötvös experiment, which determines whether the ratio of inertial to gravitational mass is the same for all bodies, regardless of size or composition. To date no difference has been found to a few parts in 10. For some discussion of the subtleties of the Eötvös experiment, such as the local mass distribution around the experimental site (including a quip about the mass of Eötvös himself), see Franklin.

Einstein's general theory modifies the distinction between nominally "inertial" and "noninertial" effects by replacing special relativity's "flat" Minkowski Space with a metric that produces non-zero curvature. In general relativity, the principle of inertia is replaced with the principle of geodesic motion, whereby objects move in a way dictated by the curvature of spacetime. As a consequence of this curvature, it is not a given in general relativity that inertial objects moving at a particular rate with respect to each other will continue to do so. This phenomenon of geodesic deviation means that inertial frames of reference do not exist globally as they do in Newtonian mechanics and special relativity.

However, the general theory reduces to the special theory over sufficiently small regions of spacetime, where curvature effects become less important and the earlier inertial frame arguments can come back into play. Consequently, modern special relativity is now sometimes described as only a "local theory". "Local" can encompass, for example, the entire Milky Way galaxy: The astronomer Karl Schwarzschild observed the motion of pairs of stars orbiting each other. He found that the two orbits of the stars of such a system lie in a plane, and the perihelion of the orbits of the two stars remains pointing in the same direction with respect to the solar system. Schwarzschild pointed out that that was invariably seen: the direction of the angular momentum of all observed double star systems remains fixed with respect to the direction of the angular momentum of the Solar System. These observations allowed him to conclude that inertial frames inside the galaxy do not rotate with respect to one another, and that the space of the Milky Way is approximately Galilean or Minkowskian.






</doc>
<doc id="14840" url="https://en.wikipedia.org/wiki?curid=14840" title="Illuminati: New World Order">
Illuminati: New World Order

Illuminati: New World Order ("INWO") is an out-of-print collectible card game (CCG) that was released in 1994 by Steve Jackson Games, based on their original boxed game Illuminati, which in turn was inspired by the 1975 book "The Illuminatus! Trilogy" by Robert Anton Wilson and Robert Shea. An OMNI sealed-deck league patterned after the Atlas Games model was also developed.

Players attempt to achieve World Domination by utilizing the powers of their chosen Illuminati (the Adepts of Hermes, the Bavarian Illuminati, the Bermuda Triangle, the Discordian Society, the Gnomes of Zürich, The Network, the Servants of Cthulhu, Shangri-La, and the UFOs). The first player to control a predetermined number of Organizations (usually twelve in a standard game) has achieved the Basic Goal and can claim victory.

Controllable Organizations include: groups such as the Men in Black, the CIA, and the Boy Sprouts; Personalities such as Diana, Princess of Wales, Saddam Hussein, Ross Perot or Björne (the purple dinosaur); and Places like Japan, California, Canada, and the Moonbase. Many Organization names are spoofs of real organizations, presumably altered to avoid lawsuits.

Other ways to achieve victory include: destroying your rival Illuminati by capturing or destroying the last Organization in their Power Structure; and/or fulfilling a Special Goal before your opponent(s) can.

Cards come in three main types: Illuminati cards, Plot cards, and Group cards. Illuminati and Plot cards both feature an illustration of a puppeteer's hand in a blue color scheme on the rear side, whereas Group cards feature a puppet on a string in a red color scheme.

Each Illuminati card represents a different Illuminated organization at the center of each player's Power Structure. They have Power, a Special Goal, and an appropriate Special Ability. Their power flows outwards into the Groups they control via Control Arrows.

Plot cards provide the bulk of the game's narrative structure, allowing players to go beyond - or even break - the rules of the game as described in the World Domination Handbook. Plot cards are identified by their overall blue color scheme (border, and/or title color). Included among the general Plots are several special types, including "Assassinations" and "Disasters" (for delivering insults to the various Personalities and Places in play), "GOAL" (special goals that can lead to surprise victories), and "New World Order" cards (a set of conditions that affect all players, typically overridden when replacement "New World Order" cards are brought into play).

Group cards represent the power elite in charge of the named organization. There are two main types of Group: Organizations and Resources.

Organizations are identified by their overall red color scheme (border and/or title). There are three main types of Organization: regular Organizations, People, and Places. They all feature Power, Resistance, Special Abilities, Alignments, Attributes, and Control Arrows (an inward arrow, and 0-3 outward arrows). Just like their Illuminati masters, Organizations can launch and defend against a variety of attacks. Provided that the attacking Organization has a free, outward-pointing Control Arrow, players can increase the size of their Power Structure via successful Attacks to Control, a mathematically determined method employed whenever a player wants to capture an Organization from their own hand, or from a rival player's Power Structure. Unless the attack is Privileged (only the target and attacker can be involved), all players can aid or undermine the attack. Attacks to Destroy follow a similar game mechanic, but result in the Organization's removal from the Power Structure, after which they are immediately discarded. The outcome of all Attacks are determined by a dice roll. Other ways to introduce Organizations to the Power Structure involve Plots, or spending Action Tokens to bring Groups into play, or by using free moves, each at appropriate times during the play cycle.

Resources represent the custodians of a variety of objects, ranging from gadgets to artefacts (such as The Shroud of Turin, Flying Saucers, and ELIZA). They are identified by their overall purple color scheme (border and/or title). Resources are introduced into play by spending Action Tokens, or by using free moves during appropriate moments in the play cycle. They go alongside the Power Structure of the player's Illuminati, and bestow a useful Special Ability or similar.

In the June 1995 edition of "Dragon" (Issue 218), Rick Swan warned that it was a complex game: "Owing to the unconventional mechanics, even experienced gamers may have trouble at first." But he gave the game a perfect rating of 6 out of 6, saying, "Resolute players who scrutinize the rules and grind their way through a few practice rounds will discover why "Illuminati" has been so durable. Not only is it an inspired concept, it’s an enlightening treatise on the fine art of backstabbing. What more could you ask from a deck of cards?"

In the September 1996 edition of "Arcane" (Issue 4), Steve Faragher rated the "Assassins" expansion set 9 out of 10 overall, saying, "With the introduction of "Assassins", it now appears to have [...] a little more game balance for tournament play. A good thing indeed.".

"INWO" won the Origins Award for "Best Card Game" in 1997.

"The INWO Book" (1995) Steve Jackson Games Incorporated.

"Illuminati: New World Order", Official Website.



</doc>
<doc id="14841" url="https://en.wikipedia.org/wiki?curid=14841" title="Integration">
Integration

Integration may refer to:









</doc>
<doc id="14843" url="https://en.wikipedia.org/wiki?curid=14843" title="Interstellar travel">
Interstellar travel

Interstellar travel is the hypothetical crewed or uncrewed travel between stars or planetary systems in a galaxy. Interstellar travel would be much more difficult than interplanetary spaceflight. Whereas the distances between the planets in the Solar System are less than 30 astronomical units (AU), the distances between stars are typically hundreds of thousands of AU, and usually expressed in light-years. Because of the vastness of those distances, practical interstellar travel based on known physics would need to occur at a high percentage of the speed of light, allowing for significant travel times, at least decades to perhaps millennia or longer.

The speeds required for interstellar travel in a human lifetime far exceed what current methods of spacecraft propulsion can provide. Even with a hypothetically perfectly efficient propulsion system, the kinetic energy corresponding to those speeds is enormous by today's standards of energy development. Moreover, collisions by the spacecraft with cosmic dust and gas can produce very dangerous effects both to passengers and the spacecraft itself.

A number of strategies have been proposed to deal with these problems, ranging from giant arks that would carry entire societies and ecosystems, to microscopic space probes. Many different spacecraft propulsion systems have been proposed to give spacecraft the required speeds, including nuclear propulsion, beam-powered propulsion, and methods based on speculative physics.

For both crewed and uncrewed interstellar travel, considerable technological and economic challenges need to be met. Even the most optimistic views about interstellar travel see it as only being feasible decades from now. However, in spite of the challenges, if or when interstellar travel is realized, a wide range of scientific benefits is expected.

Most interstellar travel concepts require a developed space logistics system capable of moving millions of tonnes to a construction / operating location, and most would require gigawatt-scale power for construction or power (such as Star Wisp or Light Sail type concepts). Such a system could grow organically if space-based solar power became a significant component of Earth's energy mix. Consumer demand for a multi-terawatt system would automatically create the necessary multi-million ton/year logistical system.

Distances between the planets in the Solar System are often measured in astronomical units (AU), defined as the average distance between the Sun and Earth, some . Venus, the closest other planet to Earth is (at closest approach) 0.28 AU away. Neptune, the farthest planet from the Sun, is 29.8 AU away. As of January 25, 2020, Voyager 1, the farthest human-made object from Earth, is 148.7 AU away.

The closest known star, Proxima Centauri, is approximately away, or over 9,000 times farther away than Neptune.

Because of this, distances between stars are usually expressed in light-years (defined as the distance that light travels in vacuum in one Julian year) or in parsecs (one parsec is 3.26 ly, the distance at which stellar parallax is exactly one arcsecond, hence the name). Light in a vacuum travels around per second, so 1 light-year is about or AU. Proxima Centauri, the nearest (albeit not naked-eye visible) star, is 4.243 light-years away.

Another way of understanding the vastness of interstellar distances is by scaling: One of the closest stars to the Sun, Alpha Centauri A (a Sun-like star), can be pictured by scaling down the Earth–Sun distance to . On this scale, the distance to Alpha Centauri A would be .

The fastest outward-bound spacecraft yet sent, Voyager 1, has covered 1/600 of a light-year in 30 years and is currently moving at 1/18,000 the speed of light. At this rate, a journey to Proxima Centauri would take 80,000 years.

A significant factor contributing to the difficulty is the energy that must be supplied to obtain a reasonable travel time. A lower bound for the required energy is the kinetic energy formula_1 where formula_2 is the final mass. If deceleration on arrival is desired and cannot be achieved by any means other than the engines of the ship, then the lower bound for the required energy is doubled to formula_3.

The velocity for a crewed round trip of a few decades to even the nearest star is several thousand times greater than those of present space vehicles. This means that due to the formula_4 term in the kinetic energy formula, millions of times as much energy is required. Accelerating one ton to one-tenth of the speed of light requires at least (world energy consumption 2008 was 143,851 terawatt-hours), without factoring in efficiency of the propulsion mechanism. This energy has to be generated onboard from stored fuel, harvested from the interstellar medium, or projected over immense distances.

A knowledge of the properties of the interstellar gas and dust through which the vehicle must pass is essential for the design of any interstellar space mission. A major issue with traveling at extremely high speeds is that interstellar dust may cause considerable damage to the craft, due to the high relative speeds and large kinetic energies involved. Various shielding methods to mitigate this problem have been proposed. Larger objects (such as macroscopic dust grains) are far less common, but would be much more destructive. The risks of impacting such objects, and methods of mitigating these risks, have been discussed in literature, but many unknowns remain and, owing to the inhomogeneous distribution of interstellar matter around the Sun, will depend on direction travelled. Although a high density interstellar medium may cause difficulties for many interstellar travel concepts, interstellar ramjets, and some proposed concepts for decelerating interstellar spacecraft, would actually benefit from a denser interstellar medium.

The crew of an interstellar ship would face several significant hazards, including the psychological effects of long-term isolation, the effects of exposure to ionizing radiation, and the physiological effects of weightlessness to the muscles, joints, bones, immune system, and eyes. There also exists the risk of impact by micrometeoroids and other space debris. These risks represent challenges that have yet to be overcome.

The physicist Robert L. Forward has argued that an interstellar mission that cannot be completed within 50 years should not be started at all. Instead, assuming that a civilization is still on an increasing curve of propulsion system velocity and not yet having reached the limit, the resources should be invested in designing a better propulsion system. This is because a slow spacecraft would probably be passed by another mission sent later with more advanced propulsion (the incessant obsolescence postulate).

On the other hand, Andrew Kennedy has shown that if one calculates the journey time to a given destination as the rate of travel speed derived from growth (even exponential growth) increases, there is a clear minimum in the total time to that destination from now. Voyages undertaken before the minimum will be overtaken by those that leave at the minimum, whereas voyages that leave after the minimum will never overtake those that left at the minimum.

There are 59 known stellar systems within 40 light years of the Sun, containing 81 visible stars. The following could be considered prime targets for interstellar missions:

Existing and near-term astronomical technology is capable of finding planetary systems around these objects, increasing their potential for exploration

Slow interstellar missions based on current and near-future propulsion technologies are associated with trip times starting from about one hundred years to thousands of years. These missions consist of sending a robotic probe to a nearby star for exploration, similar to interplanetary probes such as used in the Voyager program. By taking along no crew, the cost and complexity of the mission is significantly reduced although technology lifetime is still a significant issue next to obtaining a reasonable speed of travel. Proposed concepts include Project Daedalus, Project Icarus, Project Dragonfly, Project Longshot, and more recently Breakthrough Starshot.

Near-lightspeed nano spacecraft might be possible within the near future built on existing microchip technology with a newly developed nanoscale thruster. Researchers at the University of Michigan are developing thrusters that use nanoparticles as propellant. Their technology is called "nanoparticle field extraction thruster", or nanoFET. These devices act like small particle accelerators shooting conductive nanoparticles out into space.

Michio Kaku, a theoretical physicist, has suggested that clouds of "smart dust" be sent to the stars, which may become possible with advances in nanotechnology. Kaku also notes that a large number of nanoprobes would need to be sent due to the vulnerability of very small probes to be easily deflected by magnetic fields, micrometeorites and other dangers to ensure the chances that at least one nanoprobe will survive the journey and reach the destination.

Given the light weight of these probes, it would take much less energy to accelerate them. With onboard solar cells, they could continually accelerate using solar power. One can envision a day when a fleet of millions or even billions of these particles swarm to distant stars at nearly the speed of light and relay signals back to Earth through a vast interstellar communication network.

As a near-term solution, small, laser-propelled interstellar probes, based on current CubeSat technology were proposed in the context of Project Dragonfly.

In crewed missions, the duration of a slow interstellar journey presents a major obstacle and existing concepts deal with this problem in different ways. They can be distinguished by the "state" in which humans are transported on-board of the spacecraft.

A generation ship (or world ship) is a type of interstellar ark in which the crew that arrives at the destination is descended from those who started the journey. Generation ships are not currently feasible because of the difficulty of constructing a ship of the enormous required scale and the great biological and sociological problems that life aboard such a ship raises.

Scientists and writers have postulated various techniques for suspended animation. These include human hibernation and cryonic preservation. Although neither is currently practical, they offer the possibility of sleeper ships in which the passengers lie inert for the long duration of the voyage.

A robotic interstellar mission carrying some number of frozen early stage human embryos is another theoretical possibility. This method of space colonization requires, among other things, the development of an artificial uterus, the prior detection of a habitable terrestrial planet, and advances in the field of fully autonomous mobile robots and educational robots that would replace human parents.

Interstellar space is not completely empty; it contains trillions of icy bodies ranging from small asteroids (Oort cloud) to possible rogue planets. There may be ways to take advantage of these resources for a good part of an interstellar trip, slowly hopping from body to body or setting up waystations along the way.

If a spaceship could average 10 percent of light speed (and decelerate at the destination, for human crewed missions), this would be enough to reach Proxima Centauri in forty years. Several propulsion concepts have been proposed that might be eventually developed to accomplish this (see § Propulsion below), but none of them are ready for near-term (few decades) developments at acceptable cost.

Physicists generally believe faster-than-light travel is impossible. 
Relativistic time dilation allows a traveler to experience time more slowly, the closer their speed is to the speed of light. This apparent slowing becomes noticeable when velocities above 80% of the speed of light are attained. Clocks aboard an interstellar ship would run slower than Earth clocks, so if a ship's engines were capable of continuously generating around 1 g of acceleration (which is comfortable for humans), the ship could reach almost anywhere in the galaxy and return to Earth within 40 years ship-time (see diagram). Upon return, there would be a difference between the time elapsed on the astronaut's ship and the time elapsed on Earth.

For example, a spaceship could travel to a star 32 light-years away, initially accelerating at a constant 1.03g (i.e. 10.1 m/s) for 1.32 years (ship time), then stopping its engines and coasting for the next 17.3 years (ship time) at a constant speed, then decelerating again for 1.32 ship-years, and coming to a stop at the destination. After a short visit, the astronaut could return to Earth the same way. After the full round-trip, the clocks on board the ship show that 40 years have passed, but according to those on Earth, the ship comes back 76 years after launch.

From the viewpoint of the astronaut, onboard clocks seem to be running normally. The star ahead seems to be approaching at a speed of 0.87 light years per ship-year. The universe would appear contracted along the direction of travel to half the size it had when the ship was at rest; the distance between that star and the Sun would seem to be 16 light years as measured by the astronaut.

At higher speeds, the time on board will run even slower, so the astronaut could travel to the center of the Milky Way (30,000 light years from Earth) and back in 40 years ship-time. But the speed according to Earth clocks will always be less than 1 light year per Earth year, so, when back home, the astronaut will find that more than 60 thousand years will have passed on Earth.

Regardless of how it is achieved, a propulsion system that could produce acceleration continuously from departure to arrival would be the fastest method of travel. A constant acceleration journey is one where the propulsion system accelerates the ship at a constant rate for the first half of the journey, and then decelerates for the second half, so that it arrives at the destination stationary relative to where it began. If this were performed with an acceleration similar to that experienced at the Earth's surface, it would have the added advantage of producing artificial "gravity" for the crew. Supplying the energy required, however, would be prohibitively expensive with current technology.

From the perspective of a planetary observer, the ship will appear to accelerate steadily at first, but then more gradually as it approaches the speed of light (which it cannot exceed). It will undergo hyperbolic motion. The ship will be close to the speed of light after about a year of accelerating and remain at that speed until it brakes for the end of the journey.

From the perspective of an onboard observer, the crew will feel a gravitational field opposite the engine's acceleration, and the universe ahead will appear to fall in that field, undergoing hyperbolic motion. As part of this, distances between objects in the direction of the ship's motion will gradually contract until the ship begins to decelerate, at which time an onboard observer's experience of the gravitational field will be reversed.

When the ship reaches its destination, if it were to exchange a message with its origin planet, it would find that less time had elapsed on board than had elapsed for the planetary observer, due to time dilation and length contraction.

The result is an impressively fast journey for the crew.

All rocket concepts are limited by the rocket equation, which sets the characteristic velocity available as a function of exhaust velocity and mass ratio, the ratio of initial ("M", including fuel) to final ("M", fuel depleted) mass.

Very high specific power, the ratio of thrust to total vehicle mass, is required to reach interstellar targets within sub-century time-frames. Some heat transfer is inevitable and a tremendous heating load must be adequately handled.

Thus, for interstellar rocket concepts of all technologies, a key engineering problem (seldom explicitly discussed) is limiting the heat transfer from the exhaust stream back into the vehicle.

A type of electric propulsion, spacecraft such as Dawn use an ion engine. In an ion engine, electric power is used to create charged particles of the propellant, usually the gas xenon, and accelerate them to extremely high velocities. The exhaust velocity of conventional rockets is limited by the chemical energy stored in the fuel's molecular bonds, which limits the thrust to about 5 km/s. They produce a high thrust (about 10⁶ N), but they have a low specific impulse, and that limits their top speed. By contrast, ion engines have low force, but the top speed in principle is limited only by the electrical power available on the spacecraft and on the gas ions being accelerated. The exhaust speed of the charged particles range from 15 km/s to 35 km/s.<ref name="http://www.iflscience.com"></ref>

Nuclear-electric or plasma engines, operating for long periods at low thrust and powered by fission reactors, have the potential to reach speeds much greater than chemically powered vehicles or nuclear-thermal rockets. Such vehicles probably have the potential to power solar system exploration with reasonable trip times within the current century. Because of their low-thrust propulsion, they would be limited to off-planet, deep-space operation. Electrically powered spacecraft propulsion powered by a portable power-source, say a nuclear reactor, producing only small accelerations, would take centuries to reach for example 15% of the velocity of light, thus unsuitable for interstellar flight during a single human lifetime.

Fission-fragment rockets use nuclear fission to create high-speed jets of fission fragments, which are ejected at speeds of up to . With fission, the energy output is approximately 0.1% of the total mass-energy of the reactor fuel and limits the effective exhaust velocity to about 5% of the velocity of light. For maximum velocity, the reaction mass should optimally consist of fission products, the "ash" of the primary energy source, so no extra reaction mass need be bookkept in the mass ratio.

Based on work in the late 1950s to the early 1960s, it has been technically possible to build spaceships with nuclear pulse propulsion engines, i.e. driven by a series of nuclear explosions. This propulsion system contains the prospect of very high specific impulse (space travel's equivalent of fuel economy) and high specific power.

Project Orion team member Freeman Dyson proposed in 1968 an interstellar spacecraft using nuclear pulse propulsion that used pure deuterium fusion detonations with a very high fuel-burnup fraction. He computed an exhaust velocity of 15,000 km/s and a 100,000-tonne space vehicle able to achieve a 20,000 km/s delta-v allowing a flight-time to Alpha Centauri of 130 years. Later studies indicate that the top cruise velocity that can theoretically be achieved by a Teller-Ulam thermonuclear unit powered Orion starship, assuming no fuel is saved for slowing back down, is about 8% to 10% of the speed of light (0.08-0.1c). An atomic (fission) Orion can achieve perhaps 3%-5% of the speed of light. A nuclear pulse drive starship powered by fusion-antimatter catalyzed nuclear pulse propulsion units would be similarly in the 10% range and pure matter-antimatter annihilation rockets would be theoretically capable of obtaining a velocity between 50% to 80% of the speed of light. In each case saving fuel for slowing down halves the maximum speed. The concept of using a magnetic sail to decelerate the spacecraft as it approaches its destination has been discussed as an alternative to using propellant, this would allow the ship to travel near the maximum theoretical velocity. Alternative designs utilizing similar principles include Project Longshot, Project Daedalus, and Mini-Mag Orion. The principle of external nuclear pulse propulsion to maximize survivable power has remained common among serious concepts for interstellar flight without external power beaming and for very high-performance interplanetary flight.

In the 1970s the Nuclear Pulse Propulsion concept further was refined by Project Daedalus by use of externally triggered inertial confinement fusion, in this case producing fusion explosions via compressing fusion fuel pellets with high-powered electron beams. Since then, lasers, ion beams, neutral particle beams and hyper-kinetic projectiles have been suggested to produce nuclear pulses for propulsion purposes.

A current impediment to the development of "any" nuclear-explosion-powered spacecraft is the 1963 Partial Test Ban Treaty, which includes a prohibition on the detonation of any nuclear devices (even non-weapon based) in outer space. This treaty would, therefore, need to be renegotiated, although a project on the scale of an interstellar mission using currently foreseeable technology would probably require international cooperation on at least the scale of the International Space Station.

Another issue to be considered, would be the g-forces imparted to a rapidly accelerated spacecraft, cargo, and passengers inside (see Inertia negation).

Fusion rocket starships, powered by nuclear fusion reactions, should conceivably be able to reach speeds of the order of 10% of that of light, based on energy considerations alone. In theory, a large number of stages could push a vehicle arbitrarily close to the speed of light. These would "burn" such light element fuels as deuterium, tritium, He, B, and Li. Because fusion yields about 0.3–0.9% of the mass of the nuclear fuel as released energy, it is energetically more favorable than fission, which releases <0.1% of the fuel's mass-energy. The maximum exhaust velocities potentially energetically available are correspondingly higher than for fission, typically 4–10% of c. However, the most easily achievable fusion reactions release a large fraction of their energy as high-energy neutrons, which are a significant source of energy loss. Thus, although these concepts seem to offer the best (nearest-term) prospects for travel to the nearest stars within a (long) human lifetime, they still involve massive technological and engineering difficulties, which may turn out to be intractable for decades or centuries.

Early studies include Project Daedalus, performed by the British Interplanetary Society in 1973–1978, and Project Longshot, a student project sponsored by NASA and the US Naval Academy, completed in 1988. Another fairly detailed vehicle system, "Discovery II", designed and optimized for crewed Solar System exploration, based on the DHe reaction but using hydrogen as reaction mass, has been described by a team from NASA's Glenn Research Center. It achieves characteristic velocities of >300 km/s with an acceleration of ~1.7•10 "g", with a ship initial mass of ~1700 metric tons, and payload fraction above 10%. Although these are still far short of the requirements for interstellar travel on human timescales, the study seems to represent a reasonable benchmark towards what may be approachable within several decades, which is not impossibly beyond the current state-of-the-art. Based on the concept's 2.2% burnup fraction it could achieve a pure fusion product exhaust velocity of ~3,000 km/s.

An antimatter rocket would have a far higher energy density and specific impulse than any other proposed class of rocket. If energy resources and efficient production methods are found to make antimatter in the quantities required and store it safely, it would be theoretically possible to reach speeds of several tens of percent that of light. Whether antimatter propulsion could lead to the higher speeds (>90% that of light) at which relativistic time dilation would become more noticeable, thus making time pass at a slower rate for the travelers as perceived by an outside observer, is doubtful owing to the large quantity of antimatter that would be required.

Speculating that production and storage of antimatter should become feasible, two further issues need to be considered. First, in the annihilation of antimatter, much of the energy is lost as high-energy gamma radiation, and especially also as neutrinos, so that only about 40% of "mc" would actually be available if the antimatter were simply allowed to annihilate into radiations thermally. Even so, the energy available for propulsion would be substantially higher than the ~1% of "mc" yield of nuclear fusion, the next-best rival candidate.

Second, heat transfer from the exhaust to the vehicle seems likely to transfer enormous wasted energy into the ship (e.g. for 0.1"g" ship acceleration, approaching 0.3 trillion watts per ton of ship mass), considering the large fraction of the energy that goes into penetrating gamma rays. Even assuming shielding was provided to protect the payload (and passengers on a crewed vehicle), some of the energy would inevitably heat the vehicle, and may thereby prove a limiting factor if useful accelerations are to be achieved.

More recently, Friedwardt Winterberg proposed that a matter-antimatter GeV gamma ray laser photon rocket is possible by a relativistic proton-antiproton pinch discharge, where the recoil from the laser beam is transmitted by the Mössbauer effect to the spacecraft.

Rockets deriving their power from external sources, such as a laser, could replace their internal energy source with an energy collector, potentially reducing the mass of the ship greatly and allowing much higher travel speeds. Geoffrey A. Landis has proposed for an interstellar probe, with energy supplied by an external laser from a base station powering an Ion thruster.

A problem with all traditional rocket propulsion methods is that the spacecraft would need to carry its fuel with it, thus making it very massive, in accordance with the rocket equation. Several concepts attempt to escape from this problem:

A radio frequency (RF) resonant cavity thruster is a device that is claimed to be a spacecraft thruster. In 2016, the Advanced Propulsion Physics Laboratory at NASA reported observing a small apparent thrust from one such test, a result not since replicated. One of the designs is called EMDrive. In December 2002, Satellite Propulsion Research Ltd described a working prototype with an alleged total thrust of about 0.02 newtons powered by an 850 W cavity magnetron. The device could operate for only a few dozen seconds before the magnetron failed, due to overheating. The latest test on the EMDrive concluded that it does not work.

Proposed in 2019 by NASA scientist Dr. David Burns, the helical engine concept would use a particle accelerator to accelerate particles to near the speed of light. Since particles traveling at such speeds acquire more mass, it is believed that this mass change could create acceleration. According to Burns, the spacecraft could theoretically reach 99% the speed of light.

In 1960, Robert W. Bussard proposed the Bussard ramjet, a fusion rocket in which a huge scoop would collect the diffuse hydrogen in interstellar space, "burn" it on the fly using a proton–proton chain reaction, and expel it out of the back. Later calculations with more accurate estimates suggest that the thrust generated would be less than the drag caused by any conceivable scoop design. Yet the idea is attractive because the fuel would be collected "en route" (commensurate with the concept of "energy harvesting"), so the craft could theoretically accelerate to near the speed of light. The limitation is due to the fact that the reaction can only accelerate the propellant to 0.12c. Thus the drag of catching interstellar dust and the thrust of accelerating that same dust to 0.12c would be the same when the speed is 0.12c, preventing further acceleration.

A light sail or magnetic sail powered by a massive laser or particle accelerator in the home star system could potentially reach even greater speeds than rocket- or pulse propulsion methods, because it would not need to carry its own reaction mass and therefore would only need to accelerate the craft's payload. Robert L. Forward proposed a means for decelerating an interstellar light sail in the destination star system without requiring a laser array to be present in that system. In this scheme, a smaller secondary sail is deployed to the rear of the spacecraft, whereas the large primary sail is detached from the craft to keep moving forward on its own. Light is reflected from the large primary sail to the secondary sail, which is used to decelerate the secondary sail and the spacecraft payload. In 2002, Geoffrey A. Landis of NASA's Glen Research center also proposed a laser-powered, propulsion, sail ship that would host a diamond sail (of a few nanometers thick) powered with the use of solar energy. With this proposal, this interstellar ship would, theoretically, be able to reach 10 percent the speed of light.

A magnetic sail could also decelerate at its destination without depending on carried fuel or a driving beam in the destination system, by interacting with the plasma found in the solar wind of the destination star and the interstellar medium.

The following table lists some example concepts using beamed laser propulsion as proposed by the physicist Robert L. Forward:

The following table is based on work by Heller, Hippke and Kervella.

Achieving start-stop interstellar trip times of less than a human lifetime require mass-ratios of between 1,000 and 1,000,000, even for the nearer stars. This could be achieved by multi-staged vehicles on a vast scale. Alternatively large linear accelerators could propel fuel to fission propelled space-vehicles, avoiding the limitations of the Rocket equation.

Scientists and authors have postulated a number of ways by which it might be possible to surpass the speed of light, but even the most serious-minded of these are highly speculative.

It is also debatable whether faster-than-light travel is physically possible, in part because of causality concerns: travel faster than light may, under certain conditions, permit travel backwards in time within the context of special relativity. Proposed mechanisms for faster-than-light travel within the theory of general relativity require the existence of exotic matter and it is not known if this could be produced in sufficient quantity.

In physics, the Alcubierre drive is based on an argument, within the framework of general relativity and without the introduction of wormholes, that it is possible to modify spacetime in a way that allows a spaceship to travel with an arbitrarily large speed by a local expansion of spacetime behind the spaceship and an opposite contraction in front of it. Nevertheless, this concept would require the spaceship to incorporate a region of exotic matter, or hypothetical concept of negative mass.

A theoretical idea for enabling interstellar travel is by propelling a starship by creating an artificial black hole and using a parabolic reflector to reflect its Hawking radiation. Although beyond current technological capabilities, a black hole starship offers some advantages compared to other possible methods. Getting the black hole to act as a power source and engine also requires a way to convert the Hawking radiation into energy and thrust. One potential method involves placing the hole at the focal point of a parabolic reflector attached to the ship, creating forward thrust. A slightly easier, but less efficient method would involve simply absorbing all the gamma radiation heading towards the fore of the ship to push it onwards, and let the rest shoot out the back.

Wormholes are conjectural distortions in spacetime that theorists postulate could connect two arbitrary points in the universe, across an Einstein–Rosen Bridge. It is not known whether wormholes are possible in practice. Although there are solutions to the Einstein equation of general relativity that allow for wormholes, all of the currently known solutions involve some assumption, for example the existence of negative mass, which may be unphysical. However, Cramer "et al." argue that such wormholes might have been created in the early universe, stabilized by cosmic strings. The general theory of wormholes is discussed by Visser in the book "Lorentzian Wormholes".

The Enzmann starship, as detailed by G. Harry Stine in the October 1973 issue of "Analog", was a design for a future starship, based on the ideas of Robert Duncan-Enzmann. The spacecraft itself as proposed used a 12,000,000 ton ball of frozen deuterium to power 12–24 thermonuclear pulse propulsion units. Twice as long as the Empire State Building and assembled in-orbit, the spacecraft was part of a larger project preceded by interstellar probes and telescopic observation of target star systems.

Project Hyperion, one of the projects of Icarus Interstellar has looked into various feasibility issues of crewed interstellar travel. Its members continue to publish on crewed interstellar travel in collaboration with the Initiative for Interstellar Studies.

NASA has been researching interstellar travel since its formation, translating important foreign language papers and conducting early studies on applying fusion propulsion, in the 1960s, and laser propulsion, in the 1970s, to interstellar travel.

The NASA Breakthrough Propulsion Physics Program (terminated in FY 2003 after a 6-year, $1.2-million study, because "No breakthroughs appear imminent.") identified some breakthroughs that are needed for interstellar travel to be possible.

Geoffrey A. Landis of NASA's Glenn Research Center states that a laser-powered interstellar sail ship could possibly be launched within 50 years, using new methods of space travel. "I think that ultimately we're going to do it, it's just a question of when and who," Landis said in an interview. Rockets are too slow to send humans on interstellar missions. Instead, he envisions interstellar craft with extensive sails, propelled by laser light to about one-tenth the speed of light. It would take such a ship about 43 years to reach Alpha Centauri if it passed through the system without stopping. Slowing down to stop at Alpha Centauri could increase the trip to 100 years, whereas a journey without slowing down raises the issue of making sufficiently accurate and useful observations and measurements during a fly-by.

The 100 Year Starship (100YSS) is the name of the overall effort that will, over the next century, work toward achieving interstellar travel. The effort will also go by the moniker 100YSS. The 100 Year Starship study is the name of a one-year project to assess the attributes of and lay the groundwork for an organization that can carry forward the 100 Year Starship vision.

Harold ("Sonny") White from NASA's Johnson Space Center is a member of Icarus Interstellar, the nonprofit foundation whose mission is to realize interstellar flight before the year 2100. At the 2012 meeting of 100YSS, he reported using a laser to try to warp spacetime by 1 part in 10 million with the aim of helping to make interstellar travel possible.


A few organisations dedicated to interstellar propulsion research and advocacy for the case exist worldwide. These are still in their infancy, but are already backed up by a membership of a wide variety of scientists, students and professionals.

The energy requirements make interstellar travel very difficult. It has been reported that at the 2008 Joint Propulsion Conference, multiple experts opined that it was improbable that humans would ever explore beyond the Solar System. Brice N. Cassenti, an associate professor with the Department of Engineering and Science at Rensselaer Polytechnic Institute, stated that at least 100 times the total energy output of the entire world [in a given year] would be required to send a probe to the nearest star.

Astrophysicist Sten Odenwald stated that the basic problem is that through intensive studies of thousands of detected exoplanets, most of the closest destinations within 50 light years do not yield Earth-like planets in the star's habitable zones. Given the multitrillion-dollar expense of some of the proposed technologies, travelers will have to spend up to 200 years traveling at 20% the speed of light to reach the best known destinations. Moreover, once the travelers arrive at their destination (by any means), they will not be able to travel down to the surface of the target world and set up a colony unless the atmosphere is non-lethal. The prospect of making such a journey, only to spend the rest of the colony's life inside a sealed habitat and venturing outside in a spacesuit, may eliminate many prospective targets from the list.

Moving at a speed close to the speed of light and encountering even a tiny stationary object like a grain of sand will have fatal consequences. For example, a gram of matter moving at 90% of the speed of light contains a kinetic energy corresponding to a small nuclear bomb (around 30kt TNT).

Explorative high-speed missions to Alpha Centauri, as planned for by the Breakthrough Starshot initiative, are projected to be realizable within the 21st century. It is alternatively possible to plan for uncrewed slow-cruising missions taking millennia to arrive. These probes would not be for human benefit in the sense that one can not foresee whether there would be anybody around on earth interested in then back-transmitted science data. An example would be the Genesis mission, which aims to bring unicellular life, in the spirit of directed panspermia, to habitable but otherwise barren planets. Comparatively slow cruising Genesis probes, with a typical speed of formula_5, corresponding to about formula_6, can be decelerated using a magnetic sail. Uncrewed missions not for human benefit would hence be feasible.

In February 2017, NASA announced that its Spitzer Space Telescope had revealed seven Earth-size planets in the TRAPPIST-1 system orbiting an ultra-cool dwarf star 40 light-years away from our solar system. Three of these planets are firmly located in the habitable zone, the area around the parent star where a rocky planet is most likely to have liquid water. The discovery sets a new record for greatest number of habitable-zone planets found around a single star outside our solar system. All of these seven planets could have liquid water – the key to life as we know it – under the right atmospheric conditions, but the chances are highest with the three in the habitable zone.





</doc>
<doc id="14844" url="https://en.wikipedia.org/wiki?curid=14844" title="Interior Gateway Routing Protocol">
Interior Gateway Routing Protocol

Interior Gateway Routing Protocol (IGRP) is a distance vector interior gateway protocol (IGP) developed by Cisco. It is used by routers to exchange routing data within an autonomous system.

IGRP is a proprietary protocol. IGRP was created in part to overcome the limitations of RIP (maximum hop count of only 15, and a single routing metric) when used within large networks. IGRP supports multiple metrics for each route, including bandwidth, delay, load, and reliability; to compare two routes these metrics are combined together into a single metric, using a formula which can be adjusted through the use of pre-set constants. By default, the IGRP composite metric is a sum of the segment delays and the lowest segment bandwidth. The maximum configurable hop count of IGRP-routed packets is 255 (default 100), and routing updates are broadcast every 90 seconds (by default). IGRP uses protocol number 9 for communication.

IGRP is considered a classful routing protocol. Because the protocol has no field for a subnet mask, the router assumes that all subnetwork addresses within the same Class A, Class B, or Class C network have the same subnet mask as the subnet mask configured for the interfaces in question. This contrasts with classless routing protocols that can use variable length subnet masks. Classful protocols have become less popular as they are wasteful of IP address space.

In order to address the issues of address space and other factors, Cisco created EIGRP (Enhanced Interior Gateway Routing Protocol). EIGRP adds support for VLSM (variable length subnet mask) and adds the Diffusing Update Algorithm (DUAL) in order to improve routing and provide a loopless environment. EIGRP has completely replaced IGRP, making IGRP an obsolete routing protocol. In Cisco IOS versions 12.3 and greater, IGRP is completely unsupported. In the new Cisco CCNA curriculum (version 4), IGRP is mentioned only briefly, as an "obsolete protocol".



</doc>
<doc id="14845" url="https://en.wikipedia.org/wiki?curid=14845" title="IRS (disambiguation)">
IRS (disambiguation)

IRS is the United States Internal Revenue Service. 

IRS may also refer to:








</doc>
<doc id="14848" url="https://en.wikipedia.org/wiki?curid=14848" title="Indo-European languages">
Indo-European languages

The Indo-European languages are a large language family native to western Eurasia. It comprises most of the languages of Europe together with those of the northern Indian Subcontinent and the Iranian Plateau. A few of these languages, such as English and Spanish, have expanded through colonialism in the modern period and are now spoken across all continents. The Indo-European family is divided into several branches or sub-families, the largest of which are the Indo-Iranian, Germanic, Romance, and Balto-Slavic groups. The most populous individual languages within them are Spanish, English, Hindustani (Hindi/Urdu), Portuguese, Bengali, Punjabi, and Russian, each with over 100 million speakers. German, French, Marathi, Italian, and Persian have more than 50 million each. In total, 46% of the world's population (3.2 billion) speaks an Indo-European language as a first language, by far the highest of any language family. There are about 445 living Indo-European languages, according to the estimate by "Ethnologue", with over two thirds (313) of them belonging to the Indo-Iranian branch.

All Indo-European languages are descendants of a single prehistoric language, reconstructed as Proto-Indo-European, spoken sometime in the Neolithic era. Its precise geographical location, the Indo-European "urheimat", is unknown and has been the object of many competing hypotheses; the most widely accepted is the Kurgan hypothesis, which posits the "urheimat" to be the Pontic–Caspian steppe, associated with the Yamnaya culture around 3000 BC. By the time the first written records appeared, Indo-European had already evolved into numerous languages spoken across much of Europe and south-west Asia. Written evidence of Indo-European appeared during the Bronze Age in the form of Mycenaean Greek and the Anatolian languages, Hittite and Luwian. The oldest records are isolated Hittite words and names – interspersed in texts that are otherwise in the unrelated Old Assyrian language, a Semitic language – found in the texts of the Assyrian colony of Kültepe in eastern Anatolia in the 20th century BC. Although no older written records of the original Proto-Indo-Europeans remain, some aspects of their culture and religion can be reconstructed from later evidence in the daughter cultures. The Indo-European family is significant to the field of historical linguistics as it possesses the second-longest recorded history of any known family, after the Afroasiatic family in the form of the Egyptian language and the Semitic languages. The analysis of the family relationships between the Indo-European languages and the reconstruction of their common source was central to the development of the methodology of historical linguistics as an academic discipline in the 19th century.

The Indo-European family is not known to be linked to any other language family through any more distant genetic relationship, although several disputed proposals to that effect have been made.

During the nineteenth century, the linguistic concept of Indo-European languages was frequently used interchangeably with the racial concepts of Aryan and Japhetite.

In the 16th century, European visitors to the Indian subcontinent began to notice similarities among Indo-Aryan, Iranian, and European languages. In 1583, English Jesuit missionary and Konkani scholar Thomas Stephens wrote a letter from Goa to his brother (not published until the 20th century) in which he noted similarities between Indian languages and Greek and Latin.

Another account was made by Filippo Sassetti, a merchant born in Florence in 1540, who travelled to the Indian subcontinent. Writing in 1585, he noted some word similarities between Sanskrit and Italian (these included "devaḥ"/"dio" "God", "sarpaḥ"/"serpe" "serpent", "sapta"/"sette" "seven", "aṣṭa"/"otto" "eight", and "nava"/"nove" "nine"). However, neither Stephens' nor Sassetti's observations led to further scholarly inquiry.

In 1647, Dutch linguist and scholar Marcus Zuerius van Boxhorn noted the similarity among certain Asian and European languages and theorized that they were derived from a primitive common language which he called Scythian. He included in his hypothesis Dutch, Albanian, Greek, Latin, Persian, and German, later adding Slavic, Celtic, and Baltic languages. However, Van Boxhorn's suggestions did not become widely known and did not stimulate further research.

Ottoman Turkish traveler Evliya Çelebi visited Vienna in 1665–1666 as part of a diplomatic mission and noted a few similarities between words in German and in Persian.
Gaston Coeurdoux and others made observations of the same type. Coeurdoux made a thorough comparison of Sanskrit, Latin and Greek conjugations in the late 1760s to suggest a relationship among them. Meanwhile, Mikhail Lomonosov compared different language groups, including Slavic, Baltic ("Kurlandic"), Iranian ("Medic"), Finnish, Chinese, "Hottentot" (Khoekhoe), and others, noting that related languages (including Latin, Greek, German and Russian) must have separated in antiquity from common ancestors.

The hypothesis reappeared in 1786 when Sir William Jones first lectured on the striking similarities among three of the oldest languages known in his time: Latin, Greek, and Sanskrit, to which he tentatively added Gothic, Celtic, and Persian, though his classification contained some inaccuracies and omissions. In one of the most famous quotations in linguistics, Jones made the following prescient statement in a lecture to the Asiatic Society of Bengal in 1786, conjecturing the existence of an earlier ancestor language, which he called "a common source" but did not name:
Thomas Young first used the term "Indo-European" in 1813, deriving from the geographical extremes of the language family: from Western Europe to North India. A synonym is Indo-Germanic ("Idg." or "IdG."), specifying the family's southeasternmost and northwesternmost branches. This first appeared in French ("indo-germanique") in 1810 in the work of Conrad Malte-Brun; in most languages this term is now dated or less common than "Indo-European", although in German "indogermanisch" remains the standard scientific term. A number of other synonymous terms have also been used.

Franz Bopp wrote in 1816 "On the conjugational system of the Sanskrit language compared with that of Greek, Latin, Persian and Germanic" and between 1833 and 1852 he wrote "Comparative Grammar". This marks the beginning of Indo-European studies as an academic discipline. The classical phase of Indo-European comparative linguistics leads from this work to August Schleicher's 1861 "Compendium" and up to Karl Brugmann's "Grundriss", published in the 1880s. Brugmann's neogrammarian reevaluation of the field and Ferdinand de Saussure's development of the laryngeal theory may be considered the beginning of "modern" Indo-European studies. The generation of Indo-Europeanists active in the last third of the 20th century (such as Calvert Watkins, Jochem Schindler, and Helmut Rix) developed a better understanding of morphology and of ablaut in the wake of Kuryłowicz's 1956 "Apophony in Indo-European," who in 1927 pointed out the existence of the Hittite consonant ḫ. Kuryłowicz's discovery supported Ferdinand de Saussure's 1879 proposal of the existence of "coefficients sonantiques", elements de Saussure reconstructed to account for vowel length alternations in Indo-European languages. This led to the so-called laryngeal theory, a major step forward in Indo-European linguistics and a confirmation of de Saussure's theory.

The various subgroups of the Indo-European language family include ten major branches, listed below in alphabetical order:

In addition to the classical ten branches listed above, several extinct and little-known languages and language-groups have existed or are proposed to have existed:

Membership of languages in the Indo-European language family is determined by genealogical relationships, meaning that all members are presumed descendants of a common ancestor, Proto-Indo-European. Membership in the various branches, groups and subgroups of Indo-European is also genealogical, but here the defining factors are "shared innovations" among various languages, suggesting a common ancestor that split off from other Indo-European groups. For example, what makes the Germanic languages a branch of Indo-European is that much of their structure and phonology can be stated in rules that apply to all of them. Many of their common features are presumed innovations that took place in Proto-Germanic, the source of all the Germanic languages.

The "tree model" is considered an appropriate representation of the genealogical history of a language family if communities do not remain in contact after their languages have started to diverge. In this case, subgroups defined by shared innovations form a nested pattern. The tree model is not appropriate in cases where languages remain in contact as they diversify; in such cases subgroups may overlap, and the "wave model" is a more accurate representation. Most approaches to Indo-European subgrouping to date have assumed that the tree model is by-and-large valid for Indo-European; however, there is also a long tradition of wave-model approaches.

In addition to genealogical changes, many of the early changes in Indo-European languages can be attributed to language contact. It has been asserted, for example, that many of the more striking features shared by Italic languages (Latin, Oscan, Umbrian, etc.) might well be areal features. More certainly, very similar-looking alterations in the systems of long vowels in the West Germanic languages greatly postdate any possible notion of a proto-language innovation (and cannot readily be regarded as "areal", either, because English and continental West Germanic were not a linguistic area). In a similar vein, there are many similar innovations in Germanic and Balto-Slavic that are far more likely areal features than traceable to a common proto-language, such as the uniform development of a high vowel (*"u" in the case of Germanic, *"i/u" in the case of Baltic and Slavic) before the PIE syllabic resonants *"ṛ, *ḷ, *ṃ, *ṇ", unique to these two groups among IE languages, which is in agreement with the wave model. The Balkan sprachbund even features areal convergence among members of very different branches.

An extension to the "Ringe-Warnow model of language evolution", suggests that early IE had featured limited contact between distinct lineages, with only the Germanic subfamily exhibiting a less treelike behaviour as it acquired some characteristics from neighbours early in its evolution. The internal diversification of especially West Germanic is cited to have been radically non-treelike.

Specialists have postulated the existence of higher-order subgroups such as Italo-Celtic, Graeco-Armenian, Graeco-Aryan or Graeco-Armeno-Aryan, and Balto-Slavo-Germanic. However, unlike the ten traditional branches, these are all controversial to a greater or lesser degree.

The Italo-Celtic subgroup was at one point uncontroversial, considered by Antoine Meillet to be even better established than Balto-Slavic. The main lines of evidence included the genitive suffix "-ī"; the superlative suffix "-m̥mo"; the change of /p/ to /kʷ/ before another /kʷ/ in the same word (as in "penkʷe" > "*kʷenkʷe" > Latin "quīnque", Old Irish "cóic"); and the subjunctive morpheme "-ā-". This evidence was prominently challenged by Calvert Watkins; while Michael Weiss has argued for the subgroup.

Evidence for a relationship between Greek and Armenian includes the regular change of the second laryngeal to "a" at the beginnings of words, as well as terms for "woman" and "sheep". Greek and Indo-Iranian share innovations mainly in verbal morphology and patterns of nominal derivation. Relations have also been proposed between Phrygian and Greek, and between Thracian and Armenian. Some fundamental shared features, like the aorist (a verb form denoting action without reference to duration or completion) having the perfect active particle -s fixed to the stem, link this group closer to Anatolian languages and Tocharian. Shared features with Balto-Slavic languages, on the other hand (especially present and preterit formations), might be due to later contacts.

The Indo-Hittite hypothesis proposes that the Indo-European language family consists of two main branches: one represented by the Anatolian languages and another branch encompassing all other Indo-European languages. Features that separate Anatolian from all other branches of Indo-European (such as the gender or the verb system) have been interpreted alternately as archaic debris or as innovations due to prolonged isolation. Points proffered in favour of the Indo-Hittite hypothesis are the (non-universal) Indo-European agricultural terminology in Anatolia and the preservation of laryngeals. However, in general this hypothesis is considered to attribute too much weight to the Anatolian evidence. According to another view, the Anatolian subgroup left the Indo-European parent language comparatively late, approximately at the same time as Indo-Iranian and later than the Greek or Armenian divisions. A third view, especially prevalent in the so-called French school of Indo-European studies, holds that extant similarities in non-satem languages in general—including Anatolian—might be due to their peripheral location in the Indo-European language-area and to early separation, rather than indicating a special ancestral relationship. Hans J. Holm, based on lexical calculations, arrives at a picture roughly replicating the general scholarly opinion and refuting the Indo-Hittite hypothesis.

The division of the Indo-European languages into satem and centum groups was put forward by Peter von Bradke in 1890, although Karl Brugmann did propose a similar type of division in 1886. In the satem languages, which include the Balto-Slavic and Indo-Iranian branches, as well as (in most respects) Albanian and Armenian, the reconstructed Proto-Indo-European palatovelars remained distinct and were fricativized, while the labiovelars merged with the 'plain velars'. In the centum languages, the palatovelars merged with the plain velars, while the labiovelars remained distinct. The results of these alternative developments are exemplified by the words for "hundred" in Avestan ("satem") and Latin ("centum")—the initial palatovelar developed into a fricative in the former, but became an ordinary velar in the latter.

Rather than being a genealogical separation, the centum–satem division is commonly seen as resulting from innovative changes that spread across PIE dialect-branches over a particular geographical area; the centum–satem isogloss intersects a number of other isoglosses that mark distinctions between features in the early IE branches. It may be that the centum branches in fact reflect the original state of affairs in PIE, and only the satem branches shared a set of innovations, which affected all but the peripheral areas of the PIE dialect continuum. Kortlandt proposes that the ancestors of Balts and Slavs took part in satemization before being drawn later into the western Indo-European sphere.

Some linguists propose that Indo-European languages form part of one of several hypothetical macrofamilies. However, these theories remain highly controversial and are not accepted by most linguists in the field. Some of the smaller proposed macrofamilies include:

Other, greater proposed families including Indo-European languages, include:

Objections to such groupings are not based on any theoretical claim about the likely historical existence or non-existence of such macrofamilies; it is entirely reasonable to suppose that they might have existed. The serious difficulty lies in identifying the details of actual relationships between language families, because it is very hard to find concrete evidence that transcends chance resemblance, or is not equally likely explained as being due to borrowing (including Wanderwörter, which can travel very long distances). Because the signal-to-noise ratio in historical linguistics declines over time, at great enough time-depths it becomes open to reasonable doubt that one can even distinguish between signal and noise.

The proposed Proto-Indo-European language (PIE) is the reconstructed common ancestor of the Indo-European languages, spoken by the Proto-Indo-Europeans. From the 1960s, knowledge of Anatolian became certain enough to establish its relationship to PIE. Using the method of internal reconstruction, an earlier stage, called Pre-Proto-Indo-European, has been proposed.

PIE was an inflected language, in which the grammatical relationships between words were signaled through inflectional morphemes (usually endings). The roots of PIE are basic morphemes carrying a lexical meaning. By addition of suffixes, they form stems, and by addition of endings, these form grammatically inflected words (nouns or verbs). The reconstructed Indo-European verb system is complex and, like the noun, exhibits a system of ablaut.

The diversification of the parent language into the attested branches of daughter languages is historically unattested. The timeline of the evolution of the various daughter languages, on the other hand, is mostly undisputed, quite regardless of the question of Indo-European origins.

Using a mathematical analysis borrowed from evolutionary biology, Don Ringe and Tandy Warnow propose the following evolutionary tree of Indo-European branches:

David Anthony proposes the following sequence:

From 1500 BC the following sequence may be given:


In reconstructing the history of the Indo-European languages and the form of the Proto-Indo-European language, some languages have been of particular importance. These generally include the ancient Indo-European languages that are both well-attested and documented at an early date, although some languages from later periods are important if they are particularly linguistically conservative (most notably, Lithuanian). Early poetry is of special significance because of the rigid poetic meter normally employed, which makes it possible to reconstruct a number of features (e.g. vowel length) that were either unwritten or corrupted in the process of transmission down to the earliest extant written manuscripts.

Most noticeable of all:

Other primary sources:

Other secondary sources, of lesser value due to poor attestation:

Other secondary sources, of lesser value due to extensive phonological changes and relatively limited attestation:

As the Proto-Indo-European (PIE) language broke up, its sound system diverged as well, changing according to various sound laws evidenced in the daughter languages.

PIE is normally reconstructed with a complex system of 15 stop consonants, including an unusual three-way phonation (voicing) distinction between voiceless, voiced and "voiced aspirated" (i.e. breathy voiced) stops, and a three-way distinction among velar consonants ("k"-type sounds) between "palatal" "ḱ ǵ ǵh", "plain velar" "k g gh" and labiovelar "kʷ gʷ gʷh". (The correctness of the terms "palatal" and "plain velar" is disputed; see Proto-Indo-European phonology.) All daughter languages have reduced the number of distinctions among these sounds, often in divergent ways.

As an example, in English, one of the Germanic languages, the following are some of the major changes that happened:
None of the daughter-language families (except possibly Anatolian, particularly Luvian) reflect the plain velar stops differently from the other two series, and there is even a certain amount of dispute whether this series existed at all in PIE. The major distinction between "centum" and "satem" languages corresponds to the outcome of the PIE plain velars:

The three-way PIE distinction between voiceless, voiced and voiced aspirated stops is considered extremely unusual from the perspective of linguistic typology—particularly in the existence of voiced aspirated stops without a corresponding series of voiceless aspirated stops. None of the various daughter-language families continue it unchanged, with numerous "solutions" to the apparently unstable PIE situation:

Among the other notable changes affecting consonants are:

The following table shows the basic outcomes of PIE consonants in some of the most important daughter languages for the purposes of reconstruction. For a fuller table, see Indo-European sound laws.


The following table presents a comparison of conjugations of the thematic present indicative of the verbal root * of the English verb "to bear" and its reflexes in various early attested IE languages and their modern descendants or relatives, showing that all languages had in the early stage an inflectional verb system.

While similarities are still visible between the modern descendants and relatives of these ancient languages, the differences have increased over time. Some IE languages have moved from synthetic verb systems to largely periphrastic systems. In addition, the pronouns of periphrastic forms are in brackets when they appear. Some of these verbs have undergone a change in meaning as well.

Today, Indo-European languages are spoken by 3.2 billion native speakers across all inhabited continents, the largest number by far for any recognised language family. Of the 20 languages with the largest numbers of native speakers according to "Ethnologue", 10 are Indo-European: Spanish, English, Hindustani, Portuguese, Bengali, Russian, Punjabi, German, French, and Marathi, accounting for over 1.7 billion native speakers. Additionally, hundreds of millions of persons worldwide study Indo-European languages as secondary or tertiary languages, including in cultures which have completely different language families and historical backgrounds—there are between 600 million and one billion L2 learners of English alone.

The success of the language family, including the large number of speakers and the vast portions of the Earth that they inhabit, is due to several factors. The ancient Indo-European migrations and widespread dissemination of Indo-European culture throughout Eurasia, including that of the Proto-Indo-Europeans themselves, and that of their daughter cultures including the Indo-Aryans, Iranian peoples, Celts, Greeks, Romans, Germanic peoples, and Slavs, led to these peoples' branches of the language family already taking a dominant foothold in virtually all of Eurasia except for swathes of the Near East, North and East Asia, replacing many (but not all) of the previously-spoken pre-Indo-European languages of this extensive area. However Semitic languages remain dominant in much of the Middle East and North Africa, and Caucasian languages in much of the Caucasus region. Similarly in Europe and the Urals the Uralic languages (such as Hungarian, Finnish, Estonian etc) remain, as does Basque, a pre-Indo-European Isolate.

Despite being unaware of their common linguistic origin, diverse groups of Indo-European speakers continued to culturally dominate and often replace the indigenous languages of the western two-thirds of Eurasia. By the beginning of the Common Era, Indo-European peoples controlled almost the entirety of this area: the Celts western and central Europe, the Romans southern Europe, the Germanic peoples northern Europe, the Slavs eastern Europe, the Iranian peoples most of western and central Asia and parts of eastern Europe, and the Indo-Aryan peoples in the Indian subcontinent, with the Tocharians inhabiting the Indo-European frontier in western China. By the medieval period, only the Semitic, Dravidian, Caucasian, and Uralic languages, and the language isolate Basque remained of the (relatively) indigenous languages of Europe and the western half of Asia.

Despite medieval invasions by Eurasian nomads, a group to which the Proto-Indo-Europeans had once belonged, Indo-European expansion reached another peak in the early modern period with the dramatic increase in the population of the Indian subcontinent and European expansionism throughout the globe during the Age of Discovery, as well as the continued replacement and assimilation of surrounding non-Indo-European languages and peoples due to increased state centralization and nationalism. These trends compounded throughout the modern period due to the general global population growth and the results of European colonization of the Western Hemisphere and Oceania, leading to an explosion in the number of Indo-European speakers as well as the territories inhabited by them.

Due to colonization and the modern dominance of Indo-European languages in the fields of politics, global science, technology, education, finance, and sports, even many modern countries whose populations largely speak non-Indo-European languages have Indo-European languages as official languages, and the majority of the global population speaks at least one Indo-European language. The overwhelming majority of languages used on the Internet are Indo-European, with English continuing to lead the group; English in general has in many respects become the "lingua franca" of global communication.






</doc>
<doc id="14849" url="https://en.wikipedia.org/wiki?curid=14849" title="Illinois">
Illinois

Illinois ( ) is a state in the Midwestern and Great Lakes regions of the United States. It has the fifth largest gross domestic product (GDP),
the sixth largest population, and the 25th largest land area of all U.S. states. Illinois has been noted as a microcosm of the entire United States. With Chicago in northeastern Illinois, small industrial cities and immense agricultural productivity in the north and center of the state, and natural resources such as coal, timber, and petroleum in the south, Illinois has a diverse economic base, and is a major transportation hub. The Port of Chicago connects the state to international ports via two main routes: from the Great Lakes, via the Saint Lawrence Seaway, to the Atlantic Ocean and from the Great Lakes to the Mississippi River, via the Illinois River, through the Illinois Waterway. The Mississippi River, the Ohio River, and the Wabash River form parts of the boundaries of Illinois. For decades, Chicago's O'Hare International Airport has been ranked as one of the world's busiest airports. Illinois has long had a reputation as a bellwether both in social and cultural terms and, through the 1980s, in politics.

The capital of Illinois is Springfield, which is located in the central part of the state. Although today Illinois's largest population center is in its northeast, the state's European population grew first in the west as the French settled lands near the Mississippi River, when the region was known as Illinois Country and was part of New France. Following the American Revolutionary War, American settlers began arriving from Kentucky in the 1780s via the Ohio River, and the population grew from south to north. In 1818, Illinois achieved statehood. Following increased commercial activity in the Great Lakes after the construction of the Erie Canal, Chicago was incorporated in the 1830s on the banks of the Chicago River at one of the few natural harbors on the southern section of Lake Michigan. John Deere's invention of the self-scouring steel plow turned Illinois's rich prairie into some of the world's most productive and valuable farmland, attracting immigrant farmers from Germany and Sweden. The Illinois and Michigan Canal (1848) made transportation between the Great Lakes and the Mississippi River valley faster and cheaper, and new railroads carried immigrants to new homes in the country's west and shipped commodity crops to the nation's east. The state became a transportation hub for the nation.

By 1900, the growth of industrial jobs in the northern cities and coal mining in the central and southern areas attracted immigrants from Eastern and Southern Europe. Illinois was an important manufacturing center during both world wars. The Great Migration from the South established a large community of African Americans in the state, including Chicago, who founded the city's famous jazz and blues cultures. Chicago, the center of the Chicago Metropolitan Area, is now recognized as a global city. Chicagoland, Chicago's metropolitan area, encompasses about 65% of the state's population. The most populous metropolitan areas outside the Chicago area include, Metro East (of Greater St. Louis), Peoria and Rockford.
Three U.S. presidents have been elected while living in Illinois: Abraham Lincoln, Ulysses S. Grant, and Barack Obama. Additionally, Ronald Reagan, whose political career was based in California, was born and raised in the state. Today, Illinois honors Lincoln with its official state slogan "Land of Lincoln", which has been displayed on its license plates since 1954. The state is the site of the Abraham Lincoln Presidential Library and Museum in Springfield and the future home of the Barack Obama Presidential Center in Chicago.

"Illinois" is the modern spelling for the early French Catholic missionaries and explorers' name for the Illinois Native Americans, a name that was spelled in many different ways in the early records.

American scholars previously thought the name "Illinois" meant "man" or "men" in the Miami-Illinois language, with the original "iliniwek" transformed via French into Illinois. This etymology is not supported by the Illinois language, as the word for "man" is "ireniwa", and plural of "man" is "ireniwaki". The name "Illiniwek" has also been said to mean "tribe of superior men", which is a false etymology. The name "Illinois" derives from the Miami-Illinois verb "irenwe·wa"—"he speaks the regular way". This was taken into the Ojibwe language, perhaps in the Ottawa dialect, and modified into "ilinwe·" (pluralized as "ilinwe·k"). The French borrowed these forms, changing the /we/ ending to spell it as "-ois", a transliteration for its pronunciation in French of that time. The current spelling form, "Illinois", began to appear in the early 1670s, when French colonists had settled in the western area. The Illinois' name for themselves, as attested in all three of the French missionary-period dictionaries of Illinois, was "Inoka", of unknown meaning and unrelated to the other terms.

American Indians of successive cultures lived along the waterways of the Illinois area for thousands of years before the arrival of Europeans. The Koster Site has been excavated and demonstrates 7,000 years of continuous habitation. Cahokia, the largest regional chiefdom and Urban Center of the Pre-Columbian Mississippian culture, was located near present-day Collinsville, Illinois. They built an urban complex of more than 100 platform and burial mounds, a plaza larger than 35 football fields, and a woodhenge of sacred cedar, all in a planned design expressing the culture's cosmology. Monks Mound, the center of the site, is the largest Pre-Columbian structure north of the Valley of Mexico. It is high, long, wide, and covers . It contains about of earth. It was topped by a structure thought to have measured about in length and in width, covered an area , and been as much as high, making its peak above the level of the plaza. The finely crafted ornaments and tools recovered by archaeologists at Cahokia include elaborate ceramics, finely sculptured stonework, carefully embossed and engraved copper and mica sheets, and one funeral blanket for an important chief fashioned from 20,000 shell beads. These artifacts indicate that Cahokia was truly an urban center, with clustered housing, markets, and specialists in toolmaking, hide dressing, potting, jewelry making, shell engraving, weaving and salt making. The civilization vanished in the 15th century for unknown reasons, but historians and archeologists have speculated that the people depleted the area of resources. Many indigenous tribes engaged in constant warfare. According to Suzanne Austin Alchon, "At one site in the central Illinois River valley, one third of all adults died as a result of violent injuries." The next major power in the region was the Illinois Confederation or Illini, a political alliance. As the Illini declined during the Beaver Wars era, members of the Algonquian-speaking Potawatomi, Miami, Sauk, and other tribes including the Fox (Mesquakie), Ioway, Kickapoo, Mascouten, Piankashaw, Shawnee, Wea, and Winnebago (Ho-Chunk) came into the area from the east and north around the Great Lakes.

French explorers Jacques Marquette and Louis Jolliet explored the Illinois River in 1673. Marquette soon after founded a mission at the Grand Village of the Illinois in Illinois Country. In 1680, French explorers under René-Robert Cavelier, Sieur de La Salle and Henri de Tonti constructed a fort at the site of present-day Peoria, and in 1682, a fort atop Starved Rock in today's Starved Rock State Park. French Empire Canadiens came south to settle particularly along the Mississippi River, and Illinois was part of first New France, and then of La Louisiane until 1763, when it passed to the British with their defeat of France in the Seven Years' War. The small French settlements continued, although many French migrated west to Ste. Genevieve and St. Louis, Missouri, to evade British rule.

A few British soldiers were posted in Illinois, but few British or American settlers moved there, as the Crown made it part of the territory reserved for Indians west of the Appalachians, and then part of the British Province of Quebec. In 1778, George Rogers Clark claimed Illinois County for Virginia. In a compromise, Virginia (and other states that made various claims) ceded the area to the new United States in the 1780s and it became part of the Northwest Territory, administered by the federal government and later organized as states.

The Illinois-Wabash Company was an early claimant to much of Illinois. The Illinois Territory was created on February 3, 1809, with its capital at Kaskaskia, an early French settlement.

During the discussions leading up to Illinois's admission to the Union, the proposed northern boundary of the state was moved twice. The original provisions of the Northwest Ordinance had specified a boundary that would have been tangent to the southern tip of Lake Michigan. Such a boundary would have left Illinois with no shoreline on Lake Michigan at all. However, as Indiana had successfully been granted a northern extension of its boundary to provide it with a usable lakefront, the original bill for Illinois statehood, submitted to Congress on January 23, 1818, stipulated a northern border at the same latitude as Indiana's, which is defined as 10 miles north of the southernmost extremity of Lake Michigan. However, the Illinois delegate, Nathaniel Pope, wanted more, and lobbied to have the boundary moved further north. The final bill passed by Congress included an amendment to shift the border to 42° 30' north, which is approximately north of the Indiana northern border. This shift added to the state, including the lead mining region near Galena. More importantly, it added nearly 50 miles of Lake Michigan shoreline and the Chicago River. Pope and others envisioned a canal that would connect the Chicago and Illinois rivers and thus connect the Great Lakes to the Mississippi.

In 1818, Illinois became the 21st U.S. state. The capital remained at Kaskaskia, headquartered in a small building rented by the state. In 1819, Vandalia became the capital, and over the next 18 years, three separate buildings were built to serve successively as the capitol building. In 1837, the state legislators representing Sangamon County, under the leadership of state representative Abraham Lincoln, succeeded in having the capital moved to Springfield, where a fifth capitol building was constructed. A sixth capitol building was erected in 1867, which continues to serve as the Illinois capitol today.

Though it was ostensibly a "free state", there was nonetheless slavery in Illinois. The ethnic French had owned black slaves since the 1720s, and American settlers had already brought slaves into the area from Kentucky. Slavery was nominally banned by the Northwest Ordinance, but that was not enforced for those already holding slaves. When Illinois became a sovereign state in 1818, the Ordinance no longer applied, and about 900 slaves were held in the state. As the southern part of the state, later known as "Egypt" or "Little Egypt", was largely settled by migrants from the South, the section was hostile to free blacks. Settlers were allowed to bring slaves with them for labor, but, in 1822, state residents voted against making slavery legal. Still, most residents opposed allowing free blacks as permanent residents. Some settlers brought in slaves seasonally or as house servants. The Illinois Constitution of 1848 was written with a provision for exclusionary laws to be passed. In 1853, John A. Logan helped pass a law to prohibit all African Americans, including freedmen, from settling in the state.
The winter of 1830–1831 is called the "Winter of the Deep Snow"; a sudden, deep snowfall blanketed the state, making travel impossible for the rest of the winter, and many travelers perished. Several severe winters followed, including the "Winter of the Sudden Freeze". On December 20, 1836, a fast-moving cold front passed through, freezing puddles in minutes and killing many travelers who could not reach shelter. The adverse weather resulted in crop failures in the northern part of the state. The southern part of the state shipped food north, and this may have contributed to its name: "Little Egypt", after the Biblical story of Joseph in Egypt supplying grain to his brothers.

In 1832, the Black Hawk War was fought in Illinois and present-day Wisconsin between the United States and the Sauk, Fox (Meskwaki), and Kickapoo Indian tribes. It represents the end of Indian resistance to white settlement in the Chicago region. The Indians had been forced to leave their homes and move to Iowa in 1831; when they attempted to return, they were attacked and eventually defeated by U.S. militia. The survivors were forced back to Iowa.

By 1839, the Latter Day Saints had founded a utopian city called Nauvoo. Located in Hancock County along the Mississippi River, Nauvoo flourished, and soon rivaled Chicago for the position of the state's largest city. But in 1844, the Latter Day Saint movement founder Joseph Smith was killed in the Carthage Jail, about 30 miles away from Nauvoo. Following a succession crisis (Latter Day Saints), Brigham Young led most Latter Day Saints out of Illinois in a mass exodus to present-day Utah; after close to six years of rapid development, Nauvoo rapidly declined afterward.

After it was established in 1833, Chicago gained prominence as a Great Lakes port, and then as an Illinois and Michigan Canal port after 1848, and as a rail hub soon afterward. By 1857, Chicago was Illinois's largest city. With the tremendous growth of mines and factories in the state in the 19th century, Illinois was the ground for the formation of labor unions in the United States.

In 1847, after lobbying by Dorothea L. Dix, Illinois became one of the first states to establish a system of state-supported treatment of mental illness and disabilities, replacing local almshouses. Dix came into this effort after having met J. O. King, a Jacksonville, Illinois businessman, who invited her to Illinois, where he had been working to build an asylum for the insane. With the lobbying expertise of Dix, plans for the Jacksonville State Hospital (now known as the Jacksonville Developmental Center) were signed into law on March 1, 1847.

During the American Civil War, Illinois ranked fourth in men who served (more than 250,000) in the Union Army, a figure surpassed by only New York, Pennsylvania, and Ohio. Beginning with President Abraham Lincoln's first call for troops and continuing throughout the war, Illinois mustered 150 infantry regiments, which were numbered from the 7th to the 156th regiments. Seventeen cavalry regiments were also gathered, as well as two light artillery regiments. The town of Cairo, at the southern tip of the state at the confluence of the Mississippi and Ohio Rivers, served as a strategically important supply base and training center for the Union army. For several months, both General Grant and Admiral Foote had headquarters in Cairo.

During the Civil War, and more so afterwards, Chicago's population skyrocketed, which increased its prominence. The Pullman Strike and Haymarket Riot, in particular, greatly influenced the development of the American labor movement. From Sunday, October 8, 1871, until Tuesday, October 10, 1871, the Great Chicago Fire burned in downtown Chicago, destroying .

At the turn of the 20th century, Illinois had a population of nearly 5 million. Many people from other parts of the country were attracted to the state by employment caused by the expanding industrial base. Whites were 98% of the state's population. Bolstered by continued immigration from southern and eastern Europe, and by the African-American Great Migration from the South, Illinois grew and emerged as one of the most important states in the union. By the end of the century, the population had reached 12.4 million.

The Century of Progress World's fair was held at Chicago in 1933. Oil strikes in Marion County and Crawford County led to a boom in 1937, and by 1939, Illinois ranked fourth in U.S. oil production. Illinois manufactured 6.1 percent of total United States military armaments produced during World War II, ranking seventh among the 48 states. Chicago became an ocean port with the opening of the Saint Lawrence Seaway in 1959. The seaway and the Illinois Waterway connected Chicago to both the Mississippi River and the Atlantic Ocean. In 1960, Ray Kroc opened the first McDonald's franchise in Des Plaines (which still exists as a museum, with a working McDonald's across the street).

Illinois had a prominent role in the emergence of the nuclear age. In 1942, as part of the Manhattan Project, the University of Chicago conducted the first sustained nuclear chain reaction. In 1957, Argonne National Laboratory, near Chicago, activated the first experimental nuclear power generating system in the United States. By 1960, the first privately financed nuclear plant in the United States, Dresden 1, was dedicated near Morris. In 1967, Fermilab, a national nuclear research facility near Batavia, opened a particle accelerator, which was the world's largest for over 40 years. With eleven plants currently operating, Illinois leads all states in the amount of electricity generated from nuclear power.

In 1961, Illinois became the first state in the nation to adopt the recommendation of the American Law Institute and pass a comprehensive criminal code revision that repealed the law against sodomy. The code also abrogated common law crimes and established an age of consent of 18. The state's fourth constitution was adopted in 1970, replacing the 1870 document.

The first Farm Aid concert was held in Champaign to benefit American farmers, in 1985. The worst upper Mississippi River flood of the century, the Great Flood of 1993, inundated many towns and thousands of acres of farmland.

On August 28, 2017, Illinois Governor Bruce Rauner signed a bill into law that prohibited state and local police from arresting anyone solely due to their immigration status or due to federal detainers. Some fellow Republicans criticized Rauner for his action, claiming the bill made Illinois a sanctuary state.

Illinois is located in the Midwest region of the United States and is one of the eight states and Ontario, Canada, in the Great Lakes region of North America.

Illinois's eastern border with Indiana consists of a north–south line at 87° 31′ 30″ west longitude in Lake Michigan at the north, to the Wabash River in the south above Post Vincennes. The Wabash River continues as the eastern/southeastern border with Indiana until the Wabash enters the Ohio River. This marks the beginning of Illinois's southern border with Kentucky, which runs along the northern shoreline of the Ohio River. Most of the western border with Missouri and Iowa is the Mississippi River; Kaskaskia is an exclave of Illinois, lying west of the Mississippi and reachable only from Missouri. The state's northern border with Wisconsin is fixed at 42° 30′ north latitude. The northeastern border of Illinois lies in Lake Michigan, within which Illinois shares a water boundary with the state of Michigan, as well as Wisconsin and Indiana.

Though Illinois lies entirely in the Interior Plains, it does have some minor variation in its elevation. In extreme northwestern Illinois, the Driftless Area, a region of unglaciated and therefore higher and more rugged topography, occupies a small part of the state. Southern Illinois includes the hilly areas around the Shawnee National Forest.

Charles Mound, located in the Driftless region, has the state's highest natural elevation above sea level at . Other highlands include the Shawnee Hills in the south, and there is varying topography along its rivers; the Illinois River bisects the state northeast to southwest. The floodplain on the Mississippi River from Alton to the Kaskaskia River is known as the American Bottom.

Illinois has three major geographical divisions. Northern Illinois is dominated by Chicago metropolitan area, or Chicagoland, which is the city of Chicago and its suburbs, and the adjoining exurban area into which the metropolis is expanding. As defined by the federal government, the Chicago metro area includes several counties in Illinois, Indiana, and Wisconsin, and has a population of over 9.8 million. Chicago itself is a cosmopolitan city, densely populated, industrialized, the transportation hub of the nation, and settled by a wide variety of ethnic groups. The city of Rockford, Illinois's third-largest city and center of the state's fourth largest metropolitan area, sits along Interstates 39 and 90 some northwest of Chicago. The Quad Cities region, located along the Mississippi River in northern Illinois, had a population of 381,342 in 2011.

The midsection of Illinois is the second major division, called Central Illinois. It is an area of mainly prairie and known as the Heart of Illinois. It is characterized by small towns and medium–small cities. The western section (west of the Illinois River) was originally part of the Military Tract of 1812 and forms the conspicuous western bulge of the state. Agriculture, particularly corn and soybeans, as well as educational institutions and manufacturing centers, figure prominently in Central Illinois. Cities include Peoria; Springfield, the state capital; Quincy; Decatur; Bloomington-Normal; and Champaign-Urbana.

The third division is Southern Illinois, comprising the area south of U.S. Route 50, including Little Egypt, near the juncture of the Mississippi River and Ohio River. Southern Illinois is the site of the ancient city of Cahokia, as well as the site of the first state capital at Kaskaskia, which today is separated from the rest of the state by the Mississippi River. This region has a somewhat warmer winter climate, different variety of crops (including some cotton farming in the past), more rugged topography (due to the area remaining unglaciated during the Illinoian Stage, unlike most of the rest of the state), as well as small-scale oil deposits and coal mining. The Illinois suburbs of St. Louis, such as East St. Louis, are located in this region, and collectively, they are known as the Metro-East. The other somewhat significant concentration of population in Southern Illinois is the Carbondale-Marion-Herrin, Illinois Combined Statistical Area centered on Carbondale and Marion, a two-county area that is home to 123,272 residents. A portion of southeastern Illinois is part of the extended Evansville, Indiana, Metro Area, locally referred to as the Tri-State with Indiana and Kentucky. Seven Illinois counties are in the area.

In addition to these three, largely latitudinally defined divisions, all of the region outside the Chicago Metropolitan area is often called "downstate" Illinois. This term is flexible, but is generally meant to mean everything outside the influence of the Chicago area. Thus, some cities in "Northern" Illinois, such as DeKalb, which is west of Chicago, and Rockford—which is actually north of Chicago—are sometimes incorrectly considered to be 'downstate'.

Illinois has a climate that varies widely throughout the year. Because of its nearly 400-mile distance between its northernmost and southernmost extremes, as well as its mid-continental situation, most of Illinois has a humid continental climate (Köppen climate classification "Dfa"), with hot, humid summers and cold winters. The southern part of the state, from about Carbondale southward, has a humid subtropical climate (Koppen "Cfa"), with more moderate winters. Average yearly precipitation for Illinois varies from just over at the southern tip to around in the northern portion of the state. Normal annual snowfall exceeds in the Chicago area, while the southern portion of the state normally receives less than . The all-time high temperature was , recorded on July 14, 1954, at East St. Louis, and the all-time low temperature was , recorded on January 31, 2019, during the January 2019 North American cold wave at a weather station near Mount Carroll, and confirmed on March 5, 2019. This followed the previous record of recorded on January 5, 1999, near Congerville. Prior to the Mount Carroll record, a temperature of was recorded on January 15, 2009, at Rochelle, but at a weather station not subjected to the same quality control as official records.

Illinois averages approximately 51 days of thunderstorm activity a year, which ranks somewhat above average in the number of thunderstorm days for the United States. Illinois is vulnerable to tornadoes, with an average of 35 occurring annually, which puts much of the state at around five tornadoes per annually. While tornadoes are no more powerful in Illinois than other states, some of Tornado Alley's deadliest tornadoes on record have occurred in the state. The Tri-State Tornado of 1925 killed 695 people in three states; 613 of the victims died in Illinois.

The United States Census Bureau estimates that the population of Illinois was 12,671,821 in 2019, moving from the fifth-largest state to the sixth-largest state (losing out to Pennsylvania). Illinois's population declined by 69,259 people from July 2018 to July 2019, making it the worst decline of any state in the U.S. in raw terms. This includes a natural increase since the last census of 462,146 people (i.e., 1,438,187 births minus 976,041 deaths) and an decrease due to net migration of 622,928 people. Immigration resulted in a net increase of 242,945 people, and migration from within the U.S. resulted in a net decrease of 865,873 people.

Illinois is the most populous state in the Midwest region. Chicago, the third-most populous city in the United States, is the center of the Chicago metropolitan area or Chicagoland, as this area is nicknamed. Although Chicagoland comprises only 9% of the land area of the state, it contains 65% of the state's residents.

According to the 2010 Census, the racial composition of the state was:

In the same year 15.8% of the total population was of Hispanic or Latino origin (they may be of any race).

According to 2018 U.S. Census Bureau estimates, Illinois's population was 71.7% White (60.9% Non-Hispanic White), 5.6% Asian, 5.6% Some Other Race, 14.1% Black or African American, 0.3% Native Americans and Alaskan Native, 0.1% Pacific Islander and 2.7% from two or more races. The White population continues to remain the largest racial category in Illinois as Hispanics primarily identify as White (62.2%) with others identifying as Some Other Race (31.2%), Multiracial (3.9%), Black (1.5%), American Indian and Alaskan Native (0.8%), Asian (0.3%), and Hawaiian and Pacific Islander (0.1%). By ethnicity, 17.3% of the total population is Hispanic-Latino (of any race) and 82.7% is Non-Hispanic (of any race). If treated as a separate category, Hispanics are the largest minority group in Illinois.

The state's most populous ethnic group, non-Hispanic white, has declined from 83.5% in 1970 to 60.90% in 2018. , 49.4% of Illinois's population younger than age1 were minorities (Note: Children born to white Hispanics or to a sole full or partial minority parent are counted as minorities).
At the 2007 estimates from the U.S. Census Bureau, there were 1,768,518 foreign-born inhabitants of the state or 13.8% of the population, with 48.4% from Latin America, 24.6% from Asia, 22.8% from Europe, 2.9% from Africa, 1.2% from Canada, and 0.2% from Oceania. Of the foreign-born population, 43.7% were naturalized U.S. citizens, and 56.3% were not U.S. citizens. In 2007, 6.9% of Illinois's population was reported as being under age 5, 24.9% under age 18 and 12.1% were age 65 and over. Females made up approximately 50.7% of the population.

According to the 2007 estimates, 21.1% of the population had German ancestry, 13.3% had Irish ancestry, 8% had British ancestry, 7.9% had Polish ancestry, 6.4% had Italian ancestry, 4.6% listed themselves as American, 2.4% had Swedish ancestry, 2.2% had French ancestry, other than Basque, 1.6% had Dutch ancestry, and 1.4% had Norwegian ancestry. Illinois also has large numbers of African Americans and Latinos (mostly Mexicans and Puerto Ricans).

Chicago, along the shores of Lake Michigan, is the nation's third largest city. In 2000, 23.3% of Illinois's population lived in the city of Chicago, 43.3% in Cook County, and 65.6% in the counties of the Chicago metropolitan area: Will, DuPage, Kane, Lake, and McHenry counties, as well as Cook County. The remaining population lives in the smaller cities and rural areas that dot the state's plains. As of 2000, the state's center of population was at , located in Grundy County, northeast of the village of Mazon.

"Note: Births do not add up, because Hispanics are counted both by their ethnicity and by their race, giving a higher overall number."


Chicago is the largest city in the state and the third-most populous city in the United States, with its 2010 population of 2,695,598. The U.S. Census Bureau currently lists seven other cities with populations of over 100,000 within Illinois. Based upon the Census Bureau's official 2010 population: Aurora, a Chicago satellite town that eclipsed Rockford for the title of second-most populous city in Illinois; its 2010 population was 197,899. Rockford, at 152,871, is the third-largest city in the state, and is the largest city in the state not located within the Chicago suburbs. Joliet, located in metropolitan Chicago, is the fourth-largest city in the state, with a population of 147,433. Naperville, a suburb of Chicago, is fifth with 141,853. Naperville and Aurora share a boundary along Illinois Route 59. Springfield, the state's capital, comes in as sixth-most populous with 117,352 residents. Peoria, which decades ago was the second-most populous city in the state, is seventh with 115,007. The eighth-largest and final city in the 100,000 club is Elgin, a northwest suburb of Chicago, with a 2010 population of 108,188.

The most populated city in the state south of Springfield is Belleville, with 44,478 people at the 2010 census. It is located in the Illinois portion of Greater St. Louis (often called the Metro-East area), which has a rapidly growing population of over 700,000.

Other major urban areas include the Champaign-Urbana Metropolitan Area, which has a combined population of almost 230,000 people, the Illinois portion of the Quad Cities area with about 215,000 people, and the Bloomington-Normal area with a combined population of over 165,000.

The official language of Illinois is English, although between 1923 and 1969, state law gave official status to "the American language". Nearly 80% of people in Illinois speak English natively, and most of the rest speak it fluently as a second language. A number of dialects of American English are spoken, ranging from Inland Northern American English and African-American English around Chicago, to Midland American English in Central Illinois, to Southern American English in the far south.

Over 20% of Illinoians speak a language other than English at home, of which Spanish is by far the most widespread, at more than 12% of the total population. A sizeable number of Polish speakers is present in the Chicago Metropolitan Area. Illinois Country French has mostly gone extinct in Illinois, although it is still celebrated in the French Colonial Historic District.

Roman Catholics constitute the single largest religious denomination in Illinois; they are heavily concentrated in and around Chicago, and account for nearly 30% of the state's population. However, taken together "as a group", the various Protestant denominations comprise a greater percentage of the state's population than do Catholics. In 2010 Catholics in Illinois numbered 3,648,907. The largest Protestant denominations were the United Methodist Church with 314,461, and the Southern Baptist Convention, with 283,519 members. Illinois has one of the largest concentrations of Missouri Synod Lutherans in the United States.

Illinois played an important role in the early Latter Day Saint movement, with Nauvoo, Illinois, becoming a gathering place for Mormons in the early 1840s. Nauvoo was the location of the succession crisis, which led to the separation of the Mormon movement into several Latter Day Saint sects. The Church of Jesus Christ of Latter-day Saints, the largest of the sects to emerge from the Mormon schism, has more than 55,000 adherents in Illinois today.

A significant number of adherents of other Abrahamic faiths can be found in Illinois. Largely concentrated in the Chicago metropolitan area, followers of the Muslim, Bahá'í, and Jewish religions all call the state home. Muslims constituted the largest non-Christian group, with 359,264 adherents. Illinois has the largest concentration of Muslims by state in the country, with 2,800 Muslims per 100,000 citizens. The largest and oldest surviving Bahá'í House of Worship in the world is located in Wilmette, Illinois, The Chicago area has a very large Jewish community, particularly in the suburbs of Skokie, Buffalo Grove, Highland Park, and surrounding suburbs. Former Chicago Mayor Rahm Emanuel is the Windy City's first Jewish mayor.

Chicago is also home to a very large population of Hindus, Sikhs, Jains, and Buddhists. The Bahá'í House of Worship in Wilmette is the center of that religion's worship in North America.

The dollar gross state product for Illinois was estimated to be billion in 2019. The state's 2019 per capita gross state product was estimated to be around $72,000.

As of February 2019, the unemployment rate in Illinois reached 4.2%.

Illinois's minimum wage will rise to $15 per hour by 2025, making it one of the highest in the nation.

Illinois's major agricultural outputs are corn, soybeans, hogs, cattle, dairy products, and wheat. In most years, Illinois is either the first or second state for the highest production of soybeans, with a harvest of 427.7 million bushels (11.64 million metric tons) in 2008, after Iowa's production of 444.82 million bushels (12.11 million metric tons). Illinois ranks second in U.S. corn production with more than 1.5 billion bushels produced annually. With a production capacity of 1.5 billion gallons per year, Illinois is a top producer of ethanol, ranking third in the United States in 2011. Illinois is a leader in food manufacturing and meat processing. Although Chicago may no longer be "Hog Butcher for the World", the Chicago area remains a global center for food manufacture and meat processing, with many plants, processing houses, and distribution facilities concentrated in the area of the former Union Stock Yards. Illinois also produces wine, and the state is home to two American viticultural areas. In the area of The Meeting of the Great Rivers Scenic Byway, peaches and apples are grown. The German immigrants from agricultural backgrounds who settled in Illinois in the mid- to late 19th century are in part responsible for the profusion of fruit orchards in that area of Illinois. Illinois's universities are actively researching alternative agricultural products as alternative crops.

Illinois is one of the nation's manufacturing leaders, boasting annual value added productivity by manufacturing of over $107 billion in 2006. , Illinois is ranked as the 4th-most productive manufacturing state in the country, behind California, Texas, and Ohio. About three-quarters of the state's manufacturers are located in the Northeastern Opportunity Return Region, with 38 percent of Illinois's approximately 18,900 manufacturing plants located in Cook County. As of 2006, the leading manufacturing industries in Illinois, based upon value-added, were chemical manufacturing ($18.3 billion), machinery manufacturing ($13.4 billion), food manufacturing ($12.9 billion), fabricated metal products ($11.5 billion), transportation equipment ($7.4 billion), plastics and rubber products ($7.0 billion), and computer and electronic products ($6.1 billion).

By the early 2000s, Illinois's economy had moved toward a dependence on high-value-added services, such as financial trading, higher education, law, logistics, and medicine. In some cases, these services clustered around institutions that hearkened back to Illinois's earlier economies. For example, the Chicago Mercantile Exchange, a trading exchange for global derivatives, had begun its life as an agricultural futures market. Other important non-manufacturing industries include publishing, tourism, and energy production and distribution.

Venture capitalists funded a total of approximately $62 billion in the U.S. economy in 2016. Of this amount, Illinois-based companies received approximately $1.1 billion. Similarly, in FY 2016, the federal government spent $461 billion on contracts in the U.S. Of this amount, Illinois-based companies received approximately $8.7 billion.

Illinois is a net importer of fuels for energy, despite large coal resources and some minor oil production. Illinois exports electricity, ranking fifth among states in electricity production and seventh in electricity consumption.

The coal industry of Illinois has its origins in the middle 19th century, when entrepreneurs such as Jacob Loose discovered coal in locations such as Sangamon County. Jacob Bunn contributed to the development of the Illinois coal industry, and was a founder and owner of the Western Coal & Mining Company of Illinois. About 68% of Illinois has coal-bearing strata of the Pennsylvanian geologic period. According to the Illinois State Geological Survey, 211 billion tons of bituminous coal are estimated to lie under the surface, having a total heating value greater than the estimated oil deposits in the Arabian Peninsula. However, this coal has a high sulfur content, which causes acid rain, unless special equipment is used to reduce sulfur dioxide emissions. Many Illinois power plants are not equipped to burn high-sulfur coal. In 1999, Illinois produced 40.4 million tons of coal, but only 17 million tons (42%) of Illinois coal was consumed in Illinois. Most of the coal produced in Illinois is exported to other states and countries. In 2008, Illinois exported three million tons of coal, and was projected to export 9 million tons in 2011, as demand for energy grows in places such as China, India, and elsewhere in Asia and Europe. , Illinois was ranked third in recoverable coal reserves at producing mines in the nation. Most of the coal produced in Illinois is exported to other states, while much of the coal burned for power in Illinois (21 million tons in 1998) is mined in the Powder River Basin of Wyoming.

Mattoon was recently chosen as the site for the Department of Energy's FutureGen project, a 275-megawatt experimental zero emission coal-burning power plant that the DOE just gave a second round of funding. In 2010, after a number of setbacks, the city of Mattoon backed out of the project.

Illinois is a leading refiner of petroleum in the American Midwest, with a combined crude oil distillation capacity of nearly . However, Illinois has very limited crude oil proved reserves that account for less than 1% of the U.S. total reserves. Residential heating is 81% natural gas compared to less than 1% heating oil. Illinois is ranked 14th in oil production among states, with a daily output of approximately in 2005.

Nuclear power arguably began in Illinois with the Chicago Pile-1, the world's first artificial self-sustaining nuclear chain reaction in the world's first nuclear reactor, built on the University of Chicago campus. There are six operating nuclear power plants in Illinois: Braidwood, Byron, Clinton, Dresden, LaSalle, and Quad Cities. With the exception of the single-unit Clinton plant, each of these facilities has two reactors. Three reactors have been permanently shut down and are in various stages of decommissioning: Dresden-1 and Zion-1 and 2. Illinois ranked first in the nation in 2010 in both nuclear capacity and nuclear generation. Generation from its nuclear power plants accounted for 12 percent of the nation's total. In 2007, 48% of Illinois's electricity was generated using nuclear power. The Morris Operation is the only de facto high-level radioactive waste storage site in the United States.

Illinois has seen growing interest in the use of wind power for electrical generation. Most of Illinois was rated in 2009 as "marginal or fair" for wind energy production by the U.S. Department of Energy, with some western sections rated "good" and parts of the south rated "poor". These ratings are for wind turbines with hub heights; newer wind turbines are taller, enabling them to reach stronger winds farther from the ground. As a result, more areas of Illinois have become prospective wind farm sites. As of September 2009, Illinois had 1116.06 MW of installed wind power nameplate capacity with another 741.9 MW under construction. Illinois ranked ninth among U.S. states in installed wind power capacity, and sixteenth by potential capacity. Large wind farms in Illinois include Twin Groves, Rail Splitter, EcoGrove, and Mendota Hills.

As of 2007, wind energy represented only 1.7% of Illinois's energy production, and it was estimated that wind power could provide 5–10% of the state's energy needs. Also, the Illinois General Assembly mandated in 2007 that by 2025, 25% of all electricity generated in Illinois is to come from renewable resources.

Illinois is ranked second in corn production among U.S. states, and Illinois corn is used to produce 40% of the ethanol consumed in the United States. The Archer Daniels Midland corporation in Decatur, Illinois, is the world's leading producer of ethanol from corn.

The National Corn-to-Ethanol Research Center (NCERC), the world's only facility dedicated to researching the ways and means of converting corn (maize) to ethanol is located on the campus of Southern Illinois University Edwardsville.

University of Illinois at Urbana–Champaign is one of the partners in the Energy Biosciences Institute (EBI), a $500 million biofuels research project funded by petroleum giant BP.

Tax is collected by the Illinois Department of Revenue. State income tax is calculated by multiplying net income by a flat rate. In 1990, that rate was set at 3%, but in 2010, the General Assembly voted for a temporary increase in the rate to 5%; the new rate went into effect on January 1, 2011; the personal income rate partially sunset on January 1, 2015, to 3.75%, while the corporate income tax fell to 5.25%. Illinois failed to pass a budget from 2015 to 2017, after the 736-day budget impasse, a budget was passed in Illinois after lawmakers overturned Governor Bruce Rauner's veto; this budget raised the personal income rate to 4.95% and the corporate rate to 7%. There are two rates for state sales tax: 6.25% for general merchandise and 1% for qualifying food, drugs, and medical appliances. The property tax is a major source of tax revenue for local government taxing districts. The property tax is a local—not state—tax, imposed by local government taxing districts, which include counties, townships, municipalities, school districts, and special taxation districts. The property tax in Illinois is imposed only on real property.

On May 1, 2019, the Illinois Senate voted to a approve a constitutional amendment to change from a flat tax rate to a graduated rate, in a 73–44 vote. The governor, J.B. Pritzker, approved the bill on May 27, 2019. It was scheduled for a 2020 general election ballot vote and requires 60 percent voter approval. It needed 71 votes to pass, with taxpayers making over $250,000 to be impacted. It also includes $100 million for property tax relief.

As of 2017 Chicago had the highest state and local sales tax rate for a U.S. city with a populations above 200,000, at 10.250%. The state of Illinois has the second highest rate of real estate tax: 2.31%, which is second only to New Jersey at 2.44%.

Toll roads are a "de facto" user tax on the citizens and visitors to the state of Illinois. Illinois ranks seventh out of the 11 states with the most miles of toll roads, at 282.1 miles. Chicago ranks fourth in most expensive toll roads in America by the mile, with the Chicago Skyway charging 51.2 cents per mile. Illinois also has the 11th highest gasoline tax by state, at 37.5 cents per gallon.

Illinois has numerous museums; the greatest concentration of these are in Chicago. Several museums in Chicago are ranked as some of the best in the world. These include the John G. Shedd Aquarium, the Field Museum of Natural History, the Art Institute of Chicago, the Adler Planetarium, and the Museum of Science and Industry.

The modern Abraham Lincoln Presidential Library and Museum in Springfield is the largest and most attended presidential library in the country. The Illinois State Museum boasts a collection of 13.5 million objects that tell the story of Illinois life, land, people, and art. The ISM is among only 5% of the nation's museums that are accredited by the American Alliance of Museums. Other historical museums in the state include the Polish Museum of America in Chicago; Magnolia Manor in Cairo; Easley Pioneer Museum in Ipava; the Elihu Benjamin Washburne; Ulysses S. Grant Homes, both in Galena; and the Chanute Air Museum, located on the former Chanute Air Force Base in Rantoul.

The Chicago metropolitan area also hosts two zoos: The very large Brookfield Zoo, located about ten miles west of the city center in suburban Brookfield, contains more than 2,300 animals and covers . The Lincoln Park Zoo is located in huge Lincoln Park on Chicago's North Side, approximately north of the Loop. The zoo covers over within the park.

Illinois is a leader in music education, having hosted the Midwest Clinic International Band and Orchestra Conference since 1946, as well being home to the Illinois Music Educators Association (IMEA), one of the largest professional music educator's organizations in the country. Each summer since 2004, Southern Illinois University Carbondale has played host to the Southern Illinois Music Festival, which presents dozens of performances throughout the region. Past featured artists include the Eroica Trio and violinist David Kim.

Chicago, in the northeast corner of the state, is a major center for music in the midwestern United States where distinctive forms of blues (greatly responsible for the future creation of rock and roll), and house music, a genre of electronic dance music, were developed.

The Great Migration of poor black workers from the South into the industrial cities brought traditional jazz and blues music to the city, resulting in Chicago blues and "Chicago-style" Dixieland jazz. Notable blues artists included Muddy Waters, Junior Wells, Howlin' Wolf and both Sonny Boy Williamsons; jazz greats included Nat King Cole, Gene Ammons, Benny Goodman, and Bud Freeman. Chicago is also well known for its soul music.

In the early 1930s, Gospel music began to gain popularity in Chicago due to Thomas A. Dorsey's contributions at Pilgrim Baptist Church.

In the 1980s and 1990s, heavy rock, punk, and hip hop also became popular in Chicago. Orchestras in Chicago include the Chicago Symphony Orchestra, the Lyric Opera of Chicago, and the Chicago Sinfonietta.

John Hughes, who moved from Grosse Pointe to Northbrook, based many films of his in Chicago, and its suburbs. Ferris Bueller's Day Off, Home Alone, The Breakfast Club, and all his films take place in the fictional Shermer, Illinois (the original name of Northbrook was Shermerville, and Hughes's High School, Glenbrook North High School, is on Shermer Road). Most locations in his films include Glenbrook North, the former Maine North High School, the Ben Rose House in Highland Park, and the famous Home Alone house in Winnetka, Illinois.

As one of the United States' major metropolises, all major sports leagues have teams headquartered in Chicago.


Many minor league teams also call Illinois their home. They include:

The state features 13 athletic programs that compete in NCAA Division I, the highest level of U.S. college sports.

The two most prominent are the Illinois Fighting Illini and Northwestern Wildcats, both members of the Big Ten Conference and the only ones competing in one of the so-called "Power Five conferences". The Fighting Illini football team has won five national championships and three Rose Bowl Games, whereas the men's basketball team has won 17 conference seasons and played five Final Fours. Meanwhile, the Wildcats have won eight football conference championships and one Rose Bowl Game.

The Northern Illinois Huskies from DeKalb, Illinois compete in the Mid-American Conference winning four conference championships and earning a bid in the Orange Bowl along with producing Heisman candidate Jordan Lynch at quarterback. The Huskies are the state's only other team competing in the Football Bowl Subdivision, the top level of NCAA football.

Four schools have football programs that compete in the second level of Division I football, the Football Championship Subdivision. The Illinois State Redbirds (Normal, adjacent to Bloomington) and Southern Illinois Salukis (the latter representing Southern Illinois University's main campus in Carbondale) are members of the Missouri Valley Conference (MVC) for non-football sports and the Missouri Valley Football Conference (MVFC). The Western Illinois Leathernecks (Macomb) are full members of the Summit League, which does not sponsor football, and also compete in the MVFC. The Eastern Illinois Panthers (Charleston) are members of the Ohio Valley Conference (OVC).

The city of Chicago is home to four Division I programs that do not sponsor football. The DePaul Blue Demons, with main campuses in Lincoln Park and the Loop, are members of the Big East Conference. The Loyola Ramblers, with their main campus straddling the Edgewater and Rogers Park community areas on the city's far north side, compete in the MVC. The UIC Flames, from the Near West Side next to the Loop, are in the Horizon League. The Chicago State Cougars, from the city's south side, compete in the Western Athletic Conference.

Finally, two non-football Division I programs are located downstate. The Bradley Braves (Peoria) are MVC members, and the SIU Edwardsville Cougars (in the Metro East region across the Mississippi River from St. Louis) compete in the OVC.

The city was formerly home to several other teams that either failed to survive or belonged to leagues that folded.

The NFL's Arizona Cardinals, who currently play in the Phoenix suburb of Glendale, Arizona, played in Chicago as the Chicago Cardinals, until moving to St. Louis, Missouri after the 1959 season. An NBA expansion team known as the Chicago Packers in 1961–1962, and as the Chicago Zephyrs the following year, moved to Baltimore after the 1962–1963 season. The franchise is now known as the Washington Wizards.

The Peoria Chiefs and Kane County Cougars are minor league baseball teams affiliated with MLB. The Schaumburg Boomers and Lake County Fielders are members of the North American League, and the Southern Illinois Miners, Gateway Grizzlies, Joliet Slammers, Windy City ThunderBolts, and Normal CornBelters belong to the Frontier League.

In addition to the Chicago Wolves, the AHL also has the Rockford IceHogs serving as the AHL affiliate of the Chicago Blackhawks. The second incarnation of the Peoria Rivermen plays in the SPHL.

Motor racing oval tracks at the Chicagoland Speedway in Joliet, the Chicago Motor Speedway in Cicero and the Gateway International Raceway in Madison, near St. Louis, have hosted NASCAR, CART, and IRL races, whereas the Sports Car Club of America, among other national and regional road racing clubs, have visited the Autobahn Country Club in Joliet, the Blackhawk Farms Raceway in South Beloit and the former Meadowdale International Raceway in Carpentersville. Illinois also has several short tracks and dragstrips. The dragstrip at Gateway International Raceway and the Route 66 Raceway, which sits on the same property as the Chicagoland Speedway, both host NHRA drag races.

Illinois features several golf courses, such as Olympia Fields, Medinah, Midlothian, Cog Hill, and Conway Farms, which have often hosted the BMW Championship, Western Open, and Women's Western Open.

Also, the state has hosted 13 editions of the U.S. Open (latest at Olympia Fields in 2003), six editions of the PGA Championship (latest at Medinah in 2006), three editions of the U.S. Women's Open (latest at The Merit Club), the 2009 Solheim Cup (at Rich Harvest Farms), and the 2012 Ryder Cup (at Medinah).

The John Deere Classic is a regular PGA Tour event played in the Quad Cities since 1971, whereas the Encompass Championship is a Champions Tour event since 2013. Previously, the LPGA State Farm Classic was an LPGA Tour event from 1976 to 2011.

The Illinois state parks system began in 1908 with what is now Fort Massac State Park, becoming the first park in a system encompassing more than 60 parks and about the same number of recreational and wildlife areas.

Areas under the protection of the National Park Service include: the Illinois and Michigan Canal National Heritage Corridor near Lockport, the Lewis and Clark National Historic Trail, the Lincoln Home National Historic Site in Springfield, the Mormon Pioneer National Historic Trail, the Trail of Tears National Historic Trail, the American Discovery Trail, and the Pullman National Monument. The federal government also manages the Shawnee National Forest and the Midewin National Tallgrass Prairie.

The government of Illinois, under the Constitution of Illinois, has three branches of government: executive, legislative and judicial. The executive branch is split into several statewide elected offices, with the governor as chief executive. Legislative functions are granted to the Illinois General Assembly. The judiciary is composed of the Supreme Court and lower courts.

The Illinois General Assembly is the state legislature, composed of the 118-member Illinois House of Representatives and the 59-member Illinois Senate. The members of the General Assembly are elected at the beginning of each even-numbered year. The "Illinois Compiled Statutes" (ILCS) are the codified statutes of a general and permanent nature.

The executive branch is composed of six elected officers and their offices as well as numerous other departments. The six elected officers are: Governor, Lieutenant Governor, Attorney General, Secretary of State, Comptroller, and Treasurer. The government of Illinois has numerous departments, agencies, boards and commissions, but the so-called code departments provide most of the state's services.

The Judiciary of Illinois is the unified court system of Illinois. It consists of the Supreme Court, Appellate Court, and Circuit Courts. The Supreme Court oversees the administration of the court system.

The administrative divisions of Illinois are counties, townships, precincts, cities, towns, villages, and special-purpose districts. The basic subdivision of Illinois are the 102 counties. Eighty-five of the 102 counties are in turn divided into townships and precincts. Municipal governments are the cities, villages, and incorporated towns. Some localities possess "home rule", which allows them to govern themselves to a certain extent.

Illinois is a Democratic stronghold. Historically, Illinois was a political swing state, with near-parity existing between the Republican and the Democratic parties. However, in recent elections, the Democratic Party has gained ground, and Illinois has come to be seen as a solid "blue" state in presidential campaigns. Votes from Chicago and most of Cook County have long been strongly Democratic. However, the "collar counties" (the suburbs surrounding Chicago's Cook County, Illinois), can be seen as moderate voting districts. College towns like Carbondale, Champaign, and Normal also lean Democratic.

Republicans continue to prevail in the rural areas of northern and central Illinois, as well as southern Illinois outside of East St. Louis. From 1920 until 1972, Illinois was carried by the victor of each of these 14 presidential elections. In fact, the state was long seen as a national bellwether, supporting the winner in every election in the 20th century, except for 1916 and 1976. By contrast, Illinois has trended more toward the Democratic party, and has voted for their presidential candidates in the last six elections; in 2000, George W. Bush became the first Republican to win the presidency without carrying either Illinois or Vermont. Local politician and Chicago resident Barack Obama easily won the state's 21 electoral votes in 2008, with 61.9% of the vote. In 2010, incumbent governor Pat Quinn was re-elected with 47% of the vote, while Republican Mark Kirk was elected to the Senate with 48% of the vote. In 2012, President Obama easily carried Illinois again, with 58% to Republican candidate Mitt Romney's 41%. In 2014, Republican Bruce Rauner defeated Governor Quinn 50% to 46% to become Illinois's first Republican governor in 12 years after being sworn in on January 12, 2015, while Democratic senator Dick Durbin was re-elected with 53% of the vote. In 2016, Hillary Clinton carried Illinois with 55% of the vote, and Tammy Duckworth defeated incumbent Mark Kirk 54% to 40%. George W. Bush and Donald Trump are the only Republican presidential candidates to win without carrying either Illinois or Vermont. In 2018, Democrat JB Pritzker defeated the incumbent Bruce Rauner for the governorship with 54% of the vote.

Politics in the state have been infamous for highly visible corruption cases, as well as for crusading reformers, such as governors Adlai Stevenson and James R. Thompson. In 2006, former governor George Ryan was convicted of racketeering and bribery, leading to a six-and-a-half-year prison sentence. In 2008, then-Governor Rod Blagojevich was served with a criminal complaint on corruption charges, stemming from allegations that he conspired to sell the vacated Senate seat left by President Barack Obama to the highest bidder. Subsequently, on December 7, 2011, Rod Blagojevich was sentenced to 14 years in prison for those charges, as well as perjury while testifying during the case, totaling 18 convictions. Blagojevich was impeached and convicted by the legislature, resulting in his removal from office. In the late 20th century, Congressman Dan Rostenkowski was imprisoned for mail fraud; former governor and federal judge Otto Kerner, Jr. was imprisoned for bribery; Secretary of State Paul Powell was investigated and found to have gained great wealth through bribes, and State Auditor of Public Accounts (Comptroller) Orville Hodge was imprisoned for embezzlement. In 1912, William Lorimer, the GOP boss of Chicago, was expelled from the U.S. Senate for bribery and in 1921, Governor Len Small was found to have defrauded the state of a million dollars.

Illinois has shown a strong presence in presidential elections. Three presidents have claimed Illinois as their political base when running for president: Abraham Lincoln, Ulysses S. Grant, and most recently Barack Obama. Lincoln was born in Kentucky, but he moved to Illinois at age 21. He served in the General Assembly and represented the 7th congressional district in the U.S. House of Representatives before his election to the presidency in 1860. Ulysses S. Grant was born in Ohio and had a military career that precluded settling down, but on the eve of the Civil War and approaching middle age, he moved to Illinois and thus utilized the state as his home and political base when running for president. Barack Obama was born in Hawaii and made Illinois his home after graduating from law school, and later represented Illinois in the U.S. Senate. He then became president in 2008, running as a candidate from his Illinois base.

Ronald Reagan was born in Illinois, in the city of Tampico, raised in Dixon, Illinois, and educated at Eureka College, outside Peoria. Reagan later moved to California during his young adulthood. He then became an actor, and later became California's Governor before being elected president.

Hillary Clinton was born and raised in the suburbs of Chicago and became the first woman to represent a major political party in the general election of the U.S. presidency. Clinton ran from a platform based in New York State.

Nine African-Americans have served as members of the United States Senate. Three of them have represented Illinois, the most of any single state: Carol Moseley-Braun, Barack Obama, and Roland Burris, who was appointed to replace Obama after his election to the presidency. Moseley-Braun was the first African-American woman to become a U.S. Senator.

Three families from Illinois have played particularly prominent roles in the Democratic Party, gaining both statewide and national fame.

The Stevenson family, initially rooted in central Illinois and later based in the Chicago metropolitan area, has provided four generations of Illinois officeholders.

The Daley family's powerbase was in Chicago.

The Pritzker family is based in Chicago and have played important roles both in the private and public sectors.

The Illinois State Board of Education (ISBE) is autonomous of the governor and the state legislature, and administers public education in the state. Local municipalities and their respective school districts operate individual public schools, but the ISBE audits performance of public schools with the Illinois School Report Card. The ISBE also makes recommendations to state leaders concerning education spending and policies.

Education is compulsory from ages 7 to 17 in Illinois. Schools are commonly, but not exclusively, divided into three tiers of primary and secondary education: elementary school, middle school or junior high school, and high school. District territories are often complex in structure. Many areas in the state are actually located in "two" school districts—one for high school, the other for elementary and middle schools. And such districts do not necessarily share boundaries. A given high school may have several elementary districts that feed into it, yet some of those feeder districts may themselves feed into multiple high school districts.

Using the criterion established by the Carnegie Foundation for the Advancement of Teaching, there are eleven "National Universities" in the state. , six of these rank in the "first tier" (that is, the top quartile) among the top 500 National Universities in the United States, as determined by the "U.S. News & World Report" rankings: the University of Chicago (3), Northwestern University (10), the University of Illinois at Urbana–Champaign (41), Loyola University Chicago (89), the Illinois Institute of Technology (108), DePaul University (123), University of Illinois at Chicago (129), Illinois State University (149), Southern Illinois University Carbondale (153), and Northern Illinois University (194).

The University of Chicago is continuously ranked as one of the world's top ten universities on various independent university rankings, and its Booth School of Business, along with Northwestern's Kellogg School of Management consistently rank within the top five graduate business schools in the country and top ten globally. The University of Illinois at Urbana–Champaign is often ranked among the best engineering schools in the world and in United States.

Illinois also has more than twenty additional accredited four-year universities, both public and private, and dozens of small liberal arts colleges across the state. Additionally, Illinois supports 49 public community colleges in the Illinois Community College System.

Because of its central location and its proximity to the Rust Belt and Grain Belt, Illinois is a national crossroads for air, auto, rail, and truck traffic.

From 1962 until 1998, Chicago's O'Hare International Airport (ORD) was the busiest airport in the world, measured both in terms of total flights and passengers. While it was surpassed by Atlanta's Hartsfield in 1998 (as Chicago splits its air traffic between O'Hare and Midway airports, while Atlanta uses only one airport), with 59.3 million domestic passengers annually, along with 11.4 million international passengers in 2008, O'Hare consistently remains one of the two or three busiest airports globally, and in some years still ranks number one in total flights. It is a major hub for both United Airlines and American Airlines, and a major airport expansion project is currently underway. Midway Airport (MDW), which had been the busiest airport in the world at one point until it was supplanted by O'Hare as the busiest airport in 1962, is now the secondary airport in the Chicago metropolitan area and still ranks as one of the nation's busiest airports. Midway is a major hub for Southwest Airlines and services many other carriers as well. Midway served 17.3 million domestic and international passengers in 2008.

Illinois has an extensive passenger and freight rail transportation network. Chicago is a national Amtrak hub and in-state passengers are served by Amtrak's Illinois Service, featuring the Chicago to Carbondale "Illini" and "Saluki", the Chicago to Quincy "Carl Sandburg" and "Illinois Zephyr", and the Chicago to St. Louis "Lincoln Service". Currently there is trackwork on the Chicago–St. Louis line to bring the maximum speed up to , which would reduce the trip time by an hour and a half. Nearly every North American railway meets at Chicago, making it the largest and most active rail hub in the country. Extensive commuter rail is provided in the city proper and some immediate suburbs by the Chicago Transit Authority's 'L' system. One of the largest suburban commuter rail system in the United States, operated by Metra, uses existing rail lines to provide direct commuter rail access for hundreds of suburbs to the city and beyond.

In addition to the state's rail lines, the Mississippi River and Illinois River provide major transportation routes for the state's agricultural interests. Lake Michigan gives Illinois access to the Atlantic Ocean by way of the Saint Lawrence Seaway.

The Interstate Highways in Illinois are all segments of the Interstate Highway System that are owned and maintained by the state.

Illinois has the distinction of having the most primary (two-digit) interstates pass through it among all the 50 states with 13. Illinois also ranks third among the fifty states with the most interstate mileage, coming in after California and Texas, which are much bigger states in area.

Major U.S. Interstate highways crossing the state include: Interstate 24 (I-24), I-39, I-41, I-55, I-57, I-64, I-70, I-72, I-74, I-80, I-88, I-90, and I-94.

The Illinois Department of Transportation (IDOT) is responsible for maintaining the U.S Highways in Illinois. The system in Illinois consists of 21 primary highways.

Among the U.S. highways that pass through the state, the primary ones are: US 6, US 12, US 14, US 20, US 24, US 30, US 34, US 36, US 40, US 41, US 45, US 50, US 51, US 52, US 54, US 60, US 62, and US 67.




</doc>
<doc id="14851" url="https://en.wikipedia.org/wiki?curid=14851" title="Ian Murdock">
Ian Murdock

Ian Ashley Murdock (28April 1973 28December 2015) was an American software engineer, known for being the founder of the Debian project and Progeny Linux Systems, a commercial Linux company.

Although Murdock's parents were both from Southern Indiana, he was born in Konstanz, West Germany, on 28 April 1973, where his father was pursuing postdoctoral
research. The family returned to the United States in 1975, and Murdock grew up in Lafayette, Indiana, beginning in 1977 when his father became a professor of entomology at Purdue University. Murdock graduated from Harrison High School in 1991, and then earned his bachelor's degree in computer science from Purdue in 1996.

While a college student, Murdock founded the Debian project in August 1993, and wrote the Debian Manifesto in January 1994. Murdock conceived Debian as a Linux distribution that embraced open design, contributions, and support from the free software community. He named Debian after his then-girlfriend (later wife) Debra Lynn, and himself (Deb and Ian). They later married, had three children, and divorced in January 2008.

In January 2006, Murdock was appointed Chief Technology Officer of the Free Standards Group and elected chair of the Linux Standard Base workgroup. He continued as CTO of the Linux Foundation when the group was formed from the merger of the Free Standards Group and Open Source Development Labs.

Murdock left the Linux Foundation to join Sun Microsystems in March 2007 to lead Project Indiana, which he described as "taking the lesson that Linux has brought to the operating system and providing that for Solaris", making a full OpenSolaris distribution with GNOME and userland tools from GNU plus a network-based package management system. From March 2007 to February 2010, he was Vice President of Emerging Platforms at Sun, until the company merged with Oracle and he resigned his position with the company.

From 2011 until 2015 Murdock was Vice President of Platform and Developer Community at Salesforce Marketing Cloud, based in Indianapolis.

From November 2015 until his death Murdock was working for Docker, Inc.

Murdock died on 28 December 2015 in San Francisco. Though initially no cause of death was released, in July 2016 it was announced his death had been ruled a suicide. The police confirmed that the cause of death was due to asphyxiation caused by hanging himself with a vacuum cleaner electrical cord.

The last tweets from Murdock's Twitter account first announced that he would commit suicide, then said he would not. He reported having been accused of assault on a police officer after having been himself assaulted by the police, then declared an intent to devote his life to opposing police abuse. His Twitter account was taken down shortly afterwards.

The San Francisco police confirmed he was detained, saying he matched the description in a reported attempted break-in and that he appeared to be drunk. The police stated that he became violent and was ultimately taken to jail on suspicion of four misdemeanor counts. They added that he did not appear to be suicidal and was medically examined prior to release. Later, police returned on reports of a possible suicide. The city medical examiner's office confirmed Murdock was found dead.




</doc>
<doc id="14856" url="https://en.wikipedia.org/wiki?curid=14856" title="Inner product space">
Inner product space

In linear algebra, an inner product space is a vector space with an additional structure called an inner product. This additional structure associates each pair of vectors in the space with a scalar quantity known as the inner product of the vectors, often denoted using angle brackets (as in formula_1). Inner products allow the rigorous introduction of intuitive geometrical notions, such as the length of a vector or the angle between two vectors. They also provide the means of defining orthogonality between vectors (zero inner product). Inner product spaces generalize Euclidean spaces (in which the inner product is the dot product, also known as the scalar product) to vector spaces of any (possibly infinite) dimension, and are studied in functional analysis. The first usage of the concept of a vector space with an inner product is due to Giuseppe Peano, in 1898.

An inner product naturally induces an associated norm, (|"x"| and |"y"| are the norms of "x" and "y", in the picture) thus an inner product space is also a normed vector space. A complete space with an inner product is called a Hilbert space. An (incomplete) space with an inner product is called a pre-Hilbert space, since its completion with respect to the norm induced by the inner product is a Hilbert space. Inner product spaces over the field of complex numbers are sometimes referred to as unitary spaces.

In this article, the field of scalars denoted is either the field of real numbers or the field of complex numbers .

Formally, an inner product space is a vector space over the field together with an "inner product", i.e., with a map

that satisfies the following three properties for all vectors and all scalars :


If the positive-definite condition is replaced by merely requiring that formula_6 for all "x", then one obtains the definition of "positive semi-definite Hermitian form". A positive semi-definite Hermitian form formula_7 is an inner product if and only if for all "x", if formula_8 then "x = 0".

Positive-definiteness and linearity, respectively, ensure that:

Notice that conjugate symmetry implies that is real for all , since we have:

Conjugate symmetry and linearity in the first variable imply

that is, conjugate linearity in the second argument. So, an inner product is a sesquilinear form. Conjugate symmetry is also called Hermitian symmetry, and a conjugate-symmetric sesquilinear form is called a "Hermitian form". While the above axioms are more mathematically economical, a compact verbal definition of an inner product is a "positive-definite Hermitian form".

This important generalization of the familiar square expansion follows:

These properties, constituents of the above linearity in the first and second argument:

are otherwise known as "additivity".

In the case of , conjugate-symmetry reduces to symmetry, and sesquilinearity reduces to bilinearity. Hence an inner product on a real vector space is a "positive-definite symmetric bilinear form". That is,

and the binomial expansion becomes:

A common special case of the inner product, the scalar product or dot product, is written with a centered dot formula_16.

Some authors, especially in physics and matrix algebra, prefer to define the inner product and the sesquilinear form with linearity in the second argument rather than the first. Then the first argument becomes conjugate linear, rather than the second. In those disciplines, we would write the product as (the bra–ket notation of quantum mechanics), respectively (dot product as a case of the convention of forming the matrix product , as the dot products of rows of with columns of ). Here, the kets and columns are identified with the vectors of , and the bras and rows with the linear functionals (covectors) of the dual space , with conjugacy associated with duality. This reverse order is now occasionally followed in the more abstract literature, taking to be conjugate linear in rather than . A few instead find a middle ground by recognizing both and as distinct notations—differing only in which argument is conjugate linear.

There are various technical reasons why it is necessary to restrict the base field to and in the definition. Briefly, the base field has to contain an ordered subfield in order for non-negativity to make sense, and therefore has to have characteristic equal to 0 (since any ordered field has to have such characteristic). This immediately excludes finite fields. The basefield has to have additional structure, such as a distinguished automorphism. More generally, any quadratically closed subfield of or will suffice for this purpose (e.g., algebraic numbers, constructible numbers). However, in the cases where it is a proper subfield (i.e., neither nor ), even finite-dimensional inner product spaces will fail to be metrically complete. In contrast, all finite-dimensional inner product spaces over or , such as those used in quantum computation, are automatically metrically complete (and hence Hilbert spaces).

In some cases, one needs to consider non-negative "semi-definite" sesquilinear forms. This means that is only required to be non-negative. Treatment for these cases are illustrated below.

A simple example is the real numbers with the standard multiplication as the inner product

More generally, the real -space with the dot product is an inner product space, an example of a Euclidean vector space.

where is the transpose of .

The general form of an inner product on is known as the Hermitian form and is given by

where is any Hermitian positive-definite matrix and is the conjugate transpose of . For the real case, this corresponds to the dot product of the results of directionally-different scaling of the two vectors, with positive scale factors and orthogonal directions of scaling. It is a weighted-sum version of the dot product with positive weights—up to an orthogonal transformation.

The article on Hilbert spaces has several examples of inner product spaces, wherein the metric induced by the inner product yields a complete metric space. An example of an inner product space which induces an incomplete metric is the space of continuous complex valued functions "f" and "g" on the interval . The inner product is
This space is not complete; consider for example, for the interval the sequence of continuous "step" functions, , defined by:

This sequence is a Cauchy sequence for the norm induced by the preceding inner product, which does not converge to a "continuous" function.

For real random variables and , the expected value of their product

is an inner product. In this case, if and only if (i.e., almost surely). This definition of expectation as inner product can be extended to random vectors as well.

For real square matrices of the same size, with transpose as conjugation
is an inner product.

On an inner product space, or more generally a vector space with a nondegenerate form (hence an isomorphism ), vectors can be sent to covectors (in coordinates, via transpose), so that one can take the inner product and outer product of two vectors—not simply of a vector and a covector.

Inner product spaces are normed vector spaces for the norm defined by

As for every normed vector space, a inner product space is a metric space, for the distance defined by 

Directly from the axioms of the inner product, one can prove that the axioms of a norm are satisfied, as well as the following properties.

Let be a finite dimensional inner product space of dimension . Recall that every basis of consists of exactly linearly independent vectors. Using the Gram–Schmidt process we may start with an arbitrary basis and transform it into an orthonormal basis. That is, into a basis in which all the elements are orthogonal and have unit norm. In symbols, a basis is orthonormal if for every and for each .

This definition of orthonormal basis generalizes to the case of infinite-dimensional inner product spaces in the following way. Let be any inner product space. Then a collection

is a "basis" for if the subspace of generated by finite linear combinations of elements of is dense in (in the norm induced by the inner product). We say that is an "orthonormal basis" for if it is a basis and

if and for all .

Using an infinite-dimensional analog of the Gram-Schmidt process one may show:

Theorem. Any separable inner product space has an orthonormal basis.

Using the Hausdorff maximal principle and the fact that in a complete inner product space orthogonal projection onto linear subspaces is well-defined, one may also show that

Theorem. Any complete inner product space has an orthonormal basis.

The two previous theorems raise the question of whether all inner product spaces have an orthonormal basis. The answer, it turns out is negative. This is a non-trivial result, and is proved below. The following proof is taken from Halmos's "A Hilbert Space Problem Book" (see the references).

Parseval's identity leads immediately to the following theorem:

Theorem. Let be a separable inner product space and an orthonormal basis of . Then the map
is an isometric linear map with a dense image.

This theorem can be regarded as an abstract form of Fourier series, in which an arbitrary orthonormal basis plays the role of the sequence of trigonometric polynomials. Note that the underlying index set can be taken to be any countable set (and in fact any set whatsoever, provided is defined appropriately, as is explained in the article Hilbert space). In particular, we obtain the following result in the theory of Fourier series:

Theorem. Let be the inner product space . Then the sequence (indexed on set of all integers) of continuous functions

is an orthonormal basis of the space with the inner product. The mapping

is an isometric linear map with dense image.

Orthogonality of the sequence follows immediately from the fact that if , then

Normality of the sequence is by design, that is, the coefficients are so chosen so that the norm comes out to 1. Finally the fact that the sequence has a dense algebraic span, in the "inner product norm", follows from the fact that the sequence has a dense algebraic span, this time in the space of continuous periodic functions on with the uniform norm. This is the content of the Weierstrass theorem on the uniform density of trigonometric polynomials.

Several types of linear maps from an inner product space to an inner product space are of relevance:

From the point of view of inner product space theory, there is no need to distinguish between two spaces which are isometrically isomorphic. The spectral theorem provides a canonical form for symmetric, unitary and more generally normal operators on finite dimensional inner product spaces. A generalization of the spectral theorem holds for continuous normal operators in Hilbert spaces.

Any of the axioms of an inner product may be weakened, yielding generalized notions. The generalizations that are closest to inner products occur where bilinearity and conjugate symmetry are retained, but positive-definiteness is weakened.

If is a vector space and a semi-definite sesquilinear form, then the function:

makes sense and satisfies all the properties of norm except that does not imply (such a functional is then called a semi-norm). We can produce an inner product space by considering the quotient }. The sesquilinear form factors through .

This construction is used in numerous contexts. The Gelfand–Naimark–Segal construction is a particularly important example of the use of this technique. Another example is the representation of semi-definite kernels on arbitrary sets.

Alternatively, one may require that the pairing be a nondegenerate form, meaning that for all non-zero there exists some such that , though need not equal ; in other words, the induced map to the dual space is injective. This generalization is important in differential geometry: a manifold whose tangent spaces have an inner product is a Riemannian manifold, while if this is related to nondegenerate conjugate symmetric form the manifold is a pseudo-Riemannian manifold. By Sylvester's law of inertia, just as every inner product is similar to the dot product with positive weights on a set of vectors, every nondegenerate conjugate symmetric form is similar to the dot product with "nonzero" weights on a set of vectors, and the number of positive and negative weights are called respectively the positive index and negative index. Product of vectors in Minkowski space is an example of indefinite inner product, although, technically speaking, it is not an inner product according to the standard definition above. Minkowski space has four dimensions and indices 3 and 1 (assignment of "+" and "−" to them differs depending on conventions).

Purely algebraic statements (ones that do not use positivity) usually only rely on the nondegeneracy (the injective homomorphism ) and thus hold more generally.

The term "inner product" is opposed to outer product, which is a slightly more general opposite. Simply, in coordinates, the inner product is the product of a "covector" with an vector, yielding a 1 × 1 matrix (a scalar), while the outer product is the product of an vector with a covector, yielding an matrix. Note that the outer product is defined for different dimensions, while the inner product requires the same dimension. If the dimensions are the same, then the inner product is the "trace" of the outer product (trace only being properly defined for square matrices). In an informal summary: "inner is horizontal times vertical and shrinks down, outer is vertical times horizontal and expands out".

More abstractly, the outer product is the bilinear map sending a vector and a covector to a rank 1 linear transformation (simple tensor of type (1, 1)), while the inner product is the bilinear evaluation map given by evaluating a covector on a vector; the order of the domain vector spaces here reflects the covector/vector distinction.

The inner product and outer product should not be confused with the interior product and exterior product, which are instead operations on vector fields and differential forms, or more generally on the exterior algebra.

As a further complication, in geometric algebra the inner product and the "exterior" (Grassmann) product are combined in the geometric product (the Clifford product in a Clifford algebra) – the inner product sends two vectors (1-vectors) to a scalar (a 0-vector), while the exterior product sends two vectors to a bivector (2-vector) – and in this context the exterior product is usually called the "outer product" (alternatively, "wedge product"). The inner product is more correctly called a "scalar" product in this context, as the nondegenerate quadratic form in question need not be positive definite (need not be an inner product).




</doc>
<doc id="14858" url="https://en.wikipedia.org/wiki?curid=14858" title="Iain Banks">
Iain Banks

Iain Banks (16 February 1954 – 9 June 2013) was a Scottish author. He wrote mainstream fiction under the name Iain Banks and science fiction as Iain M. Banks, including the initial of his adopted middle name Menzies ().

After the publication and success of "The Wasp Factory" (1984), Banks began to write on a full-time basis. His first science fiction book, "Consider Phlebas", was released in 1987, marking the start of the Culture series. His books have been adapted for theatre, radio and television. In 2008, "The Times" named Banks in their list of "The 50 greatest British writers since 1945". In April 2013, Banks announced that he had inoperable cancer and was unlikely to live beyond a year. He died on 9 June 2013.

Banks was born in Dunfermline, Fife, to a mother who was a professional ice skater and a father who was an officer in the Admiralty. An only child, Banks lived in North Queensferry until the age of nine, near the naval dockyards in Rosyth where his father was based. Banks's family then moved to Gourock due to the requirements of his father's work. When someone introduced him to science fiction by giving him "Kemlo and the Zones of Silence", he continued reading the Kemlo series which made him want to write science fiction himself. After attending Gourock and Greenock High Schools, Banks studied English, philosophy and psychology at the University of Stirling (1972–1975).

After graduation Banks chose a succession of jobs that left him free to write in the evenings. These posts supported his writing throughout his twenties and allowed him to take long breaks between contracts, during which time he travelled through Europe and North America. During this period he worked as an IBM ’Expediter Analyser' (a kind of procurement clerk), a testing technician for the British Steel Corporation and a costing clerk for a law firm in London's Chancery Lane.

Banks decided to become a writer at the age of 11. He completed his first novel "The Hungarian Lift-Jet" at 16 and his second novel "TTR" (aka "The Tashkent Rambler") during his first year at Stirling University in 1972. Though he considered himself primarily a science fiction author, his lack of success at being published as such led him to pursue mainstream fiction, resulting in his first published novel "The Wasp Factory", which was published in 1984 when he was thirty. After the publication and success of "The Wasp Factory", Banks began to write full-time. His editor at Macmillan, James Hale, advised him to write one book a year and Banks agreed to this schedule.

His second novel "Walking on Glass" was published in 1985. "The Bridge" followed in 1986, and "Espedair Street", published in 1987, was later broadcast as a series on BBC Radio 4. His first published science fiction book "Consider Phlebas" was released in 1987 and was the first of several novels of the acclaimed Culture series. Banks cited Robert A. Heinlein, Isaac Asimov, Arthur C. Clarke, Brian Aldiss, M. John Harrison and Dan Simmons as literary influences. "The Crow Road", published in 1992, was adapted as a BBC television series. Banks continued to write both science fiction and mainstream novels, with his final novel "The Quarry" published in June 2013, the month of his death.

Banks published work under two names. His parents had intended to name him "Iain Menzies Banks", but his father made a mistake when registering the birth and "Iain Banks" became the officially registered name. Despite this error, Banks used the middle name and submitted "The Wasp Factory" for publication as "Iain M. Banks". Banks's editor inquired about the possibility of omitting the 'M' as it appeared "too fussy" and the potential existed for confusion with Rosie M. Banks, a romantic novelist in the Jeeves novels by P.G. Wodehouse; Banks agreed to the omission. After three mainstream novels, Banks's publishers agreed to publish his first science fiction (SF) novel "Consider Phlebas". To create a distinction between the mainstream and SF novels, Banks suggested the return of the 'M' to his name, and it was used in all of his science fiction works.
By his death in June 2013 Banks had published 26 novels. His twenty-seventh novel "The Quarry" was published posthumously. His final work, a collection of poetry, was released in February 2015. In an interview in January 2013, he also mentioned he had the plot idea for another novel in the Culture series, which would most likely have been his next book and was planned for publication in 2014.

He wrote in different categories, but enjoyed writing science fiction the most.

In September 2012 Banks was revealed as one of the Guests of Honour at the 2014 World Science Fiction Convention, Loncon 3.

Banks was the subject of "The Strange Worlds of Iain Banks" "South Bank Show" (1997), a television documentary that examined his mainstream writing, and was also an in-studio guest for the final episode of Marc Riley's "Rocket Science" radio show, broadcast on BBC Radio 6 Music.

An audio version of "The Business", set to contemporary music, arranged by Paul Oakenfold, was broadcast in October 1999 on Galaxy Fm as the tenth Urban Soundtracks.

A radio adaptation of Banks's "The State of the Art" was broadcast on BBC Radio 4 in 2009; the adaptation was written by Paul Cornell and the production was directed/produced by Nadia Molinari. In 1998 "Espedair Street" was dramatised as a serial for Radio 4, presented by Paul Gambaccini in the style of a Radio 1 documentary.

In 2011 Banks was featured on the BBC Radio 4 programme "Saturday Live". Banks reaffirmed his atheism during his "Saturday Live" appearance, whereby he explained that death is an important "part of the totality of life" and should be treated realistically, instead of feared.

Banks appeared on the BBC television programme "Question Time", a show that features political discussion. In 2006 Banks captained a team of writers to victory in a special series of BBC Two's "". Banks also won a 2006 edition of BBC One's "Celebrity Mastermind"; the author selected "Malt whisky and the distilleries of Scotland" as his specialist subject.

His final interview was with Kirsty Wark and was broadcast as "Iain Banks: Raw Spirit" on BBC2 Scotland on Wednesday 12 June 2013.

BBC One Scotland and BBC2 broadcast an adaptation of his novel Stonemouth in June 2015.

Banks was involved in the theatre production "The Curse of Iain Banks" that was written by Maxton Walker and was performed at the Edinburgh Fringe festival in 1999. Banks collaborated with the play's soundtrack composer Gary Lloyd frequently, including on a collection of songs they co-composed in tribute to the fictional band 'Frozen Gold' from Banks's novel 'Espedair Street'. Lloyd also composed the score for a spoken word and musical production of the Banks novel "The Bridge" which Banks himself voiced and featured a cast of forty musicians (released on cd by Codex Records in 1996). Lloyd recorded Banks for inclusion in the play as a disembodied voice appearing as himself in one of the cast member's dreams. Lloyd explained his collaboration with Banks on their first versions of 'Espedair Street' (subsequent versions are dated from between 2005 and 2013) in a "Guardian" article prior to the opening of "The Curse of Iain Banks":
When he [Banks] first played them to me, I think he was worried that they might not be up to scratch (some of them dated back to 1973 and had never been heard). He needn't have worried. They're fantastic. We're slaving away to get the songs to the stage where we can go into the studio and make a demo. Iain bashes out melodies on his state-of-the-art Apple Mac in Edinburgh and sends them down to me in Chester where I put them onto my Atari.
Banks' political position has been described as "left of centre", and he was an Honorary Associate of the National Secular Society and a Distinguished Supporter of the Humanist Society Scotland. As a signatory to the Declaration of Calton Hill, he was an open supporter of Scottish independence. In November 2012, Banks supported the campaign group that emerged from the Radical Independence Conference that was held during that month. Banks explained that the Scottish independence movement was motivated by co-operation and "Scots just seem to be more communitarian than the consensus expressed by the UK population as a whole".

In late 2004, Banks was a member of a group of British politicians and media figures who campaigned to have Prime Minister Tony Blair impeached following the 2003 invasion of Iraq. In protest, he cut up his passport and posted it to 10 Downing Street—in a "Socialist Review" interview, Banks explained that his passport protest occurred after he "abandoned the idea of crashing my Land Rover through the gates of Fife dockyard, after spotting the guys armed with machine guns." Banks relayed his concerns about the invasion of Iraq in his book "Raw Spirit", and the principal protagonist (Alban McGill) in the novel "The Steep Approach to Garbadale" confronts another character with arguments of a similar nature.

In 2010, Banks called for a cultural and educational boycott of Israel after the Gaza flotilla raid incident. In a letter to "The Guardian" newspaper, Banks stated that he had instructed his agent to turn down any further book translation deals with Israeli publishers:
Appeals to reason, international law, U.N. resolutions and simple human decency meanit is now obviousnothing to Israel ... I would urge all writers, artists and others in the creative arts, as well as those academics engaging in joint educational projects with Israeli institutions, to consider doing everything they can to convince Israel of its moral degradation and ethical isolation, preferably by simply having nothing more to do with this outlaw state.
An extract from Banks's contribution to the written collection "Generation Palestine: Voices from the Boycott, Divestment and Sanctions Movement", entitled "Our People", was published in "The Guardian" in the wake of the author's cancer revelation. The extract relays the author's support for the Boycott, Divestment and Sanctions (BDS) campaign that was issued by a Palestinian civil society against Israel until the country complies with what they hold are international law and Palestinian rights, that commenced in 2005 and applies the lessons from Banks's experience with South Africa's apartheid era. The continuation of Banks's boycott of Israeli publishers for the sale of the rights to his novels was also confirmed in the extract and Banks further explained, "I don't buy Israeli-sourced products or food, and my partner and I try to support Palestinian-sourced products wherever possible."

In 2002, Banks endorsed the Scottish Socialist Party.

Banks met his first wife, Annie, in London before the 1984 release of his first book. The couple lived in Faversham in the south of England, then split up in 1988. Banks returned to Edinburgh and dated another woman for two years until she left him. Iain and Annie reconciled a year later and moved to Fife. They got married in Hawaii in 1992; in 2007, after 15 years of marriage, they announced their separation.

In 1998 Banks was in a near-fatal accident when his car rolled off the road. In February 2007, Banks sold his extensive car collection, including a 3.2-litre Porsche Boxster, a Porsche 911 Turbo, a 3.8-litre Jaguar Mark II, a 5-litre BMW M5 and a daily use diesel Land Rover Defender whose power he had boosted by about 50%. Banks exchanged all of the vehicles for a Lexus RX 400h hybrid – later replaced by a diesel Toyota Yaris – and said in the future he would fly only in emergencies.

In April 2012 Banks became the "Acting Honorary Non-Executive Figurehead President Elect pro tem (trainee)" of the Science Fiction Book Club based in London. The title was his own creation and on 3 October 2012 Banks accepted a T-shirt decorated with this title.

As of 2007, Banks lived in North Queensferry, on the north side of the Firth of Forth, with his girlfriend Adele Hartley, an author and founder of the Dead by Dawn film festival. Banks and Hartley had been friends since the early 1990s, but commenced their romantic relationship in 2006, and married on 29 March 2013 after he asked her to "do me the honour of becoming my widow".

On 3 April 2013, Banks announced on his website, and one set up by him and some friends, that he had been diagnosed with terminal cancer of the gallbladder and was unlikely to live beyond a year. In his announcement, Banks stated that he would be withdrawing from all public engagements and that "The Quarry" would be his last novel. The dates of publication of "The Quarry" were brought forward at Banks's request, to 20 June 2013 in the UK and 25 June 2013 in the US and Canada. Banks died on 9 June 2013.

Banks's publisher stated that the author was "an irreplaceable part of the literary world", a sentiment that was reaffirmed by fellow Scottish author and friend since secondary school Ken MacLeod, who observed that Banks's death "left a large gap in the Scottish literary scene as well as the wider English-speaking world." British author Charles Stross wrote that "One of the giants of 20th and 21st century Scottish literature has left the building." Authors, including Neil Gaiman, Ian Rankin, Alastair Reynolds, and David Brin also paid tribute to Banks, in their blogs and elsewhere.

The asteroid 5099 Iainbanks was named after him shortly after his death. On 23 January 2015, SpaceX's CEO Elon Musk named two of the company's autonomous spaceport drone ships "Just Read The Instructions" and "Of Course I Still Love You", after ships from Banks's novel "The Player of Games". Another, "A Shortfall of Gravitas", began construction in 2018. The name is a reference to the ship "Experiencing A Significant Gravitas Shortfall", first mentioned in "Look to Windward".

On 13 May 2019, the Five Deeps Expedition achieved the deepest ocean dive record in the "DSV Limiting Factor". The support ship was named "DSSV Pressure Drop". The financial sponsor behind the Limiting Factor's design and construction, Victor Vescovo, is a great admirer of the science fiction genre and in particular of the Culture series. "Limiting Factor" and "Pressure Drop" are two of the ship names in the series.

Iain Banks received the following literary awards and nominations:

Banks's non-SF work comprises fourteen novels and one non-fiction book. Many of his novels contain elements of autobiography, and feature various locations in his native Scotland. "Raw Spirit" (subtitled "In Search of the Perfect Dram") is a travel book of Banks's visits to the distilleries of Scotland in search of the finest whisky, including his musings on other subjects such as cars and politics.



Banks wrote fourteen SF novels, ten of which were part of the Culture series, and a short story collection called "The State of the Art" (1991), which includes some stories set in the same universe. These works focus upon characters that are usually on the margins of the Culture, a post-scarcity anarchist utopia. In the same universe are other civilizations, with which the Culture sometimes enters into conflict, and sentient artificial intelligences.




Banks wrote introductions for works by other writers including:




</doc>
<doc id="14863" url="https://en.wikipedia.org/wiki?curid=14863" title="Incunable">
Incunable

An incunable, or sometimes incunabulum (plural incunables or incunabula, respectively), is a book, pamphlet, or broadside printed in Europe before the 16th century. Incunabula are not manuscripts, which are documents written by hand. there are about 30,000 distinct known incunable editions extant, but the probable number of surviving copies in Germany alone is estimated at around 125,000. Through statistical analysis, it is estimated that the number of lost editions is at least 20,000.

Incunable is the anglicised form of "incunabulum", reconstructed singular of Latin "incunabula", which meant "swaddling clothes", or "cradle", and which metaphorically could and can refer to "the earliest stages or first traces in the development of anything". A former term for incunable is fifteener, in the meaning of "fifteenth-century edition".

The term "incunabula" as a printing term was first used by the Dutch physician and humanist Hadrianus Iunius (Adriaan de Jonghe, 1511–1575) and appears in a passage from his posthumous work (written in 1569): Hadrianus Iunius, "Batavia", [...], [Lugduni Batavorum], ex officina Plantiniana, apud Franciscum Raphelengium, 1588, p. 256 l. 3: «inter prima artis [typographicae] incunabula», a term ("the first infancy of printing") to which he arbitrarily set an end of 1500 which still stands as a convention.

Only by a misunderstanding was Bernhard von Mallinckrodt (1591–1664) considered to be the inventor of this meaning of "incunabula"; the identical passage is found in his Latin pamphlet "De ortu ac progressu artis typographicae" ("On the rise and progress of the typographic art", Cologne, 1640): Bernardus a Mallinkrot, "De ortu ac progressu artis typographicae dissertatio historica", [...], Coloniae Agrippinae, apud Ioannem Kinchium, 1640 (in frontispiece: 1639), p. 29 l. 16: «inter prima artis [typographicae] incunabula», within a long passage of several pages, which he (correctly) quotes entirely in italic characters (that is between quotation marks), referring to the name of author and work cited: «Primus istorum [...] Hadrianus Iunius est, cuius integrum locum, ex "Batavia" eius, operae pretium est adscribere; [...]. Ita igitur Iunius» (ibid., p. 27 ll. 27–32, followed by the long passage, «Redeo → sordes», ibid., p. 27, l. 32 – p. 33 l. 32 [= "Batavia", p. 253 l. 28 – p. 258 l. 21]). So the source is only one, the other is a quotation.

The term "incunabula" came to denote the printed books themselves in the late 17th century. John Evelyn, in moving the Arundel Manuscripts to the Royal Society in August 1678, remarked of the printed books among the manuscripts: "The printed books, being of the oldest impressions, are not the less valuable; I esteem them almost equal to MSS." The convenient but arbitrarily chosen end date for identifying a printed book as an incunable does not reflect any notable developments in the printing process, and many books printed for a number of years after 1500 continued to be visually indistinguishable from incunables.

"Post-incunable" typically refers to books printed after 1500 up to another arbitrary end date such as 1520 or 1540. From around this period the dating of any edition becomes easier, as the practice of printers including information such as the place and year of printing became more widespread.

There are two types of "incunabula" in printing: the block book, printed from a single carved or sculpted wooden block for each page, employing the same process as the woodcut in art (these may be called "xylographic"); and the "typographic book", made with individual pieces of cast-metal movable type on a printing press. Many authors reserve the term "incunabula" for the latter kind only.

The spread of printing to cities both in the north and in Italy ensured that there was great variety in the texts chosen for printing and the styles in which they appeared. Many early typefaces were modelled on local forms of writing or derived from the various European forms of Gothic script, but there were also some derived from documentary scripts (such as most of Caxton's types), and, particularly in Italy, types modelled on handwritten scripts and calligraphy employed by humanists.

Printers congregated in urban centres where there were scholars, ecclesiastics, lawyers, and nobles and professionals who formed their major customer base. Standard works in Latin inherited from the medieval tradition formed the bulk of the earliest printed works, but as books became cheaper, vernacular works (or translations into vernaculars of standard works) began to appear.

The most famous "incunabula" include two from Mainz, the Gutenberg Bible of 1455 and the "Peregrinatio in terram sanctam" of 1486, printed and illustrated by Erhard Reuwich; the "Nuremberg Chronicle" written by Hartmann Schedel and printed by Anton Koberger in 1493; and the "Hypnerotomachia Poliphili" printed by Aldus Manutius with important illustrations by an unknown artist.

Other printers of incunabula were Günther Zainer of Augsburg, Johannes Mentelin and Heinrich Eggestein of Strasbourg, Heinrich Gran of Haguenau and William Caxton of Bruges and London. The first incunable to have woodcut illustrations was Ulrich Boner's "Der Edelstein", printed by Albrecht Pfister in Bamberg in 1461.

Many incunabula are undated, needing complex bibliographical analysis to place them correctly. The post-incunabula period marks a time of development during which the printed book evolved fully as a mature artefact with a standard format. After c. 1540 books tended to conform to a template that included the author, title-page, date, seller, and place of printing. This makes it much easier to identify any particular edition.

As noted above, the "end date" for identifying a printed book as an incunable is convenient but was chosen arbitrarily; it does not reflect any notable developments in the printing process around the year 1500. Books printed for a number of years after 1500 continued to look much like incunables, with the notable exception of the small format books printed in italic type introduced by Aldus Manutius in 1501. The term post-incunable is sometimes used to refer to books printed "after 1500—how long after, the experts have not yet agreed." For books printed in the UK, the term generally covers 1501–1520, and for books printed in mainland Europe, 1501–1540.

The data in this section were derived from the Incunabula Short-Title Catalogue (ISTC).

The number of printing towns and cities stands at 282. These are situated in some 18 countries in terms of present-day boundaries. In descending order of the number of editions printed in each, these are: Italy, Germany, France, Netherlands, Switzerland, Spain, Belgium, England, Austria, the Czech Republic, Portugal, Poland, Sweden, Denmark, Turkey, Croatia, Montenegro, and Hungary (see diagram).

The following table shows the 20 main 15th century printing locations; as with all data in this section, exact figures are given, but should be treated as close estimates (the total editions recorded in ISTC at May 2013 is 28,395):

The 18 languages that incunabula are printed in, in descending order, are: Latin, German, Italian, French, Dutch, Spanish, English, Hebrew, Catalan, Czech, Greek, Church Slavonic, Portuguese, Swedish, Breton, Danish, Frisian and Sardinian (see diagram).

Only about one edition in ten (i.e. just over 3,000) has any illustrations, woodcuts or metalcuts.

The "commonest" incunable is Schedel's "Nuremberg Chronicle" ("Liber Chronicarum") of 1493, with c 1,250 surviving copies (which is also the most heavily illustrated). Many incunabula are unique, but on average about 18 copies survive of each. This makes the Gutenberg Bible, at 48 or 49 known copies, a relatively common (though extremely valuable) edition. Counting extant incunabula is complicated by the fact that most libraries consider a single volume of a multi-volume work as a separate item, as well as fragments or copies lacking more than half the total leaves. A complete incunable may consist of a slip, or up to ten volumes.

In terms of format, the 29,000-odd editions comprise: 2,000 broadsides, 9,000 folios, 15,000 quartos, 3,000 octavos, 18 12mos, 230 16mos, 20 32mos, and 3 64mos.

ISTC at present cites 528 extant copies of books printed by Caxton, which together with 128 fragments makes 656 in total, though many are broadsides or very imperfect (incomplete).

Apart from migration to mainly North American and Japanese universities, there has been little movement of incunabula in the last five centuries. None were printed in the Southern Hemisphere, and the latter appears to possess less than 2,000 copies, about 97.75% remain north of the equator. However many incunabula are sold at auction or through the rare book trade every year.

The British Library's Incunabula Short Title Catalogue now records over 29,000 titles, of which around 27,400 are incunabula editions (not all unique works). Studies of incunabula began in the 17th century. Michel Maittaire (1667–1747) and Georg Wolfgang Panzer (1729–1805) arranged printed material chronologically in annals format, and in the first half of the 19th century, Ludwig Hain published, "Repertorium bibliographicum"— a checklist of incunabula arranged alphabetically by author: "Hain numbers" are still a reference point. Hain was expanded in subsequent editions, by Walter A. Copinger and Dietrich Reichling, but it is being superseded by the authoritative modern listing, a German catalogue, the "Gesamtkatalog der Wiegendrucke", which has been under way since 1925 and is still being compiled at the Staatsbibliothek zu Berlin. North American holdings were listed by Frederick R. Goff and a worldwide union catalogue is provided by the Incunabula Short Title Catalogue.

Notable collections, with the approximate numbers of incunabula held, include:




</doc>
<doc id="14865" url="https://en.wikipedia.org/wiki?curid=14865" title="Isotropy">
Isotropy

Isotropy is uniformity in all orientations; it is derived from the Greek "isos" (ἴσος, "equal") and "tropos" (τρόπος, "way"). Precise definitions depend on the subject area. Exceptions, or inequalities, are frequently indicated by the prefix "an", hence "anisotropy". "Anisotropy" is also used to describe situations where properties vary systematically, dependent on direction. Isotropic radiation has the same intensity regardless of the direction of measurement, and an isotropic field exerts the same action regardless of how the test particle is oriented.

Within mathematics, "isotropy" has a few different meanings:







In the study of mechanical properties of materials, "isotropic" means having identical values of a property in all directions. This definition is also used in geology and mineralogy. Glass and metals are examples of isotropic materials. Common anisotropic materials include wood, because its material properties are different parallel and perpendicular to the grain, and layered rocks such as slate.

Isotropic materials are useful since they are easier to shape, and their behavior is easier to predict. Anisotropic materials can be tailored to the forces an object is expected to experience. For example, the fibers in carbon fiber materials and rebars in reinforced concrete are oriented to withstand tension.

In industrial processes, such as etching steps, isotropic means that the process proceeds at the same rate, regardless of direction. Simple chemical reaction and removal of a substrate by an acid, a solvent or a reactive gas is often very close to isotropic. Conversely, anisotropic means that the attack rate of the substrate is higher in a certain direction. Anisotropic etch processes, where vertical etch-rate is high, but lateral etch-rate is very small are essential processes in microfabrication of integrated circuits and MEMS devices.

An isotropic antenna is an idealized "radiating element" used as a reference; an antenna that broadcasts power equally (calculated by the Poynting vector) in all directions. The gain of an arbitrary antenna is usually reported in decibels relative to an isotropic antenna, and is expressed as dBi or dB(i).








</doc>
<doc id="14868" url="https://en.wikipedia.org/wiki?curid=14868" title="International Mathematical Union">
International Mathematical Union

The International Mathematical Union (IMU) is an international non-governmental organization devoted to international cooperation in the field of mathematics across the world. It is a member of the International Science Council (ISC) and supports the International Congress of Mathematicians. Its members are national mathematics organizations from more than 80 countries.

The objectives of the International Mathematical Union (IMU) are: promoting international cooperation in mathematics, supporting and assisting the International Congress of Mathematicians (ICM) and other international scientific meetings/conferences, acknowledging outstanding research contributions to mathematics through the awarding of scientific prizes, and encouraging and supporting other international mathematical activities, considered likely to contribute to the development of mathematical science in any of its aspects, whether pure, applied, or educational.

The IMU was established in 1920, but dissolved in September 1932 and then re-established 1950 de facto at the Constitutive Convention in New York, de jure on September 10, 1951, when ten countries had become members. The last milestone was the General Assembly in March 1952, in Rome, Italy where the activities of the new IMU were inaugurated and the first Executive Committee, President and various commissions where elected. In 1952 the IMU was also readmitted to the ICSU. The past president of the Union is Shigefumi Mori (2015–2018). The current president is Carlos Kenig.

At the 16th meeting of the IMU General Assembly in Bangalore, India, in August 2010, Berlin was chosen as the location of the permanent office of the IMU, which was opened on January 1, 2011, and is hosted by the Weierstrass Institute for Applied Analysis and Stochastics (WIAS), an institute of the Gottfried Wilhelm Leibniz Scientific Community, with about 120 scientists engaging in mathematical research applied to complex problems in industry and commerce.

IMU has a close relationship to mathematics education through its International Commission on Mathematical Instruction (ICMI). This commission is organized similarly to IMU with its own Executive Committee and General Assembly.

Developing countries are a high priority for the IMU and a significant percentage of its budget, including grants received from individuals, mathematical societies, foundations, and funding agencies, is spent on activities for developing countries. Since 2011 this has been coordinated by the Commission for Developing Countries (CDC).

The Committee for Women in Mathematics (CWM) is concerned with issues related to women in mathematics worldwide. It organizes the World Meeting for Women in Mathematics formula_1 as a satellite event of ICM.

The International Commission on the History of Mathematics (ICHM) is operated jointly by the IMU and the Division of the History of Science (DHS) of the International Union of History and Philosophy of Science (IUHPS).

The Committee on Electronic Information and Communication (CEIC) advises IMU on matters concerning mathematical information, communication, and publishing.

The scientific prizes awarded by the IMU are deemed to be the highest distinctions in the mathematical world. The opening ceremony of the International Congress of Mathematicians (ICM) is where the awards are presented: Fields Medals (two to four medals are given since 1936), the Rolf Nevanlinna Prize (since 1986), the Carl Friedrich Gauss Prize (since 2006), and the Chern Medal Award (since 2010).

The IMU's members are Member Countries and each Member country is represented through an Adhering Organization, which may be its principal academy, a mathematical society, its research council or some other institution or association of institutions, or an appropriate agency of its government. A country starting to develop its mathematical culture and interested in building links to mathematicians all over the world is invited to join IMU as an Associate Member. For the purpose of facilitating jointly sponsored activities and jointly pursuing the objectives of the IMU, multinational mathematical societies and professional societies can join IMU as an Affiliate Member. Every four years the IMU membership gathers in a General Assembly (GA) which consists of delegates appointed by the Adhering Organizations, together with the members of the Executive Committee. All important decisions are made at the GA, including the election of the officers, establishment of commissions, the approval of the budget, and any changes to the statutes and by-laws.

The International Mathematical Union is administered by an Executive Committee (EC) which conducts the business of the Union. The EC consists of the President, two Vice-Presidents, the Secretary, six Members-at-Large, all elected for a term of four years, and the Past President. The EC is responsible for all policy matters and for tasks, such as choosing the members of the ICM Program Committee and various prize committees.

Every two months IMU publishes an electronic newsletter, "IMU-Net", that aims to improve communication between IMU and the worldwide mathematical community by reporting on decisions and recommendations of the Union, major international mathematical events and developments, and on other topics of general mathematical interest. IMU Bulletins are published annually with the aim to inform IMU’s members about the Union’s current activities. In 2009 IMU published the document "Best Current Practices for Journals".

The IMU took its first organized steps towards the promotion of mathematics in developing countries in the early 1970s and has, since then supported various activities. In 2010 IMU formed the Commission for Developing Countries (CDC) which brings together all of the past and current initiatives in support of mathematics and mathematicians in the developing world.

Some IMU Supported Initiatives:

IMU also supports the "International Commission on Mathematical Instruction" (ICMI) with its programmes, exhibits and workshops in emerging countries, especially in Asia and Africa.

IMU released a report in 2008, "Mathematics in Africa: Challenges and Opportunities", on the current state of mathematics in Africa and on opportunities for new initiatives to support mathematical development. In 2014, the IMU's Commission for Developing Countries CDC released an update of the report.

Additionally, reports about "Mathematics in Latin America and the Caribbean and South East Asia". were published.

In July 2014 IMU released the report: The International Mathematical Union in the Developing World: Past, Present and Future (July 2014).

In 2014, the IMU held a day-long symposium prior to the opening of the International Congress of Mathematicians (ICM), entitled "Mathematics in Emerging Nations: Achievements and Opportunities" (MENAO). Approximately 260 participants from around the world, including representatives of embassies, scientific institutions, private business and foundations attended this session. Attendees heard inspiring stories of individual mathematicians and specific developing nations.


List of presidents of the International Mathematical Union from 1952 to the present:

1952–1954: Marshall Harvey Stone (vice: Émile Borel, Erich Kamke)

1955–1958: Heinz Hopf (vice: Arnaud Denjoy, W. V. D. Hodge)

1959–1962: Rolf Nevanlinna (vice: Pavel Alexandrov, Marston Morse)

1963–1966: Georges de Rham (vice: Henri Cartan, Kazimierz Kuratowski)

1967–1970: Henri Cartan (vice: Mikhail Lavrentyev, Deane Montgomery)

1971–1974: K. S. Chandrasekharan (vice: Abraham Adrian Albert, Lev Pontryagin)

1975–1978: Deane Montgomery (vice: J. W. S. Cassels, Miron Nicolescu, Gheorghe Vrânceanu)

1979–1982: Lennart Carleson (vice: Masayoshi Nagata, Yuri Vasilyevich Prokhorov)

1983–1986: Jürgen Moser (vice: Ludvig Faddeev, Jean-Pierre Serre)

1987–1990: Ludvig Faddeev (vice: Walter Feit, Lars Hörmander)

1991–1994: Jacques-Louis Lions (vice: John H. Coates, David Mumford)

1995–1998: David Mumford (vice: Vladimir Arnold, Albrecht Dold)

1999–2002: Jacob Palis (vice: Simon Donaldson, Shigefumi Mori)

2003–2006: John M. Ball (vice: Jean-Michel Bismut, Masaki Kashiwara)

2007–2010: László Lovász (vice: Zhi-Ming Ma, Claudio Procesi)

2011–2014: Ingrid Daubechies (vice: Christiane Rousseau, Marcelo Viana)

2015–2018: Shigefumi Mori (vice: Alicia Dickenstein, Vaughan Jones)

2019–2022: Carlos Kenig (vice: Nalini Joshi, Loyiso Nongxa)




</doc>
<doc id="14869" url="https://en.wikipedia.org/wiki?curid=14869" title="International Council for Science">
International Council for Science

The International Council for Science (ICSU, after its former name, International Council of Scientific Unions) was an international non-governmental organization devoted to international cooperation in the advancement of science. Its members were national scientific bodies and international scientific unions.

In July 2018, the ICSU merged with the International Social Science Council (ISSC) to form the International Science Council (ISC).

In 2017, the ICSU comprised 122 multi-disciplinary National Scientific Members, Associates and Observers representing 142 countries and 31 international, disciplinary Scientific Unions. ICSU also had 22 Scientific Associates.

The ICSU’s mission was to strengthen international science for the benefit of society. To do this, the ICSU mobilized the knowledge and resources of the international scientific community to:


Activities focused on three areas: International Research Collaboration, Science for Policy, and Universality of Science.

In July 2018, the ICSU became the International Science Council (ISC).

The ICSU itself was one of the oldest non-governmental organizations in the world, representing the evolution and expansion of two earlier bodies known as the International Association of Academies (IAA; 1899-1914) and the International Research Council (IRC; 1919-1931). In 1998, Members agreed that the Council’s current composition and activities would be better reflected by modifying the name from the International Council of Scientific Unions to the International Council for Science, while its rich history and strong identity would be well served by retaining the existing acronym, ICSU.

The ICSU Principle of Universality of Science states: ""the free and responsible practice of science is fundamental to scientific advancement and human and environmental well-being. Such practice, in all its aspects, requires freedom of movement, association, expression and communication for scientists, as well as equitable access to data, information, and other resources for research. It requires responsibility at all levels to carry out and communicate scientific work with integrity, respect, fairness, trustworthiness, and transparency, recognising its benefits and possible harms.
"In advocating the free and responsible practice of science, ICSU promotes equitable opportunities for access to science and its benefits, and opposes discrimination based on such factors as ethnic origin, religion, citizenship, language, political or other opinion, sex, gender identity, sexual orientation, disability, or age."" 
Adherence to this Principle is a condition of ICSU membership. The Committee on Freedom and Responsibility in the conduct of Science (CFRS) "serves as the guardian of the Principle and undertakes a variety of actions to defend scientific freedoms and promote integrity and responsibility."

The ICSU Secretariat (20 staff in 2012) in Paris ensured the day-to-day planning and operations under the guidance of an elected Executive Board. Three Policy Committees − Committee on Scientific Planning and Review (CSPR), Committee on Freedom and Responsibility in the conduct of Science (CFRS) and Committee on Finance − assisted the Executive Board in its work and a General Assembly of all Members was convened every three years. ICSU has three Regional Offices − Africa, Asia and the Pacific as well as Latin America and the Caribbean.

The principal source of ICSU's finances was the contributions it receives from its members. Other sources of income are the framework contracts from UNESCO (United Nations Educational, Scientific and Cultural Organization) and grants and contracts from United Nations bodies, foundations and agencies, which are used to support the scientific activities of the ICSU Unions and interdisciplinary bodies.





</doc>
<doc id="14870" url="https://en.wikipedia.org/wiki?curid=14870" title="International Union of Pure and Applied Chemistry">
International Union of Pure and Applied Chemistry

The International Union of Pure and Applied Chemistry (IUPAC ) is an international federation of National Adhering Organizations that represents chemists in individual countries. It is a member of the International Science Council (ISC). IUPAC is registered in Zürich, Switzerland, and the administrative office, known as the "IUPAC Secretariat", is in Research Triangle Park, North Carolina, United States. This administrative office is headed by IUPAC's executive director, currently Lynn Soby.

IUPAC was established in 1919 as the successor of the International Congress of Applied Chemistry for the advancement of chemistry. Its members, the National Adhering Organizations, can be national chemistry societies, national academies of sciences, or other bodies representing chemists. There are fifty-four National Adhering Organizations and three Associate National Adhering Organizations. IUPAC's Inter-divisional Committee on Nomenclature and Symbols (IUPAC nomenclature) is the recognized world authority in developing standards for the naming of the chemical elements and compounds. Since its creation, IUPAC has been run by many different committees with different responsibilities. These committees run different projects which include standardizing nomenclature, finding ways to bring chemistry to the world, and publishing works.

IUPAC is best known for its works standardizing nomenclature in chemistry and other fields of science, but IUPAC has publications in many fields including chemistry, biology and physics. Some important work IUPAC has done in these fields includes standardizing nucleotide base sequence code names; publishing books for environmental scientists, chemists, and physicists; and improving education in science. IUPAC is also known for standardizing the atomic weights of the elements through one of its oldest standing committees, the Commission on Isotopic Abundances and Atomic Weights (CIAAW).

The need for an international standard for chemistry was first addressed in 1860 by a committee headed by German scientist Friedrich August Kekulé von Stradonitz. This committee was the first international conference to create an international naming system for organic compounds. The ideas that were formulated in that conference evolved into the official IUPAC nomenclature of organic chemistry. IUPAC stands as a legacy of this meeting, making it one of the most important historical international collaborations of chemistry societies. Since this time, IUPAC has been the official organization held with the responsibility of updating and maintaining official organic nomenclature. IUPAC as such was established in 1919. One notable country excluded from this early IUPAC is Germany. Germany's exclusion was a result of prejudice towards Germans by the Allied powers after World War I. Germany was finally admitted into IUPAC during 1929. However, Nazi Germany was removed from IUPAC during World War II.

During World War II, IUPAC was affiliated with the Allied powers, but had little involvement during the war effort itself. After the war, East and West Germany were readmitted to IUPAC in 1973. Since World War II, IUPAC has been focused on standardizing nomenclature and methods in science without interruption.

In 2016, IUPAC denounced the use of chlorine as a chemical weapon. The organization pointed out their concerns in a letter to Ahmet Üzümcü, the director of the Organisation for the Prohibition of Chemical Weapons (OPCW), in regards to the practice of utilizing chlorine for weapon usage in Syria among other locations. The letter stated, "Our organizations deplore the use of chlorine in this manner. The indiscriminate attacks, possibly carried out by a member state of the Chemical Weapons Convention (CWC), is of concern to chemical scientists and engineers around the globe and we stand ready to support your mission of implementing the CWC." According to the CWC, "the use, stockpiling, distribution, development or storage of any chemical weapons is forbidden by any of the 192 state party signatories."

IUPAC is governed by several committees that all have different responsibilities. The committees are as follows: Bureau, CHEMRAWN (Chem Research Applied to World Needs) Committee, Committee on Chemistry Education, Committee on Chemistry and Industry, Committee on Printed and Electronic Publications, Evaluation Committee, Executive Committee, Finance Committee, Interdivisional Committee on Terminology, Nomenclature and Symbols, Project Committee, and Pure and Applied Chemistry Editorial Advisory Board. Each committee is made up of members of different National Adhering Organizations from different countries.

The steering committee hierarchy for IUPAC is as follows:

IUPAC committee has a long history of officially naming organic and inorganic compounds. IUPAC nomenclature is developed so that any compound can be named under one set of standardized rules to avoid duplicate names. The first publication on IUPAC nomenclature of organic compounds was "A Guide to IUPAC Nomenclature of Organic Compounds" in 1900, which contained information from the International Congress of Applied Chemistry.

IUPAC organic nomenclature has three basic parts: the substituents, carbon chain length and chemical ending. The substituents are any functional groups attached to the main carbon chain. The main carbon chain is the longest possible continuous chain. The chemical ending denotes what type of molecule it is. For example, the ending "ane" denotes a single bonded carbon chain, as in "hexane" ().

Another example of IUPAC organic nomenclature is cyclohexanol:

Basic IUPAC inorganic nomenclature has two main parts: the cation and the anion. The cation is the name for the positively charged ion and the anion is the name for the negatively charged ion.

An example of IUPAC nomenclature of inorganic chemistry is potassium chlorate (KClO):

IUPAC also has a system for giving codes to identify amino acids and nucleotide bases. IUPAC needed a coding system that represented long sequences of amino acids. This would allow for these sequences to be compared to try to find homologies. These codes can consist of either a one letter code or a three letter code.

These codes make it easier and shorter to write down the amino acid sequences that make up proteins. The nucleotide bases are made up of purines (adenine and guanine) and pyrimidines (cytosine and thymine or uracil). These nucleotide bases make up DNA and RNA. These nucleotide base codes make the genome of an organism much smaller and easier to read.

The codes for amino acids (24 amino acids and three special codes) are:

The "Experimental Thermodynamics" books series covers many topics in the fields of thermodynamics.

IUPAC color code their books in order to make each publication distinguishable.
IUPAC and UNESCO were the lead organizations coordinating events for the International Year of Chemistry, which took place in 2011. The International Year of Chemistry was originally proposed by IUPAC at the general assembly in Turin, Italy. This motion was adopted by UNESCO at a meeting in 2008. The main objectives of the International Year of Chemistry were to increase public appreciation of chemistry and gain more interest in the world of chemistry. This event is also being held to encourage young people to get involved and contribute to chemistry. A further reason for this event being held is to honour how chemistry has made improvements to everyone's way of life.

IUPAC Presidents are elected by the IUPAC Council during the General Assembly. Below is the list of IUPAC Presidents since its inception in 1919.




</doc>
<doc id="14871" url="https://en.wikipedia.org/wiki?curid=14871" title="International Hydrographic Organization">
International Hydrographic Organization

The International Hydrographic Organization (IHO) is an intergovernmental organization representing hydrography. In October 2019 the IHO comprised 93 Member States.

A principal aim of the IHO is to ensure that the world's seas, oceans and navigable waters are properly surveyed and charted. It does this through the setting of international standards, the co-ordination of the endeavours of the world's national hydrographic offices, and through its capacity building programme.
The IHO enjoys observer status at the United Nations, where it is the recognised competent authority on hydrographic surveying and nautical charting. When referring to hydrography and nautical charting in Conventions and similar Instruments, it is the IHO standards and specifications that are normally used.

The IHO was established in 1921 as the International Hydrographic Bureau (IHB). The present name was adopted in 1970, as part of a new international Convention on the IHO adopted by the then member nations. The former name International Hydrographic Bureau was retained to describe the IHO secretariat until 8 November 2016, when a revision to the Convention on the IHO entered into force. Thereafter the IHB became known as the "IHO Secretariat", comprising an elected Secretary-General and two supporting Directors, together with a small permanent staff (17 as at August 2019), at the Organization's headquarters in Monaco.

During the 19th century, many maritime nations established hydrographic offices to provide means for improving the navigation of naval and merchant vessels by providing nautical publications, nautical charts, and other navigational services. There were substantial differences in hydrographic procedures charts, and publications. In 1889, an International Maritime Conference was held at Washington, D.C., and it was proposed to establish a "permanent international commission." Similar proposals were made at the sessions of the International Congress of Navigation held at Saint Petersburg in 1908 and the International Maritime Conference held at Saint Petersburg in 1912.

In 1919, the national Hydrographers of Great Britain and France cooperated in taking the necessary steps to convene an international conference of Hydrographers. London was selected as the most suitable place for this conference, and on 24 July 1919, the First International Conference opened, attended by the Hydrographers of 24 nations. The object of the conference was "To consider the advisability of all maritime nations adopting similar methods in preparation, construction, and production of their charts and all hydrographic publications; of rendering the results in the most convenient form to enable them to be readily used; of instituting a prompt system of mutual exchange of hydrographic information between all countries; and of providing an opportunity to consultations and discussions to be carried out on hydrographic subjects generally by the hydrographic experts of the world." This is still the major purpose of the IHO.

As a result of the 1919 Conference, a permanent organization was formed and statutes for its operations were prepared. The IHB, now the IHO, began its activities in 1921 with 18 nations as members. The Principality of Monaco was selected as the seat of the organization as a result of the offer of Albert I of Monaco to provide suitable accommodation for the Bureau in the Principality.

The IHO develops hydrographic and nautical charting standards. These standards are subsequently adopted and used by its currently 91 (as at August 2019) member countries and others in their surveys, nautical charts, and publications. The almost universal use of the IHO standards means that the products and services provided by the world's national hydrographic and oceanographic offices are consistent and recognisable by all seafarers and for other users. Much has been done in the field of standardisation since the IHO was founded.

The IHO has encouraged the formation of Regional Hydrographic Commissions (RHCs). Each RHC coordinates the national surveying and charting activities of countries within each region and acts as a forum to address other matters of common hydrographic interest. The 15 RHCs plus the IHO Hydrographic Commission on Antarctica effectively cover the world. The IHO, in partnership with the Intergovernmental Oceanographic Commission, directs the General Bathymetric Chart of the Oceans programme.

Establishment of the Chart Specifications Committee and International Charts:

Most IHO publications, including the standards, guidelines and associated documents such as the "International Hydrographic Review", "International Hydrographic Bulletin", the "Hydrographic Dictionary" and the "Year Book" are available to the general public free of charge from the IHO website. The IHO publishes the international standards related to charting and hydrography, including S-57, "IHO Transfer Standard for Digital Hydrographic Data", the encoding standard that is used primarily for electronic navigational charts.

In 2010, the IHO introduced a new, contemporary hydrographic geospatial standard for modelling marine data and information, known as S-100. S-100 and any dependent product specifications are underpinned by an on-line registry accessible via the IHO website. S-100 is aligned with the ISO 19100 series of geographic standards, thereby making it fully compatible with contemporary geospatial data standards.

Because S-100 is based on ISO 19100, it can be used by other data providers for their maritime-related (non-hydrographic) data and information. Various data and information providers from both the government and private sector are now using S-100 as part of the implementation of the e-Navigation concept that has been endorsed by the UN International Maritime Organization.



</doc>
<doc id="14872" url="https://en.wikipedia.org/wiki?curid=14872" title="IBM mainframe">
IBM mainframe

IBM mainframes are large computer systems produced by IBM since 1952. During the 1960s and 1970s, IBM dominated the large computer market. Current mainframe computers in IBM's line of business computers are developments of the basic design of the IBM System/360.

From 1952 into the late 1960s, IBM manufactured and marketed several large computer models, known as the IBM 700/7000 series. The first-generation 700s were based on vacuum tubes, while the later, second-generation 7000s used transistors. These machines established IBM's dominance in electronic data processing ("EDP"). IBM had two model categories: one (701, 704, 709, 7030, 7090, 7094, 7040, 7044) for engineering and scientific use, and one (702, 705, 705-II, 705-III, 7080, 7070, 7072, 7074, 7010) for commercial or data processing use. The two categories, scientific and commercial, generally used common peripherals but had completely different instruction sets, and there were incompatibilities even within each category.

IBM initially sold its computers without any software, expecting customers to write their own; programs were manually initiated, one at a time. Later, IBM provided compilers for the newly developed higher-level programming languages Fortran, COMTRAN and later COBOL. The first operating systems for IBM computers were written by IBM customers who did not wish to have their very expensive machines ($2M USD in the mid-1950s) sitting idle while operators set up jobs manually. These first operating systems were essentially scheduled work queues. It is generally thought that the first operating system used for real work was GM-NAA I/O, produced by General Motors' Research division in 1956. IBM enhanced one of GM-NAA I/O's successors, the SHARE Operating System, and provided it to customers under the name IBSYS. As software became more complex and important, the cost of supporting it on so many different designs became burdensome, and this was one of the factors which led IBM to develop System/360 and its operating systems.

The second generation (transistor-based) products were a mainstay of IBM's business and IBM continued to make them for several years after the introduction of the System/360. (Some IBM 7094s remained in service into the 1980s.)

Prior to System/360, IBM also sold computers smaller in scale that were not considered mainframes, though they were still bulky and expensive by modern standards. These included:

IBM had difficulty getting customers to upgrade from the smaller machines to the mainframes because so much software had to be rewritten. The 7010 was introduced in 1962 as a mainframe-sized 1410. The later Systems 360 and 370 could emulate the 1400 machines. A desk-size machine with a different instruction set, the IBM 1130, was released concurrently with the System/360 to address the niche occupied by the 1620. It used the same EBCDIC character encoding as the 360 and was mostly programmed in Fortran, which was relatively easy to adapt to larger machines when necessary.

IBM also introduced smaller machines after S/360. These included:

"Midrange computer" is a designation used by IBM for a class of computer systems which fall in between mainframes and microcomputers.

All that changed with the announcement of the System/360 (S/360) in April, 1964. The System/360 was a single series of compatible models for both commercial and scientific use. The number "360" suggested a "360 degree," or "all-around" computer system. System/360 incorporated features which had previously been present on only either the commercial line (such as decimal arithmetic and byte addressing) or the engineering and scientific line (such as floating point arithmetic). Some of the arithmetic units and addressing features were optional on some models of the System/360. However, models were upward compatible and most were also downward compatible. The System/360 was also the first computer in wide use to include dedicated hardware provisions for the use of operating systems. Among these were supervisor and application mode programs and instructions, as well as built-in memory protection facilities. Hardware memory protection was provided to protect the operating system from the user programs (tasks) and user tasks from each other. The new machine also had a larger address space than the older mainframes, 24 bits addressing 8-bit bytes vs. a typical 18 bits addressing 36-bit words. 

The smaller models in the System/360 line (e.g. the 360/30) were intended to replace the 1400 series while providing an easier upgrade path to the larger 360s. To smooth the transition from the second generation to the new line, IBM used the 360's microprogramming capability to emulate the more popular older models. Thus 360/30s with this added cost feature could run 1401 programs and the larger 360/65s could run 7094 programs. To run old programs, the 360 had to be halted and restarted in emulation mode. Many customers kept using their old software and one of the features of the later System/370 was the ability to switch to emulation mode and back under operating system control.

Operating systems for the System/360 family included OS/360 (with PCP, MFT, and MVT), BOS/360, TOS/360, and DOS/360.

The System/360 later evolved into the System/370, the System/390, and the 64-bit zSeries, System z, and zEnterprise machines. System/370 introduced virtual memory capabilities in all models other than the very first System/370 models; the OS/VS1 variant of OS/360 MFT, the OS/VS2 (SVS) variant of OS/360 MVT, and the DOS/VS variant of DOS/360 were introduced to use the virtual memory capabilities, followed by MVS, which, unlike the earlier virtual-memory operating systems, ran separate programs in separate address spaces, rather than running all programs in a single virtual address space. The virtual memory capabilities also allowed the system to support virtual machines; the VM/370 hypervisor would run one or more virtual machines running either standard System/360 or System/370 operating systems or the single-user Conversational Monitor System (CMS). A time-sharing VM system could run multiple virtual machines, one per user, with each virtual machine running an instance of CMS.

The zSeries family, introduced in 2000 with the z900, included IBM's newly designed 64-bit z/Architecture.

The different processors on current IBM mainframes are:

Note that these are essentially identical, but distinguished for software cost control: all but CP are slightly restricted such they cannot be used to run arbitrary operating systems, and thus do not count in software licensing costs (which are typically based on the number of CPs).
There are other supporting processors typically installed inside mainframes such as cryptographic accelerators (CryptoExpress), the OSA-Express networking processor, and FICON Express disk I/O processors.

Software to allow users to run "traditional" workloads on zIIPs and zAAPs was briefly marketed by Neon Enterprise Software as "zPrime" but was withdrawn from the market in 2011 after a lawsuit by IBM.

The primary operating systems in use on current IBM mainframes include z/OS (which followed MVS/ESA and OS/390 in the OS/360 lineage), z/VM (which followed VM/ESA and VM/XA in the CP-40 lineage), z/VSE (which is in the DOS/360 lineage), z/TPF (a successor of Airlines Control Program), and Linux on IBM Z such as SUSE Linux Enterprise Server and others. A few systems run MUSIC/SP and UTS (Mainframe UNIX). In October 2008, Sine Nomine Associates introduced OpenSolaris on System z.

Current IBM mainframes run all the major enterprise transaction processing environments and databases, including CICS, IMS, WebSphere Application Server, DB2, and Oracle. In many cases these software subsystems can run on more than one mainframe operating system.

There are software-based emulators for the System/370, System/390, and System z hardware, including FLEX-ES, which runs under UnixWare or Linux, and the freely available Hercules, which runs under Linux, FreeBSD, Solaris, macOS and Microsoft Windows.
IBM offers an emulator called zPDT (System z Personal Development Tool) which runs on Linux on x86-64 machines.





</doc>
<doc id="14875" url="https://en.wikipedia.org/wiki?curid=14875" title="Iowa State University">
Iowa State University

Iowa State University of Science and Technology, (Iowa State University or Iowa State), is a public land-grant research university in Ames, Iowa. It is the largest university in the state of Iowa and the third largest university in the Big 12 athletic conference. Iowa State is classified among "R1: Doctoral Universities – Very high research activity" and is a member of the Association of American Universities (AAU).

Founded in 1858 and coeducational from its start, Iowa State became the nation's first designated land-grant institution when the Iowa Legislature accepted the provisions of the 1862 Morrill Act on September 11, 1862, making Iowa the first state in the nation to do so.

Iowa State's academic offerings are administered today through eight colleges, including the graduate college, that offer over 100 bachelor's degree programs, 112 master's degree programs, and 83 doctoral programs, plus a professional degree program in Veterinary Medicine.

Iowa State University's athletic teams, the Cyclones, compete in Division I of the NCAA and are a founding member of the Big 12. The Cyclones field 16 varsity teams and have won numerous NCAA national championships.

In 1856, the Iowa General Assembly enacted legislation to establish the Iowa Agricultural College and Model Farm. This institution (now Iowa State University) was officially established on March 22, 1858, by the General Assembly. Story County was chosen as the location on June 21, 1859, beating proposals from Johnson, Kossuth, Marshall and Polk counties. The original farm of was purchased for a cost of $5,379.
Iowa was the first state in the nation to accept the provisions of the Morrill Act of 1862. Iowa subsequently designated Iowa State as the land-grant college on March 29, 1864. From the start, Iowa Agricultural College focused on the ideals that higher education should be accessible to all and that the university should teach liberal and practical subjects. These ideals are integral to the land-grant university.

The institution was coeducational from the first preparatory class admitted in 1868. The formal admitting of students began the following year, and the first graduating class of 1872 consisted of 24 men and two women.

The Farm House, the first building on the Iowa State campus, was completed in 1861 before the campus was occupied by students or classrooms. It became the home of the superintendent of the Model Farm and in later years, the deans of Agriculture, including Seaman Knapp and "Tama Jim" Wilson. Iowa State's first president, Adonijah Welch, briefly stayed at the Farm House and penned his inaugural speech in a second floor bedroom.

The college's first farm tenants primed the land for agricultural experimentation. The Iowa Experiment Station was one of the university's prominent features. Practical courses of instruction were taught, including one designed to give a general training for the career of a farmer. Courses in mechanical, civil, electrical, and mining engineering were also part of the curriculum.

In 1870, President Welch and I. P. Roberts, professor of agriculture, held three-day farmers' institutes at Cedar Falls, Council Bluffs, Washington, and Muscatine. These became the earliest institutes held off-campus by a land grant institution and were the forerunners of 20th century extension.

In 1872, the first courses were given in domestic economy (home economics, family and consumer sciences) and were taught by Mary B. Welch, the president's wife. Iowa State became the first land grant university in the nation to offer training in domestic economy for college credit.

In 1879, the School of Veterinary Science was organized, the first state veterinary college in the United States (although veterinary courses had been taught since the beginning of the College). This was originally a two-year course leading to a diploma. The veterinary course of study contained classes in zoology, botany, anatomy of domestic animals, veterinary obstetrics, and sanitary science.
William M. Beardshear was appointed President of Iowa State in 1891. During his tenure, Iowa Agricultural College truly came of age. Beardshear developed new agricultural programs and was instrumental in hiring premier faculty members such as Anson Marston, Louis B. Spinney, J.B. Weems, Perry G. Holden, and Maria Roberts. He also expanded the university administration, and added Morrill Hall (1891), the Campanile (1899), Old Botany (now Carrie Chapman Catt Hall) (1892), and Margaret Hall (1895) to the campus, all of which stand today. In his honor, Iowa State named its central administrative building (Central Building) after Beardshear in 1925. In 1898, reflecting the school's growth during his tenure, it was renamed Iowa State College of Agricultural and Mechanic Arts, or Iowa State for short.

Today, Beardshear Hall holds the offices of the President, Vice-President, Treasurer, Secretary, Registrar, Provost, and student financial aid. Catt Hall is named after alumna and famed suffragette Carrie Chapman Catt, and is the home of the College of Liberal Arts and Sciences.
In 1912 Iowa State had its first Homecoming celebration. The idea was first proposed by Professor Samuel Beyer, the college's “patron saint of athletics,” who suggested that Iowa State inaugurate a celebration for alumni during the annual football game against rival University of Iowa. Iowa State's new president, Raymond A. Pearson, liked the idea and issued a special invitation to alumni two weeks prior to the event: “We need you, we must have you. Come and see what a school you have made in Iowa State College. Find a way.” In October 2012 Iowa State marked its 100th Homecoming with a "CYtennial" Celebration.

Iowa State celebrated its first VEISHEA on May 11–13, 1922. Wallace McKee (class of 1922) served as the first chairman of the Central Committee and Frank D. Paine (professor of electrical engineering) chose the name, based on the first letters of Iowa State's colleges: Veterinary Medicine, Engineering, Industrial Science, Home Economics, and Agriculture. VEISHEA grew to become the largest student-run festival in the nation.

The Statistical Laboratory was established in 1933, with George W. Snedecor, professor of mathematics, as the first director. It was and is the first research and consulting institute of its kind in the country.

While attempting to develop a faster method of computation, mathematics and physics professor John Vincent Atanasoff conceptualized the basic tenets of what would become the world's first electronic digital computer, the Atanasoff-Berry Computer (ABC), during a drive to Illinois in 1937. These included the use of a binary system of arithmetic, the separation of computer and memory functions, and regenerative drum memory, among others. The 1939 prototype was constructed with graduate student Clifford Berry in the basement of the Physics Building.

During World War II, Iowa State was one of 131 colleges and universities nationally that took part in the V-12 Navy College Training Program which offered students a path to a Navy commission.

On July 4, 1959, the college was officially renamed Iowa State University of Science and Technology. However, the short-form name "Iowa State University" is used even in official documents such as diplomas.

Official names given to the university's divisions were the College of Agriculture, College of Engineering, College of Home Economics, College of Sciences and Humanities, and College of Veterinary Medicine.

Iowa State's eight colleges today offer more than 100 undergraduate majors and 200 fields of study leading to graduate and professional degrees. The academic program at ISU includes a liberal arts education and some of the world's leading research in the biological and physical sciences.

Breakthroughs at Iowa State changing the world are in the areas of human, social, economic, and environmental sustainability; new materials and processes for biomedical as well as industrial applications; nutrition, health, and wellness for humans and animals; transportation and infrastructure; food safety and security; plant and animal sciences; information and decision sciences; and renewable energies. The focus on technology has led directly to many research patents and inventions including the first binary computer, the ABC, Maytag blue cheese, the round hay baler, and many more.

Located on a campus, the university has grown considerably from its roots as an agricultural college and model farm and is recognized internationally today for its comprehensive research programs. It continues to grow and set a new record for enrollment in the fall of 2015 with 36,001 students.

Iowa State University is organized into eight colleges and two schools that offer 100 Bachelor's degree programs, 112 Masters programs, and 83 Ph.D programs, including one professional degree program in Veterinary Medicine.

ISU is home to the following schools:
Classified as one of Carnegie's "R1: Doctoral Universities - Very High Research Activity," Iowa State receives nearly $300 million in research grants each year.

The university is one of 62 elected members of the Association of American Universities, an organization composed of the most highly ranked public and private research universities in the U.S. and Canada.

In 2016-17 Iowa State university became part of only fifty-four institutions in the U.S. to have earned the "Innovation and Economic Prosperity University" designation by the Association of Public and Land-grant Universities.

The agriculture and forestry programs are consistently ranked in the top 20 in the world by QS. In engineering specialties, at schools whose highest degree is a doctorate, Iowa State's biological/agricultural engineering program is ranked first, the mechanical and civil are ranked 9th and 16th nationally in the U.S. by "U.S. News & World Report." Almost all of the engineering specialities at ISU are ranked in the top 30 nationally. ISU's chemistry and physics programs are considered to be some of the best in the world and are ranked in the Top 100 globally and in Top 50 nationally. ISU's Greenlee School of Journalism and Mass Communication is one of the top journalism schools in the country and is notable for being among the first group of accredited journalism and mass communication programs. Greenlee is also cited as one of the leading JMC research programs in the nation, ranked 23rd in a publication by the AEJMC.

The National Science Foundation ranks ISU 78th in the nation in total research and development expenditures and 94th in research and development expenditures for science and engineering. Currently, ISU ranks second nationally in license and options executed on its intellectual property and #2 nationally in license and options that yield income.

In 2016, ISU's landscape architecture program was ranked as the 10th best undergraduate program in the nation, and architecture as the 18th best.

The W. Robert and Ellen Sorge Parks Library contains over 2.6 million books and subscribes to more than 98,600 journal titles. Named for W. Robert Parks (1915–2003), the 11th president of Iowa State University, and his wife, Ellen Sorge Parks, the original library was built in 1925 with three subsequent additions made in 1961, 1969, and 1983. The library was dedicated and named after W. Robert and Ellen Sorge Parks in 1984.

Parks Library provides extensive research collections, services, and information literacy instruction/information for all students. Facilities consist of the main Parks Library, the e-Library, the Veterinary Medical Library, two subject-oriented reading rooms (design and mathematics), and a remote library storage building.

The Library's extensive collections include electronic and print resources that support research and study for all undergraduate and graduate programs. Nationally recognized collections support the basic and applied fields of biological and physical sciences. The Parks Library includes four public service desks: the Learning Connections Center, the Circulation Desk, the Media Center (including Maps, Media,
Microforms, and Course Reserve collections), and Special Collections. The Library's instruction program includes a required undergraduate information literacy course as well as a wide variety of subject-based seminars on the effective use of Library resources for undergraduate and graduate students.

The e-Library, accessed through the Internet, provides access to local and Web-based resources including electronic journals and books, local collections, online indexes, electronic course reserves and guides, and a broad range of subject research guides.

Surrounding the first floor lobby staircase in Parks Library are eight mural panels
designed by Iowa artist Grant Wood. As with "Breaking the Prairie Sod", Wood's other
Iowa State University mural painted two years later, Wood borrowed his theme for "When Tillage Begins Other Arts Follow" from a speech on agriculture delivered by Daniel
Webster in 1840 at the State House in Boston. Webster said, “When tillage begins, other
arts follow. The farmers therefore are the founders of human civilization.” Wood had
planned to create seventeen mural panels for the library, but only the eleven devoted to
agriculture and the practical arts were completed. The final six, which would have hung
in the main reading room (now the Periodical Room) and were to have depicted the fine
arts, were never begun.

The university has an IEOP for foreign students. Students whose native language is not English can take IEOP courses to improve their English proficiency to help them succeed at University-level study. IEOP course content also helps students prepare for English proficiency exams, like the TOEFL and IELTS. Classes included in the IEOP include Grammar, Reading, Writing, Oral Communication and Business and various bridge classes.

Iowa State is the birthplace of the first electronic digital computer, starting the world's computer technology revolution. Invented by mathematics and physics professor John Atanasoff and engineering graduate student Clifford Berry during 1937–42, the Atanasoff-Berry Computer pioneered important elements of modern computing.

On October 19, 1973, U.S. Federal Judge Earl R. Larson signed his decision following a lengthy court trial which declared the ENIAC patent of Mauchly and Eckert invalid and named Atanasoff the inventor of the electronic digital computer—the Atanasoff-Berry Computer or the ABC.

An ABC Team consisting of Ames Laboratory and Iowa State engineers, technicians, researchers and students unveiled a working replica of the Atanasoff-Berry Computer in 1997 which can be seen on display on campus in the Durham Computation Center.

The Extension Service traces its roots to farmers' institutes developed at Iowa State in the late 19th century. Committed to community, Iowa State pioneered the outreach mission of being a land-grant college through creation of the first Extension Service in 1902. In 1906, the Iowa Legislature enacted the Agricultural Extension Act making funds available for demonstration projects. It is believed this was the first specific legislation establishing state extension work, for which Iowa State assumed responsibility. The national extension program was created in 1914 based heavily on the Iowa State model.

Iowa State is widely known for VEISHEA, an annual education and entertainment festival that was held on campus each spring. The name VEISHEA was derived from the initials of ISU's five original colleges, forming an acronym as the university existed when the festival was founded in 1922:

VEISHEA was the largest student run festival in the nation, bringing in tens of thousands of visitors to the campus each year.

The celebration featured an annual parade and many open-house demonstrations of the university facilities and departments. Campus organizations exhibited products, technologies, and held fund raisers for various charity groups. In addition, VEISHEA brought speakers, lecturers, and entertainers to Iowa State, and throughout its over eight decade history, it has hosted such distinguished guests as Bob Hope, John Wayne, Presidents Harry Truman, Ronald Reagan, and Lyndon Johnson, and performers Diana Ross, Billy Joel, Sonny and Cher, The Who, The Goo Goo Dolls, Bobby V, and The Black Eyed Peas.

The 2007 VEISHEA festivities marked the start of Iowa State's year-long sesquicentennial celebration.

On August 8, 2014, President Steven Leath announced that VEISHEA would no longer be an annual event at Iowa State and the name VEISHEA would be retired.

Iowa State played a role in the development of the atomic bomb during World War II as part of the Manhattan Project, a research and development program begun in 1942 under the Army Corps of Engineers.

The process to produce large quantities of high-purity uranium metal became known as the Ames process. One-third of the uranium metal used in the world's first controlled nuclear chain reaction was produced at Iowa State under the direction of Frank Spedding and Harley Wilhelm. The Ames Project received the Army/Navy E Award for Excellence in Production on October 12, 1945 for its work with metallic uranium as a vital war material. Today, ISU is the only university in the United States that has a U.S. Department of Energy research laboratory physically located on its campus.

Iowa State is the only university in the United States that has a U.S. Department of Energy research laboratory physically located on its campus. Operated by Iowa State, the Ames Laboratory is one of ten national DOE Office of Science research laboratories.

ISU research for the government provided Ames Laboratory its start in the 1940s with the development of a highly efficient process for producing high-purity uranium for atomic energy. Today, Ames Laboratory continues its leading status in current materials research and focuses diverse fundamental and applied research strengths upon issues of national concern, cultivates research talent, and develops and transfers technologies to improve industrial competitiveness and enhance U.S. economic security. Ames Laboratory employs more than 430 full- and part-time employees, including more than 250 scientists and engineers. Students make up more than 20 percent of the paid workforce.

The Ames Laboratory is the U.S. home to 2011 Nobel Prize in Chemistry winner Dan Shechtman and is intensely engaged with the international scientific community, including hosting a large number of international visitors each year.

The ISU Research Park is a 230-acre development with over 270,000 square feet of building space located just south of the Iowa State campus in Ames. Though closely connected with the university, the research park operates independently to help tenants reach their proprietary goals, linking technology creation, business formation, and development assistance with established technology firms and the marketplace.

The ISU Research Park Corporation was established in 1987 as a not-for-profit, independent, corporation operating under a board of directors appointed by Iowa State University and the ISU Foundation. The corporation manages both the Research Park and incubator programs.

Iowa State is involved in a number of other significant research and creative endeavors, multidisciplinary collaboration, technology transfer, and strategies addressing real-world problems.

In 2010, the Biorenewables Research Laboratory opened in a LEED-Gold certified building that complements and helps replace labs and offices across Iowa State and promotes interdisciplinary, systems-level research and collaboration. The Lab houses the Bioeconomy Institute, the Biobased Industry Center, and the National Science Foundation Engineering Research Center for Biorenewable Chemicals, a partnership of six universities as well as the Max Planck Society in Germany and the Technical University of Denmark.

The Engineering Teaching and Research Complex was built in 1999 and is home to Stanley and Helen Howe Hall and Gary and Donna Hoover Hall. The complex is occupied by the Virtual Reality Applications Center (VRAC), Center for Industrial Research and Service (CIRAS), Department of Aerospace Engineering and Engineering Mechanics, Department of Materials Science and Engineering, Engineering Computer Support Services, Engineering Distance Education, and Iowa Space Grant Consortium. And the complex contains one of the world's only six-sided immersive virtual reality labs (C6), as well as the 240 seat 3D-capable Alliant Energy Lee Liu Auditorium, the Multimodal Experience Testbed and Laboratory (METaL), and the User Experience Lab (UX Lab). All of which supports the research of more than 50 faculty and 200 graduate, undergraduate, and postdoctoral students.

There is also the Iowa State University Northeast Research Farm in Nashua.

Iowa State's campus contains over 160 buildings. Several buildings, as well as the Marston Water Tower, are listed on the National Register of Historic Places. The central campus includes of trees, plants, and classically designed buildings. The landscape's most dominant feature is the central lawn, which was listed as a "medallion site" by the American Society of Landscape Architects in 1999, one of only three central campuses designated as such. The other two were Harvard University and the University of Virginia.

Thomas Gaines, in "The Campus As a Work of Art", proclaimed the Iowa State campus to be one of the twenty-five most beautiful campuses in the country. Gaines noted Iowa State's park-like expanse of central campus, and the use of trees and shrubbery to draw together ISU's varied building architecture. Over decades, campus buildings, including the Campanile, Beardshear Hall, and Curtiss Hall, circled and preserved the central lawn, creating a space where students study, relax, and socialize.

The campanile was constructed during 1897-1898 as a memorial to Margaret MacDonald Stanton, Iowa State's first dean of women, who died on July 25, 1895. The tower is located on ISU's central campus, just north of the Memorial Union. The site was selected by Margaret's husband, Edgar W. Stanton, with the help of then-university president William M. Beardshear. The campanile stands tall on a 16 by 16 foot (5 by 5 m) base, and cost $6,510.20 to construct.

The campanile is widely seen as one of the major symbols of Iowa State University. It is featured prominently on the university's official ring and the university's mace, and is also the subject of the university's alma mater, "The Bells of Iowa State".

Named for Dr. LaVerne W. Noyes, who also donated the funds to see that Alumni Hall could be completed after sitting unfinished and unused from 1905 to 1907. Dr. Noyes is an 1872 alumnus. Lake LaVerne is located west of the Memorial Union and south of Alumni Hall, Carver Hall, and Music Hall. The lake was a gift from Dr. Noyes in 1916.

Lake LaVerne is the home of two mute swans named Sir Lancelot and Elaine, donated to Iowa State by VEISHEA 1935. In 1944, 1970, and 1971 cygnets (baby swans) made their home on Lake LaVerne. Previously Sir Lancelot and Elaine were trumpeter swans but were too aggressive and in 1999 were replaced with two mute swans.

In early spring 2003, Lake LaVerne welcomed its newest and most current mute swan duo. In support of Iowa Department of Natural Resources efforts to re-establish the trumpeter swans in Iowa, university officials avoided bringing breeding pairs of male and female mute swans to Iowa State which means the current Sir Lancelot and Elaine are both female.

Iowa State has maintained a horticulture garden since 1914. Reiman Gardens is the third location for these gardens. Today's gardens began in 1993 with a gift from Bobbi and Roy Reiman. Construction began in 1994 and the Gardens' initial were officially dedicated on September 16, 1995.

Reiman Gardens has since grown to become a site consisting of a dozen distinct garden areas, an indoor conservatory and an indoor butterfly "wing", butterfly emergence cases, a gift shop, and several supporting greenhouses. Located immediately south of Jack Trice Stadium on the ISU campus, Reiman Gardens is a year-round facility that has become one of the most visited attractions in central Iowa.

The Gardens has received a number of national, state, and local awards since its opening, and its rose gardens are particularly noteworthy. It was honored with the President's Award in 2000 by All American Rose Selections, Inc., which is presented to one public garden in the United States each year for superior rose maintenance and display: “For contributing to the public interest in rose growing through its efforts in maintaining an outstanding public rose garden.”

The University Museums consist of the Brunnier Art Museum, Farm House Museum, the Art on Campus Program, the Christian Petersen Art Museum, and the Elizabeth and Byron Anderson Sculpture Garden. The Museums include a multitude of unique exhibits, each promoting the understanding and delight of the visual arts as well as attempt to incorporate a vast interaction between the arts, sciences, and technology.

The Brunnier Art Museum, Iowa's only accredited museum emphasizing a decorative arts collection, is one of the nation's few museums located within a performing arts and conference complex, the Iowa State Center. Founded in 1975, the museum is named after its benefactors, Iowa State alumnus Henry J. Brunnier and his wife Ann. The decorative arts collection they donated, called the Brunnier Collection, is extensive, consisting of ceramics, glass, dolls, ivory, jade, and enameled metals.

Other fine and decorative art objects from the University Art Collection include prints, paintings, sculptures, textiles, carpets, wood objects, lacquered pieces, silver, and furniture. About eight to 12 annual changing exhibitions and permanent collection exhibitions provide educational opportunities for all ages, from learning the history of a quilt hand-stitched over 100 years ago to discovering how scientists analyze the physical properties of artists' materials, such as glass or stone. Lectures, receptions, conferences, university classes, panel discussions, gallery walks, and gallery talks are presented to assist with further interpretation of objects.

Located near the center of the Iowa State campus, the Farm House Museum sits as a monument to early Iowa State history and culture as well as a National Historic Landmark. As the first building on campus, the Farm House was built in 1860 before campus was occupied by students or even classrooms. The college's first farm tenants primed the land for agricultural experimentation. This early practice lead to Iowa State Agricultural College and Model Farm opening its doors to Iowa students for free in 1869 under the Morrill Act (or Land-grant Act) of 1862.

Many prominent figures have made the Farm House their home throughout its 150 years of use. The first president of the College, Adonijah Welch, briefly stayed at the Farm House and even wrote his inaugural speech in a bedroom on the second floor. James “Tama Jim” Wilson resided for much of the 1890s with his family at the Farm House until he joined President William McKinley's cabinet as U.S. Secretary of Agriculture. Agriculture Dean Charles Curtiss and his young family replaced Wilson and became the longest resident of Farm House.

In 1976, over 110 years after the initial construction, the Farm House became a museum after much time and effort was put into restoring the early beauty of the modest farm home. Today, faculty, students, and community members can enjoy the museum while honoring its significance in shaping a nationally recognized land-grant university. Its collection boasts a large collection of 19th and early 20th century decorative arts, furnishings and material culture reflecting Iowa State and Iowa heritage. Objects include furnishings from Carrie Chapman Catt and Charles Curtiss, a wide variety of quilts, a modest collection of textiles and apparel, and various china and glassware items.

As with many sites on the Iowa State University Campus, The Farm House Museum has a few old myths and legends associated with it. There are rumors of a ghost changing silverware and dinnerware, unexplained rattling furniture, and curtains that have opened seemingly by themselves.

The Farm House Museum is a unique on-campus educational resource providing a changing environment of exhibitions among the historical permanent collection objects that are on display. A walk through the Farm House Museum immerses visitors in the Victorian era (1860-1910) as well as exhibits colorful Iowa and local Ames history.

Iowa State is home to one of the largest campus public art programs in the United States. Over 2,000 works of public art, including 600 by significant national and international artists, are located across campus in buildings, courtyards, open spaces and offices.

The traditional public art program began during the Depression in the 1930s when Iowa State College's President Raymond Hughes envisioned that "the arts would enrich and provide substantial intellectual exploration into our college curricula." Hughes invited Grant Wood to create the Library's agricultural murals that speak to the founding of Iowa and Iowa State College and Model Farm. He also offered Christian Petersen a one-semester sculptor residency to design and build the fountain and bas relief at the Dairy Industry Building. In 1955, 21 years later, Petersen retired having created 12 major sculptures for the campus and hundreds of small studio sculptures.

The Art on Campus Collection is a campus-wide resource of over 2000 public works of art. Programs, receptions, dedications, university classes, Wednesday Walks, and educational tours are presented on a regular basis to enhance visual literacy and aesthetic appreciation of this diverse collection.

The Christian Petersen Art Museum in Morrill Hall is named for the nation's first permanent campus artist-in-residence, Christian Petersen, who sculpted and taught at Iowa State from 1934 through 1955, and is considered the founding artist of the Art on Campus Collection.

Named for Justin Smith Morrill who created the Morrill Land-Grant Colleges Act, Morrill Hall was completed in 1891. Originally constructed to fill the capacity of a library, museum, and chapel, its original uses are engraved in the exterior stonework on the east side. The building was vacated in 1996 when it was determined unsafe and was also listed in the National Register of Historic Places the same year. In 2005, $9 million was raised to renovate the building and convert it into a museum. Completed and reopened in March 2007, Morrill Hall is home to the Christian Petersen Art Museum.

As part of University Museums, the Christian Petersen Art Museum at Morrill Hall is the home of the Christian Petersen Art Collection, the Art on Campus Program, the University Museums's Visual Literacy and Learning Program, and Contemporary Changing Art Exhibitions Program.

Located within the Christian Petersen Art Museum are the Lyle and Nancy Campbell Art Gallery, the Roy and Bobbi Reiman Public Art Studio Gallery, the Margaret Davidson Center for the Study of the Art on Campus Collection, the Edith D. and Torsten E. Lagerstrom Loaned Collections Center, and the Neva M. Petersen Visual Learning Gallery. University Museums shares the James R. and Barbara R. Palmer Small Objects Classroom in Morrill Hall.

The Elizabeth and Byron Anderson Sculpture Garden is located by the Christian Petersen Art Museum at historic Morrill Hall. The sculpture garden design incorporates sculptures, a gathering arena, and sidewalks and pathways. Planted with perennials, ground cover, shrubs, and flowering trees, the landscape design provides a distinctive setting for important works of 20th and 21st century sculpture, primarily American. Ranging from forty-four inches to nearly nine feet high and from bronze to other metals, these works of art represent the richly diverse character of modern and contemporary sculpture.

The sculpture garden is adjacent to Iowa State's central campus. Adonijah Welch, ISU's first president, envisioned a picturesque campus with a winding road encircling the college's majestic buildings, vast lawns of green grass, many varieties of trees sprinkled throughout to provide shade, and shrubbery and flowers for fragrance. Today, the central lawn continues to be an iconic place for all Iowa Staters, and enjoys national acclaim as one of the most beautiful campuses in the country. The new Elizabeth and Byron Anderson Sculpture Garden further enhances the beauty of Iowa State.

Iowa State's composting facility is capable of processing over 10,000 tons of organic waste every year. The school's $3 million revolving loan fund loans money for energy efficiency and conservation projects on campus. In the 2011 College Sustainability Report Card issued by the Sustainable Endowments Institute, the university received a B grade.

Iowa State operates 20 on-campus residence halls. The residence halls are divided into geographical areas.

The Union Drive Association
(UDA) consists of four residence halls located on the west side of campus, including Friley Hall, which has been declared one of the largest residence halls in the country.

The Richardson Court Association (RCA) consists of 12 residence halls on the east side of campus.

The Towers Residence Association (TRA) are located south of the main campus. Two of the four towers, Knapp and Storms Halls, were imploded in 2005; however, Wallace and Wilson Halls still stand.

Buchanan Hall and Geoffroy Hall are nominally considered part of the RCA, despite their distance from the other buildings.

ISU operates two apartment complexes for upperclassmen, Frederiksen Court and SUV Apartments.

The governing body for ISU students is ISU Student Government. The ISU Student Government is composed of a president, vice president, finance director, cabinet appointed by the president, a clerk appointed by the vice president, senators representing each college and residence area at the university, a nine-member judicial branch and an election commission.

ISU has over 800 student organizations on campus that represent a variety of interests. Organizations are supported by Iowa State's Student Activities Center. Many student organization offices are housed in the Memorial Union.

The Memorial Union at Iowa State University opened in September 1928 and is currently home to a number of University departments and student organizations, a bowling alley, the University Book Store, and the Hotel Memorial Union.

The original building was designed by architect, William T. Proudfoot. The building employs a classical style of architecture reflecting Greek and Roman influences. The building's design specifically complements the designs of the major buildings surrounding the University's Central Campus area, Beardshear Hall to the west, Curtiss Hall to the east, and MacKay Hall to the north. The style utilizes columns with Corinthian capitals, Palladian windows, triangular pediments, and formally balanced facades.

Designed to be a living memorial for ISU students lost in World War I, the building includes a solemn memorial hall, named the Gold Star Room, which honors the names of the dead World War I, World War II, Korean, Vietnam, and War on Terrorism veterans engraved in marble. Symbolically, the hall was built directly over a library (the Browsing Library) and a small chapel, the symbol being that no country would ever send its young men to die in a war for a noble cause without a solid foundation on both education (the library) and religion (the chapel).

Renovations and additions have continued through the years to include: elevators, bowling lanes, a parking ramp, a book store, food court, and additional wings.

The Choral Division of the Department of Music and Theater at Iowa State University consists of over 400 choristers in four main ensembles – the "Iowa State Singers", "Cantamus," the "Iowa Statesmen", and "Lyrica" – and multiple small ensembles including three a cappella groups, "Count Me In" (female), "Shy of a Dozen" (male), and "Hymn and Her" (co-ed).

ISU is home to an active Greek community. There are 50 chapters that involve 14.6 percent of undergraduate students. Collectively, fraternity and sorority members have raised over $82,000 for philanthropies and committed 31,416 hours to community service. In 2006, the ISU Greek community was named the best large Greek community in the Midwest.

The ISU Greek Community has received multiple Jellison and Sutherland Awards from Association for Fraternal Leadership and Values, formerly the Mid-American Greek Council Association. These awards recognize the top Greek Communities in the Midwest.

The first fraternity, Delta Tau Delta, was established at Iowa State in 1875, six years after the first graduating class entered Iowa State. The first sorority, I.C. Sorocis, was established only two years later, in 1877. I.C. Sorocis later became a chapter of the first national sorority at Iowa State, Pi Beta Phi. Anti-Greek rioting occurred in 1888. As reported in "The Des Moines Register", "The anti-secret society men of the college met in a mob last night about 11 o'clock in front of the society rooms in chemical and physical hall, determined to break up a joint meeting of three secret societies." In 1891, President William Beardshear banned students from joining secret college fraternities, resulting in the eventual closing of all formerly established fraternities. President Storms lifted the ban in 1904.

Following the lifting of the fraternity ban, the first thirteen national fraternities (IFC) installed on the Iowa State campus between 1904 and 1913 were, in order, Sigma Nu, Sigma Alpha Epsilon, Beta Theta Pi, Phi Gamma Delta, Alpha Tau Omega, Kappa Sigma, Theta Xi, Acacia, Phi Sigma Kappa, Delta Tau Delta, Pi Kappa Alpha, and Phi Delta Theta. Though some have suspended their chapters at various times, eleven of the original thirteen fraternities were active in 2008. Many of these chapters existed on campus as local fraternities before being reorganized as national fraternities, prior to 1904.

In the Spring of 2014, it was announced that Alpha Phi sorority would be coming to Iowa state in the Fall of 2014, with Delta Gamma sorority Following in the near future.

The "Iowa State Daily" is the university's student newspaper. The "Daily" has its roots from a news sheet titled the "Clipper", which was started in the spring of 1890 by a group of students at Iowa Agricultural College led by F.E. Davidson. The "Clipper" soon led to the creation of the "Iowa Agricultural College Student", and the beginnings of what would one day become the "Iowa State Daily". It was awarded the 2016 Best All-Around Daily Student Newspaper by the Society of Professional Journalists.

88.5 KURE is the university's student-run radio station. Programming for KURE includes ISU sports coverage, talk shows, the annual quiz contest Kaleidoquiz, and various music genres.

ISUtv is the university's student-run television station. It is housed in the former WOI-TV station that was established in 1950. The student organization of ISUtv has many programs including Newswatch, a twice weekly news spot, Cyclone InCyders, the campus sports show, Fortnightly News, a satirical/comedy program, and Cy's Eyes on the Skies, a twice weekly weather show.

The "Cyclones" name dates back to 1895. That year, Iowa suffered an unusually high number of devastating cyclones (as tornadoes were called at the time). In September, Iowa Agricultural College's football team traveled to Northwestern University and defeated that team by a score of 36–0. The next day, the "Chicago Tribune"'s headline read "Struck by a Cyclone: It Comes from Iowa and Devastates Evanston Town." The article began, "Northwestern might as well have tried to play football with an Iowa cyclone as with the Iowa team it met yesterday." The nickname stuck.

The school colors are cardinal and gold. The mascot is Cy the Cardinal, introduced in 1954. Since a cyclone was determined to be difficult to depict in costume, the cardinal was chosen in reference to the school colors. A contest was held to select a name for the mascot, with the name Cy being chosen as the winner.

The Iowa State Cyclones are a member of the Big 12 Conference and compete in NCAA Division I Football Bowl Subdivision (FBS), fielding 16 varsity teams in 12 sports. The Cyclones also compete in and are a founding member of the Central States Collegiate Hockey League of the American Collegiate Hockey Association.

Iowa State's intrastate archrival is the University of Iowa with whom it competes annually for the Iowa Corn Cy-Hawk Series trophy, an annual athletic competition between the two schools. Sponsored by the Iowa Corn Growers Association, the competition includes all head-to-head regular season competitions between the two rival universities in all sports.

Football first made its way onto the Iowa State campus in 1878 as a recreational sport, but it was not until 1892 that Iowa State organized its first team to represent the school in football. In 1894, college president William M. Beardshear spearheaded the foundation of an athletic association to officially sanction Iowa State football teams. The 1894 team finished with a 6-1 mark. The Cyclones compete each year for traveling trophies. Since 1977, Iowa State and Iowa compete annually for the Cy-Hawk Trophy. Iowa State competes in an annual rivalry game against Kansas State known as Farmageddon and against former conference foe Missouri for the Telephone Trophy. The Cyclones also compete against the Iowa Hawkeyes, their in-state rival.
The Cyclones play their home games at Jack Trice Stadium, named after Jack Trice, ISU's first African-American athlete and also the first and only Iowa State athlete to die from injuries sustained during athletic competition. Trice died three days after his first game playing for Iowa State against Minnesota in Minneapolis on October 6, 1923. Suffering from a broken collarbone early in the game, he continued to play until he was trampled by a group of Minnesota players. It is disputed whether he was trampled purposely or if it was by accident. The stadium was named in his honor in 1997 and is the only NCAA Division I-A stadium named after an African-American. Jack Trice Stadium, formerly known as Cyclone Stadium, opened on September 20, 1975, with a win against the United States Air Force Academy.

Hopes of "Hilton Magic" returning took a boost with the hiring of ISU alum, Ames native, and fan favorite Fred Hoiberg as coach of the men's basketball team in April 2010. Hoiberg ("The Mayor") played three seasons under legendary coach Johnny Orr and one season under future Chicago Bulls coach Tim Floyd during his standout collegiate career as a Cyclone (1991–95). Orr laid the foundation of success in men's basketball upon his arrival from Michigan in 1980 and is credited with building Hilton Magic. Besides Hoiberg, other Cyclone greats played for Orr and brought winning seasons, including Jeff Grayer, Barry Stevens, and walk-on Jeff Hornacek. The 1985-86 Cyclones were one of the most memorable. Orr coached the team to second place in the Big Eight and produced one of his greatest career wins, a victory over his former team and No. 2 seed Michigan in the second round of the NCAA tournament.

Under coaches Floyd (1995–98) and Larry Eustachy (1998–2003), Iowa State achieved even greater success. Floyd took the Cyclones to the Sweet Sixteen in 1997 and Eustachy led ISU to two consecutive Big 12 regular season conference titles in 1999-2000 and 2000–01, plus the conference tournament title in 2000. Seeded No. 2 in the 2000 NCAA tournament, Eustachy and the Cyclones defeated UCLA in the Sweet Sixteen before falling to Michigan State, the eventual NCAA Champion, in the regional finals by a score of 75-64 (the differential representing the Spartans' narrowest margin of victory in the tournament). Standout Marcus Fizer and Jamaal Tinsley were scoring leaders for the Cyclones who finished the season 32–5. Tinsley returned to lead the Cyclones the following year with another conference title and No. 2 seed, but ISU finished the season with a 25-6 overall record after a stunning loss to No. 15 seed Hampton in the first round.

In 2011–12, Hoiberg's Cyclones finished third in the Big 12 and returned to the NCAA Tournament, dethroning defending national champion Connecticut, 77–64, in the second round before losing in the Round of 32 to top-seeded Kentucky. All-Big 12 First Team selection Royce White led the Cyclones with 38 points and 22 rebounds in the two contests, ending the season at 23–11.

The 2013-14 campaign turned out to be another highly successful season. Iowa State went 28–8, won the Big 12 Tournament, and advanced to the Sweet Sixteen by beating North Carolina in the second round of the NCAA Tournament. The Cyclones finished 11–7 in Big 12 play, finishing in a tie for third in the league standings, and beat a school-record nine teams (9-3) that were ranked in the Associated Press top 25. The Cyclones opened the season 14–0, breaking the school record for consecutive wins. Melvin Ejim was named the Big 12 Player of the Year and an All-American by five organizations. Deandre Kane was named the Big 12 Tournament's most valuable player.

On June 8, 2015, Steve Prohm took over as head basketball coach replacing Hoiberg who left to take the head coaching position with the Chicago Bulls. In his first season with the Cyclones, Prohm secured a #4 seed in the Midwest region where the Cyclones advanced to the Sweet Sixteen before falling to top-seeded Virginia, 84–71. In 2017, Iowa State stunned 3rd ranked Kansas, 92–89, in overtime, snapping KU's 54-game home winning streak, before winning the 2017 Big 12 Men's Basketball Tournament, its third conference championship in four years, defeating West Virginia in the final.

Of Iowa State's 19 NCAA Tournament appearances, the Cyclones have reached the Sweet Sixteen six times (1944, 1986, 1997, 2000, 2014, 2016), made two appearances in the Elite Eight (1944, 2000), and reached the Final Four once in 1944.

Iowa State is known for having one of the most successful women's basketball programs in the nation. Since the founding of the Big 12, Coach Bill Fennelly and the Cyclones have won three conference titles (one regular season, two tournament), and have advanced to the Sweet Sixteen five times (1999–2001, 2009, 2010) and the Elite Eight twice (1999, 2009) in the NCAA Tournament. The team has one of the largest fan bases in the nation with attendance figures ranked third in the nation in 2009, 2010, and 2012.

Coach Christy Johnson-Lynch led the 2012 Cyclones team to a fifth straight 20-win season and fifth NCAA regional semifinal appearance in six seasons, and leading Iowa State to a 22-8 (13-3 Big 12) overall record and second-place finish in the conference. The Cyclones finished the season with seven wins over top-25 teams, including a victory over No. 1 Nebraska Cornhuskers in Iowa State's first-ever win over a top-ranked opponent in addition to providing the only Big 12 Conference loss to the 2012 conference and NCAA champion Texas Longhorns.

In 2011, Iowa State finished the season 25-6 (13-3 Big 12), placing second in the league, as well as a final national ranking of eighth. 2011 is only the second season in which an Iowa State volleyball team has ever recorded 25 wins. The Cyclones beat No. 9 Florida during the season in Gainesville, its sixth win over a top-10 team in Cyclone history. In 2009, Iowa State finished the season second in the Big 12 behind Texas with a 27–5 record and ranked No. 6, its highest ever national finish.

Johnson-Lynch is the fastest Iowa State coach to clinch 100 victories. In 2011, she became the school's winningest volleyball coach when her team defeated the Texas Tech Red Raiders, her 136th coaching victory, in straight sets.

The ISU wrestling program has captured the NCAA wrestling tournament title eight times between 1928 and 1987, and won the Big 12 Conference Tournament three consecutive years, 2007–2009. On February 7, 2010, the Cyclones became the first collegiate wrestling program to record its 1,000th dual win in program history by defeating the Arizona State Sun Devils, 30–10, in Tempe, Arizona.

In 2002, under former NCAA champion & Olympian Coach Bobby Douglas, Iowa State became the first school to produce a four-time, undefeated NCAA Division I champion, Cael Sanderson (considered by the majority of the wrestling community to be the best college wrestler ever), who also took the gold medal at the 2004 Olympic Games in Athens, Greece. Dan Gable, another legendary ISU wrestler, is famous for having lost only one match in his entire Iowa State collegiate career - his last - and winning gold at the 1972 Olympics in Munich, Germany, while not giving up a single point.

In 2013, Iowa State hosted its eighth NCAA Wrestling Championships. The Cyclones hosted the first NCAA championships in 1928.

In February 2017, former Virginia Tech coach and 2016 NWCA Coach of the Year Kevin Dresser was introduced as the new Cyclone wrestling coach, replacing Kevin Jackson.




</doc>
<doc id="14877" url="https://en.wikipedia.org/wiki?curid=14877" title="Induction">
Induction

Induction may refer to:








</doc>
<doc id="14878" url="https://en.wikipedia.org/wiki?curid=14878" title="International Astronomical Union">
International Astronomical Union

The International Astronomical Union (IAU; , UAI) is an international association of professional astronomers, at the PhD level and beyond, active in professional research and education in astronomy. Among other activities, it acts as the recognized authority for assigning designations and names to celestial bodies (stars, planets, asteroids, etc.) and any surface features on them.

The IAU is a member of the International Science Council (ISC). Its main objective is to promote and safeguard the science of astronomy in all its aspects through international cooperation. The IAU maintains friendly relations with organizations that include amateur astronomers in their membership. The IAU has its head office on the second floor of the "Institut d'Astrophysique de Paris" in the 14th arrondissement of Paris. 

This organisation has many working groups. For example, the Working Group for Planetary System Nomenclature (WGPSN), which maintains the astronomical naming conventions and planetary nomenclature for planetary bodies, and the Working Group on Star Names (WGSN), which catalogues and standardizes proper names for stars. The IAU is also responsible for the system of astronomical telegrams which are produced and distributed on its behalf by the Central Bureau for Astronomical Telegrams. The Minor Planet Center also operates under the IAU, and is a "clearinghouse" for all non-planetary or non-moon bodies in the Solar System.

The IAU was founded on 28 July 1919, at the Constitutive Assembly of the International Research Council (now the International Science Council) held in Brussels, Belgium. Two subsidiaries of the IAU were also created at this assembly: the "International Time Commission" seated at the International Time Bureau in Paris, France, and the "International Central Bureau of Astronomical Telegrams" initially seated in Copenhagen, Denmark. The 7 initial member states were Belgium, Canada, France, Great Britain, Greece, Japan, and the United States, soon to be followed by Italy and Mexico. The first executive committee consisted of Benjamin Baillaud (President, France), Alfred Fowler (General Secretary, UK), and four vice presidents: William Campbell (USA), Frank Dyson (UK), Georges Lecointe (Belgium), and Annibale Riccò (Italy). Thirty-two Commissions (referred to initially as Standing Committees) were appointed at the Brussels meeting and focused on topics ranging from relativity to minor planets. The reports of these 32 Commissions formed the main substance of the first General Assembly, which took place in Rome, Italy, 2–10 May 1922. By the end of the first General Assembly, ten additional nations (Australia, Brazil, Czecho-Slovakia, Denmark, the Netherlands, Norway, Poland, Romania, South Africa, and Spain) had joined the Union, bringing the total membership to 19 countries. Although the Union was officially formed eight months after the end of World War I, international collaboration in astronomy had been strong in the pre-war era (e.g., the Astronomische Gesellschaft Katalog projects since 1868, the Astrographic Catalogue since 1887, and the International Union for Solar research since 1904).

The first 50 years of the Union's history are well documented. Subsequent history is recorded in the form of reminiscences of past IAU Presidents and General Secretaries. Twelve of the fourteen past General Secretaries in the period 1964-2006 contributed their recollections of the Union's history in IAU Information Bulletin No. 100. Six past IAU Presidents in the period 1976–2003 also contributed their recollections in IAU Information Bulletin No. 104.

As of 1 August 2019, the IAU includes a total of 13,701 "individual members", who are professional astronomers from 102 countries worldwide. 81.7% of all individual members are male, while 18.3% are female, among them the union's former president, Mexican astronomer Silvia Torres-Peimbert.

Membership also includes 82 "national members", professional astronomical communities representing their country's affiliation with the IAU. National members include the Australian Academy of Science, the Chinese Astronomical Society, the French Academy of Sciences, the Indian National Science Academy, the National Academies (United States), the National Research Foundation of South Africa, the National Scientific and Technical Research Council (Argentina), KACST (Saudi Arabia), the Council of German Observatories, the Royal Astronomical Society (United Kingdom), the Royal Astronomical Society of New Zealand, the Royal Swedish Academy of Sciences, the Russian Academy of Sciences, and the Science Council of Japan, among many others.

The sovereign body of the IAU is its "General Assembly", which comprises all members. The Assembly determines IAU policy, approves the Statutes and By-Laws of the Union (and amendments proposed thereto) and elects various committees.

The right to vote on matters brought before the Assembly varies according to the type of business under discussion. The Statutes consider such business to be divided into two categories:


On budget matters (which fall into the second category), votes are weighted according to the relative subscription levels of the national members. A second category vote requires a turnout of at least two-thirds of national members to be valid. An absolute majority is sufficient for approval in any vote, except for Statute revision which requires a two-thirds majority. An equality of votes is resolved by the vote of the President of the Union.








Since 1922, the IAU General Assembly meets every three years, except for the period between 1938 and 1948, due to World War II.
After a Polish request in 1967, and by a controversial decision of the then President of the IAU, an "Extraordinary IAU General Assembly" was held in September 1973 in Warsaw, Poland, to commemorate the 500th anniversary of the birth of Nicolaus Copernicus, soon after the regular 1973 GA had been held in Sydney, Australia.

Sources.
Commission 46 is a Committee of the Executive Committee of the IAU, playing a special role in the discussion of astronomy development with governments and scientific academies. The IAU is affiliated with the International Council of Scientific Unions (ICSU), a non-governmental organization representing a global membership that includes both national scientific bodies and international scientific unions. They often encourage countries to become members of the IAU. The Commission further seeks to development, information or improvement of astronomical education. Part of Commission 46, is Teaching Astronomy for Development (TAD) program in countries where there is currently very little astronomical education. Another program is named the Galileo Teacher Training Program (GTTP), is a project of the International Year of Astronomy 2009, among which Hands-On Universe that will concentrate more resources on education activities for children and schools designed to advance sustainable global development. GTTP is also concerned with the effective use and transfer of astronomy education tools and resources into classroom science curricula. A strategic plan for the period 2010-2020 has been published.

In 2004 the IAU contracted with the Cambridge University Press to publish the "Proceedings of the International Astronomical Union".

In 2007, the Communicating Astronomy with the Public Journal Working Group prepared a study assessing the feasibility of the "Communicating Astronomy with the Public Journal" ("CAP Journal").




</doc>
<doc id="14879" url="https://en.wikipedia.org/wiki?curid=14879" title="Interval">
Interval

Interval may refer to:







</doc>
<doc id="14880" url="https://en.wikipedia.org/wiki?curid=14880" title="International Criminal Court">
International Criminal Court

The International Criminal Court (ICC or ICCt) is an intergovernmental organization and international tribunal that sits in The Hague, Netherlands. The ICC is the first and only permanent international court with jurisdiction to prosecute individuals for the international crimes of genocide, crimes against humanity, war crimes, and the crime of aggression. It is intended to complement existing national judicial systems and it may therefore exercise its jurisdiction only when national courts are unwilling or unable to prosecute criminals. The ICC lacks universal territorial jurisdiction, and may only investigate and prosecute crimes committed within member states, crimes committed by nationals of member states, or crimes in situations referred to the Court by the United Nations Security Council.

The ICC began operations on 1 July 2002, upon the entry into force of the Rome Statute, a multilateral treaty that serves as the court's foundational and governing document. States which become party to the Rome Statute become members of the ICC, serving on the Assembly of States Parties, which administers the court. As of November 2019, there are 123 ICC member states; 42 states have neither signed nor become parties to the Rome Statute.

The ICC has four principal organs: the Presidency, the Judicial Divisions, the Office of the Prosecutor, and the Registry. The President is the most senior judge chosen by his or her peers in the Judicial Division, which hears cases before the Court. The Office of the Prosecutor is headed by the Prosecutor who investigates crimes and initiates criminal proceedings before the Judicial Division. The Registry is headed by the Registrar and is charged with managing all the administrative functions of the ICC, including the headquarters, detention unit, and public defense office.

The Office of the Prosecutor has opened 12 official investigations and is also conducting an additional nine preliminary examinations. Thus far, 45 individuals have been indicted in the ICC, including Ugandan rebel leader Joseph Kony, former Sudanese president Omar al-Bashir, Kenyan president Uhuru Kenyatta, Libyan leader Muammar Gaddafi, Ivorian president Laurent Gbagbo, and DR Congo vice-president Jean-Pierre Bemba.

The ICC has faced a number of criticisms from states and civil society, including objections about its jurisdiction, accusations of bias, questioning of the fairness of its case-selection and trial procedures, and doubts about its effectiveness.

The establishment of an international tribunal to judge political leaders accused of international crimes was first proposed during the Paris Peace Conference in 1919 following the First World War by the Commission of Responsibilities. The issue was addressed again at a conference held in Geneva under the auspices of the League of Nations in 1937, which resulted in the conclusion of the first convention stipulating the establishment of a permanent international court to try acts of international terrorism. The convention was signed by 13 states, but none ratified it and the convention never entered into force.

Following the Second World War, the allied powers established two "ad hoc" tribunals to prosecute Axis leaders accused of war crimes. The International Military Tribunal, which sat in Nuremberg, prosecuted German leaders while the International Military Tribunal for the Far East in Tokyo prosecuted Japanese leaders. In 1948 the United Nations General Assembly first recognised the need for a permanent international court to deal with atrocities of the kind prosecuted after the Second World War. At the request of the General Assembly, the International Law Commission (ILC) drafted two statutes by the early 1950s but these were shelved during the Cold War, which made the establishment of an international criminal court politically unrealistic.

Benjamin B. Ferencz, an investigator of Nazi war crimes after the Second World War, and the Chief Prosecutor for the United States Army at the Einsatzgruppen Trial, became a vocal advocate of the establishment of international rule of law and of an international criminal court. In his first book published in 1975, entitled "Defining International Aggression: The Search for World Peace", he advocated for the establishment of such a court. A second major advocate was Robert Kurt Woetzel, who co-edited "Toward a Feasible International Criminal Court" in 1970 and created the Foundation for the Establishment of an International Criminal Court in 1971.

In June 1989 Prime Minister of Trinidad and Tobago, A. N. R. Robinson revived the idea of a permanent international criminal court by proposing the creation of such a court to deal with the illegal drug trade. Following Trinidad and Tobago's proposal, the General Assembly tasked the ILC with once again drafting a statute for a permanent court. While work began on the draft, the United Nations Security Council established two "ad hoc" tribunals in the early 1990s: The International Criminal Tribunal for the former Yugoslavia, created in 1993 in response to large-scale atrocities committed by armed forces during Yugoslav Wars, and the International Criminal Tribunal for Rwanda, created in 1994 following the Rwandan genocide. The creation of these tribunals further highlighted to many the need for a permanent international criminal court.

In 1994, the ILC presented its final draft statute for the International Criminal Court to the General Assembly and recommended that a conference be convened to negotiate a treaty that would serve as the Court's statute. To consider major substantive issues in the draft statute, the General Assembly established the Ad Hoc Committee on the Establishment of an International Criminal Court, which met twice in 1995. After considering the Committee's report, the General Assembly created the Preparatory Committee on the Establishment of the ICC to prepare a consolidated draft text. From 1996 to 1998, six sessions of the Preparatory Committee were held at the United Nations headquarters in New York City, during which NGOs provided input and attended meetings under the umbrella organisation of the Coalition for the International Criminal Court (CICC). In January 1998, the Bureau and coordinators of the Preparatory Committee convened for an Inter-Sessional meeting in Zutphen in the Netherlands to technically consolidate and restructure the draft articles into a draft.

Finally the General Assembly convened a conference in Rome in June 1998, with the aim of finalizing the treaty to serve as the Court's statute. On 17 July 1998, the Rome Statute of the International Criminal Court was adopted by a vote of 120 to seven, with 21 countries abstaining. The seven countries that voted against the treaty were China, Iraq, Israel, Libya, Qatar, the United States, and Yemen. Israel's opposition to the treaty stemmed from the inclusion in the list of war crimes "the action of transferring population into occupied territory".

Following 60 ratifications, the Rome Statute entered into force on 1 July 2002 and the International Criminal Court was formally established. The first bench of 18 judges was elected by the Assembly of States Parties in February 2003. They were sworn in at the inaugural session of the Court on 11 March 2003. The Court issued its first arrest warrants on 8 July 2005, and the first pre-trial hearings were held in 2006. The Court issued its first judgment in 2012 when it found Congolese rebel leader Thomas Lubanga Dyilo guilty of war crimes related to using child soldiers.

In 2010 the states parties of the Rome Statute held the first Review Conference of the Rome Statute of the International Criminal Court in Kampala, Uganda. The Review Conference led to the adoption of two resolutions that amended the crimes under the jurisdiction of the Court. Resolution 5 amended Article 8 on war crimes, criminalizing the use of certain kinds of weapons in non-international conflicts whose use was already forbidden in international conflicts. Resolution 6, pursuant to Article 5(2) of the Statute, provided the definition and a procedure for jurisdiction over the crime of aggression.

During the administration of Barack Obama, US opposition to the ICC evolved to "positive engagement," although no effort was made to ratify the Rome Statute. The current administration of Donald Trump is considerably more hostile to the Court, threatening prosecutions and financial sanctions on ICC judges and staff in US courts as well as imposing visa bans in response to any investigation against American nationals in connection to alleged crimes and atrocities perpetrated by the US in Afghanistan. The threat included sanctions against any of over 120 countries which have ratified the Court for cooperating in the process. Following the imposition of sanctions on 11 June 2020 by the Trump administration, the court branded the sanctions an "attack against the interests of victims of atrocity crimes" and an "unacceptable attempt to interfere with the rule of law". The UN also regretted the effect sanctions may have on trials and investigations under way, saying its independence must be protected.

In October 2016, after repeated claims that the court was biased against African states, Burundi, South Africa and the Gambia announced their withdrawals from the Rome Statute. However, following Gambia's presidential election later that year, which ended the long rule of Yahya Jammeh, Gambia rescinded its withdrawal notification. A decision by the High Court of South Africa in early 2017 ruled that withdrawal would be unconstitutional, prompting the South African government to inform the UN that it was revoking its decision to withdraw.

In November 2017, Fatou Bensouda advised the court to consider seeking charges for human rights abuses committed during the War in Afghanistan such as alleged rapes and tortures by the United States Armed Forces and the Central Intelligence Agency, crime against humanity committed by the Taliban, and war crimes committed by the Afghan National Security Forces. John Bolton, National Security Advisor of the United States, stated that ICC Court had no jurisdiction over the US, which did not ratify the Rome Statute. In 2020, overturning the previous decision not to proceed, senior judges at the ICC authorized an investigation into the alleged war crimes in Afghanistan. However, in June 2020, the decision to proceed led Trump administration to power an economic and legal attack on the court. “The US government has reason to doubt the honesty of the ICC. The Department of Justice has received substantial credible information that raises serious concerns about a long history of financial corruption and malfeasance at the highest levels of the office of the prosecutor,” Attorney General William Barr said. The ICC responded with a statement expressing “profound regret at the announcement of further threats and coercive actions.""These attacks constitute an escalation and an unacceptable attempt to interfere with the rule of law and the Court’s judicial proceedings, the statement said. "They are announced with the declared aim of influencing the actions of ICC officials in the context of the court’s independent and objective investigations and impartial judicial proceedings."

Following the announcement that the ICC would open a preliminary investigation on the Philippines in connection to its escalating drug war, President Rodrigo Duterte announced on 14 March 2018 that the Philippines would start to submit plans to withdraw, completing the process on 17 March 2019. The ICC pointed out that it retained jurisdiction over the Philippines during the period when it was a state party to the Rome Statute, from November 2011 to March 2019.

The ICC is governed by the Assembly of States Parties, which is made up of the states that are party to the Rome Statute. The Assembly elects officials of the Court, approves its budget, and adopts amendments to the Rome Statute. The Court itself, however, is composed of four organs: the Presidency, the Judicial Divisions, the Office of the Prosecutor, and the Registry.

The Court's management oversight and legislative body, the Assembly of States Parties, consists of one representative from each state party. Each state party has one vote and "every effort" has to be made to reach decisions by consensus. If consensus cannot be reached, decisions are made by vote. The Assembly is presided over by a president and two vice-presidents, who are elected by the members to three-year terms.

The Assembly meets in full session once a year, alternating between New York and The Hague, and may also hold special sessions where circumstances require. Sessions are open to observer states and non-governmental organizations.

The Assembly elects the judges and prosecutors, decides the Court's budget, adopts important texts (such as the Rules of Procedure and Evidence), and provides management oversight to the other organs of the Court. Article 46 of the Rome Statute allows the Assembly to remove from office a judge or prosecutor who "is found to have committed serious misconduct or a serious breach of his or her duties" or "is unable to exercise the functions required by this Statute".

The states parties cannot interfere with the judicial functions of the Court. Disputes concerning individual cases are settled by the Judicial Divisions.

In 2010, Kampala, Uganda hosted the Assembly's Rome Statute Review Conference.

The Court has four organs: the Presidency, the Judicial Division, the Office of the Prosecutor, and the Registry.

The Presidency is responsible for the proper administration of the Court (apart from the Office of the Prosecutor). It comprises the President and the First and Second Vice-Presidents—three judges of the Court who are elected to the Presidency by their fellow judges for a maximum of two three-year terms. The current president is Chile Eboe-Osuji, who was elected 11 March 2018, succeeding Silvia Fernández de Gurmendi (first female president).

The Judicial Divisions consist of the 18 judges of the Court, organized into three chambers—the Pre-Trial Chamber, Trial Chamber and Appeals Chamber—which carry out the judicial functions of the Court. Judges are elected to the Court by the Assembly of States Parties. They serve nine-year terms and are not generally eligible for re-election. All judges must be nationals of states parties to the Rome Statute, and no two judges may be nationals of the same state. They must be "persons of high moral character, impartiality and integrity who possess the qualifications required in their respective States for appointment to the highest judicial offices".

The Prosecutor or any person being investigated or prosecuted may request the disqualification of a judge from "any case in which his or her impartiality might reasonably be doubted on any ground". Any request for the disqualification of a judge from a particular case is decided by an absolute majority of the other judges. A judge may be removed from office if he or she "is found to have committed serious misconduct or a serious breach of his or her duties" or is unable to exercise his or her functions. The removal of a judge requires both a two-thirds majority of the other judges and a two-thirds majority of the states parties.

The Office of the Prosecutor (OTP) is responsible for conducting investigations and prosecutions. It is headed by the Chief Prosecutor, who is assisted by one or more Deputy Prosecutors. The Rome Statute provides that the Office of the Prosecutor shall act independently; as such, no member of the Office may seek or act on instructions from any external source, such as states, international organisations, non-governmental organisations or individuals.

The Prosecutor may open an investigation under three circumstances:

Any person being investigated or prosecuted may request the disqualification of a prosecutor from any case "in which their impartiality might reasonably be doubted on any ground". Requests for the disqualification of prosecutors are decided by the Appeals Chamber. A prosecutor may be removed from office by an absolute majority of the states parties if he or she "is found to have committed serious misconduct or a serious breach of his or her duties" or is unable to exercise his or her functions. However, critics of the Court argue that there are "insufficient checks and balances on the authority of the ICC prosecutor and judges" and "insufficient protection against politicized prosecutions or other abuses". Luis Moreno-Ocampo, chief ICC prosecutor, stressed in 2011 the importance of politics in prosecutions: "You cannot say al-Bashir is in London, arrest him. You need a political agreement." Henry Kissinger says the checks and balances are so weak that the prosecutor "has virtually unlimited discretion in practice".

As of 16 June 2012, the Prosecutor has been Fatou Bensouda of Gambia, who had been elected as the new Prosecutor on 12 December 2011. She has been elected for nine years. Her predecessor, Luis Moreno Ocampo of Argentina, had been in office from 2003 to 2012.

A Policy Paper is a document published by the Office of the Prosecutor occasionally where the particular considerations given to the topics in focus of the Office and often criteria for case selection are stated. While a policy paper does not give the Court jurisdiction over a new category of crimes, it promises what the Office of Prosecutor will consider when selecting cases in the upcoming term of service. OTP's policy papers are subject to revision.

On the Policy Paper published in September 2016 it was announced that the International Criminal Court will focus on environmental crimes when selecting the cases. According to this document, the Office will give particular consideration to prosecuting Rome Statute crimes that are committed by means of, or that result in, "inter alia, the destruction of the environment, the illegal exploitation of natural resources or the illegal dispossession of land".

This has been interpreted as a major shift towards the environmental crimes and a move with significant effects.

The Registry is responsible for the non-judicial aspects of the administration and servicing of the Court. This includes, among other things, "the administration of legal aid matters, court management, victims and witnesses matters, defence counsel, detention unit, and the traditional services provided by administrations in international organisations, such as finance, translation, building management, procurement and personnel". The Registry is headed by the Registrar, who is elected by the judges to a five-year term. The previous Registrar was Herman von Hebel, who was elected on 8 March 2013. The current Registrar is Peter Lewis, who was elected on 28 March 2018.

The Rome Statute requires that several criteria exist in a particular case before an individual can be prosecuted by the Court. The Statute contains three jurisdictional requirements and three admissibility requirements. All criteria must be met for a case to proceed. The three jurisdictional requirements are (1) subject-matter jurisdiction (what acts constitute crimes), (2) territorial or personal jurisdiction (where the crimes were committed or who committed them), and (3) temporal jurisdiction (when the crimes were committed).

The process to establish the Court's jurisdiction may be "triggered" by any one of three possible sources: (1) a State party, (2) the Security Council or (3) a Prosecutor. It is then up to the Prosecutor acting "ex proprio motu" ("of his own motion" so to speak) to initiate an investigation under the requirements of Article 15 of the Rome Statute. The procedure is slightly different when referred by a State Party or the Security Council, in which cases the Prosecutor does not need authorization of the Pre-Trial Chamber to initiate the investigation. Where there is a reasonable basis to proceed, it is mandatory for the Prosecutor to initiate an investigation. The factors listed in Article 53 considered for reasonable basis include whether the case would be admissible, and whether there are substantial reasons to believe that an investigation would not serve the interests of justice (the latter stipulates balancing against the gravity of the crime and the interests of the victims).

The Court's subject-matter jurisdiction means the crimes for which individuals can be prosecuted. Individuals can only be prosecuted for crimes that are listed in the Statute. The primary crimes are listed in article 5 of the Statute and defined in later articles: genocide (defined in article 6), crimes against humanity (defined in article 7), war crimes (defined in article 8), and crimes of aggression (defined in article 8 "bis") (which is not yet within the jurisdiction of the Court; see below). In addition, article 70 defines "offences against the administration of justice", which is a fifth category of crime for which individuals can be prosecuted.

Article 6 defines the crime of genocide as "acts committed with intent to destroy, in whole or in part, a national, ethnical, racial or religious group". There are five such acts which constitute crimes of genocide under article 6:
The definition of these crimes is identical to those contained within the Convention on the Prevention and Punishment of the Crime of Genocide of 1948.

In the Akayesu case the Court concluded that inciting directly and publicly others to commit génocide is in itself constitutive of a crime.

Article 7 defines crimes against humanity as acts "committed as part of a widespread or systematic attack directed against any civilian population, with knowledge of the attack". The article lists 16 such as individual crimes:


Article 8 defines war crimes depending on whether an armed conflict is either international (which generally means it is fought between states) or non-international (which generally means that it is fought between non-state actors, such as rebel groups, or between a state and such non-state actors). In total there are 74 war crimes listed in article 8. The most serious crimes, however, are those that constitute either grave breaches of the Geneva Conventions of 1949, which only apply to international conflicts, and serious violations of article 3 common to the Geneva Conventions of 1949, which apply to non-international conflicts.

There are 11 crimes which constitute grave breaches of the Geneva Conventions and which are applicable only to international armed conflicts:

There are seven crimes which constitute serious violations of article 3 common to the Geneva Conventions and which are applicable only to non-international armed conflicts:

Additionally, there are 56 other crimes defined by article 8: 35 that apply to international armed conflicts and 21 that apply to non-international armed conflicts. Such crimes include attacking civilians or civilian objects, attacking peacekeepers, causing excessive incidental death or damage, transferring populations into occupied territories, treacherously killing or wounding, denying quarter, pillaging, employing poison, using expanding bullets, rape and other forms of sexual violence, and conscripting or using child soldiers.

Article 8 "bis" defines crimes of aggression. The Statute originally provided that the Court could not exercise its jurisdiction over the crime of aggression until such time as the states parties agreed on a definition of the crime and set out the conditions under which it could be prosecuted. Such an amendment was adopted at the first review conference of the ICC in Kampala, Uganda, in June 2010. However, this amendment specified that the ICC would not be allowed to exercise jurisdiction of the crime of aggression until two further conditions had been satisfied: (1) the amendment has entered into force for 30 states parties and (2) on or after 1 January 2017, the Assembly of States Parties has voted in favor of allowing the Court to exercise jurisdiction. On 26 June 2016 the first condition was satisfied and the state parties voted in favor of allowing the Court to exercise jurisdiction on 14 December 2017. The Court's jurisdiction to prosecute crimes of aggression was accordingly activated on 17 July 2018.

The Statute, as amended, defines the crime of aggression as "the planning, preparation, initiation or execution, by a person in a position effectively to exercise control over or to direct the political or military action of a State, of an act of aggression which, by its character, gravity and scale, constitutes a manifest violation of the Charter of the United Nations." The Statute defines an "act of aggression" as "the use of armed force by a State against the sovereignty, territorial integrity or political independence of another State, or in any other manner inconsistent with the Charter of the United Nations." The article also contains a list of seven acts of aggression, which are identical to those in United Nations General Assembly Resolution 3314 of 1974 and include the following acts when committed by one state against another state:


Article 70 criminalizes certain intentional acts which interfere with investigations and proceedings before the Court, including giving false testimony, presenting false evidence, corruptly influencing a witness or official of the Court, retaliating against an official of the Court, and soliciting or accepting bribes as an official of the Court.

For an individual to be prosecuted by the Court either territorial jurisdiction or personal jurisdiction must exist. Therefore, an individual can only be prosecuted if he or she has either (1) committed a crime within the territorial jurisdiction of the Court or (2) committed a crime while being a national of a state that is within the territorial jurisdiction of the Court.

The territorial jurisdiction of the Court includes the territory, registered vessels, and registered aircraft of states which have either (1) become party to the Rome Statute or (2) accepted the Court's jurisdiction by filing a declaration with the Court.

In situations that are referred to the Court by the United Nations Security Council, the territorial jurisdiction is defined by the Security Council, which may be more expansive than the Court's normal territorial jurisdiction. For example, if the Security Council refers a situation that took place in the territory of a state that has both not become party to the Rome Statute and not lodged a declaration with the Court, the Court will still be able to prosecute crimes that occurred within that state.

The personal jurisdiction of the Court extends to all natural persons who commit crimes, regardless of where they are located or where the crimes were committed, as long as those individuals are nationals of either (1) states that are party to the Rome Statute or (2) states that have accepted the Court's jurisdiction by filing a declaration with the Court. As with territorial jurisdiction, the personal jurisdiction can be expanded by the Security Council if it refers a situation to the Court.

Temporal jurisdiction is the time period over which the Court can exercise its powers. No statute of limitations applies to any of the crimes defined in the Statute. However, the Court's jurisdiction is not completely retroactive. Individuals can only be prosecuted for crimes that took place on or after 1 July 2002, which is the date that the Rome Statute entered into force. If a state became party to the Statute, and therefore a member of the Court, after 1 July 2002, then the Court cannot exercise jurisdiction prior to the membership date for certain cases. For example, if the Statute entered into force for a state on 1 January 2003, the Court could only exercise temporal jurisdiction over crimes that took place in that state or were committed by a national of that state on or after 1 January 2003.

To initiate an investigation, the Prosecutor must (1) have a "reasonable basis to believe that a crime within the jurisdiction of the Court has been or is being committed", (2) the investigation would be consistent with the principle of complementarity, and (3) the investigation serves the interests of justice.

The principle of complementarity means that the Court will only prosecute an individual if states are unwilling or unable to prosecute. Therefore, if legitimate national investigations or proceedings into crimes have taken place or are ongoing, the Court will not initiate proceedings. This principle applies regardless of the outcome of national proceedings. Even if an investigation is closed without any criminal charges being filed or if an accused person is acquitted by a national court, the Court will not prosecute an individual for the crime in question so long as it is satisfied that the national proceedings were legitimate. However, the actual application of the complementarity principle has recently come under theoretical scrutiny.

The Court will only initiate proceedings if a crime is of "sufficient gravity to justify further action by the Court".

The Prosecutor will initiate an investigation unless there are "substantial reasons to believe that an investigation would not serve the interests of justice" when "[t]aking into account the gravity of the crime and the interests of victims". Furthermore, even if an investigation has been initiated and there are substantial facts to warrant a prosecution and no other admissibility issues, the Prosecutor must determine whether a prosecution would serve the interests of justice "taking into account all the circumstances, including the gravity of the crime, the interests of victims and the age or infirmity of the alleged perpetrator, and his or her role in the alleged crime".

The Court has jurisdiction over natural persons
A person who commits a crime within the jurisdiction of the Court is individually responsible and liable for punishment in accordance with the Rome Statute. In accordance with the Rome Statute, a person shall be criminally responsible and liable for punishment for a crime within the jurisdiction of the Court if that person: Commits such a crime, whether as an individual, jointly with another or through another person, regardless of whether that other person is criminally responsible; Orders, solicits or induces the commission of such a crime which in fact occurs or is attempted; For the purpose of facilitating the commission of such a crime, aids, abets or otherwise assists in its commission or its attempted commission, including providing the means for its commission; In any other way contributes to the commission or attempted commission of such a crime by a group of persons acting with a common purpose. In respect of the crime of genocide, directly and publicly incites others to commit genocide; Attempts to commit such a crime by taking action that commences its execution by means of a substantial step, but the crime does not occur because of circumstances independent of the person's intentions

Trials are conducted under a hybrid common law and civil law judicial system, but it has been argued the procedural orientation and character of the court is still evolving. A majority of the three judges present, as triers of fact, may reach a decision, which must include a full and reasoned statement. Trials are supposed to be public, but proceedings are often closed, and such exceptions to a public trial have not been enumerated in detail. "In camera" proceedings are allowed for protection of witnesses or defendants as well as for confidential or sensitive evidence. Hearsay and other indirect evidence is not generally prohibited, but it has been argued the court is guided by hearsay exceptions which are prominent in common law systems. There is no subpoena or other means to compel witnesses to come before the court, although the court has some power to compel testimony of those who chose to come before it, such as fines.

The Rome Statute provides that all persons are presumed innocent until proven guilty beyond reasonable doubt, and establishes certain rights of the accused and persons during investigations. These include the right to be fully informed of the charges against him or her; the right to have a lawyer appointed, free of charge; the right to a speedy trial; and the right to examine the witnesses against him or her.

To ensure "equality of arms" between defence and prosecution teams, the ICC has established an independent Office of Public Counsel for the Defence (OPCD) to provide logistical support, advice and information to defendants and their counsel. The OPCD also helps to safeguard the rights of the accused during the initial stages of an investigation. However, Thomas Lubanga's defence team say they were given a smaller budget than the Prosecutor and that evidence and witness statements were slow to arrive.

One of the great innovations of the Statute of the International Criminal Court and its Rules of Procedure and Evidence is the series of rights granted to victims. For the first time in the history of international criminal justice, victims have the possibility under the Statute to present their views and observations before the Court.

Participation before the Court may occur at various stages of proceedings and may take different forms, although it will be up to the judges to give directions as to the timing and manner of participation.

Participation in the Court's proceedings will in most cases take place through a legal representative and will be conducted "in a manner which is not prejudicial or inconsistent with the rights of the accused and a fair and impartial trial".

The victim-based provisions within the Rome Statute provide victims with the opportunity to have their voices heard and to obtain, where appropriate, some form of reparation for their suffering. It is the aim of this attempted balance between retributive and restorative justice that, it is hoped, will enable the ICC to not only bring criminals to justice but also help the victims themselves obtain some form of justice. Justice for victims before the ICC comprises both procedural and substantive justice, by allowing them to participate and present their views and interests, so that they can help to shape truth, justice and reparations outcomes of the Court.

Article 43(6) establishes a Victims and Witnesses Unit to provide "protective measures and security arrangements, counseling and other appropriate assistance for witnesses, victims who appear before the Court, and others who are at risk on account of testimony given by such witnesses." Article 68 sets out procedures for the "Protection of the victims and witnesses and their participation in the proceedings." The Court has also established an Office of Public Counsel for Victims, to provide support and assistance to victims and their legal representatives.

The ICC does not have its own witness protection program, but rather must rely on national programs to keep witnesses safe.

Victims before the International Criminal Court can also claim reparations under Article 75 of the Rome Statute. Reparations can only be claimed when a defendant is convicted and at the discretion of the Court's judges. So far the Court has ordered reparations against Thomas Lubanga. Reparations can include compensation, restitution and rehabilitation, but other forms of reparations may be appropriate for individual, collective or community victims. Article 79 of the Rome Statute establishes a Trust Fund to provide assistance before a reparation order to victims in a situation or to support reparations to victims and their families if the convicted person has no money.

One of the principles of international law is that a treaty does not create either obligations or rights for third states without their consent, and this is also enshrined in the 1969 Vienna Convention on the Law of Treaties. The co-operation of the non-party states with the ICC is envisioned by the Rome Statute of the International Criminal Court to be of voluntary nature. However, even states that have not acceded to the Rome Statute might still be subjects to an obligation to co-operate with ICC in certain cases. When a case is referred to the ICC by the UN Security Council all UN member states are obliged to co-operate, since its decisions are binding for all of them. Also, there is an obligation to respect and ensure respect for international humanitarian law, which stems from the Geneva Conventions and Additional Protocol I, which reflects the absolute nature of international humanitarian law. Although the wording of the Conventions might not be precise as to what steps have to be taken, it has been argued that it at least requires non-party states to make an effort not to block actions of ICC in response to serious violations of those Conventions.

In relation to co-operation in investigation and evidence gathering, it is implied from the Rome Statute that the consent of a non-party state is a prerequisite for ICC Prosecutor to conduct an investigation within its territory, and it seems that it is even more necessary for him to observe any reasonable conditions raised by that state, since such restrictions exist for states party to the Statute. Taking into account the experience of the International Criminal Tribunal for the former Yugoslavia (which worked with the principle of the primacy, instead of complementarity) in relation to co-operation, some scholars have expressed their pessimism as to the possibility of ICC to obtain co-operation of non-party states. As for the actions that ICC can take towards non-party states that do not co-operate, the Rome Statute stipulates that the Court may inform the Assembly of States Parties or Security Council, when the matter was referred by it, when non-party state refuses to co-operate after it has entered into an "ad hoc" arrangement or an agreement with the Court.

It is unclear to what extent the ICC is compatible with reconciliation processes that grant amnesty to human rights abusers as part of agreements to end conflict. Article 16 of the Rome Statute allows the Security Council to prevent the Court from investigating or prosecuting a case, and Article 53 allows the Prosecutor the discretion not to initiate an investigation if he or she believes that "an investigation would not serve the interests of justice". Former ICC president Philippe Kirsch has said that "some limited amnesties may be compatible" with a country's obligations genuinely to investigate or prosecute under the Statute.

It is sometimes argued that amnesties are necessary to allow the peaceful transfer of power from abusive regimes. By denying states the right to offer amnesty to human rights abusers, the International Criminal Court may make it more difficult to negotiate an end to conflict and a transition to democracy. For example, the outstanding arrest warrants for four leaders of the Lord's Resistance Army are regarded by some as an obstacle to ending the insurgency in Uganda. Czech politician Marek Benda argues that "the ICC as a deterrent will in our view only mean the worst dictators will try to retain power at all costs". However, the United Nations and the International Committee of the Red Cross maintain that granting amnesty to those accused of war crimes and other serious crimes is a violation of international law.

The official seat of the Court is in The Hague, Netherlands, but its proceedings may take place anywhere.

The Court moved into its first permanent premises in The Hague, located at Oude Waalsdorperweg 10, on 14 December 2015. Part of The Hague's International Zone, which also contains the Peace Palace, Europol, Eurojust, ICTY, OPCW and The Hague World Forum, the court facilities are situated on the site of the "Alexanderkazerne", a former military barracks, adjacent to the dune landscape on the northern edge of the city. The ICC's detention centre is a short distance away.

The land and financing for the new construction were provided by the Netherlands. In addition, the host state organised and financed the architectural design competition which started at the end of 2008.

Three architects were chosen by an international jury from a total of 171 applicants to enter into further negotiations. The Danish firm schmidt hammer lassen were ultimately selected to design the new premises since its design met all the ICC criteria, such as design quality, sustainability, functionality and costs.

Demolition of the barracks started in November 2011 and was completed in August 2012. In October 2012 the tendering procedure for the General Contractor was completed and the combination Visser & Smit Bouw and Boele & van Eesteren ("Courtys") was selected.

The building has a compact footprint and consists of six connected building volumes with a garden motif. The tallest volume with a green facade, placed in the middle of the design, is the Court Tower that accommodates 3 courtrooms. The rest of the building's volumes accommodate the offices of the different organs of the ICC.

Until late 2015, the ICC was housed in interim premises in The Hague provided by the Netherlands. Formerly belonging to KPN, the provisional headquarters were located at Maanweg 174 in the east-central portion of the city.

The ICC's detention centre accommodates both those convicted by the court and serving sentences as well as those suspects detained pending the outcome of their trial. It comprises twelve cells on the premises of the Scheveningen branch of the Haaglanden Penal Institution, The Hague, close to the ICC's new headquarters in the Alexanderkazerne. Suspects held by the International Criminal Tribunal for the former Yugoslavia are held in the same prison and share some facilities, like the fitness room, but have no contact with suspects held by the ICC.

The ICC maintains a liaison office in New York and field offices in places where it conducts its activities. As of 18 October 2007, the Court had field offices in Kampala, Kinshasa, Bunia, Abéché and Bangui.

The ICC is financed by contributions from the states parties. The amount payable by each state party is determined using the same method as the United Nations: each state's contribution is based on the country's capacity to pay, which reflects factors such as a national income and population. The maximum amount a single country can pay in any year is limited to 22% of the Court's budget; Japan paid this amount in 2008.

The Court spent €80.5 million in 2007. The Assembly of States Parties approved a budget of €90.4 million for 2008, €101.2 million for 2009, and €141.6 million for 2017. , the ICC's staff consisted of 800 persons from approximately 100 states.

To date, the Prosecutor has opened investigations in 12 situations: Burundi; two in the Central African Republic; Côte d'Ivoire; Darfur, Sudan; the Democratic Republic of the Congo; Georgia; Kenya; Libya; Mali; Uganda; and Bangladesh/Myanmar. Additionally, the Office of the Prosecutor is conducting preliminary examinations in nine situations in Afghanistan; Colombia; Guinea; Iraq / the United Kingdom; Nigeria; Palestine; the Philippines; Ukraine; and Venezuela.

The Court's Pre-Trial Chambers have 

The "Lubanga" and "Katanga-Chui" trials in the situation of the DR Congo are concluded. Mr Lubanga and Mr Katanga were convicted and sentenced to 14 and 12 years imprisonment, respectively, whereas Mr Chui was acquitted.

The "Bemba" trial in the Central African Republic situation is concluded. Mr Bemba was convicted on two counts of crimes against humanity and three counts of war crimes. This marked the first time the ICC convicted someone of sexual violence as they added rape to his conviction.

Trials in the "Ntaganda" case (DR Congo), the "Bemba et al." case and the "Laurent Gbagbo-Blé Goudé" trial in the Côte d'Ivoire situation are ongoing. The "Banda" trial in the situation of Darfur, Sudan, was scheduled to begin in 2014 but the start date was vacated. Charges against Dominic Ongwen in the Uganda situation and Ahmed al-Faqi in the Mali situation have been confirmed; both are awaiting their trials.

Currently, the Office of the Prosecutor has 

Notes

Unlike the International Court of Justice, the ICC is legally independent from the United Nations. However, the Rome Statute grants certain powers to the United Nations Security Council, which limits its functional independence. Article 13 allows the Security Council to refer to the Court situations that would not otherwise fall under the Court's jurisdiction (as it did in relation to the situations in Darfur and Libya, which the Court could not otherwise have prosecuted as neither Sudan nor Libya are state parties). Article 16 allows the Security Council to require the Court to defer from investigating a case for a period of 12 months. Such a deferral may be renewed indefinitely by the Security Council. This sort of an arrangement gives the ICC some of the advantages inhering in the organs of the United Nations such as using the enforcement powers of the Security Council, but it also creates a risk of being tainted with the political controversies of the Security Council.

The Court cooperates with the UN in many different areas, including the exchange of information and logistical support. The Court reports to the UN each year on its activities, and some meetings of the Assembly of States Parties are held at UN facilities. The relationship between the Court and the UN is governed by a "Relationship Agreement between the International Criminal Court and the United Nations".

During the 1970s and 1980s, international human rights and humanitarian Nongovernmental Organizations (or NGOs) began to proliferate at exponential rates. Concurrently, the quest to find a way to punish international crimes shifted from being the exclusive responsibility of legal experts to being shared with international human rights activism.

NGOs helped birth the ICC through advocacy and championing for the prosecution of perpetrators of crimes against humanity. NGOs closely monitor the organization's declarations and actions, ensuring that the work that is being executed on behalf of the ICC is fulfilling its objectives and responsibilities to civil society. According to Benjamin Schiff, "From the Statute Conference onward, the relationship between the ICC and the NGOs has probably been closer, more consistent, and more vital to the Court than have analogous relations between NGOs and any other international organization."

There are a number of NGOs working on a variety of issues related to the ICC. The NGO Coalition for the International Criminal Court has served as a sort of umbrella for NGOs to coordinate with each other on similar objectives related to the ICC. The CICC has 2,500 member organizations in 150 different countries. The original steering committee included representatives from the World Federalist Movement, the International Commission of Jurists, Amnesty International, the Lawyers Committee for Human Rights, Human Rights Watch, Parliamentarians for Global Action, and No Peace Without Justice. Today, many of the NGOs with which the ICC cooperates are members of the CICC. These organizations come from a range of backgrounds, spanning from major international NGOs such as Human Rights Watch and Amnesty International, to smaller, more local organizations focused on peace and justice missions. Many work closely with states, such as the International Criminal Law Network, founded and predominantly funded by the Hague municipality and the Dutch Ministries of Defense and Foreign Affairs. The CICC also claims organizations that are themselves federations, such as the International Federation of Human Rights Leagues (FIDH).

CICC members ascribe to three principles that permit them to work under the umbrella of the CICC, so long as their objectives match them:

The NGOs that work under the CICC do not normally pursue agendas exclusive to the work of the Court, rather they may work for broader causes, such as general human rights issues, victims' rights, gender rights, rule of law, conflict mediation, and peace. The CICC coordinates their efforts to improve the efficiency of NGOs' contributions to the Court and to pool their influence on major common issues. From the ICC side, it has been useful to have the CICC channel NGO contacts with the Court so that its officials do not have to interact individually with thousands of separate organizations.

NGOs have been crucial to the evolution of the ICC, as they assisted in the creation of the normative climate that urged states to seriously consider the Court's formation. Their legal experts helped shape the Statute, while their lobbying efforts built support for it. They advocate Statute ratification globally and work at expert and political levels within member states for passage of necessary domestic legislation. NGOs are greatly represented at meetings for the Assembly of States Parties, and they use the ASP meetings to press for decisions promoting their priorities. Many of these NGOs have reasonable access to important officials at the ICC because of their involvement during the Statute process. They are engaged in monitoring, commenting upon, and assisting in the ICC's activities.

The ICC often depends on NGOs to interact with local populations. The Registry Public Information Office personnel and Victims Participation and Reparations Section officials hold seminars for local leaders, professionals and the media to spread the word about the Court. These are the kinds of events that are often hosted or organized by local NGOs. Because there can be challenges with determining which of these NGOs are legitimate, CICC regional representatives often have the ability to help screen and identify trustworthy organizations.

However, NGOs are also "sources of criticism, exhortation and pressure upon" the ICC. The ICC heavily depends on NGOs for its operations. Although NGOs and states cannot directly impact the judicial nucleus of the organization, they can impart information on crimes, can help locate victims and witnesses, and can promote and organize victim participation. NGOs outwardly comment on the Court's operations, "push for expansion of its activities especially in the new justice areas of outreach in conflict areas, in victims' participation and reparations, and in upholding due-process standards and defense 'equality of arms' and so implicitly set an agenda for the future evolution of the ICC." The relatively uninterrupted progression of NGO involvement with the ICC may mean that NGOs have become repositories of more institutional historical knowledge about the ICC than its national representatives, and have greater expertise than some of the organization's employees themselves. While NGOs look to mold the ICC to satisfy the interests and priorities that they have worked for since the early 1990s, they unavoidably press against the limits imposed upon the ICC by the states that are members of the organization. NGOs can pursue their own mandates, irrespective of whether they are compatible with those of other NGOs, while the ICC must respond to the complexities of its own mandate as well as those of the states and NGOs.

Another issue has been that NGOs possess "exaggerated senses of their ownership over the organization and, having been vital to and successful in promoting the Court, were not managing to redefine their roles to permit the Court its necessary independence." Additionally, because there does exist such a gap between the large human rights organizations and the smaller peace-oriented organizations, it is difficult for ICC officials to manage and gratify all of their NGOs. "ICC officials recognize that the NGOs pursue their own agendas, and that they will seek to pressure the ICC in the direction of their own priorities rather than necessarily understanding or being fully sympathetic to the myriad constraints and pressures under which the Court operates." Both the ICC and the NGO community avoid criticizing each other publicly or vehemently, although NGOs have released advisory and cautionary messages regarding the ICC. They avoid taking stances that could potentially give the Court's adversaries, particularly the US, more motive to berate the organization.

The ICC has been accused of bias and as being a tool of Western imperialism, only punishing leaders from small, weak states while ignoring crimes committed by richer and more powerful states. This sentiment has been expressed particularly by African leaders due to an alleged disproportionate focus of the Court on Africa, while it claims to have a global mandate; until January 2016, all nine situations which the ICC had been investigating were in African countries.

The prosecution of Kenyan Deputy President William Ruto and President Uhuru Kenyatta (both charged before coming into office) led to the Kenyan parliament passing a motion calling for Kenya's withdrawal from the ICC, and the country called on the other 33 African states party to the ICC to withdraw their support, an issue which was discussed at a special African Union (AU) summit in October 2013.

Though the ICC has denied the charge of disproportionately targeting African leaders, and claims to stand up for victims wherever they may be, Kenya was not alone in criticising the ICC. Sudanese President Omar al-Bashir visited Kenya, South Africa, China, Nigeria, Saudi Arabia, United Arab Emirates, Egypt, Ethiopia, Qatar and several other countries despite an outstanding ICC warrant for his arrest but was not arrested; he said that the charges against him are "exaggerated" and that the ICC was a part of a "Western plot" against him. Ivory Coast's government opted not to transfer former first lady Simone Gbagbo to the court but to instead try her at home. Rwanda's ambassador to the African Union, Joseph Nsengimana, argued that "It is not only the case of Kenya. We have seen international justice become more and more a political matter." Ugandan President Yoweri Museveni accused the ICC of "mishandling complex African issues." Ethiopian Prime Minister Hailemariam Desalegn, at the time AU chairman, told the UN General Assembly at the General debate of the sixty-eighth session of the United Nations General Assembly: "The manner in which the ICC has been operating has left a very bad impression in Africa. It is totally unacceptable."

South African President Jacob Zuma said the perceptions of the ICC as "unreasonable" led to the calling of the special AU summit on 13 October 2015. Botswana is a notable supporter of the ICC in Africa. At the summit, the AU did not endorse the proposal for a collective withdrawal from the ICC due to lack of support for the idea. However, the summit did conclude that serving heads of state should not be put on trial and that the Kenyan cases should be deferred. Ethiopian Foreign Minister Tedros Adhanom said: "We have rejected the double standard that the ICC is applying in dispensing international justice." Despite these calls, the ICC went ahead with requiring William Ruto to attend his trial. The UNSC was then asked to consider deferring the trials of Kenyatta and Ruto for a year, but this was rejected. In November, the ICC's Assembly of State Parties responded to Kenya's calls for an exemption for sitting heads of state by agreeing to consider amendments to the Rome Statute to address the concerns.

On 7 October 2016, Burundi announced that it would leave the ICC, after the court began investigating political violence in that nation. In the subsequent two weeks, South Africa and Gambia also announced their intention to leave the court, with Kenya and Namibia reportedly also considering departure. All three nations cited the fact that all 39 people indicted by the court over its history have been African and that the court has made no effort to investigate war crimes tied to the 2003 invasion of Iraq. However, following Gambia's presidential election later that year, which ended the long rule of Yahya Jammeh, Gambia rescinded its withdrawal notification. The High Court of South Africa ruled on 2 February 2017 that the South African government's notice to withdraw was unconstitutional and invalid. On 7 March 2017 the South African government formally revoked its intention to withdraw; however, the ruling ANC revealed on 5 July 2017 that its intention to withdraw stands.

The United States Department of State argues that there are "insufficient checks and balances on the authority of the ICC prosecutor and judges" and "insufficient protection against politicized prosecutions or other abuses". The current law in the United States on the ICC is the American Service-Members' Protection Act (ASPA), 116 Stat. 820, The ASPA authorizes the President of the United States to use "all means necessary and appropriate to bring about the release of any U.S. or allied personnel being detained or imprisoned by, on behalf of, or at the request of the International Criminal Court." This authorization has led the act to be nicknamed the "Hague Invasion Act", because the freeing of U.S. citizens by force might be possible only through military action.

On 10 September 2018, John R. Bolton, in his first major address as U.S. National Security Advisor, reiterated that the ICC lacks checks and balances, exercises "jurisdiction over crimes that have disputed and ambiguous definitions," and has failed to "deter and punish atrocity crimes." The ICC, said Bolton, is "superfluous" given that "domestic judicial systems already hold American citizens to the highest legal and ethical standards." He added that the U.S. would do everything "to protect our citizens" should the ICC attempt to prosecute U.S. servicemen over alleged detainee abuse in Afghanistan. In that event, ICC judges and prosecutors would be barred from entering the U.S., their funds in the U.S. would be sanctioned and the U.S. "will prosecute them in the US criminal system. We will do the same for any company or state that assists an ICC investigation of Americans", Bolton said. He also criticized Palestinian efforts to bring Israel before the ICC over allegations of human rights abuses in the West Bank and Gaza.

ICC responded that it will continue to investigate war crimes undeterred.

On 11 June 2020, Mike Pompeo and U.S. President Donald Trump announced sanctions on officials and employees, as well as their families, involved in investigating crimes against humanity committed by US armed forces in Afghanistan. This move was widely criticized by human rights groups.

Concerning the independent Office of Public Counsel for the Defence (OPCD), Thomas Lubanga's defence team say they were given a smaller budget than the Prosecutor and that evidence and witness statements were slow to arrive.

Limitations exist for the ICC. Human Rights Watch (HRW) reported that the ICC's prosecutor team takes no account of the roles played by the government in the conflict of Uganda, Rwanda or Congo. This led to a flawed investigation, because the ICC did not reach the conclusion of its verdict after considering the governments' position and actions in the conflict.

Research suggests that prosecutions of leaders in the ICC makes dictators less likely to peacefully step down. It is also argued that justice is a means to peace: "As a result, the ICC has been used as a means
of intervention in ongoing conflicts with the expectation that the indictments, arrests, and
trials of elite perpetrators have deterrence and preventive effects for atrocity crimes.
Despite these legitimate intentions and great expectations, there is little evidence of the
efficacy of justice as a means to peace".

That the ICC cannot mount successful cases without state cooperation is problematic for several reasons. It means that the ICC acts inconsistently in its selection of cases, is prevented from taking on hard cases and loses legitimacy. It also gives the ICC less deterrent value, as potential perpetrators of war crimes know that they can avoid ICC judgment by taking over government and refusing to cooperate.

The fundamental principle of complementarity of the ICC Rome Statute is often taken for granted in the legal analysis of international criminal law and its jurisprudence. Initially the thorny issue of the actual application of the complementarity principle arose in 2008, when William Schabas published his influential paper. However, despite Schabas' theoretical impact, no substantive research was made by other scholars on this issue for quite some time. In June 2017, Victor Tsilonis advanced the same criticism which is reinforced by events, practices of the Office of the Prosecutor and ICC cases in the Essays in Honour of Nestor Courakis. His paper essentially argues that the Αl‐Senussi case arguably is the first instance of the complementarity principle's actual implementation eleven whole years after the ratification of the Rome Statute of the International Criminal Court.

On the other hand, the Chief Prosecutor, Fatou Bensouda, has invoked recently the principle of complementarity in the situation between Russia and Georgia in
Ossetia region. Moreover, following the threats of certain African states (initially Burundi, Gambia and South Africa) to withdraw their ratifications, Bensouda again referred to the principle of complementarity as a core principle of ICC's jurisdiction and has more extensively focused on the principle's application on the latest Office of The Prosecutor's Report on Preliminary Examination Activities 2016.

Some advocates have suggested that the ICC go "beyond complementarity" and systematically support national capacity for prosecutions. They argue that national prosecutions, where possible, are more cost-effective, preferable to victims and more sustainable.





</doc>
<doc id="14881" url="https://en.wikipedia.org/wiki?curid=14881" title="ICC">
ICC

ICC may refer to:
















</doc>
<doc id="14882" url="https://en.wikipedia.org/wiki?curid=14882" title="Incubus (disambiguation)">
Incubus (disambiguation)

An incubus is a male demon that has sexual intercourse with sleeping women.

Incubus may also refer to:






</doc>
<doc id="14883" url="https://en.wikipedia.org/wiki?curid=14883" title="Iberian Peninsula">
Iberian Peninsula

The Iberian Peninsula , also known as Iberia, is located in the southwest corner of Europe, defining the westernmost edge of Eurasia. The peninsula is principally divided between Spain and Portugal, comprising most of their territory, as well as a small area of Southern France, Andorra and the British overseas territory of Gibraltar. With an area of approximately , and a population of roughly 53 million, it is the second largest European peninsula by area, after the Scandinavian Peninsula.

The word "Iberia" is a noun adapted from the Latin word "Hiberia" originating in the Ancient Greek word Ἰβηρία (""), used by Greek geographers under the rule of the Roman Empire to refer to what is known today in English as the Iberian Peninsula. At that time, the name did not describe a single geographical entity or a distinct population; the same name was used for the Kingdom Kartli in the Caucasus, the core region of what would become the Kingdom of Georgia. It was Strabo who first reported the delineation of "Iberia" from Gaul ("Keltikē") by the Pyrenees and included the entire land mass southwest (he says "west") from there. With the fall of the Roman Empire and the establishment of the new Castillian language in Spain, the word "Iberia" continued the Roman word "Hiberia" and the Greek word "Ἰβηρία".

The ancient Greeks reached the Iberian Peninsula, of which they had heard from the Phoenicians, by voyaging westward on the Mediterranean. Hecataeus of Miletus was the first known to use the term "Iberia", which he wrote about circa 500 BC. Herodotus of Halicarnassus says of the Phocaeans that "it was they who made the Greeks acquainted with […] Iberia." According to Strabo, prior historians used "Iberia" to mean the country "this side of the Ἶβηρος ("")" as far north as the river Rhône in France, but currently they set the Pyrenees as the limit. Polybius respects that limit, but identifies Iberia as the Mediterranean side as far south as Gibraltar, with the Atlantic side having no name. Elsewhere he says that Saguntum is "on the seaward foot of the range of hills connecting Iberia and Celtiberia."

Strabo refers to the Carretanians as people "of the Iberian stock" living in the Pyrenees, who are distinct from either Celts or Celtiberians.

According to Charles Ebel, the ancient sources in both Latin and Greek use Hispania and Hiberia (Greek: Iberia) as synonyms. The confusion of the words was because of an overlapping in political and geographic perspectives. The Latin word "Hiberia", similar to the Greek "Iberia", literally translates to "land of the Hiberians". This word was derived from the river Ebro, which the Romans called "Hiberus". "Hiber" (Iberian) was thus used as a term for peoples living near the river Ebro. The first mention in Roman literature was by the annalist poet Ennius in 200 BC. Virgil refers to the "Ipacatos Hiberos" ("restless Iberi") in his Georgics. The Roman geographers and other prose writers from the time of the late Roman Republic called the entire peninsula "Hispania".

In Greek and Roman antiquity, the name "Hesperia" was used for both the Italian and Iberian Peninsula; in the latter case "Hesperia Ultima" (referring to its position in the far west) appears as form of disambiguation from the former among Roman writers.

As they became politically interested in the former Carthaginian territories, the Romans began to use the names "Hispania Citerior" and "Hispania Ulterior" for 'near' and 'far' Hispania. At the time Hispania was made up of three Roman provinces: Hispania Baetica, Hispania Tarraconensis, and Hispania Lusitania. Strabo says that the Romans use "Hispania" and "Iberia" synonymously, distinguishing between the "near" northern and the "far" southern provinces. (The name "Iberia" was ambiguous, being also the name of the Kingdom of Iberia in the Caucasus.)

Whatever languages may generally have been spoken on the peninsula soon gave way to Latin, except for that of the Vascones, which was preserved as a language isolate by the barrier of the Pyrenees.

The modern phrase "Iberian Peninsula" was coined by the French geographer Jean-Baptiste Bory de Saint-Vincent on his 1823 work ""Guide du Voyageur en Espagne"". Prior to that date, geographers had used the terms "Spanish Peninsula" or "Pyrenaean Peninsula"

The Iberian Peninsula has always been associated with the River Ebro (Ibēros in ancient Greek and Ibērus or Hibērus in Latin). The association was so well known it was hardly necessary to state; for example, Ibēria was the country "this side of the Ibērus" in Strabo. Pliny goes so far as to assert that the Greeks had called "the whole of Spain" Hiberia because of the Hiberus River. The river appears in the Ebro Treaty of 226 BC between Rome and Carthage, setting the limit of Carthaginian interest at the Ebro. The fullest description of the treaty, stated in Appian, uses Ibērus. With reference to this border, Polybius states that the "native name" is "Ibēr", apparently the original word, stripped of its Greek or Latin "-os" or "-us" termination.

The early range of these natives, which geographers and historians place from the present southern Spain to the present southern France along the Mediterranean coast, is marked by instances of a readable script expressing a yet unknown language, dubbed "Iberian." Whether this was the native name or was given to them by the Greeks for their residence near the Ebro remains unknown. Credence in Polybius imposes certain limitations on etymologizing: if the language remains unknown, the meanings of the words, including Iber, must also remain unknown. In modern Basque, the word "ibar" means "valley" or "watered meadow", while "ibai" means "river", but there is no proof relating the etymology of the Ebro River with these Basque names.

The Iberian Peninsula has been inhabited for at least 1.2 million years as remains found in the sites in the Atapuerca Mountains demonstrate. Among these sites is the cave of Gran Dolina, where six hominin skeletons, dated between 780,000 and one million years ago, were found in 1994. Experts have debated whether these skeletons belong to the species "Homo erectus", "Homo heidelbergensis", or a new species called "Homo antecessor".

Around 200,000 BP, during the Lower Paleolithic period, Neanderthals first entered the Iberian Peninsula. Around 70,000 BP, during the Middle Paleolithic period, the last glacial event began and the Neanderthal Mousterian culture was established. Around 37,000 BP, during the Upper Paleolithic, the Neanderthal Châtelperronian cultural period began. Emanating from Southern France, this culture extended into the north of the peninsula. It continued to exist until around 30,000 BP, when Neanderthal man faced extinction.

About 40,000 years ago, anatomically modern humans entered the Iberian Peninsula from Southern France. Here, this genetically homogeneous population (characterized by the M173 mutation in the Y chromosome), developed the M343 mutation, giving rise to Haplogroup R1b, still the most common in modern Portuguese and Spanish males. On the Iberian Peninsula, modern humans developed a series of different cultures, such as the Aurignacian, Gravettian, Solutrean and Magdalenian cultures, some of them characterized by the complex forms of the art of the Upper Paleolithic.

During the Neolithic expansion, various megalithic cultures developed in the Iberian Peninsula. An open seas navigation culture from the east Mediterranean, called the Cardium culture, also extended its influence to the eastern coasts of the peninsula, possibly as early as the 5th millennium BC. These people may have had some relation to the subsequent development of the Iberian civilization.

In the Chalcolithic ( 3000 BC), a series of complex cultures developed that would give rise to the peninsula's first civilizations and to extensive exchange networks reaching to the Baltic, Middle East and North Africa. Around 2800 – 2700 BC, the Beaker culture, which produced the "Maritime Bell Beaker", probably originated in the vibrant copper-using communities of the Tagus estuary in Portugal and spread from there to many parts of western Europe.

Bronze Age cultures developed beginning  1800 BC, when the civilization of Los Millares was followed by that of El Argar. From this centre, bronze technology spread to other cultures like the Bronze of Levante, South-Western Iberian Bronze and Las Cogotas.

In the Late Bronze Age, the urban civilisation of Tartessos developed in the area of modern western Andalusia, characterized by Phoenician influence and using the Southwest Paleohispanic script for its Tartessian language, not related to the Iberian language.

Early in the first millennium BC, several waves of Pre-Celts and Celts migrated from Central Europe, thus partially changing the peninsula's ethnic landscape to Indo-European-speaking in its northern and western regions. In Northwestern Iberia (modern Northern Portugal, Asturias and Galicia), a Celtic culture developed, the Castro culture, with a large number of hill forts and some fortified cities.

By the Iron Age, starting in the 7th century BC, the Iberian Peninsula consisted of complex agrarian and urban civilizations, either Pre-Celtic or Celtic (such as the Lusitanians, Celtiberians, Gallaeci, Astures, Celtici and others), the cultures of the Iberians in the eastern and southern zones and the cultures of the Aquitanian in the western portion of the Pyrenees.

As early as the 12th century BC, the Phoenicians, a thalassocratic civilization originally from the East Mediterranean, began to explore the coastline of the peninsula, interacting with the metal-rich communities in the southwest of the peninsula (contemporarily known as the semi-mythical Tartessos). Around 1100 BC, Phoenician merchants founded the trading colony of Gadir or Gades (modern day Cádiz). Phoenicians established a permanent trading port in the Gadir colony circa 800 BC in response to the increasing demand of silver from the Assyrian Empire.

The seafaring Phoenicians, Greeks and Carthaginians successively settled along the Mediterranean coast and founded trading colonies there over a period of several centuries. In the 8th century BC, the first Greek colonies, such as Emporion (modern Empúries), were founded along the Mediterranean coast on the east, leaving the south coast to the Phoenicians. The Greeks coined the name Iberia, after the river Iber (Ebro). In the sixth century BC, the Carthaginians arrived in the peninsula while struggling with the Greeks for control of the Western Mediterranean. Their most important colony was Carthago Nova (modern-day Cartagena, Spain).

In 218 BC, during the Second Punic War against the Carthaginians, the first Roman troops occupied the Iberian Peninsula; however, it was not until the reign of Augustus that it was annexed after 200 years of war with the Celts and Iberians. The result was the creation of the province of Hispania. It was divided into Hispania Ulterior and Hispania Citerior during the late Roman Republic, and during the Roman Empire, it was divided into Hispania Tarraconensis in the northeast, Hispania Baetica in the south and Lusitania in the southwest.

Hispania supplied the Roman Empire with silver, food, olive oil, wine, and metal. The emperors Trajan, Hadrian, Marcus Aurelius, and Theodosius I, the philosopher Seneca the Younger, and the poets Martial and Lucan were born from families living on the Iberian Peninsula.

During their 600-year occupation of the Iberian Peninsula, the Romans introduced the Latin language that influenced many of the languages that exist today in the Iberian peninsula.
In the early fifth century, Germanic peoples occupied the peninsula, namely the Suebi, the Vandals (Silingi and Hasdingi) and their allies, the Alans. Only the kingdom of the Suebi (Quadi and Marcomanni) would endure after the arrival of another wave of Germanic invaders, the Visigoths, who occupied all of the Iberian Peninsula and expelled or partially integrated the Vandals and the Alans. The Visigoths eventually occupied the Suebi kingdom and its capital city, Bracara (modern day Braga), in 584–585. They would also occupy the province of the Byzantine Empire (552–624) of Spania in the south of the peninsula and the Balearic Islands.
In 711, a Muslim army conquered the Visigothic Kingdom in Hispania. Under Tariq ibn Ziyad, the Islamic army landed at Gibraltar and, in an eight-year campaign, occupied all except the northern kingdoms of the Iberian Peninsula in the Umayyad conquest of Hispania. Al-Andalus (, tr. "al-ʾAndalūs", possibly "Land of the Vandals"), is the Arabic name given to Muslim Iberia. The Muslim conquerors were Arabs and Berbers; following the conquest, conversion and arabization of the Hispano-Roman population took place, ("muwalladum" or "Muladi"). After a long process, spurred on in the 9th and 10th centuries, the majority of the population in Al-Andalus eventually converted to Islam. The Muslims were referred to by the generic name "Moors". The Muslim population was divided per ethnicity (Arabs, Berbers, Muladi), and the supremacy of Arabs over the rest of group was a recurrent causal for strife, rivalry and hatred, particularly between Arabs and Berbers. Arab elites could be further divided in the Yemenites (first wave) and the Syrians (second wave). Christians and Jews were allowed to live as part of a stratified society under the "dhimmah" system, although Jews became very important in certain fields. Some Christians migrated to the Northern Christian kingdoms, while those who stayed in Al-Andalus progressively arabised and became known as "musta'arab" (mozarabs). The slave population comprised the "Ṣaqāliba" (literally meaning "slavs", although they were slaves of generic European origin) as well as Sudanese slaves.

The Umayyad rulers faced a major Berber Revolt in the early 740s; the uprising originally broke out in North Africa (Tangier) and later spread across the peninsula. Following the Abbasid takeover from the Umayyads and the shift of the economic centre of the Islamic Caliphate from Damascus to Baghdad, the western province of al-Andalus was marginalised and ultimately became politically autonomous as independent emirate in 756, ruled by one of the last surviving Umayyad royals, Abd al-Rahman I.

Al-Andalus became a center of culture and learning, especially during the Caliphate of Córdoba. The Caliphate reached its height of its power under the rule of Abd-ar-Rahman III and his successor al-Hakam II, becoming then, in the view of Jaime Vicens Vives, "the most powerful state in Europe". Abd-ar-Rahman III also managed to expand the clout of Al-Andalus across the Strait of Gibraltar, waging war, as well as his successor, against the Fatimid Empire.

Between the 8th and 12th centuries, Al-Andalus enjoyed a notable urban vitality, both in terms of the growth of the preexisting cities as well as in terms of founding of new ones: Córdoba reached a population of 100,000 by the 10th century, Toledo 30,000 by the 11th century and Seville 80,000 by the 12th century.

During the Middle Ages, the North of the peninsula housed many small Christian polities including the Kingdom of Castile, the Kingdom of Aragon, the Kingdom of Navarre, the Kingdom of León or the Kingdom of Portugal, as well as a number of counties that spawned from the Carolingian Marca Hispanica. Christian and Muslim polities fought and allied among themselves in variable alliances. The Christian kingdoms progressively expanded south taking over Muslim territory in what is historiographically known as the "Reconquista" (the latter concept has been however noted as product of the claim to a pre-existing Spanish Catholic nation and it would not necessarily convey adequately "the complexity of centuries of warring and other more peaceable interactions between Muslim and Christian kingdoms in medieval Iberia between 711 and 1492").

The Caliphate of Córdoba subsumed in a period of upheaval and civil war (the Fitna of al-Andalus) and collapsed in the early 11th century, spawning a series of ephemeral statelets, the "taifas". Until the mid 11th century, most of the territorial expansion southwards of the Kingdom of Asturias/León was carried out through a policy of agricultural colonization rather than through military operations; then, profiting from the feebleness of the taifa principalities, Ferdinand I of León seized Lamego and Viseu (1057–1058) and Coimbra (1064) away from the Taifa of Badajoz (at times at war with the Taifa of Seville); Meanwhile, in the same year Coimbra was conquered, in the Northeastern part of the Iberian Peninsula, the Kingdom of Aragon took Barbastro from the Hudid Taifa of Lérida as part of an international expedition sanctioned by Pope Alexander II. Most critically, Alfonso VI of León-Castile conquered Toledo and its wider taifa in 1085, in what it was seen as a critical event at the time, entailing also a huge territorial expansion, advancing from the Sistema Central to La Mancha. In 1086, following the siege of Zaragoza by Alfonso VI of León-Castile, the Almoravids, religious zealots originally from the deserts of the Maghreb, landed in the Iberian Peninsula, and, having inflicted a serious defeat to Alfonso VI at the battle of Zalaca, began to seize control of the remaining taifas.

The Almoravids in the Iberian peninsula progressively relaxed strict observance of their faith, and treated both Jews and Mozarabs harshly, facing uprisings across the peninsula, initially in the Western part. The Almohads, another North-African Muslim sect of Masmuda Berber origin who had previously undermined the Almoravid rule south of the Strait of Gibraltar, first entered the peninsula in 1146.

Somewhat straying from the trend taking place in other locations of the Latin West since the 10th century, the period comprising the 11th and 13th centuries was not one of weakening monarchical power in the Christian kingdoms. The relatively novel concept of "frontier" (Sp: "frontera"), already reported in Aragon by the second half of the 11th century become widespread in the Christian Iberian kingdoms by the beginning of the 13th century, in relation to the more or less conflictual border with Muslim lands.

By the beginning of the 13th century, a power reorientation took place in the Iberian Peninsula (parallel to the Christian expansion in Southern Iberia and the increasing commercial impetus of Christian powers across the Mediterranean) and to a large extent, trade-wise, the Iberian Peninsula reorientated towards the North away from the Muslim World.

During the Middle Ages, the monarchs of Castile and León, from Alfonso V and Alfonso VI (crowned "Hispaniae Imperator") to Alfonso X and Alfonso XI tended to embrace an imperial ideal based on a dual Christian and Jewish ideology.

Merchants from Genoa and Pisa were conducting an intense trading activity in Catalonia already by the 12th century, and later in Portugal. Since the 13th century, the Crown of Aragon expanded overseas; led by Catalans, it attained an overseas empire in the Western Mediterranean, with a presence in Mediterranean islands such as the Balearics, Sicily and Sardinia, and even conquering Naples in the mid-15th century. Genoese merchants invested heavily in the Iberian commercial enterprise with Lisbon becoming, according to Virgínia Rau, the "great centre of Genoese trade" in the early 14th century. The Portuguese would later detach their trade to some extent from Genoese influence. The Nasrid Kingdom of Granada, neighbouring the Strait of Gibraltar and founded upon a vassalage relationship with the Crown of Castile, also insinuated itself into the European mercantile network, with its ports fostering intense trading relations with the Genoese as well, but also with the Catalans, and to a lesser extent, with the Venetians, the Florentines, and the Portuguese.

Between 1275 and 1340, Granada became involved in the "crisis of the Strait", and was caught in a complex geopolitical struggle ("a kaleidoscope of alliances") with multiple powers vying for dominance of the Western Mediterranean, complicated by the unstable relations of Muslim Granada with the Marinid Sultanate. The conflict reached a climax in the 1340 Battle of Río Salado, when, this time in alliance with Granada, the Marinid Sultan (and Caliph pretender) Abu al-Hasan Ali ibn Othman made the last Marinid attempt to set up a power base in the Iberian Peninsula. The lasting consequences of the resounding Muslim defeat to an alliance of Castile and Portugal with naval support from Aragon and Genoa ensured Christian supremacy over the Iberian Peninsula and the preeminence of Christian fleets in the Western Mediterranean.

The 1348–1350 bubonic plague devastated large parts of the Iberian Peninsula, leading to a sudden economic cease. Many settlements in northern Castile and Catalonia were left forsaken. The plague had the start of the hostility and downright violence towards religious minorities (particularly the Jews) as additional consequence in the Iberian realms.

The 14th century was a period of great upheaval in the Iberian realms. After the death of Peter the Cruel of Castile (reigned 1350–69), the House of Trastámara succeeded to the throne in the person of Peter's half brother, Henry II (reigned 1369–79). In the kingdom of Aragón, following the death without heirs of John I (reigned 1387–96) and Martin I (reigned 1396–1410), a prince of the House of Trastámara, Ferdinand I (reigned 1412–16), succeeded to the Aragonese throne. The Hundred Years' War also spilled over into the Iberian peninsula, with Castile particularly taking a role in the conflict by providing key naval support to France that helped lead to that nation's eventual victory. After the accession of Henry III to the throne of Castile, the populace, exasperated by the preponderance of Jewish influence, perpetrated a massacre of Jews at Toledo. In 1391, mobs went from town to town throughout Castile and Aragon, killing an estimated 50,000 Jews, or even as many as 100,000, according to Jane Gerber. Women and children were sold as slaves to Muslims, and many synagogues were converted into churches. According to Hasdai Crescas, about 70 Jewish communities were destroyed.

During the 15th century, Portugal, which had ended its southwards territorial expansion across the Iberian Peninsula in 1249 with the conquest of the Algarve, initiated an overseas expansion in parallel to the rise of the House of Aviz, conquering Ceuta (1415) arriving at Porto Santo (1418), Madeira and the Azores, as well as establishing additional outposts along the North-African coast.

During the Late Middle Ages, the Jews acquired considerable power and influence in Castile and Aragon.

Throughout the late middle ages, the Crown of Aragon took part in the mediterranean slave trade, with Barcelona (already in the 14th century), Valencia (particularly in the 15th century) and, to a lesser extent, Palma de Mallorca (since the 13th century), becoming dynamic centres in this regard, involving chiefly eastern and muslim peoples. Castile engaged later in this economic activity, rather by adhering to the incipient atlantic slave trade involving sub-saharan people thrusted by Portugal (Lisbon being the largest slave centre in Western Europe) since the mid 15th century, with Seville becoming another key hub for the slave trade. Following the advance in the conquest of the Nasrid kingdom of Granada, the seizure of Málaga entailed the addition of another notable slave centre for the Crown of Castile.

By the end of the 15th century (1490) the Iberian kingdoms (including here the Balearic Islands) had an estimated population of 6.525 million (Crown of Castile, 4.3 million; Portugal, 1.0 million; Principality of Catalonia, 0.3 million; Kingdom of Valencia, 0.255 million; Kingdom of Granada, 0.25 million; Kingdom of Aragon, 0.25 million; Kingdom of Navarre, 0.12 million and the Kingdom of Mallorca, 0.05 million).

For three decades in the 15th century, the "Hermandad de las Marismas", the trading association formed by the ports of Castile along the Cantabrian coast, resembling in some ways the Hanseatic League, fought against the latter, an ally of England, a rival of Castile in political and economic terms. Castile sought to claim the Gulf of Biscay as its own. In 1419, the powerful Castilian navy thoroughly defeated a Hanseatic fleet in La Rochelle.

In the late 15th century, the imperial ambition of the Iberian powers was pushed to new heights by the Catholic Monarchs in Castile and Aragon, and by Manuel I in Portugal.

The last Muslim stronghold, Granada, was conquered by a combined Castilian and Aragonese force in 1492. As many as 100,000 Moors died or were enslaved in the military campaign, while 200,000 fled to North Africa. Muslims and Jews throughout the period were variously tolerated or shown intolerance in different Christian kingdoms. After the fall of Granada, all Muslims and Jews were ordered to convert to Christianity or face expulsion—as many as 200,000 Jews were expelled from Spain. Historian Henry Kamen estimates that some 25,000 Jews died en route from Spain. The Jews were also expelled from Sicily and Sardinia, which were under Aragonese rule, and an estimated 37,000 to 100,000 Jews left.

In 1497, King Manuel I of Portugal forced all Jews in his kingdom to convert or leave. That same year he expelled all Muslims that were not slaves, and in 1502 the Catholic Monarchs followed suit, imposing the choice of conversion to Christianity or exile and loss of property. Many Jews and Muslims fled to North Africa and the Ottoman Empire, while others publicly converted to Christianity and became known respectively as Marranos and Moriscos (after the old term "Moors"). However, many of these continued to practice their religion in secret. The Moriscos revolted several times and were ultimately forcibly expelled from Spain in the early 17th century. From 1609–14, over 300,000 Moriscos were sent on ships to North Africa and other locations, and, of this figure, around 50,000 died resisting the expulsion, and 60,000 died on the journey.

The change of relative supremacy from Portugal to the Hispanic Monarchy in the late 15th century has been described as one of the few cases of avoidance of the Thucydides Trap.

Challenging the conventions about the advent of modernity, Immanuel Wallerstein pushed back the origins of the capitalist modernity to the Iberian expansion of the 15th century. During the 16th century Spain created a vast empire in the Americas, with a state monopoly in Seville becoming the center of the ensuing transatlantic trade, based on bullion. Iberian imperialism, starting by the Portuguese establishment of routes to Asia and the posterior transatlantic trade with the New World by Spaniards and Portuguese (along Dutch, English and French), precipitated the economic decline of the Italian peninsula. The 16th century was one of population growth with increased pressure over resources; in the case of the Iberian Peninsula a part of the population moved to the Americas meanwhile Jews and Moriscos were banished, relocating to other places in the Mediterranean Basin. Most of the Moriscos remained in Spain after the Morisco revolt in Las Alpujarras during the mid-16th century, but roughly 300,000 of them were expelled from the country in 1609–1614, and emigrated "en masse" to North Africa.

In 1580, after the political crisis that followed the 1578 death of King Sebastian, Portugal became a dynastic composite entity of the Hapsburg Monarchy; thus, the whole peninsula was united politically during the period known as the Iberian Union (1580–1640). During the reign of Phillip II of Spain (I of Portugal), the Councils of Portugal, Italy, Flanders and Burgundy were added to the group of counselling institutions of the Hispanic Monarchy, to which the Councils of Castile, Aragon, Indies, Chamber of Castile, Inquisition, Orders, and Crusade already belonged, defining the organization of the Royal court that underpinned the through which the empire operated. During the Iberian union, the "first great wave" of the transatlantic slave trade happened, according to Enriqueta Vila Villar, as new markets opened because of the unification gave thrust to the slave trade.

By 1600, the percentage of urban population for "Spain" was roughly a 11.4%, while for Portugal the urban population was estimated as 14.1%, which were both above the 7.6% European average of the time (edged only by the Low Countries and the Italian Peninsula). Some striking differences appeared among the different Iberian realms. Castile, extending across a 60% of the territory of the peninsula and having 80% of the population was a rather urbanised country, yet with a widespread distribution of cities. Meanwhile, the urban population in the Crown of Aragon was highly concentrated in a handful of cities: Zaragoza (Kingdom of Aragon), Barcelona (Principality of Catalonia), and, to a lesser extent in the Kingdom of Valencia, in Valencia, Alicante and Orihuela. The case of Portugal presented an hypertrophied capital, Lisbon (which greatly increased its population during the 16th century, from 56,000-60,000 inhabitants by 1527, to roughly 120,000 by the third quarter of the century) with its demographic dynamism stimulated by the Asian trade, followed at great distance by Porto and Évora (both roughly accounting for 12,500 inhabitants). Throughout most of the 16th century, both Lisbon and Seville were among the Western Europe's largest and most dynamic cities.

The 17th century has been largely considered as a very negative period for the Iberian economies, seen as a time of recession, crisis or even decline, the urban dynamism chiefly moving to Northern Europe. A dismantling of the inner city network in the Castilian plateau took place during this period (with a parallel accumulation of economic activity in the capital, Madrid), with only New Castile resisting recession in the interior. Regarding the Atlantic façade of Castile, aside from the severing of trade with Northern Europe, inter-regional trade with other regions in the Iberian Peninsula also suffered to some extent. In Aragon, suffering from similar problems than Castile, the expelling of the Moriscos in 1609 in the Kingdom of Valencia aggravated the recession. Silk turned from a domestic industry into a raw commodity to be exported. However, the crisis was uneven (affecting longer the centre of the peninsula), as both Portugal and the Mediterranean coastline recovered in the later part of the century by fuelling a sustained growth.

The aftermath of the intermittent 1640–1668 Portuguese Restoration War brought the House of Braganza as the new ruling dynasty in the Portuguese territories across the world (bar Ceuta), putting an end to the Iberian Union.

Despite both Portugal and Spain starting their path towards modernization with the liberal revolutions of the first half of the 19th century, this process was, concerning structural changes in the geographical distribution of the population, relatively tame compared to what took place after World War II in the Iberian Peninsula, when strong urban development ran in parallel to substantial rural flight patterns.

The Iberian Peninsula is the westernmost of the three major southern European peninsulas—the Iberian, Italian, and Balkan. It is bordered on the southeast and east by the Mediterranean Sea, and on the north, west, and southwest by the Atlantic Ocean. The Pyrenees mountains are situated along the northeast edge of the peninsula, where it adjoins the rest of Europe. Its southern tip is very close to the northwest coast of Africa, separated from it by the Strait of Gibraltar and the Mediterranean Sea.

The Iberian Peninsula encompasses 583,254 km and has very contrasting and uneven relief. The mountain ranges of the Iberian Peninsula are mainly distributed from west to east, and in some cases reach altitudes of approximately 3000 mamsl, resulting in the region having the second highest mean altitude (637 mamsl) in Western Europe.

The Iberian Peninsula extends from the southernmost extremity at Punta de Tarifa to the northernmost extremity at Punta de Estaca de Bares over a distance between lines of latitude of about based on a degree length of per degree, and from the westernmost extremity at Cabo da Roca to the easternmost extremity at Cap de Creus over a distance between lines of longitude at 40° N latitude of about based on an estimated degree length of about for that latitude. The irregular, roughly octagonal shape of the peninsula contained within this spherical quadrangle was compared to an ox-hide by the geographer Strabo.

About three quarters of that rough octagon is the Meseta Central, a vast plateau ranging from 610 to 760 m in altitude. It is located approximately in the centre, staggered slightly to the east and tilted slightly toward the west (the conventional centre of the Iberian Peninsula has long been considered Getafe just south of Madrid). It is ringed by mountains and contains the sources of most of the rivers, which find their way through gaps in the mountain barriers on all sides.

The coastline of the Iberian Peninsula is , on the Mediterranean side and on the Atlantic side. The coast has been inundated over time, with sea levels having risen from a minimum of lower than today at the Last Glacial Maximum (LGM) to its current level at 4,000 years BP. The coastal shelf created by sedimentation during that time remains below the surface; however, it was never very extensive on the Atlantic side, as the continental shelf drops rather steeply into the depths. An estimated length of Atlantic shelf is only wide. At the isobath, on the edge, the shelf drops off to .

The submarine topography of the coastal waters of the Iberian Peninsula has been studied extensively in the process of drilling for oil. Ultimately, the shelf drops into the Bay of Biscay on the north (an abyss), the Iberian abyssal plain at on the west, and Tagus abyssal plain to the south. In the north, between the continental shelf and the abyss, is an extension called the Galicia Bank, a plateau that also contains the Porto, Vigo, and Vasco da Gama seamounts, which form the Galicia interior basin. The southern border of these features is marked by Nazaré Canyon, which splits the continental shelf and leads directly into the abyss.

The major rivers flow through the wide valleys between the mountain systems. These are the Ebro, Douro, Tagus, Guadiana and Guadalquivir. All rivers in the Iberian Peninsula are subject to seasonal variations in flow.

The Tagus is the longest river on the peninsula and, like the Douro, flows westwards with its lower course in Portugal. The Guadiana river bends southwards and forms the border between Spain and Portugal in the last stretch of its course.

The terrain of the Iberian Peninsula is largely mountainous. The major mountain systems are:


The Iberian Peninsula contains rocks of every geological period from the Ediacaran to the Recent, and almost every kind of rock is represented. World-class mineral deposits can also be found there. The core of the Iberian Peninsula consists of a Hercynian cratonic block known as the Iberian Massif. On the northeast, this is bounded by the Pyrenean fold belt, and on the southeast it is bounded by the Baetic System. These twofold chains are part of the Alpine belt. To the west, the peninsula is delimited by the continental boundary formed by the magma-poor opening of the Atlantic Ocean. The Hercynian Foldbelt is mostly buried by Mesozoic and Tertiary cover rocks to the east, but nevertheless outcrops through the Sistema Ibérico and the Catalan Mediterranean System.

The Iberian Peninsula features one of the largest Lithium deposits belts in Europe (an otherwise relatively scarce resource in the continent), scattered along the Iberian Massif's and . Also in the Iberian Massif, and similarly to other Hercynian blocks in Europe, the peninsula hosts some uranium deposits, largely located in the Central Iberian Zone unit.

The Iberian Pyrite Belt, located in the SW quadrant of the Peninsula, ranks among the most important volcanogenic massive sulphide districts on Earth, and it has been exploited for millennia.

The Iberian Peninsula's location and topography, as well as the effects of large atmospheric circulation patterns induce a NW to SE gradient of yearly precipitation (roughly from 2,000 mm to 300 mm).

The Iberian peninsula has two dominant climate types. One of these is the oceanic climate seen in the Atlantic coastal region resulting in evenly temperatures with relatively cool summers. However, most of Portugal and Spain have a Mediterranean climate with various precipitation and temperatures depending on latitude and position versus the sea. There are also more localized semi-arid climates in central Spain, with temperatures resembling a more continental Mediterranean climate. In other extreme cases highland alpine climates such as in Sierra Nevada and areas with extremely low precipitation and desert climates or semi-arid climates such as the Almería area, Murcia area and southern Alicante area. In the Spanish and Portuguese interior the hottest temperatures in Europe are found, with Córdoba averaging around in July. The Spanish Mediterranean coast usually averages around in summer. In sharp contrast A Coruña at the northern tip of Galicia has a summer daytime high average at just below . This cool and wet summer climate is replicated throughout most of the northern coastline. Winter temperatures are more consistent throughout the peninsula, although frosts are common in the Spanish interior, even though daytime highs are usually above the freezing point. In Portugal, the warmest winters of the country are found in the area of Algarve, very similar to the ones from Huelva in Spain, while most of the Portuguese Atlantic coast has fresh and humid winters, similar to Galicia.

The current political configuration of the Iberian Peninsula now comprises the bulk of Spain and Portugal, the whole microstate of Andorra, a small part of the French department of Pyrénées-Orientales (the French Cerdagne) and the British Overseas Territory of Gibraltar.

French Cerdagne is on the south side of the Pyrenees mountain range, which runs along the border between Spain and France. For example, the Segre river, which runs west and then south to meet the Ebro, has its source on the French side. The Pyrenees range is often considered the northeastern boundary of Iberian Peninsula, although the French coastline converges away from the rest of Europe north of the range.

Regarding Spain and Portugal, this chiefly excludes the Macaronesian archipelagos (Azores and Madeira vis-à-vis Portugal and the Canary Islands vis-à-vis Spain), the Balearic Islands (Spain); and the Spanish territories in North Africa (most conspicuously the cities of Ceuta and Melilla), as well as unpopulated islets and rocks.

Political divisions of the Iberian Peninsula:

The Iberian city network is dominated by 3 international metropolises (Madrid, Barcelona and Lisbon) and four regional metropolises (Valencia, Seville, Porto and Bilbao). The relatively weak integration of the network favours a competitive approach vis-à-vis the inter-relation between the different centres. Among these metropolises, Madrid stands out within the global urban hierarchy in terms of its status as a major service centre and enjoys the greatest degree of connectivity.

According to Eurostat (2019), the metropolitan regions with a population over one million are listed as follows:

The woodlands of the Iberian Peninsula are distinct ecosystems. Although the various regions are each characterized by distinct vegetation, there are some similarities across the peninsula.

While the borders between these regions are not clearly defined, there is a mutual influence that makes it very hard to establish boundaries and some species find their optimal habitat in the intermediate areas.

The endangered Iberian lynx ("Lynx pardinus") is a symbol of the Iberian mediterranean forest and of the fauna of the Iberian Peninsula altogether.

The Iberian Peninsula is an important stopover on the East Atlantic flyway for birds migrating from northern Europe to Africa. For example, curlew sandpipers rest in the region of the Bay of Cádiz.

In addition to the birds migrating through, some seven million wading birds from the north spend the winter in the estuaries and wetlands of the Iberian Peninsula, mainly at locations on the Atlantic coast. In Galicia are Ría de Arousa (a home of grey plover), Ria de Ortigueira, Ria de Corme and Ria de Laxe. In Portugal, the Aveiro Lagoon hosts "Recurvirostra avosetta", the common ringed plover, grey plover and little stint. Ribatejo Province on the Tagus supports "Recurvirostra arosetta", grey plover, dunlin, bar-tailed godwit and common redshank. In the Sado Estuary are dunlin, Eurasian curlew, grey plover and common redshank. The Algarve hosts red knot, common greenshank and turnstone. The Guadalquivir Marshes region of Andalusia and the Salinas de Cádiz are especially rich in wintering wading birds: Kentish plover, common ringed plover, sanderling, and black-tailed godwit in addition to the others. And finally, the Ebro delta is home to all the species mentioned above.

With the sole exception of Basque, which is of unknown origin, all modern Iberian languages descend from Vulgar Latin and belong to the Western Romance languages. Throughout history (and pre-history), many different languages have been spoken in the Iberian Peninsula, contributing to the formation and differentiation of the contemporaneous languages of Iberia; however, most of them have become extinct or fallen into disuse. Basque is the only non-Indo-European surviving language in Iberia and Western Europe.

In modern times, Spanish (the official language of Spain, spoken by the entire 45 million population in the country, the native language of about 36 million in Europe), Portuguese (the official language of Portugal, with a population over 10 million), Catalan (over 7 million speakers in Europe, 3.4 million with Catalan as first language), Galician (understood by the 93% of the 1.5 million Galician population) and Basque (cf. around 1 million speakers) are the most widely spoken languages in the Iberian Peninsula. Spanish and Portuguese have expanded beyond Iberia to the rest of world, becoming global languages.

Other minority romance languages with some degree of recognition include the several varieties of Astur-leonese, collectively amounting to about 0.6 million speakers, and the Aragonese (barely spoken by the 8% of the 130,000 people inhabiting the Alto Aragón).

Both Spain and Portugal have traditionally used a non-standard rail gauge (the 1,668 mm Iberian gauge) since the construction of the first railroads in the 19th century. Spain has progressively introduced the 1,435 mm standard gauge in its new high-speed rail network (one the most extensive in the world), inaugurated in 1992 with the Madrid–Seville line, followed to name a few by the Madrid–Barcelona (2008), Madrid–Valencia (2010), an Alicante branch of the latter (2013) and the connection to France of the Barcelona line. Portugal however suspended all the high-speed rail projects in the wake of the 2008 financial crisis, putting an end for the time being to the possibility of a high-speed rail connection between Lisbon, Porto and Madrid.

Handicapped by a mountainous range (the Pyrenees) hindering the connection to the rest of Europe, Spain (and subsidiarily Portugal) only has two meaningful rail connections to France able for freight transport, located at both ends of the mountain range. An international rail line across the Central Pyrenees linking Zaragoza and the French city of Pau through a tunnel existed in the past; however, an accident in the French part destroyed a stretch of the railroad in 1970 and the Canfranc Station has been a cul-de-sac since then.

There are four points connecting the Portuguese and Spanish rail networks: Valença do Minho–Tui, Vilar Formoso–Fuentes de Oñoro, Marvão-Beirã–Valencia de Alcántara and Elvas–Badajoz.

The prospect of the development (as part of a European-wide effort) of the Central, Mediterranean and Atlantic rail corridors is expected to be a way to improve the competitiveness of the ports of Tarragona, Valencia, Sagunto, Bilbao, Santander, Sines and Algeciras vis-à-vis the rest of Europe and the World.

In 1980, Morocco and Spain started a joint study on the feasibility of a fixed link (tunnel or bridge) across the Strait of Gibraltar, possibly through a connection of with Cape Malabata. Years of studies have, however, made no real progress thus far.

A transit point for many submarine cables, the Fibre-optic Link Around the Globe, Europe India Gateway, and the SEA-ME-WE 3 feature landing stations in the Iberian Peninsula. The West Africa Cable System, Main One, SAT-3/WASC, Africa Coast to Europe also land in Portugal. MAREA, a high capacity communication transatlantic cable, connects the north of the Iberian Peninsula (Bilbao) to North America (Virginia), while EllaLink is an upcoming high-capacity communication cable expected to connect the Peninsula (Sines) to South America and the mammoth 2Africa project intends to connect the peninsula to the United Kingdom, Europe and Africa (via Portugal and Barcelona) by 2023–24.

Two gas pipelines: the Pedro Duran Farell pipeline and (more recently) the Medgaz (from, respectively, Morocco and Algeria) link the Maghreb and the Iberian Peninsula, providing Spain with Algerian natural gas.

Major industries include mining, tourism, small farms, and fishing. Because the coast is so long, fishing is popular, especially sardines, tuna and anchovies. Most of the mining occurs in the Pyrenees mountains. Commodities mined include: iron, gold, coal, lead, silver, zinc, and salt.

Regarding their role in the global economy, both the microstate of Andorra and the British Overseas Territory of Gibraltar have been described as tax havens.

The Galician region of Spain, in the north-west of the Iberian Peninsula, became one of the biggest entry points of cocaine in Europe, on a par with the Dutch ports. Hashish is smuggled from Morocco via the Strait of Gibraltar.




</doc>
<doc id="14884" url="https://en.wikipedia.org/wiki?curid=14884" title="Intermediate value theorem">
Intermediate value theorem

In mathematical analysis, the intermediate value theorem states that if "f" is a continuous function whose domain contains the interval ["a", "b"], then it takes on any given value between "f"("a") and "f"("b") at some point within the interval.

This has two important corollaries: 


This captures an intuitive property of continuous functions over the real numbers: given "f" continuous on [1, 2] with the known values "f"(1) = 3 and "f"(2) = 5. Then the graph of "y" = "f"("x") must pass through the horizontal line "y" = 4 while "x" moves from 1 to 2. It represents the idea that the graph of a continuous function on a closed interval can be drawn without lifting a pencil from the paper.

The intermediate value theorem states the following:

Consider an interval formula_1 in the real numbers formula_2 and a continuous function formula_3. Then



Remark: "Version II" states that the set of function values has no gap. For any two function values formula_11, even if they are outside the interval between formula_5 and formula_6, all points in the interval formula_14 are also function values,
A subset of the real numbers with no internal gap is an interval. "Version I" is naturally contained in "Version II".

The theorem depends on, and is equivalent to, the completeness of the real numbers. The intermediate value theorem does not apply to the rational numbers ℚ because gaps exist between rational numbers; irrational numbers fill those gaps. For example, the function formula_16 for formula_17 satisfies formula_18 and formula_19. However, there is no rational number formula_20 such that formula_21, because formula_22 is an irrational number.

The theorem may be proven as a consequence of the completeness property of the real numbers as follows:

We shall prove the first case, formula_23 be the set of all formula_24 such that formula_25. Then formula_26 is non-empty since formula_27 is an element of formula_26, and formula_26 is bounded above by formula_30. Hence, by completeness, the supremum formula_31 exists. That is, formula_32 is the smallest number that is greater than or equal to every member of formula_26. We claim that formula_8.

Fix some formula_35. Since formula_36 is continuous, there is a formula_37 such that formula_38 whenever formula_39. This means that
for all formula_41. By the properties of the supremum, there exists some formula_42 that is contained in formula_26, and so
Picking formula_45, we know that formula_46 because formula_32 is the supremum of formula_26. This means that

Both inequalities
are valid for all formula_35, from which we deduce formula_8 as the only possible value, as stated.

Remark: The intermediate value theorem can also be proved using the methods of non-standard analysis, which places "intuitive" arguments involving infinitesimals on a rigorous footing.

For "u" = 0 above, the statement is also known as "Bolzano's theorem." (Since there is nothing special about "u"=0, this is obviously equivalent to the intermediate value theorem itself.) This theorem was first proved by Bernard Bolzano in 1817. Augustin-Louis Cauchy provided a proof in 1821. Both were inspired by the goal of formalizing the analysis of functions and the work of Joseph-Louis Lagrange. The idea that continuous functions possess the intermediate value property has an earlier origin. Simon Stevin proved the intermediate value theorem for polynomials (using a cubic as an example) by providing an algorithm for constructing the decimal expansion of the solution. The algorithm iteratively subdivides the interval into 10 parts, producing an additional decimal digit at each step of the iteration. Before the formal definition of continuity was given, the intermediate value property was given as part of the definition of a continuous function. Proponents include Louis Arbogast, who assumed the functions to have no jumps, satisfy the intermediate value property and have increments whose sizes corresponded to the sizes of the increments of the variable.
Earlier authors held the result to be intuitively obvious and requiring no proof. The insight of Bolzano and Cauchy was to define a general notion of continuity (in terms of infinitesimals in Cauchy's case and using real inequalities in Bolzano's case), and to provide a proof based on such definitions.

The intermediate value theorem is closely linked to the topological notion of connectedness and follows from the basic properties of connected sets in metric spaces and connected subsets of ℝ in particular:

In fact, connectedness is a topological property and (*) generalizes to topological spaces: "If formula_53 and formula_54 are topological spaces, formula_55 is a continuous map, and formula_53 is a connected space, then formula_64 is connected." The preservation of connectedness under continuous maps can be thought of as a generalization of the intermediate value theorem, a property of real valued functions of a real variable, to continuous functions in general spaces.

Recall the first version of the intermediate value theorem, stated previously:

Intermediate value theorem. "(Version I). Consider a closed interval formula_1 in the real numbers formula_2 and a continuous function formula_3. Then, if formula_68 is a real number such that formula_69, there exists formula_70 such that formula_8."

The intermediate value theorem is an immediate consequence of these two properties of connectedness:

Proof: By (**), formula_1 is a connected set. It follows from (*) that the image, formula_9, is also connected. For convenience, assume that formula_74. Then once more invoking (**), formula_75, or formula_8 for some formula_77. Since formula_78, formula_70 must actually hold, and the desired conclusion follows. The same argument applies if formula_80, so we are done.formula_81

The intermediate value theorem generalizes in a natural way: Suppose that "X" is a connected topological space and ("Y", <) is a totally ordered set equipped with the order topology, and let "f" : "X" → "Y" be a continuous map. If "a" and "b" are two points in "X" and "u" is a point in "Y" lying between "f"("a") and "f"("b") with respect to <, then there exists "c" in "X" such that "f"("c") = "u". The original theorem is recovered by noting that ℝ is connected and that its natural topology is the order topology.

The Brouwer fixed-point theorem is a related theorem that, in one dimension, gives a special case of the intermediate value theorem.

A Darboux function is a real-valued function "f" that has the "intermediate value property," i.e., that satisfies the conclusion of the intermediate value theorem: for any two values "a" and "b" in the domain of "f", and any "y" between "f"("a") and "f"("b"), there is some "c" between "a" and "b" with "f"("c") = "y". The intermediate value theorem says that every continuous function is a Darboux function. However, not every Darboux function is continuous; i.e., the converse of the intermediate value theorem is false.

As an example, take the function "f" : [0, ∞) → [−1, 1] defined by "f"("x") = sin(1/"x") for "x" > 0 and f(0) = 0. This function is not continuous at "x" = 0 because the limit of "f"("x") as "x" tends to 0 does not exist; yet the function has the intermediate value property. Another, more complicated example is given by the Conway base 13 function.

In fact, Darboux's theorem states that all functions that result from the differentiation of some other function on some interval have the intermediate value property (even though they need not be continuous).

Historically, this intermediate value property has been suggested as a definition for continuity of real-valued functions; this definition was not adopted.

A similar result is the Borsuk–Ulam theorem, which says that a continuous map from the formula_82-sphere to Euclidean formula_82-space will always map some pair of antipodal points to the same place.

"Proof for 1-dimensional case:" Take formula_36 to be any continuous function on a circle. Draw a line through the center of the circle, intersecting it at two opposite points formula_85 and formula_86. Define formula_87 to be formula_88. If the line is rotated 180 degrees, the value −"d" will be obtained instead. Due to the intermediate value theorem there must be some intermediate rotation angle for which "d" = 0, and as a consequence "f"("A") = "f"("B") at this angle.

In general, for any continuous function whose domain is some closed convex formula_82-dimensional shape and any point inside the shape (not necessarily its center), there exist two antipodal points with respect to the given point whose functional value is the same.

The theorem also underpins the explanation of why rotating a wobbly table will bring it to stability (subject to certain easily met constraints).




</doc>
<doc id="14889" url="https://en.wikipedia.org/wiki?curid=14889" title="Iran–Iraq War">
Iran–Iraq War

The Iran–Iraq War (; ; “First Gulf War”) was a protracted armed conflict that began on 22 September 1980 when Iran was invaded by neighbouring Iraq. The war lasted almost eight years, ending in a stalemate on 20 August 1988 when Iran accepted a UN-brokered ceasefire. Iraq's rationale for the invasion was primarily to cripple Iran and prevent Ayatollah Ruhollah Khomeini from exporting the 1979 Iranian Revolution movement to Shia-majority Iraq and threaten the Sunni-dominated Ba'athist leadership. Iraq had also wished to replace Iran as the dominant state in the Persian Gulf, which had not been previously possible due to pre-revolutionary Iran's goliath status in both economic and military terms as well as its close alliances with the United States and Israel. The war followed a long-running history of border disputes, as a result of which Iraq had planned to annex Iran's oil-rich Khuzestan Province and the east bank of the Shatt al-Arab (Arvand Rud).

Although Iraq hoped to take advantage of Iran's post-revolutionary chaos and expected a decisive victory in the face of a weakened Iran, the Iraqi military only made progress for three months, and by December 1980 the invasion had stalled. As fierce fighting broke out between the two sides, the Iranian military started to gain momentum against the Iraqis and regained virtually all lost territory by June 1982, pushing the Iraqis back to the pre-war border lines. Following this, the next five years saw Iran go on the offensive until Iraq took back the initiative in mid-1988, and whose major offensives led to the final conclusion of the war. There were a number of proxy forces operating for both countries—most notably the People's Mujahedin of Iran, which had sided with Iraq and the Iraqi Kurdish militias of the KDP and PUK, which had sided with Iran. The United States, United Kingdom, Soviet Union, France, and most Arab countries provided large-scale political and logistic support for Iraq, while Iran was largely isolated.

After eight years of war-exhaustion, economic devastation, decreased morale, military stalemate, lack of international sympathy against the use of weapons of mass destruction against Iranian civilians by Iraqi forces, and increased U.S.–Iran military tensions all led to a ceasefire brokered by the United Nations.

The conflict has been compared to World War I in terms of the tactics used, including large-scale trench warfare with barbed wire stretched across fortified defensive lines, manned machine gun posts, bayonet charges, Iranian human wave attacks, extensive use of chemical weapons by Iraq, and, later, deliberate attacks on civilian targets. A special feature of the war can be seen in the Iranian cult of the martyr which had been developed in the years before the revolution. The discourses on martyrdom formulated in the Iranian Shia context led to the tactics of "human wave attacks" and thus had a lasting impact on the dynamics of the war.

An estimated 500,000 Iraqi and Iranian soldiers died, in addition to a smaller number of civilians. The end of the war resulted in neither reparations nor border changes.

The Iran–Iraq War was originally referred to as the "Persian Gulf War" until the Persian Gulf War of 1990 and 1991, after which it was known as the "First Persian Gulf War". The Iraq–Kuwait conflict, which was known as the "Second Persian Gulf War," eventually became known simply as the "Persian Gulf War". The Iraq War from 2003 to 2011 has been called the "Second Persian Gulf War".

In Iran, the war is known as the "Imposed War" ( ') and the "Holy Defense" ( '). State media in Iraq dubbed the war "Saddam's Qadisiyyah" (, ""), in reference to the seventh-century Battle of al-Qādisiyyah, in which Arab warriors overcame the Sasanian Empire during the Muslim conquest of Iran.

In April 1969, Iran abrogated the 1937 treaty over the Shatt al-Arab and Iranian ships stopped paying tolls to Iraq when they used the Shatt al-Arab. The Shah argued that the 1937 treaty was unfair to Iran because almost all river borders around the world ran along the "thalweg", and because most of the ships that used the Shatt al-Arab were Iranian. Iraq threatened war over the Iranian move, but on 24 April 1969, an Iranian tanker escorted by Iranian warships (Joint Operation Arvand) sailed down the Shatt al-Arab, and Iraq—being the militarily weaker state—did nothing. The Iranian abrogation of the 1937 treaty marked the beginning of a period of acute Iraqi-Iranian tension that was to last until the Algiers Accords of 1975.

The relationship between the governments of Iran and Iraq briefly improved in 1978, when Iranian agents in Iraq discovered plans for a pro-Soviet "coup d'état" against Iraq's government. When informed of this plot, Saddam ordered the execution of dozens of his army's officers, and in a sign of reconciliation, expelled from Iraq Ruhollah Khomeini, an exiled leader of clerical opposition to the Shah. Nonetheless, Saddam considered the 1975 Algiers Agreement to be merely a truce, rather than a definite settlement, and waited for an opportunity to contest it.

Tensions between Iraq and Iran were fuelled by Iran's Islamic revolution and its appearance of being a Pan-Islamic force, in contrast to Iraq's Arab nationalism. Despite Iraq's goal of regaining the Shatt al-Arab, the Iraqi government initially seemed to welcome the Iranian Revolution, which overthrew Shah Mohammad Reza Pahlavi, who was seen as a common enemy. It is difficult to pinpoint when tensions began to build, but there were frequent cross-border skirmishes, largely at Iran's instigation.

Ayatollah Ruhollah Khomeini called on Iraqis to overthrow the Ba'ath government, which was received with considerable anger in Baghdad. On 17 July 1979, despite Khomeini's call, Saddam gave a speech praising the Iranian Revolution and called for an Iraqi-Iranian friendship based on non-interference in each other's internal affairs. When Khomeini rejected Saddam's overture by calling for Islamic revolution in Iraq, Saddam was alarmed. Iran's new Islamic administration was regarded in Baghdad as an irrational, existential threat to the Ba'ath government, especially because the Ba'ath party, having a secular nature, discriminated against and posed a threat to the fundamentalist Shia movement in Iraq, whose clerics were Iran's allies within Iraq and whom Khomeini saw as oppressed.

Saddam's primary interest in war may have also stemmed from his desire to right the supposed "wrong" of the Algiers Agreement, in addition to finally achieving his desire of annexing Khuzestan and becoming the regional superpower. Saddam's goal was to replace Egypt as the "leader of the Arab world" and to achieve hegemony over the Persian Gulf. He saw Iran's increased weakness due to revolution, sanctions, and international isolation. Saddam had invested heavily in Iraq's military since his defeat against Iran in 1975, buying large amounts of weaponry from the Soviet Union, France and Britain. By 1980, Iraq possessed 200,000 soldiers, 2,000 tanks and 450 aircraft. Watching the disintegration of the powerful Iranian army that frustrated him in 1974–1975, he saw an opportunity to attack, using the threat of Islamic Revolution as a pretext.

On 8 March 1980, Iran announced it was withdrawing its ambassador from Iraq, downgraded its diplomatic ties to the charge d'affaires level, and demanded that Iraq do the same. The following day, Iraq declared Iran's ambassador persona non-grata, and demanded his withdrawal from Iraq by 15 March. Iraq soon after expropriated the properties of 70,000 civilians believed to be of Iranian origin and expelled them from its territory. Many, if not most, of those expelled were in fact Arabic-speaking Iraqi Shias who had little to no family ties with Iran. This caused tensions between the two nations to increase further.

Iraq began planning offensives, confident that they would succeed. Iran lacked both cohesive leadership and spare parts for their American-made and British-made equipment. The Iraqis could mobilise up to 12 mechanised divisions, and morale was running high. 

In addition, the area around the Shatt al-Arab posed no obstacle for the Iraqis, as they possessed river crossing equipment. Iraq correctly deduced that Iran's defences at the crossing points around the Karkheh and Karoun Rivers were undermanned and that the rivers could be easily crossed. Iraqi intelligence was also informed that the Iranian forces in Khuzestan (which consisted of two divisions prior to the revolution) now only consisted of several ill-equipped and under-strength battalions. Only a handful of company-sized tank units remained operational.

The only qualms the Iraqis had were over the Islamic Republic of Iran Air Force (formerly the Imperial Iranian Air Force). Despite the purge of several key pilots and commanders, as well as the lack of spare parts, the air force showed its power during local uprisings and rebellions. They were also active after the failed U.S. attempt to rescue its hostages, Operation Eagle Claw. Based on these observations, Iraq's leaders decided to carry out a surprise airstrike against the Iranian air force's infrastructure prior to the main invasion.

In Iran, severe officer purges (including numerous executions ordered by Sadegh Khalkhali, the new Revolutionary Court judge), and shortages of spare parts for Iran's U.S.-made and British-made equipment had crippled Iran's once-mighty military. Between February and September 1979, Iran's government executed 85 senior generals and forced all major-generals and most brigadier-generals into early retirement.

By September 1980, the government had purged 12,000 army officers. These purges resulted in a drastic decline in the Iranian military's operational capacities. Their regular army (which, in 1978, was considered the world's fifth most powerful) had been badly weakened. The desertion rate had reached 60%, and the officer corps was devastated. The most highly skilled soldiers and aviators were exiled, imprisoned, or executed. Throughout the war, Iran never managed to fully recover from this flight of human capital.

Continuous sanctions prevented Iran from acquiring many heavy weapons, such as tanks and aircraft. When the invasion occurred, many pilots and officers were released from prison, or had their executions commuted to combat the Iraqis. In addition, many junior officers were promoted to generals, resulting in the army being more integrated as a part of the regime by the war's end, as it is today. Iran still had at least 1,000 operational tanks and several hundred functional aircraft, and could cannibalize equipment to procure spare parts.

Meanwhile, a new paramilitary organisation gained prominence in Iran, the Islamic Revolutionary Guard Corps (often shortened to "Revolutionary Guards", and known in Iran as the "Sepah-e-Pasdaran"). This was intended to protect the new regime and counterbalance the army, which was seen as less loyal. Despite having been trained as a paramilitary organisation, after the Iraqi invasion, they were forced to act as a regular army. Initially, they refused to fight alongside the army, which resulted in many defeats, but by 1982, the two groups began carrying out combined operations.

Another paramilitary militia was founded in response to the invasion, the "Army of 20 Million", commonly known as the Basij. The Basij were poorly armed and had members as young as 12 and as old as 70. They often acted in conjunction with the Revolutionary Guard, launching so-called human wave attacks and other campaigns against the Iraqis. They were subordinate to the Revolutionary Guards, and they made up most of the manpower that was used in the Revolutionary Guard's attacks.

Stephen Pelletiere wrote in his 1992 book "The Iran–Iraq War: Chaos in a Vacuum":

The most important dispute was over the Shatt al-Arab waterway. Iran repudiated the demarcation line established in the Anglo-Ottoman Convention of Constantinople of November 1913. Iran asked the border to run along the thalweg, the deepest point of the navigable channel. Iraq, encouraged by Britain, took Iran to the League of Nations in 1934, but their disagreement was not resolved. Finally in 1937 Iran and Iraq signed their first boundary treaty. The treaty established the waterway border on the eastern bank of the river except for a anchorage zone near Abadan, which was allotted to Iran and where the border ran along the thalweg. Iran sent a delegation to Iraq soon after the Ba'ath coup in 1969 and, when Iraq refused to proceed with negotiations over a new treaty, the treaty of 1937 was withdrawn by Iran. The Iranian abrogation of the 1937 treaty marked the beginning of a period of acute Iraqi-Iranian tension that was to last until the Algiers Accords of 1975.

The 1974-75 Shatt al-Arab clashes were a previous Iranian-Iraqi standoff in the region of the Shatt al-Arab waterway during the mid-1970s. Nearly 1,000 were killed in the clashes. It was the most significant dispute over the Shatt al-Arab waterway in modern times, prior to the Iran–Iraq War.

Five years later, on 17 September 1980, Iraq suddenly abrogated the Algiers Protocol following the Iranian revolution. Saddam Hussein claimed that the Islamic Republic of Iran refused to abide by the stipulations of the Algiers Protocol and, therefore, Iraq considered the Protocol null and void. Five days later, the Iraqi army crossed the border.

Iraq launched a full-scale invasion of Iran on 22 September 1980. The Iraqi Air Force launched surprise air strikes on ten Iranian airfields with the objective of destroying the Iranian Air Force. The attack failed to damage the Iranian Air Force significantly; it damaged some of Iran's airbase infrastructure, but failed to destroy a significant number of aircraft. The Iraqi Air Force was only able to strike in depth with a few MiG-23BN, Tu-22, and Su-20 aircraft, and Iran had built hardened aircraft shelters where most of its combat aircraft were stored.

The next day, Iraq launched a ground invasion along a front measuring in three simultaneous attacks. The invasion's purpose, according to Saddam, was to blunt the edge of Khomeini's movement and to thwart his attempts to export his Islamic revolution to Iraq and the Persian Gulf states. Saddam hoped that by annexing Khuzestan, he would cause such a blow to Iran's prestige that it would lead to the new government's downfall, or at least end Iran's calls for his overthrow.

Of Iraq's six divisions that invaded by ground, four were sent to Khuzestan, which was located near the border's southern end, to cut off the Shatt al-Arab from the rest of Iran and to establish a territorial security zone. The other two divisions invaded across the northern and central part of the border to prevent an Iranian counter-attack. Two of the four Iraqi divisions, one mechanised and one armoured, operated near the southern end and began a siege of the strategically important port cities of Abadan and Khorramshahr.

The two armoured divisions secured the territory bounded by the cities of Khorramshahr, Ahvaz, Susangerd, and Musian. On the central front, the Iraqis occupied Mehran, advanced towards the foothills of the Zagros Mountains, and were able to block the traditional Tehran–Baghdad invasion route by securing territory forward of Qasr-e Shirin, Iran. On the northern front, the Iraqis attempted to establish a strong defensive position opposite Suleimaniya to protect the Iraqi Kirkuk oil complex. Iraqi hopes of an uprising by the ethnic Arabs of Khuzestan failed to materialise, as most of the ethnic Arabs remained loyal to Iran. The Iraqi troops advancing into Iran in 1980 were described by Patrick Brogan as "badly led and lacking in offensive spirit". The first known chemical weapons attack by Iraq on Iran probably took place during the fighting around Susangerd.

Though the Iraqi air invasion surprised the Iranians, the Iranian air force retaliated the day after with a large-scale attack against Iraqi air bases and infrastructure in Operation Kaman 99. Groups of F-4 Phantom and F-5 Tiger fighter jets attacked targets throughout Iraq, such as oil facilities, dams, petrochemical plants, and oil refineries, and included Mosul Airbase, Baghdad, and the Kirkuk oil refinery. Iraq was taken by surprise at the strength of the retaliation, which caused the Iraqis heavy losses and economic disruption, but the Iranians took heavy losses as well as they lost many aircraft and aircrews to Iraqi air defenses.

Iranian Army Aviation's AH-1 Cobra helicopter gunships began attacks on the advancing Iraqi divisions, along with F-4 Phantoms armed with Maverick missiles; they destroyed numerous armoured vehicles and impeded the Iraqi advance, though not completely halting it. Meanwhile, Iraqi air attacks on Iran were repelled by Iran's F-14 Tomcat interceptor fighter jets, using Phoenix missiles, which downed a dozen of Iraq's Soviet-built fighters in the first two days of battle. 

The Iranian regular military, police forces, volunteer Basij, and Revolutionary Guards all conducted their operations separately; thus, the Iraqi invading forces did not face coordinated resistance. However, on 24 September, the Iranian Navy attacked Basra, Iraq, destroying two oil terminals near the Iraqi port Faw, which reduced Iraq's ability to export oil. The Iranian ground forces (primarily consisting of the Revolutionary Guard) retreated to the cities, where they set up defences against the invaders.

On 30 September, Iran's air force launched Operation Scorch Sword, striking and badly damaging the nearly-complete Osirak nuclear reactor near Baghdad. By 1 October, Baghdad had been subjected to eight air attacks. In response, Iraq launched aerial strikes against Iranian targets.

The mountainous border between Iran and Iraq made a deep ground invasion almost impossible, and air strikes were used instead. The invasion's first waves were a series of air strikes targeted at Iranian airfields. Iraq also attempted to bomb Tehran, Iran's capital and command centre, into submission.

On 22 September, a prolonged battle began in the city of Khorramshahr, eventually leaving 7,000 dead on each side. Reflecting the bloody nature of the struggle, Iranians came to call Khorramshahr "City of Blood".

The battle began with Iraqi air raids against key points and mechanised divisions advancing on the city in a crescent-like formation. They were slowed by Iranian air attacks and Revolutionary Guard troops with recoilless rifles, rocket-propelled grenades, and Molotov cocktails. The Iranians flooded the marsh areas around the city, forcing the Iraqis to traverse through narrow strips of land. Iraqi tanks launched attacks with no infantry support, and many tanks were lost to Iranian anti-tank teams. However, by 30 September, the Iraqis had managed to clear the Iranians from the outskirts of the city. The next day, the Iraqis launched infantry and armoured attacks into the city. After heavy house-to-house fighting, the Iraqis were repelled. On 14 October, the Iraqis launched a second offensive. The Iranians launched a controlled withdrawal from the city, street by street. By 24 October, most of the city was captured, and the Iranians evacuated across the Karun River. Some partisans remained, and fighting continued until 10 November.

The people of Iran, rather than turning against their still-weak Islamic Republic, rallied around their country. An estimated 200,000 fresh troops had arrived at the front by November, many of them ideologically committed volunteers.

Though Khorramshahr was finally captured, the battle had delayed the Iraqis enough to allow the large-scale deployment of the Iranian military. In November, Saddam ordered his forces to advance towards Dezful and Ahvaz, and lay sieges to both cities. However, the Iraqi offensive had been badly damaged by Iranian militias and air power. Iran's air force had destroyed Iraq's army supply depots and fuel supplies, and was strangling the country through an aerial siege. Iran's supplies had not been exhausted, despite sanctions, and the military often cannibalised spare parts from other equipment and began searching for parts on the black market. On 28 November, Iran launched Operation Morvarid (Pearl), a combined air and sea attack which destroyed 80% of Iraq's navy and all of its radar sites in the southern portion of the country. When Iraq laid siege to Abadan and dug its troops in around the city, it was unable to blockade the port, which allowed Iran to resupply Abadan by sea.

Iraq's strategic reserves had been depleted, and by now it lacked the power to go on any major offensives until nearly the end of the war. On 7 December, Hussein announced that Iraq was going on the defensive. By the end of 1980, Iraq had destroyed about 500 Western-built Iranian tanks and captured 100 others.

For the next eight months, both sides were on a defensive footing (with the exception of the Battle of Dezful), as the Iranians needed more time to reorganise their forces after the damage inflicted by the purge of 1979–80. During this period, fighting consisted mainly of artillery duels and raids. Iraq had mobilised 21 divisions for the invasion, while Iran countered with only 13 regular army divisions and one brigade. Of the regular divisions, only seven were deployed to the border. The war bogged down into World War I-style trench warfare with tanks and modern late-20th century weapons. Due to the power of anti-tank weapons such as the RPG-7, armored manoeuvre by the Iraqis was very costly, and they consequently entrenched their tanks into static positions.

Iraq also began firing Scud missiles into Dezful and Ahvaz, and used terror bombing to bring the war to the Iranian civilian population. Iran launched dozens of "human wave assaults".

On 5 January 1981, Iran had reorganised its forces enough to launch a large-scale offensive, Operation Nasr (Victory). The Iranians launched their major armoured offensive from Dezful in the direction of Susangerd, consisting of tank brigades from the 16th "Qazvin", 77th "Khorasan", and 92nd Khuzestan Armoured Divisions, and broke through Iraqi lines. However, the Iranian tanks had raced through Iraqi lines with their flanks unprotected and with no infantry support; as a result, they were cut off by Iraqi tanks. In the ensuing Battle of Dezful, the Iranian armoured divisions were nearly wiped out in one of the biggest tank battles of the war. When the Iranian tanks tried to manoeuvre, they became stuck in the mud of the marshes, and many tanks were abandoned. The Iraqis lost 45 T-55 and T-62 tanks, while the Iranians lost 100–200 Chieftain and M-60 tanks. Reporters counted roughly 150 destroyed or deserted Iranian tanks, and also 40 Iraqi tanks. 141 Iranians were killed during the battle.

The battle had been ordered by Iranian president Abulhassan Banisadr, who was hoping that a victory might shore up his deteriorating political position; instead, the failure hastened his fall. Many of Iran's problems took place because of political infighting between President Banisadr, who supported the regular army, and the hardliners who supported the IRGC. Once he was impeached and the competition ended, the performance of the Iranian military improved. 

Iran was further distracted by internal fighting between the regime and the Islamic Marxist "Mujaheddin e-Khalq" (MEK) on the streets of Iran's major cities in June 1981 and again in September. After the end of these battles, the MEK gradually leaned towards Saddam, completely taking his side by the mid-1980s. In 1986, Rajavi moved from Paris to Iraq and set up a base on the Iranian border. The Battle of Dezful became a critical battle in Iranian military thinking. Less emphasis was placed on the Army with its conventional tactics, and more emphasis was placed on the Revolutionary Guard with its unconventional tactics.

The Iraqi Air Force, badly damaged by the Iranians, was moved to the H-3 Airbase in Western Iraq, near the Jordanian border and away from Iran. However, on 3 April 1981, the Iranian air force used eight F-4 Phantom fighter bombers, four F-14 Tomcats, three Boeing 707 refuelling tankers, and one Boeing 747 command plane to launch a surprise attack on H3, destroying 27–50 Iraqi fighter jets and bombers.

Despite the successful H-3 airbase attack (in addition to other air attacks), the Iranian Air Force was forced to cancel its successful 180-day air offensive. In addition, they abandoned their attempted control of Iranian airspace. They had been seriously weakened by sanctions and pre-war purges and further damaged by a fresh purge after the impeachment crisis of President Banisadr. The Iranian Air Force could not survive further attrition, and decided to limit their losses, abandoning efforts to control Iranian airspace. The Iranian air force would henceforth fight on the defensive, trying to deter the Iraqis rather than engaging them. While throughout 1981–1982 the Iraqi air force would remain weak, within the next few years they would rearm and expand again, and begin to regain the strategic initiative.

The Iranians suffered from a shortage of heavy weapons, but had a large number of devoted volunteer troops, so they began using human wave attacks against the Iraqis. Typically, an Iranian assault would commence with poorly trained Basij who would launch the primary human wave assaults to swamp the weakest portions of the Iraqi lines en masse (on some occasions even bodily clearing minefields). This would be followed up by the more experienced Revolutionary Guard infantry, who would breach the weakened Iraqi lines, and followed up by the regular army using mechanized forces, who would maneuver through the breach and attempt to encircle and defeat the enemy.

According to historian Stephen C. Pelletiere, the idea of Iranian "human wave attacks" was a misconception. Instead, the Iranian tactics consisted of using groups of 22 man infantry squads, which moved forward to attack specific objectives. As the squads surged forward to execute their missions, that gave the impression of a "human wave attack". Nevertheless, the idea of "human wave attacks" remained virtually synonymous with any large-scale infantry frontal assault Iran carried out. Large numbers of troops would be used, aimed at overwhelming the Iraqi lines (usually the weakest portion, typically manned by the Iraqi Popular Army), regardless of losses.

According to the former Iraqi general Ra'ad al-Hamdani, the Iranian human wave charges consisted of armed "civilians" who carried most of their necessary equipment themselves into battle and often lacked command and control and logistics. Operations were often carried out during the night and deception operations, infiltrations, and maneuvers became more common. The Iranians would also reinforce the infiltrating forces with new units to keep up their momentum. Once a weak point was found, the Iranians would concentrate all of their forces into that area in an attempt to break through with human wave attacks.

The human wave attacks, while extremely bloody (tens of thousands of troops died in the process), when used in combination with infiltration and surprise, caused major Iraqi defeats. As the Iraqis would dig in their tanks and infantry into static, entrenched positions, the Iranians would manage to break through the lines and encircle entire divisions. Merely the fact that the Iranian forces used maneuver warfare by their light infantry against static Iraqi defenses was often the decisive factor in battle. However, lack of coordination between the Iranian Army and IRGC and shortages of heavy weaponry played a detrimental role, often with most of the infantry not being supported by artillery and armor.

After the Iraqi offensive stalled in March 1981, there was little change in the front other than Iran retaking the high ground above Susangerd in May. By late 1981, Iran returned to the offensive and launched a new operation (Operation Samen-ol-A'emeh (The Eighth Imam)), ending the Iraqi Siege of Abadan on 27–29 September 1981. The Iranians used a combined force of regular army artillery with small groups of armor, supported by Pasdaran (IRGC) and Basij infantry. On 15 October, after breaking the siege, a large Iranian convoy was ambushed by Iraqi tanks, and during the ensuing tank battle Iran lost 20 Chieftains and other armored vehicles and withdrew from the previously gained territory.

On 29 November 1981, Iran began Operation Tariq al-Qods with three army brigades and seven Revolutionary Guard brigades. The Iraqis failed to properly patrol their occupied areas, and the Iranians constructed a road through the unguarded sand dunes, launching their attack from the Iraqi rear. The town of Bostan was retaken from Iraqi divisions by 7 December. By this time the Iraqi Army was experiencing serious morale problems, compounded by the fact that Operation Tariq al-Qods marked the first use of Iranian "human wave" tactics, where the Revolutionary Guard light infantry repeatedly charged at Iraqi positions, oftentimes without the support of armour or air power. The fall of Bostan exacerbated the Iraqis' logistical problems, forcing them to use a roundabout route from Ahvaz to the south to resupply their troops. 6,000 Iranians and over 2,000 Iraqis were killed in the operation.

The Iraqis, realising that the Iranians were planning to attack, decided to preempt them with Operation al-Fawz al-'Azim (Supreme Success) on 19 March. Using a large number of tanks, helicopters, and fighter jets, they attacked the Iranian buildup around the Roghabiyeh pass. Though Saddam and his generals assumed they had succeeded, in reality the Iranian forces remained fully intact. The Iranians had concentrated much of their forces by bringing them directly from the cities and towns throughout Iran via trains, buses, and private cars. The concentration of forces did not resemble a traditional military buildup, and although the Iraqis detected a population buildup near the front, they failed to realize that this was an attacking force. As a result, Saddam's army was unprepared for the Iranian offensives to come.

Iran's next major offensive, led by then Colonel Ali Sayad Shirazi , was Operation Undeniable Victory. On 22 March 1982, Iran launched an attack which took the Iraqi forces by surprise: using Chinook helicopters, they landed behind Iraqi lines, silenced their artillery, and captured an Iraqi headquarters. The Iranian Basij then launched "human wave" attacks, consisting of 1,000 fighters per wave. Though they took heavy losses, they eventually broke through Iraqi lines.

The Revolutionary Guard and regular army followed up by surrounding the Iraqi 9th and 10th Armoured and 1st Mechanised Divisions that had camped close to the Iranian town of Shush. The Iraqis launched a counter-attack using their 12th Armoured division to break the encirclement and rescue the surrounded divisions. Iraqi tanks came under attack by 95 Iranian F-4 Phantom and F-5 Tiger fighter jets, destroying much of the division.

Operation Undeniable Victory was an Iranian victory; Iraqi forces were driven away from Shush, Dezful and Ahvaz. The Iranian armed forces destroyed 320–400 Iraqi tanks and armored vehicles in a costly success. In just the first day of the battle, the Iranians lost 196 tanks. By this time, most of the Khuzestan province had been recaptured.

In preparation for Operation Beit ol-Moqaddas, the Iranians had launched numerous air raids against Iraq air bases, destroying 47 jets (including Iraq's brand new Mirage F-1 fighter jets from France); this gave the Iranians air superiority over the battlefield while allowing them to monitor Iraqi troop movements.

On 29 April, Iran launched the offensive. 70,000 Revolutionary Guard and Basij members struck on several axes – Bostan, Susangerd, the west bank of the Karun River, and Ahvaz. The Basij launched human wave attacks, which were followed up by the regular army and Revolutionary Guard support along with tanks and helicopters. Under heavy Iranian pressure, the Iraqi forces retreated. By 12 May, Iran had driven out all Iraqi forces from the Susangerd area. The Iranians captured several thousand Iraqi troops and a large number of tanks. Nevertheless, the Iranians took many losses as well, especially among the Basij.

The Iraqis retreated to the Karun River, with only Khorramshahr and a few outlying areas remaining in their possession. Saddam ordered 70,000 troops to be placed around the city of Khorramshahr. The Iraqis created a hastily constructed defence line around the city and outlying areas. To discourage airborne commando landings, the Iraqis also placed metal spikes and destroyed cars in areas likely to be used as troop landing zones. Saddam Hussein even visited Khorramshahr in a dramatic gesture, swearing that the city would never be relinquished. However, Khorramshahr's only resupply point was across the Shatt al-Arab, and the Iranian air force began bombing the supply bridges to the city, while their artillery zeroed in on the besieged garrison.

In the early morning hours of 23 May 1982, the Iranians began the drive towards Khorramshahr across the Karun River. This part of Operation Beit ol-Moqaddas was spearheaded by the 77th Khorasan division with tanks along with the Revolutionary Guard and Basij. The Iranians hit the Iraqis with destructive air strikes and massive artillery barrages, crossed the Karun River, captured bridgeheads, and launched human wave attacks towards the city. Saddam's defensive barricade collapsed; in less than 48 hours of fighting, the city fell and 19,000 Iraqis surrendered to the Iranians. A total of 10,000 Iraqis were killed or wounded in Khorramshahr, while the Iranians suffered 30,000 casualties. During the whole of Operation Beit ol-Moqaddas, 33,000 Iraqi soldiers were captured by the Iranians.

The fighting had battered the Iraqi military: its strength fell from 210,000 to 150,000 troops; over 20,000 Iraqi soldiers were killed and over 30,000 captured; two out of four active armoured divisions and at least three mechanised divisions fell to less than a brigade's strength; and the Iranians had captured over 450 tanks and armoured personnel carriers.

The Iraqi Air Force was also left in poor shape: after losing up to 55 aircraft since early December 1981, they had only 100 intact fighter-bombers and interceptors. A defector who flew his MiG-21 to Syria in June 1982 revealed that the Iraqi Air Force had only three squadrons of fighter-bombers capable of mounting operations into Iran. The Iraqi Army Air Corps was in slightly better shape, and could still operate more than 70 helicopters. Despite this, the Iraqis still held 3,000 tanks, while Iran held 1,000.

At this point, Saddam believed that his army was too demoralised and damaged to hold onto Khuzestan and major swathes of Iranian territory, and withdrew his remaining forces, redeploying them in defence along the border. However, his troops continued to occupy some key Iranian border areas of Iran, including the disputed territories that prompted his invasion, notably the Shatt al-Arab waterway. In response to their failures against the Iranians in Khorramshahr, Saddam ordered the executions of Generals Juwad Shitnah and Salah al-Qadhi and Colonels Masa and al-Jalil. At least a dozen other high-ranking officers were also executed during this time. This became an increasingly common punishment for those who failed him in battle.

In April 1982, the rival Ba'athist regime in Syria, one of the few nations that supported Iran, closed the Kirkuk–Baniyas pipeline that had allowed Iraqi oil to reach tankers on the Mediterranean, reducing the Iraqi budget by $5 billion per month. Journalist Patrick Brogan wrote, "It appeared for a while that Iraq would be strangled economically before it was defeated militarily." Syria's closure of the Kirkuk–Baniyas pipeline left Iraq with the pipeline to Turkey as the only means of exporting oil. However, that pipeline had a capacity of only , which was insufficient to pay for the war. However, Saudi Arabia, Kuwait, and the other Gulf states saved Iraq from bankruptcy by providing it with an average of $60 billion in subsidies per year. Though Iraq had previously been hostile towards other Gulf states, "the threat of Persian fundamentalism was far more feared." They were especially inclined to fear Iranian victory after Ayatollah Khomeini declared monarchies to be illegitimate and an un-Islamic form of government. Khomeini's statement was widely received as a call to overthrow the Gulf monarchies. Journalists John Bulloch and Harvey Morris wrote:

The virulent Iranian campaign, which at its peak seemed to be making the overthrow of the Saudi regime a war aim on a par with the defeat of Iraq, did have an effect on the Kingdom [of Saudi Arabia], but not the one the Iranians wanted: instead of becoming more conciliatory, the Saudis became tougher, more self-confident, and less prone to seek compromise.

Saudi Arabia was said to provide Iraq with $1 billion per month starting in mid-1982.

Iraq began receiving support from the United States and west European countries as well. Saddam was given diplomatic, monetary, and military support by the United States, including massive loans, political influence, and intelligence on Iranian deployments gathered by American spy satellites. The Iraqis relied heavily on American satellite footage and radar planes to detect Iranian troop movements, and they enabled Iraq to move troops to the site before the battle.

With Iranian success on the battlefield, the United States increased its support of the Iraqi government, supplying intelligence, economic aid, and dual-use equipment and vehicles, as well as normalizing its intergovernmental relations (which had been broken during the 1967 Six-Day War). President Ronald Reagan decided that the United States "could not afford to allow Iraq to lose the war to Iran", and that the United States "would do whatever was necessary to prevent Iraq from losing". Reagan formalised this policy by issuing a National Security Decision Directive to this effect in June 1982.

In 1982, Reagan removed Iraq from the list of countries "supporting terrorism" and sold weapons such as howitzers to Iraq via Jordan. France sold Iraq millions of dollars worth of weapons, including Gazelle helicopters, Mirage F-1 fighters, and Exocet missiles. Both the United States and West Germany sold Iraq dual-use pesticides and poisons that would be used to create chemical and other weapons, such as Roland missiles.

At the same time, the Soviet Union, angered with Iran for purging and destroying the communist Tudeh Party, sent large shipments of weapons to Iraq. The Iraqi Air Force was replenished with Soviet, Chinese, and French fighter jets and attack/transport helicopters. Iraq also replenished their stocks of small arms and anti-tank weapons such as AK-47s and rocket-propelled grenades from its supporters. The depleted tank forces were replenished with more Soviet and Chinese tanks, and the Iraqis were reinvigorated in the face of the coming Iranian onslaught. Iran was portrayed as the aggressor, and would be seen as such until the 1990–1991 Persian Gulf War, when Iraq would be condemned.

Iran did not have the money to purchase arms to the same extent as Iraq did. They counted on China, North Korea, Libya, Syria, and Japan for supplying anything from weapons and munitions to logistical and engineering equipment.

On 20 June 1982, Saddam announced that he wanted to sue for peace and proposed an immediate ceasefire and withdrawal from Iranian territory within two weeks. Khomeini responded by saying the war would not end until a new government was installed in Iraq and reparations paid. He proclaimed that Iran would invade Iraq and would not stop until the Ba'ath regime was replaced by an Islamic republic. Iran supported a government in exile for Iraq, the Supreme Council of the Islamic Revolution in Iraq, led by exiled Iraqi cleric Mohammad Baqer al-Hakim, which was dedicated to overthrowing the Ba'ath party. They recruited POW's, dissidents, exiles, and Shias to join the Badr Brigade, the military wing of the organisation.

The decision to invade Iraq was taken after much debate within the Iranian government. One faction, comprising Prime Minister Mir-Hossein Mousavi, Foreign Minister Ali Akbar Velayati, President Ali Khamenei, Army Chief of Staff General Ali Sayad Shirazi as well as Major General Qasem-Ali Zahirnejad, wanted to accept the ceasefire, as most of Iranian soil had been recaptured. In particular, General Shirazi and Zahirnejad were both opposed to the invasion of Iraq on logistical grounds, and stated they would consider resigning if "unqualified people continued to meddle with the conduct of the war". Of the opposing view was a hardline faction led by the clerics on the Supreme Defence Council, whose leader was the politically powerful speaker of the "Majlis", Akbar Hashemi Rafsanjani.

Iran also hoped that their attacks would ignite a revolt against Saddam's rule by the Shia and Kurdish population of Iraq, possibly resulting in his downfall. They were successful in doing so with the Kurdish population, but not the Shia. Iran had captured large quantities of Iraqi equipment (enough to create several tank battalions, Iran once again had 1,000 tanks) and also managed to clandestinely procure spare parts as well.

At a cabinet meeting in Baghdad, Minister of Health Riyadh Ibrahim Hussein suggested that Saddam could step down temporarily as a way of easing Iran towards a ceasefire, and then afterwards would come back to power. Saddam, annoyed, asked if anyone else in the Cabinet agreed with the Health Minister's idea. When no one raised their hand in support, he escorted Riyadh Hussein to the next room, closed the door, and shot him with his pistol. Saddam returned to the room and continued with his meeting.

For the most part, Iraq remained on the defensive for the next five years, unable and unwilling to launch any major offensives, while Iran launched more than 70 offensives. Iraq's strategy changed from holding territory in Iran to denying Iran any major gains in Iraq (as well as holding onto disputed territories along the border). Saddam commenced a policy of total war, gearing most of his country towards defending against Iran. By 1988, Iraq was spending 40–75% of its GDP on military equipment. Saddam had also more than doubled the size of the Iraqi army, from 200,000 soldiers (12 divisions and three independent brigades) to 500,000 (23 divisions and nine brigades). Iraq also began launching air raids against Iranian border cities, greatly increasing the practice by 1984. By the end of 1982, Iraq had been resupplied with new Soviet and Chinese materiel, and the ground war entered a new phase. Iraq used newly acquired T-55, T-62 and T-72 tanks (as well as Chinese copies), BM-21 truck-mounted rocket launchers, and Mi-24 helicopter gunships to prepare a Soviet-type three-line defence, replete with obstacles such as barbed wire, minefields, fortified positions and bunkers. The Combat Engineer Corps built bridges across water obstacles, laid minefields, erected earthen revetments, dug trenches, built machinegun nests, and prepared new defence lines and fortifications.

Iraq began to focus on using defense in depth to defeat the Iranians. Iraq created multiple static defense lines to bleed the Iranians through sheer size. When faced against large Iranian attack, where human waves would overrun Iraq's forward entrenched infantry defences, the Iraqis would often retreat, but their static defences would bleed the Iranians and channel them into certain directions, drawing them into traps or pockets. Iraqi air and artillery attacks would then pin the Iranians down, while tanks and mechanised infantry attacks using mobile warfare would push them back. Sometimes, the Iraqis would launch "probing attacks" into the Iranian lines to provoke them into launching their attacks sooner. While Iranian human wave attacks were successful against the dug in Iraqi forces in Khuzestan, they had trouble breaking through Iraq's defense in depth lines. Iraq had a logistical advantage in their defence: the front was located near the main Iraqi bases and arms depots, allowing their army to be efficiently supplied. By contrast, the front in Iran was a considerable distance away from the main Iranian bases and arms depots, and as such, Iranian troops and supplies had to travel through mountain ranges before arriving at the front.

In addition, Iran's military power was weakened once again by large purges in 1982, resulting from another supposedly attempted coup.

The Iranian generals wanted to launch an all-out attack on Baghdad and seize it before the weapon shortages continued to manifest further. Instead, that was rejected as being unfeasible, and the decision was made to capture one area of Iraq after the other in the hopes that a series of blows delivered foremost by the Revolutionary Guards Corps would force a political solution to the war (including Iraq withdrawing completely from the disputed territories along the border).

The Iranians planned their attack in southern Iraq, near Basra. Called Operation Ramadan, it involved over 180,000 troops from both sides, and was one of the largest land battles since World War II. Iranian strategy dictated that they launch their primary attack on the weakest point of the Iraqi lines; however, the Iraqis were informed of Iran's battle plans and moved all of their forces to the area the Iranians planned to attack. The Iraqis were equipped with tear gas to use against the enemy, which would be the first major use of chemical warfare during the conflict, throwing an entire attacking division into chaos.

Over 100,000 Revolutionary Guards and Basij volunteer forces charged towards the Iraqi lines. The Iraqi troops had entrenched themselves in formidable defences, and had set up a network of bunkers and artillery positions. The Basij used human waves, and were even used to bodily clear the Iraqi minefields and allow the Revolutionary Guards to advance. Combatants came so close to one another that Iranians were able to board Iraqi tanks and throw grenades inside the hulls. By the eighth day, the Iranians had gained inside Iraq and had taken several causeways. Iran's Revolutionary Guards also used the T-55 tanks they had captured in earlier battles.

However, the attacks came to a halt and the Iranians turned to defensive measures. Seeing this, Iraq used their Mi-25 helicopters, along with Gazelle helicopters armed with Euromissile HOT, against columns of Iranian mechanised infantry and tanks. These "hunter-killer" teams of helicopters, which had been formed with the help of East German advisors, proved to be very costly for the Iranians. Aerial dogfights occurred between Iraqi MiGs and Iranian F-4 Phantoms.

On 16 July, Iran tried again further north and managed to push the Iraqis back. However, only from Basra, the poorly equipped Iranian forces were surrounded on three sides by Iraqis with heavy weaponry. Some were captured, while many were killed. Only a last-minute attack by Iranian AH-1 Cobra helicopters stopped the Iraqis from routing the Iranians. Three more similar attacks occurred around the Khorramshar-Baghdad road area towards the end of the month, but none were significantly successful. Iraq had concentrated three armoured divisions, the 3rd, 9th, and 10th, as a counter-attack force to attack any penetrations. They were successful in defeating the Iranian breakthroughs, but suffered heavy losses. The 9th Armoured Division in particular had to be disbanded, and was never reformed. The total casualty toll had grown to include 80,000 soldiers and civilians. 400 Iranian tanks and armored vehicles were destroyed or abandoned, while Iraq lost no fewer than 370 tanks.

After Iran's failure in Operation Ramadan, they carried out only a few smaller attacks. Iran launched two limited offensives aimed at reclaiming the Sumar Hills and isolating the Iraqi pocket at Naft Shahr at the international border, both of which were part of the disputed territories still under Iraqi occupation. They then aimed to capture the Iraqi border town of Mandali. They planned to take the Iraqis by surprise using Basij militiamen, army helicopters, and some armoured forces, then stretch their defences and possibly break through them to open a road to Baghdad for future exploitation. During Operation Muslim ibn Aqil (1–7 October), Iran recovered of disputed territory straddling the international border and reached the outskirts of Mandali before being stopped by Iraqi helicopter and armoured attacks. During Operation Muharram (1–21 November), the Iranians captured part of the Bayat oilfield with the help of their fighter jets and helicopters, destroying 105 Iraqi tanks, 70 APCs, and 7 planes with few losses. They nearly breached the Iraqi lines but failed to capture Mandali after the Iraqis sent reinforcements, including brand new T-72 tanks, which possessed armour that could not be pierced from the front by Iranian TOW missiles. The Iranian advance was also impeded by heavy rains. 3,500 Iraqis and an unknown number of Iranians died, with only minor gains for Iran.

After the failure of the 1982 summer offensives, Iran believed that a major effort along the entire breadth of the front would yield victory. During the course of 1983, the Iranians launched five major assaults along the front, though none achieved substantial success, as the Iranians staged more massive "human wave" attacks. By this time, it was estimated that no more than 70 Iranian fighter aircraft were still operational at any given time; Iran had its own helicopter repair facilities, left over from before the revolution, and thus often used helicopters for close air support. Iranian fighter pilots had superior training compared to their Iraqi counterparts (as most had received training from US officers before the 1979 revolution) and would continue to dominate in combat. However, aircraft shortages, the size of defended territory/airspace, and American intelligence supplied to Iraq allowed the Iraqis to exploit gaps in Iranian airspace. Iraqi air campaigns met little opposition, striking over half of Iran, as the Iraqis were able to gain air superiority towards the end of the war.

In Operation Before the Dawn, launched 6 February 1983, the Iranians shifted focus from the southern to the central and northern sectors. Employing 200,000 "last reserve" Revolutionary Guard troops, Iran attacked along a stretch near al-Amarah, Iraq, about southeast of Baghdad, in an attempt to reach the highways connecting northern and southern Iraq. The attack was stalled by of hilly escarpments, forests, and river torrents blanketing the way to al-Amarah, but the Iraqis could not force the Iranians back. Iran directed artillery on Basra, Al Amarah, and Mandali.

The Iranians suffered a large number of casualties clearing minefields and breaching Iraqi anti-tank mines, which Iraqi engineers were unable to replace. After this battle, Iran reduced its use of human wave attacks, though they still remained a key tactic as the war went on.

Further Iranian attacks were mounted in the Mandali–Baghdad north-central sector in April 1983, but were repelled by Iraqi mechanised and infantry divisions. Casualties were high, and by the end of 1983, an estimated 120,000 Iranians and 60,000 Iraqis had been killed. Iran, however, held the advantage in the war of attrition.

From early 1983–1984, Iran launched a series of four "Valfajr" (Dawn) Operations (that eventually numbered to 10). During Operation Dawn-1, in early February 1983, 50,000 Iranian forces attacked westward from Dezful and were confronted by 55,000 Iraqi forces. The Iranian objective was to cut off the road from Basra to Baghdad in the central sector. The Iraqis carried out 150 air sorties against the Iranians, and even bombed Dezful, Ahvaz, and Khorramshahr in retribution. The Iraqi counterattack was broken up by Iran's 92nd Armoured Division.

During Operation Dawn-2, the Iranians directed insurgency operations by proxy in April 1983 by supporting the Kurds in the north. With Kurdish support, the Iranians attacked on 23 July 1983, capturing the Iraqi town of Haj Omran and maintaining it against an Iraqi poison gas counteroffensive. This operation incited Iraq to later conduct indiscriminate chemical attacks against the Kurds. The Iranians attempted to further exploit activities in the north on 30 July 1983, during Operation Dawn-3. Iran saw an opportunity to sweep away Iraqi forces controlling the roads between the Iranian mountain border towns of Mehran, Dehloran and Elam. Iraq launched airstrikes, and equipped attack helicopters with chemical warheads; while ineffective, it demonstrated both the Iraqi general staff's and Saddam's increasing interest in using chemical weapons. In the end, 17,000 had been killed on both sides, with no gain for either country.

The focus of Operation Dawn-4 in September 1983 was the northern sector in Iranian Kurdistan. Three Iranian regular divisions, the Revolutionary Guard, and Kurdistan Democratic Party (KDP) elements amassed in Marivan and Sardasht in a move to threaten the major Iraqi city Suleimaniyah. Iran's strategy was to press Kurdish tribes to occupy the Banjuin Valley, which was within of Suleimaniyah and from the oilfields of Kirkuk. To stem the tide, Iraq deployed Mi-8 attack helicopters equipped with chemical weapons and executed 120 sorties against the Iranian force, which stopped them into Iraqi territory. 5,000 Iranians and 2,500 Iraqis died. Iran gained of its territory back in the north, gained of Iraqi land, and captured 1,800 Iraqi prisoners while Iraq abandoned large quantities of valuable weapons and war materiel in the field. Iraq responded to these losses by firing a series of SCUD-B missiles into the cities of Dezful, Masjid Soleiman, and Behbehan. Iran's use of artillery against Basra while the battles in the north raged created multiple fronts, which effectively confused and wore down Iraq.

Previously, the Iranians had outnumbered the Iraqis on the battlefield, but Iraq expanded their military draft (pursuing a policy of total war), and by 1984, the armies were equal in size. By 1986, Iraq had twice as many soldiers as Iran. By 1988, Iraq would have 1 million soldiers, giving it the fourth largest army in the world. Some of their equipment, such as tanks, outnumbered the Iranians' by at least five to one. Iranian commanders, however, remained more tactically skilled.

After the Dawn Operations, Iran attempted to change tactics. In the face of increasing Iraqi defense in depth, as well as increased armaments and manpower, Iran could no longer rely on simple human wave attacks. Iranian offensives became more complex and involved extensive maneuver warfare using primarily light infantry. Iran launched frequent, and sometimes smaller offensives to slowly gain ground and deplete the Iraqis through attrition. They wanted to drive Iraq into economic failure by wasting money on weapons and war mobilization, and to deplete their smaller population by bleeding them dry, in addition to creating an anti-government insurgency (they were successful in Kurdistan, but not southern Iraq). Iran also supported their attacks with heavy weaponry when possible and with better planning (although the brunt of the battles still fell to the infantry). The Army and Revolutionary Guards worked together better as their tactics improved. Human wave attacks became less frequent (although still used). To negate the Iraqi advantage of defense in depth, static positions, and heavy firepower, Iran began to focus on fighting in areas where the Iraqis could not use their heavy weaponry, such as marshes, valleys, and mountains, and frequently using infiltration tactics.

Iran began training troops in infiltration, patrolling, night-fighting, marsh warfare, and mountain warfare. They also began training thousands of Revolutionary Guard commandos in amphibious warfare, as southern Iraq is marshy and filled with wetlands. Iran used speedboats to cross the marshes and rivers in southern Iraq and landed troops on the opposing banks, where they would dig and set up pontoon bridges across the rivers and wetlands to allow heavy troops and supplies to cross. Iran also learned to integrate foreign guerrilla units as part of their military operations. On the northern front, Iran began working heavily with the Peshmerga, Kurdish guerrillas. Iranian military advisors organised the Kurds into raiding parties of 12 guerrillas, which would attack Iraqi command posts, troop formations, infrastructure (including roads and supply lines), and government buildings. The oil refineries of Kirkuk became a favourite target, and were often hit by homemade Peshmerga rockets.

By 1984, the Iranian ground forces were reorganised well enough for the Revolutionary Guard to start Operation Kheibar, which lasted from 24 February to 19 March. On 15 February 1984, the Iranians began launching attacks against the central section of the front, where the Second Iraqi Army Corps was deployed: 250,000 Iraqis faced 250,000 Iranians. The goal of this new major offensive was the capture of Basra-Baghdad Highway, cutting off Basra from Baghdad and setting the stage for an eventual attack upon the city. The Iraqi high command had assumed that the marshlands above Basra were natural barriers to attack, and had not reinforced them. The marshes negated Iraqi advantage in armor, and absorbed artillery rounds and bombs.
Prior to the attack, Iranian commandos on helicopters had landed behind Iraqi lines and destroyed Iraqi artillery. Iran launched two preliminary attacks prior to the main offensive, Operation Dawn 5 and Dawn 6. They saw the Iranians attempting to capture Kut al-Imara, Iraq and sever the highway connecting Baghdad to Basra, which would impede Iraqi coordination of supplies and defences. Iranian troops crossed the river on motorboats in a surprise attack, though only came within of the highway.

Operation Kheibar began on 24 February with Iranian infantrymen crossing the Hawizeh Marshes using motorboats and transport helicopters in an amphibious assault. The Iranians attacked the vital oil-producing Majnoon Island by landing troops via helicopters onto the islands and severing the communication lines between Amareh and Basra. They then continued the attack towards Qurna. By 27 February, they had captured the island, but suffered catastrophic helicopter losses to the IrAF. On that day, a massive array of Iranian helicopters transporting Pasdaran troops were intercepted by Iraqi combat aircraft (MiGs, Mirages and Sukhois). In what was essentially an aerial slaughter, Iraqi jets shot down 49 of the 50 Iranian helicopters. At times, fighting took place in waters over deep. Iraq ran live electrical cables through the water, electrocuting numerous Iranian troops and then displaying their corpses on state television.

By 29 February, the Iranians had reached the outskirts of Qurna and were closing in on the Baghdad–Basra highway. They had broken out of the marshes and returned to open terrain, where they were confronted by conventional Iraqi weapons, including artillery, tanks, air power, and mustard gas. 1,200 Iranian soldiers were killed in the counter-attack. The Iranians retreated back to the marshes, though they still held onto them along with Majnoon Island.

The Battle of the Marshes saw an Iraqi defence that had been under continuous strain since 15 February; they were relieved by their use of chemical weapons and defence-in-depth, where they layered defensive lines: even if the Iranians broke through the first line, they were usually unable to break through the second due to exhaustion and heavy losses. They also largely relied on Mi-24 Hind to "hunt" the Iranian troops in the marshes, and at least 20,000 Iranians were killed in the marsh battles. Iran used the marshes as a springboard for future attacks/infiltrations.

Four years into the war, the human cost to Iran had been 170,000 combat fatalities and 340,000 wounded. Iraqi combat fatalities were estimated at 80,000 with 150,000 wounded.

Unable to launch successful ground attacks against Iran, Iraq used their now expanded air force to carry out strategic bombing against Iranian shipping, economic targets, and cities in order to damage Iran's economy and morale. Iraq also wanted to provoke Iran into doing something that would cause the superpowers to be directly involved in the conflict on the Iraqi side.

The so-called "Tanker War" started when Iraq attacked the oil terminal and oil tankers at Kharg Island in early 1984. Iraq's aim in attacking Iranian shipping was to provoke the Iranians to retaliate with extreme measures, such as closing the Strait of Hormuz to all maritime traffic, thereby bringing American intervention; the United States had threatened several times to intervene if the Strait of Hormuz were closed. As a result, the Iranians limited their retaliatory attacks to Iraqi shipping, leaving the strait open to general passage.

Iraq declared that all ships going to or from Iranian ports in the northern zone of the Persian Gulf were subject to attack. They used F-1 Mirage, Super Etendard, Mig-23, Su-20/22, and Super Frelon helicopters armed with Exocet anti-ship missiles as well as Soviet-made air-to-surface missiles to enforce their threats. Iraq repeatedly bombed Iran's main oil export facility on Kharg Island, causing increasingly heavy damage. As a first response to these attacks, Iran attacked a Kuwaiti tanker carrying Iraqi oil near Bahrain on 13 May 1984, as well as a Saudi tanker in Saudi waters on 16 May. Because Iraq had become landlocked during the course of the war, they had to rely on their Arab allies, primarily Kuwait, to transport their oil. Iran attacked tankers carrying Iraqi oil from Kuwait, later attacking tankers from any Persian Gulf state supporting Iraq. Attacks on ships of noncombatant nations in the Persian Gulf sharply increased thereafter, with both nations attacking oil tankers and merchant ships of neutral nations in an effort to deprive their opponent of trade. The Iranian attacks against Saudi shipping led to Saudi F-15s shooting down a pair of F-4 Phantom II on 5 June 1984.

The air and small-boat attacks, however, did little damage to Persian Gulf state economies, and Iran moved its shipping port to Larak Island in the Strait of Hormuz.

The Iranian Navy imposed a naval blockade of Iraq, using its British-built frigates to stop and inspect any ships thought to be trading with Iraq. They operated with virtual impunity, as Iraqi pilots had little training in hitting naval targets. Some Iranian warships attacked tankers with ship-to-ship missiles, while others used their radars to guide land-based anti-ship missiles to their targets. Iran began to rely on its new Revolutionary Guard's navy, which used Boghammar speedboats fitted with rocket launchers and heavy machine guns. These speedboats would launch surprise attacks against tankers and cause substantial damage. Iran also used F-4 Phantoms II and helicopters to launch Maverick missiles and unguided rockets at tankers.

A U.S. Navy ship, , was struck on 17 May 1987 by two Exocet anti-ship missiles fired from an Iraqi F-1 Mirage plane. The missiles had been fired at about the time the plane was given a routine radio warning by "Stark". The frigate did not detect the missiles with radar, and warning was given by the lookout only moments before they struck. Both missiles hit the ship, and one exploded in crew quarters, killing 37 sailors and wounding 21.

Lloyd's of London, a British insurance market, estimated that the Tanker War damaged 546 commercial vessels and killed about 430 civilian sailors. The largest portion of the attacks was directed by Iraq against vessels in Iranian waters, with the Iraqis launching three times as many attacks as the Iranians. But Iranian speedboat attacks on Kuwaiti shipping led Kuwait to formally petition foreign powers on 1 November 1986 to protect its shipping. The Soviet Union agreed to charter tankers starting in 1987, and the United States Navy offered to provide protection for foreign tankers reflagged and flying the U.S. flag starting 7 March 1987 in Operation Earnest Will. Neutral tankers shipping to Iran were unsurprisingly not protected by Earnest Will, resulting in reduced foreign tanker traffic to Iran, since they risked Iraqi air attack. Iran accused the United States of helping Iraq.

During the course of the war, Iran attacked two Soviet merchant ships.

"Seawise Giant", the largest ship ever built, was struck by Iraqi Exocet missiles as it was carrying Iranian crude oil out of the Persian Gulf.

Meanwhile, Iraq's air force also began carrying out strategic bombing raids against Iranian cities. While Iraq had launched numerous attacks with aircraft and missiles against border cities from the beginning of the war and sporadic raids on Iran's main cities, this was the first systematic strategic bombing that Iraq carried out during the war. This would become known as the "War of the Cities". With the help of the USSR and the west, Iraq's air force had been rebuilt and expanded. Meanwhile, Iran, due to sanctions and lack of spare parts, had heavily curtailed its air force operations. Iraq used Tu-22 Blinder and Tu-16 Badger strategic bombers to carry out long-range high-speed raids on Iranian cities, including Tehran. Fighter-bombers such as the Mig-25 Foxbat and Su-22 Fitter were used against smaller or shorter range targets, as well as escorting the strategic bombers. Civilian and industrial targets were hit by the raids, and each successful raid inflicted economic damage from regular strategic bombing.

In response, the Iranians deployed their F-4 Phantoms to combat the Iraqis, and eventually they deployed F-14s as well. Most of the Iraqi air raids were intercepted by the Iranian fighter jets and air defense, but some also successfully hit their targets, becoming a major headache for Iran. By 1986, Iran also expanded their air defense network heavily to relieve the pressure on the air force. By later in the war, Iraqi raids primarily consisted of indiscriminate missile attacks while air attacks were used only on fewer, more important targets. Starting in 1987, Saddam also ordered several chemical attacks on civilian targets in Iran, such as the town of Sardasht.

Iran also launched several retaliatory air raids on Iraq, while primarily shelling border cities such as Basra. Iran also bought some Scud missiles from Libya, and launched them against Baghdad. These too inflicted damage upon Iraq.

On 7 February 1984, during the first war of the cities, Saddam ordered his air force to attack eleven Iranian cities; bombardments ceased on 22 February 1984. Though Saddam intended the attacks to demoralise Iran and force them to negotiate, they had little effect, and Iran quickly repaired the damage. Moreover, Iraq's air force took heavy losses and Iran struck back, hitting Baghdad and other Iraqi cities. The attacks resulted in tens of thousands of civilian casualties on both sides, and became known as the first "war of the cities". It was estimated that 1,200 Iranian civilians were killed during the raids in February alone. There would be five such major exchanges throughout the course of the war, and multiple minor ones. While interior cities such as Tehran, Tabriz, Qom, Isfahan and Shiraz received numerous raids, the cities of western Iran suffered the most.

By 1984, Iran's losses were estimated to be 300,000 soldiers, while Iraq's losses were estimated to be 150,000. Foreign analysts agreed that both Iran and Iraq failed to use their modern equipment properly, and both sides failed to carry out modern military assaults that could win the war. Both sides also abandoned equipment in the battlefield because their technicians were unable to carry out repairs. Iran and Iraq showed little internal coordination on the battlefield, and in many cases units were left to fight on their own. As a result, by the end of 1984, the war was a stalemate. One limited offensive Iran launched (Dawn 7) took place from 18–25 October 1984, when they recaptured the Iranian city of Mehran, which had been occupied by the Iraqis from the beginning of the war.

By 1985, Iraqi armed forces were receiving financial support from Saudi Arabia, Kuwait, and other Persian Gulf states, and were making substantial arms purchases from the Soviet Union, China, and France. For the first time since early 1980, Saddam launched new offensives.

On 6 January 1986, the Iraqis launched an offensive attempting to retake Majnoon Island. However, they were quickly bogged down into a stalemate against 200,000 Iranian infantrymen, reinforced by amphibious divisions. However, they managed to gain a foothold in the southern part of the island.

Iraq also carried out another "war of the cities" between 12–14 March, hitting up to 158 targets in over 30 towns and cities, including Tehran. Iran responded by launching 14 Scud missiles for the first time, purchased from Libya. More Iraqi air attacks were carried out in August, resulting in hundreds of additional civilian casualties. Iraqi attacks against both Iranian and neutral oil tankers in Iranian waters continued, with Iraq carrying out 150 airstrikes using French bought Super Etendard and Mirage F-1 jets as well as Super Frelon helicopters, armed with Exocet missiles.

The Iraqis attacked again on 28 January 1985; they were defeated, and the Iranians retaliated on 11 March 1985 with a major offensive directed against the Baghdad-Basra highway (one of the few major offensives conducted in 1985), codenamed Operation Badr (after the Battle of Badr, Muhammad's first military victory in Mecca). Ayatollah Khomeini urged Iranians on, declaring:

It is our belief that Saddam wishes to return Islam to blasphemy and polytheism...if America becomes victorious...and grants victory to Saddam, Islam will receive such a blow that it will not be able to raise its head for a long time...The issue is one of Islam versus blasphemy, and not of Iran versus Iraq.

This operation was similar to Operation Kheibar, though it invoked more planning. Iran used 100,000 troops, with 60,000 more in reserve. They assessed the marshy terrain, plotted points where they could land tanks, and constructed pontoon bridges across the marshes. The Basij forces were also equipped with anti-tank weapons.

The ferocity of the Iranian offensive broke through the Iraqi lines. The Revolutionary Guard, with the support of tanks and artillery, broke through north of Qurna on 14 March. That same night 3,000 Iranian troops reached and crossed the Tigris River using pontoon bridges and captured part of the Baghdad–Basra Highway 6, which they had failed to achieve in Operations Dawn 5 and 6.

Saddam responded by launching chemical attacks against the Iranian positions along the highway and by initiating the aforementioned second "war of the cities", with an air and missile campaign against twenty to thirty Iranian population centres, including Tehran. Under General Sultan Hashim Ahmad al-Tai and General Jamal Zanoun (both considered to be among Iraq's most skilled commanders), the Iraqis launched air attacks against the Iranian positions and pinned them down. They then launched a pincer attack using mechanized infantry and heavy artillery. Chemical weapons were used, and the Iraqis also flooded Iranian trenches with specially constructed pipes delivering water from the Tigris River.

The Iranians retreated back to the Hoveyzeh marshes while being attacked by helicopters, and the highway was recaptured by the Iraqis. Operation Badr resulted in 10,000–12,000 Iraqi casualties and 15,000 Iranian ones.

The failure of the human wave attacks in earlier years had prompted Iran to develop a better working relationship between the Army and the Revolutionary Guard and to mould the Revolutionary Guard units into a more conventional fighting force. To combat Iraq's use of chemical weapons, Iran began producing an antidote. They also created and fielded their own homemade drones, the Mohajer 1's, fitted with six RPG-7's to launch attacks. They were primarily used in observation, being used for up to 700 sorties.

For the rest of 1986, and until the spring of 1988, the Iranian Air Force's efficiency in air defence increased, with weapons being repaired or replaced and new tactical methods being used. For example, the Iranians would loosely integrate their SAM sites and interceptors to create "killing fields" in which dozens of Iraqi planes were lost (which was reported in the West as the Iranian Air Force using F-14s as "mini-AWACs"). The Iraqi Air Force reacted by increasing the sophistication of its equipment, incorporating modern electronic countermeasure pods, decoys such as chaff and flare, and anti-radiation missiles. Due to the heavy losses in the last war of the cities, Iraq reduced their use of aerial attacks on Iranian cities. Instead, they would launch Scud missiles, which the Iranians could not stop. Since the range of the Scud missile was too short to reach Tehran, they converted them to al-Hussein missiles with the help of East German engineers, cutting up their Scuds into three chunks and attaching them together. Iran responded to these attacks by using their own Scud missiles.

Compounding the extensive foreign help to Iraq, Iranian attacks were severely hampered by their shortages of weaponry, particularly heavy weapons as large amounts had been lost during the war. Iran still managed to maintain 1,000 tanks (often by capturing Iraqi ones) and additional artillery, but many needed repairs to be operational. However, by this time Iran managed to procure spare parts from various sources, helping them to restore some weapons. They secretly imported some weapons, such as RBS-70 anti-aircraft MANPADS. In an exception to the United States' support for Iraq, in exchange for Iran using its influence to help free western hostages in Lebanon, the United States secretly sold Iran some limited supplies (in Ayatollah Rafsanjani's postwar interview, he stated that during the period when Iran was succeeding, for a short time the United States supported Iran, then shortly after began helping Iraq again). Iran managed to get some advanced weapons, such as anti-tank TOW missiles, which worked better than rocket-propelled grenades. Iran later reverse-engineered and produced those weapons themselvesl. All of these almost certainly helped increase the effectiveness of Iran, although it did not reduce the human cost of their attacks.

On the night of 10–11 February 1986, the Iranians launched Operation Dawn 8, in which 30,000 troops comprising five Army divisions and men from the Revolutionary Guard and Basij advanced in a two-pronged offensive to capture the al-Faw peninsula in southern Iraq, the only area touching the Persian Gulf. The capture of Al Faw and Umm Qasr was a major goal for Iran. Iran began with a feint attack against Basra, which was stopped by the Iraqis; Meanwhile, an amphibious strike force landed at the foot of the peninsula. The resistance, consisting of several thousand poorly trained soldiers of the Iraqi Popular Army, fled or were defeated, and the Iranian forces set up pontoon bridges crossing the Shatt al-Arab, allowing 30,000 soldiers to cross in a short period of time. They drove north along the peninsula almost unopposed, capturing it after only 24 hours of fighting. Afterwards they dug in and set up defenses.

The sudden capture of al-Faw took the Iraqis by shock, since they had thought it impossible for the Iranians to cross the Shatt al-Arab. On 12 February 1986, the Iraqis began a counter-offensive to retake al-Faw, which failed after a week of heavy fighting. On 24 February 1986, Saddam sent one of his best commanders, General Maher Abd al-Rashid, and the Republican Guard to begin a new offensive to recapture al-Faw. A new round of heavy fighting took place. However, their attempts again ended in failure, costing them many tanks and aircraft: their 15th mechanised division was almost completely wiped out. The capture of al-Faw and the failure of the Iraqi counter-offensives were blows to the Ba'ath regime's prestige, and led the Gulf countries to fear that Iran might win the war. Kuwait in particular felt menaced with Iranian troops only away, and increased its support of Iraq accordingly.

In March 1986, the Iranians tried to follow up their success by attempting to take Umm Qasr, which would have completely severed Iraq from the Gulf and placed Iranian troops on the border with Kuwait. However, the offensive failed due to Iranian shortages of armor. By this time, 17,000 Iraqis and 30,000 Iranians were made casualties. The First Battle of al-Faw ended in March, but heavy combat operations lasted on the peninsula into 1988, with neither side being able to displace the other. The battle bogged down into a World War I-style stalemate in the marshes of the peninsula.

Immediately after the Iranian capture of al-Faw, Saddam declared a new offensive against Iran, designed to drive deep into the state. The Iranian border city of Mehran, on the foot of the Zagros Mountains, was selected as the first target. On 15–19 May, Iraqi Army's Second Corps, supported by helicopter gunships, attacked and captured the city. Saddam then offered the Iranians to exchange Mehran for al-Faw. The Iranians rejected the offer. Iraq then continued the attack, attempting to push deeper into Iran. However, Iraq's attack was quickly warded off by Iranian AH-1 Cobra helicopters with TOW missiles, which destroyed numerous Iraqi tanks and vehicles.

The Iranians built up their forces on the heights surrounding Mehran. On 30 June, using mountain warfare tactics they launched their attack, recapturing the city by 3 July. Saddam ordered the Republican Guard to retake the city on 4 July, but their attack was ineffective. Iraqi losses were heavy enough to allow the Iranians to also capture territory inside Iraq, and depleted the Iraqi military enough to prevent them from launching a major offensive for the next two years. Iraq's defeats at al-Faw and at Mehran were severe blows to the prestige of the Iraqi regime, and western powers, including the US, became more determined to prevent an Iraqi loss.

Through the eyes of international observers, Iran was prevailing in the war by the end of 1986. In the northern front, the Iranians began launching attacks toward the city of Suleimaniya with the help of Kurdish fighters, taking the Iraqis by surprise. They came within of the city before being stopped by chemical and army attacks. Iran's army had also reached the Meimak Hills, only from Baghdad. Iraq managed to contain Iran's offensives in the south, but was under serious pressure, as the Iranians were slowly overwhelming them.

Iraq responded by launching another "war of the cities". In one attack, Tehran's main oil refinery was hit, and in another instance, Iraq damaged Iran's Assadabad satellite dish, disrupting Iranian overseas telephone and telex service for almost two weeks. Civilian areas were also hit, resulting in many casualties. Iraq continued to attack oil tankers via air. Iran responded by launching Scud missiles and air attacks at Iraqi targets.

Iraq continued to attack Kharg Island and the oil tankers and facilities as well. Iran created a tanker shuttle service of 20 tankers to move oil from Kharg to Larak Island, escorted by Iranian fighter jets. Once moved to Larak, the oil would be moved to oceangoing tankers (usually neutral). They also rebuilt the oil terminals damaged by Iraqi air raids and moved shipping to Larak Island, while attacking foreign tankers that carried Iraqi oil (as Iran had blocked Iraq's access to the open sea with the capture of al-Faw). By now they almost always used the armed speedboats of the IRGC navy, and attacked many tankers. The tanker war escalated drastically, with attacks nearly doubling in 1986 (the majority carried out by Iraq). Iraq got permission from the Saudi government to use its airspace to attack Larak Island, although due to the distance attacks were less frequent there. The escalating tanker war in the Gulf became an ever-increasing concern to foreign powers, especially the United States.

In April 1986, Ayatollah Khomeini issued a fatwa declaring that the war must be won by March 1987. The Iranians increased recruitment efforts, obtaining 650,000 volunteers. The animosity between the Army and the Revolutionary Guard arose again, with the Army wanting to use more refined, limited military attacks while the Revolutionary Guard wanted to carry out major offensives. Iran, confident in its successes, began planning their largest offensives of the war, which they called their "final offensives".

Faced with their recent defeats in al-Faw and Mehran, Iraq appeared to be losing the war. Iraq's generals, angered by Saddam's interference, threatened a full-scale mutiny against the Ba'ath Party unless they were allowed to conduct operations freely. In one of the few times during his career, Saddam gave in to the demands of his generals. Up to this point, Iraqi strategy was to ride out Iranian attacks. However, the defeat at al-Faw led Saddam to declare the war to be "Al-Defa al-Mutaharakha" (The Dynamic Defense), and announcing that all civilians had to take part in the war effort. The universities were closed and all of the male students were drafted into the military. Civilians were instructed to clear marshlands to prevent Iranian amphibious infiltrations and to help build fixed defenses.

The government tried to integrate the Shias into the war effort by recruiting many as part of the Ba'ath Party. In an attempt to counterbalance the religious fervor of the Iranians and gain support from the devout masses, the regime also began to promote religion and, on the surface, Islamization, despite the fact that Iraq was run by a secular regime. Scenes of Saddam praying and making pilgrimages to shrines became common on state-run television. While Iraqi morale had been low throughout the war, the attack on al-Faw raised patriotic fervor, as the Iraqis feared invasion. Saddam also recruited volunteers from other Arab countries into the Republican Guard, and received much technical support from foreign nations as well. While Iraqi military power had been depleted in recent battles, through heavy foreign purchases and support, they were able to expand their military even to much larger proportions by 1988.

At the same time, Saddam ordered the genocidal al-Anfal Campaign in an attempt to crush the Kurdish resistance, who were now allied with Iran. The result was the deaths of several hundred thousand Iraqi Kurds, and the destruction of villages, towns, and cities.

Iraq began to try to perfect its maneuver tactics. The Iraqis began to prioritize the professionalization of their military. Prior to 1986, the conscription-based Iraqi regular army and the volunteer-based Iraqi Popular Army conducted the bulk of the operations in the war, to little effect. The Republican Guard, formerly an elite praetorian guard, was expanded as a volunteer army and filled with Iraq's best generals. Loyalty to the state was no longer a primary requisite for joining. However, due to Saddam's paranoia, the former duties of the Republican Guard were transferred to a new unit, the Special Republican Guard. Full-scale war games against hypothetical Iranian positions were carried out in the western Iraqi desert against mock targets, and they were repeated over the course of a full year until the forces involved fully memorized their attacks. Iraq built its military massively, eventually possessing the 4th largest in the world, in order to overwhelm the Iranians through sheer size.

Meanwhile, Iran continued to attack as the Iraqis were planning their strike. In 1987 the Iranians renewed a series of major human wave offensives in both northern and southern Iraq. The Iraqis had elaborately fortified Basra with 5 defensive rings, exploiting natural waterways such as the Shatt-al-Arab and artificial ones, such as "Fish Lake" and the Jasim River, along with earth barriers. Fish Lake was a massive lake filled with mines, underwater barbed wire, electrodes and sensors. Behind each waterway and defensive line was radar-guided artillery, ground attack aircraft and helicopters, all capable of firing poison gas or conventional munitions.

The Iranian strategy was to penetrate the Iraqi defences and encircle Basra, cutting off the city as well as the Al-Faw peninsula from the rest of Iraq. Iran's plan was for three assaults: a diversionary attack near Basra, the main offensive and another diversionary attack using Iranian tanks in the north to divert Iraqi heavy armor from Basra. For these battles, Iran had re-expanded their military by recruiting many new Basij and Pasdaran volunteers. Iran brought 150,000–200,000 total troops into the battles.

On 25 December 1986, Iran launched Operation Karbala-4 ("Karbala" referring to Hussein ibn Ali's Battle of Karbala). According to Iraqi General Ra'ad al-Hamdani, this was a diversionary attack. The Iranians launched an amphibious assault against the Iraqi island of Umm al-Rassas in the Shatt-Al-Arab river, parallel to Khoramshahr. They then set up a pontoon bridge and continued the attack, eventually capturing the island in a costly success but failing to advance further; the Iranians had 60,000 casualties, while the Iraqis 9,500. The Iraqi commanders exaggerated Iranian losses to Saddam, and it was assumed that the main Iranian attack on Basra had been fully defeated and that it would take the Iranians six months to recover. When the main Iranian attack, Operation Karbala 5, began, many Iraqi troops were on leave.

The Siege of Basra, code-named Operation Karbala-5 (), was an offensive operation carried out by Iran in an effort to capture the Iraqi port city of Basra in early 1987. This battle, known for its extensive casualties and ferocious conditions, was the biggest battle of the war and proved to be the beginning of the end of the Iran–Iraq War. While Iranian forces crossed the border and captured the eastern section of Basra Governorate, the operation ended in a stalemate.

At the same time as Operation Karbala 5, Iran also launched Operation Karbala-6 against the Iraqis in Qasr-e Shirin in central Iran to prevent the Iraqis from rapidly transferring units down to defend against the Karbala-5 attack. The attack was carried out by Basij infantry and the Revolutionary Guard's 31st "Ashura" and the Army's 77th "Khorasan" armored divisions. The Basij attacked the Iraqi lines, forcing the Iraqi infantry to retreat. An Iraqi armored counter-attack surrounded the Basij in a pincer movement, but the Iranian tank divisions attacked, breaking the encirclement. The Iranian attack was finally stopped by mass Iraqi chemical weapons attacks.

Operation Karbala-5 was a severe blow to Iran's military and morale. To foreign observers, it appeared that Iran was continuing to strengthen. By 1988, Iran had become self-sufficient in many areas, such as anti-tank TOW missiles, Scud ballistic missiles (Shahab-1), Silkworm anti-ship missiles, Oghab tactical rockets, and producing spare parts for their weaponry. Iran had also improved its air defenses with smuggled surface to air missiles. Iran even was producing UAV's and the Pilatus PC-7 propeller aircraft for observation. Iran also doubled their stocks of artillery, and was self-sufficient in manufacture of ammunition and small arms.

While it was not obvious to foreign observers, the Iranian public had become increasingly war-weary and disillusioned with the fighting, and relatively few volunteers joined the fight in 1987–88. Because the Iranian war effort relied on popular mobilization, their military strength actually declined, and Iran was unable to launch any major offensives after Karbala-5. As a result, for the first time since 1982, the momentum of the fighting shifted towards the regular army. Since the regular army was conscription based, it made the war even less popular. Many Iranians began to try to escape the conflict. As early as May 1985, anti-war demonstrations took place in 74 cities throughout Iran, which were crushed by the regime, resulting in some protesters being shot and killed. By 1987, draft-dodging had become a serious problem, and the Revolutionary Guards and police set up roadblocks throughout cities to capture those who tried to evade conscription. Others, particularly the more nationalistic and religious, the clergy, and the Revolutionary Guards, wished to continue the war.

The leadership acknowledged that the war was a stalemate, and began to plan accordingly. No more "final offensives" were planned. The head of the Supreme Defense Council Hashemi Rafsanjani announced during a news conference the end of human wave attacks. Mohsen Rezaee, head of the IRGC, announced that Iran would focus exclusively on limited attacks and infiltrations, while arming and supporting opposition groups inside of Iraq.

On the Iranian home front, sanctions, declining oil prices, and Iraqi attacks on Iranian oil facilities and shipping took a heavy toll on the economy. While the attacks themselves were not as destructive as some analysts believed, the U.S.-led Operation Earnest Will (which protected Iraqi and allied oil tankers, but not Iranian ones) led many neutral countries to stop trading with Iran because of rising insurance and fear of air attack. Iranian oil and non-oil exports fell by 55%, inflation reached 50% by 1987, and unemployment skyrocketed. At the same time, Iraq was experiencing crushing debt and shortages of workers, encouraging its leadership to try to end the war quickly.

By the end of 1987, Iraq possessed 5,550 tanks (outnumbering the Iranians six to one) and 900 fighter aircraft (outnumbering the Iranians ten to one). After Operation Karbala-5, Iraq only had 100 qualified fighter pilots remaining; therefore, Iraq began to invest in recruiting foreign pilots from countries such as Belgium, South Africa, Pakistan, East Germany and the Soviet Union. They replenished their manpower by integrating volunteers from other Arab countries into their army. Iraq also became self-sufficient in chemical weapons and some conventional ones and received much equipment from abroad. Foreign support helped Iraq bypass its economic troubles and massive debt to continue the war and increase the size of its military.

While the southern and central fronts were at a stalemate, Iran began to focus on carrying out offensives in northern Iraq with the help of the Peshmerga (Kurdish insurgents). The Iranians used a combination of semi-guerrilla and infiltration tactics in the Kurdish mountains with the Peshmerga. During Operation Karbala-9 in early April, Iran captured territory near Suleimaniya, provoking a severe poison gas counter-attack. During Operation Karbala-10, Iran attacked near the same area, capturing more territory. During Operation Nasr-4, the Iranians surrounded the city of Suleimaniya and, with the help of the Peshmerga, infiltrated over 140 km into Iraq and raided and threatened to capture the oil-rich city of Kirkuk and other northern oilfields. Nasr-4 was considered to be Iran's most successful individual operation of the war but Iranian forces were unable to consolidate their gains and continue their advance; while these offensives coupled with the Kurdish uprising sapped Iraqi strength, losses in the north would not mean a catastrophic failure for Iraq.

On 20 July, the UN Security Council passed the U.S.-sponsored Resolution 598, which called for an end to the fighting and a return to pre-war boundaries. This resolution was noted by Iran for being the first resolution to call for a return to the pre-war borders, and setting up a commission to determine the aggressor and compensation.

With the stalemate on land, the air/tanker war began to play an increasingly major role in the conflict. The Iranian air force had become very small, with only 20 F-4 Phantoms, 20 F-5 Tigers, and 15 F-14 Tomcats in operation, although Iran managed to restore some damaged planes to service. The Iranian Air Force, despite its once sophisticated equipment, lacked enough equipment and personnel to sustain the war of attrition that had developed, and was unable to lead an outright onslaught against Iraq. The Iraqi Air Force, however, had originally lacked modern equipment and experienced pilots, but after pleas from Iraqi military leaders, Saddam decreased political influence on everyday operations and left the fighting to his combatants. The Soviets began delivering more advanced aircraft and weapons to Iraq, while the French improved training for flight crews and technical personnel and continually introduced new methods for countering Iranian weapons and tactics. Iranian ground air defense still shot down many Iraqi aircraft.

The main Iraqi air effort had shifted to the destruction of Iranian war-fighting capability (primarily Persian Gulf oil fields, tankers, and Kharg Island), and starting in late 1986, the Iraqi Air Force began a comprehensive campaign against the Iranian economic infrastructure. By late 1987, the Iraqi Air Force could count on direct American support for conducting long-range operations against Iranian infrastructural targets and oil installations deep in the Persian Gulf. U.S. Navy ships tracked and reported movements of Iranian shipping and defences. In the massive Iraqi air strike against Kharg Island, flown on 18 March 1988, the Iraqis destroyed two supertankers but lost five aircraft to Iranian F-14 Tomcats, including two Tupolev Tu-22Bs and one Mikoyan MiG-25RB. The U.S. Navy was now becoming more involved in the fight in the Persian Gulf, launching Operations Earnest Will and Prime Chance against the Iranians.

The attacks on oil tankers continued. Both Iran and Iraq carried out frequent attacks during the first four months of the year. Iran was effectively waging a naval guerilla war with its IRGC navy speedboats, while Iraq attacked with its aircraft. In 1987, Kuwait asked to reflag its tankers to the U.S. flag. They did so in March, and the U.S. Navy began Operation Earnest Will to escort the tankers. The result of Earnest Will would be that, while oil tankers shipping Iraqi/Kuwaiti oil were protected, Iranian tankers and neutral tankers shipping to Iran would be unprotected, resulting in both losses for Iran and the undermining of its trade with foreign countries, damaging Iran's economy further. Iran deployed Silkworm missiles to attack ships, but only a few were actually fired. Both the United States and Iran jockeyed for influence in the Gulf. To discourage the United States from escorting tankers, Iran secretly mined some areas. The United States began to escort the reflagged tankers, but one was damaged by a mine while under escort. While being a public-relations victory for Iran, the United States increased its reflagging efforts. While Iran mined the Persian Gulf, their speedboat attacks were reduced, primarily attacking unflagged tankers shipping in the area.

On 24 September, US Navy SEALS captured the Iranian mine-laying ship "Iran Ajr", a diplomatic disaster for the already isolated Iranians. On 8 October, the U.S. Navy destroyed four Iranian speedboats, and in response to Iranian Silkworm missile attacks on Kuwaiti oil tankers, launched Operation Nimble Archer, destroying two Iranian oil rigs in the Persian Gulf. During November and December, the Iraqi air force launched a bid to destroy all Iranian airbases in Khuzestan and the remaining Iranian air force. Iran managed to shoot down 30 Iraqi fighters with fighter jets, anti-aircraft guns, and missiles, allowing the Iranian air force to survive to the end of the war.

On 28 June, Iraqi fighter bombers attacked the Iranian town of Sardasht near the border, using chemical mustard gas bombs. While many towns and cities had been bombed before, and troops attacked with gas, this was the first time that the Iraqis had attacked a civilian area with poison gas. One quarter of the town's then population of 20,000 was burned and stricken, and 113 were killed immediately, with many more dying and suffering health effects over following decades. Saddam ordered the attack in order to test the effects of the newly developed "dusty mustard" gas, which was designed to be even more crippling than traditional mustard gas. While little known outside of Iran (unlike the later Halabja chemical attack), the Sardasht bombing (and future similar attacks) had a tremendous effect on the Iranian people's psyche.

By 1988, with massive equipment imports and reduced Iranian volunteers, Iraq was ready to launch major offensives against Iran. In February 1988, Saddam began the fifth and most deadly "war of the cities". Over the next two months, Iraq launched over 200 al-Hussein missiles at 37 Iranian cities. Saddam also threatened to use chemical weapons in his missiles, which caused 30% of Tehran's population to leave the city. Iran retaliated, launching at least 104 missiles against Iraq in 1988 and shelling Basra. This event was nicknamed the "Scud Duel" in the foreign media. In all, Iraq launched 520 Scuds and al-Husseins against Iran and Iran fired 177 in return. The Iranian attacks were too few in number to deter Iraq from launching their attacks. Iraq also increased their airstrikes against Kharg Island and Iranian oil tankers. With their tankers protected by U.S. warships, they could operate with virtual impunity. In addition, the West supplied Iraq's air force with laser-guided smart bombs, allowing them to attack economic targets while evading anti-aircraft defenses. These attacks began to have a major toll on the Iranian economy and morale and caused many casualties.

In March 1988, the Iranians carried out Operation Dawn 10, Operation Beit ol-Moqaddas 2, and Operation Zafar 7 in Iraqi Kurdistan with the aim of capturing the Darbandikhan Dam and the power plant at Lake Dukan, which supplied Iraq with much of its electricity and water, as well as the city of Suleimaniya. Iran hoped that the capture of these areas would bring more favorable terms to the ceasefire agreement. This infiltration offensive was carried out in conjunction with the Peshmerga. Iranian airborne commandos landed behind the Iraqi lines and Iranian helicopters hit Iraqi tanks with TOW missiles. The Iraqis were taken by surprise, and Iranian F-5E Tiger fighter jets even damaged the Kirkuk oil refinery. Iraq carried out executions of multiple officers for these failures in March–April 1988, including Colonel Jafar Sadeq. The Iranians used infiltration tactics in the Kurdish mountains, captured the town of Halabja and began to fan out across the province.

Though the Iranians advanced to within sight of Dukan and captured around and 4,000 Iraqi troops, the offensive failed due to the Iraqi use of chemical warfare. The Iraqis launched the deadliest chemical weapons attacks of the war. The Republican Guard launched 700 chemical shells, while the other artillery divisions launched 200–300 chemical shells each, unleashing a chemical cloud over the Iranians, killing or wounding 60% of them, the blow was felt particularly by the Iranian 84th infantry division and 55th paratrooper division. The Iraqi special forces then stopped the remains of the Iranian force. In retaliation for Kurdish collaboration with the Iranians, Iraq launched a massive poison gas attack against Kurdish civilians in Halabja, recently taken by the Iranians, killing thousands of civilians. Iran airlifted foreign journalists to the ruined city, and the images of the dead were shown throughout the world, but Western mistrust of Iran and collaboration with Iraq led them to also blame Iran for the attack.

On 17 April 1988, Iraq launched Operation Ramadan Mubarak (Blessed Ramadan), a surprise attack against the 15,000 Basij troops on the al-Faw peninsula. The attack was preceded by Iraqi diversionary attacks in northern Iraq, with a massive artillery and air barrage of Iranian front lines. Key areas, such as supply lines, command posts, and ammunition depots, were hit by a storm of mustard gas and nerve gas, as well as by conventional explosives. Helicopters landed Iraqi commandos behind Iranian lines on al-Faw while the main Iraqi force made a frontal assault. Within 48 hours, all of the Iranian forces had been killed or cleared from the al-Faw Peninsula. The day was celebrated in Iraq as Faw Liberation Day throughout Saddam's rule. The Iraqis had planned the offensive well. Prior to the attack, the Iraqi soldiers gave themselves poison gas antidotes to shield themselves from the effect of the saturation of gas. The heavy and well executed use of chemical weapons was the decisive factor in the victory. Iraqi losses were relatively light, especially compared to Iran's casualties. The Iranians eventually managed to halt the Iraqi drive as they pushed towards Khuzestan.

To the shock of the Iranians, rather than breaking off the offensive, the Iraqis kept up their drive, and a new force attacked the Iranian positions around Basra. Following this, the Iraqis launched a sustained drive to clear the Iranians out of all of southern Iraq. One of the most successful Iraqi tactics was the "one-two punch" attack using chemical weapons. Using artillery, they would saturate the Iranian front line with rapidly dispersing cyanide and nerve gas, while longer-lasting mustard gas was launched via fighter-bombers and rockets against the Iranian rear, creating a "chemical wall" that blocked reinforcement.

The same day as Iraq's attack on al-Faw peninsula, the United States Navy launched Operation Praying Mantis in retaliation against Iran for damaging a warship with a mine. Iran lost oil platforms, destroyers, and frigates in this battle, which ended only when President Reagan decided that the Iranian navy had been damaged enough. In spite of this, the Revolutionary Guard Navy continued their speedboat attacks against oil tankers. The defeats at al-Faw and in the Persian Gulf nudged Iranian leadership towards quitting the war, especially when facing the prospect of fighting the Americans.

Faced with such losses, Khomeini appointed the cleric Hashemi Rafsanjani as the Supreme Commander of the Armed Forces, though he had in actuality occupied that position for months. Rafsanjani ordered a last desperate counter-attack into Iraq, which was launched 13 June 1988. The Iranians infiltrated through the Iraqi trenches and moved into Iraq and managed to strike Saddam's presidential palace in Baghdad using fighter aircraft. After three days of fighting, the decimated Iranians were driven back to their original positions again as the Iraqis launched 650 helicopter and 300 aircraft sorties.

On 18 June, Iraq launched Operation Forty Stars ( "chehel cheragh") in conjunction with the Mujahideen-e-Khalq (MEK) around Mehran. With 530 aircraft sorties and heavy use of nerve gas, they crushed the Iranian forces in the area, killing 3,500 and nearly destroying a Revolutionary Guard division. Mehran was captured once again and occupied by the MEK. Iraq also launched air raids on Iranian population centers and economic targets, setting 10 oil installations on fire.

On 25 May 1988, Iraq launched the first of five Tawakalna ala Allah Operations, consisting of one of the largest artillery barrages in history, coupled with chemical weapons. The marshes had been dried by drought, allowing the Iraqis to use tanks to bypass Iranian field fortifications, expelling the Iranians from the border town of Shalamcheh after less than 10 hours of combat.

On 25 June, Iraq launched the second Tawakal ala Allah operation against the Iranians on Majnoon Island. Iraqi commandos used amphibious craft to block the Iranian rear, then used hundreds of tanks with massed conventional and chemical artillery barrages to recapture the island after 8 hours of combat. Saddam appeared live on Iraqi television to "lead" the charge against the Iranians. The majority of the Iranian defenders were killed during the quick assault. The final two Tawakal ala Allah operations took place near al-Amarah and Khaneqan. By 12 July, the Iraqis had captured the city of Dehloran, inside Iran, along with 2,500 troops and much armour and material, which took four days to transport to Iraq. These losses included more than 570 of the 1,000 remaining Iranian tanks, over 430 armored vehicles, 45 self-propelled artillery, 300 towed artillery pieces, and 320 antiaircraft guns. These figures only included what Iraq could actually put to use; total amount of captured materiel was higher. Since March, the Iraqis claimed to have captured 1,298 tanks, 155 infantry fighting vehicles, 512 heavy artillery pieces, 6,196 mortars, 5,550 recoilless rifles and light guns, 8,050 man-portable rocket launchers, 60,694 rifles, 322 pistols, 454 trucks, and 1,600 light vehicles. The Iraqis withdrew from Dehloran soon after, claiming that they had "no desire to conquer Iranian territory". History professor Kaveh Farrokh considered this to be Iran's greatest military disaster during the war. Stephen Pelletier, a Journalist, Middle East expert, and Author, noted that "Tawakal ala Allah ... resulted in the absolute destruction of Iran's military machine."

During the 1988 battles, the Iranians put up little resistance, having been worn out by nearly eight years of war. They lost large amounts of equipment but managed to rescue most of their troops from being captured, leaving Iraq with relatively few prisoners. On 2 July, Iran belatedly set up a joint central command which unified the Revolutionary Guard, Army, and Kurdish rebels, and dispelled the rivalry between the Army and the Revolutionary Guard. However, this came too late and, following the capture of 570 of their operable tanks and the destruction of hundreds more, Iran was believed to have fewer than 200 remaining operable tanks on the southern front, against thousands of Iraqi ones. The only area where the Iranians were not suffering major defeats was in Kurdistan.

Saddam sent a warning to Khomeini in mid-1988, threatening to launch a new and powerful full-scale invasion and attack Iranian cities with weapons of mass destruction. Shortly afterwards, Iraqi aircraft bombed the Iranian town of Oshnavieh with poison gas, immediately killing and wounding over 2,000 civilians. The fear of an all out chemical attack against Iran's largely unprotected civilian population weighed heavily on the Iranian leadership, and they realized that the international community had no intention of restraining Iraq. The lives of the civilian population of Iran were becoming very disrupted, with a third of the urban population evacuating major cities in fear of the seemingly imminent chemical war. Meanwhile, Iraqi conventional bombs and missiles continuously hit towns and cities, destroying vital civilian and military infrastructure, and increasing the death toll. Iran replied with missile and air attacks, but not sufficiently to deter the Iraqis.

with the threat of a new and even more powerful invasion, Commander-in-Chief Rafsanjani ordered the Iranians to retreat from Haj Omran, Kurdistan on 14 July. The Iranians did not publicly describe this as a retreat, instead calling it a "temporary withdrawal". By July, Iran's army inside Iraq (except Kurdistan) had largely disintegrated. Iraq put up a massive display of captured Iranian weapons in Baghdad, claiming they captured 1,298 tanks, 5,550 recoil-less rifles, and thousands of other weapons. However, Iraq had taken heavy losses as well, and the battles were very costly.

In July 1988, Iraqi aircraft dropped bombs on the Iranian Kurdish village of Zardan. Dozens of villages, such as Sardasht, and some larger towns, such as Marivan, Baneh and Saqqez, were once again attacked with poison gas, resulting in even heavier civilian casualties. On 3 July 1988, the USS "Vincennes" shot down Iran Air Flight 655, killing 290 passengers and crew. The lack of international sympathy disturbed the Iranian leadership, and they came to the conclusion that the United States was on the verge of waging a full-scale war against them, and that Iraq was on the verge of unleashing its entire chemical arsenal upon their cities.

At this point, elements of the Iranian leadership, led by Rafsanjani (who had initially pushed for the extension of the war), persuaded Khomeini to accept a ceasefire. They stated that in order to win the war, Iran's military budget would have to be increased eightfold and the war would last until 1993. On 20 July 1988, Iran accepted Resolution 598, showing its willingness to accept a ceasefire. A statement from Khomeini was read out in a radio address, and he expressed deep displeasure and reluctance about accepting the ceasefire,

Happy are those who have departed through martyrdom. Happy are those who have lost their lives in this convoy of light. Unhappy am I that I still survive and have drunk the poisoned chalice...

The news of the end of the war was greeted with celebration in Baghdad, with people dancing in the streets; in Tehran, however, the end of the war was greeted with a somber mood.

Operation Mersad ( "ambush") was the last big military operation of the war. Both Iran and Iraq had accepted Resolution 598, but despite the ceasefire, after seeing Iraqi victories in the previous months, Mujahadeen-e-Khalq (MEK) decided to launch an attack of its own and wished to advance all the way to Teheran. Saddam and the Iraqi high command decided on a two-pronged offensive across the border into central Iran and Iranian Kurdistan. Shortly after Iran accepted the ceasefire the MEK army began its offensive, attacking into Ilam province under cover of Iraqi air power. In the north, Iraq also launched an attack into Iraqi Kurdistan, which was blunted by the Iranians.

On 26 July 1988, the MEK started their campaign in central Iran, Operation Forough Javidan (Eternal Light), with the support of the Iraqi army. The Iranians had withdrawn their remaining soldiers to Khuzestan in fear of a new Iraqi invasion attempt, allowing the Mujahedeen to advance rapidly towards Kermanshah, seizing Qasr-e Shirin, Sarpol-e Zahab, Kerend-e Gharb, and Islamabad-e-Gharb. The MEK expected the Iranian population to rise up and support their advance; the uprising never materialised but they reached deep into Iran. In response, the Iranian military launched its counter-attack, Operation Mersad, under Lieutenant General Ali Sayyad Shirazi. Iranian paratroopers landed behind the MEK lines while the Iranian Air Force and helicopters launched an air attack, destroying much of the enemy columns. The Iranians defeated the MEK in the city of Kerend-e Gharb on 29 July 1988. On 31 July, Iran drove the MEK out of Qasr-e-Shirin and Sarpol Zahab, though MEK claimed to have "voluntarily withdrawn" from the towns. Iran estimated that 4,500 MEK were killed, while 400 Iranian soldiers died.

The last notable combat actions of the war took place on 3 August 1988, in the Persian Gulf when the Iranian navy fired on a freighter and Iraq launched chemical attacks on Iranian civilians, killing an unknown number of them and wounding 2,300. Iraq came under international pressure to curtail further offensives. Resolution 598 became effective on 8 August 1988, ending all combat operations between the two countries. By 20 August 1988, peace with Iran was restored. UN peacekeepers belonging to the UNIIMOG mission took the field, remaining on the Iran–Iraq border until 1991. The majority of Western analysts believe that the war had no winners while some believed that Iraq emerged as the victor of the war, based on Iraq's overwhelming successes between April and July 1988. While the war was now over, Iraq spent the rest of August and early September clearing the Kurdish resistance. Using 60,000 troops along with helicopter gunships, chemical weapons (poison gas), and mass executions, Iraq hit 15 villages, killing rebels and civilians, and forced tens of thousands of Kurds to relocate to settlements. Many Kurdish civilians fled to Iran. By 3 September 1988, the anti-Kurd campaign ended, and all resistance had been crushed. 400 Iraqi soldiers and 50,000–100,000 Kurdish civilians and soldiers had been killed.

At the war's conclusion, it took several weeks for the Armed Forces of the Islamic Republic of Iran to evacuate Iraqi territory to honor pre-war international borders set by the 1975 Algiers Agreement. The last prisoners of war were exchanged in 2003.

The Security Council did not identify Iraq as the aggressor of the war until 11 December 1991, some 11 years after Iraq invaded Iran and 16 months following Iraq's invasion of Kuwait.

The Iran–Iraq War was the deadliest conventional war ever fought between regular armies of developing countries. Iraqi casualties are estimated at 105,000–200,000 killed, while about 400,000 had been wounded and some 70,000 taken prisoner. Thousands of civilians on both sides died in air raids and ballistic missile attacks. Prisoners taken by both countries began to be released in 1990, though some were not released until more than 10 years after the end of the conflict. Cities on both sides had also been considerably damaged. While revolutionary Iran had been bloodied, Iraq was left with a large military and was a regional power, albeit with severe debt, financial problems, and labor shortages.

According to Iranian government sources, the war cost Iran an estimated 200,000–220,000 killed, or up to 262,000 according to the conservative Western estimates. This includes 123,220 combatants, 60,711 MIA and 11,000–16,000 civilians. Combatants include 79,664 members of the Revolutionary Guard Corps and additional 35,170 soldiers from regular military. In addition, prisoners of war comprise 42,875 Iranian casualties, they were captured and kept in Iraqi detention centers from 2.5 to more than 15 years after the war was over. According to the Janbazan Affairs Organization, 398,587 Iranians sustained injuries that required prolonged medical and health care following primary treatment, including 52,195 (13%) injured due to the exposure to chemical warfare agents. From 1980 to 2012, 218,867 Iranians died due to war injuries and the mean age of combatants was 23 years old. This includes 33,430 civilians, mostly women and children. More than 144,000 Iranian children were orphaned as a consequence of these deaths. Other estimates put Iranian casualties up to 600,000.

Both Iraq and Iran manipulated loss figures to suit their purposes. At the same time, Western analysts accepted improbable estimates. By April 1988, such casualties were estimated at between 150,000 and 340,000 Iraqis dead, and 450,000 to 730,000 Iranians. Shortly after the end of the war, it was thought that Iran suffered even more than a million dead. Considering the style of fighting on the ground and the fact that neither side penetrated deeply into the other's territory, USMC analysts believe events do not substantiate the high casualties claimed. The Iraqi government has claimed 800,000 Iranians were killed in conflict, four times more than Iranian official figures. Iraqi losses were also revised downwards over time.

With the ceasefire in place, and UN peacekeepers monitoring the border, Iran and Iraq sent their representatives to Geneva, Switzerland, to negotiate a peace agreement on the terms of the ceasefire. However, peace talks stalled. Iraq, in violation of the UN ceasefire, refused to withdraw its troops from of disputed territory at the border area unless the Iranians accepted Iraq's full sovereignty over the Shatt al-Arab waterway. Foreign powers continued to support Iraq, which wanted to gain at the negotiating table what they failed to achieve on the battlefield, and Iran was portrayed as the one not wanting peace. Iran, in response, refused to release 70,000 Iraqi prisoners of war (compared to 40,000 Iranian prisoners of war held by Iraq). They also continued to carry out a naval blockade of Iraq, although its effects were mitigated by Iraqi use of ports in friendly neighbouring Arab countries. Iran also began to improve relations with many of the states that opposed it during the war. Because of Iranian actions, by 1990, Saddam had become more conciliatory, and in a letter to the now President Rafsanjani, he became more open to the idea of a peace agreement, although he still insisted on full sovereignty over the Shatt al-Arab.

By 1990, Iran was undergoing military rearmament and reorganization, and purchased $10 billion worth of heavy weaponry from the USSR and China, including aircraft, tanks, and missiles. Rafsanjani reversed Iran's self-imposed ban on chemical weapons, and ordered the manufacture and stockpile of them (Iran destroyed them in 1993 after ratifying the Chemical Weapons Convention). As war with the western powers loomed, Iraq became concerned about the possibility of Iran mending its relations with the west in order to attack Iraq. Iraq had lost its support from the West, and its position in Iran was increasingly untenable. Saddam realized that if Iran attempted to expel the Iraqis from the disputed territories in the border area, it was likely they would succeed. Shortly after his invasion of Kuwait, Saddam wrote a letter to Rafsanjani stating that Iraq recognised Iranian rights over the eastern half of the Shatt al-Arab, a reversion to "status quo ante bellum" that he had repudiated a decade earlier, and that he would accept Iran's demands and withdraw Iraq's military from the disputed territories. A peace agreement was signed finalizing the terms of the UN resolution, diplomatic relations were restored, and by late 1990-early 1991, the Iraqi military withdrew. The UN peacekeepers withdrew from the border shortly afterward. Most of the prisoners of war were released in 1990, although some remained as late as 2003. Iranian politicians declared it to be the "greatest victory in the history of the Islamic Republic of Iran".

Most historians and analysts consider the war to be a stalemate. Certain analysts believe that Iraq won, on the basis of the successes of their 1988 offensives which thwarted Iran's major territorial ambitions in Iraq and persuaded Iran to accept the ceasefire. Iranian analysts believe that they won the war because although they did not succeed in overthrowing the Iraqi government, they thwarted Iraq's major territorial ambitions in Iran, and that, two years after the war had ended, Iraq permanently gave up its claim of ownership over the entire Shatt al-Arab as well.

On 9 December 1991, Javier Pérez de Cuéllar, UN Secretary General at the time, reported that Iraq's initiation of the war was unjustified, as was its occupation of Iranian territory and use of chemical weapons against civilians:

That [Iraq's] explanations do not appear sufficient or acceptable to the international community is a fact...[the attack] cannot be justified under the charter of the United Nations, any recognized rules and principles of international law, or any principles of international morality, and entails the responsibility for conflict. Even if before the outbreak of the conflict there had been some encroachment by Iran on Iraqi territory, such encroachment did not justify Iraq's aggression against Iran—which was followed by Iraq's continuous occupation of Iranian territory during the conflict—in violation of the prohibition of the use of force, which is regarded as one of the rules of jus cogens...On one occasion I had to note with deep regret the experts' conclusion that "chemical weapons ha[d] been used against Iranian civilians in an area adjacent to an urban center lacking any protection against that kind of attack."

He also stated that had the UN accepted this fact earlier, the war would have almost certainly not lasted as long as it did. Iran, encouraged by the announcement, sought reparations from Iraq, but never received any.

Throughout the 1990s and early 2000s, Iran and Iraq relations remained balanced between a cold war and a cold peace. Despite renewed and somewhat thawed relations, both sides continued to have low level conflicts. Iraq continued to host and support the Mujahedeen-e-Khalq, which carried out multiple attacks throughout Iran up until the 2003 invasion of Iraq (including the assassination of Iranian general Ali Sayyad Shirazi in 1998, cross border raids, and mortar attacks). Iran carried out several airstrikes and missile attacks against Mujahedeen targets inside of Iraq (the largest taking place in 2001, when Iran fired 56 Scud missiles at Mujahedeen targets). In addition, according to General Hamdani, Iran continued to carry out low-level infiltrations of Iraqi territory, using Iraqi dissidents and anti-government activists rather than Iranian troops, in order to incite revolts. After the fall of Saddam in 2003, Hamdani claimed that Iranian agents infiltrated and created numerous militias in Iraq and built an intelligence system operating within the country.

In 2005, the new government of Iraq apologised to Iran for starting the war. The Iraqi government also commemorated the war with various monuments, including the Hands of Victory and the al-Shaheed Monument, both in Baghdad. The war also helped to create a forerunner for the Coalition of the Gulf War, when the Gulf Arab states banded together early in the war to form the Gulf Cooperation Council to help Iraq fight Iran.

The economic loss at the time was believed to exceed $500 billion for each country ($1.2 trillion total). In addition, economic development stalled and oil exports were disrupted. Iraq had accrued more than $130 billion of international debt, excluding interest, and was also weighed down by a slowed GDP growth. Iraq's debt to Paris Club amounted to $21 billion, 85% of which had originated from the combined inputs of Japan, the USSR, France, Germany, the United States, Italy and the United Kingdom. The largest portion of Iraq's debt, amounting to $130 billion, was to its former Arab backers, with $67 billion loaned by Kuwait, Saudi Arabia, Qatar, UAE, and Jordan. After the war, Iraq accused Kuwait of slant drilling and stealing oil, inciting its invasion of Kuwait, which in turn worsened Iraq's financial situation: the United Nations Compensation Commission mandated Iraq to pay reparations of more than $200 billion to victims of the invasion, including Kuwait and the United States. To enforce payment, Iraq was put under a complete international embargo, which further strained the Iraqi economy and pushed its external debt to private and public sectors to more than $500 billion by the end of Saddam's rule. Combined with Iraq's negative economic growth after prolonged international sanctions, this produced a debt-to-GDP ratio of more than 1,000%, making Iraq the most indebted developing country in the world. The unsustainable economic situation compelled the new Iraqi government to request that a considerable portion of debt incurred during the Iran–Iraq war be written off.

Much of the oil industry in both countries was damaged in air raids.

The war had its impact on medical science: a surgical intervention for comatose patients with penetrating brain injuries was created by Iranian physicians treating wounded soldiers, later establishing neurosurgery guidelines to treat civilians who had suffered blunt or penetrating skull injuries. Iranian physicians' experience in the war reportedly helped U.S. congresswoman Gabrielle Giffords recover after the 2011 Tucson shooting.

In addition to helping trigger the Persian Gulf War, the Iran–Iraq War also contributed to Iraq's defeat in the Persian Gulf War. Iraq's military was accustomed to fighting the slow moving Iranian infantry formations with artillery and static defenses, while using mostly unsophisticated tanks to gun down and shell the infantry and overwhelm the smaller Iranian tank force; in addition to being dependent on weapons of mass destruction to help secure victories. Therefore, they were rapidly overwhelmed by the high-tech, quick-maneuvering U.S. forces using modern doctrines such as AirLand Battle.

At first, Saddam attempted to ensure that the Iraqi population suffered from the war as little as possible. There was rationing, but civilian projects begun before the war continued. At the same time, the already extensive personality cult around Saddam reached new heights while the regime tightened its control over the military.

After the Iranian victories of the spring of 1982 and the Syrian closure of Iraq's main pipeline, Saddam did a volte-face on his policy towards the home front: a policy of austerity and total war was introduced, with the entire population being mobilised for the war effort. All Iraqis were ordered to donate blood and around 100,000 Iraqi civilians were ordered to clear the reeds in the southern marshes. Mass demonstrations of loyalty towards Saddam became more common. Saddam also began implementing a policy of discrimination against Iraqis of Iranian origin.

In the summer of 1982, Saddam began a campaign of terror. More than 300 Iraqi Army officers were executed for their failures on the battlefield. In 1983, a major crackdown was launched on the leadership of the Shia community. Ninety members of the al-Hakim family, an influential family of Shia clerics whose leading members were the émigrés Mohammad Baqir al-Hakim and Abdul Aziz al-Hakim, were arrested, and 6 were hanged. The crackdown on Kurds saw 8,000 members of the Barzani clan, whose leader (Massoud Barzani) also led the Kurdistan Democratic Party, similarly executed. From 1983 onwards, a campaign of increasingly brutal repression was started against the Iraqi Kurds, characterised by Israeli historian Efraim Karsh as having "assumed genocidal proportions" by 1988. The al-Anfal Campaign was intended to "pacify" Iraqi Kurdistan permanently. By 1983 , the Barzanis entered an alliance with Iran in defense against Saddam Hussein.

To secure the loyalty of the Shia population, Saddam allowed more Shias into the Ba'ath Party and the government, and improved Shia living standards, which had been lower than those of the Iraqi Sunnis. Saddam had the state pay for restoring Imam Ali's tomb with white marble imported from Italy. The Baathists also increased their policies of repression against the Shia. The most infamous event was the massacre of 148 civilians of the Shia town of Dujail.

Despite the costs of the war, the Iraqi regime made generous contributions to Shia "waqf" (religious endowments) as part of the price of buying Iraqi Shia support. The importance of winning Shia support was such that welfare services in Shia areas were expanded during a time in which the Iraqi regime was pursuing austerity in all other non-military fields. During the first years of the war in the early 1980s, the Iraqi government tried to accommodate the Kurds in order to focus on the war against Iran. In 1983, the Patriotic Union of Kurdistan agreed to cooperate with Baghdad, but the Kurdistan Democratic Party (KDP) remained opposed. In 1983, Saddam signed an autonomy agreement with Jalal Talabani of the Patriotic Union of Kurdistan (PUK), though Saddam later reneged on the agreement. By 1985, the PUK and KDP had joined forces, and Iraqi Kurdistan saw widespread guerrilla warfare up to the end of the war.

Israeli-British historian, Ephraim Karsh, argues that the Iranian government saw the outbreak of war as chance to strengthen its position and consolidate the Islamic revolution, noting that government propaganda presented it domestically as a glorious "jihad" and a test of Iranian national character. The Iranian regime followed a policy of total war from the beginning, and attempted to mobilise the nation as a whole. They established a group known as the Reconstruction Campaign, whose members were exempted from conscription and were instead sent into the countryside to work on farms to replace the men serving at the front.

Iranian workers had a day's pay deducted from their pay cheques every month to help finance the war, and mass campaigns were launched to encourage the public to donate food, money, and blood. To further help finance the war, the Iranian government banned the import of all non-essential items, and launched a major effort to rebuild the damaged oil plants.

According to former Iraqi general Ra'ad al-Hamdani, the Iraqis believed that in addition to the Arab revolts, the Revolutionary Guards would be drawn out of Tehran, leading to a counter-revolution in Iran that would cause Khomeini's government to collapse and thus ensure Iraqi victory. However, rather than turning against the revolutionary government as experts had predicted, Iran's people (including Iranian Arabs) rallied in support of the country and put up a stiff resistance.

In June 1981, street battles broke out between the Revolutionary Guard and the left-wing Mujaheddin e-Khalq (MEK), continuing for several days and killing hundreds on both sides. In September, more unrest broke out on the streets of Iran as the MEK attempted to seize power. Thousands of left-wing Iranians (many of whom were not associated with the MEK) were shot and hanged by the government. The MEK began an assassination campaign that killed hundreds of regime officials by the fall of 1981. On 28 June 1981, they assassinated the secretary-general of the Islamic Republican Party, Mohammad Beheshti and on 30 August, killed Iran's president, Mohammad-Ali Rajai. The government responded with mass executions of suspected MEK members, a practice that lasted until 1985.

In addition to the open civil conflict with the MEK, the Iranian government was faced with Iraqi-supported rebellions in Iranian Kurdistan, which were gradually put down through a campaign of systematic repression. 1985 also saw student anti-war demonstrations, which were crushed by government forces.

The war furthered the decline of the Iranian economy that had begun with the revolution in 1978–79. Between 1979 and 1981, foreign exchange reserves fell from $14.6 billion to $1 billion. As a result of the war, living standards dropped dramatically, and Iran was described by British journalists John Bulloch and Harvey Morris as "a dour and joyless place" ruled by a harsh regime that "seemed to have nothing to offer but endless war". Though Iran was becoming bankrupt, Khomeini interpreted Islam's prohibition of usury to mean they could not borrow against future oil revenues to meet war expenses. As a result, Iran funded the war by the income from oil exports after cash had run out. The revenue from oil dropped from $20 billion in 1982 to $5 billion in 1988.French historian Pierre Razoux argued that this sudden drop in economic industrial potential, in conjunction with the increasing aggression of Iraq, placed Iran in a challenging position that had little leeway other than accepting Iraq's conditions of peace.

In January 1985, former prime minister and anti-war Islamic Liberation Movement co-founder Mehdi Bazargan criticised the war in a telegram to the United Nations, calling it un-Islamic and illegitimate and arguing that Khomeini should have accepted Saddam's truce offer in 1982 instead of attempting to overthrow the Ba'ath. In a public letter to Khomeini sent in May 1988, he added "Since 1986, you have not stopped proclaiming victory, and now you are calling upon population to resist until victory. Is that not an admission of failure on your part?" Khomeini was annoyed by Bazargan's telegram, and issued a lengthy public rebuttal in which he defended the war as both Islamic and just.

By 1987, Iranian morale had begun to crumble, reflected in the failure of government campaigns to recruit "martyrs" for the front. Israeli historian Efraim Karsh points to the decline in morale in 1987–88 as being a major factor in Iran's decision to accept the ceasefire of 1988.

Not all saw the war in negative terms. The Islamic Revolution of Iran was strengthened and radicalised. The Iranian government-owned "Etelaat" newspaper wrote, "There is not a single school or town that is excluded from the happiness of 'holy defence' of the nation, from drinking the exquisite elixir of martyrdom, or from the sweet death of the martyr, who dies in order to live forever in paradise."

Iran's regular Army had been purged after the 1979 Revolution, with most high-ranking officers either having deserted (fled the country) or been executed.

At the beginning of the war, Iraq held a clear advantage in armour, while both nations were roughly equal in terms of artillery. The gap only widened as the war went on. Iran started with a stronger air force, but over time, the balance of power reversed in Iraq's favour (as Iraq was constantly expanding its military, while Iran was under arms sanctions). Estimates for 1980 and 1987 were:

The conflict has been compared to World War I in terms of the tactics used, including large-scale trench warfare with barbed wire stretched across trenches, manned machine gun posts, bayonet charges, human wave attacks across a no man's land, and extensive use of chemical weapons such as sulfur mustard by the Iraqi government against Iranian troops, civilians, and Kurds. The world powers United States and the Soviet Union, together with many Western and Arab countries, provided military, intelligence, economic, and political support for Iraq.

During the war, Iraq was regarded by the West and the Soviet Union as a counterbalance to post-revolutionary Iran. The Soviet Union, Iraq's main arms supplier during the war, did not wish for the end of its alliance with Iraq, and was alarmed by Saddam's threats to find new arms suppliers in the West and China if the Kremlin did not provide him with the weapons he wanted. The Soviet Union hoped to use the threat of reducing arms supplies to Iraq as leverage for forming a Soviet-Iranian alliance.

During the early years of the war, the United States lacked meaningful relations with either Iran or Iraq, the former due to the Iranian Revolution and the Iran hostage crisis and the latter because of Iraq's alliance with the Soviet Union and hostility towards Israel. Following Iran's success of repelling the Iraqi invasion and Khomeini's refusal to end the war in 1982, the United States made an outreach to Iraq, beginning with the restoration of diplomatic relations in 1984. The United States wished to both keep Iran away from Soviet influence and protect other Gulf states from any threat of Iranian expansion. As a result, it began to provide limited support to Iraq. In 1982, Henry Kissinger, former Secretary of State, outlined U.S. policy towards Iran:

The focus of Iranian pressure at this moment is Iraq. There are few governments in the world less deserving of our support and less capable of using it. Had Iraq won the war, the fear in the Gulf and the threat to our interest would be scarcely less than it is today. Still, given the importance of the balance of power in the area, it is in our interests to promote a ceasefire in that conflict; though not a cost that will preclude an eventual rapprochement with Iran either if a more moderate regime replaces Khomenini's or if the present rulers wake up to geopolitical reality that the historic threat to Iran's independence has always come from the country with which it shares a border of : the Soviet Union. A rapprochement with Iran, of course, must await at a minimum Iran's abandonment of hegemonic aspirations in the Gulf.

Richard Murphy, Assistant Secretary of State during the war, testified to Congress in 1984 that the Reagan administration believed a victory for either Iran or Iraq was "neither militarily feasible nor strategically desirable".

Support to Iraq was given via technological aid, intelligence, the sale of chemical and biological warfare technology and military equipment, and satellite intelligence. While there was direct combat between Iran and the United States, it is not universally agreed that the fighting between the United States and Iran was specifically to benefit Iraq, or for separate issues between the U.S. and Iran. American official ambiguity towards which side to support was summed up by Henry Kissinger when he remarked, "It's a pity they both can't lose." The Americans and the British also either blocked or watered down UN resolutions that condemned Iraq for using chemical weapons against the Iranians and their own Kurdish citizens.

More than 30 countries provided support to Iraq, Iran, or both; most of the aid went to Iraq. Iran had a complex clandestine procurement network to obtain munitions and critical materials. Iraq had an even larger clandestine purchasing network, involving 10–12 allied countries, to maintain ambiguity over their arms purchases and to circumvent "official restrictions". Arab mercenaries and volunteers from Egypt and Jordan formed the Yarmouk Brigade and participated in the war alongside Iraqis.

According to the Stockholm International Peace Institute, the Soviet Union, France, and China together accounted for over 90% of the value of Iraq's arms imports between 1980 and 1988.

The United States pursued policies in favour of Iraq by reopening diplomatic channels, lifting restrictions on the export of dual-use technology, overseeing the transfer of third-party military hardware, and providing operational intelligence on the battlefield. France, which from the 1970s had been one of Iraq's closest allies, was a major supplier of military hardware. The French sold weapons equal to $5 billion, which comprised well over a quarter of Iraq's total arms stockpile. Citing French magazine "Le Nouvel Observateur" as the primary source, but also quoting French officials, the "New York Times" reported France had been sending chemical precursors of chemical weapons to Iraq, since 1986. China, which had no direct stake in the victory of either side and whose interests in the war were entirely commercial, freely sold arms to both sides.

Iraq also made extensive use of front companies, middlemen, secret ownership of all or part of companies all over the world, forged end-user certificates, and other methods to hide what it was acquiring. Some transactions may have involved people, shipping, and manufacturing in as many as 10 countries. Support from Great Britain exemplified the methods by which Iraq would circumvent export controls. Iraq bought at least one British company with operations in the United Kingdom and the United States, and had a complex relationship with France and the Soviet Union, its major suppliers of actual weapons. Turkey took action against the Kurds in 1986, alleging they were attacking the Kurdistan Workers' Party (PKK), which prompted a harsh diplomatic intervention by Iran, which planned a new offensive against Iraq at the time and were counting on the support of Kurdish factions. 

The United Nations Security Council initially called for a cease-fire after a week of fighting while Iraq was occupying Iranian territory, and renewed the call on later occasions. However, the UN did not come to Iran's aid to repel the Iraqi invasion, and the Iranians thus interpreted the UN as subtly biased in favour of Iraq.

Iraq's main financial backers were the oil-rich Persian Gulf states, most notably Saudi Arabia ($30.9 billion), Kuwait ($8.2 billion), and the United Arab Emirates ($8 billion). In all, Iraq received $35 billion in loans from the West and between $30 and $40 billion from the Persian Gulf states during the 1980s.

The Iraqgate scandal revealed that a branch of Italy's largest bank, Banca Nazionale del Lavoro (BNL), in Atlanta, Georgia, relied partially on U.S. taxpayer-guaranteed loans to funnel $5 billion to Iraq from 1985 to 1989. In August 1989, when FBI agents raided the Atlanta branch of BNL, branch manager Christopher Drogoul was charged with making unauthorised, clandestine, and illegal loans to Iraq—some of which, according to his indictment, were used to purchase arms and weapons technology. According to the "Financial Times", Hewlett-Packard, Tektronix, and Matrix Churchill's branch in Ohio were among the companies shipping militarily useful technology to Iraq under the eye of the U.S. government.

While the United States directly fought Iran, citing freedom of navigation as a major "casus belli", it also indirectly supplied some weapons to Iran as part of a complex and illegal programme that became known as the Iran–Contra affair. These secret sales were partly to help secure the release of hostages held in Lebanon, and partly to make money to help the Contras rebel group in Nicaragua. This arms-for-hostages agreement turned into a major scandal.

North Korea was a major arms supplier to Iran, often acting as a third party in arms deals between Iran and the Communist bloc. Support included domestically manufactured arms and Eastern-Bloc weapons, for which the major powers wanted deniability. Among the other arms suppliers and supporters of Iran's Islamic Revolution, the major ones were Libya, Syria, and China. According to the Stockholm International Peace Institute, China was the largest foreign arms supplier to Iran between 1980 and 1988.

Syria and Libya, breaking Arab solidarity, supported Iran with arms, rhetoric and diplomacy.

Besides the United States and the Soviet Union, Yugoslavia also sold weapons to both countries for the entire duration of the conflict. Likewise, Portugal helped both countries; it was not unusual to see Iranian and Iraqi flagged ships anchored at Setúbal, waiting their turn to dock.

From 1980 to 1987, Spain sold €458 million in weapons to Iran and €172 million to Iraq. Weapons sold to Iraq included 4x4 vehicles, BO-105 helicopters, explosives, and ammunition. A research party later discovered that an unexploded chemical Iraqi warhead in Iran was manufactured in Spain.

Although neither side acquired any weapons from Turkey, both sides enjoyed Turkish civilian trade during the conflict, although the Turkish government remained neutral and refused to support the U.S.-imposed trade embargo on Iran. Turkey's export market jumped from $220 million in 1981 to $2 billion in 1985, making up 25% of Turkey's overall exports. Turkish construction projects in Iraq totaled $2.5 billion between 1974 and 1990. Trading with both countries helped Turkey to offset its ongoing economic crisis, though the benefits decreased as the war neared its end and accordingly disappeared entirely with Iraq's invasion of Kuwait and the resulting Iraq sanctions Turkey imposed in response.

American support for Ba'athist Iraq during the Iran–Iraq War, in which it fought against post-revolutionary Iran, included several billion dollars' worth of economic aid, the sale of dual-use technology, non-U.S. origin weaponry, military intelligence, and special operations training. However, the U.S. did not directly supply arms to Iraq.

U.S. government support for Iraq was not a secret and was frequently discussed in open sessions of the Senate and House of Representatives. American views toward Iraq were not enthusiastically supportive in its conflict with Iran, and activity in assistance was largely to prevent an Iranian victory. This was encapsulated by Henry Kissinger when he remarked, "It's a pity they both can't lose."

A key element of U.S. political–military and energy–economic planning occurred in early 1983. The Iran–Iraq war had been going on for three years and there were significant casualties on both sides, reaching hundreds of thousands. Within the Reagan National Security Council concern was growing that the war could spread beyond the boundaries of the two belligerents. A National Security Planning Group meeting was called chaired by Vice President George Bush to review U.S. options. It was determined that there was a high likelihood that the conflict would spread into Saudi Arabia and other Gulf states, but that the United States had little capability to defend the region. Furthermore, it was determined that a prolonged war in the region would induce much higher oil prices and threaten the fragile world recovery which was just beginning to gain momentum. On 22 May 1984, President Reagan was briefed on the project conclusions in the Oval Office by William Flynn Martin who had served as the head of the NSC staff that organized the study. The full declassified presentation can be seen here. The conclusions were threefold: firstly, oil stocks needed to be increased among members of the International Energy Agency and, if necessary, released early in the event of oil market disruption; second, the United States needed to beef up the security of friendly Arab states in the region; and thirdly, an embargo should be placed on sales of military equipment to Iran and Iraq. The plan was approved by the President and later affirmed by the G-7 leaders headed by Margaret Thatcher in the London Summit of 1984.

According to "Foreign Policy", the "Iraqis used mustard gas and sarin prior to four major offensives in early 1988 that relied on U.S. satellite imagery, maps, and other intelligence. ... According to recently declassified CIA documents and interviews with former intelligence officials like Francona, the U.S. had firm evidence of Iraqi chemical attacks beginning in 1983."

On 17 May 1987, an Iraqi Dassault Mirage F1 fighter jet launched two Exocet missiles at the , a "Perry" class frigate. The first struck the port side of the ship and failed to explode, though it left burning propellant in its wake; the second struck moments later in approximately the same place and penetrated through to crew quarters, where it exploded, killing 37 crew members and leaving 21 injured. Whether or not Iraqi leadership authorised the attack is still unknown. Initial claims by the Iraqi government (that "Stark" was inside the Iran–Iraq War zone) were shown to be false, and the motives and orders of the pilot remain unanswered. Though American officials claimed that the pilot who attacked "Stark" had been executed, an ex-Iraqi Air Force commander since stated he had not been punished, and was still alive at the time. The attack remains the only successful anti-ship missile strike on an American warship. Due to the extensive political and military cooperation between the Iraqis and Americans by 1987, the attack had little effect on relations between the two countries.

U.S. attention was focused on isolating Iran as well as maintaining freedom of navigation. It criticised Iran's mining of international waters, and sponsored , which passed unanimously on 20 July, under which the U.S. and Iranian forces skirmished during Operation Earnest Will. During Operation Nimble Archer in October 1987, the United States attacked Iranian oil platforms in retaliation for an Iranian attack on the U.S.-flagged Kuwaiti tanker "Sea Isle City".

On 14 April 1988, the frigate was badly damaged by an Iranian mine, and 10 sailors were wounded. U.S. forces responded with Operation Praying Mantis on 18 April, the U.S. Navy's largest engagement of surface warships since World War II. Two Iranian oil platforms were destroyed, and five Iranian warships and gunboats were sunk. An American helicopter also crashed. This fighting manifested in the International Court of Justice as Oil Platforms case (Islamic Republic of Iran v. United States of America), which was eventually dismissed in 2003.

In the course of escorts by the U.S. Navy, the cruiser shot down Iran Air Flight 655 on 3 July 1988, killing all 290 passengers and crew on board. The American government claimed that "Vincennes" was in international waters at the time (which was later proven to be untrue), that the Airbus A300 had been mistaken for an Iranian F-14 Tomcat, and that "Vincennes" feared that she was under attack. The Iranians maintain that "Vincennes" was in their own waters, and that the passenger jet was turning away and increasing altitude after take-off. U.S. Admiral William J. Crowe later admitted on "Nightline" that "Vincennes" was in Iranian territorial waters when it launched the missiles. At the time of the attack, Admiral Crowe claimed that the Iranian plane did not identify itself and sent no response to warning signals he had sent. In 1996, the United States expressed their regret for the event and the civilian deaths it caused.

In a declassified 1991 report, the CIA estimated that Iran had suffered more than 50,000 casualties from Iraq's use of several chemical weapons, though current estimates are more than 100,000 as the long-term effects continue to cause casualties. The official CIA estimate did not include the civilian population contaminated in bordering towns or the children and relatives of veterans, many of whom have developed blood, lung and skin complications, according to the Organization for Veterans of Iran. According to a 2002 article in the "Star-Ledger", 20,000 Iranian soldiers were killed on the spot by nerve gas. As of 2002, 5,000 of the 80,000 survivors continue to seek regular medical treatment, while 1,000 are hospital inpatients.

According to Iraqi documents, assistance in developing chemical weapons was obtained from firms in many countries, including the United States, West Germany, the Netherlands, the United Kingdom, and France. A report stated that Dutch, Australian, Italian, French and both West and East German companies were involved in the export of raw materials to Iraqi chemical weapons factories. Declassified CIA documents show that the United States was providing reconnaissance intelligence to Iraq around 1987–88 which was then used to launch chemical weapon attacks on Iranian troops and that the CIA fully knew that chemical weapons would be deployed and sarin and cyclosarin attacks followed.

On 21 March 1986, the United Nations Security Council made a declaration stating that "members are profoundly concerned by the unanimous conclusion of the specialists that chemical weapons on many occasions have been used by Iraqi forces against Iranian troops, and the members of the Council strongly condemn this continued use of chemical weapons in clear violation of the Geneva Protocol of 1925, which prohibits the use in war of chemical weapons." The United States was the only member who voted against the issuance of this statement. A mission to the region in 1988 found evidence of the use of chemical weapons, and was condemned in Security Council Resolution 612.

According to Walter Lang, senior defense intelligence officer at the U.S. Defense Intelligence Agency, "the use of gas on the battlefield by the Iraqis was not a matter of deep strategic concern" to Reagan and his aides, because they "were desperate to make sure that Iraq did not lose". He claimed that the Defense Intelligence Agency "would have never accepted the use of chemical weapons against civilians, but the use against military objectives was seen as inevitable in the Iraqi struggle for survival". The Reagan administration did not stop aiding Iraq after receiving reports of the use of poison gas on Kurdish civilians.

The United States accused Iran of using chemical weapons as well, though the allegations have been disputed. Joost Hiltermann, the principal researcher for Human Rights Watch between 1992 and 1994, conducted a two-year study that included a field investigation in Iraq, and obtained Iraqi government documents in the process. According to Hiltermann, the literature on the Iran–Iraq War reflects allegations of chemical weapons used by Iran, but they are "marred by a lack of specificity as to time and place, and the failure to provide any sort of evidence".

Analysts Gary Sick and Lawrence Potter have called the allegations against Iran "mere assertions" and stated, "No persuasive evidence of the claim that Iran was the primary culprit [of using chemical weapons] was ever presented." Policy consultant and author Joseph Tragert stated, "Iran did not retaliate with chemical weapons, probably because it did not possess any at the time".

At his trial in December 2006, Saddam said he would take responsibility "with honour" for any attacks on Iran using conventional or chemical weapons during the war, but that he took issue with the charges that he ordered attacks on Iraqis. A medical analysis of the effects of Iraqi mustard gas is described in a U.S. military textbook and contrasted effects of World War I gas.

At the time of the conflict, the UN Security Council issued statements that "chemical weapons had been used in the war". UN statements never clarified that only Iraq was using chemical weapons, and according to retrospective authors "the international community remained silent as Iraq used weapons of mass destruction against Iranian[s] as well as Iraqi Kurds."

Iran's attack on the "Osirak" nuclear reactor in September 1980 was the first attack on a nuclear reactor and one of only six military attacks on nuclear facilities in history. It was also the first instance of a pre-emptive attack on a nuclear reactor to forestall the development of a nuclear weapon, though it did not achieve its objective, as France repaired the reactor after the attack. (It took a second pre-emptive strike by the Israeli Air Force in June 1981 to disable the reactor, killing a French engineer in the process and causing France to pull out of "Osirak". The decommissioning of "Osirak" has been cited as causing a substantial delay to Iraqi acquisition of nuclear weapons.)

The Iran–Iraq War was the first and only conflict in the history of warfare in which both forces used ballistic missiles against each other. This war also saw the only confirmed air-to-air helicopter battles in history with the Iraqi Mi-25s flying against Iranian AH-1J SeaCobras (supplied by the United States before the Iranian Revolution) on several separate occasions. In November 1980, not long after Iraq's initial invasion of Iran, two Iranian SeaCobras engaged two Mi-25s with TOW wire-guided antitank missiles. One Mi-25 went down immediately, the other was badly damaged and crashed before reaching base. The Iranians repeated this accomplishment on 24 April 1981, destroying two Mi-25s without incurring losses to themselves. One Mi-25 was also downed by an IRIAF F-14A. The Iraqis hit back, claiming the destruction of a SeaCobra on 14 September 1983 (with YaKB machine gun), then three SeaCobras on 5 February 1984 and three more on 25 February 1984 (two with Falanga missiles, one with S-5 rockets). After a lull in helicopter losses, each side lost a gunship on 13 February 1986. Later, a Mi-25 claimed a SeaCobra shot down with YaKB gun on 16 February, and a SeaCobra claimed a Mi-25 shot down with rockets on 18 February. The last engagement between the two types was on 22 May 1986, when Mi-25s shot down a SeaCobra. The final claim tally was 10 SeaCobras and 6 Mi-25s destroyed. The relatively small numbers and the inevitable disputes over actual kill numbers makes it unclear if one gunship had a real technical superiority over the other. Iraqi Mi-25s also claimed 43 kills against other Iranian helicopters, such as Agusta-Bell UH-1 Hueys. Both sides, especially Iraq, also carried out air and missile attacks against population centers.

In October 1986, Iraqi aircraft began to attack civilian passenger trains and aircraft on Iranian soil, including an Iran Air Boeing 737 unloading passengers at Shiraz International Airport. In retaliation for the Iranian Operation Karbala 5, Iraq attacked 65 cities in 226 sorties over 42 days, bombing civilian neighbourhoods. Eight Iranian cities came under attack from Iraqi missiles. The bombings killed 65 children in an elementary school in Borujerd. The Iranians responded with Scud missile attacks on Baghdad and struck a primary school there. These events became known as the "War of the Cities".

Despite the war, Iran and Iraq maintained diplomatic relations and embassies in each other's countries until mid-1987.

Iran's government used human waves to attack enemy troops and even in some cases to clear minefields. Children volunteered as well. Some reports mistakenly have the Basijis marching into battle while marking their expected entry to heaven by wearing "plastic keys to paradise" around their necks, although other analysts regard this story as a hoax involving a misinterpretation of the carrying of a prayer book called "The Keys to Paradise"(Mafatih al-Janan) by Sheikh Abbas Qumi given to all volunteers.

According to journalist Robin Wright:

During the Fateh offensive in February 1987, I toured the southwest front on the Iranian side and saw scores of boys, aged anywhere from nine to sixteen, who said with staggering and seemingly genuine enthusiasm that they had volunteered to become martyrs. Regular army troops, the paramilitary Revolutionary Guards and mullahs all lauded these youths, known as baseeji [Basij], for having played the most dangerous role in breaking through Iraqi lines. They had led the way, running over fields of mines to clear the ground for the Iranian ground assault. Wearing white headbands to signify the embracing of death, and shouting "Shaheed, shaheed" (Martyr, martyr) they literally blew their way into heaven. Their numbers were never disclosed. But a walk through the residential suburbs of Iranian cities provided a clue. Window after window, block after block, displayed black-bordered photographs of teenage or preteen youths.

The relationship between these two nations has warmed immensely since the downfall of Saddam Hussein, but mostly out of pragmatic interest. Iran and Iraq share many common interests, as they share a common enemy in the Islamic State. Significant military assistance has been provided by Iran to Iraq and this has bought them a large amount of political influence in Iraq's newly elected Shia government. Iraq is also heavily dependent on the more stable and developed Iran for its energy needs, so a peaceful customer is likely a high priority for Iran, foreign policy wise.

The Iran- Iraq War is regarded as being a major trigger for rising sectarianism in the region, as it was viewed by many as a clash between Sunni Muslims (Iraq and other Arab States) and the Shia revolutionaries that had recently taken power in Iran. There remains lingering animosity however; despite the pragmatic alliance that has been formed as multiple government declarations from Iran have stated that the war will "affect every issue of internal and foreign policy" for decades to come. The sustained importance of this conflict is attributed mostly to the massive human and economic cost resulting from it, along with its ties to the Iranian Revolution. Another significant effect that the war has on Iran's policy is the issue of remaining war reparations. The UN estimates that Iraq owes about $149 billion, while Iran contends that, with both the direct and indirect effects taken into account, the cost of the war reaches a trillion. Iran has not vocalized the desire for these reparations in recent years, and has even suggested forms of financial aid. This is due most likely to Iran's interest in keeping Iraq politically stable, and imposing these reparation costs would further burden the already impoverished nation. The most important factor that governs Iraq's current foreign policy is the national government's consistent fragility following the overthrow of Saddam Hussein. Iraq's need for any and all allies that can help bring stability and bring development has allowed Iran to exert significant influence over the new Iraqi state; despite lingering memories of the war. Iraq is far too weak of a state to attempt to challenge Iran regionally, so accepting support while focusing on counter insurgency and stabilization is in their best interest.

Currently, it seems as though Iraq is being pulled in two opposing directions, between a practical relationship with Iran, who can provide a reliable source of power as well as military support to the influential Shia militias and political factions. The United States is pulling in the opposite direction as they offer Iraq significant economic aid packages, along with military support in the form of air and artillery strikes, all in the hopes to establish a stable ally in the region. If Iraq lurches too far in either direction, then the benefits offered to them by the other side will likely be gradually reduced or cut off completely. Another significant factor influencing relations is the shared cultural interests of their respective citizens, as they both wish to freely visit the multitude of holy sites located in both countries.









</doc>
<doc id="14891" url="https://en.wikipedia.org/wiki?curid=14891" title="Incremental reading">
Incremental reading

Incremental reading is a software-assisted method for learning and retaining information from reading, which involves the creation of flashcards out of electronic articles. "Incremental reading" means "reading in portions". Instead of a linear reading of articles one at a time, the method works by keeping a large reading list of electronic articles or books (often dozens or hundreds of them) and reading parts of several articles in each session. Articles in the reading list are prioritized by the user. In the course of reading, key points of articles are broken up into flashcards, which are then learned and reviewed over an extended period of time with the help of a spaced repetition algorithm.

This use of flashcards at later stages of the process is based on the spacing effect (the phenomenon whereby learning is greater when studying is spread out over time) and the testing effect (the finding that long-term memory is increased when some of the learning period is devoted to retrieving the to-be-remembered information through testing). It is targeted towards people who are trying to learn for life a large amount of information, particularly if that information comes from various sources.

The method itself is often credited to the Polish software developer Piotr Wozniak. He implemented the first version of incremental reading in 1999 in SuperMemo 99, providing the essential tools of the method: a prioritized reading list, and the possibility to extract portions of articles and to create cloze deletions. The term "incremental reading" itself appeared the next year with SuperMemo 2000. Later SuperMemo programmes subsequently enhanced the tools and techniques involved, such as webpage imports, material overload handling, etc.

Limited incremental reading support for the text editor Emacs appeared in 2007.

An Anki add-on for incremental reading was later published in 2011; for Anki 2.0 and 2.1, another add-on is available.

Incremental reading was the first of a series of related concepts invented by Piotr Wozniak: incremental image learning, incremental video, incremental audio, incremental mail processing, incremental problem solving, and incremental writing. "Incremental learning" is the term Wozniak uses to refer to those concepts as a whole.

When reading an electronic article, the user extracts the most important parts (similar to underlining or highlighting a paper article) and gradually distills them into flashcards. Flashcards are information presented in a question-answer format (making active recall possible). Cloze deletions are often used in incremental reading, as they are easy to create out of text. Both extracts and flashcards are scheduled independently from the original article.

With time and reviews, articles are supposed to be gradually converted into extracts, and extracts into flashcards. Hence, incremental reading is a method of breaking down information from electronic articles into sets of flashcards.

Contrary to extracts, flashcards are reviewed with active recall. This means that extracts such as "George Washington was the first U.S. President" must ultimately be converted into questions such as "Who was the first U.S. President?" (Answer: George Washington), or "Who was George Washington?" (Answer: the first U.S. President), etc., or cloze deletions such as "[BLANK] was the first U.S. President", "George Washington was [BLANK]", etc.

This flashcard creation process is semi-automated – the reader chooses which material to learn and edits the precise wording of the questions, while the software assists in prioritizing articles and making the flashcards, and does the scheduling: it calculates the time for the reader to review each chunk, according to the rules of a spaced repetition algorithm. This means that all processed pieces of information are presented at increasing intervals.

Individual articles are read in portions proportional to the attention span, which depends on the user, their mood, the article, etc. This allows for a substantial gain in attention, according to Piotr Wozniak.

Without the use of spaced repetition, the reader would quickly get lost in the glut of information when studying dozens of subjects in parallel. However, spaced repetition makes it possible to retain traces of the processed material in memory.



</doc>
<doc id="14892" url="https://en.wikipedia.org/wiki?curid=14892" title="Intelligence quotient">
Intelligence quotient

An intelligence quotient (IQ) is a total score derived from a set of standardized tests or subtests designed to assess human intelligence. The abbreviation "IQ" was coined by the psychologist William Stern for the German term "Intelligenzquotient", his term for a scoring method for intelligence tests at University of Breslau he advocated in a 1912 book.

Historically, IQ was a score obtained by dividing a person's mental age score, obtained by administering an intelligence test, by the person's chronological age, both expressed in terms of years and months. The resulting fraction (quotient) is multiplied by 100 to obtain the IQ score. For modern IQ tests, the median raw score of the norming sample is defined as IQ 100 and scores each standard deviation (SD) up or down are defined as 15 IQ points greater or less. By this definition, approximately two-thirds of the population scores are between IQ 85 and IQ 115. About 2.5 percent of the population scores above 130, and 2.5 percent below 70.

Scores from intelligence tests are estimates of intelligence. Unlike, for example, distance and mass, a concrete measure of intelligence cannot be achieved given the abstract nature of the concept of "intelligence". IQ scores have been shown to be associated with such factors as morbidity and mortality, parental social status, and, to a substantial degree, biological parental IQ. While the heritability of IQ has been investigated for nearly a century, there is still debate about the significance of heritability estimates and the mechanisms of inheritance.

IQ scores are used for educational placement, assessment of intellectual disability, and evaluating job applicants. In research contexts, they have been studied as predictors of job performance and income. They are also used to study distributions of psychometric intelligence in populations and the correlations between it and other variables. Raw scores on IQ tests for many populations have been rising at an average rate that scales to three IQ points per decade since the early 20th century, a phenomenon called the Flynn effect. Investigation of different patterns of increases in subtest scores can also inform current research on human intelligence.

Historically, even before IQ tests were devised, there were attempts to classify people into intelligence categories by observing their behavior in daily life. Those other forms of behavioral observation are still important for validating classifications based primarily on IQ test scores. Both intelligence classification by observation of behavior outside the testing room and classification by IQ testing depend on the definition of "intelligence" used in a particular case and on the reliability and error of estimation in the classification procedure.

The English statistician Francis Galton made the first attempt at creating a standardized test for rating a person's intelligence. A pioneer of psychometrics and the application of statistical methods to the study of human diversity and the study of inheritance of human traits, he believed that intelligence was largely a product of heredity (by which he did not mean genes, although he did develop several pre-Mendelian theories of particulate inheritance). He hypothesized that there should exist a correlation between intelligence and other observable traits such as reflexes, muscle grip, and head size. He set up the first mental testing center in the world in 1882 and he published "Inquiries into Human Faculty and Its Development" in 1883, in which he set out his theories. After gathering data on a variety of physical variables, he was unable to show any such correlation, and he eventually abandoned this research.French psychologist Alfred Binet, together with Victor Henri and Théodore Simon had more success in 1905, when they published the Binet-Simon test, which focused on verbal abilities. It was intended to identify mental retardation in school children, but in specific contradistinction to claims made by psychiatrists that these children were "sick" (not "slow") and should therefore be removed from school and cared for in asylums. The score on the Binet-Simon scale would reveal the child's mental age. For example, a six-year-old child who passed all the tasks usually passed by six-year-olds—but nothing beyond—would have a mental age that matched his chronological age, 6.0. (Fancher, 1985). Binet thought that intelligence was multifaceted, but came under the control of practical judgment.

In Binet's view, there were limitations with the scale and he stressed what he saw as the remarkable diversity of intelligence and the subsequent need to study it using qualitative, as opposed to quantitative, measures (White, 2000). American psychologist Henry H. Goddard published a translation of it in 1910. American psychologist Lewis Terman at Stanford University revised the Binet-Simon scale, which resulted in the Stanford-Binet Intelligence Scales (1916). It became the most popular test in the United States for decades.

The many different kinds of IQ tests include a wide variety of item content. Some test items are visual, while many are verbal. Test items vary from being based on abstract-reasoning problems to concentrating on arithmetic, vocabulary, or general knowledge.

The British psychologist Charles Spearman in 1904 made the first formal factor analysis of correlations between the tests. He observed that children's school grades across seemingly unrelated school subjects were positively correlated, and reasoned that these correlations reflected the influence of an underlying general mental ability that entered into performance on all kinds of mental tests. He suggested that all mental performance could be conceptualized in terms of a single general ability factor and a large number of narrow task-specific ability factors. Spearman named it "g" for "general factor" and labeled the specific factors or abilities for specific tasks "s". In any collection of test items that make up an IQ test, the score that best measures "g" is the composite score that has the highest correlations with all the item scores. Typically, the ""g"-loaded" composite score of an IQ test battery appears to involve a common strength in abstract reasoning across the test's item content.

Spearman's argument proposing a general factor of human intelligence is still accepted, in principle, by many psychometricians. Today's factor models of intelligence typically represent cognitive abilities as a three-level hierarchy, where there are a large number of narrow factors at the bottom of the hierarchy, a handful of broad, more general factors at the intermediate level, and at the apex a single factor, referred to as the "g" factor, which represents the variance common to all cognitive tasks.

During World War I, the Army needed a way to evaluate and assign recruits to appropriate tasks. This led to the development of several mental tests by Robert Yerkes, who worked with major hereditarians of American psychometrics—including Terman, Goddard—to write the test. The testing generated controversy and much public debate in the United States. Nonverbal or "performance" tests were developed for those who could not speak English or were suspected of malingering. Based on Goddard's translation of the Binet-Simon test, the tests had an impact in screening men for officer training:...the tests did have a strong impact in some areas, particularly in screening men for officer training. At the start of the war, the army and national guard maintained nine thousand officers. By the end, two hundred thousand officers presided, and two- thirds of them had started their careers in training camps where the tests were applied. In some camps, no man scoring below C could be considered for officer training.1.75 million men were tested in total, making the results the first mass-produced written tests of intelligence, though considered dubious and non-usable, for reasons including high variability of test implementation throughout different camps and questions testing for familiarity with American culture rather than intelligence. After the war, positive publicity promoted by army psychologists helped to make psychology a respected field. Subsequently, there was an increase in jobs and funding in psychology in the United States. Group intelligence tests were developed and became widely used in schools and industry.

The results of these tests, which at the time reaffirmed contemporary racism and nationalism, are considered controversial and dubious, having rested on certain contested assumptions: that intelligence was heritable, innate, and could be relegated to a single number, the tests were enacted systematically, and test questions actually tested for innate intelligence rather than subsuming environmental factors. The tests also allowed for the bolstering of jingoist narratives in the context of increased immigration, which may have influenced the passing of the Immigration Restriction Act of 1924.

L.L. Thurstone argued for a model of intelligence that included seven unrelated factors (verbal comprehension, word fluency, number facility, spatial visualization, associative memory, perceptual speed, reasoning, and induction). While not widely used, Thurstone's model influenced later theories.

David Wechsler produced the first version of his test in 1939. It gradually became more popular and overtook the Stanford-Binet in the 1960s. It has been revised several times, as is common for IQ tests, to incorporate new research. One explanation is that psychologists and educators wanted more information than the single score from the Binet. Wechsler's ten or more subtests provided this. Another is that the Stanford-Binet test reflected mostly verbal abilities, while the Wechsler test also reflected nonverbal abilities. The Stanford-Binet has also been revised several times and is now similar to the Wechsler in several aspects, but the Wechsler continues to be the most popular test in the United States.

Eugenics, a set of beliefs and practices aimed at improving the genetic quality of the human population by excluding people and groups judged to be inferior and promoting those judged to be superior, played a significant role in the history and culture of the United States during the Progressive Era, from the late 19th century until US involvement in World War II.

The American eugenics movement was rooted in the biological determinist ideas of the British Scientist Sir Francis Galton. In 1883, Galton first used the word eugenics to describe the biological improvement of human genes and the concept of being "well-born". He believed that differences in a person's ability were acquired primarily through genetics and that eugenics could be implemented through selective breeding in order for the human race to improve in its overall quality, therefore allowing for humans to direct their own evolution.

Goddard was a eugenicist. In 1908, he published his own version, "The Binet and Simon Test of Intellectual Capacity", and cordially promoted the test. He quickly extended the use of the scale to the public schools (1913), to immigration (Ellis Island, 1914) and to a court of law (1914).

Unlike Galton, who promoted eugenics through selective breeding for positive traits, Goddard went with the US eugenics movement to eliminate "undesirable" traits. Goddard used the term "feeble-minded" to refer to people who did not perform well on the test. He argued that "feeble-mindedness" was caused by heredity, and thus feeble-minded people should be prevented from giving birth, either by institutional isolation or sterilization surgeries. At first, sterilization targeted the disabled, but was later extended to poor people. Goddard's intelligence test was endorsed by the eugenicists to push for laws for forced sterilization. Different states adopted the sterilization laws at different pace. These laws, whose constitutionality was upheld by the Supreme Court in their 1927 ruling Buck v. Bell, forced over 64,000 people to go through sterilization in the United States.

California's sterilization program was so effective that the Nazis turned to the government for advice on how to prevent the birth of the "unfit". While the US eugenics movement lost much of its momentum in the 1940s in view of the horrors of Nazi Germany, advocates of eugenics (including Nazi geneticist Otmar Freiherr von Verschuer) continued to work and promote their ideas in the United States. In later decades, some eugenic principles have made a resurgence as a voluntary means of selective reproduction, with some calling them "new eugenics". As it becomes possible to test for and correlate genes with IQ (and its proxies), ethicists and embryonic genetic testing companies are attempting to understand the ways in which the technology can be ethically deployed.

Raymond Cattell (1941) proposed two types of cognitive abilities in a revision of Spearman's concept of general intelligence. Fluid intelligence (Gf) was hypothesized as the ability to solve novel problems by using reasoning, and crystallized intelligence (Gc) was hypothesized as a knowledge-based ability that was very dependent on education and experience. In addition, fluid intelligence was hypothesized to decline with age, while crystallized intelligence was largely resistant to the effects of aging. The theory was almost forgotten, but was revived by his student John L. Horn (1966) who later argued Gf and Gc were only two among several factors, and who eventually identified nine or ten broad abilities. The theory continued to be called Gf-Gc theory.

John B. Carroll (1993), after a comprehensive reanalysis of earlier data, proposed the three stratum theory, which is a hierarchical model with three levels. The bottom stratum consists of narrow abilities that are highly specialized (e.g., induction, spelling ability). The second stratum consists of broad abilities. Carroll identified eight second-stratum abilities. Carroll accepted Spearman's concept of general intelligence, for the most part, as a representation of the uppermost, third stratum.

In 1999, a merging of the Gf-Gc theory of Cattell and Horn with Carroll's Three-Stratum theory has led to the Cattell–Horn–Carroll theory (CHC Theory). It has greatly influenced many of the current broad IQ tests.

In CHC theory, a hierarchy of factors is used; "g" is at the top. Under it are ten broad abilities that in turn are subdivided into seventy narrow abilities. The broad abilities are:
Modern tests do not necessarily measure all of these broad abilities. For example, Gq and Grw may be seen as measures of school achievement and not IQ. Gt may be difficult to measure without special equipment. "g" was earlier often subdivided into only Gf and Gc, which were thought to correspond to the nonverbal or performance subtests and verbal subtests in earlier versions of the popular Wechsler IQ test. More recent research has shown the situation to be more complex. Modern comprehensive IQ tests do not stop at reporting a single IQ score. Although they still give an overall score, they now also give scores for many of these more restricted abilities, identifying particular strengths and weaknesses of an individual.

An alternative to standard IQ tests, meant to test the proximal development of children, originated in the writings of psychologist Lev Vygotsky (1896–1934) during his last two years of his life. According to Vygotsky, the maximum level of complexity and difficulty of problems that a child is capable to solve under some guidance indicates their level of potential development. The difference between this level of potential and the lower level of unassisted performance indicates the child's zone of proximal development. Combination of the two indexesthe level of actual and the zone of the proximal developmentaccording to Vygotsky, provides a significantly more informative indicator of psychological development than the assessment of the level of actual development alone. His ideas on the zone of development were later developed in a number of psychological and educational theories and practices, most notably under the banner of dynamic assessment, which seeks to measure developmental potential (for instance, in the work of Reuven Feuerstein and his associates, who has criticized standard IQ testing for its putative assumption or acceptance of "fixed and immutable" characteristics of intelligence or cognitive functioning). Dynamic assessment has been further elaborated in the work of Ann Brown, and John D. Bransford and in theories of multiple intelligences authored by Howard Gardner and Robert Sternberg.

J.P. Guilford's Structure of Intellect (1967) model of intelligence used three dimensions which when combined yielded a total of 120 types of intelligence. It was popular in the 1970s and early 1980s, but faded owing to both practical problems and theoretical criticisms.

Alexander Luria's earlier work on neuropsychological processes led to the PASS theory (1997). It argued that only looking at one general factor was inadequate for researchers and clinicians who worked with learning disabilities, attention disorders, intellectual disability, and interventions for such disabilities. The PASS model covers four kinds of processes (planning process, attention/arousal process, simultaneous processing, and successive processing). The planning processes involve decision making, problem solving, and performing activities and requires goal setting and self-monitoring.

The attention/arousal process involves selectively attending to a particular stimulus, ignoring distractions, and maintaining vigilance. Simultaneous processing involves the integration of stimuli into a group and requires the observation of relationships. Successive processing involves the integration of stimuli into serial order. The planning and attention/arousal components comes from structures located in the frontal lobe, and the simultaneous and successive processes come from structures located in the posterior region of the cortex. It has influenced some recent IQ tests, and been seen as a complement to the Cattell-Horn-Carroll theory described above.

There are a variety of individually administered IQ tests in use in the English-speaking world. The most commonly used individual IQ test series is the Wechsler Adult Intelligence Scale (WAIS) for adults and the Wechsler Intelligence Scale for Children (WISC) for school-age test-takers. Other commonly used individual IQ tests (some of which do not label their standard scores as "IQ" scores) include the current versions of the Stanford-Binet Intelligence Scales, Woodcock-Johnson Tests of Cognitive Abilities, the Kaufman Assessment Battery for Children, the Cognitive Assessment System, and the Differential Ability Scales.

IQ tests that measure intelligence also include:

IQ scales are ordinally scaled. While one standard deviation is 15 points, and two SDs are 30 points, and so on, this does not imply that mental ability is linearly related to IQ, such that IQ 50 means half the cognitive ability of IQ 100. In particular, IQ points are not percentage points.

Psychometricians generally regard IQ tests as having high statistical reliability. Reliability represents the measurement consistency of a test. A reliable test produces similar scores upon repetition. On aggregate, IQ tests exhibit high reliability, although test-takers may have varying scores when taking the same test on differing occasions, and although they may have varying scores when taking different IQ tests at the same age. Like all statistical quantities, any particular estimate of IQ has an associated standard error that measures uncertainty about the estimate. For modern tests, the standard error of measurement is about three points.

For individuals with very low scores, the 95% confidence interval may be greater than 40 points, potentially complicating the accuracy of diagnoses of intellectual disability. By the same token, high IQ scores are also significantly less reliable than those near to the population median. Reports of IQ scores much higher than 160 are considered dubious.

With regard to unrepresentative scores, low motivation or high anxiety can occasionally lower a person's score.

Reliability and validity are very different concepts. While reliability reflects reproducibility, validity refers to lack of bias. A biased test does not measure what it purports to measure. While IQ tests are generally considered to measure some forms of intelligence, they may fail to serve as an accurate measure of broader definitions of human intelligence such as creativity and social intelligence. For this reason, Psychologist Wayne Weiten argues that their construct validity must be carefully qualified, and not be overstated. According to Weiten, "IQ tests are valid measures of the kind of intelligence necessary to do well in academic work. But if the purpose is to assess intelligence in a broader sense, the validity of IQ tests is questionable."

Along these same lines, critics such as Keith Stanovich do not dispute the capacity of IQ test scores to predict some kinds of achievement, but argue that basing a concept of intelligence on IQ test scores alone neglects other important aspects of mental ability. Robert Sternberg, another significant critic of IQ as the main measure of human cognitive abilities, argued that reducing the concept of intelligence to the measure of "g" does not fully account for the different skills and knowledge types that produce success in human society.

A 2005 study found that "differential validity in prediction suggests that the WAIS-R test may contain cultural influences that reduce the validity of the WAIS-R as a measure of cognitive ability for Mexican American students," indicating a weaker positive correlation relative to sampled white students. Other recent studies have questioned the culture-fairness of IQ tests when used in South Africa. Standard intelligence tests, such as the Stanford-Binet, are often inappropriate for autistic children; the alternative of using developmental or adaptive skills measures are relatively poor measures of intelligence in autistic children, and may have resulted in incorrect claims that a majority of autistic children are of low intelligence.

Some scientists have disputed the value of IQ as a measure of intelligence altogether. In "The Mismeasure of Man" (1981, expanded edition 1996), evolutionary biologist Stephen Jay Gould compared IQ testing with the now-discredited practice of determining intelligence via craniometry, arguing that both are based on the fallacy of reification, “our tendency to convert abstract concepts into entities”. Gould's argument sparked a great deal of debate, and the book is listed as one of "Discover Magazine"'s "25 Greatest Science Books of All Time".

Despite these objections, clinical psychologists generally regard IQ scores as having sufficient statistical validity for many clinical purposes.

Differential item functioning (DIF), sometimes referred to as measurement bias, is a phenomenon when participants from different groups (e.g. gender, race, disability) with the same latent abilities give different answers to specific questions on the same IQ test. DIF analysis measures such specific items on a test alongside measuring participants' latent abilities on other similar questions. A consistent different group response to a specific question among similar type of questions can indicate an effect of DIF. It does not count as differential item functioning if both groups have an equally valid chance of giving different responses to the same questions. Such bias can be a result of culture, educational level and other factors that are independent of group traits. DIF is only considered if test-takers from different groups "with the same underlying latent ability level" have a different chance of giving specific responses. Such questions are usually removed in order to make the test equally fair for both groups. Common techniques for analyzing DIF are item response theory (IRT) based methods, Mantel-Haenszel, and logistic regression.

Since the early 20th century, raw scores on IQ tests have increased in most parts of the world. When a new version of an IQ test is normed, the standard scoring is set so performance at the population median results in a score of IQ 100. The phenomenon of rising raw score performance means if test-takers are scored by a constant standard scoring rule, IQ test scores have been rising at an average rate of around three IQ points per decade. This phenomenon was named the Flynn effect in the book "The Bell Curve" after James R. Flynn, the author who did the most to bring this phenomenon to the attention of psychologists.

Researchers have been exploring the issue of whether the Flynn effect is equally strong on performance of all kinds of IQ test items, whether the effect may have ended in some developed nations, whether there are social subgroup differences in the effect, and what possible causes of the effect might be. A 2011 textbook, "IQ and Human Intelligence", by N. J. Mackintosh, noted the Flynn effect demolishes the fears that IQ would be decreased. He also asks whether it represents a real increase in intelligence beyond IQ scores. A 2011 psychology textbook, lead authored by Harvard Psychologist Professor Daniel Schacter, noted that humans' inherited intelligence could be going down while acquired intelligence goes up.

Research has revealed that the Flynn effect has slowed or reversed course in several Western countries beginning in the late 20th century. The phenomenon has been termed the "negative Flynn effect". A study of Norwegian military conscripts' test records found that IQ scores have been falling for generations born after the year 1975, and that the underlying nature of both initial increasing and subsequent falling trends appears to be environmental rather than genetic.

IQ can change to some degree over the course of childhood. However, in one longitudinal study, the mean IQ scores of tests at ages 17 and 18 were correlated at r=0.86 with the mean scores of tests at ages five, six, and seven and at r=0.96 with the mean scores of tests at ages 11, 12, and 13.

For decades, practitioners' handbooks and textbooks on IQ testing have reported IQ declines with age after the beginning of adulthood. However, later researchers pointed out this phenomenon is related to the Flynn effect and is in part a cohort effect rather than a true aging effect. A variety of studies of IQ and aging have been conducted since the norming of the first Wechsler Intelligence Scale drew attention to IQ differences in different age groups of adults. Current consensus is that fluid intelligence generally declines with age after early adulthood, while crystallized intelligence remains intact. Both cohort effects (the birth year of the test-takers) and practice effects (test-takers taking the same form of IQ test more than once) must be controlled to gain accurate data. It is unclear whether any lifestyle intervention can preserve fluid intelligence into older ages.

The exact peak age of fluid intelligence or crystallized intelligence remains elusive. Cross-sectional studies usually show that especially fluid intelligence peaks at a relatively young age (often in the early adulthood) while longitudinal data mostly show that intelligence is stable until mid-adulthood or later. Subsequently, intelligence seems to decline slowly.

Environmental and genetic factors play a role in determining IQ. Their relative importance has been the subject of much research and debate.

The general figure for the heritability of IQ, according to an authoritative American Psychological Association report, is 0.45 for children, and rises to around 0.75 for late adolescents and adults. Heritability measures in infancy are as low as 0.2, around 0.4 in middle childhood, and as high as 0.9 in adulthood. One proposed explanation is that people with different genes tend to reinforce the effects of those genes, for example by seeking out different environments.

Family members have aspects of environments in common (for example, characteristics of the home). This shared family environment accounts for 0.25–0.35 of the variation in IQ in childhood. By late adolescence, it is quite low (zero in some studies). The effect for several other psychological traits is similar. These studies have not looked at the effects of extreme environments, such as in abusive families.

Although parents treat their children differently, such differential treatment explains only a small amount of nonshared environmental influence. One suggestion is that children react differently to the same environment because of different genes. More likely influences may be the impact of peers and other experiences outside the family.

A very large proportion of the over 17,000 human genes are thought to have an effect on the development and functionality of the brain. While a number of individual genes have been reported to be associated with IQ, none have a strong effect. Deary and colleagues (2009) reported that no finding of a strong single gene effect on IQ has been replicated. Recent findings of gene associations with normally varying intellectual differences in adults and children continue to show weak effects for any one gene.

David Rowe reported an interaction of genetic effects with socioeconomic status, such that the heritability was high in high-SES families, but much lower in low-SES families. In the US, this has been replicated in infants, children, adolescents, and adults. Outside the US, studies show no link between heritability and SES. Some effects may even reverse sign outside the US.

Dickens and Flynn (2001) have argued that genes for high IQ initiate an environment-shaping feedback cycle, with genetic effects causing bright children to seek out more stimulating environments that then further increase their IQ. In Dickens' model, environment effects are modeled as decaying over time. In this model, the Flynn effect can be explained by an increase in environmental stimulation independent of it being sought out by individuals. The authors suggest that programs aiming to increase IQ would be most likely to produce long-term IQ gains if they enduringly raised children's drive to seek out cognitively demanding experiences.

In general, educational interventions, as those described below, have shown short-term effects on IQ, but long-term follow-up is often missing. For example, in the US, very large intervention programs such as the Head Start Program have not produced lasting gains in IQ scores. Even when students improve their scores on standardized tests, they do not always improve their cognitive abilities, such as memory, attention and speed. More intensive, but much smaller projects, such as the Abecedarian Project, have reported lasting effects, often on socioeconomic status variables, rather than IQ.

Recent studies have shown that training in using one's working memory may increase IQ. A study on young adults published in April 2008 by a team from the Universities of Michigan and Bern supports the possibility of the transfer of fluid intelligence from specifically designed working memory training. Further research will be needed to determine nature, extent and duration of the proposed transfer. Among other questions, it remains to be seen whether the results extend to other kinds of fluid intelligence tests than the matrix test used in the study, and if so, whether, after training, fluid intelligence measures retain their correlation with educational and occupational achievement or if the value of fluid intelligence for predicting performance on other tasks changes. It is also unclear whether the training is durable of extended periods of time.

Musical training in childhood correlates with higher than average IQ. However, a study of 10,500 twins found no effects on IQ, suggesting that the correlation was caused by genetic confounders. A meta-analysis concluded that "Music training does not reliably enhance children and young adolescents' cognitive or academic skills, and that previous positive findings were probably due to confounding variables."

It is popularly thought that listening to classical music raises IQ. However, multiple attempted replications (e.g.) have shown that this is at best a short-term effect (lasting no longer than 10 to 15 minutes), and is not related to IQ-increase.

Several neurophysiological factors have been correlated with intelligence in humans, including the ratio of brain weight to body weight and the size, shape, and activity level of different parts of the brain. Specific features that may affect IQ include the size and shape of the frontal lobes, the amount of blood and chemical activity in the frontal lobes, the total amount of gray matter in the brain, the overall thickness of the cortex, and the glucose metabolic rate.

Health is important in understanding differences in IQ test scores and other measures of cognitive ability. Several factors can lead to significant cognitive impairment, particularly if they occur during pregnancy and childhood when the brain is growing and the blood–brain barrier is less effective. Such impairment may sometimes be permanent, sometimes be partially or wholly compensated for by later growth. 

Since about 2010, researchers such as Eppig, Hassel, and MacKenzie have found a very close and consistent link between IQ scores and infectious diseases, especially in the infant and preschool populations and the mothers of these children. They have postulated that fighting infectious diseases strains the child's metabolism and prevents full brain development. Hassel postulated that it is by far the most important factor in determining population IQ. However, they also found that subsequent factors such as good nutrition and regular quality schooling can offset early negative effects to some extent.

Developed nations have implemented several health policies regarding nutrients and toxins known to influence cognitive function. These include laws requiring fortification of certain food products and laws establishing safe levels of pollutants (e.g. lead, mercury, and organochlorides). Improvements in nutrition, and in public policy in general, have been implicated in worldwide IQ increases. 

Cognitive epidemiology is a field of research that examines the associations between intelligence test scores and health. Researchers in the field argue that intelligence measured at an early age is an important predictor of later health and mortality differences.

The American Psychological Association's report "Intelligence: Knowns and Unknowns" states that wherever it has been studied, children with high scores on tests of intelligence tend to learn more of what is taught in school than their lower-scoring peers. The correlation between IQ scores and grades is about .50. This means that the explained variance is 25%. Achieving good grades depends on many factors other than IQ, such as "persistence, interest in school, and willingness to study" (p. 81).

It has been found that the correlation of IQ scores with school performance depends on the IQ measurement used. For undergraduate students, the Verbal IQ as measured by WAIS-R has been found to correlate significantly (0.53) with the grade point average (GPA) of the last 60 hours (credits). In contrast, Performance IQ correlation with the same GPA was only 0.22 in the same study.

Some measures of educational aptitude correlate highly with IQ tests for instance, Frey and Detterman (2004) reported a correlation of 0.82 between "g" (general intelligence factor) and SAT scores; another research found a correlation of 0.81 between "g" and GCSE scores, with the explained variance ranging "from 58.6% in Mathematics and 48% in English to 18.1% in Art and Design".

According to Schmidt and Hunter, "for hiring employees without previous experience in the job the most valid predictor of future performance is general mental ability." The validity of IQ as a predictor of job performance is above zero for all work studied to date, but varies with the type of job and across different studies, ranging from 0.2 to 0.6. The correlations were higher when the unreliability of measurement methods was controlled for. While IQ is more strongly correlated with reasoning and less so with motor function, IQ-test scores predict performance ratings in all occupations. That said, for highly qualified activities (research, management) low IQ scores are more likely to be a barrier to adequate performance, whereas for minimally-skilled activities, athletic strength (manual strength, speed, stamina, and coordination) is more likely to influence performance. The prevailing view among academics is that it is largely through the quicker acquisition of job-relevant knowledge that higher IQ mediates job performance. This view has been challenged by Byington & Felps (2010), who argued that "the current applications of IQ-reflective tests allow individuals with high IQ scores to receive greater access to developmental resources, enabling them to acquire additional capabilities over time, and ultimately perform their jobs better."

In establishing a causal direction to the link between IQ and work performance, longitudinal studies by Watkins and others suggest that IQ exerts a causal influence on future academic achievement, whereas academic achievement does not substantially influence future IQ scores. Treena Eileen Rohde and Lee Anne Thompson write that general cognitive ability, but not specific ability scores, predict academic achievement, with the exception that processing speed and spatial ability predict performance on the SAT math beyond the effect of general cognitive ability.

The US military has minimum enlistment standards at about the IQ 85 level. There have been two experiments with lowering this to 80 but in both cases these men could not master soldiering well enough to justify their costs.

It has been suggested that "in economic terms it appears that the IQ score measures something with decreasing marginal value" and it "is important to have enough of it, but having lots and lots does not buy you that much". However, large-scale longitudinal studies indicate an increase in IQ translates into an increase in performance at all levels of IQ: i.e. ability and job performance are monotonically linked at all IQ levels. 

The link from IQ to wealth is much less strong than that from IQ to job performance. Some studies indicate that IQ is unrelated to net worth. The American Psychological Association's 1995 report "" stated that IQ scores accounted for about a quarter of the social status variance and one-sixth of the income variance. Statistical controls for parental SES eliminate about a quarter of this predictive power. Psychometric intelligence appears as only one of a great many factors that influence social outcomes. Charles Murray (1998) showed a more substantial effect of IQ on income independent of family background. In a meta-analysis, Strenze (2006) reviewed much of the literature and estimated the correlation between IQ and income to be about 0.23.

Some studies claim that IQ only accounts for (explains) a sixth of the variation in income because many studies are based on young adults, many of whom have not yet reached their peak earning capacity, or even their education. On pg 568 of "", Arthur Jensen claims that although the correlation between IQ and income averages a moderate 0.4 (one sixth or 16% of the variance), the relationship increases with age, and peaks at middle age when people have reached their maximum career potential. In the book, "A Question of Intelligence", Daniel Seligman cites an IQ income correlation of 0.5 (25% of the variance).

A 2002 study further examined the impact of non-IQ factors on income and concluded that an individual's location, inherited wealth, race, and schooling are more important as factors in determining income than IQ.

The American Psychological Association's 1996 report "Intelligence: Knowns and Unknowns" stated that the correlation between IQ and crime was −0.2. It was −0.19 between IQ scores and number of juvenile offenses in a large Danish sample; with social class controlled, the correlation dropped to −0.17. A correlation of 0.20 means that the explained variance is 4%. The causal links between psychometric ability and social outcomes may be indirect. Children with poor scholastic performance may feel alienated. Consequently, they may be more likely to engage in delinquent behavior, compared to other children who do well.

In his book "" (1998), Arthur Jensen cited data which showed that, regardless of race, people with IQs between 70 and 90 have higher crime rates than people with IQs below or above this range, with the peak range being between 80 and 90.

The 2009 "Handbook of Crime Correlates" stated that reviews have found that around eight IQ points, or 0.5 SD, separate criminals from the general population, especially for persistent serious offenders. It has been suggested that this simply reflects that "only dumb ones get caught" but there is similarly a negative relation between IQ and self-reported offending. That children with conduct disorder have lower IQ than their peers "strongly argues" for the theory.

A study of the relationship between US county-level IQ and US county-level crime rates found that higher average IQs were associated with lower levels of property crime, burglary, larceny rate, motor vehicle theft, violent crime, robbery, and aggravated assault. These results were "not confounded by a measure of concentrated disadvantage that captures the effects of race, poverty, and other social disadvantages of the county."

Multiple studies conducted in Scotland have found that higher IQs in early life are associated with lower mortality and morbidity rates later in life.

There is considerable variation within and overlap among these categories. People with high IQs are found at all levels of education and occupational categories. The biggest difference occurs for low IQs with only an occasional college graduate or professional scoring below 90.

With operationalization and methodology derived from the general intelligence factor "g", a new scientific understanding of collective intelligence, defined as a group's general ability to perform a wide range of tasks, aims to explain intelligent behavior of groups. Goal is to detect and explain a general intelligence factor "c" for groups, parallel to the "g" factor for individuals. As "g" is highly interrelated with the concept of IQ, this measurement of collective intelligence can be interpreted as intelligence quotient for groups (Group-IQ) even though the score is not a quotient per se. Current evidence suggests that this Group-IQ is only moderately correlated with group members' IQs but with other correlates such as group members' Theory of Mind.

Among the most controversial issues related to the study of intelligence is the observation that intelligence measures such as IQ scores vary between ethnic and racial groups. While there is little scholarly debate about the "existence" of some of these differences, current scientific consensus tells us that there is no evidence for a genetic component behind them. The existence of differences in IQ between the sexes remains controversial, and largely depends on which tests are performed.

With the advent of the concept of "g" or general intelligence, many researchers have argued that there are no significant sex differences in general intelligence, though ability in particular types of intelligence does appear to vary. Thus, while some test batteries show slightly greater intelligence in males, others show greater intelligence in females. In particular, studies have shown female subjects performing better on tasks related to verbal ability, and males performing better on tasks related to rotation of objects in space, often categorized as spatial ability. These differences obtain, as Hunt (2010) observes, "even though men and women are essentially equal in general intelligence".

Some research indicates that male advantages on some cognitive tests are minimized when controlling for socioeconomic factors. Other research has concluded that there is slightly larger variability in male scores in certain areas compared to female scores, which results in slightly more males than females in the top and bottom of the IQ distribution. 

The existence of differences between male and female performance on math-related tests is contested, and a meta-analysis focusing on gender differences in math performance found nearly identical performance for boys and girls. Currently, most IQ tests, including popular batteries such as the WAIS and the WISC-R, are constructed so that there are no overall score differences between females and males.

While the concept of "race" is a social construct, discussions of a purported relationship between race and intelligence, as well as claims of genetic differences in intelligence along racial lines, have appeared in both popular science and academic research since the inception of IQ testing in the early 20th century. Despite the tremendous amount of research done on the topic, no scientific evidence has emerged that the average IQ scores of different population groups can be attributed to genetic differences between those groups. 

A 1996 task force investigation on intelligence sponsored by the American Psychological Association concluded that there were significant variations in IQ across races. However, a systematic analysis by William Dickens and James Flynn (2006) showed the gap between black and white Americans to have closed dramatically during the period between 1972 and 2002, suggesting that, in their words, the "constancy of the Black-White IQ gap is a myth."

The problem of determining the causes underlying racial variation has been discussed at length as a classic question of "nature versus nurture", for instance by Alan S. Kaufman and Nathan Brody. Researchers such as statistician Bernie Devlin have argued that there are insufficient data to conclude that the black-white gap is due to genetic influences. Dickens and Flynn argued more positively that their results refute the possibility of a genetic origin, concluding that "the environment has been responsible" for observed differences. A review article published in 2012 by leading scholars on human intelligence reached a similar conclusion, after reviewing the prior research literature, that group differences in IQ are best understood as environmental in origin. More recently, geneticist and neuroscientist Kevin Mitchell has argued, on the basis of basic principles of population genetics, that "systematic genetic differences in intelligence between large, ancient populations" are "inherently and deeply implausible". 

The effects of stereotype threat have been proposed as an explanation for differences in IQ test performance between racial groups, as have issues related to cultural difference and access to education. 

In the United States, certain public policies and laws regarding military service, education, public benefits, capital punishment, and employment incorporate an individual's IQ into their decisions. However, in the case of Griggs v. Duke Power Co. in 1971, for the purpose of minimizing employment practices that disparately impacted racial minorities, the U.S. Supreme Court banned the use of IQ tests in employment, except when linked to job performance via a job analysis. Internationally, certain public policies, such as improving nutrition and prohibiting neurotoxins, have as one of their goals raising, or preventing a decline in, intelligence.

A diagnosis of intellectual disability is in part based on the results of IQ testing. Borderline intellectual functioning is a categorization where a person has below average cognitive ability (an IQ of 71–85), but the deficit is not as severe as intellectual disability (70 or below).

In the United Kingdom, the eleven plus exam which incorporated an intelligence test has been used from 1945 to decide, at eleven years of age, which type of school a child should go to. They have been much less used since the widespread introduction of comprehensive schools.
IQ classification is the practice used by IQ test publishers for designating IQ score ranges into various categories with labels such as "superior" or "average." IQ classification was preceded historically by attempts to classify human beings by general ability based on other forms of behavioral observation. Those other forms of behavioral observation are still important for validating classifications based on IQ tests.

There are social organizations, some international, which limit membership to people who have scores as high as or higher than the 98th percentile (2 standard deviations above the mean) on some IQ test or equivalent. Mensa International is perhaps the best known of these. The largest 99.9th percentile (3 standard deviations above the mean) society is the Triple Nine Society.




</doc>
<doc id="14894" url="https://en.wikipedia.org/wiki?curid=14894" title="Indian Institute of Technology Kanpur">
Indian Institute of Technology Kanpur

Indian Institute of Technology Kanpur (also known as IIT Kanpur or IITK) is a public technical and research university located in Kanpur, Uttar Pradesh. It was declared to be an Institute of National Importance by the Government of India under the Institutes of Technology Act.

Established in 1960 as one of the first Indian Institutes of Technology, the institute was created with the assistance of a consortium of nine US research universities as part of the Kanpur Indo-American Programme (KIAP).

IIT Kanpur was established by an Act of Parliament in 1959. The institute was started in December 1959 in a room in the canteen building of the Harcourt Butler Technological Institute at Agricultural Gardens in Kanpur. In 1963, the institute moved to its present location, on the Grand Trunk Road near the locality of Kalyanpur in Kanpur district. The campus was designed by Achyut Kavinde in a modernist style.

During the first ten years of its existence, a consortium of nine US universities (namely MIT, UCB, California Institute of Technology, Princeton University, Carnegie Institute of Technology, University of Michigan, Ohio State University,
Case Institute of Technology and Purdue University) helped set up IIT Kanpur's research laboratories and academic programmes under the Kanpur Indo-American Programme (KIAP). The first director of the institute was P. K. Kelkar (after whom the Central Library was renamed in 2002).

Under the guidance of economist John Kenneth Galbraith, IIT Kanpur was the first institute in India to offer Computer science education. The earliest computer courses were started at IIT Kanpur in August 1963 on an IBM 1620 system. The initiative for computer education came from the Electrical engineering department, then under the chairmanship of Prof. H.K. Kesavan, who was concurrently the chairman of Electrical Engineering and head of the Computer Centre. Prof. Harry Huskey of the University of California, Berkeley, who preceded Kesavan, helped with the computer activity at IIT-Kanpur. In 1971, the institute began an independent academic program in Computer Science and Engineering, leading to MTech and PhD degrees.

In 1972 the KIAP program ended, in part because of tensions due to the U.S. support of Pakistan. Government funding was also reduced as a reaction to the sentiment that the IIT's were contributing to the brain drain.

The institute's annual technical festival, Techkriti, was first started in 1995.

IIT Kanpur is located on the Grand Trunk Road, west of Kanpur City and measures close to . This land was donated by the Government of Uttar Pradesh in 1960 and by March 1963 the institute had moved to its current location.

The institute has around 6478 students with 3938 undergraduate students and 2540 postgraduate students and about 500 research associates.

IIT Kanpur is to open an extension centre in Noida with the plan of making a small convention centre there for supporting outreach activities. Its foundation was laid on 4 December 2012 on 5 acres of land allocated by Uttar Pradesh state government in the sector-62 of Noida city, which is less than an hour's journey from New Delhi and the Indira Gandhi International Airport. The cost of construction is estimated to be about 25 crores. The new campus will have an auditorium, seminar halls for organising national and international conferences and an International Relations Office along with a 7-storey guest house. Several short-term management courses and refresher courses meant for distance learning will be available at the extension center.News from. IITK. Retrieved 9 October 2013.

Being a major industrial town, Kanpur has a good connectivity by rail and by road but it lags behind in terms of air connectivity. IIT Kanpur was suffering significantly in comparison to IIT Delhi and IIT Bombay due to this reason as far as visiting companies and other dignitaries are concerned
On 1 June 2013, a helicopter ferry service was started at IIT Kanpur run by Pawan Hans Helicopters Limited. In its initial run the service connects IIT Kanpur to Lucknow, but it is planned to later extend it to New Delhi. Currently there are two flights daily to and from Lucknow Airport with a duration of 25 minutes. Lucknow Airport operates both international and domestic flights to major cities. IIT Kanpur is the first academic institution in the country to provide such a service. The estimated charges are Rs. 6000 (US$100) per person. If anyone would like to avail the facility he/she have to contact Student Placement Office (SPO) at IIT Kanpur, since the helicopter service is subject to availability of chopper rights. The campus also has airstrips which allows flight workshops and joyrides for students.

The institute has set up an office in New York with alumnus, Sanjiv Khosla designated as the overseas brand ambassador of the institute. It is located on 62, William Street, Manhattan. The office aims to hunt for qualified and capable faculty abroad, facilitate internship opportunities in North American universities and be conduit for research tie ups with various US universities. The New York Office also tries to amass funds through the alumni based there. A system that invites students and faculty of foreign institutes to IIT Kanpur is also being formulated.

Undergraduate admissions until 2012 were being done through the national-level Indian Institute of Technology Joint Entrance Examination (IIT-JEE). Following the Ministry of Human Resource Development's decision to replace IIT-JEE with a common engineering entrance examination, IIT Kanpur's admissions are now based on JEE (Joint Entrance Examination) -Advanced level along with other IITs.

Postgraduate admissions are made through the Graduate Aptitude Test in Engineering and Common Admission Test.


The Students' Gymkhana is the students' government organization of IIT Kanpur, established in 1962.

The Students' Gymkhana functions mainly through the Students' Senate, an elected student representative body composed of senators elected from each batch and the six elected executives:

The number of senators in the Students' Senate is around 50–55. A senator is elected for every 150 students of IIT Kanpur.

The meetings of the Students' Senate are chaired by the chairperson, Students' Senate, who is elected by the Senate.
The Senate lays down the guidelines for the functions of the executives, their associated councils, the Gymkhana Festivals and other matters pertaining to the Student body at large.

The Students' Senate has a say in the policy and decision making bodies of the institute. The president, Students' Gymkhana and the chairperson, Students' Senate are among the special invitees to the Institute Academic Senate. The president is usually invited to the meetings of the board of governors when matters affecting students are being discussed. Nominees of the Students' Senate are also members of the various standing Committees of the Institute Senate including the disciplinary committee, the Undergraduate and Postgraduate committee, etc. All academic departments have Departmental Undergraduate and Post Graduate Committees consisting of members of the faculty and student nominees.

Internationally, IIT Kanpur was ranked 291 in "QS World University Rankings" for 2020. It was ranked 65 in QS Asia Rankings 2020 and 25 among BRICS nations in 2019. The "Times Higher Education World University Rankings" ranked it 601–800 globally in the 2020 ranking 125 in Asia and 77 among Emerging Economies University Rankings 2020.

In India, among engineering colleges, fifth by "Outlook India" and fourth by "The Week" in 2019. It was ranked fourth among engineering colleges in India by the "National Institutional Ranking Framework" (NIRF) in 2020, and sixth overall. It ranked fourth among engineering colleges by "India Today" in 2020.

The Department of Industrial and Management Engineering was ranked 22 among management schools in India by NIRF in 2019.

IIT Kanpur offers four-year BTech programs in Aerospace Engineering, Biological Sciences and Bio-engineering, Chemical Engineering, Civil Engineering, Computer Science and Engineering, Electrical Engineering, Materials Science and Engineering and Mechanical Engineering. The admission to these programs is procured through Joint Entrance Examination. IITK offers admission only to bachelor's degree now (discontinuing the integrated course programs), but it can be extended by 1 year to make it integrated, depending on the choice of student and based on his/her performance there at undergraduate level. IIT Kanpur also offers four-year B.S. Programs in Pure and Applied Sciences (Mathematics, Physics and Chemistry in particular), Earth Science and Economics.

From 2011, IIT Kanpur has started offering a four-year BS program in sciences and has kept its BTech Program intact. Entry to the five-year MTech/MBA programs and Dual degree programme will be done based on the CPI of students instead of JEE rank. In order to reduce the number of student exams, IIT Kanpur has also abolished the earlier system of conducting two mid-term examinations. Instead, only two examinations (plus two quizzes in most courses depending on the instructor-in-charge, one before mid-semesters and the other after the mid-semesters and before the end-semesters examination), one between the semester and other towards the end of it would be held from the academic session starting July 2011 onward as per Academic Review Committee's recommendations.

Postgraduate courses in Engineering offer Master of Technology (MTech), MS (R) and PhD degrees. The institute also offers two-tier MSc courses in areas of basic sciences in which students are admitted through Joint Admission Test for MSc (JAM) exam. The institute also offers M.Des. (2 years), M.B.A. (2 years) and MSc (2 years) degrees. Admissions to MTech is made once a year through Graduate Aptitude Test in Engineering. Admissions to M. Des are made once a year through both Graduate Aptitude Test in Engineering(GATE) and Common Entrance Exam for Design (CEED). Until 2011, admissions to the M.B.A. program were accomplished through the Joint Management Entrance Test (JMET), held yearly, and followed by a Group Discussion/Personal Interview process. In 2011, JMET was replaced by Common Admission Test (CAT).

The academic departments at IIT Kanpur are:
The campus is spread over an area of . Facilities include the National Wind Tunnel Facility. Other large research centres include the Advanced Centre for Material Science, a Bio-technology centre, the Advanced Centre for Electronic Systems, and the Samtel Centre for Display Technology, Centre for Mechatronics, Centre for Laser Technology, Prabhu Goel Research Centre for Computer and Internet Security, Facility for Ecological and Analytical Testing. The departments have their own libraries.

The institute has its own airfield, for flight testing and gliding.

PK Kelkar Library (formerly Central Library) is an academic library of the institute with a collection of more than 300,000 volumes, and subscriptions to more than 1,000 periodicals. The library was renamed to its present name in 2003 after Dr. P K Kelkar, the first director of the institute. It is housed in a three-story building, with a total floor area of 6973 square metres. The Abstracting and Indexing periodicals, Microform and CD-ROM databases, technical reports, Standards and thesis are in the library. Each year, about 4,500 books and journal volumes are added to the library.

The New Core Labs (NCL) is 3-storey building with state of the art physics and chemistry laboratories for courses in the first year. The New Core Labs also has Linux and Windows computer labs for the use of first year courses and a Mathematics department laboratory housing machines with high computing power.

IIT Kanpur has set up the Startup Innovation and Incubation Centre (SIIC) "(previously known as "SIDBI" Innovation and Incubation Centre)" in collaboration with the Small Industries development Bank of India (SIDBI) aiming to aid innovation, research, and entrepreneurial activities in technology-based areas. SIIC helps business Start-ups to develop their ideas into commercially viable products.

A team of students, working under the guidance of faculty members of the institute and scientists of Indian Space Research Organisation (ISRO) have designed and built India's first nano satellite Jugnu, which was successfully launched in orbit on 12 Oct 2011 by ISRO's PSLV-C18.

The Computer Centre is one of the advanced computing service centre among academic institution in India. IT hosts IIT Kanpur website and provides personal web space for students and faculties. It also provides a spam filtered email server and high speed fibre optic Internet to all the hostels and the academics. Users have multiple options to choose among various interfaces to access mail service. It has Linux and windows laboratories equipped with dozens of high-end software like MATLAB, Autocad, Ansys, Abaqus etc. for use of students. Apart from departmental computer labs, computer centre hosts more than 300 Linux terminals and more than 100 Windows terminals and is continuously available to the students for academic work and recreation. Computer centre has recently adopted an open source software policy for its infrastructure and computing. Various high-end compute and GPU servers are remotely available from data centre for user computation.

Computer centre has multiple super computing clusters for research and teaching activity. In June 2014 IIT Kanpur launched their 2nd supercomputer which is India's 5th most powerful supercomputer as of now. The new supercomputer 'Cluster Platform SL230s Gen8' manufactured by Hewlett-Packard has 15,360 cores and a theoretical peak (Rpeak) 307.2 TFlop/s and is the world's 192th most powerful supercomputer as of June 2015.

Research is controlled by the Office of the Dean of Research and Development. Under the aegis of the Office the students publish the quarterly NERD Magazine (Notes on Engineering Research and Development) which publishes scientific and technical content created by students. Articles may be original work done by students in the form of hobby projects, term projects, internships, or theses. Articles of general interest which are informative but do not reflect original work are also accepted. The institute is part of the European Research and Education Collaboration with Asia (EURECA) programme since 2008.

Along with the magazine a student research organisation, PoWER (Promotion of Work Experience and Research) has been started. Under it several independent student groups are working on projects like the Lunar Rover for ISRO, alternate energy solutions under the Group for Environment and Energy Engineering, ICT solutions through a group Young Engineers, solution for diabetes, green community solutions through ideas like zero water and zero waste quality air approach. Through BRaIN (Biological Research and Innovation Network) students interested in solving biological problems get involved in research projects like genetically modifying fruit flies to study molecular systems and developing bio-sensors to detect alcohol levels. A budget of Rs 1.5 to 2 crore has been envisaged to support student projects that demonstrate technology.

Assisting the Indian Ordnance Factories in not only upgrading existing products, but also developing new weapon platforms.


The students of IIT Kanpur made a nano satellite called Jugnu, which was given by president Pratibha Patil to ISRO for launch. Jugnu is a remote sensing satellite which will be operated by the Indian Institute of Technology Kanpur. It is a nanosatellite which will be used to provide data for agriculture and disaster monitoring. It is a 3-kilogram (6.6 lb) spacecraft, which measures 34 centimetres (13 in) in length by 10 centimetres (3.9 in) in height and width. Its development programme cost around 25 million rupees. It has a design life of one year.
Jugnu's primary instrument is the Micro Imaging System, a near infrared camera which will be used to observe vegetation. It also carries a GPS receiver to aid tracking, and is intended to demonstrate a microelectromechanical inertial measurement unit.

IITK motorsports is the biggest and most comprehensive student initiative of the college, founded in January 2011. It is a group of students from varied disciplines who aim at designing and fabricating a Formula-style race car for international Formula SAE
(Society of Automotive Engineers) events. Most of the components of the car, except the engine, tyres and wheel rims, are designed and manufactured by the team members themselves. The car is designed to provide maximum performance under the constraints of the event, while ensuring the driveability, reliability, driver safety and aesthetics of the car are not compromised.

Researchers at IIT Kanpur have developed a series of solar powered UAVs named MARAAL-1 & MARAAL-2. Development of Maraal is notable as it is the first solar powered UAV developed in India. Maraal-2 is fully indigenous.



</doc>
<doc id="14895" url="https://en.wikipedia.org/wiki?curid=14895" title="Insulin">
Insulin

Insulin (, from Latin "insula", 'island') is a peptide hormone produced by beta cells of the pancreatic islets; it is considered to be the main anabolic hormone of the body. It regulates the metabolism of carbohydrates, fats and protein by promoting the absorption of glucose from the blood into liver, fat and skeletal muscle cells. In these tissues the absorbed glucose is converted into either glycogen via glycogenesis or fats (triglycerides) via lipogenesis, or, in the case of the liver, into both. Glucose production and secretion by the liver is strongly inhibited by high concentrations of insulin in the blood. Circulating insulin also affects the synthesis of proteins in a wide variety of tissues. It is therefore an anabolic hormone, promoting the conversion of small molecules in the blood into large molecules inside the cells. Low insulin levels in the blood have the opposite effect by promoting widespread catabolism, especially of reserve body fat.
Beta cells are sensitive to blood sugar levels so that they secrete insulin into the blood in response to high level of glucose; and inhibit secretion of insulin when glucose levels are low. Insulin enhances glucose uptake and metabolism in the cells, thereby reducing blood sugar level. Their neighboring alpha cells, by taking their cues from the beta cells, secrete glucagon into the blood in the opposite manner: increased secretion when blood glucose is low, and decreased secretion when glucose concentrations are high. Glucagon increases blood glucose level by stimulating glycogenolysis and gluconeogenesis in the liver. The secretion of insulin and glucagon into the blood in response to the blood glucose concentration is the primary mechanism of glucose homeostasis.
Decreased or loss of insulin activity results in diabetes mellitus, a condition of high blood sugar level (hyperglycaemia). There are two types of the disease. In type 1 diabetes mellitus, the beta cells are destroyed by an autoimmune reaction so that insulin can no longer be synthesized or be secreted into the blood. In type 2 diabetes mellitus, the destruction of beta cells is less pronounced than in type 1 diabetes, and is not due to an autoimmune process. Instead, there is an accumulation of amyloid in the pancreatic islets, which likely disrupts their anatomy and physiology. The pathogenesis of type 2 diabetes is not well understood but reduced population of islet beta-cells, reduced secretory function of islet beta-cells that survive, and peripheral tissue insulin resistance are known to be involved. Type 2 diabetes is characterized by increased glucagon secretion which is unaffected by, and unresponsive to the concentration of blood glucose. But insulin is still secreted into the blood in response to the blood glucose. As a result, glucose accumulates in the blood.
The human insulin protein is composed of 51 amino acids, and has a molecular mass of 5808 Da. It is a heterodimer of an A-chain and a B-chain, which are linked together by disulfide bonds. Insulin's structure varies slightly between species of animals. Insulin from animal sources differs somewhat in effectiveness (in carbohydrate metabolism effects) from human insulin because of these variations. Porcine insulin is especially close to the human version, and was widely used to treat type 1 diabetics before human insulin could be produced in large quantities by recombinant DNA technologies.

Insulin was the first peptide hormone discovered. Frederick Banting and Charles Herbert Best, working in the laboratory of J.J.R. Macleod at the University of Toronto, were the first to isolate insulin from dog pancreas in 1921. Frederick Sanger sequenced the amino acid structure in 1951, which made insulin the first protein to be fully sequenced. The crystal structure of insulin in the solid state was determined by Dorothy Hodgkin in 1969. Insulin is also the first protein to be chemically synthesised and produced by DNA recombinant technology. It is on the WHO Model List of Essential Medicines, the most important medications needed in a basic health system.

Insulin may have originated more than a billion years ago. The molecular origins of insulin go at least as far back as the simplest unicellular eukaryotes. Apart from animals, insulin-like proteins are also known to exist in the Fungi and Protista kingdoms.

Insulin is produced by beta cells of the pancreatic islets in most vertebrates and by the Brockmann body in some teleost fish. Cone snails "Conus geographus" and "Conus tulipa", venomous sea snails that hunt small fish, use modified forms of insulin in their venom cocktails. The insulin toxin, closer in structure to fishes' than to snails' native insulin, slows down the prey fishes by lowering their blood glucose levels.

The preproinsulin precursor of insulin is encoded by the "INS" gene, which is located on Chromosome 11p15.5.

A variety of mutant alleles with changes in the coding region have been identified. A read-through gene, INS-IGF2, overlaps with this gene at the 5' region and with the IGF2 gene at the 3' region.

In the pancreatic β cells, glucose is the primary physiological stimulus for the regulation of insulin synthesis. Insulin is mainly regulated through the transcription factors PDX1, NeuroD1, and MafA.

During a low-glucose state, PDX1 (pancreatic and duodenal homeobox protein 1) is located in the nuclear periphery as a result of interaction with HDAC1 and 2, which results in downregulation of insulin secretion. An increase in blood glucose levels causes phosphorylation of PDX1, which leads it to undergo nuclear translocation and bind the A3 element within the insulin promoter. Upon translocation it interacts with coactivators HAT p300 and SETD7. PDX1 affects the histone modifications through acetylation and deacetylation as well as methylation. It is also said to suppress glucagon.

NeuroD1, also known as β2, regulates insulin exocytosis in pancreatic β cells by directly inducing the expression of genes involved in exocytosis. It is localized in the cytosol, but in response to high glucose it becomes glycosylated by OGT and/or phosphorylated by ERK, which causes translocation to the nucleus. In the nucleus β2 heterodimerizes with E47, binds to the E1 element of the insulin promoter and recruits co-activator p300 which acetylates β2. It is able to interact with other transcription factors as well in activation of the insulin gene.

MafA is degraded by proteasomes upon low blood glucose levels. Increased levels of glucose make an unknown protein glycosylated. This protein works as a transcription factor for MafA in an unknown manner and MafA is transported out of the cell. MafA is then translocated back into the nucleus where it binds the C1 element of the insulin promoter.

These transcription factors work synergistically and in a complex arrangement. Increased blood glucose can after a while destroy the binding capacities of these proteins, and therefore reduce the amount of insulin secreted, causing diabetes. The decreased binding activities can be mediated by glucose induced oxidative stress and antioxidants are said to prevent the decreased insulin secretion in glucotoxic pancreatic β cells. Stress signalling molecules and reactive oxygen species inhibits the insulin gene by interfering with the cofactors binding the transcription factors and the transcription factors itself.

Several regulatory sequences in the promoter region of the human insulin gene bind to transcription factors. In general, the A-boxes bind to Pdx1 factors, E-boxes bind to NeuroD, C-boxes bind to MafA, and cAMP response elements to CREB. There are also silencers that inhibit transcription.

Contrary to an initial belief that hormones would be generally small chemical molecules, as the first peptide hormone known of its structure, insulin was found to be quite large. A single protein (monomer) of human insulin is composed of 51 amino acids, and has a molecular mass of 5808 Da. The molecular formula of human insulin is CHNOS. It is a combination of two peptide chains (dimer) named an A-chain and a B-chain, which are linked together by two disulfide bonds. The A-chain is composed of 21 amino acids, while the B-chain consists of 30 residues. The linking (interchain) disulfide bonds are formed at cysteine residues between the positions A7-B7 and A20-B19. There is an additional (intrachain) disulfide bond within the A-chain between cysteine residues at positions A4 and A11. The A-chain exhibits two α-helical regions at A1-A8 and A12-A19 which are antiparallel; while the B chain has a central α -helix (covering residues B9-B19) flanked by the disulfide bond on either sides and two β-sheets (covering B7-B10 and B20-B23). 

The amino acid sequence of insulin is strongly conserved and varies only slightly between species. Bovine insulin differs from human in only three amino acid residues, and porcine insulin in one. Even insulin from some species of fish is similar enough to human to be clinically effective in humans. Insulin in some invertebrates is quite similar in sequence to human insulin, and has similar physiological effects. The strong homology seen in the insulin sequence of diverse species suggests that it has been conserved across much of animal evolutionary history. The C-peptide of proinsulin, however, differs much more among species; it is also a hormone, but a secondary one. 

Insulin is produced and stored in the body as a hexamer (a unit of six insulin molecules), while the active form is the monomer. The hexamer is about 36000 Da in size. The six molecules are linked together as three dimeric units to form symmetrical molecule. An important feature is the presence of zinc atoms (Zn) on the axis of symmetry, which are surrounded by three water molecules and three histamine residues at position B10.

The hexamer is an inactive form with long-term stability, which serves as a way to keep the highly reactive insulin protected, yet readily available. The hexamer-monomer conversion is one of the central aspects of insulin formulations for injection. The hexamer is far more stable than the monomer, which is desirable for practical reasons; however, the monomer is a much faster-reacting drug because diffusion rate is inversely related to particle size. A fast-reacting drug means insulin injections do not have to precede mealtimes by hours, which in turn gives people with diabetes more flexibility in their daily schedules. Insulin can aggregate and form fibrillar interdigitated beta-sheets. This can cause injection amyloidosis, and prevents the storage of insulin for long periods.

Insulin is produced in the pancreas and the Brockmann body (in some fish), and released when any of several stimuli are detected. These stimuli include the rise in plasma concentrations of amino acids and glucose resulting from the digestion of food. Carbohydrates can be polymers of simple sugars or the simple sugars themselves. If the carbohydrates include glucose, then that glucose will be absorbed into the bloodstream and blood glucose level will begin to rise. In target cells, insulin initiates a signal transduction, which has the effect of increasing glucose uptake and storage. Finally, insulin is degraded, terminating the response.

In mammals, insulin is synthesized in the pancreas within the beta cells. One million to three million pancreatic islets form the endocrine part of the pancreas, which is primarily an exocrine gland. The endocrine portion accounts for only 2% of the total mass of the pancreas. Within the pancreatic islets, beta cells constitute 65–80% of all the cells.

Insulin consists of two polypeptide chains, the A- and B- chains, linked together by disulfide bonds. It is however first synthesized as a single polypeptide called preproinsulin in beta cells. Preproinsulin contains a 24-residue signal peptide which directs the nascent polypeptide chain to the rough endoplasmic reticulum (RER). The signal peptide is cleaved as the polypeptide is translocated into lumen of the RER, forming proinsulin. In the RER the proinsulin folds into the correct conformation and 3 disulfide bonds are formed. About 5–10 min after its assembly in the endoplasmic reticulum, proinsulin is transported to the trans-Golgi network (TGN) where immature granules are formed. Transport to the TGN may take about 30 minutes.

Proinsulin undergoes maturation into active insulin through the action of cellular endopeptidases known as prohormone convertases (PC1 and PC2), as well as the exoprotease carboxypeptidase E. The endopeptidases cleave at 2 positions, releasing a fragment called the C-peptide, and leaving 2 peptide chains, the B- and A- chains, linked by 2 disulfide bonds. The cleavage sites are each located after a pair of basic residues (lysine-64 and arginine-65, and arginine-31 and −32). After cleavage of the C-peptide, these 2 pairs of basic residues are removed by the carboxypeptidase. The C-peptide is the central portion of proinsulin, and the primary sequence of proinsulin goes in the order "B-C-A" (the B and A chains were identified on the basis of mass and the C-peptide was discovered later).

The resulting mature insulin is packaged inside mature granules waiting for metabolic signals (such as leucine, arginine, glucose and mannose) and vagal nerve stimulation to be exocytosed from the cell into the circulation.

The endogenous production of insulin is regulated in several steps along the synthesis pathway:

Insulin and its related proteins have been shown to be produced inside the brain, and reduced levels of these proteins are linked to Alzheimer's disease.

Insulin release is stimulated also by beta-2 receptor stimulation and inhibited by alpha-1 receptor stimulation.  In addition, cortisol, glucagon and growth hormone antagonize the actions of insulin during times of stress.  Insulin also inhibits fatty acid release by hormone sensitive lipase in adipose tissue.

Beta cells in the islets of Langerhans release insulin in two phases. The first-phase release is rapidly triggered in response to increased blood glucose levels, and lasts about 10 minutes. The second phase is a sustained, slow release of newly formed vesicles triggered independently of sugar, peaking in 2 to 3 hours. Reduced first-phase insulin release may be the earliest detectable beta cell defect predicting onset of type 2 diabetes. First-phase release and insulin sensitivity are independent predictors of diabetes.

The description of first phase release is as follows:

This is the primary mechanism for release of insulin. Other substances known to stimulate insulin release include the amino acids arginine and leucine, parasympathetic release of acetylcholine (acting via the phospholipase C pathway), sulfonylurea, cholecystokinin (CCK, also via phospholipase C), and the gastrointestinally derived incretins, such as glucagon-like peptide-1 (GLP-1) and glucose-dependent insulinotropic peptide (GIP).

Release of insulin is strongly inhibited by norepinephrine (noradrenaline), which leads to increased blood glucose levels during stress. It appears that release of catecholamines by the sympathetic nervous system has conflicting influences on insulin release by beta cells, because insulin release is inhibited by α-adrenergic receptors and stimulated by β-adrenergic receptors. The net effect of norepinephrine from sympathetic nerves and epinephrine from adrenal glands on insulin release is inhibition due to dominance of the α-adrenergic receptors.

When the glucose level comes down to the usual physiologic value, insulin release from the β-cells slows or stops. If the blood glucose level drops lower than this, especially to dangerously low levels, release of hyperglycemic hormones (most prominently glucagon from islet of Langerhans alpha cells) forces release of glucose into the blood from the liver glycogen stores, supplemented by gluconeogenesis if the glycogen stores become depleted. By increasing blood glucose, the hyperglycemic hormones prevent or correct life-threatening hypoglycemia.

Evidence of impaired first-phase insulin release can be seen in the glucose tolerance test, demonstrated by a substantially elevated blood glucose level at 30 minutes after the ingestion of a glucose load (75 or 100 g of glucose), followed by a slow drop over the next 100 minutes, to remain above 120 mg/100 ml after two hours after the start of the test. In a normal person the blood glucose level is corrected (and may even be slightly over-corrected) by the end of the test. An insulin spike is a 'first response' to blood glucose increase, this response is individual and dose specific although it was always previously assumed to be food type specific only.

Even during digestion, in general, one or two hours following a meal, insulin release from the pancreas is not continuous, but oscillates with a period of 3–6 minutes, changing from generating a blood insulin concentration more than about 800 p mol/l to less than 100 pmol/l (in rats). This is thought to avoid downregulation of insulin receptors in target cells, and to assist the liver in extracting insulin from the blood. This oscillation is important to consider when administering insulin-stimulating medication, since it is the oscillating blood concentration of insulin release, which should, ideally, be achieved, not a constant high concentration. This may be achieved by delivering insulin rhythmically to the portal vein, by light activated delivery, or by islet cell transplantation to the liver.

The blood insulin level can be measured in international units, such as µIU/mL or in molar concentration, such as pmol/L, where 1 µIU/mL equals 6.945 pmol/L. A typical blood level between meals is 8–11 μIU/mL (57–79 pmol/L).

The effects of insulin are initiated by its binding to a receptor present in the cell membrane. The receptor molecule contains an α- and β subunits. Two molecules are joined to form what is known as a homodimer. Insulin binds to the α-subunits of the homodimer, which faces the extracellular side of the cells. The β subunits have tyrosine kinase enzyme activity which is triggered by the insulin binding. This activity provokes the autophosphorylation of the β subunits and subsequently the phosphorylation of proteins inside the cell known as insulin receptor substrates (IRS). The phosphorylation of the IRS activates a signal transduction cascade that leads to the activation of other kinases as well as transcription factors that mediate the intracellular effects of insulin.

The cascade that leads to the insertion of GLUT4 glucose transporters into the cell membranes of muscle and fat cells, and to the synthesis of glycogen in liver and muscle tissue, as well as the conversion of glucose into triglycerides in liver, adipose, and lactating mammary gland tissue, operates via the activation, by IRS-1, of phosphoinositol 3 kinase (PI3K). This enzyme converts a phospholipid in the cell membrane by the name of phosphatidylinositol 4,5-bisphosphate (PIP2), into phosphatidylinositol 3,4,5-triphosphate (PIP3), which, in turn, activates protein kinase B (PKB). Activated PKB facilitates the fusion of GLUT4 containing endosomes with the cell membrane, resulting in an increase in GLUT4 transporters in the plasma membrane. PKB also phosphorylates glycogen synthase kinase (GSK), thereby inactivating this enzyme. This means that its substrate, glycogen synthase (GS), cannot be phosphorylated, and remains dephosphorylated, and therefore active. The active enzyme, glycogen synthase (GS), catalyzes the rate limiting step in the synthesis of glycogen from glucose. Similar dephosphorylations affect the enzymes controlling the rate of glycolysis leading to the synthesis of fats via malonyl-CoA in the tissues that can generate triglycerides, and also the enzymes that control the rate of gluconeogenesis in the liver. The overall effect of these final enzyme dephosphorylations is that, in the tissues that can carry out these reactions, glycogen and fat synthesis from glucose are stimulated, and glucose production by the liver through glycogenolysis and gluconeogenesis are inhibited. The breakdown of triglycerides by adipose tissue into free fatty acids and glycerol is also inhibited.

After the intracellular signal that resulted from the binding of insulin to its receptor has been produced, termination of signaling is then needed. As mentioned below in the section on degradation, endocytosis and degradation of the receptor bound to insulin is a main mechanism to end signaling. In addition, the signaling pathway is also terminated by dephosphorylation of the tyrosine residues in the various signaling pathways by tyrosine phosphatases. Serine/Threonine kinases are also known to reduce the activity of insulin.

The structure of the insulin–insulin receptor complex has been determined using the techniques of X-ray crystallography.

The actions of insulin on the global human metabolism level include:

The actions of insulin (indirect and direct) on cells include:

Insulin also influences other body functions, such as vascular compliance and cognition. Once insulin enters the human brain, it enhances learning and memory and benefits verbal memory in particular. Enhancing brain insulin signaling by means of intranasal insulin administration also enhances the acute thermoregulatory and glucoregulatory response to food intake, suggesting that central nervous insulin contributes to the co-ordination of a wide variety of homeostatic or regulatory processes in the human body. Insulin also has stimulatory effects on gonadotropin-releasing hormone from the hypothalamus, thus favoring fertility.

Once an insulin molecule has docked onto the receptor and effected its action, it may be released back into the extracellular environment, or it may be degraded by the cell. The two primary sites for insulin clearance are the liver and the kidney. The liver clears most insulin during first-pass transit, whereas the kidney clears most of the insulin in systemic circulation. Degradation normally involves endocytosis of the insulin-receptor complex, followed by the action of insulin-degrading enzyme. An insulin molecule produced endogenously by the beta cells is estimated to be degraded within about one hour after its initial release into circulation (insulin half-life ~ 4–6 minutes).

Insulin is a major regulator of endocannabinoid (EC) metabolism and insulin treatment has been shown to reduce intracellular ECs, the 2-arachidonylglycerol (2-AG) and anandamide (AEA), which correspond with insulin-sensitive expression changes in enzymes of EC metabolism. In insulin-resistant adipocytes, patterns of insulin-induced enzyme expression is disturbed in a manner consistent with elevated EC synthesis and reduced EC degradation. Findings suggest that insulin-resistant adipocytes fail to regulate EC metabolism and decrease intracellular EC levels in response to insulin stimulation, whereby obese insulin-resistant individuals exhibit increased concentrations of ECs. This dysregulation contributes to excessive visceral fat accumulation and reduced adiponectin release from abdominal adipose tissue, and further to the onset of several cardiometabolic risk factors that are associated with obesity and type 2 diabetes.

Hypoglycemia, also known as "low blood sugar", is when blood sugar decreases to below normal levels. This may result in a variety of symptoms including clumsiness, trouble talking, confusion, loss of consciousness, seizures or death. A feeling of hunger, sweating, shakiness and weakness may also be present. Symptoms typically come on quickly.

The most common cause of hypoglycemia is medications used to treat diabetes mellitus such as insulin and sulfonylureas. Risk is greater in diabetics who have eaten less than usual, exercised more than usual or have drunk alcohol. Other causes of hypoglycemia include kidney failure, certain tumors, such as insulinoma, liver disease, hypothyroidism, starvation, inborn error of metabolism, severe infections, reactive hypoglycemia and a number of drugs including alcohol. Low blood sugar may occur in otherwise healthy babies who have not eaten for a few hours.

There are several conditions in which insulin disturbance is pathologic:

Biosynthetic human insulin (insulin human rDNA, INN) for clinical use is manufactured by recombinant DNA technology. Biosynthetic human insulin has increased purity when compared with extractive animal insulin, enhanced purity reducing antibody formation. Researchers have succeeded in introducing the gene for human insulin into plants as another method of producing insulin ("biopharming") in safflower. This technique is anticipated to reduce production costs.

Several analogs of human insulin are available. These insulin analogs are closely related to the human insulin structure, and were developed for specific aspects of glycemic control in terms of fast action (prandial insulins) and long action (basal insulins). The first biosynthetic insulin analog was developed for clinical use at mealtime (prandial insulin), Humalog (insulin lispro), it is more rapidly absorbed after subcutaneous injection than regular insulin, with an effect 15 minutes after injection. Other rapid-acting analogues are NovoRapid and Apidra, with similar profiles. All are rapidly absorbed due to amino acid sequences that will reduce formation of dimers and hexamers (monomeric insulins are more rapidly absorbed). Fast acting insulins do not require the injection-to-meal interval previously recommended for human insulin and animal insulins. The other type is long acting insulin; the first of these was Lantus (insulin glargine). These have a steady effect for an extended period from 18 to 24 hours. Likewise, another protracted insulin analogue (Levemir) is based on a fatty acid acylation approach. A myristic acid molecule is attached to this analogue, which associates the insulin molecule to the abundant serum albumin, which in turn extends the effect and reduces the risk of hypoglycemia. Both protracted analogues need to be taken only once daily, and are used for type 1 diabetics as the basal insulin. A combination of a rapid acting and a protracted insulin is also available, making it more likely for patients to achieve an insulin profile that mimics that of the body's own insulin release.

Insulin is usually taken as subcutaneous injections by single-use syringes with needles, via an insulin pump, or by repeated-use insulin pens with disposable needles. Inhaled insulin is also available in the U.S. market now.

Synthetic insulin can trigger adverse effects, so some people with diabetes rely on animal-source insulin.

Unlike many medicines, insulin currently cannot be taken orally because, like nearly all other proteins introduced into the gastrointestinal tract, it is reduced to fragments, whereupon all activity is lost. There has been some research into ways to protect insulin from the digestive tract, so that it can be administered orally or sublingually.

In 1869, while studying the structure of the pancreas under a microscope, Paul Langerhans, a medical student in Berlin, identified some previously unnoticed tissue clumps scattered throughout the bulk of the pancreas. The function of the "little heaps of cells", later known as the "islets of Langerhans", initially remained unknown, but Édouard Laguesse later suggested they might produce secretions that play a regulatory role in digestion. Paul Langerhans' son, Archibald, also helped to understand this regulatory role. 

In 1889, the physician Oskar Minkowski, in collaboration with Joseph von Mering, removed the pancreas from a healthy dog to test its assumed role in digestion. On testing the urine, they found sugar, establishing for the first time a relationship between the pancreas and diabetes. In 1901, another major step was taken by the American physician and scientist Eugene Lindsay Opie, when he isolated the role of the pancreas to the islets of Langerhans: "Diabetes mellitus when the result of a lesion of the pancreas is caused by destruction of the islands of Langerhans and occurs only when these bodies are in part or wholly destroyed".

Over the next two decades researchers made several attempts to isolate the islets' secretions. In 1906 George Ludwig Zuelzer achieved partial success in treating dogs with pancreatic extract, but he was unable to continue his work. Between 1911 and 1912, E.L. Scott at the University of Chicago tried aqueous pancreatic extracts and noted "a slight diminution of glycosuria", but was unable to convince his director of his work's value; it was shut down. Israel Kleiner demonstrated similar effects at Rockefeller University in 1915, but World War I interrupted his work and he did not return to it.

In 1916, Nicolae Paulescu developed an aqueous pancreatic extract which, when injected into a diabetic dog, had a normalizing effect on blood-sugar levels. He had to interrupt his experiments because of World War I, and in 1921 he wrote four papers about his work carried out in Bucharest and his tests on a diabetic dog. Later that year, he published "Research on the Role of the Pancreas in Food Assimilation".

The name "insulin" was coined by Edward Albert Sharpey-Schafer in 1916 for a hypothetical molecule produced by pancreatic islets of Langerhans (Latin "insula" for islet or island) that controls glucose metabolism. Unbeknown to Sharpey-Schafer, Jean de Meyer had introduced very similar word "insuline" in 1909 for the same molecule.

In October 1920, Canadian Frederick Banting concluded that the digestive secretions that Minkowski had originally studied were breaking down the islet secretion, thereby making it impossible to extract successfully. A surgeon by training, Banting knew certain arteries could be tied off that would lead most of the pancreas to atrophy, while leaving the islets of Langerhans intact. He reasoned that a relatively pure extract could be made from the islets once most of the rest of the pancreas was gone. He jotted a note to himself: "Ligate pancreatic ducts of the dog. Keep dogs alive till acini degenerate leaving islets. Try to isolate internal secretion of these and relieve glycosuria."

In the spring of 1921, Banting traveled to Toronto to explain his idea to J.J.R. Macleod, Professor of Physiology at the University of Toronto. Macleod was initially skeptical, since Banting had no background in research and was not familiar with the latest literature, but he agreed to provide lab space for Banting to test out his ideas. Macleod also arranged for two undergraduates to be Banting's lab assistants that summer, but Banting required only one lab assistant. Charles Best and Clark Noble flipped a coin; Best won the coin toss and took the first shift. This proved unfortunate for Noble, as Banting kept Best for the entire summer and eventually shared half his Nobel Prize money and credit for the discovery with Best. On 30 July 1921, Banting and Best successfully isolated an extract ("isleton") from the islets of a duct-tied dog and injected it into a diabetic dog, finding that the extract reduced its blood sugar by 40% in 1 hour.

Banting and Best presented their results to Macleod on his return to Toronto in the fall of 1921, but Macleod pointed out flaws with the experimental design, and suggested the experiments be repeated with more dogs and better equipment. He moved Banting and Best into a better laboratory and began paying Banting a salary from his research grants. Several weeks later, the second round of experiments was also a success, and Macleod helped publish their results privately in Toronto that November. Bottlenecked by the time-consuming task of duct-tying dogs and waiting several weeks to extract insulin, Banting hit upon the idea of extracting insulin from the fetal calf pancreas, which had not yet developed digestive glands. By December, they had also succeeded in extracting insulin from the adult cow pancreas. Macleod discontinued all other research in his laboratory to concentrate on the purification of insulin. He invited biochemist James Collip to help with this task, and the team felt ready for a clinical test within a month.

On January 11, 1922, Leonard Thompson, a 14-year-old diabetic who lay dying at the Toronto General Hospital, was given the first injection of insulin. However, the extract was so impure that Thompson suffered a severe allergic reaction, and further injections were cancelled. Over the next 12 days, Collip worked day and night to improve the ox-pancreas extract. A second dose was injected on January 23, completely eliminating the glycosuria that was typical of diabetes without causing any obvious side-effects. The first American patient was Elizabeth Hughes, the daughter of U.S. Secretary of State Charles Evans Hughes. The first patient treated in the U.S. was future woodcut artist James D. Havens; Dr. John Ralston Williams imported insulin from Toronto to Rochester, New York, to treat Havens.

Banting and Best never worked well with Collip, regarding him as something of an interloper, and Collip left the project soon after. Over the spring of 1922, Best managed to improve his techniques to the point where large quantities of insulin could be extracted on demand, but the preparation remained impure. The drug firm Eli Lilly and Company had offered assistance not long after the first publications in 1921, and they took Lilly up on the offer in April. In November, Lilly's head chemist, George B. Walden discovered isoelectric precipitation and was able to produce large quantities of highly refined insulin. Shortly thereafter, insulin was offered for sale to the general public.

Toward the end of January 1922, tensions mounted between the four "co-discoverers" of insulin and Collip briefly threatened to separately patent his purification process. John G. FitzGerald, director of the non-commercial public health institution Connaught Laboratories, therefore stepped in as peacemaker. The resulting agreement of 25 January 1922 established two key conditions: 1) that the collaborators would sign a contract agreeing not to take out a patent with a commercial pharmaceutical firm during an initial working period with Connaught; and 2) that no changes in research policy would be allowed unless first discussed among FitzGerald and the four collaborators. It helped contain disagreement and tied the research to Connaught's public mandate.

Initially, Macleod and Banting were particularly reluctant to patent their process for insulin on grounds of medical ethics. However, concerns remained that a private third-party would hijack and monopolize the research (as Eli Lilly and Company had hinted), and that safe distribution would be difficult to guarantee without capacity for quality control. To this end, Edward Calvin Kendall gave valuable advice. He had isolated thyroxin at the Mayo Clinic in 1914 and patented the process through an arrangement between himself, the brothers Mayo, and the University of Minnesota, transferring the patent to the public university. On April 12, Banting, Best, Collip, Macleod, and FitzGerald wrote jointly to the president of the University of Toronto to propose a similar arrangement with the aim of assigning a patent to the Board of Governors of the University. The letter emphasized that:The patent would not be used for any other purpose than to prevent the taking out of a patent by other persons. When the details of the method of preparation are published anyone would be free to prepare the extract, but no one could secure a profitable monopoly.The assignment to the University of Toronto Board of Governors was completed on 15 January 1923, for the token payment of $1.00. The arrangement was congratulated in "The World's Work" in 1923 as "a step forward in medical ethics". It has also received much media attention in the 2010s regarding the issue of healthcare and drug affordability.

Following further concern regarding Eli Lilly's attempts to separately patent parts of the manufacturing process, Connaught's Assistant Director and Head of the Insulin Division Robert Defries established a patent pooling policy which would require producers to freely share any improvements to the manufacturing process without compromising affordability.

Purified animal-sourced insulin was initially the only type of insulin available for experiments and diabetics. John Jacob Abel was the first to produce the crystallised form in 1926. Evidence of the protein nature was first given by Michael Somogyi, Edward A. Doisy, and Philip A. Shaffer in 1924. It was fully proven when Hans Jensen and Earl A. Evans Jr. isolated the amino acids phenylalanine and proline in 1935.

The amino acid structure of insulin was first characterized in 1951 by Frederick Sanger, and the first synthetic insulin was produced simultaneously in the labs of Panayotis Katsoyannis at the University of Pittsburgh and Helmut Zahn at RWTH Aachen University in the mid-1960s. Synthetic crystalline bovine insulin was achieved by Chinese researchers in 1965. The complete 3-dimensional structure of insulin was determined by X-ray crystallography in Dorothy Hodgkin's laboratory in 1969.

The first genetically engineered, synthetic "human" insulin was produced using "E. coli" in 1978 by Arthur Riggs and Keiichi Itakura at the Beckman Research Institute of the City of Hope in collaboration with Herbert Boyer at Genentech. Genentech, founded by Swanson, Boyer and Eli Lilly and Company, went on in 1982 to sell the first commercially available biosynthetic human insulin under the brand name Humulin. The vast majority of insulin currently used worldwide is now biosynthetic recombinant "human" insulin or its analogues. Recently, another approach has been used by a pioneering group of Canadian researchers, using an easily grown safflower plant, for the production of much cheaper insulin.

Recombinant insulin is produced either in yeast (usually "Saccharomyces cerevisiae") or "E. coli". In yeast, insulin may be engineered as a single-chain protein with a KexII endoprotease (a yeast homolog of PCI/PCII) site that separates the insulin A chain from a C-terminally truncated insulin B chain. A chemically synthesized C-terminal tail is then grafted onto insulin by reverse proteolysis using the inexpensive protease trypsin; typically the lysine on the C-terminal tail is protected with a chemical protecting group to prevent proteolysis. The ease of modular synthesis and the relative safety of modifications in that region accounts for common insulin analogs with C-terminal modifications (e.g. lispro, aspart, glulisine). The Genentech synthesis and completely chemical synthesis such as that by Bruce Merrifield are not preferred because the efficiency of recombining the two insulin chains is low, primarily due to competition with the precipitation of insulin B chain.

The Nobel Prize committee in 1923 credited the practical extraction of insulin to a team at the University of Toronto and awarded the Nobel Prize to two men: Frederick Banting and J.J.R. Macleod. They were awarded the Nobel Prize in Physiology or Medicine in 1923 for the discovery of insulin. Banting, incensed that Best was not mentioned, shared his prize with him, and Macleod immediately shared his with James Collip. The patent for insulin was sold to the University of Toronto for one dollar.

Two other Nobel Prizes have been awarded for work on insulin. British molecular biologist Frederick Sanger, who determined the primary structure of insulin in 1955, was awarded the 1958 Nobel Prize in Chemistry. Rosalyn Sussman Yalow received the 1977 Nobel Prize in Medicine for the development of the radioimmunoassay for insulin.

Several Nobel Prizes also have an indirect connection with insulin. George Minot, co-recipient of the 1934 Nobel Prize for the development of the first effective treatment for pernicious anemia, had diabetes mellitus. Dr. William Castle observed that the 1921 discovery of insulin, arriving in time to keep Minot alive, was therefore also responsible for the discovery of a cure for pernicious anemia. Dorothy Hodgkin was awarded a Nobel Prize in Chemistry in 1964 for the development of crystallography, the technique she used for deciphering the complete molecular structure of insulin in 1969.

The work published by Banting, Best, Collip and Macleod represented the preparation of purified insulin extract suitable for use on human patients. Although Paulescu discovered the principles of the treatment, his saline extract could not be used on humans; he was not mentioned in the 1923 Nobel Prize. Professor Ian Murray was particularly active in working to correct "the historical wrong" against Nicolae Paulescu. Murray was a professor of physiology at the Anderson College of Medicine in Glasgow, Scotland, the head of the department of Metabolic Diseases at a leading Glasgow hospital, vice-president of the British Association of Diabetes, and a founding member of the International Diabetes Federation. Murray wrote:

Insufficient recognition has been given to Paulescu, the distinguished Romanian scientist, who at the time when the Toronto team were commencing their research had already succeeded in extracting the antidiabetic hormone of the pancreas and proving its efficacy in reducing the hyperglycaemia in diabetic dogs.
In a private communication, Professor Arne Tiselius, former head of the Nobel Institute, expressed his personal opinion that Paulescu was equally worthy of the award in 1923.




</doc>
<doc id="14896" url="https://en.wikipedia.org/wiki?curid=14896" title="Inductor">
Inductor

An inductor, also called a coil, choke, or reactor, is a passive two-terminal electrical component that stores energy in a magnetic field when electric current flows through it. An inductor typically consists of an insulated wire wound into a coil around a core.

When the current flowing through an inductor changes, the time-varying magnetic field induces an electromotive force ("e.m.f.") (voltage) in the conductor, described by Faraday's law of induction. According to Lenz's law, the induced voltage has a polarity (direction) which opposes the change in current that created it. As a result, inductors oppose any changes in current through them.

An inductor is characterized by its inductance, which is the ratio of the voltage to the rate of change of current. In the International System of Units (SI), the unit of inductance is the henry (H) named for 19th century American scientist Joseph Henry. In the measurement of magnetic circuits, it is equivalent to weber/ampere. Inductors have values that typically range from 1µH (10H) to 20H. Many inductors have a magnetic core made of iron or ferrite inside the coil, which serves to increase the magnetic field and thus the inductance. Along with capacitors and resistors, inductors are one of the three passive linear circuit elements that make up electronic circuits. Inductors are widely used in alternating current (AC) electronic equipment, particularly in radio equipment. They are used to block AC while allowing DC to pass; inductors designed for this purpose are called chokes. They are also used in electronic filters to separate signals of different frequencies, and in combination with capacitors to make tuned circuits, used to tune radio and TV receivers.

An electric current flowing through a conductor generates a magnetic field surrounding it. The magnetic flux linkage formula_1 generated by a given current formula_2 depends on the geometric shape of the circuit. Their ratio defines the inductance formula_3. Thus

The inductance of a circuit depends on the geometry of the current path as well as the magnetic permeability of nearby materials. An inductor is a component consisting of a wire or other conductor shaped to increase the magnetic flux through the circuit, usually in the shape of a coil or helix. Winding the wire into a coil increases the number of times the magnetic flux lines link the circuit, increasing the field and thus the inductance. The more turns, the higher the inductance. The inductance also depends on the shape of the coil, separation of the turns, and many other factors. By adding a "magnetic core" made of a ferromagnetic material like iron inside the coil, the magnetizing field from the coil will induce magnetization in the material, increasing the magnetic flux. The high permeability of a ferromagnetic core can increase the inductance of a coil by a factor of several thousand over what it would be without it.

Any change in the current through an inductor creates a changing flux, inducing a voltage across the inductor. By Faraday's law of induction, the voltage induced by any change in magnetic flux through the circuit is given by 

Reformulating the definition of formula_3 above, we obtain
It follows, that

for formula_3 independent of time.

So inductance is also a measure of the amount of electromotive force (voltage) generated for a given rate of change of current. For example, an inductor with an inductance of 1 henry produces an EMF of 1 volt when the current through the inductor changes at the rate of 1 ampere per second. This is usually taken to be the constitutive relation (defining equation) of the inductor.

The dual of the inductor is the capacitor, which stores energy in an electric field rather than a magnetic field. Its current–voltage relation is obtained by exchanging current and voltage in the inductor equations and replacing "L" with the capacitance "C".

In a circuit, an inductor can behave differently at different time instant. However, it's usually easy to think about the short-time limit and long-time limit:


The polarity (direction) of the induced voltage is given by Lenz's law, which states that the induced voltage will be such as to oppose the change in current. For example, if the current through an inductor is increasing, the induced voltage will be positive at the current's entrance point and negative at the exit point, tending to oppose the additional current. The energy from the external circuit necessary to overcome this potential "hill" is being stored in the magnetic field of the inductor. If the current is decreasing, the induced voltage will be negative at the current's entrance point and positive at the exit point, tending to maintain the current. In this case energy from the magnetic field is being returned to the circuit.

One intuitive explanation as to why a potential difference is induced on a change of current in an inductor goes as follows:

When there is a change in current through an inductor there is a change in the strength of the magnetic field. For example, if the current is increased, the magnetic field increases. This, however, does not come without a price. The magnetic field contains potential energy, and increasing the field strength requires more energy to be stored in the field. This energy comes from the electric current through the inductor. The increase in the magnetic potential energy of the field is provided by a corresponding drop in the electric potential energy of the charges flowing through the windings. This appears as a voltage drop across the windings as long as the current increases. Once the current is no longer increased and is held constant, the energy in the magnetic field is constant and no additional energy must be supplied, so the voltage drop across the windings disappears.

Similarly, if the current through the inductor decreases, the magnetic field strength decreases, and the energy in the magnetic field decreases. This energy is returned to the circuit in the form of an increase in the electrical potential energy of the moving charges, causing a voltage rise across the windings.

The work done per unit charge on the charges passing the inductor is formula_10. The negative sign indicates that the work is done "against" the emf, and is not done "by" the emf. The current formula_2 is the charge per unit time passing through the inductor. Therefore the rate of work formula_12 done by the charges against the emf, that is the rate of change of energy of the current, is given by
From the constitutive equation for the inductor, formula_14 so

In a ferromagnetic core inductor, when the magnetic field approaches the level at which the core saturates, the inductance will begin to change, it will be a function of the current formula_17. Neglecting losses, the energy formula_12 stored by an inductor with a current formula_19 passing through it is equal to the amount of work required to establish the current through the inductor. 

This is given by:
formula_20, where formula_21 is the so-called "differential inductance" and is defined as: formula_22. 
In an air core inductor or a ferromagnetic core inductor below saturation, the inductance is constant (and equal to the differential inductance), so the stored energy is

For inductors with magnetic cores, the above equation is only valid for linear regions of the magnetic flux, at currents below the saturation level of the inductor, where the inductance is approximately constant. Where this is not the case, the integral form must be used with formula_24 variable.

The constitutive equation describes the behavior of an "ideal inductor" with inductance formula_3, and without resistance, capacitance, or energy dissipation. In practice, inductors do not follow this theoretical model; "real inductors" have a measurable resistance due to the resistance of the wire and energy losses in the core, and parasitic capacitance due to electric potentials between turns of the wire.

A real inductor's capacitive reactance rises with frequency, and at a certain frequency, the inductor will behave as a resonant circuit. Above this self-resonant frequency, the capacitive reactance is the dominant part of the inductor's impedance. At higher frequencies, resistive losses in the windings increase due to the skin effect and proximity effect.

Inductors with ferromagnetic cores experience additional energy losses due to hysteresis and eddy currents in the core, which increase with frequency. At high currents, magnetic core inductors also show sudden departure from ideal behavior due to nonlinearity caused by magnetic saturation of the core.

Inductors radiate electromagnetic energy into surrounding space and may absorb electromagnetic emissions from other circuits, resulting in potential electromagnetic interference.

An early solid-state electrical switching and amplifying device called a saturable reactor exploits saturation of the core as a means of stopping the inductive transfer of current via the core.

The winding resistance appears as a resistance in series with the inductor; it is referred to as DCR (DC resistance). This resistance dissipates some of the reactive energy. The quality factor (or "Q") of an inductor is the ratio of its inductive reactance to its resistance at a given frequency, and is a measure of its efficiency. The higher the Q factor of the inductor, the closer it approaches the behavior of an ideal inductor. High Q inductors are used with capacitors to make resonant circuits in radio transmitters and receivers. The higher the Q is, the narrower the bandwidth of the resonant circuit.

The Q factor of an inductor is defined as, where "L" is the inductance, "R" is the DCR, and the product "ωL" is the inductive reactance:

"Q" increases linearly with frequency if "L" and "R" are constant. Although they are constant at low frequencies, the parameters vary with frequency. For example, skin effect, proximity effect, and core losses increase "R" with frequency; winding capacitance and variations in permeability with frequency affect "L".

At low frequencies and within limits, increasing the number of turns "N" improves "Q" because "L" varies as "N" while "R" varies linearly with "N". Similarly increasing the radius "r" of an inductor improves (or increases) "Q" because "L" varies as "r" while "R" varies linearly with "r". So high "Q" air core inductors often have large diameters and many turns. Both of those examples assume the diameter of the wire stays the same, so both examples use proportionally more wire. If the total mass of wire is held constant, then there would be no advantage to increasing the number of turns or the radius of the turns because the wire would have to be proportionally thinner.

Using a high permeability ferromagnetic core can greatly increase the inductance for the same amount of copper, so the core can also increase the Q. Cores however also introduce losses that increase with frequency. The core material is chosen for best results for the frequency band. High Q inductors must avoid saturation; one way is by using a (physically larger) air core inductor. At VHF or higher frequencies an air core is likely to be used. A well designed air core inductor may have a Q of several hundred.

Inductors are used extensively in analog circuits and signal processing. Applications range from the use of large inductors in power supplies, which in conjunction with filter capacitors remove ripple which is a multiple of the mains frequency (or the switching frequency for switched-mode power supplies) from the direct current output, to the small inductance of the ferrite bead or torus installed around a cable to prevent radio frequency interference from being transmitted down the wire. Inductors are used as the energy storage device in many switched-mode power supplies to produce DC current. The inductor supplies energy to the circuit to keep current flowing during the "off" switching periods and enables topographies where the output voltage is higher than the input voltage.

A tuned circuit, consisting of an inductor connected to a capacitor, acts as a resonator for oscillating current. Tuned circuits are widely used in radio frequency equipment such as radio transmitters and receivers, as narrow bandpass filters to select a single frequency from a composite signal, and in electronic oscillators to generate sinusoidal signals.

Two (or more) inductors in proximity that have coupled magnetic flux (mutual inductance) form a transformer, which is a fundamental component of every electric utility power grid. The efficiency of a transformer may decrease as the frequency increases due to eddy currents in the core material and skin effect on the windings. The size of the core can be decreased at higher frequencies. For this reason, aircraft use 400 hertz alternating current rather than the usual 50 or 60 hertz, allowing a great saving in weight from the use of smaller transformers. Transformers enable switched-mode power supplies that isolate the output from the input.

Inductors are also employed in electrical transmission systems, where they are used to limit switching currents and fault currents. In this field, they are more commonly referred to as reactors.

Inductors have parasitic effects which cause them to depart from ideal behavior. They create and suffer from electromagnetic interference (EMI). Their physical size prevents them from being integrated on semiconductor chips. So the use of inductors is declining in modern electronic devices, particularly compact portable devices. Real inductors are increasingly being replaced by active circuits such as the gyrator which can synthesize inductance using capacitors.
An inductor usually consists of a coil of conducting material, typically insulated copper wire, wrapped around a core either of plastic (to create an air-core inductor) or of a ferromagnetic (or ferrimagnetic) material; the latter is called an "iron core" inductor. The high permeability of the ferromagnetic core increases the magnetic field and confines it closely to the inductor, thereby increasing the inductance. Low frequency inductors are constructed like transformers, with cores of electrical steel laminated to prevent eddy currents. 'Soft' ferrites are widely used for cores above audio frequencies, since they do not cause the large energy losses at high frequencies that ordinary iron alloys do. Inductors come in many shapes. Some inductors have an adjustable core, which enables changing of the inductance. Inductors used to block very high frequencies are sometimes made by stringing a ferrite bead on a wire.

Small inductors can be etched directly onto a printed circuit board by laying out the trace in a spiral pattern. Some such planar inductors use a planar core. Small value inductors can also be built on integrated circuits using the same processes that are used to make interconnects. Aluminium interconnect is typically used, laid out in a spiral coil pattern. However, the small dimensions limit the inductance, and it is far more common to use a circuit called a "gyrator" that uses a capacitor and active components to behave similarly to an inductor. Regardless of the design, because of the low inductances and low power dissipation on-die inductors allow, they're currently only commercially used for high frequency RF circuits.
Inductors used in power regulation systems, lighting, and other systems that require low-noise operating conditions, are often partially or fully shielded. In telecommunication circuits employing induction coils and repeating transformers shielding of inductors in close proximity reduces circuit cross-talk.

The term "air core coil" describes an inductor that does not use a magnetic core made of a ferromagnetic material. The term refers to coils wound on plastic, ceramic, or other nonmagnetic forms, as well as those that have only air inside the windings. Air core coils have lower inductance than ferromagnetic core coils, but are often used at high frequencies because they are free from energy losses called core losses that occur in ferromagnetic cores, which increase with frequency. A side effect that can occur in air core coils in which the winding is not rigidly supported on a form is 'microphony': mechanical vibration of the windings can cause variations in the inductance.

At high frequencies, particularly radio frequencies (RF), inductors have higher resistance and other losses. In addition to causing power loss, in resonant circuits this can reduce the Q factor of the circuit, broadening the bandwidth. In RF inductors, which are mostly air core types, specialized construction techniques are used to minimize these losses. The losses are due to these effects:

To reduce parasitic capacitance and proximity effect, high Q RF coils are constructed to avoid having many turns lying close together, parallel to one another. The windings of RF coils are often limited to a single layer, and the turns are spaced apart. To reduce resistance due to skin effect, in high-power inductors such as those used in transmitters the windings are sometimes made of a metal strip or tubing which has a larger surface area, and the surface is silver-plated.

Small inductors for low current and low power are made in molded cases resembling resistors. These may be either plain (phenolic) core or ferrite core. An ohmmeter readily distinguishes them from similar-sized resistors by showing the low resistance of the inductor.

Ferromagnetic-core or iron-core inductors use a magnetic core made of a ferromagnetic or ferrimagnetic material such as iron or ferrite to increase the inductance. A magnetic core can increase the inductance of a coil by a factor of several thousand, by increasing the magnetic field due to its higher magnetic permeability. However the magnetic properties of the core material cause several side effects which alter the behavior of the inductor and require special construction:

Low-frequency inductors are often made with laminated cores to prevent eddy currents, using construction similar to transformers. The core is made of stacks of thin steel sheets or laminations oriented parallel to the field, with an insulating coating on the surface. The insulation prevents eddy currents between the sheets, so any remaining currents must be within the cross sectional area of the individual laminations, reducing the area of the loop and thus reducing the energy losses greatly. The laminations are made of low-conductivity silicon steel to further reduce eddy current losses.

For higher frequencies, inductors are made with cores of ferrite. Ferrite is a ceramic ferrimagnetic material that is nonconductive, so eddy currents cannot flow within it. The formulation of ferrite is xxFeO where xx represents various metals. For inductor cores soft ferrites are used, which have low coercivity and thus low hysteresis losses.

Another material is powdered iron cemented with a binder.

In an inductor wound on a straight rod-shaped core, the magnetic field lines emerging from one end of the core must pass through the air to re-enter the core at the other end. This reduces the field, because much of the magnetic field path is in air rather than the higher permeability core material and is a source of electromagnetic interference. A higher magnetic field and inductance can be achieved by forming the core in a closed magnetic circuit. The magnetic field lines form closed loops within the core without leaving the core material. The shape often used is a toroidal or doughnut-shaped ferrite core. Because of their symmetry, toroidal cores allow a minimum of the magnetic flux to escape outside the core (called "leakage flux"), so they radiate less electromagnetic interference than other shapes. Toroidal core coils are manufactured of various materials, primarily ferrite, powdered iron and laminated cores.

Probably the most common type of variable inductor today is one with a moveable ferrite magnetic core, which can be slid or screwed in or out of the coil. Moving the core farther into the coil increases the permeability, increasing the magnetic field and the inductance. Many inductors used in radio applications (usually less than 100 MHz) use adjustable cores in order to tune such inductors to their desired value, since manufacturing processes have certain tolerances (inaccuracy). Sometimes such cores for frequencies above 100 MHz are made from highly conductive non-magnetic material such as aluminum. They decrease the inductance because the magnetic field must bypass them.

Air core inductors can use sliding contacts or multiple taps to increase or decrease the number of turns included in the circuit, to change the inductance. A type much used in the past but mostly obsolete today has a spring contact that can slide along the bare surface of the windings. The disadvantage of this type is that the contact usually short-circuits one or more turns. These turns act like a single-turn short-circuited transformer secondary winding; the large currents induced in them cause power losses.

A type of continuously variable air core inductor is the "variometer". This consists of two coils with the same number of turns connected in series, one inside the other. The inner coil is mounted on a shaft so its axis can be turned with respect to the outer coil. When the two coils' axes are collinear, with the magnetic fields pointing in the same direction, the fields add and the inductance is maximum. When the inner coil is turned so its axis is at an angle with the outer, the mutual inductance between them is smaller so the total inductance is less. When the inner coil is turned 180° so the coils are collinear with their magnetic fields opposing, the two fields cancel each other and the inductance is very small. This type has the advantage that it is continuously variable over a wide range. It is used in antenna tuners and matching circuits to match low frequency transmitters to their antennas.

Another method to control the inductance without any moving parts requires an additional DC current bias winding which controls the permeability of an easily saturable core material. "See" Magnetic amplifier.

A choke is an inductor designed specifically for blocking high-frequency alternating current (AC) in an electrical circuit, while allowing DC or low-frequency signals to pass. Because the inductor resistricts or "chokes" the changes in current, this type of inductor is called a choke. It usually consists of a coil of insulated wire wound on a magnetic core, although some consist of a donut-shaped "bead" of ferrite material strung on a wire. Like other inductors, chokes resist changes in current passing through them increasingly with frequency. The difference between chokes and other inductors is that chokes do not require the high Q factor construction techniques that are used to reduce the resistance in inductors used in tuned circuits.

The effect of an inductor in a circuit is to oppose changes in current through it by developing a voltage across it proportional to the rate of change of the current. An ideal inductor would offer no resistance to a constant direct current; however, only superconducting inductors have truly zero electrical resistance.

The relationship between the time-varying voltage "v"("t") across an inductor with inductance "L" and the time-varying current "i"("t") passing through it is described by the differential equation:

When there is a sinusoidal alternating current (AC) through an inductor, a sinusoidal voltage is induced. The amplitude of the voltage is proportional to the product of the amplitude ("I") of the current and the frequency ("f") of the current.

In this situation, the phase of the current lags that of the voltage by π/2 (90°). For sinusoids, as the voltage across the inductor goes to its maximum value, the current goes to zero, and as the voltage across the inductor goes to zero, the current through it goes to its maximum value.

If an inductor is connected to a direct current source with value "I" via a resistance "R" (at least the DCR of the inductor), and then the current source is short-circuited, the differential relationship above shows that the current through the inductor will discharge with an exponential decay:

The ratio of the peak voltage to the peak current in an inductor energised from an AC source is called the reactance and is denoted "X".

Thus,

where "ω" is the angular frequency.

Reactance is measured in ohms but referred to as "impedance" rather than resistance; energy is stored in the magnetic field as current rises and discharged as current falls. Inductive reactance is proportional to frequency. At low frequency the reactance falls; at DC, the inductor behaves as a short-circuit. As frequency increases the reactance increases and at a sufficiently high frequency the reactance approaches that of an open circuit.

In filtering applications, with respect to a particular load impedance, an inductor has a corner frequency defined as:

When using the Laplace transform in circuit analysis, the impedance of an ideal inductor with no initial current is represented in the "s" domain by:

where

If the inductor does have initial current, it can be represented by: 

Inductors in a parallel configuration each have the same potential difference (voltage). To find their total equivalent inductance ("L"):

The current through inductors in series stays the same, but the voltage across each inductor can be different. The sum of the potential differences (voltage) is equal to the total voltage. To find their total inductance:

These simple relationships hold true only when there is no mutual coupling of magnetic fields between individual inductors.

Mutual inductance occurs when the magnetic field of an inductor induces a magnetic field in an adjacent inductor. Mutual induction is the basis of transformer construction.
M=(L1×L2)^(1/2)
where M is the maximum mutual inductance possible between 2 inductors and L1 and L2 are the two inductors.
In general M<=(L1×L2)^(1/2) as only a fraction of self flux is linked with the other. 
This fraction is called "Coefficient of flux linkage" or "Coefficient of coupling".
K=M÷((L1×L2)^0.5)

The table below lists some common simplified formulas for calculating the approximate inductance of several inductor constructions.




</doc>
<doc id="14899" url="https://en.wikipedia.org/wiki?curid=14899" title="Insulin pump">
Insulin pump

An insulin pump is a medical device used for the administration of insulin in the treatment of diabetes mellitus, also known as continuous subcutaneous insulin therapy.
The device configuration may vary depending on design. A traditional pump includes:

Other configurations are possible. More recent models may include disposable or semi-disposable designs for the pumping mechanism and may eliminate tubing from the infusion set.

An insulin pump is an alternative to multiple daily injections of insulin by insulin syringes or an insulin pen and allows for flexible insulin therapy when used in conjunction with blood glucose monitoring and carbohydrate counting.

Insulin pumps are used to deliver insulin on a continuous basis to a person with type I diabetes. 


Insulin pumps, cartridges, and infusion sets may be far more expensive than syringes used for insulin injection with several insulin pumps costing more than $6,000; necessary supplies can cost over $300. Another disadvantage of insulin pump use is a higher risk of developing diabetic ketoacidosis if the pump malfunctions. This can happen if the pump battery is discharged, if the insulin is inactivated by heat exposure, if the insulin reservoir runs empty, the tubing becomes loose and insulin leaks rather than being injected, or if the cannula becomes bent or kinked in the body, preventing delivery. Therefore, pump users typically monitor their blood sugars more frequently to evaluate the effectiveness of insulin delivery.


Use of insulin pumps is increasing because of:

In 1963 Dr. Arnold Kadish designed the first insulin pump to be worn as a backpack. A more wearable version was later devised by Dean Kamen in 1976. Kamen formed a company called "AutoSyringe" to market the product, which he sold to Baxter Health Care in 1981.

In 1984 an Infusaid implantable infusion device was used to treat a 22-year-old diabetic female successfully.

The insulin pump was first endorsed in the United Kingdom in 2003, by the National Institute for Health and Care Excellence (NICE).

New insulin pumps are becoming "smart" as new features are added to their design. These simplify the tasks involved in delivering an insulin bolus.

MiniMed 670G is a type of insulin pump and sensor system created by Medtronic. It was approved by the US FDA in September 2016 and was the first approved hybrid closed loop system which senses a diabetic person's basal insulin requirement and automatically adjusts its delivery to the body.


An insulin pump allows the replacement of slow-acting insulin for basal needs with a continuous infusion of rapid-acting insulin.

The insulin pump delivers a single type of rapid-acting insulin in two ways:

An insulin pump user can influence the profile of the rapid-acting insulin by shaping the bolus. Users can experiment with bolus shapes to determine what is best for any given food, which means that they can improve control of blood sugar by adapting the bolus shape to their needs.

A standard bolus is an infusion of insulin pumped completely at the onset of the bolus. It's the most similar to an injection. By pumping with a "spike" shape, the expected action is the fastest possible bolus for that type of insulin. The standard bolus is most appropriate when eating high carb low protein low fat meals because it will return blood sugar to normal levels quickly.

An extended bolus is a slow infusion of insulin spread out over time. By pumping with a "square wave" shape, the bolus avoids a high initial dose of insulin that may enter the blood and cause low blood sugar before digestion can facilitate sugar entering the blood. The extended bolus also extends the action of insulin well beyond that of the insulin alone. The extended bolus is appropriate when covering high fat high protein meals such as steak, which will be raising blood sugar for many hours past the onset of the bolus. The extended bolus is also useful for those with slow digestion (such as with gastroparesis or coeliac disease).

A combination bolus/multiwave bolus is the combination of a standard bolus spike with an extended bolus square wave. This shape provides a large dose of insulin up front, and then also extends the tail of the insulin action. The combination bolus is appropriate for high carb high fat meals such as pizza, pasta with heavy cream sauce, and chocolate cake.

A super bolus is a method of increasing the spike of the standard bolus. Since the action of the bolus insulin in the blood stream will extend for several hours, the basal insulin could be stopped or reduced during this time. This facilitates the "borrowing" of the basal insulin and including it into the bolus spike to deliver the same total insulin with faster action than can be achieved with spike and basal rate together. The super bolus is useful for certain foods (like sugary breakfast cereals) which cause a large post-prandial peak of blood sugar. It attacks the blood sugar peak with the fastest delivery of insulin that can be practically achieved by pumping.

Since the pump user is responsible to manually start a bolus, this provides an opportunity for the user to pre-bolus to improve upon the insulin pump's capability to prevent post-prandial hyperglycemia. A pre-bolus is simply a bolus of insulin given before it is actually needed to cover carbohydrates eaten.

There are two situations where a pre-bolus is helpful:

Similarly, a low blood sugar level or a low glycemic food might be best treated with a bolus "after" a meal is begun. The blood sugar level, the type of food eaten, and a person's individual response to food and insulin affect the ideal time to bolus with the pump.

The pattern for delivering basal insulin throughout the day can also be customized with a pattern to suit the pump user.

Basal insulin requirements will vary between individuals and periods of the day. The basal rate for a particular time period is determined by fasting while periodically evaluating the blood sugar level. Neither food nor bolus insulin must be taken for 4 hours before or during the evaluation period. If the blood sugar level changes dramatically during evaluation, then the basal rate can be adjusted to increase or decrease insulin delivery to keep the blood sugar level approximately steady.

For instance, to determine an individual's morning basal requirement, they must skip breakfast. On waking, they would test their blood glucose level periodically until lunch. Changes in blood glucose level are compensated with adjustments in the morning basal rate. The process is repeated over several days, varying the fasting period, until a 24-hour basal profile has been built up which keeps fasting blood sugar levels relatively steady. Once the basal rate is matched to the fasting basal insulin need, the pump user will then gain the flexibility to skip or postpone meals such as sleeping late on the weekends or working overtime on a weekday.

Many factors can change insulin requirements and require an adjustment to the basal rate:

A pump user should be educated by their diabetes care professional about basal rate determination before beginning pump therapy.

Since the basal insulin is provided as a rapid-acting insulin, the basal insulin can be immediately increased or decreased as needed with a temporary basal rate. Examples when this is helpful include:

In August 2011, an IBM researcher, Jay Radcliffe, demonstrated a security flaw in insulin pumps. Radcliffe was able to hack the wireless interface used to control the pump remotely. Pump manufacturer Medtronic later said security research by McAfee uncovered a flaw in its pumps that could be exploited.
Animas Corporation

In 2017, Animas Corporation announced the decision to discontinue both the manufacturing and sale of both their Animas Vibe and OneTouch Ping insulin pumps. The company partnered with Medtronic to ensure that Animas customers were able to stay on pump therapy and receive both the supplies and support they need. 




</doc>
<doc id="14900" url="https://en.wikipedia.org/wiki?curid=14900" title="ISO 3166">
ISO 3166

ISO 3166 is a standard published by the International Organization for Standardization (ISO) that defines codes for the names of countries, dependent territories, special areas of geographical interest, and their principal subdivisions (e.g., provinces or states). The official name of the standard is Codes for the representation of names of countries and their subdivisions.

It consists of three parts:

The first edition of ISO 3166, which included only alphabetic country codes, was published in 1974. The second edition, published in 1981, also included numeric country codes, with the third and fourth editions published in 1988 and 1993 respectively. The fifth edition, published between 1997 and 1999, was expanded into three parts to include codes for subdivisions and former countries.

The ISO 3166 standard is maintained by the ISO 3166 Maintenance Agency (ISO 3166/MA), located at the ISO central office in Geneva. Originally it was located at the Deutsches Institut für Normung (DIN) in Berlin. Its principal tasks are:

There are fifteen experts with voting rights on the ISO 3166/MA. Nine are representatives of national standards organizations:

The other six are representatives of major United Nations agencies or other international organizations who are all users of ISO 3166-1:

The ISO 3166/MA has further associated members who do not participate in the votes but who, through their expertise, have significant influence on the decision-taking procedure in the maintenance agency.

Country codes beginning with "X" are used for private custom use (reserved), never for official codes. Despite the words “private custom”, the use may include other public standards. Examples: 

Please see the List of ISO 3166 country codes.



</doc>
<doc id="14904" url="https://en.wikipedia.org/wiki?curid=14904" title="Intensive insulin therapy">
Intensive insulin therapy

Intensive insulin therapy or flexible insulin therapy is a therapeutic regimen for diabetes mellitus treatment. This newer approach contrasts with conventional insulin therapy. Rather than minimize the number of insulin injections per day (a technique which demands a rigid schedule for food and activities), the intensive approach favors flexible meal times with variable carbohydrate as well as flexible physical activities. The trade-off is the increase from 2 or 3 injections per day to 4 or more injections per day, which was considered "intensive" relative to the older approach. In North America in 2004, many endocrinologists prefer the term "flexible insulin therapy" (FIT) to "intensive therapy" and use it to refer to any method of replacing insulin that attempts to mimic the pattern of small continuous basal insulin secretion of a working pancreas combined with larger insulin secretions at mealtimes. The semantic distinction reflects changing treatment.

Long-term studies like the UK Prospective Diabetes Study ("UKPDS") and the Diabetes control and complications trial ("DCCT") showed that intensive insulin therapy achieved blood glucose levels closer to non-diabetic people and that this was associated with reduced frequency and severity of blood vessel damage. Damage to large and small blood vessels (macro- and microvascular disease) is central to the development of complications of diabetes.

This evidence convinced most physicians who specialize in diabetes care that an important goal of treatment is to make the biochemical profile of the diabetic patient (blood lipids, HbA1c, etc.) as close to the values of non-diabetic people as possible. This is especially true for young patients with many decades of life ahead.

A working pancreas continually secretes small amounts of insulin into the blood to maintain normal glucose levels, which would otherwise rise from glucose release by the liver, especially during the early morning dawn phenomenon. This insulin is referred to as "basal insulin secretion", and constitutes almost half the insulin produced by the normal pancreas.

Bolus insulin is produced during the digestion of meals. Insulin levels rise immediately as we begin to eat, remaining higher than the basal rate for 1 to 4 hours. This meal-associated ("prandial") insulin production is roughly proportional to the amount of carbohydrate in the meal.

Intensive or flexible therapy involves supplying a continual supply of insulin to serve as the "basal insulin", supplying meal insulin in doses proportional to nutritional load of the meals, and supplying extra insulin when needed to correct high glucose levels. These three components of the insulin regimen are commonly referred to as basal insulin, bolus insulin, and high glucose correction insulin.

One method of intensive insulinotherapy is based on multiple daily injections (sometimes referred to in medical literature as "MDI"). Meal insulin is supplied by injection of rapid-acting insulin before each meal in an amount proportional to the meal. Basal insulin is provided as a once or twice daily injection of dose of a long-acting insulin.

In an MDI regimen, long-acting insulins are preferred for basal use. An older insulin used for this purpose is ultralente, and beef ultralente in particular was considered for decades to be the gold standard of basal insulin. Long-acting insulin analogs such as insulin glargine (brand name "Lantus", made by Sanofi-Aventis) and insulin detemir (brand name "Levemir", made by Novo Nordisk) are also used, with insulin glargine used more than insulin detemir. Rapid-acting insulin analogs such as lispro (brand name "Humalog", made by Eli Lilly and Company) and aspart (brand name "Novolog"/"Novorapid", made by Novo Nordisk and "Apidra" made by Sanofi Aventis) are preferred by many clinicians over older regular insulin for meal coverage and high correction. Many people on MDI regimens carry insulin pens to inject their rapid-acting insulins instead of traditional syringes. Some people on an MDI regimen also use injection ports such as the I-port to minimize the number of daily skin punctures.

The other method of intensive/flexible insulin therapy is an insulin pump. It is a small mechanical device about the size of a deck of cards. It contains a syringe-like reservoir with about three days' insulin supply. This is connected by thin, disposable, plastic tubing to a needle-like cannula inserted into the patient's skin and held in place by an adhesive patch. The infusion tubing and cannula must be removed and replaced every few days.

An insulin pump can be programmed to infuse a steady amount of rapid-acting insulin under the skin. This steady infusion is termed the basal rate and is designed to supply the background insulin needs. Each time the patient eats, he or she must press a button on the pump to deliver a specified dose of insulin to cover that meal. Extra insulin is also given the same way to correct a high glucose reading. Although current pumps can include a glucose sensor, they cannot automatically respond to meals or to rising or falling glucose levels.

Both MDI and pumping can achieve similarly excellent glycemic control. Some people prefer injections because they are less expensive than pumps and do not require the wearing of a continually attached device. However, the clinical literature is very clear that patients whose basal insulin requirements tend not to vary throughout the day or do not require dosage precision smaller than 0.5 IU, are much less likely to realize much significant advantage of pump therapy. Another perceived advantage of pumps is the freedom from syringes and injections, however, infusion sets still require less frequent injections to guide infusion sets into the subcutaneous tissue.

Intensive/flexible insulin therapy requires frequent blood glucose checking. To achieve the best balance of blood sugar with either intensive/flexible method, a patient must check his or her glucose level with a meter monitoring of blood glucose several times a day. This allows optimization of the basal insulin and meal coverage as well as correction of high glucose episodes.

The two primary advantages of intensive/flexible therapy over more traditional two or three injection regimens are: 

Major disadvantages of intensive/flexible therapy are that it requires greater amounts of education and effort to achieve the goals, and it increases the daily cost for glucose monitoring four or more times a day. This cost can substantially increase when the therapy is implemented with an insulin pump and/or continuous glucose monitor.

It is a common notion that more frequent hypoglycemia is a disadvantage of intensive/flexible regimens. The frequency of hypoglycemia increases with increasing effort to achieve normal blood glucoses with most insulin regimens, but hypoglycemia can be minimized with appropriate glucose targets and control strategies. The difficulties lie in remembering to test, estimating meal size, taking the meal bolus and eating within the prescribed time, and being aware of snacks and meals that are not the expected size. When implemented correctly, flexible regimens offer greater ability to achieve good glycemic control with easier accommodation to variations of eating and physical activity.

Over the last two decades, the evidence that better glycemic control (i.e., keeping blood glucose and HbA1c levels as close to normal as possible) reduces the rates of many complications of diabetes has become overwhelming. As a result, diabetes specialists have expended increasing effort to help most people with diabetes achieve blood glucose levels as close to normal as achievable. It takes about the same amount of effort to achieve good glycemic control with a traditional two or three injection regimen as it does with flexible therapy: frequent glucose monitoring, attention to timing and amounts of meals. Many diabetes specialists no longer think of flexible insulin therapy as "intensive" or "special" treatment for a select group of patients but simply as standard care for most patients with type 1 diabetes.

The insulin pump is one device used in intensive insulinotherapy. The insulin pump is about the size of a beeper. It can be programmed to send a steady stream of insulin as "basal insulin". It contains a reservoir or cartridge holding several days' worth of insulin, the tiny battery-operated pump, and the computer chip that regulates how much insulin is pumped. The infusion set is a thin plastic tube with a fine needle at the end. There are also newer "pods" which do not require tubing. It carries the insulin from the pump to the infusion site beneath the skin. It sends a larger amount before eating meals as "bolus" doses.

The insulin pump replaces insulin injections. This device is useful for people who regularly forget to inject themselves or for people who don't like injections. This machine does the injecting by replacing the slow-acting insulin for basal needs with an ongoing infusion of rapid-acting insulin.

Basal insulin: the insulin that controls blood glucose levels between meals and overnight. It controls glucose in the fasting state.

Boluses: the insulin that is released when food is eaten or to correct a high reading.

Another device used in intensive insulinotherapy is the injection port. An injection port is a small disposable device, similar to the infusion set used with an insulin pump, configured to accept a syringe. Standard insulin injections are administered through the injection port. When using an injection port, the syringe needle always stays above the surface of the skin, thus reducing the number of skin punctures associated with intensive insulinotheraphy.


</doc>
<doc id="14906" url="https://en.wikipedia.org/wiki?curid=14906" title="Interwiki links">
Interwiki links

Interwiki linking ("W-link") is a facility for creating links to the many wikis on the World Wide Web. Users avoid pasting in entire URLs (as they would for regular web pages) and instead use a shorthand similar to links within the same wiki (intrawiki links).

Unlike domain names on the Internet, there is no globally defined list of interwiki prefixes, so owners of a wiki must define an interwiki map (InterMap) appropriate to their needs. Users generally have to create separate accounts for each wiki they intend to use (unless they intend to edit anonymously). Variations in text formatting and layout can also hinder a seamless transition from one wiki to the next.

By making wiki links simpler to type for the members of a particular community, these features help bring the different wikis closer together. Furthering that goal, interwiki "bus tours" (similar to webrings) have been created to explain the purposes and highlights of different wikis. Such examples on Wikipedia include and .

Interwiki link notation varies, depending largely on the syntax a wiki uses for markup. The two most common link patterns in wikis are CamelCase and free links (arbitrary phrases surrounded by some set delimiter, such as <nowiki>double square brackets</nowiki>). CURIE syntax—an emerging W3C standard—uses a single set of square brackets.

Interwiki links on a CamelCase-based wiki frequently take the form of "Code:PageName", where "Code" is the defined InterMap prefix for another wiki. Thus, a link "WikiPedia:InterWiki" could be rendered in HTML as a link to an article on Wikipedia: for example, . Linking from a CamelCase-wiki to a page that contains spaces in its title typically requires replacing the spaces with underscores (e.g. WikiPedia:Main_Page).

Interwiki links on wikis based on free links, such as Wikipedia, typically follow the same principle, but using the delimiters that would be used for internal links. These links can then be parsed and escaped as they would be if they were internal, allowing easier typing of spaces but potentially causing problems with other special characters. For example, on Wikipedia, codice_1 appears as , and codice_1 (former syntax: codice_1) appears as .

The MediaWiki software has an additional feature which uses similar notation to create automatic interlanguage links—for instance, the link codice_1 (with no leading colon) automatically creates a reference labeled "Other languages: | ..." at the top and bottom of, or in a sidebar next to, the article display. Various other wiki software systems have features for "semi-internal" links of this kind, such as support for namespaces or multiple sub-communities.

Most InterMap implementations simply replace the interwiki prefix with a full URL prefix, so many non-wiki websites can also be referred to using the system. A reference to a definition on the Free On-line Dictionary of Computing, for instance, could take the form codice_1 which would tell the system to append and display the link as Foldoc:foo. This makes it very easy to link to commonly referenced resources from within a wiki page, without the need to even know the form of the URL in question.

The interwiki concept can equally be applied to links "from" non-wiki websites. Advogato, for instance, offers a syntax for creating shorthand links based on a MeatBall-derived InterMap.

WordPress offers a similar "shortcode" shorthand notation for embedding images, videos, LaTeX formulas and equations, maps, etc. hosted on other websites.

Internally, a wiki that uses interwiki links needs to have a mapping from wiki-code links to full URLs. For example, codice_1 might appear as , but link to codice_7.

Since most wiki systems use URLs for individual pages where the page's title appears at the end of an otherwise unchanging address, the simplest way of defining such mappings is by substituting the interwiki prefix for the unchanging part of the URL. So in the example above, the codice_8 has simply been replaced by codice_9 in creating the target of the HTML rendered link.

Rather than creating a new list from scratch for every wiki, it is often useful to obtain a copy of that from another site. Sites such as MeatballWiki and the UseModWiki site contain comprehensive lists which are often used for this purpose - the former being publicly editable in the same way as any other wiki page, and the latter being verified as usable but potentially out of date. MediaWiki's default list of interwiki links is derived from an old version of MeatballWiki's list.





</doc>
<doc id="14907" url="https://en.wikipedia.org/wiki?curid=14907" title="Inverse function">
Inverse function

In mathematics, an inverse function (or anti-function) is a function that "reverses" another function: if the function applied to an input gives a result of , then applying its inverse function to gives the result , and vice versa, i.e., if and only if .

As an example, consider the real-valued function of a real variable given by . Thinking of this as a step-by-step procedure (namely, take a number , multiply it by 5, then subtract 7 from the result), to reverse this and get back from some output value, say , we should undo each step in reverse order. In this case it means we should add 7 to and then divide the result by 5. In functional notation this inverse function would be given by,

With we have that and .

Not all functions have inverse functions. Those that do are called "invertible". For a function to have an inverse, it must have the property that for every in there is one, and only one in so that . This property ensures that a function exists with the necessary relationship with .

Let be a function whose domain is the set , and whose codomain is the set . Then is "invertible" if there exists a function with domain and image , with the property:

If is invertible, the function is unique, which means that there is exactly one function satisfying this property (no more, no less). That function is then called "the" inverse of , and is usually denoted as , a notation introduced by John Frederick William Herschel in 1813.

Stated otherwise, a function, considered as a binary relation, has an inverse if and only if the converse relation is a function on the codomain , in which case the converse relation is the inverse function.

Not all functions have an inverse. For a function to have an inverse, each element must correspond to no more than one ; a function with this property is called one-to-one or an injection. If is to be a function on , then each element must correspond to some . Functions with this property are called surjections. This property is satisfied by definition if is the image of , but may not hold in a more general context. To be invertible, a function must be both an injection and a surjection. Such functions are called bijections. The inverse of an injection that is not a bijection, that is, a function that is not a surjection, is only a partial function on , which means that for some , is undefined. If a function is invertible, then both it and its inverse function are bijections.

There is another convention used in the definition of functions. This can be referred to as the "set-theoretic" or "graph" definition using ordered pairs in which a codomain is never referred to. Under this convention all functions are surjections, and so, being a bijection simply means being an injection. Authors using this convention may use the phrasing that a function is invertible if and only if it is an injection. The two conventions need not cause confusion as long as it is remembered that in this alternate convention the codomain of a function is always taken to be the range of the function.

The function given by is not injective since each possible result "y" (except 0) corresponds to two different starting points in – one positive and one negative, and so this function is not invertible. With this type of function it is impossible to deduce an input from its output. Such a function is called non-injective or, in some applications, information-losing.

If the domain of the function is restricted to the nonnegative reals, that is, the function is redefined to be with the same "rule" as before, then the function is bijective and so, invertible. The inverse function here is called the "(positive) square root function".
If is an invertible function with domain and range , then

Using the composition of functions we can rewrite this statement as follows:

where is the identity function on the set ; that is, the function that leaves its argument unchanged. In category theory, this statement is used as the definition of an inverse morphism.

Considering function composition helps to understand the notation . Repeatedly composing a function with itself is called iteration. If is applied times, starting with the value , then this is written as ; so , etc. Since , composing and yields , "undoing" the effect of one application of .

While the notation might be misunderstood, certainly denotes the multiplicative inverse of and has nothing to do with the inverse function of .

In keeping with the general notation, some English authors use expressions like to denote the inverse of the sine function applied to (actually a partial inverse; see below) Other authors feel that this may be confused with the notation for the multiplicative inverse of , which can be denoted as . To avoid any confusion, an inverse trigonometric function is often indicated by the prefix "arc" (for Latin "arcus"). For instance, the inverse of the sine function is typically called the arcsine function, written as . Similarly, the inverse of a hyperbolic function is indicated by the prefix "ar" (for Latin "area"). For instance, the inverse of the hyperbolic sine function is typically written as . Other inverse special functions are sometimes prefixed with the prefix "inv" if the ambiguity of the notation should be avoided.

Since a function is a special type of binary relation, many of the properties of an inverse function correspond to properties of converse relations.

If an inverse function exists for a given function , then it is unique. This follows since the inverse function must be the converse relation, which is completely determined by .

There is a symmetry between a function and its inverse. Specifically, if is an invertible function with domain and range , then its inverse has domain and range , and the inverse of is the original function . In symbols, for functions and ,

This statement is a consequence of the implication that for to be invertible it must be bijective. The involutory nature of the inverse can be concisely expressed by 
The inverse of a composition of functions is given by 
Notice that the order of and have been reversed; to undo followed by , we must first undo and then undo .

For example, let and let . Then the composition is the function that first multiplies by three and then adds five,

To reverse this process, we must first subtract five, and then divide by three,

This is the composition

If is a set, then the identity function on is its own inverse:

More generally, a function is equal to its own inverse if and only if the composition is equal to . Such a function is called an involution.

Single-variable calculus is primarily concerned with functions that map real numbers to real numbers. Such functions are often defined through formulas, such as:
A surjective function from the real numbers to the real numbers possesses an inverse as long as it is one-to-one, i.e. as long as the graph of has, for each possible value only one corresponding value, and thus passes the horizontal line test.

The following table shows several standard functions and their inverses:

One approach to finding a formula for , if it exists, is to solve the equation for . For example, if is the function

then we must solve the equation for :

Thus the inverse function is given by the formula

Sometimes the inverse of a function cannot be expressed by a formula with a finite number of terms. For example, if is the function

then is a bijection, and therefore possesses an inverse function . The formula for this inverse has an infinite number of terms:

If is invertible, then the graph of the function

is the same as the graph of the equation

This is identical to the equation that defines the graph of , except that the roles of and have been reversed. Thus the graph of can be obtained from the graph of by switching the positions of the and axes. This is equivalent to reflecting the graph across the line

A continuous function is invertible on its range (image) if and only if it is either strictly increasing or decreasing (with no local maxima or minima). For example, the function

is invertible, since the derivative

If the function is differentiable on an interval and for each , then the inverse is differentiable on . If , the derivative of the inverse is given by the inverse function theorem,

Using Leibniz's notation the formula above can be written as

This result follows from the chain rule (see the article on inverse functions and differentiation).

The inverse function theorem can be generalized to functions of several variables. Specifically, a differentiable multivariable function is invertible in a neighborhood of a point as long as the Jacobian matrix of at is invertible. In this case, the Jacobian of at is the matrix inverse of the Jacobian of at .





Even if a function is not one-to-one, it may be possible to define a partial inverse of by restricting the domain. For example, the function

is not one-to-one, since . However, the function becomes one-to-one if we restrict to the domain , in which case

(If we instead restrict to the domain , then the inverse is the negative of the square root of .) Alternatively, there is no need to restrict the domain if we are content with the inverse being a multivalued function:

Sometimes this multivalued inverse is called the full inverse of , and the portions (such as and −) are called "branches". The most important branch of a multivalued function (e.g. the positive square root) is called the "principal branch", and its value at is called the "principal value" of .

For a continuous function on the real line, one branch is required between each pair of local extrema. For example, the inverse of a cubic function with a local maximum and a local minimum has three branches (see the adjacent picture).
These considerations are particularly important for defining the inverses of trigonometric functions. For example, the sine function is not one-to-one, since

for every real (and more generally for every integer ). However, the sine is one-to-one on the interval
, and the corresponding partial inverse is called the arcsine. This is considered the principal branch of the inverse sine, so the principal value of the inverse sine is always between − and . The following table describes the principal branch of each inverse trigonometric function:

If , a left inverse for (or "retraction" of ) is a function such that composing with from the left gives the identity function:

That is, the function satisfies the rule

Thus, must equal the inverse of on the image of , but may take any values for elements of not in the image. A function with a left inverse is necessarily injective. In classical mathematics, every injective function with a nonempty domain necessarily has a left inverse; however, this may fail in constructive mathematics. For instance, a left inverse of the inclusion of the two-element set in the reals violates indecomposability by giving a retraction of the real line to the set .

A right inverse for (or "section" of ) is a function such that

That is, the function satisfies the rule

Thus, may be any of the elements of that map to under . A function has a right inverse if and only if it is surjective (though constructing such an inverse in general requires the axiom of choice).

An inverse that is both a left and right inverse must be unique. However, if is a left inverse for , then may or may not be a right inverse for ; and if is a right inverse for , then is not necessarily a left inverse for . For example, let denote the squaring map, such that for all in , and let denote the square root map, such that for all . Then for all in ; that is, is a right inverse to . However, is not a left inverse to , since, e.g., .

If is any function (not necessarily invertible), the preimage (or inverse image) of an element is the set of all elements of that map to :

The preimage of can be thought of as the image of under the (multivalued) full inverse of the function .

Similarly, if is any subset of , the preimage of is the set of all elements of that map to :

For example, take a function , where . This function is not invertible for reasons discussed . Yet preimages may be defined for subsets of the codomain:

The preimage of a single element – a singleton set – is sometimes called the "fiber" of . When is the set of real numbers, it is common to refer to as a "level set".






</doc>
<doc id="14909" url="https://en.wikipedia.org/wiki?curid=14909" title="Inertia">
Inertia

Inertia is the resistance of any physical object to any change in its velocity. This includes changes to the object's speed, or direction of motion. 
An aspect of this property is the tendency of objects to keep moving in a straight line at a constant speed, when no forces act upon them.

Inertia comes from the Latin word, "iners", meaning idle, sluggish. Inertia is one of the primary manifestations of mass, which is a quantitative property of physical systems. Isaac Newton defined inertia as his first law in his "Philosophiæ Naturalis Principia Mathematica", which states:

In common usage, the term "inertia" may refer to an object's "amount of resistance to change in velocity" or for simpler terms, "resistance to a change in motion" (which is quantified by its mass), or sometimes to its momentum, depending on the context. The term "inertia" is more properly understood as shorthand for "the principle of inertia" as described by Newton in his first law of motion: an object not subject to any net external force moves at a constant velocity. Thus, an object will continue moving at its current velocity until some force causes its speed or direction to change.

On the surface of the Earth, inertia is often masked by gravity and the effects of friction and air resistance, both of which tend to decrease the speed of moving objects (commonly to the point of rest). This misled the philosopher Aristotle to believe that objects would move only as long as force was applied to them.

The principle of inertia is one of the fundamental principles in classical physics that are still used today to describe the motion of objects and how they are affected by the applied forces on them.

Prior to the Renaissance, the most generally accepted theory of motion in Western philosophy was based on Aristotle who around about 335 BC to 322 BC said that, in the absence of an external motive power, all objects (on Earth) would come to rest and that moving objects only continue to move so long as there is a power inducing them to do so. Aristotle explained the continued motion of projectiles, which are separated from their projector, by the action of the surrounding medium, which continues to move the projectile in some way. Aristotle concluded that such violent motion in a void was impossible.

Despite its general acceptance, Aristotle's concept of motion was disputed on several occasions by notable philosophers over nearly two millennia. For example, Lucretius (following, presumably, Epicurus) stated that the "default state" of matter was motion, not stasis. In the 6th century, John Philoponus criticized the inconsistency between Aristotle's discussion of projectiles, where the medium keeps projectiles going, and his discussion of the void, where the medium would hinder a body's motion. Philoponus proposed that motion was not maintained by the action of a surrounding medium, but by some property imparted to the object when it was set in motion. Although this was not the modern concept of inertia, for there was still the need for a power to keep a body in motion, it proved a fundamental step in that direction. This view was strongly opposed by Averroes and by many scholastic philosophers who supported Aristotle. However, this view did not go unchallenged in the Islamic world, where Philoponus did have several supporters who further developed his ideas.

In the 11th century, Persian polymath Ibn Sina (Avicenna) claimed that a projectile in a vacuum would not stop unless acted upon.

In the 14th century, Jean Buridan rejected the notion that a motion-generating property, which he named "impetus", dissipated spontaneously. Buridan's position was that a moving object would be arrested by the resistance of the air and the weight of the body which would oppose its impetus. Buridan also maintained that impetus increased with speed; thus, his initial idea of impetus was similar in many ways to the modern concept of momentum. Despite the obvious similarities to more modern ideas of inertia, Buridan saw his theory as only a modification to Aristotle's basic philosophy, maintaining many other peripatetic views, including the belief that there was still a fundamental difference between an object in motion and an object at rest. Buridan also believed that impetus could be not only linear, but also circular in nature, causing objects (such as celestial bodies) to move in a circle.

Buridan's thought was followed up by his pupil Albert of Saxony (1316–1390) and the Oxford Calculators, who performed various experiments that further undermined the classical, Aristotelian view. Their work in turn was elaborated by Nicole Oresme who pioneered the practice of demonstrating laws of motion in the form of graphs.

Shortly before Galileo's theory of inertia, Giambattista Benedetti modified the growing theory of impetus to involve linear motion alone:

Benedetti cites the motion of a rock in a sling as an example of the inherent linear motion of objects, forced into circular motion.

According to historian of science Charles Coulston Gillispie, inertia "entered science as a physical consequence of Descartes' geometrization of space-matter, combined with the immutability of God."
The principle of inertia, which originated with Aristotle for "motions in a void", states that an object tends to resist a change in motion. According to Newton, an object will stay at rest or stay in motion (i.e. maintain its velocity) unless acted on by a net external force, whether it results from gravity, friction, contact, or some other force. The Aristotelian division of motion into mundane and celestial became increasingly problematic in the face of the conclusions of Nicolaus Copernicus in the 16th century, who argued that the Earth is never at rest, but is actually in constant motion around the Sun. Galileo, in his further development of the Copernican model, recognized these problems with the then-accepted nature of motion and, at least partially as a result, included a restatement of Aristotle's description of motion in a void as a basic physical principle:

A body moving on a level surface will continue in the same direction at a constant speed unless disturbed. 

Galileo writes that "all external impediments removed, a heavy body on a spherical surface concentric with the earth will maintain itself in that state in which it has been; if placed in movement towards the west (for example), it will maintain itself in that movement." This notion which is termed "circular inertia" or "horizontal circular inertia" by historians of science, is a precursor to, but distinct from, Newton's notion of rectilinear inertia. For Galileo, a motion is "horizontal" if it does not carry the moving body towards or away from the centre of the earth, and for him, "a ship, for instance, having once received some impetus through the tranquil sea, would move continually around our globe without ever stopping."

It is also worth noting that Galileo later (in 1632) concluded that based on this initial premise of inertia, it is impossible to tell the difference between a moving object and a stationary one without some outside reference to compare it against. This observation ultimately came to be the basis for Albert Einstein to develop the theory of special relativity.

The first physicist to completely break away from the Aristotelian model of motion was Isaac Beeckman in 1614.

Concepts of inertia in Galileo's writings would later come to be refined, modified and codified by Isaac Newton as the first of his Laws of Motion (first published in Newton's work, "Philosophiae Naturalis Principia Mathematica", in 1687):

Every body perseveres in its state of rest, or of uniform motion in a right line, unless it is compelled to change that state by forces impressed thereon. 
Since initial publication, Newton's Laws of Motion (and by inclusion, this first law) have come to form the basis for the branch of physics known as classical mechanics.

The term "inertia" was first introduced by Johannes Kepler in his "Epitome Astronomiae Copernicanae" (published in three parts from 1617–1621); however, the meaning of Kepler's term (which he derived from the Latin word for "idleness" or "laziness") was not quite the same as its modern interpretation. Kepler defined inertia only in terms of a resistance to movement, once again based on the presumption that rest was a natural state which did not need explanation. It was not until the later work of Galileo and Newton unified rest and motion in one principle that the term "inertia" could be applied to these concepts as it is today.

Nevertheless, despite defining the concept so elegantly in his laws of motion, even Newton did not actually use the term "inertia" to refer to his First Law. In fact, Newton originally viewed the phenomenon he described in his First Law of Motion as being caused by "innate forces" inherent in matter, which resisted any acceleration. Given this perspective, and borrowing from Kepler, Newton attributed the term "inertia" to mean "the innate force possessed by an object which resists changes in motion"; thus, Newton defined "inertia" to mean the cause of the phenomenon, rather than the phenomenon itself. However, Newton's original ideas of "innate resistive force" were ultimately problematic for a variety of reasons, and thus most physicists no longer think in these terms. As no alternate mechanism has been readily accepted, and it is now generally accepted that there may not be one which we can know, the term "inertia" has come to mean simply the phenomenon itself, rather than any inherent mechanism. Thus, ultimately, "inertia" in modern classical physics has come to be a name for the same phenomenon described by Newton's First Law of Motion, and the two concepts are now considered to be equivalent.

Albert Einstein's theory of special relativity, as proposed in his 1905 paper entitled "On the Electrodynamics of Moving Bodies" was built on the understanding of inertial reference frames developed by Galileo and Newton. While this revolutionary theory did significantly change the meaning of many Newtonian concepts such as mass, energy, and distance, Einstein's concept of inertia remained unchanged from Newton's original meaning. However, this resulted in a limitation inherent in special relativity: the principle of relativity could only apply to inertial reference frames. To address this limitation, Einstein developed his general theory of relativity ("The Foundation of the General Theory of Relativity", 1916), which provided a theory including "noninertial" (accelerated) reference frames.

A quantity related to inertia is "rotational inertia" (→ moment of inertia), the property that a rotating rigid body maintains its state of uniform rotational motion. Its angular momentum remains unchanged, unless an external torque is applied; this is also called conservation of angular momentum. Rotational inertia is often considered in relation to a rigid body. For example, a gyroscope uses the property that it resists any change in the axis of rotation.




</doc>
<doc id="14910" url="https://en.wikipedia.org/wiki?curid=14910" title="Ibanez">
Ibanez

The Hoshino Gakki company began in 1908 as the musical instrument sales division of the "Hoshino Shoten", a bookstore chain. Hoshino Gakki decided in 1935 to make Spanish-style acoustic guitars, at first using the "Ibanez Salvador" brand name in honor of Spanish luthier Salvador Ibáñez, and later simply "Ibanez".

The modern era of Ibanez guitars began in 1957. The late 1950s and 1960s Ibanez catalogues show guitars with some wild-looking designs, manufactured by Kiso Suzuki Violin, Guyatone, and their own Tama factory established in 1962. After the Tama factory stopped manufacturing guitars in 1966, Hoshino Gakki used the Teisco and FujiGen Gakki guitar factories to make Ibanez guitars, and after the Teisco String Instrument factory closed in 1969/1970, Hoshino Gakki used the FujiGen Gakki guitar factory to make Ibanez guitars.
In the 1960s, Japanese guitar makers mainly copied American guitar designs, and Ibanez-branded copies of Gibson, Fender, and Rickenbacker models appear. This resulted in the so-called lawsuit period.

Hoshino Gakki introduced Ibanez models that were definitely not copies of the Gibson or Fender designs, such as the Iceman and the Roadstar series. The company has produced its own guitar designs ever since. The late 1980s and early 1990s were an important period for the Ibanez brand. Hoshino Gakki's relationship with guitarist Steve Vai resulted in the introduction of the Ibanez JEM and the Ibanez Universe models; after the earlier successes of the Roadstar and Iceman models in the late 1970s – early 1980s, Hoshino Gakki entered the superstrat market with the RG series, a lower-priced version of their JEM series.
Hoshino Gakki also had semi-acoustic, nylon- and steel-stringed acoustic guitars manufactured under the Ibanez name. Most Ibanez guitars were made by the FujiGen guitar factory in Japan up until the mid- to late 1980s, and from then on Ibanez guitars have also been made in other Asian countries such as Korea, China, and Indonesia. During the early 1980s, the FujiGen guitar factory also produced most of the Roland guitar synthesizers, including the Stratocaster-style Roland G-505, the twin-humbucker Roland G-202 (endorsed by Adrian Belew, Eric Clapton, Dean Brown, Jeff Baxter, Yannis Spathas, Christoforos Krokidis, Steve Howe, Mike Rutherford, Andy Summers, Neal Schon and Steve Hackett) and the Ibanez X-ING IMG-2010.

Cimar and Starfield were guitar and bass brands owned by Hoshino Gakki. In the 1970s, Hoshino Gakki and Kanda Shokai shared some guitar designs, and so some Ibanez and Greco guitars have the same features. The Greco versions were sold in Japan and the Ibanez versions were sold outside Japan. From 1982, Ibanez guitars have also been sold in Japan as well.

Guitar brands such as Antoria and Mann shared some Ibanez guitar designs. The Antoria guitar brand was managed by JT Coppock Leeds Ltd England. CSL was a brand name managed by Charles Summerfield Ltd England. Maurice Summerfield of the Charles Summerfield Ltd company contributed some design ideas to Hoshino Gakki and also imported Ibanez and CSL guitars into the UK from 1964 to 1987. The Maxxas brand name came about because Hoshino Gakki thought that the guitar did not fit in with the Ibanez model range and was therefore named Maxxas by Rich Lasner from Hoshino USA.

Harry Rosenbloom, founder of the (now-closed) Medley Music of Bryn Mawr, Pennsylvania, was manufacturing handmade guitars under the name "Elger". By 1965, Rosenbloom had decided to stop manufacturing guitars and chose to become the exclusive North American distributor for Ibanez guitars. In September 1972, Hoshino began a partnership with Elger Guitars to import guitars from Japan. In September 1981, Elger was renamed "Hoshino U.S.A.", retaining the company headquarters in Bensalem, Pennsylvania as a distribution and quality-control center.

On June 28, 1977, in the Philadelphia Federal District Court, a lawsuit was filed by the Norlin Corporation, the parent company of Gibson Guitars, against Elger/Hoshino U.S.A.'s use of the Gibson headstock design and logo. Hoshino settled out of court in early 1978 and the case was officially closed on February 2, 1978.

After the lawsuit, Hoshino Gakki abandoned the strategy of copying "classic" electric guitar designs, having already introduced a plethora of original designs. Hoshino was producing their original Artist models from 1974, introducing a set-neck model in 1975. In 1977, they upgraded and extended their Artist range and introduced a number of other top-quality original designs made to match or surpass famous American brands: the Performer and short-lived Concert ranges, which competed with the Les Paul; through-neck Musicians; Studios in fixed- and through-neck construction; the radically shaped Iceman; and the Roadster which morphed into the Roadstar range, precursor to the popular superstrat era in the mid-1980s. The newer Ibanez models began incorporating more modern elements into their design such as radical body shapes, slimmer necks, 2-octave fingerboards, slim pointed headstocks, higher-output electronics, humbucker/single-coil/humbucker (H/S/H) pickup configurations, locking tremolo bridges and different finishes.













In the 1970s, the Nisshin Onpa company who owned the Maxon brand name, developed and began selling a series of effect pedals in Japan. Hoshino Gakki licensed these for sale using the name Ibanez outside Japan. These two companies eventually began doing less and less business together until Nisshin Onpa ceased manufacturing the TS-9 reissue for Hoshino Gakki in 2002.





</doc>
