<doc id="14700" url="https://en.wikipedia.org/wiki?curid=14700" title="Demographics of Italy">
Demographics of Italy

This article is about the demographic features of the population of Italy, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.

At the beginning of 2020, Italy had an estimated population of 60.3 million. Its population density, at , is higher than that of most Western European countries. However, the distribution of the population is widely uneven; the most densely populated areas are the Po Valley (that accounts for almost half of the national population) in northern Italy and the metropolitan areas of Rome and Naples in central and southern Italy, while other vast areas are very sparsely populated, like the plateaus of Basilicata, the Alps and Apennines highlands, and the island of Sardinia.

The population of the country almost doubled during the twentieth century, but the pattern of growth was extremely uneven due to large-scale internal migration from the rural South to the industrial cities of the North, a phenomenon which happened as a consequence of the Italian economic miracle of the 1950–1960s. In addition, after centuries of net emigration, from the 1980s Italy has experienced large-scale immigration for the first time in modern history. According to the Italian government, there were an estimated 5,234,000 foreign nationals resident in Italy on 1st of January 2019.

High fertility and birth rates persisted until the 1970s, after which they started to dramatically decline, leading to rapid population aging. At the end of the first decade of the 21st century, one in five Italians was over 65 years old. However, as a result of the massive immigration of the last two decades, Italy has, in recent years, experienced a significant growth in birth rates. The total fertility rate has also climbed from an all-time low of 1.18 children per woman in 1995 to 1.41 in 2008.

Since the 1984 Lateran Treaty agreement, Italy has no official religion. However, it recognizes the role the Catholic Church plays in Italian society. In 2017, 78% of the population identified as Catholic, 15% as non-believers or atheists, 2% as other Christians and 6% adhered to other religions.

70.4% of Italian population is classified as urban, a relatively low figure among developed countries. During the last two decades, Italy underwent a devolution process, that eventually led to the creation of administrative metropolitan areas, in order to give major cities and their metropolitan areas a provincial status (somehow similar to PRC's direct-controlled municipality).
According to OECD, the largest conurbations are:

Italy used to be a country of mass emigration from the late 19th century until the 1970s. Between 1898 and 1914, the peak years of Italian diaspora, approximately 750,000 Italians emigrated each year. Italian communities once thrived in the former African colonies of Eritrea (nearly 100,000 at the beginning of World War II), Somalia and Libya (150,000 Italians settled in Libya, constituting about 18% of the total Libyan population). All of Libya's Italians were expelled from the North African country in 1970. In addition, after the annexation of Istria in 1945, up to 350,000 ethnic Italians left Titoist Yugoslavia. Today, large numbers of people with full or significant Italian ancestry are found in
Brazil (25 million), Argentina (20 million), US (17.8 million), France (5 million), Venezuela (2 million), Uruguay (1.5 million), Canada (1.4 million), and Australia (800,000).

As a result of the profound economic and social changes induced by postwar industrialization, including low birth rates, an aging population and thus a shrinking workforce, during the 1980s Italy became to attract rising flows of foreign immigrants. The present-day figure of about 5 million foreign residents, that make up some 8% of the total population, include 97,000 children born in Italy to foreign nationals (14% of total births in Italy) in 2014, but exclude foreign nationals who have subsequently acquired Italian nationality; this applied to 106,000 people in 2014. The official figures also exclude illegal immigrants, the so-called "clandestini", whose numbers are very difficult to determine. In May 2008 "The Boston Globe" quoted an estimate of 670,000 for this group. Since the fall of the Berlin Wall in 1989, and more recently, the 2004 and 2007 enlargements of the European Union, the main waves of migration came from the former socialist countries of Eastern Europe (especially Romania, Albania, Ukraine and Poland). The second most important area of immigration to Italy has always been the neighbouring North Africa (in particular, Morocco, Egypt and Tunisia), with soaring arrivals as a consequence of the Arab Spring. Furthermore, in recent years, growing migration fluxes from the Far East (notably, China and the Philippines) and Latin America (Ecuador, Peru) have been recorded. Currently, there are 1.2 million Romanian-born citizens living and working in Italy. Today the Romanians make up the largest community in the country, followed by Albanians (441,027) and Moroccans (422,980). The fourth largest community in Italy are the Chinese. The majority of Chinese living in Italy came from the city of Wenzhou in the province of Zhejiang. Currently the foreign-born population of Italy was from: Europe (54%), Africa (22%), Asia (16%), the Americas (8%) and Oceania (0.06%). The distribution of immigrants is largely uneven in Italy: 84.9% of immigrants live in the northern and central parts of the country (the most economically developed areas), while only 15.1% live in the southern half of the peninsula.

Within the Italian population, there is enough cultural, linguistic, genetic and historical diversity for them to constitute several distinct groups throughout the peninsula. In this regard, peoples like the Friulians, the Ladins, the Sardinians and South Tyroleans, who also constitute recognized linguistic minorities, or even the Sicilians, are cases in point attesting to such internal diversity.

Immigrants as of 2019:

Sources: Our World In Data and the United Nations.

1871–1950

1950–2015

Source: "UN World Population Prospects"

The total fertility rate is the number of children born per woman. It is based on fairly good data for the entire period. Sources: Our World In Data and Gapminder Foundation.




In 2017, 366,406 (80,0%) babies were born to mothers with Italian citizenship, 19,010 (4,1%) were born to mothers with Romanian citizenship, 11,871 (2,6%) were born to mothers with Moroccan citizenship and 9,125 (2,0%) to mothers with Albanian citizenship.

Demographic statistics according to the World Population Review in 2019.


The following demographic statistics are from Italy's Istituto Nazionale di Statistica and Cia World Factbook.



















Christian 80% (overwhelmingly Roman Catholic with very small groups of Jehovah's Witnesses and Protestants), Muslim (about 800,000 to 1 million), atheist and agnostic 20%




Unemployment, youth ages 15–24:

Italy's official language is Italian; Ethnologue has estimated that there are about 55 million speakers of Italian in the country and a further 6.7 million outside of it, primarily in the neighboring countries and in the Italian diaspora worldwide. Italian, adopted by the central state after the unification of Italy, is a language based on the Florentine variety of Tuscan and is somewhat intermediate between the Italo-Dalmatian languages and the Gallo-Romance languages. Its development was also influenced by the Germanic languages of the post-Roman invaders. When Italy unified in 1861, only 3% of the population spoke Italian, even though an estimated 90% of Italians speak Italian as their L1 nowadays.

Italy is in fact one of the most linguistically diverse countries in Europe, as there are not only varieties of Italian specific to each cultural region, but also distinct regional and minority languages. The establishment of the national education system has led to the emergence of the former and a decrease in the use of the latter. The spread of Italian was further expanded in the 1950s and 1960s, because of the economic growth and the rise of mass media and television, with the state broadcaster (RAI) setting a colloquial variety of Italian to which the population would be exposed.

As a way to distance itself from the Italianization policies promoted because of nationalism, Italy recognized twelve languages as the Country's "historical linguistic minorities", which are promoted alongside Italian in their respective territories. French is co-official in the Aosta Valley as the province's prestige variety, under which the more commonly spoken Franco-Provencal dialects have been historically roofed. German has the same status in the province of South Tyrol as, in some parts of that province and in parts of the neighbouring Trentino, does Ladin. Slovene and Friulian are officially recognised in the provinces of Trieste, Gorizia and Udine in Venezia Giulia. In Sardinia, the Sardinian language has been the language traditionally spoken and is often regarded by linguists as constituting its own branch of Romance; in the 1990s, Sardinian has been recognized as "having equal dignity" with Italian, the introduction of which to the island officially started under the rule of the House of Savoy in the 18th century.

In these regions, official documents are either bilingual (trilingual in Ladin communities) in the co-official language(s) by default, or available as such upon request. Traffic signs are also multilingual, except in the Valle d’Aosta where French toponyms are generally used, with the exception of Aosta itself, which has retained its Latin form in Italian as well as English. Attempts to Italianize them, especially during the Fascist period, have been formally abandoned. Education is possible in minority languages where such schools are operating.

UNESCO and other authories recognize a number of other languages which are not legally protected by Italian government: Piedmontese, Venetian, Ligurian, Lombard, Emilian-Romagnolo, Neapolitan and Sicilian.

Roman Catholicism is by far the largest religion in the country, although the Catholic Church is no longer officially the state religion. In 2006, 87.8% of Italy's population self-identified as Roman Catholic, although only about one-third of these described themselves as active members (36.8%). In 2016, 71.1% of "italian citizens" self-identified as Roman Catholic.

Most Italians believe in God, or a form of a spiritual life force. According to the most recent Eurobarometer Poll 2005: 74% of Italian citizens responded that 'they believe there is a God', 16% answered that 'they believe there is some sort of spirit or life force' and 6% answered that 'they do not believe there is any sort of spirit, God, or life force'. 
There are no data collected through census.

The Italian Catholic Church is part of the global Roman Catholic Church, under the leadership of the Pope, curia in Rome, and the Conference of Italian Bishops. In addition to Italy, two other sovereign nations are included in Italian-based dioceses, San Marino and Vatican City. There are 225 dioceses in the Italian Catholic Church, see further in this article and in the article List of the Roman Catholic dioceses in Italy. Even though by law Vatican City is not part of Italy, it is in Rome, and along with Latin, Italian is the most spoken and second language of the Roman Curia.

Italy has a rich Catholic culture, especially as numerous Catholic saints, martyrs and popes were Italian themselves. Roman Catholic art in Italy especially flourished during the Middle Ages, Renaissance and Baroque periods, with numerous Italian artists, such as Michelangelo, Leonardo da Vinci, Raphael, Caravaggio, Fra Angelico, Gian Lorenzo Bernini, Sandro Botticelli, Tintoretto, Titian, Raphael and Giotto. Roman Catholic architecture in Italy is equally as rich and impressive, with churches, basilicas and cathedrals such as St Peter's Basilica, Florence Cathedral and St Mark's Basilica. Roman Catholicism is the largest religion and denomination in Italy, with around 71.1% of Italians considering themselves Catholic. Italy is also home to the greatest number of cardinals in the world, and is the country with the greatest number of Roman Catholic churches per capita.

Even though the main Christian denomination in Italy is Roman Catholicism, there are some minorities of Protestant, Waldensian, Eastern Orthodox and other Christian churches.

In the 20th century, Jehovah's Witnesses, Pentecostalism, non-denominational Evangelicalism, and Mormonism were the fastest-growing Protestant churches. Immigration from Western, Central, and Eastern Africa at the beginning of the 21st century has increased the size of Baptist, Anglican, Pentecostal and Evangelical communities in Italy, while immigration from Eastern Europe has produced large Eastern Orthodox communities.

In 2006, Protestants made up 2.1% of Italy's population, and members of Eastern Orthodox churches comprised 1.2% or more than 700,000 Eastern Orthodox Christians including 180,000 Greek Orthodox, 550,000 Pentecostals and Evangelists (0.8%), of whom 400,000 are members of the Assemblies of God, about 250,000 are Jehovah's Witnesses (0.4%), 30,000 Waldensians, 25,000 Seventh-day Adventists, 22,000 Mormons, 15,000 Baptists (plus some 5,000 Free Baptists), 7,000 Lutherans, 4,000 Methodists (affiliated with the Waldensian Church).

The longest-established religious faith in Italy is Judaism, Jews having been present in Ancient Rome before the birth of Christ. Italy has seen many influential Italian-Jews, such as Luigi Luzzatti, who took office in 1910, Ernesto Nathan served as mayor of Rome from 1907 to 1913 and Shabbethai Donnolo (died 982). During the Holocaust, Italy took in many Jewish refugees from Nazi Germany. However, with the creation of the Nazi-backed puppet Italian Social Republic, about 15% of 48,000 Italian Jews were killed. This, together with the emigration that preceded and followed the Second World War, has left only a small community of around 45,000 Jews in Italy today.

Due to immigration from around the world, there has been an increase in non-Christian religions. As of 2009, there were 1.0 million Muslims in Italy forming 1.6 percent of population; independent estimates put the Islamic population in Italy anywhere from 0.8 million to 1.5 million. Only 50,000 Italian Muslims hold Italian citizenship.

There are more than 200,000 followers of faith originating in the Indian subcontinent, including some 70,000 Sikhs with 22 gurdwaras across the country, 70,000 Hindus, and 50,000 Buddhists. There are an estimated some 4,900 Bahá'ís in Italy in 2005.




</doc>
<doc id="14701" url="https://en.wikipedia.org/wiki?curid=14701" title="Politics of Italy">
Politics of Italy

The politics of Italy are conducted through a parliamentary republic with a multi-party system. Italy has been a democratic republic since 2 June 1946, when the monarchy was abolished by popular referendum and a constituent assembly was elected to draft a constitution, which was promulgated on 1 January 1948.

Executive power is exercised by the Council of Ministers, which is led by the Prime Minister, officially referred to as "President of the Council" ("Presidente del Consiglio"). Legislative power is vested primarily in the two houses of Parliament and secondarily in the Council of Ministers, which can introduce bills and holds the majority in both houses. The judiciary is independent of the executive and the legislative branches. It is headed by the High Council of the Judiciary, a body presided over by the President, who is the head of state, though this position is separate from all branches. The current president is Sergio Mattarella, and the current prime minister is Giuseppe Conte.

The Economist Intelligence Unit rated Italy as a "flawed democracy" in 2017. A high degree of fragmentation and instability, leading to often short-lived coalition governments, is characteristic of Italian politics. Since the end of World War II, Italy has had 61 governments.

Article 1 of the Italian Constitution states: "Italy is a democratic Republic, founded on labour. Sovereignty belongs to the people and is exercised by the people in the forms and within the limits of the Constitution".

By stating that Italy is a democratic republic, the article solemnly declares the results of the constitutional referendum which took place on 2 June 1946. The State is not a hereditary property of the ruling monarch, but it is instead a "Res Publica", belonging to everyone.

The people who are called to temporarily administer the republic are not owners, but servants; and the governed are not subjects, but citizens. And the sovereignty, that is the power to make choices that involve the entire community, belongs to the people, in accordance with the concept of a democracy, from the Greek "demos" (people) and "kratìa" (power). However, this power is not to be exercised arbitrarily, but in the forms and within the limits established by the rule of law.

As the head of state, the president of the Republic represents the unity of the nation and has many of the duties previously given to the King of Italy. The president serves as a point of connection between the three branches as he is elected by the lawmakers, appoints the executive and is the president of the judiciary. The president is also commander-in-chief in the time of war.

The president of the Republic is elected for seven years by Parliament in joint session.

With article 48 of the Constitution, which guarantees the right to vote, the people exercise their power through their elected representatives in the Parliament. The Parliament has a bicameral system, and consists of the Chamber of deputies and the Senate, elected every five years.

The Constitution establishes the Government of Italy as composed of the president of the council (prime minister) and ministers. The President of Italy appoints the prime minister and, on his proposal, the ministers that form its cabinet.

The Constitution states that justice is administered in the name of the people and that judges are subject only to the law. So the judiciary is a branch that is completely autonomous and independent of all other branches of power, even though the Minister of Justice is responsible for the organization and functioning of those services involved with justice and has the power to originate disciplinary actions against judges, which are then administered by the High Council of the Judiciary, presided over by the President.

The Italian judicial system is based on Roman law, the Napoleonic code and later statutes. It is based on a mix of the adversarial and inquisitorial civil law systems, although the adversarial system was adopted in the Appeal Courts in 1988. Appeals are treated almost as new trials, and three degrees of trial are present. The third is a legitimating trial.

In November 2014, Italy accepted the compulsory jurisdiction of the International Court of Justice.

All Italian citizens older than 18 can vote, but to vote for the Senate the voter must be 25 or older.

Italy's dramatic self-renewal transformed the political landscape between 1992 and 1997. Scandal investigations touched thousands of politicians, administrators and businessmen; the shift from a proportional to an Additional Member System (with the requirement to obtain a minimum of 4% of the national vote to obtain representation) also altered the political landscape. Party changes were sweeping. The Christian Democratic party dissolved; the Italian People's Party and the Christian Democratic Center emerged. Other major parties, such as the Socialists, saw support plummet. A new liberal movement, Forza Italia, gained wide support among moderate voters. The National Alliance broke from the (alleged neo-fascist) Italian Social Movement (MSI). A trend toward two large coalitions (one on the center-left and the other on the center-right) emerged from the April 1995 regional elections. For the 1996 national elections, the center-left parties created the Olive Tree coalition while the center-right united again under the House of Freedoms. These coalitions continued into the 2001 and 2007 national elections.

This emerging bipolarity represents a major break from the fragmented, multi-party political landscape of the postwar era, although it appears to have reached a plateau since efforts via referendums to further curtail the influence of small parties were defeated in 1999, 2000 and 2009.

Five regions (Aosta Valley, Friuli-Venezia Giulia, Sardinia, Sicily and Trentino-Alto Adige/Südtirol) have special charters granting them varying degrees of autonomy. The "raisons d'être" of these charters is in most cases the presence of significant linguistic and cultural minorities, but in the case of Sicily it was to calm down separatist movements. The other 15 regions were in practice established in 1970, even if their ideation had been a much earlier idea.

There have been frequent government turnovers since 1945, indeed there have been 61 governments in this time. The dominance of the Christian Democratic party during much of the postwar period lent continuity and comparative stability to Italy's political situation, mainly dominated by the attempt of keeping the Italian Communist Party (PCI) out of power in order to maintain Cold War equilibrium in the region (see May 1947 crisis).

The communists were in the government only in the national unity governments before 1948, in which their party's secretary Palmiro Togliatti was minister of Justice. After the first democratic elections with universal suffrage in 1948 in which the Christian Democracy and their allies won against the popular front of the Italian Communist and Socialists parties, the Communist Party never returned in the government.

The system had been nicknamed the "imperfect bipolarism", referring to more proper bipolarism in other western countries (the United States, Germany, the United Kingdom, France and the like) where right-wing and left-wing parties alternated in government.

The main event in the First Republic in the 1960s was the inclusion of the Socialist party in the government after the reducing edge of the Christian Democracy (DC) had forced them to accept this alliance; attempts to incorporate the Italian Social Movement (MSI), a right party, in the Tambroni government led to riots and were short-lived.

Aldo Moro, a relatively left-leaning Christian Democrat, inspired this alliance. He would later try to include the Communist Party as well with a deal called the "historic compromise". However, this attempt at compromise was stopped by the kidnapping and murder of Moro in 1978 by the Red Brigades, an extremist left-wing terrorist organization.

The Communist Party was at this point the largest communist party in Western Europe and remained such for the rest of its existence. Their ability to attract members was largely due to their pragmatic stance, especially their rejection of extremism and to their growing independence from Moscow (see Eurocommunism). The Italian communist party was especially strong in areas like Emilia-Romagna and Tuscany, where communists had been elected to stable government positions. This practical political experience may have contributed to their taking a more pragmatic approach to politics.

On 12 December 1969, a roughly decade-long period of extremist left- and right-wing political terrorism, known as The Years of Lead (as in the metal of bullets, ), began with the Piazza Fontana bombing in the center of Milan. Neofascist Vincenzo Vinciguerra later declared the bombing to be an attempt to push the Italian state to declare a state of emergency in order to lead to a more authoritative state. A bomb left in a bank killed about twenty and was initially blamed on anarchist Giuseppe Pinelli. This accusation was hotly contested by left-wing circles, especially the Maoist Student Movement, which had support in those years from some students of Milan's universities and who considered the bombing to have all the marks of a fascist operation. Their guess proved correct, but only after many years of difficult investigations.

The strategy of tension attempted to blame the left for bombings carried out by right-wing terrorists. Fascist "black terrorists", such as "Ordine Nuovo" and the "Avanguardia Nazionale", were in the 1980s and 1990s found to be responsible for several terrorist attacks. On the other extreme of the political spectrum, the leftist Red Brigades carried out assassinations against specific persons, but were not responsible for any blind bombings. The Red Brigades killed socialist journalist Walter Tobagi and in their most famous operation kidnapped and assassinated Aldo Moro, president of the Christian Democracy, who was trying to involve the Communist Party in the government through the "compromesso storico" ("historic compromise"), to which the radical left as well as Washington were opposed.

The last and largest of the bombings, known as the Bologna massacre, destroyed the city's railway station in 1980. This was found to be a neofascist bombing, in which Propaganda Due was involved. On 24 October 1990, Prime Minister Giulio Andreotti (DC) revealed to the Parliament the existence of Gladio, NATO's secret "stay-behind" networks which stocked weapons in order to facilitate an armed resistance in case of a communist coup. In 2000, a Parliament Commission report from the Olive Tree (centre-left) coalition concluded that the strategy of tension followed by Gladio had been supported by the United States to "stop the PCI and, to a certain degree, the PSI [Italian Socialist Party] from reaching executive power in the country".

With the end of the lead years, the Communist Party gradually increased their votes under the leadership of Enrico Berlinguer. The Italian Socialist Party, led by Bettino Craxi, became more and more critical of the communists and of the Soviet Union; Craxi himself pushed in favor of Ronald Reagan's positioning of Pershing II missiles in Italy, a move many communists strongly disapproved of.

As the Socialist Party moved to more moderate positions, it attracted many reformists, some of whom were irritated by the failure of the communists to modernize. Increasingly, many on the left began to see the communists as old and out of fashion while Craxi and the socialists seemed to represent a new liberal socialism. The Communist Party surpassed the Christian Democrats only in the European elections of 1984, held barely two days after Berlinguer's death, a passing that likely drew sympathy from many voters. The election of 1984 was to be the only time the Christian Democrats did not emerge as the largest party in a nationwide election in which they participated.

In 1987, one year after the Chernobyl disaster following a referendum in that year, a nuclear phase-out was commenced. Italy's four nuclear power plants were closed down, the last in 1990. A moratorium on the construction of new plants, originally in effect from 1987 until 1993, has since been extended indefinitely.

In these years, corruption began to be more extensive, a development that would be exposed in the early 1990s and nicknamed "Tangentopoli". With the "mani pulite" investigation, starting just one year after the collapse of the Soviet Union, the whole power structure faltered and seemingly indestructible parties, such as the Christian Democrats and the Socialist Party, disbanded whereas the Communist Party changed its name to the Democratic Party of the Left and took the role of the Socialist Party as the main social democratic party in Italy. What was to follow was then called the transition to the Second Republic.

From 1992 to 1997, Italy faced significant challenges as voters, disenchanted with past political paralysis, massive government debt, extensive corruption and organized crime's considerable influence, collectively called "Tangentopoli" after being uncovered by "mani pulite", demanded political, economic and ethical reforms.

In the Italian referendums of 1993, voters approved substantial changes, including moving from a proportional to an Additional Member System, which is largely dominated by a majoritarian electoral system and the abolition of some ministries, some of which have been reintroduced with only partly modified names, such as the Ministry of Agriculture reincarnated as the Ministry of Agricultural Resources.

Major political parties, beset by scandal and loss of voter confidence, underwent far-reaching changes. New political forces and new alignments of power emerged in the March 1994 national elections. This election saw a major turnover in the new parliament, with 452 out of 630 deputies and 213 out of 315 senators elected for the first time.

The 1994 elections also swept media magnate Silvio Berlusconi (leader of Pole of Freedoms coalition) into office as prime minister. However, Berlusconi was forced to step down in December 1994 when the Lega Nord withdrew support. The Berlusconi government was succeeded by a technical government headed by Prime Minister Lamberto Dini, which left office in early 1996.

A series of center-left coalitions dominated Italy's political landscape between 1996 and 2001. In April 1996, national elections led to the victory of a center-left coalition, The Olive Tree, under the leadership of Romano Prodi. Prodi's government became the third-longest to stay in power before he narrowly lost a vote of confidence, by three votes, in October 1998.

In May 1999, the Parliament selected Carlo Azeglio Ciampi as the President of the Republic. Ciampi, a former prime minister and Minister of the Treasury and before entering the government also the governor of the Bank of Italy, was elected on the first ballot by a comfortable margin over the required two-thirds of the votes.

A new government was formed by the Democrats of the Left leader and former communist Massimo D'Alema, but in April 2000 he resigned following poor performance by his coalition in regional elections.

The succeeding center-left government, including most of the same parties, was headed by Giuliano Amato, a social democrat, who had previously served as prime minister in 1992–1993 and had at the time sworn never to return to active politics.

National elections held on 13 May 2001 returned Berlusconi to power at the head of the five-party center-right House of Freedoms coalition, comprising the Prime Minister's own party, Forza Italia, the National Alliance, the North League, the Christian Democratic Center and the United Christian Democrats.

Between 17 May 2006 and 21 February 2007, Romano Prodi served as prime minister of Italy following the narrow victory of his The Union coalition over the House of Freedoms led by Silvio Berlusconi in the April 2006 Italian elections. Following a government crisis, Prodi submitted his resignation on 21 February 2007. Three days later, he was asked by President Giorgio Napolitano to stay on as prime minister and he agreed to do so. On 28 February 2007, Prodi narrowly survived a senate no confidence vote.

On 24 January 2008, the Prodi II Cabinet went through a new crisis because Minister of Justice Clemente Mastella retracted his support to the Cabinet. Consequently, the Prodi Cabinet lost the vote of confidence and the President Giorgio Napolitano called a new general election.

The election set against two new parties, the Democratic Party (founded in October 2007 by the union of the Democrats of the Left and Democracy is Freedom – The Daisy) led by Walter Veltroni: and The People of Freedom (federation of Forza Italia, National Alliance and other parties) led by Silvio Berlusconi. The Democratic Party was in alliance with Italy of Values while The People of Freedom forged an alliance with Lega Nord and the Movement for Autonomy. The coalition led by Berlusconi won the election and the leader of the centre-right created the Berlusconi IV Cabinet.

The Monti government had the highest average age in the western world (64 years), with its youngest members being 57. The previous Italian Prime Minister Mario Monti is 70, his predecessor Silvio Berlusconi was 75 at the time of resignation (2011), the previous head of the government Romano Prodi was 70 when he stepped down (2008), the Italian President Giorgio Napolitano is 88 and his predecessor Carlo Azeglio Ciampi was 86. In 2013, the youngest among the candidates for prime minister (Pier Luigi Bersani) is 62, the others being 70 and 78. The current average age of Italian university professors is 63, of bank directors and CEOs 67, of members of parliament 56 and of labor union representatives 59.

The new Italian government headed by Enrico Letta took two months to form and made international news when Luigi Preiti shot at policemen near the building where they were swearing in the new government on Sunday 28 April 2013.

Former Prime Minister Matteo Renzi became the youngest prime minister at 39 years and his government had the youngest average age in Europe.

At different times since his entering the Italian Parliament, Silvio Berlusconi, leader of the centre-right, had repeatedly vowed to stop the "communists", while leftist parties had insisted that they would oust Berlusconi. Thus, despite the fact that the executive branch bears responsibility toward the Parliament, the governments led by Mario Monti (since 2011) and by Enrico Letta (since 2013) were called "unelected governments" because they won a vote of confidence by a Parliament coalition formed by centre-right and left-right parties that had in turn obtained parliamentary seats by taking part in the elections as competitors, rather than allies. While formally complying with law and procedures, the creation of these governments did not comply with the decision made by people through the election.

Meanwhile, in 2013, a ruling by the Constitutional Court of Italy established that the Italian electoral system employed to elect the Parliament breached a number of Constitutional requirements. Notably, the Court observed the following four facts: 1) "such a legislation deprives the elector of any margin of choice of its representatives"; 2) "all of the elected parliamentarians, with no exception, lack the support of a personal designation by the citizens"; 3) the electoral law has regulations which "exclude any ability on the part of the elector to have an influence on the election of his/her representatives"; 4) and contains conditions such that "they alter the representative relationship between electors and elected people...they coerce the electors' freedom of choice in the election of their representatives to the Parliament...and consequently they are at odds with the democratic principle, by affecting the very freedom of vote provided for by art. 48 of the Constitution". This implies that, despite being called - and acting as – a legitimate "parliament", the legislative assembly of Italy was chosen with a vote system by which the right of vote was not exercised according to the Italian fundamental chart of citizen's rights and duties. The issue was a major one, to the extent that the Constitutional Court itself ruled that the Italian Parliament should remain in charge only to reform the electoral system and then should be dissolved.

The new government led by Matteo Renzi proposed a new electoral law. The so-called Italicum was approved in 2015 and came into force on 1 July 2016.




</doc>
<doc id="14702" url="https://en.wikipedia.org/wiki?curid=14702" title="Economy of Italy">
Economy of Italy

The economy of Italy is the third-largest national economy in the European Union, the eighth-largest by nominal GDP in the world, and the 12th-largest by GDP (PPP). Italy is a founding member of the European Union, the Eurozone, the OECD, the G7 and the G20; it is the eighth-largest exporter in the world, with $514 billion exported in 2016. Its closest trade ties are with the other countries of the European Union, with whom it conducts about 59% of its total trade. The largest trading partners, in order of market share, are Germany (12.6%), France (11.1%), the United States (6.8%), Switzerland (5.7%), the United Kingdom (4.7%) and Spain (4.4%).

In the post-World War II period, Italy was transformed from an agricultural based economy which had been severely affected by the consequences of the World Wars, into one of the world's most advanced nations, and a leading country in world trade and exports. According to the Human Development Index, the country enjoys a very high standard of living, and has the world's 8th highest quality of life according to "The Economist". Italy owns the world's third-largest gold reserve, and is the third-largest net contributor to the budget of the European Union. Furthermore, the advanced country private wealth is one of the largest in the world.

Italy is a large manufacturer (overall the second in EU behind Germany) and exporter of a significant variety of products including machinery, vehicles, pharmaceuticals, furniture, food, clothing, and robots. Italy has therefore a significant trade surplus. The country is also well known for its influential and innovative business economic sector, an industrious and competitive agricultural sector (Italy is the world's largest wine producer), and manufacturers of creatively designed, high-quality products including automobiles, ships, home appliances, and designer clothing. Italy is the largest hub for luxury goods in Europe and the third luxury hub globally.

Despite these important achievements, the country's economy today suffers from structural and non-structural problems. Annual growth rates have often been below the EU average with Italy being hit particularly hard by the late-2000s recession. Massive government spending from the 1980s onwards has produced a severe rise in public debt. In addition, Italian living standards have a considerable North–South divide: the average GDP per capita in Northern and Central Italy significantly exceeds the EU average, while some regions and provinces in Southern Italy are dramatically below. In recent years, Italy's GDP per capita growth slowly caught-up with the Eurozone average while its employment rate still lags behind; however, economists dispute the official figures because of the large number of informal jobs (estimated between 10% and 20% of the labour force) that lift the inactivity or unemployment rates.

The economic history of Italy can be divided in three main phases: an initial period of struggle after the unification of the country, characterised by high emigration and stagnant growth; a central period of robust catch-up from the 1890s to the 1980s, interrupted by the Great Depression of the 1930s and the two world wars; and a final period of sluggish growth that has been exacerbated by a double-dip recession following the 2008 global financial crush, and from which the country is slowly reemerging only in recent years.

Prior to unification, the economy of the many Italian statelets was overwhelmingly agrarian; however, the agricultural surplus produced what historians call a "pre-industrial" transformation in North-western Italy starting from the 1820s, that led to a diffuse, if mostly artisanal, concentration of manufacturing activities, especially in Piedmont-Sardinia under the liberal rule of the Count of Cavour.

After the birth of the unified Kingdom of Italy in 1861, there was a deep consciousness in the ruling class of the new country's backwardness, given that the per capita GDP expressed in PPS terms was roughly half of that of Britain and about 25% less than that of France and Germany. During the 1860s and 1870s, the manufacturing activity was backward and small-scale, while the oversized agrarian sector was the backbone of the national economy. The country lacked large coal and iron deposits and the population was largely illiterate. In the 1880s, a severe farm crisis led to the introduction of more modern farming techniques in the Po valley, while from 1878 to 1887 protectionist policies were introduced with the aim to establish a heavy industry base. Some large steel and iron works soon clustered around areas of high hydropower potential, notably the Alpine foothills and Umbria in central Italy, while Turin and Milan led a textile, chemical, engineering and banking boom and Genoa captured civil and military shipbuilding.

However, the diffusion of industrialisation that characterised the northwestern area of the country largely excluded Venetia and, especially, the South. The resulting Italian diaspora concerned up to 26 million Italians, the most part in the years between 1880 and 1914; by many scholars it is considered the biggest mass migration of contemporary times. During the Great War, the still frail Italian state successfully fought a modern war, being able of arming and training some 5 million recruits. But this result came at a terrible cost: by the end of the war, Italy had lost 700,000 soldiers and had a ballooning sovereign debt amounting to billions of lira.

Italy emerged from World War I in a poor and weakened condition. The National Fascist Party of Benito Mussolini came to power in 1922, at the end of a period of social unrest. However, once Mussolini acquired a firmer hold of power, laissez-faire and free trade were progressively abandoned in favour of government intervention and protectionism.

In 1929, Italy was hit hard by the Great Depression. Trying to handle the crisis, the Fascist government nationalized the holdings of large banks which had accrued significant industrial securities, establishing the Istituto per la Ricostruzione Industriale. A number of mixed entities were formed, whose purpose was to bring together representatives of the government and of the major businesses. These representatives discussed economic policy and manipulated prices and wages so as to satisfy both the wishes of the government and the wishes of business.

This economic model based on a partnership between government and business was soon extended to the political sphere, in what came to be known as corporatism. At the same time, the aggressive foreign policy of Mussolini led to an increasing military expenditure. After the invasion of Ethiopia, Italy intervened to support Franco's nationalists in the Spanish Civil War. By 1939, Italy had the highest percentage of state-owned enterprises after the Soviet Union.

Italy's involvement in World War II as a member of the Axis powers required the establishment of a war economy. The Allied invasion of Italy in 1943 eventually caused the Italian political structure – and the economy – to rapidly collapse. The Allies, on the one hand, and the Germans on the other, took over the administration of the areas of Italy under their control. By the end of the war, Italian per capita income was at its lowest point since the beginning of the 20th century.

After the end of World War II, Italy was in rubble and occupied by foreign armies, a condition that worsened the chronic development gap towards the more advanced European economies. However, the new geopolitical logic of the Cold War made possible that the former enemy Italy, a hinge-country between Western Europe and the Mediterranean, and now a new, fragile democracy threatened by the NATO occupation forces, the proximity of the Iron Curtain and the presence of a strong Communist party, was considered by the United States as an important ally for the Free World, and received under the Marshall Plan over US$1.2 billion from 1947-51.

The end of aid through the Plan could have stopped the recovery but it coincided with a crucial point in the Korean War whose demand for metal and manufactured products was a further stimulus of Italian industrial production. In addition, the creation in 1957 of the European Common Market, with Italy as a founding member, provided more investment and eased exports.

These favorable developments, combined with the presence of a large labour force, laid the foundation for spectacular economic growth that lasted almost uninterrupted until the "Hot Autumn's" massive strikes and social unrest of 1969–70, which then combined with the later 1973 oil crisis and put an abrupt end to the prolonged boom. It has been calculated that the Italian economy experienced an average rate of growth of GDP of 5.8% per year between 1951 and 1963, and 5% per year between 1964 and 1973. Italian rates of growth were second only, but very close, to the German rates, in Europe, and among the OEEC countries only Japan had been doing better.

The 1970s were a period of economic, political turmoil and social unrest in Italy, known as Years of lead. Unemployment rose sharply, especially among the young, and by 1977 there were one million unemployed people under age 24. Inflation continued, aggravated by the increases in the price of oil in 1973 and 1979. The budget deficit became permanent and intractable, averaging about 10 percent of the gross domestic product (GDP), higher than any other industrial country. The lira fell steadily, from 560 lira to the U.S. dollar in 1973 to 1,400 lira in 1982.

The economic recession went on into the mid-1980s until a set of reforms led to the independence of the Bank of Italy and a big reduction of the indexation of wages that strongly reduced inflation rates, from 20.6% in 1980 to 4.7% in 1987. The new macroeconomic and political stability resulted in a second, export-led "economic miracle", based on small and medium-sized enterprises, producing clothing, leather products, shoes, furniture, textiles, jewelry, and machine tools. As a result of this rapid expansion, in 1987 Italy overtook the UK's economy (an event known as "il sorpasso"), becoming the fourth richest nation in the world, after the US, Japan and West Germany. The Milan stock exchange increased its market capitalization more than fivefold in the space of a few years.

However, the Italian economy of the 1980s presented a problem: it was booming, thanks to increased productivity and surging exports, but unsustainable fiscal deficits drove the growth. In the 1990s, the new Maastricht criteria boosted the urge to curb the public debt, already at 104% of GDP in 1992. The consequent restrictive economic policies worsened the impact of the global recession already underway. After a brief recover at the end of the 1990s, high tax rates and red tape caused the country to stagnate between 2000 and 2008.

Italy was among the countries hit hardest by the Great Recession of 2008–2009 and the subsequent European debt crisis. The national economy shrunk by 6.76% during the whole period, totaling seven-quarters of recession. In November 2011 the Italian bond yield was 6.74 percent for 10-year bonds, nearing a 7 percent level where Italy is thought to lose access to financial markets. According to Eurostat, in 2015 the Italian government debt stood at 128% of GDP, ranking as the second biggest debt ratio after Greece (with 175%). However, the biggest chunk of Italian public debt is owned by Italian nationals and relatively high levels of private savings and low levels of private indebtedness are seen as making it the safest among Europe's struggling economies. As a shock therapy to avoid the debt crisis and kick-start growth, the national unity government led by the economist Mario Monti launched a program of massive austerity measures, that brought down the deficit but precipitated the country in a double-dip recession in 2012 and 2013, receiving criticism from numerous economists.

From 2014 to 2019 the economy had almost fully recovered from the Great Recession of 2008. Despite not having growth rates like the rest of the countries in the Euro area.

First among the countries of Europe to be affected by the COVID-19 pandemic, which in the months since February 2020 will expand to the rest of the world.
The economy suffers a very severe shock as a result of the lockdown of most of the country's economic activity. After three months at the end of May 2020, the epidemic is under control, and the economy is starting up again, especially the manufacturing sector. The economy remains resilient although far below the values prior to the COVID 19 pandemic.
The Italian government has issued special BTP Futura to compensate for the rising costs of health care costs to deal with the COVID-19 pandemic in Italy, waiting for Europe to proceed with a unitary support through the European Recovery Fund.

The following table shows the main economic indicators in 1980–2019. Inflation under 2% is in green.

Of the world's 500 largest stock-market-listed companies measured by revenue in 2016, the Fortune Global 500, nine are headquartered in Italy. The country's major companies by sector are: Fiat Chrysler Automobiles, CNH Industrial, Ducati, Piaggio (motor vehicles); Pirelli (tyre manufacturing); Enel, Edison, A2A, Terna (energy); Eni (petrochemicals); Candy, Indesit, De'Longhi (home appliances); Leonardo that has absorbed its subsidiary companies Alenia Aermacchi, AgustaWestland and Oto Melara (defence); Avio, Telespazio (space); Beretta, Benelli (firearms); Armani, Versace, Dolce & Gabbana, Gucci, Benetton, Diesel, Prada, Luxottica, YOOX (fashion); Ferrero, Barilla, Autogrill, Lavazza, Perfetti Van Melle, Campari, Parmalat (food&beverages); Techint, Lucchini, Gruppo Riva, Danieli (steel); Prysmian, Salini Impregilo, Italcementi, Buzzi Unicem, Astaldi (construction); STMicroelectronics (electronics); Telecom Italia, Mediaset (communications); Assicurazioni Generali, Unipol (insurance); UniCredit, Intesa Sanpaolo (banking); Ferrari, Maserati, Lamborghini (luxury vehicles); Fincantieri, Ferretti, Azimut (shipbuilding).

Figures are for 2016. Figures in italic = Q3 2017

Italy has over 1.4 million people with a net wealth greater than $1 million, a total national wealth of $11.857 trillion, and represents the 5th largest cumulative net wealth globally (it accounts for 4.92% of the net wealth in the world). According to the Credit Suisse's Global Wealth Databook 2013, the median wealth per adult is $138,653 (5th in the world), while according to the Allianz's Global Wealth Report 2013, the net financial wealth per capita is €45,770 (13th in the world).

The following top 10 list of Italian billionaires is based on an annual assessment of wealth and assets compiled and published by Forbes in 2017.

Since the unification of Italy in 1861, a wide and increasing economic divide has been growing between the northern provinces and the southern half of the Italian state. This gap was mainly induced by the region-specific policies selected by the Piedmontese elite, who dominated the first post-unitary governments, and that more heavily penalized the regions farther away from the rulers’ fiercer enemies, as recently confirmed by Guilherme de Oliveira and Carmine Guerriero. To illustrate, the 1887 protectionist reform, instead of safeguarding the arboriculture sectors crushed by 1880s fall in prices, shielded the Po Valley wheat breeding and those Northern textile and manufacturing industries that had survived the liberal years thanks to state intervention. While indeed the former dominated the allocation of military clothing contracts, the latter monopolized both coal mining permits and public contracts. A similar logic guided the assignment of monopoly rights in the steamboat construction and navigation sectors and, above all, the public spending in the railway sector, which represented 53% of the 1861-1911 total. To make things worse, the resources necessary to finance this public spending effort were obtained through highly unbalanced land property taxes, which affected the key source of savings available for investment in the growth sectors absent a developed banking system. To elaborate, the 1864 reform fixed a 125 million target revenue to be raised from 9 districts resembling the pre-unitary states. Given the inability of the government to estimate the land profitability, especially because of the huge differences among the regional cadasters, this policy irreparably induced large regional discrepancies. To illustrate, the ex-Papal State (central Italy) took on the 10%, the ex-Kingdom of Two Sicilies (Southern Italy) the 40%, and the rest of the state (ex-Kingdom of Sardinia, Northern Italy) the 21%. To weigh this burden down, a 20% surcharge was added by 1868.

The 1886 cadastral reform opened the way to more egalitarian policies and, after the First World War, to the harmonization of the tax-rates, but the impact of extraction on the economies of the two blocks was at that point irreversible. While indeed a flourishing manufacturing sector was established in the North, the mix of low public spending and heavy taxation squeezed the Southern investment to the point that the local industry and export-oriented farming were wiped out. Moreover, extraction destroyed the relationship between the central state and the Southern population by unchaining first a civil war called Brigandage, which brought about 20,000 victims by 1864 and the militarization of the area, and then favoring emigration, especially from 1892 to 1921. To elaborate, extractive policies induced a dramatic fall in the accumulation of both social and human capital in the Southern regions and favored the rise of organized crime.

After the rise of Benito Mussolini, the "Iron Prefect" Cesare Mori tried to defeat the already powerful criminal organizations flourishing in the South with some degree of success. Fascist policy aimed at the creation of an Italian empire and Southern Italian ports were strategic for all commerce towards the colonies. With the invasion of Southern Italy, the Allies restored the authority of the mafia families, lost during the Fascist period, and used their influence to maintain public order.

In the 1950s the Cassa per il Mezzogiorno was set up as a huge public master plan to help industrialize the South, aiming to do this in two ways: through land reforms creating 120,000 new smallholdings, and through the "Growth Pole Strategy" whereby 60% of all government investment would go to the South, thus boosting the Southern economy by attracting new capital, stimulating local firms, and providing employment. However, the objectives were largely missed, and as a result the South became increasingly subsidized and state dependent, incapable of generating private growth itself.

Even at present, huge regional disparities persist. Problems in Southern Italy still include widespread political corruption, pervasive organized crime, and very high unemployment rates. In 2007, it was estimated that about 80% of the businesses in the Sicilian cities of Catania and Palermo paid protection money; thanks to grassroots movement like Addiopizzo, the mafia racket is slowly but constantly losing its verve. The Italian Ministry of Interior reported that organized crime generated an estimated annual profit of €13 billion.

According to the last national agricultural census, there were 1.6 million farms in 2010 (−32.4% since 2000) covering 12.7 million hectares (63% of which are located in Southern Italy). The vast majority (99%) are family-operated and small, averaging only 8 hectares in size. Of the total surface area in agricultural use (forestry excluded), grain fields take up 31%, olive tree orchards 8.2%, vineyards 5.4%, citrus orchards 3.8%, sugar beets 1.7%, and horticulture 2.4%. The remainder is primarily dedicated to pastures (25.9%) and feed grains (11.6%). The northern part of Italy produces primarily Maize corn, rice, sugar beets, soybeans, meat, fruits and dairy products, while the South specializes in wheat and citrus fruits. Livestock includes 6 million head of cattle, 8.6 million head of swine, 6.8 million head of sheep, and 0.9 million head of goats. The total annual production of the fishing industry in Italy from capture and aquaculture, including crustaceans and molluscs, is around 480,000 tons.

Italy is the largest producer of wine in the world, and one of the leading producers of olive oil, fruits (apples, olives, grapes, oranges, lemons, pears, apricots, hazelnuts, peaches, cherries, plums, strawberries, and kiwifruits), and vegetables (especially artichokes and tomatoes). The most famous Italian wines are probably the Tuscan Chianti and the Piedmontese Barolo. Other famous wines are Barbaresco, Barbera d'Asti, Brunello di Montalcino, Frascati, Montepulciano d'Abruzzo, Morellino di Scansano, Amarone della Valpolicella DOCG and the sparkling wines Franciacorta and Prosecco. Quality goods in which Italy specialises, particularly the already mentioned wines and regional cheeses, are often protected under the quality assurance labels DOC/DOP. This geographical indication certificate, which is attributed by the European Union, is considered important to avoid confusion with low-quality mass-produced ersatz products.

Italy has a smaller number of global multinational corporations than other economies of comparable size, but there is a large number of small and medium-sized enterprises, many of them grouped in clusters, which are the backbone of the Italian industry. This has produced a manufacturing sector often focused on the export of niche market and luxury products, that on one side is less capable of competing on quantity, but on the other side is more capable of facing the competition from emerging economies based on lower labor costs, with higher quality products. The industrial districts are regionalized: in the Northwest there is a large modern group of industries, as in the so-called "Industrial Triangle" (Milan-Turin-Genoa), where there is an area of intense machinery, automotive, aerospace production and shipbuilding; in the Northeast and the Center, previously rural areas that experienced social and economic development around family-based firms, there are small enterprises of low technology but high craftsmanship, specialized in clothing, leather products, footwear, furniture, textiles, machine tools, spare parts, home appliances, and jewellery; finally, in the less-developed South, the two forms exist side by side.

The origins of modern banking can be traced to medieval and early Renaissance Italy, to the rich cities like Florence, Lucca, Siena, Venice and Genoa. The Bardi and Peruzzi families dominated banking in 14th-century Florence, establishing branches in many other parts of Europe. One of the most famous Italian banks was the Medici Bank, set up by Giovanni di Bicci de' Medici in 1397. The earliest known state deposit bank, the Bank of Saint George, was founded in 1407 in Genoa, while Banca Monte dei Paschi di Siena, founded in 1472, is the oldest surviving bank in the world. Today, among the financial services companies, UniCredit is one of the largest bank in Europe by capitalization and Assicurazioni Generali is second largest insurance group in the world by revenue after AXA.

The following is a list of the main Italian banks and insurance groups ranked by total assets and gross premiums written.

In the early 1970s Italy was a major producer of pyrites (from the Tuscan Maremma), asbestos (from the Balangero mines), fluorite (found in Sicily), and salt. At the same time, it was self-sufficient in aluminum (from Gargano), sulphur (from Sicily), lead, and zinc (from Sardinia). By the beginning of the 1990s, however, it had lost all its world-ranking positions and was no longer self-sufficient in those resources. There are no substantial deposits of iron, coal, or oil. Moderate natural gas reserves, mainly in the Po Valley and offshore Adriatic Sea, have been discovered in recent years and constitute the country's most important mineral resource. Italy is one of the world's leading producers of pumice, pozzolana, and feldspar. Another mineral resource for which Italy is well-known is marble, especially the world-famous white Carrara marble from the Massa and Carrara quarries in Tuscany. Most raw materials needed for manufacturing and more than 80% of the country's energy sources are imported (99.7% of the solid fuels demand, 92.5% of oil, 91.2% of natural gas and 13% of electricity). Due to its reliance on imports, Italians pay approximately 45% more than the EU average for electricity.

Italy has managed four nuclear reactors until the 1980s, but in 1987, after the Chernobyl disaster, a large majority of Italians passed a referendum opting for phasing out nuclear power in Italy. The government responded by closing existing nuclear power plants and stopping work on projects underway, continuing to work to the nuclear energy program abroad. The national power company Enel operates seven nuclear reactors in Spain (through Endesa) and four in Slovakia (through Slovenské elektrárne), and in 2005 made an agreement with Électricité de France for a nuclear reactor in France. With these agreements, Italy has managed to access nuclear power and direct involvement in design, construction, and operation of the plants without placing reactors on Italian territory.

In the last decade, Italy has become one of the world's largest producers of renewable energy, ranking as the second largest producer in the European Union after Germany and the ninth in the world. The country is also the world's fifth largest producer of energy from solar power. Renewable sources account for the 27.5% of all electricity produced in Italy, with hydro alone reaching 12.6%, followed by solar at 5.7%, wind at 4.1%, bioenergy at 3.5%, and geothermal at 1.6%. The rest of the national demand is covered by fossil fuels (38.2% natural gas, 13% coal, 8.4% oil) and by imports.

Italy was the first country in the world to build motorways, the so-called "autostrade", reserved for motor vehicles. The Milano-Laghi motorway, connecting Milan to Varese and now parts of the A8 and A9 motorways, was devised by Piero Puricelli, a civil engineer and entrepreneur. He received the first authorization to build a public-utility fast road in 1921, and completed the construction between 1924 and 1926. By the end of the 1930s, over 400 kilometers of multi- and dual-single-lane motorways were constructed throughout Italy, linking cities and rural towns. Today there are 668,721 km of serviceable roads in Italy, including 6,661 km of motorways (mostly toll roads, national and local roads), state-owned but privately operated mainly by Atlantia company.

The railway network is also extensive, especially in the north, totalizing 16,862 km of which 69% are electrified and on which 4,937 locomotives and railcars circulate. It is the 12th largest in the world, and is operated by state-owned Ferrovie dello Stato, while the rail tracks and infrastructure are managed by Rete Ferroviaria Italiana. While a number of private railroads exist and provide mostly commuter-type services, the national railway also provides sophisticated high-speed rail service that joins the major cities. The Florence–Rome high-speed railway was the first high-speed line opened in Europe when more than half of it opened in 1977. In 1991 the TAV was created for the planning and construction of high-speed rail lines along Italy's most important and saturated transport routes (Milan-Rome-Naples and Turin-Milan-Venice). High-speed trains include ETR-class trains, with the Frecciarossa 1000 reaching 400 km/h.

There are approximately 130 airports in Italy, of which 99 have paved runways (including the two hubs of Leonardo Da Vinci International in Rome and Malpensa International in Milan), and 43 major seaports including the Port of Genoa, the country's largest and the third busiest by cargo tonnage in the Mediterranean Sea. The national inland waterway network comprises 1,477 km of navigable rivers and channels. In 2007 Italy maintained a civilian air fleet of about 389,000 units and a merchant fleet of 581 ships.

In 2015, poverty in Italy hit the highest levels in the previous 10 years. The level of absolute poverty for a two-person family was €1050.95/month. The poverty line per capita changed by region from €552.39/month to €819.13/month.The numbers of those in absolute poverty rose nearly an entire percent in 2015, from 6.8% in 2014, to 7.6% in 2015. In the south of Italy the numbers are even higher, with 10% living in absolute poverty, up from 9 percent in 2014. The north is better off at 6.7%, but this is still an increase from 5.7% in 2014. The national statistics reporting agency, ISTAT, defines absolute poverty as those who can not buy goods and services which they need to survive. In 2015, the proportion of poor households in relative poverty also increased to 13.7 from 12.9 in 2014. ISTAT defines relative poverty as people whose disposable income is less than around half the national average. The unemployment rate in February 2016 remained at 11.7%, which has been the same for almost a year, but even having a job does not guarantee freedom from poverty. Those who have at least one family member employed still suffer from 6.1% to 11.7% poverty, the higher number being for those who have factory jobs. The numbers are even higher for the younger generations because their unemployment rate is over 40%. Also, children are hit hard. In 2014, 32% of those aged 0–17 are at risk of poverty or social exclusion, which is one child out of three. In the last ISTAT report, poverty is in decline.

 


</doc>
<doc id="14703" url="https://en.wikipedia.org/wiki?curid=14703" title="Telecommunications in Italy">
Telecommunications in Italy

Telephones - main lines in use:
20.031 million (2008)

Telephones - mobile cellular:
88.58 million (2008)

Telephone system:
modern, well-developed, fast; fully automated telephone, telex, and data services
<br>"domestic:"
high-capacity cable and microwave radio relay trunks
<br>"international:"
satellite earth stations - 3 Intelsat (with a total of 5 antennas - 3 for Atlantic Ocean and 2 for Indian Ocean), 1 Inmarsat (Atlantic Ocean region), and NA Eutelsat; 21 submarine cables.

Radio broadcast stations:
AM about 100, FM about 4,600, shortwave 9 (1998)

Radios:
50.5 million (1997)

Television broadcast stations:
358 (plus 4,728 repeaters) (1995)

Televisions:
30.5 million (1997)

Internet Hosts:
22.152 million (2009)

Internet users:
24.992 million (2008)

Country code (Top-level domain): .it



</doc>
<doc id="14704" url="https://en.wikipedia.org/wiki?curid=14704" title="Transport in Italy">
Transport in Italy

Italy has a well developed transport infrastructure. 
The Italian rail network is extensive, especially in the north, and it includes a high-speed rail network that joins the major cities of Italy from Naples through northern cities such as Milan and Turin. 
Italy has 2,507 people and 12.46 km per kilometer of rail track, giving Italy the world's 13th largest rail network.

Italy's road network is also widespread, with a total length of about 487,700 km. 
It comprises both an extensive motorway network (6,400 km), mostly toll roads, and national and local roads.

Because of its long seacoast, Italy also has many harbors for the transportation of both goods and passengers. 
Transport networks in Italy are integrated into the Trans-European Transport Networks.

The Italian railway system has a length of , of which standard gauge and electrified. The active lines are 16,723 km. The network is recently growing with the construction of the new high-speed rail network.
The narrow gauge tracks are: 

A major part of the Italian rail network is managed and operated by Ferrovie dello Stato Italiane, a state owned company. Other regional agencies, mostly owned by public entities such as regional governments, operate on the Italian network. The Italian railways are subsidised by the government, receiving €8.1 billion in 2009.

Travellers who often make use of the railway during their stay in Italy might use Rail Passes, such as the European Inter-Rail or Italy's national and regional passes. These rail passes allow travellers the freedom to use regional trains during the validity period, but all high-speed and intercity trains require a 10-euro reservation fee. Regional passes, such as "Io viaggio ovunque Lombardia", offer one-day, multiple-day and monthly period of validity. There are also saver passes for adults, who travel as a group, with savings up to 20%. Foreign travellers should purchase these passes in advance, so that the passes could be delivered by post prior to the trip. When using the rail passes, the date of travel needs to be filled in before boarding the trains.

Major works to increase the commercial speed of the trains already started in 1967: the Rome-Florence "super-direct" line was built for trains up to 230 km/h, and reduced the journey time to less than two hours. This is the first high-speed train line in Europe, as its operations started in 1977.

In 2009 a new high-speed line linking Milan and Turin, operating at 300 km/h, opened to passenger traffic, reducing the journey time from two hours to one hour. In the same year, the Milan-Bologna line was open, reducing the journey time to 55 minutes. Also the Bologna-Florence high-speed line was upgraded to 300 km/h for a journey time of 35 minutes.

Since then, it is possible to travel from Turin to Salerno (ca. 950 km) in less than 5 hours. More than 100 trains per day are operated.

The main public operator of high-speed trains ("alta velocità AV", formerly Eurostar Italia) is Trenitalia, part of FSI. Trains are divided into three categories: Frecciarossa ("Red arrow") trains operate at a maximum of 300 km/h on dedicated high-speed tracks; Frecciargento (Silver arrow) trains operate at a maximum of 250 km/h on both high-speed and mainline tracks; Frecciabianca (White arrow) trains operate at a maximum of 200 km/h on mainline tracks only.

Since 2012, a new and Italy's first private train operator, NTV (branded as Italo), run high-speed services in competition with Trenitalia. Even nowadays, Italy is the only county in Europe with a private high-speed train operator.

Construction of the Milan-Venice high-speed line has begun in 2013 and in 2016 the Milan-Treviglio section has been opened to passenger traffic; the Milan-Genoa high-speed line (Terzo Valico dei Giovi) is also under construction.

Today it is possible to travel from Rome to Milan in less than 3 hours (2h 55') with the Frecciarossa 1000, the new high-speed train. To cover this route, there's a train every 30 minutes.

With the introduction of high-speed trains, intercity trains are limited to few services per day on mainline and regional tracks.

The daytime services ("Intercity" IC), while not freuquent and limited to one or two trains per route, are essential in providing access to cities and towns off the railway's mainline network. The main routes are Trieste to Rome (stopping at Venice, Bologna, Prato, Florence and Arezzo), Milan to Rome (stopping at Genoa, La Spezia, Pisa and Livorno / stopping at Parma, Modena, Bologna, Prato, Florence and Arezzo), Bologna to Lecce (stopping at Rimini, Ancona, Pescara, Bari and Brindisi) and Rome to Reggio di Calabria (stopping at Latina and Naples). In addition, the Intercity trains provide a more economical means of long-distance rail travel within Italy.

The night trains ("Intercity Notte" ICN) have sleeper compartments and washrooms, but no showers on board. Main routes are Rome to Bolzano/Bozen (calling at Florence, Bologna, Verona, Rovereto and Trento), Milan to Lecce (calling at Bologna, Rimini, Ancona, Pescara, Bari and Brindisi), Turin to Lecce (calling at Alessandria, Voghera, Piacenza, Parma, Bologna, Rimini, Pescara, Bari and Brindisi) and Reggio di Calabria to Turin (calling Naples, Rome, Livorno, La Spezia and Genova). Most portions of these ICN services run during the night; since most services take 10 to 15 hours to complete a one-way journey, their day-time portion provide extra train connections to complement with the Intercity services.

There are a total of 86 intercity trains running within Italy per day.

Trenitalia operates regional services (both fast "veloce RGV" and stopping "REG") throughout Italy.

Regional train agencies exist: their train schedules are largely connected to and shown on Trenitalia, and tickets for such train services can be purchased through Trenitalia's national network. Other regional agencies have separate ticket systems which are not mutually exchangeable with that of Trenitalia. These "regional" tickets could be purchased at local newsagents or tobacco stores instead.


In addition to these agencies, there's a great deal of other little operators, such as AMT Genova for the Genova-Casella railway.

Cities with metro systems:
Cities with commuter rail systems:

Italy has 11 rail border crossings over the Alpine mountains with her neighbouring countries: six are designated as mainline tracks and two are metre-gauge tracks. The six mainline border crossings are: two with France (one for Nice and Marseille; the other for Lyon and Dijon), two with Switzerland (one for Brig, Bern and Geneva; the other for Chiasso, Lugano, Lucerne and Zürich), and two with Austria (one for Innsbruck; the other for Villach, Graz and Vienna). The two metre-gauge track crossings are located at the border town of Tirano (enters Switzerland's Canton Graubünden/Grisons) and Domodossola (enters Switzerland's Locarno).

There is a railway line connecting Italy's northeastern port of Trieste to Slovenia, but no passenger or freight services operate on this track. Consequently, there is no direct connections between Trieste and Ljubljana, the capital of Slovenia, despite the proximity of both cities.


The Vatican City is also linked to Italy with a railway line serving a single railway station, the Vatican City railway station. This line is used only for special occasions.
San Marino used to have a narrow gauge rail connection with Italy; this was dismantled in 1944.

Italy's top ten railway stations by annual passengers are:

Italy is one of the countries with the most vehicles per capita, with 690 per 1000 people in 2010. Italy has a total of 487,700 km of paved roads, of which 6,758 km are motorways with a general speed limit of , which since 2009 was provisioned for extension up to . The speed limit in towns is usually and less commonly .

Italy has of navigable waterways for various types of commercial traffic, although of limited overall value.

In the northern regions of Lombardy and Venetia, commuter ferry boats operate on Lake Garda and Lake Como to connect towns and villages at both sides of the lakes. The waterways in Venice, including the Grand Canal, serve as the vital transportation network for local residents and tourists. Frequent shuttle ferries ("vaporetta") connect different points on the main island of Venice and other outlying islands of the lagoon. In addition, there are direct shuttle boats between Venice and the Venice Marco Polo Airport.

Italy's largest airline is Alitalia, which was privatised in 2008. Its main hub is Rome Fiumicino Airport. Alitalia also operates a regional subsidiary under the Alitalia CityLiner brand.

An important regional airline is Air Dolomiti, owned by the German Lufthansa Group. Charter and leisure carriers include Neos, Blue Panorama Airlines and Poste Air Cargo. Major Italian cargo operators are Alitalia Cargo and Cargolux Italia.

Italy is the fifth in Europe by number of passengers by air transport, with about 148 million passengers or about 10% of the European total in 2011. Most of passengers in Italy are on international flights (57%). A big share of domestic flights connect the major islands (Sardinia and Sicily) to the mainland. Domestic flights between major Italian cities as Rome and Milan still play a relevant role but are declining since the opening of the Italian high-speed rail network in recent years.

Italy has a total as of 130 airports in 2012, of which 99 have paved runways:

Airports - with unpaved runways in 2012:

This is a list of the top ten busiest airports in Italy in 2017. 

There are long-distance intercity buses run by local companies, but the services are infrequent during the week and usually provide a secondary link to railway services.

Italy does not have a nationwide coach operator. However, in 2015, the British company Megabus (Europe) launched daily intercity bus services on several domestic routes

This makes a daily total of 12 services in each direction between Rome and Bologna.

Flixbus, a company founded in the course of the opening of the German intercity bus market also serves routes in Italy both domestic and international.

Airport shuttle buses, however, are highly developed and convenient for rail travellers. Most airports in Italy are not connected to the railway network, except for Rome Fiumicino Airport, Milan Malpensa Airport and Turin Caselle Airport. In Bologna, a light-rail track is under construction to connect Bologna Airport to the main railway station.


Local buses are usually divided into urban ("urbano") and suburban ("interurbano" or "extraurbano") lines.



</doc>
<doc id="14705" url="https://en.wikipedia.org/wiki?curid=14705" title="Italian Armed Forces">
Italian Armed Forces

The Italian Armed Forces () encompass the Italian Army, the Italian Navy and the Italian Air Force. A fourth branch of the armed forces, known as the Carabinieri, take on the role as the nation's military police and are also involved in missions and operations abroad as a combat force. Despite not being a branch of the armed forces, the Guardia di Finanza is part of the military and operates a large fleet of ships, aircraft and helicopters, enabling it to patrol Italy's waters and to eventually participate in warfare scenarios. These five forces have military status and are all organized along military lines, comprising a total of 341,250 men and women with the official status of active military personnel, of which 165,500 are in the Army, Navy and Air Force. The President of the Italian Republic heads the armed forces as the President of the High Council of Defence established by article 87 of the Constitution of Italy. According to article 78, the Parliament has the authority to declare a state of war and vest the necessary powers in the Government.

The office of the Chief of Defence is organised as follows:

The ground force of Italy, the "Regio Esercito" dates back to the unification of Italy in the 1850s and 1860s. It fought in colonial engagements in China during the Boxer Rebellion, against the Ottoman Empire in Libya (1911-1912), on the Alps against the Austro-Hungarian Empire during World War I, in Abyssinia during the Interwar period, and in World War II in Albania, Greece, North Africa and Russia, as well as in the Italian Civil War. During the Cold War the Army prepared itself to defend against a Warsaw Pact invasion from the east. Since the dissolution of the Soviet Union, it has seen extensive peacekeeping service in Lebanon, Afghanistan, and Iraq. On 29 July 2004 it became a professional all-volunteer force when conscription was finally ended.

The navy of Italy was created in 1861, following the proclamation of the Kingdom of Italy, as the "Regia Marina". The new navy's baptism of fire came during the Third Italian War of Independence against the Austrian Empire. During the First World War, it spent its major efforts in the Adriatic Sea, fighting the Austro-Hungarian Navy. In the Second World War, it engaged the Royal Navy in a two-and-a-half-year struggle for the control of the Mediterranean Sea. After the war, the new "Marina Militare", being a member of the North Atlantic Treaty Organisation (NATO), has taken part in many coalition peacekeeping operations.It is a blue-water navy. The "Guardia Costiera" (Coast Guard) is a component of the navy.

The air force of Italy was founded as an independent service arm on 28 March 1923, by King Vittorio Emanuele III as the "Regia Aeronautica" (which equates to "Royal Air Force"). During the 1930s, it was involved in its first military operations in Ethiopia in 1935, and later in the Spanish Civil War between 1936 and 1939. Eventually, Italy entered World War II alongside Germany. After the armistice of 8 September 1943, Italy was divided into two sides, and the same fate befell the "Regia Aeronautica". The Air Force was split into the "Italian Co-Belligerent Air Force" in the south aligned with the Allies, and the pro-Axis "Aeronautica Nazionale Repubblicana" in the north until the end of the war. When Italy was made a republic by referendum, the air force was given its current name "Aeronautica Militare".

The "Arma dei Carabinieri" is the gendarmerie and military police of Italy. The corps was instituted in 1814 by King Victor Emmanuel I of Savoy with the aim of providing the Kingdom of Sardinia with a police corps; it is therefore older than Italy itself. The new force was divided into divisions on the scale of one division for each province of Italy. The divisions were further divided into companies and subdivided into lieutenancies, which commanded and coordinated the local police stations and were distributed throughout the national territory in direct contact with the public. The Italian unification saw the number of divisions increased, and in 1861 the "Carabinieri" were appointed the "First Force" of the new national military organization. In recent years "Carabinieri" units have been dispatched on peacekeeping missions, including Kosovo, Afghanistan, and Iraq. At the Sea Islands Conference of the G8 in 2004, the Carabinieri were given the mandate to establish a Center of Excellence for Stability Police Units (CoESPU) to spearhead the development of training and doctrinal standards for civilian police units attached to international peacekeeping missions.

Italy has joined in many UN, NATO and EU operations as well as with assistance to Russia and the other CIS nations, Middle East peace process, peacekeeping, and combating the illegal drug trade, human trafficking, piracy and terrorism.

Italy did take part in the 1982 Multinational Force in Lebanon along with US, French and British troops. Italy also participated in the 1990–91 Gulf War, with the deployment of eight Panavia Tornado IDS bomber jets; Italian Army troops were subsequently deployed to assist Kurdish refugees in northern Iraq following the conflict.

As part of Operation Enduring Freedom, Italy contributed to the international operation in Afghanistan. Italian forces have contributed to ISAF, the NATO force in Afghanistan, and to the Provincial reconstruction team. Italy has sent 3,800 troops, including one infantry company from the 2nd Alpini Regiment tasked to protect the ISAF HQ, one engineer company, one NBC platoon, one logistic unit, as well as liaison and staff elements integrated into the operation chain of command. Italian forces also command a multinational engineer task force and have deployed a platoon of Carabinieri military police.

The Italian Army did not take part in combat operations of the 2003 Iraq War, dispatching troops only when major combat operations were declared over by the U.S. President George W. Bush. Subsequently, Italian troops arrived in the late summer of 2003, and began patrolling Nasiriyah and the surrounding area. Italian participation in the military operations in Iraq was concluded by the end of 2006, with full withdrawal of Italian military personnel except for a small group of about 30 soldiers engaged in providing security for the Italian embassy in Baghdad. Italy played a major role in the 2004-2011 NATO Training Mission to assist in the development of Iraqi security forces training structures and institutions.

Since the second post-war the Italian armed force has become more and more engaged in international peace support operations, mainly under the auspices of the United Nations. The Italian armed forces are currently participating in 26 missions.




</doc>
<doc id="14706" url="https://en.wikipedia.org/wiki?curid=14706" title="Foreign relations of Italy">
Foreign relations of Italy

Foreign relations of the Italian Republic are the Italian government's external relations with the outside world. Located in Europe, Italy has been considered a major Western power since its unification in 1861. Its main allies are the NATO countries and the EU states, two entities of which Italy is a founding member.

Italy has a particular role within the Christian world because Rome is the seat of the Pope and the center of the Catholic Church. Italy acts as a mediator in the Israeli–Palestinian conflict and has many troops deployed in the Middle East, and all over the world for peacekeeping missions, and for combating organized crime, illegal drug trade, human trafficking, piracy and terrorism. Italy is currently commanding various multinational forces. The country plays also a significant role in former colonies and territories of the Italian Empire and is considered a key player in the Mediterranean region.

The "Risorgimento" was the era 1830–1870 that saw the emergence of a national consciousness. Italians achieved independence from Austria, the House of Bourbon and from the Pope, securing national unification. The papacy called France to resist unification, fearing that giving up control of the Papal States would weaken the Church and allow the liberals to dominate conservative Catholics. Italy captured Rome in 1870 and later formed the Triple Alliance (1882) with Germany and Austria.

Italy defeated the Ottoman Empire in 1911–1912. By 1914, Italy had acquired in Africa a colony on the Red Sea coast (Eritrea), a large protectorate in Somalia and administrative authority in formerly Turkish Libya. Outside of Africa, Italy possessed a small concession in Tientsin in China (following the Boxer Rebellion) and the Dodecanese Islands off the coast of Turkey.

Austria took the offensive against the terms of the alliance and Italy decided to take part in World War I as a principal allied power with France and Great Britain. Two leaders, Prime Minister Antonio Salandra and Foreign Minister Sidney Sonnino made the decisions; their primary motivation was seizure of territory from Austria, as secretly promised by Britain and France in the Treaty of London of 1915. Also, Italy occupied southern Albania and established a protectorate over Albania, which remained in place until 1920. The Allies defeated the Austrian Empire in 1918 and Italy became one of the main winners of the war. At the Paris Peace Conference in 1919, Prime Minister Vittorio Emanuele Orlando focused almost exclusively on territorial gains, but he got far less than he wanted, and Italians were bitterly resentful when they were denied control of the city of Fiume The conference, under the control of Britain, France and the United States refused to assign Dalmazia and Albania to Italy as had been promised in the Treaty of London. Britain, France and Japan divided the German overseas colonies into mandates of their own, excluding Italy. Italy also gained no territory from the breakup of the Ottoman Empire. Civil unrest erupted in Italy between nationalists who supported the war effort and opposed what they called the "mutilated victory" (as nationalists referred to it) and leftists who were opposed to the war.

The Fascist government that came to power with Benito Mussolini in 1922 sought to increase the size of the Italian empire and to satisfy the claims of Italian irredentists. In 1935–36, in its second invasion of Ethiopia Italy was successful and merged its new conquest with its older east African colonies. In 1939, Italy invaded Albania and incorporated it into the Fascist state. During the Second World War (1939–45), Italy formed the axis alliance with Japan and Germany and occupied several territories (such as parts of France, Greece, Egypt and Tunisia) but was forced in the final peace to abandon all its colonies and protectorates. Following the civil war and the economic depression caused by World War II, Italy enjoyed an economic miracle, promoted European unity, joined NATO and became an active member of the European Union. Italy was granted a United Nations trust to administer Somaliland in 1950. When Somalia became independent in 1960, Italy's eight-decade experience with colonialism ended.

Italy is part of the UN, EU, NATO, the OECD, the OSCE the DAC, the WTO, the G6, G7, G8, G10, G20, the Union for the Mediterranean, the Council of Europe, the Central European Initiative, the ASEM, the MEF. Italy leads the Uniting for Consensus and participates in prominent decision-making groups such as the EU big four, the Quint and the Contact Group.




</doc>
<doc id="14708" url="https://en.wikipedia.org/wiki?curid=14708" title="Italian language">
Italian language

Italian ("italiano", or , ) is a Romance language of the Indo-European language family. Italian is, by most measures and together with Sardinian, the closest language to Latin, from which it descends via Vulgar Latin. Italian is an official language in Italy, Switzerland (where it is the main language of Ticino and the Graubünden valleys of Calanca, Mesolcina, Bregaglia and val Poschiavo), San Marino and Vatican City. It has an official minority status in western Istria (Croatia and Slovenia). It formerly had official status in Albania, Malta, Monaco, Montenegro (Kotor) and Greece (Ionian Islands and Dodecanese) and is generally understood in Corsica (due to its close relation with the Tuscan-influenced local language) and Savoie. It also used to be an official language in the former Italian East Africa and Italian North Africa, where it still plays a significant role in various sectors. Italian is also spoken by large expatriate communities in the Americas and Australia. Italian is included under the languages covered by the European Charter for Regional or Minority languages in Bosnia and Herzegovina and in Romania, although Italian is neither a co-official nor a protected language in these countries. Many speakers of Italian are native bilinguals of both Italian (either in its standard form or regional varieties) and other regional languages.

Italian is a major European language, being one of the official languages of the Organization for Security and Co-operation in Europe and one of the working languages of the Council of Europe. It is the second most widely spoken native language in the European Union with 67 million speakers (15% of the EU population) and it is spoken as a second language by 13.4 million EU citizens (3%). Including Italian speakers in non-EU European countries (such as Switzerland, Albania and the United Kingdom) and on other continents, the total number of speakers is approximately 85 million. Italian is the main working language of the Holy See, serving as the lingua franca (common language) in the Roman Catholic hierarchy as well as the official language of the Sovereign Military Order of Malta. Italian is known as the "language of music" because of its use in musical terminology and opera; numerous Italian words referring to music have become international terms taken into various languages worldwide. Its influence is also widespread in the arts and in the food and luxury goods markets.

Italian was adopted by the state after the Unification of Italy, having previously been a literary language based on Tuscan as spoken mostly by the upper class of Florentine society. Its development was also influenced by other Italian languages and, to some minor extent, by the Germanic languages of the post-Roman invaders. The incorporation into Italian of learned words from its own ancestor language, Latin, is another form of lexical borrowing through the influence of written language, scientific terminology and the liturgical language of the Church. Throughout the Middle Ages and into the early modern period, most literate Italians were also literate in Latin and thus they easily adopted Latin words into their writing—and eventually speech—in Italian. Unlike most other Romance languages, Italian retains Latin's contrast between short and long consonants. Almost all native Italian words end with vowels, a factor that makes Italian words extremely easy to use in rhyming. Italian has a 7 vowel sound system ('e' and 'o' have mid-low and mid-high sounds); Classical Latin had 10, 5 with short and 5 with long sounds.

During the Middle Ages, the established written language in Europe was Latin, though the great majority of people were illiterate, and only a handful were well versed in the language. In the Italian peninsula, as in most of Europe, most would instead speak a local vernacular. These dialects, as they are commonly referred to, evolved from Vulgar Latin over the course of centuries, unaffected by formal standards and teachings. They are not in any sense "dialects" of standard Italian, which itself started off as one of these local tongues, but sister languages of Italian. Mutual intelligibility with Italian varies widely, as it does with Romance languages in general. The Romance dialects of Italy can differ greatly from Italian at all levels (phonology, morphology, syntax, lexicon, pragmatics) and are classified typologically as distinct languages.
The standard Italian language has a poetic and literary origin in the writings of Tuscan writers of the 12th century, and, even though the grammar and core lexicon are basically unchanged from those used in Florence in the 13th century, the modern standard of the language was largely shaped by relatively recent events. However, Romance vernacular as language spoken in the Apennine peninsula has a longer history. In fact, the earliest surviving texts that can definitely be called vernacular (as distinct from its predecessor Vulgar Latin) are legal formulae known as the Placiti Cassinesi from the Province of Benevento that date from 960–963, although the Veronese Riddle, probably from the 8th or early 9th century, contains a late form of Vulgar Latin that can be seen as a very early sample of a vernacular dialect of Italy.

The language that came to be thought of as Italian developed in central Tuscany and was first formalized in the early 14th century through the works of Tuscan writer Dante Alighieri, written in his native Florentine. Dante's epic poems, known collectively as the "Commedia," to which another Tuscan poet Giovanni Boccaccio later affixed the title "Divina", were read throughout the peninsula and his written dialect became the "canonical standard" that all educated Italians could understand. Dante is still credited with standardizing the Italian language. In addition to the widespread exposure gained through literature, the Florentine dialect also gained prestige due to the political and cultural significance of Florence at the time and the fact that it was linguistically an intermediate between the northern and the southern Italian dialects. Thus the dialect of Florence became the basis for what would become the official language of Italy.

Italian was progressively made an official language of most of the Italian states predating unification, slowly replacing Latin, even when ruled by foreign powers (like Spain in the Kingdom of Naples, or Austria in the Kingdom of Lombardy-Venetia), even though the masses kept speaking primarily their local vernaculars. Italian was also one of the many recognised languages in the Austro-Hungarian Empire.

Italy has always had a distinctive dialect for each city because the cities, until recently, were thought of as city-states. Those dialects now have considerable variety. As Tuscan-derived Italian came to be used throughout Italy, features of local speech were naturally adopted, producing various versions of regional Italian. The most characteristic differences, for instance, between Roman Italian and Milanese Italian are the gemination of initial consonants and the pronunciation of stressed "e", and of "s" in some cases: e.g. "va bene" "all right" is pronounced by a Roman (and by any standard Italian speaker), by a Milanese (and by any speaker whose native dialect lies to the north of the La Spezia–Rimini Line); "a casa" "at home" is for Roman, or for standard, for Milanese and generally northern.

In contrast to the Gallo-Italic linguistic panorama of northern Italy, the Italo-Dalmatian Neapolitan and its related dialects were largely unaffected by the Franco-Occitan influences introduced to Italy mainly by bards from France during the Middle Ages, but after the Norman conquest of southern Italy, Sicily became the first Italian land to adopt Occitan lyric moods (and words) in poetry. Even in the case of Northern Italian languages, however, scholars are careful not to overstate the effects of outsiders on the natural indigenous developments of the languages.

The economic might and relatively advanced development of Tuscany at the time (Late Middle Ages) gave its language weight, though Venetian remained widespread in medieval Italian commercial life, and Ligurian (or Genoese) remained in use in maritime trade alongside the Mediterranean. The increasing political and cultural relevance of Florence during the periods of the rise of the "Banco Medici", Humanism, and the Renaissance made its dialect, or rather a refined version of it, a standard in the arts.

The Renaissance era, known as in Italian, was seen as a time of "rebirth", which is the literal meaning of both (from French) and (Italian).

During this time, long-existing beliefs stemming from the teachings of the Roman Catholic Church began to be understood from new perspectives as humanists—individuals who placed emphasis on the human body and its full potential—began to shift focus from the church to human beings themselves. Humanists began forming new beliefs in various forms: social, political, and intellectual. The ideals of the Renaissance were evident throughout the Protestant Reformation, which took place simultaneously with the Renaissance. The Protestant Reformation began with Martin Luther's rejection of the selling of indulgences by Johann Tetzel and other authorities within the Roman Catholic Church, resulting in Luther's eventual break-off from the Roman Catholic Church in the Diet of Worms. After Luther was excommunicated from the Roman Catholic Church, he founded what was then understood to be a sect of Catholicism, later referred to as Lutheranism. Luther's preaching in favor of faith and scripture rather than tradition led him to translate the Bible into many other languages, which would allow for people from all over Europe to read the Bible. Previously, the Bible was only translated into Latin, but after this development it could be understood in many other languages, including Italian. The Italian language was able to spread even more with the help of Luther and the invention of the printing press by Johannes Gutenberg. The printing press facilitated the spread of Italian because it was able to rapidly produce texts, such as the Bible, and cut the costs of books which allowed for more people to have access to the translated Bible and new pieces of literature. The Roman Catholic Church was losing its control over the population, as it was not open to change, and there was an increasing number of reformers with differing beliefs.

Italian became the language used in the courts of every state in the Italian peninsula, as well as the prestige variety used in the island of Corsica (but not in the neighboring Sardinia, which on the contrary underwent Italianization well into the late 18th century, under Savoyard sway: the island's linguistic composition, roofed by the prestige of Spanish among the Sardinians, would therein make for a rather slow process of assimilation to the Italian cultural sphere). The rediscovery of Dante's , as well as a renewed interest in linguistics in the 16th century, sparked a debate that raged throughout Italy concerning the criteria that should govern the establishment of a modern Italian literary and spoken language. This discussion, known as (i. e., the "problem of the language"), ran through the Italian culture until the end of the 19th century, often linked to the political debate on achieving a united Italian state. Renaissance scholars divided into three main factions:

A fourth faction claimed that the best Italian was the one that the papal court adopted, which was a mixture of the Tuscan and Roman dialects. 
Eventually, Bembo's ideas prevailed, and the foundation of the Accademia della Crusca in Florence (1582–1583), the official legislative body of the Italian language, led to publication of Agnolo Monosini's Latin tome in 1604 followed by the first Italian dictionary in 1612.

The continual advancements in technology plays a crucial role in the diffusion of languages. After the invention of the printing press in the fifteenth century, the number of printing presses in Italy grew rapidly and by the year 1500 reached a total of 56, the biggest number of printing presses in all of Europe. This enabled the production of more pieces of literature at a lower cost and as the dominant language, Italian spread.

An important event that helped the diffusion of Italian was the conquest and occupation of Italy by Napoleon in the early 19th century (who was himself of Italian-Corsican descent). This conquest propelled the unification of Italy some decades after and pushed the Italian language into a lingua franca used not only among clerks, nobility, and functionaries in the Italian courts but also by the bourgeoisie.

Italian literature's first modern novel, "I promessi sposi" ("The Betrothed") by Alessandro Manzoni, further defined the standard by "rinsing" his Milanese "in the waters of the Arno" (Florence's river), as he states in the preface to his 1840 edition.

After unification, a huge number of civil servants and soldiers recruited from all over the country introduced many more words and idioms from their home languages—"ciao" is derived from the Venetian word "s-cia[v]o" ("slave"), "panettone" comes from the Lombard word "panetton", etc. Only 2.5% of Italy's population could speak the Italian standardized language properly when the nation was unified in 1861.

Italian is a Romance language, a descendant of Vulgar Latin (colloquial spoken Latin). Standard Italian is based on Tuscan, especially its Florentine dialect, and is therefore an Italo-Dalmatian language, a classification that includes most other central and southern Italian languages and the extinct Dalmatian.

According to many sources, Italian is the closest language to Latin in terms of vocabulary. According to the Ethnologue, Lexical similarity is 89% with French, 87% with Catalan, 85% with Sardinian, 82% with Spanish, 80% with Portuguese, 78% with Ladin, 77% with Romanian. Estimates may differ according to sources.

One study (analyzing the degree of differentiation of Romance languages in comparison to Latin (comparing phonology, inflection, discourse, syntax, vocabulary, and intonation) estimated that distance between Italian and Latin is higher than that between Sardinian and Latin. In particular, its vowels are the second-closest to Latin after Sardinian. As in most Romance languages, stress is distinctive.

Italian is an official language of Italy and San Marino and is spoken fluently by the majority of the countries' populations. Italian is the third most spoken language in Switzerland (after German and French), and its use has moderately declined since the 1970s. Italian is also used in administration and official documents in Vatican City.

Due to heavy Italian influence during the Italian colonial period, Italian is still understood by some in former colonies. Although it was the primary language in Libya since colonial rule, Italian greatly declined under the rule of Muammar Gaddafi, who expelled the Italian Libyan population and made Arabic the sole official language of the country. A few hundred Italian settlers returned to Libya in the 2000s; today Italian is the most spoken second language in the country and serves as a language of commerce and sometimes as a "lingua franca" between Libyans and foreigners.

Italian was the official language of Eritrea during Italian colonisation. Italian is today used in commerce and it is still spoken especially among elders; besides that, Italian words are incorporated as loan words in the main language spoken in the country (Tigrinya). The capital city of Eritrea, Asmara, still has several Italian schools, established during the colonial period. In the early 19th century, Eritrea was the country with the highest number of Italians abroad, and the Italian Eritreans grew from 4,000 during World War I to nearly 100,000 at the beginning of World War II.
In Asmara there are two Italian schools: 

Italian was also introduced to Somalia through colonialism and was the sole official language of administration and education during the colonial period but fell out of use after government, educational and economic infrastructure were destroyed in the Somali Civil War.

Albania and Malta have large populations of non-native speakers, with over half of the population having some knowledge of the Italian language.

Although over 17 million Americans are of Italian descent, only a little over one million people in the United States speak Italian at home. Nevertheless, an Italian language media market does exist in the country.

Italian immigrants to South America have also brought a presence of the language to that continent. According to some sources, Italian is the second most spoken language in Argentina after the official language of Spanish, although its number of speakers, mainly of the older generation, is decreasing.

Italian is widely taught in many schools around the world, but rarely as the first foreign language. In the 21st century, technology also allows for the continual spread of the Italian language, as people have new ways to learn how to speak, read, and write languages at their own pace and at any given time. For example, the free website and application Duolingo has 4.94 million English speakers learning the Italian language.

According to the Italian Ministry of Foreign Affairs, every year there are more than 200,000 foreign students who study the Italian language; they are distributed among the 90 Institutes of Italian Culture that are located around the world, in the 179 Italian schools located abroad, or in the 111 Italian lecturer sections belonging to foreign schools where Italian is taught as a language of culture.

From the late nineteenth to the mid-twentieth century, thousands of Italians settled in Argentina, Uruguay, Southern Brazil and Venezuela, as well as in Canada and the United States, where they formed a physical and cultural presence.

In some cases, colonies were established where variants of regional languages of Italy were used, and some continue to use this regional language. Examples are Rio Grande do Sul, Brazil, where Talian is used, and the town of Chipilo near Puebla, Mexico; each continues to use a derived form of Venetian dating back to the nineteenth century. Another example is Cocoliche, an Italian–Spanish pidgin once spoken in Argentina and especially in Buenos Aires, and Lunfardo.

Starting in late medieval times in much of Europe and the Mediterranean, Latin was replaced as the primary commercial language by Italian language variants (especially Tuscan and Venetian). These variants were consolidated during the Renaissance with the strength of Italy and the rise of humanism and the arts.

During that period, Italy held artistic sway over the rest of Europe. It was the norm for all educated gentlemen to make the Grand Tour, visiting Italy to see its great historical monuments and works of art. It thus became expected to learn at least some Italian. In England, while the classical languages Latin and Greek were the first to be learned, Italian became the second most common modern language after French, a position it held until the late eighteenth century when it tended to be replaced by German. John Milton, for instance, wrote some of his early poetry in Italian.

Within the Catholic church, Italian is known by a large part of the ecclesiastical hierarchy and is used in substitution for Latin in some official documents.

Italian loanwords continue to be used in most languages in matters of art and music (especially classical music including opera), in the design and fashion industries, in some sports like football and especially in culinary terms.

In Italy, almost all the other languages spoken as the vernacular—other than standard Italian and some languages spoken among immigrant communities—are often imprecisely called "Italian dialects", even though they are quite different, with some belonging to different linguistic branches. The only exceptions to this are twelve groups considered "historical language minorities", which are officially recognized as distinct minority languages by the law. On the other hand, Corsican (a language spoken on the French island of Corsica) is closely related to medieval Tuscan, from which Standard Italian derives and evolved.

The differences in the evolution of Latin in the different regions of Italy can be attributed to the presence of three other types of languages: substrata, superstrata, and adstrata. The most prevalent were substrata (the language of the original inhabitants), as the Italian dialects were most likely simply Latin as spoken by native cultural groups. Superstrata and adstrata were both less important. Foreign conquerors of Italy that dominated different regions at different times left behind little to no influence on the dialects. Foreign cultures with which Italy engaged in peaceful relations with, such as trade, had no significant influence either.

Throughout Italy, regional variations of Standard Italian, called Regional Italian, are spoken. Regional differences can be recognized by various factors: the openness of vowels, the length of the consonants, and influence of the local language (for example, in informal situations ', ' and ' replace the standard Italian ' in the area of Tuscany, Rome and Venice respectively for the infinitive "to go").

There is no definitive date when the various Italian variants of Latin—including varieties that contributed to modern Standard Italian—began to be distinct enough from Latin to be considered separate languages. One criterion for determining that two language variants are to be considered separate languages rather than variants of a single language is that they have evolved so that they are no longer mutually intelligible; this diagnostic is effective if mutual intelligibility is minimal or absent (e.g. in Romance, Romanian and Portuguese), but it fails in cases such as Spanish-Portuguese or Spanish-Italian, as native speakers of either pairing can understand each other well if they choose to do so. Nevertheless, on the basis of accumulated differences in morphology, syntax, phonology, and to some extent lexicon, it is not difficult to identify that for the Romance varieties of Italy, the first extant written evidence of languages that can no longer be considered Latin comes from the ninth and tenth centuries C.E. These written sources demonstrate certain vernacular characteristics and sometimes explicitly mention the use of the vernacular in Italy. Full literary manifestations of the vernacular began to surface around the 13th century in the form of various religious texts and poetry.Although these are the first written records of Italian varieties separate from Latin, the spoken language had likely diverged long before the first written records appear, since those who were literate generally wrote in Latin even if they spoke other Romance varieties in person.

Throughout the 19th and 20th centuries, the use of Standard Italian became increasingly widespread and was mirrored by a decline in the use of the dialects. An increase in literacy was one of the main driving factors (one can assume that only literates were capable of learning Standard Italian, whereas those who were illiterate had access only to their native dialect). The percentage of literates rose from 25% in 1861 to 60% in 1911, and then on to 78.1% in 1951. Tullio De Mauro, an Italian linguist, has asserted that in 1861 only 2.5% of the population of Italy could speak Standard Italian. He reports that in 1951 that percentage had risen to 87%. The ability to speak Italian did not necessarily mean it was in everyday use, and most people (63.5%) still usually spoke their native dialects. In addition, other factors such as mass emigration, industrialization, and urbanization, and internal migrations after World War II, contributed to the proliferation of Standard Italian. The Italians who emigrated during the Italian diaspora beginning in 1861 were often of the uneducated lower class, and thus the emigration had the effect of increasing the percentage of literates, who often knew and understood the importance of Standard Italian, back home in Italy. A large percentage of those who had emigrated also eventually returned to Italy, often more educated than when they had left.

The Italian dialects have declined in the modern era, as Italy unified under Standard Italian and continues to do so aided by mass media, from newspapers to radio to television.

Italian has a seven-vowel system, consisting of , as well as 23 consonants. Compared with most other Romance languages, Italian phonology is conservative, preserving many words nearly unchanged from Vulgar Latin. Some examples:

The conservative nature of Italian phonology is partly explained by its origin. Italian stems from a literary language that is derived from the 13th-century speech of the city of Florence in the region of Tuscany, and has changed little in the last 700 years or so. Furthermore, the Tuscan dialect is the most conservative of all Italian dialects, radically different from the Gallo-Italian languages less than 100 miles to the north (across the La Spezia–Rimini Line).

The following are some of the conservative phonological features of Italian, as compared with the common Western Romance languages (French, Spanish, Portuguese, Galician, Catalan). Some of these features are also present in Romanian.

Compared with most other Romance languages, Italian has many inconsistent outcomes, where the same underlying sound produces different results in different words, e.g. > "lasciare" and "lassare", > "cacciare" and "cazzare", > "sdrucciolare", "druzzolare" and "ruzzolare", > "regina" and "reina". Although in all these examples the second form has fallen out of usage, the dimorphism is thought to reflect the several-hundred-year period during which Italian developed as a literary language divorced from any native-speaking population, with an origin in 12th/13th-century Tuscan but with many words borrowed from languages farther to the north, with different sound outcomes. (The La Spezia–Rimini Line, the most important isogloss in the entire Romance-language area, passes only about 20 miles to the north of Florence.) Dual outcomes of Latin /p t k/ between vowels, such as > "luogo" but > "fuoco", was once thought to be due to borrowing of northern voiced forms, but is now generally viewed as the result of early phonetic variation within Tuscany.

Some other features that distinguish Italian from the Western Romance languages:

Standard Italian also differs in some respects from most nearby Italian languages:

Italian phonotactics do not usually permit verbs and polysyllabic nouns to end with consonants, except in poetry and song, so foreign words may receive extra terminal vowel sounds.

Italian has a shallow orthography, meaning very regular spelling with an almost one-to-one correspondence between letters and sounds. In linguistic terms, the writing system is close to being a phonemic orthography. The most important of the few exceptions are the following (see below for more details): 

The Italian alphabet is typically considered to consist of 21 letters. The letters j, k, w, x, y are traditionally excluded, though they appear in loanwords such as "jeans", "whisky", "taxi", "xenofobo", "xilofono". The letter has become common in standard Italian with the prefix "extra-", although "(e)stra-" is traditionally used; it is also common to use the Latin particle "ex(-)" to mean "former(ly)" as in: "la mia ex" ("my ex-girlfriend"), "Ex-Jugoslavia" ("Former Yugoslavia"). The letter appears in the first name "Jacopo" and in some Italian place-names, such as Bajardo, Bojano, Joppolo, Jerzu, Jesolo, Jesi, Ajaccio, among others, and in "Mar Jonio", an alternative spelling of "Mar Ionio" (the Ionian Sea). The letter may appear in dialectal words, but its use is discouraged in contemporary standard Italian. Letters used in foreign words can be replaced with phonetically equivalent native Italian letters and digraphs: , , or for ; or for (including in the standard prefix "kilo-"); , or for ; , , , or for ; and or for .

Italian has geminate, or double, consonants, which are distinguished by length and intensity. Length is distinctive for all consonants except for , , , , , which are always geminate when between vowels, and , which is always single.
Geminate plosives and affricates are realized as lengthened closures. Geminate fricatives, nasals, and are realized as lengthened continuants. There is only one vibrant phoneme but the actual pronunciation depends on context and regional accent. Generally one can find a flap consonant in unstressed position whereas is more common in stressed syllables, but there may be exceptions. Especially people from the Northern part of Italy (Parma, Aosta Valley, South Tyrol) may pronounce as , , or .

Of special interest to the linguistic study of Regional Italian is the "gorgia toscana", or "Tuscan Throat", the weakening or lenition of intervocalic , , and in the Tuscan language.

The voiced postalveolar fricative is present as a phoneme only in loanwords: for example, "garage" . Phonetic is common in Central and Southern Italy as an intervocalic allophone of : "gente" 'people' but "la gente" 'the people', "ragione" 'reason'.

Italian grammar is typical of the grammar of Romance languages in general. Cases exist for personal pronouns (nominative, oblique, accusative, dative), but not for nouns.

There are two basic classes of nouns in Italian, referred to as genders, masculine and feminine. Gender may be natural ("ragazzo" 'boy', "ragazza" 'girl') or simply grammatical with no possible reference to biological gender (masculine "costo" 'cost', feminine "costa" 'coast'). Masculine nouns typically end in "-o" ("ragazzo" 'boy'), with plural marked by "-i" ("ragazzi" 'boys'), and feminine nouns typically end in "-a", with plural marked by "-e" ("ragazza" 'girl', "ragazze" 'girls'). For a group composed of boys and girls, "ragazzi" is the plural, suggesting that "-i" is a general plural. A third category of nouns is unmarked for gender, ending in "-e" in the singular and "-i" in the plural: "legge" 'law, f. sg.', "leggi" 'laws, f. pl.'; "fiume" 'river, m. sg.', "fiumi" 'rivers, m. pl.', thus assignment of gender is arbitrary in terms of form, enough so that terms may be identical but of distinct genders: "fine" meaning 'aim', 'purpose' is masculine, while "fine" meaning 'end, ending' (e.g. of a movie) is feminine, and both are "fini" in the plural, a clear instance of "-i" as a non-gendered default plural marker. These nouns often, but not always, denote inanimates. There are a number of nouns that have a masculine singular and a feminine plural, most commonly of the pattern m. sg. "-o", f. pl. "-a" ("miglio" 'mile, m. sg.', "miglia" 'miles, f. pl.'; "paio" 'pair, m. sg., "paia" 'pairs, f. pl.'), and thus are sometimes considered neuter (these are usually derived from neuter Latin nouns). An instance of neuter gender also exists in pronouns of the third person singular.

Examples:

Nouns, adjectives, and articles inflect for gender and number (singular and plural).

Like in English, common nouns are capitalized when occurring at the beginning of a sentence. Unlike English, nouns referring to languages (e.g. Italian), speakers of languages, or inhabitants of an area (e.g. Italians) are not capitalized.

There are three types of adjectives: descriptive, invariable and form-changing. Descriptive adjectives are the most common, and their endings change to match the number and gender of the noun they modify. Invariable adjectives are adjectives whose endings do not change. The form changing adjectives "buono (good), bello (beautiful), grande (big), and santo (saint)" change in form when placed before different types of nouns. Italian has three degrees for comparison of adjectives: positive, comparative, and superlative.

The order of words in the phrase is relatively free compared to most European languages. The position of the verb in the phrase is highly mobile. Word order often has a lesser grammatical function in Italian than in English. Adjectives are sometimes placed before their noun and sometimes after. Subject nouns generally come before the verb. Italian is a null-subject language, so that nominative pronouns are usually absent, with subject indicated by verbal inflections (e.g. "amo" 'I love', "ama" '(s)he loves', "amano" 'they love'). Noun objects normally come after the verb, as do pronoun objects after imperative verbs, infinitives and gerunds, but otherwise pronoun objects come before the verb.

There are both indefinite and definite articles in Italian. There are four indefinite articles, selected by the gender of the noun they modify and by the phonological structure of the word that immediately follows the article. "Uno" is masculine singular, used before "z" ( or ), "s+consonant", "gn" (), or "ps", while masculine singular "un" is used before a word beginning with any other sound. The noun "zio" 'uncle' selects masculine singular, thus "uno zio" 'an uncle' or "uno zio anziano" 'an old uncle,' but "un mio zio" 'an uncle of mine'. The feminine singular indefinite articles are "una", used before any consonant sound, and its abbreviated form, written "un'," used before vowels: "una camicia" 'a shirt', "una camicia bianca" 'a white shirt', "un'altra camicia" 'a different shirt'. There are seven forms for definite articles, both singular and plural. In the singular: "lo", which corresponds to the uses of "uno"; "il", which corresponds to the uses with consonant of "un"; "la," which corresponds to the uses of "una"; "l'," used for both masculine and feminine singular before vowels. In the plural: "gli" is the masculine plural of "lo and l<nowiki>'</nowiki>"; "i" is the plural of "il"; and "le" is the plural of feminine "la" and "l"'.

There are numerous contractions of prepositions with subsequent articles. There are numerous productive suffixes for diminutive, augmentative, pejorative, attenuating, etc., which are also used to create neologisms.

There are 27 pronouns, grouped in clitic and tonic pronouns. Personal pronouns are separated into three groups: subject, object (which take the place of both direct and indirect objects), and reflexive. Second person subject pronouns have both a polite and a familiar form. These two different types of address are very important in Italian social distinctions. All object pronouns have two forms: stressed and unstressed (clitics). Unstressed object pronouns are much more frequently used, and come before the verb ("Lo vedo". 'I see him.'). Stressed object pronouns come after the verb, and are used when emphasis is required, for contrast, or to avoid ambiguity ("Vedo lui, ma non lei". 'I see him, but not her'). Aside from personal pronouns, Italian also has demonstrative, interrogative, possessive, and relative pronouns. There are two types of demonstrative pronouns: relatively near (this) and relatively far (that). Demonstratives in Italian are repeated before each noun, unlike in English.

There are three regular sets of verbal conjugations, and various verbs are irregularly conjugated. Within each of these sets of conjugations, there are four simple (one-word) verbal conjugations by person/number in the indicative mood (present tense; past tense with imperfective aspect, past tense with perfective aspect, and future tense), two simple conjugations in the subjunctive mood (present tense and past tense), one simple conjugation in the conditional mood, and one simple conjugation in the imperative mood. Corresponding to each of the simple conjugations, there is a compound conjugation involving a simple conjugation of "to be" or "to have" followed by a past participle. "To have" is used to form compound conjugation when the verb is transitive ("Ha detto", "ha fatto": he/she has said, he/she has made/done), while "to be" is used in the case of verbs of motion and some other intransitive verbs ("È andato", "è stato": he/she has gone, he/she has been). "To be" may be used with transitive verbs, but in such a case it makes the verb passive ("È detto", "è fatto": it is said, it is made/done). This rule is not absolute, and some exceptions do exist.

Note: the plural form of verbs could also be used as an extremely formal (for example to noble people in monarchies) singular form (see royal we).




</doc>
<doc id="14709" url="https://en.wikipedia.org/wiki?curid=14709" title="Ice-T">
Ice-T

Tracy Lauren Marrow (born February 16, 1958), better known by his stage name Ice-T, is an American rapper, singer, songwriter, musician, actor, record producer, and author. He began his career as an underground rapper in the 1980s and was signed to Sire Records in 1987, when he released his debut album "Rhyme Pays"; the second hip-hop album to carry an explicit content sticker after Slick Rick's "La Di Da Di". The following year, he founded the record label Records (named after his collective of fellow hip-hop artists called the "") and released another album, "Power", which went on to go Platinum. He also released several other albums that went Gold.

He co-founded the heavy metal band Body Count, which he introduced on his 1991 rap album "", on the track titled "Body Count". The band released their self-titled debut album in 1992. Ice-T encountered controversy over his track "Cop Killer", the lyrics of which discussed killing police officers. Ice-T asked to be released from his contract with Warner Bros. Records, and his next solo album, "Home Invasion", was released later in February 1993 through Priority Records. Body Count's next album was released in 1994, and Ice-T released two more albums in the late-1990s. Since 2000, he has portrayed NYPD Detective/Sergeant Odafin Tutuola on the NBC police drama "". In 2018, he began hosting the true crime documentary, "In Ice Cold Blood", on the Oxygen cable channel. In 2020, "In Ice Cold Blood" began its third season.

Tracy Lauren Marrow, the son of Solomon and Alice Marrow, was born in Newark, New Jersey. Solomon was African-American and Alice was American Creole. For decades, Solomon worked as a conveyor belt mechanic at the Rapistan Conveyor Company. When Marrow was a child, his family moved to upscale Summit, New Jersey. The first time race played a major part in Marrow's life was at the age of seven, when he became aware of the racism leveled by his white friends towards black children. Marrow surmised that he escaped similar treatment because they thought that he was white due to his lighter skin. Relaying this incident to his mother, she told him, "Honey, people are stupid;" her advice and this incident taught Marrow to control the way the negativity of others affected him.

His mother died of a heart attack when he was in third grade. Solomon raised Marrow as a single father for four years, with help from a housekeeper. Marrow's first experience with illicit activity occurred after a bicycle that his father bought him for Christmas was stolen. After Marrow told his father, Solomon shrugged, "Well, then, you ain't got no bike." Marrow stole parts from bicycles and assembled "three or four weird-looking, brightly-painted bikes" from the parts; his father either did not notice or never acknowledged this. When Marrow was thirteen years old, Solomon also died of a heart attack. For many years, AllMusic.com has stated that his parents "died in an auto accident", but Ice-T has stated that it was actually he who had been in a car accident, and that it was decades later.

Following his father's death, the orphaned Marrow lived with a nearby aunt briefly, then was sent to live with his other aunt and her husband in View Park-Windsor Hills, an upper middle-class Black neighborhood in South Los Angeles. While his cousin Earl was preparing to leave for college, Marrow shared a bedroom with him. Earl was a fan of rock music and listened only to the local rock radio stations; sharing a room with him sparked Marrow's interest in heavy metal music.

Marrow moved to the Crenshaw District of Los Angeles when he was in the eighth grade. He attended Palms Junior High, which was predominantly made up of white students, and included black students who travelled by bus from South Central to attend. He then attended Crenshaw High School, which was almost entirely made up of black students.

Marrow stood out from most of his friends because he did not drink alcohol, smoke tobacco, or use drugs. During Marrow's time in high school, gangs became more prevalent in the Los Angeles school system. Students who belonged to the Crips and Bloods gangs attended Crenshaw, and fought in the school's hallways. Marrow, while never an actual gang member, was affiliated with the former. Marrow began reading the novels of Iceberg Slim, which he memorized and recited to his friends, who enjoyed hearing the excerpts and told him, "Yo, kick some more of that by Ice, T", giving Marrow his famous nickname. Marrow and other Crips wrote and performed "Crip Rhymes".

His music career started with the band of the singing group The Precious Few of Crenshaw High School. Marrow and his group opened the show, dancing to a live band. The singers were Thomas Barnes, Ronald Robinson and Lapekas Mayfield.

In 1975, at the age of seventeen, Marrow began receiving Social Security benefits resulting from the death of his father and used the money to rent an apartment for $90 a month. He sold cannabis and stole car stereos to earn extra cash, but he was not making enough to support his pregnant girlfriend. Once his daughter was born, he joined the United States Army in October 1977. Marrow served a two-year and two-month tour in the 25th Infantry Division and was involved with a group of soldiers charged with the theft of a rug. While awaiting trial, he received a $2,500 bonus check and went absent without leave, returning a month later, after the rug had been returned. Marrow received a non-judicial punishment as a consequence of his dereliction of duty.

During his spell in the Army, Marrow became interested in hip hop music. He heard The Sugar Hill Gang's newly released single "Rapper's Delight" (1979), which inspired him to perform his own raps over the instrumentals of this and other early hip-hop records. The music, however, did not fit his lyrics or form of delivery.

When he was stationed in Hawaii (where prostitution was not a heavily prosecuted crime) as a squad leader at Schofield Barracks, Marrow met a pimp named Mac. Mac admired that Marrow could quote Iceberg Slim and he taught Marrow how to be a pimp himself. Marrow was also able to purchase stereo equipment cheaply in Hawaii, including two Technics turntables, a mixer, and large speakers. Once equipped, he then began to learn turntablism and rapping.

Towards the end of his tenure in the Army, Marrow learned from his commanding officer that he could receive an honorable discharge because he was a single father, so he was discharged in December 1979.

During an episode of "The Adam Carolla Podcast" that aired on June 6, 2012, Marrow claimed that after being discharged from the Army, he began a career as a bank robber. Marrow claimed he and some associates began conducting take-over bank robberies "like [in the film] "Heat"." Marrow then elaborated, explaining, "Only punks go for the drawer, we gotta go for the safe." Marrow also stated he was glad the United States justice system has statutes of limitations, which had likely expired when Marrow admitted to his involvement in multiple Class 1 Felonies in the early-to-mid 1980s.

In July 2010, Marrow was mistakenly arrested. A month later when Marrow attended court, the charges were dropped and the prosecution stated "there had been a clerical error when the rapper was arrested". Marrow gave some advice to young people who think going to jail is a mark of integrity, saying: "Street credibility has nothing to do with going to jail, it has everything to do with staying out."

After leaving the Army, Marrow wanted to stay away from gang life and violence and instead make a name for himself as a DJ. As a tribute to Iceberg Slim, Marrow adopted the stage name Ice-T. While performing as a DJ at parties, he received more attention for his rapping, which led Ice-T to pursue a career as a rapper. After breaking up with his girlfriend Caitlin Boyd, he returned to a life of crime and robbed jewelry stores with his high school friends. Ice-T's raps later described how he and his friends pretended to be customers to gain access before smashing the display glass with baby sledgehammers.

Ice-T's friends Al P. and Sean E. Sean went to prison. Al P. was caught in 1982 and sent to prison for robbing a high-end jewelry store in Laguna Niguel for $2.5 million in jewelry. Sean was arrested for possession of not only cannabis, which Sean sold, but also material stolen by Ice-T. Sean took the blame and served two years in prison. Ice-T stated that he owed a debt of gratitude to Sean because his prison time allowed him to pursue a career as a rapper. Concurrently, he wound up in a car accident and was hospitalized as a John Doe because he did not carry any form of identification due to his criminal activities. After being discharged from the hospital, he decided to abandon the criminal lifestyle and pursue a professional career rapping. Two weeks after being released from the hospital, he won an open mic competition judged by Kurtis Blow.

In 1982, Ice-T met producer Willie Strong from Saturn Records. In 1983, Strong recorded Ice-T's first single, "Cold Wind Madness", also known as "The Coldest Rap", an electro hip-hop record that became an underground success, becoming popular even though radio stations did not play it due to the song's hardcore lyrics. That same year, Ice-T released "Body Rock", another electro hip-hop single that found popularity in clubs. In 1984, Ice-T released the single "Killers", the first of his political raps, and then was a featured rapper on "Reckless", a single by DJ Chris "The Glove" Taylor and (co-producer) David Storrs. This song was almost immediately followed up with a sequel entitled "Reckless Rivalry (Combat)", which was featured in the "Breakin' " sequel, "", however it was never featured on the soundtrack album and, to this day, has never been released. Ice later recorded the songs "Ya Don't Quit" and "Dog'n the Wax (Ya Don't Quit-Part II)" with Unknown DJ, who provided a Run–D.M.C.-like sound for the songs.

Ice-T received further inspiration as an artist from Schoolly D's gangsta rap single "P.S.K. What Does It Mean?", which he heard in a club. Ice-T enjoyed the single's sound and delivery, as well as its vague references to gang life, although the real life gang, Park Side Killers, was not named in the song.

Ice-T decided to adopt Schoolly D's style, and wrote the lyrics to his first gangsta rap song, "6 in the Mornin'", in his Hollywood apartment, and created a minimal beat with a Roland TR-808. He compared the sound of the song, which was recorded as a B-side on the single "Dog'n The Wax", to that of the Beastie Boys. The single was released in 1986, and he learned that "6 in the Mornin'" was more popular in clubs than its A-side, leading Ice-T to rap about Los Angeles gang life, which he described more explicitly than any previous rapper. He intentionally did not represent any particular gang, and wore a mixture of red and blue clothing and shoes to avoid antagonizing gang-affiliated listeners, who debated his true affiliation.

Ice-T finally landed a deal with a major label Sire Records. When label founder and president Seymour Stein heard his demo, he said, "He sounds like Bob Dylan." Shortly after, he released his debut album "Rhyme Pays" in 1987 supported by DJ Evil E, DJ Aladdin and producer Afrika Islam, who helped create the mainly party-oriented sound. The record wound up being certified gold by the RIAA. That same year, he recorded the title theme song for Dennis Hopper's "Colors," a film about inner-city gang life in Los Angeles. His next album "Power" was released in 1988, under his own label Rhyme Syndicate, and it was a more assured and impressive record, earning him strong reviews and his second gold record. Released in 1989, "The Iceberg/Freedom of Speech... Just Watch What You Say" established his popularity by matching excellent abrasive music with narrative and commentative lyrics. In the same year, he appeared on Hugh Harris' single "Alice".

In 1991, he released his album "O.G. Original Gangster", which is regarded as one of the albums that defined gangsta rap. On "OG", he introduced his heavy metal band Body Count in a track of the same name. Ice-T toured with Body Count on the first annual Lollapalooza concert tour in 1991, gaining him appeal among middle-class teenagers and fans of alternative music genres. The album "Body Count" was released in March 1992. For his appearance on the heavily collaborative track "Back on the Block", a composition by jazz musician Quincy Jones that "attempt[ed] to bring together black musical styles from jazz to soul to funk to rap", Ice-T won a Grammy Award for the Best Rap Performance by a Duo or Group, an award shared by others who worked on the track including Jones and fellow jazz musician Ray Charles.

Controversy later surrounded Body Count over its song "Cop Killer". The rock song was intended to speak from the viewpoint of a criminal getting revenge on racist, brutal cops. Ice-T's rock song infuriated government officials, the National Rifle Association and various police advocacy groups. Consequently, Time Warner Music refused to release Ice-T's upcoming album "Home Invasion" because of the controversy surrounding "Cop Killer". Ice-T suggested that the furor over the song was an overreaction, telling journalist Chuck Philips "...they've done movies about nurse killers and teacher killers and student killers. Arnold Schwarzenegger blew away dozens of cops as the Terminator. But I don't hear anybody complaining about that." In the same interview, Ice-T suggested to Philips that the misunderstanding of "Cop Killer", the misclassification of it as a rap song (not a rock song), and the attempts to censor it had racial overtones: "The Supreme Court says it's OK for a white man to burn a cross in public. But nobody wants a black man to write a record about a cop killer." 

Ice-T split amicably with Sire/Warner Bros. Records after a dispute over the artwork of the album "Home Invasion". He then reactivated Rhyme Syndicate and formed a deal with Priority Records for distribution. Priority released "Home Invasion" in the spring of 1993. The album peaked at #9 on "Billboard" magazine's "Top R&B/Hip-Hop Albums" and at #14 on the "Billboard" 200, spawning several singles including "Gotta Lotta Love", "I Ain't New To This" and "99 Problems" – which would later inspire Jay-Z to record a version with new lyrics in 2003. In 2003 he released the single Beat of Life with Sandra Nasić, Trigga tha Gambler and DJ Tomekk and placed in the German charts. 

Ice-T had also collaborated with certain other heavy metal bands during this time period. For the film "Judgment Night", he did a duet with Slayer on the track "Disorder". In 1995, Ice-T made a guest performance on "Forbidden" by Black Sabbath. Another album of his, "VI – Return of the Real", was released in 1996, followed by "The Seventh Deadly Sin" in 1999.

His first rap album since 1999, "Gangsta Rap", was released on October 31, 2006. The album's cover, which "shows [Ice-T] lying on his back in bed with his ravishing wife's ample posterior in full view and one of her legs coyly draped over his private parts", was considered to be too suggestive for most retailers, many of which were reluctant to stock the album. Some reviews of the album were unenthusiastic, as many had hoped for a return to the political raps of Ice-T's most successful albums.

Ice-T appears in the film "Gift". One of the last scenes includes Ice-T and Body Count playing with Jane's Addiction in a version of the Sly and the Family Stone song "Don't Call Me Nigger, Whitey."

Besides fronting his own band and rap projects, Ice-T has also collaborated with other hard rock and metal bands, such as Icepick, Motörhead, Slayer, Pro-Pain, and Six Feet Under. He has also covered songs by hardcore punk bands such as The Exploited, Jello Biafra, and Black Flag. Ice-T made an appearance at Insane Clown Posse's Gathering of the Juggalos (2008 edition). Ice-T was also a judge for the 7th annual Independent Music Awards to support independent artists. His 2012 film "" features a who's who of underground and mainstream rappers.

In November 2011, Ice-T announced via Twitter that he was in the process of collecting beats for his next LP which was expected sometime during 2012, but , the album has not been released. A new Body Count album, "Bloodlust", was released in 2017. After the release of the album, responding to an interview question asking if he's "done with rap", he answered "I don't know" and noted that he's "really leaning more toward EDM right now".

In July 2019, Ice-T released his first solo hip hop track in 10 years titled "Feds In My Rearview". The track is the first of 3 song that are a trilogy, with the second track titled "Too Old For The Dumb Shit", described as a prequel to "Feds In My Rearview", released in September 2019. Ice-T was also featured on the 2020 hip hop posse cut "The Slayers Club" alongside R.A. the Rugged Man, Brand Nubian and others.

Ice-T was prominently featured as both a rapper and a breakdancer in “Breakin’ ‘n’ Enterin’” (1983), a documentary about the early West Coast Hip Hop scene.

Ice-T's first film appearances were in the motion pictures, "Breakin'" (1984), and its sequel, "" (1984). These films were released before Ice-T released his first LP, although he appears on the soundtrack to "Breakin". He has since stated he considers the films and his own performance in them to be "wack".

In 1991, he embarked on a serious acting career, portraying police detective Scotty Appleton in Mario Van Peebles' action thriller "New Jack City", gang leader Odessa (alongside Denzel Washington and John Lithgow) in "Ricochet" (1991), gang leader King James in "Trespass" (1992), followed by a notable lead role performance in "Surviving the Game" (1994), in addition to many supporting roles, such as J-Bone in "Johnny Mnemonic" (1995), and the marsupial mutant T-Saint in "Tank Girl" (1995). He was also interviewed in the Brent Owens documentary "Pimps Up, Ho's Down", in which he claims to have had an extensive pimping background before getting into rap. He is quoted as saying "once you max something out, it ain't no fun no more. I couldn't really get no farther." He goes on to explain his pimping experience gave him the ability to get into new businesses. "I can't act, I really can't act, I ain't no rapper, it's all game. I'm just working these niggas." Later he raps at the Players Ball.

In 1993, Ice-T along with other rappers and the three "Yo! MTV Raps" hosts Ed Lover, Doctor Dré and Fab 5 Freddy starred in the comedy "Who's the Man?", directed by Ted Demme. In the movie, he is a drug dealer who gets really frustrated when someone calls him by his real name, "Chauncey", rather than his street name, "Nighttrain."
In 1995, Ice-T had a recurring role as vengeful drug dealer Danny Cort on the television series "New York Undercover", co-created by Dick Wolf. His work on the series earned him the 1996 NAACP Image Award for Outstanding Supporting Actor in a Drama Series. In 1997, he co-created the short-lived series "Players", produced by Wolf. This was followed by a role as pimp Seymour "Kingston" Stockton in "" (1998). These collaborations led Wolf to add Ice-T to the cast of "." Since 2000 he has portrayed Odafin "Fin" Tutuola, a former undercover narcotics officer transferred to the Special Victims Unit. In 2002, the NAACP awarded Ice-T with a second Image Award, again for Outstanding Supporting Actor in a Drama Series, for his work on "Law & Order: SVU".

Around 1995, Ice-T co-presented a UK-produced magazine television series on black culture, "Baadasss TV".

In 1997, Ice-T had a pay-per-view special titled "Ice-T's Extreme Babes" which appeared on Action PPV, formerly owned by BET Networks.

In 1999, Ice-T starred in the HBO movie "Stealth Fighter" as a United States Naval Aviator who fakes his own death, steals an F-117 stealth fighter, and threatens to destroy United States military bases. He also acted in the movie "Sonic Impact", released the same year.

Ice-T made an appearance on the comedy television series "Chappelle's Show" as himself presenting the award for "Player Hater of the Year" at the "Player-Haters Ball", a parody of his own appearance at the Players Ball. He was dubbed the "Original Player Hater."

"Beyond Tough", a 2002 documentary series, aired on Discovery Channel about the world's most dangerous and intense professions, such as alligator wrestlers and Indy 500 pit crews, was hosted by Ice-T.

In 2007, Ice-T appeared as a celebrity guest star on the MTV sketch comedy show "Short Circuitz". Also in late 2007, he appeared in the short-music film "Hands of Hatred", which can be found online.
Ice-T was interviewed for the Cannibal Corpse retrospective documentary "", as well as appearing in Chris Rock's 2009 documentary "Good Hair", in which he reminisced about going to school in hair curlers.

A 2016 advertisement for GEICO features Ice-T behind a lemonade stand run by children. When people ask if it's Ice-T, the actor yells back, "No, it's lemonade!"

Ice-T's voice acting roles include Madd Dogg in the video game "", as well as Agent Cain in "". He also appears as himself in "" and "" fighting video games. He also voiced the character Aaron Griffin in the video game "Gears of War 3". Marrow also made an appearance in the 2019 video game Borderlands 3, in which he voices the character of BALEX.

On December 27, 2013, Ice-T announced that he was entering podcasting in a deal with the Paragon Collective. Ice-T co-hosts the "Ice-T: Final Level" podcast with his longtime friend, Mick Benzo (known as Zulu Beatz on Sirius XM). They discuss relevant issues, movies, video games, and do a behind the scenes of Law Order: SVU segment with featured guests from the entertainment world. The show will release new episodes bi-weekly. Guests have included Jim Norton. Ice-T released his first episode on January 7 to many accolades.

On October 20, 2006, "Ice-T's Rap School" aired and was a reality television show on VH1. It was a spin-off of the British reality show "Gene Simmons' Rock School", which also aired on VH1. In "Rap School", rapper/actor Ice-T teaches eight teens from York Preparatory School in New York called the "York Prep Crew" ("Y.P. Crew" for short). Each week, Ice-T gives them assignments and they compete for an imitation gold chain with a microphone on it. On the season finale on November 17, 2006, the group performed as an opening act for Public Enemy.

On June 12, 2011, E! reality show "Ice Loves Coco" debuted. The show is mostly about his relationship with his wife, Nicole "Coco" Austin.


Ice cites writer Iceberg Slim and rapper Schoolly D as influences, with Iceberg Slim's novels guiding his skills as a lyricist. His favorite heavy rock acts are Edgar Winter, Led Zeppelin and Black Sabbath. His hip hop albums helped shape gangsta rap, with music journalists tracing works of artists such as Tupac Shakur, Notorious B.I.G., Eminem and N.W.A to "6 in the Mornin'".

A love of rock led Ice to use guitar in his albums, to provide his songs with edge and power, and to make his raps harder. He drew on the fusion of rock and hip hop by Rick Rubin-produced acts such as Beastie Boys, Run-DMC and LL Cool J, who featured rock samples in their songs.

Body Count – whose 1992 debut album Ice described as a "rock album with a rap mentality" – is described as paving the way for the success of rap rock fusions by acts like Kid Rock and Limp Bizkit. However, Ice-T states that the band's style does not fuse the two genres, and that Body Count is solely a rock band.

In "Hip Hop Connection", Ice listed his favourite rap albums:
10. Beastie Boys, "Licensed to Ill"
9. Eric B. & Rakim, "Paid in Full"
8. N.W.A, "Straight Outta Compton"
7. Wu-Tang Clan, "Enter the Wu-Tang (36 Chambers)"
6. The Notorious B.I.G., "Ready to Die"
5. Dr. Dre, "The Chronic"
4. Boogie Down Productions, "Criminal Minded"
3. Ultramagnetic MCs, "Critical Beatdown"
2. Public Enemy, "It Takes a Nation of Millions to Hold Us Back"
1. Run-DMC, "Run-DMC

On March 20, 1976, Marrow's high school girlfriend Adrienne gave birth to their daughter LeTesha Marrow, and they continued attending high school while raising her. While filming "Breakin'" in 1984, he met his second girlfriend Darlene Ortiz, who was at the club where the film was shot. They began a relationship and Ortiz was featured on the covers of "Rhyme Pays" and "Power". Ice-T and Ortiz had son Ice Tracy Marrow, Jr. in 1992.

Ice-T married swimsuit model Nicole "Coco Marie" Austin in January 2002. In celebration of their impending 9th wedding anniversary, the couple renewed their wedding vows on June 4, 2011. As of 2006, they owned a penthouse apartment in North Bergen, New Jersey. In 2012, they were building a five-bedroom house in Edgewater, New Jersey, that was expected to be completed by the end of the year. In 2015, the couple had their first child together, daughter Chanel.

During the popularity of Public Enemy, Ice-T was closely associated with the band and his recordings of the time showed a similar political viewpoint. He was referred to as "The Soldier of the Highest Degree" in the booklet for "Fear of a Black Planet" and mentioned on the track "Leave This Off Your F***in' Charts". He also collaborated with fellow anti-censorship campaigner Jello Biafra on his album "The Iceberg/Freedom Of Speech... Just Watch What You Say!".

On June 5, 2008, Ice-T joked that he would be voting for John McCain in the 2008 American elections, speculating that his past affiliation with Body Count could hurt Barack Obama's chances if he endorsed him, so he would choose instead to ruin John McCain's campaign by saying he supported him.

Ice-T had a feud with LL Cool J in the late 1980s and early 1990s. Apparently, this was instigated by LL's claim to be "the baddest rapper in the history of rap itself". Ice-T recorded disses against LL on his 1988 album "Power". On the album was the track, "I'm Your Pusher", in which a rap music addict declines to buy an LL Cool J record. The album also contains the posse rap track, "The Syndicate", which took aim at LL's lyrical ability, claiming that rapping about oneself so frequently was a "first grade topic". The song also mocked the song's hook "I'm Bad", which identified it as an LL diss specifically. In the book "Check the Technique: Liner Notes for Hip-Hop Junkies", Ice-T said that the song "Girls L.G.B.N.A.F." was also intended as a diss to LL Cool J, by making a crude song to contrast with the love songs that LL was making at the time.

On LL's response, "To da Break of Dawn" in 1990, he dissed Kool Moe Dee (whose feud with LL was far more publicized) as well as MC Hammer. He then devoted the third verse of the song to dissing Ice-T, mocking his rap ability ("take your rhymes around the corner to rap rehab"), his background ("before you rapped, you was a downtown car thief"), and his style ("a brother with a perm deserves to get burned"). He also suggested that the success of "Power" was due to the appearance of Ice-T's girlfriend Darlene on the album cover. Ice-T appeared to have ignored the insults and he had also defended LL Cool J after his arrest in the song "Freedom of Speech".

In August 2012, Ice-T said that the rivalry was "never serious" and that he needed a nemesis to create "an exciting dispute".

In June 2008, on DJ Cisco's "Urban Legend" mixtape, Ice-T criticized Soulja Boy (AKA DeAndre Cortez Way) for "killing hip hop" and called his song "Crank That" "garbage" compared to the works of other hip-hop artists such as Rakim, Das EFX, Big Daddy Kane and Ice Cube. One of the comments in the exchange was when Ice-T told Way to "eat a dick". The two then traded numerous videos back and forth over the Internet. These videos included a cartoon and video of Ice-T dancing on Way's behalf and an apology, but reiteration of his feelings that Way's music "sucks", on Ice-T's behalf. Rapper Kanye West defended Way saying "He came from the 'hood, made his own beats, made up a new saying, new sound and a new dance with one song."




Grammy Awards
MTV Video Music Awards
MTV Movie Awards
Image Awards
Adult Video News Awards
News & Documentary Emmy Award
All Def Movie Awards





</doc>
<doc id="14711" url="https://en.wikipedia.org/wiki?curid=14711" title="Iron Age">
Iron Age

The Iron Age is the final epoch of the three-age division of the prehistory and protohistory of humanity. It was preceded by the Stone Age (Paleolithic, Mesolithic, Neolithic, and Chalcolithic) and the Bronze Age. The concept has been mostly applied to Europe and the Ancient Near East, and, by analogy, also to other parts of the Old World.

The duration of the Iron Age varies depending on the region under consideration. It is defined by archaeological convention, and the mere presence of some cast or wrought iron is not sufficient to represent an Iron Age culture; rather, the "Iron Age" begins locally when the production of iron or steel has been brought to the point where iron tools and weapons superior to their bronze equivalents become widespread. For example, Tutankhamun's meteoric iron dagger comes from the Bronze Age. In the Ancient Near East, this transition takes place in the wake of the so-called Bronze Age collapse, in the 12th century BC. The technology soon spread throughout the Mediterranean Basin region and to South Asia. Its further spread to Central Asia, Eastern Europe, and Central Europe is somewhat delayed, and Northern Europe is reached still later, by about 500 BC.

The Iron Age is taken to end, also by convention, with the beginning of the historiographical record.
This usually does not represent a clear break in the archaeological record; for the Ancient Near East, the establishment of the Achaemenid Empire c. 550 BC (considered historical by virtue of the record by Herodotus) is usually taken as a cut-off date, and in Central and Western Europe, the Roman conquests of the 1st century BC serve as marking for the end of the Iron Age. The Germanic Iron Age of Scandinavia is taken to end c. 800 AD, with the beginning of the Viking Age.

In South Asia (Indian sub-continent), the Iron Age is taken to begin with the ironworking Painted Gray Ware culture in the 18th century BC, and to end with the reign of Ashoka (3rd century BC). The use of the term "Iron Age" in the archaeology of South, East, and Southeast Asia is more recent and less common than for western Eurasia; at least in China prehistory had ended before iron-working arrived, so the term is infrequently used. The Sahel (Sudan region) and Sub-Saharan Africa are outside of the three-age system, there being no Bronze Age, but the term "Iron Age" is sometimes used in reference to early cultures practicing ironworking such as the Nok culture of Nigeria.

The three-age system was introduced in the first half of the 19th century for the archaeology of Europe in particular, and by the later 19th century expanded to the archaeology of the Ancient Near East. Its name harks back to the mythological "Ages of Man" of Hesiod. As an archaeological era, it was first introduced for Scandinavia by Christian Jürgensen Thomsen in the 1830s. By the 1860s, it was embraced as a useful division of the "earliest history of mankind" in general and began to be applied in Assyriology. The development of the now-conventional periodization in the archaeology of the Ancient Near East was developed in the 1920s to 1930s.
As its name suggests, Iron Age technology is characterized by the production of tools and weaponry by ferrous metallurgy (ironworking), more specifically from carbon steel.

Increasingly the Iron Age in Europe is being seen as a part of the Bronze Age collapse in the ancient Near East, in ancient India (with the post-Rigvedic Vedic civilization), ancient Iran, and ancient Greece (with the Greek Dark Ages). In other regions of Europe the Iron Age began in the 8th century BC in Central Europe and the 6th century BC in Northern Europe. The Near Eastern Iron Age is divided into two subsections, Iron I and Iron II. Iron I (1200–1000 BC) illustrates both continuity and discontinuity with the previous Late Bronze Age. There is no definitive cultural break between the 13th and 12th centuries BC throughout the entire region, although certain new features in the hill country, Transjordan and coastal region may suggest the appearance of the Aramaean and Sea People groups. There is evidence, however, of strong continuity with Bronze Age culture, although as one moves later into Iron Age the culture begins to diverge more significantly from that of the late 2nd millennium.

The Iron Age as an archaeological period is roughly defined as that part of the prehistory of a culture or region during which ferrous metallurgy was the dominant technology of metalworking.

The characteristic of an Iron Age culture is the mass production of tools and weapons made from steel, typically alloys with a carbon content between approximately 0.30% and 1.2% by weight.
Only with the capability of the production of carbon steel does ferrous metallurgy result in tools or weapons that are equal or superior to bronze. The use of steel has been based as much on economics as on metallurgical advancements. Early steel was made by smelting iron.

By convention, the Iron Age in the Ancient Near East is taken to last from c. 1200 BC (the Bronze Age collapse) to c. 550 BC (or 539 BC), roughly the beginning of historiography with Herodotus; the end of the proto-historical period. In Central and Western Europe, the Iron Age is taken to last from c. 800 BC to c. 1 BC, in Northern Europe from c. 500 BC to 800 AD.

In China, there is no recognizable prehistoric period characterized by ironworking, as Bronze Age China transitions almost directly into the Qin dynasty of imperial China; "Iron Age" in the context of China is sometimes used for the transitional period of c. 500 BC to 100 BC during which ferrous metallurgy was present even if not dominant.

The earliest-known iron artifacts are nine small beads dated to 3200 BC, which were found in burials at Gerzeh, Lower Egypt. They have been identified as meteoric iron shaped by careful hammering. Meteoric iron, a characteristic iron–nickel alloy, was used by various ancient peoples thousands of years before the Iron Age. Such iron, being in its native metallic state, required no smelting of ores.

Smelted iron appears sporadically in the archeological record from the middle Bronze Age. Whilst terrestrial iron is naturally abundant, its high melting point of placed it out of reach of common use until the end of the second millennium BC. Tin's low melting point of and copper's relatively moderate melting point of placed them within the capabilities of the Neolithic pottery kilns, which date back to 6000 BC and were able to produce temperatures greater than . In addition to specially designed furnaces, ancient iron production needed to develop complex procedures for the removal of impurities, the regulation of the admixture of carbon, and for hot-working to achieve a useful balance of hardness and strength in steel.

The earliest tentative evidence for iron-making is a small number of iron fragments with the appropriate amounts of carbon admixture found in the Proto-Hittite layers at Kaman-Kalehöyük and dated to 2200–2000  BC. Akanuma (2008) concludes that "The combination of carbon dating, archaeological context, and archaeometallurgical examination indicates that it is likely that the use of ironware made of steel had already begun in the third millennium BC in Central Anatolia". Souckova-Siegolová (2001) shows that iron implements were made in Central Anatolia in very limited quantities around 1800 BC and were in general use by elites, though not by commoners, during the New Hittite Empire (∼1400–1200 BC).

Similarly, recent archaeological remains of iron working in the Ganges Valley in India have been tentatively dated to 1800  BC. Tewari (2003) concludes that "knowledge of iron smelting and manufacturing of iron artifacts was well known in the Eastern Vindhyas and iron had been in use in the Central Ganga Plain, at least from the early second millennium BC". By the Middle Bronze Age increasing numbers of smelted iron objects (distinguishable from meteoric iron by the lack of nickel in the product) appeared in the Middle East, Southeast Asia and South Asia. African sites are turning up dates as early as 2000-1200 BC.

Modern archaeological evidence identifies the start of large-scale iron production in around 1200  BC, marking the end of the Bronze Age. Between 1200 BC and 1000 BC diffusion in the understanding of iron metallurgy and the use of iron objects was fast and far-flung. Anthony Snodgrass suggests that a shortage of tin, as a part of the Bronze Age Collapse and trade disruptions in the Mediterranean around 1300  BC, forced metalworkers to seek an alternative to bronze. As evidence, many bronze implements were recycled into weapons during that time. More widespread use of iron led to improved steel-making technology at a lower cost. Thus, even when tin became available again, iron was cheaper, stronger and lighter, and forged iron implements superseded cast bronze tools permanently.

The Iron Age in the Ancient Near East is believed to have begun with the discovery of iron smelting and smithing techniques in Anatolia or the Caucasus and Balkans in the late 2nd millennium BC ( 1300  BC). The earliest bloomery smelting of iron is found at Tell Hammeh, Jordan around 930 BC (C dating).

The Early Iron Age artefacts found in Kultepe site, Azerbaijan show that iron smelting was known and used in this region before the 2nd millennium BC (as early as the 3rd millennium BC).

In the Mesopotamian states of Sumer, Akkad and Assyria, the initial use of iron reaches far back, to perhaps 3000 BC. One of the earliest smelted iron artifacts known was a dagger with an iron blade found in a Hattic tomb in Anatolia, dating from 2500 BC. The widespread use of iron weapons which replaced bronze weapons rapidly disseminated throughout the Near East (North Africa, southwest Asia) by the beginning of the 1st millennium BC.

The development of iron smelting was once attributed to the Hittites of Anatolia during the Late Bronze Age.
As part of the Late Bronze Age-Early Iron Age, the Bronze Age collapse saw the slow, comparatively continuous spread of iron-working technology in the region.
It was long held that the success of the Hittite Empire during the Late Bronze Age had been based on the advantages entailed by the "monopoly" on ironworking at the time. Accordingly, the invading Sea Peoples would have been responsible for spreading the knowledge through that region.
The view of such a "Hittite monopoly" has come under scrutiny and no longer represents a scholarly consensus.
While there are some iron objects from Bronze Age Anatolia, the number is comparable to iron objects found in Egypt and other places of the same time period; and only a small number of these objects are weapons.
 

The Iron Age in Egyptian archaeology essentially corresponds to the Third Intermediate Period of Egypt.

Iron metal is singularly scarce in collections of Egyptian antiquities. Bronze remained the primary material there until the conquest by Neo-Assyrian Empire in 671 BC. The explanation of this would seem to lie in the fact that the relics are in most cases the paraphernalia of tombs, the funeral vessels and vases, and iron being considered an impure metal by the ancient Egyptians it was never used in their manufacture of these or for any religious purposes. It was attributed to Seth, the spirit of evil who according to Egyptian tradition governed the central deserts of Africa.
In the Black Pyramid of Abusir, dating before 2000 BC, Gaston Maspero found some pieces of iron. In the funeral text of Pepi I, the metal is mentioned. A sword bearing the name of pharaoh Merneptah as well as a battle axe with an iron blade and gold-decorated bronze shaft were both found in the excavation of Ugarit. A dagger with an iron blade found in Tutankhamun's tomb, 13th century BC, was recently examined and found to be of meteoric origin.

In Europe, the Iron Age is the last stage of prehistoric Europe and the first of the protohistoric periods, which initially means descriptions of a particular area by Greek and Roman writers. For much of Europe, the period came to an abrupt local end after conquest by the Romans, though ironworking remained the dominant technology until recent times. Elsewhere it may last until the early centuries AD, and either Christianization or a new conquest in the Migration Period.

Iron working was introduced to Europe in the late 11th century BC, probably from the Caucasus, and slowly spread northwards and westwards over the succeeding 500 years. The Iron Age did not start when iron first appeared in Europe but it began to replace bronze in the preparation of tools and weapons. It did not happen at the same time all around Europe; local cultural developments played a role in the transition to the Iron Age. For example, the Iron Age of Prehistoric Ireland begins around 500 BC (when the Greek Iron Age had already ended) and finishes around 400 AD. The widespread use of the technology of iron was implemented in Europe simultaneously with Asia. The prehistoric Iron Age in Central Europe divided into two periods based on historical events – Hallstatt culture (early Iron Age) and La Tène (late Iron Age) cultures. Material cultures of Hallstatt and La Tène consist of 4 phases (A, B, C, D phases).

The Iron Age in Europe is characterized by an elaboration of designs in weapons, implements, and utensils. These are no longer cast but hammered into shape, and decoration is elaborate and curvilinear rather than simple rectilinear; the forms and character of the ornamentation of the northern European weapons resemble in some respects Roman arms, while in other respects they are peculiar and evidently representative of northern art.

Citania de Briterios located in Guimaraes, Portugal is one of the examples of archaeological sites of the Iron Age. This settlement (fortified villages) covered an area of 3.8 hectares and served for Celtiberians as a stronghold against Roman invasions. İt dates more than 2500 years back. The site was researched by Francisco Martins Sarmento starting from 1874. A number of amphoras, coins, fragments of pottery, weapons, pieces of jewelry, as well as ruins of a bath and its Pedra Formosa (literally Handsome Stone) revealed here.

The Iron Age in Central Asia began when iron objects appear among the Indo-European Saka in present-day Xinjiang between the 10th century BC and the 7th century BC, such as those found at the cemetery site of Chawuhukou.

The Pazyryk culture is an Iron Age archaeological culture (c. 6th to 3rd centuries BC) identified by excavated artifacts and mummified humans found in the Siberian permafrost in the Altay Mountains.

In China, Chinese bronze inscriptions are found around 1200 BC, preceding the development of iron metallurgy, which was known by the 9th century BC, Therefore, in China prehistory had given way to history periodized by ruling dynasties by the start of iron use, so "Iron Age" is not typically used as to describe a period in Chinese history. Iron metallurgy reached the Yangtse Valley toward the end of the 6th century BC. The few objects were found at Changsha and Nanjing. The mortuary evidence suggests that the initial use of iron in Lingnan belongs to the mid-to-late Warring States period (from about 350 BC). Important non-precious husi style metal finds include Iron tools found at the tomb at Guwei-cun of the 4th century BC.

The techniques used in Lingnan are a combination of bivalve moulds of distinct southern tradition and the incorporation of piece mould technology from the "Zhongyuan". The products of the combination of these two periods are bells, vessels, weapons and ornaments, and the sophisticated cast.

An Iron Age culture of the Tibetan Plateau has tentatively been associated with the Zhang Zhung culture described in early Tibetan writings.

Iron objects were introduced to the Korean peninsula through trade with chiefdoms and state-level societies in the Yellow Sea area in the 4th century BC, just at the end of the Warring States Period but before the Western Han Dynasty began. Yoon proposes that iron was first introduced to chiefdoms located along North Korean river valleys that flow into the Yellow Sea such as the Cheongcheon and Taedong Rivers. Iron production quickly followed in the 2nd century BC, and iron implements came to be used by farmers by the 1st century in southern Korea. The earliest known cast-iron axes in southern Korea are found in the Geum River basin. The time that iron production begins is the same time that complex chiefdoms of Proto-historic Korea emerged. The complex chiefdoms were the precursors of early states such as Silla, Baekje, Goguryeo, and Gaya Iron ingots were an important mortuary item and indicated the wealth or prestige of the deceased in this period.
In Japan, iron items, such as tools, weapons, and decorative objects, are postulated to have entered Japan during the late Yayoi period ( 300 BC–AD 300) or the succeeding Kofun period ( AD 250–538), most likely through contacts with the Korean Peninsula and China.

Distinguishing characteristics of the Yayoi period include the appearance of new pottery styles and the start of intensive rice agriculture in paddy fields. Yayoi culture flourished in a geographic area from southern Kyūshū to northern Honshū. The Kofun and the subsequent Asuka periods are sometimes referred to collectively as the Yamato period; The word "kofun" is Japanese for the type of burial mounds dating from that era.

Iron was being used in Mundigak to manufacture some items in the 3rd millennium BC such as a small copper/bronze bell with an iron clapper, a copper/bronze rod with two iron decorative buttons. and a copper/bronze mirror handle with a decorative iron button. Artefacts including small knives and blades have been discovered in the Indian state of Telangana which have been dated between 2,400 BC and 1800 BC The history of metallurgy in the Indian subcontinent began prior to the 3rd millennium BC. Archaeological sites in India, such as Malhar, Dadupur, Raja Nala Ka Tila, Lahuradewa, Kosambi and Jhusi, Allahabad in present-day Uttar Pradesh show iron implements in the period 1800–1200 BC. As the evidence from the sites Raja Nala ka tila, Malhar suggest the use of Iron in c.1800/1700 BC. The extensive use of iron smelting is from Malhar and its surrounding area. This site is assumed as the center for smelted bloomer iron to this area due to its location in the Karamnasa River and Ganga River. This site shows agricultural technology as iron implements sickles, nails, clamps, spearheads, etc. by at least c.1500 BC Archaeological excavations in Hyderabad show an Iron Age burial site.

The beginning of the 1st millennium BC saw extensive developments in iron metallurgy in India. Technological advancement and mastery of iron metallurgy were achieved during this period of peaceful settlements. One ironworking centre in east India has been dated to the first millennium BC. In Southern India (present-day Mysore) iron appeared as early as 12th to 11th centuries BC; these developments were too early for any significant close contact with the northwest of the country. The Indian Upanishads mention metallurgy. and the Indian Mauryan period saw advances in metallurgy. As early as 300  BC, certainly by AD 200, high-quality steel was produced in southern India, by what would later be called the crucible technique. In this system, high-purity wrought iron, charcoal, and glass were mixed in a crucible and heated until the iron melted and absorbed the carbon.

The protohistoric Early Iron Age in Sri Lanka lasted from 1000  BC to 600  BC. however, evidence of Iron usage was found in Excavation of a Protohistoric Canoe burial Site in Haldummulla and has been dated to 2400 BC. Radiocarbon evidence has been collected from Anuradhapura and Aligala shelter in Sigiriya. The Anuradhapura settlement is recorded to extend by 800 BC and grew to by 700–600 BC to become a town. The skeletal remains of an Early Iron Age chief were excavated in Anaikoddai, Jaffna. The name 'Ko Veta' is engraved in Brahmi script on a seal buried with the skeleton and is assigned by the excavators to the 3rd century BC. Ko, meaning "King" in Tamil, is comparable to such names as Ko Atan and Ko Putivira occurring in contemporary Brahmi inscriptions in south India. It is also speculated that Early Iron Age sites may exist in Kandarodai, Matota, Pilapitiya and Tissamaharama.

Archaeology in Thailand at sites Ban Don Ta Phet and Khao Sam Kaeo yielding metallic, stone, and glass artifacts stylistically associated with the Indian subcontinent suggest Indianization of Southeast Asia beginning in the 4th to 2nd centuries BC during the late Iron Age.

In Philippines and Vietnam, the Sa Huynh culture showed evidence of an extensive trade network. Sa Huynh beads were made from glass, carnelian, agate, olivine, zircon, gold and garnet; most of these materials were not local to the region and were most likely imported. Han-Dynasty-style bronze mirrors were also found in Sa Huynh sites. Conversely, Sa Huynh produced ear ornaments have been found in archaeological sites in Central Thailand, Taiwan (Orchid Island).

In Sub-Saharan Africa, where there was no continent-wide universal Bronze Age, the use of iron succeeded immediately the use of stone.
Metallurgy was characterized by the absence of a Bronze Age, and the transition from stone to iron in tool substances. Early evidence for iron technology in Sub-Saharan Africa can be found at sites such as KM2 and KM3 in northwest Tanzania. Nubia was one of the relatively few places in Africa to have a sustained Bronze Age along with Egypt and much of the rest of North Africa.
Very early copper and bronze working sites in Niger may date to as early as 1500 BC. There is also evidence of iron metallurgy in Termit, Niger from around this period.
Nubia was a major manufacturer and exporter of iron after the expulsion of the Nubian dynasty from Egypt by the Assyrians in the 7th century BC.

Though there is some uncertainty, some archaeologists believe that iron metallurgy was developed independently in sub-Saharan West Africa, separately from Eurasia and neighboring parts of North And Northeast Africa.

Archaeological sites containing iron smelting furnaces and slag have also been excavated at sites in the Nsukka region of southeast Nigeria in what is now Igboland: dating to 2000 BC at the site of Lejja (Eze-Uzomaka 2009) and to 750 BC and at the site of Opi (Holl 2009). The site of Gbabiri (in the Central African Republic) has yielded evidence of iron metallurgy, from a reduction furnace and blacksmith workshop; with earliest dates of 896-773 BC and 907-796 BC respectively. Similarly, smelting in bloomery-type furnaces appear in the Nok culture of central Nigeria by about 550 BC and possibly a few centuries earlier.

Iron and copper working in Sub-Saharan Africa spread south and east from Central Africa in conjunction with the Bantu expansion, from the Cameroon region to the African Great Lakes in the 3rd century BC, reaching the Cape around AD 400.
However, iron working may have been practiced in Central Africa as early as the 3rd millennium BC.
Instances of carbon steel based on complex preheating principles were found to be in production around the 1st century AD in northwest Tanzania.






</doc>
<doc id="14712" url="https://en.wikipedia.org/wiki?curid=14712" title="EFnet">
EFnet

EFnet or Eris-Free network is a major Internet Relay Chat (IRC) network, with more than 35,000 users. It is the modern-day descendant of the original IRC network.

Initially, most IRC servers formed a single IRC network, to which new servers could join without restriction, but this was soon abused by people who set up servers to sabotage other users, channels, or servers. Restriction grew and, in August 1990, codice_1 was the last server indiscriminately allowing other servers to join it, Eris being the Greek goddess of strife and discord.

A group of operators, with the support of Jarkko Oikarinen, introduced a new "Q-line" into their server configurations, to "quarantine" themselves away from eris by disconnecting from any subset of the IRC network as soon as they saw eris there.

For a few days, the entire IRC network suffered frequent netsplits, but eventually the majority of servers added the Q-line and effectively created a new separate IRC net called "EFnet" (Eris-Free Network); the remaining servers which stayed connected to eris (and thus were no longer able to connect to EFnet servers) were called "A-net" (Anarchy Network). A-net soon vanished, leaving EFnet as the only IRC network.

Continuing problems with performance and abuse eventually led to the rise of another major IRC network, Undernet, which split off in October 1992.

In July 1996, disagreement on policy caused EFnet to break in two: the slightly larger European half (including Australia and Japan) formed IRCnet, while the American servers continued as EFnet. This was known as The Great Split.

In July 2001, after a string of DDoS attacks a service called "CHANFIX" (originally "JUPES") was created, which is designed to give back ops to channels which have lost ops or been taken over.

In 2007, various EFnet servers began implementing SSL.

February 2009 saw the introduction of a new "CHANFIX" module called "OPME", a mechanism for EFnet Admins to use to restore ops in an opless channel. It was proposed by Douglas Boldt to provide a much cleaner alternative to masskill, which was unnecessarily invasive and disruptive to the network.

Later in 2009, some major IRC servers were delinked: codice_2, codice_3, codice_4, EFnet's only UK server codice_5, and EFnet's only UK hub codice_6, which were sponsored by Demon Internet.

In September 2010, the two western regions of the network (United States and Canada) merged into the North American region. While the North American and European regions are technically independent of each other, today many issues within EFnet are handled at a global level.

On April 1, 2018, as an April Fools' joke, the 1990s IRC server eris.Berkeley.EDU server was resurrected. Some EFnet admins worked with the Open Computing Facility student group at UC Berkeley for months to resurrect the server for April Fools. Only a very few EFnet staff were aware of the efforts and the server was linked in via a defunct H:line for the (normally) leaf (client-only) server codice_7, bypassing the normal linking procedure. As of 12:30 UTC on April 01 2018, eris.Berkeley.EDU was once again a valid IRC server on the "Eris Free" IRC network and accepted clients. At the same time, efnet.org begin redirecting to erisnet.org.. eris.Berkeley.EDU delinked on April 02 2018 at 19:50 UTC.

EFnet has large variations in rules and policy between different servers as well as the two major regions (EU and NA). Both have their own policy structure, and each region votes on their own server applications. However, central policies are voted upon by the server admin community which is archived for referencing.

Due to EFnet's nature, it has gained recognition over the years for warez, hackers, and DoS attacks.

EFnet has always been known for its lack of IRC services that other IRC networks support (such as NickServ and ChanServ, although it had a NickServ until April 8, 1994). Instead, the "CHANFIX" service was introduced to fix "opless" channels.

Most servers on EFnet run ircd-ratbox with one running ircd-hybrid.

EFnet's channel operators are generally free to run their channels however they see fit without the intervention of IRC operators. IRC ops are primarily there to handle network and server related issues, and rarely get involved with channel-level issues.



</doc>
<doc id="14713" url="https://en.wikipedia.org/wiki?curid=14713" title="Undernet">
Undernet

The Undernet is the fourth largest publicly monitored Internet Relay Chat (IRC) network, c. 2016, with about 19 client servers serving 17,444 users in 6621 channels at any given time.

IRC clients can connect to Undernet via the global round robin irc.undernet.org, the region-specific round robins us.undernet.org and eu.undernet.org, or a specific server from the server list.

Undernet was established in October 1992 by Danny Mitchell, Donald Lambert, and Laurent Demally as an experimental network running a modified version of the EFnet irc2.7 IRCd software, created in an attempt to make it less bandwidth-consumptive and less chaotic, as netsplits and takeovers were starting to plague EFnet. The Undernet IRC daemon became known as "ircu". Undernet was formed at a time when many small IRC networks were being started and subsequently disappearing; however, it managed to grow into one of the largest and oldest IRC networks despite some initial in-fighting and setbacks. For a period in 1994, Undernet was wracked by an ongoing series of flame wars. Again in 2001, it was threatened by automated heavy spamming of its users for potential commercial gain. Undernet survived these periods relatively intact and its popularity continues to the present day.

It is notable as being the first network to utilize timestamping, originally made by Carlo Wood, in the IRC server protocol as a means to curb abuse.

Undernet uses GNUworld to provide X, its channel service bot. X operates on a username basis; a username is independent from a nickname, which cannot be registered on Undernet.

As Undernet limits channel registration to "established channels" or channels with an active userbase, Undernet introduced a version of "ChanFix" (under the nickname "C") designed to work like EFNet's CHANFIX. Its use is to protect unregistered channels. ChanFix tracks channel op usage by username basis and restores ops if channels become opless or are taken over.

Undernet also runs an open proxy scanner. This scans users currently connecting to the network for open WinGate, SOCKS version 4/5, and HTTP proxy servers. IP addresses hosting open proxy servers are automatically G-lined from the network. These changes were put in place after the 2001 Denial-of-service attacks almost destroyed the network and left Undernet without the registered channel service bot for months. In 2010, Undernet also started to g-line Tor exit nodes, instead of assigning those users a cloak like e.g. Quakenet.



</doc>
<doc id="14715" url="https://en.wikipedia.org/wiki?curid=14715" title="DALnet">
DALnet

DALnet is an Internet Relay Chat (IRC) network made up of 33 servers, with a stable population of approximately 8,000 users in about 7,000 channels.

DALnet is accessible by connecting with an IRC client to an active DALnet server on ports 6660 through 6669, and 7000. SSL users can connect on port 6697 as well. The generic round-robin address is .
DALnet was founded in July 1994 by members of the EFnet #startrek channel. This new network was known as "dal's net", after the nickname used by the administrator of the first IRC server on the network, "dalvenjah", taken from the dragon "Dalvenjah Foxfire", in a fantasy novel by Thorarinn Gunnarsson. The network was soon renamed from dal's net to DALnet.

In contrast to other IRC networks of the time, in 1995 DALnet implemented "services", a system that enforced IRC nickname and channel registrations. Traditionally, on IRC, anybody can own a channel or a nickname; if no one is using it, it can be used by anyone who chooses to do so. On DALnet, however, this was no longer the case. This service—which many users saw as a way of firmly establishing their online identities—was a significant factor in DALnet's popularity and afforded the network a distinctive reputation among IRCers. While attempts to implement a similar system had been made before and other networks have since developed registration services of their own, at the time DALnet's successful decision to allow and enforce nickname and channel registration was considered to be unique and even controversial, as it went against established practice.

From 25 users in July 1994, the number of users grew to 1,000 by November 1995, 5,000 by June 1996, 10,000 by December 1996, 50,000 by October 1999, 100,000 in November 2001, and peaked around 142,000 in April 2002, by which time the network had 44 servers. At that point DALnet was one of the four biggest IRC networks.

The network was severely disrupted in late 2002 and early 2003 by distributed denial of service (DDoS) attacks. Added to the DDoS issues was the fact that the owner of twisted.dal.net (the world's largest single IRC server, hosting more than 50,000 clients most of the time) delinked his servers (for personal reasons). The other servers on the network could not absorb the extra client load, leading to users' complete inability to connect to DALnet. The network was first crushed by attacks, and then by its own user base.

It was around this time that DALnet closed many of their channels that were dedicated to serving content such as MP3 files and movies. File transfers were still allowed but not on a large scale. This raised suspicion as to whether DALnet was being targeted by the RIAA, although this was not true, but a precautionary measure.

In 2003, DALnet put up their first anycast servers under the name "The IX Concept", and made irc.dal.net resolve to the anycast IP. Since then, most new client servers linked are anycast.

The main characteristics of DALnet is its ChanServ services which was invented on DALnet in 1995. Along with NickServ it gave a solid ground for usability and security on IRC where users got the ability to register their nicknames and their channels.

DALnet is also developing and running on its own ircd software called Bahamut which is based on ircd-hybrid and Dreamforge and was first live in the early 2000s. The name Bahamut comes from a silver-white dragon with blue eyes standing for protection, wisdom, justice and hope in Dungeons & Dragons.

The network is widely regarded as the most "friendly" of the major IRC networks.



</doc>
<doc id="14716" url="https://en.wikipedia.org/wiki?curid=14716" title="BitchX">
BitchX

BitchX is a free IRC client and has been considered to be the most popular ircII-based IRC client. The initial implementation, written by "Trench" and "HappyCrappy", was a script for the IrcII chat client. It was converted to a program in its own right by panasync (Colten Edwards). BitchX 1.1 final was released in 2004. It is written in C and is a TUI application utilizing ncurses. GTK+ toolkit support has been dropped. It works on all Unix-like operating systems, and is distributed under a BSD license. It is originally based on ircII-EPIC and eventually it was merged into the EPIC IRC client. It supports IPv6, multiple servers and SSL and a subset of UTF-8 (characters contained in ISO-8859-1) with an unofficial patch.

BitchX has frequently been noted to be a popular IRC client for Unix-like systems.

The latest official release is version 1.2

BitchX does not yet support Unicode.

It was known that early versions of BitchX were vulnerable to a denial-of-service attack in that they could be caused to crash by passing specially-crafted strings as arguments to certain IRC commands. This was before format string attacks became a well-known class of vulnerability.

The previous version of BitchX, released in 2004, has security problems allowing remote IRC servers to execute arbitrary code on the client's machine (CVE-2007-3360, CVE-2007-4584).

On April 26, 2009, Slackware removed BitchX from its distribution, citing the numerous unresolved security issues.

The aforementioned vulnerabilities were fixed in the sources for the 1.2 release.




</doc>
<doc id="14717" url="https://en.wikipedia.org/wiki?curid=14717" title="MIRC">
MIRC

mIRC is an Internet Relay Chat (IRC) client for Windows, created in 1995. It is a fully functional chat utility, and its integrated scripting language makes it extensible and versatile.

mIRC has been described as "one of the most popular IRC clients available for Windows." It has been downloaded over 40 million times from CNET's Download.com service. In 2003, Nielsen/NetRatings ranked mIRC among the top ten most popular Internet applications.

mIRC was created by Khaled Mardam-Bey (), a British programmer. He began developing the software in late 1994, and released its first version on February 28, 1995.

Mardam-Bey states that he decided to create mIRC because he felt the first IRC clients for Windows lacked some basic IRC features. He then continued developing it due to the challenge and the fact that people appreciated his work. The author states that its subsequent popularity allowed him to make a living out of mIRC. mIRC is shareware and requires payment for registration after the 30-day evaluation period.

The developer states that version 5.91 is the final one to support 16-bit Windows; 6.35 is the last to support Windows 95, NT 4.0, 98, and ME. The current version supports Windows XP and later.

mIRC has a number of distinguishing features. One is its scripting language which is further developed with each version. The scripting language can be used to make minor changes to the program like custom commands (aliases), but can also be used to completely alter the behavior and appearance of mIRC. Another claimed feature is mIRC's file sharing abilities, via the DCC protocol, featuring a built-in file server.

Starting with mIRC 7.1, released on July 30, 2010, Unicode and IPv6 are supported.

mIRC's abilities and behaviors can be altered and extended using the embedded mIRC scripting language. mIRC includes its own GUI scripting editor, with help that has been described as "extremely detailed".

Due to the level of access the language has to a user's computer — for example, being able to rename and delete files — a number of abusive scripts have been made. One example of abuse was that executed with the $decode identifier which decodes a given encoded string. The issue was reported in August 2001; even five months later, users were still being reported as having fallen prey, tricked into executing commands on their systems which result in "handing control of [their] mIRC over to somebody else". This led to changes being made in mIRC version 6.17: according to the author, $decode is now disabled by default, and various other features which can be considered dangerous are now lockable.


</doc>
<doc id="14718" url="https://en.wikipedia.org/wiki?curid=14718" title="HexChat">
HexChat

HexChat is an Internet Relay Chat client, forked from XChat. It has a choice of a tabbed document interface or tree interface, support for multiple servers, and numerous configuration options. Both command-line and graphical versions were available.

The client is available for Unix-like systems; derivatives with native support for macOS's Aqua interface are also available (see below).

XChat's main source code tree has not received any patches since 2013. In the past, some Linux distributions, such as Fedora, maintained their own versions to fix bugs, keep the program working and able to build, and to resolve security issues.

XChat's fork HexChat, formerly the Windows-specific X-Chat WDK, is installed by default on Linux Mint, and is still maintained, as of August 2018.

The Internet Relay Chat client offers a graphical user interface surrounding the basic chat window. It includes all basic functionality found in most other IRC clients, including nick completion, connecting to multiple servers, secure connections, Client-to-client protocol, Direct Client-to-Client file transfers and chats, and a plugin system for various programming languages (including at least C or C++, Perl, Python, Tcl, Ruby, Lua, CLISP, D, and DMDScript). Plugins allow extending the features and customization of the functionality of XChat.

The default view for the client window is referred to as "tree view", but can be configured for a tabbed interface instead. Tabs change color as text arrives, other users enter or leave channels, or another user addresses the user's nickname. The interface can display clickable operator ("op") commands and others, and allows customization of fonts, event sounds, timestamps, and logging. XChat implements all standard IRC commands (e.g. codice_1 or codice_2), as well as DCC chatting (codice_3), which allows chat to continue if the IRC server is disconnected.

The client runs on Unix-like operating systems, and many GNU/Linux distributions include packages in their repositories.

The main body of XChat code is licensed under the GNU General Public License. However, on August 23, 2004, the official build for Windows had become shareware, and had to be purchased after a 30-day trial period. Previous builds for Windows were removed from the official site. The authors stated that the shareware fee is required due to the excessive amount of time it took to make it compile under Windows. This created controversy as Peter Železný did not have the copyright to all of the code which he was claiming to re-license, as well as linking proprietary shareware enforcement code against GPL-licensed code, making the Windows version of XChat a violation of copyright law and the GPL.

However, since the Unix version of XChat's code is free software under the GPL, several free non-official builds for Windows are being maintained.


XChat has been described as popular, buildable for a variety of platforms. In its heyday it was rated one of the best IRC clients for Linux. "IRC Hacks: 100 Industrial-Strength Tips & Tools" goes into depth explaining its setup, configuration, and advanced features under Unix and macOS.




</doc>
<doc id="14720" url="https://en.wikipedia.org/wiki?curid=14720" title="IRC takeover">
IRC takeover

An IRC channel takeover is an acquisition of IRC channel operator status by someone other than the channel's owner. It has largely been eliminated due to the increased use of services on IRC networks.

The most common variety of channel takeover uses disconnections caused by a netsplit; this is called riding the split. After such mass disconnections, a channel may be left without users, allowing the first rejoining user to recreate the channel and gain operator status. When the servers merge, any pre-existing operators retain their status, allowing the new user to kick out the original operators and take over the channel.

A simple prevention mechanism involves "timestamping" (abbreviated to "TS"), or checking the creation dates of the channels being merged. This was first implemented by Undernet (ircu) and is now common in many IRC servers. If both channels were created at the same time, all user statuses are retained when the two are combined; if one is newer than the other, special statuses are removed from those in the newer channel.

Additionally, a newer protection involving timestamping is used when a server splits away from the main network (when it no longer detects that IRC services are available), it disallows anyone creating a channel to be given operator privileges.

Another popular form of channel takeover abuses nickname collision protection, which keeps two users from having the same nickname at once. A user on one side of a netsplit takes the nickname of a target on the other side of the split; when the servers reconnect, the nicks collide and both users are kicked from the server. The attacker then reconnects or switches nicks in a second client while the target reconnects, and proceeds to jupe (or block) the target's nickname for a period of time.

User timestamping is often used to detect these kinds of attacks in a fashion similar to channel timestamping, with the user who selected that nickname later being kicked from the server. Another protection method, called "nickhold", disallows the use of recently split nicknames. This causes fewer kicks, but causes more inconvenience to users. For this reason, timestamping is generally more common. Some servers, such as ircd-ratbox, do both. IRC services and bots can also protect against such attacks by requiring that a password be supplied to use a certain nick. Users who do not provide a password are killed after a certain amount of time.

Other methods can be used to take over a channel, though they are unrelated to flaws in IRC itself; for example, cracking the computers of channel operators, compromising channel bot shell accounts, or obtaining services passwords through social engineering.

According to "Hypermedia Seduction for Terrorist Recruiting" (2007), exploiting ICMP ping responses from broadcast addresses at multiple hosts sharing an Internet address, and forging the ping packet's return address to match a target machine's address, a single malformed packet sent to the "smurf amplifier" will be echoed to the target machine. This has been used to take over IRC servers.


</doc>
<doc id="14722" url="https://en.wikipedia.org/wiki?curid=14722" title="Irssi">
Irssi

Irssi ( (audio)) is an IRC client program for Linux, FreeBSD, macOS and Microsoft Windows. It was originally written by Timo Sirainen, and released under the terms of the GNU General Public License in January 1999.

Irssi is written in the C programming language and in normal operation uses a text-mode user interface.

According to the developers, Irssi was written from scratch, not based on ircII (like BitchX and epic). This freed the developers from having to deal with the constraints of an existing codebase, allowing them to maintain tighter control over issues such as security and customization. Numerous Perl scripts have been made available for Irssi to customise how it looks and operates. Plugins are available which add encryption and protocols such as ICQ and XMPP.

Irssi may be configured by using its user interface or by manually editing its configuration files, which use a syntax resembling Perl data structures.

Irssi was written primarily to run on Unix-like operating systems, and binaries and packages are available for Gentoo Linux, Debian, Slackware, SUSE (openSUSE), Frugalware, Fedora, FreeBSD, OpenBSD, NetBSD, DragonFly BSD, Solaris, Arch Linux, Ubuntu, NixOS, and others.

Irssi builds and runs on Microsoft Windows under Cygwin, and in 2006, an official Windows standalone build became available.

For the Unix-based macOS, text mode ports are available from the Homebrew, MacPorts, and Fink package managers, and two graphical clients have been written based on Irssi, IrssiX, and MacIrssi. The Cocoa client Colloquy was previously based on Irssi, but it now uses its own IRC core implementation.




</doc>
<doc id="14724" url="https://en.wikipedia.org/wiki?curid=14724" title="Intellectual property">
Intellectual property

Intellectual property (IP) is a category of property that includes intangible creations of the human intellect. There are many types of intellectual property, and some countries recognize more than others. The most well-known types are copyrights, patents, trademarks, and trade secrets. Early precursors to some types of intellectual property existed in societies such as Ancient Rome, but the modern concept of intellectual property developed in England in the 17th and 18th centuries. The term "intellectual property" began to be used in the 19th century, though it was not until the late 20th century that intellectual property became commonplace in the majority of the world's legal systems.

The main purpose of intellectual property law is to encourage the creation of a wide variety of intellectual goods. To achieve this, the law gives people and businesses property rights to the information and intellectual goods they create, usually for a limited period of time. This gives economic incentive for their creation, because it allows people to profit from the information and intellectual goods they create. These economic incentives are expected to stimulate innovation and contribute to the technological progress of countries, which depends on the extent of protection granted to innovators.

The intangible nature of intellectual property presents difficulties when compared with traditional property like land or goods. Unlike traditional property, intellectual property is "indivisible", since an unlimited number of people can "consume" an intellectual good without it being depleted. Additionally, investments in intellectual goods suffer from problems of appropriation: a landowner can surround their land with a robust fence and hire armed guards to protect it, but a producer of information or literature can usually do very little to stop their first buyer from replicating it and selling it at a lower price. Balancing rights so that they are strong enough to encourage the creation of intellectual goods but not so strong that they prevent the goods' wide use is the primary focus of modern intellectual property law.

The Statute of Monopolies (1624) and the British Statute of Anne (1710) are seen as the origins of patent law and copyright respectively, firmly establishing the concept of intellectual property.

"Literary property" was the term predominantly used in the British legal debates of the 1760s and 1770s over the extent to which authors and publishers of works also had rights deriving from the common law of property ("Millar v Taylor" (1769), "Hinton v Donaldson" (1773), "Donaldson v Becket" (1774). The first known use of the term "intellectual property" dates to this time, when a piece published in the "Monthly Review" in 1769 used the phrase. The first clear example of modern usage goes back as early as 1808, when it was used as a heading title in a collection of essays.

The German equivalent was used with the founding of the North German Confederation whose constitution granted legislative power over the protection of intellectual property ("Schutz des geistigen Eigentums") to the confederation. When the administrative secretariats established by the Paris Convention (1883) and the Berne Convention (1886) merged in 1893, they located in Berne, and also adopted the term intellectual property in their new combined title, the United International Bureaux for the Protection of Intellectual Property.

The organization subsequently relocated to Geneva in 1960 and was succeeded in 1967 with the establishment of the World Intellectual Property Organization (WIPO) by treaty as an agency of the United Nations. According to legal scholar Mark Lemley, it was only at this point that the term really began to be used in the United States (which had not been a party to the Berne Convention), and it did not enter popular usage there until passage of the Bayh-Dole Act in 1980.

"The history of patents does not begin with inventions, but rather with royal grants by Queen Elizabeth I (1558–1603) for monopoly privileges. Approximately 200 years after the end of Elizabeth's reign, however, a patent represents a legal right obtained by an inventor providing for exclusive control over the production and sale of his mechanical or scientific invention. demonstrating the evolution of patents from royal prerogative to common-law doctrine."

The term can be found used in an October 1845 Massachusetts Circuit Court ruling in the patent case "Davoll et al. v. Brown.", in which Justice Charles L. Woodbury wrote that "only in this way can we protect intellectual property, the labors of the mind, productions and interests are as much a man's own...as the wheat he cultivates, or the flocks he rears." The statement that "discoveries are..property" goes back earlier. Section 1 of the French law of 1791 stated, "All new discoveries are the property of the author; to assure the inventor the property and temporary enjoyment of his discovery, there shall be delivered to him a patent for five, ten or fifteen years." In Europe, French author A. Nion mentioned "propriété intellectuelle" in his "Droits civils des auteurs, artistes et inventeurs", published in 1846.

Until recently, the purpose of intellectual property law was to give as little protection as possible in order to encourage innovation. Historically, therefore, they were granted only when they were necessary to encourage invention, limited in time and scope. This is mainly as a result of knowledge being traditionally viewed as a public good, in order to allow its extensive dissemination and improvement thereof.

The concept's origins can potentially be traced back further. Jewish law includes several considerations whose effects are similar to those of modern intellectual property laws, though the notion of intellectual creations as property does not seem to exist – notably the principle of Hasagat Ge'vul (unfair encroachment) was used to justify limited-term publisher (but not author) copyright in the 16th century. In 500 BCE, the government of the Greek state of Sybaris offered one year's patent "to all who should discover any new refinement in luxury".

According to Jean-Frédéric Morin, "the global intellectual property regime is currently in the midst of a paradigm shift". Indeed, up until the early 2000s the global IP regime used to be dominated by high standards of protection characteristic of IP laws from Europe or the United States, with a vision that uniform application of these standards over every country and to several fields with little consideration over social, cultural or environmental values or of the national level of economic development. Morin argues that "the emerging discourse of the global IP regime advocates for greater policy flexibility and greater access to knowledge, especially for developing countries." Indeed, with the Development Agenda adopted by WIPO in 2007, a set of 45 recommendations to adjust WIPO's activities to the specific needs of developing countries and aim to reduce distortions especially on issues such as patients’ access to medicines, Internet users’ access to information, farmers’ access to seeds, programmers’ access to source codes or students’ access to scientific articles. However, this paradigm shift has not yet manifested itself in concrete legal reforms at the international level.

Similarly, it is based on these background that the Trade-Related Aspects of Intellectual Property Rights (TRIPS) agreement requires members of the WTO to set minimum standards of legal protection, but its objective to have a “one-fits-all” protection law on Intellectual Property has been viewed with controversies regarding differences in the development level of countries. Despite the controversy, the agreement has extensively incorporated intellectual property rights into the global trading system for the first time in 1995, and has prevailed as the most comprehensive agreement reached by the world.

Intellectual property rights include patents, copyright, industrial design rights, trademarks, plant variety rights, trade dress, geographical indications, and in some jurisdictions trade secrets. There are also more specialized or derived varieties of "sui generis" exclusive rights, such as circuit design rights (called mask work rights in the US), supplementary protection certificates for pharmaceutical products (after expiry of a patent protecting them), and database rights (in European law). The term "industrial property" is sometimes used to refer to a large subset of intellectual property rights including patents, trademarks, industrial designs, utility models, service marks, trade names, and geographical indications.

A patent is a form of right granted by the government to an inventor or their successor-in-title, giving the owner the right to exclude others from making, using, selling, offering to sell, and importing an invention for a limited period of time, in exchange for the public disclosure of the invention. An invention is a solution to a specific technological problem, which may be a product or a process and generally has to fulfill three main requirements: it has to be new, not obvious and there needs to be an industrial applicability. To enrich the body of knowledge and stimulate innovation, it is an obligation for patent owners to disclose valuable information about their inventions to the public.

A copyright gives the creator of an original work exclusive rights to it, usually for a limited time. Copyright may apply to a wide range of creative, intellectual, or artistic forms, or "works". Copyright does not cover ideas and information themselves, only the form or manner in which they are expressed.

An industrial design right (sometimes called "design right" or "design patent") protects the visual design of objects that are not purely utilitarian. An industrial design consists of the creation of a shape, configuration or composition of pattern or color, or combination of pattern and color in three-dimensional form containing aesthetic value. An industrial design can be a two- or three-dimensional pattern used to produce a product, industrial commodity or handicraft. Generally speaking, it is what makes a product look appealing, and as such, it increases the commercial value of goods.

Plant breeders' rights or plant variety rights are the rights to commercially use a new variety of a plant. The variety must amongst others be novel and distinct and for registration the evaluation of propagating material of the variety is considered.

A trademark is a recognizable sign, design or expression which distinguishes products or services of a particular trader from similar products or services of other traders.

Trade dress is a legal term of art that generally refers to characteristics of the visual and aesthetic appearance of a product or its packaging (or even the design of a building) that signify the source of the product to consumers.

A trade secret is a formula, practice, process, design, instrument, pattern, or compilation of information which is not generally known or reasonably ascertainable, by which a business can obtain an economic advantage over competitors and customers. There is no formal government protection granted; each business must take measures to guard its own trade secrets (e.g., Formula of its soft drinks is a trade secret for Coca-Cola.)

The main purpose of intellectual property law is to encourage the creation of a wide variety of intellectual goods for consumers. To achieve this, the law gives people and businesses property rights to the information and intellectual goods they create, usually for a limited period of time. Because they can then profit from them, this gives economic incentive for their creation. The intangible nature of intellectual property presents difficulties when compared with traditional property like land or goods. Unlike traditional property, intellectual property is indivisible – an unlimited number of people can "consume" an intellectual good without it being depleted. Additionally, investments in intellectual goods suffer from problems of appropriation – while a landowner can surround their land with a robust fence and hire armed guards to protect it, a producer of information or an intellectual good can usually do very little to stop their first buyer from replicating it and selling it at a lower price. Balancing rights so that they are strong enough to encourage the creation of information and intellectual goods but not so strong that they prevent their wide use is the primary focus of modern intellectual property law.

By exchanging limited exclusive rights for disclosure of inventions and creative works, society and the patentee/copyright owner mutually benefit, and an incentive is created for inventors and authors to create and disclose their work. Some commentators have noted that the objective of intellectual property legislators and those who support its implementation appears to be "absolute protection". "If some intellectual property is desirable because it encourages innovation, they reason, more is better. The thinking is that creators will not have sufficient incentive to invent unless they are legally entitled to capture the full social value of their inventions". This absolute protection or full value view treats intellectual property as another type of "real" property, typically adopting its law and rhetoric. Other recent developments in intellectual property law, such as the America Invents Act, stress international harmonization. Recently there has also been much debate over the desirability of using intellectual property rights to protect cultural heritage, including intangible ones, as well as over risks of commodification derived from this possibility. The issue still remains open in legal scholarship.

These exclusive rights allow owners of intellectual property to benefit from the property they have created, providing a financial incentive for the creation of an investment in intellectual property, and, in case of patents, pay associated research and development costs. In the United States Article I Section 8 Clause 8 of the Constitution, commonly called the Patent and Copyright Clause, reads; "The Congress shall have power 'To promote the progress of science and useful arts, by securing for limited times to authors and inventors the exclusive right to their respective writings and discoveries.'" ”Some commentators, such as David Levine and Michele Boldrin, dispute this justification.

In 2013 the United States Patent & Trademark Office approximated that the worth of intellectual property to the U.S. economy is more than US $5 trillion and creates employment for an estimated 18 million American people. The value of intellectual property is considered similarly high in other developed nations, such as those in the European Union. In the UK, IP has become a recognised asset class for use in pension-led funding and other types of business finance. However, in 2013, the UK Intellectual Property Office stated: "There are millions of intangible business assets whose value is either not being leveraged at all, or only being leveraged inadvertently".

The WIPO treaty and several related international agreements underline that the protection of intellectual property rights is essential to maintaining economic growth. The "WIPO Intellectual Property Handbook" gives two reasons for intellectual property laws:
One is to give statutory expression to the moral and economic rights of creators in their creations and the rights of the public in access to those creations. The second is to promote, as a deliberate act of Government policy, creativity and the dissemination and application of its results and to encourage fair trading which would contribute to economic and social development.

The Anti-Counterfeiting Trade Agreement (ACTA) states that "effective enforcement of intellectual property rights is critical to sustaining economic growth across all industries and globally".

Economists estimate that two-thirds of the value of large businesses in the United States can be traced to intangible assets. "IP-intensive industries" are estimated to generate 72 percent more value added (price minus material cost) per employee than "non-IP-intensive industries".

A joint research project of the WIPO and the United Nations University measuring the impact of IP systems on six Asian countries found "a positive correlation between the strengthening of the IP system and subsequent economic growth."

According to Article 27 of the Universal Declaration of Human Rights, "everyone has the right to the protection of the moral and material interests resulting from any scientific, literary or artistic production of which he is the author". Although the relationship between intellectual property and human rights is a complex one, there are moral arguments for intellectual property.

The arguments that justify intellectual property fall into three major categories. Personality theorists believe intellectual property is an extension of an individual. Utilitarians believe that intellectual property stimulates social progress and pushes people to further innovation. Lockeans argue that intellectual property is justified based on deservedness and hard work.

Various moral justifications for private property can be used to argue in favor of the morality of intellectual property, such as:


Lysander Spooner (1855) argues "that a man has a natural and absolute right—and if a natural and absolute, then necessarily a perpetual, right—of property, in the ideas, of which he is the discoverer or creator; that his right of property, in ideas, is intrinsically the same as, and stands on identically the same grounds with, his right of property in material things; that no distinction, of principle, exists between the two cases".

Writer Ayn Rand argued in her book "" that the protection of intellectual property is essentially a moral issue. The belief is that the human mind itself is the source of wealth and survival and that all property at its base is intellectual property. To violate intellectual property is therefore no different morally than violating other property rights which compromises the very processes of survival and therefore constitutes an immoral act.

Violation of intellectual property rights, called "infringement" with respect to patents, copyright, and trademarks, and "misappropriation" with respect to trade secrets, may be a breach of civil law or criminal law, depending on the type of intellectual property involved, jurisdiction, and the nature of the action.

As of 2011 trade in counterfeit copyrighted and trademarked works was a $600 billion industry worldwide and accounted for 5–7% of global trade.

Patent infringement typically is caused by using or selling a patented invention without permission from the patent holder. The scope of the patented invention or the extent of protection is defined in the claims of the granted patent. There is safe harbor in many jurisdictions to use a patented invention for research. This safe harbor does not exist in the US unless the research is done for purely philosophical purposes, or in order to gather data in order to prepare an application for regulatory approval of a drug. In general, patent infringement cases are handled under civil law (e.g., in the United States) but several jurisdictions incorporate infringement in criminal law also (for example, Argentina, China, France, Japan, Russia, South Korea).

Copyright infringement is reproducing, distributing, displaying or performing a work, or to make derivative works, without permission from the copyright holder, which is typically a publisher or other business representing or assigned by the work's creator. It is often called "piracy". While copyright is created the instant a work is fixed, generally the copyright holder can only get money damages if the owner registers the copyright. Enforcement of copyright is generally the responsibility of the copyright holder. The ACTA trade agreement, signed in May 2011 by the United States, Japan, Switzerland, and the EU, and which has not entered into force, requires that its parties add criminal penalties, including incarceration and fines, for copyright and trademark infringement, and obligated the parties to actively police for infringement. There are limitations and exceptions to copyright, allowing limited use of copyrighted works, which does not constitute infringement. Examples of such doctrines are the fair use and fair dealing doctrine.

Trademark infringement occurs when one party uses a trademark that is identical or confusingly similar to a trademark owned by another party, in relation to products or services which are identical or similar to the products or services of the other party. In many countries, a trademark receives protection without registration, but registering a trademark provides legal advantages for enforcement. Infringement can be addressed by civil litigation and, in several jurisdictions, under criminal law.

Trade secret misappropriation is different from violations of other intellectual property laws, since by definition trade secrets are secret, while patents and registered copyrights and trademarks are publicly available. In the United States, trade secrets are protected under state law, and states have nearly universally adopted the Uniform Trade Secrets Act. The United States also has federal law in the form of the Economic Espionage Act of 1996 (), which makes the theft or misappropriation of a trade secret a federal crime. This law contains two provisions criminalizing two sorts of activity. The first, , criminalizes the theft of trade secrets to benefit foreign powers. The second, , criminalizes their theft for commercial or economic purposes. (The statutory penalties are different for the two offenses.) In Commonwealth common law jurisdictions, confidentiality and trade secrets are regarded as an equitable right rather than a property right but penalties for theft are roughly the same as in the United States.

Criticism of the term "intellectual property" ranges from discussing its vagueness and abstract overreach to direct contention to the semantic validity of using words like "property" and "rights" in fashions that contradict practice and law. Many detractors think this term specially serves the doctrinal agenda of parties opposing reform in the public interest or otherwise abusing related legislations; and that it disallows intelligent discussion about specific and often unrelated aspects of copyright, patents, trademarks, etc.

Free Software Foundation founder Richard Stallman argues that, although the term "intellectual property" is in wide use, it should be rejected altogether, because it "systematically distorts and confuses these issues, and its use was and is promoted by those who gain from this confusion". He claims that the term "operates as a catch-all to lump together disparate laws [which] originated separately, evolved differently, cover different activities, have different rules, and raise different public policy issues" and that it creates a "bias" by confusing these monopolies with ownership of limited physical things, likening them to "property rights". Stallman advocates referring to copyrights, patents and trademarks in the singular and warns against abstracting disparate laws into a collective term. He argues that "to avoid spreading unnecessary bias and confusion, it is best to adopt a firm policy not to speak or even think in terms of 'intellectual property'."

Similarly, economists Boldrin and Levine prefer to use the term "intellectual monopoly" as a more appropriate and clear definition of the concept, which they argue, is very dissimilar from property rights. They further argued that “stronger patents do little or nothing to encourage innovation”, mainly explained by its tendency to create market monopolies, thereby restricting further innovations and technology transfer.

On the assumption that intellectual property rights are actual rights, Stallman says that this claim does not live to the historical intentions behind these laws, which in the case of copyright served as a censorship system, and later on, a regulatory model for the printing press that may have benefited authors incidentally, but never interfered with the freedom of average readers. Still referring to copyright, he cites legal literature such as the United States Constitution and case law to demonstrate that the law is meant to be an optional and experimental bargain to temporarily trade property rights and free speech for public, not private, benefits in the form of increased artistic production and knowledge. He mentions that "if copyright were a natural right nothing could justify terminating this right after a certain period of time".

Law professor, writer and political activist Lawrence Lessig, along with many other copyleft and free software activists, has criticized the implied analogy with physical property (like land or an automobile). They argue such an analogy fails because physical property is generally rivalrous while intellectual works are non-rivalrous (that is, if one makes a copy of a work, the enjoyment of the copy does not prevent enjoyment of the original). Other arguments along these lines claim that unlike the situation with tangible property, there is no natural scarcity of a particular idea or information: once it exists at all, it can be re-used and duplicated indefinitely without such re-use diminishing the original. Stephan Kinsella has objected to "intellectual property" on the grounds that the word "property" implies scarcity, which may not be applicable to ideas.

Entrepreneur and politician Rickard Falkvinge and hacker Alexandre Oliva have independently compared George Orwell's fictional dialect Newspeak to the terminology used by intellectual property supporters as a linguistic weapon to shape public opinion regarding copyright debate and DRM.

In civil law jurisdictions, intellectual property has often been referred to as intellectual rights, traditionally a somewhat broader concept that has included moral rights and other personal protections that cannot be bought or sold. Use of the term "intellectual rights" has declined since the early 1980s, as use of the term "intellectual property" has increased.

Alternative terms "monopolies on information" and "intellectual monopoly" have emerged among those who argue against the "property" or "intellect" or "rights" assumptions, notably Richard Stallman. The backronyms "intellectual protectionism" and "intellectual poverty", whose initials are also "IP", have found supporters as well, especially among those who have used the backronym "digital restrictions management".

The argument that an intellectual property right should (in the interests of better balancing of relevant private and public interests) be termed an "intellectual monopoly privilege" (IMP) has been advanced by several academics including Birgitte Andersen and Thomas Alured Faunce.

Some critics of intellectual property, such as those in the free culture movement, point at intellectual monopolies as harming health (in the case of pharmaceutical patents), preventing progress, and benefiting concentrated interests to the detriment of the masses, and argue that the public interest is harmed by ever-expansive monopolies in the form of copyright extensions, software patents, and business method patents. More recently scientists and engineers are expressing concern that patent thickets are undermining technological development even in high-tech fields like nanotechnology.

Petra Moser has asserted that historical analysis suggests that intellectual property laws may harm innovation:
Overall, the weight of the existing historical evidence suggests that patent policies, which grant strong intellectual property rights to early generations of inventors, may discourage innovation. On the contrary, policies that encourage the diffusion of ideas and modify patent laws to facilitate entry and encourage competition may be an effective mechanism to encourage innovation.

In support of that argument, Jörg Baten, Nicola Bianchi and Petra Moser find historical evidence that especially compulsory licensing – which allows governments to license patents without the consent of patent-owners – encouraged invention in Germany in the early 20th century by increasing the threat of competition in fields with low pre-existing levels of competition.

Peter Drahos notes, "Property rights confer authority over resources. When authority is granted to the few over resources on which many depend, the few gain power over the goals of the many. This has consequences for both political and economic freedoms with in a society."

The World Intellectual Property Organization (WIPO) recognizes that conflicts may exist between the respect for and implementation of current intellectual property systems and other human rights. In 2001 the UN Committee on Economic, Social and Cultural Rights issued a document called "Human rights and intellectual property" that argued that intellectual property tends to be governed by economic goals when it should be viewed primarily as a social product; in order to serve human well-being, intellectual property systems must respect and conform to human rights laws. According to the Committee, when systems fail to do so they risk infringing upon the human right to food and health, and to cultural participation and scientific benefits. In 2004 the General Assembly of WIPO adopted "The Geneva Declaration on the Future of the World Intellectual Property Organization" which argues that WIPO should "focus more on the needs of developing countries, and to view IP as one of many tools for development—not as an end in itself".

Ethical problems are most pertinent when socially valuable goods like life-saving medicines are given IP protection. While the application of IP rights can allow companies to charge higher than the marginal cost of production in order to recoup the costs of research and development, the price may exclude from the market anyone who cannot afford the cost of the product, in this case a life-saving drug. "An IPR driven regime is therefore not a regime that is conductive to the investment of R&D of products that are socially valuable to predominately poor populations".

Libertarians have differing views on intellectual property. Stephan Kinsella, an anarcho-capitalist on the right-wing of libertarianism, argues against intellectual property because allowing property rights in ideas and information creates artificial scarcity and infringes on the right to own tangible property. Kinsella uses the following scenario to argue this point:
[I]magine the time when men lived in caves. One bright guy—let's call him Galt-Magnon—decides to build a log cabin on an open field, near his crops. To be sure, this is a good idea, and others notice it. They naturally imitate Galt-Magnon, and they start building their own cabins. But the first man to invent a house, according to IP advocates, would have a right to prevent others from building houses on their own land, with their own logs, or to charge them a fee if they do build houses. It is plain that the innovator in these examples becomes a partial owner of the tangible property (e.g., land and logs) of others, due not to first occupation and use of that property (for it is already owned), but due to his coming up with an idea. Clearly, this rule flies in the face of the first-user homesteading rule, arbitrarily and groundlessly overriding the very homesteading rule that is at the foundation of all property rights.

Thomas Jefferson once said in a letter to Isaac McPherson on August 13, 1813:
"If nature has made any one thing less susceptible than all others of exclusive property, it is the action of the thinking power called an idea, which an individual may exclusively possess as long as he keeps it to himself; but the moment it is divulged, it forces itself into the possession of every one, and the receiver cannot dispossess himself of it. Its peculiar character, too, is that no one possesses the less, because every other possesses the whole of it. He who receives an idea from me, receives instruction himself without lessening mine; as he who lights his taper at mine, receives light without darkening me."

In 2005 the RSA launched the Adelphi Charter, aimed at creating an international policy statement to frame how governments should make balanced intellectual property law.

Another aspect of current U.S. Intellectual Property legislation is its focus on individual and joint works; thus, copyright protection can only be obtained in 'original' works of authorship.

Other criticism of intellectual property law concerns the expansion of intellectual property, both in duration and in scope.

In addition, as scientific knowledge has expanded and allowed new industries to arise in fields such as biotechnology and nanotechnology, originators of technology have sought IP protection for the new technologies. Patents have been granted for living organisms, and in the United States, certain living organisms have been patentable for over a century.

The increase in terms of protection is particularly seen in relation to copyright, which has recently been the subject of serial extensions in the United States and in Europe. With no need for registration or copyright notices, this is thought to have led to an increase in orphan works (copyrighted works for which the copyright owner cannot be contacted), a problem that has been noticed and addressed by governmental bodies around the world.

Also with respect to copyright, the American film industry helped to change the social construct of intellectual property via its trade organization, the Motion Picture Association of America. In amicus briefs in important cases, in lobbying before Congress, and in its statements to the public, the MPAA has advocated strong protection of intellectual-property rights. In framing its presentations, the association has claimed that people are entitled to the property that is produced by their labor. Additionally Congress's awareness of the position of the United States as the world's largest producer of films has made it convenient to expand the conception of intellectual property. These doctrinal reforms have further strengthened the industry, lending the MPAA even more power and authority.

The growth of the Internet, and particularly distributed search engines like Kazaa and Gnutella, have represented a challenge for copyright policy. The Recording Industry Association of America, in particular, has been on the front lines of the fight against copyright infringement, which the industry calls "piracy". The industry has had victories against some services, including a highly publicized case against the file-sharing company Napster, and some people have been prosecuted for sharing files in violation of copyright. The electronic age has seen an increase in the attempt to use software-based digital rights management tools to restrict the copying and use of digitally based works. Laws such as the Digital Millennium Copyright Act have been enacted that use criminal law to prevent any circumvention of software used to enforce digital rights management systems. Equivalent provisions, to prevent circumvention of copyright protection have existed in EU for some time, and are being expanded in, for example, Article 6 and 7 the Copyright Directive. Other examples are Article 7 of the Software Directive of 1991 (91/250/EEC), and the Conditional Access Directive of 1998 (98/84/EEC). This can hinder legal uses, affecting public domain works, limitations and exceptions to copyright, or uses allowed by the copyright holder. Some copyleft licenses, like GNU GPL 3, are designed to counter that. Laws may permit circumvention under specific conditions like when it is necessary to achieve interoperability with the circumventor's program, or for accessibility reasons; however, distribution of circumvention tools or instructions may be illegal.

In the context of trademarks, this expansion has been driven by international efforts to harmonise the definition of "trademark", as exemplified by the Agreement on Trade-Related Aspects of Intellectual Property Rights ratified in 1994, which formalized regulations for IP rights that had been handled by common law, or not at all, in member states. Pursuant to TRIPs, any sign which is "capable of distinguishing" the products or services of one business from the products or services of another business is capable of constituting a trademark.

Intellectual property has become a core tool in corporate tax planning and tax avoidance. IP is a key component of the leading multinational tax avoidance base erosion and profit shifting (BEPS) tools, which the OECD estimates costs $100–240 billion in lost annual tax revenues, and includes:
In 2017–2018, both the U.S. and the EU Commission simultaneously decided to depart from the OECD BEPS Project timetable, which was set up in 2013 to combat IP BEPS tax tools like the above, and launch their own anti-IP BEPS tax regimes:


The departure of the U.S. and EU Commission from the OECD BEPS Project process, is attributed to frustrations with the rise in IP as a key BEPS tax tool, creating intangible assets, which are then turned into royalty payment BEPS schemes (double Irish), and/or capital allowance BEPS schemes (capital allowances for intangibles). In contrast, the OECD has spent years developing and advocating intellectual property as a legal and a GAAP accounting concept.

The EU Commission's €13 billion fine of Apple's pre-2015 double Irish IP BEPS tax scheme, is the largest corporate tax fine in history.





</doc>
<doc id="14726" url="https://en.wikipedia.org/wiki?curid=14726" title="Great Famine (Ireland)">
Great Famine (Ireland)

The Great Famine ( ), also known as the Great Hunger and sometimes referred to as the Irish Potato Famine mostly outside Ireland, was a period of mass starvation and disease in Ireland from 1845 to 1849. With the most severely affected areas in the west and south of Ireland, where the Irish language was dominant, the period was contemporaneously known in Irish as , loosely translated as the "hard times" (or literally, "The Bad Life"). The worst year of the period was 1847, known as "Black '47". During the famine, about one million people died and a million more emigrated, causing Ireland's population to fall by between 20% and 25%.
The proximate cause of the famine was a natural event, a potato blight, which infected potato crops throughout Europe during the 1840s, also causing some 100,000 deaths outside Ireland and influencing much of the unrest in the widespread European Revolutions of 1848. From 1846, the impact of the blight was exacerbated by the Whig government's economic policy of laissez-faire capitalism. Longer-term causes include the system of absentee landlordism, and single-crop dependence.

The famine was a watershed in the history of Ireland, which from 1801 to 1922 was ruled directly by Westminster as part of the United Kingdom of Great Britain and Ireland. The famine and its effects permanently changed the island's demographic, political, and cultural landscape, producing an estimated two million refugees and spurring a century-long population decline. For both the native Irish and those in the resulting diaspora, the famine entered folk memory. The strained relations between many Irish and their government soured further because of the famine, heightening ethnic and sectarian tensions and boosting Irish nationalism and republicanism in Ireland and among Irish emigrants in the United States and elsewhere. Author explains, the "famine became part of the long story of betrayal & exploitation which led to the growing movement in Ireland for independence".
The potato blight returned to Europe in 1879, but by that point the Land War, described as one of the largest agrarian movements to take place in 19th-century Europe, had begun in Ireland. The movement, organized by the Land League, continued the political campaign for the Three Fs, issued in 1850 by the Tenant Right League during the Great Famine. When the potato blight returned in the 1879 famine the League boycotted "notorious landlords" and its members physically blocked evictions of farmers; the consequent reduction in homelessness and house demolition resulted in a drastic reduction in the number of deaths.

Since the Acts of Union in January 1801, Ireland had been part of the United Kingdom. Executive power lay in the hands of the Lord Lieutenant of Ireland and Chief Secretary for Ireland, who were appointed by the British government. Ireland sent 105 members of parliament to the House of Commons of the United Kingdom, and Irish representative peers elected 28 of their own number to sit for life in the House of Lords. Between 1832 and 1859, 70% of Irish representatives were landowners or the sons of landowners.

In the 40 years that followed the union, successive British governments grappled with the problems of governing a country which had, as Benjamin Disraeli put it in 1844, "a starving population, an absentee aristocracy, an alien established Protestant church, and in addition the weakest executive in the world". One historian calculated that, between 1801 and 1845, there had been 114 commissions and 61 special committees enquiring into the state of Ireland, and that "without exception their findings prophesied disaster; Ireland was on the verge of starvation, her population rapidly increasing, three-quarters of her labourers unemployed, housing conditions appalling and the standard of living unbelievably low".

Lectures printed in 1847 by John Hughes, Bishop of New York, are a contemporary exploration into the antecedent causes, particularly the political climate, in which the Irish famine occurred.

During the 18th century, the "middleman system" for managing landed property was introduced. Rent collection was left in the hands of the landlords' agents, or middlemen. This assured the landlord of a regular income, and relieved them of direct responsibility, while leaving tenants open to exploitation by the middlemen.

Catholics, the bulk of whom lived in conditions of poverty and insecurity despite Catholic emancipation in 1829, made up 80% of the population. At the top of the "social pyramid" was the "ascendancy class", the English and Anglo-Irish families who owned most of the land and held more or less unchecked power over their tenants. Some of their estates were vast; for example, the Earl of Lucan owned more than . Many of these absentee landlords lived in England. The rent revenue—collected from "impoverished tenants" who were paid minimal wages to raise crops and livestock for export—was mostly sent to England.

In 1843, the British Government considered that the land question in Ireland was the root cause of disaffection in the country. They established a Royal Commission, chaired by the Earl of Devon, to enquire into the laws regarding the occupation of land. Daniel O'Connell described this commission as "perfectly one-sided", being composed of landlords, with no tenant representation.

In February 1845, Devon reported:
It would be impossible adequately to describe the privations which they [the Irish labourer and his family] habitually and silently endure ... in many districts their only food is the potato, their only beverage water ... their cabins are seldom a protection against the weather ... a bed or a blanket is a rare luxury ... and nearly in all their pig and a manure heap constitute their only property.

The Commissioners concluded they could not "forbear expressing our strong sense of the patient endurance which the labouring classes have exhibited under sufferings greater, we believe, than the people of any other country in Europe have to sustain". The Commission stated that bad relations between landlord and tenant were principally responsible. There was no hereditary loyalty, feudal tie, or mitigating tradition of paternalism as existed in England (Ireland was a conquered country). The Earl of Clare observed of landlords that "confiscation is their common title". According to the historian Cecil Woodham-Smith, landlords regarded the land as a source of income, from which as much as possible was to be extracted. With the Irish "brooding over their discontent in sullen indignation" (in the words of the Earl of Clare), the landlords largely viewed the countryside as a hostile place in which to live. Some landlords visited their property only once or twice in a lifetime, if ever. The rents from Ireland were generally spent elsewhere; an estimated £6,000,000 was remitted out of Ireland in 1842.

The ability of middlemen was measured by the rent income they could contrive to extract from tenants. They were described in evidence before the Commission as "land sharks", "bloodsuckers", and "the most oppressive species of tyrant that ever lent assistance to the destruction of a country". The middlemen leased large tracts of land from the landlords on long leases with fixed rents, which they sublet as they saw fit. They would split a holding into smaller and smaller parcels so as to increase the amount of rent they could obtain. Tenants could be evicted for reasons such as non-payment of rents (which were high), or a landlord's decision to raise sheep instead of grain crops. A cottier paid his rent by working for the landlord.

As any improvement made on a holding by a tenant became the property of the landlord when the lease expired or was terminated, the incentive to make improvements was limited. Most tenants had no security of tenure on the land; as tenants "at will", they could be turned out whenever the landlord chose. The only exception to this arrangement was in Ulster where, under a practice known as "tenant right", a tenant was compensated for any improvement they made to their holding. According to Woodham-Smith, the commission stated that "the superior prosperity and tranquility of Ulster, compared with the rest of Ireland, were due to tenant right".

Landlords in Ireland often used their powers without compunction, and tenants lived in dread of them. Woodham-Smith writes that, in these circumstances, "industry and enterprise were extinguished and a peasantry created which was one of the most destitute in Europe".

In 1845, 24% of all Irish tenant farms were of 0.4–2 hectares (1–5 acres) in size, while 40% were of 2–6 hectares (5–15 acres). Holdings were so small that no crop other than potatoes would suffice to feed a family. Shortly before the famine, the British government reported that poverty was so widespread that one-third of all Irish small holdings could not support the tenant families after rent was paid; the families survived only by earnings as seasonal migrant labour in England and Scotland. Following the famine, reforms were implemented making it illegal to further divide land holdings.

The 1841 census showed a population of just over eight million. Two-thirds of those depended on agriculture for their survival, but they rarely received a working wage. They had to work for their landlords in return for the patch of land they needed to grow enough food for their own families. This was the system which forced Ireland and its peasantry into monoculture, since only the potato could be grown in sufficient quantity. The rights to a plot of land in Ireland could mean the difference between life and death in the early 19th century.

The potato was introduced to Ireland as a garden crop of the gentry. The potato was not popular at first; however, after an unusual promotion campaign that was supported by landowners and members of royalty, who wanted their tenants to plant and eat the crop, it rose in popularity. By the late 17th century, it had become widespread as a supplementary rather than a principal food; the main diet was still based on butter, milk, and grain products. By 1800 to 1820, the potato became a staple of the poor, especially in winter. Furthermore, a disproportionate share of the potatoes grown in Ireland were of a single variety, the Irish Lumper.

With the expansion of the economy between 1760 and 1815, the potato was increasingly adopted by the people and became a staple food year round for farmers. The widespread dependency on this single crop, and the lack of genetic variability among the potato plants in Ireland and Europe (a monoculture), were two of the reasons why the emergence of "Phytophthora infestans" had such devastating effects in Ireland and in similar areas of Europe.

Potatoes were essential to the development of the cottier system; they supported an extremely cheap workforce, but at the cost of lower living standards. For the labourer, "a potato wage" shaped the expanding agrarian economy.

The potato was also used extensively as a fodder crop for livestock immediately prior to the famine. Approximately 33% of production, amounting to , was normally used in this way.

Prior to the arrival in Ireland of the disease "Phytophthora infestans", commonly known as "blight", only two main potato plant diseases had been identified. One was called "dry rot" or "taint", and the other was a virus known popularly as "curl". "Phytophthora infestans" is an oomycete (a variety of parasitic, non-photosynthetic organisms closely related to brown algae, and not a fungus).

In 1851, the Census of Ireland Commissioners recorded 24 failures of the potato crop going back to 1728, of varying severity. General crop failures, through disease or frost, were recorded in 1739, 1740, 1770, 1800, and 1807. In 1821 and 1822, the potato crop failed in Munster and Connaught. In 1830 and 1831, Mayo, Donegal, and Galway suffered likewise. In 1832, 1833, 1834, and 1836, dry rot and curl caused serious losses, and in 1835 the potato failed in Ulster. Widespread failures throughout Ireland occurred in 1836, 1837, 1839, 1841, and 1844. According to Woodham-Smith, "the unreliability of the potato was an accepted fact in Ireland".

How and when the blight "Phytophthora infestans" arrived in Europe is still uncertain; however, it almost certainly was not present prior to 1842, and probably arrived in 1844. The origin of the pathogen has been traced to the Toluca Valley in Mexico, whence it spread first within North America and then to Europe. The 1845–46 blight was caused by the HERB-1 strain of the blight.

In 1844, Irish newspapers carried reports concerning a disease which for two years had attacked the potato crops in America. In 1843 and 1844, blight largely destroyed the potato crops in the Eastern United States. Ships from Baltimore, Philadelphia, or New York City could have carried diseased potatoes from these areas to European ports. American plant pathologist William C. Paddock posited that the blight was transported via potatoes being carried to feed passengers on clipper ships sailing from America to Ireland. Once introduced in Ireland and Europe, blight spread rapidly. By mid-August 1845, it had reached much of northern and central Europe; Belgium, The Netherlands, northern France, and southern England had all already been affected.

On 16 August 1845, "The Gardeners' Chronicle and Horticultural Gazette" reported "a blight of unusual character" on the Isle of Wight. A week later, on 23 August, it reported that "A fearful malady has broken out among the potato crop ... In Belgium the fields are said to be completely desolated. There is hardly a sound sample in Covent Garden market ... As for cure for this distemper, there is none." These reports were extensively covered in Irish newspapers. On 11 September, the "Freeman's Journal" reported on "the appearance of what is called 'cholera' in potatoes in Ireland, especially in the north". On 13 September, "The Gardeners' Chronicle" announced: "We stop the Press with very great regret to announce that the potato Murrain has unequivocally declared itself in Ireland."

Nevertheless, the British government remained optimistic over the next few weeks, as it received conflicting reports. Only when the crop was lifted (harvested) in October, did the scale of destruction become apparent. Prime Minister Sir Robert Peel wrote to Sir James Graham in mid-October that he found the reports "very alarming", but reminded him that there was, according to Woodham-Smith, "always a tendency to exaggeration in Irish news".

Crop loss in 1845 has been estimated at anywhere from one third to as high as one half of cultivated acreage. The Mansion House Committee in Dublin, to which hundreds of letters were directed from all over Ireland, claimed on 19 November 1845 to have ascertained beyond the shadow of doubt that "considerably more than one-third of the entire of the potato crop ... has been already destroyed".

In 1846, three-quarters of the harvest was lost to blight. By December, a third of a million destitute people were employed in public works. According to Cormac Ó Gráda, the first attack of potato blight caused considerable hardship in rural Ireland, from the autumn of 1846, when the first deaths from starvation were recorded. Seed potatoes were scarce in 1847. Few had been sown, so, despite average yields, hunger continued. 1848 yields were only two-thirds of normal. Since over three million Irish people were totally dependent on potatoes for food, hunger and famine were inevitable.

The Corporation of Dublin sent a memorial to the Queen, "praying her" to call Parliament together early (Parliament was at this time prorogued), and to recommend the requisition of some public money for public works, especially railways in Ireland. The Town Council of Belfast met and made similar suggestions, but neither body asked for charity, according to John Mitchel, one of the leading Repealers.
"They demanded that, if Ireland was indeed an Integral part of the realm, the common exchequer of both islands should be used—not to give alms, but to provide employment on public works of general utility ... if Yorkshire and Lancashire had sustained a like calamity in England, there is no doubt such measures as these would have been taken, promptly and liberally", Mitchel declared.

In early November 1845, a deputation from the citizens of Dublin, including the Duke of Leinster, Lord Cloncurry, Daniel O'Connell and the Lord Mayor, went to the Lord Lieutenant of Ireland, Lord Heytesbury, to offer suggestions, such as opening the ports to foreign corn, stopping distillation from grain, prohibiting the export of foodstuffs, and providing employment through public works. Lord Heytesbury urged them not to be alarmed, that they "were premature", that scientists were enquiring into all those matters, and that the Inspectors of Constabulary and Stipendiary Magistrates were charged with making constant reports from their districts; and there was no "immediate pressure on the market".

On 8 December 1845, Daniel O'Connell, head of the Repeal Association, proposed several remedies to the pending disaster. One of the first things he suggested was the introduction of "Tenant-Right" as practised in Ulster, giving the landlord a fair rent for his land, but giving the tenant compensation for any money he might have laid out on the land in permanent improvements. O'Connell noted actions taken by the Belgian legislature during the same season, as they had been hit by blight, too: shutting their ports against the export of provisions, and opening them to imports. He suggested that, if Ireland had a domestic Parliament, the ports would be thrown open and the abundant crops raised in Ireland would be kept for the people of Ireland, as the Dublin parliament had done during the food shortages of the 1780s. O'Connell maintained that only an Irish parliament would provide both food and employment for the people. He said that repeal of the Act of Union was a necessity and Ireland's only hope.

John Mitchel raised the issue of the "Potato Disease" in Ireland as early as 1844 in "The Nation", noting how powerful an agent hunger had been in certain revolutions. On 14 February 1846, he wrote about "the wretched way in which the famine was being trifled with", and asked whether the Government still did not have any conception that there might be soon "millions of human beings in Ireland having nothing to eat".

Mitchel later wrote one of the first widely circulated tracts on the famine, "The Last Conquest of Ireland (Perhaps)", published in 1861. It established the widespread view that British actions during the famine and their treatment of the Irish was a deliberate effort to murder the Irish. It contained a sentence that has since become famous: "The Almighty, indeed, sent the potato blight, but the English created the Famine." Mitchel was charged with sedition because of his writings, but this charge was dropped. He was convicted by a packed jury under the newly enacted Treason Felony Act and sentenced to 14 years transportation to Bermuda.

According to Charles Gavan Duffy, "The Nation" insisted that the one remedy was that which the rest of Europe had adopted, which even the parliaments of the Pale had adopted in periods of distress. That was to retain in the country the food raised by her people until the people were fed.

Contemporaneously, as found in letters from the period and in particular later oral memory, the name for the event is in , though with the earlier spelling standard of the era, which was Gaelic script, it is found written as in Droċ-Ṡaoġal. In the modern era, this name, while loosely translated as "the hard-time", is always denoted with a capital letter to express its specific historic meaning.

The period of the potato blight in Ireland from 1845 to 1851 was full of political confrontation. A more radical Young Ireland group seceded from the Repeal movement in July 1846, and attempted an armed rebellion in 1848. It was unsuccessful.
In 1847, William Smith O'Brien, leader of the Young Ireland party, became one of the founding members of the Irish Confederation to campaign for a Repeal of the Act of Union, and called for the export of grain to be stopped and the ports closed. The following year, he helped organise the short-lived Young Irelander Rebellion of 1848 in County Tipperary.

When Ireland had experienced food shortages in 1782–83, ports were closed to keep Irish-grown food in Ireland to feed the hungry. Irish food prices promptly dropped. Merchants lobbied against the export ban, but government in the 1780s overrode their protests. No such export ban happened in the 1840s.

Historian F. S. L. Lyons characterised the initial response of the British government to the early, less severe phase of the famine as "prompt and relatively successful". Confronted by widespread crop failure in November 1845, the Prime Minister, Sir Robert Peel, purchased £100,000 worth of maize and cornmeal secretly from America with Baring Brothers initially acting as his agents. The government hoped that they would not "stifle private enterprise" and that their actions would not act as a disincentive to local relief efforts. Due to poor weather conditions, the first shipment did not arrive in Ireland until the beginning of February 1846. The initial shipments were of unground dried kernels, but the few Irish mills in operation were not equipped for milling maize and a long and complicated milling process had to be adopted before the meal could be distributed. In addition, before the cornmeal could be consumed, it had to be "very much" cooked again, or eating it could result in severe bowel complaints. Due to its yellow colour, and initial unpopularity, it became known as "Peel's brimstone".

In October 1845, Peel moved to repeal the Corn Laws—tariffs on grain which kept the price of bread high—but the issue split his party and he had insufficient support from his own colleagues to push the measure through. He resigned the premiership in December, but the opposition was unable to form a government and he was re-appointed. In March, Peel set up a programme of public works in Ireland, but the famine situation worsened during 1846, and the repeal of the Corn Laws in that year did little to help the starving Irish; the measure split the Conservative Party, leading to the fall of Peel's ministry. On 25 June, the second reading of the government's Irish Coercion Bill was defeated by 73 votes in the House of Commons by a combination of Whigs, Radicals, Irish Repealers, and protectionist Conservatives. Peel was forced to resign as prime minister on 29 June, and the Whig leader, Lord John Russell, became prime minister.

The measures undertaken by Peel's successor, Russell, proved inadequate as the crisis deepened. The new Whig administration, influenced by the doctrine of laissez-faire, believed that the market would provide the food needed. They refused to interfere with the movement of food to England, then halted the previous government's food and relief works, leaving many hundreds of thousands of people without access to work, money, or food. Russell's ministry introduced a new programme of public works that by the end of December 1846 employed some half million but proved impossible to administer.

Charles Trevelyan, who was in charge of the administration of government relief, limited the Government's food aid programme because of a firm belief in laissez-faire.

In January 1847, the government abandoned this policy, realising that it had failed, and turned to a mixture of "indoor" and "outdoor" direct relief; the former administered in workhouses through the Irish Poor Laws, the latter through soup kitchens. The costs of the Poor Law fell primarily on the local landlords, some of whom in turn attempted to reduce their liability by evicting their tenants.

In June 1847 the Poor Law Amendment Act was passed which embodied the principle, popular in Britain, that Irish property must support Irish poverty. The landed proprietors in Ireland were held in Britain to have created the conditions that led to the famine. However, it was asserted that the British parliament since the Act of Union of 1800 was partly to blame. This point was raised in "The Illustrated London News" on 13 February 1847: "There was no law it would not pass at their request, and no abuse it would not defend for them." On 24 March, "The Times" reported that Britain had permitted in Ireland "a mass of poverty, disaffection, and degradation without a parallel in the world. It allowed proprietors to suck the very life-blood of that wretched race".

The "Gregory clause" of the Poor Law, named after William H. Gregory, M.P., prohibited anyone who held at least of an acre (0.1 ha) from receiving relief. In practice, this meant that, if a farmer, having sold all his produce to pay rent and taxes, should be reduced, as many thousands of them were, to applying for public outdoor relief, he would not get it until he had first delivered up all his land to the landlord. Of this Law, Mitchel wrote that "it is the able-bodied idler only who is to be fed—if he attempted to till but one rood of ground, he dies". This simple method of ejectment was called "passing paupers through the workhouse"—a man went in, a pauper came out. These factors combined to drive thousands of people off the land: 90,000 in 1849, and 104,000 in 1850.

In 1849 the Encumbered Estates Act allowed landlord estates to be auctioned off upon the petition of creditors. Estates with debts were then auctioned off at low prices. Wealthy British speculators purchased the lands and "took a harsh view" of the tenant farmers who continued renting. The rents were raised and tenants evicted to create large cattle grazing pastures. Between 1849 and 1854, some 50,000 families were evicted.

Throughout the entire period of the Famine, Ireland was exporting enormous quantities of food. In the magazine "History Ireland" (1997, issue 5, pp. 32–36), Christine Kinealy, a Great Hunger scholar, lecturer, and Drew University professor, relates her findings: Almost 4,000 vessels carried food from Ireland to the ports of Bristol, Glasgow, Liverpool, and London during 1847, when 400,000 Irish men, women, and children died of starvation and related diseases. She also writes that Irish exports of calves, livestock (except pigs), bacon, and ham actually increased during the Famine. This food was shipped from the most famine-stricken parts of Ireland, the ports of the west coast. A wide variety of commodities left Ireland during 1847, including peas, beans, onions, rabbits, fish, oysters, lard, honey, tongues, and seed.

Writing in 1849, English poet and social reformer Ebenezer Jones wrote that "In the year A.D. 1846, there were exported from Ireland, 3,266,193 quarters of wheat, barley and oats, besides flour, beans, peas, and rye; 186,483 cattle, 6,363 calves, 259,257 sheep, 180,827 swine; (food, that is, in the shape of meat and bread, for about one half of the Irish population), and yet this very year of A.D. 1846 was pre-eminently, owing to a land monopoly, the famine year for the Irish people." Butter was shipped in firkins, each one holding . From January-September 1847, of butter was exported to England from Ireland during nine months of the worst year of the Famine. The problem in Ireland was not lack of food, which was plentiful, but the price of it, which was beyond the reach of the poor.

The historian Cecil Woodham-Smith wrote in "" that no issue has provoked so much anger and embittered relations between England and Ireland "as the indisputable fact that huge quantities of food were exported from Ireland to England throughout the period when the people of Ireland were dying of starvation". John Ranelagh writes that Ireland remained a net exporter of food throughout most of the five-year famine. While in addition to the maize imports, four times as much wheat was imported into Ireland at the height of the famine as exported, much of the imported wheat was used as livestock feed. Woodham-Smith added that provision via the Poor law union workhouses by the Act of 1838 had to be paid by rates levied on the local property owners, and in areas where the famine was worst the tenants could not pay their rents to enable landlords to fund the rates and therefore the workhouses. Only by selling food, some of which would inevitably be exported, could a "virtuous circle" be created whereby the rents and rates would be paid, and the workhouses funded. Relief through the workhouse system was simply overwhelmed by the enormous scale and duration of the famine.

William Smith O'Brien—speaking on the subject of charity in a speech to the Repeal Association in February 1845—applauded the fact that the universal sentiment on the subject of charity was that they would accept no English charity. He expressed the view that the resources of Ireland were still abundantly adequate to maintain the population, and that, until those resources had been utterly exhausted, he hoped that there was no one in "Ireland who will so degrade himself as to ask the aid of a subscription from England". Similarly, Mitchel wrote in his "The Last Conquest of Ireland (Perhaps)", on the same subject, that no one from Ireland ever asked for charity during this period, and that it was England who sought charity on Ireland's behalf, and, having received it, was also responsible for administering it. He suggested that it has been carefully inculcated by the British Press "that the moment Ireland fell into distress, she became an abject beggar at England's gate, and that she even craved alms from all mankind". He further suggested that in Ireland no one ever asked alms or favours of any kind from England or any other nation, but that it was England herself that begged for Ireland. He also claimed that it was England that "sent 'round the hat over all the globe, asking a penny for the love of God to relieve the poor Irish", and, constituting herself the agent of all that charity, took all the profit of it. Such expressions of national pride coincide with a general notion in Ireland at the time to be entitled to government relief and with some individuals and committees from Ireland going on fundraising missions to England.

Large sums of money were donated by charities; the first foreign campaign in December 1845 included the Boston Repeal Association and the Catholic Church Calcutta is credited with making the first larger donations in 1846, summing up to around £14,000. The money raised included contributions by Irish soldiers serving there and Irish people employed by the East India Company. Russian Tsar Alexander II sent funds and Queen Victoria donated £2,000. According to legend, Sultan Abdülmecid I of the Ottoman Empire originally offered to send £10,000 but was asked either by British diplomats or his own ministers to reduce it to £1,000 to avoid donating more than the Queen. U.S. President James K. Polk donated $50 and in 1847 Congressman Abraham Lincoln donated $10 ($307 in 2019 value). Pope Pius IX also made a personal contribution of 1000 Scudi (approximately £213) for famine relief in Ireland and authorized collections in Rome. 
Most significantly, on 25 March 1847 Pius IX issued the encyclical Praedecessores nostros, which called the whole Catholic world to contribute moneywise and spiritually to Irish relief. Major figures behind international Catholic fundraising for Ireland were the rector of the Pontifical Irish College, Paul Cullen, and the President of the Society of Saint Vincent de Paul, Jules Gossin.

International fundraising activities received donations from locations as diverse as Venezuela, Australia, South Africa, Mexico, Russia and Italy. In addition to the religious, non-religious organisations came to the assistance of famine victims. The British Relief Association was one such group. Founded on 1 January 1847 by Lionel de Rothschild, Abel Smith, and other prominent bankers and aristocrats, the Association raised money throughout England, America, and Australia; their funding drive was benefited by a "Queen's Letter", a letter from Queen Victoria appealing for money to relieve the distress in Ireland. With this initial letter, the Association raised £171,533. A second, somewhat less successful "Queen's Letter" was issued in late 1847. In total, the Association raised approximately £390,000 for Irish relief.

Private initiatives such as the Central Relief Committee of the Society of Friends (Quakers) attempted to fill the gap caused by the end of government relief, and eventually the government reinstated the relief works, although bureaucracy slowed the release of food supplies. Thousands of dollars were raised in the United States, including $170 ($5,218 in 2019 value) collected from a group of Native American Choctaws in 1847. Judy Allen, editor of the Choctaw Nation of Oklahoma's newspaper "Biskinik", wrote that "It had been just 16 years since the Choctaw people had experienced the Trail of Tears, and they had faced starvation ... It was an amazing gesture." To mark the 150th anniversary, eight Irish people retraced the Trail of Tears, and the donation was publicly commemorated by President Mary Robinson.

Contributions by the United States during the famine were highlighted by Senator Henry Clay who said; "No imagination can conceive—no tongue express—no brush paint—the horrors of the scenes which are daily exhibited in Ireland." He called upon Americans to remind them that the practice of charity was the greatest act of humanity they could do. In total, 118 vessels sailed from the US to Ireland with relief goods valued to the amount of $545,145. Specific states which provided aid include South Carolina and Philadelphia, Pennsylvania. Pennsylvania was the second most important state for famine relief in the US and the second largest shipping port for aid to Ireland. The state hosted the Philadelphia Irish Famine Relief Committee. Roman Catholics, Methodists, Quakers, Presbyterians, Episcopalians, Lutherans, Moravian and Jewish groups put aside their differences in the name of humanity to help out the Irish. South Carolina rallied around the efforts to help those experiencing the famine. They raised donations of money, food and clothing to help the victims of the famine—Irish immigrants made up 39% of the white population in the southern cities. The states ignored all their racial, religious, and political differences to support the cause for relief.

The total sum of voluntary contributions for famine relief in Ireland can be estimated at £1.5 million (the real price equivalent of £135 million in 2018), of which less than £1 million came from abroad.

Landlords were responsible for paying the rates of every tenant whose yearly rent was £4 or less. Landlords whose land was crowded with poorer tenants were now faced with large bills. Many began clearing the poor tenants from their small plots, and letting the land in larger plots for over £4 which then reduced their debts. In 1846, there had been some clearances, but the great mass of evictions came in 1847. According to James S. Donnelly, Jr., it is impossible to be sure how many people were evicted during the years of the famine and its immediate aftermath. It was only in 1849 that the police began to keep a count, and they recorded a total of almost 250,000 persons as officially evicted between 1849 and 1854.

Donnelly considered this to be an underestimate, and if the figures were to include the number pressured into "voluntary" surrenders during the whole period (1846–1854), the figure would almost certainly exceed half a million persons. While Helen Litton says there were also thousands of "voluntary" surrenders, she notes also that there was "precious little voluntary about them". In some cases, tenants were persuaded to accept a small sum of money to leave their homes, "cheated into believing the workhouse would take them in".

West Clare was one of the worst areas for evictions, where landlords turned thousands of families out and demolished their derisory cabins. Captain Kennedy in April 1848 estimated that 1,000 houses, with an average of six people to each, had been levelled since November. The Mahon family of Strokestown House evicted 3,000 people in 1847, and were still able to dine on lobster soup.

After Clare, the worst area for evictions was County Mayo, accounting for 10% of all evictions between 1849 and 1854. George Bingham, 3rd Earl of Lucan, who owned over , was among the worst evicting landlords. He was quoted as saying that "he would not breed paupers to pay priests". Having turned out in the parish of Ballinrobe over 2,000 tenants alone, he then used the cleared land as grazing farms. In 1848, the Marquis of Sligo owed £1,650 to Westport Union; he was also an evicting landlord, though he claimed to be selective, saying that he was only getting rid of the idle and dishonest. Altogether, he cleared about 25% of his tenants.

In 1847, Bishop of Meath, Thomas Nulty, described his personal recollection of the evictions in a pastoral letter to his clergy:

According to Litton, evictions might have taken place earlier but for fear of the secret societies. However, they were now greatly weakened by the Famine. Revenge still occasionally took place, with seven landlords being shot, six fatally, during the autumn and winter of 1847. Ten other occupiers of land, though without tenants, were also murdered, she says.

One such landlord reprisal occurred in West Roscommon. The "notorious" landlord Maj Denis Mahon enforced thousands of his tenants into eviction before the end of 1847, with an estimated 60 percent decline in population in some parishes. He was shot dead in that year. In East Roscommon, "where conditions were more benign", the estimated decline in population was under 10 percent.

Lord Clarendon, alarmed at the number of landlords being shot and that this might mean rebellion, asked for special powers. Lord John Russell was not sympathetic to this appeal. Lord Clarendon believed that the landlords themselves were mostly responsible for the tragedy in the first place, saying that "It is quite true that landlords in England would not like to be shot like hares and partridges ... but neither does any landlord in England turn out fifty persons at once and burn their houses over their heads, giving them no provision for the future." The Crime and Outrage Act was passed in December 1847 as a compromise, and additional troops were sent to Ireland.

The "Gregory clause", described by Donnelly as a "vicious amendment to the Irish poor law", had been a successful Tory amendment to the Whig poor-relief bill which became law in early June 1847, where its potential as an estate-clearing device was widely recognised in parliament, although not in advance. At first, the poor law commissioners and inspectors viewed the clause as a valuable instrument for a more cost-effective administration of public relief, but the drawbacks soon became apparent, even from an administrative perspective. They would soon view them as little more than murderous from a humanitarian perspective. According to Donnelly, it became obvious that the quarter-acre clause was "indirectly a death-dealing instrument".

While the famine was responsible for a significant increase in emigration from Ireland, of anywhere from 45% to nearly 85% depending on the year and the county, it was not the sole cause. The beginning of mass emigration from Ireland can be traced to the mid-18th century, when some 250,000 people left Ireland over a period of 50 years to settle in the New World. Irish economist Cormac Ó Gráda estimates that between 1 million and 1.5 million people emigrated during the 30 years between 1815 (when Napoleon was defeated in Waterloo) and 1845 (when the Great Famine began). However, during the worst of the famine, emigration reached somewhere around 250,000 in one year alone, with western Ireland seeing the most emigrants.

Families did not migrate "en masse", but younger members of families did, so much so that emigration almost became a rite of passage, as evidenced by the data that show that, unlike similar emigrations throughout world history, women emigrated just as often, just as early, and in the same numbers as men. The emigrants would send remittances (reaching a total of £1,404,000 by 1851) back to family in Ireland, which, in turn, allowed another member of their family to leave.

Emigration during the famine years of 1845–1850 was primarily to England, Scotland, South Wales, North America, and Australia; one city that experienced a particularly strong influx of Irish immigrants was Liverpool, with at least one quarter of the city's population being Irish-born by 1851. Many of those fleeing to the Americas used the well-established McCorkell Line.

Of the more than 100,000 Irish that sailed to Canada in 1847, an estimated one out of five died from disease and malnutrition, including over 5,000 at Grosse Isle, Quebec, an island in the Saint Lawrence River used to quarantine ships near Quebec City. Overcrowded, poorly maintained, and badly provisioned vessels known as coffin ships sailed from small, unregulated harbours in the West of Ireland in contravention of British safety requirements, and mortality rates were high. The 1851 census reported that more than half the inhabitants of Toronto were Irish, and, in 1847 alone, 38,000 Irish flooded a city with fewer than 20,000 citizens. Other Canadian cities such as Quebec City, Montreal, Ottawa, Kingston, Hamilton, and Saint John also received large numbers. By 1871, 55% of Saint John residents were Irish natives or children of Irish-born parents. Unlike the United States, Canada could not close its ports to Irish ships because it was part of the British Empire, so emigrants could obtain cheap passage (evicted tenants received free passage) in returning empty lumber holds. However, fearing nationalist insurgencies, the British government placed harsh restrictions on Irish immigration to Canada after 1847, resulting in larger influxes to the United States.

In America, most Irish became city-dwellers; with little money, many had to settle in the cities that the ships they came on landed in. By 1850, the Irish made up a quarter of the population in Boston, New York City, Philadelphia, and Baltimore. In addition, Irish populations became prevalent in some American mining communities.

The famine marked the beginning of the depopulation of Ireland in the 19th century. Population had increased by 13–14% in the first three decades of the 19th century; between 1831 and 1841, population grew by 5%. Application of Thomas Malthus's idea of population expanding geometrically while resources increase arithmetically was popular during the famines of 1817 and 1822. By the 1830s, they were seen as overly simplistic, and Ireland's problems were seen "less as an excess of population than as a lack of capital investment". The population of Ireland was increasing no faster than that of England, which suffered no equivalent catastrophe. By 1854, between 1.5 and 2 million Irish left their country due to evictions, starvation, and harsh living conditions.

It is not known exactly how many people died during the period of the famine, although it is believed that more died from disease than from starvation. State registration of births, marriages, or deaths had not yet begun, and records kept by the Roman Catholic Church are incomplete. One possible estimate has been reached by comparing the expected population with the eventual numbers in the 1850s. A census taken in 1841 recorded a population of 8,175,124. A census immediately after the famine in 1851 counted 6,552,385, a drop of over 1.5 million in 10 years. The census commissioners estimated that, at the normal rate of population increase, the population in 1851 should have grown to just over 9 million if the famine had not occurred.

On the in-development "Great Irish Famine Online" resource, produced by the Geography department of University College Cork, the population of Ireland section states, that together with the census figures being called low, before the famine it reads that "it is now generally believed" that over 8.75 million people populated the island of Ireland prior to it striking.

In 1851, the census commissioners collected information on the number who died in each family since 1841, and the cause, season, and year of death. They recorded 21,770 total deaths from starvation in the previous decade, and 400,720 deaths from disease. Listed diseases were fever, diphtheria, dysentery, cholera, smallpox, and influenza, with the first two being the main killers (222,021 and 93,232). The commissioners acknowledged that their figures were incomplete and that the true number of deaths was probably higher:

The greater the amount of destitution of mortality ... the less will be the amount of recorded deaths derived through any household form;—for not only were whole families swept away by disease ... but whole villages were effaced from off the land.

Later historians agree that the 1851 death tables "were flawed and probably under-estimated the level of mortality". The combination of institutional and figures provided by individuals gives "an incomplete and biased count" of fatalities during the famine.

Cormac Ó Gráda, referencing the work of W. A. MacArthur, writes that specialists have long known that the Irish death tables were inaccurate. As a result, Ó Gráda says that the tables undercount the number of deaths, because information was gathered from surviving householders having to look back over the previous 10 years, and death and emigration had cleared away entire families, leaving few or no survivors to answer the census questions.

S. H. Cousens' estimate of 800,000 deaths relied heavily on retrospective information contained in the 1851 census and elsewhere, and is now regarded as too low. Modern historian Joseph Lee says "at least 800,000", and R. F. Foster estimates that "at least 775,000 died, mostly through disease, including cholera in the latter stages of the holocaust". He further notes that "a recent sophisticated computation estimates excess deaths from 1846 to 1851 as between 1,000,000 and 1,500,000 ... after a careful critique of this, other statisticians arrive at a figure of 1,000,000".

Joel Mokyr's estimates at an aggregated county level range from 1.1 million to 1.5 million deaths between 1846 and 1851. Mokyr produced two sets of data which contained an upper-bound and lower-bound estimate, which showed not much difference in regional patterns. The true figure is likely to lie between the two extremes of half and one and a half million, and the most widely accepted estimate is one million.

At least a million people are thought to have emigrated as a result of the famine. There were about 1 million long-distance emigrants between 1846 and 1851, mainly to North America. The total given in the 1851 census is 967,908. Short-distance emigrants, mainly to Britain, may have numbered 200,000 or more.
Another area of uncertainty lies in the descriptions of disease given by tenants as to the cause of their relatives' deaths. Though the 1851 census has been rightly criticised as underestimating the true extent of mortality, it does provide a framework for the medical history of the Great Famine. The diseases that badly affected the population fell into two categories: famine-induced diseases and diseases of nutritional deficiency. Of the nutritional deficiency diseases, the most commonly experienced were starvation and marasmus, as well as a condition at the time called dropsy. Dropsy (oedema) was a popular name given for the symptoms of several diseases, one of which, kwashiorkor, is associated with starvation.

However, the greatest mortality was not from nutritional deficiency diseases, but from famine-induced ailments. The malnourished are very vulnerable to infections; therefore, these were more severe when they occurred. Measles, diphteria, diarrhoea, tuberculosis, most respiratory infections, whooping cough, many intestinal parasites, and cholera were all strongly conditioned by nutritional status. Potentially lethal diseases, such as smallpox and influenza, were so virulent that their spread was independent of nutrition. The best example of this phenomenon was fever, which exacted the greatest death toll. In the popular mind, as well as medical opinion, fever and famine were closely related. Social dislocation—the congregation of the hungry at soup kitchens, food depots, and overcrowded work houses—created conditions that were ideal for spreading infectious diseases such as typhus, typhoid, and relapsing fever. 

Diarrhoeal diseases were the result of poor hygiene, bad sanitation, and dietary changes. The concluding attack on a population incapacitated by famine was delivered by Asiatic cholera, which had visited Ireland briefly in the 1830s. In the following decade, it spread uncontrollably across Asia, through Europe, and into Britain, finally reaching Ireland in 1849. Some scholars estimate that the population of Ireland was reduced by 20–25%.

Ireland's mean age of marriage in 1830 was 23.8 for women and 27.5 for men, where they had once been 21 for women and 25 for men, and those who never married numbered about 10% of the population; in 1840, they had respectively risen to 24.4 and 27.7. In the decades after the Famine, the age of marriage had risen to 28–29 for women and 33 for men, and as many as a third of Irishmen and a quarter of Irishwomen never married, due to low wages and chronic economic problems that discouraged early and universal marriage.

One consequence of the increase in the number of orphaned children was that some young women turned to sex work to provide for themselves. Some of the women who became Wrens of the Curragh were famine orphans.

The potato blight would return to Ireland in 1879 though by then the rural cottier tenant farmers and labourers of Ireland had begun the "Land War", described as one of the largest agrarian movements to take place in nineteenth-century Europe. The movement, organized by the Land League, continued the political campaign for the Tenant Right League's 1850 issued Three Fs, that were penned during the Great Famine.

By the time the potato blight returned in 1879, The Land League, which was led by Michael Davitt, who was born during the Great Famine and whose family had been evicted when Davitt was only 4-years-old, encouraged the mass boycott of "notorious landlords" with some members also physically blocking evictions. The policy, however, would soon be suppressed. Despite close to 1000 interned under the 1881 Coercion Act for suspected membership. With the reduction in the rate of homelessness and the increased physical and political networks eroding the landlordism system, the severity of the following shorter famine would be limited.

According to the linguist Erick Falc'her-Poyroux, surprisingly, for a country renowned for its rich musical heritage, only a small number of folk songs can be traced back to the demographic and cultural catastrophe brought about by the Great Famine, and he infers from this that the subject was generally avoided for decades among poorer people as it brought back too many sorrowful memories. Also, large areas of the country became uninhabited and the folk song collectors of the eighteenth and nineteenth centuries did not collect the songs they heard in the Irish language, as the language of the peasantry was often regarded as dead, or "not delicate enough for educated ears". Of the songs that have survived probably the best known is Skibbereen. Emigration has been an important sources of inspiration for songs of the Irish during the 20th century. Since the 1970s a number of songs about the famine have been written and recorded, such as "The Fields of Athenry" by Pete St. John, "Famine" by Sinéad O'Connor and "Thousands are Sailing" by the Pogues.

Contemporary opinion was sharply critical of the Russell government's response to and management of the crisis. From the start, there were accusations that the government failed to grasp the magnitude of the disaster. Sir James Graham, who had served as Home Secretary in Sir Robert Peel's late government, wrote to Peel that, in his opinion, "the real extent and magnitude of the Irish difficulty are underestimated by the Government, and cannot be met by measures within the strict rule of economical science".

This criticism was not confined to outside critics. The Lord Lieutenant of Ireland, Lord Clarendon, wrote a letter to Russell on 26 April 1849, urging that the government propose additional relief measures: "I don't think there is another legislature in Europe that would disregard such suffering as now exists in the west of Ireland, or coldly persist in a policy of extermination." Also in 1849, the Chief Poor Law Commissioner, Edward Twisleton, resigned in protest over the Rate-in-Aid Act, which provided additional funds for the Poor Law through a 6p in the pound levy on all rateable properties in Ireland. Twisleton testified that "comparatively trifling sums were required for Britain to spare itself the deep disgrace of permitting its miserable fellow subjects to die of starvation". According to Peter Gray in his book "The Irish Famine", the government spent £7 million for relief in Ireland between 1845 and 1850, "representing less than half of one percent of the British gross national product over five years. Contemporaries noted the sharp contrast with the £20 million compensation given to West Indian slave-owners in the 1830s."

Other critics maintained that, even after the government recognised the scope of the crisis, it failed to take sufficient steps to address it. John Mitchel, one of the leaders of the Young Ireland Movement, wrote in 1860:

I have called it an artificial famine: that is to say, it was a famine which desolated a rich and fertile island that produced every year abundance and superabundance to sustain all her people and many more. The English, indeed, call the famine a "dispensation of Providence"; and ascribe it entirely to the blight on potatoes. But potatoes failed in like manner all over Europe; yet there was no famine save in Ireland. The British account of the matter, then, is first, a fraud; second, a blasphemy. The Almighty, indeed, sent the potato blight, but the English created the famine.

Still other critics saw reflected in the government's response its attitude to the so-called "Irish Question". Nassau Senior, an economics professor at Oxford University, wrote that the Famine "would not kill more than one million people, and that would scarcely be enough to do any good". In 1848, Denis Shine Lawlor suggested that Russell was a student of the Elizabethan poet Edmund Spenser, who had calculated "how far English colonisation and English policy might be most effectively carried out by Irish starvation". Charles Trevelyan, the civil servant with most direct responsibility for the government's handling of the famine, described it in 1848 as "a direct stroke of an all-wise and all-merciful Providence", which laid bare "the deep and inveterate root of social evil"; he affirmed that the Famine was "the sharp but effectual remedy by which the cure is likely to be effected. God grant that the generation to which this opportunity has been offered may rightly perform its part..."

Christine Kinealy has written that "the major tragedy of the Irish Famine of 1845–52 marked a watershed in modern Irish history. Its occurrence, however, was neither inevitable nor unavoidable." The underlying factors which combined to cause the famine were aggravated by an inadequate government response. As Kinealy notes:

Several writers single out the decision of the government to permit the continued export of food from Ireland as suggestive of the policy-makers' attitudes. Leon Uris suggested that "there was ample food within Ireland", while all the Irish-bred cattle were being shipped off to England. The following exchange appeared in Act IV of George Bernard Shaw's play "Man and Superman":

Some also pointed to the structure of the British Empire as a contributing factor. James Anthony Froude wrote that "England governed Ireland for what she deemed her own interest, making her calculations on the gross balance of her trade ledgers, and leaving moral obligations aside, as if right and wrong had been blotted out of the statute book of the Universe." Dennis Clark, an Irish-American historian and critic of empire, claimed the famine was "the culmination of generations of neglect, misrule and repression. It was an epic of English colonial cruelty and inadequacy. For the landless cabin dwellers it meant emigration or extinction..."

The famine remains a controversial event in Irish history. Debate and discussion on the British government's response to the failure of the potato crop in Ireland, the exportation of food crops and livestock, the subsequent large-scale starvation, and whether or not this constituted genocide, remains a historically and politically charged issue.

During the famine, Ireland produced enough food, flax, and wool to feed and clothe double its nine million people. When Ireland had experienced a famine in 1782–83, ports were closed to keep Irish-grown food in Ireland to feed the Irish. Local food prices promptly dropped. Merchants lobbied against the export ban, but government in the 1780s overrode their protests. There was no such export ban in the 1840s. Some historians have argued that in this sense the famine was artificial, caused by the British government's choice not to stop exports.

Francis Boyle claimed that the government's actions violated sections (a), (b), and (c) of Article 2 of the CPPCG and constituted genocide in a legal opinion to the New Jersey Commission on Holocaust Education on 2 May 1996. In 1996, the U.S. state of New Jersey included the famine in the "Holocaust and Genocide Curriculum" for its secondary schools. The curriculum was pushed by various Irish American political groups and drafted by the librarian James Mullin. Following criticism of the curriculum, the New Jersey Holocaust Commission requested statements from two academics that the Irish famine was genocide, which was eventually provided by law professors Charles E. Rice and Francis Boyle, who had not been previously known for studying Irish history. They concluded that the British government deliberately pursued a race- and ethnicity-based policy aimed at destroying the Irish people and that the policy of mass starvation amounted to genocide per retrospective application of article 2 of the Hague Convention of 1948. 

The claims were contested by Peter Gray, who concluded that UK government policy "was not a policy of deliberate genocide", but a dogmatic refusal to admit that the policy was wrong. James S. Donnelly, Jr., wrote, "while genocide was not in fact committed, what happened ... had the look of genocide to a great many Irish."

Cecil Woodham-Smith claimed that while the export policy embittered the Irish, this did not implicate the policy in genocide, but rather in excessive parsimony obtuseness, short-sightedness, and ignorance. Irish historian Cormac O' Grada rejects the term, stating that the English exhibited no desire to exterminate the Irish and that the challenges for providing relief were enormous. W. D. Rubinstein also rejected the genocide claim.

Journalist Peter Duffy writes that "The government's crime, which deserves to blacken its name forever", was rooted "in the effort to regenerate Ireland" through "landlord-engineered replacement of tillage plots with grazing lands" that "took precedence over the obligation to provide food ... for its starving citizens. It is little wonder that the policy looked to many people like genocide." James S. Donnelly, Jr., a historian at the University of Wisconsin–Madison, wrote in his book, "Landlord and Tenant in Nineteenth-century Ireland":

I would draw the following broad conclusion: at a fairly early stage of the Great Famine the government's abject failure to stop or even slow down the clearances (evictions) contributed in a major way to enshrining the idea of English state-sponsored genocide in Irish popular mind. Or perhaps one should say in the Irish mind, for this was a notion that appealed to many educated and discriminating men and women, and not only to the revolutionary minority ... And it is also my contention that while genocide was not in fact committed, what happened during and as a result of the clearances had the look of genocide to a great many Irish.

Cormac Ó Gráda disagreed that the famine was genocide. He argues that "genocide includes murderous intent, and it must be said that not even the most bigoted and racist commentators of the day sought the extermination of the Irish", and also that most people in Whitehall "hoped for better times for Ireland". Additionally, he states that the claim of genocide overlooks "the enormous challenge facing relief agencies, both central and local, public and private". Ó Gráda thinks that a case of neglect is easier to sustain than that of genocide. Edward Lengel claims that views of the Irish as racially inferior, and for this reason significantly responsible for their circumstances, gained purchase in Great Britain during and immediately after the famine, especially through influential publications such as "The Medical Times" and "The Times".

The Great Famine in Ireland has been compared to the "Holodomor" ("hunger plague") that took place in the Ukraine under Stalin in 1932, which has been the subject of similar controversy and debate.

According to Liam Kennedy, "virtually all historians of Ireland" reject the genocide allegations. Historian Donald Akenson, who has written twenty-four books on Ireland, has said of the use of the word 'Holocaust' in relation of Ireland: "When you see it, you know that you are encountering famine-porn. It is inevitably part of a presentation that is historically unbalanced and, like other kinds of pornography, is distinguished by a covert (and sometimes overt) appeal to misanthropy and almost always an incitement to hatred."

The National Famine Commemoration Day is observed annually in Ireland, usually on a Sunday in May.

It is also memorialised in many locations throughout Ireland, especially in those regions that suffered the greatest losses, and also in cities overseas such as New York, with large populations descended from Irish immigrants. These include, at Custom House Quays, Dublin, the thin sculptural figures, by artist Rowan Gillespie, who are portrayed as if walking towards the emigration ships on the Dublin Quayside. There is also a large memorial at the Murrisk Millennium Peace Park at the foot of Croagh Patrick in County Mayo.

"Kindred Spirits", a large stainless steel sculpture of nine eagle feathers by artist Anex Penetek was erected in 2017 in the Irish town of Midleton, County Cork, to thank the Choctaw people for its financial assistance during the famine.

Among the memorials in the US is the Irish Hunger Memorial near a section of the Manhattan waterfront in New York City, where many Irish arrived. An annual Great Famine walk from Doolough to Louisburgh, County Mayo was inaugurated in 1988, and has been led by such notable personalities as Archbishop Desmond Tutu of South Africa and the Choctaw Nation of Oklahoma. The walk, organised by Afri, takes place on the first or second Saturday of May, and links the memory of the Great Hunger with a contemporary Human Rights issue.


Informational notes
Foototes
Citations
Bibliography
Further reading



</doc>
<doc id="14727" url="https://en.wikipedia.org/wiki?curid=14727" title="Isle of Man">
Isle of Man

The Isle of Man ( , also ), also known as Mann (), is a self-governing British Crown dependency situated in the Irish Sea between Great Britain and Ireland. The head of state, Queen Elizabeth II, holds the title of Lord of Mann and is represented by a lieutenant governor. The United Kingdom has responsibility for the island's military defence.

Humans have lived on the island since before 6500 BC. Gaelic cultural influence began in the 5th century AD, and the Manx language, a branch of the Goidelic languages, emerged. In 627 King Edwin of Northumbria conquered the Isle of Man along with most of Mercia. In the 9th century Norsemen established the thalassocratic Kingdom of the Isles, which included the Isle of Man. Magnus III, King of Norway from 1093 to 1103, reigned also as King of Mann and the Isles between 1099 and 1103.

In 1266 the island became part of Scotland under the Treaty of Perth, after being ruled by Norway. After a period of alternating rule by the kings of Scotland and England, the island came under the feudal lordship of the English Crown in 1399. The lordship revested into the British Crown in 1765, but the island never became part of the 18th-century kingdom of Great Britain, nor of its successors, the United Kingdom of Great Britain and Ireland and the present-day United Kingdom of Great Britain and Northern Ireland. It has always retained its internal self-government.

In 1881 the Isle of Man parliament, Tynwald, became the first national legislative body in the world to give women the right to vote in a general election, although this excluded married women. In 2016, UNESCO awarded the Isle of Man biosphere reserve status.

Insurance and online gambling each generate 17% of the GNP, followed by information and communications technology and banking with 9% each. Internationally, the Isle of Man is known for the Isle of Man TT (Tourist Trophy) motorcycle races and for the Manx cat, a breed of cat with short or no tails.
The inhabitants (Manx) are considered a Celtic nation.

The Manx name of the Isle of Man is ': ' () is a Manx word meaning "island"; ' () appears in the genitive case as ' (), with initial consonant mutation, hence ', "Island of Mann". The short form used in English is spelled either Mann or Man. The earliest recorded Manx form of the name is ' or ".

The Old Irish form of the name is ' or '. Old Welsh records named it as ', also reflected in ', the name for an ancient district in north Britain along the lower Firth of Forth.
The oldest known reference to the island calls it ', in Latin (Julius Caesar, 54 BC); in the 1st century AD, Pliny the Elder records it as ' or ", and Ptolemy (2nd century) as "Monœda" (, "Monaoida") or ("Monarina"), in Koine Greek.
Later Latin references have ' or ' (Orosius, 416),
and ' or ' by Irish writers. It is found in the Sagas of Icelanders as ".

The name is probably cognate with the Welsh name of the island of Anglesey, ",
usually derived from a Celtic word for 'mountain' (reflected in Welsh ', Breton ', and Scottish Gaelic ""),
from a Proto-Celtic "*moniyos".

The name was at least secondarily associated with that of Manannán mac Lir in Irish mythology (corresponding to Welsh '). In the earliest Irish mythological texts, Manannán is a king of the otherworld, but the 9th-century "Sanas Cormaic" identifies a euhemerised Manannán as "a famous merchant who resided in, and gave name to, the Isle of Man". Later, a Manannán is recorded as the first king of Mann in a Manx poem (dated 1504).

The island was cut off from the surrounding islands around 8000 BC as sea levels rose following the end of the ice age. Humans colonised it by travelling by sea some time before 6500 BC. The first occupants were hunter-gatherers and fishermen. Examples of their tools are kept at the Manx Museum.

The Neolithic Period marked the beginning of farming, and the people began to build megalithic monuments, such as Cashtal yn Ard near Maughold, King Orry's Grave at Laxey, Meayll Circle near Cregneash, and Ballaharra Stones at St John's. There were also the local Ronaldsway and Bann cultures.

During the Bronze Age, the size of burial mounds decreased. The people put bodies into stone-lined graves with ornamental containers. The Bronze Age burial mounds survived as long-lasting markers around the countryside.

The ancient Romans knew of the island and called it "." Scholars have not determined whether they conquered the island.

Around the 5th century AD, large-scale migration from Ireland precipitated a process of Gaelicisation, evidenced by Ogham inscriptions, and the Manx language developed. It is a Goidelic language closely related to Irish and Scottish Gaelic.

Vikings arrived at the end of the 8th century. They established Tynwald and introduced many land divisions that still exist. In 1266 King Magnus VI of Norway ceded the islands to Scotland in the Treaty of Perth. But Scotland's rule over Mann did not become firmly established until 1275, when the Manx were defeated in the Battle of Ronaldsway, near Castletown.

In 1290 King Edward I of England sent Walter de Huntercombe to take possession of Mann. It remained in English hands until 1313, when Robert Bruce took it after besieging Castle Rushen for five weeks. In 1314, it was retaken for the English by John Bacach of Argyll. In 1317, it was retaken for the Scots by Thomas Randolph, 1st Earl of Moray and Lord of the Isle of Man. It was held by the Scots until 1333. For some years thereafter control passed back and forth between the kingdoms until the English took it for the final time in 1346. The English Crown delegated its rule of the island to a series of lords and magnates. Tynwald passed laws concerning the government of the island in all respects and had control over its finances, but was subject to the approval of the Lord of Mann.

In 1866, the Isle of Man obtained limited home rule, with partly democratic elections to the House of Keys, but the Legislative Council was appointed by the Crown. Since then, democratic government has been gradually extended.

The Isle of Man has designated more than 250 historic sites as registered buildings.

The Isle of Man is located in the middle of the northern Irish Sea, almost equidistant from England to the east, Northern Ireland to the west, and Scotland (closest) to the north; while Wales to the south is almost the distance of the Republic of Ireland to the southwest. It is long and, at its widest point, wide. It has an area of around . Besides the island of Mann itself, the political unit of the Isle of Man includes some nearby small islands: the seasonally inhabited Calf of Man, Chicken Rock on which stands an unmanned lighthouse, St Patrick's Isle and St Michael's Isle. The last two of these are connected to the main island by permanent roads/causeways.

Ranges of hills in the north and south are separated by a central valley. The northern plain, by contrast, is relatively flat, consisting mainly of deposits from glacial advances from western Scotland during colder times. There are more recently deposited shingle beaches at the northernmost point, the Point of Ayre. The island has one mountain higher than , Snaefell, with a height of . According to an old saying, from the summit one can see six kingdoms: those of Mann, Scotland, England, Ireland, Wales, and Heaven. Some versions add a seventh kingdom, that of the sea, or Neptune.

At the 2016 census, the Isle of Man was home to 83,314 people, of whom 26,997 resided in the island's capital, Douglas, and 9,128 in the adjoining village of Onchan. The population decreased by 1.4% between the 2011 and 2016 censuses. By country of birth, those born in the Isle of Man were the largest group (49.8%), while those born in the United Kingdom were the next largest group at 40% (33.9% in England, 3% in Scotland, 2% in Northern Ireland and 1.1% in Wales), 1.8% in the Republic of Ireland and 0.75% in the Channel Islands. The remaining 8.5% were born elsewhere in the world, with 5% coming from EU countries (other than Ireland).

The "Isle of Man Full Census", last held in 2016, has been a decennial occurrence since 1821, with interim censuses being introduced from 1966. It is separate from, but similar to, the Census in the United Kingdom.

The 2001 census was conducted by the Economic Affairs Division of the Isle of Man Treasury, under the authority of the Census Act 1929.

The United Kingdom is responsible for the island's defence and ultimately for good governance, and for representing the island in international forums, while the island's own parliament and government have competence over all domestic matters.

The island's parliament, Tynwald, is claimed to have been in continuous existence since 979 or earlier, purportedly making it the oldest continuously governing body in the world, though evidence supports a much later date. Tynwald is a bicameral or tricameral legislature, comprising the House of Keys (directly elected by universal suffrage with a voting age of 16 years) and the Legislative Council (consisting of indirectly elected and ex-officio members). These two bodies also meet together in joint session as Tynwald Court.

The executive branch of government is the Council of Ministers, which is composed of members of the House of Keys. It is headed by the Chief Minister, currently (2019) Howard Quayle MHK.

Vice-regal functions of the head of state are performed by a lieutenant governor.

In various laws of the United Kingdom, "the United Kingdom" is defined to exclude the Isle of Man. Historically, the UK has taken care of its external and defence affairs, and retains paramount power to legislate for the Island. However, in 2007, the Isle of Man and the UK signed an agreement that established frameworks for the development of the international identity of the Isle of Man. There is no separate Manx citizenship. Citizenship is covered by UK law, and Manx people are classed as British citizens. The Isle of Man holds neither membership nor associate membership of the European Union, and lies outside the European Economic Area (EEA). There is a long history of relations and cultural exchange between the Isle of Man and Ireland. The Isle of Man's historic Manx language (and its modern revived variant) are closely related to both Scottish Gaelic and the Irish language, and in 1947, Irish Taoiseach Éamon de Valera spearheaded efforts to save the dying Manx language.

Under British law, the Isle of Man is not part of the United Kingdom. However, the UK takes care of its external and defence affairs. There are no independent military forces on the Isle of Man, although HMS "Ramsey" is affiliated with the town of the same name. From 1938 to 1955 there was the Manx Regiment of the British Territorial Army, which saw extensive action during the Second World War. In 1779, the Manx Fencible Corps, a fencible regiment of three companies, was raised; it was disbanded in 1783 at the end of the American War of Independence. Later, the Royal Manx Fencibles was raised at the time of the French Revolutionary Wars and Napoleonic Wars. The 1st Battalion (of 3 companies) was raised in 1793. A 2nd Battalion (of 10 companies) was raised in 1795, and it saw action during the Irish Rebellion of 1798. The regiment was disbanded in 1802. A third body of Manx Fencibles was raised in 1803 to defend the island during the Napoleonic Wars and to assist the Revenue. It was disbanded in 1811. In 2015 a multi-capability recruiting and training unit of the British Army Reserve was established in Douglas.

There is no citizenship of the Isle of Man as such under the British Nationality Acts 1948 and 1981.

The Passport Office, Isle of Man, Douglas, accepts and processes applications for the Lieutenant Governor of the Isle of Man, who is formally responsible for issuing Isle of Man–issued British passports, entitled ""British Islands – Isle of Man".

British citizens who have 'Manxman status', not being either born, naturalised or registered in the United Kingdom, and without a parent or grandparent who has, or if they have not themselves personally been resident in the United Kingdom for more than five consecutive years, do not have the same rights as other British citizens with regard to employment and establishment in other parts of the EU (other than the UK and Ireland).

Isle of Man-issued British passports can presently be issued to any British citizen resident in the Isle of Man, and also to British citizens who have a qualifying close personal connection to the Isle of Man but are now resident either in the UK or in either one of the two other Crown Dependencies.

The Isle of Man was never part of the European Union, nor did it have a special status, and thus it did not take part in the 2016 referendum on the UK's EU membership. However, Protocol 3 of the UK's Act of Accession to the Treaty of Rome included the Isle of Man within the EU's customs area, allowing for trade in Manx goods without tariffs throughout the EU. As it is not part of the EU's internal market, there are still limitations on the movement of capital, services and labour.

EU citizens are entitled to travel and reside, but not work, in the island without restriction. British citizens with Manxman status are under the same circumstances and restrictions as any other non-EU European relating country to work in the EU.

The political and diplomatic impacts of Brexit on the island are still uncertain. The UK has confirmed that the Crown Dependencies' positions were included in the Brexit negotiations. The Brexit withdrawal agreement explicitly includes the Isle of Man in its territorial scope, but makes no other mention of it. The island's government website states that after the end of the implementation period, the Isle of Man's relationship with the EU will depend on the agreement reached between the UK and the EU on their future relationship.

The Isle of Man is not itself a member of the Commonwealth of Nations. By virtue of its relationship with the United Kingdom, it takes part in several Commonwealth institutions, including the Commonwealth Parliamentary Association and the Commonwealth Games. The Government of the Isle of Man has made calls for a more integrated relationship with the Commonwealth, including more direct representation and enhanced participation in Commonwealth organisations and meetings, including Commonwealth Heads of Government Meetings. The Chief Minister of the Isle of Man has said: "A closer connection with the Commonwealth itself would be a welcome further development of the island's international relationships."

Most Manx politicians stand for election as independents rather than as representatives of political parties. Although political parties do exist, their influence is not nearly as strong as in the United Kingdom.

There are three political parties in the Isle of Man:

A number of pressure groups also exist on the island. Mec Vannin advocate the establishment of a sovereign republic. The Positive Action Group campaign for three key elements to be introduced into the governance of the island: open accountable government, rigorous control of public finances, and a fairer society.

Local government on the Isle of Man is based partly on the island's 17 ancient parishes. There are two types of local authorities:

Local authorities are under the supervision of the Isle of Man Government's Department of Local Government and the Environment (DOLGE).

Public education is under the Department of Education, Sport & Culture. Thirty-two primary schools, five secondary schools and the University College Isle of Man function under the department.

Health and social care on the Isle of Man is the responsibility of the Department of Health and Social Care. Healthcare is free for residents and visitors from the UK.

The Isle of Man Government maintains five emergency services. These are:

All of these services are controlled directly by the Department of Home Affairs of the Isle of Man Government, and are independent of the United Kingdom. Nonetheless, the Isle of Man Constabulary voluntarily submits to inspection by the British inspectorate of police, and the Isle of Man Coastguard contracts Her Majesty's Coastguard (UK) for air-sea rescue operations.

The island's sole crematorium is in Glencrutchery Road, Douglas and is operated by Douglas Borough Council. Usually staffed by four, in March 2020 an increase of staff to 12 was announced by the council leader, responding to the threat of coronavirus which could require more staff.

The Isle of Man Department for Enterprise manages the diversified economy in 12 key sectors. The largest sectors by GNP are insurance and eGaming with 17% of GNP each, followed by ICT and banking with 9% each. The 2016 census lists 41,636 total employed. The largest sectors by employment are "medical and health", "financial and business services", construction, retail and public administration. Manufacturing, focused on aerospace and the food and drink industry, employs almost 2000 workers and contributes about 5% of gross domestic product (GDP). The sector provides laser optics, industrial diamonds, electronics, plastics and aerospace precision engineering. Tourism, agriculture, fishing, once the mainstays of the economy, now make very little contributions to the island's GDP.

After 30 years of continued GDP growth, the unemployment rate is just around 1%.

The Isle of Man is a low-tax economy with no capital gains tax, wealth tax, stamp duty, or inheritance tax and a top rate of income tax of 20%. A tax cap is in force: the maximum amount of tax payable by an individual is £200,000 or £400,000 for couples if they choose to have their incomes jointly assessed. Personal income is assessed and taxed on a total worldwide income basis rather than a remittance basis. This means that all income earned throughout the world is assessable for Manx tax rather than only income earned in or brought into the island.

The standard rate of corporation tax for residents and non-residents is 0%. Retail business profits above £500,000 and banking business income are taxed at 10%, and rental (or other) income from land and buildings situated on the Isle of Man is taxed at 20%.
Trade takes place mostly with the United Kingdom. The island is in customs union with the UK, and related revenues are pooled and shared under the Common Purse Agreement. This means that the Isle of Man cannot have the lower excise revenues on alcohol and other goods that are enjoyed in the Channel Islands.

The Manx government promotes island locations for making films by offering financial support. Since 1995, over 100 films have been made on the island. Most recently the island has taken a much wider strategy to attract the general digital media industry in film, television, video and eSports.

The Isle of Man Government Lottery operated from 1986 to 1997. Since 2 December 1999 the island has participated in the United Kingdom National Lottery. The island is the only jurisdiction outside the United Kingdom where it is possible to play the UK National Lottery. Since 2010 it has also been possible for projects in the Isle of Man to receive national lottery Good Causes Funding. The good causes funding is distributed by the Manx Lottery Trust. Tynwald receives the 12% lottery duty for tickets sold in the island.

Tourist numbers peaked in the first half of the 20th century, prior to the boom in cheap travel to Southern Europe that also saw the decline of tourism in many similar English seaside resorts. The Isle of Man tourism board has recently invested in "Dark Sky Discovery" sites to diversify its tourism industry. It is expected that dark skies will generally be nominated by the public across the UK. However, the Isle of Man tourism board tasked someone from their team to nominate 27 places on the island as a civil task. This cluster of the highest quality "Milky Way" sites is now well promoted within the island. This government push has effectively given the island a headstart in the number of recognised Dark Sky sites. However, this has created a distorted view when compared to the UK where this is not promoted on a national scale. There, Dark Sky sites are expected to be nominated over time by the public across a full range of town, city and countryside locations rather than "en masse" by government departments.

The Isle of Man is seen to be "accommodating" towards Bitcoin, which is a growing technology on the island.

In 2017 an office of The International Stock Exchange was opened to provide a boost for the island's finance industry.

The main telephone provider on the Isle of Man is Manx Telecom. The island has two mobile operators: Manx Telecom, previously known as Manx Pronto, and Sure. Cloud9 operated as a third mobile operator on the island for a short time, but has since withdrawn. Broadband internet services are available through four local providers: Wi-Manx, Domicilium, Manx Computer Bureau and Manx Telecom. The island does not have its own ITU country code, but is accessed via the British country code (+44), and the island's telephone numbers are part of the British telephone numbering plan, with local dialling codes 01624 for landlines and 07524, 07624 and 07924 for mobiles. Calls to the island from the UK however, are generally charged differently from those within the UK, and may or may not be included in any "inclusive minutes" packages.

In 1996, the Isle of Man Government obtained permission to use the .im national top-level domain (TLD), and has ultimate responsibility for its use. The domain is managed from day to day by Domicilium, an island-based internet service provider.

In December 2007, the Manx Electricity Authority and its telecommunications subsidiary, e-llan Communications, commissioned the laying of a new fibre-optic link that connects the island to a worldwide fibre-optic network.

The Isle of Man has three radio stations: Manx Radio, Energy FM and 3 FM.

There is no insular television service, but local transmitters retransmit British mainland digital broadcasts via the free-to-air digital terrestrial service Freeview. The Isle of Man is served by BBC North West for BBC One and BBC Two television services, and ITV Granada for ITV.

Many television services are available by satellite, such as Sky, and Freesat from the group of satellites at 28.2° East, as well as services from a range of other satellites around Europe such as the Astra satellites at 19.2° east and Hotbird.

The Isle of Man has three newspapers, all weeklies, and all owned by Isle of Man Newspapers, a division of the Edinburgh media company Johnston Press. The "Isle of Man Courier" (distribution 36,318) is free and distributed to homes on the island. The other two newspapers are "Isle of Man Examiner" (circulation 13,276) and the "Manx Independent" (circulation 12,255).

Postal services are the responsibility of Isle of Man Post, which took over from the British General Post Office in 1973.

There is a comprehensive bus network, operated by the government-owned bus operator Bus Vannin.

The Isle of Man Sea Terminal in Douglas has regular ferries to and from Heysham and to and from Liverpool, with a more restricted timetable operating in winter. There are also limited summer-only services to and from Belfast and Dublin. The Dublin route also operates at Christmas. At the time of the Isle of Man TT a limited number of sailings operate to and from Larne in Northern Ireland. All ferries are operated by the Isle of Man Steam Packet Company.

The only commercial airport on the island is the Isle of Man Airport at Ronaldsway. There are direct scheduled and chartered flights to numerous airports in the United Kingdom and Ireland.

The island has a total of of public roads, all of which are paved. There is no overriding national speed limit; only local speed limits are set, and some roads have no speed limit. Rules about reckless driving and most other driving regulations are enforced in a similar way to the UK. There is a requirement for regular vehicle examinations for some vehicles (similar to the MoT test in the UK).

The island used to have an extensive narrow-gauge railway system, both steam-operated and electric, but the majority of the steam railway tracks were taken out of service many years ago, and the track removed. , there is a steam railway between Douglas and Port Erin, an electric railway between Douglas and Ramsey and an electric mountain railway which climbs Snaefell.

One of the oldest operating horse tram services is located on the sea front in the capital, Douglas. It was founded in 1876.

The Isle of Man has become a centre for emerging private space travel companies. A number of the competitors in the Google Lunar X Prize, a $30 million competition for the first privately funded team to send a robot to the Moon, are based on the island. The team summit for the X Prize was held on the island in October 2010. In January 2011 two research space stations owned by Excalibur Almaz arrived on the island and were kept in an aircraft hangar at the airfield at the former RAF Jurby near Jurby.

The electricity supply on the Isle of Man is run by the Manx Electricity Authority. The Isle of Man is connected to Great Britain's national grid by a 40 MW alternating current link (Isle of Man to England Interconnector). There are also hydroelectric, natural gas and diesel generators. The government has also planned a 700 MW offshore wind farm, roughly half the size of Walney Wind Farm.

The culture of the Isle of Man is often promoted as being influenced by its Celtic and, to a lesser extent, its Norse origins. Proximity to the UK, popularity as a UK tourist destination in Victorian times, and immigration from Britain has meant that the cultures of Great Britain have been influential at least since Revestment. Revival campaigns have attempted to preserve the surviving vestiges of Manx culture after a long period of Anglicisation, and there has been significantly increased interest in the Manx language, history and musical tradition.

The official language of the Isle of Man is English. Manx has traditionally been spoken but has been stated to be "critically endangered". However it now has a growing number of young speakers.

Manx is a Goidelic Celtic language and is one of a number of insular Celtic languages spoken in the British Isles. Manx has been officially recognised as a legitimate autochthonous regional language under the European Charter for Regional or Minority Languages, ratified by the United Kingdom on 27 March 2001 on behalf of the Isle of Man government.

Manx is closely related to Irish and Scottish Gaelic, but is orthographically sui generis.

On the island, the Manx greetings ' (good morning) and ' (good afternoon) can often be heard. As in Irish and Scottish Gaelic, the concepts of "evening" and "afternoon" are referred to with one word. Two other Manx expressions often heard are "Gura mie eu" ("Thank you"; familiar 2nd person singular form "Gura mie ayd") and "", meaning "time enough", this represents a stereotypical view of the Manx attitude to life.

In the 2011 Isle of Man census, approximately 1,800 residents could read, write, and speak the Manx language.

For centuries, the island's symbol has been the so-called "three legs of Mann" (), a triskelion of three legs conjoined at the thigh. The Manx triskelion, which dates back with certainty to the late 13th century, is of uncertain origin. It has been suggested that its origin lies in Sicily, an island which has been associated with the triskelion since ancient times.

The symbol appears in the island's official flag and official coat of arms, as well as its currency. The Manx triskelion may be reflected in the island's motto, , which appears as part of the island's coat of arms. The Latin motto translates into English as "whichever way you throw, it will stand" or "whithersoever you throw it, it will stand". It dates to the late 17th century when it is known to have appeared on the island's coinage. It has also been suggested that the motto originally referred to the poor quality of coinage which was common at the time—as in "however it is tested it will pass".

The ragwort or "cushag" has been referred to as the Manx national flower.

The predominant religious tradition of the island is Christianity. Before the Protestant Reformation, the island had a long history as part of the unified Western Church, and in the years following the Reformation, the religious authorities on the island, and later the population of the island, accepted the religious authority of the British monarchy and the Church of England. It has also come under the influence of Irish religious tradition. The island forms a separate diocese called Sodor and Man, which in the distant past comprised the medieval kingdom of Man and the Scottish isles ("Suðreyjar" in Old Norse). It now consists of 16 parishes, and since 1541 has formed part of the Province of York. (These modern ecclesiastical parishes do not correspond to the island's ancient parishes mentioned in this article under "Local Government", but more closely reflect the current geographical distribution of population.)

Other Christian churches also operate on the Isle of Man. The second largest denomination is the Methodist Church, whose Isle of Man District is close in numbers to the Anglican diocese. There are eight Catholic parish churches, included in the Catholic Archdiocese of Liverpool, as well as a presence of Eastern Orthodox Christians. Additionally there are five Baptist churches, four Pentecostal churches, the Salvation Army, a ward of The Church of Jesus Christ of Latter-day Saints, two congregations of Jehovah's Witnesses, two United Reformed churches, as well as other Christian churches. There is a small Muslim community, with its own mosque in Douglas, and there is also a small Jewish community; see history of the Jews in the Isle of Man.

In Manx mythology, the island was ruled by the sea god Manannán, who would draw his misty cloak around the island to protect it from invaders. One of the principal folk theories about the origin of the name "Mann" is that it is named after Manannán.

In the Manx tradition of folklore, there are many stories of mythical creatures and characters. These include the , a malevolent spirit which, according to legend, blew the roof off St Trinian's Church in a fit of rage; the ; the ; and the , a ghostly black dog which wandered the walls and corridors of Peel Castle.

The Isle of Man is also said to be home to fairies, known locally as "the little folk" or "themselves". There is a famous Fairy Bridge, and it is said to be bad luck if one fails to wish the fairies good morning or afternoon when passing over it. It used to be a tradition to leave a coin on the bridge to ensure good luck. Other types of fairies are the and the .

An old Irish story tells how Lough Neagh was formed when Ireland's legendary giant Fionn mac Cumhaill (commonly anglicised to Finn McCool) ripped up a portion of the land and tossed it at a Scottish rival. He missed, and the chunk of earth landed in the Irish Sea, thus creating the island.

Peel Castle has been proposed as a possible location of the Arthurian Avalon or as the location of the Grail Castle, site of Lancelot's encounter with the sword bridge of King Maleagant.

One of the most often-repeated myths is that people found guilty of witchcraft were rolled down Slieau Whallian, a hill near St John's, in a barrel. However this is a 19th-century legend which comes from a Scottish legend, which in turn comes from a German legend. It never happened. Separately, a witchcraft museum was opened at the Witches Mill, Castletown in 1951. There has never actually been a witches' coven on that site; the myth was only created with the opening of the museum. However, there has been a strong tradition of herbalism and the use of charms to prevent and cure illness and disease in people and animals.

The music of the Isle of Man reflects Celtic, Norse and other influences, including from its neighbours, Scotland, Ireland, England and Wales. A wide range of music is performed on the island, such as rock, blues, jazz and pop.

Its traditional folk music has undergone a revival since the 1970s, starting with a music festival called in Ramsey. This was part of a general revival of the Manx language and culture after the death of the last native speaker of Manx in 1974.

The Isle of Man was mentioned in the Who song "Happy Jack" as the homeland of the song's titular character, who is always in a state of ecstasy, no matter what happens to him. The song 'The Craic was 90 in the Isle of Man' by Christy Moore describes a lively visit during the Island's tourism heyday. The Island is also the birthplace of Maurice, Robin and Barry Gibb, of the Bee Gees.

In the past, the basic national dish of the island was "spuds and herrin", boiled potatoes and herring. This plain dish was supported by the subsistence farmers of the island, who crofted the land and fished the sea for centuries. A more recent claim for the title of national dish could be the ubiquitous chips, cheese and gravy. This dish, which is similar to poutine, is found in most of the island's fast-food outlets, and consists of thick cut chips, covered in shredded Manx Cheddar cheese and topped with a thick gravy. However, as of the Isle of Man Food & Drink Festival 2018, queenies have been crowned the Manx national dish.

Seafood has traditionally accounted for a large proportion of the local diet. Although commercial fishing has declined in recent years, local delicacies include Manx kippers (smoked herring) which are produced by the smokeries in Peel on the west coast of the island, albeit mainly from North Sea herring these days. The smokeries also produce other specialities including smoked salmon and bacon.

Crab, lobster and scallops are commercially fished, and the queen scallop ("queenies") is regarded as a particular delicacy, with a light, sweet flavour. Cod, ling and mackerel are often angled for the table, and freshwater trout and salmon can be taken from the local rivers and lakes, supported by the government fish hatchery at Cornaa on the east coast.

Cattle, sheep, pigs and poultry are all commercially farmed; Manx lamb from the hill farms is a popular dish. The Loaghtan, the indigenous breed of Manx sheep, has a rich, dark meat that has found favour with chefs, featuring in dishes on the BBC's "MasterChef" series.

Manx cheese has also found some success, featuring smoked and herb-flavoured varieties, and is stocked by many of the UK's supermarket chains. Manx cheese took bronze medals in the 2005 British Cheese Awards, and sold 578 tonnes over the year. Manx cheddar has been exported to Canada where it is available in some supermarkets.

Beer is brewed on a commercial scale by Okells Brewery, which was established in 1850 and is the island's largest brewer; and also by Bushy's Brewery and the Hooded Ram Brewery. The Isle of Man's Pure Beer Act of 1874, which resembles the German "Reinheitsgebot", is still in effect: under this Act, brewers may only use water, malt, sugar and hops in their brews.

The Isle of Man is represented as a nation in the Commonwealth Games and the Island Games and hosted the IV Commonwealth Youth Games in 2011. Manx athletes have won three gold medals at the Commonwealth Games, including the one by cyclist Mark Cavendish in 2006 in the Scratch race. The Island Games were first held on the island in 1985, and again in 2001. In 2019, FC Isle of Man was founded and is a North West Counties League team. 

Isle of Man teams and individuals participate in many sports both on and off the island including rugby union, football, gymnastics, field hockey, netball, taekwondo, bowling, obstacle course racing and cricket. It being an island, many types of watersports are also popular with residents.

The main international event associated with the island is the Isle of Man Tourist Trophy race, colloquially known as "The TT", which began in 1907. It takes place in late May and early June. The TT is now an international road racing event for motorcycles, which used to be part of the World Championship, and is long considered to be one of the "greatest motorcycle sporting events of the world". Taking place over a two-week period, it has become a festival for motorcycling culture, makes a huge contribution to the island's economy and has become part of Manx identity. For many, the Isle carries the title "road racing capital of the world".

The Manx Grand Prix is a separate motorcycle event for amateurs and private entrants that uses the same Snaefell Mountain Course in late August and early September.

Prior to the introduction of football in the 19th century, Cammag was the island's traditional sport. It is similar to the Irish hurling and the Scottish game of shinty. Nowadays there is an annual match at St John's.

The Isle of Man has two cinemas, both in Douglas. The Broadway Cinema is located in the government-owned and -run Villa Marina and Gaiety Theatre complex. It has a capacity of 154 and also doubles as a conference venue.

The Palace Cinema is located next to the derelict Castle Mona hotel and is operated by the Sefton Group. It has two screens: Screen One holds 293 customers, while Screen Two is smaller with a capacity of just 95. It was extensively refurbished in August 2011.

Two domestic animals are specifically connected to the Isle of Man, though they are also found elsewhere.

The Manx cat is a breed of cat noted for its genetic mutation that causes it to have a shortened tail. The length of this tail can range from a few inches, known as a "stumpy", to being completely nonexistent, or "rumpy". Manx cats display a range of colours and usually have somewhat longer hind legs compared to most cats. The cats have been used as a symbol of the Isle of Man on coins and stamps and at one time the Manx government operated a breeding centre to ensure the continuation of the breed.

The Manx Loaghtan sheep is a breed native to the island. It has dark brown wool and four, or sometimes six, horns. The meat is considered to be a delicacy. There are several flocks on the island and others have been started in England and Jersey.

A more recent arrival on the island is the red-necked wallaby, which is now established on the island following an escape from the Wildlife Park. The local police report an increasing number of wallaby-related calls.

There are also many feral goats in Garff, a matter which was raised in Tynwald Court in January 2018.

In March 2016, the Isle of Man became the first entire territory to be adopted into UNESCO's Network of Biosphere Reserves.

The Isle of Man has a temperate oceanic climate (Köppen "Cfb"). Average rainfall is higher than averaged over the territory of the British Isles, because the Isle of Man is far enough from Ireland for the prevailing south-westerly winds to accumulate moisture. Average rainfall is highest at Snaefell, where it is around a year. At lower levels it can be around a year. The highest recorded temperature was at Ronaldsway on 12 July 1983.

On 10 May 2019 Chief Minister Howard Quayle stated that the Isle of Man Government recognises that a state of emergency exists due to the threat of anthropogenic climate change.






</doc>
<doc id="14729" url="https://en.wikipedia.org/wiki?curid=14729" title="Italic languages">
Italic languages

The Italic languages form a branch of the Indo-European language family, whose earliest known members were spoken in the Italian Peninsula in the first millennium BC. The best known of them is Latin, the official language of the Roman Empire, which conquered the other Italic peoples before the common era. The other Italic languages became extinct in the first centuries AD as their speakers were assimilated into the Roman Empire and shifted to some form of Latin. Between the third and eighth centuries AD, Vulgar Latin (perhaps influenced by language-shift from the other Italic languages) diversified into the Romance languages, which are the only Italic languages natively spoken today.

Besides Latin, the known ancient Italic languages are Faliscan (the closest to Latin), Umbrian and Oscan (or Osco-Umbrian), and South Picene. Other Indo-European languages once spoken in the peninsula, whose inclusion in the Italic branch is disputed, are Aequian, Vestinian, Venetic and Sicel. These long-extinct languages are known only from inscriptions in archaeological finds.

In the first millennium BC, several (other) non-Italic languages were spoken in the peninsula, including members of other branches of Indo-European (such as Celtic and Greek) as well as at least one non-Indo-European one, Etruscan.

It is generally believed that those 1st millennium Italic languages descend from Indo-European languages brought by migrants to the peninsula sometime in the 2nd millennium BC. However, the source of those migrations and the history of the languages in the peninsula are still the matter of debate among historians. In particular, it is debated whether the ancient Italic languages all descended from a single Proto-Italic language after its arrival in the region, or whether the migrants brought two or more Indo-European languages that were only distantly related.

With over 800 million native speakers, the Romance languages make Italic the second-most-widely spoken branch of the Indo-European family, after Indo-Iranian. However, in academia the ancient Italic languages form a separate field of study from the medieval and modern Romance languages. This article focuses on the ancient languages. For the others, see Romance studies.

All Italic languages (including Romance) are generally written in Old Italic scripts (or the descendant Latin alphabet and its adaptations), which descend from the alphabet used to write the non-Italic Etruscan language, and ultimately from the Greek alphabet.

Historical linguists have generally concluded that the ancient Indo-European languages of the Italian peninsula, that were not identifiable as belonging to other branches of Indo-European such as Greek, all belonged to a single branch of the family, parallel for example to Celtic and Germanic. The founder of this theory is Antoine Meillet.

This unitary theory has been criticized by, among others, Alois Walde, Vittore Pisani and Giacomo Devoto, who proposed that the Latino-Faliscan and Osco-Umbrian languages constituted two distinct branches of Indo-European. This view gained acceptance in the second half of the 20th century, though proponents such as Rix would later reject the idea, and the unitary theory remains dominant.

The following classification, proposed by Michiel de Vaan (2008), is generally agreed on, although some scholars have recently rejected the position of Venetic within the Italic branch.


Proto-Italic was originally spoken by Italic tribes north of the Alps. Early contacts with Celtic and Germanic speakers are also suggested by linguistic evidence. Italic peoples probably moved towards the Italian Peninsula during the second half of the 2nd millennium BCE, gradually reaching the southern regions.

Although an equation between archeological and linguistic evidence cannot be established with certainty, the Proto-Italic language is generally associated with the Terramare (1700–1150 BCE) and Proto-Villanovan culture (1200–900 BCE).

At the start of the Iron Age, around 700 BC, Ionian Greek settlers from Euboea established colonies along the coast of southern Italy. They brought with them the alphabet, which they had learned from the Phoenicians; specifically, what we now call Western Greek alphabet. The invention quickly spread through the whole peninsula, across language and political barriers. Local adaptations (mainly minor letter shape changes and the dropping or addition of a few letters) yielded several Old Italic alphabets.

The inscriptions show that, by 700 BC, many languages were spoken in the region, including members of several branches of Indo-European and several non-Indo-European languages. The most important of the latter was Etruscan, attested by evidence from more than 10,000 inscriptions and some short texts. No relation has been found between Etruscan and any other known language, and there is still no clue about its possible origin (except for inscriptions on the island of Lemnos in the eastern Mediterranean). Other possibly non-Indo-European languages present at the time were Rhaetian in the Alpine region, Ligurian around present-day Genoa, and some unidentified language(s) in Sardinia. Those languages have left some detectable imprint in Latin.

The largest language in southern Italy, except Ionic Greek spoken in the Greek colonies, was Messapian, known due to some 260 inscriptions dating from the 6th and 5th centuries BC. There is a historical connection of Messapian with the Illyrian tribes, added to the archaeological connection in ceramics and metals existing between both peoples, which motivated the hypothesis of linguistic connection. But the evidence of Illyrian inscriptions is reduced to personal names and places, which makes it difficult to support such a hypothesis.

It has also been proposed that the Lusitanian language may have belonged to the Italic family.

In the history of Latin of ancient times, there are several periods:

As the Roman Republic extended its political dominion over the whole of the Italian peninsula, Latin became dominant over the other Italic languages, which ceased to be spoken perhaps sometime in the 1st century AD. From Vulgar Latin, the Romance languages emerged.

The Latin language gradually spread beyond Rome, along with the growth of the power of this state, displacing, beginning in the 4th and 3rd centuries BC, the languages of other Italic tribes, as well as Illyrian, Messapian and Venetic, etc. The Romanisation of the Italian Peninsula was basically complete by the 1st century BC; except for the south of Italy and Sicily, where the dominance of Greek was preserved.
The attribution of Ligurian is controversial.

The period of late Latin (2nd to 6th centuries) is characterised by a gap between written and folk-spoken language: the regional differentiation of the people's Latin was accelerated, the formation of Romance languages, finally separated by the 9th century, began on its basis; written Latin continued to be used for a long time in the administrative sphere, religion, diplomacy, trade, school, medicine, science, literature, and remains the language of the Catholic Church and the official language of the Vatican City.

The Italian peninsula has been inhabited by tool-making hominids since at least 730,000 BC, by Neanderthals since 120,000 BC, and by anatomically modern humans since about 35,000 BC.

The Pleistocene glaciations caused the human populations of central and northern Europe to migrate southwards. At the height of the last Ice Age (about 35,000 to 13,000 BC), the sea level was about 120 meters below its present state, which radically changed the geography of the Mediterranean.

Agriculture reached the peninsula from the Middle East between 7000 and 6000 BC, marking the start of the Neolithic. The seminomadic hunter-gathering lifestyle was replaced by a sedentary society, with large fortified villages and an economy based on agriculture and animal husbandry. While raised primarily as a source of meat, by 3000 BC domesticated animals were being exploited for traction (of plows and carts) and other product such as milk and wool. The making of wine and olive oil was learned from Greece by about that time.

Metallurgy also spread through the Mediterranean at this time, first of copper around 3000 BC, then bronze around 2300 BC. Use of the latter for weapons, armor, and other artifacts marks the beginning of the Bronze Age. Around 700 BC, the development of iron smelting and steelmaking marked the beginning of the Iron Age in the region.

The main debate concerning the origin of the Italic languages mirrors that on the origins of the Greek ones, except that there is no record of any "early Italic" to play the role of Mycenaean Greek.

All we know about the linguistic landscape of Italy is from inscriptions made after the introduction of the alphabet in the peninsula, around 700 BC onwards, and from Greek and Roman writers several centuries later. The oldest known samples come from Umbrian and Faliscan inscriptions from the 7th century BC. Their alphabets were clearly derived from the Etruscan alphabet, which was derived from the Western Greek alphabet not much earlier than that. There is no reliable information about the languages spoken before that time. Some conjectures can be made based on toponyms, but they cannot be verified.

There is no guarantee that the intermediate phases between those old Italic languages and Indo-European will be found. The question of whether Italic originated outside Italy or developed by assimilation of Indo-European and other elements within Italy, approximately on or within its current range there, remains.

An extreme view of some linguists and historians is that there is no such thing as "the Italic branch" of Indo-European. Namely, there never was a unique "Proto-Italic", whose diversification resulted in those languages. Some linguists, like Silvestri and Rix, further argue that no common Proto-Italic can be reconstructed such that (1) its phonological system may have developed into those of Latin and Osco-Umbrian through consistent phonetic changes, and (2) its phonology and morphology can be consistently derived from those of Proto-Indo-European. However, Rix later changed his mind and became an outspoken supporter of Italic as a family.

Those linguists propose instead that the ancestors of the 1st millennium Indo-European languages of Italy were two or more different languages, that separately descended from Indo-European in a more remote past, and separately entered Europe, possibly by different routes and/or in different epochs. That view stems in part from the difficulty in identifying a common Italic homeland in prehistory, or reconstructing an ancestral "Common Italic" or "Proto-Italic" language from which those languages could have descended. Some common features that seem to connect the languages may be just a sprachbund phenomenon – a linguistic convergence due to contact over a long period, as in the Italo-Celtic theory.

Bakkum defines Proto-Italic as a "chronological stage" without an independent development of its own, but extending over late Proto-Indo-European and the initial stages of Proto-Latin and Proto-Sabellic. Meiser's dates of 4000 BC to 1800 BC, well before Mycenaean Greek, are described by him as "as good a guess as anyone's".

General and specific characteristics of the pre-Roman Italic languages:


The Italic languages share a certain number of isoglosses and common phonetic changes with respect to the common Proto-Indo-European:

In grammar there are basically three innovations shared by the Osco-Umbrian and the Latino-Faliscan languages:

In turn, these shared innovations are one of the main arguments in favour of an Italic group, questioned by other authors.

In addition, Latin and other Italic languages have an innovative future form derived from "*-bʰō, *-bʰesi, *-bʰeti, ...". This form appears for example in the Latin form "amabo et amabis" 'I shall love and you shall love' and in the Faliscan form "cra carefo" ('tomorrow I will not have', Latin "crās carēbo").

Among the Indo-European languages, the Italic languages share a higher percentage of lexicon with the Celtic and the Germanic ones, three of the four traditional "centum" branches of Indo-European (together with Greek).

The following table shows a lexical comparison of several Italic languages:

The asterisk indicates reconstructed forms based on indirect linguistic evidence and not forms directly attested in any inscription.
From the point of view of Proto-Indo-European, the Italic languages are fairly conservative. In phonology, the Italic languages are centum languages by merging the palatals with the velars (Latin "centum" has a /k/) but keeping the combined group separate from the labio-velars. In morphology, the Italic languages preserve six cases in the noun and the adjective (nominative, accusative, genitive, dative, ablative, vocative) with traces of a seventh (locative), but the dual of both the noun and the verb has completely disappeared. From the position of both morphological innovations and uniquely shared lexical items, Italic shows the greatest similarities with Celtic and Germanic, with some of the shared lexical correspondences also being found in Baltic and Slavic.

Similar to Celtic languages, the Italic languages are also divided into P- and Q-branches, depending on the reflex of Proto-Indo-European *"kʷ". In the languages of the Osco-Umbrian branch, *"kʷ" gave "p", whereas the languages of the Latino-Faliscan branch preserved it (Latin "qu" ).




</doc>
<doc id="14730" url="https://en.wikipedia.org/wiki?curid=14730" title="Internet Relay Chat">
Internet Relay Chat

Internet Relay Chat (IRC) is an application layer protocol that facilitates communication in the form of text. The chat process works on a client/server networking model. IRC clients are computer programs that users can install on their system or web based applications running either locally in the browser or on a 3rd party server. These clients communicate with chat servers to transfer messages to other clients. IRC is mainly designed for group communication in discussion forums, called channels, but also allows one-on-one communication via private messages as well as chat and data transfer, including file sharing.

Client software is available for every major operating system that supports Internet access. As of April 2011, the top 100 IRC networks served more than half a million users at a time, with hundreds of thousands of channels operating on a total of roughly 1,500 servers out of roughly 3,200 servers worldwide. IRC usage has been declining steadily since 2003, losing 60% of its users (from 1 million to about 400,000 in 2012) and half of its channels (from half a million in 2003).

IRC was created by Jarkko Oikarinen in August 1988 to replace a program called MUT (MultiUser Talk) on a BBS called OuluBox at the University of Oulu in Finland, where he was working at the Department of Information Processing Science. Jarkko intended to extend the BBS software he administered, to allow news in the Usenet style, real time discussions and similar BBS features. The first part he implemented was the chat part, which he did with borrowed parts written by his friends Jyrki Kuoppala and Jukka Pihl. The first IRC network was running on a single server named tolsun.oulu.fi. Oikarinen found inspiration in a chat system known as Bitnet Relay, which operated on the BITNET.

Jyrki Kuoppala pushed Oikarinen to ask Oulu University to free the IRC code so that it also could be run outside of Oulu, and after they finally got it released, Jyrki Kuoppala immediately installed another server. This was the first "irc network". Oikarinen got some friends at the Helsinki University and Tampere University to start running IRC servers when his number of users increased and other universities soon followed. At this time Oikarinen realized that the rest of the BBS features probably wouldn't fit in his program.

Oikarinen got in touch with people at the University of Denver and Oregon State University. They had their own IRC network running and wanted to connect to the Finnish network. They had obtained the program from one of Oikarinen's friends, Vijay Subramaniam—the first non-Finnish person to use IRC. IRC then grew larger and got used on the entire Finnish national network—Funet—and then connected to Nordunet, the Scandinavian branch of the Internet. In November 1988, IRC had spread across the Internet and in the middle of 1989, there were some 40 servers worldwide.

In August 1990, the first major disagreement took place in the IRC world. The "A-net" (Anarchy net) included a server named eris.berkeley.edu. It was all open, required no passwords and had no limit on the number of connects. As Greg "wumpus" Lindahl explains: "it had a wildcard server line, so people were hooking up servers and nick-colliding everyone". The "Eris Free Network", EFnet, made the eris machine the first to be Q-lined (Q for quarantine) from IRC. In wumpus' words again: "Eris refused to remove that line, so I formed EFnet. It wasn't much of a fight; I got all the hubs to join, and almost everyone else got carried along." A-net was formed with the eris servers, while EFnet was formed with the non-eris servers. History showed most servers and users went with EFnet. Once ANet disbanded, the name EFnet became meaningless, and once again it was the one and only IRC network.

It is around that time that IRC was used to report on the 1991 Soviet coup d'état attempt throughout a media blackout. It was previously used in a similar fashion during the Gulf War. Chat logs of these and other events are kept in the ibiblio archive.

Another fork effort, the first that really made a big and lasting difference, was initiated by 'Wildthang' in the U.S. October 1992 (it forked off the EFnet ircd version 2.8.10). It was meant to be just a test network to develop bots on but it quickly grew to a network "for friends and their friends". In Europe and Canada a separate new network was being worked on and in December the French servers connected to the Canadian ones, and by the end of the month, the French and Canadian network was connected to the US one, forming the network that later came to be called "The Undernet".

The "undernetters" wanted to take ircd further in an attempt to make it less bandwidth consumptive and to try to sort out the channel chaos (netsplits and takeovers) that EFnet started to suffer from. For the latter purpose, the Undernet implemented timestamps, new routing and offered the CService—a program that allowed users to register channels and then attempted to protect them from troublemakers. The very first server list presented, from 15 February 1993, includes servers from USA, Canada, France, Croatia and Japan. On 15 August, the new user count record was set to 57 users.

In May 1993, RFC 1459 was published and details a simple protocol for client/server operation, channels, one-to-one and one-to-many conversations. It is notable that a significant number of extensions like CTCP, colors and formats are not included in the protocol specifications, nor is character encoding, which led various implementations of servers and clients to diverge. In fact, software implementation varied significantly from one network to the other, each network implementing their own policies and standards in their own code bases.

During the summer of 1994, the Undernet was itself forked. The new network was called DALnet (named after its founder: dalvenjah), formed for better user service and more user and channel protections. One of the more significant changes in DALnet was use of longer nicknames (the original ircd limit being 9 letters). DALnet ircd modifications were made by Alexei "Lefler" Kosut. DALnet was thus based on the Undernet ircd server, although the DALnet pioneers were EFnet abandoners. According to James Ng, the initial DALnet people were "ops in #StarTrek sick from the constant splits/lags/takeovers/etc".

DALnet quickly offered global WallOps (IRCop messages that can be seen by users who are +w (/mode NickName +w)), longer nicknames, Q:Lined nicknames (nicknames that cannot be used i.e. ChanServ, IRCop, NickServ, etc.), global K:Lines (ban of one person or an entire domain from a server or the entire network), IRCop only communications: GlobOps, +H mode showing that an IRCop is a "helpop" etc. Much of DALnet's new functions were written in early 1995 by Brian "Morpher" Smith and allow users to own nicknames, control channels, send memos, and more.

In July 1996, after months of flame wars and discussions on the mailing list, there was yet another split due to disagreement in how the development of the ircd should evolve. Most notably, the "european" (most of those servers were in Europe) side that later named itself IRCnet argued for nick and channel delays where the EFnet side argued for timestamps. There were also disagreements about policies: the European side had started to establish a set of rules directing what IRCops could and could not do, a point of view opposed by the US side.

Most (not all) of the IRCnet servers were in Europe, while most of the EFnet servers were in the US. This event is also known as "The Great Split" in many IRC societies. EFnet has since (as of August 1998) grown and passed the number of users it had then. In the (northern) autumn of the year 2000, EFnet had some 50,000 users and IRCnet 70,000.

IRC has changed much over its life on the Internet. New server software has added a multitude of new features.

, a new standardization effort is under way under a working group called IRCv3, which focuses on more advanced client features like instant notifications, better history support and improved security. , no major IRC networks have fully adopted the proposed standard.

After its golden era during the 1990s and early 2000s (240,000 users on QuakeNet in 2004), IRC has seen a significant decline, losing around 60% of users between 2003 and 2012, with users moving to newer social media platforms like Facebook or Twitter, but also to open platforms like XMPP which was developed in 1999. Certain networks like Freenode have not followed the overall trend and have more than quadrupled in size during the same period. As of 2016, Freenode is the largest IRC network with around 90,000 users.

The largest IRC networks have traditionally been grouped as the "Big Four"—a designation for networks that top the statistics. The Big Four networks change periodically, but due to the community nature of IRC there are a large number of other networks for users to choose from.

Historically the "Big Four" were:

IRC reached 6 million simultaneous users in 2001 and 10 million users in 2003, dropping to 371k in 2018.

, the largest IRC networks are:

Today, the top 100 IRC networks have around 370k users connected at peak hours. 

IRC is an open protocol that uses TCP and, optionally, TLS. An IRC server can connect to other IRC servers to expand the IRC network. Users access IRC networks by connecting a client to a server. There are many client implementations, such as mIRC, HexChat and irssi, and server implementations, e.g. the original IRCd. Most IRC servers do not require users to register an account but a nick is required before being connected.

IRC was originally a plain text protocol (although later extended), which on request was assigned port 194/TCP by IANA. However, the "de facto" standard has always been to run IRC on 6667/TCP and nearby port numbers (for example TCP ports 6660–6669, 7000) to avoid having to run the IRCd software with root privileges.

The protocol specified that characters were 8-bit but did not specify the character encoding the text was supposed to use. This can cause problems when users using different clients and/or different platforms want to converse.

All client-to-server IRC protocols in use today are descended from the protocol implemented in the irc2.4.0 version of the IRC2 server, and documented in RFC 1459. Since RFC 1459 was published, the new features in the irc2.10 implementation led to the publication of several revised protocol documents (RFC 2810, RFC 2811, RFC 2812 and RFC 2813); however, these protocol changes have not been widely adopted among other implementations.

Although many specifications on the IRC protocol have been published, there is no official specification, as the protocol remains dynamic. Virtually no clients and very few servers rely strictly on the above RFCs as a reference.

Microsoft made an extension for IRC in 1998 via the proprietary IRCX. They later stopped distributing software supporting IRCX, instead developing the proprietary MSNP.

The standard structure of a network of IRC servers is a tree. Messages are routed along only necessary branches of the tree but network state is sent to every server and there is generally a high degree of implicit trust between servers. However, this architecture has a number of problems. A misbehaving or malicious server can cause major damage to the network and any changes in structure, whether intentional or a result of conditions on the underlying network, require a net-split and net-join. This results in a lot of network traffic and spurious quit/join messages to users and temporary loss of communication to users on the splitting servers. Adding a server to a large network means a large background bandwidth load on the network and a large memory load on the server. Once established, however, each message to multiple recipients is delivered in a fashion similar to multicast, meaning each message travels a network link exactly once. This is a strength in comparison to non-multicasting protocols such as Simple Mail Transfer Protocol (SMTP) or Extensible Messaging and Presence Protocol (XMPP).

An IRC daemon can also be used on a local area network (LAN). IRC can thus be used to facilitate communication between people within the local area network (internal communication).

IRC has a line-based structure. Clients send single-line messages to the server, receive replies to those messages and receive copies of some messages sent by other clients. In most clients, users can enter commands by prefixing them with a '/'. Depending on the command, these may either be handled entirely by the client, or (generally for commands the client does not recognize) passed directly to the server, possibly with some modification.

Due to the nature of the protocol, automated systems cannot always correctly pair a sent command with its reply with full reliability and are subject to guessing.

The basic means of communicating to a group of users in an established IRC session is through a "channel". Channels on a network can be displayed using the IRC command "LIST", which lists all currently available channels that do not have the modes +s or +p set, on that particular network.

Users can "join" a channel using the "JOIN" command, in most clients available as "/join #channelname". Messages sent to the joined channels are then relayed to all other users.

Channels that are available across an entire IRC network are prefixed with a '#', while those local to a server use '&'. Other less common channel types include '+' channels—'modeless' channels without operators—and '!' channels, a form of timestamped channel on normally non-timestamped networks.

Users and channels may have "modes" that are represented by single case-sensitive letters and are set using the "MODE" command. User modes and channel modes are separate and can use the same letter to mean different things (e.g. user mode "i" is invisible mode while channel mode "i" is invite only.) Modes are usually set and unset using the mode command that takes a target (user or channel), a set of modes to set (+) or unset (-) and any parameters the modes need.

Some channel modes take parameters and other channel modes apply to a user on a channel or add or remove a mask (e.g. a ban mask) from a list associated with the channel rather than applying to the channel as a whole. Modes that apply to users on a channel have an associated symbol that is used to represent the mode in names replies (sent to clients on first joining a channel and use of the names command) and in many clients also used to represent it in the client's displayed list of users in a channel or to display an own indicator for a user's modes.

In order to correctly parse incoming mode messages and track channel state the client must know which mode is of which type and for the modes that apply to a user on a channel which symbol goes with which letter. In early implementations of IRC this had to be hard-coded in the client but there is now a de facto standard extension to the protocol called ISUPPORT that sends this information to the client at connect time using numeric 005.

There is a small design fault in IRC regarding modes that apply to users on channels: the names message used to establish initial channel state can only send one such mode per user on the channel, but multiple such modes can be set on a single user. For example, if a user holds both operator status (+o) and voice status (+v) on a channel, a new client will be unable to see the mode with less priority (i.e. voice). Workarounds for this are possible on both the client and server side but none are widely implemented.

Many daemons and networks have added extra modes or modified the behavior of modes in the above list.

A "Channel Operator" is a client on an IRC channel that manages the channel.
IRC Channel Operators can be easily seen by the a symbol or icon next to their name (varies by client implementation, commonly a "@" symbol prefix, a green circle, or a Latin letter "+o"/"o").
On most networks, an operator can:

There are also users who maintain elevated rights on their local server, or the entire network; these are called IRC operators, sometimes shortened to IRCops or Opers (not to be confused with channel operators). As the implementation of the IRCd varies, so do the privileges of the IRC operator on the given IRCd. RFC 1459 claims that IRC operators are "a necessary evil" to keep a clean state of the network, and as such they need to be able to disconnect and reconnect servers. Additionally, to prevent malicious users or even harmful automated programs from entering IRC, IRC operators are usually allowed to disconnect clients and completely ban IP addresses or complete subnets. Networks that carry services (NickServ et al.) usually allow their IRC operators also to handle basic "ownership" matters. Further privileged rights may include overriding channel bans (being able to join channels they would not be allowed to join, if they were not opered), being able to op themselves on channels where they would not be able without being opered, being auto-opped on channels always and so forth.

A hostmask is a unique identifier of an IRC client connected to an IRC server. IRC servers, services, and other clients, including bots, can use it to identify a specific IRC session.

The format of a hostmask is codice_1. The hostmask looks similar to, but should not be confused with an e-mail address.

The nick part is the nickname chosen by the user and may be changed while connected.
The user part is the username reported by ident on the client. If ident is not available on the client, the username specified when the client connected is used after being prefixed with a tilde.

The host part is the hostname the client is connecting from. If the IP address of the client cannot be resolved to a valid hostname by the server, it is used instead of the hostname.

Because of the privacy implications of exposing the IP address or hostname of a client, some IRC daemons also provide privacy features, such as InspIRCD or UnrealIRCd's "+x" mode. This hashes a client IP address or masks part of a client's hostname, making it unreadable to users other than IRCops. Users may also have the option of requesting a "virtual host" (or "vhost"), to be displayed in the hostmask to allow further anonymity. Some IRC networks such as Freenode use these as "cloaks" to indicate that a user is affiliated with a group or project.

There are three recognized uniform resource identifier (URI) schemes for Internet Relay Chat: codice_2, codice_3, and codice_4. When supported, they allow hyperlinks of various forms, including
(where items enclosed within brackets ([,]) are optional) to be used to (if necessary) connect to the specified host (or network, if known to the IRC client) and join the specified channel. (This can be used within the client itself, or from another application such as a Web browser). irc is the default URI, irc6 specifies a connection to be made using IPv6, and ircs specifies a secure connection.

Per the specification, the usual hash symbol (#) will be prepended to channel names that begin with an alphanumeric character—allowing it to be omitted. Some implementations (for example, mIRC) will do so "unconditionally" resulting in a (usually unintended) extra (for example, ##channel), if included in the URL.

Some implementations allow multiple channels to be specified, separated by commas.

Issues in the original design of IRC were the amount of shared state data being a limitation on its scalability, the absence of unique user identifications leading to the nickname collision problem, lack of protection from netsplits by means of cyclic routing, the trade-off in scalability for the sake of real-time user presence information, protocol weaknesses providing a platform for abuse, no transparent and optimizable message passing, and no encryption. Some of these issues have been addressed in "Modern IRC".

Because IRC connections may be unencrypted and typically span long time periods, they are an attractive target for DoS/DDoS attackers and hackers. Because of this, careful security policy is necessary to ensure that an IRC network is not susceptible to an attack such as a takeover war. IRC networks may also K-line or G-line users or servers that have a harming effect.

Some IRC servers support SSL/TLS connections for security purposes. This helps stop the use of packet sniffer programs to obtain the passwords of IRC users, but has little use beyond this scope due to the public nature of IRC channels. SSL connections require both client and server support (that may require the user to install SSL binaries and IRC client specific patches or modules on their computers). Some networks also use SSL for server-to-server connections, and provide a special channel flag (such as codice_5) to only allow SSL-connected users on the channel, while disallowing operator identification in clear text, to better utilize the advantages that SSL provides.

IRC served as an early laboratory for many kinds of Internet attacks, such as using fake ICMP unreachable messages to break TCP-based IRC connections (nuking) to annoy users or facilitate takeovers.

One of the most contentious technical issues surrounding IRC implementations, which survives to this day, is the merit of "Nick/Channel Delay" vs. "Timestamp" protocols. Both methods exist to solve the problem of denial-of-service attacks, but take very different approaches.
The problem with the original IRC protocol as implemented was that when two servers split and rejoined, the two sides of the network would simply merge their channels. If a user could join on a "split" server, where a channel that existed on the other side of the network was empty, and gain operator status, they would become a channel operator of the "combined" channel after the netsplit ended; if a user took a nickname that existed on the other side of the network, the server would kill both users when rejoining (i.e., 'nick-collision').
This was often abused to "mass-kill" all users on a channel, thus creating "opless" channels where no operators were present to deal with abuse. Apart from causing problems within IRC, this encouraged people to conduct denial-of-service attacks against IRC servers in order to cause netsplits, which they would then abuse.
The nick delay and channel delay strategies aim to prevent abuse by delaying reconnections and renames. After a user signs off and the nickname becomes available, or a channel ceases to exist because all its users parted (as often happens during a netsplit), the server will not allow any user to use that nickname or join that channel, until a certain period of time (the "delay") has passed. The idea behind this is that even if a netsplit occurs, it is useless to an abuser because they cannot take the nickname or gain operator status on a channel, and thus no collision of a nickname or 'merging' of a channel can occur. To some extent, this inconveniences legitimate users, who might be forced to briefly use a different name after rejoining (appending an underscore is popular).

The timestamp protocol is an alternative to nick/channel delays which resolves collisions using timestamped priority. Every nickname and channel on the network is assigned a timestampthe date and time when it was created. When a netsplit occurs, two users on each side are free to use the same nickname or channel, but when the two sides are joined, only one can survive. In the case of nicknames, the newer user, according to their TS, is killed; when a channel collides, the members (users on the channel) are merged, but the channel operators on the "losing" side of the split lose their channel operator status.

TS is a much more complicated protocol than ND/CD, both in design and implementation, and despite having gone through several revisions, some implementations still have problems with "desyncs" (where two servers on the same network disagree about the current state of the network), and allowing too much leniency in what was allowed by the 'losing' side. Under the original TS protocols, for example, there was no protection against users setting bans or other modes in the losing channel that would then be merged when the split rejoined, even though the users who had set those modes lost their channel operator status. Some modern TS-based IRC servers have also incorporated some form of ND and/or CD in addition to timestamping in an attempt to further curb abuse.

Most networks today use the timestamping approach. The timestamp versus ND/CD disagreements caused several servers to split away from EFnet and form the newer IRCnet. After the split, EFnet moved to a TS protocol, while IRCnet used ND/CD.

In recent versions of the IRCnet ircd, as well as ircds using the TS6 protocol (including Charybdis), ND has been extended/replaced by a mechanism called SAVE. This mechanism assigns every client a UID upon connecting to an IRC server. This ID starts with a number, which is forbidden in nicks (although some ircds, namely IRCnet and InspIRCd, allow clients to switch to their own UID as the nickname).

If two clients with the same nickname join from different sides of a netsplit ("nick collision"), the first server to see this collision will force "both" clients to change their nick to their UID, thus saving both clients from being disconnected. On IRCnet, the nickname will also be locked for some time (ND) to prevent both clients from changing back to the original nickname, thus colliding again.

Client software exists for various operating systems or software packages, as well as web-based or inside games. Many different clients are available for the various operating systems, including Windows, Unix and Linux, Mac OS X and mobile operating systems (such as iOS and Android). On Windows, mIRC is one of the most popular clients.

Some programs which are extensible through plug-ins also serve as platforms for IRC clients. For instance, a client called ERC, written entirely in Emacs Lisp, is included in v.22.3 of Emacs. Therefore, any platform that can run Emacs can run ERC.

A number of web browsers have built-in IRC clients, such as Opera (version 12.18 and earlier) and the ChatZilla add-on for Mozilla Firefox (for Firefox 56 and earlier; included as a built-in component of SeaMonkey). Web-based clients, such as Mibbit and open source KiwiIRC, can run in most browsers.

Games such as "War§ow", "Unreal Tournament" (up to Unreal Tournament 2004), "Uplink", "Spring Engine"-based games, 0 A.D. and "ZDaemon" have included IRC.

Ustream's chat interface is IRC with custom authentication as well as Twitch's (formerly Justin.tv).

A typical use of bots in IRC is to provide IRC services or specific functionality within a channel such as to host a chat-based game or provide notifications of external events. However, some IRC bots are used to launch malicious attacks such as denial of service, spamming, or exploitation.

A program that runs as a daemon on a server and functions as a persistent proxy is known as a BNC or bouncer. The purpose is to maintain a connection to an IRC server, acting as a relay between the server and client, or simply to act as a proxy. Should the client lose network connectivity, the BNC may stay connected and archive all traffic for later delivery, allowing the user to resume their IRC session without disrupting their connection to the server.

Furthermore, as a way of obtaining a bouncer-like effect, an IRC client (typically text-based, for example Irssi) may be run on an always-on server to which the user connects via ssh. This also allows devices that only have ssh functionality, but no actual IRC client installed themselves, to connect to the IRC, and it allows sharing of IRC sessions.

To keep the IRC client from quitting when the ssh connection closes, the client can be run inside a terminal multiplexer such as GNU Screen or tmux, thus staying connected to the IRC network(s) constantly and able to log conversation in channels that the user is interested in, or to maintain a channel's presence on the network. Modelled after this setup, in 2004 an IRC client following the client-server model, called Smuxi, was launched.

There are numerous search engines available to aid the user in finding what they are looking for on IRC. Generally the search engine consists of two parts, a "back-end" (or "spider/crawler") and a front-end "search engine".

The back-end (spider/webcrawler) is the work horse of the search engine. It is responsible for crawling IRC servers to index the information being sent across them. The information that is indexed usually consists solely of channel text (text that is publicly displayed in public channels). The storage method is usually some sort of relational database, like MySQL or Oracle.

The front-end "search engine" is the user interface to the database. It supplies users with a way to search the database of indexed information to retrieve the data they are looking for. These front-end search engines can also be coded in numerous programming languages.

Most search engines have their own spider that is a single application responsible for crawling IRC and indexing data itself; however, others are "user based" indexers. The latter rely on users to install their "add-on" to their IRC client; the add-on is what sends the database the channel information of whatever channels the user happens to be on.

Many users have implemented their own ad hoc search engines using the logging features built into many IRC clients. These search engines are usually implemented as bots and dedicated to a particular channel or group of associated channels.

IRC still lacks a single globally accepted standard convention for how to transmit characters outside the 7-bit ASCII repertoire.
IRC servers normally transfer messages from a client to another client just as byte sequences, without any interpretation or recoding of characters. The IRC protocol (unlike e.g. MIME or HTTP) lacks mechanisms for announcing and negotiating character encoding options. This has put the responsibility for choosing the appropriate character codec on the client. In practice, IRC channels have largely used the same character encodings that were also used by operating systems (in particular Unix derivatives) in the respective language communities:

Today, the UTF-8 encoding of Unicode/ISO 10646 would be the most likely contender for a single future standard character encoding for all IRC communication, if such standard ever relaxed the 510-byte message size restriction. UTF-8 is ASCII compatible and covers the superset of all other commonly used coded character set standards.

Much like conventional P2P file sharing, users can create file servers that allow them to share files with each other by using customised IRC bots or scripts for their IRC client. Often users will group together to distribute warez via a network of IRC bots.

Technically, IRC provides no file transfer mechanisms itself; file sharing is implemented by IRC "clients", typically using the Direct Client-to-Client (DCC) protocol, in which file transfers are negotiated through the exchange of private messages between clients. The vast majority of IRC clients feature support for DCC file transfers, hence the view that file sharing is an integral feature of IRC. The commonplace usage of this protocol, however, sometimes also causes DCC spam. DCC commands have also been used to exploit vulnerable clients into performing an action such as disconnecting from the server or exiting the client.







</doc>
<doc id="14731" url="https://en.wikipedia.org/wiki?curid=14731" title="Ideogram">
Ideogram

An ideogram or ideograph (from Greek "idea" and "to write") is a graphic symbol that represents an idea or concept, independent of any particular language, and specific words or phrases. Some ideograms are comprehensible only by familiarity with prior convention; others convey their meaning through pictorial resemblance to a physical object, and thus may also be referred to as pictograms.

In proto-writing, used for inventories and the like, physical objects are represented by stylized or conventionalized pictures, or pictograms. For example, the pictorial Dongba symbols without Geba annotation cannot represent the Naxi language, but are used as a mnemonic for reciting oral literature.
Some systems also use ideograms, symbols denoting abstract concepts.

The term "ideogram" is often used to describe symbols of writing systems such as Egyptian hieroglyphs, Sumerian cuneiform and Chinese characters. However, these symbols represent elements of a particular language, mostly words or morphemes (so that they are logograms), rather than objects or concepts. In these writing systems, a variety of strategies were employed in the design of logographic symbols.
Pictographic symbols depict the object referred to by the word, such as an icon of a bull denoting the Semitic word "ʾālep" "ox".
Some words denoting abstract concepts may be represented iconically, but most other words are represented using the rebus principle, borrowing a symbol for a similarly-sounding word. Later systems used selected symbols to represent the sounds of the language, for example the adaptation of the logogram for "ʾālep" "ox" as the letter aleph representing the initial sound of the word, a glottal stop.

Many signs in hieroglyphic as well as in cuneiform writing could be used either logographically or phonetically. For example, the Akkadian sign DIĜIR () could represent the god An, the word "diĝir" 'deity' or the word "an" 'sky'. The Akkadian counterpart could represent the Akkadian stem "il-" 'deity', the Akkadian word "šamu" 'sky', or the syllable "an".

Although Chinese characters are logograms, two of the smaller classes in the traditional classification are ideographic in origin:

An example of ideograms is the collection of 50 signs developed in the 1970s by the American Institute of Graphic Arts at the request of the US Department of Transportation. The system was initially used to mark airports and gradually became more widespread.

Mathematical symbols are a type of ideogram.

Inspired by inaccurate early descriptions of Chinese and Japanese characters as ideograms, many Western thinkers have sought to design universal written languages, in which symbols denote concepts rather than words. An early proposal was "An Essay towards a Real Character, and a Philosophical Language" (1668) by John Wilkins. A recent example is the system of Blissymbols, which was proposed by Charles K. Bliss in 1949 and currently includes over 2,000 symbols.



</doc>
<doc id="14732" url="https://en.wikipedia.org/wiki?curid=14732" title="Irish Republican Army (1919–1922)">
Irish Republican Army (1919–1922)

The Irish Republican Army (IRA) () was an Irish republican revolutionary paramilitary organisation. The ancestor of many groups also known as the Irish Republican Army, and distinguished from them as the Old IRA, it was descended from the Irish Volunteers, an organisation established on 25 November 1913 that staged the Easter Rising in April 1916. In 1919, the Irish Republic that had been proclaimed during the Easter Rising was formally established by an elected assembly (Dáil Éireann), and the Irish Volunteers were recognised by Dáil Éireann as its legitimate army. Thereafter, the IRA waged a guerrilla campaign against the British occupation of Ireland in the 1919–1921 Irish War of Independence.

Following the signing in 1921 of the Anglo-Irish Treaty, which ended the War of Independence, a split occurred within the IRA. Members who supported the treaty formed the nucleus of the Irish National Army. However, the majority of the IRA was opposed to the treaty. The anti-treaty IRA fought a civil war against the Free State Army in 1922–23, with the intention of creating a fully independent all-Ireland republic. Having lost the civil war, this group remained in existence, with the intention of overthrowing the governments of both the Irish Free State and Northern Ireland and achieving the Irish Republic proclaimed in 1916.

The Irish Volunteers, founded in 1913, staged the Easter Rising, which aimed at ending British rule in Ireland, in 1916. Following the suppression of the Rising, thousands of Volunteers were imprisoned or interned, leading to the break-up of the organisation. It was reorganised in 1917 following the release of first the internees and then the prisoners. At the army convention held in Dublin in October 1917, Éamon de Valera was elected president, Michael Collins Director for Organisation and Cathal Brugha Chairman of the Resident Executive, which in effect made him Chief of Staff.

Following the success of Sinn Féin in the general election of 1918 and the setting up of the First Dáil (the legislature of the Irish Republic), Volunteers commenced military action against the Royal Irish Constabulary (RIC), the paramilitary police force in Ireland, and subsequently against the British Army. It began with the Soloheadbeg Ambush, when members of the Third Tipperary Brigade led by Séumas Robinson, Seán Treacy, Dan Breen and Seán Hogan, seized a quantity of gelignite, killing two RIC constables in the process. The Dáil leadership worried that the Volunteers would not accept its authority, given that, under their own constitution, they were bound to obey "their" own executive and no other body. In August 1919, Brugha proposed to the Dáil that the Volunteers be asked to swear allegiance to the Dáil, but another year passed before the Volunteers took an oath of allegiance to the Irish Republic and its government, "throughout August 1920". During this time, the Volunteers gradually became known as the Irish Republican Army (IRA).

A power struggle continued between Brugha and Collins, both cabinet ministers, over who had the greater influence. Brugha was nominally the superior as Minister for Defence, but Collins's power base came from his position as Director of Organisation of the IRA and from his membership on the Supreme Council of the Irish Republican Brotherhood (IRB). De Valera resented Collins's clear power and influence, which he saw as coming more from the secretive IRB than from his position as a Teachta Dála (TD) and minister in the Aireacht. Brugha and de Valera both urged the IRA to undertake larger, more conventional military actions for the propaganda effect but were ignored by Collins and Mulcahy. Brugha at one stage proposed the assassination of the entire British cabinet. This was also discounted due to its presumed negative effect on British public opinion. Moreover, many members of the Dáil, notably Arthur Griffith, did not approve of IRA violence and would have preferred a campaign of passive resistance to the British rule. The Dáil belatedly accepted responsibility for IRA actions in April 1921, just three months before the end of the Irish War of Independence.

In practice, the IRA was commanded by Collins, with Richard Mulcahy as second in command. These men were able to issue orders and directives to IRA guerrilla units around the country and at times to send arms and organisers to specific areas. However, because of the localised and irregular character of the war, they were only able to exert limited control over local IRA commanders such as Tom Barry, Liam Lynch in Cork and Seán Mac Eoin in Longford.

The IRA claimed a total strength of 70,000, but only about 3,000 were actively engaged in fighting against the Crown. The IRA distrusted those Irishmen who had fought in the British Army during the First World War as potential informers, but there were a number of exceptions such as Emmet Dalton, Tom Barry and Martin Doyle. The IRA divided its members into three classes, namely "unreliable", "reliable" and "active". The "unreliable" members were those who were nominally IRA members but did not do very much for the struggle, "reliable" members played a supporting role in the war while occasionally fighting and the "active" men those who were engaged in full-time fighting. Of the IRA brigades only about one to two-thirds were considered to be "reliable" while those considered "active" were even smaller. A disproportionate number of the "active" IRA men were teachers; medical students; shoe-makers and boot-makers; those engaged in building trades like painters, carpenters, bricklayers, etc.; draper's assistants; and creamery workers. The Canadian historian Peter Hart wrote "...the guerrillas were disproportionately skilled, trained and urban". Farmers and fishermen tended to be underrepresented in the IRA. Those Irishmen engaged in white-collar trades or working as skilled labourers were much more likely to be involved in cultural nationalist groups like the Gaelic League than farmers or fishermen, and thus to have a stronger sense of Irish nationalism. Furthermore, the authority of the Crown tended to be stronger in towns and cities than in the countryside and as such those engaged in Irish nationalist activities in urban areas were much more likely to come into conflict with the Crown, thus leading to a greater chance of radicalisation. Finally, the British tactic of blowing up the homes of IRA members had the effect of discouraging many farmers from joining the struggle as the destruction of the family farm could easily reduce a farmer and his family to destitution. Of the "active" IRA members, three-quarters were in their late teens or early 20s and only 5% of the "active" men were in the age range of 40 or older. The "active" members were overwhelmingly single men with only 4% being married or engaged in a relationship. The life of an "active" IRA man with its stress of living on the run and constantly being in hiding tended to attract single men who could adjust to this lifestyle far more easily than a man in a relationship. Furthermore, the IRA preferred to recruit single men as it was found that singles could devote themselves more wholeheartedly to the struggle.

Women were active in the republican movement, but almost no women fought with the IRA whose "active" members were almost entirely male. The IRA was not a sectarian group and went out of its way to proclaim it was open to all Irishmen, but its membership was largely Catholic with virtually no Protestants serving as "active" IRA men. Hart wrote that in his study of the IRA membership that he found only three Protestants serving as "active" IRA men between 1919 and 1921. Of the 917 IRA men convicted by British courts under the Defence of the Realm Act in 1919, only one was a Protestant. The majority of those serving in the IRA were practicing Catholics, but there was a large minority of "pagans" as atheists or non-practicing Catholics were known in Ireland. The majority of the IRA men serving in metropolitan Britain were permanent residents with very few sent over from Ireland. The majority of the IRA men operating in Britain were Irish-born, but there a substantial minority who were British-born, something that made them especially insistent on asserting their Irish identity.

The IRA fought a guerrilla war against the Crown forces in Ireland from 1919 to July 1921. The most intense period of the war was from November 1920 onwards. The IRA campaign can broadly be split into three phases. The first, in 1919, involved the re-organisation of the Irish Volunteers as a guerrilla army and only sporadic attacks. Organisers such as Ernie O'Malley were sent around the country to set up viable guerrilla units. On paper, there were 100,000 or so Volunteers enrolled after the conscription crisis of 1918. However, only about 15,000 of these participated in the guerrilla war. In 1919, Collins, the IRA's Director of Intelligence, organised the "Squad"—an assassination unit based in Dublin which killed police involved in intelligence work (the Irish playwright Brendan Behan's father Stephen Behan was a member of the Squad). Typical of Collins's sardonic sense of humour, the Squad was often referred to as his "Twelve Apostles". In addition, there were some arms raids on RIC barracks. By the end of 1919, four Dublin Metropolitan Police and 11 RIC men had been killed. The RIC abandoned most of their smaller rural barracks in late 1919. Around 400 of these were burned in a co-ordinated IRA operation around the country in April 1920.

The second phase of the IRA campaign, roughly from January to July 1920, involved attacks on the fortified police barracks located in the towns. Between January and June 1920, 16 of these were destroyed and 29 badly damaged. Several events of late 1920 greatly escalated the conflict. Firstly, the British declared martial law in parts of the country—allowing for internment and executions of IRA men. Secondly they deployed paramilitary forces, the Black and Tans and Auxiliary Division, and more British Army personnel into the country. Thus, the third phase of the war (roughly August 1920 – July 1921) involved the IRA taking on a greatly expanded British force, moving away from attacking well-defended barracks and instead using ambush tactics. To this end the IRA was re-organised into "flying columns"—permanent guerrilla units, usually about 20 strong, although sometimes larger. In rural areas, the flying columns usually had bases in remote mountainous areas.

While most areas of the country saw some violence in 1919–1921, the brunt of the war was fought in Dublin and the southern province of Munster. In Munster, the IRA carried out a significant number of successful actions against British troops, for instance the ambushing and killing of 17 of 18 Auxiliaries by Tom Barry's column at Kilmicheal in West Cork in November 1920, or Liam Lynch's men killing 13 British soldiers near Millstreet early in the next year. At the Crossbarry Ambush in March 1921, 100 or so of Barry's men fought a sizeable engagement with a British column of 1,200, escaping from the British encircling manoeuvre. In Dublin, the "Squad" and elements of the IRA Dublin Brigade were amalgamated into the "Active Service Unit", under Oscar Traynor, which tried to carry out at least three attacks on British troops a day. Usually, these consisted of shooting or grenade attacks on British patrols. Outside Dublin and Munster, there were only isolated areas of intense activity. For instance, the County Longford IRA under Seán Mac Eoin carried out a number of well planned ambushes and successfully defended the village of Ballinalee against Black and Tan reprisals in a three-hour gun battle. In County Mayo, large-scale guerrilla action did not break out until spring 1921, when two British forces were ambushed at Carrowkennedy and Tourmakeady. Elsewhere, fighting was more sporadic and less intense.

In Belfast, the war had a character all of its own. The city had a Protestant and unionist majority and IRA actions were responded to with reprisals against the Catholic population, including killings (such as the McMahon killings) and the burning of many homes – as on Belfast's Bloody Sunday. The IRA in Belfast and the north generally, although involved in protecting the Catholic community from loyalists and state forces, undertook an arson campaign against factories and commercial premises. The violence in Belfast alone, which continued until October 1922 (long after the truce in the rest of the country), claimed the lives of between 400 and 500 people.

In April 1921, the IRA was again reorganised, in line with the Dáil's endorsement of its actions, along the lines of a regular army. Divisions were created based on region, with commanders being given responsibility, in theory, for large geographical areas. In practice, this had little effect on the localised nature of the guerrilla warfare.

In May 1921, the IRA in Dublin attacked and burned the Custom House. The action was a serious setback as five members were killed and eighty captured.

By the end of the war in July 1921, the IRA was hard-pressed by the deployment of more British troops into the most active areas and a chronic shortage of arms and ammunition. It has been estimated that the IRA had only about 3,000 rifles (mostly captured from the British) during the war, with a larger number of shotguns and pistols. An ambitious plan to buy arms from Italy in 1921 collapsed when the money did not reach the arms dealers. Towards the end of the war, some Thompson submachine guns were imported from the United States; however 450 of these were intercepted by the American authorities and the remainder only reached Ireland shortly before the Truce.

By June 1921, Collins' assessment was that the IRA was within weeks, possibly even days, of collapse. It had few weapons or ammunition left. Moreover, almost 5,000 IRA men had been imprisoned or interned and over 500 killed. Collins and Mulcahy estimated that the number of effective guerrilla fighters was down to 2,000–3,000. However, in the summer of 1921, the war was abruptly ended.

The Irish War of Independence was a brutal and bloody affair, with violence and acts of extreme brutality on both sides. The British recruited hundreds of World War I veterans into the RIC and sent them to Ireland. Because there was initially a shortage of RIC uniforms, the veterans at first wore a combination of dark green RIC uniforms and khaki British Army uniforms, which inspired the nickname "Black and Tans". The brutality of the Black and Tans is now well-known, although the greatest violence attributed to the Crown's forces was often that of the Auxiliary Division of the Constabulary. One of the strongest critics of the Black and Tans was King George V who in May 1921 told Lady Margery Greenwood that "he hated the idea of the Black and Tans."

The most high-profile atrocity of the war took place in Dublin in November 1920, and is still known as Bloody Sunday. In the early hours of the morning, Collins' "Squad" killed fourteen British spies, some in front of their wives. In reprisal, that afternoon, British forces opened fire on a football crowd at Croke Park, killing 14 civilians. Towards the end of the day, two prominent Republicans and a friend of theirs were arrested and killed by Crown Forces.

The IRA was also involved in the destruction of many stately homes in Munster. The Church of Ireland Gazette recorded numerous instances of Unionists and Loyalists being shot, burnt or forced from their homes during the early 1920s. In County Cork between 1920 and 1923 the IRA shot over 200 civilians of whom over 70 (or 36%) were Protestants: five times the percentage of Protestants in the civilian population. This was due to the historical inclination of Protestants towards loyalty to the United Kingdom. A convention of Irish Protestant Churches in Dublin in May 1922 signed a resolution placing "on record" that "hostility to Protestants by reason of their religion has been almost, if not wholly, unknown in the twenty-six counties in which Protestants are in the minority."

Many historic buildings in Ireland were destroyed during the war, most famously the Custom House in Dublin, which was disastrously attacked on de Valera's insistence, to the horror of the more militarily experienced Collins. As he feared, the destruction proved a pyrrhic victory for the Republic, with so many IRA men killed or captured that the IRA in Dublin suffered a severe blow.

This was also a period of social upheaval in Ireland, with frequent strikes as well as other manifestations of class conflict. In this regard, the IRA acted to a large degree as an agent of social control and stability, driven by the need to preserve cross-class unity in the national struggle, and on occasion being used to break strikes.

Assessments of the effectiveness of the IRA's campaign vary. They were never in a position to engage in conventional warfare. The political, military and financial costs of remaining in Ireland were higher than the British government was prepared to pay and this in a sense forced them into negotiations with the Irish political leaders. According to historian Michael Hopkinson, the guerrilla warfare "was often courageous and effective". Historian David Fitzpatrick observes, "The guerrilla fighters...were vastly outnumbered by the forces of the Crown... The success of the Irish Volunteers in surviving so long is therefore noteworthy."

David Lloyd George, the British Prime Minister, at the time, found himself under increasing pressure (both internationally and from within the British Isles) to try to salvage something from the situation. This was a complete reversal on his earlier position. He had consistently referred to the IRA as a "murder gang" up until then. An unexpected olive branch came from King George V, who, in a speech in Belfast called for reconciliation on all sides, changed the mood and enabled the British and Irish Republican governments to agree to a truce. The Truce was agreed on 11 July 1921. On 8 July, de Valera met General Nevil Macready, the British commander in chief in Ireland and agreed terms. The IRA was to retain its arms and the British Army was to remain in barracks for the duration of peace negotiations. Many IRA officers interpreted the truce only as a temporary break in fighting. They continued to recruit and train volunteers, with the result that the IRA had increased its number to over 72,000 men by early 1922.

Negotiations on an Anglo-Irish Treaty took place in late 1921 in London. The Irish delegation was led by Arthur Griffith and Michael Collins.

The most contentious areas of the Treaty for the IRA were abolition of the Irish Republic declared in 1919, the status of the Irish Free State as a dominion in the British Commonwealth and the British retention of the so-called Treaty Ports on Ireland's south coast. These issues were the cause of a split in the IRA and ultimately, the Irish Civil War.

Under the Government of Ireland Act 1920, Ireland was partitioned, creating Northern Ireland and Southern Ireland. Under the terms of the Anglo-Irish agreement of 6 December 1921, which ended the war (1919–21), Northern Ireland was given the option of withdrawing from the new state, the Irish Free State, and remaining part of the United Kingdom. The Northern Ireland parliament chose to do that. An Irish Boundary Commission was then set up to review the border.

Irish leaders expected that it would so reduce Northern Ireland's size, by transferring nationalist areas to the Irish Free State, as to make it economically unviable. Partition was not by itself the key breaking point between pro- and anti-Treaty campaigners; both sides expected the Boundary Commission to greatly reduce Northern Ireland. Moreover, Michael Collins was planning a clandestine guerrilla campaign against the Northern state using the IRA. In early 1922, he sent IRA units to the border areas and sent arms to northern units. It was only afterwards, when partition was confirmed, that a united Ireland became the preserve of anti-Treaty Republicans.

The IRA leadership was deeply divided over the decision by the Dáil to ratify the Treaty. Despite the fact that Michael Collins – the de facto leader of the IRA – had negotiated the Treaty, many IRA officers were against it. Of the General Headquarters (GHQ) staff, nine members were in favour of the Treaty while four opposed it. The majority of the IRA rank-and-file were against the Treaty; in January–June 1922, their discontent developed into open defiance of the elected civilian Provisional government of Ireland.

Both sides agreed that the IRA's allegiance was to the (elected) Dáil of the Irish Republic, but the anti-Treaty side argued that the decision of the Dáil to accept the Treaty (and set aside the Irish Republic) meant that the IRA no longer owed that body its allegiance. They called for the IRA to withdraw from the authority of the Dáil and to entrust the IRA Executive with control over the army. On 16 January, the first IRA division – the 2nd Southern Division led by Ernie O'Malley – repudiated the authority of the GHQ. A month later, on 18 February, Liam Forde, O/C of the IRA Mid-Limerick Brigade, issued a proclamation stating that: "We no longer recognise the authority of the present head of the army, and renew our allegiance to the existing Irish Republic". This was the first unit of the IRA to break with the pro-Treaty government.

On 22 March, Rory O'Connor held what was to become an infamous press conference and declared that the IRA would no longer obey the Dáil as (he said) it had violated its Oath to uphold the Irish Republic. He went on to say that "we repudiate the Dáil ... We will set up an Executive which will issue orders to the IRA all over the country." In reply to the question on whether this meant they intended to create a military dictatorship, O'Connor said: "You can take it that way if you like."

On 28 March, the (anti-Treaty) IRA Executive issued statement stating that Minister of Defence (Richard Mulcahy) and the Chief-of-Staff (Eoin O'Duffy) no longer exercised any control over the IRA. In addition, it ordered an end to the recruitment to the new military and police forces of the Provisional Government. Furthermore, it instructed all IRA units to reaffirm their allegiance to the Irish Republic on 2 April.
The stage was set for civil war over the Treaty.

The pro-treaty IRA soon became the nucleus of the new (regular) Irish National Army created by Collins and Richard Mulcahy. British pressure, and tensions between the pro- and anti-Treaty factions of the IRA, led to a bloody civil war, ending in the defeat of the anti-Treaty faction. On 24 May 1923, Frank Aiken, the (anti-treaty) IRA Chief-of-Staff, called a cease-fire. Many left political activity altogether, but a minority continued to insist that the new Irish Free State, created by the "illegitimate" Treaty, was an illegitimate state. They asserted that their "IRA Army Executive" was the real government of a still-existing Irish Republic. The IRA of the Civil War and subsequent organisations that have used the name claim lineage from that group, which is covered in full at Irish Republican Army (1922–1969).

"For information on later organisations using the name Irish Republican Army, see the table below. For a genealogy of organisations using the name "IRA" after 1922, see List of organisations known as the Irish Republican Army".






</doc>
<doc id="14734" url="https://en.wikipedia.org/wiki?curid=14734" title="Iron">
Iron

Iron () is a chemical element with symbol Fe (from ) and atomic number 26. It is a metal that belongs to the first transition series and group 8 of the periodic table. It is by mass the most common element on Earth, forming much of Earth's outer and inner core. It is the fourth most common element in the Earth's crust.

In its metallic state, iron is rare in the Earth's crust, limited mainly to deposition by meteorites. Iron ores, by contrast, are among the most abundant in the Earth's crust, although extracting usable metal from them requires kilns or furnaces capable of reaching or higher, about 500 °C (900 °F) higher than what is enough to smelt copper. Humans started to master that process in Eurasia only about 2000 BCE, and the use of iron tools and weapons began to displace copper alloys, in some regions, only around 1200 BCE. That event is considered the transition from the Bronze Age to the Iron Age. In the modern world, iron alloys, such as steel, inox, cast iron and special steels are by far the most common industrial metals, because of their mechanical properties and low cost.

Pristine and smooth pure iron surfaces are mirror-like silvery-gray. However, iron reacts readily with oxygen and water to give brown to black hydrated iron oxides, commonly known as rust. Unlike the oxides of some other metals, that form passivating layers, rust occupies more volume than the metal and thus flakes off, exposing fresh surfaces for corrosion.

The body of an adult human contains about 4 grams (0.005% body weight) of iron, mostly in hemoglobin and myoglobin. These two proteins play essential roles in vertebrate metabolism, respectively oxygen transport by blood and oxygen storage in muscles. To maintain the necessary levels, human iron metabolism requires a minimum of iron in the diet. Iron is also the metal at the active site of many important redox enzymes dealing with cellular respiration and oxidation and reduction in plants and animals.

Chemically, the most common oxidation states of iron are iron(II) and iron(III). Iron shares many properties of other transition metals, including the other group 8 elements, ruthenium and osmium. Iron forms compounds in a wide range of oxidation states, −2 to +7. Iron also forms many coordination compounds; some of them, such as ferrocene, ferrioxalate, and Prussian blue, have substantial industrial, medical, or research applications.

At least four allotropes of iron (differing atom arrangements in the solid) are known, conventionally denoted α, γ, δ, and ε.
The first three forms are observed at ordinary pressures. As molten iron cools past its freezing point of 1538 °C, it crystallizes into its δ allotrope, which has a body-centered cubic (bcc) crystal structure. As it cools further to 1394 °C, it changes to its γ-iron allotrope, a face-centered cubic (fcc) crystal structure, or austenite. At 912 °C and below, the crystal structure again becomes the bcc α-iron allotrope.

The physical properties of iron at very high pressures and temperatures have also been studied extensively, because of their relevance to theories about the cores of the Earth and other planets. Above approximately 10 GPa and temperatures of a few hundred kelvin or less, α-iron changes into another hexagonal close-packed (hcp) structure, which is also known as ε-iron. The higher-temperature γ-phase also changes into ε-iron, but does so at higher pressure.

Some controversial experimental evidence exists for a stable β phase at pressures above 50 GPa and temperatures of at least 1500 K. It is supposed to have an orthorhombic or a double hcp structure. (Confusingly, the term "β-iron" is sometimes also used to refer to α-iron above its Curie point, when it changes from being ferromagnetic to paramagnetic, even though its crystal structure has not changed.)

The inner core of the Earth is generally presumed to consist of an iron-nickel alloy with ε (or β) structure.

The melting and boiling points of iron, along with its enthalpy of atomization, are lower than those of the earlier 3d elements from scandium to chromium, showing the lessened contribution of the 3d electrons to metallic bonding as they are attracted more and more into the inert core by the nucleus; however, they are higher than the values for the previous element manganese because that element has a half-filled 3d subshell and consequently its d-electrons are not easily delocalized. This same trend appears for ruthenium but not osmium.

The melting point of iron is experimentally well defined for pressures less than 50 GPa. For greater pressures, published data (as of 2007) still varies by tens of gigapascals and over a thousand kelvin.

Below its Curie point of 770 °C, α-iron changes from paramagnetic to ferromagnetic: the spins of the two unpaired electrons in each atom generally align with the spins of its neighbors, creating an overall magnetic field. This happens because the orbitals of those two electrons (d and d) do not point toward neighboring atoms in the lattice, and therefore are not involved in metallic bonding.

In the absence of an external source of magnetic field, the atoms get spontaneously partitioned into magnetic domains, about 10 micrometers across, such that the atoms in each domain have parallel spins, but some domains have other orientations. Thus a macroscopic piece of iron will have a nearly zero overall magnetic field.

Application of an external magnetic field causes the domains that are magnetized in the same general direction to grow at the expense of adjacent ones that point in other directions, reinforcing the external field. This effect is exploited in devices that needs to channel magnetic fields, such as electrical transformers, magnetic recording heads, and electric motors. Impurities, lattice defects, or grain and particle boundaries can "pin" the domains in the new positions, so that the effect persists even after the external field is removed -- thus turning the iron object into a (permanent) magnet.

Similar behavior is exhibited by some iron compounds, such as the ferrites and the mineral magnetite, a crystalline form of the mixed iron(II,III) oxide (although the atomic-scale mechanism, ferrimagnetism, is somewhat different). Pieces of magnetite with natural permanent magnetization (lodestones) provided the earliest compasses for navigation. Particles of magnetite were extensively used in magnetic recording media such as core memories, magnetic tapes, floppies, and disks, until they were replaced by cobalt-based materials.

Iron has four stable isotopes: Fe (5.845% of natural iron), Fe (91.754%), Fe (2.119%) and Fe (0.282%). 20-30 artificial isotopes have also been created. Of these stable isotopes, only Fe has a nuclear spin (−). The nuclide Fe theoretically can undergo double electron capture to Cr, but the process has never been observed and only a lower limit on the half-life of 3.1×10 years has been established.

Fe is an extinct radionuclide of long half-life (2.6 million years). It is not found on Earth, but its ultimate decay product is its granddaughter, the stable nuclide Ni. Much of the past work on isotopic composition of iron has focused on the nucleosynthesis of Fe through studies of meteorites and ore formation. In the last decade, advances in mass spectrometry have allowed the detection and quantification of minute, naturally occurring variations in the ratios of the stable isotopes of iron. Much of this work is driven by the Earth and planetary science communities, although applications to biological and industrial systems are emerging.

In phases of the meteorites "Semarkona" and "Chervony Kut," a correlation between the concentration of Ni, the granddaughter of Fe, and the abundance of the stable iron isotopes provided evidence for the existence of Fe at the time of formation of the Solar System. Possibly the energy released by the decay of Fe, along with that released by Al, contributed to the remelting and differentiation of asteroids after their formation 4.6 billion years ago. The abundance of Ni present in extraterrestrial material may bring further insight into the origin and early history of the Solar System.

The most abundant iron isotope Fe is of particular interest to nuclear scientists because it represents the most common endpoint of nucleosynthesis. Since Ni (14 alpha particles) is easily produced from lighter nuclei in the alpha process in nuclear reactions in supernovae (see silicon burning process), it is the endpoint of fusion chains inside extremely massive stars, since addition of another alpha particle, resulting in Zn, requires a great deal more energy. This Ni, which has a half-life of about 6 days, is created in quantity in these stars, but soon decays by two successive positron emissions within supernova decay products in the supernova remnant gas cloud, first to radioactive Co, and then to stable Fe. As such, iron is the most abundant element in the core of red giants, and is the most abundant metal in iron meteorites and in the dense metal cores of planets such as Earth. It is also very common in the universe, relative to other stable metals of approximately the same atomic weight. Iron is the sixth most abundant element in the Universe, and the most common refractory element.

Although a further tiny energy gain could be extracted by synthesizing Ni, which has a marginally higher binding energy than Fe, conditions in stars are unsuitable for this process. Element production in supernovas and distribution on Earth greatly favor iron over nickel, and in any case, Fe still has a lower mass per nucleon than Ni due to its higher fraction of lighter protons. Hence, elements heavier than iron require a supernova for their formation, involving rapid neutron capture by starting Fe nuclei.

In the far future of the universe, assuming that proton decay does not occur, cold fusion occurring via quantum tunnelling would cause the light nuclei in ordinary matter to fuse into Fe nuclei. Fission and alpha-particle emission would then make heavy nuclei decay into iron, converting all stellar-mass objects to cold spheres of pure iron.

Iron's abundance in rocky planets like Earth is due to its abundant production by fusion in high-mass stars, where it is the last element to be produced with release of energy before the violent collapse of a supernova, which scatters the iron into space.

Metallic or native iron is rarely found on the surface of the Earth because it tends to oxidize. However, both the Earth's inner and outer core, that account for 35% of the mass of the whole Earth, are believed to consist largely of an iron alloy, possibly with nickel. Electric currents in the liquid outer core are believed to be the origin of the Earth's magnetic field. The other terrestrial planets (Mercury, Venus, and Mars) as well as the Moon are believed to have a metallic core consisting mostly of iron. The M-type asteroids are also believed to be partly or mostly made of metallic iron alloy.

The rare iron meteorites are the main form of natural metallic iron on the Earth's surface. Items made of cold-worked meteoritic iron have been found in various archaeological sites dating from a time when iron smelting had not yet been developed; and the Inuit in Greenland have been reported to use iron from the Cape York meteorite for tools and hunting weapons. About 1 in 20 meteorites consist of the unique iron-nickel minerals taenite (35–80% iron) and kamacite (90–95% iron). Native iron is also rarely found in basalts that have formed from magmas that have come into contact with carbon-rich sedimentary rocks, which have reduced the oxygen fugacity sufficiently for iron to crystallize. This is known as Telluric iron and is described from a few localities, such as Disko Island in West Greenland, Yakutia in Russia and Bühl in Germany.

Ferropericlase (Mg,Fe)O, a solid solution of periclase (MgO) and wüstite (FeO), makes up about 20% of the volume of the lower mantle of the Earth, which makes it the second most abundant mineral phase in that region after silicate perovskite (Mg,Fe)SiO; it also is the major host for iron in the lower mantle. At the bottom of the transition zone of the mantle, the reaction γ-(Mg,Fe)[SiO] ↔ (Mg,Fe)[SiO] + (Mg,Fe)O transforms γ-olivine into a mixture of silicate perovskite and ferropericlase and vice versa. In the literature, this mineral phase of the lower mantle is also often called magnesiowüstite. Silicate perovskite may form up to 93% of the lower mantle, and the magnesium iron form, (Mg,Fe)SiO, is considered to be the most abundant mineral in the Earth, making up 38% of its volume.

While iron is the most abundant element on Earth, it accounts for only 5% of the Earth's crust; thus being only the fourth most abundant element, after oxygen, silicon, and aluminium.

Most of the iron in the crust is combined with various other elements to form many iron minerals. An important class is the iron oxide minerals such as hematite (FeO), magnetite (FeO), and siderite (FeCO), which are the major ores of iron. Many igneous rocks also contain the sulfide minerals pyrrhotite and pentlandite. During weathering, iron tends to leach from sulfide deposits as the sulfate and from silicate deposits as the bicarbonate. Both of these are oxidized in aqueous solution and precipitate in even mildly elevated pH as iron(III) oxide.
Large deposits of iron are banded iron formations, a type of rock consisting of repeated thin layers of iron oxides alternating with bands of iron-poor shale and chert. The banded iron formations were laid down in the time between and .

Materials containing finely ground iron(III) oxides or oxide-hydroxides, such as ochre, have been used as yellow, red, and brown pigments since pre-historical times. They contribute as well to the color of various rocks and clays, including entire geological formations like the Painted Hills in Oregon and the Buntsandstein ("colored sandstone", British Bunter). Through "Eisensandstein" (a jurassic 'iron sandstone', e.g. from Donzdorf in Germany) and Bath stone in the UK, iron compounds are responsible for the yellowish color of many historical buildings and sculptures. The proverbial red color of the surface of Mars is derived from an iron oxide-rich regolith.

Significant amounts of iron occur in the iron sulfide mineral pyrite (FeS), but it is difficult to extract iron from it and it is therefore not exploited. In fact, iron is so common that production generally focuses only on ores with very high quantities of it.

According to the International Resource Panel's Metal Stocks in Society report, the global stock of iron in use in society is 2200 kg per capita. More-developed countries differ in this respect from less-developed countries (7000–14000 vs 2000 kg per capita).

Iron shows the characteristic chemical properties of the transition metals, namely the ability to form variable oxidation states differing by steps of one and a very large coordination and organometallic chemistry: indeed, it was the discovery of an iron compound, ferrocene, that revolutionalized the latter field in the 1950s. Iron is sometimes considered as a prototype for the entire block of transition metals, due to its abundance and the immense role it has played in the technological progress of humanity. Its 26 electrons are arranged in the configuration [Ar]3d4s, of which the 3d and 4s electrons are relatively close in energy, and thus it can lose a variable number of electrons and there is no clear point where further ionization becomes unprofitable.

Iron forms compounds mainly in the oxidation states +2 (iron(II), "ferrous") and +3 (iron(III), "ferric"). Iron also occurs in higher oxidation states, e.g. the purple potassium ferrate (KFeO), which contains iron in its +6 oxidation state. Although iron(VIII) oxide (FeO) has been claimed, the report could not be reproduced and such a species (at least with iron in its +8 oxidation state) has been found to be improbable computationally. However, one form of anionic [FeO] with iron in its +7 oxidation state, along with an iron(V)-peroxo isomer, has been detected by infrared spectroscopy at 4 K after cocondensation of laser-ablated Fe atoms with a mixture of O/Ar. Iron(IV) is a common intermediate in many biochemical oxidation reactions. Numerous organoiron compounds contain formal oxidation states of +1, 0, −1, or even −2. The oxidation states and other bonding properties are often assessed using the technique of Mössbauer spectroscopy.
Many mixed valence compounds contain both iron(II) and iron(III) centers, such as magnetite and Prussian blue (Fe(Fe[CN])). The latter is used as the traditional "blue" in blueprints.

Iron is the first of the transition metals that cannot reach its group oxidation state of +8, although its heavier congeners ruthenium and osmium can, with ruthenium having more difficulty than osmium. Ruthenium exhibits an aqueous cationic chemistry in its low oxidation states similar to that of iron, but osmium does not, favoring high oxidation states in which it forms anionic complexes. In the second half of the 3d transition series, vertical similarities down the groups compete with the horizontal similarities of iron with its neighbors cobalt and nickel in the periodic table, which are also ferromagnetic at room temperature and share similar chemistry. As such, iron, cobalt, and nickel are sometimes grouped together as the iron triad.

Unlike many other metals, iron does not form amalgams with mercury. As a result, mercury is traded in standardized 76 pound flasks (34 kg) made of iron.

Iron is by far the most reactive element in its group; it is pyrophoric when finely divided and dissolves easily in dilute acids, giving Fe. However, it does not react with concentrated nitric acid and other oxidizing acids due to the formation of an impervious oxide layer, which can nevertheless react with hydrochloric acid.

Iron forms various oxide and hydroxide compounds; the most common are iron(II,III) oxide (FeO), and iron(III) oxide (FeO). Iron(II) oxide also exists, though it is unstable at room temperature. Despite their names, they are actually all non-stoichiometric compounds whose compositions may vary. These oxides are the principal ores for the production of iron (see bloomery and blast furnace). They are also used in the production of ferrites, useful magnetic storage media in computers, and pigments. The best known sulfide is iron pyrite (FeS), also known as fool's gold owing to its golden luster. It is not an iron(IV) compound, but is actually an iron(II) polysulfide containing Fe and ions in a distorted sodium chloride structure.

The binary ferrous and ferric halides are well-known. The ferrous halides typically arise from treating iron metal with the corresponding hydrohalic acid to give the corresponding hydrated salts.
Iron reacts with fluorine, chlorine, and bromine to give the corresponding ferric halides, ferric chloride being the most common.
Ferric iodide is an exception, being thermodynamically unstable due to the oxidizing power of Fe and the high reducing power of I:

Ferric iodide, a black solid, is not stable in ordinary conditions, but can be prepared through the reaction of iron pentacarbonyl with iodine and carbon monoxide in the presence of hexane and light at the temperature of −20 °C, with oxygen and water excluded.

The standard reduction potentials in acidic aqueous solution for some common iron ions are given below:
The red-purple tetrahedral ferrate(VI) anion is such a strong oxidizing agent that it oxidizes nitrogen and ammonia at room temperature, and even water itself in acidic or neutral solutions:

The Fe ion has a large simple cationic chemistry, although the pale-violet hexaquo ion [Fe(HO)] is very readily hydrolyzed when pH increases above 0 as follows:

As pH rises above 0 the above yellow hydrolyzed species form and as it rises above 2–3, reddish-brown hydrous iron(III) oxide precipitates out of solution. Although Fe has an d configuration, its absorption spectrum is not like that of Mn with its weak, spin-forbidden d–d bands, because Fe has higher positive charge and is more polarizing, lowering the energy of its ligand-to-metal charge transfer absorptions. Thus, all the above complexes are rather strongly colored, with the single exception of the hexaquo ion – and even that has a spectrum dominated by charge transfer in the near ultraviolet region. On the other hand, the pale green iron(II) hexaquo ion [Fe(HO)] does not undergo appreciable hydrolysis. Carbon dioxide is not evolved when carbonate anions are added, which instead results in white iron(II) carbonate being precipitated out. In excess carbon dioxide this forms the slightly soluble bicarbonate, which occurs commonly in groundwater, but it oxidises quickly in air to form iron(III) oxide that accounts for the brown deposits present in a sizeable number of streams.

Due to its electronic structure, iron has a very large coordination and organometallic chemistry.

Many coordination compounds of iron are known. A typical six-coordinate anion is hexachloroferrate(III), [FeCl], found in the mixed salt tetrakis(methylammonium) hexachloroferrate(III) chloride. Complexes with multiple bidentate ligands have geometric isomers. For example, the "trans"-chlorohydridobis(bis-1,2-(diphenylphosphino)ethane)iron(II) complex is used as a starting material for compounds with the Fe(dppe) moiety. The ferrioxalate ion with three oxalate ligands (shown at right) displays helical chirality with its two non-superposable geometries labelled "Λ" (lambda) for the left-handed screw axis and "Δ" (delta) for the right-handed screw axis, in line with IUPAC conventions. Potassium ferrioxalate is used in chemical actinometry and along with its sodium salt undergoes photoreduction applied in old-style photographic processes. The dihydrate of iron(II) oxalate has a polymeric structure with co-planar oxalate ions bridging between iron centres with the water of crystallisation located forming the caps of each octahedron, as illustrated below.

Iron(III) complexes are quite similar to those of chromium(III) with the exception of iron(III)'s preference for "O"-donor instead of "N"-donor ligands. The latter tend to be rather more unstable than iron(II) complexes and often dissociate in water. Many Fe–O complexes show intense colors and are used as tests for phenols or enols. For example, in the ferric chloride test, used to determine the presence of phenols, iron(III) chloride reacts with a phenol to form a deep violet complex:

Among the halide and pseudohalide complexes, fluoro complexes of iron(III) are the most stable, with the colorless [FeF(HO)] being the most stable in aqueous solution. Chloro complexes are less stable and favor tetrahedral coordination as in [FeCl]; [FeBr] and [FeI] are reduced easily to iron(II). Thiocyanate is a common test for the presence of iron(III) as it forms the blood-red [Fe(SCN)(HO)]. Like manganese(II), most iron(III) complexes are high-spin, the exceptions being those with ligands that are high in the spectrochemical series such as cyanide. An example of a low-spin iron(III) complex is [Fe(CN)]. The cyanide ligands may easily be detached in [Fe(CN)], and hence this complex is poisonous, unlike the iron(II) complex [Fe(CN)] found in Prussian blue, which does not release hydrogen cyanide except when dilute acids are added. Iron shows a great variety of electronic spin states, including every possible spin quantum number value for a d-block element from 0 (diamagnetic) to (5 unpaired electrons). This value is always half the number of unpaired electrons. Complexes with zero to two unpaired electrons are considered low-spin and those with four or five are considered high-spin.

Iron(II) complexes are less stable than iron(III) complexes but the preference for "O"-donor ligands is less marked, so that for example [Fe(NH)] is known while [Fe(NH)] is not. They have a tendency to be oxidized to iron(III) but this can be moderated by low pH and the specific ligands used.

Organoiron chemistry is the study of organometallic compounds of iron, where carbon atoms are covalently bound to the metal atom. They are many and varied, including cyanide complexes, carbonyl complexes, sandwich and half-sandwich compounds.
Prussian blue or "ferric ferrocyanide", Fe[Fe(CN)], is an old and well-known iron-cyanide complex, extensively used as pigment and in several other applications. Its formation can be used as a simple wet chemistry test to distinguish between aqueous solutions of Fe and Fe as they react (respectively) with potassium ferricyanide and potassium ferrocyanide to form Prussian blue.

Another old example of organoiron compound is iron pentacarbonyl, Fe(CO), in which a neutral iron atom is bound to the carbon atoms of five carbon monoxide molecules. The compound can be used to make carbonyl iron powder, a highly reactive form of metallic iron. Thermolysis of iron pentacarbonyl gives triiron dodecacarbonyl, , a with a cluster of three iron atoms at its core. Collman's reagent, disodium tetracarbonylferrate, is a useful reagent for organic chemistry; it contains iron in the −2 oxidation state. Cyclopentadienyliron dicarbonyl dimer contains iron in the rare +1 oxidation state.
A landmark in this field was the discovery in 1951 of the remarkably stable sandwich compound ferrocene , by] Paulson and Kealy and independently by Miller and others, whose surprising molecular structure was determined only a year later by Woodward and Wilkinson and Fischer.
Ferrocene is still one of the most important tools and models in this class.

Iron-centered organometallic species are used as catalysts. The Knölker complex, for example, is a transfer hydrogenation catalyst for ketones.

The iron compounds produced on the largest scale in industry are iron(II) sulfate (FeSO·7HO) and iron(III) chloride (FeCl). The former is one of the most readily available sources of iron(II), but is less stable to aerial oxidation than Mohr's salt ((NH)Fe(SO)·6HO). Iron(II) compounds tend to be oxidized to iron(III) compounds in the air.

As iron has been in use for such a long time, it has many names. The source of its chemical symbol "Fe" is the Latin word "ferrum", and its descendants are the names of the element in the Romance languages (for example, French "fer", Spanish "hierro", and Italian and Portuguese "ferro"). The word "ferrum" itself possibly comes from the Semitic languages, via Etruscan, from a root that also gave rise to Old English "bræs" "brass". The English word "iron" derives ultimately from Proto-Germanic "*isarnan", which is also the source of the German name "Eisen". It was most likely borrowed from Celtic "*isarnon", which ultimately comes from Proto-Indo-European "*is-(e)ro-" "powerful, holy" and finally "*eis" "strong", referencing iron's strength as a metal. Kluge relates "*isarnon" to Illyric and Latin "ira", 'wrath'). The Balto-Slavic names for iron (e.g. Russian железо ["zhelezo"], Polish "żelazo", Lithuanian "geležis") are the only ones to come directly from the Proto-Indo-European "*gelg-" "iron". In many of these languages, the word for "iron" may also be used to denote other objects made of iron or steel, or figuratively because of the hardness and strength of the metal. The Chinese "tiě" (traditional 鐵; simplified 铁) derives from Proto-Sino-Tibetan "*hliek", and was borrowed into Japanese as 鉄 "tetsu", which also has the native reading "kurogane" "black metal" (similar to how iron is referenced in the English word blacksmith).

Iron is one of the elements undoubtedly known to the ancient world. It has been worked, or wrought, for millennia. However, iron objects of great age are much rarer than objects made of gold or silver due to the ease with which iron corrodes. The technology developed slowly, and even after the discovery of smelting it took many centuries for iron to replace bronze as the metal of choice for tools and weapons.

Beads made from meteoric iron in 3500 BC or earlier were found in Gerzah, Egypt by G.A. Wainwright. The beads contain 7.5% nickel, which is a signature of meteoric origin since iron found in the Earth's crust generally has only minuscule nickel impurities.

Meteoric iron was highly regarded due to its origin in the heavens and was often used to forge weapons and tools. For example, a dagger made of meteoric iron was found in the tomb of Tutankhamun, containing similar proportions of iron, cobalt, and nickel to a meteorite discovered in the area, deposited by an ancient meteor shower. Items that were likely made of iron by Egyptians date from 3000 to 2500 BC.

Meteoritic iron is comparably soft and ductile and easily cold forged but may get brittle when heated because of the nickel content.

 

The first iron production started in the Middle Bronze Age, but it took several centuries before iron displaced bronze. Samples of smelted iron from Asmar, Mesopotamia and Tall Chagar Bazaar in northern Syria were made sometime between 3000 and 2700 BC. The Hittites established an empire in north-central Anatolia around 1600 BC. They appear to be the first to understand the production of iron from its ores and regard it highly in their society. The Hittites began to smelt iron between 1500 and 1200 BC and the practice spread to the rest of the Near East after their empire fell in 1180 BC. The subsequent period is called the Iron Age.

Artifacts of smelted iron are found in India dating from 1800 to 1200 BC, and in the Levant from about 1500 BC (suggesting smelting in Anatolia or the Caucasus). Alleged references (compare history of metallurgy in South Asia) to iron in the Indian Vedas have been used for claims of a very early usage of iron in India respectively to date the texts as such. The rigveda term "ayas" (metal) probably refers to copper and bronze, while iron or "śyāma ayas", literally "black metal", first is mentioned in the post-rigvedic Atharvaveda.

Some archaeological evidence suggests iron was smelted in Zimbabwe and southeast Africa as early as the eighth century BC. Iron working was introduced to Greece in the late 11th century BC, from which it spread quickly throughout Europe.
The spread of ironworking in Central and Western Europe is associated with Celtic expansion. According to Pliny the Elder, iron use was common in the Roman era. The annual iron output of the Roman Empire is estimated at t, while the similarly populous and contemporary Han China produced around t. In China, iron only appears circa 700–500 BC. Iron smelting may have been introduced into China through Central Asia. The earliest evidence of the use of a blast furnace in China dates to the 1st century AD, and cupola furnaces were used as early as the Warring States period (403–221 BC). Usage of the blast and cupola furnace remained widespread during the Song and Tang Dynasties.

During the Industrial Revolution in Britain, Henry Cort began refining iron from pig iron to wrought iron (or bar iron) using innovative production systems. In 1783 he patented the puddling process for refining iron ore. It was later improved by others, including Joseph Hall.

Cast iron was first produced in China during 5th century BC, but was hardly in Europe until the medieval period. The earliest cast iron artifacts were discovered by archaeologists in what is now modern Luhe County, Jiangsu in China. Cast iron was used in ancient China for warfare, agriculture, and architecture. During the medieval period, means were found in Europe of producing wrought iron from cast iron (in this context known as pig iron) using finery forges. For all these processes, charcoal was required as fuel.

Medieval blast furnaces were about tall and made of fireproof brick; forced air was usually provided by hand-operated bellows. Modern blast furnaces have grown much bigger, with hearths fourteen meters in diameter that allow them to produce thousands of tons of iron each day, but essentially operate in much the same way as they did during medieval times.

In 1709, Abraham Darby I established a coke-fired blast furnace to produce cast iron, replacing charcoal, although continuing to use blast furnaces. The ensuing availability of inexpensive iron was one of the factors leading to the Industrial Revolution. Toward the end of the 18th century, cast iron began to replace wrought iron for certain purposes, because it was cheaper. Carbon content in iron was not implicated as the reason for the differences in properties of wrought iron, cast iron, and steel until the 18th century.

Since iron was becoming cheaper and more plentiful, it also became a major structural material following the building of the innovative first iron bridge in 1778. This bridge still stands today as a monument to the role iron played in the Industrial Revolution. Following this, iron was used in rails, boats, ships, aqueducts, and buildings, as well as in iron cylinders in steam engines. Railways have been central to the formation of modernity and ideas of progress and various languages (e.g. French, Spanish, Italian and German) refer to railways as "iron road".

Steel (with smaller carbon content than pig iron but more than wrought iron) was first produced in antiquity by using a bloomery. Blacksmiths in Luristan in western Persia were making good steel by 1000 BC. Then improved versions, Wootz steel by India and Damascus steel were developed around 300 BC and AD 500 respectively. These methods were specialized, and so steel did not become a major commodity until the 1850s.

New methods of producing it by carburizing bars of iron in the cementation process were devised in the 17th century. In the Industrial Revolution, new methods of producing bar iron without charcoal were devised and these were later applied to produce steel. In the late 1850s, Henry Bessemer invented a new steelmaking process, involving blowing air through molten pig iron, to produce mild steel. This made steel much more economical, thereby leading to wrought iron no longer being produced in large quantities.

In 1774, Antoine Lavoisier used the reaction of water steam with metallic iron inside an incandescent iron tube to produce hydrogen in his experiments leading to the demonstration of the conservation of mass, which was instrumental in changing chemistry from a qualitative science to a quantitative one.

Iron plays a certain role in mythology and has found various usage as a metaphor and in folklore. The Greek poet Hesiod's "Works and Days" (lines 109–201) lists different ages of man named after metals like gold, silver, bronze and iron to account for successive ages of humanity. The Iron Age was closely related with Rome, and in Ovid's "Metamorphoses"

An example of the importance of iron's symbolic role may be found in the German Campaign of 1813. Frederick William III commissioned then the first Iron Cross as military decoration. Berlin iron jewellery reached its peak production between 1813 and 1815, when the Prussian royal family urged citizens to donate gold and silver jewellery for military funding. The inscription "Gold gab ich für Eisen" (I gave gold for iron) was used as well in later war efforts.

For a few limited purposes when it is needed, pure iron is produced in the laboratory in small quantities by reducing the pure oxide or hydroxide with hydrogen, or forming iron pentacarbonyl and heating it to 250 °C so that it decomposes to form pure iron powder. Another method is electrolysis of ferrous chloride onto an iron cathode.

Nowadays, the industrial production of iron or steel consists of two main stages. In the first stage, iron ore is reduced with coke in a blast furnace, and the molten metal is separated from gross impurities such as silicate minerals. This stage yields an alloy -- pig iron—that contains relatively large amounts of carbon. In the second stage, the amount of carbon in the pig iron is lowered by oxidation to yield wrought iron, steel, or cast iron. Other metals can be added at this stage to form alloy steels.

The blast furnace is loaded with iron ores, usually hematite or magnetite , together with coke (coal that has been separately baked to remove volatile components). Air pre-heated to 900 °C is blown through the mixture, in sufficient amount to turn the carbon into carbon monoxide: 
This reaction raises the temperature to about 2000 °C The carbon monoxide reduces the iron ore to metallic iron
Some iron in the high-temperature lower region of the furnace reacts directly with the coke:

A flux such as limestone (calcium carbonate) or dolomite (calcium-magnesium carbonate) is also added to the furnace's load. Its purpose is to remove silicaceous minerals in the ore, which would otherwise clog the furnace. The heat of the furnace decomposes the carbonates to calcium oxide, which reacts with any excess silica to form a slag composed of calcium silicate or other products. At the furnace's temperature, the metal and the slag are both molten. They collect at the bottom as two immiscible liquid layers (with the slag on top), that are then easily separated.
The slag can be used as a material in road construction or to improve mineral-poor soils for agriculture.

In general, the pig iron produced by the blast furnace process contains up to 4–5% carbon, with small amounts of other impurities like sulfur, magnesium, phosphorus, and manganese. The high level of carbon makes it relatively weak and brittle. Reducing the amount of carbon to 0.002–2.1% by mass produces steel, which may be up to 1000 times harder than pure iron. A great variety of steel articles can then be made by cold working, hot rolling, forging, machining, etc. Removing the other impurities, instead, results in cast iron, which is used to cast articles in foundries; for example stoves, pipes, radiators, lamp-posts, and rails.

Steel products often undergo various heat treatments after they are forged to shape. Annealing consists of heating them to 700–800 °C for several hours and then gradual cooling. It makes the steel softer and more workable.
Owing to environmental concerns, alternative methods of processing iron have been developed. "Direct iron reduction" reduces iron ore to a ferrous lump called "sponge" iron or "direct" iron that is suitable for steelmaking. Two main reactions comprise the direct reduction process:

Natural gas is partially oxidized (with heat and a catalyst):

Iron ore is then treated with these gases in a furnace, producing solid sponge iron:

Silica is removed by adding a limestone flux as described above.

Ignition of a mixture of aluminium powder and iron oxide yields metallic iron via the thermite reaction:

Alternatively pig iron may be made into steel (with up to about 2% carbon) or wrought iron (commercially pure iron). Various processes have been used for this, including finery forges, puddling furnaces, Bessemer converters, open hearth furnaces, basic oxygen furnaces, and electric arc furnaces. In all cases, the objective is to oxidize some or all of the carbon, together with other impurities. On the other hand, other metals may be added to make alloy steels.

Iron is the most widely used of all the metals, accounting for over 90% of worldwide metal production. Its low cost and high strength often make it the material of choice material to withstand stress or transmit forces, such as the construction of machinery and machine tools, rails, automobiles, ship hulls, concrete reinforcing bars, and the load-carrying framework of buildings. Since pure iron is quite soft, it is most commonly combined with alloying elements to make steel.

The mechanical properties of iron and its alloys are extremely relevant to their structural applications. Those properties can be evaluated in various ways, including the Brinell test, the Rockwell test and the Vickers hardness test.

The properties of pure iron are often used to calibrate measurements or to compare tests. However, the mechanical properties of iron are significantly affected by the sample's purity: pure, single crystals of iron are actually softer than aluminium, and the purest industrially produced iron (99.99%) has a hardness of 20–30 Brinell.

An increase in the carbon content will cause a significant increase in the hardness and tensile strength of iron. Maximum hardness of 65 R is achieved with a 0.6% carbon content, although the alloy has low tensile strength. Because of the softness of iron, it is much easier to work with than its heavier congeners ruthenium and osmium.

α-Iron is a fairly soft metal that can dissolve only a small concentration of carbon (no more than 0.021% by mass at 910 °C). Austenite (γ-iron) is similarly soft and metallic but can dissolve considerably more carbon (as much as 2.04% by mass at 1146 °C). This form of iron is used in the type of stainless steel used for making cutlery, and hospital and food-service equipment.

Commercially available iron is classified based on purity and the abundance of additives. Pig iron has 3.5–4.5% carbon and contains varying amounts of contaminants such as sulfur, silicon and phosphorus. Pig iron is not a saleable product, but rather an intermediate step in the production of cast iron and steel. The reduction of contaminants in pig iron that negatively affect material properties, such as sulfur and phosphorus, yields cast iron containing 2–4% carbon, 1–6% silicon, and small amounts of manganese. Pig iron has a melting point in the range of 1420–1470 K, which is lower than either of its two main components, and makes it the first product to be melted when carbon and iron are heated together. Its mechanical properties vary greatly and depend on the form the carbon takes in the alloy.

"White" cast irons contain their carbon in the form of cementite, or iron carbide (FeC). This hard, brittle compound dominates the mechanical properties of white cast irons, rendering them hard, but unresistant to shock. The broken surface of a white cast iron is full of fine facets of the broken iron carbide, a very pale, silvery, shiny material, hence the appellation. Cooling a mixture of iron with 0.8% carbon slowly below 723 °C to room temperature results in separate, alternating layers of cementite and α-iron, which is soft and malleable and is called pearlite for its appearance. Rapid cooling, on the other hand, does not allow time for this separation and creates hard and brittle martensite. The steel can then be tempered by reheating to a temperature in between, changing the proportions of pearlite and martensite. The end product below 0.8% carbon content is a pearlite-αFe mixture, and that above 0.8% carbon content is a pearlite-cementite mixture.

In gray iron the carbon exists as separate, fine flakes of graphite, and also renders the material brittle due to the sharp edged flakes of graphite that produce stress concentration sites within the material. A newer variant of gray iron, referred to as ductile iron, is specially treated with trace amounts of magnesium to alter the shape of graphite to spheroids, or nodules, reducing the stress concentrations and vastly increasing the toughness and strength of the material.

Wrought iron contains less than 0.25% carbon but large amounts of slag that give it a fibrous characteristic. It is a tough, malleable product, but not as fusible as pig iron. If honed to an edge, it loses it quickly. Wrought iron is characterized by the presence of fine fibers of slag entrapped within the metal. Wrought iron is more corrosion resistant than steel. It has been almost completely replaced by mild steel for traditional "wrought iron" products and blacksmithing.

Mild steel corrodes more readily than wrought iron, but is cheaper and more widely available. Carbon steel contains 2.0% carbon or less, with small amounts of manganese, sulfur, phosphorus, and silicon. Alloy steels contain varying amounts of carbon as well as other metals, such as chromium, vanadium, molybdenum, nickel, tungsten, etc. Their alloy content raises their cost, and so they are usually only employed for specialist uses. One common alloy steel, though, is stainless steel. Recent developments in ferrous metallurgy have produced a growing range of microalloyed steels, also termed 'HSLA' or high-strength, low alloy steels, containing tiny additions to produce high strengths and often spectacular toughness at minimal cost.
Apart from traditional applications, iron is also used for protection from ionizing radiation. Although it is lighter than another traditional protection material, lead, it is much stronger mechanically. The attenuation of radiation as a function of energy is shown in the graph.

The main disadvantage of iron and steel is that pure iron, and most of its alloys, suffer badly from rust if not protected in some way, a cost amounting to over 1% of the world's economy. Painting, galvanization, passivation, plastic coating and bluing are all used to protect iron from rust by excluding water and oxygen or by cathodic protection. The mechanism of the rusting of iron is as follows:

The electrolyte is usually iron(II) sulfate in urban areas (formed when atmospheric sulfur dioxide attacks iron), and salt particles in the atmosphere in seaside areas.

Although the dominant use of iron is in metallurgy, iron compounds are also pervasive in industry. Iron catalysts are traditionally used in the Haber-Bosch process for the production of ammonia and the Fischer-Tropsch process for conversion of carbon monoxide to hydrocarbons for fuels and lubricants. Powdered iron in an acidic solvent was used in the Bechamp reduction the reduction of nitrobenzene to aniline.

Iron(III) oxide mixed with aluminium powder can be ignited to create a thermite reaction, used in welding large iron parts (like rails) and purifying ores. Iron(III) oxide and oxyhidroxide are used as reddish and ocher pigments.

Iron(III) chloride finds use in water purification and sewage treatment, in the dyeing of cloth, as a coloring agent in paints, as an additive in animal feed, and as an etchant for copper in the manufacture of printed circuit boards. It can also be dissolved in alcohol to form tincture of iron, which is used as a medicine to stop bleeding in canaries.

Iron(II) sulfate is used as a precursor to other iron compounds. It is also used to reduce chromate in cement. It is used to fortify foods and treat iron deficiency anemia. Iron(III) sulfate is used in settling minute sewage particles in tank water. Iron(II) chloride is used as a reducing flocculating agent, in the formation of iron complexes and magnetic iron oxides, and as a reducing agent in organic synthesis.

Iron is required for life. The iron–sulfur clusters are pervasive and include nitrogenase, the enzymes responsible for biological nitrogen fixation. Iron-containing proteins participate in transport, storage and used of oxygen. Iron proteins are involved in electron transfer.
Examples of iron-containing proteins in higher organisms include hemoglobin, cytochrome (see high-valent iron), and catalase. The average adult human contains about 0.005% body weight of iron, or about four grams, of which three quarters is in hemoglobin – a level that remains constant despite only about one milligram of iron being absorbed each day, because the human body recycles its hemoglobin for the iron content.

Iron acquisition poses a problem for aerobic organisms because ferric iron is poorly soluble near neutral pH. Thus, these organisms have developed means to absorb iron as complexes, sometimes taking up ferrous iron before oxidising it back to ferric iron. In particular, bacteria have evolved very high-affinity sequestering agents called siderophores.

After uptake in human cells, iron storage is precisely regulated. A major component of this regulation is the protein transferrin, which binds iron ions absorbed from the duodenum and carries it in the blood to cells. Transferrin contains Fe in the middle of a distorted octahedron, bonded to one nitrogen, three oxygens and a chelating carbonate anion that traps the Fe ion: it has such a high stability constant that it is very effective at taking up Fe ions even from the most stable complexes. At the bone marrow, transferrin is reduced from Fe and Fe and stored as ferritin to be incorporated into hemoglobin. 

The most commonly known and studied bioinorganic iron compounds (biological iron molecules) are the heme proteins: examples are hemoglobin, myoglobin, and cytochrome P450. These compounds participate in transporting gases, building enzymes, and transferring electrons. Metalloproteins are a group of proteins with metal ion cofactors. Some examples of iron metalloproteins are ferritin and rubredoxin. Many enzymes vital to life contain iron, such as catalase, lipoxygenases, and IRE-BP.

Hemoglobin is an oxygen carrier that occurs in red blood cells and contributes their color, transporting oxygen in the arteries from the lungs to the muscles where it is transferred to myoglobin, which stores it until it is needed for the metabolic oxidation of glucose, generating energy. Here the hemoglobin binds to carbon dioxide, produced when glucose is oxidized, which is transported through the veins by hemoglobin (predominantly as bicarbonate anions) back to the lungs where it is exhaled. In hemoglobin, the iron is in one of four heme groups and has six possible coordination sites; four are occupied by nitrogen atoms in a porphyrin ring, the fifth by an imidazole nitrogen in a histidine residue of one of the protein chains attached to the heme group, and the sixth is reserved for the oxygen molecule it can reversibly bind to. When hemoglobin is not attached to oxygen (and is then called deoxyhemoglobin), the Fe ion at the center of the heme group (in the hydrophobic protein interior) is in a high-spin configuration. It is thus too large to fit inside the porphyrin ring, which bends instead into a dome with the Fe ion about 55 picometers above it. In this configuration, the sixth coordination site reserved for the oxygen is blocked by another histidine residue.

When deoxyhemoglobin picks up an oxygen molecule, this histidine residue moves away and returns once the oxygen is securely attached to form a hydrogen bond with it. This results in the Fe ion switching to a low-spin configuration, resulting in a 20% decrease in ionic radius so that now it can fit into the porphyrin ring, which becomes planar. (Additionally, this hydrogen bonding results in the tilting of the oxygen molecule, resulting in a Fe–O–O bond angle of around 120° that avoids the formation of Fe–O–Fe or Fe–O–Fe bridges that would lead to electron transfer, the oxidation of Fe to Fe, and the destruction of hemoglobin.) This results in a movement of all the protein chains that leads to the other subunits of hemoglobin changing shape to a form with larger oxygen affinity. Thus, when deoxyhemoglobin takes up oxygen, its affinity for more oxygen increases, and vice versa. Myoglobin, on the other hand, contains only one heme group and hence this cooperative effect cannot occur. Thus, while hemoglobin is almost saturated with oxygen in the high partial pressures of oxygen found in the lungs, its affinity for oxygen is much lower than that of myoglobin, which oxygenates even at low partial pressures of oxygen found in muscle tissue. As described by the Bohr effect (named after Christian Bohr, the father of Niels Bohr), the oxygen affinity of hemoglobin diminishes in the presence of carbon dioxide.

Carbon monoxide and phosphorus trifluoride are poisonous to humans because they bind to hemoglobin similarly to oxygen, but with much more strength, so that oxygen can no longer be transported throughout the body. Hemoglobin bound to carbon monoxide is known as carboxyhemoglobin. This effect also plays a minor role in the toxicity of cyanide, but there the major effect is by far its interference with the proper functioning of the electron transport protein cytochrome a. The cytochrome proteins also involve heme groups and are involved in the metabolic oxidation of glucose by oxygen. The sixth coordination site is then occupied by either another imidazole nitrogen or a methionine sulfur, so that these proteins are largely inert to oxygen – with the exception of cytochrome a, which bonds directly to oxygen and thus is very easily poisoned by cyanide. Here, the electron transfer takes place as the iron remains in low spin but changes between the +2 and +3 oxidation states. Since the reduction potential of each step is slightly greater than the previous one, the energy is released step-by-step and can thus be stored in adenosine triphosphate. Cytochrome a is slightly distinct, as it occurs at the mitochondrial membrane, binds directly to oxygen, and transports protons as well as electrons, as follows:

Although the heme proteins are the most important class of iron-containing proteins, the iron-sulfur proteins are also very important, being involved in electron transfer, which is possible since iron can exist stably in either the +2 or +3 oxidation states. These have one, two, four, or eight iron atoms that are each approximately tetrahedrally coordinated to four sulfur atoms; because of this tetrahedral coordination, they always have high-spin iron. The simplest of such compounds is rubredoxin, which has only one iron atom coordinated to four sulfur atoms from cysteine residues in the surrounding peptide chains. Another important class of iron-sulfur proteins is the ferredoxins, which have multiple iron atoms. Transferrin does not belong to either of these classes.

The ability of sea mussels to maintain their grip on rocks in the ocean is facilitated by their use of organometallic iron-based bonds in their protein-rich cuticles. Based on synthetic replicas, the presence of iron in these structures increased elastic modulus 770 times, tensile strength 58 times, and toughness 92 times. The amount of stress required to permanently damage them increased 76 times.

Iron is pervasive, but particularly rich sources of dietary iron include red meat, oysters, lentils, beans, poultry, fish, leaf vegetables, watercress, tofu, chickpeas, black-eyed peas, and blackstrap molasses. Bread and breakfast cereals are sometimes specifically fortified with iron.

Iron provided by dietary supplements is often found as iron(II) fumarate, although iron(II) sulfate is cheaper and is absorbed equally well. Elemental iron, or reduced iron, despite being absorbed at only one-third to two-thirds the efficiency (relative to iron sulfate), is often added to foods such as breakfast cereals or enriched wheat flour. Iron is most available to the body when chelated to amino acids and is also available for use as a common iron supplement. Glycine, the least expensive amino acid, is most often used to produce iron glycinate supplements.

The U.S. Institute of Medicine (IOM) updated Estimated Average Requirements (EARs) and Recommended Dietary Allowances (RDAs) for iron in 2001. The current EAR for iron for women ages 14–18 is 7.9 mg/day, 8.1 for ages 19–50 and 5.0 thereafter (post menopause). For men the EAR is 6.0 mg/day for ages 19 and up. The RDA is 15.0 mg/day for women ages 15–18, 18.0 for 19–50 and 8.0 thereafter. For men, 8.0 mg/day for ages 19 and up. RDAs are higher than EARs so as to identify amounts that will cover people with higher than average requirements. RDA for pregnancy is 27 mg/day and, for lactation, 9 mg/day. For children ages 1–3 years 7 mg/day, 10 for ages 4–8 and 8 for ages 9–13. As for safety, the IOM also sets Tolerable upper intake levels (ULs) for vitamins and minerals when evidence is sufficient. In the case of iron the UL is set at 45 mg/day. Collectively the EARs, RDAs and ULs are referred to as Dietary Reference Intakes.

The European Food Safety Authority (EFSA) refers to the collective set of information as Dietary Reference Values, with Population Reference Intake (PRI) instead of RDA, and Average Requirement instead of EAR. AI and UL defined the same as in United States. For women the PRI is 13 mg/day ages 15–17 years, 16 mg/day for women ages 18 and up who are premenopausal and 11 mg/day postmenopausal. For pregnancy and lactation, 16 mg/day. For men the PRI is 11 mg/day ages 15 and older. For children ages 1 to 14 the PRI increases from 7 to 11 mg/day. The PRIs are higher than the U.S. RDAs, with the exception of pregnancy. The EFSA reviewed the same safety question did not establish a UL.

Infants may require iron supplements if they are bottle-fed cow's milk. Frequent blood donors are at risk of low iron levels and are often advised to supplement their iron intake.

For U.S. food and dietary supplement labeling purposes the amount in a serving is expressed as a percent of Daily Value (%DV). For iron labeling purposes 100% of the Daily Value was 18 mg, and remained unchanged at 18 mg. Compliance with the updated labeling regulations was required by 1 January 2020, for manufacturers with $10 million or more in annual food sales, and by 1 January 2021, for manufacturers with less than $10 million in annual food sales. During the first six months following the 1 January 2020 compliance date, the FDA plans to work cooperatively with manufacturers to meet the new Nutrition Facts label requirements and will not focus on enforcement actions regarding these requirements during that time. A table of the old and new adult Daily Values is provided at Reference Daily Intake.

Iron deficiency is the most common nutritional deficiency in the world. When loss of iron is not adequately compensated by adequate dietary iron intake, a state of latent iron deficiency occurs, which over time leads to iron-deficiency anemia if left untreated, which is characterised by an insufficient number of red blood cells and an insufficient amount of hemoglobin. Children, pre-menopausal women (women of child-bearing age), and people with poor diet are most susceptible to the disease. Most cases of iron-deficiency anemia are mild, but if not treated can cause problems like fast or irregular heartbeat, complications during pregnancy, and delayed growth in infants and children.

Iron uptake is tightly regulated by the human body, which has no regulated physiological means of excreting iron. Only small amounts of iron are lost daily due to mucosal and skin epithelial cell sloughing, so control of iron levels is primarily accomplished by regulating uptake. Regulation of iron uptake is impaired in some people as a result of a genetic defect that maps to the HLA-H gene region on chromosome 6 and leads to abnormally low levels of hepcidin, a key regulator of the entry of iron into the circulatory system in mammals. In these people, excessive iron intake can result in iron overload disorders, known medically as hemochromatosis. Many people have an undiagnosed genetic susceptibility to iron overload, and are not aware of a family history of the problem. For this reason, people should not take iron supplements unless they suffer from iron deficiency and have consulted a doctor. Hemochromatosis is estimated to be the cause of 0.3 to 0.8% of all metabolic diseases of Caucasians. 

Overdoses of ingested iron can cause excessive levels of free iron in the blood. High blood levels of free ferrous iron react with peroxides to produce highly reactive free radicals that can damage DNA, proteins, lipids, and other cellular components. Iron toxicity occurs when the cell contains free iron, which generally occurs when iron levels exceed the availability of transferrin to bind the iron. Damage to the cells of the gastrointestinal tract can also prevent them from regulating iron absorption, leading to further increases in blood levels. Iron typically damages cells in the heart, liver and elsewhere, causing adverse effects that include coma, metabolic acidosis, shock, liver failure, coagulopathy, adult respiratory distress syndrome, long-term organ damage, and even death. Humans experience iron toxicity when the iron exceeds 20 milligrams for every kilogram of body mass; 60 milligrams per kilogram is considered a lethal dose. Overconsumption of iron, often the result of children eating large quantities of ferrous sulfate tablets intended for adult consumption, is one of the most common toxicological causes of death in children under six. The Dietary Reference Intake (DRI) sets the Tolerable Upper Intake Level (UL) for adults at 45 mg/day. For children under fourteen years old the UL is 40 mg/day.

The medical management of iron toxicity is complicated, and can include use of a specific chelating agent called deferoxamine to bind and expel excess iron from the body.

The role of iron in cancer defense can be described as a "double-edged sword" because of its pervasive presence in non-pathological processes. People having chemotherapy may develop iron deficiency and anemia, for which intravenous iron therapy is used to restore iron levels. Iron overload, which may occur from high consumption of red meat, may initiate tumor growth and increase susceptibility to cancer onset, particularly for colorectal cancer.




</doc>
<doc id="14735" url="https://en.wikipedia.org/wiki?curid=14735" title="IEEE 802.15">
IEEE 802.15

IEEE 802.15 is a working group of the Institute of Electrical and Electronics Engineers (IEEE) IEEE 802 standards committee which specifies wireless personal area network (WPAN) standards. There are 10 major areas of development, not all of which are active. 

The number of Task Groups in IEEE 802.15 varies based on the number of active projects. The current list of active projects can be found on the IEEE 802.15 web site.

Task group one is based on Bluetooth technology. It defines physical layer (PHY) and Media Access Control (MAC) specification for wireless connectivity with fixed, portable and moving devices within or entering personal operating space. Standards were issued in 2002 and 2005.

Task group two addresses the coexistence of wireless personal area networks (WPAN) with other wireless devices operating in unlicensed frequency bands such as wireless local area networks (WLAN). The IEEE 802.15.2-2003 standard was published in 2003 and task group two went into "hibernation".

IEEE 802.15.3-2003 is a MAC and PHY standard for high-rate (11 to 55 Mbit/s) WPANs. The standard can be downloaded via the IEEE Get program, which is funded by IEEE 802 volunteers.

IEEE P802.15.3a was an attempt to provide a higher speed Ultra wideband PHY enhancement amendment to IEEE 802.15.3 for applications which involve imaging and multimedia. The members of the task group were not able to come to an agreement choosing between two technology proposals, Multi-band Orthogonal Frequency Division Multiplexing (MB-OFDM) and Direct Sequence UWB (DS-UWB), backed by two different industry alliances and was withdrawn in January 2006. Documents related to the development of IEEE 802.15.3a are archived on the IEEE document server.

IEEE 802.15.3b-2005 amendment was released on May 5, 2006. It enhanced 802.15.3 to improve implementation and interoperability of the MAC. This amendment include many optimizations, corrected errors, clarified ambiguities, and added editorial clarifications while preserving backward compatibility. Among other changes, the amendment defined the following new features:

IEEE 802.15.3c-2009 was published on September 11, 2009. The task group TG3c developed a millimeter-wave-based alternative physical layer (PHY) for the existing 802.15.3 Wireless Personal Area Network (WPAN) Standard 802.15.3-2003. The IEEE 802.15.3 Task Group 3c (TG3c) was formed in March 2005. This mmWave WPAN is defined to operate in the 57–66 GHz range. Depending on the geographical region, anywhere from 2 to 9 GHz of bandwidth is available (for example, 57–64 GHz is available as unlicensed band defined by FCC 47 CFR 15.255 in North America). The millimeter-wave WPAN allows very high data rate, short range (10 m) for applications including high speed internet access, streaming content download (video on demand, HDTV, home theater, etc.), real time streaming and wireless data bus for cable replacement. A total of three PHY modes were defined in the standard:

IEEE 802.15.4-2003 (Low Rate WPAN) deals with low data rate but very long battery life (months or even years) and very low complexity. The standard defines both the physical (Layer 1) and data-link (Layer 2) layers of the OSI model. The first edition of the 802.15.4 standard was released in May 2003. Several standardized and proprietary networks (or mesh) layer protocols run over 802.15.4-based networks, including IEEE 802.15.5, ZigBee, Thread, 6LoWPAN, WirelessHART, and ISA100.11a.

IEEE 802.15.4a (formally called IEEE 802.15.4a-2007) is an amendment to IEEE 802.15.4 specifying additional physical layers (PHYs) to the original standard. The principal interest was in providing higher precision ranging and localization capability (1 meter accuracy and better), higher aggregate throughput, adding scalability to data rates, longer range, and lower power consumption and cost. The selected baselines are two optional PHYs consisting of a UWB Pulse Radio (operating in unlicensed UWB spectrum) and a Chirp Spread Spectrum (operating in unlicensed 2.4 GHz spectrum). The Pulsed UWB Radio is based on Continuous Pulsed UWB technology (see C-UWB) and will be able to deliver communications and high precision ranging.

IEEE 802.15.4b was approved in June 2006 and was published in September 2006 as IEEE 802.15.4-2006. The IEEE 802.15 task group 4b was chartered to create a project for specific enhancements and clarifications to the IEEE 802.15.4-2003 standard, such as resolving ambiguities, reducing unnecessary complexity, increasing flexibility in security key usage, considerations for newly available frequency allocations, and others.

IEEE 802.15.4c was approved in 2008 and was published in January 2009. This defines a PHY amendment adds new rf spectrum specifications to address the Chinese regulatory changes which have opened the 314-316 MHz, 430-434 MHz, and 779-787 MHz bands for Wireless PAN use within China.

The IEEE 802.15 Task Group 4d was chartered to define an amendment to the 802.15.4-2006 standard. The amendment defines a new PHY and such changes to the MAC as are necessary to support a new frequency allocation (950 MHz -956 MHz) in Japan while coexisting with passive tag systems in the band.

The IEEE 802.15 Task Group 4e is chartered to define a MAC amendment to the existing standard 802.15.4-2006. The intent of this amendment is to enhance and add functionality to the 802.15.4-2006 MAC to a) better support the industrial markets and b) permit compatibility with modifications being proposed within the Chinese WPAN. Specific enhancements were made to add channel hopping and a variable time slot option compatible with ISA100.11a. These changes were approved in 2011.

The IEEE 802.15.4f Active RFID System Task Group is chartered to define new wireless Physical (PHY) layer(s) and enhancements to the 802.15.4-2006 standard MAC layer which are required to support new PHY(s) for active RFID system bi-directional and location determination applications.

IEEE 802.15.4g Smart Utility Networks (SUN) Task Group is chartered to create a PHY amendment to 802.15.4 to provide a standard that facilitates very large scale process control applications such as the utility smart grid network capable of supporting large, geographically diverse networks with minimal infrastructure, with potentially millions of fixed endpoints. In 2012 they released the 802.15.4g radio standard.
The Telecommunications Industry Association TR-51 committee develops standards for similar applications.

IEEE 802.15.5 provides the architectural framework enabling WPAN devices to promote interoperable, stable, and scalable wireless mesh networking. This standard is composed of two parts: low-rate WPAN mesh and high-rate WPAN mesh networks. The low-rate mesh is built on IEEE 802.15.4-2006 MAC, while the high rate mesh utilizes IEEE 802.15.3/3b MAC. The common features of both meshes include network initialization, addressing, and multihop unicasting. In addition, the low-rate mesh supports multicasting, reliable broadcasting, portability support, trace route and energy saving function, and the high rate mesh supports multihop time-guaranteed service.

Mesh networking for IEEE 802.15.1 networks is beyond scope of IEEE 802.15.5 and is carried within Bluetooth mesh working group.

In December 2011, the IEEE 802.15.6 task group approved a draft of a standard for Body Area Network (BAN) technologies. The draft was approved on 22 July 2011 by Letter Ballot to start the Sponsor Ballot process. Task Group 6 was formed in November 2007 to focus on a low-power and short-range wireless standard to be optimized for devices and operation on, in, or around the human body (but not limited to humans) to serve a variety of applications including medical, consumer electronics, and personal entertainment.

As of December 2011, The IEEE 802.15.7 Visible Light Communication Task Group has completed draft 5c of a PHY and MAC standard for Visible Light Communication (VLC). The inaugural meeting for Task Group 7 was held during January 2009, where it was chartered to write standards for free-space optical communication using visible light.

IEEE P802.15.8 received IEEE Standards Board approval on 29 March 2012 to form a Task Group to develop a standard for Peer Aware Communications (PAC) optimized for peer to peer and infrastructureless communications with fully distributed coordination operating in bands below 11 GHz. The proposed standard is targeting data rates greater than 100 kbit/s with scalable data rates up to 10 Mbit/s. Features of the proposed include:

The draft standard is under development, more information can be found on the IEEE 802.15 Task Group 8 web page.

IEEE P802.15.9 received IEEE Standards Board approval on 7 December 2011 to form a Task Group to develop a recommended practice for the transport of Key Management Protocol (KMP) datagrams. The recommended practice will define a message framework based on Information Elements as a transport method for key management protocol (KMP) datagrams and guidelines for the use of some existing KMPs with IEEE Std 802.15.4. The recommended practice will not create a new KMP.

While IEEE Std 802.15.4 has always supported datagram security, it has not provided a mechanism for establishing the keys used by this feature. Lack of key management support in IEEE Std 802.15.4 can result in weak keys, which is a common avenue for attacking the security system. Adding KMP support is critical to a proper security framework. Some of the existing KMPs that it may address are IETF's PANA, HIP, IKEv2, IEEE Std 802.1X, and 4-Way-Handshake.

The draft recommended practice is under development, more information can be found on the IEEE 802.15 web page.

IEEE P802.15.10 received IEEE Standards Board approval on 23 August 2013 to form a Task Group to develop a recommended practice for routing packets in dynamically changing 802.15.4 wireless networks (changes on the order of a minute time frame), with minimal impact to route handling. The goal is to extend the coverage area as the number of nodes
increase. The route related capabilities that the recommended practice will provide include the following:

The draft recommended practice is under development; more information can be found on the IEEE 802.15.10 web page.

The IEEE P802.15 Wireless Next Generation Standing Committee (SCwng) is chartered to facilitate and stimulate presentations and discussions on new wireless related technologies that may be subject for new 802.15 standardization projects or to address the whole 802.15 work group with issues or concerns with techniques or technologies.




</doc>
<doc id="14736" url="https://en.wikipedia.org/wiki?curid=14736" title="IEEE 802">
IEEE 802

IEEE 802 is a family of IEEE standards dealing with local area networks and metropolitan area networks.

The IEEE 802 standards are restricted to networks carrying variable-size packets, unlike cell relay networks, for example, where data is transmitted in short, uniformly sized units called cells. Isochronous networks, where data is transmitted as a steady stream of octets, or groups of octets, at regular time intervals, are also beyond the scope of the IEEE 802 standards.

The number “802” has no particular significance: it was simply the next available number IEEE could assign to the standards project, although "802" is sometimes associated with February 1980, the date of the first meeting.

The services and protocols specified in IEEE 802 map to the lower two layers (Data Link and Physical) of the seven-layer Open Systems Interconnection (OSI) networking reference model. In fact, IEEE 802 splits the OSI Data Link Layer into two sub-layers named logical link control (LLC) and media access control (MAC), so the layers can be listed like this: 

The IEEE 802 family of standards is maintained by the IEEE 802 LAN/MAN Standards Committee (LMSC). The most widely used standards are for the Ethernet family, Token Ring, Wireless LAN (Wi-Fi), Bridging and Virtual Bridged LANs. An individual working group provides the focus for each area. The groups are numbered from 802.1 to 802.12.




</doc>
<doc id="14739" url="https://en.wikipedia.org/wiki?curid=14739" title="IEEE 802.11">
IEEE 802.11

IEEE 802.11 is part of the IEEE 802 set of local area network (LAN) protocols, and specifies the set of media access control (MAC) and physical layer (PHY) protocols for implementing wireless local area network (WLAN) Wi-Fi computer communication in various frequencies, including but not limited to 2.4 GHz, 5 GHz, 6 GHz, and 60 GHz frequency bands.

They are the world's most widely used wireless computer networking standards, used in most home and office networks to allow laptops, printers, and smartphones to talk to each other and access the Internet without connecting wires. They are created and maintained by the Institute of Electrical and Electronics Engineers (IEEE) LAN/MAN Standards Committee (IEEE 802). The base version of the standard was released in 1997, and has had subsequent amendments. The standard and amendments provide the basis for wireless network products using the Wi-Fi brand. While each amendment is officially revoked when it is incorporated in the latest version of the standard, the corporate world tends to market to the revisions because they concisely denote capabilities of their products. As a result, in the marketplace, each revision tends to become its own standard.

The protocols are typically used in conjunction with IEEE 802.2, and are designed to interwork seamlessly with Ethernet, and are very often used to carry Internet Protocol traffic.

Although IEEE 802.11 specifications list channels that might be used, the radio frequency spectrum availability allowed varies significantly by regulatory domain.

The 802.11 family consists of a series of half-duplex over-the-air modulation techniques that use the same basic protocol. The 802.11 protocol family employs carrier-sense multiple access with collision avoidance whereby equipment listens to a channel for other users (including non 802.11 users) before transmitting each packet.

802.11-1997 was the first wireless networking standard in the family, but 802.11b was the first widely accepted one, followed by 802.11a, 802.11g, 802.11n, and 802.11ac. Other standards in the family (c–f, h, j) are service amendments that are used to extend the current scope of the existing standard, which may also include corrections to a previous specification.

802.11b and 802.11g use the 2.4 GHz ISM band, operating in the United States under Part 15 of the U.S. Federal Communications Commission Rules and Regulations; 802.11n can also use that band. Because of this choice of frequency band, 802.11b/g/n equipment may occasionally suffer interference in the 2.4 GHz band from microwave ovens, cordless telephones, and Bluetooth devices etc. 802.11b and 802.11g control their interference and susceptibility to interference by using direct-sequence spread spectrum (DSSS) and orthogonal frequency-division multiplexing (OFDM) signaling methods, respectively.

802.11a uses the 5 GHz U-NII band, which, for much of the world, offers at least 23 non-overlapping 20 MHz-wide channels rather than the 2.4 GHz ISM frequency band offering only three non-overlapping 20 MHz-wide channels, where other adjacent channels overlap—see list of WLAN channels. Better or worse performance with higher or lower frequencies (channels) may be realized, depending on the environment. 802.11n can use either the 2.4 GHz or 5 GHz band; 802.11ac uses only the 5 GHz band.

The segment of the radio frequency spectrum used by 802.11 varies between countries. In the US, 802.11a and 802.11g devices may be operated without a license, as allowed in Part 15 of the FCC Rules and Regulations. Frequencies used by channels one through six of 802.11b and 802.11g fall within the 2.4 GHz amateur radio band. Licensed amateur radio operators may operate 802.11b/g devices under Part 97 of the FCC Rules and Regulations, allowing increased power output but not commercial content or encryption.

In 2018, the Wi-Fi Alliance began using a consumer-friendly generation numbering scheme for the publicly used 802.11 protocols. Wi-Fi generations 1–6 refer to the 802.11b, 802.11a, 802.11g, 802.11n, 802.11ac, and 802.11ax protocols, in that order.

802.11 technology has its origins in a 1985 ruling by the U.S. Federal Communications Commission that released the ISM band for unlicensed use.

In 1991 NCR Corporation/AT&T (now Nokia Labs and LSI Corporation) invented a precursor to 802.11 in Nieuwegein, the Netherlands. The inventors initially intended to use the technology for cashier systems. The first wireless products were brought to the market under the name WaveLAN with raw data rates of 1 Mbit/s and 2 Mbit/s.

Vic Hayes, who held the chair of IEEE 802.11 for 10 years, and has been called the "father of Wi-Fi", was involved in designing the initial 802.11b and 802.11a standards within the IEEE.

In 1999, the Wi-Fi Alliance was formed as a trade association to hold the Wi-Fi trademark under which most products are sold.

The major commercial breakthrough came with Apple Inc. adopting Wi-Fi for their iBook series of laptops in 1999. It was the first mass consumer product to offer Wi-Fi network connectivity, which was then branded by Apple as AirPort. One year later IBM followed with its ThinkPad 1300 series in 2000.

The original version of the standard IEEE 802.11 was released in 1997 and clarified in 1999, but is now obsolete. It specified two net bit rates of 1 or 2 megabits per second (Mbit/s), plus forward error correction code. It specified three alternative physical layer technologies: diffuse infrared operating at 1 Mbit/s; frequency-hopping spread spectrum operating at 1 Mbit/s or 2 Mbit/s; and direct-sequence spread spectrum operating at 1 Mbit/s or 2 Mbit/s. The latter two radio technologies used microwave transmission over the Industrial Scientific Medical frequency band at 2.4 GHz. Some earlier WLAN technologies used lower frequencies, such as the U.S. 900 MHz ISM band.

Legacy 802.11 with direct-sequence spread spectrum was rapidly supplanted and popularized by 802.11b.

802.11a, published in 1999, uses the same data link layer protocol and frame format as the original standard, but an OFDM based air interface (physical layer). It operates in the 5 GHz band with a maximum net data rate of 54 Mbit/s, plus error correction code, which yields realistic net achievable throughput in the mid-20 Mbit/s. It has seen widespread worldwide implementation, particularly within the corporate workspace.

Since the 2.4 GHz band is heavily used to the point of being crowded, using the relatively unused 5 GHz band gives 802.11a a significant advantage. However, this high carrier frequency also brings a disadvantage: the effective overall range of 802.11a is less than that of 802.11b/g. In theory, 802.11a signals are absorbed more readily by walls and other solid objects in their path due to their smaller wavelength, and, as a result, cannot penetrate as far as those of 802.11b. In practice, 802.11b typically has a higher range at low speeds (802.11b will reduce speed to 5.5 Mbit/s or even 1 Mbit/s at low signal strengths). 802.11a also suffers from interference, but locally there may be fewer signals to interfere with, resulting in less interference and better throughput.

The 802.11b standard has a maximum raw data rate of 11 Mbit/s (Megabits per second), and uses the same media access method defined in the original standard. 802.11b products appeared on the market in early 2000, since 802.11b is a direct extension of the modulation technique defined in the original standard. The dramatic increase in throughput of 802.11b (compared to the original standard) along with simultaneous substantial price reductions led to the rapid acceptance of 802.11b as the definitive wireless LAN technology.

Devices using 802.11b experience interference from other products operating in the 2.4 GHz band. Devices operating in the 2.4 GHz range include microwave ovens, Bluetooth devices, baby monitors, cordless telephones, and some amateur radio equipment. As unlicensed intentional radiators in this ISM band, they must not interfere with and must tolerate interference from primary or secondary allocations (users) of this band, such as amateur radio.

In June 2003, a third modulation standard was ratified: 802.11g. This works in the 2.4 GHz band (like 802.11b), but uses the same OFDM based transmission scheme as 802.11a. It operates at a maximum physical layer bit rate of 54 Mbit/s exclusive of forward error correction codes, or about 22 Mbit/s average throughput. 802.11g hardware is fully backward compatible with 802.11b hardware, and therefore is encumbered with legacy issues that reduce throughput by ~21% when compared to 802.11a.

The then-proposed 802.11g standard was rapidly adopted in the market starting in January 2003, well before ratification, due to the desire for higher data rates as well as reductions in manufacturing costs. By summer 2003, most dual-band 802.11a/b products became dual-band/tri-mode, supporting a and b/g in a single mobile adapter card or access point. Details of making b and g work well together occupied much of the lingering technical process; in an 802.11g network, however, activity of an 802.11b participant will reduce the data rate of the overall 802.11g network.

Like 802.11b, 802.11g devices also suffer interference from other products operating in the 2.4 GHz band, for example wireless keyboards.

In 2003, task group TGma was authorized to "roll up" many of the amendments to the 1999 version of the 802.11 standard. REVma or 802.11ma, as it was called, created a single document that merged 8 amendments (802.11a, b, d, e, g, h, i, j) with the base standard. Upon approval on 8 March 2007, 802.11REVma was renamed to the then-current base standard IEEE 802.11-2007.

802.11n is an amendment that improves upon the previous 802.11 standards, that had the first draft of certification published in 2006. The 802.11n standard was retroactively labelled as Wi-Fi 4 by the Wi-Fi Alliance. The standard added support for multiple-input multiple-output antennas (MIMO). 802.11n operates on both the 2.4 GHz and the 5 GHz bands. Support for 5 GHz bands is optional. Its net data rate ranges from 54 Mbit/s to 600 Mbit/s. The IEEE has approved the amendment, and it was published in October 2009. Prior to the final ratification, enterprises were already migrating to 802.11n networks based on the Wi-Fi Alliance's certification of products conforming to a 2007 draft of the 802.11n proposal.

In May 2007, task group TGmb was authorized to "roll up" many of the amendments to the 2007 version of the 802.11 standard. REVmb or 802.11mb, as it was called, created a single document that merged ten amendments (802.11k, r, y, n, w, p, z, v, u, s) with the 2007 base standard. In addition much cleanup was done, including a reordering of many of the clauses. Upon publication on 29 March 2012, the new standard was referred to as IEEE 802.11-2012.

IEEE 802.11ac-2013 is an amendment to IEEE 802.11, published in December 2013, that builds on 802.11n. The 802.11ac standard was retroactively labelled as Wi-Fi 5 by Wi-Fi Alliance. Changes compared to 802.11n include wider channels (80 or 160 MHz versus 40 MHz) in the 5 GHz band, more spatial streams (up to eight versus four), higher-order modulation (up to 256-QAM vs. 64-QAM), and the addition of Multi-user MIMO (MU-MIMO). The Wi-Fi Alliance separated the introduction of ac wireless products into two phases ("wave"), named "Wave 1" and "Wave 2". From mid-2013, the alliance started certifying Wave 1 802.11ac products shipped by manufacturers, based on the IEEE 802.11ac Draft 3.0 (the IEEE standard was not finalized until later that year). In 2016 Wi-Fi Alliance introduced the Wave 2 certification, to provide higher bandwidth and capacity than Wave 1 products. Wave 2 products include additional features like MU-MIMO, 160 MHz channel width support, support for more 5 GHz channels, and four spatial streams (with four antennas; compared to three in Wave 1 and 802.11n, and eight in IEEE's 802.11ax specification).

IEEE 802.11ad is an amendment that defines a new physical layer for 802.11 networks to operate in the 60 GHz millimeter wave spectrum. This frequency band has significantly different propagation characteristics than the 2.4 GHz and 5 GHz bands where Wi-Fi networks operate. Products implementing the 802.11ad standard are being brought to market under the WiGig brand name. The certification program is now being developed by the Wi-Fi Alliance instead of the now defunct Wireless Gigabit Alliance. The peak transmission rate of 802.11ad is 7 Gbit/s.

IEEE 802.11ad is a protocol used for very high data rates (about 8 Gbit/s) and for short range communication (about 1–10 meters).

TP-Link announced the world's first 802.11ad router in January 2016.

The WiGig standard is not too well known, although it was announced in 2009 and added to the IEEE 802.11 family in December 2012.

IEEE 802.11af, also referred to as "White-Fi" and "Super Wi-Fi", is an amendment, approved in February 2014, that allows WLAN operation in TV white space spectrum in the VHF and UHF bands between 54 and 790 MHz. It uses cognitive radio technology to transmit on unused TV channels, with the standard taking measures to limit interference for primary users, such as analog TV, digital TV, and wireless microphones. Access points and stations determine their position using a satellite positioning system such as GPS, and use the Internet to query a geolocation database (GDB) provided by a regional regulatory agency to discover what frequency channels are available for use at a given time and position. The physical layer uses OFDM and is based on 802.11ac. The propagation path loss as well as the attenuation by materials such as brick and concrete is lower in the UHF and VHF bands than in the 2.4 GHz and 5 GHz bands, which increases the possible range. The frequency channels are 6 to 8 MHz wide, depending on the regulatory domain. Up to four channels may be bonded in either one or two contiguous blocks. MIMO operation is possible with up to four streams used for either space–time block code (STBC) or multi-user (MU) operation. The achievable data rate per spatial stream is 26.7 Mbit/s for 6 and 7 MHz channels, and 35.6 Mbit/s for 8 MHz channels. With four spatial streams and four bonded channels, the maximum data rate is 426.7 Mbit/s for 6 and 7 MHz channels and 568.9 Mbit/s for 8 MHz channels.

IEEE 802.11-2016 which was known as IEEE 802.11 REVmc, is a revision based on IEEE 802.11-2012, incorporating 5 amendments (11ae, 11aa, 11ad, 11ac, 11af). In addition, existing MAC and PHY functions have been enhanced and obsolete features were removed or marked for removal. Some clauses and annexes have been renumbered.

IEEE 802.11ah, published in 2017, defines a WLAN system operating at sub-1 GHz license-exempt bands. Due to the favorable propagation characteristics of the low frequency spectra, 802.11ah can provide improved transmission range compared with the conventional 802.11 WLANs operating in the 2.4 GHz and 5 GHz bands. 802.11ah can be used for various purposes including large scale sensor networks, extended range hotspot, and outdoor Wi-Fi for cellular traffic offloading, whereas the available bandwidth is relatively narrow. The protocol intends consumption to be competitive with low power Bluetooth, at a much wider range.

IEEE 802.11ai is an amendment to the 802.11 standard that added new mechanisms for a faster initial link setup time.

IEEE 802.11aj is a rebanding of 802.11ad for use in the 45 GHz unlicensed spectrum available in some regions of the world (specifically China).

Alternatively known as China Millimeter Wave (CMMW).

IEEE 802.11aq is an amendment to the 802.11 standard that will enable pre-association discovery of services. This extends some of the mechanisms in 802.11u that enabled device discovery to further discover the services running on a device, or provided by a network.

IEEE 802.11ax (marketed as Wi-Fi 6 by the Wi-Fi Alliance) is the successor to 802.11ac, and will increase the efficiency of WLAN networks. This project has the goal of providing 4x the throughput of 802.11ac at the user layer, having just 37% higher nominal data rates at the PHY layer. The 802.11ax standard is expected to become an official IEEE specification in September 2020. In the previous amendment of 802.11 (namely 802.11ac), Multi-User MIMO has been introduced, which is a spatial multiplexing technique. MU-MIMO allows the Access Point to form beams towards each Client, while transmitting information simultaneously. By doing so, the interference between Clients is reduced, and the overall throughput is increased, since multiple Clients can receive data at the same time. With 802.11ax, a similar multiplexing is introduced in the frequency domain, namely OFDMA. With this technique, multiple Clients are assigned with different Resource Units in the available spectrum. By doing so, an 80 MHz channel can be split into multiple Resource Units, so that multiple Clients receive different type of data over the same spectrum, simultaneously. In order to have enough subcarriers to support the requirements of OFDMA, four times as many subcarriers are needed than by the 802.11ac standard. In other words, for 20, 40, 80 and 160 MHz channels, there are 64, 128, 256 and 512 subcarriers in the 802.11ac standard, but 256, 512, 1024 and 2048 subcarriers in the 802.11ax standard. Since the available bandwidths have not changed and the number of subcarriers increases by a factor of 4, the subcarrier spacing is reduced by the same factor, which introduces 4 times longer OFDM symbols: for 802.11ac the duration of an OFDM symbol is 3.2 microseconds, and for 802.11ax it is 12.8 microseconds (both without guard intervals).

IEEE 802.11ay is a standard that is being developed. It is an amendment that defines a new physical layer for 802.11 networks to operate in the 60 GHz millimeter wave spectrum. It will be an extension of the existing 11ad, aimed to extend the throughput, range and use-cases. The main use-cases include: indoor operation, out-door back-haul and short range communications. The peak transmission rate of 802.11ay is 20 Gbit/s. The main extensions include: channel bonding (2, 3 and 4), MIMO (up to 4 streams) and higher modulation schemes.

IEEE 802.11ba Wake-up Radio (WUR) Operation is an amendment of IEEE 802.11 standard. 802.11ba enables energy efficient operation for data reception without increasing latency. The target active power consumption to receive a WUR packet is less than 1 milliwatt and supports data rates of 62.5 kbps and 250 kbps. The WUR PHY uses MC-OOK (multicarrier OOK) to achieve extremely low power consumption.

IEEE 802.11be Extremely High Throughput (EHT) is the potential next amendment of the 802.11 IEEE standard, and will likely be designated as Wi-Fi 7. It will build upon 802.11ax, focusing on WLAN indoor and outdoor operation with stationary and pedestrian speeds in the 2.4 GHz, 5 GHz, and 6 GHz frequency bands.

Across all variations of 802.11, maximum achievable throughputs are given either based on measurements under ideal conditions or in the layer-2 data rates. However, this does not apply to typical deployments in which data is being transferred between two endpoints, of which at least one is typically connected to a wired infrastructure and the other endpoint is connected to an infrastructure via a wireless link.

This means that, typically, data frames pass an 802.11 (WLAN) medium, and are being converted to 802.3 (Ethernet) or vice versa. Due to the difference in the frame (header) lengths of these two media, the application's packet size determines the speed of the data transfer. This means applications that use small packets (e.g., VoIP) create dataflows with high-overhead traffic (i.e., a low goodput). Other factors that contribute to the overall application data rate are the speed with which the application transmits the packets (i.e., the data rate) and, of course, the energy with which the wireless signal is received. The latter is determined by distance and by the configured output power of the communicating devices.

The same references apply to the attached graphs that show measurements of UDP throughput. Each represents an average (UDP) throughput (please note that the error bars are there, but barely visible due to the small variation) of 25 measurements. Each is with a specific packet size (small or large) and with a specific data rate (10 kbit/s – 100 Mbit/s). Markers for traffic profiles of common applications are included as well. These figures assume there are no packet errors, which if occurring will lower transmission rate further.

802.11b, 802.11g, and 802.11n-2.4 utilize the spectrum, one of the ISM bands. 802.11a, 802.11n and 802.11ac use the more heavily regulated band. These are commonly referred to as the "2.4 GHz and 5 GHz bands" in most sales literature. Each spectrum is sub-divided into "channels" with a center frequency and bandwidth, analogous to the way radio and TV broadcast bands are sub-divided.

The 2.4 GHz band is divided into 14 channels spaced 5 MHz apart, beginning with channel 1, which is centered on 2.412 GHz. The latter channels have additional restrictions or are unavailable for use in some regulatory domains.

The channel numbering of the spectrum is less intuitive due to the differences in regulations between countries. These are discussed in greater detail on the list of WLAN channels.

In addition to specifying the channel center frequency, 802.11 also specifies (in Clause 17) a spectral mask defining the permitted power distribution across each channel. The mask requires the signal be attenuated a minimum of 20 dB from its peak amplitude at ±11 MHz from the centre frequency, the point at which a channel is effectively 22 MHz wide. One consequence is that stations can use only every fourth or fifth channel without overlap.

Availability of channels is regulated by country, constrained in part by how each country allocates radio spectrum to various services. At one extreme, Japan permits the use of all 14 channels for 802.11b, and for 802.11g/n-2.4. Other countries such as Spain initially allowed only channels 10 and 11, and France allowed only 10, 11, 12, and 13; however, Europe now allow channels 1 through 13. North America and some Central and South American countries allow only 

Since the spectral mask defines only power output restrictions up to ±11 MHz from the center frequency to be attenuated by −50 dBr, it is often assumed that the energy of the channel extends no further than these limits. It is more correct to say that, given the separation between channels, the overlapping signal on any channel should be sufficiently attenuated to minimally interfere with a transmitter on any other channel. Due to the near-far problem a transmitter can impact (desense) a receiver on a "non-overlapping" channel, but only if it is close to the victim receiver (within a meter) or operating above allowed power levels. Conversely, a sufficiently distant transmitter on an overlapping channel can have little to no significant effect.

Confusion often arises over the amount of channel separation required between transmitting devices. 802.11b was based on direct-sequence spread spectrum (DSSS) modulation and utilized a channel bandwidth of 22 MHz, resulting in "three" "non-overlapping" channels (1, 6, and 11). 802.11g was based on OFDM modulation and utilized a channel bandwidth of 20 MHz. This occasionally leads to the belief that "four" "non-overlapping" channels (1, 5, 9, and 13) exist under 802.11g, although this is not the case as per 17.4.6.3 Channel Numbering of operating channels of the IEEE Std 802.11 (2012), which states "In a multiple cell network topology, overlapping and/or adjacent cells using different channels can operate simultaneously without interference if the distance between the center frequencies is at least 25 MHz."
and section 18.3.9.3 and Figure 18-13.

This does not mean that the technical overlap of the channels recommends the non-use of overlapping channels. The amount of inter-channel interference seen on a configuration using channels 1, 5, 9, and 13 (which is permitted in Europe, but not in North America) is barely different from a three-channel configuration, but with an entire extra channel.

However, overlap between channels with more narrow spacing (e.g. 1, 4, 7, 11 in North America) may cause unacceptable degradation of signal quality and throughput, particularly when users transmit near the boundaries of AP cells.

IEEE uses the phrase "regdomain" to refer to a legal regulatory region. Different countries define different levels of allowable transmitter power, time that a channel can be occupied, and different available channels. Domain codes are specified for the United States, Canada, ETSI (Europe), Spain, France, Japan, and China.

Most Wi-Fi certified devices default to "regdomain" 0, which means least common denominator settings, i.e., the device will not transmit at a power above the allowable power in any nation, nor will it use frequencies that are not permitted in any nation.

The "regdomain" setting is often made difficult or impossible to change so that the end users do not conflict with local regulatory agencies such as the United States' Federal Communications Commission.

The datagrams are called "frames". Current 802.11 standards specify frame types for use in transmission of data as well as management and control of wireless links.

Frames are divided into very specific and standardized sections. Each frame consists of a "MAC header", "payload", and "frame check sequence" (FCS). Some frames may not have a payload.

The first two bytes of the MAC header form a frame control field specifying the form and function of the frame. This frame control field is subdivided into the following sub-fields:

The next two bytes are reserved for the Duration ID field that indicates how long the field's transmission will take so other devices know when the channel will be available again. This field can take one of three forms: Duration, Contention-Free Period (CFP), and Association ID (AID).

An 802.11 frame can have up to four address fields. Each field can carry a MAC address. Address 1 is the receiver, Address 2 is the transmitter, Address 3 is used for filtering purposes by the receiver. Address 4 is only present in data frames transmitted between access points in an Extended Service Set or between intermediate nodes in a mesh network.

The remaining fields of the header are:

The payload or frame body field is variable in size, from 0 to 2304 bytes plus any overhead from security encapsulation, and contains information from higher layers.

The Frame Check Sequence (FCS) is the last four bytes in the standard 802.11 frame. Often referred to as the Cyclic Redundancy Check (CRC), it allows for integrity check of retrieved frames. As frames are about to be sent, the FCS is calculated and appended. When a station receives a frame, it can calculate the FCS of the frame and compare it to the one received. If they match, it is assumed that the frame was not distorted during transmission.

Management frames are not always authenticated, and allow for the maintenance, or discontinuance, of communication. Some common 802.11 subtypes include:

The body of a management frame consists of frame-subtype-dependent fixed fields followed by a sequence of information elements (IEs).

The common structure of an IE is as follows:

Control frames facilitate in the exchange of data frames between stations. Some common 802.11 control frames include:

Data frames carry packets from web pages, files, etc. within the body. The body begins with an IEEE 802.2 header, with the Destination Service Access Point (DSAP) specifying the protocol, followed by a Subnetwork Access Protocol (SNAP) header if the DSAP is hex AA, with the organizationally unique identifier (OUI) and protocol ID (PID) fields specifying the protocol. If the OUI is all zeroes, the protocol ID field is an EtherType value. Almost all 802.11 data frames use 802.2 and SNAP headers, and most use an OUI of 00:00:00 and an EtherType value.

Similar to TCP congestion control on the internet, frame loss is built into the operation of 802.11. To select the correct transmission speed or Modulation and Coding Scheme, a rate control algorithm may test different speeds. The actual packet loss rate of an Access points vary widely for different link conditions. There are variations in the loss rate experienced on production Access points, between 10% and 80%, with 30% being a common average. It is important to be aware that the link layer should recover these lost frames. If the sender does not receive an Acknowledgement (ACK) frame, then it will be resent.

Within the IEEE 802.11 Working Group, the following IEEE Standards Association Standard and Amendments exist:


802.11F and 802.11T are recommended practices rather than standards, and are capitalized as such.

802.11m is used for standard maintenance. 802.11ma was completed for 802.11-2007, 802.11mb for 802.11-2012, and 802.11mc for 802.11-2016.

Both the terms "standard" and "amendment" are used when referring to the different variants of IEEE standards.

As far as the IEEE Standards Association is concerned, there is only one current standard; it is denoted by IEEE 802.11 followed by the date that it was published. IEEE 802.11-2016 is the only version currently in publication, superseding previous releases. The standard is updated by means of amendments. Amendments are created by task groups (TG). Both the task group and their finished document are denoted by 802.11 followed by a non-capitalized letter, for example, IEEE 802.11a and IEEE 802.11b. Updating 802.11 is the responsibility of task group m. In order to create a new version, TGm combines the previous version of the standard and all published amendments. TGm also provides clarification and interpretation to industry on published documents. New versions of the IEEE 802.11 were published in 1999, 2007, 2012, and 2016.

Various terms in 802.11 are used to specify aspects of wireless local-area networking operation, and may be unfamiliar to some readers.

For example, Time Unit (usually abbreviated TU) is used to indicate a unit of time equal to 1024 microseconds. Numerous time constants are defined in terms of TU (rather than the nearly equal millisecond).

Also the term "Portal" is used to describe an entity that is similar to an 802.1H bridge. A Portal provides access to the WLAN by non-802.11 LAN STAs.

In 2001, a group from the University of California, Berkeley presented a paper describing weaknesses in the 802.11 Wired Equivalent Privacy (WEP) security mechanism defined in the original standard; they were followed by Fluhrer, Mantin, and Shamir's paper titled "Weaknesses in the Key Scheduling Algorithm of RC4". Not long after, Adam Stubblefield and AT&T publicly announced the first verification of the attack. In the attack, they were able to intercept transmissions and gain unauthorized access to wireless networks.

The IEEE set up a dedicated task group to create a replacement security solution, 802.11i (previously this work was handled as part of a broader 802.11e effort to enhance the MAC layer). The Wi-Fi Alliance announced an interim specification called Wi-Fi Protected Access (WPA) based on a subset of the then current IEEE 802.11i draft. These started to appear in products in mid-2003. IEEE 802.11i (also known as WPA2) itself was ratified in June 2004, and uses the Advanced Encryption Standard (AES), instead of RC4, which was used in WEP. The modern recommended encryption for the home/consumer space is WPA2 (AES Pre-Shared Key), and for the enterprise space is WPA2 along with a RADIUS authentication server (or another type of authentication server) and a strong authentication method such as EAP-TLS.

In January 2005, the IEEE set up yet another task group "w" to protect management and broadcast frames, which previously were sent unsecured. Its standard was published in 2009.

In December 2011, a security flaw was revealed that affects some wireless routers with a specific implementation of the optional Wi-Fi Protected Setup (WPS) feature. While WPS is not a part of 802.11, the flaw allows an attacker within the range of the wireless router to recover the WPS PIN and, with it, the router's 802.11i password in a few hours.

In late 2014, Apple announced that its iOS 8 mobile operating system would scramble MAC addresses during the pre-association stage to thwart retail footfall tracking made possible by the regular transmission of uniquely identifiable probe requests.

Wi-Fi users may be subjected to a Wi-Fi deauthentication attack to eavesdrop, attack passwords or simply to force the use of another, usually more expensive access point.

Many companies implement wireless networking equipment with non-IEEE standard 802.11 extensions either by implementing proprietary or draft features. These changes may lead to incompatibilities between these extensions.



</doc>
<doc id="14741" url="https://en.wikipedia.org/wiki?curid=14741" title="Irn-Bru">
Irn-Bru

Irn-Bru ( "iron brew") is a Scottish carbonated soft drink, often described as "Scotland's other national drink" (after whisky). It is produced in Westfield, Cumbernauld, North Lanarkshire, by A.G. Barr of Glasgow.

As well as being sold throughout the United Kingdom, Irn-Bru is available throughout the world and can usually be bought where there is a significant community of people from Scotland. Innovative and sometimes controversial marketing campaigns have kept it a top selling soft drink in Scotland, competing directly with global brands such as Coca-Cola and Pepsi.

Irn-Bru is known for its bright orange colour and unique flavour. As of 1999 it contained 0.002% of ammonium ferric citrate, sugar, 32 flavouring agents including caffeine and quinine (but not in Australia), and two controversial colourings (Sunset Yellow FCF E110 and Ponceau 4R E124). On 27 January 2010, A.G. Barr agreed to a Food Standards Agency voluntary ban on these two colourings although no date was set for their replacement. After lobbying by First Minister of Scotland Alex Salmond, a proposed restriction of Sunset Yellow to 10 mg/litre was eased to 20 mg/litre in 2011 — the same amount present in Irn-Bru. As of May 2017, Irn-Bru still contains these colourings.

The first Iron Brew drink was produced by the Maas & Waldstein chemicals company of New York in 1889 under the name IRONBREW. The drink was popular across North America and was widely copied. A similar beverage was launched in 1898 by London essence firm Stevenson & Howell who supplied soft drinks manufacturers in the UK and colonies. Many local bottlers around the UK began selling their own version of the beverage. Despite the official launch date for Barr's Iron Brew being given as 1901, the firms AG Barr & Co (Glasgow) and Robert Barr (Falkirk) jointly launched their own Iron Brew drink at least two years earlier, according to a document in the firm's archives which indicates that the drink was already enjoying strong sales by May 1899. The strongman image which Barr's adopted for their bottle labels and advertising had been trademarked by the firm Stevenson & Howell in 1898. Barr's ordered their labels directly from Stevenson & Howell who also sold Barr's many of the individual flavours with which they mixed their own drinks. An advertisement for Barr's Iron Brew dated 1900 featuring the original strongman label can be found in Falkirk's Local History Archives.

Barr's trademark application for the brand name Irn-Bru dates from July 1946 when the drink was still off sale because of wartime regulations. The firm first commercialised their drink using this new name in 1948 once government SDI consolidation of the soft drinks industry had ended. The name change followed the introduction of new labelling restrictions which cracked down on spurious health claims and introduced minimum standards for drinks claiming to contain minerals such as iron. However, according to Robert Barr OBE (chairman 1947–1978), there was also a commercial rationale behind the unusual spelling. "Iron Brew" had come to be understood as a generic product category in the UK, whereas adopting the name "Irn-Bru" allowed the firm to have a legally protected brand identity that would enable the firm to benefit from the popularity of their wartime "Adventures of Ba-Bru" comic strip advertising. (The "Iron Brew" name has continued to be used for many versions of the drink sold by rival manufacturers.)

1980 saw the introduction of Low Calorie Irn-Bru: this was re-launched in 1991 as Diet Irn-Bru and again in 2011 as Irn-Bru Sugar Free. The Irn-Bru 32 energy drink variant was launched in 2006.

Irn-Bru has long been the most popular soft drink in Scotland, with Coca-Cola second, but competition between the two brands has brought their sales to roughly equal levels as of 2003. It is also the third best selling soft drink in the UK, after Coca-Cola and Pepsi, outselling high-profile brands such as Fanta, Dr Pepper, Sprite and 7 Up. This success in defending its home market (a feat claimed only by Irn-Bru, Inca Kola and Thums Up; Thums Up sold out to Coca-Cola in 1993, and Inka Kola owners Corporación Lindley S.A. entered into a joint venture with Coca-Cola in 1999, giving up all rights to the name outside Peru) led to ongoing speculation that Coca-Cola, PepsiCo, Inc. or its UK brand franchisee Britvic would attempt to buy A.G. Barr. In November 2012 AG Barr and Britvic announced a merger proposal, in July 2013 the merger collapsed when terms could not be agreed.

Irn-Bru's advertising slogans used to be 'Scotland's other National Drink', referring to whisky, and 'Made in Scotland from girders', a reference to the rusty colour of the drink; though the closest one can come to substantiating this claim is the 0.002% ammonium ferric citrate listed in the ingredients.

Fiery Irn-Bru, a limited edition variant, was released in autumn 2011. Packaged with a black and orange design, and with the signature man icon with an added image of a fire, it had a warm, tingly feeling in the mouth once drunk. It featured the traditional Irn-Bru flavour with an aftertaste similar to ginger.

Irn-Bru was also sold in reusable 750 ml glass bottles which, like other Barr's drinks, were able to be returned to the manufacturer in exchange for a 30 pence (previously 20p) deposit paid on purchase. This scheme was widely available in shops across Scotland and led to the colloquial term for an empty: a "glass cheque". As a result of a 40% drop in returned bottles since the 90s Barr deemed the washing and re-filling process uneconomical, and on 1 January 2016 ceased the scheme.

2016 saw the introduction the current logo, conveying strength and an industrial feel, and a new diet variant called Irn-Bru Xtra in different branding to the existing sugar free variety in a similar fashion to Coca-Cola Zero and Pepsi Max.

Barr changed the formula of Irn-Bru in January 2018 in response to a sugar tax implemented in the UK in April 2018, intended to combat obesity. By reducing the sugar content to less than 5g per 100ml, Barr has made Irn-Bru exempt from the tax. The manufacturer asserts that most people will not be able to tell the difference in flavour between the old and new formulas, but fans of the drink have started the 'Save Real Irn-Bru' campaign to stop or reverse this change, and have been stocking up on the more sugary formula.

In May 2019, Barr announced a new energy drink variant of Irn-Bru called Irn-Bru Energy, which was released on 1 July 2019.

It is produced in Westfield, Cumbernauld, North Lanarkshire, since Barr's moved out of their Parkhead, Glasgow factory in the mid-2000s. In 2011, Irn-Bru closed their factory in Mansfield, making the Westfield plant in Cumbernauld the main location for production. Other manufacturing locations include the English city of Sheffield.

Irn-Bru and other Barr brands including Pineappleade, Cream Soda, Tizer, Red Kola, Barr Cola, and Limeade are still available in 750 ml reusable glass bottles.


Irn-Bru and Diet Irn-Bru are available in the following sizes:

In May 2007, A.G Barr re-designed the Irn-Bru Can and Bottle Logos.

In April 2016, A.G Barr released the redesigned Irn-Bru Can and Bottle Logos.

Barr's actively promoted their Irn-Bru from the outset, with some of their earliest ads featuring world champion wrestlers and Highland Games athletes Donald Dinnie and Alex Munro who endorsed the drink by means of personal testimonials. In the 1930s, the firm began a long-running series of comic strip ads entitled "The Adventures of Ba-Bru" which ran in various local papers from April 1939 until October 1970. The last traces of this campaign, a large neon sign featuring Ba-Bru which stood in Union St above Glasgow Central railway station, was removed in 1983 and replaced with an illuminated display featuring the tagline "Your Other National Drink".

Barr has a long-established gimmick associating Irn-Bru with Scottishness, stemming from the claim of it being Scotland's most popular soft drink. A tagline, "Made in Scotland from girders", was used for several years from the 1980s, usually featuring Irn-Bru drinkers becoming unusually strong, durable or magnetic.

An advertising campaign launched in Spring 2000 aimed to "dramatise the extraordinary appeal of Irn-Bru in a likeably maverick style". David Amers, Planning Director, said: "Irn-Bru is the likeable maverick of the soft drinks market and these ads perfectly capture the brand's spirit." One involved a grandfather (played by actor Robert Wilson) who removed his false teeth to spoil his grandson's interest in his can of Irn-Bru. A further TV advertisement featured a senior citizen in a motorised wheelchair robbing a local shopping market of a supply of Irn-Bru.

In 2004 Irn created a new concept "Phenomenal". In 2006 the company launched its first Christmas adverts. This campaign consisted of a parody commercial of a popular Christmas Cartoon, "The Snowman", and was effective in interesting American audiences in the Irn-Bru brand.

Further advertising campaigns for Irn-Bru appeared in conjunction with the release of Irn-Bru 32 in 2006.

A 2009 advertisement for the product featured a group of high school pupils performing a musical number, with the refrain "It's fizzy, it's ginger, it's phenomenal!" It was a parody of "High School Musical", and starred Jack Lowden.

In 2012 the company changed its slogan to "gets you through", which see a number of people drinking Irn-Bru to get through tough situations.

In response to the Coca-Cola 'Share a Coke' campaign, Barr decided to produce thousands of limited edition 750 ml bottles of "Irn-Bru" with the names 'Fanny', 'Senga', 'Rab' and 'Tam' on the label, mimicking that by Coca-Cola. The use of the name 'Fanny' ties in with one of "Irn-Bru"'s controversial marketing advertisements.

In December 2018, 12 years after the original Christmas advert left off, with the child being in a snow bank in Edinburgh, returned to the screens as a sequel which involved the child taking a Seaplane to chase down the Snowman who had his Irn-Bru. In the end, the drink gets stolen by Santa.

One of the most controversial Irn-Bru television adverts evoked 1950s entertainment. A mother plays the piano, while the father and two children deliver a song which ends with the mother singing: "...even though I used to be a man". This advertisement was broadcast in 2000, but when it was repeated in 2003, it led to seventeen complaints about it being offensive to members of the transgender community. Issue A14 of the Ofcom Advertising Complaints bulletin reports that the children's response to their mother's claim was not offensive. The advertisement was meant to be a joke about changing points of view over time. However, the scene involving the mother shaving at the end of the advertisement was deemed to be potentially offensive to transgender people, and so it was taken off the air.

In 2003, an Irn-Bru commercial which showed a midwife trying to entice a baby from its mother's womb during a difficult delivery sparked fifty complaints. Some saw it as upsetting to women who had suffered miscarriages.

One billboard that drew criticism featured a young woman in a bikini along with the slogan "Diet Irn-Bru. I never knew could give so much pleasure". Another featured a picture of a cow with the slogan "When I'm a burger, I want to be washed down with Irn-Bru". This billboard resulted in over 700 complaints but was cleared by advertisement watchdogs. A billboard which featured a depressed goth and the slogan "Cheer up Goth. Have an Irn-Bru." was also criticised for inciting bullying.

McCowan's also produced Irn-Bru Bars, chewy, fizzy, bright orange confectionery bars which taste very strongly of Irn-Bru, though production ended in late 2005. Irn-Bru sorbet is available in some speciality ice cream shops in Scotland.

The drink can be used as a mixer with alcoholic beverages, mainly vodka and whisky.

Barr launched an alcopop drink combining Irn-Bru and Bell's whisky, although this proved to be unpopular and was discontinued. A later attempt came in the form of an official Irn-Bru flavour in the Red Square line-up of vodka-based drinks; this too has been discontinued.

Irn-Bru is manufactured under licence in Russia by the Moscow Brewing Company. Bru and other Barr products are exported to Spain, the Netherlands, Germany, Gibraltar, Greece and Cyprus, as well as parts of Africa and Asia. It is available in the Republic of Ireland, increasingly being stocked in BWG and ADM Londis supplied stores, as well as in supermarkets owned by Dunnes Stores and Tesco Ireland. In Ireland generally, the drink mainly sells in Ulster, chiefly in Northern Ireland and County Donegal. It is also available in Malta, Belgium and, as of 2005, in Poland. It is now sold in Iceland, as of 2011. A similarly named product, using the "Iron Brew" spelling but bearing little resemblance to Irn-Bru in flavour, colour or packaging, is produced by Coca-Cola in South Africa.

In Australia, Irn-Bru was manufactured and distributed under licence by Occasio Australia Pty Ltd until 2009. It was available in 500 ml and 1.25 litre in both standard and diet. The drink enjoyed growing success in the country, with its first advertising campaign launched in Queensland in September 2007. It was initially available in major chains such as Coles and Woolworths, Caltex service stations and in many independent grocers and convenience stores. It was then delisted at Coles supermarkets. Because of manufacturing and bottling issues, Occasio ceased local production in late 2009. It is now imported direct from the UK and distributed by British Provender Pty Ltd, and can again be found in the international sections of major supermarket chains and some convenience stores.

Irn-Bru sold in Canada contained no caffeine until recently. In March 2010, Health Canada repealed the ban on caffeine on clear coloured soft drinks and now bottles of Irn-Bru have the label 'Now Contains caffeine' on the packaging. Irn-Bru in Canada is distributed by TFB & Associates Ltd from Markham, Ontario but is packaged by A.G. Barr in Glasgow, Scotland. Irn-Bru can be found at Sobeys supermarkets.

The now-defunct McKinlay soft-drink company in Glace Bay, Cape Breton, Nova Scotia, Canada, for many years offered its own non-licensed beverage called "Irn-Bru" and later "Cape Breton's Irn-Bru". It was a brown carbonated soft-drink with a fruity cola taste.

The standard Irn-Bru distributed in Canada also contains the "Not a source of iron" disclaimer on the label.
The UK version of the drink (with caffeine) is commonly imported by speciality retailers, particularly in areas with large Scottish populations.

Irn-Bru started being sold at 7-Eleven. It has often appeared in the Danish supermarket 'Netto', 'Rema 1000' and 'Normal'. Today only a few 7-Elevens in Denmark continue distributing Irn-Bru, while most Føtex and Bilka stores now stock Irn-Bru.

Imported Irn-Bru cans are found throughout Finland in most K-Citymarket locations and some independent stores.

In Hong Kong, Irn-Bru can be found in selected Wellcome supermarkets, in and around areas where the expatriate population is significant such as the Sheung Wan and Central districts.

A.G. Barr has launched its Irn-Bru product throughout the Middle East. Found mostly in LuLu supermarkets.

Irn-Bru is commonly available nationwide from supermarkets as cans and 1.25 litre plastic bottles. It is bottled by Oasis, the same company that bottles Coca-Cola. Imported Irn-Bru from Scotland is available from speciality stores.

Irn-Bru entered the Norwegian market in May 2008. They had to withdraw from the market again in 2009 as a result of problems with production agreements and lack of funding for marketing.

They were believed to be sponsoring the Norwegian First Division club Mjøndalen IF in 2009. This later turned out to be fraud carried out by a third party company, and Mjøndalen IF never received any sponsorship from Irn-Bru, even though the team played the 2009 season with Irn-Bru logo on their shirts.

Irn-Bru began being sold in Russia in 1997, and by 2002 it had become their third best selling soft drink. After its original bottler went out of business, a new deal was signed for the drink to be manufactured and distributed in larger quantities by the Pepsi Bottling Group of Russia in 2002. Its popularity has been attributed to the drink's apparent similarity to discontinued Soviet-era soft drinks. As of 2011, Irn-Bru sales in Russia were still growing.

Irn-Bru has been distributed in Spain since the early 1980s servicing primarily the large British communities residing in Spain. It can be found in key tourist areas such as the Balearic Islands, the Spanish coastal region and Canary Islands with both the regular and sugar-free variant available. Outside of the United Kingdom, Spain is among the top 10 Irn-Bru markets.

Irn-Bru and Diet Irn-Bru have been formulated since 2002 by A.G. Barr plc to meet the regulations for food colouring of the US Food and Drug Administration (FDA). Ponceau 4R, used in the UK formulation, is prohibited by the FDA. Barr uses alternative food and drink colourants manufactured by a US company approved by the FDA. The product labelling also meets US labelling standards on nutritional information and bar code. Compliant Irn-Bru is solely imported by Great Scot International in Charlotte, North Carolina, who supplies distributors and retailers throughout the US. It is supplied in 500 ml PTE bottles and 4 pack 330ml cans.





</doc>
<doc id="14742" url="https://en.wikipedia.org/wiki?curid=14742" title="Internet Standard">
Internet Standard

In computer network engineering, an Internet Standard is a normative specification of a technology or methodology applicable to the Internet. Internet Standards are created and published by the Internet Engineering Task Force (IETF).

Engineering contributions to the IETF start as an Internet Draft, may be promoted to a Request for Comments, and may eventually become an Internet Standard.

An Internet Standard is characterized by technical maturity and usefulness. The IETF also defines a Proposed Standard as a less mature but stable and well-reviewed specification. A Draft Standard is a third classification that was discontinued in 2011. A Draft Standard was an intermediary step that occurred after a Proposed Standard but prior to an Internet Standard. 

As put in RFC 2026: In general, an Internet Standard is a specification that is stable and well-understood, is technically competent, has multiple, independent, and interoperable implementations with substantial operational experience, enjoys significant public support, and is recognizably useful in some or all parts of the Internet.

An Internet Standard is documented by a Request for Comments (RFC) or a set of RFCs. A specification that is to become a Standard or part of a Standard begins as an Internet Draft, and is later, usually after several revisions, accepted and published by the RFC Editor as an RFC and labeled a "Proposed Standard". Later, an RFC is elevated as "Internet Standard", with an additional sequence number, when maturity has reached an acceptable level. Collectively, these stages are known as the "Standards Track", and are defined in RFC 2026 and RFC 6410. The label "Historic" is applied to deprecated Standards Track documents or obsolete RFCs that were published before the Standards Track was established.

Only the IETF, represented by the Internet Engineering Steering Group (IESG), can approve Standards Track RFCs. The definitive list of Internet Standards is maintained in the Official Internet Protocol Standards. Previously, STD 1 used to maintain a snapshot of the list.

Becoming a standard is a two-step process within the Internet Standards Process: "Proposed Standard" and "Internet Standard". These are called "maturity levels" and the process is called the "Standards Track".

If an RFC is part of a proposal that is on the Standards Track, then at the first stage, the standard is proposed and subsequently organizations decide whether to implement this Proposed Standard. After the criteria in RFC 6410 is met (two separate implementations, widespread use, no errata etc.), the RFC can advance to Internet Standard.

The Internet Standards Process is defined in several "Best Current Practice" documents, notably BCP 9 ( RFC 2026 and RFC 6410). There were previously three standard maturity levels: "Proposed Standard", "Draft Standard" and "Internet Standard". RFC 6410 reduced this to two maturity levels.

RFC 2026 originally characterized Proposed Standards as immature specifications, but this stance was annulled by RFC 7127.

A "Proposed Standard" specification is stable, has resolved known design choices, has received significant community review, and appears to enjoy enough community interest to be considered valuable. Usually, neither implementation nor operational experience is required for the designation of a specification as a Proposed Standard.

Proposed Standards are of such quality that implementations can be deployed in the Internet. However, as with all technical specifications, Proposed Standards may be revised if problems are found or better solutions are identified, when experiences with deploying implementations of such technologies at scale is gathered.

Many Proposed Standards are actually deployed on the Internet and used extensively, as stable protocols. Actual practice has been that full progression through the sequence of standards levels is typically quite rare, and most popular IETF protocols remain at Proposed Standard.

In October 2011, RFC 6410 merged the second and third maturity levels into one Draft Standard. Existing older "Draft Standards" retain that classification. The IESG can reclassify an old "Draft Standard" as "Proposed Standard" after two years (October 2013).

An Internet Standard is characterized by a high degree of technical maturity and by a generally held belief that the specified protocol or service provides significant benefit to the Internet community. Generally Internet Standards cover interoperability of systems on the Internet through defining protocols, message formats, schemas, and languages. The most fundamental of the Internet Standards are the ones defining the Internet Protocol.

An Internet Standard ensures that hardware and software produced by different vendors can work together. Having a standard makes it much easier to develop software and hardware that link different networks because software and hardware can be developed one layer at a time. Normally, the standards used in data communication are called protocols.

All Internet Standards are given a number in the STD series. The series was summarized in its first document, STD 1 (RFC 5000), until 2013, but this practice was retired in RFC 7100. The definitive list of Internet Standards is now maintained by the RFC Editor.

Documents submitted to the IETF editor and accepted as an RFC are not revised; if the document has to be changed, it is submitted again and assigned a new RFC number. When an RFC becomes an Internet Standard (STD), it is assigned an STD number but retains its RFC number. When an Internet Standard is updated, its number is unchanged but refers to a different RFC or set of RFCs. For example, in 2007 RFC 3700 was an Internet Standard (STD 1) and in May 2008 it was replaced with RFC 5000. RFC 3700 received "Historic" status, and RFC 5000 became STD 1.

The list of Internet standards was originally published as STD 1 but this practice has been abandoned in favor of an online list maintained by the RFC Editor.




</doc>
<doc id="14743" url="https://en.wikipedia.org/wiki?curid=14743" title="ISOC">
ISOC

ISOC is an abbreviation which may refer to:



</doc>
<doc id="14744" url="https://en.wikipedia.org/wiki?curid=14744" title="ITU-T">
ITU-T

The ITU Telecommunication Standardization Sector (ITU-T) coordinates standards for telecommunications and Information Communication Technology such as X.509 for cybersecurity, Y.3172 for machine learning, and H.264/MPEG-4 AVC for video compression, between its Member States, Private Sector Members, and Academia Members. ITU-T is one of the three Sectors (divisions or units) of the International Telecommunication Union (ITU).

The standardization efforts of ITU started in 1865 with the formation of the International Telegraph Union (ITU). ITU became a Specialized agency of the United Nations in 1947. The International Telegraph and Telephone Consultative Committee (, CCITT) was created in 1956, and was renamed ITU-T in 1993.

ITU-T has a permanent secretariat, the Telecommunication Standardization Bureau (TSB), based at the ITU headquarters in Geneva, Switzerland. The current Director of the Bureau is Chaesub Lee, whose first 4-year term commenced on 1 January 2015, and whose second 4-year term commenced on 1 January 2019. Chaesub Lee succeeded Malcolm Johnson of the United Kingdom, who was director from 1 January 2007 until 31 December 2014.

The ITU-T mission is to ensure the efficient and timely production of standards covering all fields of telecommunications and Information Communication Technology (ICTs) on a worldwide basis, as well as defining tariff and accounting principles for international telecommunication services.

The international standards that are produced by the ITU-T are referred to as ""Recommendations"" (with the word capitalized to distinguish its meaning from the common parlance sense of the word "recommendation"), as they become mandatory only when adopted as part of a national law.

Since the ITU-T is part of the ITU, which is a United Nations specialized agency, its standards carry more formal international weight than those of most other standards development organizations that publish technical specifications of a similar form.

At the initiative of Napoleon III, the French government invited international participants to a conference in Paris in 1865 to facilitate and regulate international telegraph services. A result of the conference was the founding of the forerunner of the modern ITU.

At the 1925 Paris conference, the ITU created two consultative committees to deal with the complexities of the international telephone services, known as CCIF, as the French acronym, and with long-distance telegraphy (CCIT).

In view of the basic similarity of many of the technical problems faced by the CCIF and CCIT, a decision was taken in 1956 to merge them into a single entity, the International Telegraph and Telephone Consultative Committee (CCITT, in the French acronym). The first Plenary Assembly of the new organization was held in Geneva, Switzerland in December 1956.

In 1992, the Plenipotentiary Conference (the top policy-making conference of ITU) saw a reform of ITU, giving the Union greater flexibility to adapt to an increasingly complex, interactive and competitive environment. The CCITT was renamed the Telecommunication Standardization Sector (ITU-T), as one of three Sectors of the Union alongside the Radiocommunication Sector (ITU-R) and the Telecommunication Development Sector (ITU-D).

Historically, the Recommendations of the CCITT were presented at plenary assemblies for endorsement, held every four years, and the full set of Recommendations were published after each plenary assembly. However, the delays in producing texts, and translating them into other working languages, did not suit the fast pace of change in the telecommunications industry.

The rise of the personal computer industry in the early 1980s created a new common practice among both consumers and businesses of adopting "bleeding edge" communications technology even if it was not yet standardized. Thus, standards organizations had to put forth standards much faster, or find themselves ratifying de facto standards after the fact. One of the most prominent examples of this was the Open Document Architecture project, which began in 1985 when a profusion of software firms around the world were still furiously competing to shape the future of the electronic office, and was completed in 1999 long after Microsoft Office's then-secret binary file formats had become established as the global de-facto standard.

The ITU-T now operates under much more streamlined processes. The time between an initial proposal of a draft document by a member company and the final approval of a full-status ITU-T Recommendation can now be as short as a few months (or less in some cases). This makes the standardization approval process in the ITU-T much more responsive to the needs of rapid technology development than in the ITU's historical past. New and updated Recommendations are published on an almost daily basis, and much of the library of over 3,270 Recommendations is now free of charge online. (Specifications jointly maintained by the ITU-T and ISO/IEC are not free.)

ITU-T has moreover tried to facilitate cooperation between the various forums and standard-developing organizations (SDOs). This collaboration is necessary to avoid duplication of work and the consequent risk of conflicting standards in the market place.

In the work of standardization, ITU-T cooperates with other SDOs, e.g., the International Organization for Standardization (ISO) and the Internet Engineering Task Force (IETF).

Most of the work of ITU-T is carried out by its Sector Members and Associates, while the Telecommunication Standardization Bureau (TSB) is the executive arm of ITU-T and coordinator for a number of workshops and seminars to progress existing work areas and explore new ones. The events cover a wide array of topics in the field of information and communication technologies (ICT) and attract high-ranking experts as speakers, and attendees from engineers to high-level management from all industry sectors.

The technical work, the development of Recommendations, of ITU-T is managed by Study Groups (SGs), such as Study Group 13 for network standards, Study Group 16 for multimedia standards, and Study Group 17 for security standards, which are created by the World Telecommunication Standardization Assembly (WTSA) which is held every four years. The people involved in these SGs are experts in telecommunications from all over the world. There are currently 11 SGs. Study groups meet face to face according to a calendar issued by the TSB. SGs are augmented by Focus Groups (FGs), an instrument created by ITU-T, providing a way to quickly react to ICT standardization needs and allowing great flexibility in terms of participation and working methods. The key difference between SGs and FGs is that the latter have greater freedom to organize and finance themselves, and to involve non-members in their work. Focus Groups can be created very quickly, are usually short-lived and can choose their own working methods, leadership, financing, and types of deliverables. Current Focus Groups include the ITU-WHO Focus Group on Artificial Intelligence for Health (FG-AI4H) as well as Machine Learning for 5G (which developed Y.3172), Quantum Information Technologies for Networks, and Artificial Intelligence for Assisted and Autonomous Driving.

The “Alternative Approval Process” (AAP) is a fast-track approval procedure that was developed to allow standards to be brought to market in the timeframe that industry now demands.

This dramatic overhaul of standards-making by streamlining approval procedures was implemented in 2001 and is estimated to have cut the time involved in this critical aspect of the standardization process by 80 to 90 per cent. This means that an average standard which took around four years to approve and publish until the mid nineties, and two years until 1997, can now be approved in an average of two months, or as little as five weeks.

Besides streamlining the underlying procedures involved in the approval process, an important contributory factor to the use of AAP is electronic document handling. Once the approval process has begun the rest of the process can be completed electronically, in the vast majority of cases, with no further physical meetings.

The introduction of AAP also formalizes public/private partnership in the approval process by providing equal opportunities for both Sector Members and Member States in the approval of technical standards.

A panel of SG experts drafts a proposal that is then forwarded at a SG meeting to the appropriate body which decides if it is sufficiently ready to be designated a draft text and thus gives its consent for further review at the next level.

After this Consent has been given, TSB announces the start of the AAP procedure by posting the draft text to the ITU-T web site and calling for comments. This gives the opportunity for all members to review the text. This phase, called Last Call, is a four-week period in which comments can be submitted by Member States and Sector Members.

If no comments other than editorial corrections are received, the Recommendation is considered approved since no issues were identified that might need any further work. However, if there are any comments, the SG chairman, in consultation with TSB, sets up a comment resolution process by the concerned experts. The revised text is then posted on the web for an Additional Review period of three weeks.

Similar to the Last Call phase, in Additional Review the Recommendation is considered as approved if no comments are received. If comments are received, it is apparent that there are some issues that still need more work, and the draft text and all comments are sent to the next Study Group meeting for further discussion and possible approval.

Those Recommendations considered as having policy or regulatory implications are approved through what is known as the “Traditional Approval Process” (TAP), which allows a longer period for reflection and commenting by Member States. TAP Recommendations are also translated into the six working languages of ITU (Arabic, Chinese, English, French, Russian, and Spanish).

ITU-T Recommendations are the names given to telecommunications and computer protocol specification documents published by ITU-T.

ITU-T issues Recommendations that have names like X.500, where X is the series and 500 is an identifying number. When a Recommendation is updated, it will (mostly) keep the same number, so the year of issue may be necessary to identify a specific version of a Recommendation. The term "X.500" is used both to refer to the specific X.500 Recommendation, and to the entire family of Recommendations named X.5xx, where the specific X.500 Recommendation forms the introduction and overview of the set.
The vast majority of all finalized Recommendations are available in electronic (PDF) form free of charge to all. Texts that are not free of charge include common ITU-T | ISO / IEC texts for which special arrangements exist.

In addition to the ITU-T Recommendations, which have non-mandatory status until they are adopted in national laws, ITU-T is also the custodian of a binding international treaty, the International Telecommunication Regulations. The ITRs go back to the earliest days of the ITU when there were two separate treaties, dealing with telegraph and telephone. The ITRs were adopted, as a single treaty, at the World Administrative Telegraphy and Telephone Conference held in Melbourne, 1988 (WATTC-88).

The ITRs comprise ten articles which deal, inter alia, with the definition of international telecommunication services, cooperation between countries and national administrations, safety of life and priority of telecommunications and charging and accounting principles. The adoption of the ITRs in 1988 is often taken as the start of the wider liberalization process in international telecommunications, though a few countries, including United States and United Kingdom, had made steps to liberalize their markets before 1988.

The Constitution and Convention of ITU provides for the amendment of ITRs through a World Conference on International Telecommunications (WCIT). Accordingly, in 1998 there began a process of review of the ITRs; and in 2009 extensive preparations began for such a conference, WCIT-12. In addition to "regional preparatory meetings," the ITU Secretariat developed 13 "Background Briefs on key issues" that were expected to be discussed at the conference. Convened by former ITU secretary-general Hamadoun Touré, the Conference, WCIT-12, was then held in Dubai, United Arab Emirates, during the period 3–14 December 2014.

The Standardization Sector of ITU also organizes AI for Good, the United Nations platform for the sustainable development of Artificial Intelligence.






</doc>
<doc id="14745" url="https://en.wikipedia.org/wiki?curid=14745" title="Indian">
Indian

Indian or Indians refers to people or things related to India, or to the indigenous people of the Americas, or Aboriginal Australians until the 19th century. 















</doc>
<doc id="14746" url="https://en.wikipedia.org/wiki?curid=14746" title="Internalization">
Internalization

Internalization (or internalisation) is the process of making something internal, with more specific meanings in various fields. It is the opposite of externalization.

In psychology, internalization is the outcome of a conscious mind reasoning about a specific subject; the subject is internalized, and the consideration of the subject is internal. Internalization of ideals might take place following religious conversion, or in the process of, more generally, moral conversion. Internalization is directly associated with learning within an organism (or business) and recalling what has been learned.

In psychology and sociology, internalization involves the integration of attitudes, values, standards and the opinions of others into one's own identity or sense of self. In psychoanalytic theory, internalization is a process involving the formation of the super ego. Many theorists believe that the internalized values of behavior implemented during early socialization are key factors in predicting a child's future moral character. The self-determination theory proposes a motivational continuum from the extrinsic to intrinsic motivation and autonomous self-regulation. Some research suggests a child's moral self starts to develop around age three. These early years of socialization may be the underpinnings of moral development in later childhood. Proponents of this theory suggest that children whose view of self is "good and moral" tend to have a developmental trajectory toward pro-social behavior and few signs of anti-social behavior.

In one child developmental study, researchers examined two key dimensions of early conscience – internalization of rules of conduct and empathic affects to others – as factors that may predict future social, adaptive and competent behavior. Data was collected from a longitudinal study of children, from two parent families, at age 25, 38, 52, 67 and 80 months. Children's internalization of each parent's rules and empathy toward each parent's simulated distress were observed at 25, 38 and 52 months. Parents and teachers rated their adaptive, competent, pro-social behavior and anti-social behavior at 80 months. The researchers found that first, both the history of the child's early internalization of parental rules and the history of their empathy predicted the children's competent and adaptive functioning at 80 months, as rated by parents and teachers. Second, children with stronger histories of internalization of parental rules from 25 to 52 months perceived themselves as more moral at 67 months. Third, the children that showed stronger internalization from 25 to 52 months came to see themselves as more moral and "good". These self-perceptions, in turn, predicted the way parents and teachers would rate their competent and adaptive functioning at 80 months.

In behavioral psychology, the concept of internalization may also refer to disorders and behaviors in which a person deals with stressors in manners not externally evident. Such disorders and behaviors include depression, anxiety disorder, bulimia and anorexia.

In sciences such as biology, internalization is another term for endocytosis, in which molecules such as proteins are engulfed by the cell membrane and drawn into the cell.

In economics, internalization theory explains the practice of multinational enterprises (MNEs) to execute transactions within their organization rather than relying on an outside market. It must be cheaper for an MNE to internalize the transfer of its unique ownership advantages between countries than to do so through markets. In other words, the alternative to internalization through direct investment is some form of licensing of the firm's know-how to a firm in the target economy.

In finance, internalization can refer to several concepts. "When you place an order to buy or sell a stock, your broker has choices on where to execute your order. Instead of routing your order to a market or market-makers for execution, your broker may fill the order from the firm's own inventory – this is called 'internalization'. In this way, your broker's firm may make money on the "spread" – which is the difference between the purchase price and the sale price." For a related issue regarding trade execution, see payment for order flow.




</doc>
<doc id="14747" url="https://en.wikipedia.org/wiki?curid=14747" title="Ionic">
Ionic

Ionic or Ionian may refer to:







</doc>
<doc id="14749" url="https://en.wikipedia.org/wiki?curid=14749" title="Indium">
Indium

Indium is a chemical element with the symbol In and atomic number 49. Indium is the softest metal that is not an alkali metal. It is a silvery-white metal that resembles tin in appearance. It is a post-transition metal that makes up 0.21 parts per million of the Earth's crust. Indium has a melting point higher than sodium and gallium, but lower than lithium and tin. Chemically, indium is similar to gallium and thallium, and it is largely intermediate between the two in terms of its properties. Indium was discovered in 1863 by Ferdinand Reich and Hieronymous Theodor Richter by spectroscopic methods. They named it for the indigo blue line in its spectrum. Indium was isolated the next year.

Indium is a minor component in zinc sulfide ores and is produced as a byproduct of zinc refinement. It is most notably used in the semiconductor industry, in low-melting-point metal alloys such as solders, in soft-metal high-vacuum seals, and in the production of transparent conductive coatings of indium tin oxide (ITO) on glass. Indium is considered a technology-critical element.

Indium has no biological role, though its compounds are toxic when injected into the bloodstream. Most occupational exposure is through ingestion, from which indium compounds are not absorbed well, and inhalation, from which they are moderately absorbed.

Indium is a silvery-white, highly ductile post-transition metal with a bright luster. It is so soft (Mohs hardness 1.2) that like sodium, it can be cut with a knife. It also leaves a visible line on paper. It is a member of group 13 on the periodic table and its properties are mostly intermediate between its vertical neighbours gallium and thallium. Like tin, a high-pitched cry is heard when indium is bent – a crackling sound due to crystal twinning. Like gallium, indium is able to wet glass. Like both, indium has a low melting point, 156.60 °C (313.88 °F); higher than its lighter homologue, gallium, but lower than its heavier homologue, thallium, and lower than tin. The boiling point is 2072 °C (3762 °F), higher than that of thallium, but lower than gallium, conversely to the general trend of melting points, but similarly to the trends down the other post-transition metal groups because of the weakness of the metallic bonding with few electrons delocalized.

The density of indium, 7.31 g/cm, is also greater than gallium, but lower than thallium. Below the critical temperature, 3.41 K, indium becomes a superconductor. Indium crystallizes in the body-centered tetragonal crystal system in the space group "I"4/"mmm" (lattice parameters: "a" = 325 pm, "c" = 495 pm): this is a slightly distorted face-centered cubic structure, where each indium atom has four neighbours at 324 pm distance and eight neighbours slightly further (336 pm). Indium has greater solubility in liquid mercury than any other metal (more than 50 mass percent of indium at 0 °C). Indium displays a ductile viscoplastic response, found to be size-independent in tension and compression. However it does have a size effect in bending and indentation, associated to a length-scale of order 50–100 µm, significantly large when compared with other metals.

Indium has 49 electrons, with an electronic configuration of [Kr]4d5s5p. In compounds, indium most commonly donates the three outermost electrons to become indium(III), In. In some cases, the pair of 5s-electrons are not donated, resulting in indium(I), In. The stabilization of the monovalent state is attributed to the inert pair effect, in which relativistic effects stabilize the 5s-orbital, observed in heavier elements. Thallium (indium's heavier homolog) shows an even stronger effect, causing oxidation to thallium(I) to be more probable than to thallium(III), whereas gallium (indium's lighter homolog) commonly shows only the +3 oxidation state. Thus, although thallium(III) is a moderately strong oxidizing agent, indium(III) is not, and many indium(I) compounds are powerful reducing agents. While the energy required to include the s-electrons in chemical bonding is lowest for indium among the group 13 metals, bond energies decrease down the group so that by indium, the energy released in forming two additional bonds and attaining the +3 state is not always enough to outweigh the energy needed to involve the 5s-electrons. Indium(I) oxide and hydroxide are more basic and indium(III) oxide and hydroxide are more acidic.

A number of standard electrode potentials, depending on the reaction under study, are reported for indium, reflecting the decreased stability of the +3 oxidation state:

Indium metal does not react with water, but it is oxidized by stronger oxidizing agents such as halogens to give indium(III) compounds. It does not form a boride, silicide, or carbide, and the hydride InH has at best a transitory existence in ethereal solutions at low temperatures, being unstable enough to spontaneously polymerize without coordination. Indium is rather basic in aqueous solution, showing only slight amphoteric characteristics, and unlike its lighter homologs aluminium and gallium, it is insoluble in aqueous alkaline solutions.

Indium has 39 known isotopes, ranging in mass number from 97 to 135. Only two isotopes occur naturally as primordial nuclides: indium-113, the only stable isotope, and indium-115, which has a half-life of 4.41 years, four orders of magnitude greater than the age of the Universe and nearly 30,000 times greater than that of natural thorium. The half-life of In is very long because the beta decay to Sn is spin-forbidden. Indium-115 makes up 95.7% of all indium. Indium is one of three known elements (the others being tellurium and rhenium) of which the stable isotope is less abundant in nature than the long-lived primordial radioisotopes.

The stablest artificial isotope is indium-111, with a half-life of approximately 2.8 days. All other isotopes have half-lives shorter than 5 hours. Indium also has 47 meta states, among which indium-114m1 (half-life about 49.51 days) is the most stable, more stable than the ground state of any indium isotope other than the primordial. All decay by isomeric transition. The indium isotopes lighter than In predominantly decay through electron capture or positron emission to form cadmium isotopes, while the other indium isotopes from In and greater predominantly decay through beta-minus decay to form tin isotopes.

Indium(III) oxide, InO, forms when indium metal is burned in air or when the hydroxide or nitrate is heated. InO adopts a structure like alumina and is amphoteric, that is able to react with both acids and bases. Indium reacts with water to reproduce soluble indium(III) hydroxide, which is also amphoteric; with alkalis to produce indates(III); and with acids to produce indium(III) salts:

The analogous sesquichalcogenides with sulfur, selenium, and tellurium are also known. Indium forms the expected trihalides. Chlorination, bromination, and iodination of In produce colorless InCl, InBr, and yellow InI. The compounds are Lewis acids, somewhat akin to the better known aluminium trihalides. Again like the related aluminium compound, InF is polymeric.

Direct reaction of indium with the pnictogens produces the gray or semimetallic III–V semiconductors. Many of them slowly decompose in moist air, necessitating careful storage of semiconductor compounds to prevent contact with the atmosphere. Indium nitride is readily attacked by acids and alkalis.

Indium(I) compounds are not common. The chloride, bromide, and iodide are deeply colored, unlike the parent trihalides from which they are prepared. The fluoride is known only as an unstable gaseous compound. Indium(I) oxide black powder is produced when indium(III) oxide decomposes upon heating to 700 °C.

Less frequently, indium forms compounds in oxidation state +2 and even fractional oxidation states. Usually such materials feature In–In bonding, most notably in the halides InX and [InX], and various subchalcogenides such as InSe. Several other compounds are known to combine indium(I) and indium(III), such as In(InCl)Cl, In(InBr)(InBr), InInBr.

Organoindium compounds feature In–C bonds. Most are In(III) derivatives, but cyclopentadienylindium(I) is an exception. It was the first known organoindium(I) compound, and is polymeric, consisting of zigzag chains of alternating indium atoms and cyclopentadienyl complexes. Perhaps the best-known organoindium compound is trimethylindium, In(CH), used to prepare certain semiconducting materials.

In 1863, the German chemists Ferdinand Reich and Hieronymous Theodor Richter were testing ores from the mines around Freiberg, Saxony. They dissolved the minerals pyrite, arsenopyrite, galena and sphalerite in hydrochloric acid and distilled raw zinc chloride. Reich, who was color-blind, employed Richter as an assistant for detecting the colored spectral lines. Knowing that ores from that region sometimes contain thallium, they searched for the green thallium emission spectrum lines. Instead, they found a bright blue line. Because that blue line did not match any known element, they hypothesized a new element was present in the minerals. They named the element indium, from the indigo color seen in its spectrum, after the Latin "indicum", meaning 'of India'.

Richter went on to isolate the metal in 1864. An ingot of was presented at the World Fair 1867. Reich and Richter later fell out when the latter claimed to be the sole discoverer.

Indium is created by the long-lasting (up to thousands of years) s-process (slow neutron capture) in low-to-medium-mass stars (range in mass between 0.6 and 10 solar masses). When a silver-109 atom captures a neutron, it transmutes into silver-110, which then undergoes beta decay to become cadmium-110. Capturing further neutrons, it becomes cadmium-115, which decays to indium-115 by another beta decay. This explains why the radioactive isotope is more abundant than the stable one. The stable indium isotope, indium-113, is one of the p-nuclei, the origin of which is not fully understood; although indium-113 is known to be made directly in the s- and r-processes (rapid neutron capture), and also as the daughter of very long-lived cadmium-113, which has a half-life of about eight quadrillion years, this cannot account for all indium-113.

Indium is the 68th most abundant element in Earth's crust at approximately 50 ppb. This is similar to the crustal abundance of silver, bismuth and mercury. It very rarely forms its own minerals, or occurs in elemental form. Fewer than 10 indium minerals such as roquesite (CuInS) are known, and none occur at sufficient concentrations for economic extraction. Instead, indium is usually a trace constituent of more common ore minerals, such as sphalerite and chalcopyrite. From these, it can be extracted as a by-product during smelting. While the enrichment of indium in these deposits is high relative to its crustal abundance, it is insufficient, at current prices, to support extraction of indium as the main product. 

Different estimates exist of the amounts of indium contained within the ores of other metals. However, these amounts are not extractable without mining of the host materials (see Production and availability). Thus, the availability of indium is fundamentally determined by the "rate" at which these ores are extracted, and not their absolute amount. This is an aspect that is often forgotten in the current debate, e.g. by the Graedel group at Yale in their criticality assessments, explaining the paradoxically low depletion times some studies cite.

Indium is produced exclusively as a by-product during the processing of the ores of other metals. Its main source material are sulfidic zinc ores, where it is mostly hosted by sphalerite. Minor amounts are probably also extracted from sulfidic copper ores. During the roast-leach-electrowinning process of zinc smelting, indium accumulates in the iron-rich residues. From these, it can be extracted in different ways. It may also be recovered directly from the process solutions. Further purification is done by electrolysis. The exact process varies with the mode of operation of the smelter.

Its by-product status means that indium production is constrained by the amount of sulfidic zinc (and copper) ores extracted each year. Therefore, its availability needs to be discussed in terms of supply potential. The supply potential of a by-product is defined as that amount which is economically extractable from its host materials "per year" under current market conditions (i.e. technology and price). Reserves and resources are not relevant for by-products, since they "cannot" be extracted independently from the main-products. Recent estimates put the supply potential of indium at a minimum of 1,300 t/yr from sulfidic zinc ores and 20 t/yr from sulfidic copper ores. These figures are significantly greater than current production (655 t in 2016). Thus, major future increases in the by-product production of indium will be possible without significant increases in production costs or price. The average indium price in 2016 was 240/kg, down from 705/kg in 2014.

China is a leading producer of indium (290 tonnes in 2016), followed by South Korea (195 t), Japan (70 t) and Canada (65 t). The Teck Resources refinery in Trail, British Columbia, is a large single-source indium producer, with an output of 32.5 tonnes in 2005, 41.8 tonnes in 2004 and 36.1 tonnes in 2003.

The primary consumption of indium worldwide is LCD production. Demand rose rapidly from the late 1990s to 2010 with the popularity of LCD computer monitors and television sets, which now account for 50% of indium consumption. Increased manufacturing efficiency and recycling (especially in Japan) maintain a balance between demand and supply. According to the UNEP, indium's end-of-life recycling rate is less than 1%.

In 1924, indium was found to have a valued property of stabilizing non-ferrous metals, and that became the first significant use for the element. The first large-scale application for indium was coating bearings in high-performance aircraft engines during World War II, to protect against damage and corrosion; this is no longer a major use of the element. New uses were found in fusible alloys, solders, and electronics. In the 1950s, tiny beads of indium were used for the emitters and collectors of PNP alloy-junction transistors. In the middle and late 1980s, the development of indium phosphide semiconductors and indium tin oxide thin films for liquid-crystal displays (LCD) aroused much interest. By 1992, the thin-film application had become the largest end use.

Indium(III) oxide and indium tin oxide (ITO) are used as a transparent conductive coating on glass substrates in electroluminescent panels. Indium tin oxide is used as a light filter in low-pressure sodium-vapor lamps. The infrared radiation is reflected back into the lamp, which increases the temperature within the tube and improves the performance of the lamp.

Indium has many semiconductor-related applications. Some indium compounds, such as indium antimonide and indium phosphide, are semiconductors with useful properties: one precursor is usually trimethylindium (TMI), which is also used as the semiconductor dopant in II–VI compound semiconductors. InAs and InSb are used for low-temperature transistors and InP for high-temperature transistors. The compound semiconductors InGaN and InGaP are used in light-emitting diodes (LEDs) and laser diodes. Indium is used in photovoltaics as the semiconductor copper indium gallium selenide (CIGS), also called CIGS solar cells, a type of second-generation thin-film solar cell. Indium is used in PNP bipolar junction transistors with germanium: when soldered at low temperature, indium does not stress the germanium.

Indium wire is used as a vacuum seal and a thermal conductor in cryogenics and ultra-high-vacuum applications, in such manufacturing applications as gaskets that deform to fill gaps. Indium is an ingredient in the gallium–indium–tin alloy galinstan, which is liquid at room temperature and replaces mercury in some thermometers. Other alloys of indium with bismuth, cadmium, lead, and tin, which have higher but still low melting points (between 50 and 100 °C), are used in fire sprinkler systems and heat regulators.

Indium is one of many substitutes for mercury in alkaline batteries to prevent the zinc from corroding and releasing hydrogen gas. Indium is added to some dental amalgam alloys to decrease the surface tension of the mercury and allow for less mercury and easier amalgamation.

Indium's high neutron-capture cross-section for thermal neutrons makes it suitable for use in control rods for nuclear reactors, typically in an alloy of 80% silver, 15% indium, and 5% cadmium. In nuclear engineering, the (n,n') reactions of In and In are used to determine magnitudes of neutron fluxes.

In 2009, Professor Mas Subramanian and associates at Oregon State University discovered that indium can be combined with yttrium and manganese to form an intensely blue, non-toxic, inert, fade-resistant pigment, YInMn blue, the first new blue pigment discovered in 200 years.

Indium has no metabolic role in any organism. In a similar way to aluminium salts, indium(III) ions can be toxic to the kidney when given by injection. Indium tin oxide and indium phosphide harm the pulmonary and immune systems, predominantly through ionic indium, though hydrated indium oxide is more than forty times as toxic when injected, measured by the quantity of indium introduced. Radioactive indium-111 (in very small amounts on a chemical basis) is used in nuclear medicine tests, as a radiotracer to follow the movement of labeled proteins and white blood cells in the body. Indium compounds are mostly not absorbed upon ingestion and are only moderately absorbed on inhalation; they tend to be stored temporarily in the muscles, skin, and bones before being excreted, and the biological half-life of indium is about two weeks in humans.

People can be exposed to indium in the workplace by inhalation, ingestion, skin contact, and eye contact. Indium lung is a lung disease characterized by pulmonary alveolar proteinosis and pulmonary fibrosis, first described by Japanese researchers in 2003. , 10 cases had been described, though more than 100 indium workers had documented respiratory abnormalities. The National Institute for Occupational Safety and Health has set a recommended exposure limit (REL) of 0.1 mg/m over an eight-hour workday.



</doc>
<doc id="14750" url="https://en.wikipedia.org/wiki?curid=14750" title="Iodine">
Iodine

Iodine is a chemical element with the symbol I and atomic number 53. The heaviest of the stable halogens, it exists as a lustrous, purple-black non-metallic solid at standard conditions that melts to form a deep violet liquid at 114 degrees Celsius, and boils to a violet gas at 184 degrees Celsius. However, it sublimes easily with gentle heat, resulting in a widespread misconception even taught in some science textbooks that it does not melt. The element was discovered by the French chemist Bernard Courtois in 1811, and was named two years later by Joseph Louis Gay-Lussac, after the Greek "ἰώδης" "violet-coloured".

Iodine occurs in many oxidation states, including iodide (I), iodate (), and the various periodate anions. It is the least abundant of the stable halogens, being the sixty-first most abundant element. It is the heaviest essential mineral nutrient. Iodine is essential in the synthesis of thyroid hormones. Iodine deficiency affects about two billion people and is the leading preventable cause of intellectual disabilities.

The dominant producers of iodine today are Chile and Japan. Iodine and its compounds are primarily used in nutrition. Due to its high atomic number and ease of attachment to organic compounds, it has also found favour as a non-toxic radiocontrast material. Because of the specificity of its uptake by the human body, radioactive isotopes of iodine can also be used to treat thyroid cancer. Iodine is also used as a catalyst in the industrial production of acetic acid and some polymers.

In 1811, iodine was discovered by French chemist Bernard Courtois, who was born to a manufacturer of saltpetre (an essential component of gunpowder). At the time of the Napoleonic Wars, saltpetre was in great demand in France. Saltpetre produced from French nitre beds required sodium carbonate, which could be isolated from seaweed collected on the coasts of Normandy and Brittany. To isolate the sodium carbonate, seaweed was burned and the ash washed with water. The remaining waste was destroyed by adding sulfuric acid. Courtois once added excessive sulfuric acid and a cloud of purple vapour rose. He noted that the vapour crystallised on cold surfaces, making dark crystals. Courtois suspected that this material was a new element but lacked funding to pursue it further.

Courtois gave samples to his friends, Charles Bernard Desormes (1777–1838) and Nicolas Clément (1779–1841), to continue research. He also gave some of the substance to chemist Joseph Louis Gay-Lussac (1778–1850), and to physicist André-Marie Ampère (1775–1836). On 29 November 1813, Desormes and Clément made Courtois' discovery public. They described the substance to a meeting of the Imperial Institute of France. On 6 December, Gay-Lussac announced that the new substance was either an element or a compound of oxygen. It was Gay-Lussac who suggested the name ""iode"", from the Greek word ("ioeidēs") for violet (because of the colour of iodine vapor). Ampère had given some of his sample to English chemist Humphry Davy (1778–1829), who experimented on the substance and noted its similarity to chlorine. Davy sent a letter dated 10 December to the Royal Society of London stating that he had identified a new element. Arguments erupted between Davy and Gay-Lussac over who identified iodine first, but both scientists acknowledged Courtois as the first to isolate the element.

Antonio Grossich (1849–1926), an Istrian-born surgeon, was among the first to use sterilization of the operative field. In 1908, he introduced tincture of iodine as a way for rapid sterilization of the human skin in the surgical field.

In early periodic tables, iodine was often given the symbol "J", for "Jod", its name in German.

Iodine is the fourth halogen, being a member of group 17 in the periodic table, below fluorine, chlorine, and bromine; it is the heaviest stable member of its group (the scarce and fugitive fifth halogen, the radioactive astatine, is not well-studied due to its expense and inaccessibility in large quantities, but appears to show various unusual properties due to relativistic effects). Iodine has an electron configuration of [Kr]4d5s5p, with the seven electrons in the fifth and outermost shell being its valence electrons. Like the other halogens, it is one electron short of a full octet and is hence a strong oxidising agent, reacting with many elements in order to complete its outer shell, although in keeping with periodic trends, it is the weakest oxidising agent among the stable halogens: it has the lowest electronegativity among them, just 2.66 on the Pauling scale (compare fluorine, chlorine, and bromine at 3.98, 3.16, and 2.96 respectively; astatine continues the trend with an electronegativity of 2.2). Elemental iodine hence forms diatomic molecules with chemical formula I, where two iodine atoms share a pair of electrons in order to each achieve a stable octet for themselves; at high temperatures, these diatomic molecules reversibly dissociate a pair of iodine atoms. Similarly, the iodide anion, I, is the strongest reducing agent among the stable halogens, being the most easily oxidised back to diatomic I. (Astatine goes further, being indeed unstable as At and readily oxidised to At or At, although the existence of At is not settled.)

The halogens darken in colour as the group is descended: fluorine is a very pale yellow gas, chlorine is greenish-yellow, and bromine is a reddish-brown volatile liquid. Iodine conforms to the prevailing trend, being a shiny black crystalline solid that melts at 114 °C and boils at 183 °C to form a violet gas. This trend occurs because the wavelengths of visible light absorbed by the halogens increase down the group (though astatine may not conform to it, depending on how metallic it turns out to be). Specifically, the violet colour of iodine gas results from the electron transition between the highest occupied antibonding "π" molecular orbital and the lowest vacant antibonding "σ" molecular orbital.

Elemental iodine is slightly soluble in water, with one gram dissolving in 3450 ml at 20 °C and 1280 ml at 50 °C; potassium iodide may be added to increase solubility via formation of triiodide ions, among other polyiodides. Nonpolar solvents such as hexane and carbon tetrachloride provide a higher solubility. Polar solutions, such as aqueous solutions, are brown, reflecting the role of these solvents as Lewis bases; on the other hand, nonpolar solutions are violet, the color of iodine vapour. Charge-transfer complexes form when iodine is dissolved in polar solvents, hence changing the colour. Iodine is violet when dissolved in carbon tetrachloride and saturated hydrocarbons but deep brown in alcohols and amines, solvents that form charge-transfer adducts.

The melting and boiling points of iodine are the highest among the halogens, conforming to the increasing trend down the group, since iodine has the largest electron cloud among them that is the most easily polarised, resulting in its molecules having the strongest van der Waals interactions among the halogens. Similarly, iodine is the least volatile of the halogens. Because it has the largest atomic radius among the halogens, iodine has the lowest first ionisation energy, lowest electron affinity, lowest electronegativity and lowest reactivity of the halogens.
The interhalogen bond in diiodine is the weakest of all the halogens. As such, 1% of a sample of gaseous iodine at atmospheric pressure is dissociated into iodine atoms at 575 °C. Temperatures greater than 750 °C are required for fluorine, chlorine, and bromine to dissociate to a similar extent. Most bonds to iodine are weaker than the analogous bonds to the lighter halogens. Gaseous iodine is composed of I molecules with an I–I bond length of 266.6 pm. The I–I bond is one of the longest single bonds known. It is even longer (271.5 pm) in solid orthorhombic crystalline iodine, which has the same crystal structure as chlorine and bromine. (The record is held by iodine's neighbour xenon: the Xe–Xe bond length is 308.71 pm.) As such, within the iodine molecule, significant electronic interactions occur with the two next-nearest neighbours of each atom, and these interactions give rise, in bulk iodine, to a shiny appearance and semiconducting properties. Iodine is a two-dimensional semiconductor with a band gap of 1.3 eV (125 kJ/mol): it is a semiconductor in the plane of its crystalline layers and an insulator in the perpendicular direction.

Of the thirty-seven known isotopes of iodine, only one occurs in nature, iodine-127. The others are radioactive and have half-lives too short to be primordial. As such, iodine is both monoisotopic and mononuclidic and its atomic weight is known to great precision, as it is a constant of nature.

The longest-lived of the radioactive isotopes of iodine is iodine-129, which has a half-life of 15.7 million years, decaying via beta decay to stable xenon-129. Some iodine-129 was formed along with iodine-127 before the formation of the Solar System, but it has by now completely decayed away, making it an extinct radionuclide that is nevertheless still useful in dating the history of the early Solar System or very old groundwaters, due to its mobility in the environment. Its former presence may be determined from an excess of its daughter xenon-129. Traces of iodine-129 still exist today, as it is also a cosmogenic nuclide, formed from cosmic ray spallation of atmospheric xenon: these traces make up 10 to 10 of all terrestrial iodine. It also occurs from open-air nuclear testing, and is not hazardous because of its incredibly long half-life, the longest of all fission products. At the peak of thermonuclear testing in the 1960s and 1970s, iodine-129 still made up only about 10 of all terrestrial iodine. Excited states of iodine-127 and iodine-129 are often used in Mössbauer spectroscopy.

The other iodine radioisotopes have much shorter half-lives, no longer than days. Some of them have medical applications involving the thyroid gland, where the iodine that enters the body is stored and concentrated. Iodine-123 has a half-life of thirteen hours and decays by electron capture to tellurium-123, emitting gamma radiation; it is used in nuclear medicine imaging, including single photon emission computed tomography (SPECT) and X-ray computed tomography (X-Ray CT) scans. Iodine-125 has a half-life of fifty-nine days, decaying by electron capture to tellurium-125 and emitting low-energy gamma radiation; the second-longest-lived iodine radioisotope, it has uses in biological assays, nuclear medicine imaging and in radiation therapy as brachytherapy to treat a number of conditions, including prostate cancer, uveal melanomas, and brain tumours. Finally, iodine-131, with a half-life of eight days, beta decays to an excited state of stable xenon-131 that then converts to the ground state by emitting gamma radiation. It is a common fission product and thus is present in high levels in radioactive fallout. It may then be absorbed through contaminated food, and will also accumulate in the thyroid. As it decays, it may cause damage to the thyroid. The primary risk from exposure to high levels of iodine-131 is the chance occurrence of radiogenic thyroid cancer in later life. Other risks include the possibility of non-cancerous growths and thyroiditis.

The usual means of protection against the negative effects of iodine-131 is by saturating the thyroid gland with stable iodine-127 in the form of potassium iodide tablets, taken daily for optimal prophylaxis. However, iodine-131 may also be used for medicinal purposes in radiation therapy for this very reason, when tissue destruction is desired after iodine uptake by the tissue. Iodine-131 is also used as a radioactive tracer.

Though it is the least reactive of the stable halogens, iodine is still one of the more reactive elements. For example, while chlorine gas will halogenate carbon monoxide, nitric oxide, and sulfur dioxide (to phosgene, nitrosyl chloride, and sulfuryl chloride respectively), iodine will not do so. Furthermore, iodination of metals tends to result in lower oxidation states than chlorination or bromination; for example, rhenium metal reacts with chlorine to form rhenium hexachloride, but with bromine it forms only rhenium pentabromide and iodine can achieve only rhenium tetraiodide. By the same token, however, since iodine has the lowest ionisation energy among the halogens and is the most easily oxidised of them, it has a more significant cationic chemistry and its higher oxidation states are rather more stable than those of bromine and chlorine, for example in iodine heptafluoride.

I dissociates in light with an absorbance at 578 nm wavelength.

The iodine molecule, I, dissolves in CCl and aliphatic hydrocarbons to give bright violet solutions. In these solvents the absorption band maximum occurs in the 520 – 540 nm region and is assigned to a to "σ" transition. When I reacts with Lewis bases in these solvents a blue shift in I peak is seen and the new peak (230 – 330 nm) arises that is due to the formation of adducts referred to as charge-transfer complexes. The enthalpies of formation of some Donor-I adducts are listed below. I is a Lewis acid classified as a soft acid and its acceptor properties are discussed in the ECW model. The relative acceptor strength of I toward a series of bases, versus other Lewis acids, can be illustrated by C-B plots. 

The simplest compound of iodine is hydrogen iodide, HI. It is a colourless gas that reacts with oxygen to give water and iodine. Although it is useful in iodination reactions in the laboratory, it does not have large-scale industrial uses, unlike the other hydrogen halides. Commercially, it is usually made by reacting iodine with hydrogen sulfide or hydrazine:
At room temperature, it is a colourless gas, like all of the hydrogen halides except hydrogen fluoride, since hydrogen cannot form strong hydrogen bonds to the large and only mildly electronegative iodine atom. It melts at −51.0 °C and boils at −35.1 °C. It is an endothermic compound that can exothermically dissociate at room temperature, although the process is very slow unless a catalyst is present: the reaction between hydrogen and iodine at room temperature to give hydrogen iodide does not proceed to completion. The H–I bond dissociation energy is likewise the smallest of the hydrogen halides, at 295 kJ/mol.

Aqueous hydrogen iodide is known as hydroiodic acid, which is a strong acid. Hydrogen iodide is exceptionally soluble in water: one litre of water will dissolve 425 litres of hydrogen iodide, and the saturated solution has only four water molecules per molecule of hydrogen iodide. Commercial so-called "concentrated" hydroiodic acid usually contains 48–57% HI by mass; the solution forms an azeotrope with boiling point 126.7 °C at 56.7 g HI per 100 g solution. Hence hydroiodic acid cannot be concentrated past this point by evaporation of water.

Unlike hydrogen fluoride, anhydrous liquid hydrogen iodide is difficult to work with as a solvent, because its boiling point is low, it has a small liquid range, its dielectric constant is low and it does not dissociate appreciably into HI and ions – the latter, in any case, are much less stable than the bifluoride ions () due to the very weak hydrogen bonding between hydrogen and iodine, though its salts with very large and weakly polarising cations such as Cs and (R = Me, Et, Bu) may still be isolated. Anhydrous hydrogen iodide is a poor solvent, able to dissolve only small molecular compounds such as nitrosyl chloride and phenol, or salts with very low lattice energies such as tetraalkylammonium halides.

Nearly all elements in the periodic table form binary iodides. The exceptions are decidedly in the minority and stem in each case from one of three causes: extreme inertness and reluctance to participate in chemical reactions (the noble gases); extreme nuclear instability hampering chemical investigation before decay and transmutation (many of the heaviest elements beyond bismuth); and having an electronegativity higher than iodine's (oxygen, nitrogen, and the first three halogens), so that the resultant binary compounds are formally not iodides but rather oxides, nitrides, or halides of iodine. (Nonetheless, nitrogen triiodide is named as an iodide as it is analogous to the other nitrogen trihalides.)

Given the large size of the iodide anion and iodine's weak oxidising power, high oxidation states are difficult to achieve in binary iodides, the maximum known being in the pentaiodides of niobium, tantalum, and protactinium. Iodides can be made by reaction of an element or its oxide, hydroxide, or carbonate with hydroiodic acid, and then dehydrated by mildly high temperatures combined with either low pressure or anhydrous hydrogen iodide gas. These methods work best when the iodide product is stable to hydrolysis; otherwise, the possibilities include high-temperature oxidative iodination of the element with iodine or hydrogen iodide, high-temperature iodination of a metal oxide or other halide by iodine, a volatile metal halide, carbon tetraiodide, or an organic iodide. For example, molybdenum(IV) oxide reacts with aluminium(III) iodide at 230 °C to give molybdenum(II) iodide. An example involving halogen exchange is given below, involving the reaction of tantalum(V) chloride with excess aluminium(III) iodide at 400 °C to give tantalum(V) iodide:

Lower iodides may be produced either through thermal decomposition or disproportionation, or by reducing the higher iodide with hydrogen or a metal, for example:

Most of the iodides of the pre-transition metals (groups 1, 2, and 3, along with the lanthanides and actinides in the +2 and +3 oxidation states) are mostly ionic, while nonmetals tend to form covalent molecular iodides, as do metals in high oxidation states from +3 and above. Ionic iodides MI tend to have the lowest melting and boiling points among the halides MX of the same element, because the electrostatic forces of attraction between the cations and anions are weakest for the large iodide anion. In contrast, covalent iodides tend to instead have the highest melting and boiling points among the halides of the same element, since iodine is the most polarisable of the halogens and, having the most electrons among them, can contribute the most to van der Waals forces. Naturally, exceptions abound in intermediate iodides where one trend gives way to the other. Similarly, solubilities in water of predominantly ionic iodides (e.g. potassium and calcium) are the greatest among ionic halides of that element, while those of covalent iodides (e.g. silver) are the lowest of that element. In particular, silver iodide is very insoluble in water and its formation is often used as a qualitative test for iodine.

The halogens form many binary, diamagnetic interhalogen compounds with stoichiometries XY, XY, XY, and XY (where X is heavier than Y), and iodine is no exception. Iodine forms all three possible diatomic interhalogens, a trifluoride and trichloride, as well as a pentafluoride and, exceptionally among the halogens, a heptafluoride. Numerous cationic and anionic derivatives are also characterised, such as the wine-red or bright orange compounds of and the dark brown or purplish black compounds of ICl. Apart from these, some pseudohalides are also known, such as cyanogen iodide (ICN), iodine thiocyanate (ISCN), and iodine azide (IN).
Iodine monofluoride (IF) is unstable at room temperature and disproportionates very readily and irreversibly to iodine and iodine pentafluoride, and thus cannot be obtained pure. It can be synthesised from the reaction of iodine with fluorine gas in trichlorofluoromethane at −45 °C, with iodine trifluoride in trichlorofluoromethane at −78 °C, or with silver(I) fluoride at 0 °C. Iodine monochloride (ICl) and iodine monobromide (IBr), on the other hand, are moderately stable. The former, a volatile red-brown compound, was discovered independently by Joseph Louis Gay-Lussac and Humphry Davy in 1813–4 not long after the discoveries of chlorine and iodine, and it mimics the intermediate halogen bromine so well that Justus von Liebig was misled into mistaking bromine (which he had found) for iodine monochloride. Iodine monochloride and iodine monobromide may be prepared simply by reacting iodine with chlorine or bromine at room temperature and purified by fractional crystallisation. Both are quite reactive and attack even platinum and gold, though not boron, carbon, cadmium, lead, zirconium, niobium, molybdenum, and tungsten. Their reaction with organic compounds depends on conditions. Iodine chloride vapour tends to chlorinate phenol and salicyclic acid, since when iodine chloride undergoes homolytic dissociation, chlorine and iodine are produced and the former is more reactive. However, iodine chloride in tetrachloromethane solution results in iodination being the main reaction, since now heterolytic fission of the I–Cl bond occurs and I attacks phenol as an electrophile. However, iodine monobromide tends to brominate phenol even in tetrachloromethane solution because it tends to dissociate into its elements in solution, and bromine is more reactive than iodine. When liquid, iodine monochloride and iodine monobromide dissociate into and anions (X = Cl, Br); thus they are significant conductors of electricity and can be used as ionising solvents.

Iodine trifluoride (IF) is an unstable yellow solid that decomposes above −28 °C. It is thus little-known. It is difficult to produce because fluorine gas would tend to oxidise iodine all the way to the pentafluoride; reaction at low temperature with xenon difluoride is necessary. Iodine trichloride, which exists in the solid state as the planar dimer ICl, is a bright yellow solid, synthesised by reacting iodine with liquid chlorine at −80 °C; caution is necessary during purification because it easily dissociates to iodine monochloride and chlorine and hence can act as a strong chlorinating agent. Liquid iodine trichloride conducts electricity, possibly indicating dissociation to and ions.

Iodine pentafluoride (IF), a colourless, volatile liquid, is the most thermodynamically stable iodine fluoride, and can be made by reacting iodine with fluorine gas at room temperature. It is a fluorinating agent, but is mild enough to store in glass apparatus. Again, slight electrical conductivity is present in the liquid state because of dissociation to and . The pentagonal bipyramidal iodine heptafluoride (IF) is an extremely powerful fluorinating agent, behind only chlorine trifluoride, chlorine pentafluoride, and bromine pentafluoride among the interhalogens: it reacts with almost all the elements even at low temperatures, fluorinates Pyrex glass to form iodine(VII) oxyfluoride (IOF), and sets carbon monoxide on fire.

Iodine oxides are the most stable of all the halogen oxides, because of the strong I–O bonds resulting from the large electronegativity difference between iodine and oxygen, and they have been known for the longest time. The stable, white, hygroscopic iodine pentoxide (IO) has been known since its formation in 1813 by Gay-Lussac and Davy. It is most easily made by the dehydration of iodic acid (HIO), of which it is the anhydride. It will quickly oxidise carbon monoxide completely to carbon dioxide at room temperature, and is thus a useful reagent in determining carbon monoxide concentration. It also oxidises nitrogen oxide, ethylene, and hydrogen sulfide. It reacts with sulfur trioxide and peroxydisulfuryl difluoride (SOF) to form salts of the iodyl cation, [IO], and is reduced by concentrated sulfuric acids to iodosyl salts involving [IO]. It may be fluorinated by fluorine, bromine trifluoride, sulfur tetrafluoride, or chloryl fluoride, resulting iodine pentafluoride, which also reacts with iodine pentoxide, giving iodine(V) oxyfluoride, IOF. A few other less stable oxides are known, notably IO and IO; their structures have not been determined, but reasonable guesses are I(IO) and [IO][IO] respectively.
More important are the four oxoacids: hypoiodous acid (HIO), iodous acid (HIO), iodic acid (HIO), and periodic acid (HIO or HIO). When iodine dissolves in aqueous solution, the following reactions occur:

Hypoiodous acid is unstable to disproportionation. The hypoiodite ions thus formed disproportionate immediately to give iodide and iodate:

Iodous acid and iodite are even less stable and exist only as a fleeting intermediate in the oxidation of iodide to iodate, if at all. Iodates are by far the most important of these compounds, which can be made by oxidising alkali metal iodides with oxygen at 600 °C and high pressure, or by oxidising iodine with chlorates. Unlike chlorates, which disproportionate very slowly to form chloride and perchlorate, iodates are stable to disproportionation in both acidic and alkaline solutions. From these, salts of most metals can be obtained. Iodic acid is most easily made by oxidation of an aqueous iodine suspension by electrolysis or fuming nitric acid. Iodate has the weakest oxidising power of the halates, but reacts the quickest.

Many periodates are known, including not only the expected tetrahedral , but also square-pyramidal , octahedral orthoperiodate , [IO(OH)], [IO(OH)], and . They are usually made by oxidising alkaline sodium iodate electrochemically (with lead(IV) oxide as the anode) or by chlorine gas:

They are thermodymically and kinetically powerful oxidising agents, quickly oxidising Mn to , and cleaving glycols, α-diketones, α-ketols, α-aminoalcohols, and α-diamines. Orthoperiodate especially stabilises high oxidation states among metals because of its very high negative charge of −5. Orthoperiodic acid, HIO, is stable, and dehydrates at 100 °C in a vacuum to metaperiodic acid, HIO. Attempting to go further does not result in the nonexistent iodine heptoxide (IO), but rather iodine pentoxide and oxygen. Periodic acid may be protonated by sulfuric acid to give the cation, isoelectronic to Te(OH) and , and giving salts with bisulfate and sulfate.

When iodine dissolves in strong acids, such as fuming sulfuric acid, a bright blue paramagnetic solution including cations is formed. A solid salt of the diiodine cation may be obtained by oxidising iodine with antimony pentafluoride:
The salt ISbF is dark blue, and the blue tantalum analogue ITaF is also known. Whereas the I–I bond length in I is 267 pm, that in is only 256 pm as the missing electron in the latter has been removed from an antibonding orbital, making the bond stronger and hence shorter. In fluorosulfuric acid solution, deep-blue reversibly dimerises below −60 °C, forming red rectangular diamagnetic . Other polyiodine cations are not as well-characterised, including bent dark-brown or black and centrosymmetric "C" green or black , known in the and salts among others.

The only important polyiodide anion in aqueous solution is linear triiodide, . Its formation explains why the solubility of iodine in water may be increased by the addition of potassium iodide solution:
Many other polyiodides may be found when solutions containing iodine and iodide crystallise, such as , , , and , whose salts with large, weakly polarising cations such as Cs may be isolated.

Organoiodine compounds have been fundamental in the development of organic synthesis, such as in the Hofmann elimination of amines, the Williamson ether synthesis, the Wurtz coupling reaction, and in Grignard reagents.

The carbon–iodine bond is a common functional group that forms part of core organic chemistry; formally, these compounds may be thought of as organic derivatives of the iodide anion. The simplest organoiodine compounds, alkyl iodides, may be synthesised by the reaction of alcohols with phosphorus triiodide; these may then be used in nucleophilic substitution reactions, or for preparing Grignard reagents. The C–I bond is the weakest of all the carbon–halogen bonds due to the minuscule difference in electronegativity between carbon (2.55) and iodine (2.66). As such, iodide is the best leaving group among the halogens, to such an extent that many organoiodine compounds turn yellow when stored over time due to decomposition into elemental iodine; as such, they are commonly used in organic synthesis, because of the easy formation and cleavage of the C–I bond. They are also significantly denser than the other organohalogen compounds thanks to the high atomic weight of iodine. A few organic oxidising agents like the iodanes contain iodine in a higher oxidation state than −1, such as 2-iodoxybenzoic acid, a common reagent for the oxidation of alcohols to aldehydes, and iodobenzene dichloride (PhICl), used for the selective chlorination of alkenes and alkynes. One of the more well-known uses of organoiodine compounds is the so-called iodoform test, where iodoform (CHI) is produced by the exhaustive iodination of a methyl ketone (or another compound capable of being oxidised to a methyl ketone), as follows:

Some drawbacks of using organoiodine compounds as compared to organochlorine or organobromine compounds is the greater expense and toxicity of the iodine derivatives, since iodine is expensive and organoiodine compounds are stronger alkylating agents. For example, iodoacetamide and iodoacetic acid denature proteins by irreversibly alkylating cysteine residues and preventing the reformation of disulfide linkages.

Halogen exchange to produce iodoalkanes by the Finkelstein reaction is slightly complicated by the fact that iodide is a better leaving group than chloride or bromide. The difference is nevertheless small enough that the reaction can be driven to completion by exploiting the differential solubility of halide salts, or by using a large excess of the halide salt. In the classic Finkelstein reaction, an alkyl chloride or an alkyl bromide is converted to an alkyl iodide by treatment with a solution of sodium iodide in acetone. Sodium iodide is soluble in acetone and sodium chloride and sodium bromide are not. The reaction is driven toward products by mass action due to the precipitation of the insoluble salt.

Iodine is the least abundant of the stable halogens, comprising only 0.46 parts per million of Earth's crustal rocks (compare: fluorine 544 ppm, chlorine 126 ppm, bromine 2.5 ppm). Among the 84 elements which occur in significant quantities (elements 1–42, 44–60, 62–83, and 90–92), it ranks 61st in abundance. Iodide minerals are rare, and most deposits that are concentrated enough for economical extraction are iodate minerals instead. Examples include lautarite, Ca(IO), and dietzeite, 7Ca(IO)·8CaCrO. These are the minerals that occur as trace impurities in the caliche, found in Chile, whose main product is sodium nitrate. In total, they can contain at least 0.02% and at most 1% iodine by mass. Sodium iodate is extracted from the caliche and reduced to iodide by sodium bisulfite. This solution is then reacted with freshly extracted iodate, resulting in comproportionation to iodine, which may be filtered off.

The caliche was the main source of iodine in the 19th century and continues to be important today, replacing kelp (which is no longer an economically viable source), but in the late 20th century brines emerged as a comparable source. The Japanese Minami Kanto gas field east of Tokyo and the American Anadarko Basin gas field in northwest Oklahoma are the two largest such sources. The brine is hotter than 60 °C from the depth of the source. The brine is first purified and acidified using sulfuric acid, then the iodide present is oxidised to iodine with chlorine. An iodine solution is produced, but is dilute and must be concentrated. Air is blown into the solution to evaporate the iodine, which is passed into an absorbing tower, where sulfur dioxide reduces the iodine. The hydrogen iodide (HI) is reacted with chlorine to precipitate the iodine. After filtering and purification the iodine is packed.

These sources ensure that Chile and Japan are the largest producers of iodine today. Alternatively, the brine may be treated with silver nitrate to precipitate out iodine as silver iodide, which is then decomposed by reaction with iron to form metallic silver and a solution of iron(II) iodide. The iodine may then be liberated by displacement with chlorine.

About half of all produced iodine goes into various organoiodine compounds, another 15% remains as the pure element, another 15% is used to form potassium iodide, and another 15% for other inorganic iodine compounds. Among the major uses of iodine compounds are catalysts, animal feed supplements, stabilisers, dyes, colourants and pigments, pharmaceutical, sanitation (from tincture of iodine), and photography; minor uses include smog inhibition, cloud seeding, and various uses in analytical chemistry.

The iodide and iodate anions are often used for quantitative volumetric analysis, for example in iodometry. Iodine and starch form a blue complex, and this reaction is often used to test for either starch or iodine and as an indicator in iodometry.The iodine test for starch is still used to detect counterfeit banknotes printed on starch-containing paper.

The iodine value is the mass of iodine in grams that is consumed by 100 grams of a chemical substance typically fats or oils. Iodine numbers are often used to determine the amount of unsaturation in fatty acids. This unsaturation is in the form of double bonds, which react with iodine compounds. In biology, linoleic acid (C18:2 n-6), omega-6 and alpha-linolenic (C18:3 n-3) omega-3, arachidonic acid (AA) – omega-6 (C20: 4n-6), and docosahexaenoic acid (DHA) – omega-3 (C22:6n-3) synthesized with iodine iodolipids developed among cell membranes during the evolution of life, important in the mechanism of apoptosis, carcinogenesis and degenerative diseases.

Potassium tetraiodomercurate(II), KHgI, is also known as Nessler's reagent. It is often used as a sensitive spot test for ammonia. Similarly, CuHgI is used as a precipitating reagent to test for alkaloids. Aqueous alkaline iodine solution is used in the iodoform test for methyl ketones.

The spectra of the iodine molecule, I, consists of (not exclusively) tens of thousands of sharp spectral lines in the wavelength range 500–700 nm. It is therefore a commonly used wavelength reference (secondary standard). By measuring with a spectroscopic Doppler-free technique while focusing on one of these lines, the hyperfine structure of the iodine molecule reveals itself. A line is now resolved such that either 15 components, (from even rotational quantum numbers, "J"), or 21 components (from odd rotational quantum numbers, "J") are measurable.

Cesium iodide and thallium-doped sodium iodide are used in crystal scintillators for the detection of gamma rays. The efficiency is high and energy dispersive spectroscopy is possible, but the resolution is rather poor.

Elemental iodine is used as a disinfectant either as the element, or as the water-soluble triiodide anion I generated "in situ" by adding iodide to poorly water-soluble elemental iodine (the reverse chemical reaction makes some free elemental iodine available for antisepsis). Elemental iodine may also be used to treat iodine deficiency.

In the alternative, iodine may be produced from iodophors, which contain iodine complexed with a solubilizing agent (the iodide ion may be thought of loosely as the iodophor in triiodide water solutions). Examples of such preparations include:


The antimicrobial action of iodine is quick and works at low concentrations, and thus it is used in operating theatres. Its specific mode of action is unknown. It penetrates into microorganisms and attacks particular amino acids (such as cysteine and methionine), nucleotides, and fatty acids, ultimately resulting in cell death. It also has an antiviral action, but nonlipid viruses and parvoviruses are less sensitive than lipid enveloped viruses. Iodine probably attacks surface proteins of enveloped viruses, and it may also destabilise membrane fatty acids by reacting with unsaturated carbon bonds.

In medicine, a saturated solution of potassium iodide is used to treat acute thyrotoxicosis. It is also used to block uptake of iodine-131 in the thyroid gland (see isotopes section above), when this isotope is used as part of radiopharmaceuticals (such as iobenguane) that are not targeted to the thyroid or thyroid-type tissues.

Iodine-131 (usually as iodide) is a component of nuclear fallout, and is particularly dangerous owing to the thyroid gland's propensity to concentrate ingested iodine and retain it for periods longer than this isotope's radiological half-life of eight days. For this reason, people at risk of exposure to environmental radioactive iodine (iodine-131) in fallout may be instructed to take non-radioactive potassium iodide tablets. The typical adult dose is one 130 mg tablet per 24 hours, supplying 100 mg (100,000 micrograms) of ionic iodine. (The typical daily dose of iodine for normal health is of order 100 micrograms; see "Dietary Intake" below.) Ingestion of this large dose of non-radioactive iodine minimises the uptake of radioactive iodine by the thyroid gland.

As an element with high electron density and atomic number, iodine absorbs X-rays weaker than 33.3 keV due to the photoelectric effect of the innermost electrons. Organoiodine compounds are used with intravenous injection as X-ray radiocontrast agents. This application is often in conjunction with advanced X-ray techniques such as angiography and CT scanning. At present, all water-soluble radiocontrast agents rely on iodine.

The production of ethylenediamine dihydroiodide, provided as a nutritional supplement for livestock, consumes a large portion of available iodine. Another significant use is a catalyst for the production of acetic acid by the Monsanto and Cativa processes. In these technologies, which support the world's demand for acetic acid, hydroiodic acid converts the methanol feedstock into methyl iodide, which undergoes carbonylation. Hydrolysis of the resulting acetyl iodide regenerates hydroiodic acid and gives acetic acid.

Inorganic iodides find specialised uses. Titanium, zirconium, hafnium, and thorium are purified by the van Arkel process, which involves the reversible formation of the tetraiodides of these elements. Silver iodide is a major ingredient to traditional photographic film. Thousands of kilograms of silver iodide are used annually for cloud seeding to induce rain.

The organoiodine compound erythrosine is an important food coloring agent. Perfluoroalkyl iodides are precursors to important surfactants, such as perfluorooctanesulfonic acid.

The iodine clock reaction (in which iodine also serves as a test for starch, forming a dark blue complex), is a popular educational demonstration experiment and example of a seemingly oscillating reaction (it is only the concentration of an intermediate product that oscillates).

Iodine is an essential element for life and, at atomic number "Z" = 53, is the heaviest element commonly needed by living organisms. (Lanthanum and the other lanthanides, as well as tungsten with "Z" = 74, are used by a few microorganisms.) It is required for the synthesis of the growth-regulating thyroid hormones thyroxine and triiodothyronine (T and T respectively, named after their number of iodine atoms). A deficiency of iodine leads to decreased production of T and T and a concomitant enlargement of the thyroid tissue in an attempt to obtain more iodine, causing the disease known as simple goitre. The major form of thyroid hormone in the blood is thyroxine (T), which has a longer half-life than T. In humans, the ratio of T to T released into the blood is between 14:1 and 20:1. T is converted to the active T (three to four times more potent than T) within cells by deiodinases (5'-iodinase). These are further processed by decarboxylation and deiodination to produce iodothyronamine (Ta) and thyronamine (Ta'). All three isoforms of the deiodinases are selenium-containing enzymes; thus dietary selenium is essential for T production.

Iodine accounts for 65% of the molecular weight of T and 59% of T. Fifteen to 20 mg of iodine is concentrated in thyroid tissue and hormones, but 70% of all iodine in the body is found in other tissues, including mammary glands, eyes, gastric mucosa, fetal thymus, cerebro-spinal fluid and choroid plexus, arterial walls, the cervix, and salivary glands. In the cells of those tissues, iodide enters directly by sodium-iodide symporter (NIS). The action of iodine in mammary tissue is related to fetal and neonatal development, but in the other tissues, it is (at least) partially unknown.

Recommendations by the United States Institute of Medicine are between 110 and 130 µg for infants up to 12 months, 90 µg for children up to eight years, 130 µg for children up to 13 years, 150 µg for adults, 220 µg for pregnant women and 290 µg for lactation. The Tolerable Upper Intake Level (UL) for adults is 1,100 μg/day. This upper limit was assessed by analyzing the effect of supplementation on thyroid-stimulating hormone.

The thyroid gland needs no more than 70 μg/day to synthesise the requisite daily amounts of T4 and T3. The higher recommended daily allowance levels of iodine seem necessary for optimal function of a number of body systems, including lactation, gastric mucosa, salivary glands, brain cells, choroid plexus, thymus, and arterial walls.

Natural sources of dietary iodine include seafood, such as fish, seaweeds (such as kelp) and shellfish, dairy products and eggs so long as the animals received enough iodine, and plants grown on iodine-rich soil. Iodised salt is fortified with iodine in the form of sodium iodide.

As of 2000, the median intake of iodine from food in the United States was 240 to 300 μg/day for men and 190 to 210 μg/day for women. The general US population has adequate iodine nutrition, with women of childbearing age and pregnant women having a possible mild risk of deficiency. In Japan, consumption was considered much higher, ranging between 5,280 μg/day to 13,800 μg/day from dietary seaweed or kombu kelp, often in the form of Kombu Umami extracts for soup stock and potato chips. However, new studies suggest that Japan's consumption is closer to 1,000–3,000 μg/day. The adult UL in Japan was last revised to 3,000 µg/day in 2015.

After iodine fortification programs such as iodisation of salt have been implemented, some cases of iodine-induced hyperthyroidism have been observed (so-called Jod-Basedow phenomenon). The condition seems to occur mainly in people over forty, and the risk appears higher when iodine deficiency is severe and the initial rise in iodine intake is high.

In areas where there is little iodine in the diet, typically remote inland areas and semi-arid equatorial climates where no marine foods are eaten, iodine deficiency gives rise to hypothyroidism, symptoms of which are extreme fatigue, goitre, mental slowing, depression, weight gain, and low basal body temperatures. Iodine deficiency is the leading cause of preventable intellectual disability, a result that occurs primarily when babies or small children are rendered hypothyroidic by a lack of the element. The addition of iodine to table salt has largely eliminated this problem in wealthier nations, but iodine deficiency remains a serious public health problem in the developing world today. Iodine deficiency is also a problem in certain areas of Europe. Information processing, fine motor skills, and visual problem solving are improved by iodine repletion in moderately iodine-deficient children.

Elemental iodine (I) is toxic if taken orally undiluted. The lethal dose for an adult human is 30 mg/kg, which is about 2.1–2.4 grams for a human weighing 70 to 80 kg (even if experiments on rats demonstrated that these animals could survive after eating a 14000 mg/kg dose). Excess iodine can be more cytotoxic in the presence of selenium deficiency. Iodine supplementation in selenium-deficient populations is, in theory, problematic, partly for this reason. The toxicity derives from its oxidizing properties, through which it denaturates proteins (including enzymes).

Elemental iodine is also a skin irritant, and direct contact with skin can cause damage and solid iodine crystals should be handled with care. Solutions with high elemental iodine concentration, such as tincture of iodine and Lugol's solution, are capable of causing tissue damage if used in prolonged cleaning or antisepsis; similarly, liquid Povidone-iodine (Betadine) trapped against the skin resulted in chemical burns in some reported cases.

People can be exposed to iodine in the workplace by inhalation, ingestion, skin contact, and eye contact. The Occupational Safety and Health Administration (OSHA) has set the legal limit (Permissible exposure limit) for iodine exposure in the workplace at 0.1 ppm (1 mg/m) during an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a Recommended exposure limit (REL) of 0.1 ppm (1 mg/m) during an 8-hour workday. At levels of 2 ppm, iodine is immediately dangerous to life and health.

Some people develop a hypersensitivity to products and foods containing iodine. Applications of tincture of iodine or Betadine can cause rashes, sometimes severe. Parenteral use of iodine-based contrast agents (see above) can cause reactions ranging from a mild rash to fatal anaphylaxis. Such reactions have led to the misconception (widely held, even among physicians) that some people are allergic to iodine itself; even allergies to iodine-rich seafood have been so construed. In fact, there has never been a confirmed report of a true iodine allergy, and an allergy to elemental iodine or simple iodide salts is theoretically impossible. Hypersensitivity reactions to products and foods containing iodine are apparently related to their other molecular components; thus, a person who has demonstrated an allergy to one food or product containing iodine may not have an allergic reaction to another. Patients with various food allergies (shellfish, egg, milk, etc.) do not have an increased risk for a contrast medium hypersensitivity. As with all medications, the patient's allergy history should be questioned and consulted before any containing iodine are administered.
Phosphorus can reduce elemental iodine to hydroiodic acid, which is a reagent effective for reducing ephedrine or pseudoephedrine to methamphetamine. For this reason, iodine was designated by the United States Drug Enforcement Administration as a List I precursor chemical under 21 CFR 1310.02.


</doc>
<doc id="14751" url="https://en.wikipedia.org/wiki?curid=14751" title="IKEA">
IKEA

IKEA ( , ) is a Swedish multinational group situated in the Netherlands that designs and sells , kitchen appliances and home accessories, among other useful goods and occasionally home services. Founded in Sweden in 1943 by 17-year-old Ingvar Kamprad, IKEA has been the world's largest furniture retailer since 2008. According to the Bloomberg Billionaires Index, as of January 2018, Kamprad was the eighth richest person in the world, with an estimated net worth of US$58.7 billion. The company's name is an acronym that consists of the founder's initials (Ingvar Kamprad), and those of Elmtaryd, the family farm where he was born, and the nearby village Agunnaryd (his hometown in Småland, southern Sweden).

The company is known for its modernist designs for various types of appliances and furniture, and its interior design work is often associated with an eco-friendly simplicity. In addition, the firm is known for its attention to cost control, operational details, and continuous product development that allowed IKEA to lower its prices by an average of two to three percent.

The business is a private company owned by Inter IKEA Systems B.V., registered in the Netherlands and controlled by the sons of its founder Ingvar Kamprad. The IKEA group has a complex corporate structure, which members of the European Parliament have alleged was designed to avoid over €1 billion in tax payments over the 20092014 period. It is controlled by several foundations based in the Netherlands and Liechtenstein.

, there are 433 IKEA stores operating in 52 countries and in fiscal year 2018, €38.8 billion (US$44.6 billion) worth of IKEA goods were sold. The IKEA website contains about 12,000 products and there were over 2.1 billion visitors to IKEA's websites in the year from September 2015 to August 2016. The company is responsible for approximately 1% of world commercial-product wood consumption, making it one of the largest users of wood in the retail sector.

Most of IKEA's stores are owned by INGKA, a holding company controlled by the Stichting INGKA Foundation, one of the 40 wealthiest foundations in the world. INGKA received 90% of IKEA's revenue in 2018.

In 1943, Ingvar Kamprad founded IKEA as a mail-order sales business, but began to sell furniture five years later. The first store was opened in Älmhult, Småland, in 1958, under the name Möbel-IKÉA (Möbel means "furniture" in Swedish). The first stores outside Sweden were opened in Norway (1963) and Denmark (1969). The stores spread to other parts of Europe in the 1970s, with the first store outside Scandinavia opening in Switzerland (1973), followed by West Germany (1974).
In 1973, the company's West German executives accidentally opened a store in Konstanz instead of Koblenz. Later that decade, stores opened in other parts of the world, such as Japan (1974), Australia, Canada, Hong Kong (1975), and Singapore (1978). IKEA further expanded in the 1980s, opening stores in countries such as France and Spain (1981), Belgium (1984), the United States (1985), the United Kingdom (1987), and Italy (1989). Germany, with 53 stores, is IKEA's biggest market, followed by the United States, with 51 stores. The first IKEA store in Latin America opened on 17 February 2010 in Santo Domingo, Dominican Republic.

Older IKEA stores are usually blue buildings with yellow accents (also Sweden's national colours). They are often designed in a one-way layout, leading customers counter-clockwise along what IKEA calls "the long natural way" designed to encourage the customer to see the store in its entirety (as opposed to a traditional retail store, which allows a customer to go directly to the section where the desired goods and services are displayed). There are often shortcuts to other parts of the showroom.

The sequence first involves going through the furniture showrooms making note of selected items. The customer then collects a shopping cart and proceeds to an open-shelf "Market Hall" warehouse for smaller items, then visits the self-service furniture warehouse to collect previously noted showroom products in flat pack form. Sometimes, they are directed to collect products from an external warehouse on the same site or at a site nearby after purchase. Finally, customers pay for their products at a cash register. Not all furniture is stocked at the store level, such as particular sofa colours needing to be shipped from a warehouse to the customer's home or to the store.

Most stores follow the layout of having the showroom upstairs with the marketplace and self-service warehouse downstairs. Some stores are single level, while others have separate warehouses to allow more stock to be kept on-site. Single-level stores are found predominantly in areas where the cost of land would be less than the cost of building a 2-level store. Some stores have dual-level warehouses with machine-controlled silos to allow large quantities of stock to be accessed throughout the selling day.

Most IKEA stores offer an "as-is" area at the end of the warehouse, just before the cash registers. Returned, damaged and formerly showcased products are displayed here and sold with a significant discount, but also with a no-returns policy.

IKEA uses a sales technique called "bulla bulla" in which a bunch of items are purposefully jumbled in bins, to create the impression of volume, and therefore, inexpensiveness.

Since 1958, every Ikea store includes a cafe that, until 2011, sold branded Swedish prepared specialist foods, such as meatballs, packages of gravy, lingonberry jam, various biscuits and crackers, and salmon and fish roe spread. The new label has a variety of items including chocolates, meatballs, jams, pancakes, salmon, along with various drinks.

Although the cafes primarily serve Swedish food, the menu varies based on the culture and location of each store. With restaurants in 38 different countries, the menu will incorporate local dishes including shawarma in Saudi Arabia, poutine in Canada, macarons in France, and gelato in Italy. In Indonesia, the Swedish Meatballs recipe is changed to accommodate the country's Halal requirements. Stores in Israel sell kosher food under rabbinical supervision. The kosher restaurants are separated into dairy and meat areas.

In many locations, the IKEA restaurants open daily before the rest of the store and serve breakfast. All food products are based on Swedish recipes and traditions. Food accounts for 5% of IKEA's sales.

Since August 2020, IKEA provides plant-based meatballs in all of the European stores, made from potatoes, apples, pea protein, and oats.

Every store has a kids play area, named Småland (Swedish for "small lands"; it is also the Swedish province where Kamprad was born). Parents drop off their children at a gate to the playground, and pick them up after they arrive at another entrance. In some stores, parents are given free pagers by the on-site staff, which the staff can use to summon parents whose children need them earlier than expected; in others, staff summon parents through announcements over the in-store public address system or by calling them on their cellphones.

The vast majority of IKEA stores are located outside of city centers, primarily because of land cost and traffic access. Several smaller store formats have been unsuccessfully tested in the past (the "midi" concept in the early '90s, which was tested in Ottawa and Heerlen with , or a "boutique" shop in Manhattan). A new format for a full-size, city centre store was introduced with the opening of the Manchester (United Kingdom) store, situated in Ashton-under-Lyne in 2006. Another store, in Coventry opened in December 2007. The store has seven floors and a different flow from other IKEA stores, however it closed down in 2020 due to the site being deemed unsuitable for future business . IKEA's Southampton store which opened in February 2009 is also in the city centre and built-in an urban style similar to the Coventry store. IKEA built these stores in response to UK government restrictions blocking retail establishment outside city centres.

In Hong Kong, where shop space is limited and costly, IKEA has opened three outlets in the city, most of which have the one-way layout. They are part of shopping malls, and while being tiny compared to common store design, are huge by Hong Kong standards. In addition to tailoring store sizes for specific countries, IKEA also alters the sizes of their products in order to accommodate cultural differences.

In 2015, IKEA announced that it would be attempting a smaller store design at several locations in Canada. This modified store will feature only a display gallery and a small warehouse. One location planned for Kitchener is in the place formerly occupied by a Sears Home store. The warehouses will not keep furniture stocked, and so customers will not be able to drop in to purchase and leave with furniture the same day. Instead, they will purchase the furniture in advance online or in-store and order the furniture delivered to one of the new stores, for a greatly reduced rate. IKEA claims that this new model will allow them to expand quickly into new markets rather than spending years opening a full-size store.

Japan was another market where IKEA performed badly initially, exited the market completely and then re-entered the Japanese market with an alternative store design with which it finally found success. The IKEA entered the Japan market in 1974 through a franchise arrangement with a local partner, only to withdraw in failure in 1986. Japan was one of the first markets outside its original core European market and despite Japan being the second largest economy in the world at the time IKEA did not adequately adapt its store layout strategy to the Japanese consumer. Japanese consumers did not have a culture of DIY furniture assembly, and many in the early days had no way to haul the flat-packs home to their small apartments. Nor did the store layouts familiar to European customers initially make much sense to Japanese consumers. So prior to re-entering the Japanese market in 2006 IKEA management did extensive local market research in more effective store layouts. One area of local adaptation was the room displays common to every IKEA store worldwide. Rather than just replicate a ‘typical’ European room layout the IKEA Japan management was careful to set up room displays more closely resembling Japanese apartment rooms, such as one for “a typical Japanese teenage boy who likes baseball and computer games”.

Furthermore, the IKEA has been forced to adapt its store location and services to the ‘inner-city’ format for the expansion in China, unlike other countries where IKEA stores for economic and planning restriction reasons tends to be more commonly just outside city centers due to planning restrictions. In China, planning restrictions is less of an issue than in other country markets due to the lack of cars for much of its customer base. Accordingly, in store design alternatives, IKEA has had to offer store locations and formats closer to public transportation since few customers had access to cars with which to buy and take-home DIY flat pack furniture. The store design alternative thinking and strategy in China has been to locate stores to facilitate access for non-car owning customers.

In fact, in some locations in China, IKEA stores can be found not in the usual suburban or near airport locations like in other countries, but rather places such as downtown shopping center with a ‘mini-IKEA’ store to attract shoppers. For example, one store design alternative trend that IKEA has implemented has been ‘pop-up’ stores along social media platforms in their advertising strategy for the first-time as a company to reach new customers demographics while still reinforcing its global brand locally in China.

Rather than being sold pre-assembled, much of IKEA's furniture is designed to be assembled by the customer. The company claims that this helps reduce costs and use of packaging by not shipping air; the volume of a bookcase, for example, is considerably less if it is shipped unassembled rather than assembled. This is also more practical for customers using public transport, because flat packs can be more easily carried.

IKEA contends that it has been a pioneering force in sustainable approaches to mass consumer culture. Kamprad calls this "democratic design," meaning that the company applies an integrated approach to manufacturing and design (see also environmental design). In response to the explosion of human population and material expectations in the 20th and 21st centuries, the company implements economies of scale, capturing material streams and creating manufacturing processes that hold costs and resource use down, such as the extensive use of Medium-Density Fiberboard ("MDF"), also called "particle board."

Notable items of IKEA furniture include the Poäng armchair, the Billy bookcase and the Klippan sofa, all of which have sold by the tens of millions since the late 1970s.

IKEA products are identified by one-word (rarely two-word) names. Most of the names are Scandinavian in origin. Although there are some exceptions, most product names are based on a special naming system developed by IKEA. Company founder Kamprad was dyslexic and found that naming the furniture with proper names and words, rather than a product code, made the names easier to remember.

Some of IKEA's Swedish product names have amusing or unfortunate connotations in other languages, sometimes resulting in the names being withdrawn in certain countries. Notable examples for English include the "Jerker" computer desk (discontinued several years ago ), "Fukta" plant spray, "Fartfull" workbench, and "Lyckhem" (meaning bliss).

In 2016, IKEA started a move into the smart home business. The IKEA TRÅDFRI smart lighting kit was one of the first ranges signalling this change. IKEA's media team has confirmed that smart home project will be a big move. They have also started a partnership with Philips Hue. The wireless charging furniture, integrating wireless Qi charging into everyday furniture, is another strategy for the smart home business.

A collaboration to build Sonos' smart speaker technology into furniture sold by IKEA was announced in December 2017. The first products resulting from the collaboration have launched in August 2019.

Under the product name SYMFONISK, IKEA and Sonos have made two distinct wireless speakers that integrate with existing Sonos households or can be used to start with the Sonos-ecosystem, one that's also a lamp and another that's a more traditional looking bookshelf speaker. Both products as well as accessories for the purpose of mounting the bookshelf speakers have gone on sale worldwide on 1 August.

From the start, IKEA SYMFONISK can only be controlled from the Sonos app, but IKEA will add support for the speakers in their own Home Smart app in October to be paired with scenes that control both the lights and smart blinds together with the speakers.

With the launch of a new accompanying remote, you will now be able to adjust the volume and music playback of the Symfonisk lamp or bookshelf speakers.

IKEA has also expanded its product base to include flat-pack houses and apartments, in an effort to cut prices involved in a first-time buyer's home. The IKEA product, named BoKlok was launched in Sweden in 1996 in a joint venture with Skanska. Now working in the Nordic countries and in the UK, sites confirmed in England include London, Ashton-under-Lyne, Leeds, Gateshead, Warrington and Liverpool.

At the end of September 2013, the company announced that solar panel packages, so-called "residential kits", for houses will be sold at 17 UK stores by mid-2014. The decision followed a successful pilot project at the Lakeside IKEA store, whereby one photovoltaic system was sold almost every day. The solar CIGS panels are manufactured by Solibro, a German-based subsidiary of the Chinese company Hanergy. By the end of 2014, IKEA began to sell Solibro's solar residential kits in the Netherlands and in Switzerland. In November 2015, IKEA ended its contract with Hanergy and in April 2016 started working with Solarcentury to sell solar panels in the United Kingdom. The deal would allow customers to be able to order panels online and at three stores before being expanded to all United Kingdom stores by the end of summer.

In April 2019, the company announced that it would begin test marketing a new concept, renting furniture to customers. One of the motivating factors was the fact that inexpensive IKEA products were viewed as "disposable" and often ended up being scrapped after a few years of use. This was at a time when especially younger buyers said they wanted to minimize their impact on the environment. The company understood this view. In an interview, Jesper Brodin, chief executive of Ingka Group (the largest franchisee of IKEA stores), commented that "climate change and unsustainable consumption are among the biggest challenges we face in society". The other strategic objectives of the plan were to be more affordable and more convenient. The company said it would test the rental concept in all 30 markets by 2020, expecting it to increase the number of times a piece of furniture would be used before recycling.

IKEA owns and operates the MEGA Family Shopping Centre chain in Russia.

On 8 August 2008, IKEA UK launched a virtual mobile phone network called IKEA Family Mobile, which ran on T-Mobile. At launch it was the cheapest pay-as-you-go network in the UK. In June 2015 the network announced that its services would cease to operate from 31 August 2015.

, IKEA is in joint venture with TCL to provide Uppleva integrated HDTV and entertainment system product.

In mid-August 2012, the company announced that it would establish a chain of 100 economy hotels in Europe but, unlike its few existing hotels in Scandinavia, they would not carry the IKEA name, nor would they use IKEA furniture and furnishings – they would be operated by an unnamed international group of hoteliers. As of 30 April 2018, however, the company owned only a single hotel, the IKEA Hotell in Älmhult, Sweden, but was planning to open another one, in New Haven, Connecticut, United States, after converting the historic Pirelli Building. The company received approval for the concept from the city's planning commission in mid-November 2018; the building was to include 165 rooms and the property would offer 129 dedicated parking spaces. Research in April 2019 provided no indication that the hotel had been completed as of that time.

In September 2017, IKEA announced they would be acquiring San Francisco-based TaskRabbit. The deal, completed later that year, has TaskRabbit operating as an independent company.

In March 2020, IKEA announced that it had partnered with Pizza Hut Hong Kong on a joint venture. IKEA launched a new side table called SÄVA. The table, designed to resemble a pizza saver, would be boxed in packaging resembling a pizza box, and the building instructions included a suggestion to order a Swedish meatball pizza from Pizza Hut, which would contain the same meatballs served in IKEA restaurants.

In April 2020, IKEA acquired AI imaging startup Geomagical Labs.

In July 2020, IKEA opened a concept store in the Harajuku district of Tokyo, Japan, where it launched its first ever apparel line.

IKEA is owned and operated by a complicated array of not-for-profit and for-profit corporations. The corporate structure is divided into two main parts: operations and franchising.

Inter IKEA Systems is owned by Inter IKEA Holding BV, a company registered in the Netherlands, formerly registered in Luxembourg (under the name Inter IKEA Holding SA). Inter IKEA Holding, in turn, is owned by the Interogo Foundation, based in Liechtenstein. In 2016, the INGKA Holding sold its design, manufacturing and logistics subsidiaries to Inter IKEA Holding.

In June 2013, Ingvar Kamprad resigned from the board of Inter IKEA Holding SA and his youngest son Mathias Kamprad replaced Per Ludvigsson as the chairman of the holding company. Following his decision to step down, the 87-year-old founder explained, "I see this as a good time for me to leave the board of Inter IKEA Group. By that we are also taking another step in the generation shift that has been ongoing for some years." After the 2016 company restructure, Inter IKEA Holding SA no longer exists, having reincorporated in the Netherlands. Mathias Kamprad became a board member of the Inter IKEA Group and the Interogo Foundation. Mathias and his two older brothers, who also have leadership roles at IKEA, work on the corporation's overall vision and long-term strategy.

Along with helping IKEA make a non-taxable profit, IKEA's complicated corporate structure allowed Kamprad to maintain tight control over the operations of INGKA Holding, and thus the operation of most IKEA stores. The INGKA Foundation's five-person executive committee was chaired by Kamprad. It appoints a board of INGKA Holding, approves any changes to INGKA Holding's bylaws, and has the right to preempt new share issues. If a member of the executive committee quits or dies, the other four members appoint his or her replacement.

In Kamprad's absence, the foundation's bylaws include specific provisions requiring it to continue operating the INGKA Holding group and specifying that shares can be sold only to another foundation with the same objectives as the INGKA Foundation.

The net profit of IKEA Group (which does not include Inter IKEA systems) in fiscal year 2009 (after paying franchise fees to Inter IKEA systems) was €2.538 billion on sales of €21.846 billion. Because INGKA Holding is owned by the nonprofit INGKA Foundation, none of this profit is taxed. The foundation's nonprofit status also means that the Kamprad family cannot reap these profits directly, but the Kamprads do collect a portion of IKEA sales profits through the franchising relationship between INGKA Holding and Inter IKEA Systems.

Inter IKEA Systems collected €631 million of franchise fees in 2004 but reported pre-tax profits of only €225 million in 2004. One of the major pre-tax expenses that Inter IKEA systems reported was €590 million of "other operating charges". IKEA has refused to explain these charges, but Inter IKEA Systems appears to make large payments to I.I. Holding, another Luxembourg-registered group that, according to "The Economist," "is almost certain to be controlled by the Kamprad family." I.I. Holding made a profit of €328 million in 2004.

In 2004, the Inter IKEA group of companies and I.I. Holding reported combined profits of €553m and paid €19m in taxes, or approximately 3.5 percent.

Public Eye (formerly known as Erklärung von Bern, literally "The Berne Declaration"), a non-profit organisation in Switzerland that promotes corporate responsibility, has formally criticised IKEA for its tax avoidance strategies. In 2007, the organisation nominated IKEA for one of its Public Eye "awards", which highlight corporate irresponsibility and are announced during the World Economic Forum in Davos, Switzerland.

In February 2016, the Greens / EFA group in the European Parliament issued a report entitled "IKEA: Flat Pack Tax Avoidance" on the tax planning strategies of IKEA and their possible use to avoid tax in several European countries. The report was sent to Pierre Moscovici, the European Commissioner for Economic and Financial Affairs, Taxation and Customs, and Margrethe Vestager, the European Commissioner for Competition, expressing the hope that it would be of use to them in their respective roles "to advance the fight for tax justice in Europe." Sales jumped 17 per cent to almost €132 million in the 12 months to the end of August 2015.

Although IKEA household products and furniture are designed in Sweden, they are largely manufactured in developing countries to keep costs down. For most of its products, the final assembly is performed by the end-user (consumer).

Swedwood, an IKEA subsidiary, handles production of all of the company's wood-based products, with the largest Swedwood factory located in Southern Poland. According to the subsidiary, over 16,000 employees across 50 sites in 10 countries manufacture the 100 million pieces of furniture that IKEA sells annually. IKEA furniture uses the hardwood alternative particle board. Hultsfred, a factory in southern Sweden, is the company's sole supplier.

During the 1980s, IKEA kept its costs down by using production facilities in East Germany. A portion of the workforce at those factories consisted of political prisoners. This fact, revealed in a report by Ernst & Young commissioned by the company, resulted from the intermingling of criminals and political dissidents in the state-owned production facilities IKEA contracted with, a practice which was generally known in West Germany. IKEA was one of a number of companies, including West German firms, which benefited from this practice. The investigation resulted from attempts by former political prisoners to obtain compensation. In November 2012, IKEA admitted being aware at the time of the possibility of use of forced labor and failing to exercise sufficient control to identify and avoid it. A summary of the Ernst & Young report was released on 16 November 2012.

IKEA was named one of the 100 Best Companies for Working Mothers in 2004 and 2005 by "Working Mothers" magazine. It ranked 80 in Fortune's 200 Best Companies to Work For in 2006 and in October 2008, IKEA Canada LP was named one of "Canada's Top 100 Employers" by Mediacorp Canada Inc.

In 2012, IKEA in France was accused by the independent newspaper "Le Canard enchaîné" and the investigative website "Mediapart" of spying on its employees and clients by illegally accessing French police records. The head of risk management at IKEA feared his employees were anti-globalists or potential ecoterrorists.

After initial environmental issues like the highly publicized formaldehyde scandals in the early 1980s and 1992, IKEA took a proactive stance on environmental issues and tried to prevent future incidents through a variety of measures. In 1990, IKEA invited Karl-Henrik Robèrt, founder of the Natural Step, to address its board of directors. Robert's system conditions for sustainability provided a strategic approach to improving the company's environmental performance. In 1990, IKEA adopted the Natural Step framework as the basis for its environmental plan. This led to the development of an Environmental Action Plan, which was adopted in 1992. The plan focused on structural change, allowing IKEA to "maximize the impact of resources invested and reduce the energy necessary to address isolated issues." The environmental measures taken include the following:


In 2000 IKEA introduced its code of conduct for suppliers that covers social, safety, and environmental questions. Today IKEA has around 60 auditors who perform hundreds of supplier audits every year. The main purpose of these audits is to make sure that the IKEA suppliers follow the law in each country where they are based. Most IKEA suppliers fulfill the law today with exceptions for some special issues, one being excessive working hours in Asia, in countries such as China and India.

Since March 2013, IKEA has stopped providing plastic bags to customers, but offers reusable bags for sale. The IKEA restaurants also only offer reusable plates, knives, forks, spoons, etc. Toilets in some IKEA WC-rooms have been outfitted with dual-function flushers. IKEA has recycling bins for compact fluorescent lamps (CFLs), energy-saving bulbs, and batteries.
In 2001 IKEA was one of the first companies to operate its own cross-border goods trains through several countries in Europe.

In August 2008, IKEA also announced that it had created IKEA GreenTech, a €50 million venture capital fund. Located in Lund (a university town in Sweden), it will invest in 8–10 companies in the coming five years with focus on solar panels, alternative light sources, product materials, energy efficiency and water saving and purification. The aim is to commercialise green technologies for sale in IKEA stores within 3–4 years.

To make IKEA a more sustainable company, a product life cycle was created. For the idea stage, products should be flat-packed so that more items can be shipped at once; products should also be easier to dismantle and recycle. Raw materials are used, and since wood and cotton are two of IKEA's most important manufacturing products, the company works with environmentally friendly forests and cotton, whereby the excessive use of chemicals and water is avoided.

IKEA stores recycle waste and many run on renewable energy. All employees are trained in environmental and social responsibility, while public transit is one of the priorities when the location of stores is considered. Also, the coffee and chocolate served at IKEA stores is UTZ Certified.

The last stage of the life cycle is the end of life. Most IKEA stores recycle light bulbs and drained batteries, and the company is also exploring the recycling of sofas and other home furnishing products.

According to IKEA's 2012 "Sustainability Report", 23% of all wood that the company uses meets the standards of the Forest Stewardship Council, and the report states that IKEA aims to double this percentage by 2017. The report also states that IKEA does not accept illegally logged wood and supports 13 World Wide Fund for Nature (WWF) projects. IKEA owns 125,000 acres of forest in USA and about 450,000 acres in Europe.
On 17 February 2011, IKEA announced its plans to develop a wind farm in Dalarna County, Sweden, furthering its goal of using only renewable energy to fuel its operations. , 17 United States IKEA stores are powered by solar panels, with 22 additional installations in progress, and IKEA owns the 165 MW Cameron Wind farm in Cameron County on the South Texas coast and a 42 MW coastal wind farm in Finland.

In 2011, the company examined its wood consumption and noticed that almost half of its global pine and spruce consumption was for the fabrication of pallets. The company consequently started a transition to the use of paper pallets and the "Optiledge system". The OptiLedge product is totally recyclable, made from 100% virgin high-impact copolymer polypropylene (PP). The system is a "unit load alternative to the use of a pallet. The system consists of the OptiLedge (usually used in pairs), aligned and strapped to the bottom carton to form a base layer upon which to stack more products. Corner boards are used when strapping to minimize the potential for package compression." The conversion began in Germany and Japan, before its introduction into the rest of Europe and North America. The system has been marketed to other companies, and IKEA has formed the OptiLedge company to manage and sell the product.

IKEA has expanded its sustainability plan in the UK to include electric car charge points for customers at all locations by the end of 2013. The effort will include Nissan and Ecotricity and promise to deliver an 80% charge in 30 minutes.

From 2016 they have only sold energy-efficient LED lightbulbs, lamps and light fixtures. LED lightbulbs use as little as 15% of the power of a regular incandescent light bulb.

, IKEA has signed on with 25 other companies to participate in the British Retail Consortium's Better Retail Better World initiative, which challenges companies to meet objectives outlined by the United Nations Sustainable Development Goals.

In September 2019, IKEA announced that they would be investing $2.8 billion in renewable energy infrastructure. The company is targeting making their entire supply chain climate positive by 2030.

The INGKA Foundation is officially dedicated to promoting "innovations in architecture and interior design." The net worth of the foundation exceeded the net worth of the much better known Bill & Melinda Gates Foundation (now the largest private foundation in the world) for a period. However, most of the Group's profit is spent on investment.

IKEA is involved in several international charitable causes, particularly in partnership with UNICEF, including:

IKEA also supports American Forests to restore forests and reduce pollution.

In September 2005, IKEA Social Initiative was formed to manage the company's social involvement on a global level. IKEA Social Initiative is headed by Marianne Barner.

The main partners of IKEA Social Initiative are UNICEF and Save the Children.

On 23 February 2009, at the ECOSOC event in New York, UNICEF announced that IKEA Social Initiative has become the agency's largest corporate partner, with total commitments of more than US$180 million (£281,079,000).

Examples of involvements:

In 2009, Sweden's largest television station, SVT, revealed that IKEA's money—the three per cent collection from each store—does not actually go to a charitable foundation in the Netherlands, as IKEA has said. Inter IKEA is owned by a foundation in Liechtenstein, called Interogo, which has amassed $12 billion (£18 billion), and is controlled by the Kamprad family.

IKEA publishes an annual catalogue, first published in Swedish in 1951. IKEA published 197 million catalogues in 2010, in twenty languages and sixty-one editions. It is considered to be the main marketing tool of the retail giant, consuming 70% of the company's annual marketing budget.

The catalogue is distributed both in stores and by mail, with most of it being produced by IKEA Communications AB in IKEA's hometown of Älmhult, Sweden where IKEA operates the largest photo studio in northern Europe at . The catalogue itself is printed on chlorine-free paper of 10–15% post-consumer waste, and prints approximately 175 million copies worldwide annually, more than 3 times as much as the Bible.

In 2009, IKEA changed the typeface used in its catalogue from Futura to Verdana, igniting a controversy nicknamed "Verdanagate". In 2019, the company switched fonts again, adopting IKEA Noto Sans as its official corporate typeface, including its catalogue. The typeface is a modified version of Google’s open-source font Noto Sans.

The 2013 catalogue is smartphone compatible, containing videos and photo galleries that can be accessed via an app by scanning the catalogue's pages, while the 2014 catalogue incorporates an augmented reality app that projects an item into a real-time photograph image of the user's room. The augmented reality app also provides an indication of the scale of IKEA objects in relation to the user's living environment.

In May 2017, IKEA introduced an online shopping interactive catalog, as a shopping virtual-assistant intelligent user interface web application, developed by the software house Netguru.

In 1994, IKEA ran a commercial in the United States widely thought to be the first to feature a homosexual couple; it aired for several weeks before being pulled after calls for a boycott and a bomb threat directed at IKEA stores. Other IKEA commercials appeal to the wider LGBTQ community, one featuring a transgender woman.
In 2002, the inaugural television component of the "Unböring" campaign, titled "Lamp", went on to win several awards, including a Grand Clio, Golds at the London International Awards and the ANDY Awards, and the Grand Prix at the Cannes Lions International Advertising Festival, the most prestigious awards ceremony in the advertising community.

IKEA launched a UK-wide "Home is the Most Important Place in the World" advertising campaign in September 2007 using estate agent signs with the term "Not For Sale" written on them as part of the wider campaign. After the campaign appeared in the Metro newspaper London the business news website www.mad.co.uk remarked that the IKEA campaign had amazing similarities with the marketing activity of UK home refurbishment company Onis living who had launched its own Not For Sale advertising campaign two years prior and was awarded the Interbuild 2006 Construction Marketing Award for the best campaign under £25,000.

A debate ensued between Fraser Patterson, Chief Executive of Onis and Andrew McGuinness, partner at Beattie McGuinness Bungay (BMB), the advertising and PR agency awarded the £12m IKEA account. The essence of the debate was that BMB claimed to be unaware of Onis's campaign as Onis was not an advertising agency. Onis's argument was that its advertising could be seen in prominent landmarks throughout London, having been already accredited, showing concern about the impact IKEA's campaign would have on the originality of its own. BMB and IKEA subsequently agreed to provide Onis with a feature page on the IKEA campaign site linking through to Onis's website for a period of 1 year.
In 2008, IKEA paired up with the makers of video game "The Sims 2" to make a stuff pack called "IKEA Home Stuff", featuring many IKEA products. It was released on 24 June 2008 in North America and 26 June 2008 in Europe. It is the second stuff pack with a major brand, the first being "The Sims 2 H&M Fashion Stuff".

IKEA took over the title sponsorship of Philadelphia's annual Thanksgiving Day parade in 2008, replacing Boscov's, which filed for bankruptcy in August 2008.

In November 2008, a subway train decorated in IKEA style was introduced in Novosibirsk, Russia. Four cars were turned into a mobile showroom of the Swedish design. The redesigned train, which features colourful seats and fancy curtains, carried passengers until 6 June 2009.

In 2008–2009, Oyster cards (the ticket-free system for the London Underground) were issued with IKEA-branded wallets. IKEA also sponsored the tube map.
In January 2009, just before the new store opened in Southampton, of Red Funnel was re-painted in an entirely yellow and blue livery to celebrate the opening of the new IKEA store in Southampton. This is the first time a Red Funnel ferry has been re-painted out of its own red and white colour scheme. It stayed in these colours for 12 months as part of a deal between Red Funnel and IKEA to provide home delivery services to the Isle of Wight. It was repainted with Red Funnel's red and white livery when the deal ended in January 2010.

In March 2010, IKEA developed an event in four important Metro stations in Paris, in which furniture collections are displayed in high-traffic spots, giving potential customers a chance to check out the brand's products. The Metro walls were also filled with prints that showcase IKEA interiors.

In September 2010, IKEA launched an advertisement for UK and Ireland called "Happy Inside" which had 100 cats lying on IKEA furniture in the flagship IKEA store in Wembley, London.

In April 2011, an advertising campaign was launched aiming at discovering whether men or women are messier in the home. Created by Mother, the campaign will begin with a TV advert shot in front of a live audience, featuring four stand-up comedians, two men and two women, debating which gender is messier. The idea behind the campaign is that domestic clutter leads to arguments, and thus to an unhappy home, a conflict that IKEA wants to show can be avoided with better storage. Viewers will be directed to a new Facebook page for the brand, where they are able to vote on who they believe is messier, and submit evidence using videos and photos through an app created especially for the campaign. Meanwhile, online display banners will allow other users the opportunity to vote, with online adverts promoting IKEA products demonstrating the problems confronting people, and offering solutions.

In 2016, in conjunction with Stockholm ad agency Åkestam Holst, IKEA released the "Where Life Happens" video campaign. The series focused on taboo issues like divorce and adoption, and was filmed in a non-traditional 4:3 aspect ratio. The campaign won an Epica gold award in Amsterdam.

In September 2017, IKEA launched the "IKEA Human Catalogue" campaign in which memory champion Yanjaa Wintersoul memorized all 328 pages of the catalogue in minute detail in just a week before its launch. To prove the legitimacy and accuracy of the campaign, live demonstrations were held at press conferences in IKEA stores across Malaysia, Singapore, Thailand as well as a Facebook Live event held at the Facebook Singapore headquarters and talk show demonstrations in the US with Steve Harvey among others. The advertising campaign was hugely successful winning numerous industry awards including the Webby award 2018 for best social media campaign, an Ogilvy award and is currently a contender for the Cannes Lions 2018.

In 2018, Evelina Rönnung and Hugo Wallmo were honoured for their work with Åkestam Holst on "Where Life Happens". A print ad for Sundvik cribs used pregnancy test technology developed by Mercene Labs, which allowed a woman to get a discount if the ad revealed she was pregnant. The work by Mercene Labs went on to have other uses in the medical field.

In common with some other retailers, IKEA launched a loyalty card called "IKEA Family". The card is free of charge and can be used to obtain discounts on certain products found in-store. It is available worldwide. In conjunction with the card, IKEA also publishes and sells a printed quarterly magazine titled "IKEA Family Live" which supplements the card and catalogue. The magazine is already printed in thirteen languages and an English edition for the United Kingdom was launched in February 2007. It is expected to have a subscription of over 500,000.

On 12 September 2017, IKEA announced the augmented reality app, IKEA Place, following by Apple's release of its ARkit technology and iOS 11. IKEA Place helps consumers to visualize true to scale IKEA products into real environment.

IKEA's goals of sustainability and environmental design in its merchandise have sometimes been at odds with the impact a new IKEA store can have on a community. In particular, the size of proposed IKEA stores has often seen significant opposition from members of such communities. The following are a list of issues which have received negative media attention, both regarding the size of IKEA's stores and other controversies:


IKEA has been criticised by Citytv in Canada for charging up to twice as much in their Canadian stores as for the same items sold in their American stores, despite the Canadian dollar reaching parity with the U.S. dollar.

Within the days after the launch of the South Korean edition of the official website, complaints arose from a group of consumers on IKEA's pricing policy in the country: the prices of certain products were higher than other countries. On 24 November 2014, Jang Duck-jin, head of the Fair Trade Commission's consumer policy bureau, told the media that the commission was planning to commission a consumer group to compare IKEA's product prices by country, and on 19 March 2015, the Consumers Union of Korea published a report comparing the prices of 49 IKEA products in South Korea and other countries.


In February 2013, IKEA announced it had pulled 17,000 portions of Swedish meatballs containing beef and pork from stores in Europe after testing in the Czech Republic found traces of horsemeat in the product. The company removed the Swedish meatballs from store shelves on 25 February 2013, but only made the announcement public after Swedish newspaper Svenska Dagbladet uncovered what happened. In a March 2013 media report, an IKEA representative stated that the corporation had forced Familjen Dafgård, its main meatball supplier, to cease business with 8 of its 15 suppliers and would reduce the number of purchasing countries. The offending meat was traced to a Polish abattoir.

In July 2015, IKEA, with the U.S. Consumer Product Safety Commission, through the company's Safer Homes Together advertising campaign, issued a warning in the United States, the United Kingdom, and Ireland to customers to secure the Malm chests of drawers and wardrobes firmly to the wall using free kits distributed by the company, after two deaths of young children in the U.S. in February and June 2014 when the furniture pieces tipped over on them. There were three other deaths, from 1989, from other, similar appliance models tipping over and 14 incidents of Malm chests tipping over, resulting in four injuries. The company sent out free kits on request for customers to anchor the furniture to the wall. In June 2016, after a third toddler died in the U.S., IKEA recalled all Malm dressers as well as several similar models which posed a tipping danger if not secured to the wall with the supplied kit. On 12 July 2016, bowing to two weeks of rising pressure in China, IKEA announced that it was extending this recall to that country, which – along with Europe – was initially excluded from the recall. Over 29 million dressers have been recalled. IKEA has settled wrongful death lawsuits for over $50 million in compensation to the families of the three children who were killed.

In 2019, a Polish IKEA employee protested against the company's support for LGBT by posting citations from Old and New Testament (e.g. Mat 18:6) on the company intranet, and got subsequently fired. His case was promptly picked up by the conservative ruling party, with Polish Minister of Justice criticizing it in strong terms, and some other politicians calling for boycott of IKEA. 
Polish prosecutors pressed charges against the person who made the decision to fire the employee.

In 2014, documents were found at the Securitate archives in Bucharest which indicated that IKEA's open purchase of Romanian lumber throughout the 1980s was part of a complex scheme (codenamed "Scandinavica") to fund the Securitate and allow the accumulation of foreign currency: the Romanian lumber company Tehnoforestexport would regularly overcharge IKEA, transfer the overpayments into private Securitate bank accounts, wait for interest to accrue, and then reimburse IKEA the principal. IKEA has denied complicity in Scandinavica but has begun an internal investigation to learn more.

In 2017 a French journalist team made discoveries of 200-year-old trees being made to particle board in their sub-supplier Kronospan's factory in Sebeș, Romania. Kronospan delivers particle board to Ecolor, who produces among others, the Brimnes-shelf for IKEA. Mikhail Tarasov, IKEAs Global Forestry Manager answered in an interview that the only thing they ask their suppliers for is using particle board in their furniture, nothing else. Romania has enormous troubles with illegal logging, with official numbers saying more than half of what is logged in the country is done illegally. IKEA have no transparency regarding their supply chain and questions regarding where they source their furniture and wood is considered classified.

Stockholm daily newspaper, Expressen report on Ingvar Kamprad past involvement with Swedish pro-Nazi groups as one of the member's archives revealed his name.
The archives showed Mr. Kamprad had attended a number of meetings and had befriended a leading extremist, Per Engdahl, starting in 1945 and extending well into the 1950s. The newspaper printed more details, including the text of a 1950 note from Mr. Kamprad to Mr. Engdahl in which Mr. Kamprad said he was proud to be involved with the groups. In Kamprad's replies, he denied he ever was a formal member of the rightist groups and said he was drawn to Mr. Engdahl's vision of a non-Communist, Socialist Europe. He also mentioned that his activities during that time "a part of my life which I bitterly regret."




</doc>
<doc id="14752" url="https://en.wikipedia.org/wiki?curid=14752" title="Iridium">
Iridium

Iridium is a chemical element with the symbol Ir and atomic number 77. A very hard, brittle, silvery-white transition metal of the platinum group, iridium is considered to be the second-densest metal (after osmium) with a density of as defined by experimental X-ray crystallography. However, at room temperature and standard atmospheric pressure, iridium has been calculated to have a density of , higher than osmium measured the same way. Still, the experimental X-ray crystallography value is considered to be the most accurate, as such iridium is considered to be the second densest element. It is the most corrosion-resistant metal, even at temperatures as high as 2000 °C. Although only certain molten salts and halogens are corrosive to solid iridium, finely divided iridium dust is much more reactive and can be flammable.

Iridium was discovered in 1803 among insoluble impurities in natural platinum. Smithson Tennant, the primary discoverer, named iridium after the Greek goddess Iris, personification of the rainbow, because of the striking and diverse colors of its salts. Iridium is one of the rarest elements in Earth's crust, with annual production and consumption of only three tonnes. Ir and Ir are the only two naturally occurring isotopes of iridium, as well as the only stable isotopes; the latter is the more abundant.

The most important iridium compounds in use are the salts and acids it forms with chlorine, though iridium also forms a number of organometallic compounds used in industrial catalysis, and in research. Iridium metal is employed when high corrosion resistance at high temperatures is needed, as in high-performance spark plugs, crucibles for recrystallization of semiconductors at high temperatures, and electrodes for the production of chlorine in the chloralkali process. Iridium radioisotopes are used in some radioisotope thermoelectric generators.

Iridium is found in meteorites in much higher abundance than in the Earth's crust. For this reason, the unusually high abundance of iridium in the clay layer at the Cretaceous–Paleogene boundary gave rise to the Alvarez hypothesis that the impact of a massive extraterrestrial object caused the extinction of dinosaurs and many other species 66 million years ago. Similarly, an iridium anomaly in core samples from the Pacific Ocean suggested the Eltanin impact of about 2.5 million years ago.

It is thought that the total amount of iridium in the planet Earth is much higher than that observed in crustal rocks, but as with other platinum-group metals, the high density and tendency of iridium to bond with iron caused most iridium to descend below the crust when the planet was young and still molten.

A member of the platinum group metals, iridium is white, resembling platinum, but with a slight yellowish cast. Because of its hardness, brittleness, and very high melting point, solid iridium is difficult to machine, form, or work; thus powder metallurgy is commonly employed instead. It is the only metal to maintain good mechanical properties in air at temperatures above . It has the 10th highest boiling point among all elements and becomes a superconductor at temperatures below 0.14 K.

Iridium's modulus of elasticity is the second-highest among the metals, only being surpassed by osmium. This, together with a high shear modulus and a very low figure for Poisson's ratio (the relationship of longitudinal to lateral strain), indicate the high degree of stiffness and resistance to deformation that have rendered its fabrication into useful components a matter of great difficulty. Despite these limitations and iridium's high cost, a number of applications have developed where mechanical strength is an essential factor in some of the extremely severe conditions encountered in modern technology.

The measured density of iridium is only slightly lower (by about 0.12%) than that of osmium, the densest metal known. Some ambiguity occurred regarding which of the two elements was denser, due to the small size of the difference in density and difficulties in measuring it accurately, but, with increased accuracy in factors used for calculating density, X-ray crystallographic data yielded densities of for iridium and for osmium.

Iridium is the most corrosion-resistant metal known: it is not attacked by almost any acid, aqua regia, molten metals, or silicates at high temperatures. It can, however, be attacked by some molten salts, such as sodium cyanide and potassium cyanide, as well as oxygen and the halogens (particularly fluorine) at higher temperatures. Iridium also reacts directly with sulfur at atmospheric pressure to yield iridium disulfide.

Iridium forms compounds in oxidation states between −3 and +9; the most common oxidation states are +3 and +4. Well-characterized examples of the high +6 oxidation state are rare, but include and two mixed oxides and . In addition, it was reported in 2009 that iridium(VIII) oxide () was prepared under matrix isolation conditions (6 K in Ar) by UV irradiation of an iridium-peroxo complex. This species, however, is not expected to be stable as a bulk solid at higher temperatures. The highest oxidation state (+9), which is also the highest recorded for "any" element, is only known in one cation, ; it is only known as gas-phase species and is not known to form any salts.

Iridium dioxide, , a blue black solid, is the only well-characterized oxide of iridium. A sesquioxide, , has been described as a blue-black powder which is oxidized to by . The corresponding disulfides, diselenides, sesquisulfides, and sesquiselenides are known, and has also been reported. Iridium also forms iridates with oxidation states +4 and +5, such as and , which can be prepared from the reaction of potassium oxide or potassium superoxide with iridium at high temperatures.

Although no binary hydrides of iridium, are known, complexes are known that contain and , where iridium has the +1 and +3 oxidation states, respectively. The ternary hydride is believed to contain both the and the 18-electron anion.

No monohalides or dihalides are known, whereas trihalides, , are known for all of the halogens. For oxidation states +4 and above, only the tetrafluoride, pentafluoride and hexafluoride are known. Iridium hexafluoride, , is a volatile and highly reactive yellow solid, composed of octahedral molecules. It decomposes in water and is reduced to , a crystalline solid, by iridium black. Iridium pentafluoride has similar properties but it is actually a tetramer, , formed by four corner-sharing octahedra. Iridium metal dissolves in molten alkali-metal cyanides to produce the (hexacyanoiridate) ion.
Hexachloroiridic(IV) acid, , and its ammonium salt are the most important iridium compounds from an industrial perspective. They are involved in the purification of iridium and used as precursors for most other iridium compounds, as well as in the preparation of anode coatings. The ion has an intense dark brown color, and can be readily reduced to the lighter-colored and vice versa. Iridium trichloride, , which can be obtained in anhydrous form from direct oxidation of iridium powder by chlorine at 650 °C, or in hydrated form by dissolving in hydrochloric acid, is often used as a starting material for the synthesis of other Ir(III) compounds. Another compound used as a starting material is ammonium hexachloroiridate(III), . Iridium(III) complexes are diamagnetic (low-spin) and generally have an octahedral molecular geometry.

Organoiridium compounds contain iridium–carbon bonds where the metal is usually in lower oxidation states. For example, oxidation state zero is found in tetrairidium dodecacarbonyl, , which is the most common and stable binary carbonyl of iridium. In this compound, each of the iridium atoms is bonded to the other three, forming a tetrahedral cluster. Some organometallic Ir(I) compounds are notable enough to be named after their discoverers. One is Vaska's complex, , which has the unusual property of binding to the dioxygen molecule, . Another one is Crabtree's catalyst, a homogeneous catalyst for hydrogenation reactions. These compounds are both square planar, d complexes, with a total of 16 valence electrons, which accounts for their reactivity.

An iridium-based organic LED material has been documented, and found to be much brighter than DPA or PPV, so could be the basis for flexible OLED lighting in the future.

Iridium has two naturally occurring, stable isotopes, Ir and Ir, with natural abundances of 37.3% and 62.7%, respectively. At least 37 radioisotopes have also been synthesized, ranging in mass number from 164 to 202. Ir, which falls between the two stable isotopes, is the most stable radioisotope, with a half-life of 73.827 days, and finds application in brachytherapy and in industrial radiography, particularly for nondestructive testing of welds in steel in the oil and gas industries; iridium-192 sources have been involved in a number of radiological accidents. Three other isotopes have half-lives of at least a day—Ir, Ir, and Ir. Isotopes with masses below 191 decay by some combination of β decay, α decay, and (rare) proton emission, with the exception of Ir, which decays by electron capture. Synthetic isotopes heavier than 191 decay by β decay, although Ir also has a minor electron capture decay path. All known isotopes of iridium were discovered between 1934 and 2008, with the most recent discoveries being Ir.

At least 32 metastable isomers have been characterized, ranging in mass number from 164 to 197. The most stable of these is Ir, which decays by isomeric transition with a half-life of 241 years, making it more stable than any of iridium's synthetic isotopes in their ground states. The least stable isomer is Ir with a half-life of only 2 µs. The isotope Ir was the first one of any element to be shown to present a Mössbauer effect. This renders it useful for Mössbauer spectroscopy for research in physics, chemistry, biochemistry, metallurgy, and mineralogy.

The discovery of iridium is intertwined with that of platinum and the other metals of the platinum group. Native platinum used by ancient Ethiopians and by South American cultures always contained a small amount of the other platinum group metals, including iridium. Platinum reached Europe as "platina" ("silverette"), found in the 17th century by the Spanish conquerors in a region today known as the department of Chocó in Colombia. The discovery that this metal was not an alloy of known elements, but instead a distinct new element, did not occur until 1748.

Chemists who studied platinum dissolved it in aqua regia (a mixture of hydrochloric and nitric acids) to create soluble salts. They always observed a small amount of a dark, insoluble residue. Joseph Louis Proust thought that the residue was graphite. The French chemists Victor Collet-Descotils, Antoine François, comte de Fourcroy, and Louis Nicolas Vauquelin also observed the black residue in 1803, but did not obtain enough for further experiments.

In 1803, British scientist Smithson Tennant (1761–1815) analyzed the insoluble residue and concluded that it must contain a new metal. Vauquelin treated the powder alternately with alkali and acids and obtained a volatile new oxide, which he believed to be of this new metal—which he named "ptene", from the Greek word "ptēnós", "winged". Tennant, who had the advantage of a much greater amount of residue, continued his research and identified the two previously undiscovered elements in the black residue, iridium and osmium. He obtained dark red crystals (probably of ]·"n") by a sequence of reactions with sodium hydroxide and hydrochloric acid. He named iridium after Iris (), the Greek winged goddess of the rainbow and the messenger of the Olympian gods, because many of the salts he obtained were strongly colored. Discovery of the new elements was documented in a letter to the Royal Society on June 21, 1804.

British scientist John George Children was the first to melt a sample of iridium in 1813 with the aid of "the greatest galvanic battery that has ever been constructed" (at that time). The first to obtain high-purity iridium was Robert Hare in 1842. He found it had a density of around and noted the metal is nearly immalleable and very hard. The first melting in appreciable quantity was done by Henri Sainte-Claire Deville and Jules Henri Debray in 1860. They required burning more than 300 liters of pure and gas for each kilogram of iridium.

These extreme difficulties in melting the metal limited the possibilities for handling iridium. John Isaac Hawkins was looking to obtain a fine and hard point for fountain pen nibs, and in 1834 managed to create an iridium-pointed gold pen. In 1880, John Holland and William Lofland Dudley were able to melt iridium by adding phosphorus and patented the process in the United States; British company Johnson Matthey later stated they had been using a similar process since 1837 and had already presented fused iridium at a number of World Fairs. The first use of an alloy of iridium with ruthenium in thermocouples was made by Otto Feussner in 1933. These allowed for the measurement of high temperatures in air up to .

In Munich, Germany in 1957 Rudolf Mössbauer, in what has been called one of the "landmark experiments in twentieth-century physics", discovered the resonant and recoil-free emission and absorption of gamma rays by atoms in a solid metal sample containing only Ir. This phenomenon, known as the Mössbauer effect (which has since been observed for other nuclei, such as Fe), and developed as Mössbauer spectroscopy, has made important contributions to research in physics, chemistry, biochemistry, metallurgy, and mineralogy. Mössbauer received the Nobel Prize in Physics in 1961, at the age 32, just three years after he published his discovery. In 1986 Rudolf Mössbauer was honored for his achievements with the Albert Einstein Medal and the Elliot Cresson Medal.

Iridium is one of the nine least abundant stable elements in Earth's crust, having an average mass fraction of 0.001 ppm in crustal rock; platinum is 10 times more abundant, gold is 40 times more abundant, and silver and mercury are 80 times more abundant. Tellurium is about as abundant as iridium. In contrast to its low abundance in crustal rock, iridium is relatively common in meteorites, with concentrations of 0.5 ppm or more. The overall concentration of iridium on Earth is thought to be much higher than what is observed in crustal rocks, but because of the density and siderophilic ("iron-loving") character of iridium, it descended below the crust and into Earth's core when the planet was still molten.

Iridium is found in nature as an uncombined element or in natural alloys; especially the iridium–osmium alloys, osmiridium (osmium-rich), and iridosmium (iridium-rich). In the nickel and copper deposits, the platinum group metals occur as sulfides (i.e. (Pt,Pd)S), tellurides (i.e. PtBiTe), antimonides (PdSb), and arsenides (i.e. ). In all of these compounds, platinum is exchanged by a small amount of iridium and osmium. As with all of the platinum group metals, iridium can be found naturally in alloys with raw nickel or raw copper. A number of iridium-dominant minerals, with iridium as the species-forming element, are known. They are exceedingly rare and often represent the iridium analogues of the above-given ones. The examples are irarsite and cuproiridsite, to mention some.

Within Earth's crust, iridium is found at highest concentrations in three types of geologic structure: igneous deposits (crustal intrusions from below), impact craters, and deposits reworked from one of the former structures. The largest known primary reserves are in the Bushveld igneous complex in South Africa, (near the largest known impact crater, the Vredefort crater) though the large copper–nickel deposits near Norilsk in Russia, and the Sudbury Basin (also an impact crater) in Canada are also significant sources of iridium. Smaller reserves are found in the United States. Iridium is also found in secondary deposits, combined with platinum and other platinum group metals in alluvial deposits. The alluvial deposits used by pre-Columbian people in the Chocó Department of Colombia are still a source for platinum-group metals. As of 2003, world reserves have not been estimated.

The Cretaceous–Paleogene boundary of 66 million years ago, marking the temporal border between the Cretaceous and Paleogene periods of geological time, was identified by a thin stratum of iridium-rich clay. A team led by Luis Alvarez proposed in 1980 an extraterrestrial origin for this iridium, attributing it to an asteroid or comet impact. Their theory, known as the Alvarez hypothesis, is now widely accepted to explain the extinction of the non-avian dinosaurs. A large buried impact crater structure with an estimated age of about 66 million years was later identified under what is now the Yucatán Peninsula (the Chicxulub crater). Dewey M. McLean and others argue that the iridium may have been of volcanic origin instead, because Earth's core is rich in iridium, and active volcanoes such as Piton de la Fournaise, in the island of Réunion, are still releasing iridium.

In 2019, worldwide production of iridium totaled 242,000 ounces (6860 kg).

Iridium is also obtained commercially as a by-product from nickel and copper mining and processing. During electrorefining of copper and nickel, noble metals such as silver, gold and the platinum group metals as well as selenium and tellurium settle to the bottom of the cell as "anode mud", which forms the starting point for their extraction. To separate the metals, they must first be brought into solution. Several separation methods are available depending on the nature of the mixture; two representative methods are fusion with sodium peroxide followed by dissolution in aqua regia, and dissolution in a mixture of chlorine with hydrochloric acid.

After the mixture is dissolved, iridium is separated from the other platinum group metals by precipitating ammonium hexachloroiridate () or by extracting with organic amines. The first method is similar to the procedure Tennant and Wollaston used for their separation. The second method can be planned as continuous liquid–liquid extraction and is therefore more suitable for industrial scale production. In either case, the product is reduced using hydrogen, yielding the metal as a powder or "sponge" that can be treated using powder metallurgy techniques.

Iridium prices have fluctuated over a considerable range. With a relatively small volume in the world market (compared to other industrial metals like aluminium or copper), the iridium price reacts strongly to instabilities in production, demand, speculation, hoarding, and politics in the producing countries.
As a substance with rare properties, its price has been particularly influenced by changes in modern technology:
The gradual decrease between 2001 and 2003 has been related to an oversupply of Ir crucibles used for industrial growth of large single crystals.
Likewise the prices above 1000 USD/oz between 2010 and 2014 have been explained with the installation of production facilities for single crystal sapphire used in LED backlights for TVs.

The demand for iridium surged from 2.5 tonnes in 2009 to 10.4 tonnes in 2010, mostly because of electronics-related applications that saw a rise from 0.2 to 6 tonnes – iridium crucibles are commonly used for growing large high-quality single crystals, demand for which has increased sharply. This increase in iridium consumption is predicted to saturate due to accumulating stocks of crucibles, as happened earlier in the 2000s. Other major applications include spark plugs that consumed 0.78 tonnes of iridium in 2007, electrodes for the chloralkali process (1.1 t in 2007) and chemical catalysts (0.75 t in 2007).

The high melting point, hardness and corrosion resistance of iridium and its alloys determine most of its applications. Iridium (or sometimes platinum alloys or osmium) and mostly iridium alloys have a low wear and are used, for example, for multi-pored spinnerets, through which a plastic polymer melt is extruded to form fibers, such as rayon. Osmium–iridium is used for compass bearings and for balances.

Their resistance to arc erosion makes iridium alloys ideal for electrical contacts for spark plugs, and iridium-based spark plugs are particularly used in aviation.

Pure iridium is extremely brittle, to the point of being hard to weld because the heat-affected zone cracks, but it can be made more ductile by addition of small quantities of titanium and zirconium (0.2% of each apparently works well).

Corrosion and heat resistance makes iridium an important alloying agent. Certain long-life aircraft engine parts are made of an iridium alloy, and an iridium–titanium alloy is used for deep-water pipes because of its corrosion resistance. Iridium is also used as a hardening agent in platinum alloys. The Vickers hardness of pure platinum is 56 HV, whereas platinum with 50% of iridium can reach over 500 HV.

Devices that must withstand extremely high temperatures are often made from iridium. For example, high-temperature crucibles made of iridium are used in the Czochralski process to produce oxide single-crystals (such as sapphires) for use in computer memory devices and in solid state lasers. The crystals, such as gadolinium gallium garnet and yttrium gallium garnet, are grown by melting pre-sintered charges of mixed oxides under oxidizing conditions at temperatures up to 2100 °C.

Iridium compounds are used as catalysts in the Cativa process for carbonylation of methanol to produce acetic acid.

The radioisotope iridium-192 is one of the two most important sources of energy for use in industrial γ-radiography for non-destructive testing of metals. Additionally, Ir is used as a source of gamma radiation for the treatment of cancer using brachytherapy, a form of radiotherapy where a sealed radioactive source is placed inside or next to the area requiring treatment. Specific treatments include high-dose-rate prostate brachytherapy, biliary duct brachytherapy, and intracavitary cervix brachytherapy.

In February 2019, medical scientists announced that iridium attached to albumin, creating a photosensitized molecule, can penetrate cancer cells and, after being irradiated with light (a process called photodynamic therapy), destroy the cancer cells.

Iridium is a good catalyst for the decomposition of hydrazine (into hot nitrogen and ammonia), and this is used in practice in low-thrust rocket engines; there are more details in the monopropellant rocket article.

An alloy of 90% platinum and 10% iridium was used in 1889 to construct the International Prototype Metre and kilogram mass, kept by the International Bureau of Weights and Measures near Paris. The meter bar was replaced as the definition of the fundamental unit of length in 1960 by a line in the atomic spectrum of krypton, but the kilogram prototype remained the international standard of mass until 20 May 2019, when the kilogram was redefined in terms of the Planck constant.

Iridium is often used as a coating for non-conductive materials in preparation for observation in scanning electron microscopes (SEM).
The addition of a 2-20 nm layer of iridium helps especially organic materials survive electron beam damage and reduces static charge build-up within the target area of the SEM beam's focal point. A coating of iridium also increases the signal to noise ratio associated with secondary electron emission which is essential to using SEMs for X-Ray spectrographic composition analysis. While other metals can be used for coating objects for SEM use, iridium is the preferred coating when samples will be studied with a wide variety of imaging parameters.

Iridium has been used in the radioisotope thermoelectric generators of unmanned spacecraft such as the "Voyager", "Viking", "Pioneer", "Cassini", "Galileo", and "New Horizons". Iridium was chosen to encapsulate the plutonium-238 fuel in the generator because it can withstand the operating temperatures of up to 2000 °C and for its great strength.

Another use concerns X-ray optics, especially X-ray telescopes. The mirrors of the Chandra X-ray Observatory are coated with a layer of iridium 60 nm thick. Iridium proved to be the best choice for reflecting X-rays after nickel, gold, and platinum were also tested. The iridium layer, which had to be smooth to within a few atoms, was applied by depositing iridium vapor under high vacuum on a base layer of chromium.

Iridium is used in particle physics for the production of antiprotons, a form of antimatter. Antiprotons are made by shooting a high-intensity proton beam at a "conversion target", which needs to be made from a very high density material. Although tungsten may be used instead, iridium has the advantage of better stability under the shock waves induced by the temperature rise due to the incident beam.
Carbon–hydrogen bond activation (C–H activation) is an area of research on reactions that cleave carbon–hydrogen bonds, which were traditionally regarded as unreactive. The first reported successes at activating C–H bonds in saturated hydrocarbons, published in 1982, used organometallic iridium complexes that undergo an oxidative addition with the hydrocarbon.

Iridium complexes are being investigated as catalysts for asymmetric hydrogenation. These catalysts have been used in the synthesis of natural products and able to hydrogenate certain difficult substrates, such as unfunctionalized alkenes, enantioselectively (generating only one of the two possible enantiomers).

Iridium forms a variety of complexes of fundamental interest in triplet harvesting.

Iridium–osmium alloys were used in fountain pen nib tips. The first major use of iridium was in 1834 in nibs mounted on gold. Since 1944, the famous Parker 51 fountain pen was fitted with a nib tipped by a ruthenium and iridium alloy (with 3.8% iridium). The tip material in modern fountain pens is still conventionally called "iridium", although there is seldom any iridium in it; other metals such as ruthenium, osmium, and tungsten have taken its place.

An iridium–platinum alloy was used for the touch holes or vent pieces of cannon. According to a report of the Paris Exhibition of 1867, one of the pieces being exhibited by Johnson and Matthey "has been used in a Withworth gun for more than 3000 rounds, and scarcely shows signs of wear yet. Those who know the constant trouble and expense which are occasioned by the wearing of the vent-pieces of cannon when in active service, will appreciate this important adaptation".

The pigment "iridium black", which consists of very finely divided iridium, is used for painting porcelain an intense black; it was said that "all other porcelain black colors appear grey by the side of it".

Iridium in bulk metallic form is not biologically important or hazardous to health due to its lack of reactivity with tissues; there are only about 20 parts per trillion of iridium in human tissue. Like most metals, finely divided iridium powder can be hazardous to handle, as it is an irritant and may ignite in air. Very little is known about the toxicity of iridium compounds, primarily because it is used so rarely that few people come in contact with it and those who do only with very small amounts. However, soluble salts, such as the iridium halides, could be hazardous due to elements other than iridium or due to iridium itself. At the same time, most iridium compounds are insoluble, which makes absorption into the body difficult.

A radioisotope of iridium, , is dangerous, like other radioactive isotopes. The only reported injuries related to iridium concern accidental exposure to radiation from used in brachytherapy. High-energy gamma radiation from can increase the risk of cancer. External exposure can cause burns, radiation poisoning, and death. Ingestion of Ir can burn the linings of the stomach and the intestines. Ir, Ir, and Ir tend to deposit in the liver, and can pose health hazards from both gamma and beta radiation.



</doc>
<doc id="14753" url="https://en.wikipedia.org/wiki?curid=14753" title="IOC (disambiguation)">
IOC (disambiguation)

IOC most commonly refers to the International Olympic Committee.

IOC may also refer to:



</doc>
<doc id="14761" url="https://en.wikipedia.org/wiki?curid=14761" title="International Phonetic Alphabet">
International Phonetic Alphabet

The International Phonetic Alphabet (IPA) is an alphabetic system of phonetic notation based primarily on the Latin alphabet. It was devised by the International Phonetic Association in the late 19th century as a standardized representation of the sounds of spoken language. The IPA is used by lexicographers, foreign language students and teachers, linguists, speech-language pathologists, singers, actors, constructed language creators and translators.

The IPA is designed to represent only those qualities of speech that are part of oral language: phones, phonemes, intonation and the separation of words and syllables. To represent additional qualities of speech, such as tooth gnashing, lisping, and sounds made with a cleft lip and cleft palate, an extended set of symbols, the extensions to the International Phonetic Alphabet, may be used.

IPA symbols are composed of one or more elements of two basic types, letters and diacritics. For example, the sound of the English letter may be transcribed in IPA with a single letter, , or with a letter plus diacritics, , depending on how precise one wishes to be. Often, slashes are used to signal broad or phonemic transcription; thus, is less specific than, and could refer to, either or , depending on the context and language.

Occasionally letters or diacritics are added, removed or modified by the International Phonetic Association. As of the most recent change in 2005, there are 107 letters, 52 diacritics and four prosodic marks in the IPA. These are shown in the current IPA chart, also posted below in this article and at the website of the IPA.

In 1886, a group of French and British language teachers, led by the French linguist Paul Passy, formed what would come to be known from 1897 onwards as the International Phonetic Association (in French, ""). Their original alphabet was based on a spelling reform for English known as the Romic alphabet, but in order to make it usable for other languages, the values of the symbols were allowed to vary from language to language. For example, the sound (the "sh" in "shoe") was originally represented with the letter in English, but with the digraph in French. However, in 1888, the alphabet was revised so as to be uniform across languages, thus providing the base for all future revisions. The idea of making the IPA was first suggested by Otto Jespersen in a letter to Paul Passy. It was developed by Alexander John Ellis, Henry Sweet, Daniel Jones, and Passy.

Since its creation, the IPA has undergone a number of revisions. After revisions and expansions from the 1890s to the 1940s, the IPA remained primarily unchanged until the Kiel Convention in 1989. A minor revision took place in 1993 with the addition of four letters for mid central vowels and the removal of letters for voiceless implosives. The alphabet was last revised in May 2005 with the addition of a letter for a labiodental flap. Apart from the addition and removal of symbols, changes to the IPA have consisted largely of renaming symbols and categories and in modifying typefaces.

Extensions to the International Phonetic Alphabet for speech pathology were created in 1990 and officially adopted by the International Clinical Phonetics and Linguistics Association in 1994.

The general principle of the IPA is to provide one letter for each distinctive sound (speech segment), although this practice is not followed if the sound itself is complex. This means that:

The alphabet is designed for transcribing sounds (phones), not phonemes, though it is used for phonemic transcription as well. A few letters that did not indicate specific sounds have been retired (, once used for the 'compound' tone of Swedish and Norwegian, and , once used for the moraic nasal of Japanese), though one remains: , used for the sj-sound of Swedish. When the IPA is used for phonemic transcription, the letter–sound correspondence can be rather loose. For example, and are used in the IPA "Handbook" for and .

Among the symbols of the IPA, 107 letters represent consonants and vowels, 31 diacritics are used to modify these, and 19 additional signs indicate suprasegmental qualities such as length, tone, stress, and intonation. These are organized into a chart; the chart displayed here is the
official chart as posted at the website of the IPA.

The letters chosen for the IPA are meant to harmonize with the Latin alphabet. For this reason, most letters are either Latin or Greek, or modifications thereof. Some letters are neither: for example, the letter denoting the glottal stop, , has the form of a dotless question mark, and derives originally from an apostrophe. A few letters, such as that of the voiced pharyngeal fricative, , were inspired by other writing systems (in this case, the Arabic letter "").
Despite its preference for harmonizing with the Latin script, the International Phonetic Association has occasionally admitted other letters. For example, before 1989, the IPA letters for click consonants were , , , and , all of which were derived either from existing IPA letters, or from Latin and Greek letters. However, except for , none of these letters were widely used among Khoisanists or Bantuists, and as a result they were replaced by the more widespread symbols , , , , and at the IPA Kiel Convention in 1989.

Although the IPA diacritics are fully featural, there is little systemicity in the letter forms. A retroflex articulation is consistently indicated with a right-swinging tail, as in , and implosion by a top hook, , but other pseudo-featural elements are due to haphazard derivation and coincidence. For example, all nasal consonants but uvular are based on the form : . However, the similarity between and is a historical accident; and are derived from ligatures of "gn" and "ng," and is an "ad hoc" imitation of .

Some of the new letters were ordinary Latin letters turned 180 degrees, such as (turned ). This was easily done in the era of mechanical typesetting, and had the advantage of not requiring the casting of special type for IPA symbols.
Full capital letters are not used as IPA symbols. They are, however, often used for archiphonemes and for natural classes of phonemes (that is, as wildcards). Such usage is not part of the IPA or even standardized, and may be ambiguous between authors, but it is commonly used in conjunction with the IPA. (The extIPA chart, for example, uses wildcards in its illustrations.) Capital letters are also basic to the Voice Quality Symbols sometimes used in conjunction with the IPA.

As wildcards, for {consonant} and for {vowel} are ubiquitous. Other common capital-letter symbols are for {tone/accent} (tonicity), for {nasal}, for {plosive}, for {fricative}, for {sibilant}, for {glide/semivowel}, for {liquid}, for {rhotic} or {resonant}, for {click}, for {open, front, back, close vowel} and for {labial, alveolar, post-alveolar/palatal, velar, uvular, pharyngeal, glottal consonant}, respectively, and for any sound. For example, the possible syllable shapes of Mandarin can be abstracted as ranging from (an atonic vowel) to (a consonant-glide-vowel-nasal syllable with tone). The letters can be modified with IPA diacritics, for example for {ejective}, for {implosive}, or for {prenasalized consonant}, for {nasal vowel}, for {voiced sibilant}, for {voiceless nasal}, or for {affricate}, for {palatalized consonant} and for {dental consonant}. In speech pathology, capital letters represent indeterminate sounds, and may be superscripted to indicate they are weakly articulated: e.g. is a weak indeterminate alveolar, a weak indeterminate velar.

Typical examples of archiphonemic use of capital letters are for the Turkish harmonic vowel set }, for the conflated flapped middle consonant of American English "writer" and "rider", and for the homorganic syllable-coda nasal of languages such as Spanish and Japanese (essentially equivalent to the wild-card usage of the letter).

, and have different meanings as Voice Quality Symbols, where they stand for "voice" (generally meaning secondary articulation rather than phonetic voicing), "falsetto" and "creak". They may take diacritics that indicate what kind of voice quality an utterance has, and may be used to extract a suprasegmental feature that occurs on all susceptible segments in a stretch of IPA. For instance, the transcription of Scottish Gaelic 'cat' and 'cats' (Islay dialect) can be made more economical by extracting the suprasegmental labialization of the words: and .

The International Phonetic Alphabet is based on the Latin alphabet, using as few non-Latin forms as possible. The Association created the IPA so that the sound values of most consonant letters taken from the Latin alphabet would correspond to "international usage". Hence, the letters , , , (hard) , (non-silent) , (unaspirated) , , , , (unaspirated) , (voiceless) , (unaspirated) , , , and have the values used in English; and the vowel letters from the Latin alphabet (, , , , ) correspond to the (long) sound values of Latin: is like the vowel in "machne", is as in "rle", etc. Other letters may differ from English, but are used with these values in other European languages, such as , , and .

This inventory was extended by using small-capital and cursive forms, diacritics and rotation. There are also several symbols derived or taken from the Greek alphabet, though the sound values may differ. For example, is a vowel in Greek, but an only indirectly related consonant in the IPA. For most of these, subtly different glyph shapes have been devised for the IPA, namely , , , , , , and , which are encoded in Unicode separately from their parent Greek letters, though one of them – – is not, while Greek and are generally used for Latin and .

The sound values of modified Latin letters can often be derived from those of the original letters. For example, letters with a rightward-facing hook at the bottom represent retroflex consonants; and small capital letters usually represent uvular consonants. Apart from the fact that certain kinds of modification to the shape of a letter generally correspond to certain kinds of modification to the sound represented, there is no way to deduce the sound represented by a symbol from its shape (as for example in Visible Speech) nor even any systematic relation between signs and the sounds they represent (as in Hangul).

Beyond the letters themselves, there are a variety of secondary symbols which aid in transcription. Diacritic marks can be combined with IPA letters to transcribe modified phonetic values or secondary articulations. There are also special symbols for suprasegmental features such as stress and tone that are often employed.

There are two principal types of brackets used to set off (delimit) IPA transcriptions:

Other conventions are less commonly seen:

All three of the above are provided by the IPA "Handbook". The following are not, but may be seen in IPA transcription:


IPA letters have cursive forms designed for use in manuscripts and when taking field notes.

In the early stages of the alphabet, the typographic variants of "g", opentail () and looptail (), represented different values, but are now regarded as equivalents. Opentail has always represented a voiced velar plosive, while was distinguished from and represented a voiced velar fricative from 1895 to 1900. Subsequently, represented the fricative, until 1931 when it was replaced again by .

In 1948, the Council of the Association recognized and as typographic equivalents, and this decision was reaffirmed in 1993. While the 1949 "Principles of the International Phonetic Association" recommended the use of for a velar plosive and for an advanced one for languages where it is preferable to distinguish the two, such as Russian, this practice never caught on. The 1999 "Handbook of the International Phonetic Association", the successor to the "Principles", abandoned the recommendation and acknowledged both shapes as acceptable variants.

The International Phonetic Alphabet is occasionally modified by the Association. After each modification, the Association provides an updated simplified presentation of the alphabet in the form of a chart. (See History of the IPA.) Not all aspects of the alphabet can be accommodated in a chart of the size published by the IPA. The alveolo-palatal and epiglottal consonants, for example, are not included in the consonant chart for reasons of space rather than of theory (two additional columns would be required, one between the retroflex and palatal columns and the other between the pharyngeal and glottal columns), and the lateral flap would require an additional row for that single consonant, so they are listed instead under the catchall block of "other symbols". The indefinitely large number of tone letters would make a full accounting impractical even on a larger page, and only a few examples are shown.

The procedure for modifying the alphabet or the chart is to propose the change in the "Journal of the IPA." (See, for example, August 2008 on an open central unrounded vowel and August 2011 on central approximants.) Reactions to the proposal may be published in the same or subsequent issues of the Journal (as in August 2009 on the open central vowel). A formal proposal is then put to the Council of the IPA – which is elected by the membership – for further discussion and a formal vote.

Only changes to the alphabet or chart that have been approved by the Council can be considered part of the official IPA. Nonetheless, many users of the alphabet, including the leadership of the Association itself, make personal changes or additions in their own practice, either for convenience in the broad phonetic or phonemic transcription of a particular language (see "Illustrations of the IPA" for individual languages in the "Handbook", which for example may use as a phonemic symbol for what is phonetically realized as ), or because they object to some aspect of the official version.

Of more than 160 IPA symbols, relatively few will be used to transcribe speech in any one language, with various levels of precision. A precise phonetic transcription, in which sounds are specified in detail, is known as a "narrow transcription". A coarser transcription with less detail is called a "broad transcription." Both are relative terms, and both are generally enclosed in square brackets. Broad phonetic transcriptions may restrict themselves to easily heard details, or only to details that are relevant to the discussion at hand, and may differ little if at all from phonemic transcriptions, but they make no theoretical claim that all the distinctions transcribed are necessarily meaningful in the language.
For example, the English word "little" may be transcribed broadly as , approximately describing many pronunciations. A narrower transcription may focus on individual or dialectical details: in General American, in Cockney, or in Southern US English.

Phonemic transcriptions, which express the conceptual counterparts of spoken sounds, are usually enclosed in slashes (/ /) and tend to use simpler letters with few diacritics. The choice of IPA letters may reflect theoretical claims of how speakers conceptualize sounds as phonemes, or they may be merely a convenience for typesetting. Phonemic approximations between slashes do not have absolute sound values. For instance, in English, either the vowel of "pick" or the vowel of "peak" may be transcribed as , so that "pick", "peak" would be transcribed as or as ; and neither is identical to the vowel of the French "" which is also generally transcribed . By contrast, a narrow phonetic transcription of "pick", "peak", "pique" could be: , , .

Although IPA is popular for transcription by linguists, American linguists often alternate use of the IPA with Americanist phonetic notation or use the IPA together with some nonstandard symbols, for reasons including reducing the error rate on reading handwritten transcriptions or avoiding perceived awkwardness of IPA in some situations. The exact practice may vary somewhat between languages and even individual researchers, so authors are generally encouraged to include a chart or other explanation of their choices.

Some language study programs use the IPA to teach pronunciation. For example, in Russia (and earlier in the Soviet Union) and mainland China, textbooks for children and adults for studying English and French consistently use the IPA. English teachers and textbooks in Taiwan tend to use the Kenyon and Knott system, a slight typographical variant of the IPA first used in the 1944 "Pronouncing Dictionary of American English".

Many British dictionaries, including the Oxford English Dictionary and some learner's dictionaries such as the "Oxford Advanced Learner's Dictionary" and the "Cambridge Advanced Learner's Dictionary", now use the International Phonetic Alphabet to represent the pronunciation of words. However, most American (and some British) volumes use one of a variety of pronunciation respelling systems, intended to be more comfortable for readers of English. For example, the respelling systems in many American dictionaries (such as "Merriam-Webster") use for IPA and for IPA , reflecting common representations of those sounds in written English, using only letters of the English Roman alphabet and variations of them. (In IPA, represents the sound of the French (as in ""), and represents the pair of sounds in "graopper".)
The IPA is also not universal among dictionaries in languages other than English. Monolingual dictionaries of languages with generally phonemic orthographies generally do not bother with indicating the pronunciation of most words, and tend to use respelling systems for words with unexpected pronunciations. Dictionaries produced in Israel use the IPA rarely and sometimes use the Hebrew alphabet for transcription of foreign words. Monolingual Hebrew dictionaries use pronunciation respelling for words with unusual spelling; for example, the "Even-Shoshan Dictionary" respells as because this word uses kamatz katan. Bilingual dictionaries that translate from foreign languages into Russian usually employ the IPA, but monolingual Russian dictionaries occasionally use pronunciation respelling for foreign words; for example, Sergey Ozhegov's dictionary adds нэ́ in brackets for the French word пенсне ("") to indicate that the final е does not iotate the preceding н.

The IPA is more common in bilingual dictionaries, but there are exceptions here too. Mass-market bilingual Czech dictionaries, for instance, tend to use the IPA only for sounds not found in the Czech language.

IPA letters have been incorporated into the alphabets of various languages, notably via the Africa Alphabet in many sub-Saharan languages such as Hausa, Fula, Akan, Gbe languages, Manding languages, Lingala, etc. This has created the need for capital variants. For example, Kabiyè of northern Togo has Ɖ ɖ, Ŋ ŋ, Ɣ ɣ, Ɔ ɔ, Ɛ ɛ, Ʋ ʋ. These, and others, are supported by Unicode, but appear in Latin ranges other than the IPA extensions.

In the IPA itself, however, only lower-case letters are used. The 1949 edition of the IPA handbook indicated that an asterisk may be prefixed to indicate that a word is a proper name, but this convention was not included in the 1999 "Handbook".

IPA has widespread use among classical singers during preparation as they are frequently required to sing in a variety of foreign languages, in addition to being taught by vocal coach in order to perfect the diction of their students and to globally improve tone quality and tuning. Opera librettos are authoritatively transcribed in IPA, such as Nico Castel's volumes and Timothy Cheek's book "Singing in Czech". Opera singers' ability to read IPA was used by the site "Visual Thesaurus", which employed several opera singers "to make recordings for the 150,000 words and phrases in VT's lexical database ... for their vocal stamina, attention to the details of enunciation, and most of all, knowledge of IPA".

The International Phonetic Association organizes the letters of the IPA into three categories: pulmonic consonants, non-pulmonic consonants, and vowels.

Pulmonic consonant letters are arranged singly or in pairs of voiceless (tenuis) and voiced sounds, with these then grouped in columns from front (labial) sounds on the left to back (glottal) sounds on the right. In official publications by the IPA, two columns are omitted to save space, with the letters listed among 'other symbols', and with the remaining consonants arranged in rows from full closure (occlusives: stops and nasals), to brief closure (vibrants: trills and taps), to partial closure (fricatives) and minimal closure (approximants), again with a row left out to save space. In the table below, a slightly different arrangement is made: All pulmonic consonants are included in the pulmonic-consonant table, and the vibrants and laterals are separated out so that the rows reflect the common lenition pathway of "stop → fricative → approximant," as well as the fact that several letters pull double duty as both fricative and approximant; affricates may be created by joining stops and fricatives from adjacent cells. Shaded cells represent articulations that are judged to be impossible.

Vowel letters are also grouped in pairs—of unrounded and rounded vowel sounds—with these pairs also arranged from front on the left to back on the right, and from maximal closure at top to minimal closure at bottom. No vowel letters are omitted from the chart, though in the past some of the mid central vowels were listed among the 'other symbols'.

Each character, letter or diacritic, is assigned a number, to prevent confusion between similar characters (such as and , and , or and ) in such situations as the printing of manuscripts. The categories of sounds are assigned different ranges of numbers.

A pulmonic consonant is a consonant made by obstructing the glottis (the space between the vocal cords) or oral cavity (the mouth) and either simultaneously or subsequently letting out air from the lungs. Pulmonic consonants make up the majority of consonants in the IPA, as well as in human language. All consonants in the English language fall into this category.

The pulmonic consonant table, which includes most consonants, is arranged in rows that designate manner of articulation, meaning how the consonant is produced, and columns that designate place of articulation, meaning where in the vocal tract the consonant is produced. The main chart includes only consonants with a single place of articulation.
Notes

Non-pulmonic consonants are sounds whose airflow is not dependent on the lungs. These include clicks (found in the Khoisan languages and some neighboring Bantu languages of Africa), implosives (found in languages such as Sindhi, Hausa, Swahili and Vietnamese), and ejectives (found in many Amerindian and Caucasian languages).
Notes

Affricates and co-articulated stops are represented by two letters joined by a tie bar, either above or below the letters. The six most common affricates are optionally represented by ligatures, though this is no longer official IPA usage, because a great number of ligatures would be required to represent all affricates this way. Alternatively, a superscript notation for a consonant release is sometimes used to transcribe affricates, for example for , paralleling ~ . The letters for the palatal plosives and are often used as a convenience for and or similar affricates, even in official IPA publications, so they must be interpreted with care.
Note

Co-articulated consonants are sounds that involve two simultaneous places of articulation (are pronounced using two parts of the vocal tract). In English, the in "went" is a coarticulated consonant, being pronounced by rounding the lips and raising the back of the tongue. Similar sounds are and . In some languages, plosives can be double-articulated, for example in the name of Laurent Gbagbo. 
Notes

The IPA defines a vowel as a sound which occurs at a syllable center. Below is a chart depicting the vowels of the IPA. The IPA maps the vowels according to the position of the tongue.
The vertical axis of the chart is mapped by vowel height. Vowels pronounced with the tongue lowered are at the bottom, and vowels pronounced with the tongue raised are at the top. For example, (the first vowel in "father") is at the bottom because the tongue is lowered in this position. However, (the vowel in "meet") is at the top because the sound is said with the tongue raised to the roof of the mouth.

In a similar fashion, the horizontal axis of the chart is determined by vowel backness. Vowels with the tongue moved towards the front of the mouth (such as , the vowel in "met") are to the left in the chart, while those in which it is moved to the back (such as , the vowel in "but") are placed to the right in the chart.

In places where vowels are paired, the right represents a rounded vowel (in which the lips are rounded) while the left is its unrounded counterpart.

Diphthongs are typically specified with a non-syllabic diacritic, as in or , or with a superscript for the on- or off-glide, as in or . Sometimes a tie bar is used, especially if it is difficult to tell if the diphthong is characterized by an on-glide, an off-glide or is variable: .

Notes

Diacritics are used for phonetic detail. They are added to IPA letters to indicate a modification or specification of that letter's normal pronunciation.

By being made superscript, any IPA letter may function as a diacritic, conferring elements of its articulation to the base letter. (See secondary articulation for a list of superscript IPA letters supported by Unicode.) Those superscript letters listed below are specifically provided for by the IPA; others include ( with fricative release), ( with affricate onset), (prenasalized ), ( with breathy voice), (glottalized ), ( with a flavor of ), ( with diphthongization), (compressed ). Superscript diacritics placed after a letter are ambiguous between simultaneous modification of the sound and phonetic detail at the end of the sound. For example, labialized may mean either simultaneous and or else with a labialized release. Superscript diacritics placed before a letter, on the other hand, normally indicate a modification of the onset of the sound ( glottalized , with a glottal onset).

Notes

Subdiacritics (diacritics normally placed below a letter) may be moved above a letter to avoid conflict with a descender, as in voiceless . The raising and lowering diacritics have optional forms , that avoid descenders.

The state of the glottis can be finely transcribed with diacritics. A series of alveolar plosives ranging from an open to a closed glottis phonation are:

Additional diacritics are provided by the Extensions to the IPA for speech pathology.

These symbols describe the features of a language above the level of individual consonants and vowels, such as prosody, pitch, length and stress, which often operate at the syllable, word or phrase level: that is, elements such as the intensity, tone and gemination of the sounds of a language, as well as the rhythm and intonation of speech. Although most of these symbols indicate distinctions that are phonemic at the word level, symbols also exist for intonation on a level greater than that of the word. Various ligatures of pitch/tone letters and diacritics are provided for by the Kiel convention and used in the IPA "Handbook" despite not being found in the summary of the IPA alphabet found on the one-page chart.

Officially, the stress marks appear before the stressed syllable, and thus mark the syllable boundary as well as stress (though the syllable boundary may still be explicitly marked with a period). Occasionally the stress mark is placed immediately before the nucleus of the syllable, after any consonantal onset. In such transcriptions, the stress mark does not mark a syllable boundary. The primary stress mark may be doubled for extra stress (such as prosodic stress). The secondary stress mark is sometimes seen doubled for extra-weak stress, but this convention has not been adopted by the IPA.

There are three boundary markers: for a syllable break, for a minor prosodic break and for a major prosodic break. The tags 'minor' and 'major' are intentionally ambiguous. Depending on need, 'minor' may vary from a foot break to a break in list-intonation to a continuing–prosodic-unit boundary (equivalent to a comma), and while 'major' is often any intonation break, it may be restricted to a final–prosodic-unit boundary (equivalent to a period). The 'major' symbol may also be doubled, , for a stronger break. 

Although not part of the IPA, the following additional boundary markers are often used in conjunction with the IPA: for a mora or mora boundary, for a syllable or syllable boundary, for a word boundary, for a phrase or intermediate boundary and for a prosodic boundary. For example, C# is a word-final consonant, %V a post-pausa vowel, and T% an IU-final tone (edge tone).

Phonetic pitch and phonemic tone may be indicated by either diacritics placed over the nucleus of the syllable or by Chao tone letters placed before or after the word or syllable. There are three graphic variants of the tone letters: with or without a stave, and facing left or facing right from a stave. Theoretically therefore there are seven ways to transcribe pitch/tone in the IPA, though in practice only , , , and obsolete (for high pitch/tone) are seen. Only left-facing staved letters and a few representative combinations are shown in the summary on the "Chart", and in practice it is currently more common for tone letters to occur after the syllable/word than before, as in the Chao tradition. Placement before the word is a carry-over from the pre-Kiel IPA convention, as is still the case for the stress and upstep/downstep marks. The IPA endorses the Chao tradition of using the left-facing tone letters, , for broad or underlying tone, and the right-facing letters, , for surface tone or phonetic detail, as in tone sandhi. In the Portuguese illustration in the 1999 "Handbook", tone letters are placed before a word or syllable to indicate prosodic pitch (equivalent to global rise and global fall, but allowing more than a two-way contrast), and in the Cantonese illustration they are placed after a word/syllable to indicate lexical tone. Theoretically therefore prosodic pitch and lexical tone could be simultaneously transcribed in a single text, though this is not a formalized distinction. The staveless letters are effectively obsolete and are not supported by Unicode. They were not widely accepted even before 1989 when they were the sole option for indicating pitch in the IPA, and they only ever supported three pitch levels and a few contours. 

Rising and falling pitch, as in contour tones, are indicated by combining the pitch diacritics and letters in the table, such as grave plus acute for rising and acute plus grave for falling . Only a four other combinations of diacritics are supported, and only across three levels (high, mid, low), despite the diacritics supporting five levels of pitch. The four additional explicitly approved rising and falling diacritic combinations are high/mid rising , low rising , high falling , and low/mid falling 

The Chao tone letters, on the other hand, may be combined in any pattern, and are therefore used for more complex contours and finer distinctions than the diacritics allow, such as mid-rising , extra-high falling , etc. There are 25 such possibilities.

For more complex, peaking and dipping tones, one may combine three or four tone diacritics in any permutation, though in practice only generic peaking and dipping combinations are used. For finer detail, Chao tone letters are again required (, etc., for 120 possible pitch contours) The correspondence between tone diacritics and tone letters therefore breaks down once they start combining.

A work-around for diacritics sometimes seen when a language has more than one phonemic rising or falling tone, and the author wishes to avoid the poorly legible diacritics but does not wish to employ tone letters, is to restrict generic rising and falling to the higher-pitched of the rising and falling tones, say and , and to resurrect retired IPA subscript diacritics and for the lower-pitched rising and falling tones, say and . When a language has four or six level tones, the two mid tones are sometimes transcribed as high-mid (non-standard) and low-mid . Non-standard is occasionally seen combined with acute and grave diacritcs or the macron. 

Chao tone letters generally appear after each syllable, for a language with syllable tone (), or after the phonological word, for a language with word tone (). Placement before the word or syllable (, ) is officially supported but less common.

IPA diacritics may be doubled to indicate an extra degree of the feature indicated. This is a productive process, but apart from extra-high and extra-low tones being marked by doubled high- and low-tone diacritics, and the major prosodic break being marked as a double minor break , it is not specifically regulated by the IPA. (Note that transcription marks are similar: double slashes indicate extra (morpho)-phonemic, double square brackets especially precise, and double parentheses especially unintelligible.) 

For example, the stress mark may be doubled to indicate an extra degree of stress, such as prosodic stress in English. An example in French, with a single stress mark for normal prosodic stress at the end of each prosodic unit (marked as a minor prosodic break), and a double stress mark for contrastive/emphatic stress: "." Similarly, a doubled secondary stress mark is commonly used for tertiary (extra-light) stress.

Length is commonly extended by repeating the length mark, as in English "shhh!" , or for "overlong" segments in Estonian:
(Normally additional degrees of length are handled by the extra-short or half-long diacritics, but in the Estonian examples, the first two cases are analyzed as simply short and long.)

Occasionally other diacritics are doubled:

The IPA once had parallel symbols from alternative proposals, but in most cases eventually settled on one for each sound. The rejected symbols are now considered obsolete. An example is the vowel letter , rejected in favor of . Letters for affricates and sounds with inherent secondary articulation have also been mostly rejected, with the idea that such features should be indicated with tie bars or diacritics: for is one. In addition, the rare voiceless implosives, , have been dropped and are now usually written . A retired set of click letters, , is still sometimes seen, as the official pipe letters may cause problems with legibility, especially when used with brackets ([ ] or / /), the letter , or the prosodic marks (for this reason, some publications which use the current IPA pipe letters disallow IPA brackets).

Individual non-IPA letters may find their way into publications that otherwise use the standard IPA. This is especially common with:

In addition, there are typewriter substitutions for when IPA support is not available, such as capital for .

The "Extensions to the IPA", often abbreviated as "extIPA" and sometimes called "Extended IPA", are symbols whose original purpose was to accurately transcribe disordered speech. At the Kiel Convention in 1989, a group of linguists drew up the initial extensions, which were based on the previous work of the PRDS (Phonetic Representation of Disordered Speech) Group in the early 1980s. The extensions were first published in 1990, then modified, and published again in 1994 in the "Journal of the International Phonetic Association", when they were officially adopted by the ICPLA. While the original purpose was to transcribe disordered speech, linguists have used the extensions to designate a number of unique sounds within standard communication, such as hushing, gnashing teeth, and smacking lips.

In addition to the Extensions to the IPA there are the conventions of the Voice Quality Symbols, which besides the concept of voice quality in phonetics include a number of symbols for additional airstream mechanisms and secondary articulations.

The blank cells on the IPA chart can be filled without too much difficulty if the need arises. Some "ad hoc" letters have appeared in the literature for the retroflex lateral flap and the retroflex clicks (having the expected forms of and plus a retroflex tail; the analogous for a retroflex implosive is even mentioned in the IPA "Handbook"), the voiceless lateral fricatives (now provided for by the extIPA), the epiglottal trill (arguably covered by the generally-trilled epiglottal "fricatives" ), the labiodental plosives ( in some old Bantuist texts) and the near-close central vowels ( in some publications). Diacritics can duplicate some of those, such as for the lateral flap, for the labiodental plosives and for the central vowels, and are able to fill in most of the remainder of the charts. If a sound cannot be transcribed, an asterisk may be used, either as a letter or as a diacritic (as in sometimes seen for the Korean "fortis" velar).

Representations of consonant sounds outside of the core set are created by adding diacritics to letters with similar sound values. The Spanish bilabial and dental approximants are commonly written as lowered fricatives, and respectively. Similarly, voiced lateral fricatives would be written as raised lateral approximants, . A few languages such as Banda have a bilabial flap as the preferred allophone of what is elsewhere a labiodental flap. It has been suggested that this be written with the labiodental flap letter and the advanced diacritic, .

Similarly, a labiodental trill would be written (bilabial trill and the dental sign), and labiodental stops rather than with the "ad hoc" letters sometimes found in the literature. Other taps can be written as extra-short plosives or laterals, e.g. , though in some cases the diacritic would need to be written below the letter. A retroflex trill can be written as a retracted , just as non-subapical retroflex fricatives sometimes are. The remaining consonants, the uvular laterals ( "etc.") and the palatal trill, while not strictly impossible, are very difficult to pronounce and are unlikely to occur even as allophones in the world's languages.

The vowels are similarly manageable by using diacritics for raising, lowering, fronting, backing, centering, and mid-centering. For example, the unrounded equivalent of can be transcribed as mid-centered , and the rounded equivalent of as raised or lowered (though for those who conceive of vowel space as a triangle, simple already is the rounded equivalent of ). True mid vowels are lowered or raised , while centered and (or, less commonly, ) are near-close and open central vowels, respectively. The only known vowels that cannot be represented in this scheme are vowels with unexpected roundedness, which would require a dedicated diacritic, such as protruded and compressed (or and ).

An IPA symbol is often distinguished from the sound it is intended to represent, since there is not necessarily a one-to-one correspondence between letter and sound in broad transcription, making articulatory descriptions such as "mid front rounded vowel" or "voiced velar stop" unreliable. While the "Handbook of the International Phonetic Association" states that no official names exist for its symbols, it admits the presence of one or two common names for each. The symbols also have nonce names in the Unicode standard. In some cases, the Unicode names and the IPA names do not agree. For example, IPA calls "epsilon", but Unicode calls it "small letter open E".

The traditional names of the Latin and Greek letters are usually used for unmodified letters. Letters which are not directly derived from these alphabets, such as , may have a variety of names, sometimes based on the appearance of the symbol or on the sound that it represents. In Unicode, some of the letters of Greek origin have Latin forms for use in IPA; the others use the letters from the Greek section.

For diacritics, there are two methods of naming. For traditional diacritics, the IPA notes the name in a well known language; for example, is "acute", based on the name of the diacritic in English and French. Non-traditional diacritics are often named after objects they resemble, so is called "bridge".

Geoffrey Pullum and William Ladusaw list a variety of names in use for IPA symbols, both current and retired, in addition to names of many other non-IPA phonetic symbols in their "Phonetic Symbol Guide".

IPA typeface support is increasing, and nearly complete IPA support with good diacritic rendering is provided by a few typefaces that come pre-installed with various computer operating systems, such as Calibri, as well as some freely available but commercial fonts such as Brill, but most pre-installed fonts, such as the ubiquitous Arial, Noto Sans and Times New Roman, are neither complete nor render many diacritics properly.

Typefaces that provide full IPA support, properly render diacritics and are freely available include:
Web browsers generally do not need any configuration to display IPA characters, provided that a typeface capable of doing so is available to the operating system.

Several systems have been developed that map the IPA symbols to ASCII characters. Notable systems include SAMPA and X-SAMPA. The usage of mapping systems in on-line text has to some extent been adopted in the context input methods, allowing convenient keying of IPA characters that would be otherwise unavailable on standard keyboard layouts.

Online IPA keyboard utilities are available, and they cover the complete range of IPA symbols and diacritics. In April 2019, Google's Gboard for Android and iOS added an IPA keyboard to its platform.




</doc>
<doc id="14762" url="https://en.wikipedia.org/wiki?curid=14762" title="Inspector Morse">
Inspector Morse

Detective Chief Inspector Endeavour Morse, GM, is the eponymous fictional character in the series of detective novels by British author Colin Dexter. On television, he appears in the 33-episode drama series "Inspector Morse" (1987–2000), in which John Thaw played the character, as well as the (2012–) prequel series "Endeavour", portrayed by Shaun Evans. The older Morse is a senior CID (Criminal Investigation Department) officer with the Thames Valley Police in Oxford in England and, in the prequel, Morse is a young detective constable rising through the ranks with the Oxford City Police and in later series the Thames Valley Police.

Morse presents, to some, a reasonably sympathetic personality, despite his sullen and snobbish temperament, with a classic Jaguar car (a Lancia in the early novels), a thirst for English real ale, and a love of classical music (especially opera and Wagner), poetry, art and cryptic crossword puzzles. In his later career he is usually assisted by Sergeant Robbie Lewis. Morse's partnership and formal friendship with Lewis is fundamental to the series.

Morse's father was a taxi driver, and Morse likes to explain the origin of his additional private income by saying that he "used to drive the Aga Khan". In the episode "Cherubim and Seraphim", it is revealed that Morse's parents divorced when he was 12. He remained with his mother until her death three years later, upon which he had to return to his father. Morse had a dreadful relationship with his stepmother Gwen. He claims that he only read poetry to annoy her, and that her petty bullying almost drove him to suicide. He has a half-sister named Joyce with whom he is on better terms. Morse was devastated when Joyce's daughter Marilyn took her own life.

Morse prefers to use only his surname, and is generally evasive when asked about his first name, sometimes joking that it is "Inspector". In "The Wench Is Dead" it was stated that his initial was E. At the end of "Death Is Now My Neighbour", it is revealed to be "Endeavour". Two-thirds of the way through the television episode based on the book, he gives the cryptic clue "My whole life's effort has revolved around Eve". In the series, it is noted that Morse's reluctance to use his given name led to his receiving the nickname "Pagan" while at Stamford School (which Colin Dexter, the author of the Morse novels, attended). In the novels, Morse's first name came from the vessel HMS "Endeavour"; his mother was a member of the Religious Society of Friends (Quakers) who have a tradition of "virtue names", and his father admired Captain James Cook.

Dexter was a fan of cryptic crosswords and named Morse after champion setter Jeremy Morse, one of Dexter's arch-rivals in writing crossword clues. Dexter used to walk along the bank of the River Thames at Oxford, opposite the boathouse belonging to 22nd Oxford Sea Scout Group; the building is named "T.S. Endeavour".

Although details of Morse's education are deliberately kept vague, it is hinted that he won a scholarship to study at St John's College, Oxford. He lost the scholarship as the result of poor academic performance stemming from a failed love affair, which is mentioned in the second episode of the third series, "The Last Enemy", and recounted in detail in the novel "The Riddle of the Third Mile", Chapter 7. Further details are revealed piece-by-piece in the prequel series. He often reflects on such renowned scholars as A. E. Housman who, like himself, failed to get an academic degree from Oxford.

After university, he entered the army on National Service. This included serving in West Germany with the Royal Corps of Signals as a cipher clerk. Upon leaving, he joined the police at Carshall-Newtown, before being posted to Oxford with the Oxford City Police. He was awarded the George Medal in the last episode of "Endeavour" Series 4.

Morse is ostensibly the embodiment of white, male, middle-class Englishness, with a set of prejudices and assumptions to match (even though as the son of a taxi driver his background was thoroughly working class). As a result, he may be considered a late example of the gentleman detective, a staple of British detective fiction. This is in sharp contrast to the working-class lifestyle of his assistant Lewis (named after another rival clue-writer Mrs. B. Lewis); in the novels, Lewis is Welsh , but in the TV series this is altered to a Tyneside (Geordie) background, appropriately for the actor Kevin Whately. Morse is in his forties at the start of the books ("Service of all the Dead", Chapter Six: "… a bachelor still, forty-seven years old …"), and Lewis slightly younger (eg "The Secret of Annexe 3", Chapter Twenty-Six: "a slightly younger man – another policeman, and one also in plain clothes"). John Thaw was 45 at the beginning of shooting the TV series and Kevin Whately was 36.

Morse's relationships with authority, the establishment, bastions of power and the status quo, are markedly ambiguous, as are some of his relations with women. He is frequently portrayed as patronising female characters, and once stereotyped the female sex as not naturally prone to crime, being caring and non-violent, but also often empathises with women. He is not shy to show his liking for attractive women and often dates those involved in cases. Indeed a woman he falls in love with sometimes turns out to be the culprit.

Morse is highly intelligent. He is a crossword addict and dislikes grammatical and spelling errors; in every personal or private document that he receives, he manages to point out at least one mistake. He claims that his approach to crime-solving is deductive, and one of his key tenets is that "there is a 50 per cent chance that the person who finds the body is the murderer". Morse uses immense intuition and his fantastic memory to apprehend the perpetrator.

Among Morse's conservative tastes are that he likes to drink real ale and whisky, and in the early novels, drives a Lancia. In the television and radio productions, this is altered to a suitably British classic Jaguar Mark 2. His favourite music is opera, which is echoed in the soundtracks to the television series, along with original music by Barrington Pheloung.

Morse was portrayed as being an atheist.

The novels in the series are:

Inspector Morse also appears in several stories in Dexter's short story collection, "Morse's Greatest Mystery and Other Stories" (1993, expanded edition 1994).

The Inspector Morse novels were made into a TV series (also called "Inspector Morse") for the British commercial TV network ITV. The series was made by Zenith Productions for Central (a company later acquired by Carlton) and comprises 33 two-hour episodes (100 minutes excluding commercials)—20 more episodes than there are novels—produced between 1987 and 2000. The last episode was adapted from the final novel "The Remorseful Day", in which Morse dies.

A spin-off series - similarly comprising 33 two-hour episodes and based on the television incarnation of Lewis - was titled "Lewis"; it first aired in 2006 and last showed in 2015.

In August 2011, ITV announced plans to film a prequel drama called "Endeavour", with author Colin Dexter's participation. English actor Shaun Evans was cast as a young Morse in his early career. The drama was broadcast on 2 January 2012 on ITV 1. Four new episodes were televised from 14 April 2013, showing Morse's early cases working for DI Fred Thursday and with Jim Strange, his later boss, and pathologist Max De Bryn. A second series of four episodes followed, screening in March and April 2014. In January 2016, the third series aired, also containing four episodes. A fourth series was aired, with four episodes, in January 2017. Filming of a fifth series of six episodes began in Spring 2017 with the first episode aired on 4 February 2018. In 2019 the sixth series aired, which comprises four 1 hour 30 minute episodes. A seventh series of three episodes was filmed in late 2019, and in August 2019 ITV announced that the series has been recommissioned for an eighth series. Morse was voted number two on the top 25 list in ITV's Britain's Favourite Detective first broadcast on 30 August 2020. 

An adaptation by Melville Jones of Last Bus to Woodstock featured in BBC Radio 4's Saturday Night Theatre series in June 1985, with Andrew Burt as Morse and Christopher Douglas as Lewis.

In the 1990s, an occasional BBC Radio 4 series (for "The Saturday Play") was made starring the voices of John Shrapnel as Morse and Robert Glenister as Lewis. The series was written by Guy Meredith and directed by Ned Chaillet. Episodes included: "The Wench is Dead" (23 March 1992); "Last Seen Wearing" (28 May 1994); and "The Silent World of Nicholas Quinn" (10 February 1996).

An Inspector Morse stage play appeared in 2010, written by Alma Cullen (writer of four Morse screenplays for ITV). The part of Morse was played by Colin Baker. The play, entitled "Morse—House of Ghosts", saw DCI Morse looking to his past, when an old acquaintance becomes the lead suspect in a murder case that involves the on-stage death of a young actress. The play toured the UK from August to December 2010. It was broadcast by BBC Radio 4 on 25 March 2017 with Neil Pearson playing Morse and Lee Ingleby playing Lewis.



</doc>
<doc id="14763" url="https://en.wikipedia.org/wiki?curid=14763" title="History of the Isle of Man">
History of the Isle of Man

The Isle of Man had become separated from Great Britain and Ireland by 6500 BC. It appears that colonisation took place by sea sometime during the Mesolithic era (about 6500 BC). The island has been visited by various raiders and trading peoples over the years. After being settled by people from Ireland in the first millennium, the Isle of Man was converted to Christianity and then suffered raids by Vikings from Norway. After becoming subject to Norwegian suzerainty as part of the Kingdom of Mann and the Isles, the Isle of Man later became a possession of the Scottish and then the English crowns. In 1603 during the union of the crowns of England and Scotland through James VI and I

Since 1866, the Isle of Man has been a Crown Dependency and has democratic self-government.

The Isle of Man effectively became an island around 8,500 years ago at around the time when rising sea levels caused by the melting glaciers cut Mesolithic Britain off from continental Europe for the last time. A land bridge had earlier existed between the Isle of Man and Cumbria, but the location and opening of the land bridge remain poorly understood.

The earliest traces of people on the Isle of Man date back to the Mesolithic Period, also known as the Middle Stone Age. The first residents lived in small natural shelters, hunting, gathering and fishing for their food. They used small tools made of flint or bone, examples of which have been found near the coast. Representatives of these artifacts are kept at the Manx National Heritage museum.

The Neolithic Period marked the coming of farming, improved stone tools and pottery. During this period megalithic monuments began to appear around the island. Examples are found at Cashtal yn Ard near Maughold, King Orry's Grave in Laxey, Meayll Circle near Cregneash, and Ballaharra Stones in St John's. The builders of the megaliths were not the only culture during this time; there are also remains of the local Ronaldsway culture (lasting from the late Neolithic into the Bronze Age).

The Iron Age marked the beginning of Celtic cultural influence. Large hill forts appeared on hill summits and smaller promontory forts along the coastal cliffs, whilst large timber-framed roundhouses were built.

It is likely that the first Celts to inhabit the Island were Brythonic tribes from mainland Britain. The secular history of the Isle of Man during the Brythonic period remains mysterious. It is not known if the Romans ever made a landing on the island and if they did, little evidence has been discovered. There is evidence for contact with Roman Britain as an amphora was discovered at the settlement on the South Barrule; it is hypothesised this may have been trade goods or plunder. It has been speculated that the island may have become a haven for Druids and other refugees from Anglesey after the sacking of Mona in AD 60.

It is generally assumed that Irish invasion or immigration formed the basis of the modern Manx language; Irish migration to the island probably began in the 5th century AD. This is evident in the change in language used in Ogham inscriptions. The transition between "Manx Brythonic" (a Brythonic language like modern Welsh) and "Manx Gaelic" (a Goidelic language like modern Scottish Gaelic and Irish) may have been gradual. One question is whether the present-day Manx language survives from pre-Norse days or reflects a linguistic reintroduction after the Norse invasion. The island lends its name to "Manannán", the Brythonic and Gaelic sea god who is said in myth to have once ruled the island.

Tradition attributes the island's conversion to Christianity to St Maughold (Maccul), an Irish missionary who gives his name to a parish. There are the remains of around 200 tiny early chapels called keeils scattered across the island. Evidence such as radiocarbon dating and magnetic drift points to many of these being built around AD 550–600.

The Brythonic culture of "Manaw" appears throughout early British tradition and later Welsh writings. The family origins of Gwriad ap Elidyr (father of Merfyn Frych and grandfather of Rhodri the Great) are attributed to a "Manaw" and he is sometimes named as "Gwriad Manaw". The 1896 discovery of a cross inscribed "Crux Guriat" (Cross of Gwriad) and dated to the 8th or 9th century greatly supports this theory.

The best record of any event before the incursions of the Northmen is attributed to Báetán mac Cairill, king of Ulster, who (according to the "Annals of Ulster") led an expedition to Man in 577–578, imposing his authority on the island (though some have thought this event may refer to Manau Gododdin between the Firths of Clyde and Forth, rather than the Isle of Man). After Báetán's death in 581, his rival Áedán mac Gabráin, king of Dál Riata, is said to have taken the island in 582.

Even if the supposed conquest of the Menavian islands – Mann and Anglesey – by Edwin of Northumbria, in 616, did take place, it could not have led to any permanent results, for when the English were driven from the coasts of Cumberland and Lancashire soon afterwards, they could not well have retained their hold on the island to the west of these coasts. One can speculate, however, that when Ecgfrið's Northumbrians laid Ireland waste from Dublin to Drogheda in 684, they temporarily occupied Mann.

The period of Scandinavian domination is divided into two main epochs – before and after the conquest of Mann by Godred Crovan in 1079. Warfare and unsettled rule characterise the earlier epoch, the later saw comparatively more peace.

Between about AD 800 and 815 the Vikings came to Mann chiefly for plunder. Between about 850 and 990, when they settled, the island fell under the rule of the Scandinavian Kings of Dublin and between 990 and 1079, it became subject to the powerful Earls of Orkney.

There was a mint producing coins on Mann between c. 1025 and c. 1065. These Manx coins were minted from an imported type 2 Hiberno-Norse penny die from Dublin. Hiberno-Norse coins were first minted under Sihtric, King of Dublin. This illustrates that Mann may have been under the thumb of Dublin at this time.

Little is known about the conqueror, Godred Crovan. According to the "Chronicon Manniae" he subdued Dublin, and a great part of Leinster, and held the Scots in such subjection that supposedly no one who set out to build a vessel dared to insert more than three bolts. The memory of such a ruler would be likely to survive in tradition, and it seems probable therefore that he is the person commemorated in Manx legend under the name of King Gorse or Orry. He created the Kingdom of Mann and the Isles in around 1079 including the south-western islands of Scotland until 1164, when two separate kingdoms were formed from it. In 1154, later known as the Diocese of Sodor and Man, was formed by the Catholic Church.

The islands under his rule were called the "Suðr-eyjar" (South isles, in contrast to the "Norðr-eyjar" North isles", i.e. Orkney and Shetland), consisting of the Hebrides, all the smaller western islands of Scotland, and Mann. At a later date his successors took the title of (King of Mann and of the Isles). The kingdom's capital was on St Patrick's Isle, where Peel Castle was built on the site of a Celtic monastery.

Olaf, Godred's son, exercised considerable power and according to the Chronicle, maintained such close alliance with the kings of Ireland and Scotland that no one ventured to disturb the Isles during his time (1113–1152). In 1156 his son Godred (reigned 1153–1158), who for a short period also ruled over Dublin, lost the smaller islands off the coast of Argyll as a result of a quarrel with Somerled (the ruler of Argyll). An independent sovereignty thus appeared between the two divisions of his kingdom.

In the 1130s the Catholic Church sent a small mission to establish the first bishopric on the Isle of Man, and appointed Wimund as the first bishop. He soon afterwards embarked with a band of followers on a career of murder and looting throughout Scotland and the surrounding islands.

During the whole of the Scandinavian period, the Isles remained nominally under the suzerainty of the Kings of Norway but the Norwegians only occasionally asserted it with any vigour. The first such king to assert control over the region was likely Magnus Barelegs, at the turn of the 12th century. It was not until Hakon Hakonarson's 1263 expedition that another king returned to the Isles.

From the middle of the 12th century until 1217 the suzerainty had remained of a very shadowy character; Norway had become a prey to civil dissensions. But after that date it became a reality, and Norway consequently came into collision with the growing power of the kingdom of Scotland.

Early in the 13th century, when Ragnald (reigned 1187–1229) paid homage to King John of England (reigned 1199–1216), we hear for the first time of English intervention in the affairs of Mann. But a period of Scots domination would precede the establishment of full English control.

Finally, in 1261, Alexander III of Scotland sent envoys to Norway to negotiate for the cession of the isles, but their efforts led to no result. He therefore initiated a war, which ended in the indecisive Battle of Largs against the Norwegian fleet in 1263. However, the Norwegian king Haakon Haakonsson died the following winter, and this allowed King Alexander to bring the war to a successful conclusion. Magnus Olafsson, King of Mann and the Isles (reigned 1252–1265), who had campaigned on the Norwegian side, had to surrender all the islands over which he had ruled, except Mann, for which he did homage. Two years later Magnus died and in 1266 King Magnus VI of Norway ceded the islands, including Mann, to Scotland in the Treaty of Perth in consideration of the sum of 4,000 marks (known as in Scotland) and an annuity of 100 marks. But Scotland's rule over Mann did not become firmly established till 1275, when the Manx suffered defeat in the decisive Battle of Ronaldsway, near Castletown.

In 1290 King Edward I of England sent Walter de Huntercombe to seize possession of Mann, and it remained in English hands until 1313, when Robert Bruce took it after besieging Castle Rushen for five weeks. In about 1333 King Edward III of England granted Mann to William de Montacute, 3rd Baron Montacute (later the 1st Earl of Salisbury), as his absolute possession, without reserving any service to be rendered to him.

Then, in 1346, the Battle of Neville's Cross decided the long struggle between England and Scotland in England's favour. King David II of Scotland, Robert Bruce's last male heir, had been captured in the Battle of Neville's cross and ransomed; however, when Scotland was unable to raise one of the ransom installments, David made a secret agreement with King Edward III of England to cancel it, in return for transferring the Scottish kingdom to an English prince.

Following the secret agreement, there followed a confused period when Mann sometimes experienced English rule and sometimes Scottish. In 1388 the island was "ravaged" by Sir William Douglas of Nithsdale on his way home from the destruction of the town of Carlingford.

In 1392 William de Montacute's son sold the island, including sovereignty, to Sir William le Scrope. In 1399 Henry Bolinbroke brought about the beheading of Le Scrope, who had taken the side of Richard II when Bolinbroke usurped the throne and appointed himself "Henry IV". The island then came into the de facto possession of Henry, who granted it to Henry Percy, 1st Earl of Northumberland; but following the latter's later attainder, Henry IV, in 1405, made a lifetime grant of it, with the patronage of the bishopric, to Sir John Stanley. In 1406 this grant was extended – on a feudatory basis under the English Crown – to Sir John's heirs and assigns, the feudal fee being the service of rendering homage and two falcons to all future Kings of England on their coronations.

With the accession of the Stanleys to the throne there begins a more settled epoch in Manx history. Though the island's new rulers rarely visited its shores, they placed it under governors, who, in the main, seem to have treated it with the justice of the time. Of the thirteen members of the family who ruled in Mann, the second Sir John Stanley (1414–1432), James, the 7th Earl (1627–1651), and the 10th Earl of the same name (1702–1736) had the most important influence on it. They first curbed the power of the spiritual barons, introduced trial by jury, which superseded trial by battle, and ordered the laws to be written. The second, known as the Great Stanley, and his wife, Charlotte de la Tremoille (or Tremouille), are probably the most striking figures in Manx history.

Shortly after the Wars of the Three Kingdoms began in June 1643, James Stanley, 7th Earl of Derby returned to Mann to find the island on the brink of rebellion. Among the causes were complaints at the level of tithes payable to the Church of England, and Derby's attempts to replace the Manx ‘tenure of straw’ by which many of his tenants held their lands, a customary tenure akin to freehold, with commercial leases. He managed to restore the situation through a series of meetings, but made minimal concessions.

Six months after Charles I was executed on 30 January 1649, Derby received a summons from General Ireton to surrender the island, but declined to do so. In August 1651, he and 300 Manxmen landed in Lancashire to take part in the Third English Civil War; defeated at Wigan Lane on 25 August 1651, Derby escaped with only 30 troops to join Charles II. Captured after the Battle of Worcester in September, he was imprisoned in Chester Castle, tried by court-martial and executed at Bolton on 15 October.

Soon after Stanley's death, the Manx Militia, under the command of William Christian (known by his Manx name of Illiam Dhone), rose against the Countess and captured all the insular forts except Rushen and Peel. They were then joined by a Parliamentarian force sent from the mainland, led by Colonels Thomas Birch and Robert Duckenfield, to whom the Countess surrendered after a brief resistance.

Oliver Cromwell had appointed Thomas Fairfax "Lord of Mann and the Isles" in September 1651, so that Mann continued under a monarchical government and remained in the same relation to England as before.

The restoration of Stanley government in 1660 therefore caused as little friction and alteration as its temporary cessation had. One of the first acts of the new Lord, Charles Stanley, 8th Earl of Derby, was to order Christian to be tried. He was found guilty and executed. Of the other persons implicated in the rebellion only three were excepted from the general amnesty. But by Order in Council, Charles II pardoned them, and the judges responsible for the sentence on Christian were punished.

Charles Stanley's next act was to dispute the permanency of the tenants' holdings, which they had not at first regarded as being affected by the acceptance of leases, a proceeding which led to an almost open rebellion against his authority and to the neglect of agriculture, in lieu of which the people devoted themselves to the fisheries and to contraband trade.

Charles Stanley, who died in 1672, was succeeded first by his son William Richard George Stanley, 9th Earl of Derby until his death in 1702.

The agrarian question subsided only in 1704, when James Stanley, 10th Earl of Derby, William's brother and successor, largely through the influence of Bishop Wilson, entered into a compact with his tenants, which became embodied in an Act, called the Act of Settlement. Their compact secured the tenants in the possession of their estates in perpetuity subject only to a fixed rent, and a small fine on succession or alienation. From the great importance of this act to the Manx people it has been called their "Magna Carta". As time went on, and the value of the estates increased, the rent payable to the Lord became so small in proportion as to be almost nominal, being extinguished by purchase in 1916.

James died in 1736, and the suzerainty of the isle passed to James Murray, 2nd Duke of Atholl, his first cousin and heir-male. In 1764 he was succeeded by his only surviving child Charlotte, Baroness Strange, and her husband, John Murray, who (in right of his wife) became Lord of Mann. In about 1720 the contraband trade had greatly increased. In 1726 Parliament had checked it somewhat for a time, but during the last ten years of the Atholl regime (1756–1765) it assumed such proportions that, in the interests of the Imperial revenue, it became necessary to suppress it. With a view to so doing, Parliament passed the Isle of Man Purchase Act 1765 (commonly called the "Revestment Act" by the Manx), under which it purchased the rights of the Atholls as Lords of Mann, including the customs revenues of the island, for the sum of £70,000 sterling, and granted an annuity to the Duke and Duchess. The Atholls still retained their manorial rights, the patronage of the bishopric, and certain other perquisites, until they sold them for the sum of £417,144 in 1828.
Up to the time of the revestment, Tynwald had passed laws concerning the government of the island in all respects and had control over its finances, subject to the approval of the Lord of Mann. After the revestment, or rather after the passage of the Smuggling Act 1765 (commonly called the Mischief Act by the Manx), the Parliament at Westminster legislated with respect to customs, harbours and merchant shipping, and, in measures of a general character, it occasionally inserted clauses permitting the enforcement in the island of penalties in contravention of the Acts of which they formed part. It also assumed the control of the insular customs duties. Such changes, rather than the transference of the full suzerainty to the King of Great Britain and Ireland, modified the (unwritten) constitution of the Isle of Man. Its ancient laws and tenures remained untouched, but in many ways the revestment affected it adversely. The hereditary Lords of Mann had seldom, if ever, functioned as model rulers, but most of them had taken some personal share in its government, and had interested themselves in the well-being of the inhabitants. But now the whole direction of its affairs became the work of officials who regarded the island as a pestilent nest of smugglers, from which it seemed their duty to extract as much revenue as possible.

There was some alleviation of this state of things between 1793 and 1826, when John Murray, 4th Duke of Atholl served as Governor, since, though he quarrelled with the House of Keys and unduly cared for his own pecuniary interests, he did occasionally exert himself to promote the welfare of the island. After his departure the English officials resumed their sway, but they showed more consideration than before. Moreover, since smuggling, which the Isle of Man Purchase Act had only checked – not suppressed – had by that time almost disappeared, and since the Manx revenue had started to produce a large and increasing surplus, the authorities looked more favourably on the Isle of Man, and, thanks to this fact and to the representations of the Manx people to British ministers in 1837, 1844 and 1853, it obtained a somewhat less stringent customs tariff and an occasional dole towards erecting its much neglected public works.

Since 1866, when the Isle of Man obtained a nominal measure of Home Rule, the Manx people have made remarkable progress, and currently form a prosperous community, with a thriving offshore financial centre, a tourist industry (albeit smaller than in the past) and a variety of other industries.

The Isle of Man was a base for alien civilian internment camps in both the First World War (1914–18) and the Second World War (1939–45). During the First World War there were two camps: one a requisitioned holiday camp in Douglas and the other the purpose-built Knockaloe camp near Peel in the parish of Patrick. During the Second World War there were a number of smaller camps in Douglas, Peel, Port Erin and Ramsey. The (now disbanded) Manx Regiment was raised in 1938 and saw action during the Second World War.

On 2 August 1973, a flash fire killed between 50 and 53 people at the Summerland amusement centre in Douglas.

The early-20th century saw a revival of music and dance, and a limited revival of the Manx language - although the last "native" speaker of Manx Gaelic died in the 1970s. In the middle of the 20th century the Taoiseach of the Republic of Ireland, Éamon de Valera, visited, and was so dissatisfied with the lack of support for Manx that he immediately had two recording vans sent over. During the 20th century the Manx tourist economy declined, as the English and Irish started flying to Spain for package holidays. The Manx Government responded to this by successfully promoting the island, with its low tax-rates, as an offshore financial centre, although Man has avoided a place on a 2009 UK blacklist of tax havens. The financial centre has had its detractors who have pointed to the potential for money laundering.

In 1949 an Executive Council, chaired by the Lieutenant-Governor and including members of Tynwald, was established. This marked the start of a transfer of executive power from the un-elected Lieutenant-Governor to democratically elected Manx politicians. Finance and the police passed to Manx control between 1958 and 1976. In 1980 a chairman elected by Tynwald replaced the Lieutenant-Governor as Chairman of the Executive Council. Following legislation in 1984, the Executive Council was reconstituted in 1985 to include the chairmen of the eight principal Boards; in 1986 they were given the title of Minister and the chairman was re-titled "Chief Minister". In 1986 Sir Miles Walker CBE became the first Chief Minister of the Isle of Man. In 1990 the Executive Council was renamed the "Council of Ministers".

The 1960s also saw a rise in Manx nationalism, spawning the parties Mec Vannin and the Manx National Party, as well as the now defunct (literally "Underground"), which mounted a direct-action campaign of spray-painting and attempted house-burning.

On 5 July 1973 control of the postal service passed from the UK General Post Office to the new Isle of Man Post, which began to issue its own postage stamps.

The 1990s and early 21st century have seen a greater recognition of indigenous Manx culture, including the opening of the first Manx-language primary school.

Since 1983 the Isle of Man government has designated more than 250 historic structures as Registered Buildings of the Isle of Man.






</doc>
<doc id="14764" url="https://en.wikipedia.org/wiki?curid=14764" title="Geography of the Isle of Man">
Geography of the Isle of Man

The Isle of Man is an island in the Irish Sea, between Great Britain and Ireland in Western Europe, with a population of almost 85,000. It is a British Crown dependency. It has a small islet, the Calf of Man, to its south. It is located at .

Area:

<br>"Land:"

<br>"Water:"
<br>"Total:"
This makes it:

The Isle of Man has a coastline of , and a territorial sea extending to a maximum of 12 nm from the coast, or the midpoint between other countries. The total territorial sea area is about 4000 km or 1500 sq miles, which is about 87% of the total area of the jurisdiction of the Isle of Man. The Isle of Man only holds exclusive fishing rights in the first 3 nm. The territorial sea is managed by the Isle of Man Government Department of Infrastructure.

The Raad ny Foillan long distance footpath runs around the Manx coast.

The Isle of Man enjoys a temperate climate, with cool summers and mild winters. Average rainfall is high compared to the majority of the British Isles, due to its location to the western side of Great Britain and sufficient distance from Ireland for moisture to be accumulated by the prevailing south-westerly winds. Average rainfall is highest at Snaefell, where it is around a year. At lower levels it can fall to around a year.

Temperatures remain fairly cool, with the recorded maximum being at Ronaldsway.

The island's terrain is varied. There are two mountainous areas divided by a central valley which runs between Douglas and Peel. The highest point in the Isle of Man, Snaefell, is in the northern area and reaches above sea level. The northern end of the island is a flat plain, consisting of glacial tills and marine sediments. To the south the island is more hilly, with distinct valleys. There is no land below sea level.


There are few severe natural hazards, the most common being high winds, rough seas and dense fog. In recent years there has been a marked increase in the frequency of high winds, heavy rains, summer droughts and flooding both from heavy rain and from high seas. Snow fall has decreased significantly over the past century while temperatures are increasing year round with rainfall decreasing.

Air pollution, marine pollution and waste disposal are issues in the Isle of Man.

In order of importance, international first, non-statutory last. Note that ASSIs and MNRs have equal levels of statutory protection under the Wildlife Act 1990.




There are 22 ASSIs on the Isle of Man as of May 2020. One additional ASSI has been designated but later rescinded (Ramsey Harbour).


A marine nature reserve was designated in Ramsey Bay in Oct 2011. In 2018 nine further Marine Nature Reserves were given statutory protection. The ten Marine Nature Reserves found around the Isle of Man cover over 10% of the country's territorial waters, in accordance with international requirements.


Bird Sanctuaries where formerly designated under the Wild Birds Protection Act 1932. This designation was superseded by Areas of Special Protection for Birds by the Wildlife Act 1990, however the following formerly designated Bird Sanctuaries remain protected:


The Isle of Man had 45 non-statutory wildlife sites as of 30 January 2009, covering about 195 ha (0.75 sq miles) of land and an additional of inter-tidal coast. The Manx Wildlife Trust also manages 24 nature reserves, along with the Calf of Man, as of September 2016:

The majority of the island is formed from highly faulted and folded sedimentary rocks of the Ordovician period. There is a belt of younger Silurian rocks along the west coast between Niarbyl and Peel, and a small area of Devonian sandstones around Peel.
A band of Carboniferous period rocks underlies part of the northern plain, but is nowhere seen at the surface; however similar age rocks do outcrop in the south between Castletown, Silverdale and Port St Mary. Permo-Triassic age rocks are known to lie beneath the Point of Ayre but, as with the rest of the northern plain, these rocks are concealed by substantial thicknesses of superficial deposits.

The island has significant deposits of copper, lead and silver, zinc, iron, and plumbago (a mix of graphite and clay). There are also quarries of black marble, limestone flags, clay schist, and granite. These are all modern, and there was no noticeable exploitation of metals or minerals prior to the modern era.

The island has a census-estimated population of 84,497 according to the most recent 2011 census: up from 79,805 in 2006 and 76,315 in 2001.

The island's largest town and administrative centre is Douglas, whose population is 23,000 — over a quarter of the population of the island. Neighbouring Onchan, Ramsey in the north, Peel in the west and the three southern ports of Castletown, Port Erin and Port St Mary are the island's other main settlements. Almost all its population lives on or very near the coast.




</doc>
<doc id="14765" url="https://en.wikipedia.org/wiki?curid=14765" title="Demographics of the Isle of Man">
Demographics of the Isle of Man

This article is about the demographic features of the population of the Isle of Man, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population. The UN estimate of population as at mid- is .

Anglican, The Church of Jesus Christ of Latter-day Saints, Roman Catholic, Methodist, Baptist, Presbyterian, Religious Society of Friends, Jehovah's Witnesses, Pentecostalism, Atheism, and Agnosticism.

The Church of England is the established church.



</doc>
<doc id="14766" url="https://en.wikipedia.org/wiki?curid=14766" title="Politics of the Isle of Man">
Politics of the Isle of Man

The government of the Isle of Man is a parliamentary representative democracy. As a Crown Dependency, it is not subordinate to the government of the United Kingdom. That government, however, is responsible for defence and external affairs and could intervene in the domestic affairs of the island under its residual responsibilities to guarantee "good government" in all Crown dependencies. The Monarch of the United Kingdom is also the head of state of the Isle of Man, and generally referred to as "The Queen, Lord of Mann". Legislation of the Isle of Man defines "the Crown in right of the Isle of Man" as separate from the "Crown in right of the United Kingdom". Her representative on the island is the Lieutenant Governor of the Isle of Man, but his role is mostly ceremonial, though he does have the power to grant Royal Assent (the withholding of which is the same as a veto).

Although the Isle of Man is not part of the United Kingdom, its people are British citizens under UK law — there is no separate Manx citizenship. The United Kingdom is responsible for all the island's external affairs, including citizenship, defence, good governance, and foreign relations. The island has no representation in the UK parliament.

The legislative power of the government is vested in a bicameral (sometimes called tricameral) parliament called Tynwald (said to be the world's oldest "continuously existing" parliament), which consists of the directly-elected House of Keys and the indirectly chosen Legislative Council. After every House of Keys general election, the members of Tynwald elect from amongst themselves the Chief Minister of the Isle of Man, who serves as the head of government for five years (until the next general election). Executive power is vested in the Lieutenant Governor (as Governor-in-Council), the Chief Minister, and the Isle of Man's Council of Ministers. The judiciary is independent of the executive and the legislature.

Douglas, the largest town on the Isle of Man, is its capital and seat of government, where the Government offices and the parliament chambers (Tynwald) are located.

The Head of State is the Lord of Mann, which is a hereditary position held by the British monarch (currently Queen Elizabeth II). The Lieutenant Governor is appointed by the Queen, on the advice of the UK's Secretary of State for Justice, for a five-year term and nominally exercises executive power on behalf of the Queen. The Chief Minister is elected by Tynwald following every House of Keys general election and serves for five years until the next general election.

When acting as Lord of Mann, the Queen acts on the advice of the Secretary of State for Justice and Lord Chancellor of the United Kingdom having prime responsibility as Privy Counsellor for Manx affairs.

The executive branch under the Chief Minister is referred to as "the Government" or the "Civil Service", and consists of the Council of Ministers, nine Departments, ten Statutory Boards and three Offices. Each Department is run by a Minister who reports directly to the Council of Ministers. The Civil Service has more than 2000 employees and the total number of public sector employees including the Civil Service, teachers, nurses, police, etc. is about 9000 people. This is somewhat more than 10% of the population of the island, and a full 23% of the working population. This does not include any military forces, as defence is the responsibility of the United Kingdom.

The Manx legislature is Tynwald, which consists of two chambers. The House of Keys has 24 members, elected for a five-year term in two-seat constituencies by the whole island. The minimum voting age is 16. The Legislative Council has eleven members: the President of Tynwald, the Bishop of Sodor and Man, the Attorney General (non-voting) and eight other members elected by the House of Keys for a five-year term, with four retiring at a time. (In the past they have often already been Members of the House of Keys, but must leave the Keys if elected to the Council.) There are also joint sittings of the Tynwald Court (the two houses together).

In the 2016 Manx general election, on 22 September, the Liberal Vannin Party won three seats, tying their 2011 results; all 21 remaining seats were won by independents. However one of those three members now sits as an independent. 12 of the 24 MHKs were newly elected to office. Turnout slightly improved from 2011 with 56% of eligible voters turning out, ranging from 40% in Douglas East to 65% in Ayre & Michael.

Most Manx politicians stand for election as independents rather than as representatives of political parties. Though political parties do exist, their influence is not nearly as strong as in the United Kingdom. Consequently, much Manx legislation develops through consensus among the members of Tynwald, which contrasts with the much more adversarial nature of the British Parliament.

The largest political party is the Liberal Vannin Party, which promotes liberalism, greater Manx independence and more accountability in Government.

A Manx Labour Party also exists, unaffiliated to the British Labour Party. Its candidates won a combined 1.4% of the overall vote in 2016.

A political pressure group Mec Vannin advocates the establishment of a sovereign republic.

The Isle of Man Green Party, which was founded in 2016, holds two local government seats and promotes Green politics.

The island also formerly had a Manx National Party. There are Manx members in the Celtic League, a political pressure group that advocates greater co-operation between and political autonomy for the Celtic nations.

The UK Parliament has paramount power to legislate for the Isle of Man on all matters, but it is a long-standing convention that it does not do so on domestic ("insular") matters without Tynwald's consent.

Occasionally, the UK Parliament acts against the wishes of Tynwald: the most recent example was the Marine etc. Broadcasting (Offences) Act 1967, which banned pirate radio stations from operating in Manx waters. Legislation to accomplish this was defeated on its second reading in the House of Keys, prompting Westminster to legislate directly.

The UK's secondary legislation (regulations and Statutory Instruments) cannot be extended to apply to the Isle of Man.

The Isle of Man is subject to certain European Union laws, by virtue of a being a territory for which the UK has responsibility in international law. These laws are those for areas not covered by the Protocol 3 opt-out that the UK obtained for the Isle of Man in its accession treaty: the excluded areas are free movement of persons, services and capital, and taxation and social policy harmonisation.

The UK has had several disputes with the European Court of Human Rights about the Isle of Man's laws concerning birching (corporal punishment) and sodomy.

The lowest courts in the Isle of Man are presided over by the High Bailiff and the Deputy High Bailiff, along with lay Justices of the Peace. The High Court of Justice consists of three civil divisions and is presided over by a Deemster. Appeals are dealt with by the Staff of Government Division with final appeal to the Judicial Committee of the Privy Council in the United Kingdom. The head of the Judiciary is the First Deemster and Clerk of the Rolls. The other High and Appeal Court Judges are the Second Deemster, The Deemster and the Judge of Appeal, all of whom are appointed by the Lieutenant Governor.

The Court of General Gaol Delivery is the criminal court for serious offences (effectively the equivalent of a Crown Court in England). It is theoretically not part of the High Court, but is effectively the criminal division of the court. The Second Deemster normally sits as the judge in this court. In 1992, His Honour Deemster Callow passed the last-ever sentence of death in a court in the British Islands (which was commuted to life imprisonment). Capital punishment in the Isle of Man was formally abolished by Tynwald in 1993 (although the last execution on the island took place in 1872).




</doc>
<doc id="14767" url="https://en.wikipedia.org/wiki?curid=14767" title="Economy of the Isle of Man">
Economy of the Isle of Man

The economy of the Isle of Man is a low-tax economy with insurance, online gambling operators and developers, information and communications technology (ICT), and offshore banking forming key sectors of the island's economy.

As an offshore financial centre located in the Irish Sea, the Isle of Man is within the British Isles but does not form part of the United Kingdom and is not a member of European Union. 

As of 2016, the Crown dependency's gross national income (GNI) per capita was US$89,970 as assessed by the World Bank. The Isle of Man Government's own National Income Report shows the largest sectors of the economy are insurance and eGaming with 17% of GNI each, followed by ICT and banking with 9% each, with tourist accommodation in the lowest sector at 0.3%.

After 32 years of continued Gross Domestic Product (GDP) growth, the financial year 2015/16 showed the first drop in GDP, of 0.9%, triggered by decline in eGaming revenues.

The unemployment rate remains low at around 1%.

Property prices are flat or declining, but recent figures also show an increase in resident income tax payers.

The government's policy of offering incentives to high-technology companies and financial institutions to locate on the island has expanded employment opportunities in high-income industries. Agriculture, fishing, and the hospitality industry, once the mainstays of the economy, now make declining contributions to the island's GNP. The hospitality sector contributed just of 0.3% of GNP in 2015/16, and 629 jobs in 2016. eGaming and ICT contribute the great bulk of GNP. The stability of the island's government and its openness for business make the Isle of Man an attractive alternative jurisdiction (DAW Index ranked 3).

In the Vision2020 the Isle of Man government lays out the national strategy of economic growth, seeking an increase of the economically active population an promoting the Island as an <nowiki>'Enterprise Island, "Tech Isle', 'Manufacturing centre of excellence', 'Offshore energy hub', 'Destination Island' and for 'Distinctive local food and drink'</nowiki>. The government has published its national economic strategies for several emerging sectors: aerospace, biomed, digital media, ICT.

The Isle of Man is a low-tax economy with no capital gains tax, wealth tax, stamp duty, or inheritance tax; and a top rate of income tax of 20%. A tax cap is in force: the maximum amount of tax payable by an individual is £125,000; or £250,000 for couples if they choose to have their incomes jointly assessed. Personal income is assessed and taxed on a total worldwide income basis rather than on a remittance basis. This means that all income earned throughout the world is assessable for Manx tax, rather than only income earned in or brought into the Island.

The standard rate of corporation tax for residents and non-residents is 0%; retail business profits above £500,000 and banking business income are taxed at 10%, and rental (or other) income from land and buildings situated on the Isle of Man is taxed at 20%.

Trade is mostly with the United Kingdom. The Isle of Man has free access to European Union markets for goods, but only has restricted access for services, people, or financial products.

The Isle of Man as an offshore financial centre has been repeatedly featured in the press as a tax haven, most recently in the wake of the Paradise Papers. 

The Organisation for Economic Co-operation and Development's (OECD) Global Forum on Transparency and Exchange of Information for Tax Purposes has rated the Isle of Man as 'top compliant' for a second time: a status which only three jurisdictions in the world have achieved so far. The island has become the second nation after Austria to ratify a multilateral convention with the OECD to implement measures to prevent Base Erosion and Profit Shifting (BEPS). 

In a report the European Council lists the Isle of Man together with the other two Crown Dependencies (Guernsey and Jersey) as well as Bermuda, the Cayman Islands and Vanuatu, as committed to addressing the Council's concerns of "Existence of tax regimes that facilitate offshore structures which attract profits without real economic activity" by 2018.

The Isle of Man's Department for Enterprise manages the diversified economy in twelve key sectors. The largest individual sectors by GNI are insurance and eGaming with 17% of GNI each, followed by ICT and banking with 9% each. The 2016 census lists 41,636 total employed. The largest sectors by employment are "medical and health", "financial and business services", construction, retail and public administration. Manufacturing, focused on aerospace and the food and drink industry, employs almost 2000 workers and contributes about 5% of GDP. The sector provides laser optics, industrial diamonds, electronics, plastics and aerospace precision engineering.

Insurance, banking (includes retail banking, offshore banking and other banking services), other finance and business services, and corporate service providers together contribute the most to the GNI and most of the jobs, with 10,057 people employed in 2016. 

Among the largest employers of the Island's private sector are eGaming (online gambling) companies like The Stars Group, Microgaming, Newfield, and Playtech. The Manx eGaming Association MEGA is representing the sector. Licenses are issued by the Gambling Supervision Commission.

In 2005 PokerStars, one of the world's largest online poker sites, relocated its headquarters to the Isle of Man from Costa Rica. In 2006, RNG Gaming a large gaming software developer of P2P tournaments and Get21, a multiplayer online blackjack site, based their corporate offices on the island.

The Isle of Man Government Lottery operated from 1986 to 1997. Since 2 December 1999 the island has participated in the United Kingdom National Lottery. The island is the only jurisdiction outside the United Kingdom where it is possible to play the UK National Lottery. Since 2010 it has also been possible for projects in the Isle of Man to receive national lottery Good Causes Funding. The good causes funding is distributed by the Manx Lottery Trust. Tynwald receives the 12p lottery duty for tickets sold in the Island.

The shortage of workers with ICT skills is tackled by several initiatives, like an IT and education campus, a new cyber security degree at the University College of Man, a Code Club, and a work permit waiver for skilled immigrants.

Since 1995 Isle of Man Film has co-financed and co-produced over 100 feature film and television dramas which have all filmed on the Island. 

Among the most successful productions funded in part by Isle of Man Film agency were "Waking Ned", where the Manx countryside stood in for rural Ireland, and films like "Stormbreaker", "Shergar", "Tom Brown's Schooldays", "I Capture the Castle", "The Libertine", "Island at War" (TV series), "Five Children and It", "Colour Me Kubrick", "Sparkle", and others. Other films that have been filmed on the Isle of Man include "Thomas and the Magic Railroad", "Harry Potter and the Chamber of Secrets", "Keeping Mum and Mindhorn."

2011 Isle of Man Film Oxford Economics was commissioned by Isle of Man Film Ltd to conduct a study into the economic impact of the film industry on the Isle of. Man. The recommendation of this report for Isle of Man Film was to partner with a more established film institution in the UK to source more Isle of Man film production opportunities. This led to the investment of the Isle of Man Government to take shares in Pinewood Shepperton Plc which were sold later with profit.

Once one of the busiest areas of film production in the British Isles, the Isle of Man hopes to use its strong foundation in film to grow its television and new digital media industry. In a recent Isle of Man Department of Economic Development strategic review, the Island's over 2,000 jobs counting digital sector features 'digital media' and the creative industries, and embraces partnerships with the industry and its individual sector bodies like the Isle of Media, a new media cluster. 

Hosting of motorsports events, like the Isle of Man Car Rally and the more-prominent TT motorcycle races, contributes to the tourism economy. 

Tourism in the Isle of Man developed from advances in transport to the island. In 1819 the first steamship "Robert Bruce" came to the island, only seven years after the first steam vessel in the UK. In the 1820s, tourism was growing due to improved transport. The island government's own report for the financial years 2014/15-2015/16 shows tourist accommodation to be in the lowest sector at 0.3%, ranking slightly above 'mining and quarrying' (0.1%).

Since 1999, the Isle of Man has received electricity through the world's longest submarine AC cable, the 90 kV Isle of Man to England Interconnector, as well as from a natural gas power station in Douglas, an oil power station in Peel and a small hydro-electric power station in Sulby Glen.

The Island is connected with five submarine cables to the UK and Ireland.

While the Isle of Man Communications Commission refers to Akamai’s recent State of the Internet Report for Q1 2017, with "the Island ranked 8th in the world for percentage of broadband connections with >4 Mb/s connectivity, with 96% of users connecting at speeds greater than 4Mb/s", an "international league table of broadband speeds puts the Isle of Man at 50th in the world". Manx Telecom recently announced to roll out Fibre-to-the-Home (FTTH) superfast broadband with download speeds of up to 1Gigabit per second. 

Ronaldsway Airport links the Isle of Man with six airlines to eleven UK and Irish scheduled flight destinations. 

The Steam Packet Company provides ferry services to Liverpool, Heysham, Belfast and Dublin.

Labour force—by occupation:
agriculture, forestry and fishing 3%, manufacturing 11%, construction 10%, transport and communication 8%, wholesale and retail distribution 11%, professional and scientific services 18%, public administration 6%, banking and finance 18%, tourism 2%, entertainment and catering 3%, miscellaneous services 10%

Unemployment rate:
nominally 2.0% (January 2016)

Industries:
financial services, light manufacturing, tourism

Agriculture—products:
cereals, vegetables, cattle, sheep, pigs, poultry

Exports:
$NA

Exports—commodities:
tweeds, herring, processed shellfish, beef, lamb

Exports—partners:
UK

Imports:
$NA

Imports—commodities:
timber, fertilizers, fish

Imports—partners:
UK

Debt—external:
$NA

Economic aid—recipient:
$NA

Currency:
1 Isle of Man pound = 100 pence

Exchange rates:
the Manx pound is at par with the British pound

Fiscal year:
1 April – 31 March



</doc>
<doc id="14768" url="https://en.wikipedia.org/wiki?curid=14768" title="Communications in the Isle of Man">
Communications in the Isle of Man

The Isle of Man has an extensive communications infrastructure consisting of telephone cables, submarine cables, and an array of television and mobile phone transmitters and towers.

The history of Manx telecommunications starts in 1859, when the Isle of Man Electric Telegraph Company was formed on the island with the intention of connecting across the island by telegraph, and allowing messages to be sent onwards to the UK. In August 1859, a long cable was commissioned from Glass, Elliot and Company of Greenwich and laid from Cranstal (north of Ramsey) to St Bees in Cumbria using the chartered cable ship "Resolute". The cable was single-core, with gutta-percha insulation.

Twenty miles of overhead cable were also erected from Cranstal south to Ramsey, and on to Douglas. In England, the telegraph was connected to Whitehaven and the circuits of the Electric Telegraph Company.

The telegraph offices were located at 64 Athol Street, Douglas (also the company's head office) and at East Quay, Ramsey (now Marina House).

On 10 August 1860 the company was statutorily incorporated by an Act of Tynwald with a capital of £5,500.

The currents at Cranstal proved too strong, and in 1864 the cable was taken up and relaid further south, at Port-e-Vullen in Ramsey Bay. It was later relaid to land even further south at Port Cornaa.

Following the 1869 finalisation of UK telegraph nationalisation into a General Post Office monopoly, the Isle of Man Telegraph Company was nationalised in 1870 under the Telegraph Act 1870 (an Act of Parliament) at a cost to the British Government of £16,106 (paid in 1872 following arbitration proceedings over the value). Prior to nationalisation, the island's telegraph operations had been performing poorly and the company's share price valued it at around £100.

Subsequent to nationalisation, operations were taken over by the GPO. The internal telegraph system was extended within a year to Castletown and Peel, however by then the previous lack of modern communications in Castletown had already started the Isle of Man Government on its move to Douglas.

Due to increasing usage in the years following nationalisation, further cables between Port Cornaa and St Bees were laid in 1875 and 1885.

By 1883 Smith's Directory listed several telegraph offices operated by the Post Office, in addition to those at Douglas, Ramsey, Castletown and Peel the telegraph was also available at Laxey, Ballaugh, and Port St. Mary.

Throughout the First World War, the cable landing station at Port Cornaa was guarded by the Isle of Man Volunteer Corps.

The undersea telegraph cables have been disused since the 1950s, but remain in place.

A Teleport, with several earth stations, is currently under construction on the Isle of Man. SES Satellite Leasing, the entrepreneurial investment arm of SES. The teleport is expected to enter into service in 2017. It will be a state-of-the-art facility providing satellite telemetry, tracking and commanding (TT&C) facilities and capacity management, together with a wide range of teleport services such as uplink, downlink, and contribution services for broadcasters and data centres.

The main telephone provider on the Isle of Man today is Manx Telecom.

In 1889 George Gillmore, formerly an electrician for the GPO's Manx telegraph operations, was granted a licence by the Postmaster General to operate the Isle of Man's first telephone service. Based in an exchange in Athol Street, early customers of Gilbert's telephone service included the Isle of Man Steam Packet Company and the Isle of Man Railway. Not having the resources to fund expansion or a link to England, Gillmore sold his licence to the National Telephone Company and stayed on as their manager on the island.

By 1901 there were 600 subscribers, and the telephone system had been extended to Ramsey, Castletown, Peel, Port Erin, Port St. Mary and Onchan.

On 1 January 1912 the National Telephone Company was nationalised and merged into the General Post Office by the Telephone Transfer Act 1911. Only Guernsey, Portsmouth and Hull remained outside of the GPO.

In 1922, the General Post Office offered to sell the island's telephone service to the Manx government, but the offer was not taken up. A similar arrangement in Jersey for that island's telephone service was concluded in 1923.

The first off-island telephone link was established in 1929, with the laying of a cable by the "CS Faraday" between Port Erin and Ballyhornan in Northern Ireland, a distance of 57 km, and then between Port Grenaugh and Blackpool, primarily to provide a link to Northern Ireland. The cable was completed on 6 June 1929 and the first call between the Isle of Man and the outside world was made on 28 June 1929 by Lieutenant Governor Sir Claude Hill in Douglas to the Postmaster General in Liverpool. The cable initially carried only two trunk circuits.

In 1942, a pioneering VHF frequency-modulated radio-link was established between Creg-na-Baa and the UK to provide an alternative to the sub-sea cable. This has since been discontinued.

This was augmented on 24 June 1943 by a long cable between Cemaes Bay in Anglesey and Port Erin, which had the world's first submerged repeater, laid by "HMCS Iris". The repeater doubled the possible number of circuits on the cable, and although it failed after only five months, its replacement worked for seven years.

In 1962 a further undersea cable was laid by "HMTS Ariel" between Colwyn Bay and the Island.

Historically, the telephone system on the Isle of Man had been run as a monopoly by the British General Post Office, and later British Telecommunications, and operated as part of the Liverpool telephone district.

By 1985 the privatised British Telecom had inherited the telephone operations of the GPO, including those on the Isle of Man. At this time the Manx Government announced that it would award a 20-year licence to operate the telephone system in a tender process. As part of this process, in 1986 British Telecom created a Manx-registered subsidiary company, Manx Telecom, to bid for the tender. It was believed that a local identity and management would be more politically acceptable in the tendering process as they competed with Cable & Wireless to win the licence. Manx Telecom won the tender, and commenced operations under the new identity from 1 January 1987.

On 28 March 1988 an 8,000 telephone circuit fibre optic cable, the longest unregenerated system in Europe, was inaugurated. In links Port Grenaugh to Silecroft in Cumbria, and was laid in September 1987. The cable was buried in the seabed along its entire length.

A further fibre optic cable, known as BT-MT1 was laid in October 1990 between Millom in Cumbria and Douglas, a distance of . Jointly operated by BT and Manx Telecom, it provides six channels each with a bandwidth of 140 Mbit/s. This cable remains in use today.

In July 1992, Mercury Communications laid the LANIS fibre-optic cables. LANIS-1 runs for between Port Grenaugh and Blackpool, and LANIS-2 runs for between the Isle of Man and Northern Ireland. They have six channels each with a bandwidth of 565 Mbit/s. The LANIS cables are now operated by Cable & Wireless. The LANIS-1 cable was damaged 600 m off Port Grenaugh on 27 November 2006, causing loss of the link and resulting in temporary Internet access issues for some Manx customers whilst it was awaiting repair.

On 17 November 2001 Manx Telecom became part of mmO following the demerger of BT Wireless's operations from BT Group, and the company was owned by Telefónica. On 4 June 2010 Manx Telecom was sold by Telefónica to UK private equity investor HgCapital (who were buying the majority stake), alongside telecoms management company CPS Partners

In December 2007, the Manx Electricity Authority and its telecoms subsidiary, e-llan Communications, commissioned the lighting of a new undersea fibre-optic link. It was laid in 1999 between Blackpool and Douglas as part of the Isle of Man to England Interconnector which connects the Manx electricity system to the UK's National Grid.

In December 2017, Horizon Electronics Isle of Man (formerly Horizon Electro) helped with the online TV services of the Isle of Man.

According to the CIA World Factbook, in 1999 there were 51,000 fixed telephone lines in use in the Isle of Man.

The Isle of Man is included within the UK telephone numbering system, and is accessed externally via UK area codes, rather than by its own country calling code. The area codes currently in use are: +44 1624 (landlines) and +44 7425 / +44 7624 / +44 7924 (mobiles).


Submarine cables in Manx waters are governed by the Submarine Cables Act 2003 (an Act of Tynwald).


It is also rumoured that various online gaming companies operate their own networks outside of these providers, although they do not resell that service.

The mobile phone network operated by Manx Telecom has been used by O as an environment for developing and testing new products and services prior to wider rollout. In December 2001, the company became the first telecommunications operator in Europe to launch a live 3G network. In November 2005, the company became the first in Europe to offer its customers an HSDPA (3.5G) service.

Sure built their own mobile network on the island in 2007 and following various upgrades now deliver 2G/3G and 4G services

In 1996 the Isle of Man government obtained permission to use the .im national top level domain (TLD) and has ultimate responsibility for its use. The domain is managed on a daily basis by Domicilium (IOM) Limited, an island based Internet service provider. Broadband Internet services are available through five local providers which are Manx Telecom, Sure, Wi-Manx, Domicilium, Opti-Fi Limited and BlueWave Communications.

The public-service commercial radio station for the island is Manx Radio. Manx Radio is part funded by government grant, and partly by advertising. There are two other Manx-based FM radio stations, Energy FM and 3 FM.

BBC national radio stations are also relayed locally via a transmitter located to the south of Douglas, relayed from Sandale transmitting station in Cumbria, as well as a signal feed from the Holme Moss transmitting station in West Yorkshire. The Douglas transmitter also broadcasts the BBC's DAB digital radio services and Classic FM.

Manx Radio is the only local service to broadcast on AM medium wave. No UK services are relayed via local AM transmitters. No longwave stations operate from the Island, although one (Musicmann279) was proposed. There are currently no proposals to broadcast any of the three insular FM stations on DAB.


There is no island-specific television service. Local transmitters retransmit UK Freeview broadcasts. The BBC region is BBC North West and the ITV region is Granada Television.

Many television services are available by satellite, such as Sky, and Freesat from the Astra 2/Eurobird 1 group, as well as services from a range of other satellites around Europe such as Astra 1 and Hot Bird.

Manx ViaSat-IOM, ManSat, Telesat-IOM companies uses the first communications satellite ViaSat-1 that launched in 2011 and positioned at the Isle of Man registered 115.1 degrees West longitude geostationary orbit point. In some areas, terrestrial television directly from the United Kingdom or Republic of Ireland can also be received.

Analogue television transmission ceased between 2008 and 2009, when limited local transmission of digital terrestrial television commenced. The UK's television licence regime extends to the island.

There is no island-specific opt-out of the BBC regional news programme "North West Tonight", in the way that the Channel Islands get their own version of "Spotlight".

Television was first received on the Isle of Man from the Holme Moss transmitter which started broadcasting BBC Television (later BBC One) from 12 October 1951. Signals from Holme Moss were easily received on the Isle of Man.

ITV television has been available on parts of the east of the Isle of Man on 3 May 1956 when Granada Television (and ABC Television from 5 May 1956 to 28 July 1968) transmissions started from the Winter Hill transmitting station, and to parts of the west of the island on 31 October 1959 from the Black Mountain transmitting station in Northern Ireland which broadcasts Ulster Television. Parts of the north of the island received Border Television since 1 September 1961, initially directly from the Caldbeck transmitting station in Cumberland (later became Cumbria from 1974). On 26 March 1965, Border Television commenced relay of their signal through a local transmitter on Richmond Hill, above sea level and from the centre of Douglas. The site allowed reliable reception of the Caldbeck signal, which is rebroadcast on a different frequency. The high transmission tower was re-sited from London, where it had been used for early ITV transmissions. Richmond Hill was decommissioned after the close of 405-line broadcasts, although the 200 ft tower remained in use for radio with Manx Radio transmitting on 96.9 MHz and then 97.3 MHz until 1989. Manx Radio moved their FM service to the Carnane site and the frequency changed to the current 97.2 MHz.

The television broadcasts are now transmitted from a high transmitter on a hill to the south of Douglas. The transmitter is operated by Arqiva and is directly fed using a fibre optic cable. There are further sub-relay transmitters across the island. Following a realignment of ITV regional services and the digital switchover, the Douglas relay switched ITV broadcasts to Granada Television on Thursday 17 July 2009.

The Broadcasting Act 1993 (An Act of Tynwald) allows for the establishment of local television services. Only one application for a licence to run such a service was received by the Communications Commission. That application was rejected.

According to the CIA World Factbook, in 1999 there were 27,490 televisions in use in the Isle of Man.


Isle of Man Post issues its own stamps for use within the island and for sending post off-island. Only Manx stamps are valid for sending mail using the postal system. The Isle of Man adopted postcodes in 1993 using the prefix IM to fit in with the already established UK postcode system.




</doc>
<doc id="14769" url="https://en.wikipedia.org/wiki?curid=14769" title="Transport in the Isle of Man">
Transport in the Isle of Man

There are a number of transport services around the Isle of Man, mostly consisting of paved roads, public transport, rail services, sea ports and an airport.

The island has a total of of public roads, all of which are paved. Roads are numbered using a numbering scheme similar to the numbering schemes of Great Britain and Northern Ireland; each road is assigned a letter, which represents the road's category, followed by a 1 or 2 digit number. "A" roads are the main roads of the island whilst roads labelled "B", "C", "D" or "U" decrease in size and/or quality. (The C, D and U numbers are not marked on most maps or on signposts.) There is no national speed limit – some roads may be driven at any speed which is safe and appropriate. Careless and dangerous driving laws still apply, so one may not drive at absolutely any speed, and there are local speed limits on many roads. Many unrestricted roads have frequent bends which even the most experienced driver cannot see round. Drivers are limited to in the first full year after passing their driving test (Isle of Man citizens are permitted to start driving at the age of sixteen) and some are not used to having to make progress in the same way as on a larger road network such as that in the UK: even a cautious driver can get from anywhere in the island to anywhere else in no more than sixty minutes.

Set against this is a strong culture of motor sport enthusiasm (pinnacled in the TT, but there are many events during the year) and many residents familiar with the roads are well used to traversing country roads at speeds illegal on similar roads elsewhere. This leads to a very diverse spread of both driving competence and speed. In an official survey in 2006 the introduction of blanket speed limits was refused by the population, suggesting that a large number appreciate the freedom.

There is a comprehensive bus network, operated by Bus Vannin, a department of the Isle of Man Government, with most routes originating or terminating in Douglas.

The island has a total of of railway. There are seven separate public rail or tram systems on the island:

Reduced in 2019 due to works on the promenade. These works have overrun badly, and as at October 2019 the situation with the horse trams in the 2020 season is uncertain.

All of these routes are seasonal.

The only commercial airport on the island is the Isle of Man Airport at Ronaldsway. Scheduled services operate to and from various cities in the United Kingdom and Ireland, operated by several different airlines.

The island's other paved runways are at Jurby and Andreas. Jurby remains in Isle of Man Government ownership and is used for motorsport events and, previously, airshows, while Andreas is privately owned and used by a local glider club. The old Hall Caine Airport, a grass field near Ramsey, is no longer used.

The Isle of Man Aircraft Register became operational on 1 May 2007. The register is open to all non-commercial aircraft and is intended to be of particular interest to professionally flown corporate operators.
As of November 2012 a total of 537 corporate and private aircraft had been registered.

There are ports at Castletown, Douglas, Peel, Port St Mary and Ramsey. Douglas is served by frequent ferries to/from England and occasional ferries to/from Ireland; the sole operator is the Isle of Man Steam Packet Company, with exclusive use of the Isle of Man Sea Terminal and the Douglas port linkspans under the conditions of the user agreement with the Isle of Man Government.

The Isle of Man register comprised 404 merchant ships of 1,000 GT or over at the end of 2017.



</doc>
<doc id="14773" url="https://en.wikipedia.org/wiki?curid=14773" title="Information theory">
Information theory

Information theory studies the quantification, storage, and communication of information. It was originally proposed by Claude Shannon in 1948 to find fundamental limits on signal processing and communication operations such as data compression, in a landmark paper titled "A Mathematical Theory of Communication". Its impact has been crucial to the success of the Voyager missions to deep space, the invention of the compact disc, the feasibility of mobile phones, the development of the Internet, the study of linguistics and of human perception, the understanding of black holes, and numerous other fields.

The field is at the intersection of mathematics, statistics, computer science, physics, neurobiology, information engineering, and electrical engineering. The theory has also found applications in other areas, including statistical inference, natural language processing, cryptography, neurobiology, human vision, the evolution and function of molecular codes (bioinformatics), model selection in statistics, thermal physics, quantum computing, linguistics, plagiarism detection, pattern recognition, and anomaly detection. Important sub-fields of information theory include source coding, algorithmic complexity theory, algorithmic information theory, information-theoretic security, Grey system theory and measures of information.

Applications of fundamental topics of information theory include lossless data compression (e.g. ZIP files), lossy data compression (e.g. MP3s and JPEGs), and channel coding (e.g. for DSL). Information theory is used in information retrieval, intelligence gathering, gambling, and even in musical composition.

A key measure in information theory is entropy. Entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of a random process. For example, identifying the outcome of a fair coin flip (with two equally likely outcomes) provides less information (lower entropy) than specifying the outcome from a roll of a (with six equally likely outcomes). Some other important measures in information theory are mutual information, channel capacity, error exponents, and relative entropy.

Information theory studies the transmission, processing, extraction, and utilization of information. Abstractly, information can be thought of as the resolution of uncertainty. In the case of communication of information over a noisy channel, this abstract concept was made concrete in 1948 by Claude Shannon in his paper "A Mathematical Theory of Communication", in which "information" is thought of as a set of possible messages, where the goal is to send these messages over a noisy channel, and then to have the receiver reconstruct the message with low probability of error, in spite of the channel noise. Shannon's main result, the noisy-channel coding theorem showed that, in the limit of many channel uses, the rate of information that is asymptotically achievable is equal to the channel capacity, a quantity dependent merely on the statistics of the channel over which the messages are sent.

Information theory is closely associated with a collection of pure and applied disciplines that have been investigated and reduced to engineering practice under a variety of rubrics throughout the world over the past half-century or more: adaptive systems, anticipatory systems, artificial intelligence, complex systems, complexity science, cybernetics, informatics, machine learning, along with systems sciences of many descriptions. Information theory is a broad and deep mathematical theory, with equally broad and deep applications, amongst which is the vital field of coding theory.

Coding theory is concerned with finding explicit methods, called "codes", for increasing the efficiency and reducing the error rate of data communication over noisy channels to near the channel capacity. These codes can be roughly subdivided into data compression (source coding) and error-correction (channel coding) techniques. In the latter case, it took many years to find the methods Shannon's work proved were possible.

A third class of information theory codes are cryptographic algorithms (both codes and ciphers). Concepts, methods and results from coding theory and information theory are widely used in cryptography and cryptanalysis. "See the article ban (unit) for a historical application."

The landmark event that "established" the discipline of information theory and brought it to immediate worldwide attention was the publication of Claude E. Shannon's classic paper "A Mathematical Theory of Communication" in the "Bell System Technical Journal" in July and October 1948.

Prior to this paper, limited information-theoretic ideas had been developed at Bell Labs, all implicitly assuming events of equal probability. Harry Nyquist's 1924 paper, "Certain Factors Affecting Telegraph Speed", contains a theoretical section quantifying "intelligence" and the "line speed" at which it can be transmitted by a communication system, giving the relation (recalling Boltzmann's constant), where "W" is the speed of transmission of intelligence, "m" is the number of different voltage levels to choose from at each time step, and "K" is a constant. Ralph Hartley's 1928 paper, "Transmission of Information", uses the word "information" as a measurable quantity, reflecting the receiver's ability to distinguish one sequence of symbols from any other, thus quantifying information as , where "S" was the number of possible symbols, and "n" the number of symbols in a transmission. The unit of information was therefore the decimal digit, which has since sometimes been called the hartley in his honor as a unit or scale or measure of information. Alan Turing in 1940 used similar ideas as part of the statistical analysis of the breaking of the German second world war Enigma ciphers.

Much of the mathematics behind information theory with events of different probabilities were developed for the field of thermodynamics by Ludwig Boltzmann and J. Willard Gibbs. Connections between information-theoretic entropy and thermodynamic entropy, including the important contributions by Rolf Landauer in the 1960s, are explored in "Entropy in thermodynamics and information theory".

In Shannon's revolutionary and groundbreaking paper, the work for which had been substantially completed at Bell Labs by the end of 1944, Shannon for the first time introduced the qualitative and quantitative model of communication as a statistical process underlying information theory, opening with the assertion that

With it came the ideas of

Information theory is based on probability theory and statistics. Information theory often concerns itself with measures of information of the distributions associated with random variables. Important quantities of information are entropy, a measure of information in a single random variable, and mutual information, a measure of information in common between two random variables. The former quantity is a property of the probability distribution of a random variable and gives a limit on the rate at which data generated by independent samples with the given distribution can be reliably compressed. The latter is a property of the joint distribution of two random variables, and is the maximum rate of reliable communication across a noisy channel in the limit of long block lengths, when the channel statistics are determined by the joint distribution.

The choice of logarithmic base in the following formulae determines the unit of information entropy that is used. A common unit of information is the bit, based on the binary logarithm. Other units include the nat, which is based on the natural logarithm, and the decimal digit, which is based on the common logarithm.

In what follows, an expression of the form is considered by convention to be equal to zero whenever . This is justified because formula_1 for any logarithmic base.

Based on the probability mass function of each source symbol to be communicated, the Shannon entropy , in units of bits (per symbol), is given by
where is the probability of occurrence of the -th possible value of the source symbol. This equation gives the entropy in the units of "bits" (per symbol) because it uses a logarithm of base 2, and this base-2 measure of entropy has sometimes been called the shannon in his honor. Entropy is also commonly computed using the natural logarithm (base , where is Euler's number), which produces a measurement of entropy in nats per symbol and sometimes simplifies the analysis by avoiding the need to include extra constants in the formulas. Other bases are also possible, but less commonly used. For example, a logarithm of base will produce a measurement in bytes per symbol, and a logarithm of base 10 will produce a measurement in decimal digits (or hartleys) per symbol.

Intuitively, the entropy of a discrete random variable is a measure of the amount of "uncertainty" associated with the value of when only its distribution is known.

The entropy of a source that emits a sequence of symbols that are independent and identically distributed (iid) is bits (per message of symbols). If the source data symbols are identically distributed but not independent, the entropy of a message of length will be less than .

If one transmits 1000 bits (0s and 1s), and the value of each of these bits is known to the receiver (has a specific value with certainty) ahead of transmission, it is clear that no information is transmitted. If, however, each bit is independently equally likely to be 0 or 1, 1000 shannons of information (more often called bits) have been transmitted. Between these two extremes, information can be quantified as follows. If 𝕏 is the set of all messages that could be, and is the probability of some formula_3, then the entropy, , of is defined:

(Here, is the self-information, which is the entropy contribution of an individual message, and is the expected value.) A property of entropy is that it is maximized when all the messages in the message space are equiprobable ; i.e., most unpredictable, in which case .

The special case of information entropy for a random variable with two outcomes is the binary entropy function, usually taken to the logarithmic base 2, thus having the shannon (Sh) as unit:

The of two discrete random variables and is merely the entropy of their pairing: . This implies that if and are independent, then their joint entropy is the sum of their individual entropies.

For example, if represents the position of a chess piece — the row and the column, then the joint entropy of the row of the piece and the column of the piece will be the entropy of the position of the piece.

Despite similar notation, joint entropy should not be confused with .

The or "conditional uncertainty" of given random variable (also called the "equivocation" of about ) is the average conditional entropy over :

Because entropy can be conditioned on a random variable or on that random variable being a certain value, care should be taken not to confuse these two definitions of conditional entropy, the former of which is in more common use. A basic property of this form of conditional entropy is that:

"Mutual information" measures the amount of information that can be obtained about one random variable by observing another. It is important in communication where it can be used to maximize the amount of information shared between sent and received signals. The mutual information of relative to is given by:

where ("S"pecific mutual "I"nformation) is the pointwise mutual information.

A basic property of the mutual information is that
That is, knowing "Y", we can save an average of bits in encoding "X" compared to not knowing "Y".

Mutual information is symmetric:

Mutual information can be expressed as the average Kullback–Leibler divergence (information gain) between the posterior probability distribution of "X" given the value of "Y" and the prior distribution on "X":
In other words, this is a measure of how much, on the average, the probability distribution on "X" will change if we are given the value of "Y". This is often recalculated as the divergence from the product of the marginal distributions to the actual joint distribution:

Mutual information is closely related to the log-likelihood ratio test in the context of contingency tables and the multinomial distribution and to Pearson's χ test: mutual information can be considered a statistic for assessing independence between a pair of variables, and has a well-specified asymptotic distribution.

The "Kullback–Leibler divergence" (or "information divergence", "information gain", or "relative entropy") is a way of comparing two distributions: a "true" probability distribution "p(X)", and an arbitrary probability distribution "q(X)". If we compress data in a manner that assumes "q(X)" is the distribution underlying some data, when, in reality, "p(X)" is the correct distribution, the Kullback–Leibler divergence is the number of average additional bits per datum necessary for compression. It is thus defined

Although it is sometimes used as a 'distance metric', KL divergence is not a true metric since it is not symmetric and does not satisfy the triangle inequality (making it a semi-quasimetric).

Another interpretation of the KL divergence is the "unnecessary surprise" introduced by a prior from the truth: suppose a number "X" is about to be drawn randomly from a discrete set with probability distribution "p(x)". If Alice knows the true distribution "p(x)", while Bob believes (has a prior) that the distribution is "q(x)", then Bob will be more surprised than Alice, on average, upon seeing the value of "X". The KL divergence is the (objective) expected value of Bob's (subjective) surprisal minus Alice's surprisal, measured in bits if the "log" is in base 2. In this way, the extent to which Bob's prior is "wrong" can be quantified in terms of how "unnecessarily surprised" it is expected to make him.

Other important information theoretic quantities include Rényi entropy (a generalization of entropy), differential entropy (a generalization of quantities of information to continuous distributions), and the conditional mutual information.

Coding theory is one of the most important and direct applications of information theory. It can be subdivided into source coding theory and channel coding theory. Using a statistical description for data, information theory quantifies the number of bits needed to describe the data, which is the information entropy of the source.


This division of coding theory into compression and transmission is justified by the information transmission theorems, or source–channel separation theorems that justify the use of bits as the universal currency for information in many contexts. However, these theorems only hold in the situation where one transmitting user wishes to communicate to one receiving user. In scenarios with more than one transmitter (the multiple-access channel), more than one receiver (the broadcast channel) or intermediary "helpers" (the relay channel), or more general networks, compression followed by transmission may no longer be optimal. Network information theory refers to these multi-agent communication models.

Any process that generates successive messages can be considered a of information. A memoryless source is one in which each message is an independent identically distributed random variable, whereas the properties of ergodicity and stationarity impose less restrictive constraints. All such sources are stochastic. These terms are well studied in their own right outside information theory.

Information "rate" is the average entropy per symbol. For memoryless sources, this is merely the entropy of each symbol, while, in the case of a stationary stochastic process, it is

that is, the conditional entropy of a symbol given all the previous symbols generated. For the more general case of a process that is not necessarily stationary, the "average rate" is

that is, the limit of the joint entropy per symbol. For stationary sources, these two expressions give the same result.

It is common in information theory to speak of the "rate" or "entropy" of a language. This is appropriate, for example, when the source of information is English prose. The rate of a source of information is related to its redundancy and how well it can be compressed, the subject of .

Communications over a channel—such as an ethernet cable—is the primary motivation of information theory. However, such channels often fail to produce exact reconstruction of a signal; noise, periods of silence, and other forms of signal corruption often degrade quality.

Consider the communications process over a discrete channel. A simple model of the process is shown below:

Here "X" represents the space of messages transmitted, and "Y" the space of messages received during a unit time over our channel. Let be the conditional probability distribution function of "Y" given "X". We will consider to be an inherent fixed property of our communications channel (representing the nature of the "noise" of our channel). Then the joint distribution of "X" and "Y" is completely determined by our channel and by our choice of , the marginal distribution of messages we choose to send over the channel. Under these constraints, we would like to maximize the rate of information, or the "signal", we can communicate over the channel. The appropriate measure for this is the mutual information, and this maximum mutual information is called the and is given by:
This capacity has the following property related to communicating at information rate "R" (where "R" is usually bits per symbol). For any information rate "R < C" and coding error ε > 0, for large enough "N", there exists a code of length "N" and rate ≥ R and a decoding algorithm, such that the maximal probability of block error is ≤ ε; that is, it is always possible to transmit with arbitrarily small block error. In addition, for any rate "R > C", it is impossible to transmit with arbitrarily small block error.

"Channel coding" is concerned with finding such nearly optimal codes that can be used to transmit data over a noisy channel with a small coding error at a rate near the channel capacity.



Information theoretic concepts apply to cryptography and cryptanalysis. Turing's information unit, the ban, was used in the Ultra project, breaking the German Enigma machine code and hastening the end of World War II in Europe. Shannon himself defined an important concept now called the unicity distance. Based on the redundancy of the plaintext, it attempts to give a minimum amount of ciphertext necessary to ensure unique decipherability.

Information theory leads us to believe it is much more difficult to keep secrets than it might first appear. A brute force attack can break systems based on asymmetric key algorithms or on most commonly used methods of symmetric key algorithms (sometimes called secret key algorithms), such as block ciphers. The security of all such methods currently comes from the assumption that no known attack can break them in a practical amount of time.

Information theoretic security refers to methods such as the one-time pad that are not vulnerable to such brute force attacks. In such cases, the positive conditional mutual information between the plaintext and ciphertext (conditioned on the key) can ensure proper transmission, while the unconditional mutual information between the plaintext and ciphertext remains zero, resulting in absolutely secure communications. In other words, an eavesdropper would not be able to improve his or her guess of the plaintext by gaining knowledge of the ciphertext but not of the key. However, as in any other cryptographic system, care must be used to correctly apply even information-theoretically secure methods; the Venona project was able to crack the one-time pads of the Soviet Union due to their improper reuse of key material.

Pseudorandom number generators are widely available in computer language libraries and application programs. They are, almost universally, unsuited to cryptographic use as they do not evade the deterministic nature of modern computer equipment and software. A class of improved random number generators is termed cryptographically secure pseudorandom number generators, but even they require random seeds external to the software to work as intended. These can be obtained via extractors, if done carefully. The measure of sufficient randomness in extractors is min-entropy, a value related to Shannon entropy through Rényi entropy; Rényi entropy is also used in evaluating randomness in cryptographic systems. Although related, the distinctions among these measures mean that a random variable with high Shannon entropy is not necessarily satisfactory for use in an extractor and so for cryptography uses.

One early commercial application of information theory was in the field of seismic oil exploration. Work in this field made it possible to strip off and separate the unwanted noise from the desired seismic signal. Information theory and digital signal processing offer a major improvement of resolution and image clarity over previous analog methods.

Semioticians and Winfried Nöth both considered Charles Sanders Peirce as having created a theory of information in his works on semiotics. Nauta defined semiotic information theory as the study of "the internal processes of coding, filtering, and information processing."

Concepts from information theory such as redundancy and code control have been used by semioticians such as Umberto Eco and to explain ideology as a form of message transmission whereby a dominant social class emits its message by using signs that exhibit a high degree of redundancy such that only one message is decoded among a selection of competing ones.

Information theory also has applications in Gambling and information theory, black holes, and bioinformatics.





</doc>
<doc id="14774" url="https://en.wikipedia.org/wiki?curid=14774" title="Information explosion">
Information explosion

The information explosion is the rapid increase in the amount of published information or data and the effects of this abundance. As the amount of available data grows, the problem of managing the information becomes more difficult, which can lead to information overload. The Online Oxford English Dictionary indicates use of the phrase in a March 1964 "New Statesman" article. "The New York Times" first used the phrase in its editorial content in an article by Walter Sullivan on June 7, 1964, in which he described the phrase as "much discussed". (p11.) The earliest use of the phrase seems to have been in an IBM advertising supplement to the New York Times published on April 30, 1961, and by Frank Fremont-Smith, Director of the American Institute of Biological Sciences Interdisciplinary Conference Program, in an April 1961 article in the AIBS Bulletin (p18.) 

Many sectors are seeing this rapid increase in the amount of information available such as healthcare, supermarkets, and even governments with birth certificate informations and immunization records. Another sector that is being affected by this phenomenon is journalism. Such profession, which in the past was responsible for the dissemination of information, may be suppressed by so many sources of information today.

Techniques to gather knowledge from an overabundance of electronic information (e.g., data fusion may help in data mining) have existed since the 1970s. Another common technique to deal with such amount of information is qualitative research. Such approach aims at organizing the information, synthesizing, categorizing and systematizing in order to be more usable and easier to search.


A new metric that is being used in an attempt to characterize the growth in person-specific information, is the disk storage per person (DSP), which is measured in megabytes/person (where megabytes is 10 bytes and is abbreviated MB). Global DSP (GDSP) is the total rigid disk drive space (in MB) of new units sold in a year divided by the world population in that year. The GDSP metric is a crude measure of how much disk storage could possibly be used to collect person-specific data on the world population. 
In 1983, one million fixed drives with an estimated total of 90 terabytes were sold worldwide; 30MB drives had the largest market segment. In 1996, 105 million drives, totaling 160,623 terabytes were sold with 1 and 2 gigabyte drives leading the industry. By the year 2000, with 20GB drive leading the industry, rigid drives sold for the year are projected to total 2,829,288 terabytes Rigid disk drive sales to top $34 billion in 1997.

According to Latanya Sweeney, there are three trends in data gathering today:

Type 1. Expansion of the number of fields being collected, known as the “collect more” trend.

Type 2. Replace an existing aggregate data collection with a person-specific one, known as the “collect specifically” trend.

Type 3. Gather information by starting a new person-specific data collection, known as the “collect it if you can” trend.

Since "information" in electronic media is often used synonymously with "data", the term "information explosion" is closely related to the concept of "data flood" (also dubbed "data deluge"). Sometimes the term "information flood" is used as well. All of those basically boil down to the ever-increasing amount of electronic data exchanged per time unit. The awareness about non-manageable amounts of data grew along with the advent of ever more powerful data processing since the mid-1960s.

Even though the abundance of information can be beneficial in several levels, some problems may be of concern such as privacy, legal and ethical guidelines, filtering and data accuracy. Filtering refers to finding useful information in the middle of so much data, which relates to the job of data scientists. A typical example of a necessity of data filtering (data mining) is in healthcare since in the next years is due to have EHRs (Electronic Health Records) of patients available. With so much information available, the doctors will need to be able to identify patterns and select important data for the diagnosis of the patient. On the other hand, according to some experts, having so much public data available makes it difficult to provide data that is actually anonymous.
Another point to take into account is the legal and ethical guidelines, which relates to who will be the owner of the data and how frequently he/she is obliged to the release this and for how long.
With so many sources of data, another problem will be accuracy of such. An untrusted source may be challenged by others, by ordering a new set of data, causing a repetition in the information.
According to Edward Huth, another concern is the accessibility and cost of such information. The accessibility rate could be improved by either reducing the costs or increasing the utility of the information. The reduction of costs according to the author, could be done by associations, which should assess which information was relevant and gather it in a more organized fashion.

As of August 2005, there were over 70 million web servers. there were over 135 million web servers.

According to Technorati, the number of blogs doubles about every 6 months with a total of 35.3 million blogs . This is an example of the early stages of logistic growth, where growth is approximately exponential, since blogs are a recent innovation. As the number of blogs approaches the number of possible producers (humans), saturation occurs, growth declines, and the number of blogs eventually stabilizes.




</doc>
<doc id="14775" url="https://en.wikipedia.org/wiki?curid=14775" title="Inch">
Inch

The inch (abbreviation: in or ″) is a unit of length in the (British) imperial and United States customary systems of measurement. It is equal to yard or of a foot. Derived from the Roman uncia ("twelfth"), the word "inch" is also sometimes used to translate similar units in other measurement systems, usually understood as deriving from the width of the human thumb. 

Standards for the exact length of an inch have varied in the past, but since the adoption of the international yard during the 1950s and 1960s, it has been based on the metric system and defined as exactly 25.4mm.

The English word "inch" () was an early borrowing from Latin "" ("one-twelfth; Roman inch; Roman ounce") not present in other Germanic languages. The vowel change from Latin to Old English (which became Modern English ) is known as umlaut. The consonant change from the Latin (spelled "c") to English is palatalisation. Both were features of Old English phonology; see and for more information.

"Inch" is cognate with "ounce" (), whose separate pronunciation and spelling reflect its reborrowing in Middle English from Anglo-Norman "unce" and "ounce".

In many other European languages, the word for "inch" is the same as or derived from the word for "thumb", as a man's thumb is about an inch wide (and this was even sometimes used to define the inch). Examples include ; ("inch") and ' ("thumb"); ("thumb"); Danish and ("inch") ' ("thumb"); ; ; ; ; ("inch") and ' ("thumb"); ("thumb"); ("inch") and ' ("thumb"); ("inch") and "tumme" ("thumb"); and ("duim").

The inch is a commonly used customary unit of length in the United States, Canada, and the United Kingdom. It is also used in Japan for electronic parts, especially display screens. In most of continental Europe, the inch is also used informally as a measure for display screens. For the United Kingdom, guidance on public sector use states that, since 1 October 1995, without time limit, the inch (along with the foot) is to be used as a primary unit for road signs and related measurements of distance (with the possible exception of clearance heights and widths) and may continue to be used as a secondary or supplementary indication following a metric measurement for other purposes. 

Inches are commonly used to specify the diameter of vehicle wheel rims, and the corresponding inner diameter of tyres – the last number in a Car/Truck tire size such as 235/75 R16; the first two numbers give the width (normally expressed in millimetres for cars & Light trucks) and aspect ratio of the tyre (height 75% of width in this example), the R Designates a Radial Ply Construction. Wheel manufacturers commonly specify the wheel width in inches (typically 6.5, 7, 7.5, or 8 for 235/75 tire).

The international standard symbol for inch is in (see ISO 31-1, Annex A) but traditionally the inch is denoted by a double prime, which is often approximated by double quotes, and the foot by a prime, which is often approximated by an apostrophe. For example, can be written as 3′ 2″. (This is akin to how the first and second "cuts" of the hour and degree are likewise indicated by prime and double prime symbols.) 

Subdivisions of an inch are typically written using dyadic fractions with odd number numerators; for example, would be written as ″ and not as 2.375″ nor as ″. However for engineering purposes fractions are commonly given to three or four places of decimals and have been for many years.

1 international inch is equal to:

The earliest known reference to the inch in England is from the "Laws of Æthelberht" dating to the early 7th century, surviving in a single manuscript, the "Textus Roffensis" from 1120. Paragraph LXVII sets out the fine for wounds of various depths: one inch, one shilling, two inches, two shillings, etc.

An Anglo-Saxon unit of length was the barleycorn. After 1066, 1 inch was equal to 3 barleycorns, which continued to be its legal definition for several centuries, with the barleycorn being the base unit. One of the earliest such definitions is that of 1324, where the legal definition of the inch was set out in a statute of Edward II of England, defining it as "three grains of barley, dry and round, placed end to end, lengthwise".

Similar definitions are recorded in both English and Welsh medieval law tracts. One, dating from the first half of the 10th century, is contained in the Laws of Hywel Dda which superseded those of Dyfnwal, an even earlier definition of the inch in Wales. Both definitions, as recorded in "Ancient Laws and Institutes of Wales" (vol i., pp. 184, 187, 189), are that "three lengths of a barleycorn is the inch".

King David I of Scotland in his Assize of Weights and Measures (c. 1150) is said to have defined the Scottish inch as the width of an average man's thumb at the base of the nail, even including the requirement to calculate the average of a small, a medium, and a large man's measures. However, the oldest surviving manuscripts date from the early 14th century and appear to have been altered with the inclusion of newer material.

In 1814, Charles Butler, a mathematics teacher at Cheam School, recorded the old legal definition of the inch to be "three grains of sound ripe barley being taken out the middle of the ear, well dried, and laid end to end in a row", and placed the barleycorn, not the inch, as the base unit of the English Long Measure system, from which all other units were derived. John Bouvier similarly recorded in his 1843 law dictionary that the barleycorn was the fundamental measure. Butler observed, however, that "[a]s the length of the barley-corn cannot be fixed, so the inch according to this method will be uncertain", noting that a standard inch measure was now [i.e. by 1843] kept in the Exchequer chamber, Guildhall, and "that" was the legal definition of the inch. 

This was a point also made by George Long in his 1842 "Penny Cyclopædia", observing that standard measures had since surpassed the barleycorn definition of the inch, and that to recover the inch measure from its original definition, in the event that the standard measure were destroyed, would involve the measurement of large numbers of barleycorns and taking their average lengths. He noted that this process would not perfectly recover the standard, since it might introduce errors of anywhere between one hundredth and one tenth of an inch in the definition of a yard.

Before the adoption of the international yard and pound, various definitions were in use. In the United Kingdom and most countries of the British Commonwealth, the inch was defined in terms of the Imperial Standard Yard. The United States adopted the conversion factor 1 metre = 39.37 inches by an act in 1866. In 1893, Mendenhall ordered the physical realization of the inch to be based on the international prototype metres numbers 21 and 27, which had been received from the CGPM, together with the previously adopted conversion factor.

As a result of the definitions above, the U.S. inch was effectively defined as 25.4000508 mm (with a reference temperature of 68 degrees Fahrenheit) and the U.K. inch at 25.399977 mm (with a reference temperature of 62 degrees Fahrenheit). When Carl Edvard Johansson started manufacturing gauge blocks in inch sizes in 1912, Johnanson's compromise was to manufacture gauge blocks with a nominal size of 25.4mm, with a reference temperature of 20 degrees Celsius, accurate to within a few parts per million of both official definitions. Because Johannson's blocks were so popular, his blocks became the "de facto" standard for manufacturers internationally, with other manufacturers of gauge blocks following Johannson's definition by producing blocks designed to be equivalent to his. 

In 1930, the British Standards Institution adopted an inch of exactly 25.4 mm. The American Standards Association followed suit in 1933. By 1935, industry in 16 countries had adopted the "industrial inch" as it came to be known, effectively endorsing Johannson's pragmatic choice of conversion ratio.

In 1946, the Commonwealth Science Congress recommended a yard of exactly 0.9144 metres for adoption throughout the British Commonwealth. This was adopted by Canada in 1951; the United States on 1 July 1959; Australia in 1961, effective 1 January 1964; and the United Kingdom in 1963, effective on 1 January 1964. The new standards gave an inch of exactly 25.4 mm, 1.7 millionths of an inch longer than the old imperial inch and 2 millionths of an inch shorter than the old US inch.

The United States retains the -metre definition for survey purposes, producing a 2 millionth part difference between standard and US survey inches. This is approximately  inch per mile. In fact, 12.7 kilometres is exactly standard inches and exactly survey inches. This difference is significant when doing calculations in State Plane Coordinate Systems with coordinate values in the hundreds of thousands or millions of feet.

In 2020, the U.S. NIST announced that the survey foot would be deprecated from 2022, and by implication, the survey inch with it.

Before the adoption of the metric system, several European countries had customary units whose name translates into "inch". The French "pouce" measured roughly 27.0 mm, at least when applied to describe the calibre of artillery pieces. The Amsterdam foot ("voet") consisted of 11 Amsterdam inches ("duim"). The Amsterdam foot is about 8% shorter than an English foot.

The now obsolete Scottish inch (), of a Scottish foot, was about 1.0016 imperial inches (about ).




</doc>
<doc id="14776" url="https://en.wikipedia.org/wiki?curid=14776" title="Inn">
Inn

Inns are generally establishments or buildings where travelers can seek lodging, and usually, food and drink. Inns are typically located in the country or along a highway; before the advent of motorized transportation they also provided accommodation for horses.

Inns in Europe were possibly first established when the Romans built their system of Roman roads two millennia ago. Many inns in Europe are several centuries old. In addition to providing for the needs of travelers, inns traditionally acted as community gathering places.

Historically, inns in Europe provided not only food and lodging, but stabling and fodder for the travelers' horses, as well. Famous London examples of inns include The George and The Tabard. However, there is no longer a formal distinction between an inn and several other kinds of establishments: many pubs use the name "inn", either because they are long established and may have been formerly coaching inns, or to summon up a particular kind of image.

Inns were like bed and breakfasts, with a community dining room which was also used for town meetings or rented for wedding parties. The front, facing the road, was ornamental and welcoming for travelers. The back also usually had at least one livery barn for travelers to keep their horses. There were no lobbies as in modern inns; rather, the innkeeper would answer the door for each visitor and judge the people whom he decided to accommodate. Many inns were simply large houses that had extra rooms for renting.

During the 19th century, the inn played a major role in the growing transportation system of England. Industry was on the rise, and people were traveling more in order to keep and maintain business. The English inn was considered an important part of English infrastructure, as it helped maintain a smooth flow of travel throughout the country.

As modes of transport have evolved, tourist lodging has adapted to serve each generation of traveller. A stagecoach made frequent stops at roadside coaching inns for water, food, and horses. A passenger train stopped only at designated stations in the city centre, around which were built grand railway hotels. Motorcar traffic on old-style two-lane highways might have paused at any camp, cabin court, or motel along the way, while freeway traffic was restricted to access from designated off-ramps to side roads which quickly become crowded with hotel chain operators.

The original functions of an inn are now usually split among separate establishments. For example, hotels, lodges and motels might provide the traditional functions of an inn but focus more on lodging customers than on other services; public houses (pubs) are primarily alcohol-serving establishments; and restaurants and taverns serve food and drink. (Hotels often contain restaurants serving full breakfasts and meals, thus providing all of the functions of traditional inns. Economy, limited service properties, however, lack a kitchen and bar, and therefore claim at most an included continental breakfast.)

The lodging aspect of the word "inn" lives on in some hotel brand names, like Holiday Inn, and the Inns of Court in London were once accommodations for members of the legal profession. Some laws refer to lodging operators as "innkeepers".

Other forms of inns exist throughout the world. Among them are the honjin and ryokan of Japan, caravanserai of Central Asia and the Middle East, and Jiuguan in ancient China.

In Asia Minor, during the periods of rule by the Seljuq and Ottoman Turks, impressive structures functioning as inns () were built because inns were considered socially significant. These inns provided accommodations for people and either their vehicles or animals, and served as a resting place to those travelling on foot or by other means.

These inns were built between towns if the distance between municipalities was too far for one day's travel. These structures, called caravansarais, were inns with large courtyards and ample supplies of water for drinking and other uses. They typically contained a café, in addition to supplies of food and fodder. After the caravans traveled a while they would take a break at these caravansarais, and often spend the night to rest the human travellers and their animals.

The term "inn" historically characterized a rural hotel which provided lodging, food and refreshments, and accommodations for travelers' horses. To capitalize on this nostalgic image many typically lower end and middling modern motor hotel operators seek to distance themselves from similar motels by styling themselves "inns", regardless of services and accommodations provided. Examples are Comfort Inn, Days Inn, Holiday Inn, Knights Inn, and Premier Inn.

The term "inn" is also retained in its historic use in many laws governing motels and hotels, often known as "innkeeper's acts", or refer to hôteliers and motel operators as "innkeepers" in the body of the legislation These laws typically define the innkeepers' liability for valuables entrusted to them by clients and determine whether an innkeeper holds any lien against such goods. In some jurisdictions, an offence named as "defrauding an innkeeper" prohibits fraudulently obtaining "food, lodging, or other accommodation at any hotel, inn, boarding house, or eating house"; in this context, the term is often an anachronism as the majority of modern restaurants are free-standing and not attached to coaching inns or tourist lodging.




</doc>
<doc id="14777" url="https://en.wikipedia.org/wiki?curid=14777" title="International Olympiad in Informatics">
International Olympiad in Informatics

The International Olympiad in Informatics (IOI) is an annual competitive programming competition for secondary school students. It is the second largest olympiad, after International Mathematical Olympiad, in terms of number of participating countries (83 at IOI 2017). The first IOI was held in 1989 in Pravetz, Bulgaria.

The contest consists of two days of computer programming/coding and problem-solving of algorithmic nature. To deal with problems involving very large amounts of data, it is necessary to have not only programmers, "but also creative coders, who can dream up what it is that the programmers need to tell the computer to do. The hard part isn't the programming, but the mathematics underneath it." Students at the IOI compete on an individual basis, with up to four students competing from each participating country (with 81 countries in 2012). Students in the national teams are selected through national computing contests, such as the Australian Informatics Olympiad, British Informatics Olympiad, Indian Computing Olympiad or Bundeswettbewerb Informatik (Germany).

The International Olympiad in Informatics is one of the most prestigious computer science competitions in the world. UNESCO and IFIP are patrons.

On each of the two competition days, the students are typically given three problems which they have to solve in five hours. Each student works on his/her own, with only a computer and no other help allowed, specifically no communication with other contestants, books etc. Usually to solve a task the contestant has to write a computer program (in C, C++, Pascal, or Java) and submit it before the five-hour competition time ends. The program is graded by being run with secret test data. From IOI 2010, tasks are divided into subtasks with graduated difficulty, and points are awarded only when all tests for a particular subtask yield correct results, within specific time and memory limits. In some cases, the contestant's program has to interact with a secret computer library, which allows problems where the input is not fixed, but depends on the program's actions – for example in game problems. Another type of problem has known inputs which are publicly available already during the five hours of the contest. For these, the contestants have to submit an output file instead of a program, and it is up to them whether they obtain the output files by writing a program (possibly exploiting special characteristics of the input), or by hand, or by a combination of these means. Pascal will have been removed as an available programming language by 2019.

IOI 2010 for the first time had a live web scoreboard with real-time provisional results. Submissions will be scored as soon as possible during the contest, and the results posted. Contestants will be aware of their scores, but not others', and may resubmit to improve their scores. Starting from 2012, IOI has been using the Contest Management System (CMS) for developing and monitoring the contest.

The scores from the two competition days and all problems are summed up separately for each contestant. At the awarding ceremony, contestants are awarded medals depending on their relative total score. The top 50% of the contestants are awarded medals, such that the relative number of gold : silver : bronze : no medal is approximately 1:2:3:6 (thus 1/12 of the contestants get a gold medal).

Prior to IOI 2010, students who did not receive medals did not have their scores published, making it impossible for a country to be ranked by adding together scores of its competitors unless each wins a medal. From IOI 2010, although the scores of students who did not receive medals are still not available in the official results, they are known from the live web scoreboard. In IOI 2012 the top 3 nations ranked by aggregate score (Russia, China and USA) were subsequently awarded during the closing ceremony.

Analysis of female performance shows 77.9 % of women obtain no medal, while 49.2 % of men obtain no medal. "The average female participation was 4.4% in 1989–1994 and 2.2% in 1996–2014." It also suggests women participate much more on the national level, claiming sometimes a double-digit percentage of women participate on the first stage. President of the IOI, Richard Forster, says the competition has difficulty attracting women and that in spite of trying to solve it, "none of us have hit on quite what the problem is, let alone the solution."

In IOI 2017 held in Iran, due to not being able to participate in Iran, the Israeli students participated in an offsite competition organized by IOI in Russia. Due to visa issues, the full USA team was unable to attend, although one contestant Zhezheng Luo was able to attend by traveling with the Chinese team and winning gold medal and 3rd place in standings.

In IOI 2019 held in Azerbaijan, the Armenia team did not participate due to the dispute between the two countries.

For 2020, due to COVID-19 pandemic, the IOI 2020 which will be originally scheduled to host in Singapore in July, is postponed to September, and later converted to online format. Singapore will host the onsite IOI 2021, replacing Egypt which will host IOI 2024.

The following is a list of the top performers in the history of the IOI. The sign indicates a perfect score, a rare achievement in IOI history. The sign indicates an unofficial participation, where a contestant participated in a host's second team. Also, first (I), second (II) and third (III) places among gold medalists are indicated where appropriate. This list includes only those countries where the national selection contest allows the same participant to go multiple times to the IOI.

Most participating countries use feeder competitions to select their team. A number of these are listed below:




</doc>
<doc id="14779" url="https://en.wikipedia.org/wiki?curid=14779" title="Iota">
Iota

Iota (uppercase Ι, lowercase ι; ) is the ninth letter of the Greek alphabet. It was derived from the Phoenician letter Yodh. Letters that arose from this letter include the Latin I and J, the Cyrillic І (І, і), Yi (Ї, ї), and Je (Ј, ј), and iotated letters (e.g. Yu (Ю, ю)).
In the system of Greek numerals, iota has a value of 10.

Iota represents the sound . In early forms of ancient Greek, it occurred in both long and short versions, but this distinction was lost in Koine Greek.

Iota participated as the second element in falling diphthongs, with both long and short vowels as the first element. Where the first element was long, the iota was lost in pronunciation at an early date, and was written in polytonic orthography as iota subscript, in other words as a very small ι under the main vowel. Examples include ᾼ ᾳ ῌ ῃ ῼ ῳ. The former diphthongs became digraphs for simple vowels in Koine Greek.

The word is used in a common English phrase, "not one iota", meaning "not the slightest amount", in reference to a phrase in the New Testament (Matthew 5:18): "until heaven and earth pass away, not an iota, not a dot, (King James Version: '[not] one jot or one tittle') will pass from the Law until all is accomplished". () This refers to iota, the smallest letter, or possibly Yodh, י, the smallest letter in the Hebrew alphabet.

The word 'jot' (or "iot") derives from iota.

The German, Portuguese, and Spanish name for the letter J (Jot / jota) is derived from iota.








These characters are used only as mathematical symbols. Stylized Greek text should be encoded using the normal Greek letters, with markup and formatting to indicate text style.


</doc>
<doc id="14780" url="https://en.wikipedia.org/wiki?curid=14780" title="ISP (disambiguation)">
ISP (disambiguation)

ISP often refers to Internet service provider. ISP may also refer to:







</doc>
<doc id="14783" url="https://en.wikipedia.org/wiki?curid=14783" title="Erectile dysfunction">
Erectile dysfunction

Erectile dysfunction (ED), also known as impotence, is a type of sexual dysfunction characterized by the inability to develop or maintain an erection of the penis during sexual activity. ED can have psychological consequences as it can be tied to relationship difficulties and self-image.
A physical cause can be identified in about 80% of cases. These include cardiovascular disease, diabetes mellitus, neurological problems such as following prostatectomy, hypogonadism, and drug side effects. Psychological impotence is where erection or penetration fails due to thoughts or feelings; this is somewhat less frequent, on the order of about 10% of cases. In psychological impotence, there is a strong response to placebo treatment. The term "erectile dysfunction" is not used for other disorders of erection, such as priapism.
Treatment involves addressing the underlying causes, lifestyle modifications, and addressing psychosocial issues. In many cases, a trial of pharmacological therapy with a PDE5 inhibitor, such as sildenafil, can be attempted. In some cases, treatment can involve inserting prostaglandin pellets into the urethra, injecting smooth muscle relaxants and vasodilators into the penis, a penile implant, a penis pump, or vascular reconstructive surgery. It is the most common sexual problem in men.

ED is characterized by the regular or repeated inability to achieve or maintain an erection of sufficient rigidity to accomplish sexual activity. It is defined as the "persistent or recurrent inability to achieve and maintain a penile erection of sufficient rigidity to permit satisfactory sexual activity for at least 3 months."

ED often has an impact on the emotional well-being of both men and their partners. Many men do not seek treatment due to feelings of embarrassment. About 75% of diagnosed cases of ED go untreated.

Causes of or contributors to ED include the following:

Surgical intervention for a number of conditions may remove anatomical structures necessary to erection, damage nerves, or impair blood supply. ED is a common complication of treatments for prostate cancer, including prostatectomy and destruction of the prostate by external beam radiation, although the prostate gland itself is not necessary to achieve an erection. As far as inguinal hernia surgery is concerned, in most cases, and in the absence of postoperative complications, the operative repair can lead to a recovery of the sexual life of people with preoperative sexual dysfunction, while, in most cases, it does not affect people with a preoperative normal sexual life.

ED can also be associated with bicycling due to both neurological and vascular problems due to compression. The increase risk appears to be about 1.7-fold.

Concerns that use of pornography can cause ED have little support in epidemiological studies, according to a 2015 literature review.

Penile erection is managed by two mechanisms: the reflex erection, which is achieved by directly touching the penile shaft, and the psychogenic erection, which is achieved by erotic or emotional stimuli. The former involves the peripheral nerves and the lower parts of the spinal cord, whereas the latter involves the limbic system of the brain. In both cases, an intact neural system is required for a successful and complete erection. Stimulation of the penile shaft by the nervous system leads to the secretion of nitric oxide (NO), which causes the relaxation of the smooth muscles of the corpora cavernosa (the main erectile tissue of the penis), and subsequently penile erection. Additionally, adequate levels of testosterone (produced by the testes) and an intact pituitary gland are required for the development of a healthy erectile system. As can be understood from the mechanisms of a normal erection, impotence may develop due to hormonal deficiency, disorders of the neural system, lack of adequate penile blood supply or psychological problems. Spinal cord injury causes sexual dysfunction, including ED. Restriction of blood flow can arise from impaired endothelial function due to the usual causes associated with coronary artery disease, but can also be caused by prolonged exposure to bright light.

In many cases, the diagnosis can be made based on the person's history of symptoms. In other cases, a physical examination and laboratory investigations are done to rule out more serious causes such as hypogonadism or prolactinoma.

One of the first steps is to distinguish between physiological and psychological ED. Determining whether involuntary erections are present is important in eliminating the possibility of psychogenic causes for ED. Obtaining full erections occasionally, such as nocturnal penile tumescence when asleep (that is, when the mind and psychological issues, if any, are less present), tends to suggest that the physical structures are functionally working. Similarly, performance with manual stimulation, as well as any performance anxiety or acute situational ED, may indicate a psychogenic component to ED.

Other factors leading to ED are diabetes mellitus, which is a well-known cause of neuropathy). ED is also related to generally poor physical health, poor dietary habits, obesity, and most specifically cardiovascular disease, such as coronary artery disease and peripheral vascular disease. Screening for cardiovascular risk factors, such as smoking, dyslipidemia, hypertension, and alcoholism is helpful.

In some particular cases, the simple search for a previously undetected groin hernia can prove useful since it can affect sexual functions in men and is relatively easily curable.

The current diagnostic and statistical manual of mental diseases (DSM-IV) has included a listing for ED.

Penile ultrasonography with doppler can be used to examine the penis in erected state. Most cases of ED of organic causes are related to changes in blood flow in the corpora cavernosa, represented by occlusive artery disease, most often of atherosclerotic origin, or due to failure of the veno-occlusive mechanism. Preceding the ultrasound examination with Doppler, the penis should be examined in B mode, in order to identify possible tumors, fibrotic plaques, calcifications, or hematomas, as well as to evaluate the appearance of the cavernous arteries, which can be tortuous or atheromatous.

Erection can be induced by injecting 10-20 µg of prostaglandin E1, with evaluations of the arterial flow every five minutes for 25-30 min (see image). The use of prostaglandin E1 is contraindicated in patients with a predisposition to priapism (e.g., those with sickle cell anemia), as well as in those with an anatomical deformity of the penis or a penile implant. Phentolamine (2 mg) is often added. Visual and tactile stimulation produces better results. Some authors recommend the use of sildenafil by mouth to replace the injectable drugs in cases of contraindications, although the efficacy of such medication is controversial.

Prior to the injection of the chosen drug, the flow pattern is monophasic, with low systolic velocities and an absence of diastolic flow. After injection, it is expected that systolic and diastolic peak velocities will increase, decreasing progressively with vein occlusion and becoming negative when the penis becomes rigid (see image below). The reference values vary across studies, ranging from > 25 cm/s to > 35 cm/s. Values above 35 cm/s indicate the absence of arterial disease, values below 25 cm/s indicate arterial insufficiency, and values of 25–35 cm/s are indeterminate because they are less specific (see image below). The data obtained should be correlated with the degree of erection observed. If the peak systolic velocities are normal, the final diastolic velocities should be evaluated, those above 5 cm/s being associated with venogenic ED.







Treatment depends on the underlying cause. In general, exercise, particularly of the aerobic type, is effective for preventing ED during midlife. Counseling can be used if the underlying cause is psychological, including how to lower stress or anxiety related to sex. Medications by mouth and vacuum erection devices are first-line treatments, followed by injections of drugs into the penis, as well as penile implants. Vascular reconstructive surgeries are beneficial in certain groups. Treatments, other than surgery, do not fix the underlying physiological problem, but are used as needed before sex.

The PDE5 inhibitors sildenafil (Viagra), vardenafil (Levitra) and tadalafil (Cialis) are prescription drugs which are taken by mouth. As of 2018, sildenafil is available in the UK without a prescription. Additionally, a cream combining alprostadil with the permeation enhancer DDAIP has been approved in Canada as a first line treatment for ED. Penile injections, on the other hand, can involve one of the following medications: papaverine, phentolamine, and prostaglandin E1, also known as alprostadil. In addition to injections, there is an alprostadil suppository that can be inserted into the urethra. Once inserted, an erection can begin within 10 minutes and last up to an hour. Medications to treat ED may cause a side effect called priapism.

Men with low levels of testosterone can experience ED. Taking testosterone may help maintain an erection. Men with type 2 diabetes are twice as likely to have lower levels of testosterone, and are three times more likely to experience ED than non-diabetic men.

A vacuum erection device helps draw blood into the penis by applying negative pressure. This type of device is sometimes referred to as penis pump and may be used just prior to sexual intercourse. Several types of FDA approved vacuum therapy devices are available under prescription. When pharmacological methods fail, a purpose-designed external vacuum pump can be used to attain erection, with a separate compression ring fitted to the base of the penis to maintain it. These pumps should be distinguished from other penis pumps (supplied without compression rings) which, rather than being used for temporary treatment of impotence, are claimed to increase penis length if used frequently, or vibrate as an aid to masturbation. More drastically, inflatable or rigid penile implants may be fitted surgically.

Often, as a last resort if other treatments have failed, the most common procedure is prosthetic implants which involves the insertion of artificial rods into the penis. Some sources show that vascular reconstructive surgeries are viable options for some people.

The Food and Drug Administration (FDA) does not recommend alternative therapies to treat sexual dysfunction. Many products are advertised as "herbal viagra" or "natural" sexual enhancement products, but no clinical trials or scientific studies support the effectiveness of these products for the treatment of ED, and synthetic chemical compounds similar to sildenafil have been found as adulterants in many of these products. The FDA has warned consumers that any sexual enhancement product that claims to work as well as prescription products is likely to contain such a contaminant.

Attempts to treat ED date back well over 1,000 years. In the 8th century, men of Ancient Rome and Greece wore talismans of rooster and goat genitalia, believing these talismans would serve as an aphrodisiac and promote sexual function. In the 13th century Albertus Magnus recommended ingesting roasted wolf penis as a remedy for impotence.

During the late 16th and 17th centuries in France, male impotence was considered a crime, as well as legal grounds for a divorce. The practice, which involved inspection of the complainants by court experts, was declared obscene in 1677.

The first successful vacuum erection device, or penis pump, was developed by Vincent Marie Mondat in the early 1800s. A more advanced device, based on a bicycle pump, was developed by Geddings Osbon, a Pentecostal preacher, in the 1970s. In 1982, he received FDA approval to market the product as the ErecAid®.

John R. Brinkley initiated a boom in male impotence cures in the U.S. in the 1920s and 1930s. His radio programs recommended expensive goat gland implants and "mercurochrome" injections as the path to restored male virility, including operations by surgeon Serge Voronoff.

Modern drug therapy for ED made a significant advance in 1983, when British physiologist Giles Brindley dropped his trousers and demonstrated to a shocked Urodynamics Society audience his papaverine-induced erection. The drug Brindley injected into his penis was a non-specific vasodilator, an alpha-blocking agent, and the mechanism of action was clearly corporal smooth muscle relaxation. The effect that Brindley discovered established the fundamentals for the later development of specific, safe, and orally effective drug therapies.

The current first-line treatment for ED, the oral PDE5 inhibitor, was introduced by Pfizer in 1999.

The Latin term "impotentia coeundi" describes simple inability to insert the penis into the vagina; it is now mostly replaced by more precise terms, such as "erectile dysfunction" (ED). The study of ED within medicine is covered by andrology, a sub-field within urology. Research indicates that ED is common, and it is suggested that approximately 40% of males experience symptoms compatible with ED, at least occasionally. The condition is also on occasion called "phallic impotence". Its antonym, or opposite condition, is priapism.



</doc>
<doc id="14787" url="https://en.wikipedia.org/wiki?curid=14787" title="Iran–Contra affair">
Iran–Contra affair

The Iran–Contra affair (, ), popularized in Iran as the McFarlane affair, the Iran–Contra scandal, or simply Iran–Contra, was a political scandal in the United States that occurred during the second term of the Reagan Administration. Senior administration officials secretly facilitated the sale of arms to the Khomeini government of the Islamic Republic of Iran, which was the subject of an arms embargo. The administration hoped to use the proceeds of the arms sale to fund the Contras in Nicaragua. Under the Boland Amendment, further funding of the Contras by the government had been prohibited by Congress.

The official justification for the arms shipments was that they were part of an operation to free seven American hostages being held in Lebanon by Hezbollah, a paramilitary group with Iranian ties connected to the Islamic Revolutionary Guard Corps. The plan was for Israel to ship weapons to Iran, for the United States to resupply Israel, and for Israel to pay the United States. The Iranian recipients promised to do everything in their power to achieve the release of the hostages. The first arms sales authorized to Iran were in 1981, prior to the American hostages having been taken in Lebanon.

The plan was later complicated in late 1985, when Lieutenant Colonel Oliver North of the National Security Council diverted a portion of the proceeds from the Iranian weapon sales to fund the Contras, a group of anti-Sandinista rebels, in their insurgency against the socialist government of Nicaragua. While President Ronald Reagan was a vocal supporter of the Contra cause, the evidence is disputed as to whether he personally authorized the diversion of funds to the Contras. Handwritten notes taken by Defense Secretary Caspar Weinberger on 7 December 1985 indicate that Reagan was aware of potential hostage transfers with Iran, as well as the sale of Hawk and TOW missiles to "moderate elements" within that country. Weinberger wrote that Reagan said "he could answer to charges of illegality but couldn't answer to the charge that 'big strong President Reagan passed up a chance to free the hostages. After the weapon sales were revealed in November 1986, Reagan appeared on national television and stated that the weapons transfers had indeed occurred, but that the United States did not trade arms for hostages. The investigation was impeded when large volumes of documents relating to the affair were destroyed or withheld from investigators by Reagan administration officials. On 4 March 1987, Reagan made a further nationally televised address, taking full responsibility for the affair and stating that "what began as a strategic opening to Iran deteriorated, in its implementation, into trading arms for hostages".

The affair was investigated by the U.S. Congress and by the three-person, Reagan-appointed Tower Commission. Neither investigation found evidence that President Reagan himself knew of the extent of the multiple programs. In the end, fourteen administration officials were indicted, including then-Secretary of Defense Caspar Weinberger. Eleven convictions resulted, some of which were vacated on appeal. The rest of those indicted or convicted were all pardoned in the final days of the presidency of George H. W. Bush, who had been Vice President at the time of the affair.

The United States was the largest seller of arms to Iran under Mohammad Reza Pahlavi, and the vast majority of the weapons that the Islamic Republic of Iran inherited in January 1979 were American-made. To maintain this arsenal, Iran required a steady supply of spare parts to replace those broken and worn out. After Iranian students stormed the American embassy in Tehran in November 1979 and took 52 Americans hostage, U.S. President Jimmy Carter imposed an arms embargo on Iran. After Iraq invaded Iran in September 1980, Iran desperately needed weapons and spare parts for its current weapons. After Ronald Reagan took office as President on 20 January 1981, he vowed to continue Carter's policy of blocking arms sales to Iran on the grounds that Iran supported terrorism.

A group of senior Reagan administration officials in the Senior Interdepartmental Group conducted a secret study on 21 July 1981, and concluded that the arms embargo was ineffective because Iran could always buy arms and spare parts for its American weapons elsewhere, while at the same time the arms embargo opened the door for Iran to fall into the Soviet sphere of influence as the Kremlin could sell Iran weapons if the United States would not. The conclusion was that the United States should start selling Iran arms as soon as it was politically possible to keep Iran from falling into the Soviet sphere of influence. At the same time, the openly declared goal of Ayatollah Khomeini to export his Islamic revolution all over the Middle East and overthrow the governments of Iraq, Kuwait, Saudi Arabia and the other states around the Persian Gulf led to the Americans perceiving Khomeini as a major threat to the United States.

In the spring of 1983, the United States launched Operation Staunch, a wide-ranging diplomatic effort to persuade other nations all over the world not to sell arms or spare parts for weapons to Iran. At least part of the reason the Iran–Contra affair proved so humiliating for the United States when the story first broke in November 1986 that the US was selling arms to Iran was that American diplomats, as part of Operation Staunch had, from the spring of 1983 on, been lecturing other nations about how morally wrong it was to sell arms to the Islamic Republic of Iran and applying strong pressure to prevent these arms sales to Iran.

At the same time that the American government was considering their options on selling arms to Iran, Contra militants based in Honduras were waging a guerrilla war to topple the Sandinista National Liberation Front (FSLN) revolutionary government of Nicaragua. Almost from the time he took office in 1981, a major goal of the Reagan administration was the overthrow of the left-wing Sandinista government in Nicaragua and to support the Contra rebels. The Reagan administration's policy towards Nicaragua produced a major clash between the executive and legislative arms as Congress sought to limit, if not curb altogether, the ability of the White House to support the Contras. Direct U.S. funding of the Contras insurgency was made illegal through the Boland Amendment, the name given to three U.S. legislative amendments between 1982 and 1984 aimed at limiting U.S. government assistance to Contra militants. Funding ran out for the Contras by July 1984, and in October a total ban was placed in effect. The second Boland Amendment, in effect from 3 October 1984 to 3 December 1985, stated:During the fiscal year 1985 no funds available to the Central Intelligence Agency, the Department of Defense or any other agency or entity of the United States involved in intelligence activities may be obligated or expended for the purpose of or which may have the effect of supporting directly or indirectly military or paramilitary operations in Nicaragua by any nation, organization, group, movement, or individual. In violation of the Boland Amendment, senior officials of the Reagan administration continued to secretly arm and train the Contras and provide arms to Iran, an operation they called "the Enterprise". As the Contras were heavily dependent upon U.S. military and financial support, the second Boland amendment threatened to break the Contra movement and led to President Reagan in 1984 to order the National Security Council (NSC) to "keep the Contras together 'body and soul'", no matter what Congress voted for.

A major legal debate at the center of the Iran–Contra affair concerned the question of whether the NSC was one of the "any other agency or entity of the United States involved in intelligence activities" covered by the Boland amendment. The Reagan administration argued it was not, and many in Congress argued that it was. The majority of constitutional scholars have asserted the NSC did indeed fall within the purview of the second Boland amendment, though the amendment did not mention the NSC by name. The broader constitutional question at stake was the power of Congress versus the power of the presidency. The Reagan administration argued that because the constitution assigned the right to conduct foreign policy to the executive, its efforts to overthrow the government of Nicaragua were a presidential prerogative that Congress had no right to try to halt via the Boland amendments. By contrast congressional leaders argued that the constitution had assigned Congress control of the budget, and Congress had every right to use that power not to fund projects like attempting to overthrow the government of Nicaragua that they disapproved of. As part of the effort to circumvent the Boland amendment, the NSC established "the Enterprise", an arms-smuggling network headed by a retired U.S. Air Force officer turned arms dealer Richard Secord that supplied arms to the Contras. It was ostensibly a private sector operation, but in fact was controlled by the NSC. To fund "the Enterprise", the Reagan administration was constantly on the look-out for funds that came from outside the U.S. government in order not to explicitly violate the letter of the Boland amendment, though the efforts to find alternative funding for the Contras violated the spirit of the Boland amendment. Ironically, military aid to the Contras was reinstated with Congressional consent in October 1986, a month before the scandal broke.

As reported in "The New York Times" in 1991, "continuing allegations that Reagan campaign officials made a deal with the Iranian Government of Ayatollah Ruhollah Khomeini in the fall of 1980" led to "limited investigations." However "limited," those investigations established that "Soon after taking office in 1981, the Reagan Administration secretly and abruptly changed United States policy." Secret Israeli arms sales and shipments to Iran began in that year, even as, in public, "the Reagan Administration" presented a different face, and "aggressively promoted a public campaign... to stop worldwide transfers of military goods to Iran." "The New York Times" explains: "Iran at that time was in dire need of arms and spare parts for its American-made arsenal to defend itself against Iraq, which had attacked it in September 1980," while "Israel [a U.S. ally] was interested in keeping the war between Iran and Iraq going to ensure that these two potential enemies remained preoccupied with each other." Maj. Gen. Avraham Tamir, a high-ranking Israeli Defense Ministry in 1981, said there was a "oral agreement" to allow the sale of "spare parts" to Iran. This was based on an "understanding" with Secretary Alexander Haig (which a Haig adviser denied). This account was confirmed by a former senior American diplomat with a few modifications. The diplomat claimed that "[Ariel] Sharon violated it, and Haig backed away...". A former "high-level" CIA official who saw the reports of arms sales to Iran by Israel in the early 1980's estimated that the total was about 2 billion a year. But also said that "The degree to which it was sanctioned I don't know."

On 17 June 1985, National Security Adviser Robert McFarlane wrote a National Security Decision Directive which called for the United States Of America to begin a rapprochement with the Islamic Republic of Iran. The paper read:

Dynamic political evolution is taking place inside Iran. Instability caused by the pressures of the Iraq-Iran war, economic deterioration and regime in-fighting create the potential for major changes inside Iran. The Soviet Union is better positioned than the U.S. to exploit and benefit from any power struggle that results in changes from the Iranian regime ... The U.S should encourage Western allies and friends to help Iran meet its import requirements so as to reduce the attractiveness of Soviet assistance ... This includes provision of selected military equipment.

Defense Secretary Caspar Weinberger was highly negative, writing on his copy of McFarlane's paper: "This is almost too absurd to comment on ... like asking Qaddafi to Washington for a cozy chat." Secretary of State George Shultz was also opposed, stating that having designated Iran a State Sponsor of Terrorism in January 1984, how could the United States possibly sell arms to Iran? Only the Director of the Central Intelligence Agency William Casey supported McFarlane's plan to start selling arms to Iran.

In early July 1985, the historian Michael Ledeen, a consultant of National Security Adviser Robert McFarlane, requested assistance from Israeli Prime Minister Shimon Peres for help in the sale of arms to Iran. Having talked to an Israeli diplomat David Kimche and Ledeen, McFarlane learned that the Iranians were prepared to have Hezbollah release American hostages in Lebanon in exchange for Israelis shipping Iran American weapons. Having been designated a State Sponsor of Terrorism since January 1984, Iran was in the midst of the Iran–Iraq War and could find few Western nations willing to supply it with weapons. The idea behind the plan was for Israel to ship weapons through an intermediary (identified as Manucher Ghorbanifar) to the Islamic republic as a way of aiding a supposedly moderate, politically influential faction within the regime of Ayatollah Khomeini who was believed to be seeking a rapprochement with the United States; after the transaction, the United States would reimburse Israel with the same weapons, while receiving monetary benefits. McFarlane in a memo to Shultz and Weinberger wrote:

The short term dimension concerns the seven hostages; the long term dimension involves the establishment of a private dialogue with Iranian officials on the broader relations ... They sought specifically the delivery from Israel of 100 TOW missiles ...

The plan was discussed with President Reagan on 18 July 1985 and again on 6 August 1985. Shultz at the latter meeting warned Reagan that "we were just falling into the arms-for-hostages business and we shouldn't do it."

The Americans believed that there was a moderate faction in the Islamic republic headed by Akbar Hashemi Rafsanjani, the powerful speaker of the "Majlis" who was seen as a leading potential successor to Khomeini and who was alleged to want a rapprochement with the United States. The Americans believed that Rafsanjani had the power to order Hezbollah to free the American hostages and establishing a relationship with him by selling Iran arms would ultimately place Iran back within the American sphere of influence. It remains unclear if Rafsanjani really wanted a rapprochement with the United States or was just deceiving Reagan administration officials who were willing to believe that he was a moderate who would effect a rapprochement. Rafsanjani, whose nickname is "the Shark" was described by the British journalist Patrick Brogan as a man of great charm and formidable intelligence known for his subtlety and ruthlessness whose motives in the Iran–Contra affair remain completely mysterious. The Israeli government required that the sale of arms meet high level approval from the United States government, and when McFarlane convinced them that the U.S. government approved the sale, Israel obliged by agreeing to sell the arms.

In 1985, President Reagan entered Bethesda Naval Hospital for colon cancer surgery. While the President was recovering in the hospital, McFarlane met with him and told him that representatives from Israel had contacted the National Security Agency to pass on confidential information from what Reagan later described as the "moderate" Iranian faction headed by Rafsanjani opposed to the Ayatollah's hardline anti-American policies. According to Reagan, these Iranians sought to establish a quiet relationship with the United States, before establishing formal relationships upon the death of the aging Ayatollah. In Reagan's account, McFarlane told Reagan that the Iranians, to demonstrate their seriousness, offered to persuade the Hezbollah militants to release the seven U.S. hostages. McFarlane met with the Israeli intermediaries; Reagan claimed that he allowed this because he believed that establishing relations with a strategically located country, and preventing the Soviet Union from doing the same, was a beneficial move. Although Reagan claims that the arms sales were to a "moderate" faction of Iranians, the Walsh Iran/Contra Report states that the arms sales were "to Iran" itself, which was under the control of the Ayatollah.

Following the Israeli–U.S. meeting, Israel requested permission from the United States to sell a small number of BGM-71 TOW antitank missiles to Iran, claiming that this would aid the "moderate" Iranian faction, by demonstrating that the group actually had high-level connections to the U.S. government. Reagan initially rejected the plan, until Israel sent information to the United States showing that the "moderate" Iranians were opposed to terrorism and had fought against it. Now having a reason to trust the "moderates", Reagan approved the transaction, which was meant to be between Israel and the "moderates" in Iran, with the United States reimbursing Israel. In his 1990 autobiography "An American Life", Reagan claimed that he was deeply committed to securing the release of the hostages; it was this compassion that supposedly motivated his support for the arms initiatives. The president requested that the "moderate" Iranians do everything in their capability to free the hostages held by Hezbollah. Reagan always insisted in public after the scandal broke in late 1986 that purpose behind the arms-for-hostages trade was to establish a working relationship with the "moderate" faction associated with Rafsanjani to facilitate the reestablishment of the American–Iranian alliance after the soon to be expected death of Khomeini, to end the Iran–Iraq war and end Iranian support for Islamic terrorism while downplaying the importance of freeing the hostages in Lebanon as a secondary issue. By contrast, when testifying before the Tower Commission, Reagan declared that hostage issue was the main reason for selling arms to Iran.

The following arms were supplied to Iran:

The first arms sales to Iran began in 1981, though the official paper trail has them beginning in 1985 (see above). On 20 August 1985, Israel sent 96 American-made TOW missiles to Iran through an arms dealer Manucher Ghorbanifar. Subsequently, on 14 September 1985, 408 more TOW missiles were delivered. On 15 September 1985, following the second delivery, Reverend Benjamin Weir was released by his captors, the Islamic Jihad Organization. On 24 November 1985, 18 Hawk anti-aircraft missiles were delivered.

Robert McFarlane resigned on 4 December 1985, stating that he wanted to spend more time with his family, and was replaced by Admiral John Poindexter. Two days later, Reagan met with his advisors at the White House, where a new plan was introduced. This called for a slight change in the arms transactions: instead of the weapons going to the "moderate" Iranian group, they would go to "moderate" Iranian army leaders. As each weapons delivery was made from Israel by air, hostages held by Hezbollah would be released. Israel would continue to be reimbursed by the United States for the weapons. Though staunchly opposed by Secretary of State George Shultz and Secretary of Defense Caspar Weinberger, the plan was authorized by Reagan, who stated that, "We were "not" trading arms for hostages, nor were we negotiating with terrorists". In his notes of a meeting held in the White House on 7 December 1985, Weinberger wrote he told Reagan that this plan was illegal, writing: I argued strongly that we have an embargo that makes arms sales to Iran illegal and President couldn't violate it and that 'washing' transactions through Israel wouldn't make it legal. Shultz, Don Regan agreed. Weinberger's notes have Reagan saying he "could answer charges of illegality but he couldn't answer charge that 'big strong President Reagan' passed up a chance to free hostages." Now retired National Security Advisor McFarlane flew to London to meet with Israelis and Ghorbanifar in an attempt to persuade the Iranian to use his influence to release the hostages before any arms transactions occurred; this plan was rejected by Ghorbanifar.

On the day of McFarlane's resignation, Oliver North, a military aide to the United States National Security Council (NSC), proposed a new plan for selling arms to Iran, which included two major adjustments: instead of selling arms through Israel, the sale was to be direct at a markup; and a portion of the proceeds would go to Contras, or Nicaraguan paramilitary fighters waging guerrilla warfare against the democratically elected Sandinista government. The dealings with the Iranians were conducted via the NSC with Admiral Poindexter and his deputy Colonel North, with the American historians Malcolm Byrne and Peter Kornbluh writing that Poindexter granted much power to North "...who made the most of the situation, often deciding important matters on his own, striking outlandish deals with the Iranians, and acting in the name of the president on issues that were far beyond his competence. All of these activities continued to take place within the framework of the president's broad authorization. Until the press reported on the existence of the operation, nobody in the administration questioned the authority of Poindexter's and North's team to implement the president's decisions". North proposed a $15 million markup, while contracted arms broker Ghorbanifar added a 41% markup of his own. Other members of the NSC were in favor of North's plan; with large support, Poindexter authorized it without notifying President Reagan, and it went into effect. At first, the Iranians refused to buy the arms at the inflated price because of the excessive markup imposed by North and Ghorbanifar. They eventually relented, and in February 1986, 1,000 TOW missiles were shipped to the country. From May to November 1986, there were additional shipments of miscellaneous weapons and parts.

Both the sale of weapons to Iran and the funding of the Contras attempted to circumvent not only stated administration policy, but also the Boland Amendment. Administration officials argued that regardless of Congress restricting funds for the Contras, or any affair, the President (or in this case the administration) could carry on by seeking alternative means of funding such as private entities and foreign governments. Funding from one foreign country, Brunei, was botched when North's secretary, Fawn Hall, transposed the numbers of North's Swiss bank account number. A Swiss businessman, suddenly $10 million richer, alerted the authorities of the mistake. The money was eventually returned to the Sultan of Brunei, with interest.

On 7 January 1986, John Poindexter proposed to Reagan a modification of the approved plan: instead of negotiating with the "moderate" Iranian political group, the United States would negotiate with "moderate" members of the Iranian government. Poindexter told Reagan that Ghorbanifar had important connections within the Iranian government, so with the hope of the release of the hostages, Reagan approved this plan as well. Throughout February 1986, weapons were shipped directly to Iran by the United States (as part of Oliver North's plan), but none of the hostages were released. Retired National Security Advisor McFarlane conducted another international voyage, this one to Tehran – bringing with him a gift of a bible with a handwritten inscription by Ronald Reagan and, according to George Cave, a cake baked in the shape of a key. Howard Teicher described the cake as a joke between North and Ghorbanifar. McFarlane met directly with Iranian officials associated with Rafsanjani, who sought to establish U.S.-Iranian relations in an attempt to free the four remaining hostages.

The American delegation comprised McFarlane, North, Cave (a retired CIA officer who worked in Iran in the 1960s–70s), Teicher, Israeli diplomat Amiram Nir and a CIA translator. They arrived in Tehran in an Israeli plane carrying forged Irish passports on 25 May 1986. This meeting also failed. Much to McFarlane's disgust, he did not meet ministers, and instead met in his words "third and fourth level officials". At one point, an angry McFarlane shouted: "As I am a Minister, I expect to meet with decision-makers. Otherwise, you can work with my staff." The Iranians requested concessions such as Israel's withdrawal from the Golan Heights, which the United States rejected. More importantly, McFarlane refused to ship spare parts for the Hawk missiles until the Iranians had Hezbollah release the American hostages, whereas the Iranians wanted to reverse that sequence with the spare parts being shipped first before the hostages were freed. The differing negotiating positions led to McFarlane's mission going home after four days. After the failure of the secret visit to Tehran, McFarlane advised Reagan not to talk to the Iranians anymore, advice that was disregarded.

On 26 July 1986, Hezbollah freed the American hostage Father Lawrence Jenco, former head of Catholic Relief Services in Lebanon. Following this, William Casey, head of the CIA, requested that the United States authorize sending a shipment of small missile parts to Iranian military forces as a way of expressing gratitude. Casey also justified this request by stating that the contact in the Iranian government might otherwise lose face or be executed, and hostages might be killed. Reagan authorized the shipment to ensure that those potential events would not occur. North used this release to persuade Reagan to switch over to a "sequential" policy of freeing the hostages one by one, instead of the "all or nothing" policy that the Americans had pursued until then. By this point, the Americans had grown tired of Ghobanifar who had proven himself a dishonest intermediary who played off both sides to his own commercial advantage. In August 1986, the Americans had established a new contact in the Iranian government, Ali Hashemi Bahramani, the nephew of Rafsanjani and an officer in the Revolutionary Guard. The fact that the Revolutionary Guard was deeply involved in international terrorism seemed only to attract the Americans more to Bahramani, who was seen as someone with the influence to change Iran's policies. Richard Secord, an American arms dealer, who was being used as a contact with Iran, wrote to North: "My judgment is that we have opened up a new and probably better channel into Iran". North was so impressed with Bahramani that he arranged for him to secretly visit Washington D.C and gave him a guided tour at midnight of the White House.

North frequently met with Bahramani in the summer and fall of 1986 in West Germany, discussing arms sales to Iran, the freeing of hostages held by Hezbollah and how best to overthrow President Saddam Hussein of Iraq and the establishment of "a non-hostile regime in Baghdad". In September and October 1986 three more Americans – Frank Reed, Joseph Cicippio, and Edward Tracy – were abducted in Lebanon by a separate terrorist group, who referred to them simply as "G.I. Joe," after the popular American toy. The reasons for their abduction are unknown, although it is speculated that they were kidnapped to replace the freed Americans. One more original hostage, David Jacobsen, was later released. The captors promised to release the remaining two, but the release never happened.

During a secret meeting in Frankfurt in October 1986, North told Bahramani that: "Saddam Hussein must go". North also claimed that Reagan had told him to tell Bahramani that: "Saddam Hussein is an asshole." Behramani during a secret meeting in Mainz informed North that Rafsanjani "for his own politics ... decided to get all the groups involved and give them a role to play." Thus, all the factions in the Iranian government would be jointly responsible for the talks with the Americans and "there would not be an internal war". This demand of Behramani caused much dismay on the American side as it made clear to them that they would not be dealing solely with a "moderate" faction in the Islamic Republic, as the Americans liked to pretend to themselves, but rather with all the factions in the Iranian government – including those who were very much involved in terrorism. Despite this the talks were not broken off.

After a leak by Mehdi Hashemi, a senior official in the Islamic Revolutionary Guard Corps, the Lebanese magazine "Ash-Shiraa" exposed the arrangement on 3 November 1986. The leak may have been orchestrated by a covert team led by Arthur S. Moreau Jr., assistant to the chairman of the United States Joint Chiefs of Staff, due to fears the scheme had grown out of control.

This was the first public report of the weapons-for-hostages deal. The operation was discovered only after an airlift of guns (Corporate Air Services HPF821) was downed over Nicaragua. Eugene Hasenfus, who was captured by Nicaraguan authorities after surviving the plane crash, initially alleged in a press conference on Nicaraguan soil that two of his coworkers, Max Gomez and Ramon Medina, worked for the Central Intelligence Agency. He later said he did not know whether they did or not. The Iranian government confirmed the "Ash-Shiraa" story, and ten days after the story was first published, President Reagan appeared on national television from the Oval Office on 13 November, stating:

My purpose was ... to send a signal that the United States was prepared to replace the animosity between [the U.S. and Iran] with a new relationship ... At the same time we undertook this initiative, we made clear that Iran must oppose all forms of international terrorism as a condition of progress in our relationship. The most significant step which Iran could take, we indicated, would be to use its influence in Lebanon to secure the release of all hostages held there.

The scandal was compounded when Oliver North destroyed or hid pertinent documents between 21 November and 25 November 1986. During North's trial in 1989, his secretary, Fawn Hall, testified extensively about helping North alter, shred, and remove official United States National Security Council (NSC) documents from the White House. According to "The New York Times", enough documents were put into a government shredder to jam it. North's explanation for destroying some documents was to protect the lives of individuals involved in Iran and Contra operations. It was not until 1993, years after the trial, that North's notebooks were made public, and only after the National Security Archive and Public Citizen sued the Office of the Independent Counsel under the Freedom of Information Act.

During the trial, North testified that on 21, 22 or 24 November, he witnessed Poindexter destroy what may have been the only signed copy of a presidential covert-action finding that sought to authorize CIA participation in the November 1985 Hawk missile shipment to Iran. U.S. Attorney General Edwin Meese admitted on 25 November that profits from weapons sales to Iran were made available to assist the Contra rebels in Nicaragua. On the same day, John Poindexter resigned, and President Reagan fired Oliver North. Poindexter was replaced by Frank Carlucci on 2 December 1986.

When the story broke, many legal and constitutional scholars expressed dismay that the NSC, which was supposed to be just an advisory body to assist the President with formulating foreign policy had "gone operational" by becoming an executive body covertly executing foreign policy on its own. The National Security Act of 1947, which created the NSC, gave it the vague right to perform "such other functions and duties related to the intelligence as the National Security Council may from time to time direct." However, the NSC had usually, although not always, acted as an advisory agency until the Reagan administration when the NSC had "gone operational", a situation that was condemned by both the Tower commission and by Congress as a departure from the norm. The American historian James Canham-Clyne asserted that Iran–Contra affair and the NSC "going operational" were not departures from the norm, but were the logical and natural consequence of existence of the "national security state", the plethora of shadowy government agencies with multi-million dollar budgets operating with little oversight from Congress, the courts or the media, and for whom upholding national security justified almost everything. Canham-Clyne argued that for the "national security state", the law was an obstacle to be surmounted rather than something to uphold and that the Iran–Contra affair was just "business as usual", something he asserted that the media missed by focusing on the NSC having "gone operational."

In "Veil: The Secret Wars of the CIA 1981–1987", journalist Bob Woodward chronicled the role of the CIA in facilitating the transfer of funds from the Iran arms sales to the Nicaraguan Contras spearheaded by Oliver North. According to Woodward, then-Director of the CIA William J. Casey admitted to him in February 1987 that he was aware of the diversion of funds to the Contras. The controversial admission occurred while Casey was hospitalized for a stroke, and, according to his wife, was unable to communicate. On 6 May 1987, William Casey died the day after Congress began public hearings on Iran–Contra. Independent Counsel, Lawrence Walsh later wrote: "Independent Counsel obtained no documentary evidence showing Casey knew about or approved the diversion. The only direct testimony linking Casey to early knowledge of the diversion came from [Oliver] North." Gust Avrakodos, who was responsible for the arms supplies to the Afghans at this time, was aware of the operation as well and strongly opposed it, in particular the diversion of funds allotted to the Afghan operation. According to his Middle Eastern experts the operation was pointless because the moderates in Iran were not in a position to challenge the fundamentalists. However, he was overruled by Clair George.

On 25 November 1986, President Reagan announced the creation of a Special Review Board to look into the matter; the following day, he appointed former Senator John Tower, former Secretary of State Edmund Muskie, and former National Security Adviser Brent Scowcroft to serve as members. This Presidential Commission took effect on 1 December and became known as the Tower Commission. The main objectives of the commission were to inquire into "the circumstances surrounding the Iran–Contra matter, other case studies that might reveal strengths and weaknesses in the operation of the National Security Council system under stress, and the manner in which that system has served eight different presidents since its inception in 1947". The Tower Commission was the first presidential commission to review and evaluate the National Security Council.

President Reagan appeared before the Tower Commission on 2 December 1986, to answer questions regarding his involvement in the affair. When asked about his role in authorizing the arms deals, he first stated that he had; later, he appeared to contradict himself by stating that he had no recollection of doing so. In his 1990 autobiography, "An American Life", Reagan acknowledges authorizing the shipments to Israel.

The report published by the Tower Commission was delivered to the president on 26 February 1987. The Commission had interviewed 80 witnesses to the scheme, including Reagan, and two of the arms trade middlemen: Manucher Ghorbanifar and Adnan Khashoggi. The 200-page report was the most comprehensive of any released, criticizing the actions of Oliver North, John Poindexter, Caspar Weinberger, and others. It determined that President Reagan did not have knowledge of the extent of the program, especially about the diversion of funds to the Contras, although it argued that the president ought to have had better control of the National Security Council staff. The report heavily criticized Reagan for not properly supervising his subordinates or being aware of their actions. A major result of the Tower Commission was the consensus that Reagan should have listened to his National Security Advisor more, thereby placing more power in the hands of that chair.

In January 1987, Congress announced it was opening an investigation into the Iran–Contra affair. Depending upon one's political perspective, the Congressional investigation into the Iran–Contra affair was either an attempt by the legislative arm to gain control over an out-of-control executive arm, a partisan "witch hunt" by the Democrats against a Republican administration or a feeble effort by Congress that did far too little to rein in the "imperial presidency" that had run amok by breaking numerous laws. The Democratic-controlled United States Congress issued its own report on 18 November 1987, stating that "If the president did not know what his national security advisers were doing, he should have." The congressional report wrote that the president bore "ultimate responsibility" for wrongdoing by his aides, and his administration exhibited "secrecy, deception and disdain for the law". It also read that "the central remaining question is the role of the President in the Iran–Contra affair. On this critical point, the shredding of documents by Poindexter, North and others, and the death of Casey, leave the record incomplete".

Reagan expressed regret regarding the situation in a nationally televised address from the Oval Office on 4 March 1987, and in two other speeches. Reagan had not spoken to the American people directly for three months amidst the scandal, and he offered the following explanation for his silence:

The reason I haven't spoken to you before now is this: You deserve the truth. And as frustrating as the waiting has been, I felt it was improper to come to you with sketchy reports, or possibly even erroneous statements, which would then have to be corrected, creating even more doubt and confusion. There's been enough of that.

Reagan then took full responsibility for the acts committed:
First, let me say I take full responsibility for my own actions and for those of my administration. As angry as I may be about activities undertaken without my knowledge, I am still accountable for those activities. As disappointed as I may be in some who served me, I'm still the one who must answer to the American people for this behavior.

Finally, the president acknowledged that his previous assertions that the U.S. did not trade arms for hostages were incorrect:
A few months ago I told the American people I did not trade arms for hostages. My heart and my best intentions still tell me that's true, but the facts and the evidence tell me it is not. As the Tower board reported, what began as a strategic opening to Iran deteriorated, in its implementation, into trading arms for hostages. This runs counter to my own beliefs, to administration policy, and to the original strategy we had in mind.

To this day, Reagan's role in these transactions is not definitively known. It is unclear exactly what Reagan knew and when, and whether the arms sales were motivated by his desire to save the U.S. hostages. Oliver North wrote that "Ronald Reagan knew of and approved a great deal of what went on with both the Iranian initiative and private efforts on behalf of the contras and he received regular, detailed briefings on both...I have no doubt that he was told about the use of residuals for the Contras, and that he approved it. Enthusiastically." Handwritten notes by Defense Secretary Weinberger indicate that the President was aware of potential hostage transfers with Iran, as well as the sale of Hawk and TOW missiles to what he was told were "moderate elements" within Iran. Notes taken by Weinberger on 7 December 1985 record that Reagan said that "he could answer charges of illegality but he couldn't answer charge that 'big strong President Reagan passed up a chance to free hostages'". The Republican-written "Report of the Congressional Committees Investigating the Iran–Contra Affair" made the following conclusion: There is some question and dispute about precisely the level at which he chose to follow the operation details. There is no doubt, however, ... [that] the President set the US policy towards Nicaragua, with few if any ambiguities, and then left subordinates more or less free to implement it.

Domestically, the affair precipitated a drop in President Reagan's popularity. His approval ratings suffered "the largest single drop for any U.S. president in history", from 67% to 46% in November 1986, according to a "The New York Times"/CBS News poll. The "Teflon President", as Reagan was nicknamed by critics, survived the affair, however, and his approval rating recovered.

Internationally, the damage was more severe. Magnus Ranstorp wrote, "U.S. willingness to engage in concessions with Iran and the Hezbollah not only signaled to its adversaries that hostage-taking was an extremely useful instrument in extracting political and financial concessions for the West but also undermined any credibility of U.S. criticism of other states' deviation from the principles of no-negotiation and no concession to terrorists and their demands."

In Iran, Mehdi Hashemi, the leaker of the scandal, was executed in 1987, allegedly for activities unrelated to the scandal. Though Hashemi made a full video confession to numerous serious charges, some observers find the coincidence of his leak and the subsequent prosecution highly suspicious.


The Independent Counsel, Lawrence E. Walsh, chose not to re-try North or Poindexter. In total, several dozen people were investigated by Walsh's office.

During his election campaign in 1988, Vice President Bush denied any knowledge of the Iran–Contra affair by saying he was "out of the loop". Though his diaries included that he was "one of the few people that know fully the details", he repeatedly refused to discuss the incident and won the election.

A book published in 2008 by Israeli journalist and terrorism expert Ronen Bergman asserts that Bush was also personally and secretly briefed on the affair by Amiram Nir, a counterterrorism adviser to the then Israeli prime minister, Yitzhak Shamir, when Bush was on a visit to Israel. "Nir could have incriminated the incoming President. That Nir was killed in a mysterious chartered airplane crash in Mexico in December 1988 has given rise to numerous conspiracy theories", writes Bergman.

On 24 December 1992, having been defeated for reelection, and nearing the end of his term in office, lame duck President George H. W. Bush pardoned five administration officials who had been found guilty on charges relating to the affair. They were: 

Bush also pardoned Caspar Weinberger, who had not yet come to trial. Attorney General William P. Barr advised the President on these pardons, especially that of Caspar Weinberger.

In response to these Bush pardons, Independent Counsel Lawrence E. Walsh, who headed the investigation of Reagan Administration officials' criminal conduct in the Iran–Contra scandal, stated that "the Iran–Contra cover-up, which has continued for more than six years, has now been completed." Walsh noted that in issuing the pardons Bush appears to have been preempting being implicated himself in the crimes of Iran–Contra by evidence that was to come to light during the Weinberger trial, and noted that there was a pattern of "deception and obstruction" by Bush, Weinberger and other senior Reagan administration officials.

The Iran–Contra affair and the ensuing deception to protect senior administration officials (including President Reagan) was cast as an example of post-truth politics by Malcolm Byrne of George Washington University.

The 100th Congress formed a Joint Committee of the United States Congress (Congressional Committees Investigating The Iran-Contra Affair) and held hearings in mid-1987. Transcripts were published as: "Iran–Contra Investigation: Joint Hearings Before the Senate Select Committee on Secret Military Assistance to Iran and the Nicaraguan Opposition and the House Select Committee to Investigate Covert Arms Transactions with Iran" (U.S. GPO 1987–88). A closed Executive Session heard classified testimony from North and Poindexter; this transcript was published in a redacted format. The joint committee's final report was "Report of the Congressional Committees Investigating the Iran–Contra Affair With Supplemental, Minority, and Additional Views" (U.S. GPO 17 November 1987). The records of the committee are at the National Archives, but many are still non-public.

Testimony was also heard before the House Foreign Affairs Committee, House Permanent Select Committee on Intelligence, and Senate Select Committee on Intelligence and can be found in the Congressional Record for those bodies. The Senate Intelligence Committee produced two reports: "Preliminary Inquiry into the Sale of Arms to Iran and Possible Diversion of Funds to the Nicaraguan Resistance" (2 February 1987) and "Were Relevant Documents Withheld from the Congressional Committees Investigating the Iran–Contra Affair?" (June 1989).

The Tower Commission Report was published as the "Report of the President's Special Review Board". U.S. GPO 26 February 1987. It was also published as "The Tower Commission Report", Bantam Books, 1987, 

The Office of Independent Counsel/Walsh investigation produced four interim reports to Congress. Its final report was published as the "Final Report of the Independent Counsel for Iran/Contra Matters". Walsh's records are available at the National Archives.





</doc>
<doc id="14788" url="https://en.wikipedia.org/wiki?curid=14788" title="Infocom">
Infocom

Infocom was a software company based in Cambridge, Massachusetts that produced numerous works of interactive fiction. They also produced one notable business application, a relational database called "Cornerstone".

Infocom was founded on June 22, 1979, by staff and students of Massachusetts Institute of Technology, and lasted as an independent company until 1986, when it was bought by Activision. Activision shut down the Infocom division in 1989, although they released some titles in the 1990s under the Infocom "Zork" brand. Activision abandoned the Infocom trademark in 2002.

Infocom games are text adventures where users direct the action by entering short strings of words to give commands when prompted. Generally the program will respond by describing the results of the action, often the contents of a room if the player has moved within the virtual world. The user reads this information, decides what to do, and enters another short series of words. Examples include "go west" or "take flashlight".

Infocom games were written using a roughly LISP-like programming language called "ZIL" (Zork Implementation Language or Zork Interactive Language—it was referred to as both) that compiled into a byte code able to run on a standardized virtual machine called the Z-machine. As the games were text based and used variants of the same Z-machine interpreter, the interpreter had to be ported to new computer architectures only once per architecture, rather than once per game. Each game file included a sophisticated parser which allowed the user to type complex instructions to the game. Unlike earlier works of interactive fiction which only understood commands of the form 'verb noun', Infocom's parser could understand a wider variety of sentences. For instance one might type "open the large door, then go west", or "go to festeron".

With the Z-machine, Infocom was able to release most of their games for most popular home computers of the day simultaneously—the Apple II family, Atari 800, IBM PC compatibles, Amstrad CPC/PCW (one disc worked on both machines), Commodore 64, Commodore Plus/4, Commodore 128, Kaypro CP/M, Texas Instruments TI-99/4A, the Mac, Atari ST, the Commodore Amiga, and the Radio Shack TRS-80. The company was also known for shipping creative props, or "feelies" (and even "smellies"), with its games.

Inspired by "Colossal Cave", Marc Blank and Dave Lebling created what was to become the first Infocom game, "Zork", in 1977 at MIT's Laboratory for Computer Science. Despite the development of a revolutionary virtual memory system that allowed games to be much larger than the average personal computer's normal capacity, the enormous mainframe-developed game had to be split into three roughly equal parts. "Zork I" was released originally for the TRS-80 in 1980. Infocom was founded on June 22, 1979; the founding members were Tim Anderson, Joel Berez, Marc Blank, Mike Broos, Scott Cutler, Stu Galley, Dave Lebling, J. C. R. Licklider, Chris Reeve, and Al Vezza.

Lebling and Blank each authored several more games, and additional game writers (or "Implementors") were hired, notably including Steve Meretzky. Other popular and inventive titles included a number of sequels and spinoff games in the "Zork" series, "The Hitchhiker's Guide to the Galaxy" by Douglas Adams, and "A Mind Forever Voyaging".

In its first few years of operation, text adventures proved to be a huge revenue stream for the company. Whereas most computer games of the era would achieve initial success and then suffer a significant drop-off in sales, Infocom titles continued to sell for years and years. Employee Tim Anderson said of their situation, "It was phenomenal – we had a basement that just printed money." By 1983 Infocom was perhaps the most dominant computer-game company; for example, all ten of its games were on the "Softsel" top 40 list of best-selling computer games for the week of December 12, 1983, with "Zork" in first place and two others in the top ten. In late 1984, management declined an offer by publisher Simon & Schuster to acquire Infocom for $28 million, far more than the board of directors's valuation of $10–12 million. In 1993, "Computer Gaming World" described this era as the "Cambridge Camelot, where the Great Underground Empire was formed".

As an in-joke, the number 69,105 made a number of appearances in Infocom games.

Infocom games were popular, "InfoWorld" said, in part because "in offices all over America (more than anyone realizes) executives and managers are playing games on their computers". An estimated 25% had a computer game "hidden somewhere in their drawers", "Inc." reported, and they preferred Infocom adventures to arcade games. The company stated that year that 75% of players were over 25 years old and that 80% were men; more women played its games than other companies', especially the mysteries. Most players enjoyed reading books; in 1987 president Joel Berez stated, "[Infocom's] audience tends to be composed of heavy readers. We sell to the minority that does read".

A 1996 article in "Next Generation" said Infocom's "games were noted for having more depth than any other adventure games, before or since." Three components proved key to Infocom's success: marketing strategy, rich storytelling and feelies. Whereas most game developers sold their games mainly in software stores, Infocom also distributed their games via bookstores. Infocom's products appealed more to those with expensive computers, such as the Apple Macintosh, IBM PC, and Commodore Amiga. Berez stated that "there is no noticeable correlation between graphics machines and our penetration. There is a high correlation between the price of the machine and our sales ... people who are putting more money into their machines tend to buy more of our software". Since their games were text-based, patrons of bookstores were drawn to the Infocom games as they were already interested in reading. Unlike most computer software, Infocom titles were distributed under a no-returns policy, which allowed them to make money from a single game for a longer period of time.

Next, Infocom titles featured strong storytelling and rich descriptions, eschewing the inherent restrictions of graphic displays and allowing users to use their own imaginations for the lavish and exotic locations the games described. Infocom's puzzles were unique in that they were usually tightly integrated into the storyline, and rarely did gamers feel like they were being made to jump through one arbitrary hoop after another, as was the case in many of the competitors' games. The puzzles were generally logical but also required close attention to the clues and hints given in the story, causing many gamers to keep copious notes as they went along.

Sometimes, though, Infocom threw in puzzles just for the humor of it—if the user never ran into these, they could still finish the game. But discovering these early Easter Eggs was satisfying for some fans of the games. For example, one popular Easter egg was in the "Enchanter" game, which involves collecting magic spells to use in accomplishing the quest. One of these is a summoning spell, which the player needs to use to summon certain characters at different parts of the game. At one point the game mentions the "Implementers" who were responsible for creating the land of Zork. If the player tries to summon the Implementers, the game produces a vision of Dave Lebling and Marc Blank at their computers, surprised at this "bug" in the game and working feverishly to fix it.

Third, the inclusion of "feelies"—imaginative props and extras tied to the game's theme—provided copy protection against copyright infringement. Some games were unsolvable without the extra content provided with the boxed game. And because of the cleverness and uniqueness of the feelies, users rarely felt like they were an intrusion or inconvenience, as was the case with most of the other copy-protection schemes of the time.

Although Infocom started out with "Zork", and although the "Zork" world was the centerpiece of their product line throughout the "Zork" and "Enchanter" series, the company quickly branched out into a wide variety of story lines: fantasy, science-fiction, mystery, horror, historical adventure, children's stories, and others that defied easy categorization. In an attempt to reach out to female customers, Infocom also produced "Plundered Hearts", which cast the gamer in the role of the heroine of a swashbuckling adventure on the high seas, and which required the heroine to use more feminine tactics to win the game, since hacking-and-slashing was not a very ladylike way to behave. And to compete with the "Leisure Suit Larry" style games that were also appearing, Infocom also came out with "Leather Goddesses of Phobos" in 1986, which featured "tame", "suggestive", and "lewd" playing modes. It included among its "feelies" a "scratch-and-sniff" card with six odors that corresponded to cues given to the player during the game.

Originally, hints for the game were provided as a "pay-per-hint" service created by Mike Dornbrook, called the Zork Users Group (ZUG). Dornbrook also started Infocom's customer newsletter, called "The New Zork Times", to discuss game hints and preview and showcase new products.

The pay-per-hint service eventually led to the development of InvisiClues: books with hints, maps, clues, and solutions for puzzles in the games. The answers to the puzzles were printed in invisible ink that only became visible when rubbed with a special marker that was provided with each book. Usually, two or more answers were given for each question that a gamer might have. The first answer would provide a subtle hint, the second a less subtle hint, and so forth until the last one gave an explicit walkthrough. Gamers could thus reveal only the hints that they needed to have to play the game. To prevent the mere questions (printed in normal ink) from giving away too much information about the game, a certain number of misleading fake questions were included in every InvisiClues book. Answers to these questions would start by giving misleading or impossible to carry out answers, before the final answer revealed that the question was a fake (and usually admonishing the player that revealing random clues from the book would spoil their enjoyment of the game). The InvisiClues books were regularly ranked in near the top of best seller lists for computer books.

In the Solid Gold line of re-releases, InvisiClues were integrated into the game. By typing "HINT" twice the player would open up a screen of possible topics where they could then reveal one hint at a time for each puzzle, just like the books.

Infocom also released a small number of "interactive fiction paperbacks" (gamebooks), which were based on the games and featured the ability to choose a different path through the story. Similar to the "Choose Your Own Adventure" series, every couple of pages the book would give the reader the chance to make a choice, such as which direction they wanted to go or how they wanted to respond to another character. The reader would then choose one of the given answers and turn to the appropriate page. These books, however, never did sell particularly well, and quickly disappeared from the bookshelves.

Despite their success with computer games, Vezza and other company founders hoped to produce successful business programs like Lotus Development, also founded by people from MIT and located in the same building as Infocom. Lotus released its first product, 1-2-3, in January 1983; within a year it had earned $53 million, compared to Infocom's $6 million. In 1982 Infocom started putting resources into a new division to produce business products. In 1985 they released a database product, "Cornerstone", aimed at capturing the then booming database market for small business. Though this application was hailed upon its release for ease of use, it sold only 10,000 copies; not enough to cover the development expenses.

The program failed for a number of reasons. Although it was packaged in a slick hard plastic carrying case and was a very good database for personal and home use, it was originally priced at USD$495 per copy and used copy-protected disks. Another serious miscalculation was that the program did not include any kind of scripting language, so it was not promoted by any of the database consultants that small businesses typically hired to create and maintain their DB applications. Reviewers were also consistently disappointed that Infocom—noted for the natural language syntax of their games—did not include a natural language query ability, which had been the most anticipated feature for this database application. In a final disappointment, "Cornerstone" was available only for IBM PCs; while "Cornerstone" had been programmed with its own virtual machine for maximum portability, it was not ported to any of the other platforms that Infocom supported for their games, so that feature had become essentially irrelevant. And because Cornerstone used this virtual machine for its processing, it suffered from slow, lackluster performance.

Infocom's games' sales benefited significantly from the portability offered by running on top of a virtual machine. "InfoWorld" wrote in 1984 that "the company always sells games for computers you don't normally think of as game machines, such as the DEC Rainbow or the Texas Instruments Professional Computer. This is one of the key reasons for the continued success of old titles such as Zork." Dornbrook estimated that year that of the 1.8 million home computers in America, one half million homes had Infocom games ("all, if you count the pirated games"). Computer companies sent prototypes of new systems to encourage Infocom to port Z-machine to them; the virtual machine supported more than 20 different systems, including orphaned computers for which Infocom games were among the only commercial products. The company produced the only third-party games available for the Macintosh at launch, and Berlyn promised that all 13 of its games would be available for the Atari ST within one month of its release.

The virtual machine significantly slowed "Cornerstone"s execution speed, however. Businesses were moving "en masse" to the IBM PC platform by that time, so portability was no longer a significant differentiator. Infocom had sunk much of the money from games sales into "Cornerstone"; this, in addition to a slump in computer game sales, left the company in a very precarious financial position. By the time Infocom removed the copy-protection and reduced the price to less than $100, it was too late, and the market had moved on to other database solutions.

By 1982 the market was moving to graphic adventures. Infocom was interested in producing them, that year proposing to Penguin Software that Antonio Antiochia, author of its "Transylvania", provide artwork. Within Infocom the game designers tended to oppose graphics, while marketing and business employees supported using them for the company to remain competitive. The partnership negotiations failed, in part because of the difficulty of adding graphics to the Z-machine, and Infocom instead began a series of advertisements mocking graphical games as "graffiti" compared to the human imagination. The marketing campaign was very successful, and Infocom's success led to other companies like Broderbund and Electronic Arts also releasing their own text games.

After Cornerstone's failure, Infocom laid off half of its 100 employees, and Activision acquired the company on June 13, 1986 for $7.5 million. The merger was pushed by Activision's CEO Jim Levy, who was a fan of Infocom games and felt their two companies were in similar situations. Berez stated that although the two companies' headquarters and product lines would remain separate, "One of the effects of the merger will be for both of us to broaden our horizons". He said that "We're looking at graphics a lot", while Activision was reportedly interested in using Infocom's parser. 

While relations were cordial between the two companies at first, Activision's ousting of Levy with new CEO Bruce Davis created problems in the working relationship with Infocom. Davis believed that his company had paid too much for Infocom and initiated a lawsuit against them to recoup some of the cost, along with changing the way Infocom was run. For example:

By 1988, rumors spread of disputes between Activision and Infocom. Infocom employees reportedly believed that Activision gave poorer-quality games to Infocom, such as Tom Snyder Productions' unsuccessful "Infocomics". Activision moved Infocom development to California in 1989, and the company was now just a publishing label. Rising costs and falling profits, exacerbated by the lack of new products in 1988 and technical issues with its DOS products, caused Activision to close Infocom in 1989, after which some of the remaining Infocom designers such as Steve Meretzky moved to the company Legend Entertainment, founded by Bob Bates and Mike Verdu, to continue creating games in the Infocom tradition.

Activision itself was struggling in the marketplace following Davis' promotion to CEO. Activision had rebranded itself as Mediagenic and tried to produce business productivity software, but became significantly in debt. In 1991, Mediagenic was purchased by Bobby Kotick, who put into measures immediately to try to turn the company around, which included returning to its Activision name, and putting to use its past IP properties. This included the Infocom games; Kotick recognized the value of the branding of "Zork" and other titles. Activision began to sell bundles of the Infocom games that year, packaged as themed collections (usually by genre, such as the Science Fiction collection); in 1991, they published "The Lost Treasures of Infocom", followed in 1992 by "The Lost Treasures of Infocom II". These compilations featured nearly every game produced by Infocom before 1988. ("Leather Goddesses of Phobos" was not included in either bundle, but could be ordered via a coupon included with "Lost Treasures II".) The compilations lacked the "feelies" that came with each game, but in some cases included photographs of them. In 1996, the first bundles were followed by "Classic Text Adventure Masterpieces of Infocom", a single CD-ROM which contained the works of both collections. This release, however, was missing "The Hitchhiker's Guide to the Galaxy" and "Shogun" because the licenses from Douglas Adams' and James Clavell's estates had expired. Under Kotick's leadership, Activision also developed "Return to Zork", published under its Infocom label.

Eventually, Activision abandoned the "Infocom" name. The brand name was registered by Oliver Klaeffling of Germany in 2007, then was abandoned the following year. The Infocom trademark was then held by Pete Hottelet's Omni Consumer Products, who registered the name around the same time as Klaeffling in 2007. As of March 2017, the trademark is owned by infocom.xyz, according to Bob Bates.




With the exception of "The Hitchhiker's Guide to the Galaxy" and "Shogun", the copyrights to the Infocom games are believed to be still held by Activision. "Dungeon", the mainframe precursor to the commercial Zork trilogy, is believed to be free for non-commercial use. but prohibited for commercial use. It was this copy that the popular Fortran mainframe version was based on. The C version was based on the Fortran version. and is available from The Interactive Fiction Archive as original FORTRAN source code, a Z-machine story file and as various native source ports. Many Infocom titles can be downloaded via the Internet, but only in violation of the copyright. Activision did at one point release the original trilogy for free-of-charge download as a promotion but prohibited redistribution and have since discontinued this. There are currently at least four Infocom sampler and demos available from the IF Archive as Z-machine story files which require a Z-machine interpreter to play. Interpreters are available for most computer platforms, the most widely used being the Frotz, Zip, and Nitfol interpreters.

Five games ("Zork I", "Planetfall", "The Hitchhiker's Guide to the Galaxy", "Wishbringer" and "Leather Goddesses of Phobos") were re-released in Solid Gold format. The Solid Gold versions of those games include a built-in InvisiClues hint system.

In 2012, Activision released "Lost Treasures of Infocom" for iOS devices. In-app purchases provide access for 27 of the titles. It also lacks "Shogun" and "The Hitchhiker's Guide to the Galaxy" as well as "Beyond Zork", "Zork Zero" and "Nord and Bert".

Efforts have been made to make the Infocom games source code available for preservation. In 2008, Jason Scott, a video game preservationist contributing towards the Internet Archive, received the so-called "Infocom Drive", a large archive of the entire contents of Infocom's main server made during the last few days before the company was relocated to California; besides source code for all of Infocom's games (including unreleased ones), it also contained the software manuals, design documents and other essential content alongside Infocom's business documentation. Scott later published all of the source files in their original Z-engine format to GitHub in 2019.

"Zork" made a cameo appearance as an easter egg in Activision and Treyarch's "". It can be accessed during the main menu and runs much like a DOS program.



</doc>
<doc id="14789" url="https://en.wikipedia.org/wiki?curid=14789" title="Interactive fiction">
Interactive fiction

Interactive fiction, often abbreviated IF, is software simulating environments in which players use text commands to control characters and influence the environment. Works in this form can be understood as literary narratives, either in the form of interactive narratives or interactive narrations. These works can also be understood as a form of video game, either in the form of an adventure game or role-playing game. In common usage, the term refers to text adventures, a type of adventure game where the entire interface can be "text-only", however, graphical text adventures still fall under the text adventure category if the main way to interact with the game is by typing text. Some users of the term distinguish between interactive fiction, known as "Puzzle-free", that focuses on narrative, and "text adventures" that focus on puzzles.

Due to their text-only nature, they sidestepped the problem of writing for widely divergent graphics architectures. This feature meant that interactive fiction games were easily ported across all the popular platforms at the time, including CP/M (not known for gaming or strong graphics capabilities). The number of interactive fiction works is increasing steadily as new ones are produced by an online community, using freely available development systems.

The term can also be used to refer to digital versions of literary works that are not read in a linear fashion, known as gamebooks, where the reader is instead given choices at different points in the text; these decisions determine the flow and outcome of the story. The most famous example of this form of printed fiction is the "Choose Your Own Adventure" book series, and the collaborative "" format has also been described as a form of interactive fiction. The term "interactive fiction" is sometimes used also to refer to visual novels, a type of interactive narrative software popular in Japan.

Text adventures are one of the oldest types of computer games and form a subset of the adventure genre. The player uses text input to control the game, and the game state is relayed to the player via text output. Interactive fiction usually relies on reading from a screen and on typing input, although text-to-speech synthesizers allow blind and visually impaired users to play interactive fiction titles as audio games.

Input is usually provided by the player in the form of simple sentences such as "get key" or "go east", which are interpreted by a text parser. Parsers may vary in sophistication; the first text adventure parsers could only handle two-word sentences in the form of verb-noun pairs. Later parsers, such as those built on ZIL (Zork Implementation Language), could understand complete sentences. Later parsers could handle increasing levels of complexity parsing sentences such as "open the red box with the green key then go north". This level of complexity is the standard for works of interactive fiction today.

Despite their lack of graphics, text adventures include a physical dimension where players move between rooms. Many text adventure games boasted their total number of rooms to indicate how much gameplay they offered. These games are unique in that they may create an "illogical space", where going north from area A takes you to area B, but going south from area B did not take you back to area A. This can create mazes that do not behave as players expect, and thus players must maintain their own map. These illogical spaces are much more rare in today's era of 3D gaming, and the Interactive Fiction community in general decries the use of mazes entirely, claiming that mazes have become arbitrary 'puzzles for the sake of puzzles' and that they can, in the hands of inexperienced designers, become immensely frustrating for players to navigate.

Interactive fiction shares much in common with Multi-User Dungeons ('MUDs'). MUDs, which became popular in the mid-1980s, rely on a textual exchange and accept similar commands from players as do works of IF; however, since interactive fiction is single player, and MUDs, by definition, have multiple players, they differ enormously in gameplay styles. MUDs often focus gameplay on activities that involve communities of players, simulated political systems, in-game trading, and other gameplay mechanics that are not possible in a single player environment.

Interactive fiction features two distinct modes of writing: the player input and the game output. As described above, player input is expected to be in simple command form (imperative sentences). A typical command may be:> PULL Lever

The responses from the game are usually written from a second-person point of view, in present tense. This is because, unlike in most works of fiction, the main character is closely associated with the player, and the events are seen to be happening as the player plays. While older text adventures often identified the protagonist with the player directly, newer games tend to have specific, well-defined protagonists with separate identities from the player. The classic essay "Crimes Against Mimesis" discusses, among other IF issues, the nature of "You" in interactive fiction. A typical response might look something like this, the response to "look in tea chest" at the start of "Curses":

"That was the first place you tried, hours and hours ago now, and there's nothing there but that boring old book. You pick it up anyway, bored as you are." 

Many text adventures, particularly those designed for humour (such as "Zork", "The Hitchhiker's Guide to the Galaxy", and "Leather Goddesses of Phobos"), address the player with an informal tone, sometimes including sarcastic remarks (see the transcript from "Curses", above, for an example). The late Douglas Adams, in designing the IF version of his 'Hitchhiker's Guide to the Galaxy', created a unique solution to the final puzzle of the game: the game requires the one solitary item that the player "didn't" choose at the outset of play.

Some IF works dispense with second-person narrative entirely, opting for a first-person perspective ('I') or even placing the player in the position of an observer, rather than a direct participant. In some 'experimental' IF, the concept of self-identification is eliminated entirely, and the player instead takes the role of an inanimate object, a force of nature, or an abstract concept; experimental IF usually pushes the limits of the concept and challenges many assumptions about the medium.

Though neither program was developed as a narrative work, the software programs ELIZA (1964–1966) and SHRDLU (1968–1970) can formally be considered early examples of interactive fiction, as both programs used natural language processing to take input from their user and respond in a virtual and conversational manner. ELIZA simulated a psychotherapist that appeared to provide human-like responses to the user's input, while SHRDLU employed an artificial intelligence that could move virtual objects around an environment and respond to questions asked about the environment's shape. The development of effective natural language processing would become an essential part of interactive fiction development.

Around 1975, Will Crowther, a programmer and an amateur caver, wrote the first text adventure game, "Adventure" (originally called "ADVENT" because a filename could only be six characters long in the operating system he was using, and later named "Colossal Cave Adventure"). Having just gone through a divorce, he was looking for a way to connect with his two young children. Over the course of a few weekends, he wrote a text based cave exploration game that featured a sort of guide/narrator who talked in full sentences and who understood simple two-word commands that came close to natural English. Adventure was programmed in Fortran for the PDP-10. Crowther's original version was an accurate simulation of part of the real Colossal Cave, but also included fantasy elements (such as axe-wielding dwarves and a magic bridge).

Stanford University graduate student Don Woods discovered "Adventure" while working at the Stanford Artificial Intelligence Laboratory, and in 1977 obtained and expanded Crowther's source code (with Crowther's permission). Woods's changes were reminiscent of the writings of J. R. R. Tolkien, and included a troll, elves, and a volcano some claim is based on Mount Doom, but Woods says was not.

In early 1977, "Adventure" spread across ARPAnet, and has survived on the Internet to this day. The game has since been ported to many other operating systems, and was included with the floppy-disk distribution of Microsoft's MS-DOS 1.0 OS. "Adventure" is a cornerstone of the online IF community; there currently exist dozens of different independently programmed versions, with additional elements, such as new rooms or puzzles, and various scoring systems.

The popularity of "Adventure" led to the wide success of interactive fiction during the late 1970s, when home computers had little, if any, graphics capability. Many elements of the original game have survived into the present, such as the command 'xyzzy', which is now included as an Easter Egg in modern games, such as "Microsoft Minesweeper".

"Adventure" was also directly responsible for the founding of Sierra Online (later Sierra Entertainment); Ken and Roberta Williams played the game and decided to design one of their own, but with graphics.

Adventure International was founded by Scott Adams (not to be confused with the creator of "Dilbert"). In 1978, Adams wrote "Adventureland", which was loosely patterned after (the original) "Colossal Cave Adventure". He took out a small ad in a computer magazine in order to promote and sell "Adventureland", thus creating the first commercial adventure game. In 1979 he founded Adventure International, the first commercial publisher of interactive fiction. That same year, "Dog Star Adventure" was published in source code form in "SoftSide", spawning legions of similar games in BASIC.

The largest company producing works of interactive fiction was Infocom, which created the "Zork" series and many other titles, among them "Trinity", "The Hitchhiker's Guide to the Galaxy" and "A Mind Forever Voyaging".

In June 1977, Marc Blank, Bruce K. Daniels, Tim Anderson, and Dave Lebling began writing the mainframe version of "Zork" (also known as "Dungeon"), at the MIT Laboratory for Computer Science. The game was programmed in a computer language called MDL, a variant of LISP.

The term Implementer was the self-given name of the creators of the text adventure series Zork. It is for this reason that game designers and programmers can be referred to as an implementer, often shortened to Imp, rather than a writer.

In early 1979, the game was completed. Ten members of the "MIT Dynamics Modelling Group" went on to join Infocom when it was incorporated later that year.

In order to make its games as portable as possible, Infocom developed the Z-machine, a custom virtual machine that could be implemented on a large number of platforms, and took standardized "story files" as input.

In a non-technical sense, Infocom was responsible for developing the interactive style that would be emulated by many later interpreters. The Infocom parser was widely regarded as the best of its era. It accepted complex, complete sentence commands like "put the blue book on the writing desk" at a time when most of its competitors parsers were restricted to simple two-word verb-noun combinations such as "put book". The parser was actively upgraded with new features like undo and error correction, and later games would 'understand' multiple sentence input: 'pick up the gem and put it in my bag. take the newspaper clipping out of my bag then burn it with the book of matches'.

Several companies offered optional commercial feelies (physical props associated with a game). The tradition of 'feelies' (and the term itself) is believed to have originated with "Deadline" (1982), the third Infocom title after "Zork I" and "II". When writing this game, it was not possible to include all of the information in the limited (80KB) disk space, so Infocom created the first feelies for this game; extra items that gave more information than could be included within the digital game itself. These included police interviews, the coroner's findings, letters, crime scene evidence and photos of the murder scene.

These materials were very difficult for others to copy or otherwise reproduce, and many included information that was essential to completing the game. Seeing the potential benefits of both aiding game-play immersion and providing a measure of creative copy-protection, in addition to acting as a deterrent to software piracy, Infocom and later other companies began creating feelies for numerous titles. In 1987, Infocom released a special version of the first three "Zork" titles together with plot-specific coins and other trinkets. This concept would be expanded as time went on, such that later game feelies would contain passwords, coded instructions, page numbers, or other information that would be required to successfully complete the game.

Interactive fiction became a standard product for many software companies. By 1982 "Softline" wrote that "the demands of the market are weighted heavily toward hi-res graphics" in games like Sierra's "The Wizard and the Princess" and its imitators. Such graphic adventures became the dominant form of the genre on computers with graphics, like the Apple II. By 1982 Adventure International began releasing versions of its games with graphics. The company went bankrupt in 1985. Synapse Software and Acornsoft were also closed in 1985. Leaving Infocom as the leading company producing text-only adventure games on the Apple II with sophisticated parsers and writing, and still advertising its lack of graphics as a virtue. The company was bought by Activision in 1986 after the failure of "Cornerstone", Infocom's database software program, and stopped producing text adventures a few years later. Soon after Telaium/Trillium also closed.

Probably the first commercial work of interactive fiction produced outside the U.S. was the dungeon crawl game of "Acheton", produced in Cambridge, England, and first commercially released by Acornsoft (later expanded and reissued by Topologika). Other leading companies in the UK were Magnetic Scrolls and Level 9 Computing. Also worthy of mention are Delta 4, Melbourne House, and the homebrew company Zenobi.

In the early 1980s Edu-Ware also produced interactive fiction for the Apple II as designated by the "if" graphic that was displayed on startup. Their titles included the "Prisoner" and "Empire" series ("Empire I: World Builders", "Empire II: Interstellar Sharks", "Empire III: Armageddon").

In 1981, CE Software published "SwordThrust" as a commercial successor to the "Eamon" gaming system for the Apple II. SwordThrust and Eamon were simple two-word parser games with many role-playing elements not available in other interactive fiction. While SwordThrust published seven different titles, it was vastly overshadowed by the non-commercial Eamon system which allowed private authors to publish their own titles in the series. By March 1984, there were 48 titles published for the Eamon system (and over 270 titles in total as of March 2013).

In Italy, interactive fiction games were mainly published and distributed through various magazines in included tapes. The largest number of games were published in the two magazines Viking and Explorer, with versions for the main 8-bit home computers (Sinclair ZX Spectrum, Commodore 64 and MSX). The software house producing those games was Brainstorm Enterprise, and the most prolific IF author was Bonaventura Di Bello, who produced 70 games in the Italian language. The wave of interactive fiction in Italy lasted for a couple of years thanks to the various magazines promoting the genre, then faded and remains still today a topic of interest for a small group of fans and less known developers, celebrated on Web sites and in related newsgroups.

In Spain, interactive fiction was considered a minority genre, and was not very successful. The first Spanish interactive fiction commercially released was "Yenght" in 1983, by Dinamic Software, for the ZX Spectrum. Later on, in 1987, the same company produced an interactive fiction about "Don Quijote". After several other attempts, the company Aventuras AD, emerged from Dinamic, became the main interactive fiction publisher in Spain, including titles like a Spanish adaptation of "Colossal Cave Adventure", an adaptation of the Spanish comic "El Jabato", and mainly the "Ci-U-Than" trilogy, composed by "La diosa de Cozumel" (1990), "Los templos sagrados" (1991) and "Chichen Itzá" (1992). During this period, the Club de Aventuras AD (CAAD), the main Spanish speaking community around interactive fiction in the world, was founded, and after the end of Aventuras AD in 1992, the CAAD continued on its own, first with their own magazine, and then with the advent of Internet, with the launch of an active internet community that still produces interactive non-commercial fiction nowadays.

Legend Entertainment was founded by Bob Bates and Mike Verdu in 1989. It started out from the ashes of Infocom. The text adventures produced by Legend Entertainment used (high-resolution) graphics as well as sound. Some of their titles include "Eric the Unready", the "Spellcasting" series and "Gateway" (based on Frederik Pohl's novels).

The last text adventure created by Legend Entertainment was "Gateway II" (1992), while the last game ever created by Legend was "" (2003) – a well-known first-person shooter action game using the Unreal Engine for both impressive graphics and realistic physics. In 2004, Legend Entertainment was acquired by Atari, who published "Unreal II" and released for both Microsoft Windows and Microsoft's Xbox.

Many other companies such as Level 9 Computing, Magnetic Scrolls, Delta 4 and Zenobi had closed by 1992.

In 1991 and 1992, Activision released "The Lost Treasures of Infocom" in two volumes, a collection containing most of Infocom's games, followed in 1996 by "Classic Text Adventure Masterpieces of Infocom".

After the decline of the commercial interactive fiction market in the 1990s, an online community eventually formed around the medium. In 1987, the Usenet newsgroup rec.arts.int-fiction was created, and was soon followed by rec.games.int-fiction. By custom, the topic of rec.arts.int-fiction is interactive fiction authorship and programming, while rec.games.int-fiction encompasses topics related to playing interactive fiction games, such as hint requests and game reviews. As of late 2011, discussions between writers have mostly moved from rec.arts.int-fiction to the Interactive Fiction Community Forum.

One of the most important early developments was the reverse-engineering of Infocom's Z-Code format and Z-Machine virtual machine in 1987 by a group of enthusiasts called the InfoTaskForce and the subsequent development of an interpreter for Z-Code story files. As a result, it became possible to play Infocom's work on modern computers.

For years, amateurs with the IF community produced interactive fiction works of relatively limited scope using the Adventure Game Toolkit and similar tools.

The breakthrough that allowed the interactive fiction community to truly prosper, however, was the creation and distribution of two sophisticated development systems. In 1987, Michael J. Roberts released TADS, a programming language designed to produce works of interactive fiction. In 1993, Graham Nelson released Inform, a programming language and set of libraries which compiled to a Z-Code story file. Each of these systems allowed anyone with sufficient time and dedication to create a game, and caused a growth boom in the online interactive fiction community.

Despite the lack of commercial support, the availability of high quality tools allowed enthusiasts of the genre to develop new high quality games. Competitions such as the annual Interactive Fiction Competition for short works, the Spring Thing for longer works, and the XYZZY Awards, further helped to improve the quality and complexity of the games. Modern games go much further than the original "Adventure" style, improving upon Infocom games, which relied extensively on puzzle solving, and to a lesser extent on communication with non-player characters, to include experimentation with writing and story-telling techniques.

While the majority of modern interactive fiction that is developed is distributed for free, there are some commercial endeavors. In 1998, Michael Berlyn, a former Implementor at Infocom, started a new game company, Cascade Mountain Publishing, whose goals were to publish interactive fiction. Despite the Interactive Fiction community providing social and financial backing Cascade Mountain Publishing went out of business in 2000.

Other commercial endeavours include Peter Nepstad's "", several games by Howard Sherman published as Malinche Entertainment, The General Coffee Company's "Future Boy!," "Cypher", a graphically enhanced cyberpunk game and various titles by "Textfyre". Emily Short was commissioned to develop the game "City of Secrets" but the project fell through and she ended up releasing it herself.

The increased effectiveness of natural-language-generation in artificial intelligence (AI) has led to instances of interactive fiction which use AI to dynamically generate new, open-ended content, instead of being constrained to pre-written material. The most notable example of this is "AI Dungeon", released in 2019, which generates content using the GPT-3 (previously GPT-2) natural-language-generating neural network, created by OpenAI.


The original interactive fiction "Colossal Cave Adventure" was programmed in Fortran, originally developed by IBM. "Adventure"s parsers could only handle two-word sentences in the form of verb-noun pairs.

Infocom's games of 1979–88, such as "Zork", were written using a LISP-like programming language called ZIL (Zork Implementation Language or Zork Interactive Language, it was referred to as both) that compiled into a byte code able to run on a standardized virtual machine called the Z-machine. As the games were text based and used variants of the same Z-machine interpreter, the interpreter only had to be ported to a computer once, rather than once each game. Each game file included a sophisticated parser which allowed the user to type complex instructions to the game. Unlike earlier works of interactive fiction which only understood commands of the form 'verb noun', Infocom's parser could understand a wider variety of sentences. For instance one might type "open the large door, then go west", or "go to the hall". With the Z-machine, Infocom was able to release most of their games for most popular home computers of the time simultaneously, including Apple II family, Atari 800, IBM PC compatibles, Amstrad CPC/PCW (one disc worked on both machines), Commodore 64, Commodore Plus/4, Commodore 128, Kaypro CP/M, Texas Instruments TI-99/4A, the Mac, Atari ST, the Commodore Amiga and the Radio Shack TRS-80. Infocom was also known for shipping creative props, or "feelies" (and even "smellies"), with its games.

During the 1990s Interactive fiction was mainly written with C-like languages, such as TADS 2 and Inform 6. A number of systems for writing interactive fiction now exist. The most popular remain Inform, TADS, or ADRIFT, but they diverged in their approach to IF-writing during the 2000s, giving today's IF writers an objective choice. By 2006 IFComp, most games were written for Inform, with a strong minority of games for TADS and ADRIFT, followed by a small number of games for other systems.

While familiarity with a programming language leads many new authors to attempt to produce their own complete IF application, most established IF authors recommend use of a specialised IF language, arguing that such systems allow authors to avoid the technicalities of producing a full featured parser, while allowing broad community support. The choice of authoring system usually depends on the author's desired balance of ease of use versus power, and the portability of the final product.

Other development systems include:

Interpreters are the software used to play the works of interactive fiction created with a development system. Since they need to interact with the player, the "story files" created by development systems are programs in their own right. Rather than running directly on any one computer, they are programs run by Interpreters, or virtual machines, which are designed specially for IF. They may be part of the development system, or can be compiled together with the work of fiction as a standalone executable file.

The Z-machine was designed by the founders of Infocom, in 1979. They were influenced by the then-new idea of a virtual Pascal computer, but replaced P with Z for Zork, the celebrated adventure game of 1977–79. The Z-machine evolved during the 1980s but over 30 years later, it remains in use essentially unchanged. Glulx was designed by Andrew Plotkin in the late 1990s as a new-generation IF virtual machine. It overcomes the technical constraint on the Z-machine by being a 32-bit rather than 16-bit processor. Frotz is a modern Z-machine interpreter originally written in C (programming language) by Stefan Jokisch in 1995 for DOS. Over time it was ported to other platforms, such as Unix, RISC OS, Mac OS and most recently iOS. Modern Glulx interpreters are based on "Glulxe", by Andrew Plotkin, and "Git", by Iain Merrick. Other interpreters include Zoom for Mac OS X, or for Unix or Linux, maintained by Andrew Hunter, and Spatterlight for Mac OS X, maintained by Tor Andersson.

In addition to commercial distribution venues and individual websites, many works of free interactive fiction are distributed through community websites. These include the Interactive Fiction Database (IFDb), The Interactive Fiction Reviews Organization (IFRO), a game catalog and recommendation engine, and the Interactive Fiction Archive.

Works may be distributed for playing with in a separate interpreter. In which case they are often made available in the Blorb package format that many interpreters support. A filename ending .zblorb is a story file intended for a Z-machine in a Blorb wrapper, while a filename ending .gblorb is a story file intended for a Glulx in a Blorb wrapper. It is not common but IF files are sometimes also seen without a Blorb wrapping, though this usually means cover art, help files, and so forth are missing, like a book with the covers torn off. Z-machine story files usually have names ending .z5 or .z8, the number being a version number, and Glulx story files usually end .ulx.

Alternatively, works may be distributed for playing in a web browser. For example, the 'Parchment' project is for web browser-based IF Interpreter, for both Z-machine and Glulx files.

Some software such as Twine publishes directly to HTML, the standard language used to create web pages, reducing the requirement for an Interpreter or virtual machine.





</doc>
