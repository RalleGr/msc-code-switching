<doc id="21849" url="https://en.wikipedia.org/wiki?curid=21849" title="Nintendo 64">
Nintendo 64

The (officially abbreviated as N64, hardware model number pre-term: NUS, stylized as NINTENDO) is a home video game console developed and marketed by Nintendo. Named for its 64-bit central processing unit, it was released in June 1996 in Japan, September 1996 in North America, and March 1997 in Europe and Australia. It was the last major home console to use the ROM cartridge as its primary storage format until the Switch in 2017. The Nintendo 64 was discontinued in 2002 following the launch of its successor, the GameCube, in 2001.

Codenamed "Project Reality", the Nintendo 64 design was mostly complete by mid-1995, but its launch was delayed until 1996, when "Time" named it Machine of the Year. It was launched with three games: "Super Mario 64", "Pilotwings 64" and "Saikyō Habu Shōgi" (exclusive to Japan). As part of the fifth generation of video game consoles, it competed primarily with the Sony PlayStation and Sega Saturn. The suggested retail price at its United States launch was , and as of December 2009, 32.93 million units had been sold worldwide. In 2015, "IGN" named it the ninth-greatest video game console of all time.

Around the end of the 1980s, Nintendo led the video game industry with its Nintendo Entertainment System (NES). Although the NES follow-up console, the Super NES (SNES), was successful, sales took a hit from the Japanese recession. Competition from long-time rival Sega, and relative newcomer Sony, emphasized Nintendo's need to develop a successor for the SNES, or risk losing market dominance to its competitors. Further complicating matters, Nintendo also faced a backlash from third-party developers unhappy with Nintendo's strict licensing policies.

Silicon Graphics, Inc. (SGI), a long-time leader in graphics visualization and supercomputing, was interested in expanding its business by adapting its technology into the higher volume realm of consumer products, starting with the video game market. Based upon its MIPS R4000 family of supercomputing and workstation CPUs, SGI developed a CPU requiring a fraction of the resources—consuming only 0.5 watts of power instead of 1.5 to 2 watts, with an estimated target price of instead of –200. The company created a design proposal for a video game system, seeking an already well established partner in that market. Jim Clark, founder of SGI, initially offered the proposal to Tom Kalinske, who was the CEO of Sega of America. The next candidate would be Nintendo.

The historical details of these preliminary negotiations were controversial between the two competing suitors. Tom Kalinske said that he and Joe Miller of Sega of America were "quite impressed" with SGI's prototype, inviting their hardware team to travel from Japan to meet with SGI. The engineers from Sega Enterprises claimed that their evaluation of the early prototype had uncovered several unresolved hardware issues and deficiencies. Those were subsequently resolved, but Sega had already decided against SGI's design. Nintendo resisted that summary conclusion, arguing that the real reason for SGI's ultimate choice of partner is that Nintendo was a more appealing business partner than Sega. While Sega demanded exclusive rights to the chip, Nintendo was willing to license the technology on a non-exclusive basis. Michael Slater, publisher of "Microprocessor Report" said, "The mere fact of a business relationship there is significant because of Nintendo's phenomenal ability to drive volume. If it works at all, it could bring MIPS to levels of volume [SGI] never dreamed of".

Jim Clark met with Nintendo CEO Hiroshi Yamauchi in early 1993, thus initiating Project Reality. On August 23, 1993, the two companies announced a global joint development and licensing agreement surrounding Project Reality, projecting that the yet unnamed eventual product would be "developed specifically for Nintendo, will be unveiled in arcades in 1994, and will be available for home use by late 1995 ... below $250". This announcement coincided with Nintendo's August 1993 Shoshinkai trade show.

"Reality Immersion Technology" is the name SGI had given the set of core components, which would be first utilized in Project Reality: the MIPS R4300i CPU, the MIPS Reality Coprocessor, and the embedded software. Some chip technology and manufacturing was provided by NEC, Toshiba, and Sharp. SGI had recently acquired MIPS Computer Systems (renamed to MIPS Technologies), and the two worked together to be ultimately responsible for the design of the Reality Immersion Technology chips under engineering director Jim Foran and chief hardware architect Tim Van Hook.
The initial Project Reality game development platform was developed and sold by SGI in the form of its Onyx supercomputer costing – and loaded with the namesake RealityEngine2 graphics boards and four 150 MHz R4400 CPUs. Its software includes early Project Reality application and emulation APIs based on Performer and OpenGL. This graphics supercomputing platform had served as the source design which SGI had reduced down to become the Reality Immersion Technology for Project Reality.

The Project Reality team prototyped a game controller for the development system by modifying a Super NES controller to have a primitive analog joystick and Z trigger. Under maximal secrecy even from the rest of the company, a LucasArts developer said his team would "furtively hide the prototype controller in a cardboard box while we used it. In answer to the inevitable questions about what we were doing, we replied jokingly that it was a new type of controllera bowl of liquid that absorbed your thoughts through your fingertips. Of course, you had to think in Japanese..."

On June 23, 1994, Nintendo announced the new official name of the still unfinished console as "Ultra 64". The first group of elite developers selected by Nintendo was nicknamed the "Dream Team": Silicon Graphics, Inc.; Alias Research, Inc.; Software Creations; Rambus, Inc.; MultiGen, Inc.; Rare, Ltd. and Rare Coin-It Toys & Games, Inc.; WMS Industries, Inc.; Acclaim Entertainment, Inc.; Williams Entertainment, Inc.; Paradigm Simulation, Inc.; Spectrum Holobyte; DMA Design Ltd.; Angel Studios; Ocean; Time Warner Interactive; and Mindscape.

By purchasing and developing upon Project Reality's graphics supercomputing platform, Nintendo and its Dream Team could begin prototyping their games according to SGI's estimated console performance profile, prior to the finalization of the console hardware specifications. When the Ultra 64 hardware was finalized, that supercomputer-based prototyping platform was later supplanted by a much cheaper and fully accurate console simulation board to be hosted within a low-end SGI Indy workstation in July 1995. SGI's early performance estimates based upon its supercomputing platform were ultimately reported to have been fairly accurate to the final Ultra 64 product, allowing LucasArts developers to port their "Star Wars" game prototype to console reference hardware in only three days.

The console's design was publicly revealed for the first time in late Q2 1994. Images of the console displayed the Nintendo Ultra 64 logo and a ROM cartridge, but no controller. This prototype console's form factor would be retained by the product when it eventually launched. Having initially indicated the possibility of utilizing the increasingly popular CD-ROM if the medium's endemic performance problems were solved, the company now announced a much faster but space-limited cartridge-based system, which prompted open analysis by the gaming press. The system was frequently marketed as the world's first 64-bit gaming system, often stating the console was more powerful than the first moon landing computers. Atari had already claimed to have made the first 64-bit game console with their Atari Jaguar, but the Jaguar only uses a general 64-bit architecture in conjunction with two 32-bit RISC processors and a 16/32-bit Motorola 68000.

Later in Q2 1994, Nintendo signed a licensing agreement with Midway's parent company which enabled Midway to develop and market arcade games with the Ultra 64 brand, and formed a joint venture company called "Williams/Nintendo" to market Nintendo-exclusive home conversions of these games. The result is two Ultra 64 branded arcade games, "Killer Instinct" and "Cruis'n USA". Not derived from Project Reality's console-based branch of Ultra 64, the arcade branch uses a different MIPS CPU, has no Reality Coprocessor, and uses onboard ROM chips and a hard drive instead of a cartridge. "Killer Instinct" features 3D character artwork pre-rendered into 2D form, and computer-generated movie backgrounds that are streamed off the hard drive and animated as the characters move horizontally.

Previously, the plan had been to release the console with the name "Ultra Famicom" in Japan and "Nintendo Ultra 64" in other markets. Rumors circulated attributing the name change to the possibility of legal action by Konami's ownership of the Ultra Games trademark. Nintendo said that trademark issues were not a factor, and the sole reason for any name change was to establish a single worldwide brand and logo for the console. The new global name "Nintendo 64" was proposed by "Earthbound" series developer Shigesato Itoi. The prefix for the model numbering scheme for hardware and software across the Nintendo 64 platform is "NUS-", a reference to the console's original name of "Nintendo Ultra Sixty-four".

The newly renamed Nintendo 64 console was fully unveiled to the public in playable form on November 24, 1995, at Nintendo's 7th Annual Shoshinkai trade show. Eager for a preview, "hordes of Japanese schoolkids huddled in the cold outside ... the electricity of anticipation clearly rippling through their ranks". "Game Zero" magazine disseminated photos of the event two days later. Official coverage by Nintendo followed later via the "Nintendo Power" website and print magazine.

The console was originally slated for release by Christmas of 1995. In May 1995, Nintendo delayed the release to April 1996. Consumers anticipating a Nintendo release the following year at a lower price than the competition reportedly reduced the sales of competing Sega and Sony consoles during the important Christmas shopping season. "Electronic Gaming Monthly" editor Ed Semrad even suggested that Nintendo may have announced the April 1996 release date with this end in mind, knowing in advance that the system would not be ready by that date.

In its explanation of the delay, Nintendo claimed it needed more time for Nintendo 64 software to mature, and for third-party developers to produce games. Adrian Sfarti, a former engineer for SGI, attributed the delay to hardware problems; he claimed that the chips underperformed in testing and were being redesigned. In 1996, the Nintendo 64's software development kit was completely redesigned as the Windows-based Partner-N64 system, by Kyoto Microcomputer, Co. Ltd. of Japan.

The Nintendo 64's release date was later delayed again, to June 23, 1996. Nintendo said the reason for this latest delay, and in particular the cancellation of plans to release the console in all markets worldwide simultaneously, was that the company's marketing studies now indicated that they would not be able to manufacture enough units to meet demand by April 1996, potentially angering retailers in the same way Sega had done with its surprise early launch of the Saturn in North America and Europe.

To counteract the possibility that gamers would grow impatient with the wait for the Nintendo 64 and purchase one of the several competing consoles already on the market, Nintendo ran ads for the system well in advance of its announced release dates, with slogans like "Wait for it..." and "Is it worth the wait? Only if you want the best!"

"Popular Electronics" called the launch a "much hyped, long-anticipated moment". Several months before the launch, "GamePro" reported that many gamers, including a large percentage of their own editorial staff, were already saying they favored the Nintendo 64 over the Saturn and PlayStation.
The console was first released in Japan on June 23, 1996. Though the initial shipment of 300,000 units sold out on the first day, Nintendo successfully avoided a repeat of the Super Famicom launch day pandemonium, in part by using a wider retail network which included convenience stores. The remaining 200,000 units of the first production run shipped on June 26 and 30, with almost all of them reserved ahead of time. In the months between the Japanese and North American launches, the Nintendo 64 saw brisk sales on the American grey market, with import stores charging as much as $699 plus shipping for the system. The Nintendo 64 was first sold in North America on September 26, 1996, though having been advertised for the 29th. It was launched with just two games in the United States, "Pilotwings 64" and "Super Mario 64"; "Cruis'n USA" was pulled from the lineup less than a month before launch because it did not meet Nintendo's quality standards. In 1994, prior to the launch, Nintendo of America chairman Howard Lincoln emphasized the quality of first-party games, saying "... we're convinced that a few great games at launch are more important than great games mixed in with a lot of dogs". The PAL version of the console was released in Europe on March 1, 1997. According to Nintendo of America representatives, Nintendo had been planning a simultaneous launch in Japan, North America, and Europe, but market studies indicated that worldwide demand for the system far exceeded the number of units they could have ready by launch, potentially leading to consumer and retailer frustration.
Originally intended to be priced at , the console was ultimately launched at to make it competitive with Sony and Sega offerings, as both the Saturn and PlayStation had been lowered to $199.99 earlier that summer. Nintendo priced the console as an impulse purchase, a strategy from the toy industry. The price of the console in the United States was further reduced in August 1998.

The Nintendo 64's North American launch was backed with a $54 million marketing campaign by Leo Burnett Worldwide (meaning over $100 in marketing per North American unit that had been manufactured up to this point). While the competing Saturn and PlayStation both set teenagers and adults as their target audience, the Nintendo 64's target audience was pre-teens.

To boost sales during the slow post-Christmas season, Nintendo and General Mills worked together on a promotional campaign that appeared in early 1999. The advertisement by Saatchi and Saatchi, New York began on January 25 and encouraged children to buy Fruit by the Foot snacks for tips to help them with their Nintendo 64 games. Ninety different tips were available, with three variations of thirty tips each.

Nintendo advertised its Funtastic Series of peripherals with a $10 million print and television campaign from February 28 to April 30, 2000. Leo Burnett Worldwide was in charge again.

The Nintendo 64's central processing unit (CPU) is the NEC VR4300 at 93.75 MHz. "Popular Electronics" said it had power similar to the Pentium processors found in desktop computers. Except for its narrower 32-bit system bus, the VR4300 retained the computational abilities of the more powerful 64-bit MIPS R4300i, though software rarely took advantage of 64-bit data precision operations. Nintendo 64 games generally used faster and more compact 32-bit data-operations, as these were sufficient to generate 3D-scene data for the console's RSP (Reality Signal Processor) unit. In addition, 32-bit code executes faster and requires less storage space (which is at a premium on the Nintendo 64's cartridges).

In terms of its random-access memory (RAM), the Nintendo 64 is one of the first modern consoles to implement a unified memory subsystem, instead of having separate banks of memory for CPU, audio, and video, for example. The memory itself consists of 4 megabytes of Rambus RDRAM, expandable to 8 MB with the Expansion Pak. Rambus was quite new at the time and offered Nintendo a way to provide a large amount of bandwidth for a relatively low cost.

Audio may be processed by the Reality Coprocessor or the CPU and is output to a DAC with up to sample rate.

The system allows for video output in two formats: composite video and S-Video. The composite and S-Video cables are the same as those used with the preceding SNES and succeeding GameCube platforms.

The Nintendo 64 supports 16.8 million colors. The system can display resolutions from 320×240 up to 640×480 pixels. Most games that make use of the system's higher resolution 640x480 mode require use of the Expansion Pak RAM upgrade; several do not, such as Acclaim's "NFL Quarterback Club" series and EA Sports's second generation "Madden", "FIFA", "Supercross", and "NHL" games. The majority of games use the system's low resolution 320×240 mode. Many games support a video display ratio of up to using either anamorphic widescreen or letterboxing.

The Nintendo 64 is one of the first gaming consoles to have four controller ports. According to Shigeru Miyamoto, Nintendo opted to have four controller ports because the Nintendo 64 is the company's first console which can handle a four player split screen without significant slowdown.

The Nintendo 64 comes in several colors. The standard Nintendo 64 is dark gray, nearly black, and the controller is light gray (later releases in the U.S. and in Australia included a bonus second controller in Atomic Purple). Various colorations and special editions were released.

Most Nintendo 64 game cartridges are gray in color, but some games have a colored cartridge. Fourteen games have black cartridges, and other colors (such as yellow, blue, red, gold and green) were each used for six or fewer games. Several games, such as "", were released both in standard gray and in colored, limited edition versions.

The programming characteristics of the Nintendo 64 present unique challenges, with distinct potential advantages. "The Economist" described effective programming for the Nintendo 64 as being "horrendously complex". As with many other game consoles and other types of embedded systems, the Nintendo 64's architectural optimizations are uniquely acute, due to a combination of oversight on the part of the hardware designers, limitations on 3D technology of the time, and manufacturing capabilities.

As the Nintendo 64 reached the end of its lifecycle, hardware development chief Genyo Takeda repeatedly referred to the programming challenges using the word "hansei" ( "reflective regret"). Looking back, Takeda said "When we made Nintendo 64, we thought it was logical that if you want to make advanced games, it becomes technically more difficult. We were wrong. We now understand it's the cruising speed that matters, not the momentary flash of peak power".

Nintendo initially stated that while the Nintendo 64 units for each region use essentially identical hardware design, regional lockout chips would prevent games from one region from being played on a Nintendo 64 console from a different region. Following the North American launch, however, they admitted that the cartridges contain no such chips, and the regional lockout is enforced by differing notches in the back of the cartridges.

A total of 388 games were released for the console, though there were a few that were exclusively sold in Japan. For comparison, rivals PlayStation and the Sega Saturn received around 1,100 games and 600 games respectively, while previous Nintendo consoles such as the NES and SNES had 768 and 725 games released in the United States. However, the Nintendo 64 game library included a high number of critically acclaimed and widely sold games. According to TRSTS reports, three of the top five best-selling games in the U.S. for December 1996 were Nintendo 64 games (both of the remaining two were Super NES games). "Super Mario 64" is the best selling game of the generation, with 11 million units sold beating the PlayStation's "Gran Turismo" (at 10.85 million) and "Final Fantasy VII" (at 9.72 million) in sales. The game also received much praise from critics and helped to pioneer three-dimensional control schemes. "GoldenEye 007" was important in the evolution of the first-person shooter, and has been named one of the greatest in the genre. "" set the standard for future 3D action-adventure games and is considered by many to be one of the greatest games ever made. This trend followed Hiroshi Yamauchi's strategy, announced during his speech at the Nintendo 64's November 1995 unveiling, that Nintendo restrict the number of titles produced for the Nintendo 64 so that developers would focus on developing games to a higher standard instead of trying to outdo their competitors with sheer quantity.

The most graphically demanding Nintendo 64 games that arrived on larger 32 or 64 MB cartridges are the most advanced and detailed of the 32-bit/64-bit generation. In order to maximize use of the Nintendo 64 hardware developers had to create their own custom microcode. Nintendo 64 games running on custom microcode benefited from much higher polygon counts in tandem with more advanced lighting, animation, physics and AI routines than its 32-bit competition. "Conker's Bad Fur Day" is arguably the pinnacle of its generation combining multicolored real-time lighting that illuminates each area to real-time shadowing and detailed texturing replete with a full in game facial animation system. The Nintendo 64's graphics chip is capable of executing many more advanced and complex rendering techniques than its competitors. It is the first home console to feature trilinear filtering, which allowed textures to look very smooth. This contrasted with the Saturn and PlayStation, which used nearest-neighbor interpolation and produced more pixelated textures. Overall however the results of the Nintendo cartridge system were mixed and this was tied primarily to its storage medium.

The smaller storage size of ROM cartridges limited the number of available textures. As a result, many games which utilized much smaller 8 or 12 MB cartridges are forced to stretch textures over larger surfaces. Compounded by a limit of 4,096 bytes of on-chip texture memory, the end-result is often a distorted, out-of-proportion appearance. Many titles that feature larger 32 or 64 MB cartridges avoided this issue entirely, notable games include "Resident Evil 2", "Sin and Punishment: Successor of the Earth", and "Conker's Bad Fur Day" as they feature more ROM space, allowing for more detailed graphics by utilizing multiple, multi-layered textures across all surfaces.

Nintendo 64 games are ROM cartridge based. Cartridge size varies from 4 to 64 MB. Many cartridges include the ability to save games internally.
Nintendo cited several advantages for making the Nintendo 64 cartridge-based. Primarily cited was the ROM cartridges' very fast load times in comparison to disc-based games. While loading screens appear in many PlayStation games, they are rare in Nintendo 64 games. Although vulnerable to long-term environmental damage the cartridges are far more resistant to physical damage than compact discs. Nintendo also cited the fact that cartridges are more difficult to pirate than CDs, thus resisting copyright infringement, albeit at the expense of lowered profit margin for Nintendo. While unauthorized N64 interface devices for the PC were later developed, these devices are rare, when compared to a regular CD drive used on the PlayStation which suffered widespread copyright infringement.

On the downside, cartridges took longer to manufacture than CDs, with each production run (from order to delivery) taking two weeks or more. This meant that publishers of Nintendo 64 games had to attempt to predict demand for a game ahead of its release. They risked being left with a surplus of expensive cartridges for a failed game or a weeks-long shortage of product if they underestimated a game's popularity. The cost of producing a Nintendo 64 cartridge was also far higher than for a CD. Publishers passed these expenses onto the consumer. Nintendo 64 games cost an average of $10 more when compared to games produced for rival consoles. The higher cost also created the potential for much greater losses to the game's publisher in the case of a flop, making the less risky CD medium tempting for third party companies. Some third party companies also complained that they were at an unfair disadvantage against Nintendo first party developers when publishing games for the Nintendo 64, since Nintendo owned the manufacturing plant where cartridges for their consoles are made and therefore could sell their first party games at a lower price.

As fifth generation games became more complex in content, sound and graphics, games began to exceed the limits of cartridge storage capacity. Nintendo 64 cartridges had a maximum of 64 MB of data, whereas CDs held 650 MB. The "Los Angeles Times" initially defended the quality control incentives associated with working with limited storage on cartridges, citing Nintendo's position that cartridge game developers tend to "place a premium on substance over flash", and noted that the launch titles lack the "poorly acted live-action sequences or half-baked musical overtures" which it says tend to be found on CD-ROM games. However, the cartridge's limitations became apparent with software ported from other consoles, so Nintendo 64 versions of cross-platform games were truncated or redesigned with the storage limits of a cartridge in mind. For instance this meant fewer textures, and/or shorter music tracks, while full motion video was not usually feasible for use in cutscenes unless heavily compressed and of very brief length.

The era's competing systems from Sony and Sega (the PlayStation and Saturn, respectively) used CD-ROM discs to store their games. As a result, game developers who had traditionally supported Nintendo game consoles were now developing games for the competition. Some third-party developers, such as Square and Enix, whose "Final Fantasy VII" and "Dragon Warrior VII" were initially planned for the Nintendo 64, switched to the PlayStation, citing the insufficient storage capacity of the N64 cartridges. Some who remained released fewer games to the Nintendo 64; Konami released fifty PlayStation games, but only twenty nine for the Nintendo 64. New Nintendo 64 game releases were infrequent while new games were coming out rapidly for the PlayStation.

Through the difficulties with third parties, the Nintendo 64 supported popular games such as "GoldenEye 007", giving it a long market life. Additionally, Nintendo's strong first-party franchises such as "Mario" had strong name brand appeal. Second-parties of Nintendo, such as Rare, helped.

Nintendo's controversial selection of the cartridge medium for the Nintendo 64 has been cited as a key factor in Nintendo losing its dominant position in the gaming market. The ROM cartridges are constrained by small capacity and high production expenses, compared to the compact disc format used by its chief competitors. Some of the cartridge's advantages are difficult for developers to manifest prominently, requiring innovative solutions which only came late in the console's life cycle. Another of its technical drawbacks is a limited-size texture cache, which force textures of limited dimensions and reduced color depth, appearing stretched when covering in-game surfaces. Some third-party publishers that supported Nintendo's previous consoles reduced their output or stopped publishing for the console; the Nintendo 64's most successful games came from first-party or second-party studios.

Several Nintendo 64 games have been released for the Wii's and Wii U's Virtual Console service and are playable with the Classic Controller, GameCube controller, Wii U Pro Controller, or Wii U GamePad. There are some differences between these versions and the original cartridge versions. For example, the games run in a higher resolution and at a more consistent framerate than their Nintendo 64 counterparts. Some features, such as Rumble Pak functionality, are not available in the Wii versions. Some features are also changed on the Virtual Console releases. For example, the VC version of "Pokémon Snap" allows players to send photos through the Wii's message service, while "Wave Race 64"'s in-game content was altered due to the expiration of the Kawasaki license. Several games developed by Rare were released on Microsoft's Xbox Live Arcade service, including "Banjo-Kazooie", "Banjo-Tooie", and "Perfect Dark", following Microsoft's acquisition of Rareware in 2002. One exception is "Donkey Kong 64", released in April 2015 on the Wii U Virtual Console, as Nintendo retained the rights to the game.

Several unofficial emulators have been developed in order to play Nintendo 64 games on other platforms, such as PCs, Mac and cell phones.

Nintendo 64 accessories include the Rumble Pak and the Transfer Pak.

The controller is shaped like an "M", employing a joystick in the center. "Popular Electronics" called its shape "evocative of some alien space ship". While noting that the three handles could be confusing, the magazine said "the separate grips allow different hand positions for various game types".

Nintendo released a peripheral platform called 64DD, where "DD" stands for "Disk Drive". Connecting to the expansion slot at the bottom of the system, the 64DD turns the Nintendo 64 console into an Internet appliance, a multimedia workstation, and an expanded gaming platform. This large peripheral allows players to play Nintendo 64 disk-based games, capture images from an external video source, and it allowed players to connect to the now-defunct Japanese Randnet online service. Not long after its limited mail-order release, the peripheral was discontinued. Only nine games were released, including the four "Mario Artist" games ("Paint Studio", "Talent Studio", "Communication Kit", and "Polygon Studio"). Many more planned games were eventually released in cartridge format or on other game consoles. The 64DD and the accompanying Randnet online service were released only in Japan, though always having been announced for America and Europe.

To illustrate the fundamental significance of the 64DD to all game development at Nintendo, lead designer Shigesato Itoi said: "I came up with a lot of ideas because of the 64DD. All things start with the 64DD. There are so many ideas I wouldn’t have been allowed to come up with if we didn’t have the 64DD". Shigeru Miyamoto concluded: "Almost every new project for the N64 is based on the 64DD. ... we’ll make the game on a cartridge first, then add the technology we’ve cultivated to finish it up as a full-out 64DD game".

The Nintendo 64 received generally positive reviews from critics. Reviewers praised the console's advanced 3D graphics and gameplay, while criticizing the lack of games. On G4techTV's "Filter", the Nintendo 64 was voted up to No. 1 by registered users.

In February 1996, "Next Generation" magazine called the Nintendo Ultra 64 the "best kept secret in videogames" and the "world's most powerful game machine". It called the system's November 24, 1995 unveiling at Shoshinkai "the most anticipated videogaming event of the 1990s, possibly of all time". Previewing the Nintendo 64 shortly prior to its launch, "Time" magazine praised the realistic movement and gameplay provided by the combination of fast graphics processing, pressure-sensitive controller, and the "Super Mario 64" game. The review praised the "fastest, smoothest game action yet attainable via joystick at the service of equally virtuoso motion", where "[f]or once, the movement on the screen feels real". Asked if gamers should buy a Nintendo 64 at launch, buy it later, or buy a competing system, a panel of six "GamePro" editors voted almost unanimously to buy at launch; one editor said gamers who already own a PlayStation and are on a limited budget should buy it later, and all others should buy it at launch.

At launch, the "Los Angeles Times" called the system "quite simply, the fastest, most graceful game machine on the market". Its form factor was described as small, light, and "built for heavy play by kids" unlike the "relatively fragile Sega Saturn". Showing concern for a major console product launch during a sharp, several-year long, decline in the game console market, the review said that the long-delayed Nintendo 64 was "worth the wait" in the company's pursuit of quality. Nintendo's "penchant for perfection" in game quality control was praised, though with concerns about having only two launch titles at retail and twelve expected by Christmas. Describing the quality control incentives associated with cartridge-based development, the "Times" cited Nintendo's position that cartridge game developers tend to "place a premium on substance over flash", and noted that the launch titles lack the "poorly acted live-action sequences or half-baked musical overtures" which it says tend to be found on CD-ROM games. Praising Nintendo's controversial choice of the cartridge medium with its "nonexistent" load times and "continuous, fast-paced action CD-ROMs simply cannot deliver", the review concluded that "the cartridge-based Nintendo 64 delivers blistering speed and tack-sharp graphics that are unheard of on personal computers and make competing 32-bit, disc-based consoles from Sega and Sony seem downright sluggish".

"Time" named it the 1996 Machine of the Year, saying the machine had "done to video-gaming what the 707 did to air travel". The magazine said the console achieved "the most realistic and compelling three-dimensional experience ever presented by a computer". "Time" credited the Nintendo 64 with revitalizing the video game market, "rescuing this industry from the dustbin of entertainment history". The magazine suggested that the Nintendo 64 would play a major role in introducing children to digital technology in the final years of the 20th century. The article concluded by saying the console had already provided "the first glimpse of a future where immensely powerful computing will be as common and easy to use as our televisions". The console also won the 1996 Spotlight Award for Best New Technology.

"Popular Electronics" complimented the system's hardware, calling its specifications "quite impressive". It found the controller "comfortable to hold, and the controls to be accurate and responsive".

In a 1997 year-end review, a team of five "Electronic Gaming Monthly" editors gave the Nintendo 64 scores of 8.0, 7.0, 7.5, 7.5, and 9.0. They highly praised the power of the hardware and the quality of the first-party games, especially those developed by Rare's and Nintendo's internal studios, but also commented that the third-party output to date had been mediocre and the first-party output was not enough by itself to provide Nintendo 64 owners with a steady stream of good games or a full breadth of genres.

Developer Factor 5, which created some of the system's most technologically advanced games along with the system's audio development tools for Nintendo, said, "[T]he N64 is really sexy because it combines the performance of an SGI machine with a cartridge. We're big arcade fans, and cartridges are still the best for arcade games or perhaps a really fast CD-ROM. But there's no such thing for consoles yet [as of 1998]".

Lee Hutchinson of Ars Technica, who had been a Babbage's employee in the mid-1990s, reminisced on his experience of the PlayStation's strong debut and of then wondering whether Nintendo's new console could do as well:

The Nintendo 64 was in heavy demand upon its release. David Cole, industry analyst, said "You have people fighting to get it from stores". "Time" called the purchasing interest "that rare and glorious middle-class Cabbage Patch-doll frenzy". The magazine said celebrities Matthew Perry, Steven Spielberg, and Chicago Bulls players called Nintendo to ask for special treatment to get their hands on the console. The console had only two launch games but "Super Mario 64" was its killer app, Hutchinson said:

During the system's first three days on the market, retailers sold 350,000 of 500,000 available console units. During its first four months, the console yielded 500,000 unit sales in North America. Nintendo successfully outsold Sony and Sega early in 1997 in the United States; and by the end of its first full year, 3.6 million units were sold in the U.S. "BusinessWire" reported that the Nintendo 64 was responsible for Nintendo's sales having increased by 156% by 1997.

After a strong launch year, the decision to use the cartridge format is said to have contributed to the diminished release pace and higher price of games compared to the competition, and thus Nintendo was unable to maintain its lead in the United States. The console would continue to outsell the Sega Saturn throughout the generation, but would trail behind the PlayStation.

Nintendo's efforts to attain dominance in the key 1997 holiday shopping season were also hurt by game delays. Five high-profile Nintendo games slated for release by Christmas 1997 ("", "Banjo-Kazooie", "Conker's Quest", "Yoshi's Story", and "Major League Baseball Featuring Ken Griffey Jr.") were delayed until 1998, and "Diddy Kong Racing" was announced at the last minute in an effort to somewhat fill the gaps.

In Japan, the console was not as successful, failing to outsell the PlayStation and even the Sega Saturn. Benimaru Itō, a developer for "Mother 3" and friend of Shigeru Miyamoto, speculated in 1997 that the Nintendo 64's lower popularity in Japan was due to the lack of role-playing video games.

Nintendo reported that the system's vintage hardware and software sales had ceased by 2004, three years after the GameCube's launch; as of December 31, 2009, the Nintendo 64 had yielded a lifetime total of 5.54 million system units sold in Japan, 20.63 million in the Americas, and 6.75 million in other regions, for a total of 32.93 million units.

The Nintendo 64 remains one of the most recognized video game systems in history and its games still have impact on the games industry. Designed in tandem with the controller, "Super Mario 64" and "" are widely considered by critics and the public to be two of the greatest and most influential games of all time. "GoldenEye 007" is one of the most influential games for the shooter genre.

The Aleck 64 is a Nintendo 64 design in arcade form, designed by Seta in cooperation with Nintendo, and sold from 1998 to 2003 only in Japan.




</doc>
<doc id="21850" url="https://en.wikipedia.org/wiki?curid=21850" title="GNU nano">
GNU nano

GNU nano is a text editor for Unix-like computing systems or operating environments using a command line interface. It emulates the Pico text editor, part of the Pine email client, and also provides additional functionality.
Unlike Pico, nano is licensed under the GNU General Public License (GPL). Released as free software by Chris Allegretta in 1999, nano became part of the GNU Project in 2001.

GNU nano was first created in 1999 with the name "TIP" ("TIP Isn't Pico"), by Chris Allegretta. His motivation was to create a free software replacement for Pico, which was not distributed under a free software license. The name was changed to nano on 10 January 2000 to avoid a naming conflict with the existing Unix utility "tip". The name comes from the system of SI prefixes, in which nano is 1000 times larger than pico. In February 2001, nano became a part of the GNU Project.

GNU nano implements several features that Pico lacks, including syntax highlighting, line numbers, regular expression search and replace, line-by-line scrolling, multiple buffers, indenting groups of lines, rebindable key support, and the undoing and redoing of edit changes.

On 11 August 2003, Chris Allegretta officially handed the source code maintenance of nano to David Lawrence Ramsey. On 20 December 2007, Ramsey stepped down as nano's maintainer.

On version 2.6.0 in June 2016, the current principal developer and the other active members of the nano project decided in consensus to leave the GNU project, because of their objections over the Free Software Foundation's copyright assignment policy, and their belief that decentralized copyright ownership does not impede the ability to enforce the GNU General Public License. The step was acknowledged by Debian and Arch Linux, while the GNU project resisted the move and called it a "fork". On 19 August 2016, Chris Allegretta announced the return of the project to the GNU family, following concessions from GNU on copyright assignment for Nano specifically, which happened when version 2.7.0 was released in September 2016.

GNU nano, like Pico, is keyboard-oriented, controlled with control keys. For example, saves the current file; goes to the search menu. GNU nano puts a two-line "shortcut bar" at the bottom of the screen, listing many of the commands available in the current context. For a complete list, gets the help screen.

Unlike Pico, nano uses meta keys to toggle its behavior. For example, toggles smooth scrolling mode on and off. Almost all features that can be selected from the command line can be dynamically toggled. On keyboards without the meta key it is often mapped to the escape key, , such that in order to simulate, say, one has to press the key, then release it, and then press the key.

GNU nano can also use pointer devices, such as a mouse, to activate functions that are on the shortcut bar, as well as position the cursor.



</doc>
<doc id="21851" url="https://en.wikipedia.org/wiki?curid=21851" title="Nieuwe Waterweg">
Nieuwe Waterweg

The Nieuwe Waterweg ("New Waterway") is a ship canal in the Netherlands from het Scheur (a branch of the Rhine-Meuse-Scheldt delta) west of the town of Maassluis to the North Sea at Hook of Holland: the Maasmond, where the Nieuwe Waterweg connects to the Maasgeul. It is the artificial mouth of the river Rhine.

The Nieuwe Waterweg, which opened in 1872 and has a length of approximately , was constructed to keep the city and port of Rotterdam accessible to seafaring vessels as the natural Meuse-Rhine branches silted up. The Waterway is a busy shipping route since it is the primary access to one of the busiest ports in the world, the Port of Rotterdam. At the entrance to the sea, a flood protection system called Maeslantkering has been installed (completed in 1997). There are no bridges or tunnels across the Nieuwe Waterweg.

By the middle of the 19th century, Rotterdam was already one of the largest port cities in the world, mainly because of transshipment of goods from Germany to Great Britain. The increase in shipping traffic created a capacity problem: there were too many branches in the river delta, making the port difficult to reach.

In 1863, a law was passed that allowed for the provision of a new canal for large ocean-going ships from Rotterdam to the North Sea. Hydraulic engineer Pieter Caland was commissioned to design a canal cutting through the "Hook of Holland” and to extend the Mouth of Rhine to the sea. The designs for this were already done back in 1731 by Nicolaas Samuelsz Cruquius but the implementation could no longer be postponed to prevent the decline of the harbour of Rotterdam.

Construction began on 31 October 1863. The first phase consisted of the expropriation of farm lands from Rozenburg to Hoek van Holland.

During the second phase two dikes were built parallel to each other, which took 2 years. Caland proposed to extend the dikes 2 km into the sea to disrupt the coastal sea currents and decrease silt deposits in the shipping lane.

Upon the completion of the dikes, the third phase began by the digging of the actual waterway. This began on 31 October 1866 and was completed three years later. The large amounts of removed soil were in turn used to reinforce other dams and dikes.

The last phase consisted of the removal of the dam separating the new waterway from the sea and river. In 1872, the Nieuwe Waterweg was completed and Rotterdam was easily accessible.

Because of the currents and erosion, the shipping lane has been widened somewhat. Yet because of the draft of today's supertankers, it needs to be dredged constantly.

In 1997, the last part of the Delta Works, the Maeslantkering, was put in operation near the mouth of the Nieuwe Waterweg. This storm surge barrier protects Rotterdam against north westerly Beaufort Force 10 to 12 storms.

The Nieuwe Waterweg gives the Port of Rotterdam its deep-water access to the North Sea. From Hook of Holland it stretches for approximately where the waterway continues as the Nieuwe Maas. The very first Nieuwe Waterweg—a breach through the dunes at Hook of Holland—was only long, but in around 1877 the channel was made much larger and wider and the current Nieuwe Waterweg was created. Currently the width of the channel is between and it is dredged to a depth of below Amsterdam Ordnance Datum.

It is this channel, together with the dredged channels in the North Sea, Maasgeul and Eurogeul, that allows ships like the MS "Berge Stahl" and MV "Vale Rio de Janeiro" (both with a draught of 23 meters) to enter Europoort.

The Dutch government agency Rijkswaterstaat is responsible for maintaining the channel.

The point where the Nieuwe Waterweg enters into the North Sea, between Hook of Holland on the north bank and the Maasvlakte to the south, is called the Maasmond. It is marked with two navigation light-towers called the Paddestoelen ("mushrooms"). The Nieuwe Waterweg connects, in the North Sea, to the Maasgeul. This dredged channel in the North Sea is being widened to to facilitate the largest container vessels for the new Maasvlakte 2 that opened in 2013.


</doc>
<doc id="21853" url="https://en.wikipedia.org/wiki?curid=21853" title="Neijia">
Neijia

Neijia (内家) is a term in Chinese martial arts, grouping those styles that practice "neijing", usually translated as internal martial arts, occupied with spiritual, mental or qi-related aspects, as opposed to an "external" approach focused on physiological aspects. The distinction dates to the 17th century, but its modern application is due to publications by Sun Lutang, dating to the period of 1915 to 1928. Neijing is developed by using "neigong", or "internal exercises," as opposed to "external exercises" (wàigōng 外功),

Wudangquan is a more specific grouping of internal martial arts named for their association with the Taoist monasteries of the Wudang Mountains, Hubei in Chinese popular legend. These styles were enumerated by Sun Lutang as Taijiquan, Xingyiquan and Baguazhang, but most also include Bajiquan and the legendary Wudang Sword.

Some other Chinese arts, not in the Wudangquan group, such as Qigong, Liuhebafa, Bak Mei Pai, Zi Ran Men (Nature Boxing), Bok Foo Pai and Yiquan are frequently classified (or classify themselves) as "internal".

The term "neijia" and the distinction between internal and external martial arts first appears in Huang Zongxi's 1669 "Epitaph for Wang Zhengnan". Stanley Henning proposes that the "Epitaph"'s identification of the internal martial arts with the Taoism indigenous to China and of the external martial arts with the foreign Buddhism of Shaolin—and the Manchu Qing Dynasty to which Huang Zongxi was opposed—was an act of political defiance rather than one of technical classification.

In 1676 Huang Zongxi's son, Huang Baijia, who learned martial arts from Wang Zhengnan, compiled the earliest extant manual of internal martial arts, the "Nèijiā quánfǎ".

Beginning in 1914, Sun Lutang together with Yang Shao-hou, Yang Chengfu and Wu Chien-ch'uan taught t'ai chi to the public at the Beijing Physical Education Research Institute. Sun taught there until 1928, a seminal period in the development of modern Yang, Wu and Sun-style tai ji quan. Sun Lutang from 1915 also published martial arts texts.

In 1928, Kuomintang generals Li Jing Lin, Zhang Zi Jiang, and Fung Zu Ziang organized a national martial arts tournament in China; they did so to screen the best martial artists in order to begin building the Central Martial Arts Academy (Zhongyang Guoshuguan). The generals separated the participants of the tournament into Shaolin and Wudang. Wudang participants were recognized as having "internal" skills. These participants were generally practitioners of t'ai chi ch'uan, Xingyiquan and Baguazhang. All other participants competed under the classification of Shaolin. One of the winners in the "internal" category was the Baguazhang master Fu Chen Sung.

Sun Lutang identified the following as the criteria that distinguish an internal martial art:

Sun Lutang's eponymous style of t'ai chi ch'uan fuses principles from all three arts he named as neijia. Similarities applying classical principles between taiji, xingyi, and baquazhang include: Loosening (song) the soft tissue, opening shoulder and hip gates or gua, cultivating qi or intrinsic energy, issuing various jin or compounded energies. Taijiquan is characterized by an ever-present peng jin or expanding energy. Xingyiquan is characterized by its solely forward moving pressing ji jin energy. Baguazhang is characterized by its “dragon body” circular movements. Some Chinese martial arts other than the ones Sun named also teach what are termed internal practices, despite being generally classified as external (e.g. Wing Chun that also is internal ). Some non-Chinese martial arts also claim to be internal, for example Aikido and Kito Ryu. Many martial artists, especially outside of China, disregard the distinction entirely. Some neijia schools refer to their arts as "soft style" martial arts.

Internal styles focus on awareness of the spirit, mind, qi ("energy") and the use of relaxed ("" ) leverage rather than muscular tension. Pushing hands is a training method commonly used in neijia arts to develop sensitivity and softness.

Much time may nevertheless be spent on basic physical training, such as stance training ("zhan zhuang"), stretching and strengthening of muscles, as well as on empty hand and weapon forms which can be quite demanding.

Some forms in internal styles are performed slowly, although some include sudden outbursts of explosive movements (fa jin), such as those the Chen style of Taijiquan is famous for teaching earlier than some other styles (e.g. Yang and Wu). The reason for the generally slow pace is to improve coordination and balance by increasing the work load, and to require the student to pay minute attention to their whole body and its weight as they perform a technique. At an advanced level, and in actual fighting, internal styles are performed quickly, but the goal is to learn to involve the entire body in every motion, to stay relaxed, with deep, controlled breathing, and to coordinate the motions of the body and the breathing accurately according to the dictates of the forms while maintaining perfect balance.

The reason for the label "internal," according to most schools, is that there is a focus on the internal aspects earlier in the training, once these internal relationships are apprehended (the theory goes) they are then applied to the external applications of the styles in question.

External styles are characterized by fast and explosive movements and a focus on physical strength and agility. External styles include both the traditional styles focusing on application and fighting, as well as the modern styles adapted for competition and exercise. Examples of external styles are Shaolinquan, with its direct explosive attacks and many Wushu forms that have spectacular aerial techniques. External styles begin with a training focus on muscular power, speed and application, and generally integrate their qigong aspects in advanced training, after their desired "hard" physical level has been reached.

Some say that there is no differentiation between the so-called internal and external systems of the Chinese martial arts, while other well known teachers have expressed differing opinions. For example, the Taijiquan teacher Wu Jianquan:

Those who practice Shaolinquan leap about with strength and force; people not proficient at this kind of training soon lose their breath and are exhausted. Taijiquan is unlike this. Strive for quiescence of body, mind and intention.

Many internal schools teach forms that are practised for health benefits only. Thus, T'ai chi ch'uan in spite of its roots in martial arts has become similar in scope to Qigong, the purely meditative practice based on notions of circulation of qi. With purely a health emphasis, T'ai chi classes have become popular in hospitals, clinics, community and senior centers in the last twenty years or so, as baby boomers age and the art's reputation as a low stress training for seniors became better known.

Traditionalists feel that a school not teaching martial aspects somewhere in their syllabus cannot be said to be actually teaching the art itself, that they have accredited themselves prematurely. Traditional teachers also believe that understanding the core theoretical principles of neijia and the ability to apply them are a necessary gateway to health benefits.

Internal styles have been associated in legend and in much popular fiction with the Taoist monasteries of the Wudang Mountains in central China.

Neijia are a common theme in Chinese Wuxia novels and films, and are usually represented as originating in Wudang or similar mythologies. Often, genuine internal practices are highly exaggerated to the point of making them seem miraculous, as in the novels of Jin Yong and Gu Long. Internal concepts have also been a source of comedy, such as in the films "Shaolin Soccer" and "Kung Fu Hustle".

In the Naruto series, Neji Hyūga's name and techniques were based on neijia.





</doc>
<doc id="21854" url="https://en.wikipedia.org/wiki?curid=21854" title="Navigation">
Navigation

Navigation is a field of study that focuses on the process of monitoring and controlling the movement of a craft or vehicle from one place to another. The field of navigation includes four general categories: land navigation, marine navigation, aeronautic navigation, and space navigation.

It is also the term of art used for the specialized knowledge used by navigators to perform navigation tasks. All navigational techniques involve locating the navigator's position compared to known locations or patterns.

Navigation, in a broader sense, can refer to any skill or study that involves the determination of position and direction. In this sense, navigation includes orienteering and pedestrian navigation.

In the European medieval period, navigation was considered part of the set of "seven mechanical arts", none of which were used for long voyages across open ocean. Polynesian navigation is probably the earliest form of open-ocean navigation, it was based on memory and observation recorded on scientific instruments like the Marshall Islands Stick Charts of Ocean Swells. Early Pacific Polynesians used the motion of stars, weather, the position of certain wildlife species, or the size of waves to find the path from one island to another.

Maritime navigation using scientific instruments such as the mariner's astrolabe first occurred in the Mediterranean during the Middle Ages. Although land astrolabes were invented in the Hellenistic period and existed in classical antiquity and the Islamic Golden Age, the oldest record of a sea astrolabe is that of Majorcan astronomer Ramon Llull dating from 1295. The perfecting of this navigation instrument is attributed to Portuguese navigators during early Portuguese discoveries in the Age of Discovery. The earliest known description of how to make and use a sea astrolabe comes from Spanish cosmographer Martín Cortés de Albacar's "Arte de Navegar" ("The Art of Navigation") published in 1551, based on the principle of the archipendulum used in constructing the Egyptian pyramids.

Open-seas navigation using the astrolabe and the compass started during the Age of Discovery in the 15th century. The Portuguese began systematically exploring the Atlantic coast of Africa from 1418, under the sponsorship of Prince Henry. In 1488 Bartolomeu Dias reached the Indian Ocean by this route. In 1492 the Spanish monarchs funded Christopher Columbus's expedition to sail west to reach the Indies by crossing the Atlantic, which resulted in the Discovery of the Americas. In 1498, a Portuguese expedition commanded by Vasco da Gama reached India by sailing around Africa, opening up direct trade with Asia. Soon, the Portuguese sailed further eastward, to the Spice Islands in 1512, landing in China one year later.

The first circumnavigation of the earth was completed in 1522 with the Magellan-Elcano expedition, a Spanish voyage of discovery led by Portuguese explorer Ferdinand Magellan and completed by Spanish navigator Juan Sebastián Elcano after the former's death in the Philippines in 1521. The fleet of seven ships sailed from Sanlúcar de Barrameda in Southern Spain in 1519, crossed the Atlantic Ocean and after several stopovers rounded the southern tip of South America. Some ships were lost, but the remaining fleet continued across the Pacific making a number of discoveries including Guam and the Philippines. By then, only two galleons were left from the original seven. The "Victoria" led by Elcano sailed across the Indian Ocean and north along the coast of Africa, to finally arrive in Spain in 1522, three years after its departure. The "Trinidad" sailed east from the Philippines, trying to find a maritime path back to the Americas, but was unsuccessful. The eastward route across the Pacific, also known as the "tornaviaje" (return trip) was only discovered forty years later, when Spanish cosmographer Andrés de Urdaneta sailed from the Philippines, north to parallel 39°, and hit the eastward Kuroshio Current which took its galleon across the Pacific. He arrived in Acapulco on October 8, 1565.

The term stems from the 1530s, from Latin "navigationem" (nom. "navigatio"), from "navigatus", pp. of "navigare" "to sail, sail over, go by sea, steer a ship," from "navis" "ship" and the root of "agere" "to drive".

Roughly, the latitude of a place on Earth is its angular distance north or south of the equator. Latitude is usually expressed in degrees (marked with °) ranging from 0° at the Equator to 90° at the North and South poles. The latitude of the North Pole is 90° N, and the latitude of the South Pole is 90° S. Mariners calculated latitude in the Northern Hemisphere by sighting the North Star Polaris with a sextant and using sight reduction tables to correct for height of eye and atmospheric refraction. The height of Polaris in degrees above the horizon is the latitude of the observer, within a degree or so.

Similar to latitude, the longitude of a place on Earth is the angular distance east or west of the prime meridian or Greenwich meridian. Longitude is usually expressed in degrees (marked with °) ranging from 0° at the Greenwich meridian to 180° east and west. Sydney, for example, has a longitude of about 151° east. New York City has a longitude of 74° west. For most of history, mariners struggled to determine longitude. Longitude can be calculated if the precise time of a sighting is known. Lacking that, one can use a sextant to take a lunar distance (also called "the lunar observation", or "lunar" for short) that, with a nautical almanac, can be used to calculate the time at zero longitude (see Greenwich Mean Time). Reliable marine chronometers were unavailable until the late 18th century and not affordable until the 19th century. For about a hundred years, from about 1767 until about 1850, mariners lacking a chronometer used the method of lunar distances to determine Greenwich time to find their longitude. A mariner with a chronometer could check its reading using a lunar determination of Greenwich time.

In navigation, a rhumb line (or loxodrome) is a line crossing all meridians of longitude at the same angle, i.e. a path derived from a defined initial bearing. That is, upon taking an initial bearing, one proceeds along the same bearing, without changing the direction as measured relative to true or magnetic north.

Most modern navigation relies primarily on positions determined electronically by receivers collecting information from satellites. Most other modern techniques rely on crossing lines of position or LOP.

A line of position can refer to two different things, either a line on a chart or a line between the observer and an object in real life. A bearing is a measure of the direction to an object. If the navigator measures the direction in real life, the angle can then be drawn on a nautical chart and the navigator will be on that line on the chart.

In addition to bearings, navigators also often measure distances to objects. On the chart, a distance produces a circle or arc of position. Circles, arcs, and hyperbolae of positions are often referred to as lines of position.

If the navigator draws two lines of position, and they intersect he must be at that position. A fix is the intersection of two or more LOPs.

If only one line of position is available, this may be evaluated against the dead reckoning position to establish an estimated position.

Lines (or circles) of position can be derived from a variety of sources:

There are some methods seldom used today such as "dipping a light" to calculate the geographic range from observer to lighthouse

Methods of navigation have changed through history. Each new method has enhanced the mariner's ability to complete his voyage. One of the most important judgments the navigator must make is the best method to use. Some types of navigation are depicted in the table.

The practice of navigation usually involves a combination of these different methods.

By mental navigation checks, a pilot or a navigator estimates tracks, distances, and altitudes which will then help the pilot avoid gross navigation errors.

Piloting (also called pilotage) involves navigating an aircraft by visual reference to landmarks, or a water vessel in restricted waters and fixing its position as precisely as possible at frequent intervals. More so than in other phases of navigation, proper preparation and attention to detail are important. Procedures vary from vessel to vessel, and between military, commercial, and private vessels.

A military navigation team will nearly always consist of several people. A military navigator might have bearing takers stationed at the gyro repeaters on the bridge wings for taking simultaneous bearings, while the civilian navigator must often take and plot them himself. While the military navigator will have a bearing book and someone to record entries for each fix, the civilian navigator will simply pilot the bearings on the chart as they are taken and not record them at all.

If the ship is equipped with an ECDIS, it is reasonable for the navigator to simply monitor the progress of the ship along the chosen track, visually ensuring that the ship is proceeding as desired, checking the compass, sounder and other indicators only occasionally. If a pilot is aboard, as is often the case in the most restricted of waters, his judgement can generally be relied upon, further easing the workload. But should the ECDIS fail, the navigator will have to rely on his skill in the manual and time-tested procedures.

Celestial navigation systems are based on observation of the positions of the Sun, Moon, Planets and navigational stars. Such systems are in use as well for terrestrial navigating as for interstellar navigating. By knowing which point on the rotating earth a celestial object is above and measuring its height above the observer's horizon, the navigator can determine his distance from that subpoint. A nautical almanac and a marine chronometer are used to compute the subpoint on earth a celestial body is over, and a sextant is used to measure the body's angular height above the horizon. That height can then be used to compute distance from the subpoint to create a circular line of position. A navigator shoots a number of stars in succession to give a series of overlapping lines of position. Where they intersect is the celestial fix. The moon and sun may also be used. The sun can also be used by itself to shoot a succession of lines of position (best done around local noon) to determine a position.

In order to accurately measure longitude, the precise time of a sextant sighting (down to the second, if possible) must be recorded. Each second of error is equivalent to 15 seconds of longitude error, which at the equator is a position error of .25 of a nautical mile, about the accuracy limit of manual celestial navigation.

The spring-driven marine chronometer is a precision timepiece used aboard ship to provide accurate time for celestial observations. A chronometer differs from a spring-driven watch principally in that it contains a variable lever device to maintain even pressure on the mainspring, and a special balance designed to compensate for temperature variations.

A spring-driven chronometer is set approximately to Greenwich mean time (GMT) and is not reset until the instrument is overhauled and cleaned, usually at three-year intervals. The difference between GMT and chronometer time is carefully determined and applied as a correction to all chronometer readings. Spring-driven chronometers must be wound at about the same time each day.

Quartz crystal marine chronometers have replaced spring-driven chronometers aboard many ships because of their greater accuracy. They are maintained on GMT directly from radio time signals. This eliminates chronometer error and watch error corrections. Should the second hand be in error by a readable amount, it can be reset electrically.

The basic element for time generation is a quartz crystal oscillator. The quartz crystal is temperature compensated and is hermetically sealed in an evacuated envelope. A calibrated adjustment capability is provided to adjust for the aging of the crystal.

The chronometer is designed to operate for a minimum of 1 year on a single set of batteries. Observations may be timed and ship's clocks set with a comparing watch, which is set to chronometer time and taken to the bridge wing for recording sight times. In practice, a wrist watch coordinated to the nearest second with the chronometer will be adequate.

A stop watch, either spring wound or digital, may also be used for celestial observations. In this case, the watch is started at a known GMT by chronometer, and the elapsed time of each sight added to this to obtain GMT of the sight.

All chronometers and watches should be checked regularly with a radio time signal. Times and frequencies of radio time signals are listed in publications such as Radio Navigational Aids.

The second critical component of celestial navigation is to measure the angle formed at the observer's eye between the celestial body and the sensible horizon. The sextant, an optical instrument, is used to perform this function. The sextant consists of two primary assemblies. The frame is a rigid triangular structure with a pivot at the top and a graduated segment of a circle, referred to as the "arc", at the bottom. The second component is the index arm, which is attached to the pivot at the top of the frame. At the bottom is an endless vernier which clamps into teeth on the bottom of the "arc". The optical system consists of two mirrors and, generally, a low power telescope. One mirror, referred to as the "index mirror" is fixed to the top of the index arm, over the pivot. As the index arm is moved, this mirror rotates, and the graduated scale on the arc indicates the measured angle ("altitude").

The second mirror, referred to as the "horizon glass", is fixed to the front of the frame. One half of the horizon glass is silvered and the other half is clear. Light from the celestial body strikes the index mirror and is reflected to the silvered portion of the horizon glass, then back to the observer's eye through the telescope. The observer manipulates the index arm so the reflected image of the body in the horizon glass is just resting on the visual horizon, seen through the clear side of the horizon glass.

Adjustment of the sextant consists of checking and aligning all the optical elements to eliminate "index correction". Index correction should be checked, using the horizon or more preferably a star, each time the sextant is used. The practice of taking celestial observations from the deck of a rolling ship, often through cloud cover and with a hazy horizon, is by far the most challenging part of celestial navigation.

Inertial navigation system (INS) is a dead reckoning type of navigation system that computes its position based on motion sensors. Before actually navigating, the initial latitude and longitude and the INS's physical orientation relative to the earth (e.g., north and level) are established. After alignment, an INS receives impulses from motion detectors that measure (a) the acceleration along three axes (accelerometers), and (b) rate of rotation about three orthogonal axes (gyroscopes). These enable an INS to continually and accurately calculate its current latitude and longitude (and often velocity). 

Advantages over other navigation systems are that, once aligned, an INS does not require outside information. An INS is not affected by adverse weather conditions and it cannot be detected or jammed. Its disadvantage is that since the current position is calculated solely from previous positions and motion sensors, its errors are cumulative, increasing at a rate roughly proportional to the time since the initial position was input. Inertial navigation systems must therefore be frequently corrected with a location 'fix' from some other type of navigation system. 

The first inertial system is considered to be the V-2 guidance system deployed by the Germans in 1942. However, inertial sensors are traced to the early 19th century. The advantages INSs led their use in aircraft, missiles, surface ships and submarines. For example, the U.S. Navy developed the Ships Inertial Navigation System (SINS) during the Polaris missile program to ensure a reliable and accurate navigation system to initial its missile guidance systems. Inertial navigation systems were in wide use until satellite navigation systems (GPS) became available. INSs are still in common use on submarines (since GPS reception or other fix sources are not possible while submerged) and long-range missiles.

A radio direction finder or RDF is a device for finding the direction to a radio source. Due to radio's ability to travel very long distances "over the horizon", it makes a particularly good navigation system for ships and aircraft that might be flying at a distance from land.

RDFs works by rotating a directional antenna and listening for the direction in which the signal from a known station comes through most strongly. This sort of system was widely used in the 1930s and 1940s. RDF antennas are easy to spot on German World War II aircraft, as loops under the rear section of the fuselage, whereas most US aircraft enclosed the antenna in a small teardrop-shaped fairing.

In navigational applications, RDF signals are provided in the form of "radio beacons", the radio version of a lighthouse. The signal is typically a simple AM broadcast of a morse code series of letters, which the RDF can tune in to see if the beacon is "on the air". Most modern detectors can also tune in any commercial radio stations, which is particularly useful due to their high power and location near major cities.

Decca, OMEGA, and LORAN-C are three similar hyperbolic navigation systems. Decca was a hyperbolic low frequency radio navigation system (also known as multilateration) that was first deployed during World War II when the Allied forces needed a system which could be used to achieve accurate landings. As was the case with Loran C, its primary use was for ship navigation in coastal waters. Fishing vessels were major post-war users, but it was also used on aircraft, including a very early (1949) application of moving-map displays. The system was deployed in the North Sea and was used by helicopters operating to oil platforms.

The OMEGA Navigation System was the first truly global radio navigation system for aircraft, operated by the United States in cooperation with six partner nations. OMEGA was developed by the United States Navy for military aviation users. It was approved for development in 1968 and promised a true worldwide oceanic coverage capability with only eight transmitters and the ability to achieve a four-mile (6 km) accuracy when fixing a position. Initially, the system was to be used for navigating nuclear bombers across the North Pole to Russia. Later, it was found useful for submarines. Due to the success of the Global Positioning System the use of Omega declined during the 1990s, to a point where the cost of operating Omega could no longer be justified. Omega was terminated on September 30, 1997 and all stations ceased operation.

LORAN is a terrestrial navigation system using low frequency radio transmitters that use the time interval between radio signals received from three or more stations to determine the position of a ship or aircraft. The current version of LORAN in common use is LORAN-C, which operates in the low frequency portion of the EM spectrum from 90 to 110 kHz. Many nations are users of the system, including the United States, Japan, and several European countries. Russia uses a nearly exact system in the same frequency range, called CHAYKA. LORAN use is in steep decline, with GPS being the primary replacement. However, there are attempts to enhance and re-popularize LORAN. LORAN signals are less susceptible to interference and can penetrate better into foliage and buildings than GPS signals.

When a vessel is within radar range of land or special radar aids to navigation, the navigator can take distances and angular bearings to charted objects and use these to establish arcs of position and lines of position on a chart. A fix consisting of only radar information is called a radar fix.

Types of radar fixes include "range and bearing to a single object," "two or more bearings," "tangent bearings," and "two or more ranges."

Parallel indexing is a technique defined by William Burger in the 1957 book "The Radar Observer's Handbook". This technique involves creating a line on the screen that is parallel to the ship's course, but offset to the left or right by some distance. This parallel line allows the navigator to maintain a given distance away from hazards.

Some techniques have been developed for special situations. One, known as the "contour method," involves marking a transparent plastic template on the radar screen and moving it to the chart to fix a position.

Another special technique, known as the Franklin Continuous Radar Plot Technique, involves drawing the path a radar object should follow on the radar display if the ship stays on its planned course. During the transit, the navigator can check that the ship is on track by checking that the pip lies on the drawn line.

Global Navigation Satellite System or GNSS is the term for satellite navigation systems that provide positioning with global coverage. A GNSS allow small electronic receivers to determine their location (longitude, latitude, and altitude) to within a few metres using time signals transmitted along a line of sight by radio from satellites. Receivers on the ground with a fixed position can also be used to calculate the precise time as a reference for scientific experiments.

As of October 2011, only the United States NAVSTAR Global Positioning System (GPS) and the Russian GLONASS are fully globally operational GNSSs. The European Union's Galileo positioning system is a next generation GNSS in the final deployment phase, and became operational in 2016. China has indicated it may expand its regional Beidou navigation system into a global system.

More than two dozen GPS satellites are in medium Earth orbit, transmitting signals allowing GPS receivers to determine the receiver's location, speed and direction.

Since the first experimental satellite was launched in 1978, GPS has become an indispensable aid to navigation around the world, and an important tool for map-making and land surveying. GPS also provides a precise time reference used in many applications including scientific study of earthquakes, and synchronization of telecommunications networks.

Developed by the United States Department of Defense, GPS is officially named NAVSTAR GPS (NAVigation Satellite Timing And Ranging Global Positioning System). The satellite constellation is managed by the United States Air Force 50th Space Wing. The cost of maintaining the system is approximately US$750 million per year, including the replacement of aging satellites, and research and development. Despite this fact, GPS is free for civilian use as a public good.

Modern smartphones act as personal GPS navigators for civilians who own them. Overuse of these devices, whether in the vehicle or on foot, can lead to a relative inability to learn about navigated environments, resulting in sub-optimal navigation abilities when and if these devices become unavailable . Typically a compass is also provided to determine direction when not moving.

The day's work in navigation is a minimal set of tasks consistent with prudent navigation. The definition will vary on military and civilian vessels, and from ship to ship, but the traditional method takes a form resembling:

Navigation on ships is usually always conducted on the Bridge. It may also take place in adjacent space, where chart tables and publications are available.

Passage planning or voyage planning is a procedure to develop a complete description of vessel's voyage from start to finish. The plan includes leaving the dock and harbor area, the en route portion of a voyage, approaching the destination, and mooring. According to international law, a vessel's captain is legally responsible for passage planning, however on larger vessels, the task will be delegated to the ship's navigator.

Studies show that human error is a factor in 80 percent of navigational accidents and that in many cases the human making the error had access to information that could have prevented the accident. The practice of voyage planning has evolved from penciling lines on nautical charts to a process of risk management.

Passage planning consists of four stages: appraisal, planning, execution, and monitoring, which are specified in "International Maritime Organization Resolution A.893(21), Guidelines For Voyage Planning," and these guidelines are reflected in the local laws of IMO signatory countries (for example, Title 33 of the U.S. Code of Federal Regulations), and a number of professional books or publications. There are some fifty elements of a comprehensive passage plan depending on the size and type of vessel.

The appraisal stage deals with the collection of information relevant to the proposed voyage as well as ascertaining risks and assessing the key features of the voyage. This will involve considering the type of navigation required e.g. Ice navigation, the region the ship will be passing through and the hydrographic information on the route. In the next stage, the written plan is created. The third stage is the execution of the finalised voyage plan, taking into account any special circumstances which may arise such as changes in the weather, which may require the plan to be reviewed or altered. The final stage of passage planning consists of monitoring the vessel's progress in relation to the plan and responding to deviations and unforeseen circumstances.

Electronic integrated bridge concepts are driving future navigation system planning. Integrated systems take inputs from various ship sensors, electronically display positioning information, and provide control signals required to maintain a vessel on a preset course. The navigator becomes a system manager, choosing system presets, interpreting system output, and monitoring vessel response.

Navigation for cars and other land-based travel typically uses maps, landmarks, and in recent times computer navigation ("satnav", short for satellite navigation), as well as any means available on water.

Computerized navigation commonly relies on GPS for current location information, a navigational map database of roads and navigable routes, and uses algorithms related to the shortest path problem to identify optimal routes.

Professional standards for navigation depend on the type of navigation and vary by country. For marine navigation, Merchant Navy deck officers are trained and internationally certified according to the STCW Convention. Leisure and amateur mariners may undertake lessons in navigation at local/regional training schools. Naval officers receive navigation training as part of their naval training. 

In land navigation, courses and training is often provided to young persons as part of general or extra-curricular education. Land navigation is also an essential part of army training. Additionally, organisations such as the Scouts and DoE programme teach navigation to their students. Orienteering organisations are a type of sports that require navigational skills using a map and compass to navigate from point to point in diverse and usually unfamiliar terrain whilst moving at speed.

In aviation, pilots undertake air navigation training as part of learning to fly. 

Professional organisations also assist to encourage improvements in navigation or bring together navigators in learned environments. The Royal Institute of Navigation (RIN) is a learned society with charitable status, aimed at furthering the development of navigation on land and sea, in the air and in space. It was founded in 1947 as a forum for mariners, pilots, engineers and academics to compare their experiences and exchange information. In the US, the Institute of Navigation (ION) is a non-profit professional organisation advancing the art and science of positioning, navigation and timing.

Numerous nautical publications are available on navigation, which are published by professional sources all over the world. In the UK, the United Kingdom Hydrographic Office, the Witherby Publishing Group and the Nautical Institute provide numerous navigational publications, including the comprehensive Admiralty Manual of Navigation.

In the US, Bowditch's American Practical Navigator is a free available encyclopedia of navigation issued by the US Government.




</doc>
<doc id="21857" url="https://en.wikipedia.org/wiki?curid=21857" title="Nonfiction">
Nonfiction

Nonfiction or non-fiction is any document or content that purports in good faith to represent truth and accuracy regarding information, events, or people. Nonfiction content may be presented either objectively or subjectively, and may sometimes take the form of a story. Nonfiction is one of the fundamental divisions of narrative (specifically, prose) writing— in contrast to fiction, which offers information, events, or characters expected to be partly or largely imaginary, or else leaves open if and how the work refers to reality.

Nonfiction's specific factual assertions and descriptions may or may not be accurate, and can give either a true or a false account of the subject in question. However, authors of such accounts genuinely believe or claim them to be truthful at the time of their composition or, at least, pose them to a convinced audience as historically or empirically factual. Reporting the beliefs of others in a nonfiction format is not necessarily an endorsement of the veracity of those beliefs, but rather an exercise in representing the topic. Works of nonfiction need not necessarily be written text, since statements expressed by pictures or film can also purport to present a factual account of a subject.

The numerous literary and creative devices used within fiction are generally thought inappropriate for use in nonfiction. They are still present particularly in older works but they are often muted so as not to overshadow the information within the work. Simplicity, clarity and directness are some of the most important considerations when producing nonfiction. Audience is important in any artistic or descriptive endeavor, but it is perhaps most important in nonfiction. In fiction, the writer believes that readers will make an effort to follow and interpret an indirectly or abstractly presented progression of theme, whereas the production of nonfiction has more to do with the direct provision of information. Understanding of the potential readers' use for the work and their existing knowledge of a subject are both fundamental for effective nonfiction. Despite the claim to truth of nonfiction, it is often necessary to persuade the reader to agree with the ideas and so a balanced, coherent and informed argument is vital. However, the boundaries between fiction and nonfiction are continually blurred and argued upon, especially in the field of biography; as Virginia Woolf said: "if we think of truth as something of granite-like solidity and of personality as something of rainbow-like intangibility and reflect that the aim of biography is to weld these two into one seamless whole, we shall admit that the problem is a stiff one and that we need not wonder if biographers, for the most part failed to solve it."

Semi-fiction is fiction implementing a great deal of nonfiction, e.g. a fictional description based on a true story.

Common literary examples of nonfiction include expository, argumentative, functional, and opinion pieces; essays on art or literature; biographies; memoirs; journalism; and historical, scientific, technical, or economic writings (including electronic ones).

Journals, photographs, textbooks, travel books, blueprints, and diagrams are also often considered nonfictional. Including information that the author knows to be untrue within any of these works is usually regarded as dishonest. Other works can legitimately be either fiction or nonfiction, such as journals of self-expression, letters, magazine articles, and other expressions of imagination. Though such works are mostly either or the other, a blend of both is also possible. Some fiction may include nonfictional elements. Some nonfiction may include elements of unverified supposition, deduction, or imagination for the purpose of smoothing out a narrative, but the inclusion of open falsehoods would discredit it as a work of nonfiction. The publishing and bookselling business sometimes uses the phrase "literary nonfiction" to distinguish works with a more literary or intellectual bent, as opposed to the greater collection of nonfiction subjects.




</doc>
<doc id="21861" url="https://en.wikipedia.org/wiki?curid=21861" title="Cryptonomicon">
Cryptonomicon

Cryptonomicon is a 1999 novel by American author Neal Stephenson, set in two different time periods. One group of characters are World War II-era Allied codebreakers and tactical-deception operatives affiliated with the Government Code and Cypher School at Bletchley Park (UK), and disillusioned Axis military and intelligence figures. The second narrative is set in the late 1990s, with characters that are (in part) descendants of those of the earlier time period, who employ cryptologic, telecom, and computer technology to build an underground data haven in the fictional Sultanate of Kinakuta. Their goal is to facilitate anonymous Internet banking using electronic money and (later) digital gold currency, with a long-term objective to distribute Holocaust Education and Avoidance Pod (HEAP) media for instructing genocide-target populations on defensive warfare.

"Cryptonomicon" is closer to the genres of historical fiction and contemporary techno-thriller than to the science fiction of Stephenson's two previous novels, "Snow Crash" and "The Diamond Age". It features fictionalized characterizations of such historical figures as Alan Turing, Albert Einstein, Douglas MacArthur, Winston Churchill, Isoroku Yamamoto, Karl Dönitz, Hermann Göring, and Ronald Reagan, as well as some highly technical and detailed descriptions of modern cryptography and information security, with discussions of prime numbers, modular arithmetic, and Van Eck phreaking.

According to Stephenson:
The title is a play on "Necronomicon", the title of a book mentioned in the stories of horror writer H. P. Lovecraft:
The novel's Cryptonomicon, described as a "cryptographer's bible", is a fictional book summarizing America's knowledge of cryptography and cryptanalysis. Begun by John Wilkins (the Cryptonomicon is mentioned in "Quicksilver") and amended over time by William Friedman, Lawrence Waterhouse, and others, the Cryptonomicon is described by Katherine Hayles as "a kind of Kabala created by a Brotherhood of Code that stretches across centuries. To know its contents is to qualify as a Morlock among the Eloi, and the elite among the elite are those gifted enough actually to contribute to it."

The action takes place in two periods—World War II and the late 1990s, during the Internet boom and Asian financial crisis.

In 1942, Lawrence Pritchard Waterhouse, a young United States Navy code breaker and mathematical genius, is assigned to the newly formed joint British and American Detachment 2702. This ultra-secret unit's role is to hide the fact that Allied intelligence has cracked the German Enigma code. The detachment stages events, often behind enemy lines, that provide alternative explanations for the Allied intelligence successes. United States Marine sergeant Bobby Shaftoe, a veteran of China and Guadalcanal, serves in unit 2702, carrying out Waterhouse's plans. At the same time, Japanese soldiers, including mining engineer Goto Dengo, a "friendly enemy" of Shaftoe's, are assigned to build a mysterious bunker in the mountains in the Philippines as part of what turns out to be a literal suicide mission.

Circa 1997, Randy Waterhouse (Lawrence's grandson) joins his old role-playing game companion Avi Halaby in a new startup, providing Pinoy-grams (inexpensive, non-real-time video messages) to migrant Filipinos via new fiber-optic cables. The Epiphyte Corporation uses this income stream to fund the creation of a data haven in the nearby fictional Sultanate of Kinakuta. Vietnam veteran Doug Shaftoe, the son of Bobby Shaftoe, and his daughter Amy, do the undersea surveying for the cables and engineering work on the haven, which is overseen by Goto Furudenendu, heir-apparent to Goto Engineering. Complications arise as figures from the past reappear seeking gold or revenge.


Fictionalized versions of several historical figures appear in the World War II storyline:

The precise date of this storyline is not established, but the ages of characters, the technologies described, and certain date-specific references suggest that it is set in the late 1990s, at the time of the internet boom and the Asian financial crisis.



Portions of "Cryptonomicon" are notably complex. Several pages are spent explaining in detail some of the concepts behind cryptography and data storage security, including a description of Van Eck phreaking.

Stephenson also includes a precise description of (and even Perl script for) the Solitaire (or Pontifex) cipher, a cryptographic algorithm developed by Bruce Schneier for use with a deck of playing cards, as part of the plot. The perl script was written by well known cryptographer and cypherpunk, Ian Goldberg.
$f=$d?-1:1;$D=pack('C*',33..86);$p=shift;
$p=~y/a-z/A-Z/;$U='$D=~s/(.*)U$/U$1/;
$D=~s/U(.)/$1U/;';($V=$U)=~s/U/V/g;
$p=~s/[A-Z]/$k=ord($&)-64,&e/eg;$k=0;
while(<>){y/a-z/A-Z/;y/A-Z//dc;$o.=$_}$o.='X'
while length ($o)%5&&!$d;
$o=~s/./chr(($f*&e+ord($&)-l3)%26+65)/eg;
$o=~s/X*$// if $d;$o=~s/.{5}/$& /g;
print"$o\n";sub v{$v=ord(substr($D,$_[0]))-32;
sub e{eval"$U$V$V";$D=~s/(.*)([UV].*[UV])(.*)/$3$2$l/;
Since the original printing of the script, Stephenson has made several changes. The first was to remediate a typesetting error on the eighth line that caused the perl script to be useless. The second change was to add semi-colons as line breaks to facilitate readers without fluency in Perl in transcribing and running the script themselves.

A verbose and annotated version of the script appears on Bruce Schneier's web site.

Several of the characters in the book communicate with each other through the use of One-time pads. A one-time pad (OTP) is an encryption technique that requires a single-use pre-shared key of at least the same length as the encrypted message.

The story posits a variation of the OTP technique wherein there is no pre-shared key - the key is instead generated algorithmically.

He also describes computers using a fictional operating system, Finux. The name is a thinly veiled reference to Linux, a kernel originally written by the Finnish native Linus Torvalds. Stephenson changed the name so as not to be creatively constrained by the technical details of Linux-based operating systems.


An excerpt from "Cryptonomicon" was originally published in the short story collection "Disco 2000", edited by Sarah Champion and published in 1998.

Stephenson's subsequent work, a trio of novels dubbed "The Baroque Cycle", provides part of the deep backstory to the characters and events featured in "Cryptonomicon". Set in the late 17th and early 18th centuries, the novels feature ancestors of several characters in "Cryptonomicon", as well as events and objects which affect the action of the later-set book. The subtext implies the existence of secret societies or conspiracies, and familial tendencies and groupings found within those darker worlds.

The short story "Jipi and the Paranoid Chip" appears to take place some time after the events of "Cryptonomicon". In the story, the construction of the Crypt has triggered economic growth in Manila and Kinakuta, in which Goto Engineering, and Homa/Homer Goto, a Goto family heir, are involved. The IDTRO ("Black Chamber") is also mentioned.

Stephenson's 2019 novel, "Fall; or, Dodge in Hell", is promoted as a sequel to "Reamde" (2011), but as the story unfolds, it is revealed that "Fall", "Reamde", "Cryptonomicon" and "The Baroque Cycle" are all set in the same fictional universe, with references to the Waterhouse, Shaftoe and Hacklheber families, as well as Societas Eruditorum and Epiphyte Corporation. Two "Wise" entities from "The Baroque Cycle" also appear in "Fall," including Enoch Root. 

Peter Thiel states in his book "Zero to One" that "Cryptonomicon" was required reading during the early days of PayPal.

According to critic Jay Clayton, the book is written for a technical or geek audience. Despite the technical detail, the book drew praise from both Stephenson's science fiction fan base and literary critics and buyers. In his book "Charles Dickens in Cyberspace: The Afterlife of the Nineteenth Century in Postmodern Culture" (2003), Jay Clayton calls Stephenson’s book the “ultimate geek novel” and draws attention to the “literary-scientific-engineering-military-industrial-intelligence alliance” that produced discoveries in two eras separated by fifty years, World War II and the Internet age. In July 2012, io9 included the book on its list of "10 Science Fiction Novels You Pretend to Have Read".





</doc>
<doc id="21862" url="https://en.wikipedia.org/wiki?curid=21862" title="In the Beginning... Was the Command Line">
In the Beginning... Was the Command Line

In the Beginning... Was the Command Line is an essay by Neal Stephenson which was originally published online in 1999 and later made available in book form (November 1999, ). The essay is a commentary on why the proprietary operating systems business is unlikely to remain profitable in the future because of competition from free software. It also analyzes the corporate/collective culture of the Microsoft, Apple, and free software communities.

Stephenson explores the GUI as a metaphor in terms of the increasing interposition of abstractions between humans and the actual workings of devices (in a similar manner to "Zen and the Art of Motorcycle Maintenance") and explains the beauty hackers feel in good-quality tools. He does this with a car analogy. He compares four operating systems, Mac OS by Apple Computer to a luxury European car, Windows by Microsoft to a station wagon, Linux to a free tank, and BeOS to a batmobile. Stephenson argues that people continue to buy the station wagon despite free tanks being given away, because people do not want to learn how to operate a tank; they know that the station wagon dealership has a machine shop that they can take their car to when it breaks down. Because of this attitude, Stephenson argues that Microsoft is not really a monopoly, as evidenced by the free availability of other choice OSes, but rather has simply accrued enough mindshare among the people to have them coming back. He compares Microsoft to Disney, in that both are selling a vision to their customers, who in turn "want to believe" in that vision.

Stephenson relays his experience with the Debian bug tracking system (#6518). He then contrasts it with Microsoft's approach. Debian developers responded from around the world within a day. He was completely frustrated with his initial attempt to achieve the same response from Microsoft, but he concedes that his subsequent experience was satisfactory. The difference he notes is that Debian developers are personally accessible and transparently own up to defects in their OS distribution, while Microsoft pretends errors don't exist.

The essay was written before the advent of Mac OS X. A recurring theme is the full power of the command line compared with easier to learn graphical user interfaces (GUIs) which are described as broken mixed metaphors for 'power users'. He then mentions GUIs which allow traditional terminal windows to be used. In a Slashdot interview in 2004, in response to the question:

... have you embraced the new UNIX based MacOS X as the OS you want to use when you "Just want to go to Disneyland"?

he replied:
I embraced OS X as soon as it was available and have never looked back. So a lot of "In the Beginning...was the Command Line" is now obsolete. I keep meaning to update it, but if I'm honest with myself, I have to say this is unlikely.

With Neal Stephenson's permission, Garrett Birkel responded to "In the Beginning...was the Command Line" in 2004, bringing it up to date and critically discussing Stephenson's argument. Birkel's response is interspersed throughout the original text, which remains untouched.




</doc>
<doc id="21863" url="https://en.wikipedia.org/wiki?curid=21863" title="Netscape Navigator">
Netscape Navigator

Netscape Navigator was a proprietary web browser, and the original browser of the Netscape line, from versions 1 to 4.08, and 9.x. It was the flagship product of the Netscape Communications Corp and was the dominant web browser in terms of usage share in the 1990s, but by around 2003 its use had almost disappeared. This was primarily due to the increased use of Microsoft's Internet Explorer web browser software, and partly because the Netscape Corporation (later purchased by AOL) did not sustain Netscape Navigator's technical innovation in the late 1990s.

The business demise of Netscape was a central premise of Microsoft's antitrust trial, wherein the Court ruled that Microsoft's bundling of Internet Explorer with the Windows operating system was a monopolistic and illegal business practice. The decision came too late for Netscape, however, as Internet Explorer had by then become the dominant web browser in Windows.

The Netscape Navigator web browser was succeeded by the Netscape Communicator suite in 1997. Netscape Communicator's 4.x source code was the base for the Netscape-developed Mozilla Application Suite, which was later renamed SeaMonkey. Netscape's Mozilla Suite also served as the base for a browser-only spinoff called Mozilla Firefox.

The Netscape Navigator name returned in 2007 when AOL announced version 9 of the Netscape series of browsers, Netscape Navigator 9. On December 28, 2007, AOL canceled its development but continued supporting the web browser with security updates until March 1, 2008. AOL allows downloading of archived versions of the Netscape Navigator web browser family. AOL maintains the Netscape website as an Internet portal.

Netscape Navigator was inspired by the success of the Mosaic web browser, which was co-written by Marc Andreessen, a part-time employee of the National Center for Supercomputing Applications at the University of Illinois. After Andreessen graduated in 1993, he moved to California and there met Jim Clark, the recently departed founder of Silicon Graphics. Clark believed that the Mosaic browser had great commercial possibilities and provided the seed money. Soon Mosaic Communications Corporation was in business in Mountain View, California, with Andreessen as a vice-president. Since the University of Illinois was unhappy with the company's use of the Mosaic name, the company changed its name to Netscape Communications (suggested by product manager Greg Sands ) and named its flagship web browser Netscape Navigator.

Netscape announced in its first press release (October 13, 1994) that it would make Navigator available without charge to all non-commercial users, and beta versions of version 1.0 and 1.1 were indeed freely downloadable in November 1994 and March 1995, with the full version 1.0 available in December 1994. Netscape's initial corporate policy regarding Navigator claimed that it would make Navigator freely available for non-commercial use in accordance with the notion that Internet software should be distributed for free.

However, within two months of that press release, Netscape apparently reversed its policy on who could freely obtain and use version 1.0 by only mentioning that educational and non-profit institutions could use version 1.0 at no charge.

The reversal was complete with the availability of version 1.1 beta on March 6, 1995, in which a press release states that the final 1.1 release would be available at no cost only for academic and non-profit organizational use. Gone was the notion expressed in the first press release that Navigator would be freely available in the spirit of Internet software.

Some security experts and cryptographers found out that all released Netscape versions had major security problems with crashing the browser with long URLs and 40 bits encryption keys.

The first few releases of the product were made available in "commercial" and "evaluation" versions; for example, version "1.0" and version "1.0N". The "N" evaluation versions were completely identical to the commercial versions; the letter was there to remind people to pay for the browser once they felt they had tried it long enough and were satisfied with it. This distinction was formally dropped within a year of the initial release, and the full version of the browser continued to be made available for free online, with boxed versions available on floppy disks (and later CDs) in stores along with a period of phone support. During this era, "Internet Starter Kit" books were popular, and usually included a floppy disk or CD containing internet software, and this was a popular means of obtaining Netscape's and other browsers. Email support was initially free, and remained so for a year or two until the volume of support requests grew too high.

During development, the Netscape browser was known by the code name "Mozilla", which became the name of a Godzilla-like cartoon dragon mascot used prominently on the company's web site. The Mozilla name was also used as the User-Agent in HTTP requests by the browser. Other web browsers claimed to be compatible with Netscape's extensions to HTML, and therefore used the same name in their User-Agent identifiers so that web servers would send them the same pages as were sent to Netscape browsers. Mozilla is now a generic name for matters related to the open source successor to Netscape Communicator and is most identified with the browser Firefox.

When the consumer Internet revolution arrived in the mid1990s, Netscape was well positioned to take advantage of it. With a good mix of features and an attractive licensing scheme that allowed free use for non-commercial purposes, the Netscape browser soon became the de facto standard, particularly on the Windows platform. Internet service providers and computer magazine publishers helped make Navigator readily available.

An innovation that Netscape introduced in 1994 was the on-the-fly display of web pages, where text and graphics appeared on the screen as the web page downloaded. Earlier web browsers would not display a page until all graphics on it had been loaded over the network connection; this meant a user might have only a blank page for several minutes. With Netscape, people using dial-up connections could begin reading the text of a web page within seconds of entering a web address, even before the rest of the text and graphics had finished downloading. This made the web much more tolerable to the average user.

Through the late 1990s, Netscape made sure that Navigator remained the technical leader among web browsers. New features included cookies, frames, proxy auto-config, and JavaScript (in version 2.0). Although those and other innovations eventually became open standards of the W3C and ECMA and were emulated by other browsers, they were often viewed as controversial. Netscape, according to critics, was more interested in bending the web to its own de facto "standards" (bypassing standards committees and thus marginalizing the commercial competition) than it was in fixing bugs in its products. Consumer rights advocates were particularly critical of cookies and of commercial web sites using them to invade individual privacy.

In the marketplace, however, these concerns made little difference. Netscape Navigator remained the market leader with more than 50% usage share. The browser software was available for a wide range of operating systems, including Windows (3.1, 95, 98, NT), Macintosh, Linux, OS/2, and many versions of Unix including OSF/1, Sun Solaris, BSD/OS, IRIX, AIX, and HP-UX, and looked and worked nearly identically on every one of them. Netscape began to experiment with prototypes of a web-based system, known internally as “Constellation”, which would allow a user to access and edit his or her files anywhere across a network no matter what computer or operating system he or she happened to be using.

Industry observers forecast the dawn of a new era of connected computing. The underlying operating system, it was believed, would not be an important consideration; future applications would run within a web browser. This was seen by Netscape as a clear opportunity to entrench Navigator at the heart of the next generation of computing, and thus gain the opportunity to expand into all manner of other software and service markets.

With the success of Netscape showing the importance of the web (more people were using the Internet due in part to the ease of using Netscape), Internet browsing began to be seen as a potentially profitable market. Following Netscape's lead, Microsoft started a campaign to enter the web browser software market. Like Netscape before them, Microsoft licensed the Mosaic source code from Spyglass, Inc. (which in turn licensed code from University of Illinois). Using this basic code, Microsoft created Internet Explorer (IE).

The competition between Microsoft and Netscape dominated the Browser Wars. Internet Explorer, Version 1.0 (shipped in the Internet Jumpstart Kit in Microsoft Plus! For Windows 95) and IE, Version 2.0 (the first cross-platform version of the web browser, supporting both Windows and Mac OS) were thought by many to be inferior and primitive when compared to contemporary versions of Netscape Navigator. With the release of IE version 3.0 (1996) Microsoft was able to catch up with Netscape competitively, with IE Version 4.0 (1997) further improvement in terms of market share. IE 5.0 (1999) improved stability and took significant market share from Netscape Navigator for the first time.

There were two versions of Netscape Navigator 3.0, the Standard Edition and the Gold Edition. The latter consisted of the Navigator browser with e-mail, news readers, and a WYSIWYG web page compositor; however, these extra functions enlarged and slowed the software, rendering it prone to crashing.

This Gold Edition was renamed Netscape Communicator starting with version 4.0; the name change diluted its name-recognition and confused users. Netscape CEO James L. Barksdale insisted on the name change because Communicator was a general-purpose "client" application, which contained the Navigator "browser".

The aging Netscape Communicator 4.x was slower than Internet Explorer 5.0. Typical web pages had become heavily illustrated, often JavaScript-intensive, and encoded with HTML features designed for specific purposes but now employed as global layout tools (HTML tables, the most obvious example of this, were especially difficult for Communicator to render). The Netscape browser, once a solid product, became crash-prone and buggy; for example, some versions re-downloaded an entire web page to re-render it when the browser window was re-sized (a nuisance to dial-up users), and the browser would usually crash when the page contained simple Cascading Style Sheets, as proper support for CSS never made it into Communicator 4.x. At the time that Communicator 4.0 was being developed, Netscape had a competing technology called JavaScript Style Sheets. Near the end of the development cycle, it became obvious that CSS would prevail, so Netscape quickly implemented a CSS to JSSS converter, which then processed CSS as JSSS (this is why turning JavaScript off also disabled CSS). Moreover, Netscape Communicator's browser interface design appeared dated in comparison to Internet Explorer and interface changes in Microsoft and Apple's operating systems.

By the end of the decade, Netscape's web browser had lost dominance over the Windows platform, and the August 1997 Microsoft financial agreement to invest one hundred and fifty million dollars in Apple required that Apple make Internet Explorer the default web browser in new Mac OS distributions. The latest IE Mac release at that time was Internet Explorer version 3.0 for Macintosh, but Internet Explorer 4 was released later that year.

Microsoft succeeded in having ISPs and PC vendors distribute Internet Explorer to their customers instead of Netscape Navigator, mostly due to Microsoft using its leverage from Windows OEM licenses, and partly aided by Microsoft's investment in making IE brandable, such that a customized version of IE could be offered. Also, web developers used proprietary, browser-specific extensions in web pages. Both Microsoft and Netscape did this, having added many proprietary HTML tags to their browsers, which forced users to choose between two competing and almost incompatible web browsers.

In March 1998, Netscape released most of the development code base for Netscape Communicator under an open source license. Only pre-alpha versions of Netscape 5 were released before the open source community decided to scrap the Netscape Navigator codebase entirely and build a new web browser around the Gecko layout engine which Netscape had been developing but which had not yet incorporated. The community-developed open source project was named "Mozilla", Netscape Navigator's original code name. America Online bought Netscape; Netscape programmers took a pre-beta-quality form of the Mozilla codebase, gave it a new GUI, and released it as Netscape 6. This did nothing to win back users, who continued to migrate to Internet Explorer. After the release of Netscape 7 and a long public beta test, Mozilla 1.0 was released on June 5, 2002. The same code-base, notably the Gecko layout engine, became the basis of independent applications, including Firefox and Thunderbird.

On December 28, 2007, the Netscape developers announced that AOL had canceled development of Netscape Navigator, leaving it unsupported as of March 1, 2008. Despite this, archived and unsupported versions of the browser remain available for download.

Netscape's contributions to the web include JavaScript, which was submitted as a new standard to Ecma International. The resultant ECMAScript specification allowed JavaScript support by multiple web browsers and its use as a cross-browser scripting language, long after Netscape Navigator itself had dropped in popularity. Another example is the FRAME tag, that is widely supported today, and that has been incorporated into official web standards such as the "HTML 4.01 Frameset" specification.

In a 2007 "PC World" column, the original Netscape Navigator was considered the "best tech product of all time" due to its impact on the Internet.




</doc>
<doc id="21865" url="https://en.wikipedia.org/wiki?curid=21865" title="Neurotransmitter">
Neurotransmitter

Neurotransmitters are chemical messengers that transmit a message from a nerve cell across the synapse to a target cell. The target can be another nerve cell, or a muscle cell, or a gland cell. They are chemicals made by the nerve cell specifically to transmit the message. 

Neurotransmitters are released from synaptic vesicles in synapses into the synaptic cleft, where they are received by neurotransmitter receptors on the target cell. Many neurotransmitters are synthesized from simple and plentiful precursors such as amino acids, which are readily available and only require a small number of biosynthetic steps for conversion. Neurotransmitters are essential to the function of complex neural systems. The exact number of unique neurotransmitters in humans is unknown, but more than 200 have been identified.

Neurotransmitters are stored in synaptic vesicles, clustered close to the cell membrane at the axon terminal of the presynaptic neuron. Neurotransmitters are released into and diffuse across the synaptic cleft, where they bind to specific receptors on the membrane of the postsynaptic neuron. Binding of neurotransmitters may influence the postsynaptic neuron in either an excitation or inhibitory way, depolarizing or repolarizing it respectively.

Most of the neurotransmitters are about the size of a single amino acid; however, some neurotransmitters may be the size of larger proteins or peptides. A released neurotransmitter is typically available in the synaptic cleft for a short time before it is metabolized by enzymes, pulled back into the presynaptic neuron through reuptake, or bound to a postsynaptic receptor. Nevertheless, short-term exposure of the receptor to a neurotransmitter is typically sufficient for causing a postsynaptic response by way of synaptic transmission.

Generally, a neurotransmitter is released at the presynaptic terminal in response to a threshold action potential or graded electrical potential in the presynaptic neuron. However, low level 'baseline' release also occurs without electrical stimulation.

Until the early 20th century, scientists assumed that the majority of synaptic communication in the brain was electrical. However, through histological examinations by Ramón y Cajal, a 20 to 40 nm gap between neurons, known today as the synaptic cleft, was discovered. The presence of such a gap suggested communication via chemical messengers traversing the synaptic cleft, and in 1921 German pharmacologist Otto Loewi confirmed that neurons can communicate by releasing chemicals. Through a series of experiments involving the vagus nerves of frogs, Loewi was able to manually slow the heart rate of frogs by controlling the amount of saline solution present around the vagus nerve. Upon completion of this experiment, Loewi asserted that sympathetic regulation of cardiac function can be mediated through changes in chemical concentrations. Furthermore, Otto Loewi is credited with discovering acetylcholine (ACh)—the first known neurotransmitter.

There are four main criteria for identifying neurotransmitters:

However, given advances in pharmacology, genetics, and chemical neuroanatomy, the term "neurotransmitter" can be applied to chemicals that:

The anatomical localization of neurotransmitters is typically determined using immunocytochemical techniques, which identify the location of either the transmitter substances themselves or of the enzymes that are involved in their synthesis. Immunocytochemical techniques have also revealed that many transmitters, particularly the neuropeptides, are co-localized, that is, a neuron may release more than one transmitter from its synaptic terminal. Various techniques and experiments such as staining, stimulating, and collecting can be used to identify neurotransmitters throughout the central nervous system.

There are many different ways to classify neurotransmitters. Dividing them into amino acids, peptides, and monoamines is sufficient for some classification purposes.

Major neurotransmitters:

In addition, over 50 neuroactive peptides have been found, and new ones are discovered regularly. Many of these are co-released along with a small-molecule transmitter. Nevertheless, in some cases, a peptide is the primary transmitter at a synapse. Beta-Endorphin is a relatively well-known example of a peptide neurotransmitter because it engages in highly specific interactions with opioid receptors in the central nervous system.

Single ions (such as synaptically released zinc) are also considered neurotransmitters by some, as well as some gaseous molecules such as nitric oxide (NO), carbon monoxide (CO), and hydrogen sulfide (HS). The gases are produced in the neural cytoplasm and are immediately diffused through the cell membrane into the extracellular fluid and into nearby cells to stimulate production of second messengers. Soluble gas neurotransmitters are difficult to study, as they act rapidly and are immediately broken down, existing for only a few seconds.

The most prevalent transmitter is glutamate, which is excitatory at well over 90% of the synapses in the human brain. The next most prevalent is gamma-Aminobutyric Acid, or GABA, which is inhibitory at more than 90% of the synapses that do not use glutamate. Although other transmitters are used in fewer synapses, they may be very important functionally: the great majority of psychoactive drugs exert their effects by altering the actions of some neurotransmitter systems, often acting through transmitters other than glutamate or GABA. Addictive drugs such as cocaine and amphetamines exert their effects primarily on the dopamine system. The addictive opiate drugs exert their effects primarily as functional analogs of opioid peptides, which, in turn, regulate dopamine levels.

Neurons form elaborate networks through which nerve impulses—action potentials—travel. Each neuron has as many as 15,000 connections with neighboring neurons.

Neurons do not touch each other (except in the case of an electrical synapse through a gap junction); instead, neurons interact at contact points called synapses: a junction within two nerve cells, consisting of a miniature gap within which impulses are carried by a neurotransmitter. A neuron transports its information by way of a nerve impulse called an action potential. When an action potential arrives at the synapse's presynaptic terminal button, it may stimulate the release of neurotransmitters. These neurotransmitters are released into the synaptic cleft to bind onto the receptors of the postsynaptic membrane and influence another cell, either in an inhibitory or excitatory way. The next neuron may be connected to many more neurons, and if the total of excitatory influences minus inhibitory influences is great enough, it will also "fire". That is to say, it will create a new action potential at its axon hillock, releasing neurotransmitters and passing on the information to yet another neighboring neuron.

A neurotransmitter can influence the function of a neuron through a remarkable number of mechanisms. In its direct actions in influencing a neuron's electrical excitability, however, a neurotransmitter acts in only one of two ways: excitatory or inhibitory. A neurotransmitter influences trans-membrane ion flow either to increase (excitatory) or to decrease (inhibitory) the probability that the cell with which it comes in contact will produce an action potential. Thus, despite the wide variety of synapses, they all convey messages of only these two types, and they are labeled as such. Type I synapses are excitatory in their actions, whereas type II synapses are inhibitory. Each type has a different appearance and is located on different parts of the neurons under its influence.

Type I (excitatory) synapses are typically located on the shafts or the spines of dendrites, whereas type II (inhibitory) synapses are typically located on a cell body. In addition, Type I synapses have round synaptic vesicles, whereas the vesicles of type II synapses are flattened. The material on the presynaptic and post-synaptic membranes is denser in a Type I synapse than it is in a type II, and the type I synaptic cleft is wider. Finally, the active zone on a Type I synapse is larger than that on a Type II synapse.

The different locations of type I and type II synapses divide a neuron into two zones: an excitatory dendritic tree and an inhibitory cell body. From an inhibitory perspective, excitation comes in over the dendrites and spreads to the axon hillock to trigger an action potential. If the message is to be stopped, it is best stopped by applying inhibition on the cell body, close to the axon hillock where the action potential originates. Another way to conceptualize excitatory–inhibitory interaction is to picture excitation overcoming inhibition. If the cell body is normally in an inhibited state, the only way to generate an action potential at the axon hillock is to reduce the cell body's inhibition. In this "open the gates" strategy, the excitatory message is like a racehorse ready to run down the track, but first, the inhibitory starting gate must be removed.

As explained above, the only direct action of a neurotransmitter is to activate a receptor. Therefore, the effects of a neurotransmitter system depend on the connections of the neurons that use the transmitter, and the chemical properties of the receptors that the transmitter binds to.

Here are a few examples of important neurotransmitter actions:

Neurons expressing certain types of neurotransmitters sometimes form distinct systems, where activation of the system affects large volumes of the brain, called volume transmission. Major neurotransmitter systems include the noradrenaline (norepinephrine) system, the dopamine system, the serotonin system, and the cholinergic system, among others. Trace amines have a modulatory effect on neurotransmission in monoamine pathways (i.e., dopamine, norepinephrine, and serotonin pathways) throughout the brain via signaling through trace amine-associated receptor 1. A brief comparison of these systems follows:

Understanding the effects of drugs on neurotransmitters comprises a significant portion of research initiatives in the field of neuroscience. Most neuroscientists involved in this field of research believe that such efforts may further advance our understanding of the circuits responsible for various neurological diseases and disorders, as well as ways to effectively treat and someday possibly prevent or cure such illnesses.

Drugs can influence behavior by altering neurotransmitter activity. For instance, drugs can decrease the rate of synthesis of neurotransmitters by affecting the synthetic enzyme(s) for that neurotransmitter. When neurotransmitter syntheses are blocked, the amount of neurotransmitters available for release becomes substantially lower, resulting in a decrease in neurotransmitter activity. Some drugs block or stimulate the release of specific neurotransmitters. Alternatively, drugs can prevent neurotransmitter storage in synaptic vesicles by causing the synaptic vesicle membranes to leak. Drugs that prevent a neurotransmitter from binding to its receptor are called receptor antagonists. For example, drugs used to treat patients with schizophrenia such as haloperidol, chlorpromazine, and clozapine are antagonists at receptors in the brain for dopamine. Other drugs act by binding to a receptor and mimicking the normal neurotransmitter. Such drugs are called receptor agonists. An example of a receptor agonist is morphine, an opiate that mimics effects of the endogenous neurotransmitter β-endorphin to relieve pain. Other drugs interfere with the deactivation of a neurotransmitter after it has been released, thereby prolonging the action of a neurotransmitter. This can be accomplished by blocking re-uptake or inhibiting degradative enzymes. Lastly, drugs can also prevent an action potential from occurring, blocking neuronal activity throughout the central and peripheral nervous system. Drugs such as tetrodotoxin that block neural activity are typically lethal.

Drugs targeting the neurotransmitter of major systems affect the whole system, which can explain the complexity of action of some drugs. Cocaine, for example, blocks the re-uptake of dopamine back into the presynaptic neuron, leaving the neurotransmitter molecules in the synaptic gap for an extended period of time. Since the dopamine remains in the synapse longer, the neurotransmitter continues to bind to the receptors on the postsynaptic neuron, eliciting a pleasurable emotional response. Physical addiction to cocaine may result from prolonged exposure to excess dopamine in the synapses, which leads to the downregulation of some post-synaptic receptors. After the effects of the drug wear off, an individual can become depressed due to decreased probability of the neurotransmitter binding to a receptor. Fluoxetine is a selective serotonin re-uptake inhibitor (SSRI), which blocks re-uptake of serotonin by the presynaptic cell which increases the amount of serotonin present at the synapse and furthermore allows it to remain there longer, providing potential for the effect of naturally released serotonin. AMPT prevents the conversion of tyrosine to L-DOPA, the precursor to dopamine; reserpine prevents dopamine storage within vesicles; and deprenyl inhibits monoamine oxidase (MAO)-B and thus increases dopamine levels.

An agonist is a chemical capable of binding to a receptor, such as a neurotransmitter receptor, and initiating the same reaction typically produced by the binding of the endogenous substance. An agonist of a neurotransmitter will thus initiate the same receptor response as the transmitter. In neurons, an agonist drug may activate neurotransmitter receptors either directly or indirectly. Direct-binding agonists can be further characterized as full agonists, partial agonists, inverse agonists.

Direct agonists act similar to a neurotransmitter by binding directly to its associated receptor site(s), which may be located on the presynaptic neuron or postsynaptic neuron, or both. Typically, neurotransmitter receptors are located on the postsynaptic neuron, while neurotransmitter autoreceptors are located on the presynaptic neuron, as is the case for monoamine neurotransmitters; in some cases, a neurotransmitter utilizes retrograde neurotransmission, a type of feedback signaling in neurons where the neurotransmitter is released postsynaptically and binds to target receptors located on the presynaptic neuron. Nicotine, a compound found in tobacco, is a direct agonist of most nicotinic acetylcholine receptors, mainly located in cholinergic neurons. Opiates, such as morphine, heroin, hydrocodone, oxycodone, codeine, and methadone, are μ-opioid receptor agonists; this action mediates their euphoriant and pain relieving properties.

Indirect agonists increase the binding of neurotransmitters at their target receptors by stimulating the release or preventing the reuptake of neurotransmitters. Some indirect agonists trigger neurotransmitter release and prevent neurotransmitter reuptake. Amphetamine, for example, is an indirect agonist of postsynaptic dopamine, norepinephrine, and serotonin receptors in each their respective neurons; it produces both neurotransmitter release into the presynaptic neuron and subsequently the synaptic cleft and prevents their reuptake from the synaptic cleft by activating TAAR1, a presynaptic G protein-coupled receptor, and binding to a site on VMAT2, a type of monoamine transporter located on synaptic vesicles within monoamine neurons.

An antagonist is a chemical that acts within the body to reduce the physiological activity of another chemical substance (as an opiate); especially one that opposes the action on the nervous system of a drug or a substance occurring naturally in the body by combining with and blocking its nervous receptor.

There are two main types of antagonist: direct-acting Antagonist and indirect-acting Antagonists:

An antagonist drug is one that attaches (or binds) to a site called a receptor without activating that receptor to produce a biological response. It is therefore said to have no intrinsic activity. An antagonist may also be called a receptor "blocker" because they block the effect of an agonist at the site. The pharmacological effects of an antagonist, therefore, result in preventing the corresponding receptor site's agonists (e.g., drugs, hormones, neurotransmitters) from binding to and activating it. Antagonists may be "competitive" or "irreversible".

A competitive antagonist competes with an agonist for binding to the receptor. As the concentration of antagonist increases, the binding of the agonist is progressively inhibited, resulting in a decrease in the physiological response. High concentration of an antagonist can completely inhibit the response. This inhibition can be reversed, however, by an increase of the concentration of the agonist, since the agonist and antagonist compete for binding to the receptor. Competitive antagonists, therefore, can be characterized as shifting the dose–response relationship for the agonist to the right. In the presence of a competitive antagonist, it takes an increased concentration of the agonist to produce the same response observed in the absence of the antagonist.

An irreversible antagonist binds so strongly to the receptor as to render the receptor unavailable for binding to the agonist. Irreversible antagonists may even form covalent chemical bonds with the receptor. In either case, if the concentration of the irreversible antagonist is high enough, the number of unbound receptors remaining for agonist binding may be so low that even high concentrations of the agonist do not produce the maximum biological response.

While intake of neurotransmitter precursors does increase neurotransmitter synthesis, evidence is mixed as to whether neurotransmitter release and postsynaptic receptor firing is increased. Even with increased neurotransmitter release, it is unclear whether this will result in a long-term increase in neurotransmitter signal strength, since the nervous system can adapt to changes such as increased neurotransmitter synthesis and may therefore maintain constant firing. Some neurotransmitters may have a role in depression and there is some evidence to suggest that intake of precursors of these neurotransmitters may be useful in the treatment of mild and moderate depression.

-DOPA, a precursor of dopamine that crosses the blood–brain barrier, is used in the treatment of Parkinson's disease. For depressed patients where low activity of the neurotransmitter norepinephrine is implicated, there is only little evidence for benefit of neurotransmitter precursor administration. L-phenylalanine and L-tyrosine are both precursors for dopamine, norepinephrine, and epinephrine. These conversions require vitamin B6, vitamin C, and S-adenosylmethionine. A few studies suggest potential antidepressant effects of L-phenylalanine and L-tyrosine, but there is much room for further research in this area.

Administration of L-tryptophan, a precursor for serotonin, is seen to double the production of serotonin in the brain. It is significantly more effective than a placebo in the treatment of mild and moderate depression. This conversion requires vitamin C. 5-hydroxytryptophan (5-HTP), also a precursor for serotonin, is more effective than a placebo.

Diseases and disorders may also affect specific neurotransmitter systems. The following are disorders involved in either an increase, decrease, or imbalance of certain neurotransmitters.

Dopamine:

For example, problems in producing dopamine (mainly in the substantia nigra) can result in Parkinson's disease, a disorder that affects a person's ability to move as they want to, resulting in stiffness, tremors or shaking, and other symptoms. Some studies suggest that having too little or too much dopamine or problems using dopamine in the thinking and feeling regions of the brain may play a role in disorders like schizophrenia or attention deficit hyperactivity disorder (ADHD). Dopamine is also involved in addiction and drug use, as most recreational drugs cause an influx of dopamine in the brain (especially opioid and methamphetamines) that produces a pleasurable feeling, which is why users constantly crave drugs.

Serotonin:

Similarly, after some research suggested that drugs that block the recycling, or reuptake, of serotonin seemed to help some people diagnosed with depression, it was theorized that people with depression might have lower-than-normal serotonin levels. Though widely popularized, this theory was not borne out in subsequent research. Therefore, selective serotonin reuptake inhibitors (SSRIs) are used to increase the amounts of serotonin in synapses.

Glutamate:

Furthermore, problems with producing or using glutamate have been suggestively and tentatively linked to many mental disorders, including autism, obsessive compulsive disorder (OCD), schizophrenia, and depression. Having too much glutamate has been linked to neurological diseases such as Parkinson's disease, multiple sclerosis, Alzheimer's disease, stroke, and ALS (amyotrophic lateral sclerosis).
Generally, there are no scientifically established "norms" for appropriate levels or "balances" of different neurotransmitters. It is in most cases pragmatically impossible to even measure levels of neurotransmitters in a brain or body at any distinct moments in time. Neurotransmitters regulate each other's release, and weak consistent imbalances in this mutual regulation were linked to temperament in healthy people 
. Strong imbalances or disruptions to neurotransmitter systems have been associated with many diseases and mental disorders. These include Parkinson's, depression, insomnia, Attention Deficit Hyperactivity Disorder (ADHD), anxiety, memory loss, dramatic changes in weight and addictions. Chronic physical or emotional stress can be a contributor to neurotransmitter system changes. Genetics also plays a role in neurotransmitter activities. Apart from recreational use, medications that directly and indirectly interact one or more transmitter or its receptor are commonly prescribed for psychiatric and psychological issues. Notably, drugs interacting with serotonin and norepinephrine are prescribed to patients with problems such as depression and anxiety—though the notion that there is much solid medical evidence to support such interventions has been widely criticized. Studies shown that dopamine imbalance has an influence on multiple sclerosis and other neurological disorders.

A neurotransmitter must be broken down once it reaches the post-synaptic cell to prevent further excitatory or inhibitory signal transduction. This allows new signals to be produced from the adjacent nerve cells. When the neurotransmitter has been secreted into the synaptic cleft, it binds to specific receptors on the postsynaptic cell, thereby generating a postsynaptic electrical signal. The transmitter must then be removed rapidly to enable the postsynaptic cell to engage in another cycle of neurotransmitter release, binding, and signal generation. Neurotransmitters are terminated in three different ways:

For example, choline is taken up and recycled by the pre-synaptic neuron to synthesize more ACh. Other neurotransmitters such as dopamine are able to diffuse away from their targeted synaptic junctions and are eliminated from the body via the kidneys, or destroyed in the liver. Each neurotransmitter has very specific degradation pathways at regulatory points, which may be targeted by the body's regulatory system or by recreational drugs.



</doc>
<doc id="21868" url="https://en.wikipedia.org/wiki?curid=21868" title="Neutronium">
Neutronium

Neutronium (sometimes shortened to neutrium, also referred to as neutrite) is a hypothetical substance composed purely of neutrons. The word was coined by scientist Andreas von Antropoff in 1926 (before the discovery of the neutron) for the hypothetical "element of atomic number zero" (with zero protons in its nucleus) that he placed at the head of the periodic table (denoted by dash, no element symbol). However, the meaning of the term has changed over time, and from the last half of the 20th century onward it has been also used to refer to extremely dense substances resembling the neutron-degenerate matter theorized to exist in the cores of neutron stars; hereinafter ""degenerate" neutronium" will refer to this. Science fiction and popular literature frequently use the term "neutronium" to refer to a highly dense phase of matter composed primarily of neutrons.

Neutronium is used in popular physics literature to refer to the material present in the cores of neutron stars (stars which are too massive to be supported by electron degeneracy pressure and which collapse into a denser phase of matter). This term is very rarely used in scientific literature, for three reasons: there are multiple definitions for the term "neutronium"; there is considerable uncertainty over the composition of the material in the cores of neutron stars (it could be neutron-degenerate matter, strange matter, quark matter, or a variant or combination of the above); the properties of neutron star material should depend on depth due to changing pressure (see below), and no sharp boundary between the crust (consisting primarily of atomic nuclei) and almost protonless inner layer is expected to exist.

When neutron star core material is presumed to consist mostly of free neutrons, it is typically referred to as neutron-degenerate matter in scientific literature.

The term "neutronium" was coined in 1926 by Andreas von Antropoff for a conjectured form of matter made up of neutrons with no protons or electrons, which he placed as the chemical element of atomic number zero at the head of his new version of the periodic table. It was subsequently placed in the middle of several spiral representations of the periodic system for classifying the chemical elements, such as those of Charles Janet (1928), E. I. Emerson (1944), and John D. Clark (1950).

Although the term is not used in the scientific literature either for a condensed form of matter, or as an element, there have been reports that, besides the free neutron, there may exist two bound forms of neutrons without protons. If neutronium were considered to be an element, then these neutron clusters could be considered to be the isotopes of that element. However, these reports have not been further substantiated.
Although not called "neutronium", the National Nuclear Data Center's "Nuclear Wallet Cards" lists as its first "isotope" an "element" with the symbol n and atomic number "Z" = 0 and mass number "A" = 1. This isotope is described as decaying to element H with a half life of .

Neutron matter is equivalent to a chemical element with atomic number 0, which is to say that it is equivalent to a species of atoms having no protons in their atomic nuclei. It is extremely radioactive; its only legitimate equivalent isotope, the free neutron, has a half-life of only 10 minutes, which is comparable to half that of the most stable known isotope of francium. Neutron matter decays quickly into hydrogen. Neutron matter has no electronic structure on account of its total lack of electrons. As an equivalent element, however, it could be classified as a noble gas.

Bulk neutron matter has never been viewed. It is assumed that neutron matter would appear as a chemically inert gas, if enough could be collected together to be viewed as a bulk gas or liquid, because of the general appearance of the elements in the noble gas column of the periodic table.

While this lifetime is long enough to permit the study of neutronium's chemical properties, there are serious practical problems. Having no charge or electrons, neutronium would not interact strongly with ordinary low-energy photons (visible light) and would feel no electrostatic forces, so it would diffuse into the walls of most containers made of ordinary matter. Certain materials are able to resist diffusion or absorption of ultracold neutrons due to nuclear-quantum effects, specifically reflection caused by the strong interaction. At ambient temperature and in the presence of other elements, thermal neutrons readily undergo neutron capture to form heavier (and often radioactive) isotopes of that element.

Neutron matter at standard pressure and temperature is predicted by the ideal gas law to be less dense than even hydrogen, with a density of only (roughly 27 times less dense than air and half as dense as hydrogen gas). Neutron matter is predicted to remain gaseous down to absolute zero at normal pressures, as the zero-point energy of the system is too high to allow condensation. However, neutron matter should in theory form a degenerate gaseous Bose–Einstein condensate at these temperatures, composed of neutron pairs called "dineutrons". At higher temperatures, neutron matter will only condense with sufficient pressure, and solidify with even greater pressure. Such pressures exist in neutron stars, where the extreme pressure causes the neutron matter to become degenerate. However, in the presence of atomic matter compressed to the state of electron degeneracy, β decay may be inhibited due to the Pauli exclusion principle, thus making free neutrons stable. Also, elevated pressures should make neutrons degenerate themselves.

Compared to ordinary elements, neutronium should be more compressible due to the absence of electrically charged protons and electrons. This makes neutronium more energetically favorable than (positive-"Z") atomic nuclei and leads to their conversion to (degenerate) neutronium through electron capture, a process that is believed to occur in stellar cores in the final seconds of the lifetime of massive stars, where it is facilitated by cooling via emission. As a result, degenerate neutronium can have a density of , roughly 14 orders of magnitude denser than the densest known ordinary substances. It was theorized that extreme pressures of order might deform the neutrons into a cubic symmetry, allowing tighter packing of neutrons, or cause a strange matter formation.

The term "neutronium" has been popular in science fiction since at least the middle of the 20th century, such as the in . It typically refers to an extremely dense, incredibly strong form of matter. While presumably inspired by the concept of neutron-degenerate matter in the cores of neutron stars, the material used in fiction bears at most only a superficial resemblance, usually depicted as an extremely strong solid under Earth-like conditions, or possessing exotic properties such as the ability to manipulate time and space. In contrast, all proposed forms of neutron star core material are fluids and are extremely unstable at pressures lower than that found in stellar cores. According to one analysis, a neutron star with a mass below about 0.2 solar masses would explode.



</doc>
<doc id="21869" url="https://en.wikipedia.org/wiki?curid=21869" title="Neutron star">
Neutron star

A neutron star is the collapsed core of a massive supergiant star, which had a total mass of between 10 and 25 solar masses, possibly more if the star was especially metal-rich . Neutron stars are the smallest and densest stellar objects, excluding black holes and hypothetical white holes, quark stars, and strange stars. Neutron stars have a radius on the order of and a mass of about 1.4 solar masses. They result from the supernova explosion of a massive star, combined with gravitational collapse, that compresses the core past white dwarf star density to that of atomic nuclei.

Once formed, they no longer actively generate heat, and cool over time; however, they may still evolve further through collision or accretion. Most of the basic models for these objects imply that neutron stars are composed almost entirely of neutrons (subatomic particles with no net electrical charge and with slightly larger mass than protons); the electrons and protons present in normal matter combine to produce neutrons at the conditions in a neutron star. Neutron stars are partially supported against further collapse by neutron degeneracy pressure, a phenomenon described by the Pauli exclusion principle, just as white dwarfs are supported against collapse by electron degeneracy pressure. However, neutron degeneracy pressure is not by itself sufficient to hold up an object beyond 0.7 and repulsive nuclear forces play a larger role in supporting more massive neutron stars. If the remnant star has a mass exceeding the Tolman–Oppenheimer–Volkoff limit of around 2 solar masses, the combination of degeneracy pressure and nuclear forces is insufficient to support the neutron star and it continues collapsing to form a black hole.

Neutron stars that can be observed are very hot and typically have a surface temperature of around . They are so dense that a normal-sized matchbox containing neutron-star material would have a weight of approximately 3 billion tonnes, the same weight as a 0.5 cubic kilometre chunk of the Earth (a cube with edges of about 800 metres) from Earth's surface. Their magnetic fields are between 10 and 10 (100 million to 1 quadrillion) times stronger than Earth's magnetic field. The gravitational field at the neutron star's surface is about (200 billion) times that of Earth's gravitational field.

As the star's core collapses, its rotation rate increases as a result of conservation of angular momentum, and newly formed neutron stars hence rotate at up to several hundred times per second. Some neutron stars emit beams of electromagnetic radiation that make them detectable as pulsars. Indeed, the discovery of pulsars by Jocelyn Bell Burnell and Antony Hewish in 1967 was the first observational suggestion that neutron stars exist. The radiation from pulsars is thought to be primarily emitted from regions near their magnetic poles. If the magnetic poles do not coincide with the rotational axis of the neutron star, the emission beam will sweep the sky, and when seen from a distance, if the observer is somewhere in the path of the beam, it will appear as pulses of radiation coming from a fixed point in space (the so-called "lighthouse effect"). The fastest-spinning neutron star known is PSR J1748-2446ad, rotating at a rate of 716 times a second or 43,000 revolutions per minute, giving a linear speed at the surface on the order of (i.e., nearly a quarter the speed of light).

There are thought to be around one billion neutron stars in the Milky Way, and at a minimum several hundred million, a figure obtained by estimating the number of stars that have undergone supernova explosions. However, most are old and cold and radiate very little; most neutron stars that have been detected occur only in certain situations in which they do radiate, such as if they are a pulsar or part of a binary system. Slow-rotating and non-accreting neutron stars are almost undetectable; however, since the "Hubble Space Telescope" detection of RX J185635−3754, a few nearby neutron stars that appear to emit only thermal radiation have been detected. Soft gamma repeaters are conjectured to be a type of neutron star with very strong magnetic fields, known as magnetars, or alternatively, neutron stars with fossil disks around them.

Neutron stars in binary systems can undergo accretion which typically makes the system bright in X-rays while the material falling onto the neutron star can form hotspots that rotate in and out of view in identified X-ray pulsar systems. Additionally, such accretion can "recycle" old pulsars and potentially cause them to gain mass and spin-up to very fast rotation rates, forming the so-called millisecond pulsars. These binary systems will continue to evolve, and eventually the companions can become compact objects such as white dwarfs or neutron stars themselves, though other possibilities include a complete destruction of the companion through ablation or merger. The merger of binary neutron stars may be the source of short-duration gamma-ray bursts and are likely strong sources of gravitational waves. In 2017, a direct detection (GW170817) of the gravitational waves from such an event was made, and gravitational waves have also been indirectly detected in a system where two neutron stars orbit each other.

Any main-sequence star with an initial mass of above 8 times the mass of the sun () has the potential to produce a neutron star. As the star evolves away from the main sequence, subsequent nuclear burning produces an iron-rich core. When all nuclear fuel in the core has been exhausted, the core must be supported by degeneracy pressure alone. Further deposits of mass from shell burning cause the core to exceed the Chandrasekhar limit. Electron-degeneracy pressure is overcome and the core collapses further, sending temperatures soaring to over . At these temperatures, photodisintegration (the breaking up of iron nuclei into alpha particles by high-energy gamma rays) occurs. As the temperature climbs even higher, electrons and protons combine to form neutrons via electron capture, releasing a flood of neutrinos. When densities reach nuclear density of , a combination of strong force repulsion and neutron degeneracy pressure halts the contraction. The infalling outer envelope of the star is halted and flung outwards by a flux of neutrinos produced in the creation of the neutrons, becoming a supernova. The remnant left is a neutron star. If the remnant has a mass greater than about , it collapses further to become a black hole.

As the core of a massive star is compressed during a Type II supernova or a Type Ib or Type Ic supernova, and collapses into a neutron star, it retains most of its angular momentum. But, because it has only a tiny fraction of its parent's radius (and therefore its moment of inertia is sharply reduced), a neutron star is formed with very high rotation speed, and then over a very long period it slows. Neutron stars are known that have rotation periods from about 1.4 ms to 30 s. The neutron star's density also gives it very high surface gravity, with typical values ranging from 10 to 10 m/s (more than 10 times that of Earth). One measure of such immense gravity is the fact that neutron stars have an escape velocity ranging from 100,000 km/s to 150,000 km/s, that is, from a third to half the speed of light. The neutron star's gravity accelerates infalling matter to tremendous speed. The force of its impact would likely destroy the object's component atoms, rendering all the matter identical, in most respects, to the rest of the neutron star.

A neutron star has a mass of at least 1.1 solar masses (). The upper limit of mass for a neutron star is called the Tolman–Oppenheimer–Volkoff limit and is generally held to be around , but a recent estimate puts the upper limit at . The maximum observed mass of neutron stars is about for PSR J0740+6620 discovered in September, 2019. Compact stars below the Chandrasekhar limit of are generally white dwarfs whereas compact stars with a mass between and are expected to be neutron stars, but there is an interval of a few tenths of a solar mass where the masses of low-mass neutron stars and high-mass white dwarfs can overlap. It is thought that beyond the stellar remnant will overcome the strong force repulsion and neutron degeneracy pressure so that gravitational collapse will occur to produce a black hole, but the smallest observed mass of a stellar black hole is about . Between and , hypothetical intermediate-mass stars such as quark stars and electroweak stars have been proposed, but none have been shown to exist.

The temperature inside a newly formed neutron star is from around 10 to 10 kelvin. However, the huge number of neutrinos it emits carry away so much energy that the temperature of an isolated neutron star falls within a few years to around 10 kelvin. At this lower temperature, most of the light generated by a neutron star is in X-rays.

Some researchers have proposed a neutron star classification system using Roman numerals (not to be confused with the Yerkes luminosity classes for non-degenerate stars) to sort neutron stars by their mass and cooling rates: type I for neutron stars with low mass and cooling rates, type II for neutron stars with higher mass and cooling rates, and a proposed type III for neutron stars with even higher mass, approaching , and with higher cooling rates and possibly candidates for exotic stars.

Neutron stars have overall densities of to ( to times the density of the Sun), which is comparable to the approximate density of an atomic nucleus of . The neutron star's density varies from about in the crust—increasing with depth—to about or (denser than an atomic nucleus) deeper inside. A neutron star is so dense that one teaspoon (5 milliliters) of its material would have a mass over , about 900 times the mass of the Great Pyramid of Giza. In the enormous gravitational field of a neutron star, that teaspoon of material would weigh , which is 15 times what the Moon would weigh if it were placed on the surface of the Earth. The entire mass of the Earth at neutron star density would fit into a sphere of 305 m in diameter (the size of the Arecibo Observatory). The pressure increases from to from the inner crust to the center.

The equation of state of matter at such high densities is not precisely known because of the theoretical difficulties associated with extrapolating the likely behavior of quantum chromodynamics, superconductivity, and superfluidity of matter in such states. The problem is exacerbated by the empirical difficulties of observing the characteristics of any object that is hundreds of parsecs away, or farther.

A neutron star has some of the properties of an atomic nucleus, including density (within an order of magnitude) and being composed of nucleons. In popular scientific writing, neutron stars are therefore sometimes described as "giant nuclei". However, in other respects, neutron stars and atomic nuclei are quite different. A nucleus is held together by the strong interaction, whereas a neutron star is held together by gravity. The density of a nucleus is uniform, while neutron stars are predicted to consist of multiple layers with varying compositions and densities.

The magnetic field strength on the surface of neutron stars ranges from c. 10 to 10 tesla. These are orders of magnitude higher than in any other object: for comparison, a continuous 16 T field has been achieved in the laboratory and is sufficient to levitate a living frog due to diamagnetic levitation. Variations in magnetic field strengths are most likely the main factor that allows different types of neutron stars to be distinguished by their spectra, and explains the periodicity of pulsars.

The neutron stars known as magnetars have the strongest magnetic fields, in the range of 10 to 10 tesla, and have become the widely accepted hypothesis for neutron star types soft gamma repeaters (SGRs) and anomalous X-ray pulsars (AXPs). The magnetic energy density of a 10 T field is extreme, exceeding the mass−energy density of ordinary matter. Fields of this strength are able to polarize the vacuum to the point that the vacuum becomes birefringent. Photons can merge or split in two, and virtual particle-antiparticle pairs are produced. The field changes electron energy levels and atoms are forced into thin cylinders. Unlike in an ordinary pulsar, magnetar spin-down can be directly powered by its magnetic field, and the magnetic field is strong enough to stress the crust to the point of fracture. Fractures of the crust cause starquakes, observed as extremely luminous millisecond hard gamma ray bursts. The fireball is trapped by the magnetic field, and comes in and out of view when the star rotates, which is observed as a periodic soft gamma repeater (SGR) emission with a period of 5–8 seconds and which lasts for a few minutes.

The origins of the strong magnetic field are as yet unclear. One hypothesis is that of "flux freezing", or conservation of the original magnetic flux during the formation of the neutron star. If an object has a certain magnetic flux over its surface area, and that area shrinks to a smaller area, but the magnetic flux is conserved, then the magnetic field would correspondingly increase. Likewise, a collapsing star begins with a much larger surface area than the resulting neutron star, and conservation of magnetic flux would result in a far stronger magnetic field. However, this simple explanation does not fully explain magnetic field strengths of neutron stars.

The gravitational field at a neutron star's surface is about times stronger than on Earth, at around . Such a strong gravitational field acts as a gravitational lens and bends the radiation emitted by the neutron star such that parts of the normally invisible rear surface become visible.
If the radius of the neutron star is 3"GM"/"c" or less, then the photons may be trapped in an orbit, thus making the whole surface of that neutron star visible "from a single vantage point", along with destabilizing photon orbits at or below the 1 radius distance of the star.

A fraction of the mass of a star that collapses to form a neutron star is released in the supernova explosion from which it forms (from the law of mass–energy equivalence, ). The energy comes from the gravitational binding energy of a neutron star.

Hence, the gravitational force of a typical neutron star is huge. If an object were to fall from a height of one meter on a neutron star 12 kilometers in radius, it would reach the ground at around 1400 kilometers per second. However, even before impact, the tidal force would cause spaghettification, breaking any sort of an ordinary object into a stream of material.

Because of the enormous gravity, time dilation between a neutron star and Earth is significant. For example, eight years could pass on the surface of a neutron star, yet ten years would have passed on Earth, not including the time-dilation effect of its very rapid rotation.

Neutron star relativistic equations of state describe the relation of radius vs. mass for various models. The most likely radii for a given neutron star mass are bracketed by models AP4 (smallest radius) and MS2 (largest radius). BE is the ratio of gravitational binding energy mass equivalent to the observed neutron star gravitational mass of "M" kilograms with radius "R" meters,

Given current values

and star masses "M" commonly reported as multiples of one solar mass,

then the relativistic fractional binding energy of a neutron star is

A neutron star would not be more compact than 10,970 meters radius (AP4 model). Its mass fraction gravitational binding energy would then be 0.187, −18.7% (exothermic). This is not near 0.6/2 = 0.3, −30%.

The equation of state for a neutron star is not yet known. It is assumed that it differs significantly from that of a white dwarf, whose equation of state is that of a degenerate gas that can be described in close agreement with special relativity. However, with a neutron star the increased effects of general relativity can no longer be ignored. Several equations of state have been proposed (FPS, UU, APR, L, SLy, and others) and current research is still attempting to constrain the theories to make predictions of neutron star matter. This means that the relation between density and mass is not fully known, and this causes uncertainties in radius estimates. For example, a neutron star could have a radius of 10.7, 11.1, 12.1 or 15.1 kilometers (for EOS FPS, UU, APR or L respectively).

Current understanding of the structure of neutron stars is defined by existing mathematical models, but it might be possible to infer some details through studies of neutron-star oscillations. Asteroseismology, a study applied to ordinary stars, can reveal the inner structure of neutron stars by analyzing observed spectra of stellar oscillations.

Current models indicate that matter at the surface of a neutron star is composed of ordinary atomic nuclei crushed into a solid lattice with a sea of electrons flowing through the gaps between them. It is possible that the nuclei at the surface are iron, due to iron's high binding energy per nucleon. It is also possible that heavy elements, such as iron, simply sink beneath the surface, leaving only light nuclei like helium and hydrogen. If the surface temperature exceeds 10 kelvin (as in the case of a young pulsar), the surface should be fluid instead of the solid phase that might exist in cooler neutron stars (temperature <10 kelvin).

The "atmosphere" of a neutron star is hypothesized to be at most several micrometers thick, and its dynamics are fully controlled by the neutron star's magnetic field. Below the atmosphere one encounters a solid "crust". This crust is extremely hard and very smooth (with maximum surface irregularities of ~5 mm), due to the extreme gravitational field.

Proceeding inward, one encounters nuclei with ever-increasing numbers of neutrons; such nuclei would decay quickly on Earth, but are kept stable by tremendous pressures. As this process continues at increasing depths, the neutron drip becomes overwhelming, and the concentration of free neutrons increases rapidly. In that region, there are nuclei, free electrons, and free neutrons. The nuclei become increasingly small (gravity and pressure overwhelming the strong force) until the core is reached, by definition the point where mostly neutrons exist. The expected hierarchy of phases of nuclear matter in the inner crust has been characterized as "nuclear pasta", with fewer voids and larger structures towards higher pressures.
The composition of the superdense matter in the core remains uncertain. One model describes the core as superfluid neutron-degenerate matter (mostly neutrons, with some protons and electrons). More exotic forms of matter are possible, including degenerate strange matter (containing strange quarks in addition to up and down quarks), matter containing high-energy pions and kaons in addition to neutrons, or ultra-dense quark-degenerate matter.

Neutron stars are detected from their electromagnetic radiation. Neutron stars are usually observed to pulse radio waves and other electromagnetic radiation, and neutron stars observed with pulses are called pulsars.

Pulsars' radiation is thought to be caused by particle acceleration near their magnetic poles, which need not be aligned with the rotational axis of the neutron star. It is thought that a large electrostatic field builds up near the magnetic poles, leading to electron emission. These electrons are magnetically accelerated along the field lines, leading to curvature radiation, with the radiation being strongly polarized towards the plane of curvature. In addition, high energy photons can interact with lower energy photons and the magnetic field for electron−positron pair production, which through electron–positron annihilation leads to further high energy photons.

The radiation emanating from the magnetic poles of neutron stars can be described as "magnetospheric radiation", in reference to the magnetosphere of the neutron star. It is not to be confused with "magnetic dipole radiation", which is emitted because the magnetic axis is not aligned with the rotational axis, with a radiation frequency the same as the neutron star's rotational frequency.

If the axis of rotation of the neutron star is different to the magnetic axis, external viewers will only see these beams of radiation whenever the magnetic axis point towards them during the neutron star rotation. Therefore, periodic pulses are observed, at the same rate as the rotation of the neutron star.

In addition to pulsars, non-pulsating neutron stars have also been identified, although they may have minor periodic variation in luminosity. This seems to be a characteristic of the X-ray sources known as Central Compact Objects in Supernova remnants (CCOs in SNRs), which are thought to be young, radio-quiet isolated neutron stars.

In addition to radio emissions, neutron stars have also been identified in other parts of the electromagnetic spectrum. This includes visible light, near infrared, ultraviolet, X-rays, and gamma rays. Pulsars observed in X-rays are known as X-ray pulsars if accretion-powered, while those identified in visible light are known as optical pulsars. The majority of neutron stars detected, including those identified in optical, X-ray, and gamma rays, also emit radio waves; the Crab Pulsar produces electromagnetic emissions across the spectrum. However, there exist neutron stars called radio-quiet neutron stars, with no radio emissions detected.

Neutron stars rotate extremely rapidly after their formation due to the conservation of angular momentum; in analogy to spinning ice skaters pulling in their arms, the slow rotation of the original star's core speeds up as it shrinks. A newborn neutron star can rotate many times a second.

Over time, neutron stars slow, as their rotating magnetic fields in effect radiate energy associated with the rotation; older neutron stars may take several seconds for each revolution. This is called "spin down". The rate at which a neutron star slows its rotation is usually constant and very small.

The periodic time ("P") is the rotational period, the time for one rotation of a neutron star. The spin-down rate, the rate of slowing of rotation, is then given the symbol formula_8 ("P"-dot), the derivative of "P" with respect to time. It is defined as periodic time increase per unit time; it is a dimensionless quantity, but can be given the units of s⋅s (seconds per second).

The spin-down rate ("P"-dot) of neutron stars usually falls within the range of 10 to 10 s⋅s, with the shorter period (or faster rotating) observable neutron stars usually having smaller "P"-dot. As a neutron star ages, its rotation slows (as "P" increases); eventually, the rate of rotation will become too slow to power the radio-emission mechanism, and the neutron star can no longer be detected.

"P" and "P"-dot allow minimum magnetic fields of neutron stars to be estimated. "P" and "P"-dot can be also used to calculate the "characteristic age" of a pulsar, but gives an estimate which is somewhat larger than the true age when it is applied to young pulsars.

"P" and "P"-dot can also be combined with neutron star's moment of inertia to estimate a quantity called "spin-down luminosity", which is given the symbol formula_9 ("E"-dot). It is not the measured luminosity, but rather the calculated loss rate of rotational energy that would manifest itself as radiation. For neutron stars where the spin-down luminosity is comparable to the actual luminosity, the neutron stars are said to be "rotation powered". The observed luminosity of the Crab Pulsar is comparable to the spin-down luminosity, supporting the model that rotational kinetic energy powers the radiation from it. With neutron stars such as magnetars, where the actual luminosity exceeds the spin-down luminosity by about a factor of one hundred, it is assumed that the luminosity is powered by magnetic dissipation, rather than being rotation powered.

"P" and "P"-dot can also be plotted for neutron stars to create a "P"–"P"-dot diagram. It encodes a tremendous amount of information about the pulsar population and its properties, and has been likened to the Hertzsprung–Russell diagram in its importance for neutron stars.

Neutron star rotational speeds can increase, a process known as spin up. Sometimes neutron stars absorb orbiting matter from companion stars, increasing the rotation rate and reshaping the neutron star into an oblate spheroid. This causes an increase in the rate of rotation of the neutron star of over a hundred times per second in the case of millisecond pulsars.

The most rapidly rotating neutron star currently known, PSR J1748-2446ad, rotates at 716 revolutions per second. A 2007 paper reported the detection of an X-ray burst oscillation, which provides an indirect measure of spin, of 1122 Hz from the neutron star XTE J1739-285, suggesting 1122 rotations a second. However, at present, this signal has only been seen once, and should be regarded as tentative until confirmed in another burst from that star.

Sometimes a neutron star will undergo a glitch, a sudden small increase of its rotational speed or spin up. Glitches are thought to be the effect of a starquake—as the rotation of the neutron star slows, its shape becomes more spherical. Due to the stiffness of the "neutron" crust, this happens as discrete events when the crust ruptures, creating a starquake similar to earthquakes. After the starquake, the star will have a smaller equatorial radius, and because angular momentum is conserved, its rotational speed has increased.

Starquakes occurring in magnetars, with a resulting glitch, is the leading hypothesis for the gamma-ray sources known as soft gamma repeaters.

Recent work, however, suggests that a starquake would not release sufficient energy for a neutron star glitch; it has been suggested that glitches may instead be caused by transitions of vortices in the theoretical superfluid core of the neutron star from one metastable energy state to a lower one, thereby releasing energy that appears as an increase in the rotation rate.

An "anti-glitch", a sudden small decrease in rotational speed, or spin down, of a neutron star has also been reported. It occurred in the magnetar 1E 2259+586, that in one case produced an X-ray luminosity increase of a factor of 20, and a significant spin-down rate change. Current neutron star models do not predict this behavior. If the cause was internal, it suggests differential rotation of solid outer crust and the superfluid component of the magnetar's inner structure.

At present, there are about 2,000 known neutron stars in the Milky Way and the Magellanic Clouds, the majority of which have been detected as radio pulsars. Neutron stars are mostly concentrated along the disk of the Milky Way, although the spread perpendicular to the disk is large because the supernova explosion process can impart high translational speeds (400 km/s) to the newly formed neutron star.

Some of the closest known neutron stars are RX J1856.5−3754, which is about 400 light-years from Earth, and PSR J0108−1431 about 424 light years. RX J1856.5-3754 is a member of a close group of neutron stars called The Magnificent Seven. Another nearby neutron star that was detected transiting the backdrop of the constellation Ursa Minor has been nicknamed Calvera by its Canadian and American discoverers, after the villain in the 1960 film "The Magnificent Seven". This rapidly moving object was discovered using the ROSAT/Bright Source Catalog.

Neutron stars are only detectable with modern technology during the earliest stages of their lives (almost always less than 1 million years) and are vastly outnumbered by older neutron stars that would only be detectable through their blackbody radiation and gravitational effects on other stars.

About 5% of all known neutron stars are members of a binary system. The formation and evolution of binary neutron stars can be a complex process. Neutron stars have been observed in binaries with ordinary main-sequence stars, red giants, white dwarfs, or other neutron stars. According to modern theories of binary evolution, it is expected that neutron stars also exist in binary systems with black hole companions. The merger of binaries containing two neutron stars, or a neutron star and a black hole, has been observed through the emission of gravitational waves.

Binary systems containing neutron stars often emit X-rays, which are emitted by hot gas as it falls towards the surface of the neutron star. The source of the gas is the companion star, the outer layers of which can be stripped off by the gravitational force of the neutron star if the two stars are sufficiently close. As the neutron star accretes this gas, its mass can increase; if enough mass is accreted, the neutron star may collapse into a black hole.

The distance between two neutron stars in a close binary system is observed to shrink as gravitational waves are emitted. Ultimately, the neutron stars will come into contact and coalesce.
The coalescence of binary neutron stars is one of the leading models for the origin of short gamma-ray bursts. Strong evidence for this model came from the observation of a kilonova associated with the short-duration gamma-ray burst GRB 130603B, and finally confirmed by detection of gravitational wave GW170817 and short GRB 170817A by LIGO, Virgo, and 70 observatories covering the electromagnetic spectrum observing the event. The light emitted in the kilonova is believed to come from the radioactive decay of material ejected in the merger of the two neutron stars. This material may be responsible for the production of many of the chemical elements beyond iron, as opposed to the supernova nucleosynthesis theory.

Neutron stars can host exoplanets. These can be original, circumbinary, captured, or the result of a second round of planet formation. Pulsars can also strip the atmosphere off from a star, leaving a planetary-mass remnant, which may be understood as a chthonian planet or a stellar object depending on interpretation. For pulsars, such pulsar planets can be detected with the pulsar timing method, which allows for high precision and detection of much smaller planets than with other methods. Two systems have been definitively confirmed. The first exoplanets ever to be detected were the three planets Draugr, Poltergeist and Phobetor around PSR B1257+12, discovered in 1992–1994. Of these, Draugr is the smallest exoplanet ever detected, at a mass of twice that of the Moon. Another system is PSR B1620−26, where a circumbinary planet orbits a neutron star-white dwarf binary system. Also, there are several unconfirmed candidates. Pulsar planets receive little visible light, but massive amounts of ionizing radiation and high-energy stellar wind, which makes them rather hostile environments.

At the meeting of the American Physical Society in December 1933 (the proceedings were published in January 1934), Walter Baade and Fritz Zwicky proposed the existence of neutron stars, less than two years after the discovery of the neutron by James Chadwick. In seeking an explanation for the origin of a supernova, they tentatively proposed that in supernova explosions ordinary stars are turned into stars that consist of extremely closely packed neutrons that they called neutron stars. Baade and Zwicky correctly proposed at that time that the release of the gravitational binding energy of the neutron stars powers the supernova: "In the supernova process, mass in bulk is annihilated". Neutron stars were thought to be too faint to be detectable and little work was done on them until November 1967, when Franco Pacini pointed out that if the neutron stars were spinning and had large magnetic fields, then electromagnetic waves would be emitted. Unbeknown to him, radio astronomer Antony Hewish and his research assistant Jocelyn Bell at Cambridge were shortly to detect radio pulses from stars that are now believed to be highly magnetized, rapidly spinning neutron stars, known as pulsars.

In 1965, Antony Hewish and Samuel Okoye discovered "an unusual source of high radio brightness temperature in the Crab Nebula". This source turned out to be the Crab Pulsar that resulted from the great supernova of 1054.

In 1967, Iosif Shklovsky examined the X-ray and optical observations of Scorpius X-1 and correctly concluded that the radiation comes from a neutron star at the stage of accretion.

In 1967, Jocelyn Bell Burnell and Antony Hewish discovered regular radio pulses from PSR B1919+21. This pulsar was later interpreted as an isolated, rotating neutron star. The energy source of the pulsar is the rotational energy of the neutron star. The majority of known neutron stars (about 2000, as of 2010) have been discovered as pulsars, emitting regular radio pulses.

In 1971, Riccardo Giacconi, Herbert Gursky, Ed Kellogg, R. Levinson, E. Schreier, and H. Tananbaum discovered 4.8 second pulsations in an X-ray source in the constellation Centaurus, Cen X-3. They interpreted this as resulting from a rotating hot neutron star. The energy source is gravitational and results from a rain of gas falling onto the surface of the neutron star from a companion star or the interstellar medium.

In 1974, Antony Hewish was awarded the Nobel Prize in Physics "for his decisive role in the discovery of pulsars" without Jocelyn Bell who shared in the discovery.

In 1974, Joseph Taylor and Russell Hulse discovered the first binary pulsar, PSR B1913+16, which consists of two neutron stars (one seen as a pulsar) orbiting around their center of mass. Albert Einstein's general theory of relativity predicts that massive objects in short binary orbits should emit gravitational waves, and thus that their orbit should decay with time. This was indeed observed, precisely as general relativity predicts, and in 1993, Taylor and Hulse were awarded the Nobel Prize in Physics for this discovery.

In 1982, Don Backer and colleagues discovered the first millisecond pulsar, PSR B1937+21. This object spins 642 times per second, a value that placed fundamental constraints on the mass and radius of neutron stars. Many millisecond pulsars were later discovered, but PSR B1937+21 remained the fastest-spinning known pulsar for 24 years, until PSR J1748-2446ad (which spins more than 700 times a second) was discovered.

In 2003, Marta Burgay and colleagues discovered the first double neutron star system where both components are detectable as pulsars, PSR J0737−3039. The discovery of this system allows a total of 5 different tests of general relativity, some of these with unprecedented precision.

In 2010, Paul Demorest and colleagues measured the mass of the millisecond pulsar PSR J1614−2230 to be , using Shapiro delay. This was substantially higher than any previously measured neutron star mass (, see PSR J1903+0327), and places strong constraints on the interior composition of neutron stars.

In 2013, John Antoniadis and colleagues measured the mass of PSR J0348+0432 to be , using white dwarf spectroscopy. This confirmed the existence of such massive stars using a different method. Furthermore, this allowed, for the first time, a test of general relativity using such a massive neutron star.

In August 2017, LIGO and Virgo made first detection of gravitational waves produced by colliding neutron stars.

In October 2018, astronomers reported that GRB 150101B, a gamma-ray burst event detected in 2015, may be directly related to the historic GW170817 and associated with the merger of two neutron stars. The similarities between the two events, in terms of gamma ray, optical and x-ray emissions, as well as to the nature of the associated host galaxies, are "striking", suggesting the two separate events may both be the result of the merger of neutron stars, and both may be a kilonova, which may be more common in the universe than previously understood, according to the researchers.

In July 2019, astronomers reported that a new method to determine the Hubble constant, and resolve the discrepancy of earlier methods, has been proposed based on the mergers of pairs of neutron stars, following the detection of the neutron star merger of GW170817. Their measurement of the Hubble constant is (km/s)/Mpc.







</doc>
<doc id="21871" url="https://en.wikipedia.org/wiki?curid=21871" title="Nassau, Bahamas">
Nassau, Bahamas

Nassau () is the capital and largest city of The Bahamas. With a population of 274,400 as of 2016, or just over 70% of the entire population of the Bahamas (≈391,000), Nassau is commonly defined as a primate city, dwarfing all other towns in the country. It is the centre of commerce, education, law, administration and media of the country.

Lynden Pindling International Airport, the major airport for the Bahamas, is located about west of the city centre of Nassau, and has daily flights to major cities in Canada, the Caribbean, the United Kingdom and the United States. The city is located on the island of New Providence, which functions much like a business district.

Nassau is the site of the House of Assembly and various judicial departments and was considered historically to be a stronghold of pirates. The city was named in honour of William III of England, Prince of Orange-Nassau.

Nassau's modern growth began in the late eighteenth century, with the influx of thousands of Loyalists and their slaves to the Bahamas following the American War of Independence. Many of them settled in Nassau (then and still the commerce capital of the Bahamas) and eventually came to outnumber the original inhabitants.

As the population of Nassau grew, so did its populated areas. Today the city dominates the entire island and its satellite, Paradise Island. However, until the post-Second World War era, the outer suburbs scarcely existed. Most of New Providence was uncultivated bush until Loyalists were resettled there following the American Revolutionary War; they established several plantations, such as Clifton and Tusculum. Slaves were imported as labour.

After the British abolished the international slave trade in 1807, they resettled thousands of Africans liberated from slave ships by the Royal Navy on New Providence (at Adelaide Village and Gambier Village), along with other islands such as Grand Bahama, Exuma, Abaco and Inagua. In addition, slaves freed from American ships, such as the Creole case in 1841, were allowed to settle there. The largest concentration of Africans historically lived in the "Over-the-Hill" suburbs of Grants Town and Bain Town to the south of the city of Nassau, while most of the inhabitants of European descent lived on the island's northern coastal ridges.

The town that would be called Nassau was founded in 1670 by British noblemen who brought British settlers with them to New Providence. They built a fort, and named it Charles Town in honour of England’s King Charles II. During this time there were frequent wars with the Spanish, and Charles Town was used as a base for privateering against them. In 1684 the town was burned to the ground during the Raid on Charles Town. It was rebuilt in 1695 under Governor Nicholas Trott and renamed Nassau in honour of William of Orange. William was the Dutch Stadtholder ("stadhouder" in Dutch), and after 1689 he was William III, the King of England, Scotland and Ireland. William belonged to a branch of the House of Nassau, from which the city takes its name. The name Nassau ultimately derives from the town of Nassau in Germany.

Lacking effective governors after Trott, Nassau fell on hard times. In 1703 Spanish and French allied forces briefly occupied Nassau. More so, Nassau suffered greatly during the War of Spanish Succession and had witnessed Spanish incursions during 1703, 1704 and 1706. From 1703 to 1718 there was no legitimate governor in the colony. Thomas Walker was the island's last remaining appointed official and although evidence is scarce, it appears that he was acting in the role of deputy governor upon Benjamin Hornigold's arrival in 1713. By this time, the sparsely settled Bahamas had become a pirate haven known as New Providence. The Governor of Bermuda stated that there were over 1,000 pirates in Nassau and that they outnumbered the mere hundred inhabitants of the town. They proclaimed Nassau a pirate republic, recognising the island's prosperous state in which it offered fresh fruit, meat and water and plenty of protection amid its waterways. Nassau's harbour was tailor-made for defence and it could take around 500 vessels, though it was too shallow to accept large battleships. Benjamin Hornigold, along with his great rival Henry Jennings, became the unofficial overlord of a veritable pirate republic which played host to the self-styled Flying Gang. Examples of pirates that used Nassau as their base are Charles Vane, Thomas Barrow (who declared himself "Governor of New Providence"), Benjamin Hornigold, Calico Jack Rackham, Anne Bonny, Mary Read, and the infamous Edward Teach, better known as "Blackbeard".

In 1718, the British sought to regain control of the islands and appointed Captain Woodes Rogers as Royal governor. He successfully clamped down on the pirates, reformed the civil administration, and restored commerce. Rogers cleaned up Nassau and rebuilt the fort, using his own wealth to try to overcome problems. In 1720, the Spanish attacked Nassau but failed to capture the town and the island.

During the wars in the Thirteen Colonies, Nassau experienced an economic boom. With funds from privateering, a new fort, street lights and over 2300 sumptuous houses were built and Nassau was extended. In addition to this, mosquito breeding swamps were filled.

In 1776, the Battle of Nassau resulted in a brief occupation by American Continental Marines during the American War of Independence, where the Marines staged their first amphibious raid on Fort Montague after attempting to sneak up on Fort Nassau. In 1778 after an overnight invasion, American raiders led by Captain Rathburn, left with ships, gunpowder and military stores after stopping in Nassau for only two weeks. In 1782 Spain captured Nassau for the last time when Don Juan de Cagigal, governor-general of Cuba, attacked New Providence with 5,000 men. Andrew Deveaux, an American Loyalist who resettled on the island, set forth and recaptured the island for the British Crown with just 220 men and 150 muskets to face a force of 600 trained soldiers.

Lord Dunmore governed the colony from 1787 to 1796. He oversaw the construction of Fort Charlotte and Fort Fincastle in Nassau.

During the American Civil War, Nassau served as a port for blockade runners making their way to and from ports along the southern Atlantic Coast for continued trade with the Confederacy.

In the 1920s and 1930s, Nassau profited from Prohibition in the United States.

Located on New Providence Island, Nassaus harbour has a blend of old world and colonial architecture, and a busy port. The tropical climate and natural environment of the Bahamas have made Nassau an attractive tourist destination.

Nassau developed directly behind the port area. New Providence provides 200 km² of relatively flat and low-lying land intersected by low ridges (none of which restricted settlement). In the centre of the island there are several shallow lakes that are tidally connected.

The city's proximity to the United States (290 km east-southeast of Miami, Florida) has contributed to its popularity as a holiday resort, especially after the United States imposed a ban on travel to Cuba in 1963. The Atlantis resort on nearby Paradise Island accounts for more tourist arrivals to the city than any other hotel property of Nassau. The mega-resort employs over 6,000 Bahamians, and is the largest employer outside government.

Nassau has a tropical savanna climate (Köppen: "Aw"), bordering on a tropical monsoon climate (Köppen: "Am"), with hot wet summers, and mild dry winters. Temperatures are relatively consistent throughout the course of the year. During the wet season from May through October, average daytime high temperatures are , while during the dry season from November through April daytime temperatures are between , rarely falling below .

During the 19th century, Nassau became urbanized, attracting rural residents. Growth since the 1950s has been outwards from the town. The 1788 heart of Nassau was just a few blocks of buildings between Government House and the harbour, but the town gradually expanded east to Malcolm's Park, south to Wulff Road, and west to Nassau Street. Grants Town and Bain Town south of the city became the main residential areas for those of African descent, and until about 30 years ago was the most populous part of the city.

Those of European descent built houses along the shore, east as far as Fort Montagu, west as far as Saunders Beach, and along the ridge edging the city. During the 20th century, the city spread east to Village Road and west to Fort Charlotte and Oakes Field. This semicircle of residential development was the main area of settlement until after the Second World War, and marks a distinct phase in the city's expansion, the outer boundary to this zone being the effective limit of the continuous built-up area. The wealthier residents continued to spread east (to East End Point) and West (to Lyford Cay).

In the last 40 years, residential development has been quite different. It has consisted mainly of planned middle-income sub-divisions. Since the 1960s, government has sponsored low-cost housing developments at Yellow Elder, Elizabeth Estates, and Pinewood Gardens, in the outer ring.

The city centre is the hub for all activities in Nassau. Thousands of people visit daily, to shop, dine, sightsee and to enjoy the tropical climate of the city. While the busiest part of central city is the Bay Street thoroughfare and the Woodes Rogers Walk, located across the street from the port and parallel to Bay, the area extends for several blocks in each direction. It starts at West Bay, around the Junkanoo Beach area. A few hotels and restaurants are located on West Bay.

The next landmark is the British Colonial Hotel, which marks the beginning of Bay Street proper. Pirates of Nassau Museum is just across from the British Colonial Hilton. The next few blocks of Bay Street are wall-to-wall boutiques, with a few restaurants and clubs interspersed throughout the retailers.

Historical landmarks are also in the vicinity, including Vendue House, Christ Church Cathedral, and the Nassau Public Library. Although the tourist part of the city centre peters out after about seven blocks, smaller, more local shops are located down Bay Street. At this point, Bay Street becomes East Bay.

The Straw Market is a tourist destination in the city centre. A new market was opened in 2011 after a fire in 2001 destroyed the original Fish, Vegetable and Straw Market. The market is open on all sides, and contains a number of Bahamian craft stores.

Cable Beach is recognised as the hotel district of Nassau. Five hotels—two of which are all-inclusive—are located on this strip. The area is also known for its dining, the Crystal Palace Casino, and the golden sands of Cable Beach. Most of the area's restaurants are located either in the hotels or across the street. There is little to no nightlife. There is a bit of shopping, most of it located in the Wyndham. The commercial future of Cable Beach is being re-imagined with the development of Baha Mar, a resort and casino project that will bring more than 2,000 hotel rooms and the largest gaming and convention facility in the Caribbean to this section of New Providence Island. As of April 2017, it is officially open, but not yet complete.

Nassau had a population of 128,420 females and 117,909 males and was home to 70,222 households with an average family size of 3.5 according to the 2010 census. Nassau's large population in relation to the remainder of the Bahamas is the result of waves of immigration from the Family Islands to the capital. Consequently, this has led to the decline in the population of the lesser developed islands and the rapid growth of Nassau.

In January 2018, the U.S. Department of State issued the latest in a series of travel advisories due to violent crime. Tourists are often targeted, and armed robbery has increased on all of New Providence.

Lynden Pindling International Airport (formerly Nassau International Airport) is located on the western side of Nassau. New Providence Airport on Paradise Island was closed in 1999 with runway removed and integrated into the resort on the island.

Ferries (boats) provide water travel around Nassau to the surrounding islands, namely Paradise Island. Prince George Wharf is the main port in the city that serves cruise ships with ports of call in Nassau. Transportation and shipping around the Family Islands is primarily through mailboats based at Potters Cay. International shipping is done through the Arawak Port Department on Arawak Cay. High speed excursions to Exuma, Spanish Wells and Harbour Island are available daily.

Public jitney buses and taxis provide transport in and around Nassau. Rental cars are also available in the city and at the airport.

Major roads in Nassau include:


The major road in Nassau is Bay Street for tourists. Bay Street runs the entire length of the Island from East to West. Bay Street also provides beachfront views. The downtown area and the cruise ships are in walking distance.

The Bahamas is a left-hand traffic country, but many cars are imported from the US are left-hand drive.

Nassau has been recognized as a part of the UNESCO Creative Cities Network as a city of Crafts and Folk Art. It is one of only three Caribbean cities to receive this honour.

The city's chief festival is Junkanoo, an energetic, colourful street parade of brightly costumed people dancing to the rhythmic accompaniment of cowbells, drums and whistles. The word 'Junkanoo' is named after the founder 'John Kanoo'. The celebration occurs on December 26, July 10 and January 1, beginning in the early hours of the morning (1:00 a.m.) and ending around 10 a.m. At the end of the Junkanoo procession, judges award cash prizes for the best music, costumes, and overall group presentation. These Bahamians spend all year preparing their handmade costumes by using coloured crepe paper and cardboard.

Nassau was the main location (however, the filming locations were based around South Africa) for the Starz Network show "Black Sails" (2014-2017).

Nassau was featured as an important location in several movies, including the Beatles film "Help!" and the James Bond films "Thunderball", (1965) and "Never Say Never Again", (a remake of "Thunderball") (1983) and also for part of the action in "Casino Royale" (2006). In 1981, it was used as a location for the ocean scene (in the film portrayed as being in Greece) in "For Your Eyes Only."

Several other late-20th- and 21st-century movies have been set here, including "After the Sunset", "Into the Blue" (2005), and "Flipper" (1996).

It hosted the Miss Universe 2009 pageant.

Nassau was featured as a primary location in the 2013 video game "" (2013).

Nassau Town is mentioned in Sloop John B, a Bahamian folk song. Since the early 1950s there have been many recordings of the song, the best known being by The Beach Boys on their Pet Sounds album.

Nassau has six sister cities worldwide:





</doc>
<doc id="21873" url="https://en.wikipedia.org/wiki?curid=21873" title="Nastassja Kinski">
Nastassja Kinski

Nastassja Aglaia Kinski (née Nakszynski; born 24 January 1961) is a German actress and former model who has appeared in more than 60 films in Europe and the United States. Her worldwide breakthrough was with "Stay as You Are" (1978). She then came to global prominence with her Golden Globe Award-winning performance as the title character in the Roman Polanski-directed film "Tess" (1979). Other notable films in which she acted include the erotic horror film "Cat People" (1982), the Wim Wenders dramas "Paris, Texas" (1984) and "Faraway, So Close!" (1993), and the biographical drama film, "An American Rhapsody" (2001). Kinski is fluent in four languages: German, English, French and Italian. She is the daughter of German actor Klaus Kinski.

Kinski was born in West Berlin as Nastassja Aglaia Nakszynski. She is the daughter of renowned German actor Klaus Kinski and his second wife, actress Ruth Brigitte Tocki. She is of partial Polish descent, for her grandfather Bruno Nakszynski was a Germanized ethnic Pole. Kinski has two half-siblings: Pola and Nikolai Kinski. Her parents divorced in 1968. After the age of 10, Kinski rarely saw her father. Her mother struggled financially to support them; they eventually lived in a commune in Munich.

In a 1999 interview, Kinski denied that her father had molested her as a child, but said he had abused her "in other ways". In 2013, when interviewed about the allegations of sexual abuse made by her half-sister Pola Kinski, she confirmed that he attempted with her, but did not succeed. She said, "He was no father. Ninety-nine percent of the time I was terrified of him. He was so unpredictable that the family lived in constant terror." When asked what she would say to him now, if she had the chance, she replied, "I would do anything to put him behind bars for life. I am glad he is no longer alive."

Kinski began working as a model as a teenager in Germany. Actress Lisa Kreuzer of the German New Wave helped get her the role of the mute Mignon in Wim Wenders 1975 film "The Wrong Move", in which at the age of 13 she was depicted topless. She later played one of the leading roles in Wenders' film "Paris, Texas" (1984) and appeared in his "Faraway, So Close" (1993).

In 1976, while still a teenager, Kinski had her first two major roles: in Wolfgang Petersen's feature film-length episode "Reifezeugnis" of the German TV crime series "Tatort." Next, she appeared in the British horror film "To the Devil a Daughter" (1976), produced by Hammer Film Productions, which was released in the UK just 40 days after Kinski's fifteenth birthday, making it a virtual certainty she was only fourteen when her scenes were shot (including full frontal nudity). In regards to her early films, Kinski has stated that she felt exploited by the industry. In an interview with "W", she said, "If I had had somebody to protect me or if I had felt more secure about myself, I would not have accepted certain things. Nudity things. And inside it was just tearing me apart."

In 1978 Kinski starred in the Italian romance "Stay as You Are" ("Così come sei") with Marcello Mastroianni, gaining her recognition in the United States after New Line Cinema released it there in December 1979. "Time" wrote that she was "simply ravishing, genuinely sexy and high-spirited without being painfully aggressive about it." The film also received a major international release from Columbia Pictures.

Kinski met the director Roman Polanski at a party in 1976. He urged her to study method acting with Lee Strasberg in the United States and she was offered the title role in Polanski's upcoming film, "Tess" (1979). In 1978, Kinski underwent extensive preparation for the portrayal of an English peasant girl, which included acquiring a Dorset accent through elocution studies:

The film was nominated for six awards, including Best Picture, at the 53rd Academy Awards, and won three.

In 1981 Richard Avedon photographed Kinski with a Burmese python coiled around her nude body. The image, which first appeared in the October 1981 issue of US "Vogue", was released as a poster and became a best-seller, further confirming her status as a sex symbol.

In 1982 she starred in Francis Ford Coppola's romantic musical "One from the Heart", her first film made in the United States. "Texas Monthly" described her as acting "as a Felliniesque circus performer to represent the twinkling evanescence of Eros." The film failed at the box office and was a major loss for Coppola's new Zoetrope Studios. That year, she was also in the erotic horror movie "Cat People". Dudley Moore's comedy "Unfaithfully Yours" and an adaptation of John Irving's "The Hotel New Hampshire" followed in 1984.

Kinski reteamed with Wenders for the 1984 film "Paris, Texas". One of her most acclaimed films to date, it won the top award at the Cannes Film Festival. Throughout the 1980s, Kinski split her time between Europe and the United States, making "Moon in the Gutter" (1983), "Harem" (1985) and "Torrents of Spring" (1989) in Europe, and "Exposed" (1983), "Maria's Lovers" (1984), and "Revolution" (1985) in the United States.

During the 1990s Kinski appeared in a number of American films, including the action movie "Terminal Velocity" opposite Charlie Sheen, the Mike Figgis 1997 adultery tale "One Night Stand", "Your Friends & Neighbors" (1998), John Landis's "Susan's Plan" (1998), and "The Lost Son" (1999).

Her most recent films include David Lynch's "Inland Empire" (2006) and Rotimi Rainwater's "Sugar" (2013). In 2016, she competed in the German "Let's Dance" show.

In 1976, when Kinski was aged 15, she reportedly began a romantic relationship with director Roman Polanski, who at the time was 43. Polanski confirmed the relationship in a 1994 interview with Diane Sawyer: "...what about Nastassja Kinski? She was young and we had a love affair." However, in a 1999 interview in "The Guardian", Kinski was quoted as saying that there was no affair and that, "There was a flirtation. There could have been a seduction, but there was not. He had respect for me."

In the late 1970s, Kinski was roommates with a pre-fame Demi Moore. In her 2019 memoir "Inside Out", Moore wrote: "We know each other in a way that no one else could."

Kinski has three children by three different men. Her first child, son Aljosha Nakszynski (born 29 June 1984), was fathered by actor Vincent Spano, her co-star in "Maria's Lovers". On 10 September 1984, Kinski married Egyptian filmmaker Ibrahim Moussa, with whom she had daughter Sonja Kinski (born 2 March 1986). The marriage was dissolved in July 1992. From 1992 until 1995, Kinski lived with musician Quincy Jones, though she kept her own apartment on Hilgard Avenue, near UCLA, at the time. They had a daughter, Kenya Julia Niambi Sarah Jones (born 9 February 1993), a model known professionally as Kenya Kinski-Jones.

In 1997, Kinski dated married producer Jonathan D. Krane during a brief separation from his wife, actress Sally Kellerman. Over the course of her career, Kinski has also been romantically linked with Paul Schrader, Jean-Jacques Beineix, Rob Lowe, Jon Voight, Gérard Depardieu, Dudley Moore, Miloš Forman and Wim Wenders. As of 2012, she was dating actor Rick Yune.

In 2001 Kinski stated in an interview in "The Daily Telegraph" that she was affected by the sleep disorder narcolepsy.




</doc>
<doc id="21875" url="https://en.wikipedia.org/wiki?curid=21875" title="Nuremberg trials">
Nuremberg trials

The Nuremberg trials () were a series of military tribunals held after World War II by the Allied forces under international law and the laws of war. The trials were most notable for the prosecution of prominent members of the political, military, judicial, and economic leadership of Nazi Germany, who planned, carried out, or otherwise participated in the Holocaust and other war crimes. The trials were held in Nuremberg, Germany, and their decisions marked a turning point between classical and contemporary international law.

The first and best known of the trials was that of the major war criminals before the International Military Tribunal (IMT). It was described as "the greatest trial in history" by Sir Norman Birkett, one of the British judges present throughout. Held between 20 November 1945 and 1 October 1946, the Tribunal was given the task of trying 24 of the most important political and military leaders of the Third Reich. Primarily treated here is the first trial, conducted by the International Military Tribunal. Further trials of lesser war criminals were conducted under Control Council Law No. 10 at the U.S. Nuremberg Military Tribunal (NMT), which included the Doctors' trial and the Judges' Trial.

The categorization of the crimes and the constitution of the court represented a juridical advance that would be followed afterward by the United Nations for the development of an international jurisprudence in matters of war crimes, crimes against humanity, and wars of aggression, and led to the creation of the International Criminal Court. For the first time in international law, the Nuremberg indictments also mention genocide (count three, war crimes: "the extermination of racial and national groups, against the civilian populations of certain occupied territories to destroy particular races and classes of people and national, racial, or religious groups, particularly Jews, Poles, and Gypsies and others.")

A precedent for trying those accused of war crimes had been set at the end of World War I in the Leipzig War Crimes Trials held in May to July 1921 before the "Reichsgericht" (German Supreme Court) in Leipzig, although these had been on a very limited scale and largely regarded as ineffectual. At the beginning of 1940, the Polish government-in-exile asked the British and French governments to condemn the German invasion of their country. The British initially declined to do so; however, in April 1940, a joint declaration was issued by the British, French, and Polish. Relatively bland because of Anglo-French reservations, it proclaimed the trio's ""desire to make a formal and public protest to the conscience of the world against the action of the German government whom they must hold responsible for these crimes which cannot remain unpunished.""

Three-and-a-half years later, the stated intention to punish the Germans was much more trenchant. On 1 November 1943, the Soviet Union, the United Kingdom, and the United States published their "Declaration on German Atrocities in Occupied Europe", which gave a "full warning" that, when the Nazis were defeated, the Allies would "pursue them to the uttermost ends of the earth ... so that justice may be done. ... The above declaration is without prejudice to the case of the major war criminals whose offenses have no particular geographical location and who will be punished by a joint decision of the Government of the Allies." This intention by the Allies to dispense justice was reiterated at the Yalta Conference and at Potsdam in 1945.

British War Cabinet documents, released on 2 January 2006, showed that as early as December 1944 the Cabinet had discussed their policy for the punishment of the leading Nazis if captured. The British Prime Minister, Winston Churchill, had then advocated a policy of summary execution in some circumstances, with the use of an Act of Attainder to circumvent legal obstacles, being dissuaded from this only by talks with US and Soviet leaders later in the war.
In late 1943, during the Tripartite Dinner Meeting at the Tehran Conference, the Soviet leader, Joseph Stalin, proposed executing 50,000–100,000 German staff officers. US President Franklin D. Roosevelt joked that perhaps 49,000 would do. Churchill, believing them to be serious, denounced the idea of "the cold-blooded execution of soldiers who fought for their country" and that he would rather be "taken out in the courtyard and shot" himself than partake in any such action. However, he also stated that war criminals must pay for their crimes and that, under the Moscow Document which he had written, they should be tried at the places where the crimes were committed. Churchill was vigorously opposed to executions "for political purposes." According to the minutes of a meeting between Roosevelt and Stalin at Yalta, on 4 February 1945, at the Livadia Palace, President Roosevelt "said that he had been very much struck by the extent of German destruction in Crimea and therefore he was more bloodthirsty in regard to the Germans than he had been a year ago, and he hoped that Marshal Stalin would again propose a toast to the execution of 50,000 officers of the German Army."

Henry Morgenthau Jr., US Secretary of the Treasury, suggested a plan for the total denazification of Germany; this was known as the Morgenthau Plan. The plan advocated the forced de-industrialisation of Germany and the summary execution of so-called "arch-criminals", i.e. the major war criminals. Roosevelt initially supported this plan, and managed to convince Churchill to support it in a less drastic form. Later, details were leaked in the US generating widespread condemnation by the nation's newspapers and propaganda was published about the plan in Germany. Roosevelt, aware of strong public disapproval, abandoned the plan, but did not adopt an alternative position on the matter. The demise of the Morgenthau Plan created the need for an alternative method of dealing with the Nazi leadership. The plan for the "Trial of European War Criminals" was drafted by Secretary of War Henry L. Stimson and the War Department. Following Roosevelt's death in April 1945, the new president, Harry S. Truman, gave strong approval for a judicial process. After a series of negotiations between Britain, the US, the Soviet Union, and France, details of the trial were worked out. The trials were to commence on 20 November 1945, in the Bavarian city of Nuremberg.

On 20 April 1942, representatives from the nine countries occupied by Germany met in London to draft the "Inter-Allied Resolution on German War Crimes". At the meetings in Tehran (1943), Yalta (1945), and Potsdam (1945), the three major wartime powers, the United Kingdom, United States, and the Soviet Union, agreed on the format of punishment for those responsible for war crimes during World War II. France was also awarded a place on the tribunal. The legal basis for the trial was established by the London Charter, which was agreed upon by the four so-called Great Powers on 8 August 1945, and which restricted the trial to "punishment of the major war criminals of the European Axis countries".

Some 200 German war crimes defendants were tried at Nuremberg, and 1,600 others were tried under the traditional channels of military justice. The legal basis for the jurisdiction of the court was that defined by the Instrument of Surrender of Germany. Political authority for Germany had been transferred to the Allied Control Council which, having sovereign power over Germany, could choose to punish violations of international law and the laws of war. Because the court was limited to violations of the laws of war, it did not have jurisdiction over crimes that took place before the outbreak of war on 1 September 1939.

Leipzig and Luxembourg were briefly considered as the location for the trial. The Soviet Union had wanted the trials to take place in Berlin, as the capital city of the 'fascist conspirators', but Nuremberg was chosen as the site for two reasons, the first being decisive:

As a compromise with the Soviets, it was agreed that while the location of the trial would be Nuremberg, Berlin would be the official home of the Tribunal authorities.<ref name="PRO-LCO-2/2980-1"></ref><ref name="PRO-LCO-2/2980-2"></ref> It was also agreed that France would become the permanent seat of the IMT and that the first trial (several were planned) would take place in Nuremberg.

Most of the accused had previously been detained at Camp Ashcan, a processing station and interrogation center in Luxembourg, and were moved to Nuremberg for the trial.
Each of the four countries provided one judge and an alternative, as well as a prosecutor.


Assisting Jackson were the lawyers Telford Taylor, William S. Kaplan and Thomas J. Dodd, and Richard Sonnenfeldt, a US Army interpreter. Assisting Shawcross were Major Sir David Maxwell-Fyfe and Sir John Wheeler-Bennett. Mervyn Griffith-Jones, who was later to become famous as the chief prosecutor in the "Lady Chatterley's Lover" obscenity trial, was also on Shawcross's team. Shawcross also recruited a young barrister, Anthony Marreco, who was the son of a friend of his, to help the British team with the heavy workload.

The vast majority of the defense attorneys were German lawyers. These included Georg Fröschmann, Heinz Fritz (Hans Fritzsche), Otto Kranzbühler (Karl Dönitz), Otto Pannenbecker (Wilhelm Frick), Alfred Thoma (Alfred Rosenberg), Kurt Kauffmann (Ernst Kaltenbrunner), Hans Laternser (general staff and high command), Franz Exner (Alfred Jodl), Alfred Seidl (Hans Frank), Otto Stahmer (Hermann Göring), Walter Ballas (Gustav Krupp von Bohlen und Halbach), Hans Flächsner (Albert Speer), Günther von Rohrscheidt (Rudolf Hess), Egon Kubuschok (Franz von Papen), Robert Servatius (Fritz Sauckel), Fritz Sauter (Joachim von Ribbentrop), Walther Funk (Baldur von Schirach), Hanns Marx (Julius Streicher), Otto Nelte (Wilhelm Keitel), and Herbert Kraus/Rudolph Dix (both working for Hjalmar Schacht). The main counsel were supported by a total of 70 assistants, clerks and lawyers. The defense witnesses included several men who took part in war crimes themselves during World War II, such as Rudolf Höss. The men testifying for the defense hoped to receive more lenient sentences. All of the men testifying on behalf of the defense were found guilty on several counts.

The International Military Tribunal was opened on 19 November 1945 in the Palace of Justice in Nuremberg. The first session was presided over by the Soviet judge, Nikitchenko. The prosecution entered indictments against 24 major war criminals and seven organizations – the leadership of the Nazi party, the Reich Cabinet, the Schutzstaffel (SS), Sicherheitsdienst (SD), the Gestapo, the Sturmabteilung (SA) and the "General Staff and High Command", comprising several categories of senior military officers. These organizations were to be declared "criminal" if found guilty.

The indictments were for:
The 24 accused were, with respect to each charge, either indicted but not convicted (I), indicted and found guilty (G), or not charged (—), as listed below by defendant, charge, and eventual outcome:


The accusers were successful in unveiling the background of developments leading to the outbreak of World War II, which cost around 50 million lives in Europe alone, as well as the extent of the atrocities committed in the name of the Hitler regime. Twelve of the accused were sentenced to death, seven received prison sentences (ranging from 10 years to life sentence), three were acquitted, and two were not charged.

The death sentences were carried out on 16 October 1946 by hanging using the standard drop method instead of long drop. The U.S. Army denied claims that the drop length was too short, which may cause the condemned to die slowly from strangulation instead of quickly from a broken neck, but evidence remains that some of the condemned men choked in agony for 14 to 28 minutes. The executioner was John C. Woods. The executions took place in the gymnasium of the court building (demolished in 1983).

Although the rumor has long persisted that the bodies were taken to Dachau and burned there, they were incinerated in a crematorium in Munich, and the ashes scattered over the river Isar. The French judges suggested that the military condemned (Göring, Keitel, and Jodl) be shot by a firing squad, as is standard for military courts-martial, but this was opposed by Biddle and the Soviet judges, who argued that the military officers had violated their military ethos and were not worthy of the more dignified death by shooting. The prisoners sentenced to incarceration were transferred to Spandau Prison in 1947.

Of the 12 defendants sentenced to death by hanging, two were not hanged: Martin Bormann was convicted in absentia (he had, unknown to the Allies, died while trying to escape from Berlin in May 1945), and Hermann Göring committed suicide the night before the execution. The remaining 10 defendants sentenced to death were hanged.

The definition of what constitutes a war crime is described by the Nuremberg principles, a set of guidelines document which was created as a result of the trial. The medical experiments conducted by German doctors and prosecuted in the so-called Doctors' Trial led to the creation of the Nuremberg Code to control the future trials involving human subjects, a set of research ethics principles for human experimentation.

Of the indicted organizations the following were found not to be criminal:


The American authorities conducted subsequent Nuremberg Trials in their occupied zone.

Other trials conducted after the first Nuremberg trial include the following:

While Sir Geoffrey Lawrence of Britain was the judge who was chosen to serve as the president of the court, arguably, the most prominent of the judges at the trial was his American counterpart, Francis Biddle. Before the trial, Biddle had been Attorney General of the United States but had been asked to resign by Truman earlier in 1945.

Some accounts argue that Truman had appointed Biddle as the main American judge for the trial as an apology for asking for his resignation. Ironically, Biddle was known during his time as Attorney General for opposing the idea of prosecuting Nazi leaders for crimes committed before the beginning of the war, even sending out a memorandum on 5 January 1945 on the subject. The note also expressed Biddle's opinion that instead of proceeding with the original plan for prosecuting entire organizations, there should simply be more trials that would prosecute specific offenders.

Biddle soon changed his mind, as he approved a modified version of the plan on 21 January 1945, likely due to time constraints, since the trial would be one of the main issues which were to be discussed at Yalta. At trial, the Nuremberg tribunal ruled that any member of an organization convicted of war crimes, such as the SS or Gestapo, who had joined after 1939 would be considered a war criminal. Biddle managed to convince the other judges to make an exemption for any member who was drafted or had no knowledge of the crimes being committed by these organizations.

Justice Robert H. Jackson played an important role in not only the trial itself but also in the creation of the International Military Tribunal, as he led the American delegation to London that, in the summer of 1945, argued in favor of prosecuting the Nazi leadership as a criminal conspiracy. According to Airey Neave, Jackson was also the one behind the prosecution's decision to include membership in any of the six criminal organizations in the indictments at the trial, though the IMT rejected this because it was wholly without precedent in either international law or the domestic laws of any of the Allies. Jackson also attempted to have Alfried Krupp be tried in place of his father, Gustav and even suggested that Alfried volunteer be tried in his father's place. Both proposals were rejected by the IMT, particularly by Lawrence and Biddle, and some sources indicate that this resulted in Jackson being viewed unfavorably by the latter.

Thomas Dodd was a prosecutor for the United States. There was an immense amount of evidence backing the prosecutors' case, especially since meticulous records of the Nazis' actions had been kept. There were records taken in by the prosecutors that had signatures from specific Nazis signing for everything from stationery supplies to Zyklon B gas, which was used to kill the inmates of the death camps. Thomas Dodd showed a series of pictures to the courtroom after reading through the documents of crimes committed by the defendants. The showing consisted of pictures displaying the atrocities performed by the defendants. The pictures had been gathered when the inmates were liberated from the concentration camps.

Henry F. Gerecke, a Lutheran pastor, and Sixtus O'Connor, a Roman Catholic priest, were sent to minister to the Nazi defendants. Photographs of the trial were taken by a team of about a dozen US Army still photographers, under the direction of chief photographer Ray D'Addario.

The Tribunal is celebrated for establishing that "crimes against international law are committed by men, not by abstract entities, and only by punishing individuals who commit such crimes can the provisions of international law be enforced." The creation of the IMT was followed by trials of lesser Nazi officials and the trials of Nazi doctors, who performed experiments on people in prison camps. It served as the model for the International Military Tribunal for the Far East which tried Japanese officials for crimes against peace and crimes against humanity. It also served as the model for the Eichmann trial and present-day courts at The Hague, for trying crimes committed during the Balkan wars of the early 1990s, and at Arusha, for trying the people responsible for the genocide in Rwanda.

The Nuremberg trials had a great influence on the development of international criminal law. The Conclusions of the Nuremberg trials served as models for:

The International Law Commission, acting on the request of the United Nations General Assembly, produced in 1950 the report "Principles of International Law Recognized in the Charter of the Nürnberg Tribunal and in the Judgement of the Tribunal" (Yearbook of the International Law Commission, 1950, vol. II). See Nuremberg Principles.

The influence of the tribunal can also be seen in the proposals for a permanent international criminal court, and the drafting of international criminal codes, later prepared by the International Law Commission.

Tourists can visit courtroom 600 on days when no trial is on. A permanent exhibition has been dedicated to the trials.

The Nuremberg trials initiated a movement for the prompt establishment of a permanent international criminal court, eventually leading over fifty years later to the adoption of the Statute of the International Criminal Court. This movement was brought about because, during the trials, there were conflicting court methods between the German court system and the U.S. court system. The crime of conspiracy was unheard of in the civil law systems of the Continent. Therefore, the German defense found it unfair to charge the defendants with conspiracy to commit crimes, while the judges from common-law countries were used to doing so.

Critics of the Nuremberg trials argued that the charges against the defendants were only defined as "crimes" after they were committed and that therefore the trial was invalid, and thus seen as a form of "victor's justice". 

As Michael Biddiss observed in a 1995 edition of "History Today", "the Nuremberg Trial continues to haunt us. ... It is a question also of the weaknesses and strengths of the proceedings themselves."

Quincy Wright, writing eighteen months after the conclusion of the IMT, explained the opposition to the Tribunal thus:Chief Justice of the United States Supreme Court Harlan Fiske Stone called the Nuremberg trials a fraud. "(Chief U.S. prosecutor) Jackson is away conducting his high-grade lynching party in Nuremberg, ... I don't mind what he does to the Nazis, but I hate to see the pretense that he is running a court and proceeding according to common law. This is a little too sanctimonious a fraud to meet my old-fashioned ideas", Stone wrote.

Jackson, in a letter discussing the weaknesses of the trial, in October 1945 told U.S. President Harry S. Truman that the Allies themselves "have done or are doing some of the very things we are prosecuting the Germans for. The French are so violating the Geneva Convention in the treatment of prisoners of war that our command is taking back prisoners sent to them. We are prosecuting plunder and our Allies are practicing it. We say aggressive war is a crime and one of our allies asserts sovereignty over the Baltic States based on no title except conquest."

Associate Supreme Court Justice William O. Douglas charged that the Allies were guilty of "substituting power for principle" at Nuremberg. "I thought at the time and still think that the Nuremberg trials were unprincipled," he wrote. "Law was created "ex post facto" to suit the passion and clamor of the time."

U.S. Deputy Chief Counsel Abraham Pomerantz resigned in protest at the low caliber of the judges assigned to try the industrial war criminals, such as those at I.G. Farben.

Robert A. Taft, a US Senate Majority Leader from Ohio and son of William Howard Taft, criticized the Nuremberg Trials for trying Nazi war criminals under "ex post facto" laws, which resulted in his failure to secure the Republican nomination for President in 1948.

Several Germans who agreed with the idea of punishment for war crimes admitted trepidation concerning the trials. A contemporary German jurist said:

The validity of the court has been questioned on several grounds:
One criticism that was made of the IMT was that some treaties were not binding on the Axis powers because they were not signatories. This was addressed in the judgment relating to war crimes and crimes against humanity, which contains an expansion of customary law: "the [Hague] Convention expressly stated that it was an attempt 'to revise the general laws and customs of war,' which is thus recognized to be then existing, but by 1939 these rules laid down in the Convention were recognized by all civilized nations and were regarded as being declaratory of the laws and customs of war which are referred to in Article 6 (b) of the [London] Charter."

The Nuremberg Trials employed four official languages: English, French, German and Russian. To address the complex linguistic issues that clouded over the proceedings, interpretation and translation departments had to be established. However, it was feared that consecutive interpretation would slow down the proceedings significantly. What is therefore unique in both the Nuremberg tribunals and history of the interpretation profession was the introduction of an entirely new technique, extempore simultaneous interpretation. This technique of interpretation requires the interpreter to listen to a speaker in a source (or passive) language and orally translate that speech into another language in real-time, that is, simultaneously, through headsets and microphones. Interpreters were split into four sections, one for each official language, with three interpreters per section working from the other three languages into the fourth (their mother tongue). For instance, the English booth consisted of three interpreters, one working from German into English, one working from French, and one from Russian, etc. Defendants who did not speak any of the four official languages were provided with consecutive court interpreters. Some of the languages heard over the course of the proceedings included Yiddish, Hungarian, Czech, Ukrainian, and Polish.

The equipment used to establish this system was provided by IBM and included an elaborate setup of cables that were hooked up to headsets and single earphones directly from the four interpreting booths (often referred to as "the aquarium"). Four channels existed for each working language, as well as a root channel for the proceedings without interpretation. The switching of channels was controlled by a setup at each table in which the listener merely had to turn a dial to switch between languages. People tripping over the floor-laid cables often led to the headsets getting disconnected, with several hours at a time sometimes being taken to repair the problem and continue with the trials.

Interpreters were recruited and examined by the respective countries in which the official languages were spoken: the United States, United Kingdom, France, the Soviet Union, Germany, Switzerland, and Austria, as well as in special cases Belgium and the Netherlands. Many were former translators, army personnel, and linguists, some were experienced consecutive interpreters, others were ordinary individuals and even recent secondary school-graduates who led international lives in multilingual environments. It was and still is believed, that the qualities that made the best interpreters were not just a perfect understanding of two or more languages, but more importantly a broad sense of culture, encyclopedic knowledge, inquisitiveness, as well as a naturally calm disposition.
With the simultaneous technique being extremely new, interpreters practically trained themselves, but many could not handle the pressure or the psychological strain. Many often had to be replaced, many returned to the translation department, and many left. Serious doubts were given as to whether interpretation provided a fair trial for the defendants, particularly because of fears of mistranslation and errors made on transcripts. The translation department had to also deal with the overwhelming problem of being understaffed and overburdened with an influx of documents that could not be kept up with. More often than not, interpreters were stuck in a session without having proper documents in front of them and were relied upon to do sight translation or double translation of texts, causing further problems and extensive criticism. Other problems that arose included complaints from lawyers and other legal professionals about questioning and cross-examination. Legal professionals were most often appalled at the slower speed at which they had to conduct their tasks because of the extended time required for interpreters to render an interpretation properly. Additionally, several interpreters protested the idea of using vulgar language, especially if it referred to Jews or the conditions of the Nazi concentration camps. Bilingual/trilingual members who attended the trials picked up quickly on this aspect of character and were equally quick to file complaints.

Yet, despite the extensive trial and error, without the interpretation system the trials would not have been possible and in turn, revolutionized the way multilingual issues were addressed in tribunals and conferences. A number of the interpreters following the trials were immediately recruited into the newly formed United Nations, while others returned to their ordinary lives, pursued other careers, or worked freelance. Outside the boundaries of the trials, many interpreters continued their positions on weekends interpreting for dinners, private meetings between judges, and excursions between delegates. Others worked as investigators or editors or aided the translation department when they could, often using it as an opportunity to sharpen their skills and to correct poor interpretations on transcripts before they were available for public record.

For further reference, a book titled "The Origins of Simultaneous Interpretation: The Nuremberg Trial", written by interpreter Francesca Gaiba was published by the University of Ottawa Press in 1998.

Today, all major international organizations, as well as any conference or government that uses more than one official language, uses extempore simultaneous interpretation. Notable bodies include the Parliament of Kosovo with three official languages, the Parliament of Canada with two official languages, the Parliament of South Africa with eleven official languages, the European Union with twenty-four official languages, and the United Nations with six official working languages.


These citations refer to documents at 





</doc>
<doc id="21876" url="https://en.wikipedia.org/wiki?curid=21876" title="Natasha Stott Despoja">
Natasha Stott Despoja

Natasha Jessica Stott Despoja AO (born 9 September 1969) is an Australian politician, diplomat, advocate and author. She is the founding Chair of the Board of Our Watch, the national foundation to prevent violence against women and their children, and was previously the Australian Ambassador for Women and Girls at the Department of Foreign Affairs and Trade from 2013 to 2016. She was also a Member of the World Bank Gender Advisory Council from 2015 to 2017 and a Member of the United Nations High Level Working Group on the Health and Human Rights of Women, Children and Adolescents in 2017.

Stott Despoja began her parliamentary career after being appointed to the Senate at the age of 26 serving as an Australian Democrats Senator for South Australia from 1995 to 2008. She went on to serve as the Deputy Leader and Leader of the Australian Democrats. She holds the record for being the youngest woman to sit in the Parliament of Australia and the longest serving Australian Democrats Senator.

Stott Despoja was born in Adelaide to Shirley Stott Despoja, an Australian-born journalist and Mario Despoja, who was from Croatia (then part of Yugoslavia). She attended Stradbroke Primary and Pembroke School and later graduated from the University of Adelaide in 1991. She was President of the Students' Association of the University of Adelaide (SAUA) and the South Australian Women's Officer for the National Union of Students. She then went on to work as a political advisor to Senator John Coulter and Senator Cheryl Kernot.

When John Coulter had to stand down for health reasons in 1995, Stott Despoja was the successful candidate to replace him. Her performance was recognized when she was re-elected not only in the 1996 election the following year, but again in the 2001 election. In 1997 she had been promoted to become the deputy leader of the Democrats from her position as party spokesperson for parliamentary portfolios such as Science and Technology, Higher Education, IT, Employment & Youth Affairs.

During the passage of the Goods and Services Tax (GST) legislation in 1999, Stott Despoja, along with Andrew Bartlett, split from the party's other senators by opposing the package, which had been negotiated by Lees and prime minister John Howard. She said that she refused to break promises made by the party during the election. The party had gone to the election stating that they would work with whichever party formed government to improve their tax package. The Australian Democrats traditionally permitted parliamentary representatives to cast a conscience vote on any issue but, on this occasion, close numbers in the Senate placed greater pressure than usual on the dissenters.

In 2004, Stott Despoja took 11 weeks' leave from the Senate following the birth of her first child before returning to full duties as Democrat spokesperson on, inter alia, Higher Education, Status of Women, and Work and Family.

During her political career she also introduced 24 Private Member's Bills on issues including paid maternity leave, the Republic, genetic privacy, stem cells, captioning and same sex marriage. Stott Despoja regularly attends the Sydney Gay and Lesbian Mardi Gras.

On 22 October 2006, after undergoing emergency surgery for an ectopic pregnancy, she announced that she would not be contesting the 2007 election to extend her term beyond 30 June 2008. She was the Australian Democrats' longest-serving senator. Her retirement coincided with the ending of her party's federal parliamentary representation; the Democrats' support had collapsed after 2002 and they won no seats at the 2004 and 2007 half-senate elections.

Stott Despoja became the leader of her party on 6 April 2001. The preceding leader Meg Lees left the party in the following year. Stott Despoja faced criticism with calm resolution from Democrat senators and the general public, but she opted to resign on 21 August 2002 after 16 months. She had been faced with little alternative after four of her six colleagues forced a ten-point reform agenda upon her. The agenda was proposed by John Cherry and she was opposed to its content. She announced her resignation in a speech to the Senate, concluding with a "pledge to bring the party back home to the members again", and referring to her colleagues' attitude towards her.

She was replaced as leader by Bartlett following a membership ballot interval during which Brian Greig acted in the position.

Stott Despoja has been a casual host on ABC 891 radio, a guest panellist on Channel 10's "The Project" and a columnist for the Australian business news website "Business Spectator".

She was a board member of non-profit organisations the South Australian Museum (SAM) from 2009 to 2013; the Museum of Australian Democracy (MOAD) from 2010 to 2013; and the Advertising Standards Board (ASB) from 2008 to 2013. She was a deputy chair at beyondblue (Australia's national depression initiative).
She has been an ambassador for Ovarian Cancer Australia (OCA), The Orangutan Project (TOP); Cancer Australia; secondbite; and the HIV/AIDS anti-stigma campaign, ENUF, (along with her husband Ian Smith).

She was on the board of the Burnet Institute (Australia's largest virology and communicable disease research institute) from 2008 until December 2013, when Foreign Minister Julie Bishop announced the appointment of Stott Despoja as Australia's new Ambassador for Women and Girls, a role she held until 2016. This involved visiting some 45 countries to promote women's economic empowerment and leadership and to help reduce violence against women and girls.

Stott Despoja has also been an election observer for the US-based National Democratic Institute (NDI) in Nigeria (2011); visited Burkina Faso for Oxfam (2012); and went to Laos (2011) and Burma (2013) with The Burnet Institute. She was mentioned in June 2014 as a possible replacement for Kevin Scarce as the next Governor of South Australia, however Hieu Van Le was chosen.

On 21 July 2015, Stott Despoja returned to the Burnet Institute as a Patron.

In July 2013, Stott Despoja was the founding chair of Our Watch, originally named Foundation to Prevent Violence Against Women and their Children, and still occupies this position. A joint initiative of the Victorian and Commonwealth Governments, the organisation is based in Melbourne.

Stott Despoja has authored a large number of essays, reports and non-fiction works on a range of topics, both during and since her political career.

In March 2019 she published "On Violence", with the publisher's blurb asking "Why is violence against women endemic, and how do we stop it?". Stott Despoja posits that violence against women is "Australia's national emergency", with one woman dying at the hands of her partner or someone she knows every week. This violence is preventable, and that we need to "create a new normal".

In 1999, she was appointed a Global Leader for Tomorrow by the World Economic Forum (WEF).

Despoja was awarded a Member of the Order of Australia in June 2011 for her "service to the Parliament of Australia, particularly as a Senator for South Australia, through leadership roles with the Australian Democrats, to education, and as a role model for women".

She is listed as one of the "Gender Equality Top 100" by the UK organisation Apolitical.

In June 2019 Despoja was appointed as an Officer of the Order of Australia for her "distinguished service to the global community as an advocate for gender equality, and through roles in a range of organisations"

Stott Despoja is married to former Liberal party advisor, Ian Smith and has two children.




 

 


</doc>
<doc id="21881" url="https://en.wikipedia.org/wiki?curid=21881" title="Nuremberg Code">
Nuremberg Code

The Nuremberg Code () is a set of research ethics principles for human experimentation created as a result of the Nuremberg trials at the end of the Second World War.

The origin of the Nuremberg Code began in pre–World War II German politics, particularly during the 1930s and 1940s. The pre-war German Medical Association was considered to be a progressive yet democratic association with great concerns for public health, one example being the legislation of compulsory health insurance for German workers. However, starting in the mid-1920s, German physicians, usually proponents of racial hygiene, were accused by the public and the medical society of unethical medical practices. The use of racial hygiene was supported by the German government in order to create an Aryan "master race", and to exterminate those who did not fit into their criteria. Racial hygiene extremists merged with National Socialism to promote the use of biology to accomplish their goals of racial purity, a core concept in the Nazi ideology. Physicians were attracted to the scientific ideology and aided in the establishment of National Socialist Physicians' League in 1929 to "purify the German medical community of 'Jewish Bolshevism'." Criticism was becoming prevalent; Alfons Stauder, member of the Reich Health Office, claimed that the "dubious experiments have no therapeutic purpose", and Fredrich von Muller, physician and the president of the Deutsche Akademie, joined the criticism.

In response to the criticism of unethical human experimentation, the Reich government issued "Guidelines for New Therapy and Human Experimentation" in Weimar, Germany. The guidelines were based on beneficence and non-maleficence, but also stressed legal doctrine of informed consent. The guidelines clearly distinguished the difference between therapeutic and non-therapeutic research. For therapeutic purposes, the guidelines allowed administration without consent only in dire situations, but for non-therapeutic purposes any administration without consent was strictly forbidden. However, the guidelines from Weimar were negated by Adolf Hitler. By 1942, the Nazi party included more than 38,000 German physicians, who helped carry out medical programs such as the Sterilization Law.

After World War II, a series of trials were held to hold members of the Nazi party responsible for a multitude of war crimes. The trials were approved by President Harry Truman on May 2, 1945 and were led by the United States, Great Britain, and the Soviet Union. They began on November 20, 1945 in Nuremberg, Germany, in what became known as the Nuremberg trials. In one of the trials, which became known as the "Doctors' Trial", German physicians responsible for conducting unethical medical procedures on humans during the war were tried. They focused on physicians who conducted inhumane and unethical human experiments in concentration camps, in addition to those who were involved in over 3,500,000 sterilizations of German citizens.

Several of the accused argued that their experiments differed little from those used before the war, and that there was no law that differentiated between legal and illegal experiments. This worried Drs. Andrew Ivy and Leo Alexander, who worked with the prosecution during the trial. In April 1947, Dr. Alexander submitted a memorandum to the United States Counsel for War Crimes outlining six points for legitimate medical research.

The Nuremberg code, which stated explicit voluntary consent from patients are required for human experimentation was drafted on August 9, 1947. On August 20, 1947, the judges delivered their verdict against Karl Brandt and 22 others. The verdict reiterated the memorandum's points and, in response to expert medical advisers for the prosecution, revised the original six points to ten. The ten points became known as the "Nuremberg Code", which includes such principles as informed consent and absence of coercion; properly formulated scientific experimentation; and beneficence towards experiment participants. It is thought to have been mainly based on the Hippocratic Oath, which was interpreted as endorsing the experimental approach to medicine while protecting the patient.

The ten points of the code were given in the section of the verdict entitled "Permissible Medical Experiments":
The Nuremberg Code was initially ignored, but gained much greater significance about 20 years after it was written. As a result, there were substantial rival claims for the creation of the Code. Some claimed that Harold Sebring, one of the three U.S. judges who presided over the Doctors' Trial, was the author. Leo Alexander, MD and Andrew Ivy, MD, the prosecution's chief medical expert witnesses, were also each identified as authors. In his letter to Maurice H. Pappworth, an English physician and the author of the book "Human Guinea Pigs", Andrew Ivy claimed sole authorship of the Code. Leo Alexander, approximately 30 years after the trial, also claimed sole authorship. However, after careful reading of the transcript of the Doctors' Trial, background documents, and the final judgements, it is more accepted that the authorship was shared and the Code grew out of the trial itself.

Dr. Ravindra Ghooi from India has written a paper on this code and in his opinion, the code borrows heavily from the 1931 guidelines without acknowledging its source and thus could be considered plagiarized.

The Nuremberg Code has not been officially accepted as law by any nation or as official ethics guidelines by any association. In fact, the Code's reference to Hippocratic duty to the individual patient and the need to provide information was not initially favored by the American Medical Association. The Western world initially dismissed the Nuremberg Code as a "code for barbarians" and not for civilized physicians and investigators. Additionally, the final judgment did not specify whether the Nuremberg Code should be applied to cases such as political prisoners, convicted felons, and healthy volunteers. The lack of clarity, the brutality of the unethical medical experiments, and the uncompromising language of the Nuremberg Code created an image that the Code was designed for singularly egregious transgressions.

However, the Code is considered to be the most important document in the history of clinical research ethics, which had a massive influence on global human rights. The Nuremberg Code and the related Declaration of Helsinki are the basis for the Code of Federal Regulations Title 45 Part 46, which are the regulations issued by the United States Department of Health and Human Services for the ethical treatment of human subjects, and are used in Institutional Review Boards (IRBs). In addition, the idea of informed consent has been universally accepted and now constitutes Article 7 of the United Nations' International Covenant on Civil and Political Rights. It also served as the basis for International Ethical Guidelines for Biomedical Research Involving Human Subjects proposed by the World Health Organization.


</doc>
<doc id="21885" url="https://en.wikipedia.org/wiki?curid=21885" title="Nim">
Nim

Nim is a mathematical game of strategy in which two players take turns removing (or "nimming") objects from distinct heaps or piles. On each turn, a player must remove at least one object, and may remove any number of objects provided they all come from the same heap or pile. Depending on the version being played, the goal of the game is either to avoid taking the last object, or to take the last object.

Variants of Nim have been played since ancient times. The game is said to have originated in China—it closely resembles the Chinese game of 捡石子 "jiǎn-shízi", or "picking stones"—but the origin is uncertain; the earliest European references to Nim are from the beginning of the 16th century. Its current name was coined by Charles L. Bouton of Harvard University, who also developed the complete theory of the game in 1901, but the origins of the name were never fully explained.

Nim is typically played as a "misère game", in which the player to take the last object loses. Nim can also be played as a "normal play" game, where the player taking the last object wins. This is called normal play because the last move is a winning move in most games, even though it is not the normal way that Nim is played. In either normal play or a misère game, when the number of heaps with at least two objects is exactly equal to one, the next player who takes next can easily win. If this removes either all or all but one objects from the heap that has two or more, then no heaps will have more than one object, so the players are forced to alternate removing exactly one object until the game ends. If the player leaves an even number of non-zero heaps (as the player would do in normal play), the player takes last; if the player leaves an odd number of heaps (as the player would do in misère play), then the other player takes last.

Normal play Nim (or more precisely the system of nimbers) is fundamental to the Sprague–Grundy theorem, which essentially says that in normal play every impartial game is equivalent to a Nim heap that yields the same outcome when played in parallel with other normal play impartial games (see disjunctive sum).

While all normal play impartial games can be assigned a Nim value, that is not the case under the misère convention. Only tame games can be played using the same strategy as misère Nim.

Nim is a special case of a poset game where the poset consists of disjoint chains (the heaps).

The evolution graph of the game of Nim with three heaps is the same as three branches of the evolution graph of the Ulam-Warburton automaton.

At the 1940 New York World's Fair Westinghouse displayed a machine, the Nimatron, that played Nim. From May 11, 1940 to October 27, 1940 only a few people were able to beat the machine in that six week period; if they did they were presented with a coin that said Nim Champ. It was also one of the first ever electronic computerized games. Ferranti built a Nim playing computer which was displayed at the Festival of Britain in 1951. In 1952 Herbert Koppel, Eugene Grant and Howard Bailer, engineers from the W. L. Maxon Corporation, developed a machine weighing which played Nim against a human opponent and regularly won. A Nim Playing Machine has been described made from TinkerToy.

The game of Nim was the subject of Martin Gardner's February 1958 Mathematical Games column in Scientific American. A version of Nim is played—and has symbolic importance—in the French New Wave film "Last Year at Marienbad" (1961).

The normal game is between two players and played with three heaps of any number of objects. The two players alternate taking any number of objects from any single one of the heaps. The goal is to be the last to take an object. In misère play, the goal is instead to ensure that the opponent is forced to take the last remaining object.

The following example of a normal game is played between fictional players Bob and Alice who start with heaps of three, four and five objects.

The practical strategy to win at the game of "Nim" is for a player to get the other into one of the following positions, and every successive turn afterwards they should be able to make one of the smaller positions. Only the last move changes between misere and normal play.
<nowiki>*</nowiki> Only valid for normal play.

<nowiki>**</nowiki> Only valid for misere.

For the generalisations, "n" and "m" can be any value > 0, and they may be the same.

Nim has been mathematically solved for any number of initial heaps and objects, and there is an easily calculated way to determine which player will win and what winning moves are open to that player.

The key to the theory of the game is the binary digital sum of the heap sizes, that is, the sum (in binary) neglecting all carries from one digit to another. This operation is also known as "bitwise xor" or "vector addition over GF(2)" (bitwise addition modulo 2). Within combinatorial game theory it is usually called the nim-sum, as it will be called here. The nim-sum of "x" and "y" is written to distinguish it from the ordinary sum, . An example of the calculation with heaps of size 3, 4, and 5 is as follows:

An equivalent procedure, which is often easier to perform mentally, is to express the heap sizes as sums of distinct powers of 2, cancel pairs of equal powers, and then add what is left:

In normal play, the winning strategy is to finish every move with a nim-sum of 0. This is always possible if the nim-sum is not zero before the move. If the nim-sum is zero, then the next player will lose if the other player does not make a mistake. To find out which move to make, let X be the nim-sum of all the heap sizes. Find a heap where the nim-sum of X and heap-size is less than the heap-size - the winning strategy is to play in such a heap, reducing that heap to the nim-sum of its original size with X. In the example above, taking the nim-sum of the sizes is . The nim-sums of the heap sizes A=3, B=4, and C=5 with X=2 are
The only heap that is reduced is heap A, so the winning move is to reduce the size of heap A to 1 (by removing two objects).

As a particular simple case, if there are only two heaps left, the strategy is to reduce the number of objects in the bigger heap to make the heaps equal. After that, no matter what move your opponent makes, you can make the same move on the other heap, guaranteeing that you take the last object.

When played as a misère game, Nim strategy is different only when the normal play move would leave only heaps of size one. In that case, the correct move is to leave an odd number of heaps of size one (in normal play, the correct move would be to leave an even number of such heaps).

These strategies for normal play and a misère game are the same until the number of heaps with at least two objects is exactly equal to one. At that point, the next player removes either all or all but one objects from the heap that has two or more, so no heaps will have more than one object (in other words, all remaining heaps have exactly one object each), so the players are forced to alternate removing exactly exactly one object until the game ends. In normal play, the player leaves an even number of non-zero heaps, and player takes last; in misère play), the players leaves an odd number of non-zero heaps, so the other player takes last.

In a misère game with heaps of sizes three, four and five, the strategy would be applied like this:

The previous strategy for a misère game can be easily implemented (for example in Python, below).
import functools

MISERE = 'misere'
NORMAL = 'normal'

def nim(heaps, game_type):

if __name__ == "__main__":

The soundness of the optimal strategy described above was demonstrated by C. Bouton.

Theorem. In a normal Nim game, the player making the first move has a winning strategy if and only if the nim-sum of the sizes of the heaps is not zero. Otherwise, the second player has a winning strategy.

"Proof:" Notice that the nim-sum (⊕) obeys the usual associative and commutative laws of addition (+) and also satisfies an additional property, "x" ⊕ "x" = 0.

Let "x", ..., "x" be the sizes of the heaps before a move, and "y", ..., "y" the corresponding sizes after a move. Let "s" = "x" ⊕ ... ⊕ "x" and "t" = "y" ⊕ ... ⊕ "y". If the move was in heap "k", we have "x" = "y" for all "i" ≠ "k", and "x" > "y". By the properties of ⊕ mentioned above, we have

The theorem follows by induction on the length of the game from these two lemmas.

Lemma 1. If "s" = 0, then "t" ≠ 0 no matter what move is made.

"Proof:" If there is no possible move, then the lemma is vacuously true (and the first player loses the normal play game by definition). Otherwise, any move in heap "k" will produce "t" = "x" ⊕ "y" from (*). This number is nonzero, since "x" ≠ "y".

Lemma 2. If "s" ≠ 0, it is possible to make a move so that "t" = 0.

"Proof:" Let "d" be the position of the leftmost (most significant) nonzero bit in the binary representation of "s", and choose "k" such that the "d"th bit of "x" is also nonzero. (Such a "k" must exist, since otherwise the "d"th bit of "s" would be 0.)
Then letting "y" = "s" ⊕ "x", we claim that "y" < "x": all bits to the left of "d" are the same in "x" and "y", bit "d" decreases from 1 to 0 (decreasing the value by 2), and any change in the remaining bits will amount to at most 2−1. The first player can thus make a move by taking "x" − "y" objects from heap "k", then

The modification for misère play is demonstrated by noting that the modification first arises in a position that has only one heap of size 2 or more. Notice that in such a position "s" ≠ 0, therefore this situation has to arise when it is the turn of the player following the winning strategy. The normal play strategy is for the player to reduce this to size 0 or 1, leaving an even number of heaps with size 1, and the misère strategy is to do the opposite. From that point on, all moves are forced.

In another game which is commonly known as Nim (but is better called the subtraction game, an upper bound is imposed on the number of objects that can be removed in a turn. Instead of removing arbitrarily many objects, a player can only remove 1 or 2 or ... or "k" at a time. This game is commonly played in practice with only one heap (for instance with "k" = 3 in the game "Thai 21" on , where it appeared as an Immunity Challenge).

Bouton's analysis carries over easily to the general multiple-heap version of this game. The only difference is that as a first step, before computing the Nim-sums, we must reduce the sizes of the heaps modulo "k" + 1. If this makes all the heaps of size zero (in misère play), the winning move is to take "k" objects from one of the heaps. In particular, in ideal play from a single heap of "n" objects, the second player can win if and only if

This follows from calculating the nim-sequence of "S"(1,2...,"k"),
from which the strategy above follows by the Sprague–Grundy theorem.

The game "21" is played as a misère game with any number of players who take turns saying a number. The first player says "1" and each player in turn increases the number by 1, 2, or 3, but may not exceed 21; the player forced to say "21" loses. This can be modeled as a subtraction game with a heap of 21–"n" objects. The winning strategy for the two-player version of this game is to always say a multiple of 4; it is then guaranteed that the other player will ultimately have to say 21 – so in the standard version where the first player opens with "1", they start with a losing move.

The 21 game can also be played with different numbers, like "Add at most 5; lose on 34".

A sample game of 21 in which the second player follows the winning strategy:

A similar version is the "100 game": two players start from 0 and alternately add a number from 1 to 10 to the sum. The player who reaches 100 wins. The winning strategy is to reach a number in which the digits are subsequent (e.g. 01, 12, 23, 34...) and control the game by jumping through all the numbers of this sequence. Once reached 89, the opponent has lost; they can only choose numbers from 90 to 99, and the next answer can in any case be 100).

In another variation of Nim, besides removing any number of objects from a single heap, one is permitted to remove the same number of objects from each heap.

Yet another variation of Nim is 'Circular Nim', where any number of objects are placed in a circle, and two players alternately remove one, two or three adjacent objects. For example, starting with a circle of ten objects,

three objects are taken in the first move

then another three

then one

but then three objects cannot be taken out in one move.

In Grundy's game, another variation of Nim, a number of objects are placed in an initial heap, and two players alternately divide a heap into two nonempty heaps of different sizes. Thus, six objects may be divided into piles of 5+1 or 4+2, but not 3+3. Grundy's game can be played as either misère or normal play.

"Greedy Nim" is a variation where the players are restricted to choosing stones from only the largest pile. It is a finite impartial game. "Greedy Nim Misère" has the same rules as Greedy Nim, but here the last player able to make a move loses.

Let the largest number of stones in a pile be "m", the second largest number of stones in a pile be "n". Let "p" be the number of piles having "m" stones, "p" be the number of piles having "n" stones. Then there is a theorem that game positions with "p" even are "P" positions.
Thus there exists a move to a state where "p" is even. Conversely, if "p" is even, if any move is possible ("p" ≠ 0) then it must take the game to a state where "p" is odd. The final position of the game is even ("p" = 0). Hence each position of the game with "p" even must be a "P" position.

A generalization of multi-heap Nim was called "Nimformula_2" or "index-"k"" Nim by E. H. Moore, who analyzed it in 1910. In index-"k" Nim, instead of removing objects from only one heap, players can remove objects from at least one but up to "k" different heaps. The number of elements that may be removed from each heap may be either arbitrary, or limited to at most "r" elements, like in the "subtraction game" above.

The winning strategy is as follows: Like in ordinary multi-heap Nim, one considers the binary representation of the heap sizes (or heap sizes modulo "r" + 1). In ordinary Nim one forms the XOR-sum (or sum modulo 2) of each binary digit, and the winning strategy is to make each XOR sum zero. In the generalization to index-"k" Nim, one forms the sum of each binary digit modulo "k" + 1.

Again the winning strategy is to move such that this sum is zero for every digit. Indeed, the value thus computed is zero for the final position, and given a configuration of heaps for which this value is zero, any change of at most "k" heaps will make the value non-zero. Conversely, given a configuration with non-zero value, one can always take from at most "k" heaps, carefully chosen, so that the value will become zero.

Building Nim is a variant of Nim where the two players first construct the game of Nim. Given "n" stones and "s" empty piles, the players alternate turns placing exactly one stone into a pile of their choice. Once all the stones are placed, a game of Nim begins, starting with the next player that would move. This game is denoted "BN(n,s)".

"n"-d Nim is played on a formula_3 board, where any number of continuous pieces can be removed from any hyper-row. The starting position is usually the full board, but other options are allowed.

The starting board is a disconnected graph, and players take turn to remove adjacent vertices.

Candy Nim is a version of normal play Nim, in which players tried to achieve two goals at the same time: taking the last object (in this case, "candy"), and taking the maximum number of candies by the end of the game.






</doc>
<doc id="21886" url="https://en.wikipedia.org/wiki?curid=21886" title="Ninon de l'Enclos">
Ninon de l'Enclos

Anne "Ninon" de l'Enclos also spelled Ninon de Lenclos and Ninon de Lanclos (10 November 1620 – 17 October 1705) was a French author, courtesan, and patron of the arts.

Born Anne de l'Enclos in Paris on 10 November 1620, she was nicknamed "Ninon" at an early age by her father, Henri de l'Enclos, a lutenist and published composer, who taught her to sing and play the lute. In 1632, he was exiled from France after a duel. When Ninon's mother died ten years later, the unmarried Ninon entered a convent, only to leave the next year. For the remainder of her life, she was determined to remain unmarried and independent.

Returning to Paris, she became a popular figure in the salons, and her own drawing room became a centre for the discussion and consumption of the literary arts. In her early thirties she was responsible for encouraging the young Molière, and when she died she left money for the son of her notary, a nine-year-old named François Marie Arouet, later to become known as Voltaire, so he could buy books.

It was during this period that her life as a courtesan began. Ninon took a succession of notable and wealthy lovers, including the king's cousin the Great Condé, Gaston de Coligny, and François, duc de La Rochefoucauld. These men did not support her, however; she prided herself on her independent income. "Ninon always had crowds of adorers but never more than one lover at a time, and when she tired of the present occupier, she said so frankly and took another. Yet such was the authority of this wanton, that no man dared fall out with his successful rival; he was only too happy to be allowed to visit as a familiar friend," Saint-Simon wrote. In 1652, Ninon took up with Louis de Mornay, the marquis de Villarceaux, by whom she had a son, also named Louis. She lived with the marquis until 1655, when she returned to Paris. When she would not return to him, the marquis fell into a fever; to console him, Ninon cut her hair and sent the shorn locks to him, starting a vogue for bobbed hair "à la Ninon".

This life (less acceptable then as it would become in later years) and her opinions on organized religion caused her some trouble, and she was imprisoned in the Madelonnettes Convent in 1656 at the behest of Anne of Austria, Queen of France and regent for her son Louis XIV. Not long after, however, she was visited by Christina, former queen of Sweden. Impressed, Christina wrote to Cardinal Mazarin on Ninon's behalf and arranged for her release.

In response, as an author she defended the possibility of living a good life in the absence of religion, notably in 1659's "La coquette vengée" ("The Flirt Avenged"). She was also noted for her wit; among her numerous sayings and quips are "Much more genius is needed to make love than to command armies" and "We should take care to lay in a stock of provisions, but not of pleasures: these should be gathered day by day." A picture of Ninon, under the name of Damo, was sketched in Mlle de Scudéry's "Clélie" (1654–1661).

Starting in the late 1660s she retired from her courtesan lifestyle and concentrated more on her literary friends – from 1667, she hosted her gatherings at "l'hôtel Sagonne", which was considered "the" location of the salon of Ninon de l'Enclos despite other locales in the past. During this time she was a friend of Jean Racine, the great French playwright. Later she would become a close friend with the devout Françoise d'Aubigné, better known as Madame de Maintenon, the lady-in-waiting who would later become the second wife of Louis XIV. Saint-Simon wrote that "The lady did not like her to be mentioned in her presence, but dared not disown her, and wrote cordial letters to her from time to time, to the day of her death". Ninon eventually died at the age of 84, as a very wealthy woman. To the end, she "was convinced that she had no soul, and never abandoned that conviction, not even in advanced old age, not even at the hour of her death."

Ninon de l'Enclos is a relatively obscure figure in the English-speaking world, but is much better known in France where her name is synonymous with wit and beauty. Saint-Simon noted "Ninon made friends among the great in every walk of life, had wit and intelligence enough to keep them, and, what is more, to keep them friendly with one another."

Dorothy Parker wrote the poem "Ninon De L'Enclos On Her Last Birthday" and also referred to Ninon in another of her poems, "Words Of Comfort To Be Scratched On A Mirror", writing, "Ninon was ever the chatter of France." L'Enclos is the eponymous heroine of Charles Lecocq's 1896 opéra comique, "Ninette".




</doc>
<doc id="21888" url="https://en.wikipedia.org/wiki?curid=21888" title="National Institute of Standards and Technology">
National Institute of Standards and Technology

The National Institute of Standards and Technology (NIST) is a physical sciences laboratory and a non-regulatory agency of the United States Department of Commerce. Its mission is to promote innovation and industrial competitiveness. NIST's activities are organized into laboratory programs that include nanoscale science and technology, engineering, information technology, neutron research, material measurement, and physical measurement. From 1901–1988, the agency was named the National Bureau of Standards.

The Articles of Confederation, ratified by the colonies in 1781, contained the clause, "The United States in Congress assembled shall also have the sole and exclusive right and power of regulating the alloy and value of coin struck by their own authority, or by that of the respective states—fixing the standards of weights and measures throughout the United States". Article 1, section 8, of the Constitution of the United States (1789), transferred this power to Congress; "The Congress shall have power...To coin money, regulate the value thereof, and of foreign coin, and fix the standard of weights and measures".

In January 1790, President George Washington, in his first annual message to Congress stated that, "Uniformity in the currency, weights, and measures of the United States is an object of great importance, and will, I am persuaded, be duly attended to", and ordered Secretary of State Thomas Jefferson to prepare a plan for Establishing Uniformity in the Coinage, Weights, and Measures of the United States, afterwards referred to as the Jefferson report. On October 25, 1791, Washington appealed a third time to Congress, "A uniformity of the weights and measures of the country is among the important objects submitted to you by the Constitution and if it can be derived from a standard at once invariable and universal, must be no less honorable to the public council than conducive to the public convenience", but it was not until 1838, that a uniform set of standards was worked out.

In 1821, John Quincy Adams had declared "Weights and measures may be ranked among the necessities of life to every individual of human society".
From 1830 until 1901, the role of overseeing weights and measures was carried out by the Office of Standard Weights and Measures, which was part of the United States Department of the Treasury.

In 1901, in response to a bill proposed by Congressman James H. Southard (R, Ohio), the National Bureau of Standards was founded with the mandate to provide standard weights and measures, and to serve as the national physical laboratory for the United States. (Southard had previously sponsored a bill for metric conversion of the United States.) 

President Theodore Roosevelt appointed Samuel W. Stratton as the first director. The budget for the first year of operation was $40,000. The Bureau took custody of the copies of the kilogram and meter bars that were the standards for US measures, and set up a program to provide metrology services for United States scientific and commercial users. A laboratory site was constructed in Washington, DC, and instruments were acquired from the national physical laboratories of Europe. In addition to weights and measures, the Bureau developed instruments for electrical units and for measurement of light. In 1905 a meeting was called that would be the first "National Conference on Weights and Measures".

Initially conceived as purely a metrology agency, the Bureau of Standards was directed by Herbert Hoover to set up divisions to develop commercial standards for materials and products. Some of these standards were for products intended for government use, but product standards also affected private-sector consumption. Quality standards were developed for products including some types of clothing, automobile brake systems and headlamps, antifreeze, and electrical safety. During World War I, the Bureau worked on multiple problems related to war production, even operating its own facility to produce optical glass when European supplies were cut off. Between the wars, Harry Diamond of the Bureau developed a blind approach radio aircraft landing system. During World War II, military research and development was carried out, including development of radio propagation forecast methods, the proximity fuze and the standardized airframe used originally for Project Pigeon, and shortly afterwards the autonomously radar-guided Bat anti-ship guided bomb and the Kingfisher family of torpedo-carrying missiles.
In 1948, financed by the United States Air Force, the Bureau began design and construction of SEAC, the Standards Eastern Automatic Computer. The computer went into operation in May 1950 using a combination of vacuum tubes and solid-state diode logic. About the same time the Standards Western Automatic Computer, was built at the Los Angeles office of the NBS by Harry Huskey and used for research there. A mobile version, DYSEAC, was built for the Signal Corps in 1954.

Due to a changing mission, the "National Bureau of Standards" became the "National Institute of Standards and Technology" in 1988.

Following September 11, 2001, NIST conducted the official investigation into the collapse of the World Trade Center buildings.

NIST, known between 1901 and 1988 as the National Bureau of Standards (NBS), is a measurement standards laboratory, also known as a National Metrological Institute (NMI), which is a non-regulatory agency of the United States Department of Commerce. The institute's official mission is to:

NIST had an operating budget for fiscal year 2007 (October 1, 2006September 30, 2007) of about $843.3 million. NIST's 2009 budget was $992 million, and it also received $610 million as part of the American Recovery and Reinvestment Act. NIST employs about 2,900 scientists, engineers, technicians, and support and administrative personnel. About 1,800 NIST associates (guest researchers and engineers from American companies and foreign countries) complement the staff. In addition, NIST partners with 1,400 manufacturing specialists and staff at nearly 350 affiliated centers around the country. NIST publishes the Handbook 44 that provides the "Specifications, tolerances, and other technical requirements for weighing and measuring devices".

The Congress of 1866 made use of the metric system in commerce a legally protected activity through the passage of Metric Act of 1866. On May 20, 1875, 17 out of 20 countries signed a document known as the "Metric Convention" or the "Treaty of the Meter", which established the International Bureau of Weights and Measures under the control of an international committee elected by the General Conference on Weights and Measures.

NIST is headquartered in Gaithersburg, Maryland, and operates a facility in Boulder, Colorado. NIST's activities are organized into laboratory programs and extramural programs. Effective October 1, 2010, NIST was realigned by reducing the number of NIST laboratory units from ten to six. NIST Laboratories include:

Extramural programs include:

NIST also operates a neutron science user facility: the NIST Center for Neutron Research (NCNR). The NCNR provides scientists access to a variety of neutron scattering instruments, which they use in many research fields (materials science, fuel cells, biotechnology, etc.).

The SURF III Synchrotron Ultraviolet Radiation Facility is a source of synchrotron radiation, in continuous operation since 1961. SURF III now serves as the US national standard for source-based radiometry throughout the generalized optical spectrum. All NASA-borne, extreme-ultraviolet observation instruments have been calibrated at SURF since the 1970s, and SURF is used for measurement and characterization of systems for extreme ultraviolet lithography.

The Center for Nanoscale Science and Technology (CNST) performs research in nanotechnology, both through internal research efforts and by running a user-accessible cleanroom nanomanufacturing facility. This "NanoFab" is equipped with tools for lithographic patterning and imaging (e.g., electron microscopes and atomic force microscopes).

NIST has seven standing committees:

As part of its mission, NIST supplies industry, academia, government, and other users with over 1,300 Standard Reference Materials (SRMs). These artifacts are certified as having specific characteristics or component content, used as calibration standards for measuring equipment and procedures, quality control benchmarks for industrial processes, and experimental control samples.

NIST publishes the "Handbook 44" each year after the annual meeting of the National Conference on Weights and Measures (NCWM). Each edition is developed through cooperation of the Committee on Specifications and Tolerances of the NCWM and the Weights and Measures Division (WMD) of the NIST. The purpose of the book is a partial fulfillment of the statutory responsibility for "cooperation with the states in securing uniformity of weights and measures laws and methods of inspection".

NIST has been publishing various forms of what is now the "Handbook 44" since 1918 and began publication under the current name in 1949. The 2010 edition conforms to the concept of the primary use of the SI (metric) measurements recommended by the Omnibus Foreign Trade and Competitiveness Act of 1988.

NIST is developing government-wide identity document standards for federal employees and contractors to prevent unauthorized persons from gaining access to government buildings and computer systems.

In 2002, the National Construction Safety Team Act mandated NIST to conduct an investigation into the collapse of the World Trade Center buildings 1 and 2 and the 47-story 7 World Trade Center. The "World Trade Center Collapse Investigation", directed by lead investigator Shyam Sunder, covered three aspects, including a technical building and fire safety investigation to study the factors contributing to the probable cause of the collapses of the WTC Towers (WTC 1 and 2) and WTC 7. NIST also established a research and development program to provide the technical basis for improved building and fire codes, standards, and practices, and a dissemination and technical assistance program to engage leaders of the construction and building community in implementing proposed changes to practices, standards, and codes. NIST also is providing practical guidance and tools to better prepare facility owners, contractors, architects, engineers, emergency responders, and regulatory authorities to respond to future disasters. The investigation portion of the response plan was completed with the release of the final report on 7 World Trade Center on November 20, 2008. The final report on the WTC Towers—including 30 recommendations for improving building and occupant safety—was released on October 26, 2005.

NIST works in conjunction with the Technical Guidelines Development Committee of the Election Assistance Commission to develop the Voluntary Voting System Guidelines for voting machines and other election technology.
Four scientific researchers at NIST have been awarded Nobel Prizes for work in physics: William Daniel Phillips in 1997, Eric Allin Cornell in 2001, John Lewis Hall in 2005 and David Jeffrey Wineland in 2012, which is the largest number for any US government laboratory. All four were recognized for their work related to laser cooling of atoms, which is directly related to the development and advancement of the atomic clock. In 2011, Dan Shechtman was awarded the Nobel in chemistry for his work on quasicrystals in the Metallurgy Division from 1982 to 1984. In addition, John Werner Cahn was awarded the 2011 Kyoto Prize for Materials Science, and the National Medal of Science has been awarded to NIST researchers Cahn (1998) and Wineland (2007). Other notable people who have worked at NBS or NIST include:

Since 1989, the director of NIST has been a Presidential appointee and is confirmed by the United States Senate, and since that year the average tenure of NIST directors has fallen from 11 years to 2 years in duration. Since the 2011 reorganization of NIST, the director also holds the title of Under Secretary of Commerce for Standards and Technology. Fifteen individuals have officially held the position (in addition to four acting directors who have served on a temporary basis).

In September 2013, both "The Guardian" and "The New York Times" reported that NIST allowed the National Security Agency (NSA) to insert a cryptographically secure pseudorandom number generator called Dual EC DRBG into NIST standard SP 800-90 that had a kleptographic backdoor that the NSA can use to covertly predict the future outputs of this pseudorandom number generator thereby allowing the surreptitious decryption of data. Both papers report that the NSA worked covertly to get its own version of SP 800-90 approved for worldwide use in 2006. The whistle-blowing document states that "eventually, NSA became the sole editor". The reports confirm suspicions and technical grounds publicly raised by cryptographers in 2007 that the EC-DRBG could contain a kleptographic backdoor (perhaps placed in the standard by NSA).

NIST responded to the allegations, stating that "NIST works to publish the strongest cryptographic standards possible" and that it uses "a transparent, public process to rigorously vet our recommended standards". The agency stated that "there has been some confusion about the standards development process and the role of different organizations in it...The National Security Agency (NSA) participates in the NIST cryptography process because of its recognized expertise. NIST is also required by statute to consult with the NSA." Recognizing the concerns expressed, the agency reopened the public comment period for the SP800-90 publications, promising that "if vulnerabilities are found in these or any other NIST standards, we will work with the cryptographic community to address them as quickly as possible”. Due to public concern of this cryptovirology attack, NIST rescinded the EC-DRBG algorithm from the NIST SP 800-90 standard.





</doc>
<doc id="21891" url="https://en.wikipedia.org/wiki?curid=21891" title="NATO reporting name">
NATO reporting name

NATO reporting names are code names for military equipment from Russia, China, and historically, the Eastern Bloc (Soviet Union and other nations of the Warsaw Pact). They provide unambiguous and easily understood English words in a uniform manner in place of the original designations, which either may have been unknown to the Western world at the time or easily confused codes. For example, the Russian bomber jet Tupolev Tu-160 is simply called "Blackjack".

NATO maintains lists of the names. The assignment of the names for the Russian and Chinese aircraft was once managed by the five-nation Air Standardization Coordinating Committee (ASCC), but that is no longer the case.

The United States Department of Defense (DOD) expands on the NATO reporting names in some cases. NATO refers to surface-to-air missile systems mounted on ships or submarines with the same names as the corresponding land-based systems, but the US DoD assigns a different series of numbers with a different suffix (i.e., SA-N- vs. SA-) for these systems. The names are kept the same as a convenience. Where there is no corresponding system, a new name is devised.

The Soviet Union did not always assign official "popular names" to its aircraft, but unofficial nicknames were common as in any air force. Generally, Soviet pilots did not use the NATO names, preferring a different, Russian, nickname. An exception was that Soviet airmen appreciated the MiG-29's codename "Fulcrum", as an indication of its pivotal role in Soviet air defence.

To reduce the risk of confusion, unusual or made-up names were allocated, the idea being that the names chosen would be unlikely to occur in normal conversation, and be easier to memorise. For fixed-wing aircraft, single-syllable words denoted piston-prop and turboprop, while multiple-syllable words denoted jets. Bombers had names starting with the letter "B" and names like "Badger" (2 syllables: jet), "Bear" (single syllable: propeller), and "Blackjack" were used. "Frogfoot," the reporting name for the Sukhoi Su-25, references the aircraft's close air support role. Transports had names starting with "C" (as in "cargo"), which resulted in names like "Condor" or "Candid".

The initial letter of the name indicated the use of that equipment.


The first letter indicates the type of aircraft, like "B"ear for a bomber aircraft, or "F"ulcrum for a fighter aircraft. For fixed-wing aircraft, a one-syllable name refers to a propeller aircraft and a two-syllable name refers to an aircraft with jet engines. This distinction is not made for helicopters.

Before the 1980s, reporting names for submarines were taken from the NATO spelling alphabet. Modifications of existing designs were given descriptive terms, such as “Whiskey Long Bin”. From the 1980s, new designs were given names derived from Russian words, such as “Akula”, or “shark”. These names did not correspond to the Soviet names. Coincidentally, “Akula”, which was assigned to an attack submarine by NATO, was the actual Soviet name for the ballistic missile submarine NATO dubbed “Typhoon”.




</doc>
<doc id="21892" url="https://en.wikipedia.org/wiki?curid=21892" title="List of NATO reporting names for surface-to-surface missiles">
List of NATO reporting names for surface-to-surface missiles

NATO reporting name for SS series surface-to-surface missiles, with Soviet designations:


US DoD designations for SS-N series naval surface-to-surface missiles (fired from ships and submarines), with Soviet designations:


"See also": NATO reporting name


</doc>
<doc id="21893" url="https://en.wikipedia.org/wiki?curid=21893" title="List of NATO reporting names for air-to-air missiles">
List of NATO reporting names for air-to-air missiles

NATO reporting name for AA series air-to-air missiles, with Soviet designations:


"See also": NATO reporting name


</doc>
<doc id="21894" url="https://en.wikipedia.org/wiki?curid=21894" title="List of NATO reporting names for air-to-surface missiles">
List of NATO reporting names for air-to-surface missiles

NATO reporting name for AS series air-to-surface missiles, with Soviet designations:

Note: the Soviet / Russian designation is a Cyrillic letter "Х", which is translated as "Kh" or "H". Also, sometimes a combination ("complex") of a missile with its aircraft is marked with a letter "K" (for example, a missile Kh-22 with an aircraft is a "complex K-22"). The Cyrillic "X" (read "Kh") in the designation of Soviet ASMs is in fact a Latin "X" ("ecs") for Xperimental, as used by the design bureau. With passing time, however, this was ignored and used in Soviet/Russian as well as foreign literature as the Cyrillic Kh.


"See also": NATO reporting name


</doc>
<doc id="21895" url="https://en.wikipedia.org/wiki?curid=21895" title="List of NATO reporting names for anti-tank missiles">
List of NATO reporting names for anti-tank missiles

NATO reporting name for AT series anti-tank guided missiles, with Soviet designations:

"See also:" NATO reporting name, List of anti-tank guided missiles



</doc>
<doc id="21896" url="https://en.wikipedia.org/wiki?curid=21896" title="List of NATO reporting names for surface-to-air missiles">
List of NATO reporting names for surface-to-air missiles

NATO reporting name for SA series surface-to-air missiles, with Soviet designations:


U.S. DoD designations for SA-N series naval surface-to-air missiles, with Soviet designations. Note that these are not standard NATO names, NATO uses the regular SA series for naval SAMS also, however the US DoD refers to them by these names:




</doc>
<doc id="21897" url="https://en.wikipedia.org/wiki?curid=21897" title="List of NATO reporting names for bomber aircraft">
List of NATO reporting names for bomber aircraft

This is a list of NATO reporting name/ASCC names for bombers, with Soviet Union and Chinese designations. Bombers had names starting with the letter "B"; single-syllable words denoted propeller driven aircraft (piston and turboprop engines), while two syllable words were used for jets. 



</doc>
<doc id="21898" url="https://en.wikipedia.org/wiki?curid=21898" title="List of NATO reporting names for fighter aircraft">
List of NATO reporting names for fighter aircraft

The "Five Eyes" Air Force Interoperability Council (AFIC) assigns codenames for fighters and other military aircraft originating in, or operated by, the air forces of the former Warsaw Pact, including Russia and the People's Republic of China. While the AFIC names designations are sometimes known as "NATO designations" and similar names, Five Eyes is a separate military alliance comprising the United States, United Kingdom, Canada and two non-NATO countries, Australia and New Zealand. 



</doc>
<doc id="21899" url="https://en.wikipedia.org/wiki?curid=21899" title="List of NATO reporting names for helicopters">
List of NATO reporting names for helicopters

Helicopters, NATO/ASCC names:



</doc>
<doc id="21900" url="https://en.wikipedia.org/wiki?curid=21900" title="List of NATO reporting names for transport aircraft">
List of NATO reporting names for transport aircraft

NATO reporting name/ASCC names for transport aircraft and their Soviet, Russian and Chinese designations:

NATO reporting name


</doc>
<doc id="21901" url="https://en.wikipedia.org/wiki?curid=21901" title="List of NATO reporting names for miscellaneous aircraft">
List of NATO reporting names for miscellaneous aircraft

NATO reporting name/Air Standardization Coordinating Committee (ASCC) names for miscellaneous aircraft, with Soviet designations, sorted by reporting name:


NATO reporting name/ASCC names for miscellaneous aircraft, with Soviet designations, sorted by Soviet designation:



</doc>
<doc id="21904" url="https://en.wikipedia.org/wiki?curid=21904" title="List of NATO reporting names for submarines">
List of NATO reporting names for submarines

This is a list of NATO reporting names for submarines, with Russian and Soviet Navy designations. The names were derived from the NATO phonetic alphabet before "Akula" first appeared in 1985.







</doc>
<doc id="21907" url="https://en.wikipedia.org/wiki?curid=21907" title="Seven Laws of Noah">
Seven Laws of Noah

The Seven Laws of Noah ( "Sheva Mitzvot B'nei Noach"), also referred to as the Noahide Laws or the Noachide Laws (from the Hebrew pronunciation of "Noah"), are a set of imperatives which, according to the Talmud, were given by God as a binding set of laws for the "children of Noah" – that is, all of humanity.

According to Jewish tradition, non-Jews who adhere to these laws are said to be followers of Noahidism and regarded as righteous gentiles, who are assured of a place in "Olam Haba" (, the world to come), the final reward of the righteous.
The Seven Laws of Noah include prohibitions against worshipping idols, cursing God, murder, adultery and sexual immorality, theft, eating flesh torn from a living animal, as well as the obligation to establish courts of justice.

The seven Noahide laws as traditionally enumerated are the following:


According to the Talmud, the rabbis agree that the seven laws were given to the sons of Noah. However, they disagree on precisely which laws were given to Adam and Eve. Six of the seven laws are exegetically derived from passages in Genesis, with the seventh being the establishing of courts.

The earliest complete rabbinic version of the seven laws can be found in the Tosefta:
According to the Genesis flood narrative, a deluge covered the whole world, killing every surface-dwelling creature except Noah, his wife, his sons and their wives, and the animals taken aboard Noah's Ark. According to this, all modern humans are descendants of Noah, thus the name Noahide Laws is referred to the laws that apply to all of humanity. After the flood, God sealed a covenant with Noah with the following admonitions ():

The Book of Jubilees, generally dated to the 2nd century BCE, may include an early reference to Noahide Law at verses 7:20–28:

The "Jewish Encyclopedia" article on Saul of Tarsus states:
The article "New Testament" states:
Rabbi Menachem Mendel Schneerson, the Lubavitcher Rebbe, published and spoke about the Seven Laws of Noah many times. He taught, based on a detailed reading of Maimonides' Hilchot Melachim, Talmud and Scripture, that the Seven Laws originally given to Noah were given yet again, through Moses at Sinai, and it is exclusively through that giving of the Torah that the Seven Laws derive their current force. What has changed with the giving of the Torah is that now, it is the duty of the Jewish people to bring the rest of the world to fulfill the Seven Laws.

The posek Rabbi Moshe Weiner details the sources for all seven of the laws in his legal compendium "Sheva Mitzvot Hashem." Adam, the first man, was given six laws. When Noah was permitted to eat meat, the proviso against meat from a living animal was added. At Sinai, the Laws were given again through Moses, together with their details. He concurs with the Rebbe, based on his analysis of other Acharonim that the current authority for the Laws is from the Torah from Sinai, where before it was from ancestral oral tradition from Noah and Adam. To enumerate his sources for each of the laws:


Michael Kogan, a professor of religious studies, the laws were not mentioned in Genesis but were extracted from the Torah by second-century rabbis.

David Novak presents a range of theories regarding the origin of the Noachide laws, including the Bible, Hittite law, the Maccabean period, and the Roman period.

According to the Talmud, the Noahide Laws apply to all humanity. In Judaism, בני נח "B'nei Noah" (Hebrew, "Descendants of Noah", "Children of Noah") refers to all of humankind. The Talmud also states: "Righteous people of all nations have a share in the world to come". Any non-Jew who lives according to these laws is regarded as one of "the righteous among the gentiles".

The rabbis agree that the seven laws were given to the sons of Noah. However, they disagree on precisely which laws were given to Adam and Eve. Six of the seven laws are exegetically derived from passages in Genesis. The Talmud adds extra laws beyond the seven listed in the Tosefta which are attributed to different rabbis, such as the grafting of trees and sorcery among others, Ulla going so far as to make a list of 30 laws. The Talmud expands the scope of the seven laws to cover about 100 of the 613 mitzvoth.

In practice Jewish law makes it very difficult to apply the death penalty. No record exists of a gentile having been put to death for violating the seven laws. Some of the categories of capital punishment recorded in the Talmud are recorded as having never been carried out. It is thought that the rabbis included discussion of them in anticipation of the coming messianic age.

The Talmud lists the punishment for blaspheming the Ineffable Name of God as death. The sons of Noah are to be executed by decapitation for most crimes, considered one of the lightest capital punishments, by stoning if he has intercourse with a Jewish betrothed woman, or by strangulation if the Jewish woman has completed the marriage ceremonies, but had not yet consummated the marriage. In Jewish law the only form of blasphemy which is punishable by death is blaspheming the Ineffable Name (). Some Talmudic rabbis held that only those offences for which a Jew would be executed, are forbidden to gentiles. The Talmudic rabbis discuss which offences and sub-offences are capital offences and which are merely forbidden.

Maimonides states that anyone who does not accept the seven laws is to be executed, as God compelled the world to follow these laws. However, for the other prohibitions such as the grafting of trees and bestiality he holds that the sons of Noah are not to be executed. Maimonides adds a universalism lacking from earlier Jewish sources. The Talmud differs from Maimonides in that it considers the seven laws enforceable by Jewish authorities on non-Jews living within a Jewish nation. Nahmanides disagrees with Maimonides' reasoning. He limits the obligation of enforcing the seven laws to non-Jewish authorities taking the matter out of Jewish hands. The Tosafot seems to agree with Nahmanides reasoning. According to some opinions, punishment is the same whether the individual transgresses with knowledge of the law or is ignorant of the law.

Various rabbinic sources have different positions on the way the seven laws are to be subdivided in categories. Maimonides', in his Mishneh Torah, included the grafting of trees. Like the Talmud, he interpreted the prohibition against homicide as including a prohibition against abortion. David ben Solomon ibn Abi Zimra, a commentator on Maimonides, expressed surprise that he left out castration and sorcery which were also listed in the Talmud.

The Talmudist Ulla said that here are 30 laws which the sons of Noah took upon themselves. However he only lists three, namely the three that the Gentiles follow: not to create a Ketubah between males, not to sell carrion or human flesh in the market and to respect the Torah. The rest of the laws are not listed. Though the authorities seem to take it for granted that Ulla's thirty commandments included the original seven, an additional thirty laws is also possible from the reading. Two different lists of the 30 laws exist. Both lists include an additional twenty-three mitzvot which are subdivisions or extensions of the seven laws. One from the 16th-century work "Asarah Maamarot" by Rabbi Menahem Azariah da Fano and a second from the 10th century Samuel ben Hofni which was recently published from his Judeo-Arabic writings after having been found in the Cairo Geniza. Rabbi Zvi Hirsch Chajes suggests Menahem Azariah of Fano enumerated commandments are not related to the first seven, nor based on Scripture, but instead were passed down by oral tradition.

In earlier times, a Gentile living in the Land of Israel who accepted the Seven Laws in front of a rabbinical court was known as a "ger toshav" (literally stranger/resident). The regulations regarding Jewish-Gentile relations are modified in the case of a "ger toshav".

Historically, some rabbinic opinions consider non-Jews not only not obliged to adhere to all the remaining laws of the Torah, but actually forbidden to observe them.

Noahide law differs radically from Roman law for gentiles ("Jus Gentium"), if only because the latter was enforceable judicial policy. Rabbinic Judaism has never adjudicated any cases under Noahide law, Jewish scholars disagree about whether Noahide law is a functional part of Halakha ("Jewish law").

Some modern views hold that penalties are a detail of the Noahide Laws and that Noahides themselves must determine the details of their own laws for themselves. According to this school of thought – see N. Rakover, "Law and the Noahides" (1998); M. Dallen, "The Rainbow Covenant" (2003) – the Noahide Laws offer mankind a set of absolute values and a framework for righteousness and justice, while the detailed laws that are currently on the books of the world's states and nations are presumptively valid.

In recent years, the term "Noahide" has come to refer to non-Jews who strive to live in accord with the seven Noahide Laws; the terms "observant Noahide" or "Torah-centered Noahides" would be more precise but these are infrequently used. Support for the use of "Noahide" in this sense can be found with the Ritva, who uses the term "Son of Noah" to refer to a Gentile who keeps the seven laws, but is not a Ger Toshav. The rainbow, referring to the Noahide or First Covenant (Genesis 9), is the symbol of many organized Noahide groups, following .

To various modern theologians the Noahide laws represent the inclusive nature of Judaism because they affirm the equality of Jews and non-Jews. To other intellectuals these seven laws represent natural law which are accessible to all through intellect and do not require revelation. According to Robert Eisen the second stream of thought ignores how a non-Jew could access these laws without the Jewish revelations. To Eisen, these set of laws impose a Jewish understanding of morality upon non-Jews. To Eisen, the Noahide laws represent more of a barrier between Jews and non-Jews, because non-Jews are forbidden to observe Jewish laws.

The Jewish scholar Maimonides (12th century) held that Gentiles may have a part in the world to come just by observing Noahide law and accepting it as given by Moses. Such children of Noah become the status of "Chasidei Umot HaOlam"—Pious People of the World, and are different from children of Noah who only keep the seven laws out of moral/ethical reasoning alone. He writes in his book of laws:"

Some later editions of the Mishneh Torah differ by one letter and read "Nor one of their wise men." The later reading is narrower. Spinoza read Maimonides as using nor and accused him of being narrow and particularistic. Other philosophers such as Hermann Cohen and Moses Mendelssohn have used more inclusive interpretations of the passage by Maimonides.

In either reading, Maimonides appears to exclude philosophical Noahides from being Righteous Gentiles. Thus Maimonides emphasizes that a truly Righteous Gentile follows the seven laws because they are divinely revealed and thus are followed out of obedience to God. According to Steven Schwarzschild, this position has its source in Maimonides' adoption of Aristotle's skeptical attitude towards the ability of reason to arrive at moral truths, and "many of the most outstanding spokesmen of Judaism themselves dissented sharply from" this position, which is "individual and certainly somewhat eccentric" in comparison to other Jewish thinkers.

The Apostolic Decree recorded in Acts 15 is commonly seen as a parallel to Noahide Law; however, some modern scholars dispute the connection between Acts 15 and Noahide Law, the content of Noahide Law, the historical reliability of the Acts of the Apostles, and the nature of biblical law in Christianity. The Apostolic Decree is still observed by Eastern Orthodoxy and includes some food restrictions.

The 18th-century rabbi Jacob Emden proposed that Jesus, and Paul after him, intended to convert the gentiles to the Noahide laws while calling on the Jews to keep the full Law of Moses.

Maimonides stated that God commanded Moses to compel the world to accept these seven commandments. In 1983 Rabbi Menachem M. Schneerson urged his followers to actively engage in activities to inform non-Jews about these seven commandments, which had not been done in previous generations.

After Rabbi Schneerson started his Noahide Campaign in the 1980s, a codification of the exact obligations of the Gentiles in the spirit of the classical Shulchan Aruch was needed. In 2005, Rabbi Moshe Weiner of Jerusalem accepted to produce an in-depth codification of the Noahide precepts. The work is called "Sefer Sheva Mitzvot HaShem", (The Book of Seven Divine Commandments) published 2008/2009. As it was approved by both of the then presiding chief rabbis of Israel (Rabbi Shlomo Moshe Amar and Rabbi Yonah Metzger) as well as by other Hasidic and non-Hasidic halachic authorities, it can claim an authoritative character and is referred as a "Shulchan Aruch" for Gentiles at many places.

In 1987 President Ronald Reagan signed a proclamation speaking of "the historical tradition of ethical values and principles, which have been the bedrock of society from the dawn of civilization when they were known as the Seven Noahide Laws, transmitted through God to Moses on Mount Sinai", and in 1991, Congress stated in the preamble to the 1991 bill that established Education Day in honor of the birthday of Menachem Mendel Schneerson, the leader of the Chabad movement:
In January 2004, Sheikh Mowafak Tarif, the spiritual leader of Israeli Druze, signed a declaration, which called on non-Jews living in Israel to observe the Noahide Laws. He was joined by the mayor of Shefa-'Amr.





</doc>
<doc id="21911" url="https://en.wikipedia.org/wiki?curid=21911" title="Naturism">
Naturism

Naturism is a lifestyle of non-sexual nudity, and the cultural movement which advocates and defends that lifestyle. Both may also be referred to as nudism. Though the two terms are largely interchangeable, "nudism" emphasizes the practice of nudity, whereas "naturism" highlights an attitude favoring harmony with nature and respect for the environment, into which that practice is integrated. That said, naturists come from a range of philosophical and cultural backgrounds; there is no single naturist ideology. Naturism may be practiced individually, within a familial or social context, or in public.

Ethical or philosophical nudism has a long history, with many advocates of the benefits of enjoying nature without clothing. At the turn of the 20th century, organizations emerged to promote social nudity and to establish private campgrounds and resorts for that purpose. Since the 1960s, with the acceptance of public places for clothing-optional recreation, individuals who do not identify themselves as naturists or nudists have been able to casually participate in nude activities. Nude recreation opportunities vary widely around the world, from isolated places known mainly to locals to officially-designated nude beaches and parks.

The XIV Congress of the International Naturist Federation (Agde, France, 1974) defined naturism as:

Many contemporary naturists and naturist organisations advocate that the practice of social nudity should not be linked with sexual activity. Some recent studies show that naturism can help growing self-esteem, thus having a positive impact on living a well balanced sexuality too. For various social, cultural, and historical reasons, the lay public, the media, and many contemporary naturists and their organisations have or present a simplified view of the relationship between naturism and sexuality. Current research has begun to explore this complex relationship.

The International Naturist Federation explains:

The usage and definition of these terms varies geographically and historically. Naturism and nudism have the same meaning in the United States, but there is a clear distinction between the two terms in Great Britain.

In naturist parlance, the terms "textile" or "textilist" refer to non-naturist persons, behaviours or facilities (e.g. "the textile beach starts at the flag", "they are a mixed couple – he is naturist, she is textile"). "Textile" is the predominant term used in the UK ("textilist" is unknown in British naturist magazines, including "H&E naturist"), although some naturists avoid it due to perceived negative or derogatory connotations. "Textilist" is said to be used interchangeably with "textile", but no dictionary definition to this effect exists, nor are there any equivalent examples of use in mainstream literature such as those for "textile".

At naturist organised events or venues, clothing is usually optional. At naturist swimming pools or sunbathing places, however, complete nudity is expected (weather permitting). This rule is sometimes a source of controversy among naturists. Staff at a naturist facility are usually required to be clothed due to health and safety regulations.

Facilities for naturists are classified in various ways. A landed or members' naturist club is one that owns its own facilities. Non-landed (or travel) clubs meet at various locations, such as private residences, swimming pools, hot springs, landed clubs and resorts, or rented facilities. Landed clubs can be run by members on democratic lines or by one or more owners who make the rules. In either case, they can determine membership criteria and the obligations of members. This usually involves sharing work necessary to maintain or develop the site.

The international naturist organizations were mainly composed of representatives of landed clubs. Nudist colony is no longer a favored term, and can be used by naturists to address landed clubs that have rigid non-inclusive membership criteria.

A holiday centre is a facility that specializes in providing apartments, chalets and camping pitches for visiting holidaymakers. A center is run commercially, and visitors are not members and have no say in the management. Most holiday centers expect visitors to hold an INF card (that is, to belong to an INF-affiliated organization), but some have relaxed this requirement, relying on the carrying of a trade card. Holiday centers vary in size. Larger holiday centres may have swimming pools, sports pitches, an entertainment program, kids' clubs, restaurants and supermarkets. Some holiday centres allow regular visitors to purchase their own chalets, and generations of the same families may visit each year. Holiday centres are more tolerant of clothing than members-only clubs; total nudity is usually compulsory in the swimming pools and may be expected on the beaches, while on the football pitches, or in the restaurants in the evening, it is rare.

A naturist resort is, to a European, a private property with accomodation and facilities where naturism is the norm. Centre Helio Marine in Vendays Montalivet, Aquitaine, France (the first naturist resort, extablished in 1950); the naturist village of Charco del Palo on Lanzarote, Canary Islands; Vera Playa in Spain; and Vritomartis Resort in Greece are examples.

In US usage, a naturist resort can mean a holiday centre.

Freikörperkultur (FKK)—literally translated as 'free body culture'—is the name for the general movement in Germany. The abbreviation is recognised also outside of Germany and can be found on informal signs indicating the direction to a remote naturist beach.

In some European countries, such as Denmark, all beaches are clothing optional, while in others like Germany and experimentally in France, there are naturist sunbathing areas in public parks, e.g., in Munich and Berlin. Beaches in some holiday destinations, such as Crete, are also clothing-optional, except some central urban beaches. There are two centrally located clothes-optional beaches in Barcelona. Sweden allows nudity on all beaches.

In a survey by "The Daily Telegraph", Germans and Austrians were most likely to have visited a nude beach (28%), followed by Norwegians (18%), Spaniards (17%), Australians (17%), and New Zealanders (16%). Of the nationalities surveyed, the Japanese (2%) were the least likely to have visited a nude beach. This result may indicate the lack of nude beaches in Japan; however, the Japanese are open with regard to family bathing nude at home and at onsen (hot springs).

From Woodstock to Edinburgh, and Nambassa in the southern hemisphere communal nudity is commonly recorded at music and counterculture festivals.

The Nambassa hippie festivals held in New Zealand in the late 1970s is an example of non-sexual naturism. Of the 75,000 patrons who attended the 1979 Nambassa 3 day counterculture Festival an estimated 35% of festival attendees spontaneously chose to remove their clothing, preferring complete or partial nudity.
A few camps organize activities in the nude, including the famous oil wrestling by camp Gymnasium.

Organized by the Federación Nudista de México (Mexican Nudist Federation) since 2016 when Zipolite beach nudity was legalized, FESTIVAL NUDISTA ZIPOLITE occurs annually on the first weekend of February.

Nudist festivals are held to celebrate particular days of the year, and in many such events nude bodypainting is also common, such as Neptune Day Festival held in Koktebel, Crimea to depict mythological events. 
The prevalence of naturism tends to increase during the summer months especially when the temperature is higher with some regions hosting first-time naturists and people who have recently started to practice the naturist lifestyle. Some studies have observed that among some of these naturists, they are clothed during other seasons, thus making them seasonal naturists.

Nudity in social contexts has been practised in various forms by many cultures at all time periods. In modern Western society, social nudity is most frequently encountered in the contexts of bathing, swimming and in saunas, whether in single-sex groups, within the family or with mixed-sex friends, but throughout history and in many tropical cultures until now, nudity is a norm at many sports events and competitions.

The first known use of the word "naturisme" occurred in 1778. A French-speaking Belgian, Jean Baptiste Luc Planchon (1734–1781), used the term to advocate nudism as a means of improving the "hygiène de vie" or healthy living.

The earliest known naturist club in the western sense of the word was established in British India in 1891. The 'Fellowship of the Naked Trust' was founded by Charles Edward Gordon Crawford, a widower, who was a District and Sessions Judge for the Bombay Civil Service. The commune was based in Matheran and had just three members at the beginning; Crawford and two sons of an Anglican missionary, Andrew and Kellogg Calderwood. The commune fell apart when Crawford was transferred to Ratnagiri; he died soon after in 1894.
In 1902, a series of philosophical papers was published in Germany by Dr. Heinrich Pudor, under the pseudonym Heinrich Scham, who coined the term "Nacktkultur". In 1906 he went on to write a three volume treatise with his new term as its title, which discussed the benefits of nudity in co-education and advocated participating in sports while being free of cumbersome clothing. Richard Ungewitter ("Nacktheit", 1906, "Nackt", 1908, etc.) proposed that combining physical fitness, sunlight, and fresh air bathing, and then adding the nudist philosophy, contributed to mental and psychological fitness, good health, and an improved moral-life view. Major promoters of these ideas included Adolf Koch and Hans Suren. Germany published the first journal of nudism between 1902 and 1932.

The wide publication of those papers and others, contributed to an explosive worldwide growth of nudism, in which nudists participated in various social, recreational, and physical fitness activities in the nude. The first organized club for nudists on a large scale, "Freilichtpark" (Free-Light Park), was opened near Hamburg in 1903 by Paul Zimmerman.
In 1919, German doctor Kurt Huldschinsky discovered that exposure to sunlight helped to cure rickets in many children, causing sunlight to be associated with improved health.
In France in the early 20th century, the brothers Gaston and André Durville, both of them physicians, studied the effects of psychology, nutrition, and environment on health and healing. They became convinced of the importance of natural foods and the natural environment on human well-being and health. They named this concept . The profound effect of clean air and sunlight on human bodies became evident to them and so nudity became a part of their naturism.
Naturism became a more widespread phenomenon in the 1920s, in Germany, the United Kingdom, France and other European countries and spread to the United States where it became established in the 1930s.

By 1951, the national federations united to form the International Naturist Federation or INF. Some naturists preferred not to join clubs, and after 1945, pressure was put to designate beaches for naturist use.
From the middle of the 20th century, with changing leisure patterns, commercial organisations began opening holiday resorts to attract naturists who expected the same – or better – standards of comfort and amenity offered to non-naturists. More recently, naturist holiday options have expanded to include cruises.

Naturism was part of a literary movement in the late 1800s (see the writings of André Gide) which also influenced the art movements of the time specifically Henri Matisse and other Fauve painters. This movement was based on the French concept of "joie de vivre", the idea of reveling freely in physical sensations and direct experiences and a spontaneous approach to life.


There are documented psychological benefits of naturist activities, including greater life satisfaction, more positive body image, and higher self-esteem. Social nudity leads to acceptance in spite of differences in age, body shape, fitness, and health.

Christian naturism contains various members associated with most denominations. Although beliefs vary, a common theme is that much of Christianity has misinterpreted the events regarding the Garden of Eden, and God was displeased with Adam and Eve for covering their bodies with fig leaves.

In most European countries, nudity is not explicitly forbidden. Whether it is tolerated on beaches which are not marked as official nudist beaches varies greatly. The only country with substantially different laws is Denmark, where beach nudity is explicitly allowed on all beaches, except for two in the far west of the country.

Organized naturism in Belgium began in 1924 when engineer Joseph-Paul Swenne founded the Belgian League of Heliophilous Propaganda (usually abbreviated to ) in Uccle near Brussels. This was followed four years later by , founded by Jozef Geertz and hosted on the country estate of entrepreneur Oswald Johan de Schampelaere. Belgian naturism was influenced in equal part by French naturism and German . Today Belgian naturists are represented by the (FBN).

Croatia is world-famous for naturism, which accounts for about 15% of its tourism industry. It was also the first European country to develop commercial naturist resorts. During a 1936 Adriatic cruise, King Edward VIII and Wallis Simpson stopped at a beach on the island of Rab where King Edward obtained a special permission from the local government to swim naked, thereby designating it the world's first official nude beach.

In Finnish culture, nudism is considered to be a relatively normal way to live. It is not uncommon to see entire families spending time together naked. Families may be naked while bathing in a sauna, swimming in a pool, or playing on a beach, and it's not unusual to see children playing naked in a family yard for example. Nudity as a whole is considered less taboo than many other countries.

Marcel Kienné de Mongeot is credited with starting naturism in France in 1920. His family had suffered from tuberculosis, and he saw naturism as a cure and a continuation of the traditions of the ancient Greeks. In 1926, he started the magazine ' (later called ') and the first French naturist club, at Garambouville, near Evreux. The court action that he initiated, established that nudism was legal on private property that was fenced and screened.

Drs. André and Gaston Durville bought a 70 hectare site on the Île du Levant where they established the village of Héliopolis. The village was open to the public. In 1925 Dr François Fougerat de David de Lastours wrote a thesis on heliotherapy, and in that year opened the . In 1936, the naturist movement was officially recognised.

Albert and Christine Lecocq were active members of many of these clubs, but after disagreements left and In 1944 Albert and Christine Lecocq founded the with members in 84 cities. In 1948 they founded the , in 1949 they started the magazine, "", and in 1950 opened the CHM Montalivet, the world's first naturist holiday centre, where the INF was formed.

German naturism was part of the movement and the youth movement of 1896, from Steglitz, Berlin which promoted ideas of fitness and vigour. At the same time doctors of the were using heliotherapy, treating diseases such as TB, rheumatism and scrofula with exposure to sunlight.

, a term coined in 1903 by Heinrich Pudor, flourished. connected nudity, vegetarianism and social reform. It was practised in a network of 200 members clubs. The movement gained prominence in the 1920s as offering a health giving life-style with Utopian ideals. Germany published the first naturist journal between 1902 and 1932.
It became politicised by radical socialists who believed it would lead to classlessness and a breaking down of society. It became associated with pacificism.

In 1926, Adolf Koch established a school of naturism in Berlin; encouraging a mixing of the sexes, open air exercises, and a programme of "sexual hygiene". In 1929, the Berlin school hosted the first International Congress on Nudity.

After the war, East Germans were free to practice naturism, chiefly at beaches rather than clubs (private organizations being regarded as potentially subversive). Naturism became a large element in DDR politics. The subsection of the Workers Sports Organisation had 60,000 members.
Today, following reunification there are many clubs, parks and beaches open to naturists,
though nudity has become less common in the former eastern zone. Germans are typically the most commonly seen foreigners at nude beaches in France and around Europe.

Public nudity is prohibited in Greece and there are no official nude beaches. There are, however, numerous unofficial nude beaches especially on the islands frequented by tourists, like Crete, Mykonos or Karpathos but also on smaller islands like Skopelos or Skiathos where nudity is tolerated, usually at the more remote ends or secluded areas of beaches.

On the other hand, toplessness is not illegal and is widely practiced by locals and tourists alike as there are no cultural taboos against it.

Public nudity is generally prohibited in Italy as a civil offence and can be punished with high fines, with the exception of the official naturist beaches and places where's a tradition of naturist attendance, as shown by a recent absolution sentence. Furthermore, in the recent decade, some regions have created laws to help the naturist tourism industry, and actually there are thirteen official naturist beaches in all Italy, where nudity is officially guaranteed by administrative acts. On all other public beaches in Italy, police can potentially impose substantial fines.

On the other hand, female toplessness has been officially legalized (in a nonsexual context) in all public beaches and swimming pools throughout the country (unless otherwise specified by region, province or municipality by-laws) on 20 March 2000, when the Supreme Court of Cassation (through sentence No. 3557) has determined that the exposure of the nude female breast, after several decades, is now considered a "commonly accepted behavior", and therefore, has "entered into the social costume".

The oldest Dutch naturist association is ("Sun and Life"), founded in 1946 with the aim of promoting healthy physical and mental development and a natural way of life. The national association is (NFN), which in 2017 adopted the new brand name ("Simply Naked") in an effort to become more accessible to casual naturists and strengthen the acceptance of nude recreation.

In general, Dutch people are very tolerant of beach nudity, as long as it does not impact on others, or involve inappropriate staring or sexual behaviour. Topless sunbathing is permitted on most beaches except where prohibited by signage.

The "Federação Portuguesa de Naturismo" (Portuguese Naturist Federation) or FPN was founded on 1 March 1977 in Lisbon. In the 21th century, naturism is considered a tolerated practice, whereas there are many officially-designated nudist beaches.

In today's Poland naturism is practiced in number of the seaside and inland beaches. Most Polish beaches are actually clothing-optional rather than naturist. One such beach is Międzyzdroje-Lubiewo.

Beginnings of naturism in Slovenia started in the year 1852, when a 29 year old Swiss physician Arnold Rikli visited Bled for the first time. In the following years he started to promote healthy way of living, because he considered water, air and light to be the source for his healing therapy. He continued to build spa centers which included light therapy and hydrotherapy treatment.

Public nudity in Spain is not illegal since there is no law banning its practice. Spanish legislation foresees felony for exhibitionism but restricts its scope to obscene exposure in front of children or mentally impaired individuals, i.e. with sexual connotation.

There are, however, some municipalities (like San Pedro del Pinatar) where public nudity has been banned by means of by-laws. Other municipalities (like Barcelona, Salou, Platja de Palma and Sant Antoni de Portmany) have used similar provisions to regulate partial nudity, requiring people to cover their torsos on the streets. Some naturist associations have appealed these by-laws on the grounds that a fundamental right (freedom of expression, as they understand nudism to be self-expression) cannot be regulated with such a mechanism. Some courts have ruled in favour of nudist associations.

Nudism in Spain is normally practised by the seaside, on beaches or small coves with a tradition of naturism. In Vera (Andalusia), there is a wide residential area formed by nudist urbanisations. Nudist organisations may organise some activities elsewhere in inner territory.

Legal provisions regarding partial nudity (or toplessness) are analogous to those regarding full nudity, but social tolerance towards toplessness is higher. The law does not require women to cover their breasts in public swimming, or on any beach in Spain. The governments of the municipalities of Galdakao and L'Ametlla del Vallès legalized female toplessness on their public pools in March 2016 and June 2018, respectively.

In the United Kingdom, the first official nudist club was established in Wickford, Essex in 1924. According to Michael Farrar, writing for British Naturism the club adopted the name "Moonella Group" from the name of the owner of the ground, "Moonella", and called its site The Camp. Moonella, who was still living in 1965 but whose identity remains to be discovered, had inherited a house with land in 1923 and made it available to certain members of the New Gymnosophy Society. This society had been founded a few years before by H.C. Booth, M.H. Sorensen and Rex Wellbye under the name of the English Gymnosophical Society. It met for discussions at the Minerva Cafe at 144 High Holborn in London, the headquarters of the Women's Freedom League. Those who were permitted to join the Moonella Group were carefully selected, and the club was run by an "aristocracy" of the original members, all of whom had "club names" to preserve their anonymity. The club closed in 1926 because of building on adjacent land.
By 1943 there were a number of these so-called "sun clubs" and together they formed the British Sun Bathers Association or BSBA. In 1954 a group of clubs unhappy with the way the BSBA was being run split off to form the Federation of British Sun Clubs or FBSC. In 1961, the BSBA Annual Conference agreed that the term nudist was inappropriate and should be discarded in favour of naturist. The two organisations rivalled each other before eventually coming together again in 1964 as the Central Council for British Naturism or CCBN. This organisation structure has remained much the same but it is now called British Naturism which is often abbreviated to BN.

The first official nude beach was opened at Fairlight Glen in Covehurst Bay near Hastings in 1978 (not to be confused with Fairlight Cove, which is 2 km to the east) followed later by the beaches at Brighton and Fraisthorpe. Bridlington opened in April 1980.

Australia's first naturist club was founded in Sydney in 1931 by the French-born anarchist and pacifist Kleber Claux. In 1975, the southern half of Maslin Beach, south of Adelaide was declared Australia's first official nude beach. The beach is almost long, so the area reserved for nude bathing is away from other beach users.

Nudist clubs (known as "sun clubs") were established in Dunedin and Auckland in early 1938; the Auckland Sun Group went into recess shortly afterwards due to the outbreak of World War II. In 1958 the allied nudist clubs of New Zealand established the New Zealand Sunbathing Association, later renamed the New Zealand Naturist Federation. The Federation today includes 17 affiliated clubs with a total membership, in 2012, of 1,600 people. In 2016 the Federation in conjunction with Tourism New Zealand hosted the World Congress of the International Naturist Federation at the Wellington Naturist Club, marking the second time the Congress had ever been held in the Southern Hemisphere.

Outside formal naturist organizations, social nudity is practised in a variety of contexts in New Zealand culture. It is a feature of many summer music festivals, including Convergence, Kiwiburn, Luminate, Rhythm & Vines, and Splore, in a tradition going back to Nambassa in the late 1970s. It is also associated with the culture of rugby, most prominently in the nude rugby match held in Dunedin each winter from 2002 to 2014 (and sporadically thereafter) as pre-match entertainment for the first professional rugby game of the season, and in the mock public holiday "National Nude Day", an event in which viewers of the TV2 talk show "SportsCafe" were invited – chiefly by former rugby player Marc Ellis, the show's most irrepressibly comic presenter – to send in photos and video of themselves performing daily activities in the nude.

Whilst a large proportion of New Zealanders today are tolerant of nudity, especially on beaches, there remains a contingent who consider it obscene. Naturists who engage in casual public nudity, even in places where this is lawful, run the risk of having the police called on them by disapproving people. Legally, nudity is permissible on any beach where it is "known to occur", in consequence of which New Zealand has no official nude beaches. The "indecent exposure" provision of the Summary Offences Act is in practice reserved for cases of public sexual gratification, but public nudity may still be prosecuted under the "offensive behaviour" provision.

In Canada, individuals around the country became interested in nudism, skinny-dipping, and physical culture in the early part of the 20th century. After 1940 they had their own Canadian magazine, "Sunbathing & Health", which occasionally carried local news. Canadians had scattered groups in several cities during the 1930s and 1940s, and some of these groups attracted enough interest to form clubs on private land. The most significant clubs were the Van Tan Club, formed in 1939, and continues today in North Vancouver, BC., and, in Ontario, the Sun Air Club.

Canadians who served in the military during the Second World War met like-minded souls from across the country, and often visited clubs while in Europe. They were a ready pool of recruits for post-war organizers. A few years later, the wave of post-war immigration brought many Europeans with their own extensive experience, and they not only swelled the ranks of membership, but often formed their own clubs, helping to expand nudism from coast to coast.

Most of those clubs united in the Canadian Sunbathing Association, which affiliated with the American Sunbathing Association in 1954. Several disagreements between eastern and western members of the CSA resulted in the breakup of CSA into the Western Canadian Sunbathing Association (WCSA) and the Eastern Canadian Sunbathing Association (ECSA) in 1960. The ECSA endured much in-fighting over the next decade and a half, leading to its official demise in 1978. The WCSA continues today as the American Association for Nude Recreation – Western Canadian Region (www.aanr-wc.com), a region of the American Association for Nude Recreation (AANR) which itself was formerly known as the ASA.

In 1977 the (FQN) was founded in Quebec, by Michel Vaïs, who had experienced European naturism at Montalivet. In 1985 the Federation of Canadian Naturists (FCN) was formed with the support of the FQN. In 1988 the FQN and FCN formed the FQN-FCN Union as the official Canadian representative in the International Naturist Federation (INF).

Federación Nudista de México is a members organization with both individual and organization members. It promotes social nudity in Mexico, and it is recognized by the International Naturist Federation as the official national naturist organization in that country.

As of 2016, Playa Zipolite is Mexico's first and only legal public nude beach. A "free beach" and unofficially nudist for more than 50 years, this beach is reputed to be the best place for nudism in the country. The numerous nude sunbathers, and the long tradition, make it safe for nudism and naturism. Annually since 2016, on the first weekend of February, Zipolite has hosted Festival Nudista Zipolite that in 2019 attracted 7,000-8,000 visitors.

Kurt Barthel founded the American League for Physical Culture in 1929 and organized the first nudist event. In about 1930 they organized the American Gymnosophical Association. Barthel founded America's first official nudist camp, Sky Farm in New Jersey, in May, 1932. Around 1932, AGA established the Rock Lodge Club as a nudist facility in Stockholm, New Jersey and Ilsley Boone, a Dutch Reformed minister, formed the Christian naturism movement. Naturism began expanding nationwide. Nudism venues were teetotal until 1970.

The American Association for Nude Recreation (AANR) is the national naturist organization. Arnd Krüger compared nudists in Germany and the United States and came to the conclusion that in Germany the racial aspects ("Zuchtwahl") were important for the breakthrough (e.g. the Commanding General of the Army served as patron for nudists events), while in the U.S. nudism was far more commercial and had thus more difficulties.

In 2008, Florida Young Naturists held their first Naked Bash, which has since been repeated multiple times per year and has grown into one of the largest young naturist gatherings in the world.

In 2009, a campaign to promote Nudism in the United States occurred with an effort by AANR to record the largest simultaneous Skinny Dip at several U.S. Clubs and beaches, occurring on July 11 of that year.

In 2010, a new organization formed called Young Naturists and Nudists America which was mostly focused around the younger generation as well as social issues, such as body image. Young Naturists and Nudists America closed in 2017.

In the seventies, nudity on Bali's remote and deserted beaches was common but with the massive growth of tourism, this practice has disappeared. In 2002, nudity was declared illegal on Petitenget Beach, the last beach in Seminyak that tolerated discreet nudity. Individuals began to practice nudity in private villas and resorts. Laki Uma Villa, the first naturist facility to open, was for gay men only. Bali au Naturel, the first adult-only nudist resort for both genders, opened its doors in 2004. It subsequently expanded from 3 to 15 rooms and added from two more swimming pools. Indonesia has an underground naturist community, who defy the laws against public nudity there.

Nudism was successfully introduced in 2012 by The Thailand Naturist Association in Pattaya (Chan Resort), and six more nudist resorts have been created all over Thailand. Barefeet Resort in Bangkok, Lemon Tree in Phuket, Oriental Village in Chiangmai, Phuan Naturist Village in Huay Yai, and Peace Blue Naturist resort in Phukett all members of the Naturist Association of Thailand as well as other international naturist organizations.

Magazines published by, for or purportedly about naturists can be grouped:


Magazines in the second and, occasionally, third grouping feature naturist editorial and advertising, while some naturists argue over which magazines belonged in which of these categories – these views may change as publishers and editors change. Many clubs and groups have benefitted from magazines which, while not exclusively or even predominantly naturist in character, made naturist information available to many who would not otherwise have been aware of it. (These days, the information and advertising provided online, and the wide availability of free online porn, has meant the disappearance of old-style 'skin' magazines presenting significant glamour content masquerading as or alongside naturist content. Naturist magazines have to appeal strongly to naturists to succeed – they cannot sit on the fence between naturism and glamour.)
Some naturists still feel that the worthwhile editorial content in some magazines is not a fair balance for the disapproved-of photographic content.

Some naturist clubs have been willing to allow filming by the media on their grounds, though content that proved not to be of genuine naturism can end up being parodied by the media as the norm.

Some commercial 'naturist' DVDs are dominated by imagery of naked children. Such material can be marketed in ways that appear to appeal directly to paedophile inclinations, and ownership of these DVDs (and their earlier video cassette incarnations) has resulted in successful British prosecutions for possession of indecent images of children. One case was appealed, unsuccessfully, to the European Court of Human Rights. The precedents set by the court cases mean that possession in Britain of any naturist image of a child is, potentially, grounds for prosecution.

Photo shoots, including major high-profile works by Spencer Tunick, are done in public places including beaches.








</doc>
<doc id="21916" url="https://en.wikipedia.org/wiki?curid=21916" title="Nordea">
Nordea

Nordea Bank Abp (), commonly referred to as Nordea, is a European financial services group operating in northern Europe and based in Helsinki, Finland. The bank is the result of the successive mergers and acquisitions of the Finnish, Danish, Norwegian and Swedish banks of Merita Bank, Unibank, Kreditkassen (Christiania Bank) and Nordbanken that took place between 1997 and 2000. The Baltic states are today also considered part of the home market. The largest shareholder of Nordea is Sampo, a Finnish insurance company with around 20% of the shares. Nordea is listed on the Copenhagen Stock Exchange, Helsinki Stock Exchange and Stockholm Stock Exchange.

Nordea operates across both the Nordic and Baltic regions with over 1,400 branches. The bank is present in 20 countries around the world, operating through full-service branches, subsidiaries and representative offices, although it primarily provides services in Finland, Norway, Denmark, Sweden, Estonia, Latvia and Lithuania.

Nordea serves 11 million private and 700,000 active corporate customers. The group also operates an Internet bank, which has more than 5.9 million online customers and performs more than 260 million payments per year.

Nordea is the result of the successive mergers and acquisitions of the Swedish, Finnish, Danish and Norwegian banks of Nordbanken, Merita Bank, Unibank and Kreditkassen (Christiania Bank og Kreditkasse) that took place between 1997 and 2000.

PK-banken was formed in 1974 by a merger between Postbanken (formed 1884) and Sveriges Kreditbank (formed 1923), both state-owned.

The private Nordbanken was formed in 1986 by a merger of two smaller private local banks, Uplandsbanken and Sundsvallsbanken. The Swedish banking crisis of 1991, resulting from deregulated markets and a housing price bubble, forced the government to nationalise Nordbanken for 64 billion kronor. Bad debts were transferred to the asset-management companies Securum and Retriva, which sold off the assets.

The name Nordea comes from the Swedish bank Nordbanken; this developed from PK-banken (Post och Kreditbanken) which in 1990 purchased the smaller private bank Nordbanken, and picked up that name. The name is also a contraction of the words Nordic and ideas.

Merita Bank was a 1995 merger of the former main rivals in Finland, the originally Svecoman Union Bank of Finland (Föreningsbanken i Finland) founded in 1862 and the Fennoman National Share Bank (Kansallis-Osake-Pankki) founded in 1889.

Nordea was the subject of an online phishing scam in 2007. The company estimated 8 million kr ($1.1 million) was stolen. Customers were targeted over a period of 15 months with phishing emails containing a trojan horse. Nordea refunded affected customers.

Nordea converted its subsidiaries operating in Denmark, Finland, and Norway to branches under the Swedish holding company, Nordea AB, in January 2017. In August 2017, DNB ASA and Nordea combined their operations in Estonia, Latvia and Lithuania to create Luminor Bank.

Nordea announced plans to move its corporate headquarters to Helsinki, Finland in September 2017. In October 2018, Nordea completed the move of its corporate headquarters to Helsinki, Finland.

In March 2019, public service broadcasting company, Yle, aired a program that revealed money laundering allegations against Nordea. The company was the biggest Nordic lender allegedly involved in the multi-million-dollar money laundering scheme, according to Bloomberg.

In October 2019, Nordic banks agreed to fund common payment services in order to boost businesses.

Nordea is owned by:


Nordea Markets is the international markets operation of Nordea. It handles a broad range of investment banking products and services including fixed income, currencies, commodities, equities, debt capital markets, and corporate finance. It also supplies advisory services and internationally acknowledged economic research and analysis.

There are approximately 2,200 employees including Financial Risk Control and Capital Markets Services. Its main operational centres are in Copenhagen (also the main trading floor), Helsinki, Oslo and Stockholm, and with regional offices also in Brazil (São Paulo), China (Beijing and Shanghai), Estonia (Tallinn), Germany (Frankfurt), Latvia (Riga), Lithuania (Vilnius), Luxembourg (Luxembourg City), Poland (Warsaw), Russia (Moscow), Singapore, Switzerland (Zurich), the United Kingdom (London), and United States (New York City).

The largest financial group in the Nordic region, Nordea has, despite warnings from the Swedish Financial Supervisory Authority (FI) been active in using offshore companies in tax havens according to the Panama papers.

The Nordea section in Luxembourg, between the years 2004 and 2014, founded nearly 400 offshore companies in Panama and the British Virgin Islands for its customers.

The Swedish Financial Supervisory Authority (FI) has pointed out that there are "serious deficiencies" in how Nordea monitors money laundering, and has given the bank two warnings. In 2015, Nordea had to pay the largest possible fine - over 5 million EUR.

In 2012, Nordea asked Mossack Fonseca to change documents retroactively so that three Danish customers power of attorney documents had been in force since 2010.

The director for Nordea Private banking Thorben Sanders admits that before 2009 they did not screen for customers that tried to evade tax. "At the end of 2009 we decided that our bank should not be a means of tax evasion" says Thorben Sanders.

As a consequence of the leaked documents, the Swedish Financial Supervisory Authority (FI) stated on 4 April 2016 that it had started an investigation into the conduct of Nordea, the largest financial group in the Nordic region. The Swedish minister of Finance Magdalena Andersson characterized the conduct of Nordea as "a crime" and "totally unacceptable". Nordea CEO Casper von Koskull stated that he was disappointed with the shortcomings within Nordea's operating principles, saying that "this cannot be tolerated".

Other Swedish banks are mentioned in the documents, but mention of Nordea occurs 10,902 times and the second-most mentioned bank has 764 matches.

Stefan Löfven, Prime Minister of Sweden, said in 2016 that he was very critical of the conduct of Nordea and its role, and said: "They are on the list of shame too".

Nordea bank loaned billions of euros to shipping companies that own vessels in secrecy jurisdictions such as Bermuda, Cyprus, Panama, BVI, the Cayman Islands and the Isle of Man. In the Paradise Papers, Nordea was shown to have lent a significant amount of money to customers based in tax havens.





</doc>
<doc id="21918" url="https://en.wikipedia.org/wiki?curid=21918" title="Normal subgroup">
Normal subgroup

In abstract algebra, a normal subgroup is a subgroup that is invariant under conjugation by members of the group of which it is a part. In other words, a subgroup of the group is normal in if and only if for all and . The usual notation for this relation is formula_1.

Normal subgroups are important because they (and only they) can be used to construct quotient groups of the given group. Furthermore, the normal subgroups of are precisely the kernels of group homomorphisms with domain , which means that they can be used to internally classify those homomorphisms.

Évariste Galois was the first to realize the importance of the existence of normal subgroups.

A subgroup of a group is called a normal subgroup of if it is invariant under conjugation; that is, the conjugation of an element of by an element of is always in . The usual notation for this relation is formula_1, and the definition may be written in symbols as
formula_3

For any subgroup of , the following conditions are equivalent to being a normal subgroup of . Therefore, any one of them may be taken as the definition:




Given two normal subgroups, and , of , their intersection formula_19and their product formula_20 are also normal subgroups of . 

The normal subgroups of form a lattice under subset inclusion with least element, , and greatest element, . The meet of two normal subgroups, and , in this lattice is their intersection and the join is their product.

The lattice is complete and modular.

If is a normal subgroup, we can define a multiplication on cosets as follows:formula_21This relation defines a mapping formula_22. To show that this mapping is well-defined, one needs to prove that the choice of representative elements formula_23 does not affect the result. To this end, consider some other representative elements formula_24. Then there are formula_25 such that formula_26. It follows that formula_27where we also used the fact that formula_28 is a "normal" subgroup, and therefore there is formula_29 such that formula_30. This proves that this product is a well-defined mapping between cosets.

With this operation, the set of cosets is itself a group, called the quotient group and denoted with . There is a natural homomorphism, , given by . This homomorphism maps formula_28 into the identity element of , which is the coset , that is, formula_32.

In general, a group homomorphism, sends subgroups of to subgroups of . Also, the preimage of any subgroup of is a subgroup of . We call the preimage of the trivial group in the kernel of the homomorphism and denote it by . As it turns out, the kernel is always normal and the image of , , is always isomorphic to (the first isomorphism theorem). In fact, this correspondence is a bijection between the set of all quotient groups of , , and the set of all homomorphic images of (up to isomorphism). It is also easy to see that the kernel of the quotient map, , is itself, so the normal subgroups are precisely the kernels of homomorphisms with domain .










</doc>
<doc id="21919" url="https://en.wikipedia.org/wiki?curid=21919" title="Munkar and Nakir">
Munkar and Nakir

Munkar and Nakir () (English translation: "The Denied and The Denier") in Islamic eschatology, are angels who test the faith of the dead in their graves.

These angels are described as having solid black eyes, having a shoulder span measured in miles, and carrying hammers "so large, that if all of mankind tried at once to move them a single inch, they would fail". When they speak, tongues of fire come from their mouths. If one answers their questions incorrectly, one is beaten every day, other than Friday, until Allah (God) gives permission for the beating to stop.

Muslims believe that after a person dies, his soul passes through a stage called barzakh, where it exists in the grave (even if the person's body was destroyed, the soul will still rest in the earth near their place of death). The questioning will begin when the funeral and burial is over. Nakir and Munkar prop the deceased soul upright in the grave and ask three questions: 


A righteous believer will respond correctly, saying that their Lord is Allah, that Muhammad is their prophet and that their religion is Islam. If the deceased answers correctly, the time spent awaiting the resurrection is pleasant and may enter heaven. Those who do not answer as described above are chastised until the day of judgment. There is belief that the fire which represents the own bad deeds can already be seen in Barzakh, and that the spiritual pain caused by this can lead to purification of the soul.

Muslims believe that a person will correctly answer the questions not by remembering the answers before death but by their iman (faith) and deeds such as salat (prayer) and shahadah (the Islamic profession of faith).

Munkar and Nakir bear some similarity to Zoroastrian divinities. Some of these, such as Mithra, Sraosha and Rashnu have a role in the judgement of souls. Rashnu is described as a figure who holds a set of scales, like some angels of the grave. E.G. Brown has suggested that a continuity exists between Rashnu and Munkar and Nakir. 
A mythical figure in Mandaean religion, Abathur Muzania is similar to Rashnu. He holds the same position in the world of the dead, and also holds a set of scales. "Muzania" means scales (mizan) in Aramaic. According to recent research, Munkar and Nakir are originally astrological figures and a transformation of the Mesopotamian astral god Nergal. 
Aksoy shows in his new research that the Mesopotamian god Nergal has almost the same characteristics as Munkar and Nakir. He begins with "Nakru" which is an epithet of Nergal and means 'enemy'. The Assyrian "nakru", like the names Munkar and Nakir, comes from the same root, from the proto-Semitic NKR. Some scholars use a different spelling; "nakuru". which is almost the same as Nakir. Moreover, Nergal is a lord of the Underworld and the grave (Assyrian "qabru": grave). Like Munkar and Nakir, he has a terrifying voice that can cause panic among men and gods. He holds a shining mace and his breath can burn his enemies. Because he is related to fire most scholars suggest that he was originally a sun god. Furthermore, he is identified with the celestial twins ("Gemini") in the Babylonian astral mythology which forms a direct link to Munkar and Nakir.
There is no reference to Munkar and Nakir in the Quran. Their names are first mentioned by Tirmidhi in the hadith tradition. Tirmidhi is known to have visited Iraq. This suggests that the names of Munkar and Nakir are introduced to Islamic beliefs during an early stage in the Islamization of Mesopotamia (or Iraq). The Mesopotamians still believed in the sun god Shamash, as well as Nergal and several other Babylonian gods at the time Islam was introduced. Thus, Nergal the god of the Underworld who is symbolized by the planet Mars, is a possible prototype for Munkar and Nakir. Astrologically, Munkar and Nakir share more clues in their Martian characteristics which connect them to Nergal.



</doc>
<doc id="21920" url="https://en.wikipedia.org/wiki?curid=21920" title="Napalm">
Napalm

Napalm is an incendiary mixture of a gelling agent and a volatile petrochemical (usually gasoline (petrol) or diesel fuel). The title is a portmanteau of the names of two of the constituents of the original thickening and gelling agents: co-precipitated aluminium salts of naphthenic acid and palmitic acid. Napalm B is the more modern version of napalm (utilizing Polystyrene derivatives) and, although distinctly different in its chemical composition, is often referred to simply as "napalm".
A team led by chemist Louis Fieser originally developed napalm for the United States Chemical Warfare Service in 1942 in a secret laboratory at Harvard University. Of immediate first interest was its viability as an incendiary device to be used in fire bombing campaigns during World War II; its potential to be coherently projected into a solid stream that would carry for distance (instead of the bloomy fireball of pure gasoline) resulted in widespread adoption in infantry/combat engineer flamethrowers as well.

Napalm burns at temperatures ranging from . In addition, it burns for a greater duration than gasoline, as well as being more easily dispersed and sticking tenaciously to its targets. These traits make it extremely effective (and controversial) in the anti-structure and antipersonnel role. It has been widely used in both the air and ground role, with the largest use to date being via air-dropped bombs in World War II (most notably in the devastating incendiary attacks on Japanese cities in 1945), and later close air support roles in Korea and Vietnam. Napalm also has fueled most of the flamethrowers (tank, ship and infantry-based) used since World War II, giving them much greater range, and was used in this role as a common weapon of urban combat by both the Axis and the Allies in World War II. Multiple nations (including the United States, China, Russia, Iran and North Korea) maintain large stockpiles of napalm-based weapons of various types.

Napalm was used in flamethrowers, bombs and tanks in World War II. It is believed to have been formulated to burn at a specific rate and to adhere to surfaces to increase its stopping power. During combustion, napalm rapidly deoxygenates the available air and generates large amounts of carbon monoxide and carbon dioxide.

Alternative compositions exist for different uses, e.g. triethylaluminium, a pyrophoric compound that aids ignition.

Use of fire in warfare has a long history. Greek fire, also described as "sticky fire" (πῦρ κολλητικόν, "pýr kolletikón"), is believed to have had a petroleum base. The development of napalm was precipitated by the use of jellied gasoline mixtures by the Allied forces during World War II. Latex, used in these early forms of incendiary devices, became scarce, since natural rubber was almost impossible to obtain after the Japanese army captured the rubber plantations in Malaya, Indonesia, Vietnam, and Thailand.

This shortage of natural rubber prompted chemists at US companies such as DuPont and Standard Oil, and researchers at Harvard University, to develop factory-made alternatives—artificial rubber for all uses, including vehicle tires, tank tracks, gaskets, hoses, medical supplies and rain clothing. A team of chemists led by Louis Fieser at Harvard University was the first to develop synthetic napalm, during 1942. "The production of napalm was first entrusted to Nuodex Products, and by the middle of April 1942 they had developed a brown, dry powder that was not sticky by itself, but when mixed with gasoline turned into an extremely sticky and inflammable substance." One of Fieser's colleagues suggested adding phosphorus to the mix which increased the "ability to penetrate deeply...into the musculature, where it would continue to burn day after day."

On 4 July 1942, the first test occurred on the football field near the Harvard Business School. Tests under operational conditions were carried out at Jefferson Proving Ground on condemned farm buildings, and subsequently at Dugway Proving Ground on buildings designed and constructed to represent those to be found in German and Japanese towns. This new mixture of chemicals was widely used in the Second World War in incendiary bombs and in flamethrowers.

From 1965 to 1969, the Dow Chemical Company manufactured napalm B for the American armed forces. After news reports of napalm B's deadly and disfiguring effects were published, Dow Chemical experienced boycotts of its products, and its recruiters for new chemists, chemical engineers, etc., graduating from college were subject to campus boycotts and protests. The management of the company decided that its "first obligation was the government." Meanwhile, napalm B became a symbol for the Vietnam War.

Napalm was first employed in incendiary bombs and went on to be used as fuel for flamethrowers.

The first recorded strategic use of napalm incendiary bombs occurred in an attack by the US Army Air Force on Berlin on 6 March 1944, using American AN-M76 incendiary bombs with PT-1 (Pyrogel) filler.
The first known tactical use by the USAAF was by the 368th Fighter Group, Ninth Air Force Northeast of Compiègne, France 27 May 1944
and the British De Havilland Mosquito FB Mk.VIs of No. 140 Wing RAF, Second Tactical Air Force on 14 July 1944, which also employed the AN-M76 incendiary in a reprisal attack on the 17th SS Panzergrenadier Division ""Götz von Berlichingen"" in Bonneuil-Matours. Soldiers of this Waffen SS unit had captured and then killed a British SAS prisoner-of-war, Lt. Tomos Stephens, taking part in Operation Bulbasket, and seven local Resistance fighters. Although it was not known at the time of the air strike, 31 other POWs from the same SAS unit, and an American airman who had joined up with the SAS unit, had also been executed.

Further use of napalm by American forces occurred in the Pacific theater of operations, where in 1944 and 1945, napalm was used as a tactical weapon against Japanese bunkers, pillboxes, tunnels, and other fortifications, especially on Saipan, Iwo Jima, the Philippines, and Okinawa, where deeply dug-in Japanese troops refused to surrender. Napalm bombs were dropped by aviators of the U.S. Navy, the United States Army Air Forces, and the U.S. Marine Corps in support of ground troops.

When the U.S. Army Air Forces on the Marianas Islands ran out of conventional thermite incendiary bombs for their B-29 Superfortresses to drop on large Japanese cities, its top commanders, such as General Curtis LeMay, used napalm bombs to continue with fire raids.

In the European Theater of Operations napalm was used by American forces in the siege of La Rochelle in April 1945 against German soldiers (and inadvertently French civilians in Royan) – about two weeks before the end of the war.

In its first known post-WWII use, U.S.-supplied napalm was used in the Greek Civil War by the Greek National Army as part of Operation Coronis against the Democratic Army of Greece (DSE) – the military branch of the Communist Party of Greece (KKE).

Napalm was also widely used by the United States during the Korean War. The ground forces in North Korea holding defensive positions were often outnumbered by Chinese and North Koreans, but U.S. Air Force and Navy aviators had control of the air over nearly all of the Korean Peninsula. Hence, the American and other U.N. aviators used napalm B for close air support of the ground troops along the border between North Korea and South Korea, and also for attacks in North Korea. Napalm was used most notably during the battle "Outpost Harry" in South Korea during the night of 10–11 June 1953. Eighth Army chemical officer Donald Bode reported that on an "average good day" UN pilots used 70,000 gallons of napalm, with approximately 60,000 gallons of this thrown by US forces. The "New York Herald Tribune" hailed "Napalm, the No. 1 Weapon in Korea". Winston Churchill, among others, criticized American use of napalm in Korea, calling it "very cruel", as the US/UN forces, he said, were "splashing it all over the civilian population", "tortur[ing] great masses of people". The American official who took this statement declined to publicize it.

At the same time the French Air Force regularly used napalm for close air support of ground operations in the First Indochina War (1946–1954). At first the canisters were simply pushed out the side doors of Ju-52 planes that had been captured in Germany, later mostly B-26 bombers were used.

Napalm became an intrinsic element of U.S. military action during the Vietnam War as forces made increasing use of it for its tactical and psychological effects. Reportedly about 388,000 tons of U.S. napalm bombs were dropped in the region between 1963 and 1973, compared to 32,357 tons used over three years in the Korean War, and 16,500 tons dropped on Japan in 1945. The U.S. Air Force and U.S. Navy used napalm with great effect against all kinds of targets, such as troops, tanks, buildings, jungles, and even railroad tunnels. The effect was not always purely physical as napalm had psychological effects on the enemy as well.

A variant of napalm was produced in Rhodesia for a type of ordnance known as "Frantan" between 1968 and 1978 and was deployed extensively by the Rhodesian Air Force during that country's bush war. In May 1978, Herbert Ushewokunze, minister of health for the Zimbabwe African National Union (ZANU) produced photographic evidence of purported civilian victims of Rhodesian napalm strikes, which he circulated during a tour of the US. The government of Mozambique and the Zimbabwe African People's Union (ZAPU) also issued claims at around the same time that napalm strikes against guerrilla targets had become a common feature in Rhodesian military operations both at home and abroad.

The South African Air Force frequently deployed napalm from Atlas Impala strike aircraft during raids on guerrilla bases in Angola during the South African Border War.

Other instances of napalm's use include by France during the Algerian War (1954–1962), the Portuguese Colonial War (1961–1974), the Six-Day War by Israel (1967), in Nigeria (1969), India and Pakistan (1965 and 1971), Egypt (1973), by Morocco during the Western Sahara War (1975–1991), by Argentina (1982), by Iran (1980–88), by Iraq (1980–88, 1991), By IPKF (Indian Peace keeping force) in 1987 against Tamils (LTTE) in Sri Lanka, by Angola during the Angolan Civil War, and Yugoslavia (1991–1996). Recently, Turkey has been accused of using Napalm in its war against Kurdish militias over Afrin. Turkey's General Staff, however, denies this.

When used as a part of an incendiary weapon, napalm can cause severe burns (ranging from superficial to subdermal), asphyxiation, unconsciousness, and death. In this implementation, napalm fires can create an atmosphere of greater than 20% carbon monoxide and firestorms with self-perpetuating winds of up to .

Napalm is effective against dug-in enemy personnel. The burning incendiary composition flows into foxholes, trenches and bunkers, and drainage and irrigation ditches and other improvised troop shelters. Even people in undamaged shelters can be killed by hyperthermia, radiant heat, dehydration, asphyxiation, smoke exposure, or carbon monoxide poisoning.

One firebomb released from a low-flying plane can damage an area of .

International law does not specifically prohibit the use of napalm or other incendiaries against military targets, but use against civilian populations was banned by the United Nations Convention on Certain Conventional Weapons (CCW) in 1980. Protocol III of the CCW restricts the use of all incendiary weapons, but a number of countries have not acceded to all of the protocols of the CCW. According to the Stockholm International Peace Research Institute (SIPRI), countries are considered a party to the convention, which entered into force as international law in December 1983, as long as they ratify at least two of the five protocols. Approximately 25 years after the General Assembly adopted it, the United States signed it on 21 January 2009, President Barack Obama's first full day in office. Its ratification, however, is subject to a reservation that says that the treaty can be ignored if it would save civilian lives.




</doc>
<doc id="21921" url="https://en.wikipedia.org/wiki?curid=21921" title="Northern Crusades">
Northern Crusades

The Northern Crusades or Baltic Crusades were Christian colonization and Christianization campaigns undertaken by Catholic Christian military orders and kingdoms, primarily against the pagan Baltic, Finnic and West Slavic peoples around the southern and eastern shores of the Baltic Sea, and to a lesser extent also against Orthodox Christian Slavs (East Slavs).

The most notable campaigns were the Livonian and Prussian crusades. Some of these wars were called crusades during the Middle Ages, but others, including most of the Swedish ones, were first dubbed crusades by 19th-century romantic nationalist historians. However, crusades against Baltic indigenous peoples were authorized by Pope Alexander III in the bull "Non parum animus noster", in 1171 or 1172.

At the outset of the northern crusades, Christian monarchs across northern Europe commissioned forays into territories that comprise modern-day Estonia, Finland, Latvia, Lithuania, Poland and Russia. Pagans or eastern Orthodox Christians, the indigenous populations suffered forced baptisms and the ravages of military occupation. Spearheading, but by no means monopolizing these incursions, the ascendant Teutonic Order profited immensely from the crusades, as did German merchants who fanned out along trading routes traversing the Baltic frontier. 
The official starting point for the Northern Crusades was Pope Celestine III's call in 1195, but the Catholic kingdoms of Scandinavia, Poland and the Holy Roman Empire had begun moving to subjugate their pagan neighbors even earlier. The non-Christian people who were objects of the campaigns at various dates included:

Armed conflict between the Baltic Finns, Balts and Slavs who dwelt by the Baltic shores and their Saxon and Danish neighbors to the north and south had been common for several centuries before the crusade. The previous battles had largely been caused by attempts to destroy castles and sea trade routes to gain economic advantage in the region, and the crusade basically continued this pattern of conflict, albeit now inspired and prescribed by the Pope and undertaken by Papal knights and armed monks.

The campaigns started with the 1147 Wendish Crusade against the Polabian Slavs (or "Wends") of what is now northern and eastern Germany. The crusade occurred parallel to the Second Crusade to the Holy Land, and continued irregularly until the 16th century.

The Swedish crusades were campaigns by Sweden against Finns, Tavastians and Karelians during period from 1150 to 1293.

The Danes are known to have made two crusades to Finland in 1191 and in 1202. The latter one was led by the Bishop of Lund Anders Sunesen with his brother.

By the 12th century, the peoples inhabiting the lands now known as Estonia, Latvia and Lithuania formed a pagan wedge between increasingly powerful rival Christian states – the Orthodox Church to their east and the Catholic Church to their west. The difference in creeds was one of the reasons they had not yet been effectively converted. During a period of more than 150 years leading up to the arrival of German crusaders in the region, Estonia was attacked thirteen times by Russian principalities, and by Denmark and Sweden as well. Estonians for their part made raids upon Denmark and Sweden. There were peaceful attempts by some Catholics to convert the Estonians, starting with missions dispatched by Adalbert, Archbishop of Bremen in 1045-1072. However, these peaceful efforts seem to have had limited success.

Moving in the wake of German merchants who were now following the old trading routes of the Vikings, a monk named Meinhard landed at the mouth of the Daugava river in present-day Latvia in 1180 and was made bishop in 1186. Pope Celestine III proclaimed a crusade against the Baltic heathens in 1195, which was reiterated by Pope Innocent III and a crusading expedition led by Meinhard's successor, Bishop Berthold of Hanover, landed in Livonia (part of present-day Latvia, surrounding the Gulf of Riga) in 1198. Although the crusaders won their first battle, Bishop Berthold was mortally wounded and the crusaders were repulsed.

In 1199, Albert of Buxhoeveden was appointed by the Archbishop Hartwig II of Bremen to Christianise the Baltic countries. By the time Albert died 30 years later, the conquest and formal Christianisation of present-day Estonia and northern Latvia was complete. Albert began his task by touring the Empire, preaching a Crusade against the Baltic countries, and was assisted in this by a Papal Bull which declared that fighting against the Baltic heathens was of the same rank as participating in a crusade to the Holy Land. Although he landed in the mouth of the Daugava in 1200 with only 23 ships and 500 soldiers, the bishop's efforts ensured that a constant flow of recruits followed. The first crusaders usually arrived to fight during the spring and returned to their homes in the autumn. To ensure a permanent military presence, the Livonian Brothers of the Sword were founded in 1202. The founding by Bishop Albert of the market at Riga in 1201 attracted citizens from the Empire and economic prosperity ensued. At Albert's request, Pope Innocent III dedicated the Baltic countries to the Virgin Mary to popularize recruitment to his army and the name "Mary's Land" has survived up to modern times. This is noticeable in one of the names given to Livonia at the time, Terra Mariana (Land of Mary).

In 1206, the crusaders subdued the Livonian stronghold in Turaida on the right bank of Gauja River, the ancient trading route to the Northwestern Rus. In order to gain control over the left bank of Gauja, the stone castle was built in Sigulda before 1210. By 1211, the Livonian province of Metsepole (now Limbaži district) and the mixed Livonian-Latgallian inhabited county of Idumea (now Straupe) was converted to the Roman Catholic faith. The last battle against the Livonians was the siege of Satezele hillfort near to Sigulda in 1212. The Livonians, who had been paying tribute to the East Slavic Principality of Polotsk, had at first considered the Germans as useful allies. The first prominent Livonian to be christened was their leader Caupo of Turaida. As the German grip tightened, the Livonians rebelled against the crusaders and the christened chief, but were put down. Caupo of Turaida remained an ally of the crusaders until his death in the Battle of St. Matthew's Day in 1217.

The German crusaders enlisted newly baptised Livonian warriors to participate in their campaigns against Latgallians and Selonians (1208–1209), Estonians (1208–1227) and against Semigallians, Samogitians and Curonians (1219–1290).

After the subjugation of the Livonians the crusaders turned their attention to the Latgallian principalities to the east, along the Gauja and Daugava rivers. The military alliance in 1208 and later conversion from Greek Orthodoxy to Roman Catholicism of the Principality of Tālava was the only peaceful subjugation of the Baltic tribes during the Nordic crusades. The ruler of Tālava, Tālivaldis ("Talibaldus de Tolowa"), became the most loyal ally of German crusaders against the Estonians, and he died a Catholic martyr in 1215. The war against the Latgallian and Selonian countries along the Daugava waterway started in 1208 by occupation of the Orthodox Principality of Koknese and the Selonian Sēlpils hillfort. The campaign continued in 1209 with an attack on the Orthodox Principality of Jersika (known as "Lettia"), accused by crusaders of being in alliance with Lithuanian pagans. After defeat the king of Jersika, Visvaldis, became the vassal of the Bishop of Livonia and received part of his country (Southern Latgale) as a fiefdom. The Selonian stronghold of Sēlpils was briefly the seat of a Selonian diocese (1218–1226), and then came under the rule of the Livonian Order (and eventually the stone castle of Selburg was built in its place). Only in 1224, with the division of Tālava and Adzele counties between the Bishop of Rīga and the Order of the Swordbearers, did Latgallian countries finally become the possession of German conquerors. The territory of the former Principality of Jersika was divided by the Bishop of Rīga and the Livonian Order in 1239.

By 1208, the Germans were strong enough to begin operations against the Estonians, who were at that time divided into eight major and several smaller counties led by elders with limited co-operation between them. In 1208-27, war parties of the different sides rampaged through the Livonian, Northern Latgallian, and Estonian counties, with Livonians and Latgallians normally as allies of the Crusaders, and the Principalities of Polotsk and Pskov appearing as allies of different sides at different times. Hill forts, which were the key centres of Estonian counties, were besieged and captured a number of times. A truce between the war-weary sides was established for three years (1213–1215) and proved generally more favourable to the Germans, who consolidated their political position, while the Estonians were unable to develop their system of loose alliances into a centralised state. The Livonian leader Kaupo was killed in battle near Viljandi (Fellin) on 21 September 1217, but the battle was a crushing defeat for the Estonians, whose leader Lembitu was also killed. Since 1211, his name had come to the attention of the German chroniclers as a notable Estonian elder, and he had become the central figure of the Estonian resistance.

The Christian kingdoms of Denmark and Sweden were also greedy for conquests on the Eastern shores of the Baltic. While the Swedes made only one failed foray into western Estonia in 1220, the Danish Fleet headed by King Valdemar II of Denmark had landed at the Estonian town of Lindanisse (present-day Tallinn) in 1219. After the Battle of Lindanise the Danes established a fortress, which was besieged by Estonians in 1220 and 1223, but held out. Eventually, the whole of northern Estonia came under Danish control.

The last Estonian county to hold out against the invaders was the island county of Saaremaa (Ösel), whose war fleets had raided Denmark and Sweden during the years of fighting against the German crusaders.

In 1206, a Danish army led by king Valdemar II and Andreas, the Bishop of Lund landed on Saaremaa and attempted to establish a stronghold without success. In 1216 the Livonian Brothers of the Sword and the bishop Theodorich joined forces and invaded Saaremaa over the frozen sea. In return the Oeselians raided the territories in Latvia that were under German rule the following spring. In 1220, the Swedish army led by king John I of Sweden and the bishop Karl of Linköping conquered Lihula in Rotalia in Western Estonia. Oeselians attacked the Swedish stronghold the same year, conquered it and killed the entire Swedish garrison including the Bishop of Linköping.

In 1222, the Danish king Valdemar II attempted the second conquest of Saaremaa, this time establishing a stone fortress housing a strong garrison. The Danish stronghold was besieged and surrendered within five days, the Danish garrison returned to Revel, leaving bishop Albert of Riga's brother Theodoric, and few others, behind as hostages for peace. The castle was razed to the ground by the Oeselians.

A 20,000 strong army under Papal legate William of Modena crossed the frozen sea while the Saaremaa fleet was icebound, in January 1227. After the surrender of two major Oeselian strongholds, Muhu and Valjala, the Oeselians formally accepted Christianity.

In 1236, after the defeat of the Livonian Brothers of the Sword in the Battle of Saule, military action on Saaremaa broke out again. In 1261, warfare continued as the Oeselians had once more renounced Christianity and killed all the Germans on the island. A peace treaty was signed after the united forces of the Livonian Order, the Bishopric of Ösel-Wiek, and Danish Estonia, including mainland Estonians and Latvians, defeated the Oeselians by conquering their stronghold at Kaarma. Soon thereafter, the Livonian Order established a stone fort at Pöide.

Although the Curonians had attacked Riga in 1201 and 1210, Albert of Buxhoeveden, considering Courland a tributary of Valdemar II of Denmark, had been reluctant to conduct a large scale campaign against them. After Albert's death in 1229, the crusaders secured the peaceful submission of Vanemane (a county with a mixed Livonian, Oselian, and Curonian population in the northeastern part of Courland) by treaty in 1230. In the same year the papal vice-legat Baldouin of Alnea annulled this agreement and concluded an agreement with the ruler ("rex") of Bandava in the central Courland Lammechinus, delivering his kingdom into the hands of the papacy. Baldouin became the popes's delegate in Courland and bishop of Semigallia; however, the Germans complained about him to the Roman Curia, and in 1234 Pope Gregory IX removed Baldouin as his delegate.

After their decisive defeat in the Battle of Saule by the Samogitians and Semigallians, the remnants of the Swordbrothers were reorganized in 1237 as a subdivision of the Teutonic Order, and became known as the Livonian Order. In 1242, under the leadership of the master of the Livonian Order Andrew of Groningen, the crusaders began the military conquest of Courland. They defeated the Curonians as far south as Embūte, near the contemporary border with Lithuania, and founded their main fortress at Kuldīga. In 1245 Pope Innocent IV allotted two-thirds of conquered Courland to the Livonian Order, and one third to the Bishopric of Courland.

At the Battle of Durbe in 1260 a force of Samogitians and Curonians overpowered the united forces of the Livonian and Teutonic Orders; over the following years, however, the Crusaders gradually subjugated the Curonians, and in 1267 concluded the peace treaty stipulating the obligations and the rights of their defeated rivals. The unconquered southern parts of their territories (Ceklis and Megava) were united under the rule of the Grand Duchy of Lithuania.

The conquest of Semigallian counties started in 1219 when crusaders from Rīga occupied Mežotne, the major port on the Lielupe waterway, and founded the Bishopric of Semigallia. After several unsuccessful campaigns against the pagan Semigallian duke Viestards and his Samogitian kinsfolk, the Roman Curia decided in 1251 to abolish the Bishopric of Semigallia, and divided its territories between the Bishopric of Rīga and the Order of Livonia. In 1265 a stone castle was built at Jelgava, on the Lielupe, and became the main military base for crusader attacks against the Semigallians. In 1271 the capital hillfort of Tērvete was conquered, but Semigallians under the Duke Nameisis rebelled in 1279, and the Lithuanians under Traidenis defeated Livonian Order forces in the Battle of Aizkraukle. Duke Nameisis' warriors unsuccessfully attacked Rīga in 1280, in response to which around 14,000 crusaders besieged Turaida castle in 1281. To conquer the remaining Semigallian hillforts the Order's master Villekin of Endorpe built a castle called "Heiligenberg" right next to the Tērvete castle in 1287. The same year the Semigallians made another attempt to conquer Rīga, but again failed to take it. On their return home Livonian knights attacked them, but were defeated at the Battle of Garoza, in which the Orders' master Villekin and at least 35 knights lost their lives. The new master of the Order Cuno of Haciginstein organised the last campaigns against the Semigallians in 1289 and 1290; the hillforts of Dobele, Rakte and Sidabre were conquered and most of the Semigallian warriors joined the Samogitian and Lithuanian forces.

Konrad I, the Polish Duke of Masovia, unsuccessfully attempted to conquer pagan Prussia in crusades in 1219 and 1222. Taking the advice of the first Bishop of Prussia, Christian of Oliva, Konrad founded the crusading Order of Dobrzyń (or "Dobrin") in 1220. However, this order was largely ineffective, and Konrad's campaigns against the Old Prussians were answered by incursions into the already captured territory of Culmerland (Chełmno Land). Subjected to constant Prussian counter-raids, Konrad wanted to stabilize the north of the Duchy of Masovia in this fight over border area of Chełmno Land. Masovia had only been conquered in the 10th century and native Prussians, Yotvingians, and Lithuanians were still living in the territory, where no settled borders existed. Konrad's military weakness led him in 1226 to ask the Roman Catholic monastic order of the Teutonic Knights to come to Prussia and suppress the Old Prussians.

The Northern Crusades provided a rationale for the growth and expansion of the Teutonic Order of German crusading knights which had been founded in Palestine at the end of the 12th century. Duke Konrad I of Masovia in west-central Poland appealed to the Knights to defend his borders and subdue the pagan Old Prussians in 1226. After the subjugation of the Prussians, the Teutonic Knights fought against the Grand Duchy of Lithuania.

When the Livonian knights were crushed by Samogitians in the Battle of Saule in 1236, coinciding with a series of revolts in Estonia, the Livonian Order was inherited by the Teutonic Order, allowing the Teutonic Knights to exercise political control over large territories in the Baltic region. Mindaugas, the King of Lithuania, was baptised together with his wife after his coronation in 1253, hoping that this would help stop the Crusaders' attacks, which it did not. The Teutonic Knights failed to subdue Lithuania, which officially converted to (Catholic) Christianity in 1386 on the marriage of Grand Duke Jogaila to the 11-year-old Queen Jadwiga of Poland. However, even after the country was officially converted, the conflict continued up until the 1410 Battle of Grunwald, also known as the First Battle of Tannenberg, when the Lithuanians and Poles, helped by the Tatars, Moldovans and the Czechs, defeated the Teutonic knights.

In 1221, Pope Honorius III was again worried about the situation in the Finnish-Novgorodian Wars after receiving alarming information from the Archbishop of Uppsala. He authorized the Bishop of Finland to establish a trade embargo against the "barbarians" that threatened the Christianity in Finland. The nationality of the "barbarians", presumably a citation from Archbishop's earlier letter, remains unknown, and was not necessarily known even by the Pope. However, as the trade embargo was widened eight years later, it was specifically said to be against the Russians. Based on Papal letters from 1229, the Bishop of Finland requested, the Pope enforce a trade embargo against Novgorodians on the Baltic Sea, at least in Visby, Riga and Lübeck. A few years later, the Pope also requested the Livonian Brothers of the Sword send troops to protect Finland. Whether any knights ever arrived remains unknown.

The Teutonic Order's attempts to conquer Orthodox Russia (particularly the Republics of Pskov and Novgorod), an enterprise endorsed by Pope Gregory IX, accompanied the Northern Crusades. One of the major blows for the idea of the conquest of Russia was the Battle of the Ice in 1242. With or without the Pope's blessing, Sweden also undertook several crusades against Orthodox Novgorod.




</doc>
<doc id="21922" url="https://en.wikipedia.org/wiki?curid=21922" title="Neoteny">
Neoteny

Neoteny (), also called juvenilization, is the delaying or slowing of the physiological (or somatic) development of an organism, typically an animal. Neoteny is found in modern humans. In progenesis (also called paedogenesis), sexual development is accelerated.
Both neoteny and progenesis result in paedomorphism (or paedomorphosis), a type of heterochrony. Some authors define paedomorphism as the retention of larval traits, as seen in salamanders.

Both neoteny and progenesis cause the retention in adults of traits previously seen only in the young. Such retention is important in evolutionary biology, domestication and evolutionary developmental biology.

The origins of the concept of neoteny have been traced to the Bible (as argued by Ashley Montagu) and to the poet William Wordsworth's "The Child is the father of the Man" (as argued by Barry Bogin). The term itself was invented in 1885 by Julius Kollmann as he described the axolotl's maturation while remaining in a tadpole-like aquatic stage complete with gills, unlike other adult amphibians like frogs and toads.

The word "neoteny" is borrowed from the German "Neotenie", the latter constructed by Kollmann from the Greek νέος ("neos", "young") and τείνειν ("teínein", "to stretch, to extend"). The adjective is either "neotenic" or "neotenous". For the opposite of "neotenic", different authorities use either "gerontomorphic" or "peramorphic". Bogin points out that Kollmann had intended the meaning to be "retaining youth", but had evidently confused the Greek "teínein" with the Latin "tenere", which had the meaning he wanted, "to retain", so that the new word would mean "the retaining of youth (into adulthood)".

In 1926 Louis Bolk described neoteny as the major process in humanization. In his 1977 book "Ontogeny and Phylogeny", Stephen Jay Gould noted that Bolk's account constituted an attempted justification for "scientific" racism and sexism, but acknowledged that Bolk had been right in the core idea that humans differ from other primates in becoming sexually mature in an infantile stage of body development.

Neoteny in humans is the slowing or delaying of body development, compared to non-human primates, resulting in features such as a large head, a flat face, and relatively short arms. These neotenic changes may have been brought about by sexual selection in human evolution. In turn, they may have permitted the development of human capacities such as emotional communication. However, humans also have relatively large noses and long legs, both peramorphic (not neotenic) traits. Some evolutionary theorists have proposed that neoteny was a key feature in human evolution. Gould argued that the "evolutionary story" of humans is one where we have been "retaining to adulthood the originally juvenile features of our ancestors". J. B. S. Haldane mirrors Gould's hypothesis by stating a "major evolutionary trend in human beings" is "greater prolongation of childhood and retardation of maturity." Delbert D. Thiessen said that "neoteny becomes more apparent as early primates evolved into later forms" and that primates have been "evolving toward flat face." However, in light of some groups using neoteny-based arguments to support racism, Gould also argued "that the whole enterprise of ranking groups by degree of neoteny is fundamentally unjustified" (Gould, 1996, pg. 150). Doug Jones argued that human evolution's trend toward neoteny may have been caused by sexual selection in human evolution for neotenous facial traits in women by men with the resulting neoteny in male faces being a "by-product" of sexual selection for neotenous female faces.

Neoteny is seen in domesticated animals such as dogs and mice. This is because there are more resources available, less competition for those resources, and with the lowered competition the animals expend less energy obtaining those resources. This allows them to mature and reproduce more quickly than their wild counterparts. The environment that domesticated animals are raised in determines whether or not neoteny is present in those animals. Evolutionary neoteny can arise in a species when those conditions occur, and a species becomes sexually mature ahead of its "normal development". Another explanation for the neoteny in domesticated animals can be the selection for certain behavioral characteristics. Behavior is linked to genetics which therefore means that when a behavioral trait is selected for, a physical trait may also be selected for due to mechanisms like linkage disequilibrium. Often, juvenile behaviors are selected for in order to more easily domesticate a species; aggressiveness in certain species comes with adulthood when there is a need to compete for resources. If there is no need for competition, then there is no need for aggression. Selecting for juvenile behavioral characteristics can lead to neoteny in physical characteristics because, for example, with the reduced need for behaviors like aggression there is no need for developed traits that would help in that area. Traits that may become neotenized due to decreased aggression may be a shorter muzzle and smaller general size among the domesticated individuals. Some common neotenous physical traits in domesticated animals (mainly dogs, pigs, ferrets, cats, and even foxes) include: floppy ears, changes in reproductive cycle, curly tails, piebald coloration, fewer or shortened vertebra, large eyes, rounded forehead, large ears, and shortened muzzle.

When the role of dogs expanded from just being working dogs to also being companions, humans started selective breeding dogs for morphological neoteny, and this selective breeding for "neoteny or paedomorphism" had the effect of enhancing the bond between humans and dogs. Humans bred dogs to have more "juvenile physical traits" as adults such as short snouts and wide-set eyes which are associated with puppies, because people usually consider these traits to be more attractive. Some breeds of dogs with short snouts and broad heads such as the Komondor, Saint Bernard and Maremma Sheepdog are more morphologically neotenous than other breeds of dogs. Cavalier King Charles spaniels are an example of selection for neoteny, because they exhibit large eyes, pendant-shaped ears and compact feet, giving them a morphology similar to puppies as adults.

In 2004, a study that used 310 wolf skulls and over 700 dog skulls representing 100 breeds concluded that the evolution of dog skulls can generally not be described by heterochronic processes such as neoteny although some pedomorphic dog breeds have skulls that resemble the skulls of juvenile wolves. By 2011, the finding by the same researcher was simply "Dogs are not paedomorphic wolves."

Neoteny has been observed in many other species. It is important to note the difference between partial and full neoteny when looking at other species in order to distinguish between juvenile traits that are only advantageous in the short term and traits that provide a benefit throughout the organism's life; this might then provide some insight into the cause of neoteny in those species. Partial neoteny is the retention of the larval form beyond the usual age of maturation with the possibility of the development of sexual organs progenesis, but eventually the organism still matures into the adult form; this can be seen in "Lithobates clamitans". Full neoteny is seen in "Ambystoma mexicanum" and some populations of "Ambystoma tigrinum", which remain in their larval form for the duration of their life. "Lithobates clamitans" is partially neotenous: it delays its maturation through the winter season, because it is not advantageous for it to metamorphose into the adult form until there are more resources available: it can find those resources much more easily in the larval form. This would fall under both of the main causes of neoteny; the energy required to survive in the winter as a newly formed adult is too costly, so the organism exhibits neotenous characteristics until a time when it is capable of better survival as an adult. "Ambystoma tigrinum" retains its neotenous features for a similar reason, however the retention is permanent due to the lack of resources available throughout its lifetime. This is another example of an environmental cause of neoteny in that the species retains juvenile characteristics because the environment limits the ability of the organism to fully come into its adult form. A few species of birds show partial neoteny. A couple of examples of such species are the manakin birds "Chiroxiphia linearis" and "Chiroxiphia caudata". The males of both species retain their juvenile plumage into adulthood, but they eventually lose it once they are fully mature. In certain species of birds the retention of juvenile plumage is often linked to the molting times within each species. In order to ensure there is no overlap between the molting and mating times, the birds may show partial neoteny in regards to their plumage so that the males do not attain their bright adult plumage before the females are prepared to mate. In this instance, neoteny is present because there is no need for the males to molt early and it would be a waste of energy for them to try to mate while the females are still immature.

Neoteny is commonly seen in flightless insects like the females in the order Strepsiptera. The flightless trait in insects has evolved many separate times; environments that may have contributed to the separate evolution of this trait are: high altitudes, isolation on islands, and insects that reside in colder climates. These environmental factors may be responsible for the flightless trait, because in these situations it would be disadvantageous to have a population that is more dispersed, so flightlessness would be favored due to the boundaries it poses to dispersal. Also, in cooler temperatures heat is lost more rapidly through wings, thus the circumstance favors flightlessness. Another couple of main points to note about insects are that the females in certain groups become sexually mature without metamorphosing into adulthood, and some insects which grow up in certain conditions do not ever develop wings. Flightlessness in some female insects has been linked to higher fecundity, this would increase the fitness of the individual because the female is producing more offspring and therefore passing on more of her genes. In those instances, neoteny occurs because it is more advantageous for the females to remain flightless in order to conserve energy which thereby increases their fecundity. Aphids are a great example of insects that may never develop wings due to their environmental setting. If resources are abundant there is no need to grow wings and disperse. When the nutrition of a host plant is abundant, aphids may not grow wings, remaining on the host plant for the duration of their lives; however, if the resources become diminished, their offspring may develop wings in order to disperse to other host plants.

Two common environments that tend to favor neoteny are high-altitude and cool environments because neotenous individuals have a higher fitness than those that metamorphose into the adult form. This is because the energy required for metamorphosis is too costly for the individual's fitness, also the conditions favor neoteny due to the ability of neotenous individuals to utilize the available resources more easily. This trend can be seen in the comparison of salamander species of lower and higher altitudes. The neotenous individuals have higher survivorship as well as higher fecundity than the salamanders that had gone to the adult form in the higher altitude and cooler environment. Insects in cooler environments tend to show neoteny in flight because wings have a high surface area and lose heat quickly, thus it is not advantageous for insects in that environment to metamorphose into adults.

Many species of salamander, and amphibians in general, are known to have neotenized characteristics because of the environment they live in. Axolotl and olm are species of salamander that retain their juvenile aquatic form throughout adulthood, examples of full neoteny. Gills are a common juvenile characteristic in amphibians that are kept after maturation; an example of this would be a comparison of the tiger salamander and the rough-skinned newt, both of which retain gills into adulthood.

Pygmy chimpanzees (bonobos) share many physical characteristics with humans. A prime example is their neotenous skulls. The shape of their skull does not change into adulthood; it only increases in size. This is due to sexual dimorphism and an evolutionary change in timing of development. Juveniles became sexually mature before their bodies had fully developed into adulthood, and due to some selective advantage the neotenic structure of the skull remained in later generations.

In some species, energy costs result in neoteny, as in the insect families Gerridae, Delphacidae, and Carabidae. Many of the species in these families have smaller, neotenous wings or no wings at all. Similarly, some cricket species shed their wings in adulthood, while in beetles of the genus "Ozopemon", the males (thought to be the first example of neoteny in the Coleoptera) are significantly smaller than the females, through inbreeding. In the termite "Kalotermes flavicollis", neoteny is seen in females during molting.

In other species, environmental conditions cause neoteny, as in the northwestern salamander ("Ambystoma gracile"), where higher altitude is correlated with greater neotenic tendencies, perhaps to help conserve energy as mentioned above. Similarly, neoteny is found in a few species of the crustacean family Ischnomesidae, which live in deep ocean waters.

Neoteny is usually used to describe animal development; however, neoteny is also seen in the cell organelles. It was suggested that subcellular neoteny could explain why sperm cells have atypical centrioles. One of the two sperm centrioles of fruit fly exhibits the retention of “juvenile” centriole structure, which can be described as centriolar “neoteny”. This neotenic, atypical centriole, is known as the Proximal Centriole-Like. Typical centrioles form via a step by step process in which a cartwheel form, then develops to become a procentriole, and further matures into a centriole. The neotenic centriole of fruit fly resembles an early procentriole.




</doc>
<doc id="21923" url="https://en.wikipedia.org/wiki?curid=21923" title="National Rail">
National Rail

National Rail (NR) in the United Kingdom is the trading name licensed for use by the Rail Delivery Group, an unincorporated association whose membership consists of the passenger train operating companies (TOCs) of England, Scotland, and Wales. The TOCs run the passenger services previously provided by the British Railways Board, from 1965 using the brand name British Rail. Northern Ireland, which is bordered by the Republic of Ireland, has a different system. National Rail services share a ticketing structure and inter-availability that generally do not extend to services which were not part of British Rail.

"National" Rail should not be confused with "Network" Rail. National Rail is a brand used to promote passenger railway services, and providing some harmonisation for passengers in ticketing, while Network Rail is the organisation which owns and manages most of the fixed assets of the railway network, including tracks, stations and signals.

The two generally coincide where passenger services are run. Most major Network Rail lines also carry freight traffic and some lines are freight only. There are some scheduled passenger services on privately managed, non-Network Rail lines, for example Heathrow Express, which partly runs on Network Rail track. The London Underground also overlaps with Network Rail in places.

Twenty eight privately owned train operating companies, each franchised for a defined term by government, operate passenger trains on the main rail network in Great Britain. The Rail Delivery Group is the trade association representing the TOCs and provides core services, including the provision of the National Rail Enquiries service. It also runs Rail Settlement Plan, which allocates ticket revenue to the various TOCs, and Rail Staff Travel, which manages travel facilities for railway staff. It does not compile the national timetable, which is the joint responsibility of the Office of Rail Regulation (allocation of paths) and Network Rail (timetable production and publication).

Since the privatisation of British Rail there is no longer a single approach to design on railways in Great Britain. The look and feel of signage, liveries and marketing material is largely the preserve of the individual TOCs. 

However, National Rail continues to use BR's famous double-arrow symbol, designed by Gerald Burney of the Design Research Unit. It has been incorporated in the National Rail logotype and is displayed on tickets, the National Rail website and other publicity. The trademark rights to the double arrow symbol remain state-owned, being vested in the Secretary of State for Transport.

The double arrow symbol is also used to indicate a railway station on British traffic signs.

The National Rail (NR) logo was introduced by ATOC in 1999, and was used on the Great Britain public timetable for the first time in the edition valid from 26 September in that year. Rules for its use are set out in the Corporate Identity Style Guidelines published by the Rail Delivery Group, available on its website. "In 1964 the Design Research Unit—Britain’s first multi-disciplinary design agency founded in 1943 by Misha Black, Milner Gray and Herbert Read—was commissioned to breathe new life into the nation’s neglected railway industry". The NR title is sometimes described as a "brand". As it was used by British Rail, the single operator before franchising, its use also maintains continuity and public familiarity; and it avoids the need to replace signage.

The lettering used in the National Rail logotype is a modified form of the typeface Sassoon Bold. Some train operating companies continue to use the former British Rail Rail Alphabet lettering to varying degrees in station signage, although its use is no longer universal; however it remains compulsory (under Railway Group Standards) for safety signage in trackside areas and is still common (although not universal) on rolling stock.

The British Rail typefaces of choice from 1965 were Helvetica and Univers, with others (particularly Frutiger) coming into use during the sectorisation period after 1983. TOCs may use what they like: examples include Futura (Stagecoach Group), Helvetica (FirstGroup and National Express), Frutiger (Arriva Trains Wales), Bliss (CrossCountry), and a modified version of Precious by London Midland.

Although TOCs compete against each other for franchises, and for passengers on routes where more than one TOC operates, the strapline used with the National Rail logo is 'Britain's train companies working together'.

Several conurbations have their own metro or tram systems, most of which are not part of National Rail. These include the London Underground, Docklands Light Railway, London Tramlink, Blackpool Tramway, Glasgow Subway, Tyne & Wear Metro, Manchester Metrolink, Sheffield Supertram, West Midlands Metro and Nottingham Express Transit. On the other hand, the largely self-contained Merseyrail system is part of the National Rail network, and urban rail networks around Birmingham, Cardiff, Glasgow and West Yorkshire consist entirely of National Rail services.

London Overground is a hybrid: its services are operated via a concession awarded by Transport for London, and are branded accordingly, but until 2010 all its routes used infrastructure owned by Network Rail, that is because of the East London line which was a London Underground line converted to a mainline but it is still National Rail. London Overground now also owns some infrastructure in its own right, following the reopening of the former London Underground East London line. Since all the previous LO routes were operated by National Rail franchise Silverlink until November 2007, they have continued to be shown in the National Rail timetable and are still considered to be a part of National Rail.

Heathrow Express and Eurostar are also not part of the National Rail network despite sharing of stations (Heathrow Express also share its route with GWR and TfL Rail). Northern Ireland Railways were never part of British Rail, which was limited to England, Scotland and Wales, and therefore are not part of the National Rail network.

There are many privately owned or heritage railways in Great Britain which are not part of the National Rail network and mostly operate for heritage or pleasure purposes rather than as public transport, but some have connections to National rail track.

National Rail services have a common ticketing structure inherited from British Rail. Through tickets are available between any pair of stations on the network, and can be bought from any station ticket office. Most tickets are inter-available between the services of all operators on routes appropriate to the journey being made. Operators on some routes offer operator-specific tickets that are cheaper than the inter-available ones.

Through tickets involving Heathrow Express and London Underground are also available. Oyster pay-as-you-go can be used on National Rail in Greater London from 2 January 2010. These same areas can also be journeyed to using a contactless debit/credit card. Contactless also covers some areas that oyster doesn't such as the Crossrail line to Reading, or the Thameslink station at Oakleigh Park.

Passengers without a valid ticket boarding a train at a station where ticket-buying facilities are available are required to pay the full Open Single or Return fare. On some services penalty fares apply - a ticketless passenger may be charged the greater of £20 or twice the full single fare to the next stop. Penalty Fares can be collected only by authorised Revenue Protection Inspectors, not by ordinary Guards.

National Rail distributes a number of technical manuals on which travel on the railways in Great Britain is based, such as the National Rail Conditions of Travel, via their website.

Pocket timetables for individual operators or routes are available free at staffed stations. The last official printed timetable with up to 3000 pages was published in 2007. Now the only complete print edition is published by Middleton Press (as of October 2016). A digital version of the full timetable is available as a pdf file without charge on the Network Rail website, however passengers are recommended to obtain their timetables from the individual train companies.

The National Rail Enquiries website includes a journey planner, fare and live departure information. The site is designed to complement the myriad different websites of Britain's privatised rail companies, so when users have selected which tickets they wish to buy, they are redirected to the most relevant train company website, where they can buy their tickets without booking fees.

In 2012 the website was joined by a mobile app mirroring its functionality. The app is available for iPhone, Android and Windows Phone. However Trainline remains the most downloaded rail app in the UK with 9.4 million users.




</doc>
<doc id="21926" url="https://en.wikipedia.org/wiki?curid=21926" title="Naked singularity">
Naked singularity

In general relativity, a naked singularity is a hypothetical gravitational singularity without an event horizon. In a black hole, the singularity is completely enclosed by a boundary known as the event horizon, inside which the gravitational force of the singularity is so strong that light cannot escape. Hence, objects inside the event horizon—including the singularity itself—cannot be directly observed. A naked singularity, by contrast, would be observable from the outside.

The theoretical existence of naked singularities is important because their existence would mean that it would be possible to observe the collapse of an object to "infinite density". It would also cause foundational problems for general relativity, because general relativity cannot make predictions about the future evolution of space-time near a singularity. In generic black holes, this is not a problem, as an outside viewer cannot observe the space-time within the event horizon.

Naked singularities have not been observed in nature. Astronomical observations of black holes indicate that their rate of rotation falls below the threshold to produce a naked singularity (spin parameter 1). GRS 1915+105 comes closest to the limit, with a spin parameter of 0.82-1.00.

According to the cosmic censorship hypothesis, gravitational singularities may not be observable. If loop quantum gravity is correct, naked singularities may be possible in nature.

From concepts drawn from rotating black holes, it is shown that a singularity, spinning rapidly, can become a ring-shaped object. This results in two event horizons, as well as an ergosphere, which draw closer together as the spin of the singularity increases. When the outer and inner event horizons merge, they shrink toward the rotating singularity and eventually expose it to the rest of the universe.

A singularity rotating fast enough might be created by the collapse of dust or by a supernova of a fast-spinning star. Studies of pulsars and some computer simulations (Choptuik, 1997) have been performed.

Mathematician Demetrios Christodoulou, a winner of the Shaw Prize, has shown that contrary to what had been expected, singularities which are not hidden in a black hole also occur. However, he then showed that such "naked singularities" are unstable.

Disappearing event horizons exist in the Kerr metric, which is a spinning black hole in a vacuum. Specifically, if the angular momentum is high enough, the event horizons could disappear. Transforming the Kerr metric to Boyer–Lindquist coordinates, it can be shown that the formula_1 coordinate (which is not the radius) of the event horizon is

formula_2,

where formula_3, and formula_4. In this case, "event horizons disappear" means when the solutions are complex for formula_5, or formula_6. However, this corresponds to a case where formula_7 exceeds formula_8 (or in Planck units, , i.e. the spin exceeds what is normally viewed as the upper limit of its physically possible values.

Disappearing event horizons can also be seen with the Reissner–Nordström geometry of a charged black hole. In this metric, it can be shown that the horizons occur at

formula_9,

where formula_3, and formula_11. Of the three possible cases for the relative values of formula_12 and formula_13, the case where formula_14 causes both formula_5 to be complex. This means the metric is regular for all positive values of formula_1, or in other words, the singularity has no event horizon. However, this corresponds to a case where formula_17 exceeds formula_18 (or in Planck units, , i.e. the charge exceeds what is normally viewed as the upper limit of its physically possible values.

See Kerr–Newman metric for a spinning, charged ring singularity.

A naked singularity could allow scientists to observe an infinitely dense material, which would under normal circumstances be impossible by the cosmic censorship hypothesis. Without an event horizon of any kind, some speculate that naked singularities could actually emit light.

The cosmic censorship hypothesis says that a gravitational singularity would remain hidden by the event horizon. LIGO events, including GW150914, are consistent with these predictions. Although data anomalies would have resulted in the case of a singularity, the nature of those anomalies remains unknown.

Some research has suggested that if loop quantum gravity is correct, then naked singularities could exist in nature, implying that the cosmic censorship hypothesis does not hold. Numerical calculations and some other arguments have also hinted at this possibility.





</doc>
<doc id="21927" url="https://en.wikipedia.org/wiki?curid=21927" title="National Party of Australia">
National Party of Australia

The National Party of Australia, also known as The Nationals or The Nats, is an Australian political party. Traditionally representing graziers, farmers, and rural voters generally, it began as the Australian Country Party in 1920 at a federal level. It later adopted the name National Country Party in 1975, before taking its current name in 1982.

Federally, and in New South Wales, and to an extent in Victoria and historically in Western Australia, it has in government been the minor party in a centre-right Coalition with the Liberal Party of Australia, and its leader has usually served as Deputy Prime Minister. In Opposition the Coalition was usually maintained, but even otherwise the party still generally continued to work in co-operation with the Liberal Party of Australia (as had their predecessors the Nationalist Party of Australia and United Australia Party). In Queensland, however, the Country Party (later National Party) was the senior coalition party between 1925 and 2008, after which it merged in that state with the junior Liberal Party of Australia to form the Liberal National Party (LNP). Despite taking a conservative position socially, the National Party has long pursued agrarian socialist economic policies. Ensuring support for farmers, either through government grants and subsidies or through community appeals, is a major focus of National Party policy. According to Ian McAllister, the National Party is the only remaining agrarian socialist party from the "wave of agrarian socialist parties set up around the Western world in the 1920s".

The current leader of the National Party is Michael McCormack, who won a leadership spill following Barnaby Joyce's resignation in February 2018. The deputy leader of the Nationals, since 4 February 2020, is David Littleproud.

 
The Country Party was formally founded in 1913 in Western Australia, and nationally in 1920, from a number of state-based parties such as the Victorian Farmers' Union (VFU) and the Farmers and Settlers Party of New South Wales. Australia's first Country Party was founded in 1912 by Harry J. Stephens, editor of "The Farmer & Settler", but, under fierce opposition from rival newspapers, failed to gain momentum.

The VFU won a seat in the House of Representatives at the Corangamite by-election held in December 1918, with the help of the newly introduced preferential voting system. At the 1919 federal election the state-based Country Parties won federal seats in New South Wales, Victoria and Western Australia. They also began to win seats in state parliaments. In 1920 the Country Party was established as a national party led by William McWilliams from Tasmania. In his first speech as leader, McWilliams laid out the principles of the new party, stating "we crave no alliance, we spurn no support but we intend drastic action to secure closer attention to the needs of primary producers" McWilliams was deposed as party leader in favour of Earle Page in April 1921, following instances where McWilliams voted against the party line. McWilliams later left the Country Party to sit as an Independent.

According to historian B. D. Graham (1959), the graziers who operated the sheep stations were politically conservative. They disliked the Labor Party, which represented their workers, and feared that Labor governments would pass unfavorable legislation and listen to foreigners and communists. The graziers were satisfied with the marketing organisation of their industry, opposed any change in land tenure and labour relations, and advocated lower tariffs, low freight rates, and low taxes. On the other hand, Graham reports, the small farmers, not the graziers, founded the Country party. The farmers advocated government intervention in the market through price support schemes and marketing pools. The graziers often politically and financially supported the Country party, which in turn made the Country party more conservative.

The Country Party's first election as a united party, in 1922, saw it in an unexpected position of power. It won enough seats to deny the Nationalists an overall majority. It soon became apparent that the price for Country support would be a full-fledged coalition with the Nationalists. However, Page let it be known that his party would not serve under Hughes, and forced his resignation. Page then entered negotiations with the Nationalists' new leader, Stanley Bruce, for a coalition government. Page wanted five seats for his Country Party in a Cabinet of 11, including the Treasurer portfolio and the second rank in the ministry for himself. These terms were unusually stiff for a prospective junior coalition partner in a Westminster system, and especially so for such a new party. Nonetheless, with no other politically realistic coalition partner available, Bruce readily agreed, and the "Bruce-Page Ministry" was formed. This began the tradition of the Country Party leader ranking second in Coalition cabinets.

Page remained dominant in the party until 1939, and briefly served as caretaker Prime Minister between the death of Joseph Lyons and the election of Robert Menzies as his successor. However, Page gave up the leadership rather than serve under Menzies. The coalition was re-formed under Archie Cameron in 1940, and continued until October 1941 despite the election of Arthur Fadden as leader after the 1940 election. Fadden was well regarded within conservative circles and proved to be a loyal deputy to Menzies in the difficult circumstances of 1941. When Menzies was forced to resign as Prime Minister, the UAP was so bereft of leadership that Fadden briefly succeeded him (despite the Country Party being the junior partner in the governing coalition). However, the two independents who had been propping up the government rejected Fadden's budget and brought the government down. Fadden stood down in favour of Labor leader John Curtin.

The Fadden-led Coalition made almost no headway against Curtin, and was severely defeated in the 1943 election. After that loss, Fadden became deputy Leader of the Opposition under Menzies, a role that continued after Menzies folded the UAP into the Liberal Party of Australia in 1944. Fadden remained a loyal partner of Menzies, though he was still keen to assert the independence of his party. Indeed, in the lead up to the 1949 federal election, Fadden played a key role in the defeat of the Chifley Labor government, frequently making inflammatory claims about the "socialist" nature of the Labor Party, which Menzies could then "clarify" or repudiate as he saw fit, thus appearing more "moderate". In 1949, Fadden became Treasurer in the second Menzies government and remained so until his retirement in 1958. His successful partnership with Menzies was one of the elements that sustained the coalition, which remained in office until 1972 (Menzies himself retired in 1966).

Fadden's successor, Trade Minister John McEwen, took the then unusual step of declining to serve as Treasurer, believing he could better ensure that the interests of Australian primary producers were safeguarded. Accordingly, McEwen personally supervised the signing of the first post-war trade treaty with Japan, new trade agreements with New Zealand and Britain, and Australia's first trade agreement with the USSR (1965). In addition to this, he insisted on developing an all-encompassing system of tariff protection that would encourage the development of those secondary industries that would "value add" Australia's primary produce. His success in this endeavour is sometimes dubbed "McEwenism". This was the period of the Country Party's greatest power, as was demonstrated in 1962 when McEwen was able to insist that Menzies sack a Liberal Minister who claimed that Britain's entry into the European Economic Community was unlikely to severely impact on the Australian economy as a whole.

Menzies retired in 1966 and was succeeded by Harold Holt. McEwen thus became the longest-tenured member of the government, with the informal right to veto government policy. The most significant instance in which McEwen exercised this right came when Holt disappeared in December 1967. John Gorton became the new Liberal Prime Minister in January 1968. McEwen was sworn in as interim Prime Minister pending the election of the new Liberal leader. Logically, the Liberals' deputy leader, William McMahon, should have succeeded Holt. However, McMahon was a staunch free-trader, and there were also rumors that he was homosexual. As a result, McEwen told the Liberals that he and his party would not serve under McMahon. McMahon stood down in favour of John Gorton. It was only after McEwen announced his retirement that MacMahon was able to successfully challenge Gorton for the Liberal leadership. McEwen's reputation for political toughness led to him being nicknamed "Black Jack" by his allies and enemies alike.

At the state level, from 1957 to 1989, the Country Party under Frank Nicklin and Joh Bjelke-Petersen dominated governments in Queensland—for the last six of those years ruling in its own right, without the Liberals. It also took part in governments in New South Wales, Victoria, and Western Australia.

However, successive electoral redistributions after 1964 indicated that the Country Party was losing ground electorally to the Liberals as the rural population declined, and the nature of some parliamentary seats on the urban/rural fringe changed. A proposed merger with the Democratic Labor Party (DLP) under the banner of "National Alliance" was rejected when it failed to find favour with voters at the 1974 state election.

Also in 1974, the Northern Territory members of the party joined with its Liberal party members to form the independent Country Liberal Party. This party continues to represent both parent parties in that territory. A separate party, the Joh-inspired NT Nationals, competed in the 1987 election with former Chief Minister Ian Tuxworth winning his seat of Barkly by a small margin. However, this splinter group were not endorsed by the national executive and soon disappeared from the political scene.

The National Party was confronted by the impact of demographic shifts from the 1970s: between 1971 and 1996, the population of Sydney and surrounds grew by 34%, with even larger growth in coastal New South Wales, while more remote rural areas grew by a mere 13%, further diminishing the National Party's base. On 2 May 1975 at the federal convention in Canberra, the Country Party changed its name to the National Country Party of Australia as part of a strategy to expand into urban areas. This had some success in Queensland under Joh Bjelke-Petersen, but nowhere else. The party briefly walked out of the coalition agreement in Western Australia in May 1975, returning within the month. However, the party split in two over the decision and other factors in late 1978, with a new National Party forming and becoming independent, holding three seats in the Western Australian lower house, while the National Country Party remained in coalition and also held three seats. They reconciled after the Burke Labor government came to power in 1983.

The 1980s were dominated by the feud between Bjelke-Petersen and the federal party leadership. Bjelke-Petersen briefly triumphed in 1987, forcing the Nationals to tear up the Coalition agreement and support his bid to become Prime Minister. The "Joh for Canberra" campaign backfired spectacularly when a large number of three-cornered contests allowed Labor to win a third term under Bob Hawke; however, in 1987 the National Party won a bump in votes and recorded its highest vote in more than four decades, but it also recorded a new low in the proportion of seats won. The collapse of Joh for Canberra also proved to be the Queensland Nationals' last hurrah; Bjelke-Petersen was forced into retirement a few months after the federal election, and his party was heavily defeated in 1989. The federal National Party were badly defeated at the 1990 election, with leader Charles Blunt one of five MPs to lose his seat.

Blunt's successor as leader, Tim Fischer, recovered two seats at the 1993 election, but lost an additional 1.2% of the vote from its 1990 result. In 1996, as the Coalition won a significant victory over the Keating Labor government, the National Party recovered another two seats, and Fischer became Deputy Prime Minister under John Howard.

The Nationals experienced difficulties in the late 1990s from two fronts – firstly from the Liberal Party, who were winning seats on the basis that the Nationals were not seen to be a sufficiently separate party, and from the One Nation Party riding a swell of rural discontent with many of the policies such as multiculturalism and gun control embraced by all of the major parties. The rise of Labor in formerly safe National-held areas in rural Queensland, particularly on the coast, has been the biggest threat to the Queensland Nationals.

At the 1998 Federal election, the National Party recorded only 5.3% of the vote in the House of Representatives, its lowest ever, and won only 16 seats, at 10.8% its second lowest proportion of seats.

The National Party under Fischer and his successor, John Anderson, rarely engaged in public disagreements with the Liberal Party, which weakened the party's ability to present a separate image to rural and regional Australia. In 2001 the National Party recorded its second-worst result at 5.6% winning 13 seats, and its third lowest at 5.9% at the 2004 election, winning only 12 seats.

Australian psephologist Antony Green argues that two important trends have driven the National Party's decline at a federal level: "the importance of the rural sector to the health of the nation's economy" and "the growing chasm between the values and attitudes of rural and urban Australia". Green has suggested that the result has been that "Both have resulted in rural and regional voters demanding more of the National Party, at exactly the time when its political influence has declined. While the National Party has never been the sole representative of rural Australia, it is the only party that has attempted to paint itself as representing rural voters above all else",

In June 2005 party leader John Anderson announced that he would resign from the ministry and as Leader of the Nationals due to a benign prostate condition, he was succeeded by Mark Vaile. At the following election the Nationals vote declined further, with the party winning a mere 5.4% of the vote and securing only 10 seats.

In 2010, under the leadership of Warren Truss the party received its lowest vote to date, at only 3.4%, however they secured a slight increase in seats from 10 to 12. At the following election in 2010 the national Party's fortunes improved slightly with a vote of 4.2% and an increase in seats from 12 to 15.

At the 2016 Double dissolution election, under the Leadership of Barnaby Joyce the party secured 4.6% of the vote and 16 seats. In 2018, reports emerged that the National Party leader and Deputy Prime Minister, Barnaby Joyce was expecting a child with his former communications staffer Vikki Campion. Joyce resigned after revelations that he had been engaged in an extramarital affair. Later in the same year it was revealed that the NSW National party and its youth wing, the Young Nationals had been infiltrated by neo-Nazis with more than 30 members being investigated for alleged links to neo-Nazism. Leader McCormack denounced the infiltration, and several suspected neo-Nazis were expelled from the party and its youth wing.

At the 2019 Australian federal election, despite severe drought, perceived inaction over the plight of the Murray-Darling Basin, a poor performance in the New South Wales state election and sex scandals surrounding the member for Mallee, Andrew Broad and party leader Barnaby Joyce, the National Party saw only a small decline in vote, down 0.10% to attain 4.51% of the primary vote.

The official state and territorial party organisations (or equivalents) of the National Party are:
The National Party does not have a branch in the Australian Capital Territory.

Queensland is the only state in which the Nationals have consistently been the stronger coalition partner. The Nationals were the senior partner in the non-Labor Coalition from the 1950s until the Coalition was broken up in 1983 and at the following state election the Nationals (under Joh Bjelke-Petersen) came up one seat short of a majority at the election, but gained one a few weeks later after Don Lane and Brian Austin crossed the floor to join the Nationals. The Nationals subsequently governed in their own right until 1989.

The Queensland branch of the Country Party was the first to change its name to the National Party in April 1974.

The continued success of the Australian Labor Party at a state level has put pressure on the Nationals' links with the Liberal Party, their traditional coalition partner. In most states, the Coalition agreement is not in force when the parties are in opposition, allowing the two parties greater freedom of action.

In Queensland the National Party merged with the Liberal Party to form the Liberal National Party (LNP) in 2008. The LNP led by Lawrence Springborg went on to lose the March 2009 election to Anna Bligh's Australian Labor Party. However, in the 2012 state election, the LNP defeated the Labor Party in a landslide, but lost government in the 2015 landslide to Labor, also losing in 2018.

In South Australia, for the first time in the Nationals' history, in 2002 the single Nationals member in the House of Assembly entered the Rann Labor Government as a Minister forming an informal coalition between the two parties. Since the 2010 South Australian State election, the Nationals in South Australia have no representative in either the House of Assembly or the Upper House or at a Federal level. There existed a distinctly different Country Party in South Australia which merged with the Liberal Federation to become the Liberal and Country League in 1932.

The Nationals have had a very intermittent history of organising in Tasmania compared to the main states with limited electoral success. The most recent attempt began in 2013 but disagreements between the state and federal parties led to the latter cutting ties and the state party renamed itself the Tasmania Party before disappearing off the scene. In May 2018 federal senator Steve Martin joined the party and declared he would seek to relaunch it in the state. The Nationals contested Tasmanian seats (both houses) in the 2019 Australian federal election.

The Nationals were stung in early 2006, when their only Victorian senator, Julian McGauran, defected to the Liberals and created a serious rift between the Nationals and the Liberals. Several commentators believed that changing demographics and unfavourable preference deals would demolish the Nationals at the state election that year, but they went on to enjoy considerable success by winning two extra lower house seats. The Nationals were in a coalition government with the Liberals at a State level in Victoria until their defeat at the 2014 election. Following the election, the ABC reported that the coalition parties would "review" whether to continue their joint working arrangement into opposition. However, both outgoing Nationals leader Peter Ryan and incoming Liberal leader Matthew Guy indicated they felt the coalition should continue.

Western Australia's National Party chose to assert its independence after an acrimonious co-habitation with the Liberals on the 2005 campaign trail. Unlike its New South Wales and Queensland counterparts, the WA party had decided to oppose Liberal candidates in the 2008 election. The party aimed to hold the balance of power in the state "as an independent conservative party" ready to negotiate with the Liberals or Labor to form a minority government. After the election, the Nationals negotiated an agreement to form a government with the Liberals and an independent MP, though not described as a "traditional coalition" due to the reduced cabinet collective responsibility of National cabinet members.

Western Australia's one-vote-one-value reforms will cut the number of rural seats in the state assembly to reflect the rural population level: this, coupled with the Liberals' strength in country areas has put the Nationals under significant pressure.

The Nationals have rarely organised or stood in the Australian Capital Territory. Federally their only candidatures were in the 1974 election when they contested both constituencies and secured 3.5% of the territory vote. In the first election to the Legislative Assembly in 1989 David Adams, a former Liberal member of the old House of Assembly, headed a list of three candidates for the Nationals. They received 1947 votes (1.37%) and did not get any members elected. The party has not stood in the territory since.

The Nationals see their main role as giving a voice to Australians who live outside the country's metropolitan areas.

Traditionally, the leader of the National Party serves as Deputy Prime Minister when the Coalition is in government. This tradition dates back to the creation of the office in 1968.

The National Party's support base and membership are closely associated with the agricultural community. Historically anti-union, the party has vacillated between state support for primary industries ("agrarian socialism") and free agricultural trade and has opposed tariff protection for Australia's manufacturing and service industries. This vacillation prompted those opposed to the policies of the Nationals to joke that its real aim was to "capitalise its gains and socialise its losses!". It is usually pro-mining, pro-development, and anti-environmentalist.

"Countrymindedness" was a slogan that summed up the ideology of the Country Party from 1920 through the early 1970s. It was an ideology that was physiocratic, populist, and decentralist; it fostered rural solidarity and justified demands for government subsidies. "Countrymindedness" grew out of the failure of the country areas to participate in the rapid economic and population expansions that occurred after 1890. The growth of the ideology into urban areas came as most country people migrated to jobs in the cities. Its decline was due mainly to the reduction of real and psychological differences between country and city brought about by the postwar expansion of the Australian urban population and to the increased affluence and technological changes that accompanied it.

The Nationals vote is in decline and its traditional supporters are turning instead to prominent independents such as Bob Katter, Tony Windsor and Peter Andren in Federal Parliament and similar independents in the Parliaments of New South Wales, Queensland and Victoria, many of whom are former members of the National Party. In fact since the 2004 Federal election, National Party candidates received fewer first preference votes than the Australian Greens.

Demographic changes are not helping, with fewer people living and employed on the land or in small towns, the continued growth of the larger provincial centres, and, in some cases, the arrival of left-leaning "city refugees" in rural areas. The Liberals have also gained support as the differences between the coalition partners on a federal level have become invisible. This was highlighted in January 2006, when Nationals Senator Julian McGauran defected to the Liberals, saying that there was "no longer any real distinguishing policy or philosophical difference".
In Queensland, Nationals leader Lawrence Springborg advocated merger of the National and Liberal parties at a state level in order to present a more effective opposition to the Labor Party. Previously this plan had been dismissed by the Queensland branch of the Liberal party, but the idea received in-principle support from the Liberals. Federal leader Mark Vaile stated the Nationals will not merge with the Liberal Party at a federal level. The plan was opposed by key Queensland Senators Ron Boswell and Barnaby Joyce, and was scuttled in 2006. After suffering defeat in the 2006 Queensland poll, Lawrence Springborg was replaced by Jeff Seeney, who indicated he was not interested in merging with the Liberal Party until the issue is seriously raised at a Federal level.

In September 2008, Joyce replaced CLP Senator and Nationals deputy leader Nigel Scullion as leader of the Nationals in the Senate, and stated that his party in the upper house would no longer necessarily vote with their Liberal counterparts in the upper house, which opened up another possible avenue for the Rudd Labor Government to get legislation through. Joyce was elected leader in a party-room ballot on 11 February 2016, following the retirement of former leader and Deputy Prime Minister Warren Truss. Joyce was one of five politicians disqualified from parliament in October 2017 for holding dual citizenship, along with former deputy leader, Fiona Nash.

Merger plans came to a head in May 2008, when the Queensland state Liberal Party gave an announcement not to wait for a federal blueprint but instead to merge immediately. The new party, the Liberal National Party, was founded in July 2008.

The Country Party's first senators began their terms in 1926, but the party had no official leader in the upper chamber until 1935. Instead, the party nominated a "representative" or "liaison officer" where necessary – usually William Carroll. This was so that its members "were first and foremost representatives of their states, able to enjoy complete freedom of action and speech in the Senate and not beholden to the dictates of [...] a party Senate leader". On 3 October 1935, Charles Hardy was elected as Carroll's replacement and began using the title "Leader of the Country Party in the Senate". This usage was disputed by Carroll and Bertie Johnston, but a subsequent party meeting on 10 October confirmed Hardy's position. However, after Hardy's term ended in 1938 (due to his defeat at the 1937 election), the party did not elect another Senate leader until 1949 – apparently due to its small number of senators.

, the party does not have federal or state representation in Tasmania or South Australia, though the Tasmanian and South Australian branches last contested in the 2019 Australian federal election. The National Party does not have a branch in the Australian Capital Territory.

For the 2015-2016 financial year, the top ten disclosed donors to the National Party were: Manildra Group ($182,000), Ognis Pty Ltd ($100,000), Trepang Services ($70,000), Northwake Pty Ltd ($65,000), Hancock Prospecting ($58,000), Bindaree Beef ($50,000), Mowburn Nominees ($50,000), Retail Guild of Australia ($48,000), CropLife International ($43,000) and Macquarie Group ($38,000).

The National Party also receives undisclosed funding through several methods, such as "associated entities". John McEwen House, Pilliwinks and Doogary are entities which have been used to funnel donations to the National Party without disclosing the source.




</doc>
<doc id="21930" url="https://en.wikipedia.org/wiki?curid=21930" title="Northern blot">
Northern blot

The northern blot, or RNA blot, is a technique used in molecular biology research to study gene expression by detection of RNA (or isolated mRNA) in a sample.

With northern blotting it is possible to observe cellular control over structure and function by determining the particular gene expression rates during differentiation and morphogenesis, as well as in abnormal or diseased conditions. Northern blotting involves the use of electrophoresis to separate RNA samples by size, and detection with a hybridization probe complementary to part of or the entire target sequence. The term 'northern blot' actually refers specifically to the capillary transfer of RNA from the electrophoresis gel to the blotting membrane. However, the entire process is commonly referred to as northern blotting. The northern blot technique was developed in 1977 by James Alwine, David Kemp, and George Stark at Stanford University, with contributions from Gerhard Heinrich. Northern blotting takes its name from its similarity to the first blotting technique, the Southern blot, named for biologist Edwin Southern. The major difference is that RNA, rather than DNA, is analyzed in the northern blot.

A general blotting procedure starts with extraction of total RNA from a homogenized tissue sample or from cells. Eukaryotic mRNA can then be isolated through the use of oligo (dT) cellulose chromatography to isolate only those RNAs with a poly(A) tail. RNA samples are then separated by gel electrophoresis. Since the gels are fragile and the probes are unable to enter the matrix, the RNA samples, now separated by size, are transferred to a nylon membrane through a capillary or vacuum blotting system. A nylon membrane with a positive charge is the most effective for use in northern blotting since the negatively charged nucleic acids have a high affinity for them. The transfer buffer used for the blotting usually contains formamide because it lowers the annealing temperature of the probe-RNA interaction, thus eliminating the need for high temperatures, which could cause RNA degradation. Once the RNA has been transferred to the membrane, it is immobilized through covalent linkage to the membrane by UV light or heat. After a probe has been labeled, it is hybridized to the RNA on the membrane. Experimental conditions that can affect the efficiency and specificity of hybridization include ionic strength, viscosity, duplex length, mismatched base pairs, and base composition. The membrane is washed to ensure that the probe has bound specifically and to prevent background signals from arising. The hybrid signals are then detected by X-ray film and can be quantified by densitometry. To create controls for comparison in a northern blot, samples not displaying the gene product of interest can be used after determination by microarrays or RT-PCR.

The RNA samples are most commonly separated on agarose gels containing formaldehyde as a denaturing agent for the RNA to limit secondary structure. The gels can be stained with ethidium bromide (EtBr) and viewed under UV light to observe the quality and quantity of RNA before blotting. Polyacrylamide gel electrophoresis with urea can also be used in RNA separation but it is most commonly used for fragmented RNA or microRNAs. An RNA ladder is often run alongside the samples on an electrophoresis gel to observe the size of fragments obtained but in total RNA samples the ribosomal subunits can act as size markers. Since the large ribosomal subunit is 28S (approximately 5kb) and the small ribosomal subunit is 18S (approximately 2kb) two prominent bands appear on the gel, the larger at close to twice the intensity of the smaller.

Probes for northern blotting are composed of nucleic acids with a complementary sequence to all or part of the RNA of interest, they can be DNA, RNA, or oligonucleotides with a minimum of 25 complementary bases to the target sequence. RNA probes (riboprobes) that are transcribed in vitro are able to withstand more rigorous washing steps preventing some of the background noise. Commonly cDNA is created with labelled primers for the RNA sequence of interest to act as the probe in the northern blot. The probes must be labelled either with radioactive isotopes (P) or with chemiluminescence in which alkaline phosphatase or horseradish peroxidase (HRP) break down chemiluminescent substrates producing a detectable emission of light. The chemiluminescent labelling can occur in two ways: either the probe is attached to the enzyme, or the probe is labelled with a ligand (e.g. biotin) for which the ligand (e.g., avidin or streptavidin) is attached to the enzyme (e.g. HRP). X-ray film can detect both the radioactive and chemiluminescent signals and many researchers prefer the chemiluminescent signals because they are faster, more sensitive, and reduce the health hazards that go along with radioactive labels. The same membrane can be probed up to five times without a significant loss of the target RNA.

Northern blotting allows one to observe a particular gene's expression pattern between tissues, organs, developmental stages, environmental stress levels, pathogen infection, and over the course of treatment. The technique has been used to show overexpression of oncogenes and downregulation of tumor-suppressor genes in cancerous cells when compared to 'normal' tissue, as well as the gene expression in the rejection of transplanted organs. If an upregulated gene is observed by an abundance of mRNA on the northern blot the sample can then be sequenced to determine if the gene is known to researchers or if it is a novel finding. The expression patterns obtained under given conditions can provide insight into the function of that gene. Since the RNA is first separated by size, if only one probe type is used variance in the level of each band on the membrane can provide insight into the size of the product, suggesting alternative splice products of the same gene or repetitive sequence motifs. The variance in size of a gene product can also indicate deletions or errors in transcript processing. By altering the probe target used along the known sequence it is possible to determine which region of the RNA is missing.

Analysis of gene expression can be done by several different methods including RT-PCR, RNase protection assays, microarrays, RNA-Seq, serial analysis of gene expression (SAGE), as well as northern blotting. Microarrays are quite commonly used and are usually consistent with data obtained from northern blots; however, at times northern blotting is able to detect small changes in gene expression that microarrays cannot. The advantage that microarrays have over northern blots is that thousands of genes can be visualized at a time, while northern blotting is usually looking at one or a small number of genes.

A problem in northern blotting is often sample degradation by RNases (both endogenous to the sample and through environmental contamination), which can be avoided by proper sterilization of glassware and the use of RNase inhibitors such as DEPC (diethylpyrocarbonate). The chemicals used in most northern blots can be a risk to the researcher, since formaldehyde, radioactive material, ethidium bromide, DEPC, and UV light are all harmful under certain exposures. Compared to RT-PCR, northern blotting has a low sensitivity, but it also has a high specificity, which is important to reduce false positive results.

The advantages of using northern blotting include the detection of RNA size, the observation of alternate splice products, the use of probes with partial homology, the quality and quantity of RNA can be measured on the gel prior to blotting, and the membranes can be stored and reprobed for years after blotting.

For northern blotting for the detection of acetylcholinesterase mRNA the nonradioactive technique was compared to a radioactive technique and found as sensitive as the radioactive one, but requires no protection against radiation and is less time consuming.

Researchers occasionally use a variant of the procedure known as the reverse northern blot. In this procedure, the substrate nucleic acid (that is affixed to the membrane) is a collection of isolated DNA fragments, and the probe is RNA extracted from a tissue and radioactively labelled.
The use of DNA microarrays that have come into widespread use in the late 1990s and early 2000s is more akin to the reverse procedure, in that they involve the use of isolated DNA fragments affixed to a substrate, and hybridization with a probe made from cellular RNA. Thus the reverse procedure, though originally uncommon, enabled northern analysis to evolve into gene expression profiling, in which many (possibly all) of the genes in an organism may have their expression monitored.




</doc>
<doc id="21932" url="https://en.wikipedia.org/wiki?curid=21932" title="Narrow-gauge railway">
Narrow-gauge railway

A narrow-gauge railway (narrow-gauge railroad in the US) is a railway with a track gauge narrower than standard . Most narrow-gauge railways are between and .

Since narrow-gauge railways are usually built with tighter curves, smaller structure gauges, and lighter rails, they can be less costly to build, equip, and operate than standard- or broad-gauge railways (particularly in mountainous or difficult terrain). Lower-cost narrow-gauge railways are often used in mountainous terrain, where engineering savings can be substantial. Lower-cost narrow-gauge railways are often built to serve industries as well as sparsely populated communities where the traffic potential would not justify the cost of a standard- or broad-gauge line. Narrow-gauge railways have specialized use in mines and other environments where a small structure gauge necessitates a small loading gauge.

In some countries, narrow gauge is the standard; Japan, Indonesia, Taiwan, New Zealand, South Africa, and the Australian states of Queensland, Western Australia and Tasmania have a gauge, and Malaysia and Thailand have metre-gauge railways. Narrow-gauge trams, particularly metre-gauge, are common in Europe. Non-industrial, narrow-gauge mountain railways are (or were) common in the Rocky Mountains of the United States and the Pacific Cordillera of Canada, Mexico, Switzerland, Bulgaria, the former Yugoslavia, Greece, and Costa Rica. 

A narrow-gauge railway is one where the distance between the inside edges of the rails is less than . Historically, the term was sometimes used to refer to standard-gauge railways, to distinguish them from broad-gauge railways, but this use no longer applies.

The earliest recorded railway appears in Georgius Agricola's 1556 "De re metallica", which shows a mine in Bohemia with a railway of about gauge. During the 16th century, railways were primarily restricted to hand-pushed, narrow-gauge lines in mines throughout Europe. In the 17th century, mine railways were extended to provide transportation above ground. These lines were industrial, connecting mines with nearby transportation points (usually canals or other waterways). These railways were usually built to the same narrow gauge as the mine railways from which they developed.

The world's first steam locomotive, built in 1802 by Richard Trevithick for the Coalbrookdale Company, ran on a plateway. The first commercially successful steam locomotive was Matthew Murray's Salamanca built in 1812 for the Middleton Railway in Leeds. Salamanca was also the first rack-and-pinion locomotive. During the 1820s and 1830s, a number of industrial narrow-gauge railways in the United Kingdom used steam locomotives. In 1842, the first narrow-gauge steam locomotive outside the UK was built for the -gauge Antwerp-Ghent Railway in Belgium. The first use of steam locomotives on a public, passenger-carrying narrow-gauge railway was in 1865, when the Ffestiniog Railway introduced passenger service after receiving its first locomotives two years earlier.

Many narrow-gauge railways were part of industrial enterprises and served primarily as industrial railways, rather than general carriers. Common uses for these industrial narrow-gauge railways included mining, logging, construction, tunnelling, quarrying, and conveying agricultural products. Extensive narrow-gauge networks were constructed in many parts of the world; 19th-century mountain logging operations often used narrow-gauge railways to transport logs from mill to market. Significant sugarcane railways still operate in Cuba, Fiji, Java, the Philippines, and Queensland, and narrow-gauge railway equipment remains in common use for building tunnels.

The first use of an internal combustion engine to power a narrow-gauge locomotive was in 1902. F. C. Blake built a 7 hp petrol locomotive for the Richmond Main Sewerage Board sewage plant at Mortlake. This gauge locomotive was probably the third petrol-engined locomotive built.

Extensive narrow-gauge rail systems served the front-line trenches of both sides in World War I. They were a short-lived military application, and after the war the surplus equipment created a small boom in European narrow-gauge railway building.

The heavy-duty narrow-gauge railways in Queensland, South Africa, and New Zealand demonstrate that if track is built to a heavy-duty standard, performance almost as good as a standard-gauge line is possible. 

Two-hundred-car trains operate on the Sishen–Saldanha railway line in South Africa, and high-speed Tilt Trains run in Queensland. In South Africa and New Zealand, the loading gauge is similar to the restricted British loading gauge; in New Zealand, some British Rail Mark 2 carriages have been rebuilt with new bogies for use by Tranz Scenic (Wellington-Palmerston North service), Tranz Metro (Wellington-Masterton service), and Transdev Auckland (Auckland suburban services).

Another example of a heavy-duty narrow-gauge line is Brazil's EFVM. gauge, it has over-100-pound rail () and a loading gauge almost as large as US non-excess-height lines. The line has a number of locomotives and 200-plus-car trains. 

Narrow gauge's reduced stability means that its trains cannot run at speeds as high as on broader gauges. For example, if a curve with standard-gauge rail can allow speed up to , the same curve with narrow-gauge rail can only allow speed up to .

In Japan and Queensland, recent permanent-way improvements have allowed trains on gauge tracks to exceed . Queensland Rail's Electric Tilt Train, the fastest train in Australia and the fastest gauge train in the world, set a record of . The speed record for narrow-gauge rail is , set in South Africa in 1978.

A special gauge railcar was built for the Otavi Mining and Railway Company with a design speed of 137 km/h. 
Curve radius is also important for high speeds: narrow-gauge railways allow sharper curves, but these limit a vehicle's safe speed.

Many narrow gauges, from gauge and gauge, are in present or former use. They fall into several broad categories:

 track gauge (also known as Scotch gauge) was adopted by early 19th-century railways, primarily in the Lanarkshire area of Scotland. lines were also constructed, and both were eventually converted to standard gauge.

 between the inside of the rail heads, its name and classification vary worldwide and it has about of track.


As its name implies, metre gauge is a track gauge of . It has about of track.

According to Italian law, track gauges in Italy were defined from the centre of each rail rather than the inside edges of the rails. This gauge, measured between the edges of the rails, is known as Italian metre gauge.

There were a number of large railroad systems in North America; notable examples include the Denver & Rio Grande and Rio Grande Southern in Colorado and the South Pacific Coast and West Side Lumber Co of California. was also a common track gauge in South America, Ireland and on the Isle of Man. was a common gauge in Europe. Swedish three-foot-gauge railways () are unique to that country.

A few railways and tramways were built to gauge, including Nankai Main Line (later converted to ), Ocean Pier Railway at Atlantic City, Seaton Tramway (converted from ) and Waiorongomai Tramway.

 gauge railways are commonly used for rack railways. Imperial gauge railways were generally constructed in the former British colonies. Bosnian gauge and railways are predominantly found in Russia and Eastern Europe.

Gauges such as , and were used in parts of the UK, particularly for railways in Wales and the borders, with some industrial use in the coal industry. Some sugar cane lines in Cuba were .

 gauge railways were generally constructed in the former British colonies. , and were used in Europe.

Gauges below were rare. Arthur Percival Heywood developed gauge estate railways in Britain and Decauville produced a range of industrial railways running on and tracks, most commonly in restricted environments such as underground mine railways, parks and farms, in France. Several gauge railways were built in Britain to serve ammunition depots and other military facilities, particularly during World War I.




</doc>
<doc id="21933" url="https://en.wikipedia.org/wiki?curid=21933" title="Neutron activation analysis">
Neutron activation analysis

Neutron activation analysis (NAA) is the nuclear process used for determining the concentrations of elements in a vast amount of materials. NAA allows discrete sampling of elements as it disregards the chemical form of a sample, and focuses solely on its nucleus. The method is based on neutron activation and therefore requires a source of neutrons. The sample is bombarded with neutrons, causing the elements to form radioactive isotopes. The radioactive emissions and radioactive decay paths for each element are well known. Using this information, it is possible to study spectra of the emissions of the radioactive sample, and determine the concentrations of the elements within it. A particular advantage of this technique is that it does not destroy the sample, and thus has been used for analysis of works of art and historical artifacts. NAA can also be used to determine the activity of a radioactive sample.

If NAA is conducted directly on irradiated samples it is termed Instrumental Neutron Activation Analysis (INAA). In some cases irradiated samples are subjected to chemical separation to remove interfering species or to concentrate the radioisotope of interest, this technique is known as Radiochemical Neutron Activation Analysis (RNAA).

NAA can perform non-destructive analyses on solids, liquids, suspensions, slurries, and gases with no or minimal preparation. Due to the penetrating nature of incident neutrons and resultant gamma rays, the technique provides a true bulk analysis. As different radioisotopes have different half-lives, counting can be delayed to allow interfering species to decay eliminating interference. Until the introduction of ICP-AES and PIXE, NAA was the standard analytical method for performing multi-element analyses with minimum detection limits in the sub-ppm range. Accuracy of NAA is in the region of 5%, and relative precision is often better than 0.1%. There are two noteworthy drawbacks to the use of NAA; even though the technique is essentially non-destructive, the irradiated sample will remain radioactive for many years after the initial analysis, requiring handling and disposal protocols for low-level to medium-level radioactive material; also, the number of suitable activation nuclear reactors is declining; with a lack of irradiation facilities, the technique has declined in popularity and become more expensive.

Neutron activation analysis is a sensitive multi-element analytical technique used for both qualitative and quantitative analysis of major, minor, trace and rare elements. NAA was discovered in 1936 by Hevesy and Levi, who found that samples containing certain rare earth elements became highly radioactive after exposure to a source of neutrons. This observation led to the use of induced radioactivity for the identification of elements. NAA is significantly different from other spectroscopic analytical techniques in that it is based not on electronic transitions but on nuclear transitions. To carry out an NAA analysis, the specimen is placed into a suitable irradiation facility and bombarded with neutrons. This creates artificial radioisotopes of the elements present. Following irradiation, the artificial radioisotopes decay with emission of particles or, more importantly gamma rays, which are characteristic of the element from which they were emitted.

For the NAA procedure to be successful, the specimen or sample must be selected carefully. In many cases small objects can be irradiated and analysed intact without the need of sampling. But, more commonly, a small sample is taken, usually by drilling in an inconspicuous place. About 50 mg (one-twentieth of a gram) is a sufficient sample, so damage to the object is minimised. It is often good practice to remove two samples using two different drill bits made of different materials. This will reveal any contamination of the sample from the drill bit material itself. The sample is then encapsulated in a vial made of either high purity linear polyethylene or quartz. These sample vials come in many shapes and sizes to accommodate many specimen types. The sample and a standard are then packaged and irradiated in a suitable reactor at a constant, known neutron flux. A typical reactor used for activation uses uranium fission, providing a high neutron flux and the highest available sensitivities for most elements. The neutron flux from such a reactor is in the order of 10 neutrons cm s. The type of neutrons generated are of relatively low kinetic energy (KE), typically less than 0.5 eV. These neutrons are termed thermal neutrons. Upon irradiation, a thermal neutron interacts with the target nucleus via a non-elastic collision, causing neutron capture. This collision forms a compound nucleus which is in an excited state. The excitation energy within the compound nucleus is formed from the binding energy of the thermal neutron with the target nucleus. This excited state is unfavourable and the compound nucleus will almost instantaneously de-excite (transmutate) into a more stable configuration through the emission of a prompt particle and one or more characteristic prompt gamma photons. In most cases, this more stable configuration yields a radioactive nucleus. The newly formed radioactive nucleus now decays by the emission of both particles and one or more characteristic delayed gamma photons. This decay process is at a much slower rate than the initial de-excitation and is dependent on the unique half-life of the radioactive nucleus. These unique half-lives are dependent upon the particular radioactive species and can range from fractions of a second to several years. Once irradiated, the sample is left for a specific decay period, then placed into a detector, which will measure the nuclear decay according to either the emitted particles, or more commonly, the emitted gamma rays.

NAA can vary according to a number of experimental parameters. The kinetic energy of the neutrons used for irradiation will be a major experimental parameter. The above description is of activation by slow neutrons, slow neutrons are fully moderated within the reactor and have KE <0.5 eV. Medium KE neutrons may also be used for activation, these neutrons have been only partially moderated and have KE of 0.5 eV to 0.5 MeV, and are termed epithermal neutrons. Activation with epithermal neutrons is known as Epithermal NAA (ENAA). High KE neutrons are sometimes used for activation, these neutrons are unmoderated and consist of primary fission neutrons. High KE or fast neutrons have a KE >0.5 MeV. Activation with fast neutrons is termed Fast NAA (FNAA).
Another major experimental parameter is whether nuclear decay products (gamma rays or particles) are measured during neutron irradiation (prompt gamma), or at some time after irradiation (delayed gamma, DGNAA). PGNAA is generally performed by using a neutron stream tapped off the nuclear reactor via a beam port. Neutron fluxes from beam ports are the order of 10 times weaker than inside a reactor. This is somewhat compensated for by placing the detector very close to the sample reducing the loss in sensitivity due to low flux. PGNAA is generally applied to elements with extremely high neutron capture cross-sections; elements which decay too rapidly to be measured by DGNAA; elements that produce only stable isotopes; or elements with weak decay gamma ray intensities. PGNAA is characterised by short irradiation times and short decay times, often in the order of seconds and minutes.
DGNAA is applicable to the vast majority of elements that form artificial radioisotopes. DG analyses are often performed over days, weeks or even months. This improves sensitivity for long-lived radionuclides as it allows short-lived radionuclide to decay, effectively eliminating interference. DGNAA is characterised by long irradiation times and long decay times, often in the order of hours, weeks or longer.

a range of different sources can be used:

Some reactors are used for the neutron irradiation of samples for radioisotope production for a range of purposes. The sample can be placed in an irradiation container which is then placed in the reactor; if epithermal neutrons are required for the irradiation then cadmium can be used to filter out the thermal neutrons.

A relatively simple Farnsworth–Hirsch fusor can be used to generate neutrons for NAA experiments. The advantages of this kind of apparatus is that it is compact, often benchtop-sized, and that it can simply be turned off and on. A disadvantage is that this type of source will not produce the neutron flux that can be obtained using a reactor.

For many workers in the field a reactor is an item which is too expensive, instead it is common to use a neutron source which uses a combination of an alpha emitter and beryllium. These sources tend to be much weaker than reactors.

These can be used to create pulses of neutrons, they have been used for some activation work where the decay of the target isotope is very rapid. For instance in oil wells.

There are a number of detector types and configurations used in NAA. Most are designed to detect the emitted gamma radiation. The most common types of gamma detectors encountered in NAA are the gas ionisation type, scintillation type and the semiconductor type. Of these the scintillation and semiconductor type are the most widely employed. There are two detector configurations utilised, they are the planar detector, used for PGNAA and the well detector, used for DGNAA. The planar detector has a flat, large collection surface area and can be placed close to the sample. The well detector ‘surrounds’ the sample with a large collection surface area.

Scintillation-type detectors use a radiation-sensitive crystal, most commonly thallium-doped sodium iodide (NaI(Tl)), which emits light when struck by gamma photons. These detectors have excellent sensitivity and stability, and a reasonable resolution.

Semiconductor detectors utilise the semiconducting element germanium. The germanium is processed to form a p-i-n (positive-intrinsic-negative) diode, and when cooled to ~77 K by liquid nitrogen to reduce dark current and detector noise, produces a signal which is proportional to the photon energy of the incoming radiation. There are two types of germanium detector, the lithium-drifted germanium or Ge(Li) (pronounced ‘jelly’), and the high-purity germanium or HPGe.
The semiconducting element silicon may also be used but germanium is preferred, as its higher atomic number makes it more efficient at stopping and detecting high energy gamma rays. Both Ge(Li) and HPGe detectors have excellent sensitivity and resolution, but Ge(Li) detectors are unstable at room temperature, with the lithium drifting into the intrinsic region ruining the detector. The development of undrifted high purity germanium has overcome this problem.

Particle detectors can also be used to detect the emission of alpha (α) and beta (β) particles which often accompany the emission of a gamma photon but are less favourable, as these particles are only emitted from the surface of the sample and are often absorbed or attenuated by atmospheric gases requiring expensive vacuum conditions to be effectively detected. Gamma rays, however, are not absorbed or attenuated by atmospheric gases, and can also escape from deep within the sample with minimal absorption.

NAA can detect up to 74 elements depending upon the experimental procedure, with minimum detection limits ranging from 0.1 to 1x10 ng g depending on element under investigation. Heavier elements have larger nuclei, therefore they have a larger neutron capture cross-section and are more likely to be activated. Some nuclei can capture a number of neutrons and remain relatively stable, not undergoing transmutation or decay for many months or even years. Other nuclei decay instantaneously or form only stable isotopes and can only be identified by PGNAA.

Neutron Activation Analysis has a wide variety of applications including within the fields of archaeology, soil science, geology, forensics, and the semiconductor industry. Forensically, hairs subjected to a detailed forensic neutron analysis to determine whether they had sourced from the same individuals was first used in the trial of John Norman Collins.

Archaeologists use NAA in order to determine the elements that comprise certain artifacts. This technique is used because it is nondestructive and it can relate an artifact to its source by its chemical signature. This method has proven to be very successful at determining trade routes, particularly for obsidian, with the ability of NAA to distinguish between chemical compositions. In agricultural processes, the movement of fertilizers and pesticides is influenced by surface and subsurface movement as it infiltrates the water supplies. In order to track the distribution of the fertilizers and pesticides, bromide ions in various forms are used as tracers that move freely with the flow of water while having minimal interaction with the soil. Neutron activation analysis is used to measure bromide so that extraction is not necessary for analysis. NAA is used in geology to aid in researching the processes that formed the rocks through the analysis of the rare earth elements and trace elements. It also assists in locating ore deposits and tracking certain elements. Neutron activation analysis is also used to create standards in the semiconductor industry. Semiconductors require a high level of purity, with contamination significantly reducing the quality of the semiconductor. NAA is used to detect trace impurities and establish contamination standards, because it involves limited sample handling and high sensitivity.


</doc>
<doc id="21935" url="https://en.wikipedia.org/wiki?curid=21935" title="Nondeterministic Turing machine">
Nondeterministic Turing machine

In theoretical computer science, a nondeterministic Turing machine (NTM) is a theoretical model of computation whose governing rules specify more than one possible action when in some given situations. That is, an NTM's next state is "not" completely determined by its action and the current symbol it sees (unlike a deterministic Turing machine). 

NTMs are sometimes used in thought experiments to examine the abilities and limitations of computers. One of the most important open problems in theoretical computer science is the P vs. NP problem, which (among other equivalent formulations) concerns the question of how difficult it is to simulate nondeterministic computation with a deterministic computer.

In essence, a Turing machine is imagined to be a simple computer that reads and writes symbols one at a time on an endless tape by strictly following a set of rules. It determines what action it should perform next according to its internal "state" and "what symbol it currently sees". An example of one of a Turing Machine's rules might thus be: "If you are in state 2 and you see an 'A', change it to 'B', move left, and change to state 3."

In a deterministic Turing machine (DTM), the set of rules prescribes at most one action to be performed for any given situation.

A deterministic Turing machine has a "transition function" that, for a given state and symbol under the tape head, specifies three things: 
For example, an X on the tape in state 3 might make the DTM write a Y on the tape, move the head one position to the right, and switch to state 5.

In contrast to a deterministic Turing machine, in a nondeterministic Turing machine (NTM) the set of rules may prescribe more than one action to be performed for any given situation. For example, an X on the tape in state 3 might allow the NTM to:
or

How does the NTM "know" which of these actions it should take? There are two ways of looking at it. One is to say that the machine is the "luckiest possible guesser"; it always picks a transition that eventually leads to an accepting state, if there is such a transition. The other is to imagine that the machine "branches" into many copies, each of which follows one of the possible transitions. Whereas a DTM has a single "computation path" that it follows, an NTM has a "computation tree". If at least one branch of the tree halts with an "accept" condition, the NTM accepts the input.

A nondeterministic Turing machine can be formally defined as a 6-tuple formula_1, where

The difference with a standard (deterministic) Turing machine is that, for deterministic Turing machines, the transition relation is a function rather than just a relation.

Configurations and the "yields" relation on configurations, which describes the possible actions of the Turing machine given any possible contents of the tape, are as for standard Turing machines, except that the "yields" relation is no longer single-valued. (If the machine is deterministic, the possible computations are all prefixes of a single, possibly infinite, path.)

The input for an NTM is provided in the same manner as for a deterministic Turing machine: the machine is started in the configuration in which the tape head is on the first character of the string (if any), and the tape is all blank otherwise.

An NTM accepts an input string if and only if "at least one" of the possible computational paths starting from that string puts the machine into an accepting state. When simulating the many branching paths of an NTM on a deterministic machine, we can stop the entire simulation as soon as "any" branch reaches an accepting state.

As a mathematical construction used primarily in proofs, there are a variety of minor variations on the definition of an NTM, but these variations all accept equivalent languages.

The head movement in the output of the transition relation is often encoded numerically instead of using letters to represent moving the head Left (-1), Stationary (0), and Right (+1); giving a transition function output of formula_11. It is common to omit the stationary (0) output, and instead insert the transitive closure of any desired stationary transitions. 

Some authors add an explicit "reject" state,
which causes the NTM to halt without accepting. This definition still retains the asymmetry that "any" nondeterministic branch can accept, but "every" branch must reject for the string to be rejected.

Any computational problem that can be solved by a DTM can also be solved by a NTM, and vice versa. However, it is believed that in general the time complexity may not be the same.

NTMs include DTMs as special cases, so every computation that can be carried out by a DTM can also be carried out by the equivalent NTM.

It might seem that NTMs are more powerful than DTMs, since they can allow trees of possible computations arising from the same initial configuration, accepting a string if any one branch in the tree accepts it. However, it is possible to simulate NTMs with DTMs, and in fact this can be done in more than one way.

One approach is to use a DTM of which the configurations represent multiple configurations of the NTM, and the DTM's operation consists of visiting each of them in turn, executing a single step at each visit, and spawning new configurations whenever the transition relation defines multiple continuations.

Another construction simulates NTMs with 3-tape DTMs, of which the first tape always holds the original input string, the second is used to simulate a particular computation of the NTM, and the third encodes a path in the NTM's computation tree. The 3-tape DTMs are easily simulated with a normal single-tape DTM.

In the second construction, the constructed DTM effectively performs a breadth-first search of the NTM's computation tree, visiting all possible computations of the NTM in order of increasing length until it finds an accepting one. Therefore, the length of an accepting computation of the DTM is, in general, exponential in the length of the shortest accepting computation of the NTM. This is believed to be a general property of simulations of NTMs by DTMs. The P = NP problem, the most famous unresolved question in computer science, concerns one case of this issue: whether or not every problem solvable by a NTM in polynomial time is necessarily also solvable by a DTM in polynomial time.

An NTM has the property of bounded nondeterminism. That is, if an NTM always halts on a given input tape "T" then it halts in a bounded number of steps, and therefore can only have a bounded number of possible configurations.

Because quantum computers use quantum bits, which can be in superpositions of states, rather than conventional bits, there is sometimes a misconception that quantum computers are NTMs. However, it is believed by experts (but has not been proven) that the power of quantum computers is, in fact, incomparable to that of NTMs; that is, problems likely exist that an NTM could efficiently solve that a quantum computer cannot and vice versa. In particular, it is likely that NP-complete problems are solvable by NTMs but not by quantum computers in polynomial time. 

Intuitively speaking, while a quantum computer can indeed be in a superposition state corresponding to all possible computational branches having been executed at the same time (similar to an NTM), the final measurement will collapse the quantum computer into a randomly selected branch. This branch then does not, in general, represent the sought-for solution, unlike the NTM, which is allowed to pick the right solution among the exponentially many branches.





</doc>
<doc id="21937" url="https://en.wikipedia.org/wiki?curid=21937" title="Nitrogen narcosis">
Nitrogen narcosis

Narcosis while diving (also known as nitrogen narcosis, inert gas narcosis, raptures of the deep, Martini effect) is a reversible alteration in consciousness that occurs while diving at depth. It is caused by the anesthetic effect of certain gases at high pressure. The Greek word (narkōsis), "the act of making numb", is derived from (narkē), "numbness, torpor", a term used by Homer and Hippocrates. Narcosis produces a state similar to drunkenness (alcohol intoxication), or nitrous oxide inhalation. It can occur during shallow dives, but does not usually become noticeable at depths less than .

Except for helium and probably neon, all gases that can be breathed have a narcotic effect, although widely varying in degree. The effect is consistently greater for gases with a higher lipid solubility, and there is good evidence that the two properties are mechanistically related. As depth increases, the mental impairment may become hazardous. Divers can learn to cope with some of the effects of narcosis, but it is impossible to develop a tolerance. Narcosis affects all divers, although susceptibility varies widely among individuals and from dive to dive.

Narcosis may be completely reversed in a few minutes by ascending to a shallower depth, with no long-term effects. Thus narcosis while diving in open water rarely develops into a serious problem as long as the divers are aware of its symptoms, and are able to ascend to manage it. Diving much beyond is generally considered outside the scope of recreational diving. In order to dive at greater depths, as narcosis and oxygen toxicity become critical risk factors, specialist training is required in the use of various helium-containing gas mixtures such as trimix or heliox. These mixtures prevent narcosis by replacing some or all of the inert fraction of the breathing gas with non-narcotic helium.

Narcosis results from breathing gases under elevated pressure, and may be classified by the principal gas involved. The noble gases, except helium and probably neon, as well as nitrogen, oxygen and hydrogen cause a decrement in mental function, but their effect on psychomotor function (processes affecting the coordination of sensory or cognitive processes and motor activity) varies widely. The effect of carbon dioxide is a consistent diminution of mental and psychomotor function. The noble gases argon, krypton, and xenon are more narcotic than nitrogen at a given pressure, and xenon has so much anesthetic activity that it is a usable anesthetic at 80% concentration and normal atmospheric pressure. Xenon has historically been too expensive to be used very much in practice, but it has been successfully used for surgical operations, and xenon anesthesia systems are still being proposed and designed.

Due to its perception-altering effects, the onset of narcosis may be hard to recognize. At its most benign, narcosis results in relief of anxiety – a feeling of tranquility and mastery of the environment. These effects are essentially identical to various concentrations of nitrous oxide. They also resemble (though not as closely) the effects of alcohol or cannabis and the familiar benzodiazepine drugs such as diazepam and alprazolam. Such effects are not harmful unless they cause some immediate danger to go unrecognized and unaddressed. Once stabilized, the effects generally remain the same at a given depth, only worsening if the diver ventures deeper.

The most dangerous aspects of narcosis are the impairment of judgement, multi-tasking and coordination, and the loss of decision-making ability and focus. Other effects include vertigo and visual or auditory disturbances. The syndrome may cause exhilaration, giddiness, extreme anxiety, depression, or paranoia, depending on the individual diver and the diver's medical or personal history. When more serious, the diver may feel overconfident, disregarding normal safe diving practices. Slowed mental activity, as indicated by increased reaction time and increased errors in cognitive function, are effects which increase the risk of a diver mismanaging an incident. Narcosis reduces both the perception of cold discomfort and shivering and thereby affects the production of body heat and consequently allows a faster drop in the core temperature in cold water, with reduced awareness of the developing problem.

The relation of depth to narcosis is sometimes informally known as "Martini's law", the idea that narcosis results in the feeling of one martini for every below depth. Professional divers use such a calculation only as a rough guide to give new divers a metaphor, comparing a situation they may be more familiar with.

Reported signs and symptoms are summarized against typical depths in meters and feet of sea water in the following table, closely adapted from "Deeper into Diving" by Lippman and Mitchell:

The cause of narcosis is related to the increased solubility of gases in body tissues, as a result of the elevated pressures at depth (Henry's law). Modern theories have suggested that inert gases dissolving in the lipid bilayer of cell membranes cause narcosis. More recently, researchers have been looking at neurotransmitter receptor protein mechanisms as a possible cause of narcosis. The breathing gas mix entering the diver's lungs will have the same pressure as the surrounding water, known as the ambient pressure. After any change of depth, the pressure of gases in the blood passing through the brain catches up with ambient pressure within a minute or two, which results in a delayed narcotic effect after descending to a new depth. Rapid compression potentiates narcosis owing to carbon dioxide retention.

A divers' cognition may be affected on dives as shallow as , but the changes are not usually noticeable. There is no reliable method to predict the depth at which narcosis becomes noticeable, or the severity of the effect on an individual diver, as it may vary from dive to dive even on the same day.

Significant impairment due to narcosis is an increasing risk below depths of about , corresponding to an ambient pressure of about . Most sport scuba training organizations recommend depths of no more than because of the risk of narcosis. When breathing air at depths of  – an ambient pressure of about  – narcosis in most divers leads to hallucinations, loss of memory, and unconsciousness. A number of divers have died in attempts to set air depth records below . Because of these incidents, "Guinness World Records" no longer reports on this figure.

Narcosis has been compared with altitude sickness regarding its variability of onset (though not its symptoms); its effects depend on many factors, with variations between individuals. Thermal cold, stress, heavy work, fatigue, and carbon dioxide retention all increase the risk and severity of narcosis. Carbon dioxide has a high narcotic potential and also causes increased blood flow to the brain, increasing the effects of other gases. Increased risk of narcosis results from increasing the amount of carbon dioxide retained through heavy exercise, shallow or skip breathing, or because of poor gas exchange in the lungs.

Narcosis is known to be additive to even minimal alcohol intoxication. Other sedative and analgesic drugs, such as opiate narcotics and benzodiazepines, add to narcosis.

The precise mechanism is not well understood, but it appears to be the direct effect of gas dissolving into nerve membranes and causing temporary disruption in nerve transmissions. While the effect was first observed with air, other gases including argon, krypton and hydrogen cause very similar effects at higher than atmospheric pressure. Some of these effects may be due to antagonism at NMDA receptors and potentiation of GABA receptors, similar to the mechanism of nonpolar anesthetics such diethyl ether or ethylene. However, their reproduction by the very chemically inactive gas argon makes them unlikely to be a strictly chemical bonding to receptors in the usual sense of a chemical bond. An indirect physical effect – such as a change in membrane volume – would therefore be needed to affect the ligand-gated ion channels of nerve cells. Trudell "et al." have suggested non-chemical binding due to the attractive van der Waals force between proteins and inert gases.

Similar to the mechanism of ethanol's effect, the increase of gas dissolved in nerve cell membranes may cause altered ion permeability properties of the neural cells' lipid bilayers. The partial pressure of a gas required to cause a measured degree of impairment correlates well with the lipid solubility of the gas: the greater the solubility, the less partial pressure is needed.

An early theory, the Meyer-Overton hypothesis, suggested that narcosis happens when the gas penetrates the lipids of the brain's nerve cells, causing direct mechanical interference with the transmission of signals from one nerve cell to another. More recently, specific types of chemically gated receptors in nerve cells have been identified as being involved with anesthesia and narcosis. However, the basic and most general underlying idea, that nerve transmission is altered in many diffuse areas of the brain as a result of gas molecules dissolved in the nerve cells' fatty membranes, remains largely unchallenged.

The management of narcosis is simply to ascend to shallower depths; the effects then disappear within minutes. In the event of complications or other conditions being present, ascending is always the correct initial response. Should problems remain, then it is necessary to abort the dive. The decompression schedule can still be followed unless other conditions require emergency assistance.

The symptoms of narcosis may be caused by other factors during a dive: ear problems causing disorientation or nausea; early signs of oxygen toxicity causing visual disturbances; or hypothermia causing rapid breathing and shivering. Nevertheless, the presence of any of these symptoms should imply narcosis. Alleviation of the effects upon ascending to a shallower depth will confirm the diagnosis. Given the setting, other likely conditions do not produce reversible effects. In the rare event of misdiagnosis when another condition is causing the symptoms, the initial management – ascending closer to the surface – is still essential.

The most straightforward way to avoid nitrogen narcosis is for a diver to limit the depth of dives. Since narcosis becomes more severe as depth increases, a diver keeping to shallower depths can avoid serious narcosis. Most recreational dive schools will only certify basic divers to depths of , and at these depths narcosis does not present a significant risk. Further training is normally required for certification up to on air, and this training should include a discussion of narcosis, its effects, and cure. Some diver training agencies offer specialized training to prepare recreational divers to go to depths of , often consisting of further theory and some practice in deep dives under close supervision. Scuba organizations that train for diving beyond recreational depths, may forbid diving with gases that cause too much narcosis at depth in the average diver, and strongly encourage the use of other breathing gas mixes containing helium in place of some or all of the nitrogen in air – such as trimix and heliox – because helium has no narcotic effect. The use of these gases forms part of technical diving and requires further training and certification.

While the individual diver cannot predict exactly at what depth the onset of narcosis will occur on a given day, the first symptoms of narcosis for any given diver are often more predictable and personal. For example, one diver may have trouble with eye focus (close accommodation for middle-aged divers), another may experience feelings of euphoria, and another feelings of claustrophobia. Some divers report that they have hearing changes, and that the sound their exhaled bubbles make becomes different. Specialist training may help divers to identify these personal onset signs, which may then be used as a signal to ascend to avoid the narcosis, although severe narcosis may interfere with the judgement necessary to take preventive action.

Deep dives should be made only after a gradual training to test the individual diver's sensitivity to increasing depths, with careful supervision and logging of reactions. Scientific evidence does not show that a diver can train to overcome any measure of narcosis at a given depth or become tolerant of it.

Equivalent narcotic depth (END) is a commonly used way of expressing the narcotic effect of different breathing gases. The National Oceanic and Atmospheric Administration (NOAA) Diving Manual now states that oxygen and nitrogen should be considered equally narcotic. Standard tables, based on relative lipid solubilities, list conversion factors for narcotic effect of other gases. For example, hydrogen at a given pressure has a narcotic effect equivalent to nitrogen at 0.55 times that pressure, so in principle it should be usable at more than twice the depth. Argon, however, has 2.33 times the narcotic effect of nitrogen, and is a poor choice as a breathing gas for diving (it is used as a drysuit inflation gas, owing to its low thermal conductivity). Some gases have other dangerous effects when breathed at pressure; for example, high-pressure oxygen can lead to oxygen toxicity. Although helium is the least intoxicating of the breathing gases, at greater depths it can cause high pressure nervous syndrome, a still mysterious but apparently unrelated phenomenon. Inert gas narcosis is only one factor influencing the choice of gas mixture; the risks of decompression sickness and oxygen toxicity, cost, and other factors are also important.

Because of similar and additive effects, divers should avoid sedating medications and drugs, such as cannabis and alcohol before any dive. A hangover, combined with the reduced physical capacity that goes with it, makes nitrogen narcosis more likely. Experts recommend total abstinence from alcohol for at least 12 hours before diving, and longer for other drugs.

Narcosis is potentially one of the most dangerous conditions to affect the scuba diver below about . Except for occasional amnesia of events at depth, the effects of narcosis are entirely removed on ascent and therefore pose no problem in themselves, even for repeated, chronic or acute exposure. Nevertheless, the severity of narcosis is unpredictable and it can be fatal while diving, as the result of illogical behavior in a dangerous environment.

Tests have shown that all divers are affected by nitrogen narcosis, though some experience lesser effects than others. Even though it is possible that some divers can manage better than others because of learning to cope with the subjective impairment, the underlying behavioral effects remain. These effects are particularly dangerous because a diver may feel they are not experiencing narcosis, yet still be affected by it.

French researcher Victor T. Junod was the first to describe symptoms of narcosis in 1834, noting "the functions of the brain are activated, imagination is lively, thoughts have a peculiar charm and, in some persons, symptoms of intoxication are present." Junod suggested that narcosis resulted from pressure causing increased blood flow and hence stimulating nerve centers. Walter Moxon (1836–1886), a prominent Victorian physician, hypothesized in 1881 that pressure forced blood to inaccessible parts of the body and the stagnant blood then resulted in emotional changes. The first report of anesthetic potency being related to lipid solubility was published by Hans H. Meyer in 1899, entitled "Zur Theorie der Alkoholnarkose". Two years later a similar theory was published independently by Charles Ernest Overton. What became known as the Meyer-Overton Hypothesis may be illustrated by a graph comparing narcotic potency with solubility in oil.

In 1939, Albert R. Behnke and O. D. Yarborough demonstrated that gases other than nitrogen also could cause narcosis. For an inert gas the narcotic potency was found to be proportional to its lipid solubility. As hydrogen has only 0.55 the solubility of nitrogen, deep diving experiments using hydrox were conducted by Arne Zetterström between 1943 and 1945. Jacques-Yves Cousteau in 1953 famously described it as "l’ivresse des grandes profondeurs" or the "rapture of the deep".

Further research into the possible mechanisms of narcosis by anesthetic action led to the "minimum alveolar concentration" concept in 1965. This measures the relative concentration of different gases required to prevent motor response in 50% of subjects in response to stimulus, and shows similar results for anesthetic potency as the measurements of lipid solubility. The (NOAA) Diving Manual was revised to recommend treating oxygen as if it were as narcotic as nitrogen, following research by Christian J. Lambertsen "et al." in 1977 and 1978.




</doc>
<doc id="21938" url="https://en.wikipedia.org/wiki?curid=21938" title="Neoproterozoic">
Neoproterozoic

The Neoproterozoic Era is the unit of geologic time from .

It is the last era of the Precambrian Supereon and the Proterozoic Eon; it is subdivided into the Tonian, Cryogenian, and Ediacaran Periods. It is preceded by the Mesoproterozoic era and succeeded by the Paleozoic era of the Phanerozoic eon.

The most severe glaciation known in the geologic record occurred during the Cryogenian, when ice sheets may have reached the equator and formed a "Snowball Earth".

The earliest fossils of complex multicellular life are found in the Ediacaran period. These organisms make up the Ediacaran biota, including the oldest definitive animals in the fossil record.

According to Rino and co-workers, the sum of the continental crust formed in the Pan-African orogeny and the Grenville orogeny makes the Neoproterozoic the period of Earth's history that has produced most continental crust.

At the onset of the Neoproterozoic the supercontinent Rodinia, which had assembled during the late Mesoproterozoic, straddled the equator. During the Tonian, rifting commenced which broke Rodinia into a number of individual land masses.

Possibly as a consequence of the low-latitude position of most continents, several large-scale glacial events occurred during the Neoproterozoic Era including the Sturtian and Marinoan glaciations of the Cryogenian Period.

These glaciations are believed to have been so severe that there were ice sheets at the equator—a state known as the "Snowball Earth".

Neoproterozoic time is subdivided into the Tonian (1000–720 Ma), Cryogenian (720–635 Ma) and Ediacaran (635–541 Ma) periods. 

In the regional timescale of Russia, the Tonian and Cryogenian correspond to the Late Riphean; the Ediacaran corresponds to the Early to middle Vendian. Russian geologists divide the Neoproterozoic of Siberia into the Mayanian (from 1000 to 850 Ma) followed by the Baikalian (from 850 to 650 Ma, loosely equivalent to the Cryogenian).

The idea of the Neoproterozoic Era was introduced in the 1960s. Nineteenth-century paleontologists set the start of multicellular life at the first appearance of hard-shelled arthropods called trilobites and archeocyathid sponges at the beginning of the Cambrian Period. In the early 20th century, paleontologists started finding fossils of multicellular animals that predated the Cambrian. A complex fauna was found in South West Africa in the 1920s but was inaccurately dated. Another fauna was found in South Australia in the 1940s, but it was not thoroughly examined until the late 1950s. Other possible early animal fossils were found in Russia, England, Canada, and elsewhere (see Ediacaran biota). Some were determined to be pseudofossils, but others were revealed to be members of rather complex biotas that remain poorly understood. At least 25 regions worldwide have yielded metazoan fossils older than the classical Precambrian–Cambrian boundary (which is currently dated at ).

A few of the early animals appear possibly to be ancestors of modern animals. Most fall into ambiguous groups of frond-like organisms; discoids that might be holdfasts for stalked organisms ("medusoids"); mattress-like forms; small calcareous tubes; and armored animals of unknown provenance.

These were most commonly known as Vendian biota until the formal naming of the Period, and are currently known as Ediacaran Period biota. Most were soft bodied. The relationships, if any, to modern forms are obscure. Some paleontologists relate many or most of these forms to modern animals. Others acknowledge a few possible or even likely relationships but feel that most of the Ediacaran forms are representatives of unknown animal types.

In addition to Ediacaran biota, two other types of biota were discovered in China (the Doushantuo Formation and Hainan Formation).

The nomenclature for the terminal period of the Neoproterozoic Era has been unstable. Russian and Nordic geologists referred to the last period of the Neoproterozoic as the Vendian, while Chinese geologists referred to it as the Sinian, and most Australians and North Americans used the name Ediacaran.

However, in 2004, the International Union of Geological Sciences ratified the Ediacaran Period to be a geological age of the Neoproterozoic, ranging from to million years ago. The Ediacaran Period boundaries are the only Precambrian boundaries defined by biologic Global Boundary Stratotype Section and Points, rather than the absolute Global Standard Stratigraphic Ages.



</doc>
<doc id="21939" url="https://en.wikipedia.org/wiki?curid=21939" title="National Security Agency">
National Security Agency

The National Security Agency (NSA) is a national-level intelligence agency of the United States Department of Defense, under the authority of the Director of National Intelligence. The NSA is responsible for global monitoring, collection, and processing of information and data for foreign and domestic intelligence and counterintelligence purposes, specializing in a discipline known as signals intelligence (SIGINT). The NSA is also tasked with the protection of U.S. communications networks and information systems. The NSA relies on a variety of measures to accomplish its mission, the majority of which are clandestine.

Originating as a unit to decipher coded communications in World War II, it was officially formed as the NSA by President Harry S. Truman in 1952. Since then, it has become the largest of the U.S. intelligence organizations in terms of personnel and budget. The NSA currently conducts worldwide mass data collection and has been known to physically bug electronic systems as one method to this end. The NSA is also alleged to have been behind such attack software as Stuxnet, which severely damaged Iran's nuclear program. The NSA, alongside the Central Intelligence Agency (CIA), maintains a physical presence in many countries across the globe; the CIA/NSA joint Special Collection Service (a highly classified intelligence team) inserts eavesdropping devices in high value targets (such as presidential palaces or embassies). SCS collection tactics allegedly encompass "close surveillance, burglary, wiretapping, [and] breaking and entering".

Unlike the CIA and the Defense Intelligence Agency (DIA), both of which specialize primarily in foreign human espionage, the NSA does not publicly conduct human-source intelligence gathering. The NSA is entrusted with providing assistance to, and the coordination of, SIGINT elements for other government organizations – which are prevented by law from engaging in such activities on their own. As part of these responsibilities, the agency has a co-located organization called the Central Security Service (CSS), which facilitates cooperation between the NSA and other U.S. defense cryptanalysis components. To further ensure streamlined communication between the signals intelligence community divisions, the NSA Director simultaneously serves as the Commander of the United States Cyber Command and as Chief of the Central Security Service.

The NSA's actions have been a matter of political controversy on several occasions, including its spying on anti–Vietnam War leaders and the agency's participation in economic espionage. In 2013, the NSA had many of its secret surveillance programs revealed to the public by Edward Snowden, a former NSA contractor. According to the leaked documents, the NSA intercepts and stores the communications of over a billion people worldwide, including United States citizens. The documents also revealed the NSA tracks hundreds of millions of people's movements using cellphones' metadata. Internationally, research has pointed to the NSA's ability to surveil the domestic Internet traffic of foreign countries through "boomerang routing".

The origins of the National Security Agency can be traced back to April 28, 1917, three weeks after the U.S. Congress declared war on Germany in World War I. A code and cipher decryption unit was established as the Cable and Telegraph Section which was also known as the Cipher Bureau. It was headquartered in Washington, D.C. and was part of the war effort under the executive branch without direct Congressional authorization. During the course of the war it was relocated in the army's organizational chart several times. On July 5, 1917, Herbert O. Yardley was assigned to head the unit. At that point, the unit consisted of Yardley and two civilian clerks. It absorbed the navy's Cryptanalysis functions in July 1918. World War I ended on November 11, 1918, and the army cryptographic section of Military Intelligence (MI-8) moved to New York City on May 20, 1919, where it continued intelligence activities as the Code Compilation Company under the direction of Yardley.

After the disbandment of the U.S. Army cryptographic section of military intelligence, known as MI-8, in 1919, the U.S. government created the Cipher Bureau, also known as Black Chamber. The Black Chamber was the United States' first peacetime cryptanalytic organization. Jointly funded by the Army and the State Department, the Cipher Bureau was disguised as a New York City commercial code company; it actually produced and sold such codes for business use. Its true mission, however, was to break the communications (chiefly diplomatic) of other nations. Its most notable known success was at the Washington Naval Conference, during which it aided American negotiators considerably by providing them with the decrypted traffic of many of the conference delegations, most notably the Japanese. The Black Chamber successfully persuaded Western Union, the largest U.S. telegram company at the time, as well as several other communications companies to illegally give the Black Chamber access to cable traffic of foreign embassies and consulates. Soon, these companies publicly discontinued their collaboration.

Despite the Chamber's initial successes, it was shut down in 1929 by U.S. Secretary of State Henry L. Stimson, who defended his decision by stating, "Gentlemen do not read each other's mail".

During World War II, the Signal Intelligence Service (SIS) was created to intercept and decipher the communications of the Axis powers. When the war ended, the SIS was reorganized as the Army Security Agency (ASA), and it was placed under the leadership of the Director of Military Intelligence.

On May 20, 1949, all cryptologic activities were centralized under a national organization called the Armed Forces Security Agency (AFSA). This organization was originally established within the U.S. Department of Defense under the command of the Joint Chiefs of Staff. The AFSA was tasked to direct Department of Defense communications and electronic intelligence activities, except those of U.S. military intelligence units. However, the AFSA was unable to centralize communications intelligence and failed to coordinate with civilian agencies that shared its interests such as the Department of State, Central Intelligence Agency (CIA) and the Federal Bureau of Investigation (FBI). In December 1951, President Harry S. Truman ordered a panel to investigate how AFSA had failed to achieve its goals. The results of the investigation led to improvements and its redesignation as the National Security Agency.

The National Security Council issued a memorandum of October 24, 1952, that revised National Security Council Intelligence Directive (NSCID) 9. On the same day, Truman issued a second memorandum that called for the establishment of the NSA. The actual establishment of the NSA was done by a November 4 memo by Robert A. Lovett, the Secretary of Defense, changing the name of the AFSA to the NSA, and making the new agency responsible for all communications intelligence. Since President Truman's memo was a classified document, the existence of the NSA was not known to the public at that time. Due to its ultra-secrecy the U.S. intelligence community referred to the NSA as "No Such Agency".

In the 1960s, the NSA played a key role in expanding U.S. commitment to the Vietnam War by providing evidence of a North Vietnamese attack on the American destroyer during the Gulf of Tonkin incident.

A secret operation, code-named "MINARET", was set up by the NSA to monitor the phone communications of Senators Frank Church and Howard Baker, as well as major civil rights leaders, including Martin Luther King, Jr., and prominent U.S. journalists and athletes who criticized the Vietnam War. However, the project turned out to be controversial, and an internal review by the NSA concluded that its Minaret program was "disreputable if not outright illegal".

The NSA mounted a major effort to secure tactical communications among U.S. forces during the war with mixed success. The NESTOR family of compatible secure voice systems it developed was widely deployed during the Vietnam War, with about 30,000 NESTOR sets produced. However a variety of technical and operational problems limited their use, allowing the North Vietnamese to exploit and intercept U.S. communications.
In the aftermath of the Watergate scandal, a congressional hearing in 1975 led by Senator Frank Church revealed that the NSA, in collaboration with Britain's SIGINT intelligence agency Government Communications Headquarters (GCHQ), had routinely intercepted the international communications of prominent anti-Vietnam war leaders such as Jane Fonda and Dr. Benjamin Spock. The Agency tracked these individuals in a secret filing system that was destroyed in 1974. Following the resignation of President Richard Nixon, there were several investigations of suspected misuse of FBI, CIA and NSA facilities. Senator Frank Church uncovered previously unknown activity, such as a CIA plot (ordered by the administration of President John F. Kennedy) to assassinate Fidel Castro. The investigation also uncovered NSA's wiretaps on targeted U.S. citizens.

After the Church Committee hearings, the Foreign Intelligence Surveillance Act of 1978 was passed into law. This was designed to limit the practice of mass surveillance in the United States.

In 1986, the NSA intercepted the communications of the Libyan government during the immediate aftermath of the Berlin discotheque bombing. The White House asserted that the NSA interception had provided "irrefutable" evidence that Libya was behind the bombing, which U.S. President Ronald Reagan cited as a justification for the 1986 United States bombing of Libya.

In 1999, a multi-year investigation by the European Parliament highlighted the NSA's role in economic espionage in a report entitled 'Development of Surveillance Technology and Risk of Abuse of Economic Information'. That year, the NSA founded the NSA Hall of Honor, a memorial at the National Cryptologic Museum in Fort Meade, Maryland. The memorial is a, "tribute to the pioneers and heroes who have made significant and long-lasting contributions to American cryptology". NSA employees must be retired for more than fifteen years to qualify for the memorial.

NSA's infrastructure deteriorated in the 1990s as defense budget cuts resulted in maintenance deferrals. On January 24, 2000, NSA headquarters suffered a total network outage for three days caused by an overloaded network. Incoming traffic was successfully stored on agency servers, but it could not be directed and processed. The agency carried out emergency repairs at a cost of $3 million to get the system running again. (Some incoming traffic was also directed instead to Britain's GCHQ for the time being.) Director Michael Hayden called the outage a "wake-up call" for the need to invest in the agency's infrastructure.

In the 1990s the defensive arm of the NSA—the Information Assurance Directorate (IAD)—started working more openly; the first public technical talk by an NSA scientist at a major cryptography conference was J. Solinas' presentation on efficient Elliptic Curve Cryptography algorithms at Crypto 1997. The IAD's cooperative approach to academia and industry culminated in its support for a transparent process for replacing the outdated Data Encryption Standard (DES) by an Advanced Encryption Standard (AES). Cybersecurity policy expert Susan Landau attributes the NSA's harmonious collaboration with industry and academia in the selection of the AES in 2000—and the Agency's support for the choice of a strong encryption algorithm designed by Europeans rather than by Americans—to Brian Snow, who was the Technical Director of IAD and represented the NSA as cochairman of the Technical Working Group for the AES competition, and Michael Jacobs, who headed IAD at the time.

After the terrorist attacks of September 11, 2001, the NSA believed that it had public support for a dramatic expansion of its surveillance activities. According to Neal Koblitz and Alfred Menezes, the period when the NSA was a trusted partner with academia and industry in the development of cryptographic standards started to come to an end when, as part of the change in the NSA in the post-September 11 era, Snow was replaced as Technical Director, Jacobs retired, and IAD could no longer effectively oppose proposed actions by the offensive arm of the NSA.

In the aftermath of the September 11 attacks, the NSA created new IT systems to deal with the flood of information from new technologies like the Internet and cellphones. ThinThread contained advanced data mining capabilities. It also had a "privacy mechanism"; surveillance was stored encrypted; decryption required a warrant. The research done under this program may have contributed to the technology used in later systems. ThinThread was cancelled when Michael Hayden chose Trailblazer, which did not include ThinThread's privacy system.

Trailblazer Project ramped up in 2002 and was worked on by Science Applications International Corporation (SAIC), Boeing, Computer Sciences Corporation, IBM, and Litton Industries. Some NSA whistleblowers complained internally about major problems surrounding Trailblazer. This led to investigations by Congress and the NSA and DoD Inspectors General. The project was cancelled in early 2004.

Turbulence started in 2005. It was developed in small, inexpensive "test" pieces, rather than one grand plan like Trailblazer. It also included offensive cyber-warfare capabilities, like injecting malware into remote computers. Congress criticized Turbulence in 2007 for having similar bureaucratic problems as Trailblazer. It was to be a realization of information processing at higher speeds in cyberspace.

The massive extent of the NSA's spying, both foreign and domestic, was revealed to the public in a series of detailed disclosures of internal NSA documents beginning in June 2013. Most of the disclosures were leaked by former NSA contractor Edward Snowden.

NSA's eavesdropping mission includes radio broadcasting, both from various organizations and individuals, the Internet, telephone calls, and other intercepted forms of communication. Its secure communications mission includes military, diplomatic, and all other sensitive, confidential or secret government communications.

According to a 2010 article in "The Washington Post", "[e]very day, collection systems at the National Security Agency intercept and store 1.7 billion e-mails, phone calls and other types of communications. The NSA sorts a fraction of those into 70 separate databases."

Because of its listening task, NSA/CSS has been heavily involved in cryptanalytic research, continuing the work of predecessor agencies which had broken many World War II codes and ciphers (see, for instance, Purple, Venona project, and JN-25).

In 2004, NSA Central Security Service and the National Cyber Security Division of the Department of Homeland Security (DHS) agreed to expand NSA Centers of Academic Excellence in Information Assurance Education Program.

As part of the National Security Presidential Directive 54/Homeland Security Presidential Directive 23 (NSPD 54), signed on January 8, 2008, by President Bush, the NSA became the lead agency to monitor and protect all of the federal government's computer networks from cyber-terrorism.

Operations by the National Security Agency can be divided in three types:

"Echelon" was created in the incubator of the Cold War. Today it is a legacy system, and several NSA stations are closing.

NSA/CSS, in combination with the equivalent agencies in the United Kingdom (Government Communications Headquarters), Canada (Communications Security Establishment), Australia (Australian Signals Directorate), and New Zealand (Government Communications Security Bureau), otherwise known as the UKUSA group, was reported to be in command of the operation of the so-called ECHELON system. Its capabilities were suspected to include the ability to monitor a large proportion of the world's transmitted civilian telephone, fax and data traffic.

During the early 1970s, the first of what became more than eight large satellite communications dishes were installed at Menwith Hill. Investigative journalist Duncan Campbell reported in 1988 on the "ECHELON" surveillance program, an extension of the UKUSA Agreement on global signals intelligence SIGINT, and detailed how the eavesdropping operations worked. On November 3, 1999 the BBC reported that they had confirmation from the Australian Government of the existence of a powerful "global spying network" code-named Echelon, that could "eavesdrop on every single phone call, fax or e-mail, anywhere on the planet" with Britain and the United States as the chief protagonists. They confirmed that Menwith Hill was "linked directly to the headquarters of the US National Security Agency (NSA) at Fort Meade in Maryland".

NSA's United States Signals Intelligence Directive 18 (USSID 18) strictly prohibited the interception or collection of information about "... U.S. persons, entities, corporations or organizations..." without explicit written legal permission from the United States Attorney General when the subject is located abroad, or the Foreign Intelligence Surveillance Court when within U.S. borders. Alleged Echelon-related activities, including its use for motives other than national security, including political and industrial espionage, received criticism from countries outside the UKUSA alliance.

The NSA was also involved in planning to blackmail people with "SEXINT", intelligence gained about a potential target's sexual activity and preferences. Those targeted had not committed any apparent crime nor were they charged with one.

In order to support its facial recognition program, the NSA is intercepting "millions of images per day".

The Real Time Regional Gateway is a data collection program introduced in 2005 in Iraq by NSA during the Iraq War that consisted of gathering all electronic communication, storing it, then searching and otherwise analyzing it. It was effective in providing information about Iraqi insurgents who had eluded less comprehensive techniques. This "collect it all" strategy introduced by NSA director, Keith B. Alexander, is believed by Glenn Greenwald of "The Guardian" to be the model for the comprehensive worldwide mass archiving of communications which NSA is engaged in as of 2013.

A dedicated unit of the NSA locates targets for the CIA for extrajudicial assassination in the Middle East. The NSA has also spied extensively on the European Union, the United Nations and numerous governments including allies and trading partners in Europe, South America and Asia.

In June 2015, WikiLeaks published documents showing that NSA spied on French companies.

In July 2015, WikiLeaks published documents showing that NSA spied on federal German ministries since the 1990s. Even Germany's Chancellor Angela Merkel's cellphones and phone of her predecessors had been intercepted.

Edward Snowden revealed in June 2013 that between February 8 and March 8, 2013, the NSA collected about 124.8 billion telephone data items and 97.1 billion computer data items throughout the world, as was displayed in charts from an internal NSA tool codenamed Boundless Informant. Initially, it was reported that some of these data reflected eavesdropping on citizens in countries like Germany, Spain and France, but later on, it became clear that those data were collected by European agencies during military missions abroad and were subsequently shared with NSA.

In 2013, reporters uncovered a secret memo that claims the NSA created and pushed for the adoption of the Dual EC DRBG encryption standard that contained built-in vulnerabilities in 2006 to the United States National Institute of Standards and Technology (NIST), and the International Organization for Standardization (aka ISO). This memo appears to give credence to previous speculation by cryptographers at Microsoft Research. Edward Snowden claims that the NSA often bypasses encryption altogether by lifting information before it is encrypted or after it is decrypted.

XKeyscore rules (as specified in a file xkeyscorerules100.txt, sourced by German TV stations NDR and WDR, who claim to have excerpts from its source code) reveal that the NSA tracks users of privacy-enhancing software tools, including Tor; an anonymous email service provided by the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) in Cambridge, Massachusetts; and readers of the "Linux Journal".

Linus Torvalds, the founder of Linux kernel, joked during a LinuxCon keynote on September 18, 2013, that the NSA, who are the founder of SELinux, wanted a backdoor in the kernel. However, later, Linus' father, a Member of the European Parliament (MEP), revealed that the NSA actually did this.

IBM Notes was the first widely adopted software product to use public key cryptography for client–server and server–server authentication and for encryption of data. Until US laws regulating encryption were changed in 2000, IBM and Lotus were prohibited from exporting versions of Notes that supported symmetric encryption keys that were longer than 40 bits. In 1997, Lotus negotiated an agreement with the NSA that allowed export of a version that supported stronger keys with 64 bits, but 24 of the bits were encrypted with a special key and included in the message to provide a "workload reduction factor" for the NSA. This strengthened the protection for users of Notes outside the US against private-sector industrial espionage, but not against spying by the US government.

While it is assumed that foreign transmissions terminating in the U.S. (such as a non-U.S. citizen accessing a U.S. website) subject non-U.S. citizens to NSA surveillance, recent research into boomerang routing has raised new concerns about the NSA's ability to surveil the domestic Internet traffic of foreign countries. Boomerang routing occurs when an Internet transmission that originates and terminates in a single country transits another. Research at the University of Toronto has suggested that approximately 25% of Canadian domestic traffic may be subject to NSA surveillance activities as a result of the boomerang routing of Canadian Internet service providers.

A document included in NSA files released with Glenn Greenwald's book "No Place to Hide" details how the agency's Tailored Access Operations (TAO) and other NSA units gain access to hardware. They intercept routers, servers and other network hardware being shipped to organizations targeted for surveillance and install covert implant firmware onto them before they are delivered. This was described by an NSA manager as "some of the most productive operations in TAO because they preposition access points into hard target networks around the world."

Computers seized by the NSA due to interdiction are often modified with a physical device known as Cottonmouth. Cottonmouth is a device that can be inserted in the USB port of a computer in order to establish remote access to the targeted machine. According to NSA's Tailored Access Operations (TAO) group implant catalog, after implanting Cottonmouth, the NSA can establish a network bridge "that allows the NSA to load exploit software onto modified computers as well as allowing the NSA to relay commands and data between hardware and software implants."

NSA's mission, as set forth in Executive Order 12333 in 1981, is to collect information that constitutes "foreign intelligence or counterintelligence" while "not" "acquiring information concerning the domestic activities of United States persons". NSA has declared that it relies on the FBI to collect information on foreign intelligence activities within the borders of the United States, while confining its own activities within the United States to the embassies and missions of foreign nations.

The appearance of a 'Domestic Surveillance Directorate' of the NSA was soon exposed as a hoax in 2013.

NSA's domestic surveillance activities are limited by the requirements imposed by the Fourth Amendment to the U.S. Constitution. The Foreign Intelligence Surveillance Court for example held in October 2011, citing multiple Supreme Court precedents, that the Fourth Amendment prohibitions against unreasonable searches and seizures applies to the contents of all communications, whatever the means, because "a person's private communications are akin to personal papers." However, these protections do not apply to non-U.S. persons located outside of U.S. borders, so the NSA's foreign surveillance efforts are subject to far fewer limitations under U.S. law. The specific requirements for domestic surveillance operations are contained in the Foreign Intelligence Surveillance Act of 1978 (FISA), which does not extend protection to non-U.S. citizens located outside of U.S. territory.

George W. Bush, president during the 9/11 terrorist attacks, approved the Patriot Act shortly after the attacks to take anti-terrorist security measures. Title 1, 2, and 9 specifically authorized measures that would be taken by the NSA. These titles granted enhanced domestic security against terrorism, surveillance procedures, and improved intelligence, respectively. On March 10, 2004, there was a debate between President Bush and White House Counsel Alberto Gonzales, Attorney General John Ashcroft, and Acting Attorney General James Comey. The Attorneys General were unsure if the NSA's programs could be considered constitutional. They threatened to resign over the matter, but ultimately the NSA's programs continued. On March 11, 2004, President Bush signed a new authorization for mass surveillance of Internet records, in addition to the surveillance of phone records. This allowed the president to be able to override laws such as the Foreign Intelligence Surveillance Act, which protected civilians from mass surveillance. In addition to this, President Bush also signed that the measures of mass surveillance were also retroactively in place.

Under the PRISM program, which started in 2007, NSA gathers Internet communications from foreign targets from nine major U.S. Internet-based communication service providers: Microsoft, Yahoo, Google, Facebook, PalTalk, AOL, Skype, YouTube and Apple. Data gathered include email, video and voice chat, videos, photos, VoIP chats such as Skype, and file transfers.

Former NSA director General Keith Alexander claimed that in September 2009 the NSA prevented Najibullah Zazi and his friends from carrying out a terrorist attack. However, this claim has been debunked and no evidence has been presented demonstrating that the NSA has ever been instrumental in preventing a terrorist attack.

Besides the more traditional ways of eavesdropping in order to collect signals intelligence, NSA is also engaged in hacking computers, smartphones and their networks. These operations are conducted by the Tailored Access Operations (TAO) division, which has been active since at least circa 1998.

According to the "Foreign Policy" magazine, "... the Office of Tailored Access Operations, or TAO, has successfully penetrated Chinese computer and telecommunications systems for almost 15 years, generating some of the best and most reliable intelligence information about what is going on inside the People's Republic of China."

In an interview with "Wired" magazine, Edward Snowden said the Tailored Access Operations division accidentally caused Syria's internet blackout in 2012.

The NSA is led by the Director of the National Security Agency (DIRNSA), who also serves as Chief of the Central Security Service (CHCSS) and Commander of the United States Cyber Command (USCYBERCOM) and is the highest-ranking military official of these organizations. He is assisted by a Deputy Director, who is the highest-ranking civilian within the NSA/CSS.

NSA also has an Inspector General, head of the Office of the Inspector General (OIG), a General Counsel, head of the Office of the General Counsel (OGC) and a Director of Compliance, who is head of the Office of the Director of Compliance (ODOC).

Unlike other intelligence organizations such as CIA or DIA, NSA has always been particularly reticent concerning its internal organizational structure.

As of the mid-1990s, the National Security Agency was organized into five Directorates:
Each of these directorates consisted of several groups or elements, designated by a letter. There were for example the A Group, which was responsible for all SIGINT operations against the Soviet Union and Eastern Europe, and G Group, which was responsible for SIGINT related to all non-communist countries. These groups were divided in units designated by an additional number, like unit A5 for breaking Soviet codes, and G6, being the office for the Middle East, North Africa, Cuba, Central and South America.

, NSA has about a dozen directorates, which are designated by a letter, although not all of them are publicly known. The directorates are divided in divisions and units starting with the letter of the parent directorate, followed by a number for the division, the sub-unit or a sub-sub-unit.

The main elements of the organizational structure of the NSA are:


In the year 2000, a leadership team was formed, consisting of the Director, the Deputy Director and the Directors of the Signals Intelligence (SID), the Information Assurance (IAD) and the Technical Directorate (TD). The chiefs of other main NSA divisions became associate directors of the senior leadership team.

After president George W. Bush initiated the President's Surveillance Program (PSP) in 2001, the NSA created a 24-hour Metadata Analysis Center (MAC), followed in 2004 by the Advanced Analysis Division (AAD), with the mission of analyzing content, Internet metadata and telephone metadata. Both units were part of the Signals Intelligence Directorate.

A 2016 proposal would combine the Signals Intelligence Directorate with Information Assurance Directorate into Directorate of Operations.

NSANet stands for National Security Agency Network and is the official NSA intranet. It is a classified network, for information up to the level of TS/SCI to support the use and sharing of intelligence data between NSA and the signals intelligence agencies of the four other nations of the Five Eyes partnership. The management of NSANet has been delegated to the Central Security Service Texas (CSSTEXAS).

NSANet is a highly secured computer network consisting of fiber-optic and satellite communication channels which are almost completely separated from the public Internet. The network allows NSA personnel and civilian and military intelligence analysts anywhere in the world to have access to the agency's systems and databases. This access is tightly controlled and monitored. For example, every keystroke is logged, activities are audited at random and downloading and printing of documents from NSANet are recorded.

In 1998, NSANet, along with NIPRNET and SIPRNET, had "significant problems with poor search capabilities, unorganized data and old information". In 2004, the network was reported to have used over twenty commercial off-the-shelf operating systems. Some universities that do highly sensitive research are allowed to connect to it.

The thousands of Top Secret internal NSA documents that were taken by Edward Snowden in 2013 were stored in "a file-sharing location on the NSA's intranet site"; so, they could easily be read online by NSA personnel. Everyone with a TS/SCI-clearance had access to these documents. As a system administrator, Snowden was responsible for moving accidentally misplaced highly sensitive documents to safer storage locations.

The NSA maintains at least two watch centers:


The number of NSA employees is officially classified but there are several sources providing estimates.
In 1961, NSA had 59,000 military and civilian employees, which grew to 93,067 in 1969, of which 19,300 worked at the headquarters at Fort Meade. In the early 1980s NSA had roughly 50,000 military and civilian personnel. By 1989 this number had grown again to 75,000, of which 25,000 worked at the NSA headquarters. Between 1990 and 1995 the NSA's budget and workforce were cut by one third, which led to a substantial loss of experience.

In 2012, the NSA said more than 30,000 employees worked at Fort Meade and other facilities. In 2012, John C. Inglis, the deputy director, said that the total number of NSA employees is "somewhere between 37,000 and one billion" as a joke, and stated that the agency is "probably the biggest employer of introverts." In 2013 "Der Spiegel" stated that the NSA had 40,000 employees. More widely, it has been described as the world's largest single employer of mathematicians. Some NSA employees form part of the workforce of the National Reconnaissance Office (NRO), the agency that provides the NSA with satellite signals intelligence.

As of 2013 about 1,000 system administrators work for the NSA.

The NSA received criticism early on in 1960 after two agents had defected to the Soviet Union. Investigations by the House Un-American Activities Committee and a special subcommittee of the United States House Committee on Armed Services revealed severe cases of ignorance in personnel security regulations, prompting the former personnel director and the director of security to step down and leading to the adoption of stricter security practices. Nonetheless, security breaches reoccurred only a year later when in an issue of "Izvestia" of July 23, 1963, a former NSA employee published several cryptologic secrets.

The very same day, an NSA clerk-messenger committed suicide as ongoing investigations disclosed that he had sold secret information to the Soviets on a regular basis. The reluctance of Congressional houses to look into these affairs had prompted a journalist to write, "If a similar series of tragic blunders occurred in any ordinary agency of Government an aroused public would insist that those responsible be officially censured, demoted, or fired." David Kahn criticized the NSA's tactics of concealing its doings as smug and the Congress' blind faith in the agency's right-doing as shortsighted, and pointed out the necessity of surveillance by the Congress to prevent abuse of power.

Edward Snowden's leaking of the existence of PRISM in 2013 caused the NSA to institute a "two-man rule", where two system administrators are required to be present when one accesses certain sensitive information. Snowden claims he suggested such a rule in 2009.

The NSA conducts polygraph tests of employees. For new employees, the tests are meant to discover enemy spies who are applying to the NSA and to uncover any information that could make an applicant pliant to coercion. As part of the latter, historically "EPQs" or "embarrassing personal questions" about sexual behavior had been included in the NSA polygraph. The NSA also conducts five-year periodic reinvestigation polygraphs of employees, focusing on counterintelligence programs. In addition the NSA conducts periodic polygraph investigations in order to find spies and leakers; those who refuse to take them may receive "termination of employment", according to a 1982 memorandum from the director of NSA.
There are also "special access examination" polygraphs for employees who wish to work in highly sensitive areas, and those polygraphs cover counterintelligence questions and some questions about behavior. NSA's brochure states that the average test length is between two and four hours. A 1983 report of the Office of Technology Assessment stated that "It appears that the NSA [National Security Agency] (and possibly CIA) use the polygraph not to determine deception or truthfulness per se, but as a technique of interrogation to encourage admissions." Sometimes applicants in the polygraph process confess to committing felonies such as murder, rape, and selling of illegal drugs. Between 1974 and 1979, of the 20,511 job applicants who took polygraph tests, 695 (3.4%) confessed to previous felony crimes; almost all of those crimes had been undetected.

In 2010 the NSA produced a video explaining its polygraph process. The video, ten minutes long, is titled "The Truth About the Polygraph" and was posted to the Web site of the Defense Security Service. Jeff Stein of "The Washington Post" said that the video portrays "various applicants, or actors playing them—it's not clear—describing everything bad they had heard about the test, the implication being that none of it is true." AntiPolygraph.org argues that the NSA-produced video omits some information about the polygraph process; it produced a video responding to the NSA video. George Maschke, the founder of the Web site, accused the NSA polygraph video of being "Orwellian".

After Edward Snowden revealed his identity in 2013, the NSA began requiring polygraphing of employees once per quarter.

The number of exemptions from legal requirements has been criticized. When in 1964 the Congress was hearing a bill giving the director of the NSA the power to fire at will any employee, "The Washington Post" wrote: "This is the very definition of arbitrariness. It means that an employee could be discharged and disgraced on the basis of anonymous allegations without the slightest opportunity to defend himself." Yet, the bill was accepted by an overwhelming majority. Also, every person hired to a job in the US after 2007, at any private organization, state or federal government agency, "must" be reported to the New Hire Registry, ostensibly to look for child support evaders, "except" that employees of an intelligence agency may be excluded from reporting if the director deems it necessary for national security reasons.

When the agency was first established, its headquarters and cryptographic center were in the Naval Security Station in Washington, D.C. The COMINT functions were located in Arlington Hall in Northern Virginia, which served as the headquarters of the U.S. Army's cryptographic operations. Because the Soviet Union had detonated a nuclear bomb and because the facilities were crowded, the federal government wanted to move several agencies, including the AFSA/NSA. A planning committee considered Fort Knox, but Fort Meade, Maryland, was ultimately chosen as NSA headquarters because it was far enough away from Washington, D.C. in case of a nuclear strike and was close enough so its employees would not have to move their families.

Construction of additional buildings began after the agency occupied buildings at Fort Meade in the late 1950s, which they soon outgrew. In 1963 the new headquarters building, nine stories tall, opened. NSA workers referred to the building as the "Headquarters Building" and since the NSA management occupied the top floor, workers used "Ninth Floor" to refer to their leaders. COMSEC remained in Washington, D.C., until its new building was completed in 1968. In September 1986, the Operations 2A and 2B buildings, both copper-shielded to prevent eavesdropping, opened with a dedication by President Ronald Reagan. The four NSA buildings became known as the "Big Four." The NSA director moved to 2B when it opened.

Headquarters for the National Security Agency is located at in Fort George G. Meade, Maryland, although it is separate from other compounds and agencies that are based within this same military installation. Fort Meade is about southwest of Baltimore, and northeast of Washington, D.C. The NSA has two dedicated exits off Baltimore–Washington Parkway. The Eastbound exit from the Parkway (heading toward Baltimore) is open to the public and provides employee access to its main campus and public access to the National Cryptology Museum. The Westbound side exit, (heading toward Washington) is labeled "NSA Employees Only". The exit may only be used by people with the proper clearances, and security vehicles parked along the road guard the entrance.

NSA is the largest employer in the state of Maryland, and two-thirds of its personnel work at Fort Meade. Built on of Fort Meade's , the site has 1,300 buildings and an estimated 18,000 parking spaces.

The main NSA headquarters and operations building is what James Bamford, author of "Body of Secrets", describes as "a modern boxy structure" that appears similar to "any stylish office building." The building is covered with one-way dark glass, which is lined with copper shielding in order to prevent espionage by trapping in signals and sounds. It contains , or more than , of floor space; Bamford said that the U.S. Capitol "could easily fit inside it four times over."

The facility has over 100 watchposts, one of them being the visitor control center, a two-story area that serves as the entrance. At the entrance, a white pentagonal structure, visitor badges are issued to visitors and security clearances of employees are checked. The visitor center includes a painting of the NSA seal.

The OPS2A building, the tallest building in the NSA complex and the location of much of the agency's operations directorate, is accessible from the visitor center. Bamford described it as a "dark glass Rubik's Cube". The facility's "red corridor" houses non-security operations such as concessions and the drug store. The name refers to the "red badge" which is worn by someone without a security clearance. The NSA headquarters includes a cafeteria, a credit union, ticket counters for airlines and entertainment, a barbershop, and a bank. NSA headquarters has its own post office, fire department, and police force.

The employees at the NSA headquarters reside in various places in the Baltimore-Washington area, including Annapolis, Baltimore, and Columbia in Maryland and the District of Columbia, including the Georgetown community. The NSA maintains a shuttle service from the Odenton station of MARC to its Visitor Control Center and has done so since 2005.

Following a major power outage in 2000, in 2003 and in follow-ups through 2007, "The Baltimore Sun" reported that the NSA was at risk of electrical overload because of insufficient internal electrical infrastructure at Fort Meade to support the amount of equipment being installed. This problem was apparently recognized in the 1990s but not made a priority, and "now the agency's ability to keep its operations going is threatened."

On August 6, 2006, "The Baltimore Sun" reported that the NSA had completely maxed out the grid, and that Baltimore Gas & Electric (BGE, now Constellation Energy) was unable to sell them any more power. NSA decided to move some of its operations to a new satellite facility.

BGE provided NSA with 65 to 75 megawatts at Fort Meade in 2007, and expected that an increase of 10 to 15 megawatts would be needed later that year. In 2011, the NSA was Maryland's largest consumer of power. In 2007, as BGE's largest customer, NSA bought as much electricity as Annapolis, the capital city of Maryland.

One estimate put the potential for power consumption by the new Utah Data Center at 40 million per year.

In 1995, "The Baltimore Sun" reported that the NSA is the owner of the single largest group of supercomputers.

NSA held a groundbreaking ceremony at Fort Meade in May 2013 for its High Performance Computing Center 2, expected to open in 2016. Called Site M, the center has a 150 megawatt power substation, 14 administrative buildings and 10 parking garages. It cost 3.2 billion and covers . The center is and initially uses 60 megawatts of electricity.

Increments II and III are expected to be completed by 2030, and would quadruple the space, covering with 60 buildings and 40 parking garages. Defense contractors are also establishing or expanding cybersecurity facilities near the NSA and around the Washington metropolitan area.

The DoD Computer Security Center was founded in 1981 and renamed the National Computer Security Center (NCSC) in 1985. NCSC was responsible for computer security throughout the federal government. NCSC was part of NSA, and during the late 1980s and the 1990s, NSA and NCSC published Trusted Computer System Evaluation Criteria in a six-foot high Rainbow Series of books that detailed trusted computing and network platform specifications. The Rainbow books were replaced by the Common Criteria, however, in the early 2000s.

As of 2012, NSA collected intelligence from four geostationary satellites. Satellite receivers were at Roaring Creek Station in Catawissa, Pennsylvania and Salt Creek Station in Arbuckle, California. It operated ten to twenty taps on U.S. telecom switches. NSA had installations in several U.S. states and from them observed intercepts from Europe, the Middle East, North Africa, Latin America, and Asia.

NSA had facilities at Friendship Annex (FANX) in Linthicum, Maryland, which is a 20 to 25-minute drive from Fort Meade; the Aerospace Data Facility at Buckley Air Force Base in Aurora outside Denver, Colorado; NSA Texas in the Texas Cryptology Center at Lackland Air Force Base in San Antonio, Texas; NSA Georgia at Fort Gordon in Augusta, Georgia; NSA Hawaii in Honolulu; the Multiprogram Research Facility in Oak Ridge, Tennessee, and elsewhere.

On January 6, 2011, a groundbreaking ceremony was held to begin construction on NSA's first Comprehensive National Cyber-security Initiative (CNCI) Data Center, known as the "Utah Data Center" for short. The $1.5B data center is being built at Camp Williams, Utah, located south of Salt Lake City, and will help support the agency's National Cyber-security Initiative. It is expected to be operational by September 2013. Construction of Utah Data Center finished in May 2019.

In 2009, to protect its assets and access more electricity, NSA sought to decentralize and expand its existing facilities in Fort Meade and Menwith Hill, the latter expansion expected to be completed by 2015.

The "Yakima Herald-Republic" cited Bamford, saying that many of NSA's bases for its Echelon program were a legacy system, using outdated, 1990s technology. In 2004, NSA closed its operations at Bad Aibling Station (Field Station 81) in Bad Aibling, Germany. In 2012, NSA began to move some of its operations at Yakima Research Station, Yakima Training Center, in Washington state to Colorado, planning to leave Yakima closed. As of 2013, NSA also intended to close operations at Sugar Grove, West Virginia.

Following the signing in 1946–1956 of the UKUSA Agreement between the United States, United Kingdom, Canada, Australia and New Zealand, who then cooperated on signals intelligence and ECHELON, NSA stations were built at GCHQ Bude in Morwenstow, United Kingdom; Geraldton, Pine Gap and Shoal Bay, Australia; Leitrim and Ottawa, Ontario, Canada; Misawa, Japan; and Waihopai and Tangimoana, New Zealand.

NSA operates RAF Menwith Hill in North Yorkshire, United Kingdom, which was, according to BBC News in 2007, the largest electronic monitoring station in the world. Planned in 1954, and opened in 1960, the base covered in 1999.

The agency's European Cryptologic Center (ECC), with 240 employees in 2011, is headquartered at a US military compound in Griesheim, near Frankfurt in Germany. A 2011 NSA report indicates that the ECC is responsible for the "largest analysis and productivity in Europe" and focuses on various priorities, including Africa, Europe, the Middle East and counterterrorism operations.

In 2013, a new Consolidated Intelligence Center, also to be used by NSA, is being built at the headquarters of the United States Army Europe in Wiesbaden, Germany. NSA's partnership with Bundesnachrichtendienst (BND), the German foreign intelligence service, was confirmed by BND president Gerhard Schindler.

Thailand is a "3rd party partner" of the NSA along with nine other nations. These are non-English-speaking countries that have made security agreements for the exchange of SIGINT raw material and end product reports.

Thailand is the site of at least two US SIGINT collection stations. One is at the US Embassy in Bangkok, a joint NSA-CIA Special Collection Service (SCS) unit. It presumably eavesdrops on foreign embassies, governmental communications, and other targets of opportunity.

The second installation is a FORNSAT (foreign satellite interception) station in the Thai city of Khon Kaen. It is codenamed INDRA, but has also been referred to as LEMONWOOD. The station is approximately in size and consists of a large 3,700–4,600 m (40,000–50,000 ft) operations building on the west side of the ops compound and four radome-enclosed parabolic antennas. Possibly two of the radome-enclosed antennas are used for SATCOM intercept and two antennas used for relaying the intercepted material back to NSA. There is also a PUSHER-type circularly-disposed antenna array (CDAA) just north of the ops compound.

NSA activated Khon Kaen in October 1979. Its mission was to eavesdrop on the radio traffic of Chinese army and air force units in southern China, especially in and around the city of Kunming in Yunnan Province. Back in the late 1970s the base consisted only of a small CDAA antenna array that was remote-controlled via satellite from the NSA listening post at Kunia, Hawaii, and a small force of civilian contractors from Bendix Field Engineering Corp. whose job it was to keep the antenna array and satellite relay facilities up and running 24/7.

According to the papers of the late General William Odom, the INDRA facility was upgraded in 1986 with a new British-made PUSHER CDAA antenna as part of an overall upgrade of NSA and Thai SIGINT facilities whose objective was to spy on the neighboring communist nations of Vietnam, Laos, and Cambodia.

The base apparently fell into disrepair in the 1990s as China and Vietnam became more friendly towards the US, and by 2002 archived satellite imagery showed that the PUSHER CDAA antenna had been torn down, perhaps indicating that the base had been closed. At some point in the period since 9/11, the Khon Kaen base was reactivated and expanded to include a sizeable SATCOM intercept mission. It is likely that the NSA presence at Khon Kaen is relatively small, and that most of the work is done by civilian contractors.
NSA has been involved in debates about public policy, both indirectly as a behind-the-scenes adviser to other departments, and directly during and after Vice Admiral Bobby Ray Inman's directorship. NSA was a major player in the debates of the 1990s regarding the export of cryptography in the United States. Restrictions on export were reduced but not eliminated in 1996.

Its secure government communications work has involved the NSA in numerous technology areas, including the design of specialized communications hardware and software, production of dedicated semiconductors (at the Ft. Meade chip fabrication plant), and advanced cryptography research. For 50 years, NSA designed and built most of its computer equipment in-house, but from the 1990s until about 2003 (when the U.S. Congress curtailed the practice), the agency contracted with the private sector in the fields of research and equipment.

NSA was embroiled in some minor controversy concerning its involvement in the creation of the Data Encryption Standard (DES), a standard and public block cipher algorithm used by the U.S. government and banking community. During the development of DES by IBM in the 1970s, NSA recommended changes to some details of the design. There was suspicion that these changes had weakened the algorithm sufficiently to enable the agency to eavesdrop if required, including speculation that a critical component—the so-called S-boxes—had been altered to insert a "backdoor" and that the reduction in key length might have made it feasible for NSA to discover DES keys using massive computing power. It has since been observed that the S-boxes in DES are particularly resilient against differential cryptanalysis, a technique which was not publicly discovered until the late 1980s but known to the IBM DES team.

The involvement of NSA in selecting a successor to Data Encryption Standard (DES), the Advanced Encryption Standard (AES), was limited to hardware performance testing (see AES competition). NSA has subsequently certified AES for protection of classified information when used in NSA-approved systems.

The NSA is responsible for the encryption-related components in these legacy systems:

The NSA oversees encryption in following systems which are in use today:

The NSA has specified Suite A and Suite B cryptographic algorithm suites to be used in U.S. government systems; the Suite B algorithms are a subset of those previously specified by NIST and are expected to serve for most information protection purposes, while the Suite A algorithms are secret and are intended for especially high levels of protection.

The widely used SHA-1 and SHA-2 hash functions were designed by NSA. SHA-1 is a slight modification of the weaker SHA-0 algorithm, also designed by NSA in 1993. This small modification was suggested by NSA two years later, with no justification other than the fact that it provides additional security. An attack for SHA-0 that does not apply to the revised algorithm was indeed found between 1998 and 2005 by academic cryptographers. Because of weaknesses and key length restrictions in SHA-1, NIST deprecates its use for digital signatures, and approves only the newer SHA-2 algorithms for such applications from 2013 on.

A new hash standard, SHA-3, has recently been selected through the competition concluded October 2, 2012 with the selection of Keccak as the algorithm. The process to select SHA-3 was similar to the one held in choosing the AES, but some doubts have been cast over it, since fundamental modifications have been made to Keccak in order to turn it into a standard. These changes potentially undermine the cryptanalysis performed during the competition and reduce the security levels of the algorithm.

NSA promoted the inclusion of a random number generator called Dual EC DRBG in the U.S. National Institute of Standards and Technology's 2007 guidelines. This led to speculation of a backdoor which would allow NSA access to data encrypted by systems using that pseudorandom number generator (PRNG).

This is now deemed to be plausible based on the fact that output of next iterations of PRNG can provably be determined if relation between two internal Elliptic Curve points is known. Both NIST and RSA are now officially recommending against the use of this PRNG.

Because of concerns that widespread use of strong cryptography would hamper government use of wiretaps, NSA proposed the concept of key escrow in 1993 and introduced the Clipper chip that would offer stronger protection than DES but would allow access to encrypted data by authorized law enforcement officials. The proposal was strongly opposed and key escrow requirements ultimately went nowhere. However, NSA's Fortezza hardware-based encryption cards, created for the Clipper project, are still used within government, and NSA ultimately declassified and published the design of the Skipjack cipher used on the cards.

Perfect Citizen is a program to perform vulnerability assessment by the NSA on U.S. critical infrastructure. It was originally reported to be a program to develop a system of sensors to detect cyber attacks on critical infrastructure computer networks in both the private and public sector through a network monitoring system named "Einstein". It is funded by the Comprehensive National Cybersecurity Initiative and thus far Raytheon has received a contract for up to $100 million for the initial stage.

NSA has invested many millions of dollars in academic research under grant code prefix "MDA904", resulting in over 3,000 papers NSA/CSS has, at times, attempted to restrict the publication of academic research into cryptography; for example, the Khufu and Khafre block ciphers were voluntarily withheld in response to an NSA request to do so. In response to a FOIA lawsuit, in 2013 the NSA released the 643-page research paper titled, "Untangling the Web: A Guide to Internet Research," written and compiled by NSA employees to assist other NSA workers in searching for information of interest to the agency on the public Internet.

NSA has the ability to file for a patent from the U.S. Patent and Trademark Office under gag order. Unlike normal patents, these are not revealed to the public and do not expire. However, if the Patent Office receives an application for an identical patent from a third party, they will reveal NSA's patent and officially grant it to NSA for the full term on that date.

One of NSA's published patents describes a method of geographically locating an individual computer site in an Internet-like network, based on the latency of multiple network connections. Although no public patent exists, NSA is reported to have used a similar locating technology called trilateralization that allows real-time tracking of an individual's location, including altitude from ground level, using data obtained from cellphone towers.

The heraldic insignia of NSA consists of an eagle inside a circle, grasping a key in its talons. The eagle represents the agency's national mission. Its breast features a shield with bands of red and white, taken from the Great Seal of the United States and representing Congress. The key is taken from the emblem of Saint Peter and represents security.

When the NSA was created, the agency had no emblem and used that of the Department of Defense. The agency adopted its first of two emblems in 1963. The current NSA insignia has been in use since 1965, when then-Director, LTG Marshall S. Carter (USA) ordered the creation of a device to represent the agency.

The NSA's flag consists of the agency's seal on a light blue background.
Crews associated with NSA missions have been involved in a number of dangerous and deadly situations. The USS "Liberty" incident in 1967 and USS "Pueblo" incident in 1968 are examples of the losses endured during the Cold War.

The National Security Agency/Central Security Service Cryptologic Memorial honors and remembers the fallen personnel, both military and civilian, of these intelligence missions. It is made of black granite, and has 171 names carved into it, It is located at NSA headquarters. A tradition of declassifying the stories of the fallen was begun in 2001.

In the United States, at least since 2001, there has been legal controversy over what signal intelligence can be used for and how much freedom the National Security Agency has to use signal intelligence. In 2015, the government made slight changes in how it uses and collects certain types of data, specifically phone records. The government was not analyzing the phone records as of early 2019.

On December 16, 2005, "The New York Times" reported that, under White House pressure and with an executive order from President George W. Bush, the National Security Agency, in an attempt to thwart terrorism, had been tapping phone calls made to persons outside the country, without obtaining warrants from the United States Foreign Intelligence Surveillance Court, a secret court created for that purpose under the Foreign Intelligence Surveillance Act (FISA).

One such surveillance program, authorized by the U.S. Signals Intelligence Directive 18 of President George Bush, was the Highlander Project undertaken for the National Security Agency by the U.S. Army 513th Military Intelligence Brigade. NSA relayed telephone (including cell phone) conversations obtained from ground, airborne, and satellite monitoring stations to various U.S. Army Signal Intelligence Officers, including the 201st Military Intelligence Battalion. Conversations of citizens of the U.S. were intercepted, along with those of other nations.

Proponents of the surveillance program claim that the President has executive authority to order such action, arguing that laws such as FISA are overridden by the President's Constitutional powers. In addition, some argued that FISA was implicitly overridden by a subsequent statute, the Authorization for Use of Military Force, although the Supreme Court's ruling in "Hamdan v. Rumsfeld" deprecates this view. In the August 2006 case "ACLU v. NSA", U.S. District Court Judge Anna Diggs Taylor concluded that NSA's warrantless surveillance program was both illegal and unconstitutional. On July 6, 2007, the 6th Circuit Court of Appeals vacated the decision on the grounds that the ACLU lacked standing to bring the suit.

On January 17, 2006, the Center for Constitutional Rights filed a lawsuit, CCR v. Bush, against the George W. Bush Presidency. The lawsuit challenged the National Security Agency's (NSA's) surveillance of people within the U.S., including the interception of CCR emails without securing a warrant first.

In September 2008, the Electronic Frontier Foundation (EFF) filed a class action lawsuit against the NSA and several high-ranking officials of the Bush administration, charging an "illegal and unconstitutional program of dragnet communications surveillance," based on documentation provided by former AT&T technician Mark Klein.

As a result of the USA Freedom Act passed by Congress in June 2015, the NSA had to shut down its bulk phone surveillance program on November 29 of the same year. The USA Freedom Act forbids the NSA to collect metadata and content of phone calls unless it has a warrant for terrorism investigation. In that case the agency has to ask the telecom companies for the record, which will only be kept for six months.

In May 2008, Mark Klein, a former AT&T employee, alleged that his company had cooperated with NSA in installing Narus hardware to replace the FBI Carnivore program, to monitor network communications including traffic between U.S. citizens.

NSA was reported in 2008 to use its computing capability to analyze "transactional" data that it regularly acquires from other government agencies, which gather it under their own jurisdictional authorities. As part of this effort, NSA now monitors huge volumes of records of domestic email data, web addresses from Internet searches, bank transfers, credit-card transactions, travel records, and telephone data, according to current and former intelligence officials interviewed by "The Wall Street Journal". The sender, recipient, and subject line of emails can be included, but the content of the messages or of phone calls are not.

A 2013 advisory group for the Obama administration, seeking to reform NSA spying programs following the revelations of documents released by Edward J. Snowden. mentioned in 'Recommendation 30' on page 37, "...that the National Security Council staff should manage an interagency process to review on a regular basis the activities of the US Government regarding attacks that exploit a previously unknown vulnerability in a computer application." Retired cyber security expert Richard A. Clarke was a group member and stated on April 11, 2014 that NSA had no advance knowledge of Heartbleed.

In August 2013 it was revealed that a 2005 IRS training document showed that NSA intelligence intercepts and wiretaps, both foreign and domestic, were being supplied to the Drug Enforcement Administration (DEA) and Internal Revenue Service (IRS) and were illegally used to launch criminal investigations of US citizens. Law enforcement agents were directed to conceal how the investigations began and recreate an apparently legal investigative trail by re-obtaining the same evidence by other means.

In the months leading to April 2009, the NSA intercepted the communications of U.S. citizens, including a Congressman, although the Justice Department believed that the interception was unintentional. The Justice Department then took action to correct the issues and bring the program into compliance with existing laws. United States Attorney General Eric Holder resumed the program according to his understanding of the Foreign Intelligence Surveillance Act amendment of 2008, without explaining what had occurred.

Polls conducted in June 2013 found divided results among Americans regarding NSA's secret data collection. Rasmussen Reports found that 59% of Americans disapprove, Gallup found that 53% disapprove, and Pew found that 56% are in favor of NSA data collection.

On April 25, 2013, the NSA obtained a court order requiring Verizon's Business Network Services to provide metadata on all calls in its system to the NSA "on an ongoing daily basis" for a three-month period, as reported by "The Guardian" on June 6, 2013. This information includes "the numbers of both parties on a call ... location data, call duration, unique identifiers, and the time and duration of all calls" but not "[t]he contents of the conversation itself". The order relies on the so-called "business records" provision of the Patriot Act.

In August 2013, following the Snowden leaks, new details about the NSA's data mining activity were revealed. Reportedly, the majority of emails into or out of the United States are captured at "selected communications links" and automatically analyzed for keywords or other "selectors". Emails that do not match are deleted.

The utility of such a massive metadata collection in preventing terrorist attacks is disputed. Many studies reveal the dragnet like system to be ineffective. One such report, released by the New America Foundation concluded that after an analysis of 225 terrorism cases, the NSA "had no discernible impact on preventing acts of terrorism."

Defenders of the program said that while metadata alone cannot provide all the information necessary to prevent an attack, it assures the ability to "connect the dots" between suspect foreign numbers and domestic numbers with a speed only the NSA's software is capable of. One benefit of this is quickly being able to determine the difference between suspicious activity and real threats. As an example, NSA director General Keith B. Alexander mentioned at the annual Cybersecurity Summit in 2013, that metadata analysis of domestic phone call records after the Boston Marathon bombing helped determine that rumors of a follow-up attack in New York were baseless.

In addition to doubts about its effectiveness, many people argue that the collection of metadata is an unconstitutional invasion of privacy. , the collection process remains legal and grounded in the ruling from Smith v. Maryland (1979). A prominent opponent of the data collection and its legality is U.S. District Judge Richard J. Leon, who issued a report in 2013 in which he stated: "I cannot imagine a more 'indiscriminate' and 'arbitrary invasion' than this systematic and high tech collection and retention of personal data on virtually every single citizen for purposes of querying and analyzing it without prior judicial approval...Surely, such a program infringes on 'that degree of privacy' that the founders enshrined in the Fourth Amendment".

As of May 7, 2015, the United States Court of Appeals for the Second Circuit ruled that the interpretation of Section 215 of the Patriot Act was wrong and that the NSA program that has been collecting Americans' phone records in bulk is illegal. It stated that Section 215 cannot be clearly interpreted to allow government to collect national phone data and, as a result, expired on June 1, 2015. This ruling "is the first time a higher-level court in the regular judicial system has reviewed the N.S.A. phone records program." The replacement law known as the USA Freedom Act, which will enable the NSA to continue to have bulk access to citizens' metadata but with the stipulation that the data will now be stored by the companies themselves. This change will not have any effect on other Agency procedures - outside of metadata collection - which have purportedly challenged Americans' Fourth Amendment rights;, including Upstream collection, a mass of techniques used by the Agency to collect and store American's data/communications directly from the Internet backbone.

Under the Upstream collection program, the NSA paid telecommunications companies hundreds of millions of dollars in order to collect data from them. While companies such as Google and Yahoo! claim that they do not provide "direct access" from their servers to the NSA unless under a court order, the NSA had access to emails, phone calls and cellular data users. Under this new ruling, telecommunications companies maintain bulk user metadata on their servers for at least 18 months, to be provided upon request to the NSA. This ruling made the mass storage of specific phone records at NSA datacenters illegal, but it did not rule on Section 215's constitutionality.

In a declassified document it was revealed that 17,835 phone lines were on an improperly permitted "alert list" from 2006 to 2009 in breach of compliance, which tagged these phone lines for daily monitoring. Eleven percent of these monitored phone lines met the agency's legal standard for "reasonably articulable suspicion" (RAS).

The NSA tracks the locations of hundreds of millions of cellphones per day, allowing it to map people's movements and relationships in detail. The NSA has been reported to have access to all communications made via Google, Microsoft, Facebook, Yahoo, YouTube, AOL, Skype, Apple and Paltalk, and collects hundreds of millions of contact lists from personal email and instant messaging accounts each year. It has also managed to weaken much of the encryption used on the Internet (by collaborating with, coercing or otherwise infiltrating numerous technology companies to leave "backdoors" into their systems), so that the majority of encryption is inadvertently vulnerable to different forms of attack.

Domestically, the NSA has been proven to collect and store metadata records of phone calls, including over 120 million US Verizon subscribers, as well as intercept vast amounts of communications via the internet (Upstream). The government's legal standing had been to rely on a secret interpretation of the Patriot Act whereby the entirety of US communications may be considered "relevant" to a terrorism investigation if it is expected that even a tiny minority may relate to terrorism. The NSA also supplies foreign intercepts to the DEA, IRS and other law enforcement agencies, who use these to initiate criminal investigations. Federal agents are then instructed to "recreate" the investigative trail via parallel construction.

The NSA also spies on influential Muslims to obtain information that could be used to discredit them, such as their use of pornography. The targets, both domestic and abroad, are not suspected of any crime but hold religious or political views deemed "radical" by the NSA.

According to a report in "The Washington Post" in July 2014, relying on information provided by Snowden, 90% of those placed under surveillance in the U.S. are ordinary Americans, and are not the intended targets. The newspaper said it had examined documents including emails, text messages, and online accounts that support the claim.

Despite White House claims that these programs have congressional oversight, many members of Congress were unaware of the existence of these NSA programs or the secret interpretation of the Patriot Act, and have consistently been denied access to basic information about them. The United States Foreign Intelligence Surveillance Court, the secret court charged with regulating the NSA's activities is, according to its chief judge, incapable of investigating or verifying how often the NSA breaks even its own secret rules. It has since been reported that the NSA violated its own rules on data access thousands of times a year, many of these violations involving large-scale data interceptions. NSA officers have even used data intercepts to spy on love interests; "most of the NSA violations were self-reported, and each instance resulted in administrative action of termination."

The NSA has "generally disregarded the special rules for disseminating United States person information" by illegally sharing its intercepts with other law enforcement agencies. A March 2009 FISA Court opinion, which the court released, states that protocols restricting data queries had been "so frequently and systemically violated that it can be fairly said that this critical element of the overall ... regime has never functioned effectively." In 2011 the same court noted that the "volume and nature" of the NSA's bulk foreign Internet intercepts was "fundamentally different from what the court had been led to believe". Email contact lists (including those of US citizens) are collected at numerous foreign locations to work around the illegality of doing so on US soil.

Legal opinions on the NSA's bulk collection program have differed. In mid-December 2013, U.S. District Judge Richard Leon ruled that the "almost-Orwellian" program likely violates the Constitution, and wrote, "I cannot imagine a more 'indiscriminate' and 'arbitrary invasion' than this systematic and high-tech collection and retention of personal data on virtually every single citizen for purposes of querying and analyzing it without prior judicial approval. Surely, such a program infringes on 'that degree of privacy' that the Founders enshrined in the Fourth Amendment. Indeed, I have little doubt that the author of our Constitution, James Madison, who cautioned us to beware 'the abridgement of freedom of the people by gradual and silent encroachments by those in power,' would be aghast."

Later that month, U.S. District Judge William Pauley ruled that the NSA's collection of telephone records is legal and valuable in the fight against terrorism. In his opinion, he wrote, "a bulk telephony metadata collection program [is] a wide net that could find and isolate gossamer contacts among suspected terrorists in an ocean of seemingly disconnected data" and noted that a similar collection of data prior to 9/11 might have prevented the attack.

At a March 2013 Senate Intelligence Committee hearing, Senator Ron Wyden asked Director of National Intelligence James Clapper, "does the NSA collect any type of data at all on millions or hundreds of millions of Americans?" Clapper replied "No, sir. ... Not wittingly. There are cases where they could inadvertently perhaps collect, but not wittingly." This statement came under scrutiny months later, in June 2013, details of the PRISM surveillance program were published, showing that "the NSA apparently can gain access to the servers of nine Internet companies for a wide range of digital data." Wyden said that Clapper had failed to give a "straight answer" in his testimony. Clapper, in response to criticism, said, "I responded in what I thought was the most truthful, or least untruthful manner." Clapper added, "There are honest differences on the semantics of what -- when someone says ‘collection’ to me, that has a specific meaning, which may have a different meaning to him."

NSA whistler-blower Edward Snowden additionally revealed the existence of XKeyscore, a top secret NSA program that allows the agency to search vast databases of "the metadata as well as the content of emails and other internet activity, such as browser history," with capability to search by "name, telephone number, IP address, keywords, the language in which the internet activity was conducted or the type of browser used." XKeyscore "provides the technological capability, if not the legal authority, to target even US persons for extensive electronic surveillance without a warrant provided that some identifying information, such as their email or IP address, is known to the analyst."

Regarding the necessity of these NSA programs, Alexander stated on June 27 2013 that the NSA's bulk phone and Internet intercepts had been instrumental in preventing 54 terrorist "events", including 13 in the US, and in all but one of these cases had provided the initial tip to "unravel the threat stream". On July 31 NSA Deputy Director John Inglis conceded to the Senate that these intercepts had not been vital in stopping any terrorist attacks, but were "close" to vital in identifying and convicting four San Diego men for sending US$8,930 to Al-Shabaab, a militia that conducts terrorism in Somalia.

The U.S. government has aggressively sought to dismiss and challenge Fourth Amendment cases raised against it, and has granted retroactive immunity to ISPs and telecoms participating in domestic surveillance.

The U.S. military has acknowledged blocking access to parts of "The Guardian" website for thousands of defense personnel across the country, and blocking the entire "Guardian" website for personnel stationed throughout Afghanistan, the Middle East, and South Asia.

An October 2014 United Nations report condemned mass surveillance by the United States and other countries as violating multiple international treaties and conventions that guarantee core privacy rights.

An exploit dubbed EternalBlue, which was claimed to have been created by the NSA by hacker group The Shadow Brokers and whistleblower Edward Snowden, was used in the unprecedented worldwide WannaCry ransomware attack in May 2017. The exploit had been leaked online by a hacking group, The Shadow Brokers, nearly a month prior to the attack. A number of experts have pointed the finger at the NSA's non-disclosure of the underlying vulnerability, and their loss of control over the EternalBlue attack tool that exploited it. Edward Snowden said that if the NSA had "privately disclosed the flaw used to attack hospitals when they found it, not when they lost it, [the attack] might not have happened". Wikipedia co-founder, Jimmy Wales, stated that he joined "with Microsoft and the other leaders of the industry in saying this is a huge screw-up by the government ... the moment the NSA found it, they should have notified Microsoft so they could quietly issue a patch and really chivvy people along, long before it became a huge problem."





</doc>
<doc id="21944" url="https://en.wikipedia.org/wiki?curid=21944" title="Nervous system">
Nervous system

In biology, the nervous system is a highly complex part of an animal that coordinates its actions and sensory information by transmitting signals to and from different parts of its body. The nervous system detects environmental changes that impact the body, then works in tandem with the endocrine system to respond to such events. Nervous tissue first arose in wormlike organisms about 550 to 600 million years ago. In vertebrates it consists of two main parts, the central nervous system (CNS) and the peripheral nervous system (PNS). The CNS consists of the brain and spinal cord. The PNS consists mainly of nerves, which are enclosed bundles of the long fibers or axons, that connect the CNS to every other part of the body. Nerves that transmit signals from the brain are called "motor" or "efferent" nerves, while those nerves that transmit information from the body to the CNS are called "sensory" or "afferent". Spinal nerves serve both functions and are called "mixed" nerves. The PNS is divided into three separate subsystems, the somatic, autonomic, and enteric nervous systems. Somatic nerves mediate voluntary movement. The autonomic nervous system is further subdivided into the sympathetic and the parasympathetic nervous systems. The sympathetic nervous system is activated in cases of emergencies to mobilize energy, while the parasympathetic nervous system is activated when organisms are in a relaxed state. The enteric nervous system functions to control the gastrointestinal system. Both autonomic and enteric nervous systems function involuntarily. Nerves that exit from the cranium are called cranial nerves while those exiting from the spinal cord are called spinal nerves.

At the cellular level, the nervous system is defined by the presence of a special type of cell, called the neuron, also known as a "nerve cell". Neurons have special structures that allow them to send signals rapidly and precisely to other cells. They send these signals in the form of electrochemical waves traveling along thin fibers called axons, which cause chemicals called neurotransmitters to be released at junctions called synapses. A cell that receives a synaptic signal from a neuron may be excited, inhibited, or otherwise modulated. The connections between neurons can form neural pathways, neural circuits, and larger networks that generate an organism's perception of the world and determine its behavior. Along with neurons, the nervous system contains other specialized cells called glial cells (or simply glia), which provide structural and metabolic support.

Nervous systems are found in most multicellular animals, but vary greatly in complexity. The only multicellular animals that have no nervous system at all are sponges, placozoans, and mesozoans, which have very simple body plans. The nervous systems of the radially symmetric organisms ctenophores (comb jellies) and cnidarians (which include anemones, hydras, corals and jellyfish) consist of a diffuse nerve net. All other animal species, with the exception of a few types of worm, have a nervous system containing a brain, a central cord (or two cords running in parallel), and nerves radiating from the brain and central cord. The size of the nervous system ranges from a few hundred cells in the simplest worms, to around 300 billion cells in African elephants.

The central nervous system functions to send signals from one cell to others, or from one part of the body to others and to receive feedback. Malfunction of the nervous system can occur as a result of genetic defects, physical damage due to trauma or toxicity, infection, or simply senesence. The medical specialty of neurology studies disorders of the nervous system and looks for interventions that can prevent or treat them. In the peripheral nervous system, the most common problem is the failure of nerve conduction, which can be due to different causes including diabetic neuropathy and demyelinating disorders such as multiple sclerosis and amyotrophic lateral sclerosis. Neuroscience is the field of science that focuses on the study of the nervous system.
The nervous system derives its name from nerves, which are cylindrical bundles of fibers (the axons of neurons), that emanate from the brain and spinal cord, and branch repeatedly to innervate every part of the body. Nerves are large enough to have been recognized by the ancient Egyptians, Greeks, and Romans, but their internal structure was not understood until it became possible to examine them using a microscope. The author Michael Nikoletseas wrote:
"It is difficult to believe that until approximately year 1900 it was not known that neurons are the basic units of the brain (Santiago Ramón y Cajal). Equally surprising is the fact that the concept of chemical transmission in the brain was not known until around 1930 (Henry Hallett Dale and Otto Loewi). We began to understand the basic electrical phenomenon that neurons use in order to communicate among themselves, the action potential, in the 1950s (Alan Lloyd Hodgkin, Andrew Huxley and John Eccles). It was in the 1960s that we became aware of how basic neuronal networks code stimuli and thus basic concepts are possible (David H. Hubel and Torsten Wiesel). The molecular revolution swept across US universities in the 1980s. It was in the 1990s that molecular mechanisms of behavioral phenomena became widely known (Eric Richard Kandel)." A microscopic examination shows that nerves consist primarily of axons, along with different membranes that wrap around them and segregate them into fascicles. The neurons that give rise to nerves do not lie entirely within the nerves themselves—their cell bodies reside within the brain, spinal cord, or peripheral ganglia.

All animals more advanced than sponges have nervous systems. However, even sponges, unicellular animals, and non-animals such as slime molds have cell-to-cell signalling mechanisms that are precursors to those of neurons. In radially symmetric animals such as the jellyfish and hydra, the nervous system consists of a nerve net, a diffuse network of isolated cells. In bilaterian animals, which make up the great majority of existing species, the nervous system has a common structure that originated early in the Ediacaran period, over 550 million years ago.

The nervous system contains two main categories or types of cells: neurons and glial cells.

The nervous system is defined by the presence of a special type of cell—the neuron (sometimes called "neurone" or "nerve cell"). Neurons can be distinguished from other cells in a number of ways, but their most fundamental property is that they communicate with other cells via synapses, which are membrane-to-membrane junctions containing molecular machinery that allows rapid transmission of signals, either electrical or chemical. Many types of neuron possess an axon, a protoplasmic protrusion that can extend to distant parts of the body and make thousands of synaptic contacts; axons typically extend throughout the body in bundles called nerves.

Even in the nervous system of a single species such as humans, hundreds of different types of neurons exist, with a wide variety of morphologies and functions. These include sensory neurons that transmute physical stimuli such as light and sound into neural signals, and motor neurons that transmute neural signals into activation of muscles or glands; however in many species the great majority of neurons participate in the formation of centralized structures (the brain and ganglia) and they receive all of their input from other neurons and send their output to other neurons.

Glial cells (named from the Greek for "glue") are non-neuronal cells that provide support and nutrition, maintain homeostasis, form myelin, and participate in signal transmission in the nervous system. In the human brain, it is estimated that the total number of glia roughly equals the number of neurons, although the proportions vary in different brain areas. Among the most important functions of glial cells are to support neurons and hold them in place; to supply nutrients to neurons; to insulate neurons electrically; to destroy pathogens and remove dead neurons; and to provide guidance cues directing the axons of neurons to their targets. A very important type of glial cell (oligodendrocytes in the central nervous system, and Schwann cells in the peripheral nervous system) generates layers of a fatty substance called myelin that wraps around axons and provides electrical insulation which allows them to transmit action potentials much more rapidly and efficiently. Recent findings indicate that glial cells, such as microglia and astrocytes, serve as important resident immune cells within the central nervous system.

The nervous system of vertebrates (including humans) is divided into the central nervous system (CNS) and the peripheral nervous system (PNS).

The (CNS) is the major division, and consists of the brain and the spinal cord. The spinal canal contains the spinal cord, while the cranial cavity contains the brain. The CNS is enclosed and protected by the meninges, a three-layered system of membranes, including a tough, leathery outer layer called the dura mater. The brain is also protected by the skull, and the spinal cord by the vertebrae.

The peripheral nervous system (PNS) is a collective term for the nervous system structures that do not lie within the CNS. The large majority of the axon bundles called nerves are considered to belong to the PNS, even when the cell bodies of the neurons to which they belong reside within the brain or spinal cord. The PNS is divided into somatic and visceral parts. The somatic part consists of the nerves that innervate the skin, joints, and muscles. The cell bodies of somatic sensory neurons lie in dorsal root ganglia of the spinal cord. The visceral part, also known as the autonomic nervous system, contains neurons that innervate the internal organs, blood vessels, and glands. The autonomic nervous system itself consists of two parts: the sympathetic nervous system and the parasympathetic nervous system. Some authors also include sensory neurons whose cell bodies lie in the periphery (for senses such as hearing) as part of the PNS; others, however, omit them.

The vertebrate nervous system can also be divided into areas called gray matter and white matter. Gray matter (which is only gray in preserved tissue, and is better described as pink or light brown in living tissue) contains a high proportion of cell bodies of neurons. White matter is composed mainly of myelinated axons, and takes its color from the myelin. White matter includes all of the nerves, and much of the interior of the brain and spinal cord. Gray matter is found in clusters of neurons in the brain and spinal cord, and in cortical layers that line their surfaces. There is an anatomical convention that a cluster of neurons in the brain or spinal cord is called a nucleus, whereas a cluster of neurons in the periphery is called a ganglion. There are, however, a few exceptions to this rule, notably including the part of the forebrain called the basal ganglia.

Sponges have no cells connected to each other by synaptic junctions, that is, no neurons, and therefore no nervous system. They do, however, have homologs of many genes that play key roles in synaptic function. Recent studies have shown that sponge cells express a group of proteins that cluster together to form a structure resembling a postsynaptic density (the signal-receiving part of a synapse). However, the function of this structure is currently unclear. Although sponge cells do not show synaptic transmission, they do communicate with each other via calcium waves and other impulses, which mediate some simple actions such as whole-body contraction.

Jellyfish, comb jellies, and related animals have diffuse nerve nets rather than a central nervous system. In most jellyfish the nerve net is spread more or less evenly across the body; in comb jellies it is concentrated near the mouth. The nerve nets consist of sensory neurons, which pick up chemical, tactile, and visual signals; motor neurons, which can activate contractions of the body wall; and intermediate neurons, which detect patterns of activity in the sensory neurons and, in response, send signals to groups of motor neurons. In some cases groups of intermediate neurons are clustered into discrete ganglia.

The development of the nervous system in radiata is relatively unstructured. Unlike bilaterians, radiata only have two primordial cell layers, endoderm and ectoderm. Neurons are generated from a special set of ectodermal precursor cells, which also serve as precursors for every other ectodermal cell type.

The vast majority of existing animals are bilaterians, meaning animals with left and right sides that are approximate mirror images of each other. All bilateria are thought to have descended from a common wormlike ancestor that appeared in the Ediacaran period, 550–600 million years ago. The fundamental bilaterian body form is a tube with a hollow gut cavity running from mouth to anus, and a nerve cord with an enlargement (a "ganglion") for each body segment, with an especially large ganglion at the front, called the "brain".

Even mammals, including humans, show the segmented bilaterian body plan at the level of the nervous system. The spinal cord contains a series of segmental ganglia, each giving rise to motor and sensory nerves that innervate a portion of the body surface and underlying musculature. On the limbs, the layout of the innervation pattern is complex, but on the trunk it gives rise to a series of narrow bands. The top three segments belong to the brain, giving rise to the forebrain, midbrain, and hindbrain.

Bilaterians can be divided, based on events that occur very early in embryonic development, into two groups (superphyla) called protostomes and deuterostomes. Deuterostomes include vertebrates as well as echinoderms, hemichordates (mainly acorn worms), and Xenoturbellidans. Protostomes, the more diverse group, include arthropods, molluscs, and numerous types of worms. There is a basic difference between the two groups in the placement of the nervous system within the body: protostomes possess a nerve cord on the ventral (usually bottom) side of the body, whereas in deuterostomes the nerve cord is on the dorsal (usually top) side. In fact, numerous aspects of the body are inverted between the two groups, including the expression patterns of several genes that show dorsal-to-ventral gradients. Most anatomists now consider that the bodies of protostomes and deuterostomes are "flipped over" with respect to each other, a hypothesis that was first proposed by Geoffroy Saint-Hilaire for insects in comparison to vertebrates. Thus insects, for example, have nerve cords that run along the ventral midline of the body, while all vertebrates have spinal cords that run along the dorsal midline.

Worms are the simplest bilaterian animals, and reveal the basic structure of the bilaterian nervous system in the most straightforward way. As an example, earthworms have dual nerve cords running along the length of the body and merging at the tail and the mouth. These nerve cords are connected by transverse nerves like the rungs of a ladder. These transverse nerves help coordinate the two sides of the animal. Two ganglia at the head (the "nerve ring") end function similar to a simple brain. Photoreceptors on the animal's eyespots provide sensory information on light and dark.

The nervous system of one very small roundworm, the nematode "Caenorhabditis elegans", has been completely mapped out in a connectome including its synapses. Every neuron and its cellular lineage has been recorded and most, if not all, of the neural connections are known. In this species, the nervous system is sexually dimorphic; the nervous systems of the two sexes, males and female hermaphrodites, have different numbers of neurons and groups of neurons that perform sex-specific functions. In "C. elegans", males have exactly 383 neurons, while hermaphrodites have exactly 302 neurons.

Arthropods, such as insects and crustaceans, have a nervous system made up of a series of ganglia, connected by a ventral nerve cord made up of two parallel connectives running along the length of the belly. Typically, each body segment has one ganglion on each side, though some ganglia are fused to form the brain and other large ganglia. The head segment contains the brain, also known as the supraesophageal ganglion. In the insect nervous system, the brain is anatomically divided into the protocerebrum, deutocerebrum, and tritocerebrum. Immediately behind the brain is the subesophageal ganglion, which is composed of three pairs of fused ganglia. It controls the mouthparts, the salivary glands and certain muscles. Many arthropods have well-developed sensory organs, including compound eyes for vision and antennae for olfaction and pheromone sensation. The sensory information from these organs is processed by the brain.

In insects, many neurons have cell bodies that are positioned at the edge of the brain and are electrically passive—the cell bodies serve only to provide metabolic support and do not participate in signalling. A protoplasmic fiber runs from the cell body and branches profusely, with some parts transmitting signals and other parts receiving signals. Thus, most parts of the insect brain have passive cell bodies arranged around the periphery, while the neural signal processing takes place in a tangle of protoplasmic fibers called neuropil, in the interior.

A neuron is called "identified" if it has properties that distinguish it from every other neuron in the same animal—properties such as location, neurotransmitter, gene expression pattern, and connectivity—and if every individual organism belonging to the same species has one and only one neuron with the same set of properties. In vertebrate nervous systems very few neurons are "identified" in this sense—in humans, there are believed to be none—but in simpler nervous systems, some or all neurons may be thus unique. In the roundworm "C. elegans", whose nervous system is the most thoroughly described of any animal's, every neuron in the body is uniquely identifiable, with the same location and the same connections in every individual worm. One notable consequence of this fact is that the form of the "C. elegans" nervous system is completely specified by the genome, with no experience-dependent plasticity.

The brains of many molluscs and insects also contain substantial numbers of identified neurons. In vertebrates, the best known identified neurons are the gigantic Mauthner cells of fish. Every fish has two Mauthner cells, in the bottom part of the brainstem, one on the left side and one on the right. Each Mauthner cell has an axon that crosses over, innervating neurons at the same brain level and then travelling down through the spinal cord, making numerous connections as it goes. The synapses generated by a Mauthner cell are so powerful that a single action potential gives rise to a major behavioral response: within milliseconds the fish curves its body into a C-shape, then straightens, thereby propelling itself rapidly forward. Functionally this is a fast escape response, triggered most easily by a strong sound wave or pressure wave impinging on the lateral line organ of the fish. Mauthner cells are not the only identified neurons in fish—there are about 20 more types, including pairs of "Mauthner cell analogs" in each spinal segmental nucleus. Although a Mauthner cell is capable of bringing about an escape response individually, in the context of ordinary behavior other types of cells usually contribute to shaping the amplitude and direction of the response.

Mauthner cells have been described as command neurons. A command neuron is a special type of identified neuron, defined as a neuron that is capable of driving a specific behavior individually. Such neurons appear most commonly in the fast escape systems of various species—the squid giant axon and squid giant synapse, used for pioneering experiments in neurophysiology because of their enormous size, both participate in the fast escape circuit of the squid. The concept of a command neuron has, however, become controversial, because of studies showing that some neurons that initially appeared to fit the description were really only capable of evoking a response in a limited set of circumstances.

At the most basic level, the function of the nervous system is to send signals from one cell to others, or from one part of the body to others. There are multiple ways that a cell can send signals to other cells. One is by releasing chemicals called hormones into the internal circulation, so that they can diffuse to distant sites. In contrast to this "broadcast" mode of signaling, the nervous system provides "point-to-point" signals—neurons project their axons to specific target areas and make synaptic connections with specific target cells. Thus, neural signaling is capable of a much higher level of specificity than hormonal signaling. It is also much faster: the fastest nerve signals travel at speeds that exceed 100 meters per second.

At a more integrative level, the primary function of the nervous system is to control the body. It does this by extracting information from the environment using sensory receptors, sending signals that encode this information into the central nervous system, processing the information to determine an appropriate response, and sending output signals to muscles or glands to activate the response. The evolution of a complex nervous system has made it possible for various animal species to have advanced perception abilities such as vision, complex social interactions, rapid coordination of organ systems, and integrated processing of concurrent signals. In humans, the sophistication of the nervous system makes it possible to have language, abstract representation of concepts, transmission of culture, and many other features of human society that would not exist without the human brain.

Most neurons send signals via their axons, although some types are capable of dendrite-to-dendrite communication. (In fact, the types of neurons called amacrine cells have no axons, and communicate only via their dendrites.) Neural signals propagate along an axon in the form of electrochemical waves called action potentials, which produce cell-to-cell signals at points where axon terminals make synaptic contact with other cells.

Synapses may be electrical or chemical. Electrical synapses make direct electrical connections between neurons, but chemical synapses are much more common, and much more diverse in function. At a chemical synapse, the cell that sends signals is called presynaptic, and the cell that receives signals is called postsynaptic. Both the presynaptic and postsynaptic areas are full of molecular machinery that carries out the signalling process. The presynaptic area contains large numbers of tiny spherical vessels called synaptic vesicles, packed with neurotransmitter chemicals. When the presynaptic terminal is electrically stimulated, an array of molecules embedded in the membrane are activated, and cause the contents of the vesicles to be released into the narrow space between the presynaptic and postsynaptic membranes, called the synaptic cleft. The neurotransmitter then binds to receptors embedded in the postsynaptic membrane, causing them to enter an activated state. Depending on the type of receptor, the resulting effect on the postsynaptic cell may be excitatory, inhibitory, or modulatory in more complex ways. For example, release of the neurotransmitter acetylcholine at a synaptic contact between a motor neuron and a muscle cell induces rapid contraction of the muscle cell. The entire synaptic transmission process takes only a fraction of a millisecond, although the effects on the postsynaptic cell may last much longer (even indefinitely, in cases where the synaptic signal leads to the formation of a memory trace).

There are literally hundreds of different types of synapses. In fact, there are over a hundred known neurotransmitters, and many of them have multiple types of receptors. Many synapses use more than one neurotransmitter—a common arrangement is for a synapse to use one fast-acting small-molecule neurotransmitter such as glutamate or GABA, along with one or more peptide neurotransmitters that play slower-acting modulatory roles. Molecular neuroscientists generally divide receptors into two broad groups: chemically gated ion channels and second messenger systems. When a chemically gated ion channel is activated, it forms a passage that allows specific types of ions to flow across the membrane. Depending on the type of ion, the effect on the target cell may be excitatory or inhibitory. When a second messenger system is activated, it starts a cascade of molecular interactions inside the target cell, which may ultimately produce a wide variety of complex effects, such as increasing or decreasing the sensitivity of the cell to stimuli, or even altering gene transcription.

According to a rule called Dale's principle, which has only a few known exceptions, a neuron releases the same neurotransmitters at all of its synapses. This does not mean, though, that a neuron exerts the same effect on all of its targets, because the effect of a synapse depends not on the neurotransmitter, but on the receptors that it activates. Because different targets can (and frequently do) use different types of receptors, it is possible for a neuron to have excitatory effects on one set of target cells, inhibitory effects on others, and complex modulatory effects on others still. Nevertheless, it happens that the two most widely used neurotransmitters, glutamate and GABA, each have largely consistent effects. Glutamate has several widely occurring types of receptors, but all of them are excitatory or modulatory. Similarly, GABA has several widely occurring receptor types, but all of them are inhibitory. Because of this consistency, glutamatergic cells are frequently referred to as "excitatory neurons", and GABAergic cells as "inhibitory neurons". Strictly speaking, this is an abuse of terminology—it is the receptors that are excitatory and inhibitory, not the neurons—but it is commonly seen even in scholarly publications.

One very important subset of synapses are capable of forming memory traces by means of long-lasting activity-dependent changes in synaptic strength. The best-known form of neural memory is a process called long-term potentiation (abbreviated LTP), which operates at synapses that use the neurotransmitter glutamate acting on a special type of receptor known as the NMDA receptor. The NMDA receptor has an "associative" property: if the two cells involved in the synapse are both activated at approximately the same time, a channel opens that permits calcium to flow into the target cell. The calcium entry initiates a second messenger cascade that ultimately leads to an increase in the number of glutamate receptors in the target cell, thereby increasing the effective strength of the synapse. This change in strength can last for weeks or longer. Since the discovery of LTP in 1973, many other types of synaptic memory traces have been found, involving increases or decreases in synaptic strength that are induced by varying conditions, and last for variable periods of time. The reward system, that reinforces desired behaviour for example, depends on a variant form of LTP that is conditioned on an extra input coming from a reward-signalling pathway that uses dopamine as neurotransmitter. All these forms of synaptic modifiability, taken collectively, give rise to neural plasticity, that is, to a capability for the nervous system to adapt itself to variations in the environment.

The basic neuronal function of sending signals to other cells includes a capability for neurons to exchange signals with each other. Networks formed by interconnected groups of neurons are capable of a wide variety of functions, including feature detection, pattern generation and timing, and there are seen to be countless types of information processing possible. Warren McCulloch and Walter Pitts showed in 1943 that even artificial neural networks formed from a greatly simplified mathematical abstraction of a neuron are capable of universal computation.
Historically, for many years the predominant view of the function of the nervous system was as a stimulus-response associator. In this conception, neural processing begins with stimuli that activate sensory neurons, producing signals that propagate through chains of connections in the spinal cord and brain, giving rise eventually to activation of motor neurons and thereby to muscle contraction, i.e., to overt responses. Descartes believed that all of the behaviors of animals, and most of the behaviors of humans, could be explained in terms of stimulus-response circuits, although he also believed that higher cognitive functions such as language were not capable of being explained mechanistically. Charles Sherrington, in his influential 1906 book "The Integrative Action of the Nervous System", developed the concept of stimulus-response mechanisms in much more detail, and Behaviorism, the school of thought that dominated Psychology through the middle of the 20th century, attempted to explain every aspect of human behavior in stimulus-response terms.

However, experimental studies of electrophysiology, beginning in the early 20th century and reaching high productivity by the 1940s, showed that the nervous system contains many mechanisms for maintaining cell excitability and generating patterns of activity intrinsically, without requiring an external stimulus. Neurons were found to be capable of producing regular sequences of action potentials, or sequences of bursts, even in complete isolation. When intrinsically active neurons are connected to each other in complex circuits, the possibilities for generating intricate temporal patterns become far more extensive. A modern conception views the function of the nervous system partly in terms of stimulus-response chains, and partly in terms of intrinsically generated activity patterns—both types of activity interact with each other to generate the full repertoire of behavior.

The simplest type of neural circuit is a reflex arc, which begins with a sensory input and ends with a motor output, passing through a sequence of neurons connected in series. This can be shown in the "withdrawal reflex" causing a hand to jerk back after a hot stove is touched. The circuit begins with sensory receptors in the skin that are activated by harmful levels of heat: a special type of molecular structure embedded in the membrane causes heat to change the electrical field across the membrane. If the change in electrical potential is large enough to pass the given threshold, it evokes an action potential, which is transmitted along the axon of the receptor cell, into the spinal cord. There the axon makes excitatory synaptic contacts with other cells, some of which project (send axonal output) to the same region of the spinal cord, others projecting into the brain. One target is a set of spinal interneurons that project to motor neurons controlling the arm muscles. The interneurons excite the motor neurons, and if the excitation is strong enough, some of the motor neurons generate action potentials, which travel down their axons to the point where they make excitatory synaptic contacts with muscle cells. The excitatory signals induce contraction of the muscle cells, which causes the joint angles in the arm to change, pulling the arm away.

In reality, this straightforward schema is subject to numerous complications. Although for the simplest reflexes there are short neural paths from sensory neuron to motor neuron, there are also other nearby neurons that participate in the circuit and modulate the response. Furthermore, there are projections from the brain to the spinal cord that are capable of enhancing or inhibiting the reflex.

Although the simplest reflexes may be mediated by circuits lying entirely within the spinal cord, more complex responses rely on signal processing in the brain. For example, when an object in the periphery of the visual field moves, and a person looks toward it many stages of signal processing are initiated. The initial sensory response, in the retina of the eye, and the final motor response, in the oculomotor nuclei of the brain stem, are not all that different from those in a simple reflex, but the intermediate stages are completely different. Instead of a one or two step chain of processing, the visual signals pass through perhaps a dozen stages of integration, involving the thalamus, cerebral cortex, basal ganglia, superior colliculus, cerebellum, and several brainstem nuclei. These areas perform signal-processing functions that include feature detection, perceptual analysis, memory recall, decision-making, and motor planning.

Feature detection is the ability to extract biologically relevant information from combinations of sensory signals. In the visual system, for example, sensory receptors in the retina of the eye are only individually capable of detecting "points of light" in the outside world. Second-level visual neurons receive input from groups of primary receptors, higher-level neurons receive input from groups of second-level neurons, and so on, forming a hierarchy of processing stages. At each stage, important information is extracted from the signal ensemble and unimportant information is discarded. By the end of the process, input signals representing "points of light" have been transformed into a neural representation of objects in the surrounding world and their properties. The most sophisticated sensory processing occurs inside the brain, but complex feature extraction also takes place in the spinal cord and in peripheral sensory organs such as the retina.

Although stimulus-response mechanisms are the easiest to understand, the nervous system is also capable of controlling the body in ways that do not require an external stimulus, by means of internally generated rhythms of activity. Because of the variety of voltage-sensitive ion channels that can be embedded in the membrane of a neuron, many types of neurons are capable, even in isolation, of generating rhythmic sequences of action potentials, or rhythmic alternations between high-rate bursting and quiescence. When neurons that are intrinsically rhythmic are connected to each other by excitatory or inhibitory synapses, the resulting networks are capable of a wide variety of dynamical behaviors, including attractor dynamics, periodicity, and even chaos. A network of neurons that uses its internal structure to generate temporally structured output, without requiring a corresponding temporally structured stimulus, is called a central pattern generator.

Internal pattern generation operates on a wide range of time scales, from milliseconds to hours or longer. One of the most important types of temporal pattern is circadian rhythmicity—that is, rhythmicity with a period of approximately 24 hours. All animals that have been studied show circadian fluctuations in neural activity, which control circadian alternations in behavior such as the sleep-wake cycle. Experimental studies dating from the 1990s have shown that circadian rhythms are generated by a "genetic clock" consisting of a special set of genes whose expression level rises and falls over the course of the day. Animals as diverse as insects and vertebrates share a similar genetic clock system. The circadian clock is influenced by light but continues to operate even when light levels are held constant and no other external time-of-day cues are available. The clock genes are expressed in many parts of the nervous system as well as many peripheral organs, but in mammals, all of these "tissue clocks" are kept in synchrony by signals that emanate from a master timekeeper in a tiny part of the brain called the suprachiasmatic nucleus.

A mirror neuron is a neuron that fires both when an animal acts and when the animal observes the same action performed by another. Thus, the neuron "mirrors" the behavior of the other, as though the observer were itself acting. Such neurons have been directly observed in primate species. Birds have been shown to have imitative resonance behaviors and neurological evidence suggests the presence of some form of mirroring system. In humans, brain activity consistent with that of mirror neurons has been found in the premotor cortex, the supplementary motor area, the primary somatosensory cortex and the inferior parietal cortex. The function of the mirror system is a subject of much speculation. Many researchers in cognitive neuroscience and cognitive psychology consider that this system provides the physiological mechanism for the perception/action coupling (see the common coding theory). They argue that mirror neurons may be important for understanding the actions of other people, and for learning new skills by imitation. Some researchers also speculate that mirror systems may simulate observed actions, and thus contribute to theory of mind skills, while others relate mirror neurons to language abilities. However, to date, no widely accepted neural or computational models have been put forward to describe how mirror neuron activity supports cognitive functions such as imitation. There are neuroscientists who caution that the claims being made for the role of mirror neurons are not supported by adequate research.

In vertebrates, landmarks of embryonic neural development include the birth and differentiation of neurons from stem cell precursors, the migration of immature neurons from their birthplaces in the embryo to their final positions, outgrowth of axons from neurons and guidance of the motile growth cone through the embryo towards postsynaptic partners, the generation of synapses between these axons and their postsynaptic partners, and finally the lifelong changes in synapses which are thought to underlie learning and memory.

All bilaterian animals at an early stage of development form a gastrula, which is polarized, with one end called the animal pole and the other the vegetal pole. The gastrula has the shape of a disk with three layers of cells, an inner layer called the endoderm, which gives rise to the lining of most internal organs, a middle layer called the mesoderm, which gives rise to the bones and muscles, and an outer layer called the ectoderm, which gives rise to the skin and nervous system.

In vertebrates, the first sign of the nervous system is the appearance of a thin strip of cells along the center of the back, called the neural plate. The inner portion of the neural plate (along the midline) is destined to become the central nervous system (CNS), the outer portion the peripheral nervous system (PNS). As development proceeds, a fold called the neural groove appears along the midline. This fold deepens, and then closes up at the top. At this point the future CNS appears as a cylindrical structure called the neural tube, whereas the future PNS appears as two strips of tissue called the neural crest, running lengthwise above the neural tube. The sequence of stages from neural plate to neural tube and neural crest is known as neurulation.

In the early 20th century, a set of famous experiments by Hans Spemann and Hilde Mangold showed that the formation of nervous tissue is "induced" by signals from a group of mesodermal cells called the "organizer region". For decades, though, the nature of neural induction defeated every attempt to figure it out, until finally it was resolved by genetic approaches in the 1990s. Induction of neural tissue requires inhibition of the gene for a so-called bone morphogenetic protein, or BMP. Specifically the protein BMP4 appears to be involved. Two proteins called Noggin and Chordin, both secreted by the mesoderm, are capable of inhibiting BMP4 and thereby inducing ectoderm to turn into neural tissue. It appears that a similar molecular mechanism is involved for widely disparate types of animals, including arthropods as well as vertebrates. In some animals, however, another type of molecule called Fibroblast Growth Factor or FGF may also play an important role in induction.

Induction of neural tissues causes formation of neural precursor cells, called neuroblasts. In drosophila, neuroblasts divide asymmetrically, so that one product is a "ganglion mother cell" (GMC), and the other is a neuroblast. A GMC divides once, to give rise to either a pair of neurons or a pair of glial cells. In all, a neuroblast is capable of generating an indefinite number of neurons or glia.

As shown in a 2008 study, one factor common to all bilateral organisms (including humans) is a family of secreted signaling molecules called neurotrophins which regulate the growth and survival of neurons. Zhu et al. identified DNT1, the first neurotrophin found in flies. DNT1 shares structural similarity with all known neurotrophins and is a key factor in the fate of neurons in Drosophila. Because neurotrophins have now been identified in both vertebrate and invertebrates, this evidence suggests that neurotrophins were present in an ancestor common to bilateral organisms and may represent a common mechanism for nervous system formation.

The central nervous system is protected by major physical and chemical barriers. Physically, the brain and spinal cord are surrounded by tough meningeal membranes, and enclosed in the bones of the skull and vertebral column, which combine to form a strong physical shield. Chemically, the brain and spinal cord are isolated by the blood–brain barrier, which prevents most types of chemicals from moving from the bloodstream into the interior of the CNS. These protections make the CNS less susceptible in many ways than the PNS; the flip side, however, is that damage to the CNS tends to have more serious consequences.

Although nerves tend to lie deep under the skin except in a few places such as the ulnar nerve near the elbow joint, they are still relatively exposed to physical damage, which can cause pain, loss of sensation, or loss of muscle control. Damage to nerves can also be caused by swelling or bruises at places where a nerve passes through a tight bony channel, as happens in carpal tunnel syndrome. If a nerve is completely transected, it will often regenerate, but for long nerves this process may take months to complete. In addition to physical damage, peripheral neuropathy may be caused by many other medical problems, including genetic conditions, metabolic conditions such as diabetes, inflammatory conditions such as Guillain–Barré syndrome, vitamin deficiency, infectious diseases such as leprosy or shingles, or poisoning by toxins such as heavy metals. Many cases have no cause that can be identified, and are referred to as idiopathic. It is also possible for nerves to lose function temporarily, resulting in numbness as stiffness—common causes include mechanical pressure, a drop in temperature, or chemical interactions with local anesthetic drugs such as lidocaine.

Physical damage to the spinal cord may result in loss of sensation or movement. If an injury to the spine produces nothing worse than swelling, the symptoms may be transient, but if nerve fibers in the spine are actually destroyed, the loss of function is usually permanent. Experimental studies have shown that spinal nerve fibers attempt to regrow in the same way as nerve fibers, but in the spinal cord, tissue destruction usually produces scar tissue that cannot be penetrated by the regrowing nerves.





</doc>
<doc id="21946" url="https://en.wikipedia.org/wiki?curid=21946" title="Nutcracker">
Nutcracker

A nutcracker is a tool designed to open nuts by cracking their shells. There are many designs, including levers, screws, and ratchets. A well-known type portrays a person whose mouth forms the jaws of the nutcracker, though many of these are meant for decoration.

Nuts were historically opened using a hammer and anvil, often made of stone. Some nuts such as walnuts can also be opened by hand, by holding the nut in the palm of the hand and applying pressure with the other palm or thumb, or using another nut.

Manufacturers produce modern functional nutcrackers usually somewhat resembling pliers, but with the pivot point at the end beyond the nut, rather than in the middle. These are also used for cracking the shells of crab and lobster to make the meat inside available for eating. Hinged lever nutcrackers, often called a "pair of nutcrackers", may date back to Ancient Greece. By the 14th century in Europe, nutcrackers were documented in England, including in the "Canterbury Tales", and in France. The lever design may derive from blacksmiths' pincers. Materials included metals such as silver, cast-iron and bronze, and wood including boxwood, especially those from France and Italy. More rarely, porcelain was used. Many of the wooden carved nutcrackers were in the form of people and animals.

During the Victorian era, fruit and nuts were presented at dinner and ornate and often silver-plated nutcrackers were produced to accompany them on the dinner table. Nuts have long been a popular choice for desserts, particularly throughout Europe. The nutcrackers were placed on dining tables to serve as a fun and entertaining center of conversation while diners awaited their final course. At one time, nutcrackers were actually made of metals such as brass, and it was not until the 1800s in Germany that the popularity of wooden ones began to spread.

The late 19th century saw two shifts in nutcracker production: the rise in figurative and decorative designs, particularly from the Alps where they were sold as souvenirs, and a switch to industrial manufacture, including availability in mail-order catalogues, rather than artisan production. After the 1960s, the availability of pre-shelled nuts led to a decline in ownership of nutcrackers and a fall in the tradition of nuts being put in children's Christmas stockings.

In the 17th century, screw nutcrackers were introduced that applied more gradual pressure to the shell, some like a vise. The spring-jointed nutcracker was patented by Henry Quackenbush in 1913. A ratchet design, similar to a car jack, that gradually increases pressure on the shell to avoid damaging the kernel inside is used by the Crackerjack, patented in 1947 by Cuthbert Leslie Rimes of Morley, Leeds and exhibited at the Festival of Britain. Unshelled nuts are still popular in China, where a key device is inserted into the crack in walnuts, pecans, and macadamias and twisted to open the shell.

Nutcrackers in the form of wood carvings of a soldier, knight, king, or other profession have existed since at least the 15th century. Figurative nutcrackers are a good luck symbol in Germany, and a folktale recounts that a puppet-maker won a nutcracking challenge by creating a doll with a mouth for a lever to crack the nuts. These nutcrackers portray a person with a large mouth which the operator opens by lifting a lever in the back of the figurine. Originally one could insert a nut in the big-toothed mouth, press down and thereby crack the nut. Modern nutcrackers in this style serve mostly for decoration, mainly at Christmas time, a season of which they have long been a traditional symbol. Pyotr Ilyich Tchaikovsky's ballet "The Nutcracker", based on a story by E. T. A. Hoffmann, derives its name from this festive holiday decoration.

The carving of nutcrackers— as well as of religious figures and of cribs— developed as a cottage industry in forested rural areas of Germany. The most famous nutcracker carvings come from Sonneberg in Thuringia (also a center of dollmaking) and as part of the industry of wooden toymaking in the Ore Mountains. Wood-carving usually provided the only income for the people living there. Today the travel industry supplements their income by bringing visitors to the remote areas. Carvings by famous names like Junghanel, Klaus Mertens, Karl, Olaf Kolbe, Petersen, Christian Ulbricht and especially the Steinbach nutcrackers have become collectors' items.

Decorative nutcrackers became popular in the United States after the Second World War, following the first US production of "The Nutcracker" ballet in 1940 and the exposure of US soldiers to the dolls during the war. In the United States, few of the decorative nutcrackers are now functional, though expensive working designs are still available. Many of the woodworkers in Germany were in Erzgebirge, in the Soviet zone after the end of the war, and they mass-produced poorly-made designs for the US market. With the increase in pre-shelled nuts, the need for functionality was also lessened. After the 1980s, Chinese and Taiwanese imports that copied the traditional German designs took over. The recreated "Bavarian village" of Leavenworth, Washington, features a nutcracker museum. Many other materials also serve to make decorated nutcrackers, such as porcelain, silver, and brass; the museum displays samples. The United States Postal Service (USPS) issued four stamps in October 2008 with custom-made nutcrackers made by Richmond, Virginia artist Glenn Crider.

Some artists, among them the multi-instrumentalist Mike Oldfield, have used the sound nutcrackers make in music.

Many animals shell nuts to eat them, including using tools. The Capuchin monkey is a fine example. Parrots use their beaks as natural nutcrackers, in much the same way smaller birds crack seeds. In this case, the pivot point stands opposite the nut, at the jaw.



</doc>
<doc id="21949" url="https://en.wikipedia.org/wiki?curid=21949" title="Nicolai Abildgaard">
Nicolai Abildgaard

Nicolai Abraham Abildgaard (September 11, 1743 – June 4, 1809) was a Danish neoclassical and royal history painter, sculptor, architect, and professor of painting, mythology, and anatomy at the New Royal Danish Academy of Art in Copenhagen, Denmark. Many of his works were in the royal Christiansborg Palace (some destroyed by fire 1794), Fredensborg Palace, and Levetzau Palace at Amalienborg.

Nicolai Abraham Abildgaard was born in Copenhagen, Denmark, as the son of Anne Margrethe (née Bastholm) and Søren Abildgaard, a noted antiquarian draughtsman.

Abildgaard was trained by a painting master before he joined the Royal Danish Academy of Art ("Det Kongelige Danske Kunstakademi") in Copenhagen, where he studied under the guidance of Johan Edvard Mandelberg and Johannes Wiedewelt. He won a series of medallions at the Academy for his brilliance from 1764 to 1767. The Large Gold Medallion from the Academy won in 1767 included a travel stipend, which he waited five years to receive. He assisted Professor Johan Mandelberg of the Academy as an apprentice around 1769 and for painting decorations for the royal palace at Fredensborg. These paintings are classical, influenced by French classical artists such as Claude Lorrain and Nicolas Poussin. Mandelberg had studied in Paris under François Boucher.

Although artists of that time usually journeyed to Paris for further studies, Abildgaard chose to travel to Rome, where he stayed from 1772 to 1777. He took a side trip to Naples in 1776 with Jens Juel. His ambitions focused in the genre of history painting. While in Rome, he studied Annibale Carracci's frescoes at the Palazzo Farnese and the paintings of Rafael, Titian, and Michelangelo. In addition he studied various other artistic disciplines (sculpture, architecture, decoration, wall paintings) and developed his knowledge of mythology, antiquities, anatomy, and perspective.

In the company of Swedish sculptor Johan Tobias Sergel and painter Johann Heinrich Füssli, he began to move away from the classicism he had learned at the Academy. He developed an appreciation for the literature of Shakespeare, Homer, and Ossian (the putative Gaelic poet). He worked with themes from Greek as well as Norse mythology, which placed him at the forefront of Nordic romanticism.

He left Rome in June 1777 with the hope of becoming professor at the Academy in Copenhagen. He stopped for a stay in Paris and arrived in Denmark in December of the same year.

In 1778, soon after joining the Academy, he was appointed to a professorship. He taught mythology and anatomy in addition to painting of the neoclassical style. Beyond his position at the Academy, he was very productive as an artist from 1777 to 1794. He produced not only monumental works, but also smaller pieces such as vignettes and illustrations. He designed Old Norse costumes. He illustrated the works of Socrates and Ossian. Additionally he did some sculpting, etching, and authoring. He was interested in all manners of mythological, biblical, and literary allusion.
He taught some famous painters, including Asmus Jacob Carstens, sculptor Bertel Thorvaldsen, and painters J. L. Lund and Christoffer Wilhelm Eckersberg. After his death, Lund and Eckersberg went on to become his successors as Academy professors. Eckersberg, referred to as the ""Father of Danish painting,"" went on to lay the foundation for the period of art known as the Golden Age of Danish Painting, as professor at the same Academy.
As royal historical painter, Abildgaard was commissioned around 1780 by the Danish government to paint large monumental pieces, a history of Denmark, to decorate the entirety of the Knights' Room ("Riddersal)" at Christiansborg Palace. It was a prestigious and lucrative assignment. The paintings combined historical depictions with allegorical and mythological elements that glorified and flattered the government. The door pieces depicted, in allegory, four historical periods in Europe's history. Abilgaard used pictorial allegory like ideograms, to communicate ideas and transmit messages through symbols to a refined public who was initiated into this form of symbology. Abildgaard's professor Johan Edvard Mandelberg supplied the decorations to the room.

He made a failed attempt to be elected to the post of Academy Director in 1787 and was unanimously elected to the post two years later, serving as director during the period 1789–1791. He had the reputation for being a tyrant and for taking as many of the academy's monumental assignments as possible for himself.

Abilgaard was also known as a religious freethinker and an advocate of political reform. In spite of his service to (and in his artwork the glorification of) the government, he was hardly a great supporter of the monarchy or of the state church. He supported the emancipation of the farmers and participated in the collection of monies for the Freedom Monument ("Frihedsstøtten") in 1792. He contributed a design for the monument, as well as for two of the reliefs at its base. He got caught into controversies at the end of the 18th century because of his controversial statements and satirical drawings. He was inspired by the French Revolution, and in 1789–1790 he tried to incorporate these revolutionary ideals into the Knights' Room at Christiansborg Palace. However, the King rejected his designs.
His showdowns with the establishment culminated in 1794, when his allegorical painting "Jupiter Weighs the Fate of Mankind" ("Jupiter vejer menneskenes skæbne") was exhibited at the Salon. He was politically isolated and cut out of the public debate by censors.

The fire at Christiansborg Palace, in February 1794, also had a dampening effect on his career, for seven of the ten monumental paintings of the grandiose project were destroyed in that accident. The project was stopped and so were his earnings.
However, after that devastating fire accident, he started getting decorative assignments and also got the opportunity to practice as an architect. He decorated the Levetzau Palace (now known as Christian VIII's Palace) at Amalienborg (1794–1798), recently occupied home of King Christian VII of Denmark's half-brother Frederik. His protégé Bertel Thorvaldsen headed the sculptural efforts. He also planned for rebuilding the Christiansborg Palace, but he could not get the assignment. 

At the start of the 19th century, his interest in painting was restored when he painted four scenes from Terence's comedy "Andria." 
In 1804 he received a commission for a series of painting for the throne room in the new palace, but disagreements between the artist and the crown prince put a halt to this project. He continued, however, to provide the court with designs for furniture and room decorations.

He was once again selected to serve as the Academy's director from 1801 until his death.

Abildgaard married Anna Maria Oxholm (1762-1822) in 1781 .
His second marriage in 1803 was to Juliane Marie Ottesen (1777-1848). He had two sons and a daughter from the marriage. 
He died at Frederiksdal in 1809. Nicolai Abraham Abildgaard is buried in Copenhagen's Assistens Cemetery.

Though Nicolai Abildgaard won immense fame in his own generation and helped lead the way to the period of art known as the Golden Age of Danish Painting, his works are scarcely known outside of Denmark. His style was classical, though with a romantic trend. According to the "Encyclopædia Britannica Eleventh Edition", "he was a cold theorist, inspired not by nature but by art. He had a keen sense of color. As a technical painter, he attained remarkable success, his tone being very harmonious and even, but the effect to a foreigner's eye is rarely interesting." A portrait of him painted by Jens Juel was made into a medallion by his friend Johan Tobias Sergel. August Vilhelm Saabye sculpted a statue of him in 1868, based on contemporary portraits.



 


</doc>
<doc id="21950" url="https://en.wikipedia.org/wiki?curid=21950" title="Khyber Pakhtunkhwa">
Khyber Pakhtunkhwa

Khyber Pakhtunkhwa (often abbreviated KP or KPK) ( ; ), formerly known as the North-West Frontier Province (NWFP) (), is one of the four provinces of Pakistan, located in the northwestern region of the country along the International border with Afghanistan.

It was previously known as the North-West Frontier Province until 2010 when the name was changed to Khyber Pakhtunkhwa by the 18th Amendment to Pakistan's Constitution and is known colloquially by various other names. Khyber Pakhtunkhwa is the third-largest province of Pakistan by the size of both population and economy, though it is geographically the smallest of four. Within Pakistan, Khyber Pakhtunkhwa shares a border with Punjab, Balochistan, Azad Kashmir, Gilgit-Baltistan and Islamabad. It is home to 17.9% of Pakistan's total population, with the majority of the province's inhabitants being Pashtuns and Hindko speakers.

The province is the site of the ancient kingdom Gandhara, including the ruins of its capital Pushkalavati near modern-day Charsadda. Once a stronghold of Buddhism, the history of the region was characterized by frequent invasions by various empires due to its geographical proximity to the Khyber Pass.

On 2 March 2017, the Government of Pakistan considered a proposal to merge the Federally Administered Tribal Areas (FATA) with Khyber Pakhtunkhwa, and to repeal the Frontier Crimes Regulations, which were applicable to the tribal areas at the time. However, some political parties opposed the merger, and called for the tribal areas to instead become a separate province of Pakistan. On 24 May 2018, the National Assembly of Pakistan voted in favour of an amendment to the Constitution of Pakistan to merge the Federally Administered Tribal Areas with Khyber Pakhtunkhwa province. The Khyber Pakhtunkhwa Assembly then approved the historic FATA-KP merger bill on 28 May 2018 making FATA officially part of Khyber Pakhtunkhwa, which was then signed by President Mamnoon Hussain, completing the process of this historic merger.

"Khyber Pakhtunkhwa" means the "Khyber side of the land of the Pashtuns, where the word "Pakhtunkhwa" means "Land of the Pashtuns", while according to some scholars, it refers to "Pashtun culture and society".

When the British established it as a province, they called it "North West Frontier Province" (abbreviated as NWFP) due to its relative location being in north west of their Indian Empire. After the creation of Pakistan, Pakistan continued with this name but a Pashtun nationalist party, Awami National Party demanded that the province name be changed to "Pakhtunkhwa". Their logic behind that demand was that Punjabi people, Sindhi people and Balochi people have their provinces named after their ethnicities but that is not the case for Pashtun people.

Pakistan Muslim League (N) was against that name since it was too similar to Bacha Khan's demand of a separate nation of Pashtunistan. PML-N wanted to name the province something other than which does not carry Pashtun identity in it as they argued that there were other minor ethnicities living in the province especially Hindkowans who spoke Hindko, thus the word "Khyber" was introduced with the name because it is the name of a major pass which connects Pakistan to Afghanistan.

During the times of Indus Valley Civilization (3300 BCE – 1300 BCE) the modern Khyber Pakhtunkhwa's Khyber Pass, through Hindu Kush provided a route to other neighboring regions and was used by merchants on trade excursions. From 1500 BCE, Indo-Aryan peoples started to enter in the region(of modern-day Iran, Pakistan, Afghanistan, North India) after having passed Khyber Pass.

The Gandharan civilization, which reached its zenith between the sixth and first centuries BCE, and which features prominently in the Hindu epic poem, the Mahabharatha, had one of its cores over the modern Khyber Pakhtunkhwa province.
Vedic texts refer to the area as the province of Pushkalavati. The area was once known to be a great center of learning.

At around 516 BCE., Darius Hystaspes sent Scylax, a Greek seaman from Karyanda, to explore the course of the Indus river. Darius Hystaspes subsequently subdued the races dwelling west of the Indus and north of Kabul. Gandhara was incorporated into the Persian Empire as one of its far easternmost satrapy system of government. The satrapy of Gandhara is recorded to have sent troops for Xerxes' invasion of Greece in 480 BCE.

In the spring of 327 BCE, Alexander the Great crossed the Indian Caucasus (Hindu Kush) and advanced to Nicaea, where Omphis, king of Taxila and other chiefs joined him. Alexander then dispatched part of his force through the valley of the Kabul River, while he himself advanced into modern Khyber Pakhtunkhwa's Bajaur and Swat regions with his troops. Having defeated the Aspasians, from whom he took 40,000 prisoners and 230,000 oxen, Alexander crossed the Gouraios (Panjkora River) and entered into the territory of the Assakenoi – also in modern-day Khyber Pakhtunkhwa. Alexander then made Embolima (thought to be the region of Amb in Khyber Pakhtunkhwa) his base. The ancient region of Peukelaotis (modern Hashtnagar, north-west of Peshawar) submitted to the Greek invasion, leading to Nicanor, a Macedonian, being appointed satrap of the country west of the Indus, which includes the modern Khyber Pakhtunkhwa province.

After Alexander's death in 323 BCE, Porus obtained possession of the region but was murdered by Eudemus in 317 BCE. Eudemus then left the region, and with his departure, Macedonian power collapsed. Sandrocottus (Chandragupta), the founder of the Mauryan dynasty, then declared himself master of the province. His grandson, Ashoka, made Buddhism the dominant religion in ancient Gandhara.
After Ashoka's death the Mauryan empire collapse, just as in the west the Seleucid power was rising. The Greek princes of neighboring Bactria (in modern Afghanistan) took advantage of the power vacuum to declare their independence. The Bactrian kingdoms were then attacked from the west by the Parthians and from the north (about 139 BCE) by the Sakas, a Central Asian tribe. Local Greek rulers still exercised a feeble and precarious power along the borderland, but the last vestige of Greek dominion was extinguished by the arrival of the Yueh-chi.

The Yueh-Chi were a race of nomads that were themselves forced southwards out of Central Asia by the nomadic Xiongnu people. The Kushan clan of the Yuek Chi seized vast swathes of territory under the rule of Kujula Kadphises. His successors, Vima Takto and Vima Kadphises, conquered the north-western portion of the Indian subcontinent. Vima Kadphises was then succeeded by his son, the legendary Buddhist king Kanishka, who himself was succeeded by Huvishka, and Vasudeva I.

After the Saffarids had left in Kabul, the Hindu Shahis had once again been placed into power. The restored Hindu Shahi kingdom was founded by the Brahmin minister Kallar in 843 CE. Kallar had moved the capital into Udabandhapura in modern-day Khyber Pakhtunkhwa from Kabul. Trade had flourished and many gems, textiles, perfumes, and other goods had been exported West. Coins minted by the Shahis have been found all over the Indian subcontinent. The Shahis had built Hindu temples with many idols, all of which were later looted by invaders. The ruins of these temples can be found at Nandana, Malot, Siv Ganga, and Ketas, as well as across the west bank of the Indus river.

At its height King Jayapala, the rule of the Shahi kingdom had extended to Kabul from the West, Bajaur to the North, Multan to the South, and the present day India-Pakistan border to the East. Jayapala saw a danger from the rise to power of the Ghaznavids and invaded their capital city of Ghazni both in the reign of Sebuktigin and in that of his son Mahmud. This had initiated the Muslim Ghaznavid and Hindu Shahi struggles. Sebuktigin, however, defeated him and forced Jayapala to pay an indemnity. Eventually, Jayapala refused payment and took to war once more. The Shahis were decisively defeated by Mahmud of Ghazni after the defeat of Jayapala at the Battle of Peshawar on 27 November 1001. Over time, Mahmud of Ghazni had pushed further into the subcontinent, as far as east as modern day Agra. During his campaigns, many Hindu temples and Buddhist monasteries had been looted and destroyed, as well as many people being converted to Islam.

Following the collapse of Ghaznavid rule, local Pashtuns of the Delhi Sultanate controlled the region.
Several Turkic and Pashtun dynasties ruled from Delhi, having shifted their capital from Lahore to Delhi. Several Muslim dynasties ruled modern Khyber Pakhtunkhwa during the Delhi Sultanate period: the Mamluk dynasty (1206–90), the Khalji dynasty (1290–1320), the Tughlaq dynasty (1320–1413), the Sayyid dynasty (1414–51), and the Lodi dynasty (1451–1526).

Tanoli tribe of Ghilji confederation from Ghazni Afghanistan came with Sabuktagin and settled in the mountainous area of Hazara called Tanawal (Amb).

Yusufzai Pashtun tribes from the Kabul and Jalalabad valleys began migrating to the Valley of Peshawar beginning in the 15th century, and displaced the Swatis of bhittani confederation ( a predominant Pashtun tribe of Hazara div ) and Dilazak Pashtun tribes across the Indus River to Hazara Division.

Mughal suzerainty over the Khyber Pakhtunkhwa region was partially established after Babar, the founder of the Mughal Empire, invaded the region in 1505 CE via the Khyber Pass. The Mughal Empire noted the importance of the region as a weak point in their empire's defenses, and determined to hold Peshawar and Kabul at all cost against any threats from the Uzbek "Shaybanids".

He was forced to retreat westwards to Kabul but returned to defeat the Lodis in July 1526, when he captured Peshawar from Daulat Khan Lodi, though the region was never considered to be fully subjugated to the Mughals.

Under the reign of Babar's son, Humayun, a direct Mughal rule was briefly challenged with the rise of the Pashtun Emperor, Sher Shah Suri, who began construction of the famous Grand Trunk Road – which links Kabul, Afghanistan with Chittagong, Bangladesh over 2000 miles to the east. Later, local rulers once again pledged loyalty to the Mughal emperor.

Yusufzai tribes rose against Mughals during the Yusufzai Revolt of 1667, and engaged in pitched-battles with Mughal battalions in Peshawar and Attock. Afridi tribes resisted Aurangzeb rule during the Afridi Revolt of the 1670s. The Afridis massacred a Mughal battalion in the Khyber Pass in 1672 and shut the pass to lucrative trade routes. Following another massacre in the winter of 1673, Mughal armies led by Emperor Aurangzeb himself regained control of the entire area in 1674, and enticed tribal leaders with various awards in order to end the rebellion.

Referred to as the "Father of Pashto Literature" and hailing from the city of Akora Khattak, the warrior-poet Khushal Khan Khattak actively participated in revolt against the Mughals and became renowned for his poems that celebrated the rebellious Pashtun warriors.

On 18 November 1738, Peshawar was captured from the Mughal governor Nawab Nasir Khan by the Afsharid armies during the Persian invasion of the Mughal Empire under Nader Shah.

The area fell subsequently under the rule of Ahmad Shah Durrani, founder of the Afghan Durrani Empire, following a grand nine-day long assembly of leaders, known as the "loya jirga". In 1749, the Mughal ruler was induced to cede Sindh, the Punjab region and the important trans Indus River to Ahmad Shah in order to save his capital from Afghan attack. In short order, the powerful army brought under its control the Tajik, Hazara, Uzbek, Turkmen, and other tribes of northern Afghanistan. Ahmad Shah invaded the remnants of the Mughal Empire a third time, and then a fourth, consolidating control over the Kashmir and Punjab regions, with Lahore being governed by Afghans. He sacked Delhi in 1757 but permitted the Mughal dynasty to remain in nominal control of the city as long as the ruler acknowledged Ahmad Shah's suzerainty over Punjab, Sindh, and Kashmir. Leaving his second son Timur Shah to safeguard his interests, Ahmad Shah left India to return to Afghanistan.

Their rule was interrupted by a brief invasion of the Hindu Marathas, ruled over the region following the 1758 Battle of Peshawar for eleven months till early 1759 when the Durrani rule was re-established.

Under the reign of Timur Shah, the Mughal practice of using Kabul as a summer capital and Peshawar as a winter capital was reintroduced, Peshawar's Bala Hissar Fort served as the residence of Durrani kings during their winter stay in Peshawar.

Mahmud Shah Durrani became king, and quickly sought to seize Peshawar from his half-brother, Shah Shujah Durrani. Shah Shujah was then himself proclaimed king in 1803, and recaptured Peshawar while Mahmud Shah was imprisoned at Bala Hissar fort until his eventual escape. In 1809, the British sent an emissary to the court of Shah Shujah in Peshawar, marking the first diplomatic meeting between the British and Afghans. Mahmud Shah allied himself with the "Barakzai" Pashtuns, and amassed an army in 1809, and captured Peshawar from his half-brother, Shah Shujah, establishing Mahmud Shah's second reign, which lasted under 1818.

Ranjit Singh invaded Peshawar in 1818 but soon lost it to the Afghans. Following the Sikh victory against Azim Khan, half-brother of Emir Dost Mohammad Khan, at the Battle of Nowshera in March 1823, Ranjit Singh captured the Peshawar Valley. An 1835 attempt by Dost Muhammad Khan to re-occupy Peshawar failed when his army declined to engage in combat with the Dal Khalsa. Dost Muhammad Khan's son, Mohammad Akbar Khan engaged with Sikh forces the Battle of Jamrud of 1837, and failed to recapture it.

During Sikh rule, an Italian named Paolo Avitabile was appointed an administrator of Peshawar, and is remembered for having unleashed a reign of fear there. The city's famous Mahabat Khan, built in 1630 in the Jeweler's Bazaar, was badly damaged and desecrated by the Sikhs, who also rebuilt the Bala Hissar fort during their occupation of Peshawar.

British East India Company defeated the Sikhs during the Second Anglo-Sikh War in 1849, and incorporated small parts of the region into the Province of Punjab. While Peshawar was the site of a small revolt against British during the Mutiny of 1857, local Pashtun tribes throughout the region generally remained neutral or supportive of the British as they detested the Sikhs, in contrast to other parts of British India which rose up in revolt against the British. However, British control of parts of the region was routinely challenged by Wazir tribesmen in Waziristan and other Pashtun tribes, who resisted any foreign occupation until Pakistan was created. By the late 19th century, the official boundaries of Khyber Pakhtunkhwa region still had not been defined as the region was still claimed by the Kingdom of Afghanistan. It was only in 1893 The British demarcated the boundary with Afghanistan under a treaty agreed to by the Afghan king, Abdur Rahman Khan, following the Second Anglo-Afghan War. In 1901, the North-West Frontier Province was formally created by the British administration on the British side of the Durand Line, although the princely states of Swat, Dir, Chitral, and Amb were allowed to maintain their autonomy under the terms of maintaining friendly ties with the British. As the British war effort during World War One demanded the reallocation of resources from British India to the European war fronts, some tribesmen from Afghanistan crossed the Durand Line in 1917 to attack British posts in an attempt to gain territory and weaken the legitimacy of the border. The validity of the Durand Line, however, was re-affirmed in 1919 by the Afghan government with the signing of the Treaty of Rawalpindi, which ended the Third Anglo-Afghan War – a war in which Waziri tribesmen allied themselves with the forces of Afghanistan's King Amanullah in their resistance to British rule. The Wazirs and other tribes, taking advantage of instability on the frontier, continued to resist British occupation until 1920 – even after Afghanistan had signed a peace treaty with the British.

British campaigns to subdue tribesmen along the Durand Line, as well as three Anglo-Afghan wars, made travel between Afghanistan and the densely populated heartlands of Khyber Pakhtunkhwa increasingly difficult. The two regions were largely isolated from one another from the start of the Second Anglo-Afghan War in 1878 until the start of World War II in 1939 when conflict along the Afghan frontier largely dissipated. Concurrently, the British continued their large public works projects in the region, and extended the Great Indian Peninsula Railway into the region, which connected the modern Khyber Pakhtunkhwa region to the plains of India to the east. Other projects, such as the Attock Bridge, Islamia College University, Khyber Railway, and establishment of cantonments in Peshawar, Kohat, Mardan, and Nowshera further cemented British rule in the region.
During this period, North-West Frontier Province was a "scene of repeated outrages on Hindus." During the independence period there was a Congress-led ministry in the province, which was led by secular Pashtun leaders, including Bacha Khan, who preferred joining India instead of Pakistan. The secular Pashtun leadership was also of the view that if joining India was not an option then they should espouse the cause of an independent ethnic Pashtun state rather than Pakistan. The secular stance of Bacha Khan had driven a wedge between the ulama of the otherwise pro-Congress (and pro-Indian unity) Jamiat Ulema Hind (JUH) and Bacha Khan's Khudai Khidmatgars. The directives of the ulama in the province began to take on communal tones. The ulama saw the Hindus in the province as a 'threat' to Muslims. Accusations of molesting Muslim women were levelled at Hindu shopkeepers in Nowshera, a town where anti-Hindu sermons were delivered by maulvis.

Tensions also rose in 1936 over the abduction of a Hindu girl in Bannu. British Indian court ruled against the marriage of a Hindu-converted Muslim girl at Bannu, after the girl's family filed a case of abduction and forced conversion. The ruling was based on the fact that the girl was a minor and was asked to make her decision of conversion and marriage after she reaches the age of majority, till then she was asked to live with a third party. The verdict 'enraged' the Muslims - especially the Pashtun tribesmen. The Dawar Maliks and mullahs left the Tochi far the Khaisora Valley to the south to rouse the Torikhel Wazir. The enraged tribesmen mustered two large lashkars 10,000 strong and battled the Bannu Brigade, with heavy casualties on both sides. Widespread lawlessness erupted as tribesmen blocked roads, overran outposts and ambushed convoys. The British retaliated by sending two columns converging in the Khaisora river valley. They suppressed the agitation by imposing fines and by destroying the houses of the ringleaders, including that of Haji Mirzali Khan (Faqir of Ipi). However, the pyrrhic nature of the victory and the subsequent withdrawal of the troops was credited by the Wazirs to be a manifestation of the power of Mirzali Khan. He succeeded in inducing a semblance of tribal unity, as the British noticed with dismay, among various sections of Tori Khel Wazirs, the Mahsud and the Bettani. He cemented his position as a religious leader by declaring a Jihad against the British. This move also helped rally support from Pashtun tribesmen across the border.

Such controversies stirred up anti-Hindu sentiments amongst the province's Muslim population. By 1947 the majority of the ulama in the province began supporting the Muslim League's idea of Pakistan.

In June 1947, Mirzali Khan (Faqir of Ipi), Bacha Khan, and other Khudai Khidmatgars declared the Bannu Resolution, demanding that the Pashtuns be given a choice to have an independent state of Pashtunistan composing all Pashtun majority territories of British India, instead of being made to join the new state of Pakistan. However, the British Raj refused to comply with the demand of this resolution, as their departure from the region required regions under their control to choose either to join India or Pakistan, with no third option.

By 1947 Pashtun nationalists were advocating for a united India, and no prominent voices advocated for a union with Afghanistan.

Immediately prior to the 1947 Partition of India, the British held a referendum in the NWFP to allow voters to choose between joining India or Pakistan. The polling began on 6 July 1947 and the referendum results were made public on 20 July 1947. According to the official results, there were 572,798 registered voters, out of which 289,244 (99.02%) votes were cast in favour of Pakistan, while 2,874 (0.98%) were cast in favour of India. The Muslim League declared the results as valid since over half of all eligible voters backed merger with Pakistan.

The then Chief Minister Dr. Khan Sahib, along with his brother Bacha Khan and the Khudai Khidmatgars, boycotted the referendum, citing that it did not have the options of the NWFP becoming independent or joining Afghanistan.

Their appeal for boycott had an effect, as according to an estimate, the total turnout for the referendum was 15% lower than the total turnout in the 1946 elections, although over half of all eligible voters backed merger with Pakistan.

Bacha Khan pledged allegiance to the new state of Pakistan in 1947, and thereafter abandoned his goals of an independent Pashtunistan and a united India in favour of supporting increased autonomy for the NWFP under Pakistani rule. He was subsequently arrested by Pakistan several times for his opposition to strong centralized rule. He later claimed that "Pashtunistan was never a reality". The idea of Pashtunistan never helped Pashtuns and it only caused suffering for them. He further claimed that the "successive governments of Afghanistan only exploited the idea for their own political goals".

After the creation of Pakistan in 1947, Afghanistan was the sole member of the United Nations to vote against Pakistan's accession to the UN because of Kabul's claim to the Pashtun territories on the Pakistani side of the Durand Line.
Afghanistan's Loya Jirga of 1949 declared the Durand Line invalid, which led to border tensions with Pakistan, and decades of mistrust between the two states. Afghan governments have also periodically refused to recognize Pakistan's inheritance of British treaties regarding the region. As had been agreed to by the Afghan government following the Second Anglo-Afghan War and after the treaty ending Third Anglo-Afghan War, no option was available to cede the territory to the Afghans, even though Afghanistan continued to claim the entire region as it was part of the Durrani Empire prior the conquest of the region by the Sikhs in 1818.

In 1950, Afghan-backed separatists in the Waziristan region declared the independence of Pashtunistan as an independent nation o dr the entirety of the NWFP. A Pashtun tribal jirga, held in Razmak, Waziristan, appointed Mirzali Khan as the President of the National Assembly for Pashtunistan. His popularity among the people of Waziristan declined over the years. He died a natural death in 1960 in Gurwek, Waziristan.

Growing participation of Pashtuns in the Pakistani government, however, resulted in the erosion of the support for the secessionist Pashtunistan movement by the end of the 1960s.

All the princely states within the boundaries of the NWFP were allowed to maintain certain autonomy following independence in 1947, but In 1969, the autonomous princely states of Swat, Dir, Chitral, and Amb were fully merged into the province.

For travelers, the area remained relatively peaceful in the 1960s and '70s. It was the usual route on the Hippie trail overland from Europe to India, with buses running from Kabul to Peshawar. While waiting to cross at the border visitors were however cautioned not to stray from the main road.

As a result of the Soviet invasion of Afghanistan in 1979, over five million Afghan refugees poured into Pakistan, mostly choosing to reside in the NWFP (, nearly 3 million remained). The North-West Frontier Province became a base for the Afghan resistance fighters and the Deobandi ulama of the province played a significant role in the Afghan 'jihad', with Madrasa Haqqaniyya becoming a prominent organisational and networking base for the anti-Soviet Afghan fighters. The province remained heavily influenced by events in Afghanistan thereafter. The 1989–1992 Civil war in Afghanistan following the withdrawal of Soviet forces led to the rise of the Afghan Taliban, which had emerged in the border region between Afghanistan, Balochistan, and FATA as a formidable political force.

In 2010, the province was renamed "Khyber Pakhtunkhwa." Protests arose among the local Hindkowan, Chitrali, Kohistani and Kalash populations over the name change, as they began to demand their own provinces. The Hindkowans, Kohistanis and Chitralis are last remains of ancient Gandhari people and they jointly protested for preservation of their culture. Seven people were killed and 100 injured in protests on 11April 2011.
The Awami National Party sought to rename the province "Pakhtunkhwa", which translates to "Land of Pashtuns" in the Pashto language. The name change was largely opposed by non-Pashtuns, and by political parties such as the Pakistan Muslim League-N, who draw much of their support from non-Pashtun regions of the province, and by the Islamist Muttahida Majlis-e-Amal coalition.

Khyber Pakhtunkhwa has been a site of militancy and terrorism that started after the attacks of 11 September 2001, and intensified when the Pakistani Taliban began an attempt to seize power in Pakistan starting in 2004. Armed conflict began in 2004, when tensions, rooted in the Pakistan Army's search for al-Qaeda fighters in Pakistan's mountainous Waziristan area (in the Federally Administered Tribal Areas), escalated into armed resistance. 
Fighting is ongoing between the Pakistani Army and armed militant groups such as the Tehrik-i-Taliban Pakistan (TTP), Jundallah, Lashkar-e-Islam (LeI), Tehreek-e-Nafaz-e-Shariat-e-Mohammadi (TNSM), al-Qaeda, and elements of organized crime have led to the deaths of over 50,000 Pakistanis since the country joined the U.S-led War on Terror, with Khyber Pakhtunkhwa being the site of most of the conflict.

Khyber Pakhtunkhwa is also the main theater for Pakistan's Zarb-e-Azb operation – a broad military campaign against militants located in the province, and neighboring FATA. By 2014, casualty rates in the country as a whole dropped by 40% as compared to 2011–2013, with even greater drops noted in Khyber Pakhtunkhwa, despite the province being the site of a large massacre of schoolchildren by terrorists in December 2014.

Khyber Pakhtunkhwa sits primarily on the Iranian plateau and comprises the junction where the slopes of the Hindu Kush mountains on the Eurasian plate give way to the Indus-watered hills approaching South Asia. This situation has led to seismic activity in the past. The famous Khyber Pass links the province to Afghanistan, while the Kohalla Bridge in Circle Bakote Abbottabad is a major crossing point over the Jhelum River in the east.

Geographically the province could be divided into two zones: the northern one extending from the ranges of the Hindu Kush to the borders of Peshawar basin and the southern one extending from Peshawar to the Derajat basin.

The northern zone is cold and snowy in winters with heavy rainfall and pleasant summers with the exception of Peshawar basin, which is hot in summer and cold in winter. It has moderate rainfall.

The southern zone is arid with hot summers and relatively cold winters and scanty rainfall. The Sheikh Badin Hills, a spur of clay and sandstone hills that stretch east from the Sulaiman Mountains to the Indus River, separates Dera Ismail Khan District from the "Marwat" plains of the Lakki Marwat. The highest peak in the range is the limestone Sheikh Badin Mountain, which is protected by the Sheikh Badin National Park. Near the Indus River, terminus of the Sheikh Badin Hills is a spur of limestone hills known as the "Kafir Kot" hills, where the ancient Hindu complex of Kafir Kot is located.

The major rivers that criss-cross the province are the Kabul, Swat, Chitral, Kunar, Siran, Panjkora, Bara, Kurram, Dor, Haroo, Gomal and Zhob.

Its snow-capped peaks and lush green valleys of unusual beauty have enormous potential for tourism.

The climate of Khyber Pakhtunkhwa varies immensely for a region of its size, encompassing most of the many climate types found in Pakistan. The province stretching southwards from the Baroghil Pass in the Hindu Kush covers almost six degrees of latitude; it is mainly a mountainous region. Dera Ismail Khan is one of the hottest places in South Asia while in the mountains to the north the weather is mild in the summer and intensely cold in the winter. The air is generally very dry; consequently, the daily and annual range of temperature is quite large.

Rainfall also varies widely. Although large parts of Khyber Pakhtunkhwa are typically dry, the province also contains the wettest parts of Pakistan in its eastern fringe specially in monsoon season from mid June to mid September.

Chitral District lies completely sheltered from the monsoon that controls the weather in eastern Pakistan, owing to its relatively westerly location and the shielding effect of the Nanga Parbat massif. In many ways, Chitral District has more in common regarding climate with Central Asia than South Asia. The winters are generally cold even in the valleys, and heavy snow during the winter blocks passes and isolates the region. In the valleys, however, summers can be hotter than on the windward side of the mountains due to lower cloud cover: Chitral can reach frequently during this period. However, the humidity is extremely low during these hot spells and, as a result the summer climate is less torrid than in the rest of the Indian subcontinent.

Most precipitation falls as thunderstorms or snow during winter and spring, so that the climate at the lowest elevations is classed as Mediterranean ("Csa"), continental Mediterranean ("Dsa") or semi-arid ("BSk"). Summers are extremely dry in the north of Chitral district and receive only a little rain in the south around Drosh.

At elevations above , as much as a third of the snow which feeds the large Karakoram and Hindukush glaciers comes from the monsoon since these elevations are too high to be shielded from its moisture.

On the southern flanks of Nanga Parbat and in Upper and Lower Dir Districts, rainfall is much heavier than further north because moist winds from the Arabian Sea are able to penetrate the region. When they collide with the mountain slopes, winter depressions provide heavy precipitation. The monsoon, although short, is generally powerful. As a result, the southern slopes of Khyber Pakhtunkhwa are the wettest part of Pakistan. Annual rainfall ranges from around in the most sheltered areas to as much as in parts of Abbottabad and Mansehra Districts.

This region's climate is classed at lower elevations as humid subtropical ("Cfa" in the west; "Cwa" in the east); whilst at higher elevations with a southerly aspect, it becomes classed as humid continental ("Dfb"). However, accurate data for altitudes above are practically nonexistent here, in Chitral, or in the south of the province.

The seasonality of rainfall in central Khyber Pakhtunkhwa shows very marked gradients from east to west. At Dir, March remains the wettest month due to frequent frontal cloud-bands, whereas in Hazara more than half the rainfall comes from the monsoon. This creates a unique situation characterized by a bimodal rainfall regime, which extends into the southern part of the province described below.

Since cold air from the Siberian High loses its chilling capacity upon crossing the vast Karakoram and Himalaya ranges, winters in central Khyber Pakhtunkhwa are somewhat milder than in Chitral. Snow remains very frequent at high altitudes but rarely lasts long on the ground in the major towns and agricultural valleys. Outside of winter, temperatures in central Khyber Pakhtunkhwa are not so hot as in Chitral. 

Significantly higher humidity when the monsoon is active means that heat discomfort can be greater. However, even during the most humid periods the high altitudes typically allow for some relief from the heat overnight.

As one moves further away from the foothills of the Himalaya and Karakoram ranges, the climate changes from the humid subtropical climate of the foothills to the typically arid climate of Sindh, Balochistan and southern Punjab. As in central Khyber Pakhtunkhwa, the seasonality of precipitation shows a very sharp gradient from west to east, but the whole region very rarely receives significant monsoon rainfall. Even at high elevations, annual rainfall is less than and in some places as little as .

Temperatures in southern Khyber Pakhtunkhwa are extremely hot: Dera Ismail Khan in the southernmost district of the province is known as one of the hottest places in the world with temperatures known to have reached . In the cooler months, nights can be cold and frosts remain frequent; snow is very rare, and daytime temperatures remain comfortably warm with abundant sunshine.

There are about 29 National Parks in Pakistan and about 18 in Khyber Pakhtunkhwa.

The province of Khyber Pakhtunkhwa had a population of 35.53 million at the time of the 2017 Census of Pakistan. The largest ethnic group is the Pashtun, who historically have been living in the areas for centuries. Around 1.5 million Afghan refugees also remain in the province, the majority of whom are Pashtuns followed by Tajiks, Hazaras, Gujjar and other smaller groups. Despite having lived in the province for over two decades, they are registered as citizens of Afghanistan.

The Pashtuns of Khyber Pakhtunkhwa observe tribal code of conduct called "Pakhtunwali" which has four high value components called "nang" (honor), "badal" (revenge), "melmastiya" (hospitality) and "nanawata" (rights to refuge).

Urdu, being the national and official language, serves as a lingua franca for inter-ethnic communications, and sometime Pashto and Urdu are the second and third languages among communities which speak other ethnic languages.

The most widely spoken language is Pashto, native to % of the population. Other languages with significant numbers of speakers include Hindko (%), Saraiki (%), Khowar and Kohistani. In 2011 the provincial government approved in principle the introduction of these five regional languages as compulsory subjects for schools in the areas where they are spoken.

The majority of the residents of the Khyber Pakhtunkhwa overwhelmingly follows and professes the Sunni principles of Islam while the small followers of Shia principles of Islam are found among the Isma'ilis in the Chitral district. The tribe of Kalasha in southern Chitral still retain an ancient form of Hinduism mixed with Animism. There are very small numbers of residents who are the adherents of Roman Catholicism denomination of Christianity, Hinduism and Sikhism.


The Provincial Assembly is a unicameral legislature, which consists of 145 members elected to serve for a constitutionally bounded term of five years. Historically, the province perceived to be a stronghold of the Awami National Party (ANP); a pro-Russian, by procommunist, left-wing and nationalist party. Since the 1970s, the Pakistan Peoples Party (PPP) also enjoyed considerable support in the province due to its socialist agenda. Khyber Pakhtunkhwa was thought to be another leftist region of the country after Sindh.

After the nationwide general elections held in 2002, a plurality voting swing in the province elected one of Pakistan's only religiously-based provincial governments led by the ultra-conservative Muttahida Majlis-e-Amal (MMA) during the administration of President Pervez Musharraf. The American involvement in neighboring Afghanistan contributed towards the electoral victory of the Islamic coalition led by Jamaat-e-Islami Pakistan (JeI) whose social policies made the province a ground-swell of anti-Americanism. The electoral victory of MMA was also in context of guided democracy in the Musharraff administration that barred the mainstream political parties, the leftist Pakistan Peoples Party and the centre-right Pakistan Muslim League (N) (PML(N)), whose chairmen and presidents having been barred from participation in the elections.

Policy enforcement of a range of social restrictions, though the implementation of strict Shariah was introduced by the Muttahida Majlis-e-Amal government but the law was never fully enacted due to objections of the Governor of Khyber Pakhtunkhwa backed by the Musharraff administration. Restrictions on public musical performances were introduced, as well as a ban prohibiting music to be played in public places as part of the "Prohibition of Dancing and Music Bill, 2005" – which led to the creation of a thriving underground music scene in Peshawar. The Islamist government also attempted to enforce compulsory "hijab" on women, and wished to enforce gender segregation in the province's educational institutions. The coalition further tried to prohibit male doctors from performing ultrasounds on women, and tried to close the province's cinemas. In 2005, the coalition successfully passed the "Prohibition of Use of Women in Photograph Bill, 2005," leading to the removal of all public advertisements that featured women.

At the height of Taliban insurgency in Pakistan, the religious coalition lost its grip in the general elections held in 2008, and the religious coalition was swept out of power by the leftist Awami National Party which also witnessed the resignation of President Musharraf in 2008. The ANP government eventually led the initiatives to repeal the major Islamist's social programs, with the backing of the federal government led by PPP in Islamabad. Public disapproval of ANP's leftist program integrated in civil administration with the sounded allegations of corruption as well as popular opposition against religious program promoted by the MMA swiftly shifted the province's leniency towards the right-wing spectrum led by the PTI in 2012. In 2013, the provincial politics shifted towards the right wing, national conservatism when the PTI, led by Imran Khan, was able to form the minority government in coalition with the JeI; the province now serves as the stronghold of the rightist PTI and is perceived as right-wing spectrum of the country.

In non-Pashtun areas, such as Abbottabad, and Hazara Division, the PML(N), the centre-right party, enjoys considerable public support over economical and public policy issues and has a substantial vote bank.

The executive branch of the Kyber Pakhtunkhwa is led by the Chief Minister elected by popular vote in the Provincial assembly while the Governor, a ceremonial figure representing the federal government in Islamabad, is appointed from the necessary advice of the Prime Minister of Pakistan by the President of Pakistan.

The provincial cabinet is then appointed by the Chief Minister who takes the Oath of office from the Governor. In matters of civil administration, the Chief Secretary assists the Chief Minister on executing its right to ensure the writ of the government and the constitution.

The Peshawar High Court is the province's highest court of law whose judges are appointed by the approval of the Supreme Judicial Council in Islamabad, interpreting the laws and overturn those they find unconstitutional.

Khyber Pakhtunkhwa is divided into seven Divisions – Bannu, Dera Ismail Khan, Hazara, Kohat, Malakand, Mardan, and Peshawar. Each division is split up into anywhere between two and nine districts, and there are 35 districts in the entire province. Below you can find a list showing each district ordered by alphabetical order. A full list showing different characteristics of each district, such as their population, area, and a map showing their location can be found at the main article.


Peshawar is the capital and largest city of Khyber Pakhtunkhwa. The city is the most populous and comprises more than one-eighth of the province's population and Bannu NA35 is the largest NA Seat of the province.

Khyber Pakhtunkhwa has the third largest provincial economy in Pakistan. Khyber Pakhtunkhwa's share of Pakistan's GDP has historically comprised 10.5%, although the province accounts for 11.9% of Pakistan's total population. The part of the economy that Khyber Pakhtunkhwa dominates is forestry, where its share has historically ranged from a low of 34.9% to a high of 81%, giving an average of 61.56%. Currently, Khyber Pakhtunkhwa accounts for 10% of Pakistan's GDP, 20% of Pakistan's mining output and, since 1972, it has seen its economy grow in size by 3.6 times.

Agriculture remains important and the main cash crops include wheat, maize, tobacco (in Swabi), rice, sugar beets, as well as fruits are grown in the province.

Some manufacturing and high tech investments in Peshawar has helped improve job prospects for many locals, while trade in the province involves nearly every product. The bazaars in the province are renowned throughout Pakistan. Unemployment has been reduced due to establishment of industrial zones.

Workshops throughout the province support the manufacture of small arms and weapons. The province accounts for at least 78% of the marble production in Pakistan.

The Sharmai Hydropower Project is a proposed power generation project located in Upper Dir District of Khyber Pakhtunkhwa on the Panjkora River with an installed capacity of 150MW. The project feasibility study was carried out by Japanese consulting company Nippon Koei.

The Awami National Party sought to rename the province "Pakhtunkhwa", which translates to "Land of Pakhtuns" in the Pashto language. This was opposed by some of the non-Pashtuns, and especially by parties such as the Pakistan Muslim League-N (PML-N) and Muttahida Majlis-e-Amal (MMA). The PML-N derives its support in the province from primarily non-Pashtun Hazara regions.

In 2010 the announcement that the province would have a new name led to a wave of protests in the Hazara region. On 15 April 2010 Pakistan's senate officially named the province "Khyber Pakhtunkhwa" with 80 senators in favour and 12 opposed. The MMA, who until the elections of 2008 had a majority in the Khyber Pakhtunkhwa government, had proposed "Afghania" as a compromise name.

After the 2008 general election, the Awami National Party formed a coalition provincial government with the Pakistan Peoples Party. The Awami National Party has its strongholds in the Pashtun areas of Pakistan, particularly in the Peshawar valley, while Karachi in Sindh has one of the largest Pashtun populations in the world—around 7 million by some estimates. In the 2008 election, the ANP won two Sindh assembly seats in Karachi. The Awami National Parbeen instrumental in fighting the Taliban. In the 2013 general election Pakistan Tehreek-e-Insaf won a majority in the provincial assembly and has now formed their government in coalition with Jamaat-e-Islami Pakistan.

The following is a list of some of the major NGOs working in Khyber Pakhtunkhwa:

Hindko and Pashto folk music are popular in Khyber Pakhtunkhwa and have a rich tradition going back hundreds of years. The main instruments are the rubab, mangey and harmonium. Khowar folk music is popular in Chitral and northern Swat. The tunes of Khowar music are very different from those of Pashto, and the main instrument is the Chitrali sitar. A form of band music composed of clarinets (Surnai) and drums is popular in Chitral. It is played at polo matches and dances. The same form of band music is played in the neighbouring Northern Areas.

Sources:

This is a chart of the education market of Khyber Pakhtunkhwa estimated by the government in 1998.

Khyber Pakhtunkhwa (KPK) province has 9 government medical colleges



Cricket is the main sport played in Khyber Pakhtunkhwa. It has produced world-class sportsmen like Shahid Afridi, Younis Khan, Fakhar Zaman and Umar Gul. Besides producing cricket players, Khyber Pakhtunkhwa has the honour of being the birthplace of many world-class squash players, including greats like Hashim Khan, Qamar Zaman, Jahangir Khan and Jansher Khan.




</doc>
<doc id="21952" url="https://en.wikipedia.org/wiki?curid=21952" title="Naiad (moon)">
Naiad (moon)

Naiad , (also known as Neptune III and previously designated as S/1989 N 6) named after the naiads of Greek legend, is the innermost satellite of Neptune with a distance of 48.224 km of the planet's center. The moon is tidally locked to Neptune causing its orbit to decrease in altitude and eventually crash into Neptune's atmosphere and become a new ring.

Naiad was discovered sometime before mid-September 1989 from the images taken by the "Voyager 2" probe. The last moon to be discovered during the flyby, it was designated S/1989 N 6. The discovery was announced on 29 September 1989, in the IAU Circular No. 4867, and mentions "25 frames taken over 11 days", implying a discovery date of sometime before September 18. The name was given on 16 September 1991.

Naiad is irregularly shaped. It is likely that it is a rubble pile re-accreted from fragments of Neptune's original satellites, which were smashed up by perturbations from Triton soon after that moon's capture into a very eccentric initial orbit.

Naiad is in a 73:69 orbital resonance with the next outward moon, Thalassa, in a "dance of avoidance". As it orbits Neptune, the more inclined Naiad successively passes Thalassa twice from above and then twice from below, in a cycle that repeats every ~21.5 Earth days. The two moons are about 3540 km apart when they pass each other. Although their orbital radii differ by only 1850 km, Naiad swings ~2800 km above or below Thalassa's orbital plane at closest approach. Thus this resonance, like many such orbital correlations, serves to stabilize the orbits by maximizing separation at conjunction. However, the role of orbital inclination in maintaining this avoidance in a case where eccentricities are minimal is unusual.

Since the "Voyager 2" flyby, the Neptune system has been extensively studied from ground-based observatories and the Hubble Space Telescope as well. In 2002–03 the Keck telescope observed the system using adaptive optics and detected easily the largest four inner satellites. Thalassa was found with some image processing, but Naiad was not located. Hubble has the ability to detect all the known satellites and possible new satellites even dimmer than those found by "Voyager 2". On October 8, 2013 the SETI Institute announced that Naiad had been located in archived Hubble imagery from 2004. The suspicion that the loss of positioning was due to considerable errors in Naiad's ephemeris proved correct as Naiad was ultimately located 80 degrees from its expected position.



</doc>
<doc id="21953" url="https://en.wikipedia.org/wiki?curid=21953" title="Nilo-Saharan languages">
Nilo-Saharan languages

The Nilo-Saharan languages are a proposed family of African languages spoken by some 50–60 million people, mainly in the upper parts of the Chari and Nile rivers, including historic Nubia, north of where the two tributaries of the Nile meet. The languages extend through 17 nations in the northern half of Africa: from Algeria to Benin in the west; from Libya to the Democratic Republic of the Congo in the centre; and from Egypt to Tanzania in the east.

As indicated by its hyphenated name, Nilo-Saharan is a family of the African interior, including the greater Nile Basin and the Central Sahara Desert. Eight of its proposed constituent divisions (excluding Kunama, Kuliak, and Songhay) are found in the modern two nations of Sudan and South Sudan, through which the Nile River flows.

In his book "The Languages of Africa" (1963), Joseph Greenberg named the group and argued it was a genetic family. It contains the languages which are not included in the Niger–Congo, Afroasiatic or Khoisan groups. Although some linguists have referred to the phylum as "Greenberg's wastebasket," into which he placed all the otherwise unaffiliated non-click languages of Africa, specialists in the field have accepted its reality since Greenberg's classification. Its supporters accept that it is a challenging proposal to demonstrate but contend that it looks more promising the more work is done.

Some of the constituent groups of Nilo-Saharan are estimated to predate the African neolithic. Thus, the unity of Eastern Sudanic is estimated to date to at least the 5th millennium BC. Nilo-Saharan genetic unity would necessarily be much older still and date to the late Upper Paleolithic.

This larger classification system is not accepted by all linguists, however. "Glottolog" (2013), for example, a publication of the Max Planck Institute in Germany, does not recognise the unity of the Nilo-Saharan family or even of the Eastern Sudanic branch; Georgiy Starostin (2016) likewise does not accept a relationship between the branches of Nilo-Saharan, though he leaves open the possibility that some of them may prove to be related to each other once the necessary reconstructive work is done.

The constituent families of Nilo-Saharan are quite diverse. One characteristic feature is a tripartite singulative–collective–plurative number system, which Blench (2010) believes is a result of a noun-classifier system in the protolanguage. The distribution of the families may reflect ancient water courses in a green Sahara during the Neolithic Subpluvial, when the desert was more habitable than it is today.

Within the Nilo-Saharan languages are a number of languages with at least a million speakers (most data from SIL's "Ethnologue" 16 (2009)). In descending order:

Some other important Nilo-Saharan languages under 1 million speakers:

The total for all speakers of Nilo-Saharan languages according to "Ethnologue" 16 is 38–39 million people. However, the data spans a range from ca. 1980 to 2005, with a weighted median at ca. 1990. Given population growth rates, the figure in 2010 might be half again higher, or about 60 million.

The Saharan family (which includes Kanuri, Kanembu, the Tebu languages, and Zaghawa) was recognized by Heinrich Barth in 1853, the Nilotic languages by Karl Richard Lepsius in 1880, the various constituent branches of Central Sudanic (but not the connection between them) by Friedrich Müller in 1889, and the Maban family by Maurice Gaudefroy-Demombynes in 1907. The first inklings of a wider family came in 1912, when Diedrich Westermann included three of the (still independent) Central Sudanic families within Nilotic in a proposal he called "Niloto-Sudanic"; this expanded Nilotic was in turn linked to Nubian, Kunama, and possibly Berta, essentially Greenberg's Macro-Sudanic (Chari–Nile) proposal of 1954. 

In 1920 G. W. Murray fleshed out the Eastern Sudanic languages when he grouped Nilotic, Nubian, Nera, Gaam, and Kunama. Carlo Conti Rossini made similar proposals in 1926, and in 1935 Westermann added Murle. In 1940 A. N. Tucker published evidence linking five of the six branches of Central Sudanic alongside his more explicit proposal for East Sudanic. In 1950 Greenberg retained Eastern Sudanic and Central Sudanic as separate families, but accepted Westermann's conclusions of four decades earlier in 1954 when he linked them together as "Macro-Sudanic" (later "Chari–Nile", from the Chari and Nile Watersheds). 

Greenberg's later contribution came in 1963, when he tied Chari–Nile to Songhai, Saharan, Maban, Fur, and Koman-Gumuz and coined the current name "Nilo-Saharan" for the resulting family. Lionel Bender noted that Chari–Nile was a historical artifact of the discovery of the family and did not reflect an exclusive relationship between these languages, and the group has been abandoned, with its constituents becoming primary branches of Nilo-Saharan—or, equivalently, Chari–Nile and Nilo-Saharan have merged, with the name "Nilo-Saharan" retained. When it was realized that the Kadu languages were not Niger–Congo, they were commonly assumed to therefore be Nilo-Saharan, but this remains somewhat controversial.

Progress has been made since Greenberg established the plausibility of the family. Koman and Gumuz remain poorly attested and are difficult to work with, while arguments continue over the inclusion of Songhai. Blench (2010) believes that the distribution of Nilo-Saharan reflects the waterways of the wet Sahara 12,000 years ago, and that the protolanguage had noun classifiers, which today are reflected in a diverse range of prefixes, suffixes, and number marking.

Dimmendaal (2008) notes that Greenberg (1963) based his conclusion on strong evidence and that the proposal as a whole has become more convincing in the decades since. Mikkola (1999) reviewed Greenberg's evidence and found it convincing. Roger Blench notes morphological similarities in all putative branches, which leads him to believe that the family is likely to be valid.

Koman and Gumuz are poorly known and have been difficult to evaluate until recently. Songhay is markedly divergent, in part due to massive influence from the Mande languages . Also problematic are the Kuliak languages, which are spoken by hunter-gatherers and appear to retain a non-Nilo-Saharan core; Blench believes they may have been similar to Hadza or Dahalo and shifted incompletely to Nilo-Saharan. 

Anbessa Tefera and Peter Unseth consider the poorly attested Shabo language to be Nilo-Saharan, though unclassified within the family due to lack of data; Dimmendaal and Blench consider it to be a language isolate on current evidence. Proposals have sometimes been made to add Mande (usually included in Niger–Congo), largely due to its many noteworthy similarities with Songhay rather than with Nilo-Saharan as a whole, however this relationship is more likely due to a close relationship between Songhay and Mande many thousands of years ago in the early days of Nilo-Saharan, so the relationship is probably more one of ancient contact than a genetic link .

The extinct Meroitic language of ancient Kush has been accepted by linguists such as Rille, Dimmendaal, and Blench as Nilo-Saharan, though others argue for an Afroasiatic affiliation. It is poorly attested.

There is little doubt that the constituent families of Nilo-Saharan—of which only Eastern Sudanic and Central Sudanic show much internal diversity—are valid groups. However, there have been several conflicting classifications in grouping them together. Each of the proposed higher-order groups has been rejected by other researchers: Greenberg's Chari–Nile by Bender and Blench, and Bender's Core Nilo-Saharan by Dimmendaal and Blench. What remains are eight (Dimmendaal) to twelve (Bender) constituent families of no consensus arrangement.

Joseph Greenberg, in "The Languages of Africa", set up the family with the following branches. The Chari–Nile core are the connections that had been suggested by previous researchers.
Gumuz was not recognized as distinct from neighboring Koman; it was separated out (forming "Komuz") by Bender (1989).

Lionel Bender came up with a classification which expanded upon and revised that of Greenberg. He considered Fur and Maban to constitute a Fur–Maban branch, added Kadu to Nilo-Saharan, removed Kuliak from Eastern Sudanic, removed Gumuz from Koman (but left it as a sister node), and chose to posit Kunama as an independent branch of the family. By 1991 he had added more detail to the tree, dividing Chari–Nile into nested clades, including a Core group in which Berta was considered divergent, and coordinating Fur–Maban as a sister clade to Chari–Nile.

Bender revised his model of Nilo-Saharan again in 1996, at which point he split Koman and Gumuz into completely separate branches of Core Nilo-Saharan.

Christopher Ehret came up with a novel classification of Nilo-Saharan as a preliminary part of his then-ongoing research into the macrofamily. His evidence for the classification was not fully published until much later (see Ehret 2001 below), and so it did not attain the same level of acclaim as competing proposals, namely those of Bender and Blench.

By 2000 Bender had entirely abandoned the Chari–Nile and Komuz branches. He also added Kunama back to the "Satellite–Core" group and simplified the subdivisions therein. He retracted the inclusion of Shabo, stating that it could not yet be adequately classified but might prove to be Nilo-Saharan once sufficient research has been done. This tentative and somewhat conservative classification held as a sort of standard for the next decade.

Ehret's updated classification was published in his book "A Historical–Comparative Reconstruction of Nilo-Saharan" (2001). This model is notable in that it consists of two primary branches: Gumuz–Koman, and a "Sudanic" group containing the rest of the families (see "Sudanic languages § Nilo-Saharan" for more detail). Also, unusually, Songhay is well-nested within a core group and coordinate with Maban in a "Western Sahelian" clade, and Kadu is not included in Nilo-Saharan. Note that "Koman" in this classification is equivalent to Komuz, i.e. a family with Gumuz and Koman as primary branches, and Ehret renames the traditional Koman group as "Western Koman".

Niger-Saharan, a language macrofamily linking the Niger-Congo and Nilo-Saharan phyla, was proposed by Blench (2006). It is highly controversial and not accepted by mainstream linguistics. Blench's (2006) internal classification of the Niger-Saharan macrophylum is as follows.


According to Blench (2006), typological features common to both Niger-Congo and Nilo-Saharan include:

With a better understanding of Nilo-Saharan classifiers, and the affixes or number marking they have developed into in various branches, Blench believes that all of the families postulated as Nilo-Saharan belong together. He proposes the following tentative internal classification, with Songhai closest to Saharan, a relationship that had not previously been suggested:

? Mimi of Decorse

By 2015, and again in 2017, Blench had refined the subclassification of this model, linking Maban with Fur, Kadu with Eastern Sudanic, and Kuliak with the node that contained them, for the following structure:

Georgiy Starostin (2016), using lexicostatistics based on Swadesh lists, is more inclusive than Glottolog, and in addition finds probable and possible links between the families that will require reconstruction of the protolanguages for confirmation.

In addition to the families listed in "Glottolog" (previous section), Starostin considers the following to be established:


A relationship of Nyima with Nubian, Nara, and Tama (NNT) is considered "highly likely" and close enough that proper comparative work should be able to demonstrate the connection if it's valid, though it would fall outside NNT proper (see Eastern Sudanic languages). 

Other units that are "highly likely" to eventually prove to be valid families are:

In summary, at this level of certainty, "Nilo-Saharan" constitutes ten distinct and separate language families: Eastern Sudanic, Central Sudanic – Kadu, Maba–Kunama, Komuz, Saharan, Songhai, Kuliak, Fur, Berta, and Shabo. 

Possible further "deep" connections, which cannot be evaluated until the proper comparative work on the constituent branches has been completed, are:


There are faint suggestions that Eastern and Central Sudanic may be related (essentially the old Chari–Nile clade), though that possibility is "unexplorable under current conditions" and could be complicated if Niger–Congo were added to the comparison. Starostin finds no evidence that the Komuz, Kuliak, Saharan, Songhai, or Shabo languages are related to any of the other Nilo-Saharan languages. Mimi-D and Meroitic were not considered, though Starostin had previously proposed that Mimi-D was also an isolate despite its slight similarity to Central Sudanic.

In a follow up study published in 2017, Starostin reiterated his previous points as well as explicitly accepting a genetic relationship between Macro-East Sudanic and Macro-Central Sudanic. Starostin names this proposal "Macro-Sudanic"

Gerrit J. Dimmendaal suggests the following subclassification of Nilo-Saharan:

Dimmendaal et al. consider the evidence for the inclusion of Kadu and Songhay too weak to draw any conclusions at present, whereas whereas there is some evidence that Koman and Gumuz belong together and may be Nilo-Saharan.

The large Northeastern division is based on several typological markers:

In summarizing the literature to date, Hammarström et al. in "Glottolog" do not accept that the following families are demonstrably related with current research:


Proposals for the external relationships of Nilo-Saharan typically center on Niger–Congo: Gregersen (1972) grouped the two together as "Kongo–Saharan". However, Blench (2011) proposed that the similarities between Niger–Congo and Nilo-Saharan (specifically Atlantic–Congo and Central Sudanic) are due to contact, with the noun-class system of Niger–Congo developed from, or elaborated on the model of, the noun classifiers of Central Sudanic.

Nilo-Saharan languages present great differences, being a highly diversified group. It has proven difficult to reconstruct many aspects of Proto-Nilo-Saharan. Two very different reconstructions of the proto-language have been proposed by Lionel Bender and Christopher Ehret.

The consonant system reconstructed by Bender for Proto-Nilo-Saharan is:
The phonemes correspond to coronal plosives, the phonetic details are difficult to specify, but clearly, they remain distinct from and supported by many phonetic correspondences (another author, C. Ehret, reconstructs for the coronal area the sound and which perhaps are closer to the phonetic detail of , see infra)

Bender gave a list of about 350 cognates and discussed in depth the grouping and the phonological system proposed by Ch. Ehret. Blench (2000) compares both systems (Bender's and Ehret's) and prefers the former because it is more secure and is based in more reliable data. For example, Bender points out that there is a set of phonemes including implosives /*ɓ, *ɗ, *ʄ, *ɠ/, ejectives /*pʼ, *tʼ, (*sʼ), *cʼ, *kʼ/ and prenasal constants /*b, *d, (*t), *ɟ, *g/, but it seems that they can be reconstructed only for core groups (E, I, J, L) and the collateral group (C, D, F, G, H), but not for Proto-Nilo-Saharan.

Christopher Ehret used a less clear methodology and proposed a maximalist phonemic system:

Ehret's maximalist system has been criticized by Bender and Blench. These authors state that the correspondences used by Ehret are not very clear and because of this many of the sounds in the table may only be allophonic variations.

Dimmendaal (2016) cites the following morphological elements as stable across Nilo-Saharan:

Sample basic vocabulary in different Nilo-Saharan branches:

"Note": In table cells with slashes, the singular form is given before the slash, while the plural form follows the slash.






</doc>
<doc id="21957" url="https://en.wikipedia.org/wiki?curid=21957" title="Nuclear pore">
Nuclear pore

A nuclear pore is a part of a large complex of proteins, known as a nuclear pore complex that spans the nuclear envelope, which is the double membrane surrounding the eukaryotic cell nucleus. There are approximately 1,000 nuclear pore complexes (NPCs) in the nuclear envelope of a vertebrate cell, but it varies depending on cell type and the stage in the life cycle. 
The human nuclear pore complex (hNPC) is a 110 megadalton (MDa) structure. The proteins that make up the nuclear pore complex are known as nucleoporins; each NPC contains at least 456 individual protein molecules and is composed of 34 distinct nucleoporin proteins.About half of the nucleoporins typically contain solenoid protein domains—either an alpha solenoid or a beta-propeller fold, or in some cases both as separate structural domains. The other half show structural characteristics typical of "natively unfolded" or intrinsically disordered proteins, i.e. they are highly flexible proteins that lack ordered tertiary structure. These disordered proteins are the "FG" nucleoporins, so called because their amino-acid sequence contains many phenylalanine—glycine repeats.

Nuclear pore complexes allow the transport of molecules across the nuclear envelope. This transport includes RNA and ribosomal proteins moving from nucleus to the cytoplasm and proteins (such as DNA polymerase and lamins), carbohydrates, signaling molecules and lipids moving into the nucleus. It is notable that the "nuclear pore complex" (NPC) can actively conduct 1000 translocations per complex per second. Although smaller molecules simply diffuse through the pores, larger molecules may be recognized by specific signal sequences and then be diffused with the help of nucleoporins into or out of the nucleus. It has been recently shown that these nucleoporins have specific evolutionary conserved features encoded in their sequences that provide insight into how they regulate the transport of molecules through the nuclear pore. Nucleoporin-mediated transport is not directly energy requiring, but depends on concentration gradients associated with the RAN cycle. Each of the eight protein subunits surrounding the actual pore (the outer ring) projects a spoke-shaped protein over the pore channel. The center of the pore often appears to contain a plug-like structure. It is yet unknown whether this corresponds to an actual plug or is merely cargo caught in transit.

The entire nuclear pore complex has a diameter of about 120 nanometers in vertebrates. The diameter of the channel ranges from 5.2 nanometers in humans to 10.7 nm in the frog "Xenopus laevis", with a depth of roughly 45 nm. mRNA, which is single-stranded, has a thickness of about 0.5 to 1 nm. The molecular mass of the mammalian NPC is about 124 megadaltons (MDa) and it contains approximately 30 different protein components, each in multiple copies. In contrast, the yeast "Saccharomyces cerevisiae" is smaller, with a mass of only 66 MDa.

Small particles (< ~30-60 kDa) are able to pass through the nuclear pore complex by passive diffusion. Larger particles are also able to diffuse passively through the large diameter of the pore, at rates that decrease gradually with molecular weight. Efficient passage through the complex requires several protein factors, and in particular, nuclear transport receptors that bind to cargo molecules and mediate their translocation across the NPC, either into the nucleus (importins) or out of it (exportins). The largest family of nuclear transport receptors are karyopherins, which includes dozens of both importins and exportins; this family is further subdivided to the karyopherin-α and the karyopherin-β subfamilies. Other nuclear transport receptors include NTF2 and some NTF2-like proteins.

Three models have been suggested to explain the translocation mechanism:


Any cargo with a "nuclear localization signal" (NLS) exposed will be destined for quick and efficient transport through the pore. Several NLS sequences are known, generally containing a conserved sequence with basic residues such as PKKKRKV. Any material with an NLS will be taken up by importins to the nucleus.

The classical scheme of NLS-protein importation begins with Importin-α first binding to the NLS sequence, which then acts as a bridge for Importin-β to attach. The importinβ—importinα—cargo complex is then directed towards the nuclear pore and diffuses through it. Once the complex is in the nucleus, RanGTP binds to Importin-β and displaces it from the complex. Then the "cellular apoptosis susceptibility protein" (CAS), an exportin which in the nucleus is bound to RanGTP, displaces Importin-α from the cargo. The NLS-protein is thus free in the nucleoplasm. The Importinβ-RanGTP and Importinα-CAS-RanGTP complex diffuses back to the cytoplasm where GTPs are hydrolyzed to GDP leading to the release of Importinβ and Importinα which become available for a new NLS-protein import round.

Although cargo passes through the pore with the assistance of chaperone proteins, the translocation through the pore itself is not energy-dependent. However, the whole import cycle needs the hydrolysis of 2 GTPs and is thus energy-dependent and has to be considered as active transport. The import cycle is powered by the nucleo-cytoplasmic RanGTP gradient. This gradient arises from the exclusive nuclear localization of RanGEFs, proteins that exchange GDP to GTP on Ran molecules. Thus there is an elevated RanGTP concentration in the nucleus compared to the cytoplasm.

Some molecules or macromolecular complexes need to be exported from the nucleus to the cytoplasm, as do ribosome subunits and messenger RNAs. Thus there is an export mechanism similar to the import mechanism.

In the classical export scheme, proteins with a "nuclear export sequence" (NES) can bind in the nucleus to form a heterotrimeric complex with an exportin and RanGTP (for example the exportin CRM1). The complex can then diffuse to the cytoplasm where GTP is hydrolysed and the NES-protein is released. CRM1-RanGDP diffuses back to the nucleus where GDP is exchanged to GTP by RanGEFs. This process is also energy dependent as it consumes one GTP. Export with the exportin CRM1 can be inhibited by Leptomycin B.

There are different export pathways through the NPC for each RNA class that exists. RNA export is also signal mediated (NES); the NES is in RNA-binding proteins (except for tRNA which has no adapter). It is notable that all viral RNAs and cellular RNAs (tRNA, rRNA, U snRNA, microRNA) except mRNA are dependent on RanGTP. Conserved mRNA export factors are necessary for mRNA nuclear export. Export factors are Mex67/Tap (large subunit) and Mtr2/p15 (small subunit). In higher eukaryotes, mRNA export is thought to be dependent on splicing which in turn recruits a protein complex, TREX, to spliced messages. TREX functions as an adapter for TAP, which is a very poor RNA binding protein. However, there are alternative mRNA export pathways that do not rely on splicing for specialized messages such as histones. Recent work also suggest an interplay between splicing-dependent export and one of these alternative mRNA export pathways for secretory and mitochondrial transcripts.

As the NPC controls access to the genome, it is essential that it exists in large amounts in stages of the cell cycle where plenty of transcription is necessary. For example, cycling mammalian and yeast cells double the amount of NPC in the nucleus between the G1 and G2 phase of the cell cycle, and oocytes accumulate large numbers of NPCs to prepare for the rapid mitosis that exists in the early stages of development. Interphase cells must also keep up a level of NPC generation to keep the levels of NPC in the cell constant as some may get damaged. Some cells can even increase the NPC numbers due to increased transcriptional demand.

There are several theories as to how NPCs are assembled. As the immunodepletion of certain protein complexes, such as the Nup 107–160 complex, leads to the formation of poreless nuclei, it seems likely that the Nup complexes are involved in fusing the outer membrane of the nuclear envelope with the inner and not that the fusing of the membrane begins the formation of the pore. There are several ways that this could lead to the formation of the full NPC.


During mitosis the NPC appears to disassemble in stages. Peripheral nucleoporins such as the Nup 153 Nup 98 and Nup 214 disassociate from the NPC. The rest, which can be considered a scaffold proteins remain stable, as cylindrical ring complexes within the nuclear envelope. This disassembly of the NPC peripheral groups is largely thought to be phosphate driven, as several of these nucleoporins are phosphorylated during the stages of mitosis. However, the enzyme involved in the phosphorylation is unknown in vivo. In metazoans (which undergo open mitosis) the NE degrades quickly after the loss of the peripheral Nups. The reason for this may be due to the change in the NPC’s architecture. This change may make the NPC more permeable to enzymes involved in the degradation of the NE such as cytoplasmic tubulin, as well as allowing the entry of key mitotic regulator proteins. In organisms that undergo a semi-open mitosis such as the filamentous fungus "Aspergillus nidulans", 14 out of the 30 nucleoporins disassemble from the core scaffold structure, driven by the activation of the NIMA and Cdk1 kinases that phosphorylate nucleoporins and open nuclear pores thereby widening the nuclear pore and allowing the entry of mitotic regulators.

It was shown, in fungi that undergo closed mitosis (where the nucleus does not disassemble), that the change of the permeability barrier of the NE was due to changes within the NPC and is what allows the entry of mitotic regulators. In Aspergillus nidulans NPC composition appears to be effected by the mitotic kinase NIMA, possibly by phosphorylating the nucleoporins Nup98 and Gle2/Rae1. This remodelling seems to allow the protein complex cdc2/cyclinB to enter the nucleus as well as many other proteins, such as soluble tubulin. The NPC scaffold remains intact throughout the whole closed mitosis. This seems to preserve the integrity of the NE.



</doc>
<doc id="21958" url="https://en.wikipedia.org/wiki?curid=21958" title="Nucleolus">
Nucleolus

The nucleolus (, plural: nucleoli ) is the largest structure in the nucleus of eukaryotic cells. It is best known as the site of ribosome biogenesis. Nucleoli also participate in the formation of signal recognition particles and play a role in the cell's response to stress. Nucleoli are made of proteins, DNA and RNA and form around specific chromosomal regions called nucleolar organizing regions. Malfunction of nucleoli can be the cause of several human conditions called "nucleolopathies" and the nucleolus is being investigated as a target for cancer chemotherapy.

The nucleolus was identified by bright-field microscopy during the 1830s. Little was known about the function of the nucleolus until 1964, when a study of nucleoli by John Gurdon and Donald Brown in the African clawed frog "Xenopus laevis" generated increasing interest in the function and detailed structure of the nucleolus. They found that 25% of the frog eggs had no nucleolus and that such eggs were not capable of life. Half of the eggs had one nucleolus and 25% had two. They concluded that the nucleolus had a function necessary for life. In 1966 Max L. Birnstiel and collaborators showed via nucleic acid hybridization experiments that DNA within nucleoli code for ribosomal RNA.

Three major components of the nucleolus are recognized: the fibrillar center (FC), the dense fibrillar component (DFC), and the granular component (GC). Transcription of the rDNA occurs in the FC. The DFC contains the protein fibrillarin, which is important in rRNA processing. The GC contains the protein nucleophosmin, (B23 in the external image) which is also involved in ribosome biogenesis.

However, it has been proposed that this particular organization is only observed in higher eukaryotes and that it evolved from a bipartite organization with the transition from anamniotes to amniotes. Reflecting the substantial increase in the DNA intergenic region, an original fibrillar component would have separated into the FC and the DFC.

Another structure identified within many nucleoli (particularly in plants) is a clear area in the center of the structure referred to as a nucleolar vacuole.
Nucleoli of various plant species have been shown to have very high concentrations of iron in contrast to human and animal cell nucleoli.

The nucleolus ultrastructure can be seen through an electron microscope, while the organization and dynamics can be studied through fluorescent protein tagging and fluorescent recovery after photobleaching (FRAP). Antibodies against the PAF49 protein can also be used as a marker for the nucleolus in immunofluorescence experiments.

Although usually only one or two nucleoli can be seen, a diploid human cell has ten nucleolus organizer regions (NORs) and could have more nucleoli. Most often multiple NORs participate in each nucleolus.

In ribosome biogenesis, two of the three eukaryotic RNA polymerases (pol I and III) are required, and these function in a coordinated manner. In an initial stage, the rRNA genes are transcribed as a single unit within the nucleolus by RNA polymerase I. In order for this transcription to occur, several pol I-associated factors and DNA-specific trans-acting factors are required. In yeast, the most important are: UAF (upstream activating factor), TBP (TATA-box binding protein), and core binding factor (CBF)) which bind promoter elements and form the preinitiation complex (PIC), which is in turn recognized by RNA pol. In humans, a similar PIC is assembled with SL1, the promoter selectivity factor (composed of TBP and TBP-associated factors, or TAFs), transcription initiation factors, and UBF (upstream binding factor). RNA polymerase I transcribes most rRNA transcripts 28S, 18S, and 5.8S) but the 5S rRNA subunit (component of the 60S ribosomal subunit) is transcribed by RNA polymerase III.

Transcription of rRNA yields a long precursor molecule (45S pre-rRNA) which still contains the ITS and ETS. Further processing is needed to generate the 18S RNA, 5.8S and 28S RNA molecules. In eukaryotes, the RNA-modifying enzymes are brought to their respective recognition sites by interaction with guide RNAs, which bind these specific sequences. These guide RNAs belong to the class of small nucleolar RNAs (snoRNAs) which are complexed with proteins and exist as small-nucleolar-ribonucleoproteins (snoRNPs). Once the rRNA subunits are processed, they are ready to be assembled into larger ribosomal subunits. However, an additional rRNA molecule, the 5S rRNA, is also necessary. In yeast, the 5S rDNA sequence is localized in the intergenic spacer and is transcribed in the nucleolus by RNA pol.

In higher eukaryotes and plants, the situation is more complex, for the 5S DNA sequence lies outside the Nucleolus Organiser Region (NOR) and is transcribed by RNA pol III in the nucleoplasm, after which it finds its way into the nucleolus to participate in the ribosome assembly. This assembly not only involves the rRNA, but ribosomal proteins as well. The genes encoding these r-proteins are transcribed by pol II in the nucleoplasm by a "conventional" pathway of protein synthesis (transcription, pre-mRNA processing, nuclear export of mature mRNA and translation on cytoplasmic ribosomes). The mature r-proteins are then imported into the nucleus and finally the nucleolus. Association and maturation of rRNA and r-proteins result in the formation of the 40S (small) and 60S (large) subunits of the complete ribosome. These are exported through the nuclear pore complexes to the cytoplasm, where they remain free or become associated with the endoplasmic reticulum, forming rough endoplasmic reticulum (RER).

In human endometrial cells, a network of nucleolar channels is sometimes formed. The origin and function of this network has not yet been clearly identified.

In addition to its role in ribosomal biogenesis, the nucleolus is known to capture and immobilize proteins, a process known as nucleolar detention. Proteins that are detained in the nucleolus are unable to diffuse and to interact with their binding partners. Targets of this post-translational regulatory mechanism include VHL, PML, MDM2, POLD1, RelA, HAND1 and hTERT, among many others. It is now known that long noncoding RNAs originating from intergenic regions of the nucleolus are responsible for this phenomenon.




</doc>
<doc id="21961" url="https://en.wikipedia.org/wiki?curid=21961" title="Nucleon">
Nucleon

In chemistry and physics, a nucleon is either a proton or a neutron, considered in its role as a component of an atomic nucleus. The number of nucleons in a nucleus defines an isotope's mass number (nucleon number).

Until the 1960s, nucleons were thought to be elementary particles, not made up of smaller parts. Now they are known to be composite particles, made of three quarks bound together by the so-called strong interaction. The interaction between two or more nucleons is called internucleon interaction or nuclear force, which is also ultimately caused by the strong interaction. (Before the discovery of quarks, the term "strong interaction" referred to just internucleon interactions.)

Nucleons sit at the boundary where particle physics and nuclear physics overlap. Particle physics, particularly quantum chromodynamics, provides the fundamental equations that explain the properties of quarks and of the strong interaction. These equations explain quantitatively how quarks can bind together into protons and neutrons (and all the other hadrons). However, when multiple nucleons are assembled into an atomic nucleus (nuclide), these fundamental equations become too difficult to solve directly (see lattice QCD). Instead, nuclides are studied within nuclear physics, which studies nucleons and their interactions by approximations and models, such as the nuclear shell model. These models can successfully explain nuclide properties, as for example, whether or not a particular nuclide undergoes radioactive decay.

The proton and neutron are in a scheme of categories being at once fermions, hadrons and baryons. The proton carries a positive net charge and the neutron carries a zero net charge; the proton's mass is only about 0.13% less than the neutron's. Thus, they can be viewed as two states of the same nucleon, and together form an isospin doublet (). In isospin space, neutrons can be transformed into protons via SU(2) symmetries, and vice versa. These nucleons are acted upon equally by the strong interaction, which is invariant under rotation in isospin space. According to the Noether theorem, isospin is conserved with respect to the strong interaction.

Protons and neutrons are best known in their role as nucleons, i.e., as the components of atomic nuclei, but they also exist as free particles. Free neutrons are unstable, with a half-life of around 13 minutes, but they have important applications (see neutron radiation and neutron scattering). Protons not bound to other nucleons are the nuclei of hydrogen atoms when bound with an electron or — if not bound to anything — are ions or cosmic rays.

Both the proton and the neutron are composite particles, meaning each is composed of smaller parts, namely three quarks each; although once thought to be so, neither is an elementary particle. A proton is composed of two up quarks and one down quark, while the neutron has one up quark and two down quarks. Quarks are held together by the strong force, or equivalently, by gluons, which mediate the strong force at the quark level.

An up quark has electric charge  "e", and a down quark has charge  "e", so the summed electric charges of proton and neutron are +"e" and 0, respectively. Thus, the neutron has a charge of 0 (zero), and therefore is electrically neutral; indeed, the term "neutron" comes from the fact that a neutron is electrically neutral.

The masses of the proton and neutron are quite similar: The proton is or , while the neutron is or . The neutron is roughly 0.13% heavier. The similarity in mass can be explained roughly by the slight difference in masses of up quarks and down quarks composing the nucleons. However, a detailed explanation remains an unsolved problem in particle physics.

The spin of the nucleon is , which means they are fermions and, like electrons, are subject to the Pauli exclusion principle: No more than one nucleon, e.g. in an atomic nucleus, may occupy the same quantum state.

The isospin and spin quantum numbers of the nucleon have two states each, resulting in four combinations in total. An alpha particle is composed of four nucleons occupying all four combinations, namely it has two protons (having opposite spin) and two neutrons (also having opposite spin) and its net nuclear spin is zero. In larger nuclei constituent nucleons, to avoid Pauli exclusion, are compelled to have relative motion which may also contribute to nuclear spin via the orbital quantum number. They spread out into nuclear shells analogous to electron shells known from chemistry.

The magnetic moment of a proton, denoted , is (where μ represents the atomic-scale unit of measure called the "nuclear magneton"). The magnetic moment of a neutron is = . These parameters are also important in NMR / MRI scanning.

A neutron in free state is an unstable particle, with a half-life around ten minutes. It undergoes decay (a type of radioactive decay) by turning into a proton while emitting an electron and an electron antineutrino. (See the Neutron article for more discussion of neutron decay.) A proton by itself is thought to be stable, or at least its lifetime is too long to measure. This is an important discussion in particle physics, (see Proton decay).

Inside a nucleus, on the other hand, combined protons and neutrons (nucleons) can be stable or unstable depending on the nuclide, or nuclear species. Inside some nuclides, a neutron can turn into a proton (producing other particles) as described above; the reverse can happen inside other nuclides, where a proton turns into a neutron (producing other particles) through decay, or electron capture. And inside still other nuclides, both protons and neutrons are stable and do not change form.

Both nucleons have corresponding antiparticles: the antiproton and the antineutron, which have the same mass and opposite charge as the proton and neutron respectively, and they interact in the same way. (This is generally believed to be "exactly" true, due to CPT symmetry. If there is a difference, it is too small to measure in all experiments to date.) In particular, antinucleons can bind into an "antinucleus". So far, scientists have created antideuterium and antihelium-3 nuclei.

 The masses of the proton and neutron are known with far greater precision in atomic mass units (u) than in MeV/c, due to the relatively poorly known value of the elementary charge. The conversion factor used is 1 u = MeV/c.
The masses of their antiparticles are assumed to be identical, and no experiments have refuted this to date. Current experiments show any percent difference between the masses of the proton and antiproton must be less than and the difference between the neutron and antineutron masses is on the order of MeV/c.

† "The P(939) nucleon represents the excited state of a normal proton or neutron, for example, within the nucleus of an atom. Such particles are usually stable within the nucleus, i.e. Lithium-6."

In the quark model with SU(2) flavour, the two nucleons are part of the ground state doublet. The proton has quark content of "uud", and the neutron, "udd". In SU(3) flavour, they are part of the ground state octet (8) of spin baryons, known as the Eightfold way. The other members of this octet are the hyperons strange isotriplet , , the and the strange isodoublet . One can extend this multiplet in SU(4) flavour (with the inclusion of the charm quark) to the ground state 20-plet, or to SU(6) flavour (with the inclusion of the top and bottom quarks) to the ground state 56-plet.

The article on isospin provides an explicit expression for the nucleon wave functions in terms of the quark flavour eigenstates.

Although it is known that the nucleon is made from three quarks, , it is not known how to solve the equations of motion for quantum chromodynamics. Thus, the study of the low-energy properties of the nucleon are performed by means of models. The only first-principles approach available is to attempt to solve the equations of QCD numerically, using lattice QCD. This requires complicated algorithms and very powerful supercomputers. However, several analytic models also exist:

The Skyrmion models the nucleon as a topological soliton in a non-linear SU(2) pion field. The topological stability of the Skyrmion is interpreted as the conservation of baryon number, that is, the non-decay of the nucleon. The local topological winding number density is identified with the local baryon number density of the nucleon. With the pion isospin vector field oriented in the shape of a hedgehog space, the model is readily solvable, and is thus sometimes called the "hedgehog model". The hedgehog model is able to predict low-energy parameters, such as the nucleon mass, radius and axial coupling constant, to approximately 30% of experimental values.

The "MIT bag model" confines three non-interacting quarks to a spherical cavity, with the boundary condition that the quark vector current vanish on the boundary. The non-interacting treatment of the quarks is justified by appealing to the idea of asymptotic freedom, whereas the hard boundary condition is justified by quark confinement.

Mathematically, the model vaguely resembles that of a radar cavity, with solutions to the Dirac equation standing in for solutions to the Maxwell equations and the vanishing vector current boundary condition standing for the conducting metal walls of the radar cavity. If the radius of the bag is set to the radius of the nucleon, the bag model predicts a nucleon mass that is within 30% of the actual mass.

Although the basic bag model does not provide a pion-mediated interaction, it describes excellently the nucleon-nucleon forces through the 6 quark bag s-channel mechanism using the P matrix. 
The "chiral bag model" merges the "MIT bag model" and the "Skyrmion model". In this model, a hole is punched out of the middle of the Skyrmion, and replaced with a bag model. The boundary condition is provided by the requirement of continuity of the axial vector current across the bag boundary.

Very curiously, the missing part of the topological winding number (the baryon number) of the hole punched into the Skyrmion is exactly made up by the non-zero vacuum expectation value (or spectral asymmetry) of the quark fields inside the bag. , this remarkable trade-off between topology and the spectrum of an operator does not have any grounding or explanation in the mathematical theory of Hilbert spaces and their relationship to geometry.

Several other properties of the chiral bag are notable: It provides a better fit to the low energy nucleon properties, to within 5–10%, and these are almost completely independent of the chiral bag radius (as long as the radius is less than the nucleon radius). This independence of radius is referred to as the "Cheshire Cat principle", after the fading of Lewis Carroll's Cheshire Cat to just its smile. It is expected that a first-principles solution of the equations of QCD will demonstrate a similar duality of quark-pion descriptions.



</doc>
<doc id="21966" url="https://en.wikipedia.org/wiki?curid=21966" title="Nicolas Chauvin">
Nicolas Chauvin

Nicolas Chauvin () is a legendary, possibly apocryphal French soldier and patriot who is supposed to have served in the First Army of the French Republic and subsequently in "La Grande Armée" of Napoleon. His name is the eponym of "chauvinism", originally a term for excessive nationalistic fervor, but later used to refer to any form of bigotry or bias (e.g., male chauvinism).

According to the stories that developed about him, Chauvin was born in Rochefort, around 1780. He enlisted at age 18, and served honorably and well. He is said to have been wounded 17 times in his nation's service, resulting in his severe disfigurement and maiming. For his loyalty and dedication, Napoleon himself presented the soldier with a Sabre of Honor and a pension of 200 francs.

Chauvin's distinguished record of service and his love and devotion for Napoleon, which endured despite the price he willingly paid for them, is said to have earned him only ridicule and derision in Restoration France, when Bonapartism became increasingly unpopular. 

Historical research has not identified any biographical details of a real Nicolas Chauvin, leading to the claim that he may have been a wholly fictional figure. Researcher Gérard Puymège concluded that Nicolas Chauvin did not exist, believing him to be a legend, which crystallized under the Restoration and July Monarchy, from the pen of songwriters, vaudeville and historians. He argues that the figure of Chauvin continues the long tradition of the mythological farmer-soldier or "miles gloriosus" ("boastful soldier") from ancient Roman theater, or the "alazon" of ancient Greek comedy. Chauvin was originally popularized by Cogniard brothers' "La Cocarde Tricolore" (1831), where instead of a Napoleonic veteran he was a young naive soldier learning blindly aggressive patriotism during the Algerian campaign of 1830.

Many writers and historians falsely attribute to Chauvin the exploits of other Bonapartists. It is claimed that he served in the Old Guard at the Battle of Waterloo, which is certainly possible considering his age. When the Old Guard was surrounded and made its last stand at La Belle Alliance, he supposedly shouted in defiance to a call for their honorable surrender: "The Old Guard dies but does not surrender!", implying blind and unquestioned zealous devotion to one's country (or other group of reference). The apocryphal phrase was attributed to the Old Guard's commander, Pierre Cambronne, but Cambronne's actual reply was later asserted by other sources to be ""Merde!"" ("Shit!").


</doc>
<doc id="21968" url="https://en.wikipedia.org/wiki?curid=21968" title="Nicotinamide">
Nicotinamide

Nicotinamide (NAM) is a form of vitamin B found in food and used as a dietary supplement and medication. As a supplement, it is used by mouth to prevent and treat pellagra (niacin deficiency). While nicotinic acid (niacin) may be used for this purpose, nicotinamide has the benefit of not causing skin flushing. As a cream, it is used to treat acne.
Side effects are minimal. At high doses liver problems may occur. Normal amounts are safe for use during pregnancy. Nicotinamide is in the vitamin B family of medications, specifically the vitamin B complex. It is an amide of nicotinic acid. Foods that contain nicotinamide include yeast, meat, milk, and green vegetables.
Nicotinamide was discovered between 1935 and 1937. It is on the World Health Organization's List of Essential Medicines, the safest and most effective medicines needed in a health system. Nicotinamide is available as a generic medication and over the counter. Commercially, nicotinamide is made from either nicotinic acid or nicotinonitrile. In a number of countries grains have nicotinamide added to them.

Nicotinamide is the preferred treatment for pellagra, caused by niacin deficiency. While niacin may be used, nicotinamide has the benefit of not causing skin flushing.

Nicotinamide cream is used as a treatment for acne. It has anti-inflammatory actions, which may benefit people with inflammatory skin conditions.

Nicotinamide increases the biosynthesis of ceramides in human keratinocytes in vitro and improves the epidermal permeability barrier in vivo. The application of 2% topical nicotinamide for 2 and 4 weeks has been found to be effective in lowering the sebum excretion rate. Nicotinamide has been shown to prevent "Cutibacterium acnes"-induced activation of toll-like receptor 2, which ultimately results in the down-regulation of pro-inflammatory interleukin-8 production.

Nicotinamide at doses of 500 to 1000 mg a day decreases the risk of skin cancers, other than melanoma, in those at high risk.

Nicotinamide has minimal side effects. At high doses liver problems may occur. Normal doses are safe during pregnancy.

The structure of nicotinamide consists of a pyridine ring to which a primary amide group is attached in the "meta" position. It is an amide of nicotinic acid. As an aromatic compound, it undergoes electrophilic substitution reactions and transformations of its two functional groups. Examples of these reactions reported in "Organic Syntheses" include the preparation of 2-chloronicotinonitrile by a two-step process via the "N"-oxide,

from nicotinonitrile by reaction with phosphorus pentoxide, and from 3-aminopyridine by reaction with a solution of sodium hypobromite, prepared "in situ" from bromine and sodium hydroxide.

The hydrolysis of nicotinonitrile is catalysed by the enzyme nitrile hydratase from "Rhodococcus rhodochrous" J1, producing 3500 tons per annum of nicotinamide for use in animal feed. The enzyme allows for a more selective synthesis as further hydrolysis of the amide to nicotinic acid is avoided. Nicotinamide can also be made from nicotinic acid. According to "Ullmann's Encyclopedia of Industrial Chemistry", worldwide 31,000 tons of nicotinamide were sold in 2014.

Nicotinamide, as a part of the coenzyme nicotinamide adenine dinucleotide (NADH / NAD) is crucial to life. In cells, nicotinamide is incorporated into NAD and nicotinamide adenine dinucleotide phosphate (NADP). NAD and NADP are coenzymes in a wide variety of enzymatic oxidation-reduction reactions, most notably glycolysis, the citric acid cycle, and the electron transport chain. If humans ingest nicotinamide, it will likely undergo a series of reactions that transform it into NAD, which can then undergo a transformation to form NADP. This method of creation of NAD is called a salvage pathway. However, the human body can produce NAD from the amino acid tryptophan and niacin without our ingestion of nicotinamide.

NAD acts as an electron carrier that helps with the interconversion of energy between nutrients and the cell's energy currency, adenosine triphosphate (ATP). In oxidation-reduction reactions, the active part of the coenzyme is the nicotinamide. In NAD, the nitrogen in the aromatic nicotinamide ring is covalently bonded to adenine dinucleotide. The formal charge on the nitrogen is stabilized by the shared electrons of the other carbon atoms in the aromatic ring. When a hydride atom is added onto NAD to form NADH, the molecule loses its aromaticity, and therefore a good amount of stability. This higher energy product later releases its energy with the release of a hydride, and in the case of the electron transport chain, it assists in forming adenosine triphosphate.

When one mole of NADH is oxidized, 158.2 kJ of energy will be released.

Nicotinamide occurs as a component of a variety of biological systems, including within the vitamin B family and specifically the vitamin B complex. It is also a critically important part of the structures of NADH and NAD, where the "N"-substituted aromatic ring in the oxidised NAD form undergoes reduction with hydride attack to form NADH. The NADPH/NADP structures have the same ring, and are involved in similar biochemical reactions.

Nicotinamide occurs in trace amounts mainly in meat, fish, nuts, and mushrooms, as well as to a lesser extent in some vegetables. It is commonly added to cereals and other foods. Many multivitamins contain 20–30 mg of vitamin B and it is also available in higher doses.


A 2015 trial found nicotinamide to reduce the rate of new nonmelanoma skin cancers and actinic keratoses in a group of people at high risk for the conditions.

Nicotinamide has been investigated for many additional disorders, including treatment of bullous pemphigoid nonmelanoma skin cancers.

Nicotinamide may be beneficial in treating psoriasis.

There is tentative evidence for a potential role of nicotinamide in treating acne, rosacea, autoimmune blistering disorders, ageing skin, and atopic dermatitis. Niacinamide also inhibits poly(ADP-ribose) polymerases (PARP-1), enzymes involved in the rejoining of DNA strand breaks induced by radiation or chemotherapy. ARCON (accelerated radiotherapy plus carbogen inhalation and nicotinamide) has been studied in cancer.

Research has suggested nicotinamide may play a role in the treatment of HIV.



</doc>
<doc id="21970" url="https://en.wikipedia.org/wiki?curid=21970" title="Virtual Boy">
Virtual Boy

The Virtual Boy is a 32-bit tabletop portable video game console developed and manufactured by Nintendo. Released in 1995, it was marketed as the first console capable of displaying stereoscopic "3D" graphics. The player uses the console like a head-mounted display, placing the head against the eyepiece to see a red monochrome display. The games use a parallax effect to create the illusion of depth. Sales failed to meet targets, and Nintendo ceased distribution and game development in 1996, having released only 22 games for the system.

Development of the Virtual Boy lasted four years and began under the project name VR32. Nintendo entered a licensing agreement to use a stereoscopic LED eyepiece technology which had been developed since the 1980s by US company Reflection Technology. It also built a factory in China to be used only for Virtual Boy manufacturing. Over the course of development, the console technology was downscaled due to high costs and potential health concerns, and an increasing amount of resources were reallocated to the development of the Nintendo 64, Nintendo's next home console. Lead game designer Shigeru Miyamoto had little involvement with the Virtual Boy software. The Virtual Boy was pushed to market in an unfinished state in 1995 to focus on the Nintendo 64.

The Virtual Boy was panned by critics and was a commercial failure, even after repeated price drops. Its failure has been attributed to its high price, monochrome display, unimpressive stereoscopic effect, lack of true portability, and health concerns. Stereoscopic technology in video game consoles reemerged in later years to more success, including Nintendo's 3DS handheld console.

Since 1985, a red LED eyepiece display technology called Scanned Linear Array was developed by Massachusetts-based Reflection Technology, Inc. (RTI). The company produced a stereoscopic head-tracking prototype called the Private Eye, featuring a tank game. Seeking funding and partnerships by which to develop it into a commercial technology, RTI demonstrated Private Eye to the consumer electronics market, including Mattel and Hasbro. Sega declined the technology, due to its single-color display and concerns about motion sickness.

Nintendo enthusiastically received the Private Eye, as led by Gunpei Yokoi, the general manager of Nintendo's R&D1 and the inventor of the Game & Watch and Game Boy handheld consoles. He saw this as a unique technology that competitors would find difficult to emulate. Additionally, the resulting game console was intended to enhance Nintendo's reputation as an innovator and to "encourage more creativity" in games. Codenaming the project "VR32", Nintendo entered into an exclusive agreement with Reflection Technology, Inc. to license the technology for its displays. While Nintendo's Research & Development 3 division (R&D3) was focused on developing the Nintendo 64, the other two engineering units were free to experiment with new product ideas.

Spending four years in development and eventually building a dedicated manufacturing plant in China, Nintendo worked to turn its VR32 vision into an affordable and health-conscious console design. Yokoi retained RTI's choice of red LED because it was the cheapest, and because unlike a backlit LCD, its perfect blackness could achieve a more immersive sense of infinite depth. RTI and Nintendo said a color LCD system would have been prohibitively expensive, retailing for more than . A color LCD system was also said to have caused "jumpy images in tests". With ongoing concerns about motion sickness, the risk of developing lazy eye conditions in young children, and Japan's new Product Liability Act of 1995, Nintendo eliminated the head tracking functionality and converted its headmounted goggle design into a stationary, heavy, precision steel-shielded, tabletop form factor conformant to the recommendation of the Schepens Eye Research Institute.

Several technology demonstrations were used to show the Virtual Boy's capabilities. "Driving Demo" is one of the more advanced demos; its 30-second clip shows a first-person view of driving by road signs and palm trees. This demo was shown at E3 and CES in 1995. The startup screen of the Virtual Boy prototype was shown at Shoshinkai 1994. A "very confident" projection of "sales in Japan of 3 million hardware units and 14 million software units by March of 1996" was given to the press. The demo of what would have been a "Star Fox" game showed an Arwing doing various spins and motions. Cinematic camera angles were a key element, as they are in "Star Fox 2". It was shown at E3 and CES in 1995.

As a result of increasing competition for internal resources alongside the flagship Nintendo 64, and little involvement from lead game designer Shigeru Miyamoto, Virtual Boy software was developed without Nintendo's full attention. According to David Sheff's book "Game Over", the increasingly reluctant Yokoi never intended for the increasingly downscaled Virtual Boy to be released in its final form. However, Nintendo pushed it to market so that it could focus development resources on its next console, the Nintendo 64.

"The New York Times" previewed the Virtual Boy on November 13, 1994. The console was officially announced via press release the next day, November 14. Nintendo promised that Virtual Boy would "totally immerse players into their own private universe". Initial press releases and interviews about the system focused on its technological capabilities, avoiding discussion of the actual games that would be released. The system was demonstrated the next day at Nintendo's Shoshinkai 1994 trade show. Nintendo of America showed the Virtual Boy at the Consumer Electronics Show on January 6, 1995.

Even with cost-saving measures in place, Nintendo priced the Virtual Boy at a relatively high . Though slightly less expensive and significantly less powerful than a home console, this was considerably more costly than the Game Boy handheld. With seemingly more advanced graphics than Game Boy, the Virtual Boy was not intended to replace the handheld in Nintendo's product line, as use of the Virtual Boy requires a steady surface and completely blocks the player's peripheral vision. "Design News" described the Virtual Boy as the logical evolution of the View-Master 3D image viewer.

The Virtual Boy was released on July 21, 1995 in Japan and on August 16, 1995 in North America with the launch games "Mario's Tennis", "Red Alarm", "Teleroboxer", and "Galactic Pinball". It was not released in PAL markets. In North America, Nintendo shipped "Mario's Tennis" with every Virtual Boy sold, as a pack-in game. Nintendo had initially projected sales of 3 million consoles and 14 million games. The system arrived later than other 32-bit systems like PlayStation, 3DO, and Saturn, but at a lower price.
At the system's release, Nintendo of America projected hardware sales of 1.5 million units and software sales numbering 2.5 million by the end of the year. Nintendo had shipped 350,000 units of the Virtual Boy by December 1995, around three and a half months after its North American release. The system is number 5 on "GamePro"s "Top 10 Worst Selling Consoles of All Time" list in 2007.
The Virtual Boy had a short market timespan following its disappointing sales. The last game officially released for the Virtual Boy was "3D Tetris", released on March 22, 1996. More games were announced for the system at the Electronic Entertainment Expo in May 1996, but these games were never released. The Virtual Boy was discontinued that year without any announcement. In June 1996, Nintendo reported to "Famitsu" worldwide sales of 770,000 Virtual Boy units, including 140,000 in Japan. "Next Generation" reported that 13,000 Virtual Boy units were sold in December 1996.

Nintendo extensively advertised the Virtual Boy and claimed to have spent on early promotional activities. Advertising promoted the system as a paradigm shift from past consoles; some pieces used cavemen to indicate a historical evolution, while others utilized psychedelic imagery. Nintendo targeted an older audience with advertisements for the Virtual Boy, shifting away from the traditional child-focused approach it had employed in the past. Nintendo portrayed the system as a type of virtual reality, as its name indicates. Nintendo also focused on the technological aspects of the new console in its press releases, neglecting to detail specific games.

Confronted with the challenge of showing 3-dimensional gameplay on 2-dimensional advertisements, the company partnered with Blockbuster and NBC in a coordinated effort. A $5 million campaign promoted NBC's fall lineup alongside the Virtual Boy. American viewers were encouraged via television advertisements on NBC to rent the console for $10 at a local Blockbuster. This made it affordable for a large number of gamers to try the system, and produced 750,000 rentals. Upon returning the unit, renters received a coupon for $10 off the sale of a Virtual Boy from any store. 3,000 Blockbuster locations were included in the promotion, which consisted of sweepstakes with prizes including trips to see the taping of NBC shows. The popular rental system proved harmful to the Virtual Boy's long-term success, allowing gamers to see just how non-immersive the console was. By mid-1996, Blockbuster was selling its Virtual Boy units at $50 each. The marketing campaign overall was commonly thought of as a failure.

The central processing unit is a 32-bit RISC chip, making the Virtual Boy Nintendo's first 32-bit system. The Virtual Boy system uses a pair of 1×224 linear arrays (one per eye) and rapidly scans the array across the eye's field of view using flat oscillating mirrors. These mirrors vibrate back and forth at a very high speed, thus the mechanical humming noise from inside the unit. Each Virtual Boy game cartridge has a yes/no option to automatically pause every 15–30 minutes so that the player may take a break before any injuries come to the eyes. One speaker per ear provides the player with audio.

The Virtual Boy is the first video game console that was supposed to be capable of displaying stereoscopic "3D" graphics, marketed as a form of virtual reality. Whereas most video games use monocular cues to achieve the illusion of three dimensions on a two-dimensional screen, the Virtual Boy creates an illusion of depth through the effect known as parallax. Like using a head-mounted display, the user looks into an eyepiece made of neoprene on the front of the machine, and then an eyeglass-style projector allows viewing of the monochromatic (in this case, red) image.

The display consists of two 2-bit monochrome red screens of 384×224 pixels and a frame rate of approximately 50.27 Hz. It uses an oscillating mirror to transform a single line of LED-based pixels into a full field of pixels. Nintendo claimed that a color display would have made the system too expensive and resulted in "jumpy" images, so the company opted for a monochrome display. To achieve a color display, Nintendo would have used a combination of red, green, and blue LEDs. At the time, blue LEDs were still considerably expensive and would in turn raise the price of the final product. This in combination with the other drawbacks helped influence Nintendo's decision to release the Virtual Boy as a monochrome device.

The Virtual Boy was meant to be used while sitting down at a table, although Nintendo said it would release a harness for players to use while standing.

The Virtual Boy's heavy emphasis on three-dimensional movement requires the controller to operate along a Z-axis. Its controller is an attempt to implement dual digital D-pads to control elements in the 3D environment. The controller is shaped like an "M", like a Nintendo 64 controller. The player holds onto either side of the controller which has a unique extendable power supply that slides onto the back, housing the system's six AA batteries. The batteries can be substituted with a wall adapter, via a "slide-on" attachment for constant power.

In more traditional 2-dimensional games, the two directional pads are interchangeable. For others with a more 3D environment, like "Red Alarm", "3D Tetris", or "Teleroboxer", each pad controls a different feature. The symmetry of the controller also allows left-handed gamers to reverse the controls, as does the Atari Lynx.

During development, Nintendo promised the ability to link systems for competitive play. A Virtual Boy link cable was being worked on at Nintendo as late as the third quarter of 1996. The system's EXT (extension) port, located on the underside of the system below the controller port, was never officially supported since no "official" multiplayer games were ever published. Although "Waterworld" and "Faceball" were intended to use the EXT port for multiplayer play, the multiplayer features in the former were removed and the latter game was canceled. Later a reproduction link cable was made.

Nintendo initially showcased three games for the Virtual Boy. It planned three launch games, and two or three per month thereafter. Given the system's short lifespan, only 22 games were actually released. Of them, 19 games were released in the Japanese market, and 14 were released in North America. Third party support was extremely limited compared to previous Nintendo platforms. According to Gunpei Yokoi, Nintendo president Hiroshi Yamauchi had dictated that only a select few third party developers be shown the Virtual Boy hardware before its formal unveiling, to limit the risk of poor-quality software appearing on the system.

When asked if Virtual Boy games were going to be available for download on the Virtual Console for the Nintendo 3DS, Nintendo of America President Reggie Fils-Aime said he could not answer, as he was unfamiliar with the platform. He noted that, given his lack of familiarity, he would be hard-pressed to make the case for the inclusion of the games on the Virtual Console.

The hobbyist community at "Planet Virtual Boy" has developed Virtual Boy software. Two previously unreleased games, "Bound High" and "Niko-Chan Battle" (the Japanese version of "Faceball") were released.

The Virtual Boy was overwhelmingly panned by critics and was a commercial failure. It failed for several reasons including "its high price, the discomfort caused by play [...] and what was widely judged to have been a poorly handled marketing campaign".

Gamers who previewed the system at the Shoshinkai trade show in 1994 complained that the "Mario" demo was not realistic enough, was not in full color, and didn't motion-track the image when players turn their heads. In the lead editorial of "Electronic Gaming Monthly" following the show, Ed Semrad predicted that the Virtual Boy would have poor launch sales due to the monochrome screen, lack of true portability, unimpressive lineup of games seen at the Shoshinkai show, and the price, which he argued was as low as it could get given the hardware but still too expensive for the experience the system offered. "Next Generation"s editors were also dubious of the Virtual Boy's prospects when they left the show, and concluded their article on the system by commenting, "But who will buy it? It's not portable, it's awkward to use, it's 100% antisocial (unlike multiplayer SNES/Genesis games), it's too expensive and – most importantly – the 'VR' (i.e. 3D effect) doesn't add to the game at all: it's just a novelty."

Following its release, reviews of the Virtual Boy tended to praise its novelty but questioned its ultimate purpose and longtime viability. "The Los Angeles Times" described the gameplay as being "at once familiar and strange." The column praised the quality of motion and immersive graphics but considered the hardware itself tedious to use and non-portable. A later column by the same reviewer found the system to be somewhat asocial, although it held out hope for the console's future. Reviewing the system shortly after its North American launch, "Next Generation" said, "Unusual and innovative, the Virtual Boy can be seen as a gamble in the same way that the Game Boy was, but it's a lot harder to see the VB succeeding to the same world-conquering extent that the Game Boy did." They elaborated that while the sharp display and unique 3D effect are impressive, aspects such as the monochrome display and potential vision damage to young gamers severely limit the system's appeal. They added that the software library was decent, but failed to capitalize on Nintendo's best-selling franchises ("Zelda" and "Metroid" games were absent, and the "Mario" games were not in the same style as the series's most successful installments) and lacked a system seller to compare with the Game Boy's "Tetris".

Though Nintendo had promised a virtual reality experience, the monochrome display limits the Virtual Boy's potential for immersion. Reviewers often considered the 3-dimensional features a gimmick, added to games that were essentially 2- or even 1-dimensional. "The Washington Post" said that even when a game gives the impression of 3-dimensionality, it suffers from "hollow vector graphics". Yokoi, the system's inventor, said the system did best with action and puzzle games, although those types of games provided only minimal immersion. Multiple critics lamented the absence of head-tracking in the Virtual Boy hardware. Critics found that, as a result, players were unable to immerse themselves in the game worlds of Virtual Boy games. Instead, they interacted simply via a controller, in the manner of any traditional 2-dimensional game. Boyer said the console "struggles to merge the two distinct media forms of home consoles and virtual reality devices". Though the device employs some basic virtual reality techniques, it does so like the traditional home console with no bodily feedback incorporated into gameplay.

Many reviewers complained of painful and frustrating physiological symptoms when playing the Virtual Boy. Bill Frischling, writing for "The Washington Post", experienced "dizziness, nausea and headaches". Reviewers attributed the problems to both the monochromatic display and uncomfortable ergonomics. Several prominent scientists concluded that the long-term side effects could be more serious, and articles published in magazines such as "Electronic Engineering Times" and CMP Media's "TechWeb" speculated that using any immersive headset such as the Virtual Boy could cause sickness, flashbacks, and even permanent brain damage. Nintendo, in the years after Virtual Boy's demise, has been frank about its failure. Howard Lincoln, chairman of Nintendo of America, said flatly that the Virtual Boy "just failed".

According to "Game Over", Nintendo laid the blame for the machine's faults directly on its creator, Gunpei Yokoi. The commercial failure of the Virtual Boy was said by members of the video game press to be a contributing factor to Yokoi's withdrawal from Nintendo, although he had planned to retire years prior and finished another more successful project for the company, the Game Boy Pocket, which was released shortly before his departure. According to his Nintendo and Koto colleague Yoshihiro Taki, Yokoi had originally decided to retire at age 50 to do as he pleased but had simply delayed it. Nintendo held that Yokoi's departure was "absolutely coincidental" to the market performance of any Nintendo hardware. "The New York Times" maintained that Yokoi kept a close relationship with Nintendo. After leaving Nintendo, Yokoi founded his own company, Koto, and collaborated with Bandai to create the WonderSwan, a handheld system competing with the Game Boy.

The commercial failure of the Virtual Boy reportedly did little to alter Nintendo's development approach and focus on innovation. While the console itself is said to have failed in many regards, its focus on peripherals and haptic technology reemerged in later years. The original inventor, Reflection Technology, Inc., was reportedly financially "devastated" by the Virtual Boy's performance, with dwindling operations by 1997.

With the launch of the Nintendo 3DS console in 2011, Nintendo released a handheld gaming console with autostereoscopic "3D" visuals; meaning that the console produces the desired depth effects without any special glasses and is portable. In the period leading up to the release of the Nintendo 3DS, Shigeru Miyamoto discussed his view of the issues with the Virtual Boy. One was the actual use of the three-dimensional effects; while it was designed to render wireframe graphics, the effects are generally used to separate two-dimensional games into different planes separated by depth. Further, Miyamoto stated that the graphics are not as appealing, and while developing the Nintendo 64, had ruled out the use of wireframe graphics as too sparse to draw player characters. Finally, he stated that he perceived the Virtual Boy as a novelty that should not have used the Nintendo license so prominently.

In February 2016, Tatsumi Kimishima stated that Nintendo was "looking into" virtual reality but also explained that it would take more time and effort for them to assess the technology, and in a February 2017 interview with Nikkei, he stated that the company was "studying" VR, and would add it to the Nintendo Switch once it is figured out how users can play for long durations without any issues. Nintendo introduced a VR accessory for the Switch as part of Labo, a line of player-assembled cardboard toys leveraging the console's hardware and Joy-Con controllers. In this case, the console is used as a display for the headset, similarly to VR viewers for smartphones (such as Cardboard).

Nintendo has referenced the Virtual Boy in other games, such as "Tomodachi Life"—where a trailer for the life simulation game included a scene of several Mii characters worshiping the console. In "Luigi's Mansion 3", Luigi uses a device by Professor E. Gadd known as the "Virtual Boo" to access maps and other information in-game (succeeding the use of devices referencing the Game Boy Color and first-generation Nintendo DS in previous installments). This interface is rendered in the console's red and black color scheme, while E. Gadd is shown to be optimistic that the device would "fly off the shelves".




</doc>
<doc id="21971" url="https://en.wikipedia.org/wiki?curid=21971" title="Nuclear">
Nuclear

Nuclear may refer to:







</doc>
<doc id="21974" url="https://en.wikipedia.org/wiki?curid=21974" title="NSAP address">
NSAP address

A Network Service Access Point address (NSAP address), defined in ISO/IEC 8348, is an identifying label for a Service Access Point (SAP) used in OSI networking.

These are roughly comparable to IP addresses used in the Internet Protocol; they can specify a piece of equipment connected to an Asynchronous Transfer Mode (ATM) network. A specific stream, analogous to a TCP/IP port or socket, is specified by using a Transport Service Access Point (TSAP). ATM can also use a Presentation (PSAP) and Session (SSAP) Access Point, but these may also be unspecified; this is up to the application.

NSAP addresses are allocated by the International Organization for Standardization (ISO), through a system of delegated authorities, which are generally national standards organizations. One of the schemes to generate NSAPs uses E.164 which is the addressing format describing telephone numbers.

NSAP addresses do not specify where a network terminal is located. Routing equipment must translate NSAP addresses to SNPAs (SubNetwork Point of Attachment) to route OSI packets; VCI (virtual circuit identifier) numbers are an example of a datalink layer SNPAs in ATM; when OSI packets are sent encapsulated in IP packets the IP address is considered an SNPA.

Currently SDH/SONET networks are a major part of the network infrastructure and NSAPs are used extensively. They are usually assigned by the Network Management/NOC personnel and agreed upon within an organization to be unique (to that organization and based on geographical location using country code telephone prefixes) and are required before any operational connectivity is established at the commissioning stage.

NSAP addresses are used in the following OSI-based network technologies: 

NSAP-style addresses are used in the IS-IS routing protocol.

The NSEL (Network-Selector) is a field in the NSAP address that identifies the network layer service to which a packet should be sent. This part of the address for a router will always be 0x00. In the IS-IS routing protocol, the field is sometimes referred to as the SEL field.



</doc>
<doc id="21976" url="https://en.wikipedia.org/wiki?curid=21976" title="American submarine NR-1">
American submarine NR-1

Deep Submergence Vessel "NR-1" was a unique United States Navy (USN) nuclear-powered ocean engineering and research submarine, built by the Electric Boat Division of General Dynamics at Groton, Connecticut. "NR-1" was launched on 25 January 1969, completed initial sea trials 19 August 1969, and was home-ported at Naval Submarine Base New London. "NR-1" was the smallest nuclear submarine ever put into operation. The vessel was casually known as "Nerwin" and was never officially named or commissioned. The U.S. Navy is allocated a specific number of warships by the U.S. Congress. Admiral Hyman Rickover avoided using one of those allocations, and he also wanted to avoid the oversight that a warship receives from various bureaus.

"NR-1"s missions included search, object recovery, geological survey, oceanographic research, and installation and maintenance of underwater equipment. "NR-1" had the unique capability to remain at one site and completely map or search an area with a high degree of accuracy, and this was a valuable asset on several occasions.

In the 1970s and 1980s, "NR-1" conducted numerous classified missions involving recovery of objects from the floor of the deep sea. These missions remain classified and few details have been made public. One publicly acknowledged mission in 1976 was to recover parts of an F-14 that were lost from the deck of an aircraft carrier and sank with at least one AIM-54A Phoenix air-to-air missile. The secrecy typical of USN submarine operations was heightened by Rickover's personal involvement, and he shared details of "NR-1" operations only on a need-to-know basis. Rickover envisioned building a small fleet of "NR-1" type submarines, but only one was built due to budget restrictions.

Following the loss of the Space Shuttle "Challenger" in 1986, "NR-1" was used to search for, identify, and recover critical parts of the "Challenger" craft. It could remain on the sea floor without resurfacing frequently, and was a major tool for searching deep waters. "NR-1" remained submerged and on station even when heavy weather and rough seas hit the area and forced all other search and recovery ships into port.

In October, 1994, a survey was done by the "NR-1" off the Florida straits 65 km southwest of Key West where it encountered and explored an uncharted sink hole. On 2 December 1998, an advisory committee approved the name "NR-1" for the hole.
In 1995, Robert Ballard used the "NR-1" and its support ship to explore the wreck of , the sister ship of , which sank off the coast of Greece while serving as a hospital ship during World War I.

On 25 February 2007, "NR-1" arrived in Galveston, Texas, towed by "Carolyn Chouest", in preparation for an expedition to survey the Flower Garden Banks National Marine Sanctuary and other sites in the Gulf of Mexico.

"NR-1" was deactivated on 21 November 2008 at the U.S. Navy submarine base at Groton, Connecticut, defuelled at Portsmouth Naval Shipyard in Kittery, Maine, then sent to Puget Sound Naval Shipyard to be scrapped. On 13 November 2013, the U.S. Navy announced that salvaged pieces of the sub would be put on display at the Submarine Force Library and Museum in Groton.

"NR-1" performed underwater search and recovery, oceanographic research missions, and installation and maintenance of underwater equipment to a depth of almost half a nautical mile. Its features included extending bottoming wheels, three viewing ports, exterior lighting, television and still cameras for color photographic studies, an object recovery claw, a manipulator that could be fitted with various gripping and cutting tools, and a work basket that could be used in conjunction with the manipulator to deposit or recover items in the sea. Surface vision was provided by a television periscope permanently installed on a fixed mast in her sail area.
"NR-1" had sophisticated electronics, computers, and sonar systems that aided in navigation, communications, and object location and identification. It could maneuver or hold a steady position on or close to the seabed or underwater ridges, detect and identify objects at a considerable distance, and lift objects off the ocean floor.

"NR-1" was equipped with two electric motor-driven propellers and its maneuverability was enhanced by four ducted thrusters, two forward and two aft. The vehicle had diving planes mounted on the sail, and a conventional rudder.

"NR-1" could travel submerged at approximately for long periods, limited only by consumable supplies—primarily food. It could study and map the ocean bottom, including temperature, currents, and other information for military, commercial, and scientific uses. Its nuclear propulsion provided independence from surface support ships and essentially unlimited endurance.

"NR-1"s size limited its crew comforts. The crew of about 10 men could stay at sea for as long as a month, but had no kitchen or bathing facilities. They ate frozen TV dinners, bathed once a week with a bucket of water, and burned chlorate candles to produce oxygen. The sub was so slow that it was towed to sea by a surface vessel, and so tiny that the crew felt the push and pull of the ocean's currents. "Everybody on "NR-1" got sick," said Allison J. Holifield, who commanded the sub in the mid-1970s. "It was only a matter of whether you were throwing up or not throwing up."

"NR-1" was generally towed to and from remote mission locations by an accompanying surface tender, which was also capable of conducting research in conjunction with the submarine. "NR-1"s last mother ship was MV "Carolyn Chouest", which provided towing, communications, berthing, and direct mission support for all "NR-1" operations—a versatile platform and an indispensable member of the "NR-1" deep submergence team. "NR-1" command was manned with thirty-five Navy personnel and ten civilian contractor personnel. "NR-1" carried as many as thirteen persons (crew and specialists) at one time, including three of the four assigned officers. (The operations officer rode on "Carolyn Chouest"). All personnel who crewed "NR-1" were nuclear-trained and specifically screened and interviewed by the Director, Navy Nuclear Propulsion Program.





</doc>
<doc id="21977" url="https://en.wikipedia.org/wiki?curid=21977" title="Neo Geo Pocket">
Neo Geo Pocket

The Neo Geo Pocket is a monochrome handheld game console released by SNK. It was the company's first handheld system and is part of the Neo Geo family. It debuted in Japan in late 1998 but never saw an American release, being exclusive to Japan, Asia and Europe.

The Neo Geo Pocket is considered to be an unsuccessful console. Lower than expected sales resulted in its discontinuation in 1999, and was immediately succeeded by the Neo Geo Pocket Color, a full color device allowing the system to compete more easily with the dominant Game Boy Color handheld, and which also saw an American release. Though the system enjoyed only a short life, there were some significant games released on the system such as "Samurai Shodown", and "King of Fighters R-1".


The Neo Geo Pocket is forward compatible with the majority of Neo Geo Pocket Color titles, although games for the color system will play in monochrome on a Neo Geo Pocket. Likewise, the Neo Geo Pocket Color is backward compatible and the entire Neo Geo Pocket library can be played on the color system.

Only ten monochrome games were released for the Neo Geo Pocket before it was discontinued:

After the release of the Neo Geo Pocket Color, several of these titles began receiving re-releases, updated to include color. All but King of Fighters R-1, Melon-chan's Growth Diary, Samurai Shodown and the original Neo Geo Cup 98 (the "Plus" version was updated instead) were eventually re-released for the color system.

Each video game is stored on a plastic cartridge, officially called a "Software Cartridge" by SNK. The cartridge provides the code and game data to the console's CPU. Some cartridges include a small battery with SRAM, flash memory chip, or EEPROM, which allows game data to be saved when the console is turned off. If the battery runs out in a cartridge, then the save data will be lost, however, it is possible to replace the battery with a new battery. To do this, the cartridge must be unscrewed, opened up, and the old battery would be removed and replaced. This may require desoldering the dead battery and soldering the replacement in place. SNK used round, flat watch batteries for saving information on the cartridges.

The cartridge is inserted into the console cartridge slot. If the cartridge is removed while the power is on, and the Neo Geo Pocket does not automatically reset, the game freezes; the Neo Geo Pocket may exhibit unexpected behavior, such as rows of zeros appearing on the screen, the sound remaining at the same pitch as was emitted the instant the game was pulled out, saved data may be corrupted, and hardware may be damaged. This applies to most video game consoles that use cartridges.

Even if Neo Geo Pocket Color exclusive titles like "" was bypassed by using a B/W Neo Geo Pocket model, the game would not run, and an image on the screen would inform the user that the game is only compatible with Neo Geo Pocket Color systems, similarity to if you slot a Game Boy Color exclusive game on Game Boy, Game Boy Pocket, Game Boy Light or Super Game Boy.

All cartridges used the same cartridge design, and it was smaller than the Game Boy's cartridges.


</doc>
<doc id="21978" url="https://en.wikipedia.org/wiki?curid=21978" title="Neil Kinnock">
Neil Kinnock

Neil Gordon Kinnock, Baron Kinnock (born 28 March 1942) is a British politician. A member of the Labour Party, he served as a Member of Parliament from 1970 until 1995, first for Bedwellty and then for Islwyn. He was the Leader of the Labour Party and Leader of the Opposition from 1983 until 1992.

Kinnock led the Labour Party to a surprise fourth consecutive defeat at the 1992 general election, despite the party being ahead in most opinion polls, which had predicted either a narrow Labour victory or a hung parliament. Shortly afterwards, he resigned as Leader of the Labour Party, being succeeded in the ensuing leadership election by John Smith. He left the House of Commons in 1995 to become a European Commissioner. He went on to become the Vice-President of the European Commission under Romano Prodi from 1999–2004. Until the summer of 2009, he was also Chairman of the British Council and President of Cardiff University.

Kinnock, an only child, was born in Tredegar, Wales. His father, Gordon Herbert Kinnock was a former coal miner who later worked as a labourer; and his mother Mary Kinnock (née Howells) was a district nurse. Gordon died of a heart attack in November 1971 aged 64; Mary died the following month aged 61.

In 1953, at eleven years old, Kinnock began his secondary education at Lewis School, Pengam, which he later criticised for its record on caning. He went on to the University College of South Wales and Monmouthshire in Cardiff (now Cardiff University), where he graduated in 1965 with a degree in Industrial Relations and History. The following year, Kinnock obtained a postgraduate diploma in education. Between August 1966 and May 1970, he worked as a tutor for a Workers' Educational Association (WEA).

He has been married to Glenys Kinnock since 1967. They have two children – son Stephen Kinnock (born January 1970, now a Labour MP), and daughter Rachel Kinnock (born 1971).

In June 1969, he won the Labour Party nomination for Bedwellty in South Wales, which became Islwyn for the 1983 general election.

He was first elected to the House of Commons on 18 June 1970, and became a member of the National Executive Committee of the Labour Party in October 1978. Upon his becoming an MP, his father said "Remember Neil, MP stands not just for Member of Parliament, but also for Man of Principle."

Following Labour's defeat at the 1979 general election, James Callaghan appointed Kinnock to the Shadow Cabinet as education spokesman. His ambition was noted by other MPs, and David Owen's opposition to the changes to the electoral college was thought to be motivated by the realisation that they would favour Kinnock's succession. Kinnock remained as education spokesman following the resignation of Callaghan as Leader of the Labour Party and the election of Michael Foot as his successor in late 1980.

In 1981, while still serving as Labour's education spokesman, Kinnock was alleged to have effectively scuppered Tony Benn's attempt to replace Denis Healey as Labour's Deputy Leader by first supporting the candidacy of the more traditionalist Tribunite John Silkin and then urging Silkin supporters to abstain on the second, run-off, ballot.

Kinnock was known as a left-winger, and gained prominence for his attacks on Margaret Thatcher's handling of the Falklands War in 1982, although it was in fact this conflict which saw support for the Conservative government increase, and contribute to its landslide re-election the following year.

Following Labour's landslide defeat at the 1983 general election, Michael Foot resigned as Leader of the Labour Party aged 69, and from the outset; it was expected that the much younger Kinnock would succeed him. He was finally elected as Labour Party leader on 2 October 1983, with 71% of the vote, and Roy Hattersley was elected as his deputy; their prospective partnership was considered to be a "dream ticket".

His first period as party leader between the 1983 and 1987 general elections was dominated by his struggle with the hard-left Militant tendency, then still a dominant force in the party. Kinnock was determined to move the party's political standing to a centrist position, in order to improve its chances of winning a future general election. Although Kinnock had come from the Tribune left-wing of the party, he parted company with many of his former allies after his appointment to the Shadow Cabinet.

The Labour Party was also threatened by the rise of the Social Democratic Party/Liberal Alliance, which pulled out more centrist adherents. On a broader perspective, the traditional Labour voter was disappearing in the face of de-industrialisation that the Conservative government had accepted since 1979. Kinnock focused on modernising the party, and upgrading its technical skills such as use of the media and keeping track of voters, while at the same time battling the Militants. Under his leadership, the Labour Party abandoned unpopular old positions, especially the nationalisation of certain industries, although this process was not completed until future party leader Tony Blair removed Clause IV from the party's manifesto in 1995. He stressed economic growth, which had a much broader appeal to the middle-class than the idea of redistributing wealth to benefit the poor. He accepted membership in the European Economic Community, whereas the party had pledged immediate withdrawal from it under Michael Foot. He discarded the rhetoric of class warfare.

All this meant that Kinnock had made plenty of enemies on the left-wing of the party by the time he was elected as leader, though a substantial number of former Bennites gave him strong support. He was almost immediately in serious difficulty as a result of Arthur Scargill's decision to lead his union, the National Union of Mineworkers (NUM) into a national strike (in opposition to pit closures) without a nationwide ballot. The NUM was widely regarded as the labour movement's praetorian guard and the strike convulsed the Labour movement. Kinnock supported the aim of the strike – which he dubbed the "case for coal" – but, as an MP from a mining area, was bitterly critical of the tactics employed. When heckled at a Labour Party rally for referring to the killing of David Wilkie as "an outrage", Kinnock lost his temper and accused the hecklers of "living like parasites off the struggle of the miners" and implied that Scargill had lied to the striking miners. In 1985, he made his criticisms public in a speech to Labour's conference:
In 2004, Kinnock said of Scargill, "Oh I detest him. I did then, I do now, and it's mutual. He hates me as well. And I'd much prefer to have his savage hatred than even the merest hint of friendship from that man."

The strike's defeat early in the year, and the bad publicity associated with the entryism practised by the Trotskyist Militant group were the immediate context for the 1985 Labour Party conference. Earlier in the year, left-wing councils had protested at Government restriction of their budgets by refusing to set budgets, resulting in a budget crisis in the Militant-dominated Liverpool City Council. Kinnock attacked Militant and their conduct in a speech delivered at the conference:
One Liverpool MP, Eric Heffer, a member of the NEC left the conference stage in disgust at Kinnock's comments. In June 1986, the Labour Party finally expelled the deputy leader of Liverpool council, the high-profile Militant supporter Derek Hatton, who was found guilty of "manipulating the rules of the district Labour party". By 1986, the party's position appeared to strengthen further with excellent local election results and a thorough rebranding of the party under the direction of Kinnock's director of communications Peter Mandelson, as well as seizing the Fulham seat in West London from the Conservatives at an April by-election. Labour, now sporting a continental social democratic style emblem of a rose (replacing the party's first logo, the Liberty logo), appeared to be able to run the governing Conservatives close, but Margaret Thatcher did not let Labour's makeover go unchallenged.

The Conservatives' 1986 conference was well-managed, and effectively relaunched the Conservatives as a party of radical free-market economic liberalism. Labour suffered from a persistent image of extremism, especially as Kinnock's campaign to root out Militant dragged on as figures on the hard left of the party tried to stop its progress. Opinion polls showed that voters favoured retaining the United Kingdom's nuclear weapons, (Labour's policy, supported by Kinnock, was of unilateral nuclear disarmament), and believed that the Conservatives would be better than Labour at defending the country.

In early 1987, Labour lost a by-election in Greenwich to the SDP's Rosie Barnes. As a result, Labour faced the 1987 general election in some danger of finishing third in the popular vote, with the Conservatives once again expected to secure a comfortable victory. In secret, Labour's aim was to secure second place in order to remain as Official Opposition.

Mandelson and his team had revolutionised Labour's communications – a transformation symbolised by a party election broadcast popularly known as "Kinnock: The Movie". This was directed by Hugh Hudson and featured Kinnock's 1985 conference speech, and shots of him and his wife Glenys walking on the Great Orme in Llandudno (so emphasising his appeal as a family man and associating him with images of Wales away from the coal mining communities where he grew up), and a speech to that year's Welsh Labour Party conference asking why he was the "first Kinnock in a thousand generations" to go to university.

On polling day, Labour easily took second place, but with only a 31% share of the vote to the SDP-Liberal Alliance's 22%. Labour was still more than ten percentage points behind the Conservatives, who retained a three-figure majority in the House of Commons. However, the Conservative government's majority had come down from 144 seats in 1983 to 102. Significantly, Labour had gained twenty seats at the election.

Labour won extra seats in Scotland, Wales and Northern England, but lost ground particularly in Southern England and London, where the Conservatives still dominated. The Conservatives also regained the Fulham seat which it had lost to Labour at a by-election just over a year earlier.

A few months after the general election, Kinnock gained brief attention in the United States in August 1987 when it was discovered that then-US Senator Joe Biden for Delaware (and future 47th Vice President) plagiarised one of Kinnock's speeches during his 1988 presidential campaign in a speech at a Democratic Party debate in Iowa. This led to Biden's withdrawal of his presidential campaign.

The second period of Kinnock's leadership was dominated by his drive to reform the party's policies to gain office. This began with an exercise dubbed the policy review, the most high-profile aspect of which was a series of consultations with the public known as "Labour Listens" in the autumn of 1987.

After Labour Listens, the party went on, in 1988, to produce a new statement of aims and values—meant to supplement and supplant the formulation of Clause IV of the party's constitution (though, crucially, this was not actually replaced until 1995 under the leadership of Tony Blair) and was closely modelled on Anthony Crosland's social-democratic thinking—emphasising equality rather than public ownership. At the same time, the Labour Party's commitment to unilateral nuclear disarmament was dropped, and reforms of Party Conference and the National Executive meant that local parties lost much of their ability to influence policy.

In 1988, Kinnock was challenged by Tony Benn for the party leadership. Later many identified this as a particularly low period in Kinnock's leadership — as he appeared mired in internal battles after five years of leadership with the Conservatives still dominating the scene, and being ahead in the opinion polls. In the end, though, Kinnock won a decisive victory over Benn and would soon enjoy a substantial rise in support.

The policy review — reporting in 1989 —coincided with Labour's move ahead in the polls as the poll tax row was destroying Conservative support, and Labour won big victories in local council elections as well as several parliamentary by-elections during 1989 and 1990. Labour overtook the Conservatives at the 1989 European elections, winning 40% of the vote; the first time Labour had finished in first place at a national election in fifteen years.

In December 1989, he abandoned the Labour policy on closed shops—a decision seen by many as a move away from traditional socialist policies to a more European-wide agenda, and also a move to rid the party of its image of being run by trade unions.

Kinnock was also perceived as scoring in debates over Margaret Thatcher in the Commons—previously an area in which he was seen as weak—and finally Michael Heseltine challenged Thatcher's leadership and she resigned on 28 November 1990 to be succeeded by then-Chancellor of the Exchequer, John Major. Kinnock greeted Thatcher's resignation by describing it as "very good news" and demanded an immediate general election.

Public reaction to Major's elevation was highly positive. A new Prime Minister and the fact that Kinnock was now the longest-serving current leader of a major party reduced the impact of calls for "Time for a Change". Neil Kinnock's showing in the opinion polls dipped; before Thatcher's resignation, Labour had been up to 10 points ahead of the Conservatives in the opinion polls (an Ipsos MORI poll in April 1990 had actually shown Labour as being more than 20 points ahead of the Conservatives), but many opinion polls were actually showing the Conservatives with a higher amount of support than Labour, in spite of the deepening recession.

By now Militant had finally been routed in the party, and their two MPs were expelled at the end of 1991, in addition to a number of supporters. The majority in the group were now disenchanted with entryism, and chose to function outside Labour's ranks, forming the Socialist Party.

In the three years leading up to the 1992 general election, Labour had consistently topped the opinion polls, with 1991 seeing the Conservatives (rejuvenated by the arrival of a new leader with John Major the previous November) snatch the lead off Labour more than once before Labour regained it. The rise in Conservative support came in spite of the economic recession and sharp rise in unemployment which affected Britain in 1991. Since Major's election as Leader of the Conservative Party (and becoming Prime Minister), Kinnock had spent the end of 1990 and most of 1991 putting pressure on Major to call a general election that year, but Major had held out and by the autumn he had insisted that there would be no general election in 1991.

Labour had gained four seats from the Conservatives in by-elections since the 1987 general election, having initially suffered disappointing results in some by-elections, namely a loss of the Govan constituency in Glasgow to the Scottish National Party in November 1988. However, by the end of 1991, the Conservative majority still stood at 88 seats and Labour needed to win more than ninety new seats to gain an overall majority, although there was still the hope of forming a minority or coalition government if Labour failed to win a majority.

In the run-up to the election, held on 9 April 1992, most opinion polls had suggested that the election would result in either a hung parliament or a narrow Labour majority.

At the 1992 general election, Labour made considerable progress – reducing the Conservatives' majority to just 21 seats. It came as a shock to many when the Conservatives won a majority, but the "triumphalism" perceived by some observers of a Labour Party rally in Sheffield (together with Kinnock's performance on the podium) may have helped put floating voters off. Although internal polls suggested no impact, while public polls suggested a decline in support had already occurred, most of those directly involved in the campaign believe that the rally really came to widespread attention only after the electoral defeat itself, with Kinnock himself changing his mind to a rejection of its negative impact over time.

On the day of the general election, "The Sun" newspaper ran a front page featuring Kinnock with the headline "If Kinnock wins today will the last person to leave Britain please turn out the lights". Kinnock blamed the newspaper in his resignation speech for Labour losing the election, along with other right-wing media sections who had backed the Conservatives in the run-up to the election. The following day's headline in "The Sun" was "It's The Sun Wot Won It", which Rupert Murdoch, many years later at his April 2012 appearance before the Leveson Inquiry, stated was both "tasteless and wrong" and led to the editor Kelvin MacKenzie receiving a reprimand.

The Labour-supporting "Daily Mirror" had backed Kinnock for the 1987 general election and again in 1992. Less expected was the "Financial Times" backing Kinnock at the 1992 general election.
Kinnock himself later claimed to have half-expected his defeat at the 1992 general election and proceeded to turn himself into a media personality, even hosting a chat show on BBC Wales and twice appearing on the topical panel show "Have I Got News for You" within a year of the defeat. Many years later, he returned to appear as a guest host of the programme.

Kinnock announced his resignation as Labour Party leader on 13 April 1992, ending nearly a decade in the role. John Smith, previously Shadow Chancellor, was elected on 18 July as his successor.

He remains on the Advisory Council of the Institute for Public Policy Research, which he helped set up in the 1980s.

He was an enthusiastic supporter of Ed Miliband's campaign for leadership of the Labour Party in 2010, and was reported as telling activists, when Miliband won, "We've got our party back" – although Miliband, like Kinnock, failed to lead the party back into government, and resigned after the Conservatives were re-elected with a slim majority in 2015.

In 2011, he participated in the family history TV programme "Coming Home" where he discovered hitherto unknown information about his family.

Kinnock was appointed one of the UK's two members of the European Commission, which he served first as Transport Commissioner under President Jacques Santer, in early 1995; marking the end of his 25 years in the House of Commons. This came less than a year after the death of his successor, John Smith and the election of Tony Blair as the party's new leader.

He was obliged to resign as part of the forced, collective resignation of the Commission in 1999. He was re-appointed to the Commission under new President Romano Prodi. He now became one of the Vice-Presidents of the European Commission, with responsibility for Administrative Reform and the Audit, Linguistics and Logistics Directorates General. His term of office as a Commissioner was due to expire on 30 October 2004, but was delayed owing to the withdrawal of the new Commissioners. During this second term of office on the Commission, he was responsible for introducing new staff regulations for EU officials, a significant feature of which was substantial salary cuts for everyone employed after 1 May 2004, reduced pension prospects for many others, and gradually worsening employment conditions. This made him disliked by many EU staff members, although the pressure on budgets that largely drove these changes had actually been imposed on the Commission from above by the Member States in Council.

In February 2004, it was announced that with effect from 1 November 2004, Kinnock would become head of the British Council. Coincidentally, at the same time, his son Stephen became head of the British Council branch in St. Petersburg, Russia. At the end of October, it was announced that he would become a member of the House of Lords (intending to be a working peer), when he was able to leave his EU responsibilities. In 1977, he had remained in the House of Commons, with Dennis Skinner, while other MPs walked to the Lords to hear the Queen's speech opening the new parliament. He had dismissed going to the Lords in recent interviews. Kinnock explained his change of attitude, despite the continuing presence of ninety hereditary peers and appointment by patronage, by asserting that the Lords was a good base for campaigning.

He was introduced to the House of Lords on 31 January 2005, after being created, on 28 January, Baron Kinnock, of Bedwellty in the County of Gwent. On assuming his seat, he stated; "I accepted the kind invitation to enter the House of Lords as a working peer for practical political reasons." When his peerage was first announced, he said, "It will give me the opportunity... to contribute to the national debate on issues like higher education, research, Europe and foreign policy."

His peerage meant that the Labour and Conservative parties were equal in numbers in the upper house of Parliament (subsequently the number of Labour members overtook the number of Conservative members for many years). Kinnock was a long-time critic of the House of Lords, and his acceptance of a peerage led him to be accused of hypocrisy, by Will Self, among others.

In the build up to the 1979 Welsh devolution referendum, the Labour government was in favour of devolution for Wales. Kinnock was one of just six MPs in South Wales who campaigned against devolution, with Kinnock personally backing an amendment to the Wales Act stating that devolution would require not only a simple majority, but also the backing of 40% of the entire electorate.

Kinnock has often referred to himself as a "unionist", he was controversially dismissive of Welsh identity, stating that "between the mid-sixteenth century and the mid-eighteenth century Wales had practically no history at all, and even before that it was the history of rural brigands who have been ennobled by being called princes".

Kinnock strongly opposed Brexit. Kinnock stated, “The truth is that we can either take the increasingly plain risks and costs of leaving the EU or have the stability, growth and revenues vital for crucial public services like the NHS and social care. Recognising that, we should stop Brexit to save the NHS – or, at very least, mitigate the damage by seeking European Economic Area membership.”

He is married to Glenys Kinnock, the UK's Minister for Africa and the United Nations from 2009–2010, and a Labour Member of the European Parliament (MEP) from 1994–2009. When she was made a life peer in 2009, they became one of the few couples both to hold titles in their own right. The pair met in the early 1960s whilst studying at University College, Cardiff, where they were known as "the power and the glory" (Glenys the power), and they married on 25 March 1967. Previously living together in Peterston-super-Ely, a village near the western outskirts of Cardiff, in 2008 they relocated to Tufnell Park, London, to be closer to their daughter and grandchildren.

They have a son, Stephen and a daughter, Rachel. Neil Kinnock, through his son Stephen, is also the father-in-law of Helle Thorning-Schmidt, the former Danish Prime Minister.

On 26 April 2006, Kinnock was given a six-month driving ban after being found guilty of two speeding offences along the M4 motorway, west of London.

Neil Kinnock is a Cardiff City F.C. fan and regularly attends matches. He is also a follower of rugby union and supports London Welsh RFC at club level, regularly attending Wales games.

He was portrayed by both Chris Barrie and Steve Coogan in the satirical TV programme, "Spitting Image" and by Euan Cuthbertson in the Scottish film "In Search of La Che".

Kinnock has been described as an agnostic and an atheist. Like his wife, he is a Patron of Humanists UK.



 


</doc>
<doc id="21979" url="https://en.wikipedia.org/wiki?curid=21979" title="Netscape">
Netscape

Netscape Communications Corporation (originally Mosaic Communications Corporation) was an independent American computer services company with headquarters in Mountain View, California and then Dulles, Virginia. Its Netscape web browser was once dominant but lost to Internet Explorer and other competitors in the so-called first browser war, with its market share falling from more than 90 percent in the mid-1990s to less than 1 percent in 2006. Netscape created the JavaScript programming language, the most widely used language for client-side scripting of web pages. The company also developed SSL which was used for securing online communications before its successor TLS took over.

Netscape stock traded from 1995 until 1999 when the company was acquired by AOL in a pooling-of-interests transaction ultimately worth US$10 billion. In February 1998, approximately one year prior to its acquisition by AOL, Netscape released the source code for its browser and created the Mozilla Organization to coordinate future development of its product. The Mozilla Organization rewrote the entire browser's source code based on the Gecko rendering engine, and all future Netscape releases were based on this rewritten code. When AOL scaled back its involvement with Mozilla Organization in the early 2000s, the Organization proceeded to establish the Mozilla Foundation in July 2003 to ensure its continued independence with financial and other assistance from AOL. The Gecko engine is used to power the Mozilla Foundation's Firefox browser.

Netscape's browser development continued until December 2007, when AOL announced that the company would stop supporting it by early 2008. As of 2011, AOL continued to use the Netscape brand to market a discount Internet service provider.

AOL renamed the Netscape Communications Corporation to New Aurora Corporation, and transferred the Netscape brand to themselves. AOL sold the former Netscape company to Microsoft, who in turn sold them to Facebook. The former Netscape company is currently a non-operating subsidiary of Facebook, still known as New Aurora Corporation. 
Today, Netscape is a brand name owned by Verizon Media, a subsidiary of Verizon Communications.

Netscape was the first company to attempt to capitalize on the emerging World Wide Web. It was founded under the name Mosaic Communications Corporation on April 4, 1994, the brainchild of Jim Clark who had recruited Marc Andreessen as co-founder and Kleiner Perkins as investors. The first meeting between Clark and Andreessen was never truly about a software or service like Netscape, but more about a product that was similar to Nintendo. Clark recruited other early team members from SGI and NCSA Mosaic. Jim Barksdale came on board as CEO in January 1995. Jim Clark and Marc Andreessen originally created a 20-page concept pitch for an online gaming network to Nintendo for the Nintendo 64 console, but a deal was never reached. Marc Andreessen explains, "If they had shipped a year earlier, we probably would have done that instead of Netscape."

The company's first product was the web browser, called "Mosaic Netscape 0.9," released on October 13, 1994. Within four months of its release, it had already taken three-quarters of the browser market. It became the main browser for Internet users in such a short time due to its superiority over other competition, like Mosaic. This browser was subsequently renamed Netscape Navigator, and the company took the "Netscape" name (coined by employee Greg Sands, although it was also a trademark of Cisco Systems) on November 14, 1994, to avoid trademark ownership problems with NCSA, where the initial Netscape employees had previously created the NCSA Mosaic web browser. The Mosaic Netscape web browser did not use any NCSA Mosaic code. The internal codename for the company's browser was "Mozilla", which stood for "Mosaic killer", as the company's goal was to displace NCSA Mosaic as the world's number one web browser. A cartoon Godzilla-like lizard mascot was drawn by artist-employee Dave Titus, which went well with the theme of crushing the competition. The Mozilla mascot featured prominently on Netscape's website in the company's early years. However, the need to project a more "professional" image (especially towards corporate clients) led to this being removed.
On August 9, 1995, Netscape made an extremely successful IPO. The stock was set to be offered at US$14 per share, but a last-minute decision doubled the initial offering to US$28 per share. The stock's value soared to US$75 during the first day of trading, nearly a record for first-day gain. The stock closed at US$58.25, which gave Netscape a market value of US$2.9 billion. While it was somewhat unusual for a company to go public prior to becoming profitable, Netscape's revenues had, in fact, doubled every quarter in 1995. The success of this IPO subsequently inspired the use of the term "Netscape moment" to describe a high-visibility IPO that signals the dawn of a new industry. During this period, Netscape also pursued a publicity strategy (crafted by Rosanne Siino, then head of public relations) packaging Andreessen as the company's "rock star." The events of this period ultimately landed Andreessen, barefoot, on the cover of "Time" magazine. The IPO also helped kickstart widespread investment in internet companies that created the dot-com bubble.

Netscape advertised that "the web is for everyone" and stated one of its goals was to "level the playing field" among operating systems by providing a consistent web browsing experience across them. The Netscape web browser interface was identical on any computer. Netscape later experimented with prototypes of a web-based system which would enable users to access and edit their files anywhere across a network, no matter what computer or operating system they happened to be using. This did not escape the attention of Microsoft, which viewed the commoditization of operating systems as a direct threat to its bottom line, i.e. a move from Windows to another operating system would yield a similar browsing experience thus reducing barriers to change. It is alleged that several Microsoft executives visited the Netscape campus in June 1995 to propose dividing the market (an allegation denied by Microsoft and, if true, would have breached antitrust laws), which would have allowed Microsoft to produce web browser software for Windows while leaving all other operating systems to Netscape. Netscape refused the proposition.

Microsoft released version 1.0 of Internet Explorer as a part of the Windows 95 Plus Pack add-on. According to former Spyglass developer Eric Sink, Internet Explorer was based not on NCSA Mosaic as commonly believed, but on a version of Mosaic developed at Spyglass (which itself was based upon NCSA Mosaic). Microsoft quickly released several successive versions of Internet Explorer, bundling them with Windows, never charging for them, financing their development and marketing with revenues from other areas of the company. This period of time became known as the browser wars, in which Netscape Communicator and Internet Explorer added many new features and went through many version numbers (not always in a logical fashion) in attempts to outdo each other. But Internet Explorer had the upper hand, as the amount of manpower and capital dedicated to it eventually surpassed the resources available in Netscape's entire business. By version 3.0, IE was roughly a feature-for-feature equivalent of Netscape Communicator, and by version 4.0, it was generally considered to be more stable on Windows than on the Macintosh platform. Microsoft also targeted other Netscape products with free workalikes, such as the Internet Information Server (IIS), a web server which was bundled with Windows NT.

Netscape could not compete with this strategy. In fact, it didn't attempt to. Netscape Navigator was not free to the general public until January 1998, while Internet Explorer and IIS have always been free or came bundled with an operating system and/or other applications. Meanwhile, Netscape faced increasing criticism for the bugs in its products; critics claimed that the company suffered from "featuritis" – putting a higher priority on adding new features than on making them work properly. This was particularly true with Netscape Navigator 2, which was only on the market for five months in early 1996 before being replaced by Netscape Navigator 3. The tide of public opinion, having once lauded Netscape as the David to Microsoft's Goliath, steadily turned negative, especially when Netscape experienced its first bad quarter at the end of 1997 and underwent a large round of layoffs in January 1998. Later, former Netscape executives Mike Homer and Peter Currie described the period as "hectic and crazy" and that the company was undone by factors both internal and external.

January 1998 was also the month that Netscape started the open source Mozilla project. Netscape publicly released the source code of Netscape Communicator 5.0 in the hopes that it would become a popular open source project. It placed this code under the Netscape Public License, which was similar to the GNU General Public License but allowed Netscape to continue to publish proprietary work containing the publicly released code. However, after having released the Communicator 5.0 code this way, Netscape proceeded to work on Communicator 5.0 which was focused on improving email and enterprise functionality. It eventually became clear that the Communicator 5.0 browser was too difficult to develop, and open source development was halted on this codebase. Instead, the open source development shifted to a next-generation browser built from scratch. Using the newly built Gecko layout engine, this browser had a much more modular architecture than Communicator 5.0 and was, therefore, easier to develop with a large number of programmers. It also included an XML user interface language named XUL that allowed single development of a user interface that ran on Windows, Macintosh, and Unix. The slogan for this open sourcing effort, "Free The Lizard", carried comedic sexual overtones.

The United States Department of Justice filed an antitrust case against Microsoft in May 1998. Netscape was not a plaintiff in the case, though its executives were subpoenaed and it contributed much material to the case, including the entire contents of the 'Bad Attitude' internal discussion forum. In October 1998, Netscape acquired web directory site NewHoo for the sum of US$1 million, renamed it the Open Directory Project, and released its database under an open content license.

On November 24, 1998, America Online (AOL) announced it would acquire Netscape Communications in a tax-free stock-swap valued at US$4.2 billion. During this time, Andreessen's view of Netscape changed; to him, it was no longer just a browser, intranet, extranet, or a software company, but rather an amalgamation of products and services. By the time the deal closed on March 17, 1999, it was valued at US$10 billion. This merger was ridiculed by many who believed that the two corporate cultures could not possibly mesh; one of its most prominent critics was longtime Netscape developer Jamie Zawinski. The acquisition was seen as a way for AOL to gain a bargaining chip against Microsoft, to let it become less dependent on the Internet Explorer web browser. Others believed that AOL was interested in Netcenter, or Netscape's web properties, which drew some of the highest traffic worldwide. Eventually, Netscape's server products and its Professional Services group became part of iPlanet, a joint marketing and development alliance between AOL and Sun Microsystems. On November 14, 2000, AOL released Netscape 6, based on the Mozilla 0.6 source code. (Version 5 was skipped.) Unfortunately, Mozilla 0.6 was far from being stable yet, and so the effect of Netscape 6 was to further drive people away from the Netscape brand. It was not until August 2001 that Netscape 6.1 appeared, based on Mozilla 0.9.2 which was significantly more robust. A year later came Netscape 7.0, based on the Mozilla 1.0 core.

During the acquisition of Netscape by AOL, joint development and marketing of Netscape software products would occur through the Sun-Netscape Alliance. The software, in the newly branded iPlanet, included "messaging and calendar, collaboration, web, application, directory, and certificate servers", as well as "production-ready applications for e-commerce, including commerce exchange, procurement, selling, and billing." In March 2002, when the alliance was ended, "iPlanet became a division of Sun... Sun retained the intellectual property rights for all products and the engineering"

On July 15, 2003, Time Warner (formerly AOL Time Warner) disbanded Netscape. Most of the programmers were laid-off, and the Netscape logo was removed from the building. However, the Netscape 7.2 web browser (developed in-house rather than with Netscape staff, with some work outsourced to Sun's Beijing development center) was released by AOL on August 18, 2004.

On October 12, 2004, the popular developer website Netscape DevEdge was shut down by AOL. DevEdge was an important resource for Internet-related technologies, maintaining definitive documentation on the Netscape browser, documentation on associated technologies like HTML and JavaScript, and popular articles written by industry and technology leaders such as Danny Goodman. Some content from DevEdge has been republished at the Mozilla website.

After the Sun acquisition by Oracle in January 2010, Oracle continued to sell iPlanet branded applications, which originated from Netscape. Applications include Oracle iPlanet Web Server and Oracle iPlanet Web Proxy Server.

The Netscape brand name continued to be used extensively. The company once again had its own programming staff devoted to the development and support for the series of web browsers. Additionally, Netscape also maintained the Propeller web portal, which was a popular social-news site, similar to Digg, which was given a new look in June 2006. AOL marketed a discount ISP service under the Netscape brand name.

A new version of the Netscape browser, Netscape Navigator 9, based on Firefox 2, was released in October 2007. It featured a green and grey interface. In November 2007, IE had 77.4% of the browser market, Firefox 16.0%, and Netscape 0.6%, according to Net Applications, an Internet metrics firm. On December 28, 2007, AOL announced that on February 1, 2008 it would drop support for the Netscape web browser and would no longer develop new releases. The date was later extended to March 1 to allow a major security update and to add a tool to assist users in migrating to other browsers. These additional features were included in the final version of Netscape Navigator 9 (version 9.0.0.6), released on February 20, 2008.

Netscape Navigator was Netscape's web browser from versions 1.0–4.8. The first beta versions were released in 1994 and were called Mosaic and later Mosaic Netscape. Then, a legal challenge from the National Center for Supercomputing Applications (makers of NCSA Mosaic), which many of Netscape's founders used to develop, led to the name Netscape Navigator. The company's name also changed from Mosaic Communications Corporation to Netscape Communications Corporation.

The browser was easily the most advanced available and so was an instant success, becoming a market leader while still in beta. Netscape's feature-count and market share continued to grow rapidly after version 1.0 was released. Version 2.0 added a full email reader called Netscape Mail, thus transforming Netscape from a single-purpose web browser to an Internet suite. The main distinguishing feature of the email client was its ability to display HTML email. During this period, the entire suite was called Netscape Navigator.

Version 3.0 of Netscape (the first beta was codenamed "Atlas") was the first to face any serious competition in the form of Microsoft Internet Explorer 3.0. But Netscape remained the most popular browser at that time.

Netscape also released a Gold version of Navigator 3.0 that incorporated WYSIWYG editing with drag and drop between web editor and email components.

Netscape 4 addressed the problem of Netscape Navigator being used as both the name of the suite and the browser contained within it by renaming the suite to Netscape Communicator. After five preview releases in 1996–1997, Netscape released the final version of Netscape Communicator in June 1997. This version, more or less based on Netscape Navigator 3 Code, updated and added new features. The new suite was successful, despite increasing competition from Internet Explorer (IE) 4.0 and problems with the outdated browser core. IE was slow and unstable on the Mac platform until version 4.5. Despite this, Apple entered into an agreement with Microsoft to make IE the default browser on new Mac OS installations, a further blow to Netscape's prestige. The Communicator suite was made up of Netscape Navigator, Netscape Mail & Newsgroups, Netscape Address Book and Netscape Composer (an HTML editor).

On January 22, 1998, Netscape Communications Corporation announced that all future versions of its software would be available free of charge and developed by an open source community, Mozilla. Netscape Communicator 5.0 was announced (codenamed "Gromit"). However, its release was greatly delayed, and meanwhile, there were newer versions of Internet Explorer, starting with version 4. These had more features than the old Netscape version, including better support of HTML 4, CSS, DOM, and ECMAScript; eventually, the more advanced Internet Explorer 5.0 became the market leader.

In October 1998, Netscape Communicator 4.5 was released. It featured various functionality improvements, especially in the Mail and Newsgroups component, but did not update the browser core, whose functionality was essentially identical to that of version 4.08. One month later, Netscape Communications Corporation was bought by AOL. In November, work on Netscape 5.0 was canceled in favor of developing a completely new program from scratch.

In 1998, an informal group called the Mozilla Organization was formed and largely funded by Netscape (the vast majority of programmers working on the code were paid by Netscape) to coordinate the development of Netscape 5 (codenamed "Gromit"), which would be based on the Communicator source code. However, the aging Communicator code proved difficult to work with and the decision was taken to scrap Netscape 5 and re-write the source code. The re-written source code was in the form of the Mozilla web browser, on which, with a few additions, Netscape 6 was based.

This decision meant that Netscape's next major version was severely delayed. In the meantime, Netscape was taken over by AOL who, acting under pressure from the Web Standards Project, forced its new division to release Netscape 6.0 in 2000. The suite again consisted of Netscape Navigator and the other Communicator components, with the addition of a built-in AOL Instant Messenger client, Netscape Instant Messenger. However, it was clear that Netscape 6 was not yet ready for release and it flopped badly. It was based on Mozilla 0.6, which was not ready to be used by the general public yet due to many serious bugs that would cause it to crash often or render web pages slowly. Later versions of Netscape 6 were much-improved (especially 6.2.x was regarded as a good release), but the browser still struggled to make an impact on a disappointed community.

Netscape 7.0 (based on Mozilla 1.0.1) was released in August 2002 as a direct continuation of Netscape 6 with very similar components. It picked up a few users, but was still very much a minority browser. It did, however, come with the popular Radio@Netscape Internet radio client. AOL had decided to deactivate Mozilla's popup-blocker functionality in Netscape 7.0, which created an outrage in the community. AOL reversed the decision and allowed Netscape to reinstate the popup-blocker for Netscape 7.01. Netscape also introduced a new AOL-free-version (without the usual AOL add-ons) of the browser suite. Netscape 7.1 (codenamed "Buffy" and based on Mozilla 1.4) was released in June 2003.

In 2003, AOL closed down its Netscape division and laid-off or reassigned all of Netscape's employees. Mozilla.org continued, however, as the independent Mozilla Foundation, taking on many of Netscape's ex-employees. AOL continued to develop Netscape in-house (with help from Sun's Beijing development center), but, due to there being no staff committed to it, improvements were minimal. One year later, in August 2004, the last version based on Mozilla was released: Netscape 7.2, based on Mozilla 1.7.2.

After an official poll posted on Netscape's community support board in late 2006, speculation arose of the Netscape 7 series of suites being fully supported and updated by Netscape's in-house development team. This was not to be.

Between 2005 and 2007, Netscape's releases became known as "Netscape Browser". AOL chose to base Netscape Browser on the relatively successful Mozilla Firefox, a re-written version of Mozilla produced by the Mozilla Foundation. This release is not a full Internet suite as before, but is solely a web browser.

Other controversial decisions include the browser only being released for Microsoft Windows and featuring both the Gecko rendering engine of previous releases and the Trident engine used in Internet Explorer, and switching between them based on a "compatibility list" that came with the browser. This effectively exposed users to the security vulnerabilities in both and resulted in a completely different user experience based on which site they were on. Examples are handling of right-to-left or bi-directional text, user interface widgets, bugs and web standards violations in Trident, etc. On top of this, Netscape Browser 8 even broke Internet Explorer's ability to open XML files by damaging a Windows Registry key, and would do so every time it was opened, even if the user fixed it manually.

AOL's acquisition of Netscape Communications in November 1998 made it less of a surprise when the company laid off the Netscape team and outsourced development to Mercurial Communications. Netscape Browser 8.1.3 was released on April 2, 2007, and included general bug fixes identified in versions 8.0–8.1.2

Netscape Navigator 9's features were said to include newsfeed support and become more integrated with the Propeller Internet portal, alongside more enhanced methods of discussion, submission and voting on web pages. It also sees the browser return to multi-platform support across Windows, Linux and Mac OS X. Like Netscape version 8.x, the new release was based upon the popular Mozilla Firefox (version 2.0), and supposedly had full support of all Firefox add-ons and plugins, some of which Netscape was already providing. Also for the first time since 2004, the browser was produced in-house with its own programming staff. A beta of the program was first released on June 5, 2007. The final version was released on October 15, 2007.

AOL officially announced that support for Netscape Navigator would end on March 1, 2008, and recommended that its users download either the Flock or Firefox browsers, both of which were based on the same technology.

The decision met mixed reactions from communities, with many arguing that the termination of product support is significantly belated. Internet security site "Security Watch" stated that a trend of infrequent security updates for AOL's Netscape caused the browser to become a "security liability", specifically the 2005–2007 versions, Netscape Browser 8. Asa Dotzler, one of Firefox's original bug testers, greeted the news with "good riddance" in his blog post, but praised the various members of the Netscape team over the years for enabling the creation of Mozilla in 1998. Others protested and petitioned AOL to continue providing vital security fixes to unknowing or loyal users of its software, as well as protection of a well-known brand.

On June 11, 2007, Netscape announced Netscape Mercury, a standalone email and news client that was to accompany Navigator 9. Mercury was based on Mozilla Thunderbird. The product was later renamed Netscape Messenger 9, and an alpha version was released. In December 2007, AOL announced it was canceling Netscape's development of Messenger 9 as well as Navigator 9.

Netscape's initial product line consisted of:

Netscape's later products included:

Between June 2006 and September 2007, AOL operated Netscape's website as social news website similar to Digg. The format did not do well as traffic dropped 55.1 percent between November 2006 and August 2007. In September 2007, AOL reverted Netscape's website to a traditional news portal, and rebranded the social news portal as "Propeller", moving the site to the domain "propeller.com." AOL shut down the Propeller website on October 1, 2010.

Netscape operated a search engine, Netscape Search, which now redirects to AOL Search (which itself now merely serves Bing (formerly Google) search results). Another version of Netscape Search was incorporated into Propeller.

Netscape also operates a number of country-specific Netscape portals, including Netscape Canada among others. The portal of Netscape Germany was shut down in June 2008.

The Netscape Blog was written by Netscape employees discussing the latest on Netscape products and services. Netscape NewsQuake (formerly "Netscape Reports") is Netscape's news and opinion blog, including video clips and discussions. As of January 2012, no new posts have been made on either of these blogs since August 2008.

Netscape created the JavaScript web page scripting language. It also pioneered the development of push technology, which effectively allowed websites to send regular updates of information (weather, stock updates, package tracking, etc.) directly to a user's desktop (aka "webtop"); Netscape's implementation of this was named Netcaster. However, businesses quickly recognized the use of push technology to deliver ads to users that annoyed them, so Netcaster was short-lived.

Netscape was notable for its cross-platform efforts. Its client software continued to be made available for Windows (3.1, 95, 98, NT), Macintosh, Linux, OS/2, BeOS, and many versions of Unix including DEC, Sun Solaris, BSDI, IRIX, IBM AIX, and HP-UX. Its server software generally was only available for Unix and Windows NT, though some of its servers were made available on Linux, and a version of Netscape FastTrack Server was made available for Windows 95/98. Today, most of Netscape's server offerings live on as the Sun Java System, formerly under the Sun ONE branding. Although Netscape Browser 8 was Windows only, multi-platform support exists in the Netscape Navigator 9 series of browsers.

Netscape ISP is a dial-up Internet service once offered at US$9.95 per month. The company serves web pages in a compressed format to increase effective speeds up to 1300 kbit/s (average 500 kbit/s). The Internet service provider is now run by Verizon under the Netscape brand. The low-cost ISP was officially launched on January 8, 2004. Its main competitor is NetZero. Netscape ISP is no longer actively marketed, but for a time its advertising was aimed at a younger demographic, e.g., college students, and people just out of school, as an affordable way to gain access to the Internet.

Netscape drove much traffic from various links included in the browser menus to its web properties. Some say it was very late to leverage this traffic for what would become the start of the major online portal wars. When it did, Netcenter, the new name for its site entered the race with Yahoo!, Infoseek, and MSN, which Google would only join years later.

The original Netscape.com was discontinued in June 2006, replaced by the site that would eventually become Propeller.com. Two continuations of the original Netscape.com portal are available; Compuserve.com, the Web site of Compuserve, and ISP.Netscape.com, the website for Netscape's dial-up discount ISP service, continue to use the Netscape.com layout as it was before June 2006. Of the two, only the latter explicitly uses the Netscape branding.

Netscape.com is currently an AOL Netscape-branded mirror duplicate of the AOL.com portal with the URL, replacing the former social news website in September 2007. The social news site moved to the Propeller.com domain, where it stayed until ending operations in October 2010. It features facilities such as news, sports, horoscopes, dating, movies, music and more. The change has come to much criticism amongst many site users, because the site has effectively become an AOL clone, and simply re-directs to regional AOL portals in some areas across the globe. Netscape's exclusive features, such as the Netscape Blog, Netscape NewsQuake, Netscape Navigator, My Netscape and Netscape Community pages, are less accessible from the AOL Netscape designed portal and in some countries not accessible at all without providing a full URL or completing an Internet search. The new AOL Netscape site was originally previewed in August 2007 before moving the existing site in September 2007.

Netscape.co.uk now redirects to AOL Search, with no Netscape branding at all.

DMOZ (from directory.mozilla.org, its original domain name, also known as the Open Directory Project or ODP), was a multilingual open content directory of World Wide Web links owned by Netscape that was constructed and maintained by a community of volunteer editors. It closed in 2017.

Netscape also has a wide variety of community-based forums within Netscape Forum Center, including its browser's community support board. To post on the forums, users must possess an AOL Screenname account in which to sign in, referred to within the site as the "Netscape Network". The same service is also available through Compuserve Forum Center.





</doc>
<doc id="21980" url="https://en.wikipedia.org/wiki?curid=21980" title="Newfoundland and Labrador">
Newfoundland and Labrador

Newfoundland and Labrador () is the easternmost province of Canada. Situated in the country's Atlantic region, it is composed of the insular region of Newfoundland and the continental region of Labrador to the northwest, with a combined area of . In 2018, the province's population was estimated at 525,073. About 92% of the province's population lives on the island of Newfoundland (and its neighbouring smaller islands), of whom more than half live on the Avalon Peninsula.

The province is Canada's most linguistically homogeneous, with 97.0% of residents reporting English (Newfoundland English) as their mother tongue in the 2016 census. Historically, Newfoundland was also home to unique varieties of French and Irish, as well as the extinct Beothuk language. In Labrador, the indigenous languages Innu-aimun and Inuktitut are also spoken.

Newfoundland and Labrador's capital and largest city, St. John's, is Canada's 20th-largest census metropolitan area and is home to almost 40 per cent of the province's population. St. John's is the seat of government, home to the House of Assembly of Newfoundland and Labrador and to the highest court in the jurisdiction, the Newfoundland and Labrador Court of Appeal.

A former colony and then dominion of the United Kingdom, Newfoundland gave up its independence in 1933, following significant economic distress caused by the Great Depression and the aftermath of Newfoundland's participation in World War I. It became the tenth province to enter Confederation on 31 March 1949, as "Newfoundland". On 6 December 2001, an amendment was made to the Constitution of Canada to change the province's name to Newfoundland and Labrador.
The name "New founde lande" was uttered by King Henry VII about the land explored by the Cabots. In Portuguese it is , which literally means "new land" which is also the French name for the Province's island region (). The name "Terra Nova" is in wide use on the island (e.g. Terra Nova National Park). The influence of early Portuguese exploration is also reflected in the name of Labrador, which derives from the surname of the Portuguese navigator .

Labrador's name in the Inuttitut/Inuktitut language (spoken in Nunatsiavut) is (ᓄᓇᑦᓱᐊᒃ), meaning "the big land" (a common English nickname for Labrador). Newfoundland's Inuttitut/Inuktitut name is (ᐃᒃᑲᕈᒥᒃᓗᐊᒃ) meaning "place of many shoals".

Newfoundland and Labrador is the most easterly province in Canada, and is at the north-eastern corner of North America. The Strait of Belle Isle separates the province into two geographical parts: Labrador, which is a large area of mainland Canada, and Newfoundland, an island in the Atlantic Ocean. The province also includes over 7,000 tiny islands.

Newfoundland is roughly triangular. Each side is about long, and its area is . Newfoundland and its neighbouring small islands (excluding French possessions) have an area of . Newfoundland extends between latitudes 46°36′N and 51°38′N.

Labrador is roughly triangular in shape: the western part of its border with Quebec is the drainage divide of the Labrador Peninsula. Lands drained by rivers that flow into the Atlantic Ocean are part of Labrador, and the rest belongs to Quebec. Most of Labrador's southern boundary with Quebec follows the 52nd parallel of latitude. Labrador's extreme northern tip, at 60°22′N, shares a short border with Nunavut. Labrador's area (including associated small islands) is . Together, Newfoundland and Labrador make up 4.06% of Canada's area, with a total area of .

Labrador is the easternmost part of the Canadian Shield, a vast area of ancient metamorphic rock comprising much of northeastern North America. Colliding tectonic plates have shaped much of the geology of Newfoundland. Gros Morne National Park has a reputation as an outstanding example of tectonics at work, and as such has been designated a World Heritage Site. The Long Range Mountains on Newfoundland's west coast are the northeasternmost extension of the Appalachian Mountains.

The north-south extent of the province (46°36′N to 60°22′N), prevalent westerly winds, cold ocean currents and local factors such as mountains and coastline combine to create the various climates of the province.

Most of Newfoundland has a humid continental climate (Dfb under the Köppen climate classification system): cool summer subtype. Newfoundland and Labrador has a wide range of climates and weather, due to its geography. The island of Newfoundland spans 5 degrees of latitude, comparable to the Great Lakes.
The province has been divided into six climate types, but broadly Newfoundland has a cool summer subtype of a humid continental climate, which is greatly influenced by the sea since no part of the island is more than 100 km (62 mi) from the ocean. Northern Labrador is classified as a polar tundra climate, southern Labrador has a subarctic climate.

Monthly average temperatures, rainfall and snowfall for four places are shown in the attached graphs. St. John's represents the east coast, Gander the interior of the island, Corner Brook the west coast of the island and Wabush the interior of Labrador. Climate data for 56 places in the province is available from Environment Canada.

The data for the graphs is the average over thirty years. Error bars on the temperature graph indicate the range of daytime highs and night time lows. Snowfall is the total amount that fell during the month, not the amount accumulated on the ground. This distinction is particularly important for St. John's, where a heavy snowfall can be followed by rain, so no snow remains on the ground.

Surface water temperatures on the Atlantic side reach a summer average of inshore and offshore to winter lows of inshore and offshore. Sea temperatures on the west coast are warmer than Atlantic side by 1 to 3 °C (1 to 5 °F). The sea keeps winter temperatures slightly higher and summer temperatures a little lower on the coast than inland. The maritime climate produces more variable weather, ample precipitation in a variety of forms, greater humidity, lower visibility, more clouds, less sunshine, and higher winds than a continental climate.

Human habitation in Newfoundland and Labrador can be traced back about 9,000 years. The Maritime Archaic peoples were groups of Archaic cultures of sea-mammal hunters in the subarctic. They prospered along the Atlantic Coast of North America from about 7000 BC to 1500 BC. Their settlements included longhouses and boat-topped temporary or seasonal houses. They engaged in long-distance trade, using as currency white chert, a rock quarried from northern Labrador to Maine. The southern branch of these people was established on the north peninsula of Newfoundland by 5,000 years ago. The Maritime Archaic period is best known from a mortuary site in Newfoundland at Port au Choix.

The Maritime Archaic peoples were gradually displaced by people of the Dorset culture (Late Paleo-Eskimo) who also occupied Port au Choix. The number of their sites discovered on Newfoundland indicates they may have been the most numerous group of Aboriginal people to live there. They thrived from about 2000 BC to AD 800. Many of their sites were on exposed headlands and outer islands. They were more oriented to the sea than earlier peoples, and had developed sleds and boats similar to kayaks. They burned seal blubber in soapstone lamps.

Many of these sites, such as Port au Choix, recently excavated by Memorial archaeologist, Priscilla Renouf, are quite large and show evidence of a long-term commitment to place. Renouf has excavated huge amounts of harp seal bones at Port au Choix, indicating that this place was a prime location for the hunting of these animals.

The people of the Dorset Culture (800 BC – AD 1500) were highly adapted to a cold climate, and much of their food came from hunting sea mammals through holes in the ice. The massive decline in sea ice during the Medieval Warm Period would have had a devastating impact upon their way of life.
The appearance of the Beothuk culture is believed to be the most recent cultural manifestation of peoples who first migrated from Labrador to Newfoundland around 1 AD. The Inuit, found mostly in Labrador, are the descendants of what anthropologists call the Thule people, who emerged from western Alaska around AD 1000 and spread eastwards across the High Arctic, reaching Labrador around 1300–1500. Researchers believe the Dorset culture lacked the dogs, larger weapons and other technologies that gave the expanding Inuit people an advantage. Over time, groups started to focus on resources available to them locally.

The inhabitants eventually organized themselves into small bands of a few families, grouped into larger tribes and chieftainships. The Innu are the inhabitants of an area they refer to as "Nitassinan", i.e. most of what is now referred to as northeastern Quebec and Labrador. Their subsistence activities were historically centred on hunting and trapping caribou, deer and small game. Coastal clans also practiced agriculture, fished and managed maple sugar bush. The Innu engaged in tribal warfare along the coast of Labrador with the Inuit groups that had large populations.

The Mi'kmaq of southern Newfoundland spent most of their time on the shores harvesting seafood; during the winter they would move inland to the woods to hunt. Over time, the Mi'kmaq and Innu divided their lands into traditional "districts". Each district was independently governed and had a district chief and a council. The council members were band chiefs, elders and other worthy community leaders. In addition to the district councils, the Mi'kmaq tribes also had (have) a Grand Council or "Santé Mawiómi", which according to oral tradition was formed before 1600.

By the time European contact with Newfoundland began in the early 16th century, the Beothuk were the only indigenous group living permanently on the island. Unlike other groups in the Northeastern area of the Americas, the Beothuk never established sustained trading relations with European settlers. Instead, their trading interactions were sporadic, and they largely attempted to avoid contact in order to preserve their culture. The establishment of English fishing operations on the outer coastline of the island, and their later expansion into bays and inlets, cut off access for the Beothuk to their traditional sources of food.

In the 18th century, as the Beothuk were driven further inland by these encroachments, violence between Beothuk and settlers escalated, with each retaliating against the other in their competition for resources. By the early 19th century, violence, starvation, and exposure to tuberculosis had decimated the Beothuk population, and they were extinct by 1829.

Geneticists have suggested some Icelanders may carry Beothuk DNA, which has been passed down matrilineally over the centuries. This suggests that when the Vikings abandoned their colonization of Newfoundland around 1000 AD, they might have brought back Beothuk women to Europe.

The oldest confirmed accounts of European contact date from a thousand years ago as described in the Viking (Norse) Icelandic Sagas. Around the year 1001, the sagas refer to Leif Ericson landing in three places to the west, the first two being Helluland (possibly Baffin Island) and Markland (possibly Labrador). Leif's third landing was at a place he called Vinland (possibly Newfoundland). Archaeological evidence of a Norse settlement was found in L'Anse aux Meadows, Newfoundland, which was declared a World Heritage site by UNESCO in 1978.

There are several other unconfirmed accounts of European discovery and exploration, one tale by men from the Channel Islands being blown off course in the late 15th century into a strange land full of fish, and another from Portuguese maps that depict the Terra do Bacalhau, or land of codfish, west of the Azores. The earliest, though, is the Voyage of Saint Brendan, the fantastical account of an Irish monk who made a sea voyage in the early 6th century. While the story became a part of myth and legend, some historians believe it is based on fact.

In 1496 John Cabot obtained a charter from English King Henry VII to "sail to all parts, countries and seas of the East, the West and of the North, under our banner and ensign and to set up our banner on any new-found-land" and on 24 June 1497, landed in Cape Bonavista. Historians disagree on whether Cabot landed in Nova Scotia in 1497 or in Newfoundland, or possibly Maine, if he landed at all, but the governments of Canada and the United Kingdom recognise Bonavista as being Cabot's "official" landing place. In 1499 and 1500, Portuguese mariners João Fernandes Lavrador and Pêro de Barcelos explored and mapped the coast, the former's name appearing as "Labrador" on topographical maps of the period.

Based on the Treaty of Tordesillas, the Portuguese Crown claimed it had territorial rights in the area John Cabot visited in 1497 and 1498. Subsequently, in 1501 and 1502 the Corte-Real brothers, Miguel and Gaspar, explored Newfoundland and Labrador, claiming them as part of the Portuguese Empire. In 1506, king Manuel I of Portugal created taxes for the cod fisheries in Newfoundland waters. João Álvares Fagundes and Pêro de Barcelos established seasonal fishing outposts in Newfoundland and Nova Scotia around 1521, and older Portuguese settlements may have existed. Sir Humphrey Gilbert, provided with letters patent from Queen Elizabeth I, landed in St John's in August 1583, and formally took possession of the island.

Sometime before 1563 Basque fishermen, who had been fishing cod shoals off Newfoundland's coasts since the beginning of the sixteenth century, founded Plaisance (today Placentia), a seasonal haven which French fishermen later used. In the Newfoundland will of the Basque seaman Domingo de Luca, dated 1563 and now in an archive in Spain, he asks "that my body be buried in this port of Plazençia in the place where those who die here are usually buried". This will is the oldest known civil document written in Canada.

Twenty years later, in 1583, Newfoundland became England's first possession in North America and one of the earliest permanent English colonies in the New World when Sir Humphrey Gilbert claimed it for Elizabeth I. European fishing boats had visited Newfoundland continuously since Cabot's second voyage in 1498 and seasonal fishing camps had existed for a century prior. Fishing boats originated from Basque, England, France, and Portugal. However, this changed during the initial stages of Anglo-Spanish War, when Bernard Drake led a devastating raid on the Spanish and Portuguese fisheries in 1585. This provided an opportunity to secure the island and led to the appointment of Proprietary Governors to establish colonial settlements on the island from 1610 to 1728. John Guy became governor of the first settlement at Cuper's Cove. Other settlements included Bristol's Hope, Renews, New Cambriol, South Falkland and Avalon (which became a province in 1623). The first governor given jurisdiction over all of Newfoundland was Sir David Kirke in 1638.

Explorers quickly realized the waters around Newfoundland had the best fishing in the North Atlantic. By 1620, 300 fishing boats worked the Grand Banks, employing some 10,000 sailors; many continuing to come from the Basque Country, Normandy, or Brittany. They dried and salted cod on the coast and sold it to Spain and Portugal. Heavy investment by Sir George Calvert, 1st Baron Baltimore, in the 1620s in wharves, warehouses, and fishing stations failed to pay off. French raids hurt the business, and the weather was terrible, so he redirected his attention to his other colony in Maryland. After Calvert left, small-scale entrepreneurs such as Sir David Kirke made good use of the facilities. Kirke became the first governor of Newfoundland in 1638. A triangular trade with New England, the West Indies, and Europe gave Newfoundland an important economic role. By the 1670s there were 1,700 permanent residents and another 4,500 in the summer months.

In 1655 France appointed a governor in Plaisance (Placentia), the former Basque fishing settlement, thus starting a formal French colonization period in Newfoundland as well as a period of periodic war and unrest between England and France in the region. The Mi'kmaq, as allies of the French, were amenable to limited French settlement in their midst and fought alongside them against the English. English attacks on Placentia provoked retaliation by New France explorer Pierre Le Moyne d'Iberville who during King William's War in the 1690s destroyed nearly every English settlement on the island. The entire population of the English colony was either killed, captured for ransom, or sentenced to expulsion to England, with the exception of those who withstood the attack at Carbonear Island and those in the then remote Bonavista.

After France lost political control of the area after the Siege of Port Royal in 1710, the Mí'kmaq engaged in warfare with the British throughout Dummer's War (1722–1725), King George's War (1744–1748), Father Le Loutre's War (1749–1755) and the French and Indian War (1754–1763). The French colonization period lasted until the Treaty of Utrecht of 1713, which ended the War of the Spanish Succession: France ceded to the British its claims to Newfoundland (including its claims to the shores of Hudson Bay) and to the French possessions in Acadia. Afterward, under the supervision of the last French governor, the French population of Plaisance moved to Île Royale (now Cape Breton Island), part of Acadia which remained then under French control.

In the Treaty of Utrecht (1713), France had acknowledged British ownership of the island. However, in the Seven Years' War (1756–1763), control of Newfoundland once again became a major source of conflict between Britain, France and Spain who all pressed for a share in the valuable fishery there. Britain's victories around the globe led William Pitt to insist nobody other than Britain should have access to Newfoundland. The Battle of Signal Hill took place in Newfoundland in 1762 when a French force landed and tried to occupy the island, only to be repulsed by the British.

From 1763 to 1767 James Cook made a detailed survey of the coasts of Newfoundland and southern Labrador while commander of . (The following year, 1768, Cook began his first circumnavigation of the world.) In 1796 a Franco-Spanish expedition again succeeded in raiding the coasts of Newfoundland and Labrador, destroying many of the settlements.

By the Treaty of Utrecht (1713), French fishermen gained the right to land and cure fish on the "French Shore" on the western coast. (They had a permanent base on nearby St. Pierre and Miquelon islands; the French gave up their French Shore rights in 1904.) In 1783 the British signed the Treaty of Paris with the United States that gave American fishermen similar rights along the coast. These rights were reaffirmed by treaties in 1818, 1854 and 1871 and confirmed by arbitration in 1910.

In 1854 the British government established Newfoundland's responsible government.
In 1855, Philip Francis Little, a native of Prince Edward Island, won a parliamentary majority over Hugh Hoyles and the Conservatives. Little formed the first Newfoundland administration (1855-1858). Newfoundland rejected confederation with Canada in the 1869 general election. Prime Minister of Canada Sir John Thompson came very close to negotiating Newfoundland's entry into Confederation in 1892.

Newfoundland remained a colony until acquiring Dominion status in 1907. A dominion constituted a self-governing state of the British Empire or British Commonwealth and the Dominion of Newfoundland was relatively autonomous from British rule.

Newfoundland's own regiment, the 1st Newfoundland Regiment, fought in the First World War. On July 1, 1916, nearly the entire regiment was wiped out at Beaumont-Hamel on the first day on the Somme. The regiment went on to serve with distinction in several subsequent battles, earning the prefix "Royal". Despite people's pride in the accomplishments of the regiment, the Dominion's war debt due to the regiment and the cost of maintaining a trans-island railway led to increased and ultimately unsustainable government debt in the post-war era.

Since the early 1800s, Newfoundland and Quebec (or Lower Canada) had been in a border dispute over the Labrador region. In 1927, however, the British government ruled the area known as modern-day Labrador was to be considered part of the Dominion of Newfoundland.

Due to Newfoundland's high debt load arising from World War I and construction of the Newfoundland Railway, and decreasing revenue due to the collapse of fish prices, the dominion legislature voted itself out of existence in 1933 in exchange for loan guarantees by the Crown and a promise it would be re-established. On February 16, 1934, the Commission of Government was sworn in, ending 79 years of responsible government. The Commission consisted of seven persons appointed by the British government. For 15 years, no elections took place, and no legislature was convened.

When prosperity returned with World War II, agitation began to end the Commission and reinstate responsible government. Instead, the British government created the National Convention in 1946, reflecting the efforts toward self-determination that arose in Europe following the war. The Convention, chaired by Judge Cyril J. Fox, consisted of 45 elected members from across the dominion and was formally tasked with advising on the future of Newfoundland.

Several motions were made by Joey Smallwood (a convention member who later served as the first provincial premier of Newfoundland) to examine joining Canada by sending a delegation to Ottawa. The first motion was defeated, although the Convention later decided to send delegations to both London and Ottawa to explore alternatives. In January 1948, the National Convention voted against putting Confederation onto the referendum 29 to 16, but the British, which controlled the National Convention and the subsequent referendum, overruled this vote. Those who supported Confederation were extremely disappointed with the recommendations of the National Convention and organized a petition, signed by more than 50,000 Newfoundlanders, demanding confederation with Canada be placed before the people in the upcoming referendum. As most historians agree, the British government keenly wanted Confederation on the ballot and ensured it would be.

Three main factions actively campaigned during the lead-up to the referenda. Smallwood led the Confederate Association (CA), advocating entry into the Canadian Confederation. They campaigned through a newspaper known as "The Confederate". The Responsible Government League (RGL), led by Peter Cashin, advocated an independent Newfoundland with a return to responsible government. Their newspaper was "The Independent". A third, the smaller Economic Union Party (EUP), led by Chesley Crosbie, advocated closer economic ties with the United States. Though a 1947 poll found 80% of Newfoundland residents wanting to become Americans, the EUP failed to gain much attention and merged with the RGL after the first referendum.

The first referendum took place on June 3, 1948; 44.6% of people voted for responsible government, 41.1% voted for confederation with Canada, while 14.3% voted for the Commission of Government. Since none of the choices had gained over 50%, a second referendum with only the two more popular choices was held on July 22, 1948. The official outcome of that referendum was 52.3% for confederation with Canada and 47.7% for responsible (independent) government. After the referendum, the British governor named a seven-man delegation to negotiate Canada's offer on behalf of Newfoundland. After six of the delegation signed, the British government passed the British North America Act, 1949 through Parliament. Newfoundland officially joined Canada at midnight on March 31, 1949.

As documents in British and Canadian archives became available in the 1980s, it became clear Canada and the United Kingdom wanted Newfoundland to join Canada. Some have charged it was a conspiracy to manoeuvre Newfoundland into Confederation in exchange for forgiveness of Britain's war debt and for other considerations. Yet, most historians who have examined the government documents have concluded that, while Britain engineered the inclusion of a Confederation option in the referendum, Newfoundlanders made the final decision themselves, if by a narrow margin.

Following the referendum, there was a rumour the referendum had been narrowly won by the "responsible government" side, but the result had been fixed by the British governor. Shortly after the referendum, several boxes of ballots from St. John's were burned by order of Herman William Quinton, one of only two commissioners who supported confederation. Some have argued that independent oversight of the vote tallying was lacking, though the process was supervised by respected Corner Brook Magistrate Nehemiah Short, who had also overseen elections to the National Convention.

Newfoundland and Labrador has a population of 519,716, more than half of whom live on the Avalon Peninsula of Newfoundland, site of the capital and historical early settlement. Since 2006, the population of the province has started to increase for the first time since the early 1990s. In the 2006 census the population of the province decreased by 1.5% compared to 2001, and stood at 505,469. But, by the 2011 census, the population had risen by 1.8%.

The largest single religious denomination by number of adherents according to the 2011 National Household Survey was the Roman Catholic Church, at 35.8% of the province's population (181,590 members). The major Protestant denominations made up 57.3% of the population, with the largest groups being the Anglican Church of Canada at 25.1% of the total population (127,255 members), the United Church of Canada at 15.5% (78,380 members), and the Pentecostal churches at 6.5% (33,195 members), with other Protestant denominations in much smaller numbers. Non-Christians constituted only 6.8% of the population, with the majority of those respondents indicating "no religious affiliation" (6.2% of the population).

According to the 2001 Canadian census, the largest ethnic group in Newfoundland and Labrador is English (39.4%), followed by Irish (19.7%), Scots (6.0%), French (5.5%), and First Nations (3.2%). While half of all respondents also identified their ethnicity as "Canadian," 38% report their ethnicity as "Newfoundlander" in a 2003 Statistics Canada Ethnic Diversity Survey.

More than 100,000 Newfoundlanders have applied for membership in the Qalipu Mi'kmaq First Nation Band, equivalent to one-fifth of the total population.

Newfoundland English is any of several accents and dialects of English found in the province of Newfoundland and Labrador. Most of these differ substantially from the English commonly spoken elsewhere in neighbouring Canada and the North Atlantic. Many Newfoundland dialects are similar to the West Country dialects of the West Country in England, particularly the city of Bristol and counties Cornwall, Devon, Dorset, Hampshire and Somerset, while others resemble dialects of Ireland's southeast, particularly Waterford, Wexford, Kilkenny and Cork. Still others blend elements of both, and there is also a Scottish influence on the dialects. While the Scots came in smaller numbers than the English and Irish, they had a large influence on Newfoundland society.

Newfoundland was also the only place outside Europe to have its own distinct name in Irish: "Talamh an Éisc", which means 'land of the fish'. The Irish language is now extinct in Newfoundland. Scots Gaelic was also once spoken in the southwest of Newfoundland, following the settlement there, from the middle of the 19th century, of small numbers of Gaelic-speaking Scots from Cape Breton, Nova Scotia. Some 150 years later, the language has not entirely disappeared, although it has no fluent speakers.

A vestigial community of French speakers exists on Newfoundland's Port au Port Peninsula; a remnant of the "French Shore" along the island's west coast.

Several aboriginal languages are spoken in the Province, representing the Algonquian (Mi'kmaq and Innu) and Eskimo-Aleut (Inuktitut) language families.

Languages of the population - mother tongue (2011)

For many years, Newfoundland and Labrador had experienced a depressed economy. Following the collapse of the cod fishery during the early 1990s, the province suffered record unemployment rates and the population decreased by roughly 60,000. Due to a major energy and resources boom, the provincial economy has had a major turnaround since the turn of the 21st century. Unemployment rates decreased, the population stabilized and had moderate growth. The province has gained record surpluses, which has rid it of its status as a "have not" province.

Economic growth, gross domestic product (GDP), exports and employment resumed in 2010, after suffering the impacts of the late-2000s recession. In 2010, total capital investment in the province grew to C$6.2 billion, an increase of 23.0% compared to 2009. 2010 GDP reached $28.1 billion, compared to $25.0 billion in 2009.

Service industries accounted for the largest share of GDP, especially financial services, health care and public administration. Other significant industries are mining, oil production and manufacturing. The total labour force in 2018 was 261,400 people. Per capita GDP in 2017 was $62,573, higher than the national average and third only to Alberta and Saskatchewan out of Canadian provinces.

Mines in Labrador, the iron ore mine at Wabush/Labrador City, and the nickel mine in Voisey's Bay produced a total of $3.3 billion worth of ore in 2010. A mine at Duck Pond (30 km (18 mi) south of the now-closed mine at Buchans), started producing copper, zinc, silver and gold in 2007, and prospecting for new ore bodies continues. Mining accounted for 3.5% of the provincial GDP in 2006. The province produces 55% of Canada's total iron ore. Quarries producing dimension stone such as slate and granite, account for less than $10 million worth of material per year. 

Oil production from offshore oil platforms on the Hibernia, White Rose and Terra Nova oil fields on the Grand Banks was of , which contributed to more than 15 per cent of the province's GDP in 2006. Total production from the Hibernia field from 1997 to 2006 was with an estimated value of $36 billion. This will increase with the inclusion of the latest project, Hebron. Remaining reserves are estimated at almost as of December 31, 2006. Exploration for new reserves is ongoing. On June 16, 2009, provincial premier Danny Williams announced a tentative agreement to expand the Hibernia oil field. The government negotiated a 10-per-cent equity stake in the Hibernia South expansion, which will add an estimated $10 billion to Newfoundland and Labrador's treasury.

Newsprint is produced by one paper mill in Corner Brook with a capacity of 420,000 tonnes (462,000 tons) per year. The value of newsprint exports varies greatly from year to year, depending on the global market price. Lumber is produced by numerous mills in Newfoundland. Apart from seafood processing, paper manufacture and oil refining, manufacturing in the province consists of smaller industries producing food, brewing and other beverage production.

The fishing industry remains an important part of the provincial economy, employing roughly 20,000 and contributing over $440 million to the GDP. The combined harvest of fish such as cod, haddock, halibut, herring and mackerel was 92,961 tonnes in 2017, with a combined value of $141 million. Shellfish, such as crab, shrimp and clams, accounted for 101,922 tonnes in the same year, yielding $634 million. The value of products from the seal hunt was $1.9 million. Aquaculture is a new industry for the province, which in 2015 produced over 22,000 tonnes of Atlantic salmon, mussels and steelhead trout worth over $161 million. Oysters production is also expected to start in the province.

Agriculture in Newfoundland is limited to areas south of St. John's, Cormack, Wooddale, areas near Musgravetown and in the Codroy Valley. Potatoes, rutabagas, turnips, carrots and cabbage are grown for local consumption. Poultry and eggs are also produced. Wild blueberries, partridgeberries (lingonberries) and bakeapples (cloudberries) are harvested commercially and used in jams and wine making. Dairy production is another huge part of the Newfoundland Agriculture Industry.

Tourism is also a significant contributor to the province's economy. In 2006 nearly 500,000 non-resident tourists visited Newfoundland and Labrador, spending an estimated $366 million. In 2017, non-resident tourists spent an estimated $575 million. Tourism is most popular throughout the months of June–September, the warmest months of the year with the longest hours of daylight.

Newfoundland and Labrador is governed by a parliamentary government within the construct of constitutional monarchy; the monarchy in Newfoundland and Labrador is the foundation of the executive, legislative, and judicial branches. The sovereign is Queen Elizabeth II, who also serves as head of state of 15 other Commonwealth countries, each of Canada's nine other provinces and the Canadian federal realm; she resides in the United Kingdom. The Queen's representative in Newfoundland and Labrador is the Lieutenant Governor of Newfoundland and Labrador, presently Judy Foote.

The direct participation of the royal and viceroyal figures in governance is limited; in practice, their use of the executive powers is directed by the Executive Council, a committee of ministers of the Crown responsible to the unicameral, elected House of Assembly. The Council is chosen and headed by the Premier of Newfoundland and Labrador, the head of government. After each general election, the lieutenant governor will usually appoint as premier the leader of the political party that has a majority or plurality in the House of Assembly. The leader of the party with the second-most seats usually becomes the Leader of Her Majesty's Loyal Opposition and is part of an adversarial parliamentary system intended to keep the government in check.

Each of the 40 Members of the House of Assembly (MHA) is elected by simple plurality in an electoral district. General elections must be called by the lieutenant governor on the second Tuesday in October four years after the previous election, or may be called earlier, on the advice of the premier, should the government lose a confidence vote in the legislature. Traditionally, politics in the province have been dominated by both the Liberal Party and the Progressive Conservative Party. However, in the 2011 provincial election the New Democratic Party, which had only ever attained minor success, had a major breakthrough and placed second in the popular vote behind the Progressive Conservatives.

Before 1950, the visual arts were a minor aspect of Newfoundland cultural life, compared to the performing arts such as music or theatre. Until about 1900, most art was the work of visiting artists, who included members of the Group of Seven, Rockwell Kent, and Eliot O'Hara. Artists such as Newfoundland-born Maurice Cullen and Robert Pilot travelled to Europe to study art in prominent ateliers. 

By the turn of the 20th century, amateur art was made by people living and working in the province. These artists included J.W. Hayward and his son Thomas B. Hayward, Agnes Marian Ayre, and Harold B. Goodridge, the last of whom worked on a number of mural commissions, notably one for the lobby of the Confederation Building in St. John's. Local art societies became prominent in the 1940s, particularly The Art Students Club, which opened in 1940.

After Newfoundland and Labrador joined Canada in 1949, government grants fostered a supportive environment for visual artists, primarily painters. The visual arts of the province developed significantly in the second half of the century, with the return of young Newfoundland artists whom had studied abroad. Amongst the first were Rae Perlin, who studied at the Art Students League in New York, and Helen Parsons Shepherd and her husband Reginald Shepherd, who both graduated from the Ontario College of Art. The Shepherds established the province's first art school, the Newfoundland Academy of Art, in a home in downtown St. John's.

Newfoundland-born painters Christopher Pratt and Mary Pratt (painter) returned to the province in 1961 to work at the newly established Memorial University Art Gallery as its first curator, later transitioning to painting full-time in Salmonier. Wesleyville's David Blackwood graduated from the Ontario College of Art in the early 1960s and achieved acclaim with his images of Newfoundland culture and history, though he no longer resides in the province. Newfoundland-born artist Gerald Squires returned in 1969.

The creation of The Memorial University Extension Services and St. Michael's Printshop in the 1960s and 1970s attracted a number of visual artists to the province to teach and create art. Similarly, the school in Hibb's Hole (now Hibb's Cove), established by painter George Noseworthy, brought professional artists such as Anne Meredith Barry to the province. A notable artist during this period is Marlene Creates.

From 1980 to present, opportunities for artists continued to develop, as galleries such as the Art Gallery of Newfoundland and Labrador (which later became The Rooms Provincial Art Gallery), the Resource Centre for the Arts, and Eastern Edge were established. Fine arts education programs were established at post-secondary institutions such as Sir Wilfred Grenfell College in Corner Brook, the Western Community College (now College of the North Atlantic) in Stephenville, and the Anna Templeton Centre in St. John's.

Newfoundland and Labrador's arts community is recognized nationally and internationally. The creation of Fogo Island Arts in 2008 on Fogo Island created a residency-based contemporary art program for artists, filmmakers, writers, musicians, curators, designers, and thinkers. In 2013 and 2015, the province was represented at the Venice Biennale as Official Collateral Projects. In 2015, Philippa Jones became the first Newfoundland and Labrador artist to be included in the National Gallery of Canada contemporary art biennial. Other notable contemporary artists who have received national and international attention include Will Gill, Kym Greeley, Ned Pratt and Peter Wilkins.

As of 2011, a study documented approximately 1,200 artists, representing 0.47% of the province's labour force.

Newfoundland and Labrador has a folk musical heritage based on the Irish, English and Scottish traditions that were brought to its shores centuries ago. Though similar in its Celtic influence to neighbouring Nova Scotia and Prince Edward Island, Newfoundland and Labrador are more Irish than Scottish, and have more elements imported from English and French music than those provinces. Much of the region's music focuses on the strong seafaring tradition in the area, and includes sea shanties and other sailing songs. Some modern traditional musicians include Great Big Sea, The Ennis Sisters, The Dardanelles, Ron Hynes, and Jim Payne.

The Newfoundland Symphony Orchestra began in St John's in 1962 as a 20-piece string orchestra known as the St. John's Orchestra. Principals from this form a string quartet which performs regularly. A school of music at Memorial University schedules a variety of concerts and has a chamber orchestra and jazz band. Two members of its faculty, Nancy Dahn on violin and Timothy Steeves on piano, perform as Duo Concertante and are responsible for establishing an annual music festival in August, the Tuckamore Festival. Both the school of music and Opera on the Avalon produce operatic works. Memorial's Research Centre for the Study of Music, Media, and Place, houses Memorial's graduate program in ethnomusicology. A leading institution for research in ethnomusicology, the Centre offers academic lectures, scholarly residencies, conferences, symposia, and outreach activities to the province on music and culture.

The pre-confederation and current provincial anthem is the "Ode to Newfoundland", written by British colonial governor Sir Charles Cavendish Boyle in 1902 during his administration of Newfoundland (1901 to 1904). It was adopted as the official Newfoundland anthem on May 20, 1904. In 1980, the province re-adopted the song as an official provincial anthem, making this the only province in Canada to officially adopt an anthem. "The Ode to Newfoundland" is still sung at public events in Newfoundland and Labrador.

Margaret Duley (1894–1968) was Newfoundland's first novelist to gain an international audience. Her works include "The Eyes of the Gull" (1936), "Cold Pastoral" (1939) and "Highway to Valour" (1941). Subsequent novelists include Harold Horwood, author of "Tomorrow Will Be Sunday" (1966) and "White Eskimo" (1972), and Percy Janes, author of "House of Hate" (1970).

Michael Crummey's debut novel, "River Thieves" (2001), became a Canadian bestseller. Other novels include "The Wreckage" (2005) and "Galore" (2009).

Wayne Johnston's fiction deals primarily with the province of Newfoundland and Labrador, often in a historical setting. His novels include "The Story of Bobby O'Malley", "The Time of Their Lives", and "The Divine Ryans", which was made into a movie. "The Colony of Unrequited Dreams" was a historical portrayal of Newfoundland politician Joey Smallwood. Other novels include "The Colony of Unrequited Dreams", and "The Navigator of New York" (2002).

Lisa Moore's first two books, "Degrees of Nakedness" (1995) and "Open" (2002), are short-story collections. Her first novel, "Alligator" (2005), is set in St. John's and incorporates her Newfoundland heritage. "February" tells the story of Helen O'Mara, who lost her husband Cal on the oil rig "Ocean Ranger", which sank off the coast of Newfoundland during a Valentine's Day storm in 1982.

Other contemporary novelists include Joel Thomas Hynes, author of "We'll All Be Burnt in Our Beds Some Night" (2017), Jessica Grant author of "Come Thou Tortoise" (2009), and Kenneth J. Harvey, author of "The Town That Forgot How to Breathe" (2003), "Inside" (2006), and "Blackstrap Hawco" (2008).

The earliest works of poetry in British North America, mainly written by visitors and targeted at a European audience, described the new territories in optimistic terms. One of the first works was Robert Hayman's "Quodlibets", a collection of verses composed in Newfoundland and published in 1628.

After World War II, Newfoundland poet E. J. Pratt described the struggle to make a living from the sea in poems about maritime life and the history of Canada. In 1923, his first commercial poetry collection, "Newfoundland Verse", was released. It is frequently archaic in diction, and reflects a pietistic and late-Romantic lyrical sensibility. The collection has humorous and sympathetic portraits of Newfoundland characters, and creates an elegiac mood in poems concerning sea tragedies or Great War losses. With illustrations by Group of Seven member Frederick Varley, "Newfoundland Verse" proved to be Pratt's "breakthrough collection." He went on to publish 18 more books of poetry in his lifetime. "Recognition came with the narrative poems "The Witches' Brew" (1925), "Titans" (1926), and "The Roosevelt and the Antinoe" (1930), and though he published a substantial body of lyric verse, it is as a narrative poet that Pratt is remembered." Pratt's poetry "frequently reflects his Newfoundland background, though specific references to it appear in relatively few poems, mostly in "Newfoundland Verse"", says The Canadian Encyclopedia. "But the sea and maritime life are central to many of his poems, for example, "The Cachalot" (1926), which describes duels between a whale and its foes, a giant squid and a whaling ship and crew.

Amongst more recent poets are Tom Dawe, Al Pittman, Mary Dalton, Agnes Walsh, Patrick Warner and John Steffler. Canadian poet Don McKay has resided in St John's in recent years.

"1967 marked the opening of the St. John's Arts and Culture Centre and the first all-Canadian Dominion Drama Festival. Playwrights across Canada began writing, and this explosion was also felt in Newfoundland and Labrador. Subregional festivals saw Newfoundland plays compete - "Wreaker"s by Cassie Brown, "Tomorrow Will Be Sunday" by Tom Cahill, and "Holdin' Ground" by Ted Russell. Cahill's play went on to receive top honours and a performance at Expo 67 in Montreal. Joining Brown and Cahill in the seventies were Michael Cook and Al Pittman, both prolific writers".

Newfoundland and Labrador's present provincial flag, designed by Newfoundland artist Christopher Pratt, was officially adopted by the legislature on May 28, 1980, and first flown on "Discovery Day" that year.

The blue is meant to represent the sea, the white represents snow and ice, the red represents the efforts and struggles of the people, and the gold represents the confidence of Newfoundlanders and Labradorians. The blue triangles are a tribute to the Union Flag, and represent the British heritage of the province. The two red triangles represent Labrador (the mainland portion of the province) and the island. In Pratt's words, the golden arrow points towards a "brighter future".

What has commonly but mistakenly been called the Newfoundland tricolour "Pink, White and Green"(sic) is the flag of the Catholic Church affiliated Star of the Sea Association (SOSA). It originated in the late nineteenth century and enjoyed popularity among people who were under the impression that it was the Native Flag of Newfoundland which was created before 1852 by the Newfoundland Natives'Society. The true Native Flag (red-white-green tricolour) was widely flown into the late nineteenth century. Neither tricolour was ever adopted by the Newfoundland government.

A 1976 article reported the tricolour flag was created in 1843 by then Roman Catholic Bishop of Newfoundland, Michael Anthony Fleming. The colours were intended to represent the symbolic union of Newfoundland's historically dominant ethnic/religious groups: English, Scots and Irish. Though popular, there is no historical evidence to support this legend. Recent scholarship suggests the green-white-pink flag was first used in the late 1870s or early 1880s by the Roman Catholic "Star of the Sea Association" a fishermen's aid and benefit organization established by the Catholic Church in 1871. It resembled the unofficial flag of Ireland. The tricolour flag remained relatively unknown outside of St. John's and the Avalon peninsula until the growth of the tourist industry since the late 20th century. It has been used as an emblem on items in gift shops in St. John's and other towns. Some tourists assume it is the Irish flag.

The "Pink, White and Green"(sic) has been adopted by some residents as a symbol of ties with Irish heritage and as a political statement. Many of the province's Protestants, who make up nearly 60% of the province's total population, may not identify with this heritage. At the same time, many of the province's Catholics, approximately 37% of the total population (with at least 22% of the population claiming Irish ancestry), think the current provincial flag does not satisfactorily represent them. But, a government-sponsored poll in 2005 revealed that 75% of Newfoundlanders rejected adoption of the Tricolour flag as the province's official flag.

Labrador has its own unofficial flag, created in 1973 by Mike Martin, former Member of the Legislative Assembly for Labrador South.

Newfoundland and Labrador has a somewhat different sports culture from the rest of Canada, owing in part to its long history separate from the rest of Canada and under British rule. Ice hockey, however, remains popular; a minor league professional team called the Newfoundland Growlers of the ECHL plays at the Mile One Centre in St. John's since the 2018–19 season. The area had an intermittent American Hockey League presence with the St. John's Maple Leafs then St. John's IceCaps until 2017, and the Newfoundland Senior Hockey League has teams around the island. Since the departure of the St. John's Fog Devils in 2008, Newfoundland and Labrador is the only province in Canada to not have a team in the major junior Canadian Hockey League (should one ever join it would be placed in the QMJHL, which hosted the Fog Devils and has jurisdiction over Atlantic Canada).

Association football (soccer) and rugby union are both more popular in Newfoundland and Labrador than the rest of Canada in general. Soccer is hosted at King George V Park, a 6,000-seat stadium built as Newfoundland's national stadium during the time as an independent dominion. Swilers Rugby Park is home of the Swilers RFC rugby union club, as well as the Atlantic Rock, one of the four regional teams in the Canadian Rugby Championship. Other sports facilities in Newfoundland and Labrador include Pepsi Centre, an indoor arena in Corner Brook; and St. Patrick's Park, a baseball park in St. John's.

Gridiron football, be it either American or Canadian, is almost nonexistent; it is the only Canadian province other than Prince Edward Island to have never hosted a Canadian Football League or Canadian Interuniversity Sport game, and it was not until 2013 the province saw its first amateur teams form.

Cricket was once a popular sport. The earliest mention is in the "Newfoundland Mercantile Journal", Thursday September 16, 1824, indicating the St. John's Cricket Club was an established club at this time. The St. John's Cricket club was one of the first cricket clubs in North America. Other centres were at Harbour Grace, Twillingate, and Trinity. The heyday of the game was the late nineteenth and early twentieth century, at which time there was league in St. John's, as well as an interschool tournament. John Shannon Munn is Newfoundland's most famous cricketer, having represented Oxford University. After the first World War, cricket declined in popularity and was replaced by soccer and baseball. However, with the arrival of immigrants from the Indian subcontinent,
cricket is once again gaining interest in the province.

Within the province, the Newfoundland and Labrador Department of Transportation and Works operates or sponsors 15 automobile, passenger and freight ferry routes which connect various communities along the province's significant coastline.

A regular passenger and car ferry service, lasting about 90 minutes, crosses the Strait of Belle Isle, connecting the province's island of Newfoundland with the region of Labrador on the mainland. The ferry "MV Apollo" travels from St. Barbe, Newfoundland on the Great Northern Peninsula to the port town of Blanc-Sablon, Quebec, located on the provincial border and beside the town of L'Anse-au-Clair, Labrador. The "MV Sir Robert Bond" once provided seasonal ferry service between Lewisporte on the island and the towns of Cartwright and Happy Valley–Goose Bay in Labrador, but has not run since the completion of the Trans-Labrador Highway in 2010, allowing access from Blanc-Sablon, Quebec, to major parts of Labrador. Several smaller ferries connect numerous other coastal towns and offshore island communities around the island of Newfoundland and up the Labrador coast as far north as Nain.

Inter-provincial ferry services are provided by Marine Atlantic, a federal Crown corporation which operates auto-passenger ferries from North Sydney, Nova Scotia, to the towns of Port aux Basques and Argentia on the southern coast of Newfoundland island.

The St. John's International Airport YYT and the Gander International Airport YQX are the only airports in the province that are part of the National Airports System. The St. John's International Airport handles nearly 1,200,000 passengers a year making it the busiest airport in the province and the eleventh busiest airport in Canada. The airport is currently undergoing a major expansion of the terminal building which is scheduled to be complete in 2021. The Deer Lake Airport YDF handles over 300,000 passengers a year.

The Newfoundland Railway operated on the island of Newfoundland from 1898 to 1988. With a total track length of , it was the longest narrow-gauge railway system in North America.




</doc>
<doc id="21981" url="https://en.wikipedia.org/wiki?curid=21981" title="New Oxford American Dictionary">
New Oxford American Dictionary

The New Oxford American Dictionary (NOAD) is a single-volume dictionary of American English compiled by American editors at the Oxford University Press.

"NOAD" is based upon the "New Oxford Dictionary of English" ("NODE"), published in the United Kingdom in 1998, although with substantial editing, additional entries, and the inclusion of illustrations. It is based on a corpus linguistics analysis of Oxford's 200 million word database of contemporary American English.

"NOAD" includes a diacritical respelling scheme to convey pronunciations, as opposed to the Gimson phonemic IPA system that is used in "NODE".

Published in September 2001, the first edition was edited by Elizabeth J. Jewell and Frank Abate.

Published in May 2005, the second edition was edited by Erin McKean. The edition added nearly 3,000 new words, senses, and phrases. It was in a large format, with 2096 pages, and was 8½" by 11" in size. It included a CD-ROM with the full text of the dictionary for Palm OS devices.

Since 2005 Apple Inc.'s Mac OS X operating system has come bundled with a dictionary application and widget which credits as its source "Oxford American Dictionaries", and contains the full text of "NOAD2". The Amazon Kindle reading device also uses "NOAD" as its built-in dictionary, along with a choice for the "Oxford Dictionary of English".

Oxford University Press published "NOAD2" in electronic form in 2006 at the OxfordAmericanDictionary.com, and in 2010, along with the "Oxford Dictionary of English", as part of Oxford Dictionaries Online.

Published in August 2010, the third edition was edited by Angus Stevenson and Christine A. Lindberg. This edition includes over 2,000 new words, senses, and phrases, and over 1,000(1225) illustrations; hundreds of new and revised explanatory notes, new "Word Trends" feature charts usage for rapidly changing words and phrases.

The dictionary includes an entry for the word "esquivalience", which it defines as meaning "the willful avoidance of one's official responsibilities". This is a fictitious entry, intended to protect the copyright of the publication. The entry was invented by Christine Lindberg, one of the editors of the "NOAD".

With the publication of the second edition, a rumor circulated that the dictionary contained a fictitious entry in the letter 'e'. "New Yorker" contributing editor Henry Alford combed the section, and discussed several unusual entries he found with a group of American lexicographers. Most found "esquivalience" to be the most likely candidate, and when Alford approached "NOAD" editor in chief Erin McKean she confirmed it was a fake entry, which had been present since the first edition, in order to protect the copyright of the CD-ROM edition. Of the word, she said "its inherent fakeitude is fairly obvious".

The fake entry apparently ensnared dictionary.com, which included an entry for it (that has since been removed) which it attributed to "Webster's New Millennium Dictionary", both of which are owned by the private company Lexico. Possibly due to its licensing of Oxford dictionaries, Google Dictionary included the word, listing three meanings and giving usage examples.






</doc>
<doc id="21983" url="https://en.wikipedia.org/wiki?curid=21983" title="New Latin">
New Latin

New Latin (also called Neo-Latin or Modern Latin)
was a revival in the use of Latin in original, scholarly, and scientific works between 1375 and 1900s. Modern scholarly and technical nomenclature, such as in zoological and botanical taxonomy and international scientific vocabulary, draws extensively from New Latin vocabulary. In such use, New Latin is subject to new word formation. As a language for full expression in prose or poetry, however, it is often distinguished from its successor, Contemporary Latin.

Classicists use the term "Neo-Latin" to describe the Latin that developed in Renaissance Italy as a result of renewed interest in classical civilization in the 14th and 15th centuries.

Neo-Latin also describes the use of the Latin language for any purpose, scientific or literary, during and after the Renaissance. The beginning of the period cannot be precisely identified; however, the spread of secular education, the acceptance of humanistic literary norms, and the wide availability of Latin texts following the invention of printing, mark the transition to a new era of scholarship at the end of the 15th century. The end of the New Latin period is likewise indeterminate, but Latin as a regular vehicle of communicating ideas became rare after the first few decades of the 19th century, and by 1900 it survived primarily in international scientific vocabulary and taxonomy. The term "New Latin" came into widespread use towards the end of the 1890s among linguists and scientists.

New Latin was, at least in its early days, an international language used throughout Catholic and Protestant Europe, as well as in the colonies of the major European powers. This area consisted of most of Europe, including Central Europe and Scandinavia; its southern border was the Mediterranean Sea, with the division more or less corresponding to the modern eastern borders of Finland, the Baltic states, Poland, Slovakia, Hungary and Croatia.

Russia's acquisition of Kiev in the later 17th century introduced the study of Latin to Russia. Nevertheless, the use of Latin in Orthodox eastern Europe did not reach high levels due to their strong cultural links to the cultural heritage of Ancient Greece and Byzantium, as well as Greek and Old Church Slavonic languages.

Though Latin and New Latin are considered dead (having no native speakers), large parts of their vocabulary have seeped into English and several Germanic languages. In the case of English, about 60% of the lexicon can trace its origin to Latin, thus many English speakers can recognize New Latin terms with relative ease as cognates are quite common.

New Latin was inaugurated by the triumph of the humanist reform of Latin education, led by such writers as Erasmus, More, and Colet. Medieval Latin had been the practical working language of the Roman Catholic Church, taught throughout Europe to aspiring clerics and refined in the medieval universities. It was a flexible language, full of neologisms and often composed without reference to the grammar or style of classical (usually pre-Christian) authors. The humanist reformers sought both to purify Latin grammar and style, and to make Latin applicable to concerns beyond the ecclesiastical, creating a body of Latin literature outside the bounds of the Church. Attempts at reforming Latin use occurred sporadically throughout the period, becoming most successful in the mid-to-late 19th century.

The Protestant Reformation (1520–1580), though it removed Latin from the liturgies of the churches of Northern Europe, may have advanced the cause of the new secular Latin. The period during and after the Reformation, coinciding with the growth of printed literature, saw the growth of an immense body of New Latin literature, on all kinds of secular as well as religious subjects.

The heyday of New Latin was its first two centuries (1500–1700), when in the continuation of the Medieval Latin tradition, it served as the lingua franca of science, education, and to some degree diplomacy in Europe. Classic works such as Newton's Principia Mathematica (1687) were written in the language. Throughout this period, Latin was a universal school subject, and indeed, the pre-eminent subject for elementary education in most of Europe and other places of the world that shared its culture. All universities required Latin proficiency (obtained in local grammar schools) to obtain admittance as a student. Latin was an official language of Poland—recognised and widely used between the 9th and 18th centuries, commonly used in foreign relations and popular as a second language among some of the nobility.

Through most of the 17th century, Latin was also supreme as an international language of diplomatic correspondence, used in negotiations between nations and the writing of treaties, e.g. the peace treaties of Osnabrück and Münster (1648). As an auxiliary language to the local vernaculars, New Latin appeared in a wide variety of documents, ecclesiastical, legal, diplomatic, academic, and scientific. While a text written in English, French, or Spanish at this time might be understood by a significant cross section of the learned, only a Latin text could be certain of finding someone to interpret it anywhere between Lisbon and Helsinki.

As late as the 1720s, Latin was still used conversationally, and was serviceable as an international auxiliary language between people of different countries who had no other language in common. For instance, the Hanoverian king George I of Great Britain (reigned 1714–1727), who had no command of spoken English, communicated in Latin with his Prime Minister Robert Walpole, who knew neither German nor French.

By about 1700, the growing movement for the use of national languages (already found earlier in literature and the Protestant religious movement) had reached academia, and an example of the transition is Newton's writing career, which began in New Latin and ended in English (e.g. "Opticks", 1704). A much earlier example is Galileo c. 1600, some of whose scientific writings were in Latin, some in Italian, the latter to reach a wider audience. By contrast, while German philosopher Christian Wolff (1679–1754) popularized German as a language of scholarly instruction and research, and wrote some works in German, he continued to write primarily in Latin, so that his works could more easily reach an international audience (e.g., "Philosophia moralis," 1750–53).

Likewise, in the early 18th century, French replaced Latin as a diplomatic language, due to the commanding presence in Europe of the France of Louis XIV. At the same time, some (like King Frederick William I of Prussia) were dismissing Latin as a useless accomplishment, unfit for a man of practical affairs. The last international treaty to be written in Latin was the Treaty of Vienna in 1738; after the War of the Austrian Succession (1740–48) international diplomacy was conducted predominantly in French.

A diminishing audience combined with diminishing production of Latin texts pushed Latin into a declining spiral from which it has not recovered. As it was gradually abandoned by various fields, and as less written material appeared in it, there was less of a practical reason for anyone to bother to learn Latin; as fewer people knew Latin, there was less reason for material to be written in the language. Latin came to be viewed as esoteric, irrelevant, and too difficult. As languages like French, Italian, German, and English became more widely known, use of a 'difficult' auxiliary language seemed unnecessary—while the argument that Latin could expand readership beyond a single nation was fatally weakened if, in fact, Latin readers did not compose a majority of the intended audience.

As the 18th century progressed, the extensive literature in Latin being produced at the beginning slowly contracted. By 1800 Latin publications were far outnumbered, and often outclassed, by writings in the modern languages as impact of Industrial Revolution. Latin literature lasted longest in very specific fields (e.g. botany and zoology) where it had acquired a technical character, and where a literature available only to a small number of learned individuals could remain viable. By the end of the 19th century, Latin in some instances functioned less as a language than as a code capable of concise and exact expression, as for instance in physicians' prescriptions, or in a botanist's description of a specimen. In other fields (e.g. anatomy or law) where Latin had been widely used, it survived in technical phrases and terminology. The perpetuation of Ecclesiastical Latin in the Roman Catholic Church through the 20th century can be considered a special case of the technicalizing of Latin, and the narrowing of its use to an elite class of readers.

By 1900, creative Latin composition, for purely artistic purposes, had become rare. Authors such as Arthur Rimbaud and Max Beerbohm wrote Latin verse, but these texts were either school exercises or occasional pieces. The last survivals of New Latin to convey non-technical information appear in the use of Latin to cloak passages and expressions deemed too indecent (in the 19th century) to be read by children, the lower classes, or (most) women. Such passages appear in translations of foreign texts and in works on folklore, anthropology, and psychology, e.g. Krafft-Ebing's "Psychopathia Sexualis" (1886).

Latin as a language held a place of educational pre-eminence until the second half of the 19th century. At that point its value was increasingly questioned; in the 20th century, educational philosophies such as that of John Dewey dismissed its relevance. At the same time, the philological study of Latin appeared to show that the traditional methods and materials for teaching Latin were dangerously out of date and ineffective.

In secular academic use, however, New Latin declined sharply and then continuously after about 1700. Although Latin texts continued to be written throughout the 18th and into the 19th century, their number and their scope diminished over time. By 1900, very few new texts were being created in Latin for practical purposes, and the production of Latin texts had become little more than a hobby for Latin enthusiasts.

Around the beginning of the 19th century came a renewed emphasis on the study of Classical Latin as the spoken language of the Romans of the 1st centuries BC and AD. This new emphasis, similar to that of the Humanists but based on broader linguistic, historical, and critical studies of Latin literature, led to the exclusion of Neo-Latin literature from academic studies in schools and universities (except for advanced historical language studies); to the abandonment of New Latin neologisms; and to an increasing interest in the reconstructed Classical pronunciation, which displaced the several regional pronunciations in Europe in the early 20th century.

Coincident with these changes in Latin instruction, and to some degree motivating them, came a concern about lack of Latin proficiency among students. Latin had already lost its privileged role as the core subject of elementary instruction; and as education spread to the middle and lower classes, it tended to be dropped altogether. By the mid-20th century, even the trivial acquaintance with Latin typical of the 19th-century student was a thing of the past.

Ecclesiastical Latin, the form of New Latin used in the Roman Catholic Church, remained in use throughout the period and after. Until the Second Vatican Council of 1962–65 all priests were expected to have competency in it, and it was studied in Catholic schools. It is today still the official language of the Church, and all Catholic priests of the Latin liturgical rites are required by canon law to have competency in the language. Use of Latin in the Mass, largely abandoned through the later 20th century, has recently seen a resurgence due in large part to Pope Benedict XVI's 2007 "motu proprio" "Summorum Pontificum" and its use by traditional Catholic priests and their organizations.

New Latin is also the source of the biological system of binomial nomenclature and classification of living organisms devised by Carl Linnaeus, although the rules of the ICZN allow the construction of names that deviate considerably from historical norms. (See also classical compounds.) Another continuation is the use of Latin names for the surface features of planets and planetary satellites (planetary nomenclature), originated in the mid-17th century for selenographic toponyms. New Latin has also contributed a vocabulary for specialized fields such as anatomy and law; some of these words have become part of the normal, non-technical vocabulary of various European languages.

New Latin had no single pronunciation, but a host of local variants or dialects, all distinct both from each other and from the historical pronunciation of Latin at the time of the Roman Republic and Roman Empire. As a rule, the local pronunciation of Latin used sounds identical to those of the dominant local language; the result of a concurrently evolving pronunciation in the living languages and the corresponding spoken dialects of Latin. Despite this variation, there are some common characteristics to nearly all of the dialects of New Latin, for instance:

The regional dialects of New Latin can be grouped into families, according to the extent to which they share common traits of pronunciation. The major division is between Western and Eastern family of New Latin. The Western family includes most Romance-speaking regions (France, Spain, Portugal, Italy) and the British Isles; the Eastern family includes Central Europe (Germany and Poland), Eastern Europe (Russia and Ukraine) and Scandinavia (Denmark, Sweden).

The Western family is characterized, "inter alia", by having a front variant of the letter "g" before the vowels "æ, e, i, œ, y" and also pronouncing "j" in the same way (except in Italy). In the Eastern Latin family, "j" is always pronounced , and "g" had the same sound (usually ) in front of both front and back vowels; exceptions developed later in some Scandinavian countries.

The following table illustrates some of the variation of New Latin consonants found in various countries of Europe, compared to the Classical Latin pronunciation of the 1st centuries BC-AD. In Eastern Europe, the pronunciation of Latin was generally similar to that shown in the table below for German, but usually with for "z" instead of .

New Latin texts are primarily found in early printed editions, which present certain features of spelling and the use of diacritics distinct from the Latin of antiquity, medieval Latin manuscript conventions, and representations of Latin in modern printed editions.

In spelling, New Latin, in all but the earliest texts, distinguishes the letter "u" from "v" and "i" from "j". In older texts printed down to c. 1630, "v" was used in initial position (even when it represented a vowel, e.g. in "vt", later printed "ut") and "u" was used elsewhere, e.g. in "nouus", later printed "novus". By the mid-17th century, the letter "v" was commonly used for the consonantal sound of Roman V, which in most pronunciations of Latin in the New Latin period was (and not ), as in "vulnus" "wound", "corvus" "crow". Where the pronunciation remained , as after "g", "q" and "s", the spelling "u" continued to be used for the consonant, e.g. in "lingua", "qualis", and "suadeo".

The letter "j" generally represented a consonantal sound (pronounced in various ways in different European countries, e.g. , , , ). It appeared, for instance, in "jam" "already" or "jubet" "orders" (earlier spelled "iam" and "iubet").
It was also found between vowels in the words "ejus", "hujus", "cujus" (earlier spelled "eius, huius, cuius"), and pronounced as a consonant; likewise in such forms as "major" and "pejor". "J" was also used when the last in a sequence of two or more "i"'s, e.g. "radij" (now spelled "radii") "rays", "alijs" "to others", "iij", the Roman numeral 3; however, "ij" was for the most part replaced by "ii" by 1700.

In common with texts in other languages using the Roman alphabet, Latin texts down to c. 1800 used the letter-form "ſ" (the "long s") for "s" in positions other than at the end of a word; e.g. "ipſiſſimus".

The digraphs "ae" and "oe" were rarely so written (except when part of a word in all capitals, e.g. in titles, chapter headings, or captions) ; instead the ligatures "æ" and "œ" were used, e.g. "Cæsar", "pœna". More rarely (and usually in 16th- to early 17th-century texts) the e caudata is found substituting for either.

Three kinds of diacritic were in common use: the acute accent ´, the grave accent `, and the circumflex accent ˆ. These were normally only marked on vowels (e.g. í, è, â); but see below regarding "que".
The acute accent marked a stressed syllable, but was usually confined to those where the stress was not in its normal position, as determined by vowel length and syllabic weight. In practice, it was typically found on the vowel in the syllable immediately preceding a final clitic, particularly "que" "and", "ve" "or" and "ne", a question marker; e.g. "idémque" "and the same (thing)". Some printers, however, put this acute accent over the "q" in the enclitic "que", e.g. "eorumq́ue" "and their". The acute accent fell out of favor by the 19th century.

The grave accent had various uses, none related to pronunciation or stress. It was always found on the preposition "à" (variant of "ab" "by" or "from") and likewise on the preposition "è" (variant of "ex" "from" or "out of"). It might also be found on the interjection "ò" "O". Most frequently, it was found on the last (or only) syllable of various adverbs and conjunctions, particularly those that might be confused with prepositions or with inflected forms of nouns, verbs, or adjectives. Examples include "certè" "certainly", "verò" "but", "primùm" "at first", "pòst" "afterwards", "cùm" "when", "adeò" "so far, so much", "unà" "together", "quàm" "than". In some texts the grave was found over the clitics such as "que", in which case the acute accent did not appear before them.

The circumflex accent represented metrical length (generally not distinctively pronounced in the New Latin period) and was chiefly found over an "a" representing an ablative singular case, e.g. "eâdem formâ" "with the same shape". It might also be used to distinguish two words otherwise spelled identically, but distinct in vowel length; e.g. "hîc" "here" differentiated from "hic" "this", "fugêre" "they have fled" (="fūgērunt") distinguished from "fugere" "to flee", or "senatûs" "of the senate" distinct from "senatus" "the senate". It might also be used for vowels arising from contraction, e.g. "nôsti" for "novisti" "you know", "imperâsse" for "imperavisse" "to have commanded", or "dî" for "dei" or "dii".









</doc>
<doc id="21986" url="https://en.wikipedia.org/wiki?curid=21986" title="Naive Set Theory (book)">
Naive Set Theory (book)

Naive Set Theory is a mathematics textbook by Paul Halmos providing an undergraduate introduction to set theory. Originally published by "Van Nostrand" in 1960, it was reprinted in the Springer-Verlag Undergraduate Texts in Mathematics series in 1974.

While the title states that it is naive, which is usually taken to mean without axioms, the book does introduce all the axioms of ZFC set theory (except the Axiom of Foundation), and gives correct and rigorous definitions for basic objects. Where it differs from a "true" axiomatic set theory book is its character: there are no discussions of axiomatic minutiae, and there is next to nothing about advanced topics like large cardinals. Instead, it tries to be intelligible to someone who has never thought about set theory before.

Halmos later stated that it was the fastest book he wrote, taking about six months, and that the book "wrote itself".

As noted above, the book omits the Axiom of Foundation. Halmos repeatedly dances around the issue of whether or not a set can contain itself.
But Halmos does let us prove that there are certain sets that cannot contain themselves.






</doc>
<doc id="21989" url="https://en.wikipedia.org/wiki?curid=21989" title="Nitrogen fixation">
Nitrogen fixation

Nitrogen fixation is a process by which molecular nitrogen in the air is converted into ammonia () or related nitrogenous compounds in soil. Atmospheric nitrogen is molecular dinitrogen, a relatively nonreactive molecule that is metabolically useless to all but a few microorganisms. Biological nitrogen fixation converts into ammonia, which is metabolized by most organisms.

Nitrogen fixation is essential to life because fixed inorganic nitrogen compounds are required for the biosynthesis of all nitrogen-containing organic compounds, such as amino acids and proteins, nucleoside triphosphates and nucleic acids. As part of the nitrogen cycle, it is essential for agriculture and the manufacture of fertilizer. It is also, indirectly, relevant to the manufacture of all nitrogen chemical compounds, which includes some explosives, pharmaceuticals, and dyes.

Nitrogen fixation is carried out naturally in soil by microorganisms termed diazotrophs that include bacteria such as "Azotobacter" and archaea. Some nitrogen-fixing bacteria have symbiotic relationships with plant groups, especially legumes. Looser non-symbiotic relationships between diazotrophs and plants are often referred to as associative, as seen in nitrogen fixation on rice roots. Nitrogen fixation occurs between some termites and fungi. It occurs naturally in the air by means of NO production by lightning.

All biological nitrogen fixation is effected by enzymes called nitrogenases. These enzymes contain iron, often with a second metal, usually molybdenum but sometimes vanadium.

Nitrogen can be fixed by lightning that converts nitrogen and oxygen into (nitrogen oxides). may react with water to make nitrous acid or nitric acid, which seeps into the soil, where it makes nitrate, which is of use to plants. Nitrogen in the atmosphere is highly stable and nonreactive due to the triple bond between atoms in the molecule. Lightning produces enough energy and heat to break this bond allowing nitrogen atoms to react with oxygen, forming . These compounds cannot be used by plants, but as this molecule cools, it reacts with oxygen to form . This molecule in turn reacts with water to produce (nitric acid), or its ion (nitrate), which is usable by plants.

Biological nitrogen fixation was discovered by German agronomist Hermann Hellriegel and Dutch microbiologist Martinus Beijerinck. Biological nitrogen fixation (BNF) occurs when atmospheric nitrogen is converted to ammonia by a nitrogenase enzyme. The overall reaction for BNF is:

<chem>N2 + 16ATP + 8e- + 8H+ -> 2NH3 +H2 + 16ADP + 16</chem>formula_1

The process is coupled to the hydrolysis of 16 equivalents of ATP and is accompanied by the co-formation of one equivalent of . The conversion of into ammonia occurs at a metal cluster called FeMoco, an abbreviation for the iron-molybdenum cofactor. The mechanism proceeds via a series of protonation and reduction steps wherein the FeMoco active site hydrogenates the substrate. In free-living diazotrophs, nitrogenase-generated ammonia is assimilated into glutamate through the glutamine synthetase/glutamate synthase pathway. The microbial nif genes required for nitrogen fixation are widely distributed in diverse environments. 

Nitrogenases are rapidly degraded by oxygen. For this reason, many bacteria cease production of the enzyme in the presence of oxygen. Many nitrogen-fixing organisms exist only in anaerobic conditions, respiring to draw down oxygen levels, or binding the oxygen with a protein such as leghemoglobin.

Diazotrophs are widespread within domain Bacteria including cyanobacteria (e.g. the highly significant "Trichodesmium" and "Cyanothece"), as well as green sulfur bacteria, Azotobacteraceae, rhizobia and "Frankia". Several obligately anaerobic bacteria fix nitrogen including many (but not all) "Clostridium" spp. Some archaea also fix nitrogen, including several methanogenic taxa, which are significant contributors to nitrogen fixation in oxygen-deficient soils.

Cyanobacteria inhabit nearly all illuminated environments on Earth and play key roles in the carbon and nitrogen cycle of the biosphere. In general, cyanobacteria can use various inorganic and organic sources of combined nitrogen, such as nitrate, nitrite, ammonium, urea, or some amino acids. Several cyanobacteria strains are also capable of diazotrophic growth, an ability that may have been present in their last common ancestor in the Archean eon. Nitrogen fixation by cyanobacteria in coral reefs can fix twice as much nitrogen as on land—around 660 kg/ha/year. The colonial marine cyanobacterium "Trichodesmium" is thought to fix nitrogen on such a scale that it accounts for almost half of the nitrogen fixation in marine systems globally.

Marine surface lichens and non-photosynthetic bacteria belonging in Proteobacteria and Planctomycetes fixate significant atmospheric nitrogen.

Plants that contribute to nitrogen fixation include those of the legume family—Fabaceae— with taxa such as kudzu, clover, soybean, alfalfa, lupin, peanut and rooibos. They contain symbiotic rhizobia bacteria within nodules in their root systems, producing nitrogen compounds that help the plant to grow and compete with other plants. When the plant dies, the fixed nitrogen is released, making it available to other plants; this helps to fertilize the soil. The great majority of legumes have this association, but a few genera (e.g., "Styphnolobium") do not. In many traditional farming practices, fields are rotated through various types of crops, which usually include one consisting mainly or entirely of clover.

Fixation efficiency in soil is dependent on many factors, including the legume and air and soil conditions. For example, nitrogen fixation by red clover can range from 50-200 lb./acre.

Other nitrogen fixing families include:

The ability to fix nitrogen is present in other families that belong to the orders Cucurbitales, Fagales and Rosales, which together with the Fabales form a clade of eurosids. The ability to fix nitrogen is not universally present in these families. For example, of 122 Rosaceae genera, only four fix nitrogen. Fabales were the first lineage to branch off this nitrogen-fixing clade; thus, the ability to fix nitrogen may be plesiomorphic and subsequently lost in most descendants of the original nitrogen-fixing plant; however, it may be that the basic genetic and physiological requirements were present in an incipient state in the most recent common ancestors of all these plants, but only evolved to full function in some of them.
Several nitrogen-fixing symbiotic associations involve cyanobacteria (such as "Nostoc"):

"Rhopalodia gibba", a diatom alga, is a eukaryote with cyanobacterial -fixing endosymbiont organelles. The spheroid bodies reside in the cytoplasm of the diatoms and are inseparable from their hosts.

Some scientists are working towards introducing the genes responsible for nitrogen fixation directly into plant DNA. As all known examples of nitrogen fixation takes place in prokaryotes, transferring the functionality to eukaryotes such as plant is a challenge; one team is using yeast as their eukaryotic test organism. A major problem to overcome is the oxygen-sensitivity of the produced enzymes, as well as the energy requirements. Having the process taking place inside of mitocondria or chloroplasts is being considered.

The possibility that atmospheric nitrogen reacts with certain chemicals was first observed by Desfosses in 1828. He observed that mixtures of alkali metal oxides and carbon react at high temperatures with nitrogen. With the use of barium carbonate as starting material, the first commercial process became available in the 1860s, developed by Margueritte and Sourdeval. The resulting barium cyanide could be reacted with steam yielding ammonia.

Prior to 1900, Tesla experimented with industrial nitrogen fixation "by using currents of extremely high frequency or rate of vibration".

In 1898 Frank and Caro decoupled the process and produced calcium carbide and in a subsequent step reacted it with nitrogen to calcium cyanamide. The Ostwald process for the production of nitric acid was discovered in 1902. The Frank-Caro and Ostwald processes dominated industrial fixation until the discovery of the Haber process in 1909.

The most common ammonia production method is the Haber process. Fertilizer production is now the largest source of human-produced fixed nitrogen in the terrestrial ecosystem. Ammonia is a required precursor to fertilizers, explosives, and other products. The Haber process requires high pressures (around 200 atm) and high temperatures (at least 400 °C), which are routine conditions for industrial catalysis. This process uses natural gas as a hydrogen source and air as a nitrogen source.

Much research has been conducted on the discovery of catalysts for nitrogen fixation, often with the goal of reducing energy requirements. However, such research has thus far failed to approach the efficiency and ease of the Haber process. Many compounds react with atmospheric nitrogen to give dinitrogen complexes. The first dinitrogen complex to be reported was ().

Achieving catalytic chemical nitrogen fixation at ambient conditions is an ongoing scientific endeavor. Guided by the example of nitrogenase, this area of homogeneous catalysis is ongoing, with particular emphasis on hydrogenation.

Metallic lithium burns in an atmosphere of nitrogen and then converts to lithium nitride. Hydrolysis of the resulting nitride gives ammonia. In a related process, trimethylsilyl chloride, lithium and nitrogen react in the presence of a catalyst to give tris(trimethylsilyl)amine. This can then be used for reaction with α,δ,ω-triketones to give tricyclic pyrroles. Processes involving lithium metal are however of no practical interest since they are non-catalytic and re-reducing the ion residue is difficult.

Beginning in the 1960s several homogeneous systems were identified that convert nitrogen to ammonia, sometimes catalytically, but often operating via ill-defined mechanisms. The original discovery is described in an early review:
"Vol'pin and co-workers, using a non-protic Lewis acid, aluminium tribromide, were able to demonstrate the truly catalytic effect of titanium by treating dinitrogen with a mixture of titanium tetrachloride, metallic aluminium, and aluminium tribromide at 50 °C, either in the absence or in the presence of a solvent, e.g. benzene. As much as 200 mol of ammonia per mol of was obtained after hydrolysis.…"
The quest for well-defined intermediates led to the characterization of many transition metal dinitrogen complexes. While few of these well-defined complexes function catalytically, their behavior illuminated likely stages in nitrogen fixation. Fruitful early studies focused on (dppe) (M = Mo, W), which protonates to give intermediates with ligand M=N−. In 1995, a molybdenum(III) amido complex was discovered that cleaved to give the corresponding molybdenum (VI) nitride. This and related terminal nitrido complexes have been used to make nitriles.

In 2003 a molybdenum amido complex was found to catalyze the reduction of , albeit with few turnovers. In these systems, like the biological one, hydrogen is provided to the substrate heterolytically, by means of protons and a strong reducing agent rather than with .

In 2011, another molybdenum-based system was discovered, but with a diphosphorus pincer ligand. Photolytic nitrogen splitting is also considered.

Nitrogen fixation at a p-block element was published in 2018 whereby one molecule of dinitrogen is bound by two transient Lewis-base-stabilized borylene species. The resulting dianion was subsequently oxidized to a neutral compound, and reduced using water.

With the help of catalysis and energy provided by electricity and light, can be produced directly from nitrogen and water at ambient temperature and pressure.

As of 2019 research was considering alternate means of supplying nitrogen in agriculture. Instead of using fertilizer, researchers were considering using different species of bacteria and separately, coating seeds with probiotics that encourage the growth of nitrogen-fixing bacteria.




</doc>
<doc id="21994" url="https://en.wikipedia.org/wiki?curid=21994" title="Navigation research">
Navigation research

Whereas originally the term Navigation applies to the process of directing a ship to a destination, Navigation research deals with fundamental aspects of navigation in general. It can be defined as "The process of determining and maintaining a course or trajectory to a goal location" (Franz, Mallot, 2000).
It concerns basically all moving agents, biological or artificial, or remote-controlled.

Franz and Mallot proposed a navigation hierarchy in "Robotics and Autonomous Systems 30" (2006):
There are two basic methods for navigation:


In human navigation people visualize different routes in their minds to plan how to get from one place to another. The things which they rely on to plan these routes vary from person to person and are the basis of the differing navigational strategies.
Some people use measures of distance and absolute directional terms (north, south, east, and west) in order to visualize the best pathway from point to point. The use of these more general, external cues as directions is considered part of an allocentric navigation strategy. Allocentric navigation is typically seen in males and is beneficial primarily in large and/or unfamiliar environments. This likely has some basis in evolution when males would have to navigate through large and unfamiliar environments while hunting. The use of allocentric strategies when navigating primarily activates the hippocampus and parahippocampus in the brain. This navigation strategy relies more on a mental, spatial map than visible cues, giving it an advantage in unknown areas but a flexibility to be used in smaller environments as well. The fact that it is mainly males that favor this strategy is likely related to the generalization that males are better navigators than females as it is better able to be applied in a greater variety of settings.

Egocentric navigation relies on more local landmarks and personal directions (left/right) to navigate and visualize a pathway. This reliance on more local and well-known stimuli for finding their way makes it difficult to apply in new locations, but is instead most effective in smaller, familiar environments. Evolutionarily, egocentric navigation likely comes from our ancestors who would forage for their food and need to be able to return to the same places daily to find edible plants. This foraging usually occurred in relatively nearby areas and was most commonly done by the females in hunter-gatherer societies. Females, today, are typically better at knowing where various landmarks are and often rely on them when giving directions. Egocentric navigation causes high levels of activation in the right parietal lobe and prefrontal regions of the brain that are involved in visuospatial processing.

Outdoor robots can use GPS in a similar way to automotive navigation systems.
Alternative systems can be used with floor plan instead of maps for indoor robots, combined with localization wireless hardware.




</doc>
<doc id="21995" url="https://en.wikipedia.org/wiki?curid=21995" title="Naguib Mahfouz">
Naguib Mahfouz

Naguib Mahfouz (, ; 11 December 1911 – 30 August 2006) was an Egyptian writer who won the 1988 Nobel Prize for Literature. He is regarded as one of the first contemporary writers of Arabic literature, along with Taha Hussein, to explore themes of existentialism. He published 34 novels, over 350 short stories, dozens of movie scripts, hundreds of op-ed columns for Egyptian newspapers, and five plays over a 70-year career. Many of his works have been made into Egyptian and foreign films.

Mahfouz was born in a lower middle-class Muslim Egyptian family in Old Cairo in 1911. He was the seventh and the youngest child, with four brothers and two sisters, all of them much older than him. (Experientially, he grew up an "only child.") The family lived in two popular districts of Cairo: first, in the Bayt al-Qadi neighborhood in the Gamaleya quarter in the old city, from where they moved in 1924 to Abbaseya, then a new Cairo suburb north of the old city, locations that would provide the backdrop for many of Mahfouz's later writings. His father, Abdel-Aziz Ibrahim, whom Mahfouz described as having been "old-fashioned", was a civil servant, and Mahfouz eventually followed in his footsteps in 1934. Mahfouz's mother, Fatimah, was the daughter of Mustafa Qasheesha, an Al-Azhar sheikh, and although illiterate herself, took the boy Mahfouz on numerous excursions to cultural locations such as the Egyptian Museum and the Pyramids.

The Mahfouz family were devout Muslims and Mahfouz had a strict Islamic upbringing. In an interview, he elaborated on the stern religious climate at home during his childhood. He stated that "You would never have thought that an artist would emerge from that family."

The Egyptian Revolution of 1919 had a strong effect on Mahfouz, although he was at the time only seven years old. From the window he often saw British soldiers firing at the demonstrators, men and women. "You could say ... that the one thing which most shook the security of my childhood was the 1919 revolution", he later said.

In his early years, Mahfouz read extensively and was influenced by Hafiz Najib, Taha Hussein and Salama Moussa, the Fabian intellectual.

After completing his secondary education, Mahfouz was admitted in 1930 to the Egyptian University (now Cairo University), where he studied philosophy, graduating in 1934. By 1936, having spent a year working on an M.A. in philosophy, he decided to discontinue his studies and become a professional writer. Mahfouz then worked as a journalist for al-Risala, and contributed short stories to "Al-Hilal" and "Al-Ahram".

After receiving his bachelor's degree in Philosophy from Cairo University in 1934, Mahfouz joined the Egyptian civil service, where he continued to work in various positions and ministries until retirement in 1971. He served first as a clerk at Cairo University, then, in 1938, in the Ministry of Islamic Endowments (Awqaf) as parliamentary secretary to the Minister of Islamic Endowments. In 1945, he requested a transfer to the al-Ghuri Mausoleum library, where he interviewed residents of his childhood neighborhood as part of the "Good Loans Project." In the 1950s, he worked as Director of Censorship in the Bureau of Arts, as Director of the Foundation for the Support of the Cinema, and finally as a consultant to the Ministry of Culture.

Mahfouz published 34 novels, over 350 short stories, dozens of movie scripts and five plays over a 70-year career. Possibly his most famous work, "The Cairo Trilogy", depicts the lives of three generations of different families in Cairo from World War I until after the 1952 military coup that overthrew King Farouk. He was a board member of the publisher "Dar el-Ma'aref". Many of his novels were serialized in "Al-Ahram", and his writings also appeared in his weekly column, "Point of View". Before the Nobel Prize only a few of his novels had appeared in the West.

Most of Mahfouz's early works were set in Cairo. "Abath Al-Aqdar (Mockery of the Fates)" (1939), "Rhadopis" (1943), and "Kifah Tibah (The Struggle of Thebes)" (1944) were historical novels written as part of a larger unfulfilled 30-novel project. Inspired by Sir Walter Scott (1771–1832), Mahfouz planned to cover the entire history of Egypt in a series of books. However, following the third volume, his interest shifted to current settings and issues, as well as the psychological impact of social change on ordinary people.

Mahfouz's prose is characterised by the blunt expression of his ideas. His written works cover a broad range of topics, including the controversial and taboo such as socialism, homosexuality, and God. Writing about some of these subjects was prohibited in Egypt.

Mahfouz's works often deal with Egypt's development during the 20th century, and combined intellectual and cultural influences from both East and West. His own exposure to foreign literature began in his youth with the enthusiastic consumption of Western detective stories, Russian classics, and modernist writers as Marcel Proust, Franz Kafka and James Joyce. Mahfouz's stories are almost always set in the heavily populated urban quarters of Cairo, where his characters, usually ordinary people, try to cope with the modernization of society and the temptations of Western values.

Mahfouz's central work in the 1950s was the "Cairo Trilogy", which he completed before the July Revolution. The novels were titled with the street names "Palace Walk", "Palace of Desire", and "Sugar Street". Mahfouz set the story in the parts of Cairo where he grew up. The novels depict the life of the patriarch el-Sayyed Ahmed Abdel Gawad and his family over three generations, from World War I to the 1950s, when King Farouk I was overthrown. Mahfouz stopped writing for some years after finishing the trilogy.

Disappointed in the Nasser régime, which had overthrown the monarchy in 1952, he started publishing again in 1959, now prolifically pouring out novels, short stories, journalism, memoirs, essays, and screenplays. He stated in a 1998 interview that he "long felt that Nasser was one of the greatest political leaders in modern history. I only began to fully appreciate him after he nationalized the Suez Canal."

His 1966 novel "Tharthara Fawq Al-Nīl" (Adrift on the Nile) is one of his most popular works. It was later made into a film called "Chitchat on the Nile" during the régime of Anwar al-Sadat. The story criticizes the decadence of Egyptian society during the Nasser era. It was banned by Sadat to avoid provoking Egyptians who still loved former president Nasser. Copies of the banned book were hard to find prior to the late 1990s.

The "Children of Gebelawi" (1959, also known as "Children of the Alley") one of Mahfouz's best known works, portrayed the patriarch Gebelaawi and his children, average Egyptians living the lives of Cain and Abel, Moses, Jesus, and Mohammed. Gebelawi builds a mansion in an oasis in the middle of a barren desert; his estate becomes the scene of a family feud that continues for generations. "Whenever someone is depressed, suffering or humiliated, he points to the mansion at the top of the alley at the end opening out to the desert, and says sadly, 'That is our ancestor's house, we are all his children, and we have a right to his property. Why are we starving? What have we done?'" The book was banned throughout the Arab world except in Lebanon until 2006 when it was first published in Egypt. The work was prohibited because of its alleged blasphemy through the allegorical portrayal of God and the monotheistic Abrahamic faiths of Judaism, Christianity, and Islam.

In the 1960s, Mahfouz further developed the theme that humanity is moving further away from God in his existentialist novels. In "The Thief and the Dogs" (1961) he depicted the fate of a Marxist thief who has been released from prison and plans revenge.

In the 1960s and 1970s Mahfouz began to construct his novels more freely and often used interior monologues. In "Miramar" (1967) he employed a form of multiple First-person narratives. Four narrators, among them a Socialist and a Nasserite opportunist, represent different political views. In the center of the story is an attractive servant girl. In "Arabian Nights and Days" (1981) and in "The Journey of Ibn Fatouma" (1983) he drew on traditional Arabic narratives as subtexts. "Akhenaten: Dweller in Truth" (1985) deals with conflict between old and new religious truths.

Many of his novels were first published in serialized form, including "Children of Gebelawi" and "Midaq Alley" which was also adapted into a Mexican film starring Salma Hayek called "El callejón de los milagros".

Most of Mahfouz's writings deal mainly with politics, a fact he acknowledged: "In all my writings, you will find politics. You may find a story which ignores love or any other subject, but not politics; it is the very axis of our thinking".

He espoused Egyptian nationalism in many of his works, and expressed sympathies for the post-World-War-era Wafd Party. He was also attracted to socialist and democratic ideals early in his youth. The influence of socialist ideals is strongly reflected in his first two novels, "Al-Khalili" and "New Cairo", as well as many of his later works. Parallel to his sympathy for socialism and democracy was his antipathy towards Islamic extremism.

In his youth, Mahfouz had personally known Sayyid Qutb when Qutb was showing a greater interest in literary criticism than in Islamic fundamentalism; Qutb later became a significant influence on the Muslim Brotherhood. In the mid-1940s, Qutb was one of the first critics to recognize Mahfouz's talent, and by the 1960s, near the end of Qutb's life, Mahfouz even visited him in the hospital. But later, in the semi-autobiographical novel "Mirrors", Mahfouz drew a negative portrait of Qutb. He was disillusioned with the 1952 revolution and by Egypt's defeat in the 1967 Six-Day War. He had supported the principles of the revolution, but became disenchanted, saying that the practices failed to live up to the original ideals.

Mahfouz's writing influenced a new generation of Egyptian lawyers, including Nabil Mounir and Reda Aslan.

Mahfouz's translated works received praise from American critics:

"The alleys, the houses, the palaces and mosques and the people who live among them are evoked as vividly in Mahfouz's work as the streets of London were conjured by Dickens."
—"Newsweek"

"Throughout Naguib Mahfouz's fiction there is a pervasive sense of metaphor, of a literary artist who is using his fiction to speak directly and unequivocally to the condition of his country. His work is imbued with love for Egypt and its people, but it is also utterly honest and unsentimental."
—"Washington Post"

"Mahfouz's work is freshly nuanced and hauntingly lyrical. The Nobel Prize acknowledges the universal significance of [his] fiction."
—"Los Angeles Times"

"Mr. Mahfouz embodied the essence of what makes the bruising, raucous, chaotic human anthill of Cairo possible."
—"The Economist"

Mahfouz was awarded the 1988 Nobel Prize in Literature, the only Arab writer to have won the award. Shortly after winning the prize Mahfouz was quoted as saying: 
The Swedish letter to Mahfouz praised his "rich and complex work":

Because Mahfouz found traveling to Sweden difficult at his age, he did not attend the award ceremony.

Mahfouz did not shrink from controversy outside of his work. As a consequence of his support for Sadat's Camp David peace treaty with Israel in 1978, his books were banned in many Arab countries until after he won the Nobel Prize. Like many Egyptian writers and intellectuals, Mahfouz was on an Islamic fundamentalist "death list".

He defended British-Indian writer Salman Rushdie after Ayatollah Ruhollah Khomeini condemned Rushdie to death in a 1989 fatwa, but also criticized Rushdie's novel "The Satanic Verses" as "insulting" to Islam. Mahfouz believed in freedom of expression, and, although he did not personally agree with Rushdie's work, he spoke out against the "fatwa" condemning him to death for it.

In 1989, after Ayatollah Khomeini's "fatwa" calling for Rushdie and his publishers to be killed, Mahfouz called Khomeini a terrorist. Shortly after, Mahfouz joined 80 other intellectuals in declaring that "no blasphemy harms Islam and Muslims so much as the call for murdering a writer."

The appearance of "The Satanic Verses" revived the controversy surrounding Mahfouz's novel "Children of Gebelawi". Death threats against Mahfouz followed, including one from the "blind sheikh," Egyptian-born Omar Abdul-Rahman. Mahfouz was given police protection, but in 1994 an extremist succeeded in attacking the 82-year-old novelist by stabbing him in the neck outside his Cairo home.

He survived, permanently affected by damage to nerves of his right upper limb. After the incident Mahfouz was unable to write for more than a few minutes a day and consequently produced fewer and fewer works. Subsequently, he lived under constant bodyguard protection. Finally, in the beginning of 2006, the novel was published in Egypt with a preface written by Ahmad Kamal Aboul-Magd. After the threats, Mahfouz stayed in Cairo with his lawyer, Nabil Mounir Habib. Mahfouz and Mounir would spend most of their time in Mounir's office; Mahfouz used Mounir's library as a reference for most of his books. Mahfouz stayed with Mounir until his death.

Mahfouz remained a bachelor until age 43 because he believed that, with its numerous restrictions and limitations, marriage would hamper his literary future. "I was afraid of marriage . . . especially when I saw how busy my brothers and sisters were with social events because of it. This one went to visit people, that one invited people. I had the impression that married life would take up all my time. I saw myself drowning in visits and parties. No freedom."

However, in 1954, he quietly married a Coptic Orthodox woman from Alexandria, Atiyyatallah Ibrahim, with whom he had two daughters, Fatima and Umm Kalthum. The couple initially lived on a houseboat in the Agouza section of Cairo on the west bank of the Nile, then moved to an apartment along the river in the same area. Mahfouz avoided public exposure, especially inquiries into his private life, which might have become, as he put it, "a silly topic in journals and radio programs."





</doc>
<doc id="21999" url="https://en.wikipedia.org/wiki?curid=21999" title="Nomenklatura">
Nomenklatura

The nomenklatura (; ) were a category of people within the Soviet Union and other Eastern Bloc countries who held various key administrative positions in the bureaucracy, running all spheres of those countries' activity: government, industry, agriculture, education, etc., whose positions were granted only with approval by the communist party of each country or region.

Virtually all members of the nomenklatura were members of a communist party. Critics of Stalin, such as Milovan Đilas, critically defined them as a "new class". The arch-critic of Stalin, Trotsky, used the term "caste" rather than "class", because he saw the Soviet Union as a degenerated workers' state, not as a new-class society. Richard Pipes, an anti-communist Harvard historian, claimed that the nomenklatura system mainly reflected a continuation of the old Tsarist regime, as many former Tsarist officials or "careerists" joined the Bolshevik government during and after the Russian Civil War of 1917–1922.

The "nomenklatura" forming a "de facto" elite of public powers in the former Eastern Bloc; one may compare them to the western "establishment" holding or controlling both private and public powers (for example, in media, finance, trade, industry, the state and institutions).

The Russian term is derived from the Latin "nomenclatura", meaning a system of names.

The term was popularized in the West by the Soviet dissident Michael Voslenski, who in 1970 wrote a book titled "Nomenklatura: The Soviet Ruling Class" ().

The nomenklatura referred to the Communist Party's governance to make appointments to key positions throughout the governmental system, as well as throughout the party's own hierarchy. Specifically, the nomenklatura consisted of two separate lists: one was for key positions, appointments to which were made by authorities within the party; the other was for persons who were potential candidates for appointment to those positions. The Politburo, as part of its nomenklatura authority, maintained a list of ministerial and ambassadorial positions that it had the power to fill, as well as a separate list of potential candidates to occupy those positions.

Coextensive with the nomenklatura were patron-client relations. Officials who had the authority to appoint individuals to certain positions cultivated loyalties among those whom they appointed. The patron (the official making the appointment) promoted the interests of clients in return for their support. Powerful patrons, such as the members of the Politburo, had many clients. Moreover, an official could be both a client (in relation to a higher-level patron) and a patron (to other, lower-level officials).

Because a client was beholden to his patron for his position, the client was eager to please his patron by carrying out his policies. The Soviet power structure essentially consisted (according to its critics) of groups of vassals (clients) who had an overlord (the patron). The higher the patron, the more clients the patron had. Patrons protected their clients and tried to promote their careers. In return for the patron's efforts to promote their careers, the clients remained loyal to their patron. Thus, by promoting his clients' careers, the patron could advance his own power.

The nomenklatura system arose early in Soviet history. Vladimir Lenin wrote that appointments were to take the following criteria into account: reliability, political attitude, qualifications, and administrative ability. Joseph Stalin, who was the first general secretary of the party, was also known as "Comrade File Cabinet" (Tovarishch Kartotekov) for his assiduous attention to the details of the party's appointments. Seeking to make appointments in a more systematic fashion, Stalin built the party's patronage system and used it to distribute his clients throughout the party bureaucracy.

Under Stalin's direction in 1922, the party created departments of the Central Committee and other organs at lower levels that were responsible for the registration and appointment of party officials. Known as uchraspred, these organs supervised appointments to important party posts. According to American sovietologist Seweryn Bialer, after Leonid Brezhnev's accession to power in October 1964, the party considerably expanded its appointment authority. However, in the late 1980s some official statements indicated that the party intended to reduce its appointment authority, particularly in the area of economic management, in line with Mikhail Gorbachev's reform efforts.

At the all-union level, the Party Building and Cadre Work Department supervised party nomenklatura appointments. This department maintained records on party members throughout the country, made appointments to positions on the all-union level, and approved nomenklatura appointments on the lower levels of the hierarchy. The head of this department sometimes was a member of the Secretariat and was often a protégé of the general secretary.

Every party committee and party organizational department, from the all-union level in Moscow to the district and city levels, prepared two lists according to their needs. The basic (osnovnoi) list detailed positions in the political, administrative, economic, military, cultural, and educational bureaucracies that the committee and its department had responsibility for filling. The registered (uchetnyi) list enumerated the persons suitable for these positions.

An official in the party or government bureaucracy could not advance in the nomenklatura without the assistance of a patron. In return for this assistance in promoting his career, the client carried out the policies of the patron. Patron–client relations thus help to explain the ability of party leaders to generate widespread support for their policies. The presence of patron–client relations between party officials and officials in other bureaucracies also helped to account for the large-scale control the party exercised over the Soviet society. All of the 2 million members of the nomenklatura system understood that they held their positions only as a result of a favor bestowed on them by a superior official in the party and that they could easily be replaced if they manifested disloyalty to their patron. Self-interest dictated that members of the nomenklatura submit to the control of their patrons in the party.

Clients sometimes could attempt to supplant their patron. For example, Nikita Khrushchev, one of Lazar M. Kaganovich's former protégés, helped to oust the latter in 1957. Seven years later, Leonid Brezhnev, a client of Khrushchev, helped to remove his boss from power. The power of the general secretary was consolidated to the extent that he placed his clients in positions of power and influence. The ideal for the general secretary, writes Soviet émigré observer Michael Voslensky, "is to be overlord of vassals selected by oneself."

Several factors explain the entrenchment of patron–client relations. Firstly, in a centralized government system, promotion in the bureaucratic-political hierarchy was the only path to power. Secondly, the most important criterion for promotion in this hierarchy was approval from one's supervisors, who evaluated their subordinates on the basis of political criteria and their ability to contribute to the fulfillment of the economic plan. Thirdly, political rivalries were present at all levels of the party and state bureaucracies but were especially prevalent at the top. Power and influence decided the outcomes of these struggles, and the number and positions of one's clients were critical components of that power and influence. Fourthly, because fulfillment of the economic plan was decisive, systemic pressures led officials to conspire together and use their ties to achieve that goal.

The faction led by Brezhnev provides a good case study of patron–client relations in the Soviet system. Many members of the Brezhnev faction came from Dnipropetrovsk, where Brezhnev had served as first secretary of the provincial party organization. Andrei P. Kirilenko, a Politburo member and Central Committee secretary under Brezhnev, was first secretary of the regional committee of Dnipropetrovsk. Volodymyr Shcherbytsky, named as first secretary of the Ukrainian apparatus under Brezhnev, succeeded Kirilenko in that position. Nikolai Alexandrovich Tikhonov, appointed by Brezhnev as first deputy chairman of the Soviet Union's Council of Ministers, graduated from the Dnipropetrovsk Metallurgical Institute, and presided over the economic council of Dnipropetrovsk Oblast. Finally, Nikolai Shchelokov, minister of internal affairs under Brezhnev, was a former chairman of the Dnipropetrovsk soviet.

Patron–client relations had implications for policy making in the party and government bureaucracies. Promotion of trusted subordinates into influential positions facilitated policy formation and policy execution. A network of clients helped to ensure that a patron's policies could be carried out. In addition, patrons relied on their clients to provide an accurate flow of information on events throughout the country. This information assisted policymakers in ensuring that their programs were being implemented.

Milovan Đilas, a critic of Stalin, wrote of the nomenklatura as the "new class" in his book "", and he claimed that it was seen by ordinary citizens as a bureaucratic elite that enjoyed special privileges and had supplanted the earlier wealthy capitalist élites.

Some Marxists, such as Ernest Mandel, have criticised Đilas and the theory of state capitalism: The hypothesis that the Soviet bureaucracy is a new ruling class does not correspond to a serious analysis of the real development and the real contradictions of Soviet society and economy in the last fifty years. Such a hypothesis must imply, from the point of view of historical materialism, that a new exploitative mode of production has emerged in that country. If this were so, we would be confronted, for the first time in history, with a "ruling class" whose general behavior and private interests (which of course dictate that behavior) run counter to the needs and inner logic of the existing socio-economic system. Indeed, one of the main characteristics of the Soviet economy is the impossibility of reconciling the needs of planning, of optimizing economic growth (not from an "absolute" point of view, but from within the logic of the system itself) with the material self-interest of the bureaucracy.




</doc>
<doc id="22000" url="https://en.wikipedia.org/wiki?curid=22000" title="Neural Darwinism">
Neural Darwinism

Neural Darwinism, a large scale theory of brain function by Gerald Edelman, was initially published in 1978, in a book called "The Mindful Brain" (MIT Press). It was extended and published in the 1987 book "Neural Darwinism – The Theory of Neuronal Group Selection".

In 1972, Edelman was awarded the Nobel Prize in Medicine or Physiology (shared with Rodney Porter of Great Britain) for his work in immunology showing how the population of lymphocytes capable of binding to a foreign antigen is increased by differential clonal multiplication following antigen discovery. Essentially, this proved that the human body is capable of creating complex adaptive systems as a result of local events with feedback. Edelman's interest in selective systems expanded into the fields of neurobiology and neurophysiology, and in "Neural Darwinism", Edelman puts forth a theory called "neuronal group selection". It contains three major parts:

With neuronal heterogeneity (by Edelman called "degeneracy"), it is possible to test the many circuits (on the order of 30 billion neurons with an estimated one quadrillion connections between them in the human brain) with a diverse set of inputs, to see which neuronal groups respond "appropriately" statistically. Functional "distributed" (widespread) brain circuits thus emerge as a result.

Edelman goes into some detail about how brain development depends on a variety of cell adhesion molecules (CAMs) and substrate adhesion molecules (SAMs) on cell surfaces which allow cells to dynamically control their intercellular binding properties. This surface modulation allows cell collectives to effectively "signal" as the group aggregates, which helps govern morphogenesis. So morphology depends on CAM and SAM function. And CAM and SAM function also depend on developing morphology.

Edelman theorized that cell proliferation, cell migration, cell death, neuron arbor distribution, and neurite branching are also governed by similar selective processes.

Once the basic variegated anatomical structure of the brain is laid down during early development, it is more or less fixed. But given the numerous and diverse collection of available circuitry, there are bound to be functionally equivalent albeit anatomically non-isomorphic neuronal groups capable of responding to certain sensory input. This creates a competitive environment where circuit groups proficient in their responses to certain inputs are "chosen" through the enhancement of the synaptic efficacies of the selected network. This leads to an increased probability that the same network will respond to similar or identical signals at a future time. This occurs through the strengthening of neuron-to-neuron synapses. And these adjustments allow for neural plasticity along a fairly quick timetable.

The last part of the theory attempts to explain how we experience spatiotemporal consistency in our interaction with environmental stimuli. Edelman called it "reentry" and proposes a model of reentrant signaling whereby a disjunctive, multimodal sampling of the same stimulus event correlated in time leads to self-organizing intelligence. Put another way, multiple neuronal groups can be used to sample a given stimulus set in parallel and communicate between these disjunctive groups with incurred latency.

It has been suggested that Friedrich Hayek had earlier proposed a similar idea in his book "The Sensory Order: An Inquiry into the Foundations of Theoretical Psychology", published in 1952 (Herrmann-Pillath, 1992). Other leading proponents include Jean-Pierre Changeux, Daniel Dennett and Linda B. Smith. William Calvin proposes true replication in the brain, whereas Edelman opposes the idea that there are true replicators in the brain.

Criticism of Neural "Darwinism" was made by Francis Crick on the basis that neuronal groups are instructed by the environment rather than undergoing blind variation. A recent review by Fernando, Szathmary and Husbands explains why Edelman's Neural Darwinism is not Darwinian because it does not contain units of evolution as defined by John Maynard Smith. It is selectionist in that it satisfies the Price equation, but there is no mechanism in Edelman's theory that explains how information can be transferred between neuronal groups. A recent theory called Evolutionary Neurodynamics being developed by Eors Szathmary and Chrisantha Fernando has proposed several means by which true replication may take place in the brain. These neuronal models have been extended by Fernando in a later paper. In the most recent model, three plasticity mechanisms i) multiplicative STDP, ii) LTD, and iii) Heterosynaptic competition, are responsible for copying of connectivity patterns from one part of the brain to another. Exactly the same plasticity rules can explain experimental data for how infants do causal learning in the experiments conducted by Alison Gopnik. It has also been shown that by adding Hebbian learning to neuronal replicators the power of neuronal evolutionary computation may actually be greater than natural selection in organisms.

Jean Piaget (1896–1980) often used the concept of the "schème" (a supposed unit of action-coding), which he left as an abstraction. However, later theorizing led to the hypothesis that such "schèmes" were probably RNA-like molecules, at least in their simplest cases. Such molecular sites would need to intercommunicate mainly via infra-red signals: messages which would be able to travel through fatty tissue such as myelin, but would be blocked by water barriers (of >20 microns). This "new" "[R]-system" was proposed as a cooperative alternative arrangement, more concerned with digital signals and data required "for advanced thinking"—(whereas the traditional "[A]-system" of action-potentials and synapses would perhaps cope more with activities such as logistics, muscle-control, and pattern-recognition which can probably manage using "analogue" devices—a division of labour).

Whether or not one accepts those actual details, such a "molecule-based" system offers (i) an obvious scope for clear-cut encoding, (ii) an obvious explanation for any inherited behaviour-traits, (iii) a vastly greater number of candidate-codes from which to select-or-waste in a Darwinian contest; etc. Hence, this might be seen as overcoming Crick's objections, at least partially.





</doc>
<doc id="22003" url="https://en.wikipedia.org/wiki?curid=22003" title="Neil Peart">
Neil Peart

Neil Ellwood Peart OC (; September 12, 1952 – January 7, 2020) was a Canadian musician, songwriter, and author, best known as the drummer and primary lyricist of the rock band Rush. Peart earned numerous awards for his musical performances, including an induction into the "Modern Drummer" Readers Poll Hall of Fame in 1983, making him the youngest person ever so honoured. His drumming was renowned for its technical proficiency and his live performances for their exacting nature and stamina.

Peart was born in Hamilton, Ontario, and grew up in Port Dalhousie (now part of St. Catharines). During adolescence, he floated between regional bands in pursuit of a career as a full-time drummer. After a discouraging stint in England to concentrate on his music, Peart returned home, where he joined Rush, a Toronto band, in mid-1974, six years after its formation. They released nineteen studio albums, with ten exceeding a million copies sold in the United States. "Billboard" ranks the band third for the "most consecutive gold or platinum albums by a rock band".
Early in his career, Peart's performance style was deeply rooted in hard rock. He drew most of his inspiration from drummers such as Keith Moon, Ginger Baker, and John Bonham, players who were at the forefront of the British hard rock scene. As time passed, he began to emulate jazz and big band musicians Gene Krupa and Buddy Rich. In 1994, Peart became a friend and pupil of jazz instructor Freddie Gruber. It was during this time that Peart decided to revamp his playing style by incorporating jazz and swing components.

In addition to serving as Rush's primary lyricist, Peart published several memoirs about his travels. His lyrics for Rush addressed universal themes and diverse subjects including science fiction, fantasy, and philosophy, as well as secular, humanitarian, and libertarian themes. Peart wrote a total of seven nonfiction books focused on his travels and personal stories.

On December 7, 2015, Peart announced his retirement from music in an interview with "Drumhead Magazine", though bandmate Geddy Lee insisted Peart was quoted out of context, and suggested Peart was "simply taking a break". However, in January 2018, bandmate Alex Lifeson confirmed that Rush was retiring due to Peart's health issues. During his last years Peart lived in Santa Monica, California with his wife, Carrie Nuttall, and daughter. After a three and a half year illness, Peart died of glioblastoma on January 7, 2020, at age 67.

Peart was born on September 12, 1952, to Glen and Betty Peart and lived his early years on his family's farm in Hagersville, on the outskirts of Hamilton. The first child of four, his brother Danny and sisters Judy and Nancy were born after the family moved to St. Catharines when Peart was two years old. At this time his father became parts manager for Dalziel Equipment, an International Harvester farm machinery dealer. In 1956 the family moved to the Port Dalhousie area of the town. Peart attended Gracefield School and later Lakeport Secondary School, and described his childhood as happy and says he experienced a warm family life. By early adolescence he became interested in music and acquired a transistor radio, which he would use to tune into pop music stations broadcasting from Toronto, Hamilton, Welland, Ontario and Buffalo, New York.

His first exposure to musical training came in the form of piano lessons, which he later said in his instructional video "A Work in Progress" did not have much impact on him. He had a penchant for drumming on various objects around the house with a pair of chopsticks, so for his thirteenth birthday his parents bought him a pair of drum sticks, a practice drum, and some lessons, with the promise that if he stuck with it for a year they would buy him a kit.

His parents bought him a drum kit for his fourteenth birthday and he began taking lessons from Don George at the Peninsula Conservatory of Music. His stage debut took place that year at the school's Christmas pageant in St. Johns Anglican Church Hall in Port Dalhousie. His next appearance was at Lakeport High School with his first group, The Eternal Triangle. This performance contained an original number titled "LSD Forever". At this show he performed his first solo.

Peart got a job in Lakeside Park, in Port Dalhousie on the shores of Lake Ontario, which later inspired a song of the same name on the Rush album "Caress of Steel". He worked on the Bubble Game and Ball Toss, but his tendency to take it easy when business was slack resulted in his termination. By his late teens, Peart had played in local bands such as Mumblin' Sumpthin', the Majority, and JR Flood. These bands practiced in basement recreation rooms and garages and played church halls, high schools, and skating rinks in towns across Southern Ontario such as Mitchell, Seaforth, and Elmira. They also played in the northern Ontario city of Timmins. Tuesday nights were filled with jam sessions at the Niagara Theatre Centre.

At eighteen years old after struggling to achieve success as a drummer in Canada, Peart travelled to London, England, hoping to further his career as a professional musician. Despite playing in several bands and picking up occasional session work, he was forced to support himself by selling jewelry at a shop called The Great Frog on Carnaby Street.

While in London, he came across the writings of novelist and Objectivist Ayn Rand. Rand's writings became a significant early philosophical influence on Peart, as he found many of her writings on individualism and Objectivism inspiring. References to Rand's philosophy can be found in his early lyrics, most notably "Anthem" from 1975's "Fly by Night" and "2112" from 1976's "2112".

After eighteen months Peart became disillusioned by his lack of progress in the music business; he placed his aspiration of becoming a professional musician on hold and returned to Canada. Upon returning to St. Catharines, he worked for his father selling tractor parts at Dalziel Equipment.

After returning to Canada, Peart was recruited to play drums for a St. Catharines band known as Hush, who played on the Southern Ontario bar circuit. Soon after, a mutual acquaintance convinced Peart to audition for the Toronto-based band Rush, which needed a replacement for its original drummer John Rutsey. Geddy Lee and Alex Lifeson oversaw the audition. His future bandmates describe his arrival that day as somewhat humorous, as he arrived in shorts, driving a battered old Ford Pinto with his drums stored in trashcans. Peart felt the entire audition was a complete disaster. While Lee and Peart hit it off on a personal level (both sharing similar tastes in books and music), Lifeson had a less favourable impression of Peart.

After some discussion between Lee and Lifeson, Peart officially joined the band on July 29, 1974, two weeks before the group's first US tour. Peart procured a silver Slingerland kit which he played at his first gig with the band, opening for Uriah Heep and Manfred Mann's Earth Band in front of over 11,000 people at the Civic Arena, Pittsburgh, Pennsylvania, on August 14, 1974.

Peart soon settled into his new position, also becoming the band's primary lyricist. Before joining Rush he had written few songs, but, with the other members largely uninterested in writing lyrics, Peart's previously underutilized writing became as noticed as his musicianship. The band was working hard to establish themselves as a recording act, and Peart, along with the rest of the band, began to undertake extensive touring.

His first recording with the band, 1975's "Fly by Night," was fairly successful, winning the Juno Award for most promising new act, but the follow-up, "Caress of Steel," for which the band had high hopes, was greeted with hostility by both fans and critics. In response to this negative reception, most of which was aimed at the B side-spanning epic "The Fountain of Lamneth", Peart responded by penning "2112" on their next album of the same name in 1976. The album, despite record company indifference, became their breakthrough and gained a following in the United States. The supporting tour culminated in a three-night stand at Massey Hall in Toronto, a venue Peart had dreamed of playing in his days on the Southern Ontario bar circuit and where he was introduced as "The Professor on the drum kit" by Lee.

Peart returned to England for Rush's Northern European Tour and the band stayed in the United Kingdom to record the next album, 1977's "A Farewell to Kings" in Rockfield Studios in Wales. They returned to Rockfield to record the follow-up, "Hemispheres", in 1978, which they wrote entirely in the studio. The recording of five studio albums in four years, coupled with as many as 300 gigs a year, convinced the band to take a different approach thereafter. Peart has described his time in the band up to this point as "a dark tunnel".

In 1992, Peart was invited by Buddy Rich's daughter, Cathy Rich, to play at the Buddy Rich Memorial Scholarship Concert in New York City. Peart accepted and performed for the first time with the Buddy Rich Big Band. Peart remarked that he had little time to rehearse, and noted that he was embarrassed to find the band played a different arrangement of the song than the one he had learned. Feeling that his performance left much to be desired, Peart decided to produce and play on two Buddy Rich tribute albums titled "" in 1994 and 1997 in order to regain his aplomb.

While producing the first Buddy Rich tribute album, Peart was struck by the tremendous improvement in ex-Journey drummer Steve Smith's playing, and asked him his "secret". Smith responded he had been studying with drum teacher Freddie Gruber.

In early 2007, Peart and Cathy Rich again began discussing yet another Buddy tribute concert. At the recommendation of bassist Jeff Berlin, Peart decided to once again augment his swing style with formal drum lessons, this time under the tutelage of another pupil of Freddie Gruber, Peter Erskine, himself an instructor of Steve Smith. On October 18, 2008, Peart once again performed at the Buddy Rich Memorial Concert at New York's Hammerstein Ballroom. The concert has since been released on DVD.

On August 10, 1997, soon after the conclusion of Rush's Test for Echo Tour, Peart's first daughter (and, at the time, his only child) Selena Taylor, 19, was killed in a single-car crash on Highway 401 near the town of Brighton, Ontario. His common-law wife of 23 years, Jacqueline Taylor, succumbed to cancer 10 months later on June 20, 1998. Peart attributes her death to the result of a "broken heart" and called it "a slow suicide by apathy. She just didn't care."

In his book "", Peart writes that he told his bandmates at Selena's funeral, "consider me retired". Peart took a long sabbatical to mourn and reflect, and travelled extensively throughout North and Central America on his motorcycle, covering . After his journey, Peart decided to return to the band. Peart wrote the book as a chronicle of his geographical and emotional journey.

Peart was introduced to photographer Carrie Nuttall in Los Angeles by long-time Rush photographer Andrew MacNaughtan. They married on September 9, 2000. In early 2001, Peart announced to his bandmates that he was ready to return to recording and performing. The product of the band's return was the 2002 album "Vapor Trails". At the start of the ensuing tour in support of the album, it was decided amongst the band members that Peart would not take part in the daily grind of press interviews and "meet and greet" sessions upon their arrival in a new city that typically monopolize a touring band's daily schedule. Peart has always shied away from these types of in-person encounters, and it was decided that exposing him to a lengthy stream of questions about the tragic events of his life was not necessary.

After the release of "Vapor Trails" and his reunion with bandmates, Peart returned to work as a full-time musician. In the June 2009 edition of Peart's website's "News, Weather, and Sports", titled "Under the Marine Layer", he announced that he and Nuttall were expecting their first child. Olivia Louise Peart was born later that year.

In the mid-2010s, Peart acquired U.S. citizenship.

Peart announced his retirement in an interview in December 2015:
Peart had been suffering from chronic tendinitis and shoulder problems. In January 2018, Alex Lifeson confirmed that Rush is "basically done". Peart remained friends with his former bandmates.

Peart died from glioblastoma, an aggressive form of brain cancer, on January 7, 2020, in Santa Monica, California. He had been diagnosed three and a half years earlier, and the illness was a closely guarded secret in Peart's inner circle until his death. His family made the announcement on January 10.

From the official Rush website:
Peart's death was widely lamented by fans and fellow musicians alike, who considered it a substantial loss for popular music.

Peart's drumming skill and technique are well-regarded by fans, fellow musicians, and music journalists. His influences were eclectic, ranging from Pete Thomas, John Bonham, Michael Giles, Ginger Baker, Phil Collins, Chris Sharrock, Steve Gadd, Stewart Copeland, Michael Shrieve and Keith Moon, to fusion and jazz drummers Billy Cobham, Buddy Rich, Bill Bruford and Gene Krupa. The Who was the first group that inspired him to write songs and play the drums. Peart is distinguished for playing "butt-end out", reversing stick orientation for greater impact and increased rimshot capacity. "When I was starting out", Peart said, "if I broke the tips off my sticks I couldn't afford to buy new ones, so I would just turn them around and use the other end. I got used to it, and continue to use the heavy end of lighter sticks – it gives me a solid impact, but with less 'dead weight' to sling around."

Peart had long played matched grip but shifted to traditional as part of his style reinvention in the mid-1990s under the tutelage of jazz coach Freddie Gruber. He played traditional grip throughout his first instructional DVD "A Work in Progress" and on Rush's "Test for Echo" studio album. Peart went back to using primarily matched, though he continued to switch to traditional at times when playing songs from "Test for Echo" and during moments when traditional grip felt more appropriate, such as during the rudimental snare drum section of his drum solo. He discussed the details of these switches in the DVD "Anatomy of a Drum Solo".

"Variety" wrote: "Widely considered one of the most innovative drummers in rock history, Peart was famous for his state-of-the-art drum kits – more than 40 different drums were not out of the norm – precise playing style and on stage showmanship."

"USA Today"s writers compared him favorably to other top shelf rock drummers. He was "considered one of the best rock drummers of all time, alongside John Bonham of Led Zeppelin; Ringo Starr of The Beatles; Keith Moon of The Who; Ginger Baker of Cream and Stewart Copeland of The Police." Being "known for his technical proficiency", the Modern Drummer Hall of Fame inducted him in 1983.

Music critic Amanda Petrusich in "The New Yorker" wrote: "Watching Peart play the drums gave the impression that he might possess several phantom limbs. The sound was merciless."

With Rush, Peart played Slingerland, Tama, Ludwig, and Drum Workshop drums, in that order.

Peart played Zildjian A-series cymbals and Wuhan china cymbals until the early 2000s when he switched to Paragon, a line created for him by Sabian. In concert starting in 1984 on the Grace Under Pressure Tour, Peart used an elaborate 360-degree drum kit that would rotate as he played different sections of the kit.

During the late 1970s, Peart augmented his acoustic setup with diverse percussion instruments, including orchestra bells, tubular bells, wind chimes, crotales, timbales, timpani, gong, temple blocks, bell tree, triangle, and melodic cowbells. From the mid-1980s, Peart replaced several of these pieces with MIDI trigger pads. This was done in order to trigger sounds sampled from various pieces of acoustic percussion that would otherwise consume far too much stage area. Some purely electronic non-instrumental sounds were also used. One classic MIDI pad used is the Malletkat Express, which is a two-octave electronic MIDI device that resembles a xylophone or piano. The Malletkat Express is composed of rubber pads for the "keys" so that any stick can be used. Beginning with 1984's "Grace Under Pressure", he used Simmons electronic drums in conjunction with Akai digital samplers. Peart performed several songs primarily using the electronic portion of his drum kit. (e.g. "Red Sector A", "Closer to the Heart" on "A Show of Hands" and "Mystic Rhythms" on "".)

Shortly after making the choice to include electronic drums and triggers, Peart added what became another trademark of his kit: a rotating drum riser. During live Rush shows, the riser allowed Peart to swap the prominent portions of the kit (traditional acoustic in front, electronic in back). A staple of Peart's live drum solos was the in-performance rotation-and-swap of the front and back kits as part of the solo, a special effect that provided a symbolic transition of drum styles within the solo.
In the early 2000s, Peart began taking full advantage of the advances in electronic drum technology, primarily incorporating Roland V-Drums and continued use of samplers with his existing set of acoustic percussion. His digitally-sampled library of both traditional and exotic sounds expanded over the years with his music.

In April 2006, Peart took delivery of his third DW set, configured similarly to the R30 set, in a Tobacco Sunburst finish over curly maple exterior ply, with chrome hardware. He referred to this set, which he used primarily in Los Angeles, as the "West Coast kit". Besides using it on recordings with Vertical Horizon, he played it while composing parts for Rush's album, "Snakes & Arrows". It featured a custom 23-inch bass drum; all other sizes remained the same as the R30 kit.

On March 20, 2007 Peart revealed that Drum Workshop prepared a new set of red-painted DW maple shells with black hardware and gold "Snakes & Arrows" logos for him to play on the Snakes & Arrows Tour.

Peart also designed his own signature series drumstick with Pro-Mark, the Promark PW747W, Neil Peart Signature drumsticks, made of Japanese white oak.

During the 2010–11 Time Machine Tour Peart used a new DW kit which was outfitted with copper-plated hardware and time machine designs to match the tour's steampunk themes. Matching Paragon cymbals with clock imagery were also used.

Peart was noted for his distinctive in-concert drum solos, characterized by exotic percussion instruments and long, intricate passages in odd time signatures. His complex arrangements sometimes result in complete separation of upper- and lower-limb patterns; an ostinato dubbed "The Waltz" is a typical example. His solos were featured on every live album released by the band. On the early live albums ("All the World's a Stage" & "Exit... Stage Left"), the drum solo was included as part of a song. On all subsequent live albums through "", the drum solo has been included as a separate track. The "Clockwork Angels Tour" album includes three short solos instead of a single long one: two interludes played during other songs and one standalone. Similarly, the "R40 Live" album includes two short solos performed as interludes.

Peart's instructional DVD "Anatomy of a Drum Solo" is an in-depth examination of how he constructs a solo that is musical rather than indulgent, using his solo from the 2004 R30 30th anniversary tour as an example.

Peart was the main lyricist for Rush. Literature heavily influenced his writings. In his early days with Rush, much of his lyrical output was influenced by fantasy, science fiction, mythology, and philosophy.

The 1980 album "Permanent Waves" saw Peart cease to use fantasy and mythological themes. 1981's "Moving Pictures" showed that Peart was still interested in heroic, mythological figures, but now placed firmly in a modern, realistic context. The song "Limelight" from the same album is an autobiographical account of Peart's reservations regarding his own popularity and the pressures with fame. From "Permanent Waves" onward, most of Peart's lyrics began to revolve around social, emotional, and humanitarian issues, usually from an objective standpoint and employing the use of metaphors and symbolic representation.

1984's "Grace Under Pressure" strung together such despondent topics as the Holocaust ("Red Sector A") and the death of close friends ("Afterimage"). Starting with 1987's "Hold Your Fire" and including 1989's "Presto", 1991's "Roll the Bones", and 1993's "Counterparts", Peart would continue to explore diverse lyrical motifs, even addressing the topic of love and relationships, ("Open Secrets", "Ghost of a Chance", "Speed of Love", "Cold Fire", "Alien Shore") a subject which he purposefully avoided in the past, out of fear of using clichés. 2002's "Vapor Trails" was heavily devoted to Peart's personal issues, along with other humanitarian topics such as the 9/11 terrorist attacks ("Peaceable Kingdom"). The album "Snakes & Arrows" dealt primarily and vociferously with Peart's opinions regarding faith and religion.

The song "2112" focuses on the struggle of an individual against the collectivist forces of a totalitarian state. This became the band's breakthrough release, but also brought unexpected criticism, mainly because of the credit of inspiration Peart gave to Ayn Rand in the liner notes. "There was a remarkable backlash, especially from the English press, this being the late seventies, when collectivism was still in style, especially among journalists", Peart said. "They were calling us 'Junior fascists' and 'Hitler lovers'. It was a total shock to me".

Regarding his seeming ideological fealty to Rand's philosophy of Objectivism, Peart said, "For a start, the extent of my influence by the writings of Ayn Rand should not be overstated. I am no one's disciple." The lyrics of "Faithless" exhibit a life stance which has been closely identified with secular humanism. Peart explicitly discussed his religious views in "The Masked Rider: Cycling in West Africa", in which he wrote: "I'm a linear thinking agnostic, but not an atheist, folks."

In 2007, Peart was ranked No. 2 (after Sting) on the now defunct magazine "Blender"'s list of "worst lyricists in rock". In contrast, Allmusic called him "one of rock's most accomplished lyricists".

For most of his career, Peart had never publicly identified with any political party or organization in Canada or the United States. Even so, his political and philosophical views have often been analyzed through his work with Rush and through other sources. In October 1993, shortly before that year's Canadian federal election, Peart appeared with then-Liberal Party leader Jean Chrétien in an interview broadcast in Canada on MuchMusic, but stated in that interview that he was an undecided voter.

Peart has often been categorized as an Objectivist and an admirer of Ayn Rand. This is largely based on his work with Rush in the 1970s, particularly the song "Anthem" and the album "2112"; the latter specifically credited Rand's work. However, in his 1994 "Rush Backstage Club Newsletter", while contending the "individual is paramount in matters of justice and liberty," Peart specifically distanced himself from a strictly Objectivist line. In a June 2012 "Rolling Stone" interview, when asked if Rand's words still speak to him, Peart replied, "Oh, no. That was forty years ago. But it was important to me at the time in a transition of finding myself and having faith that what I believed was worthwhile." Peart has also subscribed to a personal philosophy that he called "Tryism", which holds that anything one tries to attain will be attained if one tries hard enough.

Although Peart was sometimes assumed to be a "Conservative" or "Republican" rock star, he criticized the US Republican Party by stating that the philosophy of the party is "absolutely opposed to Christ's teachings." In 2005, he described himself as a "left-leaning libertarian", and is often cited as a libertarian celebrity.

In a 2015 interview with "Rolling Stone", Peart stated that he saw the US Democratic Party as the lesser evil: "For a person of my sensibility, you’re only left with the Democratic party."

Peart was a member of the Canadian charity Artists Against Racism and worked with them on a radio public service announcement.

Peart authored seven non-fiction books, the latest released in September 2016.

Peart's first book, titled "", was written in 1996 about a month-long bicycling tour through Cameroon in November 1988. The book details Peart's travels through towns and villages with four fellow riders. The original had a limited print run, but after the critical and commercial success of Peart's second book, "Masked Rider" was re-issued by ECW Press and remains in print.

After losing his wife and (at the time) only daughter, Peart embarked on a lengthy motorcycle road trip spanning North America. His experiences were penned in "". Peart and the rest of the band were always able to keep his private life at a distance from his public image in Rush. However, "Ghost Rider" is a first-person narrative of Peart on the road on a BMW R1100GS motorcycle, in an effort to put his life back together as he embarked on an extensive journey.

Years later, after his marriage to Nuttall, Peart took another road trip, this time by car. In his third book, "", he reflects on his life, his career, his family, and music. As with his previous two books, it is a first-person narrative.

Three decades after Peart joined Rush, the band found itself on its . Released in September 2006, "Roadshow: Landscape with Drums – A Concert Tour by Motorcycle" details the tour both from behind Neil's drumkit and on his BMW R1150GS and R1200GS motorcycles.

Peart's next book, "Far and Away: A Prize Every Time", was published by ECW Press in May 2011. This book, which he worked on for two years, is formed around his traveling in North and South America. It tells how he found in a Brazilian town a unique combination of West African and Brazilian music. In 2014, a follow-up book, "Far and Near: On Days like These", was published by ECW. It covers travels in North America and Europe. Another book, "Far and Wide: Bring That Horizon to Me!", was published in 2016 and is based on his travels between stops on the R40 Live Tour of 2015.

Nonfiction works include:

Peart worked with science fiction author Kevin J. Anderson to develop a novelization of Rush's 2012 album "Clockwork Angels"; the book was published by ECW Press. The two collaborated again on a loose sequel, "Clockwork Lives", published in 2015. Snippets of the band's lyrics can be found throughout both stories.

Fiction works include:


Peart had a brief cameo in the 2007 film "Aqua Teen Hunger Force Colon Movie Film for Theaters", in which samples of his drumming were played.

Peart also had a brief cameo in the 2008 film "Adventures of Power" and in the DVD extra does a drum-off competition.

Peart appeared in concert with Rush in the 2009 film "I Love You, Man", as well as a "Funny or Die" web short in which the film's main characters sneak into the band's dressing room.

Apart from Rush's video releases as a band, Peart has released the following DVDs (the first originally in VHS tape format) as an individual:

Peart received the following awards in the "Modern Drummer" magazine
reader's poll:


Peart received the following awards from "DRUM!" magazine for 2007:


Peart received the following awards from "DRUM!" magazine for 2008:


Peart received the following awards from "DRUM!" magazine for 2009:

Peart received the following awards from "DRUM!" magazine for 2010:

Other honors and awards






</doc>
<doc id="22007" url="https://en.wikipedia.org/wiki?curid=22007" title="North Atlantic Treaty">
North Atlantic Treaty

The North Atlantic Treaty, also referred to as the Washington Treaty, is the treaty that forms the legal basis of, and is implemented by, the North Atlantic Treaty Organization (NATO). The treaty was signed in Washington, D.C. on 4 April 1949.

The treaty was signed in Washington, D.C. on 4 April 1949 by a committee which was chaired by US diplomat Theodore Achilles. Earlier secret talks had been held at the Pentagon between 22 March and 1 April 1948, of which Achilles said:

The talks lasted about two weeks and by the time they finished, it had been secretly agreed that there would be a treaty, and I had a draft of one in the bottom drawer of my safe. It was never shown to anyone except Jack [Hickerson]. I wish I had kept it, but when I left the Department in 1950, I dutifully left it in the safe and I have never been able to trace it in the archives. It drew heavily on the Rio Treaty, and a bit of the Brussels Treaty, which had not yet been signed, but of which we were being kept heavily supplied with drafts. The eventual North Atlantic Treaty had the general form, and a good bit of the language of my first draft, but with a number of important differences.

According to Achilles, another important author of the treaty was John D. Hickerson:

More than any human being Jack was responsible for the nature, content, and form of the Treaty...It was a one-man Hickerson treaty.

As a fundamental component of NATO, the North Atlantic Treaty is a product of the US' desire to avoid overextension at the end of World War II, and consequently pursue multilateralism in Europe. It is part of the US' collective defense arrangement with Western European powers, following a long and deliberative process. The treaty was created with an armed attack by the Soviet Union against Western Europe in mind, but the mutual self-defense clause was never invoked during the Cold War. Rather, it was invoked for the first and only time in 2001 during Operation Eagle Assist in response to the September 11 attacks. 

By signing the North Atlantic Treaty, parties are "determined to safeguard the freedom, common heritage and civilization of the peoples, founded on the principles of democracy, individual liberty and the rule of law." 

The following twelve states signed the treaty and thus became the founding members of NATO. The following leaders signed the agreement as plenipotentiaries of their countries in Washington, D.C. on 4 April 1949:


The following 18 states joined the treaty after the 12 founding states:
Article 1 of the treaty states that member parties "settle any international disputes in which they may be involved by peaceful means in such a manner that international peace and security, and justice, are not endangered, and to refrain in their international relations from the threat or use of force in any manner inconsistent with the purposes of the United Nations."

Members seek to promote stability and well-being in the North Atlantic area through preservation of peace and security in accordance with the Charter of the United Nations. 

The treaty includes Article 4, which calls for consultation over military matters when "the territorial integrity, political independence or security of any of the parties is threatened."

It has been invoked four times by Turkey: in 2003 over the Iraq War, in June 2012 after the shooting down of a Turkish military jet by Syria, in October 2012 after Syrian attacks on Turkey and their counterattacks, and in February 2020 amid increasing tensions as part of the Northwestern Syria offensive.

An Article 4 meeting was invoked by Latvia, Lithuania, and Poland in March 2014 as a response to the extraterritorial Crimean crisis.

Turkey announced plans to convoke under Article 4 an extraordinary meeting on 28 July 2015, ostensibly in response to the 2015 Suruç bombing, which it attributed to ISIS, and other security issues along its southern border. A press statement released by the Alliance declared that "Turkey requested the meeting in view of the seriousness of the situation after the heinous terrorist attacks in recent days, and also to inform allies of the measures it is taking." The US announced through "The New York Times" on 27 July that it had already agreed "in general terms on a plan that envisions American warplanes, Syrian insurgents and Turkish forces working together to sweep Islamic State militants from a 60-mile-long strip of northern Syria along the Turkish border... long-range artillery could be used across the border." Concerns were expressed that the plan would put allied warplanes closer than ever to areas that Syrian aircraft regularly bomb; the plan did not determine the reaction if Syrian warplanes attack allied personnel on the ground in what is Syrian territory. Turkish Prime minister Ahmet Davutoglu said the operations will continue as long as Turkey faces a threat, and discussed the situation with UN secretary-general Ban Ki-moon in a telephone call over the weekend of 26 July. The US said that Turkey "has a right to take action" against the PKK, a Kurdish insurrectionary group that has sought since 1984 autonomy from Turkey. A news report also disclosed prior to the 28 July meeting that Turkey had violated Iraqi airspace in its pursuit of the PKK.

The key section of the treaty is Article 5. Its commitment clause defines the "casus foederis". It commits each member state to consider an armed attack against "one" member state, in Europe or North America, to be an armed attack against "them all".

It has been invoked only once in NATO history: by the United States after the September 11 attacks in 2001. The invocation was confirmed on 4 October 2001, when NATO determined that the attacks were indeed eligible under the terms of the North Atlantic Treaty. The eight official actions taken by NATO in response to the 9/11 attacks included Operation Eagle Assist and Operation Active Endeavour, a naval operation in the Mediterranean which was designed to prevent the movement of terrorists or weapons of mass destruction, as well as enhancing the security of shipping in general. Active Endeavour began on 4 October 2001. It is a common misconception that NATO involvement in Afghanistan was a result of Article 5's invocation.

In April 2012, Turkish Prime Minister Tayyip Erdoğan considered invoking Article 5 of the NATO treaty to protect Turkish national security in a dispute over the Syrian Civil War. The alliance responded quickly and a spokesperson said the alliance was "monitoring the situation very closely and will continue to do so" and "takes it very seriously protecting its members." On April 17, Turkey said it would raise the issue quietly in the next NATO ministerial meeting. On April 29, the Syrian foreign ministry wrote that it had received Erdoğan's message, which he had repeated a few days before, loud and clear. On 25 June, the Turkish Deputy Prime Minister said that he intended to raise Article 5 at a specially-convened NATO meeting because of the downing of an "unarmed" Turkish military jet which was "13 sea miles" from Syria over "international waters" on a "solo mission to test domestic radar systems". A Syrian Foreign Ministry spokesman insisted that the plane "flying at an altitude of 100 meters inside the Syrian airspace in a clear breach of Syrian sovereignty" and that the "jet was shot down by anti-aircraft fire," the bullets of which "only have a range of 2.5 kilometers (1.5 miles)" rather than by radar-guided missile. On 5 August, Erdoğan stated, "The tomb of Suleyman Shah [in Syria] and the land surrounding it is our territory. We cannot ignore any unfavorable act against that monument, as it would be an attack on our territory, as well as an attack on NATO land... Everyone knows his duty, and will continue to do what is necessary." NATO Secretary-General Rasmussen later said in advance of the October 2012 ministerial meeting that the alliance was prepared to defend Turkey, and acknowledged that this border dispute concerned the alliance, but underlined the alliance's hesitancy over a possible intervention: "A military intervention can have unpredicted repercussions. Let me be very clear. We have no intention to interfere militarily [at present with Syria]." On 27 March 2014, recordings were released on YouTube of a conversation purportedly involving then Turkish foreign minister Ahmet Davutoğlu, Foreign Ministry Undersecretary Feridun Sinirlioğlu, then National Intelligence Organization (MİT) head Hakan Fidan, and Deputy Chief of General Staff General Yaşar Güler. The recording has been reported as being probably recorded at Davutoğlu's office at the Foreign Ministry on 13 March. Transcripts of the conversation reveal that as well as exploring the options for Turkish forces engaging in false flag operations inside Syria, the meeting involved a discussion about using the threat to the tomb as an excuse for Turkey to intervene militarily inside Syria. Davutoğlu stated that Erdoğan told him that he saw the threat to the tomb as an "opportunity".

Prior to the meeting of Defence Ministers and recently appointed Secretary-General Jens Stoltenberg at Brussels in late June 2015, it was stated by a journalist, who referenced an off-the-record interview with an official source, that "Entirely legal activities, such as running a pro-Moscow TV station, could become a broader assault on a country that would require a NATO response under Article Five of the Treaty... A final strategy is expected in October 2015." In another report, the journalist reported that "as part of the hardened stance, the UK has committed £750,000 of its money to support a counter-propaganda unit at NATO's headquarters in Brussels."

Article 6 states that the treaty covers only member states' territories in Europe and North America, and islands in the North Atlantic north of the Tropic of Cancer, plus French Algeria. It was the opinion in August 1965 of the US State Department, the US Defense Department and the legal division of NATO that an attack on the U.S. state of Hawaii would not trigger the treaty, but an attack on the other 49 would.

On 16 April 2003, NATO agreed to take command of the International Security Assistance Force (ISAF) in Afghanistan, which includes troops from 42 countries. The decision came at the request of Germany and the Netherlands, the two states leading ISAF at the time of the agreement, and all nineteen NATO ambassadors approved it unanimously. The handover of control to NATO took place on 11 August, and marked the first time in NATO's history that it took charge of a mission outside the north Atlantic area.

Three official footnotes are have been released to reflect the changes made since the treaty was written:


Regarding Article 6:


Regarding Article 11:






</doc>
<doc id="22009" url="https://en.wikipedia.org/wiki?curid=22009" title="Nitronium ion">
Nitronium ion

The nitronium ion, , is a cation. It is an onium ion because of its tetravalent nitrogen atom and +1 charge, similar in that regard to ammonium. It is created by the removal of an electron from the paramagnetic nitrogen dioxide molecule, or the protonation of nitric acid (with removal of HO).

It is stable enough to exist in normal conditions, but it is generally reactive and used extensively as an electrophile in the nitration of other substances. The ion is generated "in situ" for this purpose by mixing concentrated sulfuric acid and concentrated nitric acid according to the equilibrium:

The nitronium ion is isoelectronic with carbon dioxide and nitrous oxide, and has the same linear structure and bond angle of 180°. For this reason it has a similar vibrational spectrum to carbon dioxide. Historically, the nitronium ion was detected by Raman spectroscopy, because its symmetric stretch is Raman-active but infrared-inactive. The Raman-active symmetrical stretch was first used to identify the ion in nitrating mixtures.

A few stable nitronium salts with anions of weak nucleophilicity can be isolated. These include nitronium perchlorate (), nitronium tetrafluoroborate (), nitronium hexafluorophosphate (), nitronium hexafluoroarsenate (), and nitronium hexafluoroantimonate (). These are all very hygroscopic compounds.

The solid form of dinitrogen pentoxide, NO, actually consists of nitronium and nitrate ions, so it is an ionic compound, [][], not a molecular solid. However, dinitrogen pentoxide in liquid or gaseous state is molecular and does not contain nitronium ions.

The compounds nitryl fluoride, NOF, and nitryl chloride, NOCl, are not nitronium salts but molecular compounds, as shown by their low boiling points (−72 °C and −6 °C respectively) and short N–X bond lengths (N–F 135 pm, N–Cl 184 pm).

Addition of one electron forms the neutral nitryl radical, ; in fact, this is fairly stable and known as the compound nitrogen dioxide.

The related negatively charged species is , the nitrite ion.



</doc>
<doc id="22011" url="https://en.wikipedia.org/wiki?curid=22011" title="Neo Geo (system)">
Neo Geo (system)

The , stylised as NEO•GEO, also written as NEOGEO, is a cartridge-based arcade system board and fourth-generation home video game console released on April 26, 1990, by Japanese game company SNK Corporation. It was the first system in SNK's Neo Geo family. The Neo Geo was marketed as 24-bit; its CPU is technically a 16/32-bit 68000-based system with an 8/16-bit Z80 coprocessor, while its GPU chipset has a 24-bit graphics data bus.

The Neo Geo originally launched as the MVS (Multi Video System) coin-operated arcade machine. The MVS offers owners the ability to put up to six different cartridges into a single cabinet, a unique feature that was also a key economic consideration for operators with limited floorspace, as well as saving money long-term. With its games stored on self-contained cartridges, a game cabinet can be exchanged for a different game title by swapping the game's ROM cartridge and cabinet artwork. A home console version was also made, called AES (Advanced Entertainment System). It was originally launched as a rental console for video game stores in Japan (called Neo Geo Rental System), with its high price causing SNK not to release it for home use – this was later reversed due to high demand and it came into the market as a luxury console. The AES had the same raw specs as the MVS and had full compatibility, thus managed to bring a true arcade experience to home users. The Neo Geo was revived along with the brand overall in December 2012 through the introduction of the Neo Geo X handheld and home system.

The Neo Geo was a very powerful system when released, more powerful than any video game console at the time, and many arcade systems such as rival Capcom's CPS, which did not surpass it until the CP System II in 1993. The Neo Geo MVS was a success during the 1990s due to the cabinet's low cost, six ROM slots, and compact size. Several successful video game series were released for the platform, such as "Fatal Fury", "Art of Fighting", "Samurai Shodown", "The King of Fighters" and "Metal Slug". The AES had a very niche market in Japan, though sales were very low in the U.S. due to its high price for both the hardware and software; but it has since gained a cult following and is now considered a collectable. Neo Geo hardware production lasted seven years, discontinued in 1997, whereas game software production lasted until 2004, making Neo Geo the longest supported arcade system of all time. The AES console was succeeded by the Neo Geo CD and the MVS arcade by the Hyper Neo Geo 64. As of March 1997, the Neo Geo and the Neo Geo CD combined had sold 980,000 units worldwide. In 2009, the Neo Geo was ranked 19th out of the 25 best video game consoles of all time by video game website IGN.

The Neo Geo hardware was an evolution of an older SNK/Alpha Denshi M68000 arcade platform that was used in "Time Soldiers" in 1987, further developed in the SNK M68000 hardware platform as used for "" in 1988. Contrary to other popular arcade hardware of the time, the SNK/Alpha Denshi hardware used sprite strips instead of the more common tilemap based backgrounds. The Neo Geo hardware was essentially developed by Alpha Denshi's Eiji Fukatsu, adding sprite scaling through the use of scaling tables stored in ROM as well as support for a much higher amount of data on cartridges and better sound hardware.

The Neo Geo was announced on January 31, 1990 in Osaka, Japan and released on April 26, 1990. Initially, the AES home system was only available for rent to commercial establishments, such as hotel chains, bars and restaurants, and other venues. When customer response indicated that some gamers were willing to buy a console, SNK expanded sales and marketing into the home console market in 1991. 

Neo Geo's graphics and sound are largely superior to other contemporary home consoles, arcades, and even computers such as the Sharp X68000. 
Unlike earlier systems, the Neo Geo AES was intended to reproduce the same quality of game as the arcade MVS system. The MVS was one of the most powerful arcade units at the time, allowing the game ROM to be loaded from interchangeable cartridges instead of using custom, dedicated hardware cabinets for each game.

Early attempts to port arcade games to home systems had been limited by inferior hardware. In a well-known incident when Pac-Man was ported to the Atari 2600 many visual features of the original game had to be changed to compensate for the lack of ROM space and the hardware struggled when multiple ghosts appeared on the screen creating a flickering effect. The poor performance of the ported Pac-Man is cited as a cause of the video game crash of 1983.

In the United States, the console's debut price was planned to be and included two joystick controllers and a game: either "Baseball Stars Professional" or "NAM-1975". However, the price was raised and its American launch debuted as the Gold System at (). Later, the Gold System was bundled with "Magician Lord" and "Fatal Fury". The Silver System package, launched at , included one joystick controller and no pack-in game. Other games were launched at about and up. At double or quadruple the price of the competition, the console and its games were accessible only to a niche market. Although its high price tag kept it out of the mainstream gaming market, it outlasted the market lifespan of its more popular Nintendo and Sega rivals, and also made a unique niche market for it. In addition, its full compatibility meant that no additional money was being spent on porting or marketing for the AES, since the MVS' success was thus automatically feeding the AES, making the console profitable for SNK.

When realtime 3D graphics became the norm in the arcade industry, the Neo Geo's 2D hardware was unable to do likewise. Despite this, Neo Geo arcade games retained profitability through the mid-1990s, and the system was one of three 1995 recipients of the American Amusement Machine Association's Diamond Awards (which are based strictly on sales achievements). SNK developed a new console in 1994, called the Neo Geo CD. A new arcade was also made in 1997, called Hyper Neo Geo 64. However these two systems had low popularity and only a few games.

Despite the failures of these 3D hardware, and the ceasing of manufacturing home consoles by the end of 1997, SNK continued making software for the original 2D Neo Geo. Despite being very aged by the end of the decade, the Neo Geo continued getting popular releases, such as the critically acclaimed "The King of Fighters 2002". The last official game by SNK for the Neo Geo system, "Samurai Shodown V Special", was released in 2004, 14 years after the system's introduction.

On August 31, 2007, SNK stopped offering maintenance and repairs to Neo Geo home consoles, handhelds, and games.

The Neo Geo X, an officially licensed device with a collection of Neo Geo games pre-installed, was first released in 2012 by TOMMO Inc. After just one year and a lukewarm reception due to its price and poor quality of the emulation, on October 2nd, 2013, SNK Playmore terminated the license agreement and demanded an immediate cease and desist of distribution and sales of all licensed products.

In a 1993 review, "GamePro" gave the Neo Geo a "thumbs up". Though they voiced several criticisms, noting that the system was not as powerful as the soon-to-launch 3DO and had few releases which were not fighting games, they generally praised both the hardware and games library, and recommended that gamers who could not afford the console (which was still priced at $649.99) play the games in the arcade.

Each joystick controller is 280mm (width) × 190mm (depth) × 95mm (height) ( 11 × 8 × 2.5 in.) and contains the same four-button layout as the arcade MVS cabinet.

The arcade machines have a memory card system by which a player could save a game to return to at a later time and could also be used to continue play on the SNK home console of the same name.

The arcade version of the video game hardware is often referred to as the "MVS," or Multi Video System (available in 1-slot, 2-slot, 4-slot, and 6-slot variations, differing in the amount of game cartridges loaded into the machine at the time), with its console counterpart referred to as the "AES", or Advanced Entertainment System. Early motherboard revisions contain daughterboards, used to enhance the clarity of the video output.

The MVS and AES hardware can execute identical machine code. Owners can move EPROMs from one type to the other, and the game will still run. The program specifics for both MVS and AES game options are contained on every game ROM, whether the cartridge is intended for home or arcade use. However, the arcade and home cartridges do have a different pinout. They were designed this way to prevent arcade operators from buying the cheaper home carts and then using them in arcades. It has been found that in a few home version games, one could unlock the arcade version of the game by inputting a special code.

The original specification for ROM size is up to 330 megabits, hence the system displaying "Max 330 Mega Pro-Gear Spec" upon startup. While no technical advances were required to achieve it, some games over 100 megabits, such as "Top Hunter", followed this screen by displaying an animation proclaiming "The 100Mega Shock!". The original ROM size specification was later enhanced on cartridges with bank switching memory technology, increasing the maximum cartridge size to around 716 megabits. These new cartridges also cause the system to display "Giga Power Pro-Gear Spec" upon startup or during attract mode, indicating this enhancement.
The system uses seven different specialist processors, which divide the workload for the visuals, audio and gameplay.


RAM: 214 KB SRAM

On-board ROM: 512 KB

The SNK custom video chipset allows the system to draw sprites in vertical strips which are 16 pixels wide, and can be 16 to 512 pixels tall; it can draw up to 96 sprites per scanline for a total of 380 sprites on the screen at a time. Unlike most other video game consoles of its time, the Neo Geo does not use scrolling tilemap background layers. Instead, it has a single non-scrolling tilemap layer called the fix layer, while any scrolling layers rely exclusively on drawing sprites to create the scrolling backgrounds (like the Sega Y Board). By laying multiple sprites side by side, the system can simulate a tilemap background layer. The Neo Geo sprite system represents a step between conventional sprites and tilemaps.


The onboard Yamaha YM2610 sound chip gives the system 15 channels of sound.








The Neo Geo is the first home game console to feature a removable memory card for saved games.

The GameTap subscription service has included a Neo Geo emulator and a small library of Neo Geo games. In 2007 Nintendo announced that Neo Geo games would appear on the Wii's Virtual Console, starting with "", "Art of Fighting", and "World Heroes". Neo Geo games were released through Xbox Live Arcade and PlayStation Network, including "Fatal Fury Special", "Samurai Shodown II", "Metal Slug 3", "" and "The King of Fighters '98". Many Neo Geo games were released on the PlayStation 4, Xbox One, and Nintendo Switch through the Arcade Collection Archives (ACA) service. 

Homebrew activity began after the console's discontinuation, both by noncommercial hobbyists and commercially.

Neo Geo has a community of collectors. Because of the limited production runs received by cartridges amongst the sizable available arcade library, some of the rarest Neo Geo games can sell for well over $1,000. The most valuable game is the European AES version of "Kizuna Encounter". The MVS market provides a cheaper alternative to the expensive and rare home cartridges, and complete arcade kits are priced at a premium. It is also possible to play the MVS cartridges, which generally cost much less, on the AES home system through the use of adapters.




</doc>
<doc id="22012" url="https://en.wikipedia.org/wiki?curid=22012" title="Neo Geo CD">
Neo Geo CD

The is the second home video game console of SNK Corporation's Neo Geo family, released in September 9, 1994, four years after its cartridge-based equivalent. This is the same platform, converted to the cheaper CD format retailing at $49 to $79 per title, compared to the $300 cartridges. The system was originally priced at US$399, or £399 in the UK. The unit's 1× CD-ROM drive is slow, with very long loading times. The system can also play Audio CDs. All three versions of the system have no region-lock. The Neo Geo CD was launched bundled with a control pad instead of a joystick like the AES version. However, the original AES joystick can be used with all three Neo Geo CD models.

As of March 1997, there had been 570,000 Neo Geo CD units sold worldwide.

The Neo Geo CD was first unveiled at the 1994 Tokyo Toy Show. The console uses the same CPU set-up as the arcade and cartridge-based Neo Geo systems, facilitating conversions. SNK planned to release Neo Geo CD versions of every Neo Geo game still in the arcades.

Three versions of the Neo Geo CD were released:

The front-loading version is the original console design, with the top-loading version having been developed shortly before the Neo Geo CD launch as a scaled-down, cheaper alternative model. The CDZ was released on December 29, 1995 as the Japanese market replacement for SNK's previous efforts (the ""front loader"" and the ""top loader""). The Neo Geo CD had met with limited success due to it being plagued with slow loading times that could vary from 30 to 60 seconds between loads, depending on the game.

In response to criticism of the Neo Geo CD's long load times, SNK planned to produce a model with a double speed CD-ROM drive for North America, compared to the single speed drive of the Japanese and European models. However, the system missed its planned North American launch date of October 1995, and while SNK declined to give a specific reason for the delay, in their announcement of the new January 1996 launch date they stated that they had decided against using a double speed drive. Their Japanese division had produced an excess number of single speed units and found that modifying these units to double speed was more expensive than they had initially thought, so SNK opted to sell them as they were, postponing production of a double speed model until they had sold off the stock of single speed units. 

The CDZ was only officially sold in Japan during its production. However, its faster loading times, lack of a "region lock", and the fact that it could play older CD software, made it a popular import item for enthusiasts in both Europe and North America. The system's technical specs are identical to the previous models except that it includes a double-speed CD-ROM drive.

In response to reader inquiries about Neo Geo CD software, "GamePro" reported in an issue cover dated May 1997 that SNK had quietly discontinued the console by this time.

Criticism of the system's generally long loading times began even before launch; a report in "Electronic Gaming Monthly" on the Neo Geo CD's unveiling noted, "At the show, they were showing a demo of "Fatal Fury 2". The prototype of the machine that they showed was single speed, and the load time was 14-28 seconds between rounds. You can see that the screen[shot] on the right is a load screen."

Approximately one month after launch, SNK reported that they had sold the Neo Geo CD's entire initial shipment of 50,000 units.

Reviewing the Neo Geo CD in late 1995, "Next Generation" noted SNK's reputation for fun games but argued that their failure to upgrade the Neo Geo system with 3D capabilities would keep them from producing any truly "cutting edge" games, and limit the console to the same small cult following as the Neo Geo AES system although with less expensive games. They gave it 1 1/2 out of 5 stars.


The system is also capable of reading Redbook standard compact disc audio.

In addition to the multi-AV port (nearly identical to the one used on the Sega Genesis model 1, though they are not interchangeable), all Neo Geo CD models had composite RCA A/V and S-Video out jacks on the rear of the console.

The CD system's 56 Mbit / 7 MB of RAM was split accordingly:


While the Neo Geo CD library consists primarily of ports of MVS and AES titles, there are a few MVS arcade games which were not officially released for the Neo Geo AES and ported instead to the Neo Geo CD. These include "Puzzle Bobble", "Janshin Densetsu: Quest of Jongmaster" (a Mahjong game also released for the PC Engine), "Power Spikes II", "Neo Drift Out: New Technology", and "" ("Futsal: 5-on-5 Mini Soccer").

A few games which were unreleased in MVS and AES formats were also released exclusively for the Neo Geo CD. These include "Ironclad: Tesshō Rusha" ("Chōtetsu Burikingā", "BRIKIN'GER"), "Crossed Swords II", "ZinTrick" ("Oshidashi Zintorikku"), "ADK World", "Neo Geo CD Special", "The King of Fighters '96 Neo Collection", "Samurai Shodown RPG" ("Shinsetsu Samurai Spirits: Bushidō Retsuden"; an RPG spin-off of the "Samurai Shodown" series that was also released for the Sony PlayStation and Sega Saturn), and "Idol-Mahjong Final Romance 2" (an arcade game which is not an MVS game, but was ported directly to the Neo Geo CD).



</doc>
<doc id="22015" url="https://en.wikipedia.org/wiki?curid=22015" title="Neopets">
Neopets

Neopets (originally stylized NeoPets," and currently neopets") is a virtual pet website. Users can own virtual pets ("Neopets"), and buy virtual items for them using one of two virtual currencies. One currency, called Neopoints, can be earned within the site, and the other, Neocash, can either be purchased with real-world money, or won by chance in-game.

The website was launched by Adam Powell and Donna Williams in late 1999. Two years later, a consortium of investors led by Doug Dohring bought a controlling interest in the company and in June 2005, Viacom bought "Neopets" Inc. for US$160 million. On March 17, 2014, Viacom sold Neopets to JumpStart Games for an unannounced amount. On July 3, 2017, Chinese company NetDragon acquired JumpStart.

"Neopets" allows users to create and care for digital pets called "Neopets" and explore the virtual world of Neopia. There is no set objective for the users, but they are expected to feed and care for their Neopets when they grow hungry or ill. Neopets will not die if neglected, but their health can limit their gameplay. Neopets come in a variety of species and colors and users can create or adopt their own. Users can obtain items to interact with their Neopet, such as books to read and toys to play with them. Neopets can be customised with certain clothing items, paint brushes, transformation potions, and accessories. Users can build a customisable Neohome for their Neopets and furnish it with furniture, wallpaper, and flooring. Neopets can battle against other Neopets or non-player characters in the Battledome but they cannot die there.

Neopia is a virtual planet with fantasy lands inhabited by Neopets and other virtual creatures. Each land has a different theme, such as pirates or prehistory, and their own shops, games, and attractions. Neopia follows its own calendar and time zone, which runs concurrent with real-world Pacific Time, and has tie-ins with certain real-world holidays such as Halloween and Christmas. It has its own economy and stock market based on Neopoints. Users can earn Neopoints through various means including playing games and selling items, which can be invested or used to buy various virtual goods and services. While there is no set objective for users, interactive storylines are sometimes released that introduce changes to the planet such as new lands.

The site is regularly updated with features like new games, items and content. In addition to the site content updated by the "Neopets" staff members, users also contribute content to the site. User contributions come in the form of prescreened submissions and readily editable content that is automatically filtered, such as the site's weekly electronic newspaper "The Neopian Times". There are different types of submissions that will be accepted.

Users can earn Neopoints from playing games. Games come in many different genres, which include action, puzzles, and luck & chance. Most games have set maximum earnings or playtime. Players may also earn trophies and other awards from games if they score high enough or perform better than other users. Many single-player and multiplayer browser games are available. Users can also participate in contests and spotlights judged by staff to showcase the users' talents. Quests to retrieve items may also be performed for specific NPCs. Challenges may be made against other players or random players in a "World Challenge" for a prize piece and Neopoints from the jackpot for certain Flash games. Monthly competitions also exist for multiplayer games with four week-long elimination rounds.

The economy is based on Neopoints. Users can also exchange real money for Neocash, used exclusively for the NC Mall. Users can earn Neopoints through playing games, selling items, and other transactions. Once earned, they can be saved in the bank, used to buy items from other users or non-player character (NPC) shops, used to buy and sell stocks in the Neopian stock market called the NEODAQ, or used to buy various other things. Items can be bought from shops found throughout the world of Neopia that are run by NPCs who may allow bargaining. Users can open their own shops to sell items, sometimes after obtaining those items at a lower price from sources such as other shops or charities. Items may also be exchanged through trading or auctions.

"Neopets" has a community for users to chat with and contact other users. Each user has their own profile they can edit with HTML and CSS and are represented by avatars provided by the website, as users cannot upload their own. Most avatars must be "unlocked" by completing certain in-game tasks, such as winning a contest or getting a high score on a game.

Users may request other users to be "Neofriends" or block other users from contacting them. To comply with COPPA, users under 13 years of age cannot access any of the site's communication features without sending in parental consent via fax. The main features include:


Discussions through these features are restricted and may not involve topics such as dating and romance or controversial topics like politics and religion. Continuous moderation is performed by paid "Neopets" staff members, and users can help moderate the site by reporting messages they believe are inappropriate or offensive. Messages are also automatically filtered to prevent users from posting messages with profanity or lewd content.

"Neopets" was conceived in 1997 by Adam Powell, a British student at the University of Nottingham at the time. He shared this idea with Donna Williams and the two started work on the site in September 1999, with Powell responsible for the programming and the database and Williams the web design and art. The site launched on November 15, 1999 from offices in Portsmouth Road, Guildford, a location still commemorated on the site. Powell stated that the original goal was to "keep university students entertained, and possibly make some cash from banner advertising". The site contained popular culture references, such as a Neopet that was simply a picture of Bruce Forsyth.

The user base grew by word of mouth and by Christmas 1999, "Neopets" was logging 600,000 page views daily and sought investors to cover the high cost of running the site. Later in the month, American businessman Doug Dohring was introduced to the creators of the site and, along with other investors, bought a majority share in January of the following year. Dohring founded "Neopets, Inc." in February 2000 and began business on April 28. Intellectual property that did not belong to "Neopets" was removed but the site kept the British spellings. The website made money from the first paying customers using an advertising method trademarked as "immersive advertising" and touted as "an evolutionary step forward in the traditional marketing practice of product placement" in television and film. In 2004, "Neopets" released a premium version and started showing advertisements on the basic site that were not shown to premium members.

Media conglomerate Viacom purchased "Neopets, Inc." on June 20, 2005 for $160 million and announced plans to focus more on the use of banner ads over the site's existing immersive advertising. The website was redesigned on April 27, 2007 and included changes to the user interface and the ability to customise Neopets. In June, Viacom promoted "Neopets" through minishows on Nickelodeon. Promotions included the second Altador Cup and led to an increase in traffic through the site. The first Altador Cup was released as an international online gaming event to coincide with the 2006 FIFA World Cup to improve interactivity between users and had 10.4 million participants the first year. On July 17, the NC Mall was launched in a partnership with Korean gaming company Nexon Corporation. It allowed users to use real money to purchase Neocash to buy exclusive virtual items. On June 17, 2008, Viacom formed the Nickelodeon Kids & Family Virtual Worlds Group to "encompass all paid and subscription gaming initiatives across all relevant platforms", including "Neopets". By June 2011, "Neopets" announced that the website had logged 1 trillion page views since its creation.

In July 2009, the "Neopets" site was the target of an identity theft hacking scheme that attempted to trick users into clicking a link that would allow them to gain items or Neopoints. Upon doing so, malware was installed onto the user's computer. According to reports, the hack was aimed not at child players' "Neopets" accounts, but at using the malware to steal the financial data and identities of their parents. Viacom stated that it was investigating the issue, and that the hack was a version of social engineering rather than an "indictment of Neopets security practices". In an on-site newsletter for players, "Neopets" denied the report and claimed that the site's security measures prevented the posting of such links.

JumpStart acquired Neopets from Viacom in April 2014. Server migration began in September. JumpStart-owned Neopets was immediately characterized by glitches and site lag. On 6 March 2015, much of the Neopets Team remaining from Viacom were laid off. On July 3, 2017, Chinese company NetDragon acquired JumpStart.

On the weekend of 27–28 June 2015, the site's chat filters, designed to prevent adult language and content, stopped working. The site's forums and other user-edited spaces were flooded with adult content and obscene images.

In a statement on Facebook, JumpStart apologized, explaining that the issue was due to a "facility move," and that during that move, the moderation team was not able to access the Neopets community.

In 2016, Motherboard reported that the login data of 70 million Neopets accounts was stolen. It contained not only usernames and passwords but also email addresses, birth dates, IP addresses, and PINs. It turned out this information was being stored in plain text by Neopets and had first been retrieved in 2012; every single account created prior to that year was affected. Neopets responded by posting about the leak on their official Facebook page and sent emails out to all affected players telling them to change their passwords.

In the 2000s, "Neopets" was consistently noted as one of the "stickiest" sites for children's entertainment. Stickiness is a measure of the average amount of time spent on a website. A press release from "Neopets" in 2001 stated that Neopets.com led in site "stickiness" in May and June, with the average user spending 117 minutes a week. "Neopets" also led in the average number of hours spent per user per month in December 2003 with an average of 4 hours and 47 minutes. A 2004 article stated that Nielsen//NetRatings reported that people were spending around three hours a month on "Neopets", more than any other site in its Nielsen category. By May 2005, a "Neopets"-affiliated video game producer cited about 35 million unique users, 11 million unique IP addresses per month, and 4 billion web page views per month. This producer also described 20% of the users as 18 or older, with the median of the remaining 80% at about 14. "Neopets" was consistently ranked among the top ten "stickiest" sites by both Nielsen//NetRatings and comScore Media Metrix in 2005 and 2006. According to Nielsen//NetRatings, in 2007, "Neopets" lost about 15% of its audience over the previous year. In February 2008, comScore ranked it as the stickiest kids entertainment site with the average user spending 2 hours and 45 minutes per month.

Described as an online cross of "Pokémon" and "Tamagotchi", "Neopets" has received both praise and criticism. It has been praised for having educational content. Children can learn HTML to edit their own pages. They can also learn how to handle money by participating in the economy. Reviews from About.com and MMO Hut considered the multitude of possible activities a positive aspect. Most of the users are female, higher than in other massively multiplayer online games (MMOGs) but equivalent to social-networking-driven communities. Lucy Bradshaw, a vice president of Electronic Arts, attributes the popularity among girls to the openness of the site and said, "Games that have a tendency to satisfy on more than one dimension have a tendency to have a broader appeal and attract girls".

Luck & chance games draw criticism from parents as they introduce children to gambling. In Australia, a cross-promotion with McDonald's led to controversy with "Neopets"' luck/chance games in October 2004. Australian tabloid television show "Today Tonight" featured a nine-year-old boy who claimed the site requires one to gamble in order to earn enough Neopoints to feed one's Neopet or else it would be sent to the pound. While gambling is not required, nor are pets sent to the pound if unfed, the website includes games of chance based on real games such as blackjack and lottery scratchcards. After this incident, "Neopets" prohibited users under the age of 13 from playing most games that involve gambling.

Immersive advertising is a trademarked term for the way "Neopets" displayed advertisements to generate profit after Doug Dohring bought the site. Unlike pop-up and banner ads, immersive ads integrate advertisements into the site's content in interactive forms, including games and items. Players could earn Neopoints from them by playing advergames and taking part in online marketing surveys. Prior to the arrival of the NC Mall, it contributed to 60% of the revenue from the site with paying Fortune 1000 companies including Disney, General Mills, and McDonald's.

It was a contentious issue with the site with regard to the ethics of marketing to children. It drew criticism from parents, psychologists, and consumer advocates who argued that children may not know that they are being advertised to, as it blurred the line between site content and advertisement. Children under eight had difficulty recognizing ads and half a million of the 25 million users were under the age of eight in 2005. Dohring responded to such criticism stating that of the 40 percent of users twelve and younger, very few were seven or eight years old and that preschoolers were not their target audience.

Others criticised the functionality of the site. Susan Linn, another psychologist and author of "Consuming Kids: The Hostile Takeover of Childhood" considered the purpose of this site was to keep children in front of advertisements. Kalle Lasn, editor-in-chief and co-founder of "Adbusters" magazine, said the site encouraged kids to spend hours in front of a screen and recruited them to consumerism. "Neopets" executives stated that paid content comprised less than 1% of the site's total content. Children were not required to play or use sponsor games and items, and all ads were marked as such.

After Neopets was purchased by Viacom, and due to criticism towards the immersive advertising model, banner advertisements and other more clear-cut forms of advertising were integrated into the site. Neocash (discussed elsewhere in this article) was also introduced as an additional revenue source.

The popularity of "Neopets" spawned real-world merchandise including stickers, books, cereals, video games and more, sold at mainstream outlets and online retailers. The most common items were plushies (stuffed animals). Each piece of merchandise has a code which can be redeemed at the site for an in-game reward. "Neopets, Inc." had always planned to "bring the online and offline worlds together in ways that have never been done before". An investment banker at Allen & Company in New York said that "Neopets" was the only online media he had seen "that might have the ability to capture market share in the offline world". Merchandise has been almost entirely discontinued save for a line of enamel pins released by Overpowered in 2018.

Neopets, Inc. signed various licensing deals with companies such as Viacom Consumer Products, Thinkway Toys, and Jakks Pacific over the years. Wizards of the Coast released the "Neopets Trading Card Game" in September 2003, which was promoted in three of General Mills "Big G" cereals and ten Simon Property Group malls. It received two different nominations for "Toy of the Year" as well as other recognitions. "Neopets: The Official Magazine" was a bi-monthly magazine released the same month but it was replaced in 2008 by "Beckett Plushie Pals", which featured "Neopets" news as well as other companies' products such as Webkinz.

In 2005, "Neopets" expanded to film and video game deals. The first movie was to be written by Ron Lieber and produced by Dylan Sellers and John A. Davis, but the project has since been cancelled with no other projects announced. Two video games were released by Sony Computer Entertainment, "" for the PlayStation 2 in 2005 and "" for the PlayStation Portable in 2006. 2006 also saw the release of "Neopets" mobile, which allowed users to visit the new land of Lutari Island, but it was discontinued on June 30, 2009, leaving the island completely inaccessible. In 2007, MumboJumbo developed a match-3 game "Neopets: Codestone Quest". "Neopets" wouldn't attempt another mobile game until 2015, when they released a match-3 game called "Ghoul Catchers" for Android and iOS. However, they still released other games during those years, including "Treasure Keepers" for Facebook and "Puzzle Adventure" for Nintendo DS, Wii, and PC. In early 2019, Jumpstart announced that they were making a full mobile-friendly "Neopets" app, and it was scheduled to launch late in the summer of 2019.




</doc>
